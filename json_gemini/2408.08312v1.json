{"title": "HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning", "authors": ["Hongyu Li", "Snehal Dikhale", "Jinda Cui", "Soshi Iba", "Nawid Jamali"], "abstract": "To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer.", "sections": [{"title": "I. INTRODUCTION", "content": "Tactile sensing is a critical modality for humans to interact with everyday objects [1]. Tactile sensors can be divided into two broad categories [2]: vision-based [3, 4], and taxel-based [5-7]. Recently, vision-based tactile sensors have gained popularity, partly due to their pixel-based representation, which makes them amenable to deep learning approaches [8\u201310]. However, their size limits full coverage on multi-fingered hands [11, 12]. In contrast, taxel-based sensors remain underexplored because they present many challenges to deep learning approaches, including low spatial resolution and a lack of consensus on how to represent and process taxel-based sensors. However, they continue to remain of interest to the robotic manipulation community due to their unique ability to directly respond to the underlying phenomena measured, thereby offering valuable opportunities for enhancing robotic manipulation [13, 14].\nThe encoding and processing of taxel signals is still an open research question. Taxel-based sensors present unique challenges before they can be used in downstream tasks. These challenges include 1) the development of effective representations for tactile sensor data, and 2) their inherently low resolution [2, 12, 15], which has been a long-standing barrier hindering tactile dexterous manipulation [15] and perception, such as in-hand 6D pose estimation [16-18]. As suggested by Dahiya et al. [15], one promising line of research is the use of super-resolution algorithms.\nIn this paper, we present a two-stage solution to address the aforementioned challenges. In the first stage, we propose a method for learning a representation of the tactile signals in an embedding space using contrastive learning. This approach generalizes across various taxel layouts, different objects, and multiple tasks. Our key intuition is that by exploiting the correspondence between the taxel signals and their contact surface, we can learn a geometrically-informative representation. To this end, we propose graphs to represent the tactile signals, with a novel graph construction strategy and convolution kernel for tactile processing.\nIn the second stage, we map low-resolution taxel signals to a high-resolution three-dimensional surface using a multi-contact strategy to reduce uncertainty in taxel signals, a process we term hyper-resolution. Unlike super-resolution, which focuses on upscaling data within the same domain, hyper-resolution extends beyond mere upscaling within the same domain across different domains and modalities. For example, in image processing, super-resolution produces the same image with a higher resolution. However, hyper-resolution maps low-resolution taxel signals to capture various object properties, such as the three-dimensional surface of the object, surface texture, etc. This distinction allows hyper-resolution to provide informative data beneficial for tasks such as 6D pose estimation."}, {"title": "II. RELATED WORK", "content": "Representation learning is the process of encoding informative features from raw data to make it suitable for machine learning tasks. Most of the earlier works use supervised learning methods [20]. Recently, there has been a growing interest in self-supervised learning [21-23] and multi-modal learning [24]. While most of the prior studies focus on domains such as vision [21-23] and language [24-26], in this paper, we are interested in whether the same paradigm can be applied in the tactile domain.\nSeveral prior works have investigated tactile representation learning. In the image-based sensor domain, Villalonga et al. [27] leverage the contrastive framework MoCo [22], and Caddeo et al. [28] utilize an autoencoder to learn a representation. However, their transfer from image-based to taxel-based data is non-trivial. Guzey et al. [14] learn a representation for taxel-based sensors using BYOL [21]. However, their primary focus is on dexterous manipulation, and they do not explore various downstream tasks or representation learning approaches. Therefore, the most effective paradigm for taxel-based signals remains a topic for further exploration.\nContact localization has been utilized to achieve tactile super-resolution. In contact localization, the goal is to estimate the contact location from a given tactile observation. Early works use probabilistic approaches to estimate the probability distribution of the contact location [29-32]. Piacenza et al. [12] adopt a data-driven approach for contact localization in 3D space. With the advancements in computer vision, recent works have increasingly explored vision-based tactile sensors [27, 28, 30, 33-35], which lend themselves well to deep learning techniques due to their pixel-based output. However, the fusion of taxel-based sensors with deep learning methodologies remains relatively underexplored.\nIn the seminal work Lepora et al. [36] propose taxel-based tactile super-resolution using the Bayesian perception method. Recently, there has been a shift from probabilistic-based approaches to learning-based ones. Wu et al. [2] propose a method wherein the taxel-based tactile signal is interpreted as a 2D image, subsequently enhanced using SRGAN [37]. This process results in a higher-resolution representation of the contact surface between the sensor and the object. However, interpreting tactile signals as a 2D image limits their application to 2D arrangements; the 3D arrangement found in curved fingers cannot be accurately represented. Moreover, they do not utilize the geometric information of the object in contact. In this paper, we present a geometrically-informed hyper-resolution algorithm invariant to sensor arrangements."}, {"title": "III. METHODOLOGY", "content": "Given a sparse taxel-based tactile signal, our goal is to obtain a high-resolution depiction of the contact surface between the sensor and the object of interest. To achieve this, we propose a two-stage solution. The first stage, representation learning, involves using a graph neural network and contrastive learning to learn a geometrically-informed representation of the tactile signals. The second stage, hyper-resolution, uses the learned representation to map low-resolution taxel signals into a high-resolution contact surface using multi-contact localization."}, {"title": "A. Representation Learning", "content": "Figure 2 shows an overview of the proposed tactile representation learning framework. In this section, we detail the different components of our framework.\n1) Taxel Representaion: Tactile data can be represented as point clouds, images, or graphs. The point cloud representation, however, fails to encode the absence of contact, and the image representation struggles to capture the 3D spatial arrangement of tactile sensors. In our research, we opted for the graph representation because it encodes both the spatial arrangement of the taxel signals and the absence of contact.\nWe use an undirected spatial graph $G = (V, E)$ to represent the taxel data, where $V$ and $E$ are the vertices and edges of the graph, respectively. Each vertex corresponds to a taxel of the tactile sensor, and each edge represents the spatial proximity between two taxels. The vertices have two types of features: the 3D coordinates of each taxel, $X \\in \\mathbb{R}^{t \\times 3}$, and their corresponding signals, $K \\in \\mathbb{R}^t$, where $t$ is the number of taxels in the tactile sensors. For taxel signals with three axes, $K$ is considered the Euclidean norm of the signals from all three axes. We combine $X$ and $K$ into a matrix $V = [X|K]$ of dimension $\\mathbb{R}^{t \\times 4}$. To improve sim-to-real transfer, we simplify the taxel signal into a boolean activation state [38, 39].\nEdges $E$ connect the taxel vertices $V$ in the taxel graph and construct the graph. Since we are considering tactile sensors with arbitrary spatial arrangements, we propose that tactile message passing should be relative to the spatial distance. We use a radius graph to construct $E$. In this graph, an edge connects two vertices if their distance is within a certain radius. This approach ensures that the interaction between sensors is stronger when they are closer to each other.\n2) Tactile Encoder $E_t$: We process the constructed taxel graph using a graph neural network (GNN). Specifically, the taxel graph passes through three message passing layers [40], a pooling layer, and a non-linear layer output head. The message-passing layer is defined as\n$n'_i = \\gamma(n_i, \\bigoplus_{j \\in N(i)} \\phi(n_i, n_j, e_{j,i})),$  (1)\nwhere $n_i$ is the vertex feature, and $e_{j,i}$ is the edge feature between vertices $i$ and $j$. $\\bigoplus_{j \\in N(i)}$ is a differentiable aggregation function, such as maximum or summation, and $\\gamma$ and $\\phi$ are two differentiable functions such as MLPs. Our observation is that the taxel signals rely on relative features with respect to their neighbors instead of absolute features. For example, a 4 \u00d7 4 taxel pad with evenly high activation signals and evenly low activation signals should represent the same contact surface (flat surface). Therefore, we propose to use the EdgeConv operator [41], which leverages the relative features between vertices $n_i$ and $n_j$:\n$n'_i = \\sum_{j \\in N(i)} \\phi(x_i, h_i, x_j - x_i, h_j - h_i)),$ (2)\nwhere $h_i$ and $h_j$ are the hidden features of node i and j.\n3) Sensor-Object Contact Surface Representation: To map the low-resolution tactile signals to the high-resolution surface shape, we need to represent the contact surface between the sensor and the object. To this end, we represent the sensor-object contact surface as a cube encapsulating the object's surface that is in contact with the sensor. The cube's height and width are set to the sensor's dimensions, and its depth, $d_p$, represents the penetration into the object's surface. In our experiments, we set $d_p = 0.8$ cm, as lower values of $d_p$ increased the risk of mesh collision during initialization. The intersection between the object O and this cube is referred to as a contact surface patch. The contact surface patch captures the local geometry of the object at the contact point, and can be used to learn a correspondence between the low-resolution tactile signal and the high-resolution object surface shape. We view the contact surface patch as the hyper-resolution space of that respective tactile sensor.\n4) Learning the Tactile Representation: We propose the contrastive learning framework shown in Fig. 2 to learn the tactile representation that leverages the inherent relationship between the contact surface and the tactile signals. For example, when the sensor is pressed against a flat surface, the taxel signals should demonstrate the flatness feature. On the contrary, when the sensor is pressed against a curved surface, the signals should demonstrate the curvature feature and distinguish itself from the flatness feature. Our key insight for learning an effective representation of tactile signals is to exploit this correspondence.\nInspired by the vision-language learning framework CLIP [24], we draw $N$ random pairs of tactile sensor signals (the blue box) and corresponding contact surfaces (the red box) for each data sample. While the CLIP framework is typically used for visual-language tasks, we adapt it for tactile representation learning. This adaptation requires two significant modifications to the original formulation. 1) Due to the lack of an existing dataset for taxel-based tactile sensors, we need to collect the required paired data. 2) Since the original encoders (vision and language) are incompatible with taxel signals, we need to design a neural network model to encode taxel signals.\nThe tactile graph described in Section III-A.1 is passed to the tactile encoder $E_t$ (Sec. III-A.2) and encoded into tactile embedding $T \\in \\mathbb{R}^n$, where $n$ represents the embedding size. Second, the contact surface data (red box), represented as a point cloud, is encoded through the surface encoder $E_s$ into surface embedding $S \\in \\mathbb{R}^n$, which has the same size as the tactile embedding $\\mathbb{R}^n$. We choose PointNet [42] as our surface encoder. The embedding size $n$ is empirically set as 128.\nThe final step involves learning tactile representation. This is achieved by computing the dot product of the tactile embedding and surface embedding $T \\cdot S^T$, resulting in a $N \\times N$ matrix as depicted in the bottom right corner of Fig. 2. This matrix contains $N$ positive pairs and $N^2 - N$ negative pairs. The dot product operation measures the cosine similarity between the tactile embedding and surface embedding. We optimize both encoders using a symmetric cross-entropy loss [24] such that the $N \\times N$ matrix turns into an identity matrix $I_{N \\times N}$. By doing so, we learn a representation that brings matching pairs closer together and pushes non-matching pairs farther apart in the embedding space."}, {"title": "B. Multi-Contact Localization for Hyper-Resolution", "content": "We develop an innovative approach that used multi-contact localization to transform sparse touch data into detailed object surface geometry, thereby achieving hyper-resolution from tactile sensors with limited spatial resolution. Consider a collection of tactile patterns and their corresponding object surface details, analogous to a library of tactile experiences. Given the spatial sparsity inherent in these tactile patterns, confidently associating a tactile pattern with its object surface is a non-trivial task. To address this challenge, we reason over multiple simultaneous contacts with the object to increase confidence in our estimates.\nHaving a set of objects $O$, we first collect a contact database $B_o$ for each object $o \\in O$ offline, which consists of the contact surface patches $S_{o,c}$, the corresponding contact signals $C_o$, and the 6D poses of the sensor $P_{o,c}$ in a common frame of reference $F_c$. We omit the subscript o in the following paragraph for simplicity and note that these data are object o specific. For each sample $b_i$ in the database $B_o$, $b_i = (S_{c,i} \\in S_c, C_i \\in C_o, P_{c,i} \\in P_c)$. The 6D pose $p_{c,i} \\in \\mathbb{R}^7$ is represented as the concatenation of 3D translation $\\mathbb{R}^3$ and 3D rotation in quaternion form $\\mathbb{R}^4$. To ease online computation, we preprocess the contact surface patches $S_c$ and encode them into embeddings $S_t$ using the pretrained surface encoder $E_s$.\nDuring deployment, we assume there is $N_c$ number of sensors that are in contact with the object o. We denote $I$ as the set of these sensors and the actual pose of each sensor $j \\in I$ as $P_{u,j} \\in \\mathbb{R}^7$. The robot's forward kinematics is used to transform the sensor poses to a common frame of reference. Each sensor j has a sensor reading $d_j$ represented as the taxel graph (Sec. III-A.1). We encode the collection of sensor readings $D = \\{d_i | i \\in 1,2,\\ldots, N_c\\}$ into taxel embeddings T using tactile encoder $E_t$ such that $T = \\{T_i = E_t(d_i) | d_i \\in D\\}$. We measure the similarity between T and the surface embeddings $S_t$ stored in the database and rank the candidate poses $P_c$ accordingly. We take top $d_c$ candidates to reduce the computation in later steps and obtain a distribution of contact locations $\\Omega_i = P_c$ [28].\nFor any two sensors in contact $j_a, j_b \\in I$, we obtain their respective distribution $\\Omega_a, \\Omega_b$ for contact location candidates. We filter the pair-wise Euclidean distance between each candidate location $p_{c,a} \\in \\Omega_a, p_{c,b} \\in \\Omega_b$ using the actual sensor poses $P_a, P_b$:\n$\\Omega_{d,a} = \\{p_{c,a} | ||p_{c,a} - p_{c,b}|| - ||P_a - P_b|| \\leq \\delta_n\\}$ (3)\n$\\Omega_{d,b} = \\{p_{c,b} | ||p_{c,a} - p_{c,b}|| - ||P_a - P_b|| \\leq \\delta_n\\}$,\nresulting in two distance filtered sets $\\Omega_{d,a}$ and $\\Omega_{d,b}$. $\\delta_n$ is a threshold to offset noises, such as in calibration and forward kinematics. Repeating this operation on all $N_c$ sensors in contact, we obtain $N_c$ distance filter sets $\\Omega_{d,1}, \\Omega_{d,2}, \\ldots, \\Omega_{d,Nc}$.\nWe desire to find an optimal solution $\\Psi \\in \\Omega_{d,1} \\times \\Omega_{d,2} \\times \\cdots \\times \\Omega_{d,Nc}$ that maximizes the similarities between taxel embeddings T and the surface embeddings $S_t$. To achieve this, we build a multipartite graph K with $N_c$ partites. Each candidate left in $\\Omega_{d,a}$ is added as a node, and edges are added if the distance constraint (Eqn. 3) is satisfied. We use Paton's algorithm [43] to find the cycle $\\Psi$ with the largest joint probability."}, {"title": "IV. DATASETS", "content": "We collected two datasets using NVIDIA Isaac Sim for a subset of YCB objects [19]. We used an Allegro Hand equipped with XELA tactile sensors, resembling our real-world setup. The Allegro Hand has eleven 4 \u00d7 4 flat pad sensors, three 4 \u00d7 6 flat pad sensors, and four curved tip sensors (each has 30 taxels). The first dataset, Section IV-A, is a comprehensive database of tactile sensors interacting with the objects. This dataset serves as our tactile experience library and is used to evaluate tactile representation learning. The second dataset, Section IV-B, consists of the Allegro Hand holding an object and executing random trajectories. This dataset is used to evaluate the performance of our methods on a downstream task, namely the in-hand 6D pose estimation task."}, {"title": "A. Contact Database", "content": "We first construct a dataset that captures tactile experiences across the entire surface of an object at various points. The tactile sensor is simulated using the Contact Sensor provided by Isaac Sim. We sample 2048 points on the object mesh using Poisson disk. Each point corresponds to a 3D position $\\mathbb{R}^3$ and its respective surface normal $\\mathbb{R}^3$. Like previous works [27, 28, 34], we randomly chose a subset of points to collect our tactile experience. For each selected point, we align the tactile sensor's z-axis with the surface normal and conduct eight contact trials. In each trial, the sensor is rotated 45\u00b0. We start by positioning the sensor 2.5 cm away from the point and then gradually push it towards the surface along the normal. Once the sensor is in contact with the object, we collect the tactile observations and their corresponding poses. This process is repeated for each type of taxel sensor on the Allegro Hand, which includes 4 x 4,4 \u00d7 6, and curved tips, to compile a comprehensive contact database."}, {"title": "B. In-hand Object Dataset", "content": "To evaluate our framework on downstream tasks such as 6D in-hand pose estimation, we also collected a simulated dataset, which consists of the Allegro Hand holding an object and executing random trajectories. For each object, we collect 16,000 samples for the training set and 4,000 samples for the validation set. The dataset is collected using the following procedures:\n1) The hand is initialized to face upwards.\n2) The object is dropped from a height of 2cm above the hand, with its pose randomly initialized.\n3) The hand performs the grasping action.\n4) If the grasp fails (e.g., the object falls), return to step 1."}, {"title": "V. EXPERIMENTS", "content": "We utilize the AdamW [44] optimizer with a learning rate of 0.001. We pre-train the tactile encoder for 100 epochs using all objects and optimize the pose estimation model for each object for 500 epochs.\nWe begin by qualitatively assessing the tactile representation learned through our approach (Section V-A). Next, we examine the effectiveness of our method in the hyper-resolution task (Section V-B). We then ablate the chosen graph operators and constructors (Section V-C) and study the impact of multi-contact localization on hyper-resolution (Section V-D). In addition, we test our approach on two downstream tasks: in-hand object pose estimation (Section V-E) and surface classification (Section V-F)."}, {"title": "A. Qualitative Analysis of Learned Tactile Representation", "content": "To evaluate the quality of our learned tactile representation, we performed a qualitative analysis using visualizations of the tactile embeddings. We used the contact database (Section IV-A) to generate the tactile embeddings for each contact using our learned tactile encoder. We then applied principal component analysis (PCA) to reduce the dimensionality of the embeddings to 3, and used the resulting values as RGB colors for visualization.\nThe results, as depicted in Fig. 1, reveal that the tactile embeddings effectively capture the geometric features of the contact surface, such as flatness, curvature, and edges. For example, the flat surfaces on the Master Chef Can, Sugar Box, and Mustard Bottle are all represented in shades of purple and blue, while the curved surfaces are depicted in yellow and green. The edges of the Master Chef Can are highlighted in lime green, indicating a stark contrast between the neighboring points. Notably, the tactile embeddings demonstrate consistency across different objects and sensor types, demonstrating the generalization of our representation."}, {"title": "B. Hyper-Resolution Performance Evaluation", "content": "In this section, we evaluate the performance of our proposed hyper-resolution algorithm, which maps the low-resolution taxel signals to high-resolution contact surface patches using a contact database. We compare our method with two baselines: image-based approaches (CNN) [2, 14], and point-cloud-based approaches (PointNet) [17, 18].\nWe use two metrics to measure the accuracy of our hyper-resolution: Chamfer distance (CD) and rank. Chamfer distance computes the average minimum distance between two point sets, and reflects the geometric similarity between the estimated and ground truth contact surfaces. Rank measures the precision of identifying the correct surface based on the similarity between the tactile embeddings and the surface embeddings. The rank of an algorithm is then the average rank it assigns to the ground truth surface across all tactile contact points in the database in Section IV-A. A lower rank means a better performance. Table I shows the results of these experiments. We observe that our method outperforms both of the baselines on both metrics, demonstrating the effectiveness of our hyper-resolution algorithm."}, {"title": "C. Comparison of Graph Operators and Constructors", "content": "We ablate the impact of different graph operators and constructors on the quality of the learned tactile representation. We compared our proposed EdgeConv operator with two seminal works: TacGNN [45], GCN [46]. We also examined different graph constructors, such as KNN and radius graphs, with different parameters."}, {"title": "D. Effect of Multi-Contact Localization on Hyper-Resolution", "content": "In this section, we evaluate how the number of contacts affects our hyper-resolution algorithm. Taxel-based sensors perceive coarser geometry features, making it challenging to estimate the corresponding surface from a single observation. Previous studies [27, 28] have confirmed performance gains by incorporating multi-contacts on vision-based tactile sensors. In this study, we extend this concept to taxel-based sensors.\nFigure 6 shows the results of the quantitative analysis of this experiment. We observe that the CD decreases as the number of contacts increases, indicating that the hyper-resolution quality improves with more contacts. This is because more contacts provide more information and constraints about the object surface, reducing the ambiguity and uncertainty in the hyper-resolution.\nWe provide a visualization sample in Fig. 3. The second and third column shows the likelihood map of a single contact and multi-contact with the object in each row, respectively. Brighter colors indicate a higher likelihood. We notice the curved surfaces are accurately depicted with brighter colors, suggesting that the algorithm correctly identifies these contacts as originating from a curved surface. The third column shows the refined likelihood map after applying multi-contact reasoning. After multi-contact reasoning, the true contact areas are brightly colored while all other areas are dark, accurately pinpointing the potential origin of the tactile input on the object."}, {"title": "E. Effect of HyperTaxel on In-Hand 6D Pose Estimation", "content": "In this section, we evaluate the effectiveness of our approach by integrating it with ViTa [17], an existing visuotactile model for 6D pose estimation. ViTa uses visual and tactile data to represent the object's surface, but low-resolution tactile data can affect its performance. Fig. 7 shows the modified pipeline, which includes our hyper-resolution method to map the sparse low-resolution tactile data to a high-resolution object surface representation. This representation is then fed into the ViTa algorithm without any further changes. Following prior works [16, 17], we evaluate the performance using three metrics: position error (cm), angular error (deg), and ADD (cm). Position error is the L2 norm of the difference between the estimated and ground truth translation vectors, $||t - \\hat{t}||_2$. Angular error is the inverse cosine of the inner product of the estimated and ground truth quaternions, $\\cos^{-1}(2(\\hat{R}, R)^2 - 1)$, and ADD measures the pairwise distances between the 3D model points transformed using estimated and ground truth 6D poses, $\\frac{1}{m}\\sum_{x\\in 0} ||(\\hat{R}x + \\hat{T}) - (Rx + T)||$, where x is the 3D point, and m is the number of 3D points on the object model o.\nWe verify the performance of our proposed hyper-resolution algorithm in the synthetic pose estimation data collected in Sec. IV-B. Table III shows the results. We first compare the vision-only baseline DenseFusion [47] with the visuotactile baseline ViTa [17]. By adding tactile information, ViTa has a 1.67 degrees lower angular error and 0.03 cm lower position error. ViTa+HyperTaxel outperforms all of them. An object-wise analysis, depicted in Fig. 8, reveals enhancements for most objects. Notably, our approach faces challenges on power drill objects which reveals one potential limitation. Since our offline collected database relies on random sampling on the object model, our current choice of sample number might not capture the complex geometry of the power drill accurately. In the future, this limitation might be lifted by scaling up the samples on the object."}, {"title": "F. Real Robot Results", "content": "We deployed our model, trained on synthetic data on a multi-fingered gripper (Allegro Hand equipped with XELA tactile sensors) affixed to a Sawyer robot. The tactile sensors capture surface contact points on the object. We use real YCB objects to evaluate the performance of our framework in a real-world robot environment.\nA good representation should demonstrate ability to distinguish different surface types. We evaluate our representation on the surface classification task to demonstrate its ability to cluster tactile signals based on the geometric features of the contact surfaces, such as flatness and curvature. To conduct this experiment, we use a real-world object that has both flat and curved surfaces: the Master Chef can. As shown in Fig. 9, we press the tactile sensors (both the curved tip and the 4x4 flat pad) on different parts of the can. We collect the tactile signals from multiple contacts on each surface, covering the flat and curved areas as evenly as possible. A video demonstration of this process is available in our supplementary material.\nWe then encode the tactile signals using three representations: BYOL, AE, and ours. We also compare against directly using the raw data (Raw). We then apply the K-means clustering algorithm to classify them into two classes: flat and curved. We observe that our method outperforms the baselines on all metrics, indicating that our representation can effectively cluster the tactile signals based on the geometric features of the contact surfaces."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we presented a novel framework, Hyper-Taxel, for learning a geometrically-informed representation of taxel-based tactile signals to achieve hyper-resolution of contact surfaces between the sensor and the object. We introduced a graph-based representation of tactile signals and a contrastive learning objective to learn a correspondence between the low-resolution taxel signals and the high-resolution contact surfaces. We proposed a multi-contact localization algorithm to reduce the uncertainty and ambiguity in the taxel signals and map them to the object surface geometry. We conducted extensive experiments on synthetic and also presented real-world experiments and showed that our framework outperforms the baselines. We demonstrated that the learned representation can capture the geometric features of the contact surface and generalize across different objects and taxel arrangements. We also showed that the hyper-resolution algorithm can improve the performance of the visuotactile pose estimation model and enable robust sim-to-real transfer."}]}