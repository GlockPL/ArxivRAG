{"title": "ONE-PROMPT-ONE-STORY:\nFREE-LUNCH CONSISTENT TEXT-TO-IMAGE\nGENERATION USING A SINGLE PROMPT", "authors": ["Tao Liu", "Kai Wang", "Senmao Li", "Joost van de Weijer", "Fahad Shahbaz Khan", "Shiqi Yang", "Yaxing Wang", "Jian Yang", "Ming-Ming Cheng"], "abstract": "Text-to-image generation models can create high-quality images from input\nprompts. However, they struggle to support the consistent generation of identity-\npreserving requirements for storytelling. Existing approaches to this problem typ-\nically require extensive training in large datasets or additional modifications to\nthe original model architectures. This limits their applicability across different\ndomains and diverse diffusion model configurations. In this paper, we first ob-\nserve the inherent capability of language models, coined context consistency, to\ncomprehend identity through context with a single prompt. Drawing inspiration\nfrom the inherent context consistency, we propose a novel training-free method\nfor consistent text-to-image (T2I) generation, termed \u201cOne-Prompt-One-Story\u201d\n(1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a\nsingle input for T2I diffusion models, initially preserving character identities. We\nthen refine the generation process using two novel techniques: Singular-Value\nReweighting and Identity-Preserving Cross-Attention, ensuring better alignment\nwith the input description for each frame. In our experiments, we compare our\nmethod against various existing consistent T2I generation approaches to demon-\nstrate its effectiveness through quantitative metrics and qualitative assessments.\nCode is available at https://github.com/byliutao/1Prompt1Story.", "sections": [{"title": "INTRODUCTION", "content": "Text-based image generation (T2I) (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022)\naims to generate high-quality images from textual prompts, depicting various subjects in various\nscenes. The ability of T2I diffusion models to maintain subject consistency across a wide range of\nscenes is crucial for applications such as animation (Hu, 2024; Guo et al., 2024), storytelling (Yang\net al., 2024; Gong et al., 2023; Cheng et al., 2024), video generation models (Khachatryan et al.,\n2023; Blattmann et al., 2023) and other narrative-driven visual applications. However, achieving\nconsistent T2I generation remains a challenge for existing models, as shown in Fig. 1 (up).\n\nRecent studies tackle the challenge of maintaining subject consistency through diverse approaches.\nMost methods require time-consuming training on large datasets for clustering identities (Avrahami\net al., 2023), learning large mapping encoders (Gal et al., 2023b; Ruiz et al., 2024), or perform-\ning fine-tuning (Ryu, 2023; Kopiczko et al., 2024), which carries the risk of inducing language\ndrift (Heng & Soh, 2024; Wu et al., 2024a; Huang et al., 2024), etc. Several recent training-free\napproaches (Tewel et al., 2024; Zhou et al., 2024) demonstrate remarkable results in generating\nimages with consistent subjects by leveraging shared internal activations from the pre-trained mod-\nels. These methods require extensive memory resources or complex module designs to strengthen\nthe T2I diffusion model to generate satisfactory consistent images. However, they all neglect the\ninherent property of long prompts that identity information is implicitly maintained by context un-\nderstanding, which we refer to as the context consistency of language models. For example, the dog\nobject in \"A dog is watching the movie. Afterward, the dog is lying in the garden.\u201d can be easily un-\nderstood as the same without any confusion since it appears in the same paragraph and is connected"}, {"title": "RELATED WORK", "content": "T2I personalized generation. T2I personalization is also referred to T2I model adaptation. This\naims to adapt a given model to a new concept by providing a few images and binding the new\nconcept to a unique token. As a result, the adaptation model can generate various renditions of the\nnew concept. One of the most representative methods is DreamBooth (Ruiz et al., 2023), where\nthe pre-trained T2I model learns to bind a modified unique identifier to a specific subject given a\nfew images, while it also updates the T2I model parameters. Recent approaches (Kumari et al.,\n2023; Han et al., 2023b; Shi et al., 2023) follow this pipeline and further improve the quality of\nthe generation. Another representative, Textual Inversion (Gal et al., 2023a), focuses on learning\nnew concept tokens instead of fine-tuning the T2I generative models. Textual Inversion finds new\npseudo-words by conducting personalization in the text embedding space. The coming works (Dong\net al., 2022; Voynov et al., 2023; Han et al., 2023a; Zeng et al., 2024) follow similar techniques.\n\nConsistent T2I generation. Despite recent advances, T2I personalization methods often require ex-\ntensive training to effectively learn modifier tokens. This training process can be time-consuming,\nwhich limits their practical impact. More recently, there has been a shift towards developing consis-\ntent T2I generation approaches (Wang et al., 2024b;a), which can be considered a specialized form of\nT2I personalization. These methods mainly focus on generating human faces that possess semanti-\ncally similar attributes to the input images. Importantly, they aim to achieve this identity-preserving\nT2I generation without the need for additional fine-tuning. They mainly take advantage of PEFT\ntechniques (Ryu, 2023; Kopiczko et al., 2024) or pre-training with large datasets (Ruiz et al., 2024;\nXiao et al., 2023) to learn the image encoder to be customized in the semantic space. For exam-\nple, PhotoMaker (Li et al., 2023b) enhances its ability to extract identity embeddings by fine-tuning\npart of the transformer layers in the image encoder and merging the class and image embeddings.\nThe Chosen One (Avrahami et al., 2023) utilizes an identity clustering method to iteratively identify\nimages with a similar appearance from a set of images generated by identical prompts.\n\nHowever, most consistent T2I generation methods (Akdemir & Yanardag, 2024; Wang et al., 2024a)\nstill require training the parameters of the T2I models, sacrificing compatibility with existing pre-\ntrained community models, or fail to ensure high face fidelity. Additionally, as most of these sys-\ntems (Li et al., 2023b; Gal et al., 2023b; Ruiz et al., 2024) are designed specifically for human faces,\nthey encounter limitations when applied to non-human subjects. Even for the state-of-the-art ap-\nproaches, including StoryDiffusion (Zhou et al., 2024), The Chosen One (Avrahami et al., 2023)\nand ConsiStory (Tewel et al., 2024), they either require time-consuming iterative clustering or high\nmemory demand in generation to achieve identity consistency.\n\nStorytelling. Story generation (Li et al., 2019; Maharana et al., 2021), also referred to as sto-\nrytelling, is one of the active research directions that is highly related to character consistency.\nRecent researches (Tao et al., 2024; Wang et al., 2023) have integrated the prominent pre-trained\nT2I diffusion models (Rombach et al., 2022; Ramesh et al., 2022) and the majority of these ap-\nproaches require intense training over storytelling datasets. For example, Make-a-Story (Rahman\net al., 2023) introduces a visual memory module designed to capture and leverage contextual in-\nformation throughout the storytelling process. StoryDALL-E (Maharana et al., 2022) extends the\nstory generation paradigm to story continuation, using DALL-E capabilities to achieve substantial\nimprovements over previous GAN-based methodologies. Note that the story continuation shares\nsimilarities with consistent Text-to-Image generation by using reference images. However, current\nconsistent T2I generation methods prioritize preserving human face identities, whereas story contin-\nuation involves supporting various subjects or even multiple subjects within the generated images.\n\nIn this paper, our proposed consistent T2I framework, 1Prompt1Story, diverges significantly from\nprevious approaches in storytelling and consistent T2I generation methods. We explore the inherent"}, {"title": "METHOD", "content": "Consistent T2I generation aims to generate a set of images depicting consistent subjects in different\nscenarios using a set of prompts. These prompts start with an identity prompt, followed by the frame\nprompts for each subsequent visualization frame. In this section, we first empirically show that\ndifferent frame descriptions included in a concatenated prompt can maintain identity consistency\ndue to the inherent context consistency property of language models. We examine this observa-\ntion through comprehensive analyses in Sec. 3.1 and propose the basic Naive Prompt Reweight-\ning pipeline of our method IPrompt1Story. Following that, to ensure that each frame description\nwithin the prompt is expressed individually while diminishing the impact of other frame prompts,\nwe introduce Singular-Value Reweighting and Identity-Preserving Cross-Attention in Sec. 3.2. The\nillustration of IPrompt1Story is shown in Fig. 4 and Algorithm 1 in the Appendix."}, {"title": "CONTEXT CONSISTENCY", "content": "Latent Diffusion Models. We build our approach on the SDXL (Podell et al., 2023) model, a\nlatent diffusion model that contains two main components: an autoencoder (i.e., an encoder & and\na decoder D) and a diffusion model (i.e., \u0404\u03b8 parameterized by \u03b8). The model ee is trained with the\nfollowing loss function:\n\n$L_{LDM} := E_{zo \\sim \u00a3(x),\u20ac \\sim N(0,1),t \\sim Uniform(1,T)} [||\u20ac - 6_{\\theta} (z_t, t, T_{\\epsilon} (P))||_2^2]$,\n\nwhere e is a UNet that conditions on the latent variable zt, a timestep t ~ Uniform(1, T), and a\ntext embedding \u03c4\u03b5(P). In text-guided diffusion models, images are generated based on a textual\ncondition, with C = \u03c4\u03b5 (P) \u2208 RM \u00d7D, where M is the number of tokens, D is the feature dimension\nof each token, and \u03c4\u03b5 is the CLIP text encoder (Radford et al., 2021). For a given input, the model\neo (zt, t, C) produces a cross-attention map. Let fz, denote the feature map output from 60. We can\nobtain a query matrix Q = lo(fz\u2081) using the projection network lo. Similarly, the key matrix K\nis computed from the text embedding C using another projection network lx, such that K = lx(C).\nThe cross-attention map At is then calculated as: $A_t = softmax(Q \\cdot K^T /\\sqrt{d})$, where d is the\ndimension of the query and key matrices. The entry [At]ij represents the attention weight of the\nj-th token to the i-th token.\n\nProblem Setups. In the T2I diffusion models, the text embedding C = \u03c4\u03b5(P) \u2208 RM\u00d7D is with M\ntokens. The M tokens contain a start token [SOT], followed by |P| tokens corresponding to the\nprompt, and M - |P| \u2013 1 padding end tokens [EOT] . Previous consistent T2I generation works\n(Avrahami et al., 2023; Tewel et al., 2024; Zhou et al., 2024) generate images from a set of N\nprompts. This set of prompts starts with an identity prompt Po that describes the relevant attribute of\nthe subject and continues with multiple frame prompt Pi, where i = 1, . . ., N describes each frame\nscenario. However, this separate generation pipeline ignores the inherent language property, i.e., the\ncontext consistency, by which identity is consistently ensured by the context information inherent in\nlanguage models. This property stems from the self-attention mechanism within Transformer-based\ntext encoders (Radford et al., 2021; Vaswani et al., 2017), which allows learning the interaction\nbetween phrases in the text embedding space.\n\nIn the following, we analyze the context consistency under different prompt configurations in both\ntextual space and image space. Specifically, we refer to the conventional prompt setups as multi-\nprompt generation, which is commonly used in existing consistent T2I generation methods. The\nmulti-prompt generation uses N prompts separately for each generated frame, each sharing the same\nidentity prompt and the corresponding frame prompt as [Po; Pi], i \u2208 [1, N]. In contrast, our single-\nprompt generation concatenates all the prompts as [P0; P1; ... ; PN] for each frame generation,\nwhich we refer as the Prompt Consolidation (PCon)."}, {"title": "CONTEXT CONSISTENCY IN TEXT EMBEDDINGS", "content": "Empirically, we find that the frame prompt {Pi | i = 1,..., N} in the single-prompt generation\nsetup have relatively small semantic distances among each other in the textual embedding space,\nwhereas those across multi-prompt generation have comparatively larger distances. For instance,\nwe set the identity frame Po = \u201cA watercolor of a cute kitten", "in a garden": "dressed in a superhero cape", "wearing\na collar with a bell": "sitting in a basket\\\", and \\\"dressed in a cute sweater\", respectively. Under the\nmulti-prompt setup, each frame is generated by the text embedding defined as $C_i = T_{\\epsilon}([P_0; P_i]) =$\n$[c_{SOT}, c_{P_0}, c_{P_i}, c_{EOT}]$, (i = 1, ..., N), while the text embedding of the Prompt Consolidation in\nthe single-prompt case is $C = T_{\\epsilon} ([P_0; P_1; . . . ; P_N]) = [c_{SOT}, c_{P_0}, c_{P_1}, \u2026\u2026\u2026,c_{P_N},c_{EOT}]$.\n\nTo analyze the distances among the frame\nprompts, we extract ci from Ci for multi-\nprompt setup and apply t-SNE for 2D visual-\nization (Fig. 2-left). Similarly, we extract all\ncPi from C for the single-prompt setup (Fig. 2-\nleft). As can be observed, the text embeddings\nof frame prompts under the multi-prompt setup\nare widely distributed in the text representation\nspace (red dots) with an average Euclidean L2\ndistance of 71.25. In contrast, the embeddings\nin the single-prompt case exhibit more compact\ndistributions (blue dots), with a much smaller\naverage L2 distance of 46.42. We also per-\nformed a similar distance analysis on all prompt\nsets in our benchmark ConsiStory+. As shown\nin Fig.2-right, we can conclude a similar obser-\nvation that the frame prompts share more sim-\nilar semantic information and identity consis-\ntency within the single-prompt setup.\"\n    },\n    {\n      \"title\": \"CONTEXT CONSISTENCY IN IMAGE GENERATION\",\n      \"content\": \"To demonstrate that context consistency is also maintained in the image space, we further conducted\nimage generation experiments using the prompt example above. The images generated by the SDXL\nmodel with the multi-prompt configuration, as illustrated in Fig. 3 (left, the first row), show various\ncharacters that lack identity consistency. Instead, we use our proposed concatenated prompt P =\n[P0; P1; ... ;PN]. To generate the i-th frame (i = 1, ..., N), we reweight the ci corresponding\nto the desired frame prompt Pi by a magnification factor while rescaling the embeddings of the\nother frame prompts by a reduction factor. This modified text embedding is then imported to the\nT2I model to generate the frame image. We refer to this simplistic reweighting approach as Naive\nPrompt Reweighting (NPR). By this means, the T2I model synthesizes frame images with the same\nsubject identity. However, the backgrounds get blended among these frames, as shown in Fig. 3\n(left, the second row). By contrast, our full model 1Prompt1Story introduced in Sec. 3.2 generates\nimages with better consistent identity and text-image alignment for each frame prompt, as shown in\nFig. 3 (left, the last row).\n\nTo visualize identity similarity among images, we removed backgrounds using CarveKit (Selin,\n2023) and extracted visual features with DINO-v2 (Oquab et al., 2023; Darcet et al., 2023). These\nfeatures are then projected into the 2D space by t-SNE (Hinton & Roweis, 2002) (as shown in Fig. 3\n(mid)). Our complete approach 1Prompt1Story obviously obtains better identity consistency than\nthe other two comparison methods, while Naive Prompt Reweighting shows improvements over\nthe SDXL baseline. We also applied the analysis across our extended benchmark ConsiStory+ and\ncalculated the average pairwise distance, as shown in Fig. 3 (right). These results further consolidate\nour conclusion that the frame prompts in a single-prompt setup share more identity consistency than\nthe multi-prompt case.\"\n    },\n    {\n      \"title\": \"ONE-PROMPT-ONE-STORY\",\n      \"content\": \"As also observed from the above section, simply concatenating the prompts as Naive Prompt\nReweighting cannot guarantee that the generated images accurately reflect the frame prompt de-\nscriptions, for which we assume that the T2I model cannot accurately capture the correct partition\nof the concatenated prompt embeddings. Furthermore, the various semantics within the consoli-\ndated descriptions interact with each other (Chefer et al., 2023; Rassin et al., 2024). To mitigate\nthis issue, we propose additional techniques based on the Prompt Consolidation (PCon), namely\nSingular-Value Reweighting (SVR) and Identity-Preserving Cross-Attention (IPCA).\n\nSingular-Value Reweighting. After the Prompt Consolidation as C = \u03c4\u03b5([P0; P1;...;PN]) =\n[CSOT, cP0, CP1, ..., CPN, CEOT], we require the current frame prompt to be better expressed in\nthe T2I generation, which we denote as Pexp = Pj, (j = 1, ..., N). We also expect the remaining\nframes to be suppressed in the generation, which we denote as Psup = Pk, k \u2208 [1, N]\\{j}. Thus,\nthe N frame prompts of the subject description can be written as {Pexp,Psup}. As the [EOT]\ntoken contains significant semantic information (Li et al., 2023a; Wu et al., 2024b), the semantic\ninformation corresponding to Pexp, in both P; and [EOT], needs to be enhanced, while the semantic\ninformation corresponding to Psup, in Pk, k \u2260 j and [EOT], need to be suppressed. We extract\nthe token embeddings for both express and suppress sets as Xexp = [cPi,cEOT] and $X_{sup} =$\n[cP1,...,cPj\u22121,cPj+1,...,cPN,\u0108EOT].\n\nInspired by (Gu et al., 2014; Li et al., 2023a), we assume that the main singular values of Xexp\ncorrespond to the fundamental information of Pexp. We then perform SVD decomposition as:\n$X_{exp} = UE V^T$, where \u03a3 = diag(\u03c3\u03bf,\u03c31,\u2026,\u03c3nj), the singular values \u03c3\u03bf \u2265 \u00b7\u00b7\u00b7 \u2265 \u03c3\u03b7; 2.\nTo enhance the expression of the frame Pj, we introduce the augmentation for each singular value,\ntermed as SVR+ and formulated as:\n\n$\\hat{\\sigma} = \\beta e^{\\alpha \\sigma} * \\sigma$.\n\nwhere the symbol e is the exponential, \u03b1 and \u03b2 are parameters with positive numbers. We recover\nthe tokens as $\\hat{X}_{exp} = U\\hat{\u03a3}V^T$, with the updated $\u03a3 = diag(\\hat{\\sigma}_0, \\hat{\\sigma}_1,\u2026\u2026, \\hat{\\sigma}_{nj})$. The new prompt\nembedding is defined as $\\hat{X}_{exp} = [\\hat{c}_{P_i}, \\hat{c}_{EOT}]$, and $\\hat{C} = [c_{SOT},c_{P_0},\u2026\u2026,\\hat{c}_{P_i},\u2026\u2026\u2026, c_{P_N},\\hat{c}_{EOT}]$.\nNote that there is an updated $\\hat{X}_{sup} = [c_{P_1}, ..., c_{P_{j-1}}, c_{P_{j+1}},...,c_{P_N},\\hat{c}_{EOT}]$.\n\nSimilarly, we suppress the expression of the remaining frames. Since Xsup contains information\nrelated to multiple frames, the main singular values of SVD in &sup only capture a small portion\nof these descriptions, which may lead to insufficient weakening of such semantics (as shown in the\nAppendix of Fig. 11-right). Therefore, we propose to weaken each frame prompt in &sup separately.\nWe construct the matrix as $\\mathcal{X}_{sup}^k = [c_{P_k}, \\hat{c}_{EOT}]$,k \u2260 j to perform SVD with the singular values\"\n    },\n    {\n      \"title\": \"EXPERIMENTS\",\n      \"content\": \"Comparison Methods and Benchmark. We compare our method with the following consistent T2I\ngeneration approaches: BLIP-Diffusion (Li et al., 2024), Textual Inversion (TI)(Gal et al., 2023a),\"\n    },\n    {\n      \"title\": \"EXPERIMENTAL RESULTS\",\n      \"content\": \"Qualitative Comparison. In Fig. 5, we present the qualitative comparison results. Our method\n1Prompt1Story demonstrates well-balanced performance in several key aspects, including identity\npreservation, accurate frame descriptions, and diversity in the pose of objects. In contrast, other\nmethods exhibit shortcomings in one or more of these aspects. Specifically, PhotoMaker, ConsiS-\ntory, and StoryDiffusion all produce inconsistent identities for the subject \\\"dragon"}, {"title": "CONCLUSION", "content": "In this paper, we addressed the critical challenge of maintaining subject consistency in text-to-\nimage (T2I) generation by leveraging the inherent property of context consistency found in natu-\nral language. Our proposed method, One-Prompt-One-Story (1Prompt1Story), effectively utilizes"}, {"title": "APPENDIX", "content": "A BOARDER IMPACTS AND LIMITATIONS\n\nBoarder Impacts. The application of T2I models in consistent image generation offers exten-\nsive potential for various downstream applications, enabling the adaptation of images to different\ncontexts. In particular, synthesizing consistent characters has diverse applications, however, it is a\nchallenging task for diffusion models. Our IPrompt1Story can help the users customize their desired\ncharacters in different story scenarios, resulting in significant time and resource savings. Notably,\ncurrent methods have inherent limitations, as discussed in this paper. However, our model can serve\nas an intermediary solution while offering valuable insights for further advancements.\n\nLimitations. While our method 1Prompt1Story can achieve high-fidelity consistent T2I genera-\ntion, it is not free of limitations. Firstly, we have to know all the prompts in advance. Additionally,\nthe length of the input prompt is constrained by the maximum capacity of the text encoder. Al-\nthough we proposed a sliding window technique that facilitates infinite-length story generation in\nAppendix D.2, this approach may encounter issues where the identity of the generated images grad-\nually diverges and becomes less consistent.\n\nB IMPLEMENTATION DETAILS\n\nB.1 MODEL CONFIGURATIONS\n\nWe generate subject-consistent images by modifying text embeddings and cross-attention modules\nat inference time, without any training or optimization processes. Our primary base model is the pre-\ntrained Stable Diffusion XL (SDXL)3. SDXL has two text encoders: the CLIP L/14 encoder (Rad-\nford et al., 2021) and the OpenCLIP bigG/14 encoder (Cherti et al., 2023). We separately update the\ntext embeddings produced by each encoder. For Naive Prompt Reweighting, we multiply the text\nembedding corresponding to the frame prompt that needs to be expressed by a factor of 2, while the\ntext embedding corresponding to the frame prompts that need to be suppressed is multiplied by a\nfactor of 0.5, keeping the CEOT unchanged.\n\nIn our method, 1Prompt1Story, we set the parameters as follows: a = 0.01, \u03b2 = 0.05 in Eq.2,\nand a' = 0.01, \u03b2' = 1.0 in Eq.3. During the generation process, we initialize all frames with the\nsame noise and apply a dropout rate of 0.5 to the token features in K corresponding to Po. In the\nimplementation of IPCA, the concatenated K and V are derived from the original text embeddings\nprior to applying SVR. We design an attention mask where all values in the column corresponding\nto Pi, i \u2208 [1, N] are set to zero, while all other positions are set to one. The natural logarithm of this\nmask is then added to the original attention map. Our full algorithm is presented in Algorithm 1.\nFollowing (Tewel et al., 2024; Alaluf et al., 2024; Luo et al., 2023), we use Free-U (Si et al., 2024)\nto enhance the generation quality. All generated images based on SDXL are produced at a resolution\nof 1024 \u00d7 1024 using a Quadro RTX 3090 GPU with 24GB VRAM.\n\nB.2 BENCHMARK DETAILS\n\nTo evaluate the effectiveness of our method, we developed ConsiStory+, an extended prompt bench-\nmark based on ConsiStory (Tewel et al., 2024). We enhanced both the diversity and size of the\noriginal benchmark, which only comprised 100 sets of 5 prompts across 4 superclasses. Our ex-\npansion resulted in 200 sets, with each set containing between 5 and 10 prompts, categorized into\n8 superclasses: humans, animals, fantasy, inanimate, fairy tales, nature, technology, and foods. The\nextended prompt benchmark was generated using ChatGPT 4.0-turbo4, involving two main steps.\nFirst, we expanded the 100 prompt sets from the original benchmark, increasing each to a length of\n5 to 10 prompts, as shown in Fig. 9 (left). Then, we generated new prompt sets for each of the new\nsuperclasses, as illustrated in Fig. 9 (right). The prompt sets collected through these two steps were\ncombined to form our benchmark, ConsiStory+."}, {"title": "COMPARISON METHOD IMPLEMENTATIONS", "content": "We compare our method with all other approaches based on Stable Diffusion XL, except for BLIP-\nDiffusion (Li et al., 2024), which is based on Stable Diffusion v1.55. The DDIM steps is set to the\ndefault value in the open-source code of each method. Below are the third-party packages we used\nfor method implementations:\n\n\u2022 The unofficial implementation of Textual Inversion (TI) (Gal et al., 2023a) at https://\ngithub.com/oss-roettger/XL-Textual-Inversion.\n\u2022 The unofficial implementation of The Chosen One (Avrahami et al., 2023) at https://\ngithub.com/ZichengDuan/TheChosenOne."}, {"title": "ADDITIONAL ABLATION STUDY", "content": "C.1 ROBUSTNESS TO DIVERSE DESCRIPTION ORDERS\n\nTo validate the robustness of our method regarding the order of frame prompts, we used the same\nthree frame prompts: \"wearing a scarf in a meadow\", \"playing in the snow\", and \"at the edge of\na river\" to create six different sequences for images generation. The identity prompt was consis-\ntently set to \"a photo of a fox\" and each sequence used the same seed for a generation. As shown\nin Fig. 10, our method 1Prompt1Story generates images with identity consistency across different\norders. Furthermore, the content of the images generated from varying sequences is closely aligned\nwith the text descriptions, further demonstrating our method Singular-Value Reweighting effective-\nness in suppressing content of unrelated frame prompts."}, {"title": "Singular-Value Reweighting ANALYSIS", "content": "Our Singular-Value Reweighting algorithm comprises two successive components: SVR+ enhances\nthe frame prompts we wish to express, while SVR- iteratively weakens the frame prompts we aim to\nsuppress. In our experiments, we first apply SVR+, followed by SVR-. In particular, we found that\nperforming SVR- before SVR+ also yields similar results (see Fig. 11-left).\n\nIn the process of applying SVR-, we employed a strategy of iteratively suppressing each frame\nprompt. In fact, we could also concatenate the text embeddings corresponding to all frame prompts\nfor suppression. To explore this, we conducted further ablation study specifically on the SVR-\ncomponent. Assuming that we have n frames to generate, we discovered that merging the text\nembeddings corresponding to the n 1 frames we wish to suppress with cEOT and subsequently\nperforming the SVD decomposition does not effectively extract the main components of all frame\nprompts included in cEOT. Consequently, applying Eq. 3 to weaken the eigenvalues based on their\nmagnitude fails to adequately eliminate the descriptions of all suppressed frames. we refer to this as\n\"joint suppress\u201d, as illustrated in Fig. 11 (right, the first row). In contrast, if we handle each frame\nprompt to be suppressed individually and iteratively perform SVD and the operations from Eq. 3,\nwhich we term \u201citerative suppress\u201d, we can more effectively suppress all irrelevant frame prompts,\nas shown in Fig. 11 (right, the second row).\n\nIn our SVR, we enhance only the current frame prompt that needs to be expressed. An alternative\noption is to enhance the identity prompt simultaneously. We found that doing so can make the\nobject's identity more consistent; however, it also introduces some negative effects, the background\nand subject's pose appearing more similar across images, as shown in Fig. 12. Furthermore, to\ndemonstrate the role of the CEOT in SVR, we conducted an ablation study on the CEOT component.\nSpecifically, we kept the CEOT part of the text embedding unchanged during the SVR process and\nused this text embedding to generate images. As shown in Fig. 13, the results indicate that without\nperforming SVR on the CEOT, the backgrounds of different frame prompts tend to blend together."}, {"title": "Naive Prompt Reweighting ABLATION STUDY", "content": ""}]}