{"title": "Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution", "authors": ["Ankita Sinha", "Wendi Cui", "Kamalika Das", "Jiaxin Zhang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities; however, optimizing their prompts has historically prioritized performance metrics at the expense of crucial safety and security considerations. To overcome this shortcoming, we introduce \"Survival of the Safest\" (SOS), an innovative multi-objective prompt optimization framework that enhances both performance and security in LLMs simultaneously. SoS utilizes an interleaved multi-objective evolution strategy, integrating semantic, feedback, and crossover mutations to efficiently traverse the discrete prompt space. Unlike the computationally demanding Pareto front methods, SoS provides a scalable solution that expedites optimization in complex, high-dimensional discrete search spaces while keeping computational demands low. Our approach accommodates flexible weighting of objectives and generates a pool of optimized candidates, empowering users to select prompts that optimally meet their specific performance and security needs. Experimental evaluations across diverse benchmark datasets affirm SoS's efficacy in delivering high performance and notably enhancing safety and security compared to single-objective methods. This advancement marks a significant stride towards the deployment of LLM systems that are both high-performing and secure across varied industrial applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated impressive capabilities in a variety of fields (Bubeck et al., 2023; Yang et al., 2023). Nevertheless, their outputs can differ substantially depending on the phrasing of the input prompt, even when employing the same model (Pryzant et al., 2023; Honovich et al., 2022; Zhou et al., 2023; Fernando et al., 2023). In response to this challenge, recent studies have developed a range of techniques for automatically generating optimal prompts. These include gradient-based methods, evolutionary strategies, reinforcement learning (RL) approaches, and fine-tuning practices (Chen et al., 2023; Pryzant et al., 2023; Zhou et al., 2023; Deng et al., 2022; Li et al., 2023). Considering the complexity of natural language and the intricacy involved in optimization (Yang and Li, 2023a; Cui et al., 2024), these techniques typically focus on optimizing a single metric such as performance accuracy.\nWhile optimizing prompts for a specific objective often improves performance, this method can introduce substantial safety and security concerns when implemented in real-world applications (Zhou et al., 2024). Developing robust prompts that can resist adversarial attacks, such as prompt injection and privacy leakage, is crucial (Liu et al., 2024; Zhou et al., 2024; Yuan et al., 2024). Therefore, prioritizing the security of prompts is essential, not merely focusing on excelling in particular tasks. This is especially true in sensitive fields like finance, healthcare, criminal justice, and social services (Paulus et al., 2024; Yao et al., 2024). The growing awareness of potential safety risks linked with LLMs has led to heightened attention from"}, {"title": "2 Problem Formulation", "content": "Prompt Optimization (PO). Considering the task T specified by a dataset D = (Q, A) of input/output pairs, the LLM L produces the corresponding output A via prompting with the concatenation of prompt p and a given input Q, i.e., [p; Q]. The objective of prompt optimization is to design the best natural language prompt p* that maximizes the performance of L on T.\nMulti-objective PO. Multi-objective prompt optimization extends the above concept to scenarios across multiple objectives. Instead of seeking expensive Pareto-frontiers, we formulate the optimal prompt p* that performs best across these objectives O by assigning specific weights W and maximizing the weighted sum of the metric function F across all objectives,\n$p^* = arg \\underset{p \\in X}{max} E_{(Q,A)} [\\sum_{i=1}^{n} W_i f_i(p)],$ (1)\nwhere {W1, ..., wn} \u2208 W are the specific weights of different objectives {01, ..., On} \u2208 O such that \u03a3i=1 Wi = 1, wi \u2265 0, and {f1, ..., fn} \u2208 F are the specific metric function to evaluate each of objectives. X denotes the high-dimensional sample space for a natural language prompt.\nSecure Multi-objective PO. Specifically, we address our target problem by searching for the optimal and secure prompt p* given L that maximizes the performance towards a metric function K\u2208 F (e.g., KPI) without safety concerns, measured by a score function S \u2208 F. This can be formally defined as the weighted sum of the metric function across both objectives, formulated as:\np = arg max E(Q,A) [W1 \u00b7 K(p) + W2 \u00b7 S(p)],\nPEX"}, {"title": "3 SoS: Survival of the Safest", "content": "Our proposed SoS framework leverages evolutionary principles to iteratively refine a set of prompts, aiming to discover solutions that excel across multiple, potentially orthogonal objectives. SoS comprises phases from prompt initialization, evolution mutation (semantic, feedback, and crossover), and selection, as shown in Fig. 2."}, {"title": "3.1 Evolution Operators", "content": "We introduce three mutation operators that are used in the SoS framework:\nSemantic Operator: It is a function operator Os for introducing controlled lexical variations into the existing candidate prompts while preserving the semantic meaning, see the meta-prompt details in Table 10 in Appendix.\nFeedback Operator: It typically consists of two LLM functional agents: a feedback generator, which analyzes past mistakes and provides improvement suggestions, and an feedback improver, which utilizes these suggestions to generate new candidates. In the multi-objective setting, each objective should have its dedicated feedback generator, allowing users to inject prior knowledge of how to succeed in this objective into the process. Specifically, we define two feedback operators: (1) security feedback operator Of and (2) KPI feedback operator OF. More details about the definition can be found in Table 7-9 in Appendix.\nCrossover Operator: It is a function operator Oc that takes two parent candidates to generate a new offspring candidate that shares traits from both parents, with potential superior performance. Example prompts can be found in Table 6."}, {"title": "3.2 SoS Framework", "content": "Prompt Initialization. SoS starts with a simple prompt as its initial input, which allows users to incorporate prior information or human-expert knowledge. Then SoS employs semantic mutation operator Os to generate a batch of random candidate prompts, aiming to enhance diversity while preserving the original intent. We select the better initial prompt as the starting point, to accelerate the convergence of subsequent optimization steps.\nPrompt Selection. Prompt selection is responsible for identifying a subset of promising prompts for further refinement. Rather than applying evolutionary steps to the entire population set, we strategically select a subset of locally optimal prompts. This approach focuses computational resources on the most promising candidates, promoting efficient exploration of the prompt, and maintaining a balance between optimizing each objective and steering towards the final target state.\nDefinition 1. Locally-optimal Prompt: A prompt p* is defined as locally optimal with respect to"}, {"title": "Prompt Evolution.", "content": "As shown in Fig.2, we propose to utilize feedback mutations (O, OF) repeatedly in an interleaved manner for each objective until there is no performance gain, defined as an improvement above a threshold of for the best candidate. We named this strategy as exhaustive-interleaved evolution that ensures sufficient optimization for each objective. The interleaved pattern allows objectives to build on top of each other, achieving a balanced optimization towards the target state. Fig. 3 (right) shows the evolution of KPI and security objectives during iteration through exhaustive-interleaved strategy.\nBeyond the exhaustive-interleaved evolution, we also investigate two possible alternatives for comparison: (1) Sequential-interleaved evolution, shown in Figure 3-left-(b), that employs feedback mutator interactively to optimize security and KPI in turn without running to convergence for each objective. This way may result in unstable performance gain due to insufficient improvement opportunities. (2) Parallel evolution, shown in Figure 3-left-(c) that optimizes each objective independently and in parallel, with populations subsequently cross-mutated. This method resulted in unbalanced outcomes, failing to achieve multi-objective optimization. We provide the algorithm details of SoS with exhaustive-interleaved strategy in Algorithm 1."}, {"title": "Weighted Evaluation.", "content": "To ensure the final candidate meets the prioritized configuration of each objective, SoS implements a weight-based evaluation system. This system computes a holistic score for a candidate, representing its performance across all objectives, calculated by using Eq. (2). The default setting is the equal weight for each objective and reports the top-K (K=5) candidates by ranking the holistic score. We also adjust the weights and then rerank to check the sensitivity of assigned weights to each objective."}, {"title": "4 Experiments", "content": "4.1 Experiment setup\nDataset. We benchmark our methods on three instruction induction tasks Honovich et al. (2022): Sentiment Analysis, Orthography Analysis, Taxonomy of Animals, and three Big Bench Hard (BBH) (Suzgun et al., 2022) tasks: Disambiguation QA, Logical Five, and Color Reasoning. For each task, we have allocated 50 data points for evaluation and an equal number for testing. To evaluate safety and security, we utilize the SaladBench dataset (Li et al., 2024) and selected 150 data points, which are distributed equally across six distinct categories namely: (i) Representation Toxicity Harms, (ii) Misinformation Harms, (iii) Information Safety Harms, (iv) Malicious Use, (v) Human Autonomy Integrity Harms, and (vi) Socioeconomic Harms.\nBaselines. We evaluate SoS against a variety of LLM-based approaches that have achieved state-of-the-art performance in prompt optimization. (1) APE (Zhou et al., 2023): utilizes an iterative Monte Carlo Search strategy that emphasizes exploration. (2) PromptBreeder (Fernando et al., 2023) and (3) PhaseEvo (Cui et al., 2024): connect LLMs with evolution algorithms (EAs) to tackle prompt optimization tasks. (4) InstructZero (Chen et al., 2023): convert the instruction to a soft prompt and then optimize by Bayesian optimization. More experimental details are provided in Appendix A."}, {"title": "4.2 Main Results", "content": "Table 1 presents a comparison between SoS and single-objective baselines, which, while generally demonstrating robust performance, often fall short in achieving the security objective. The table presents the results for SoS under varying weights represented by a for security and 1 a for performance. PhaseEvo (Cui et al., 2024) remains the top performer in terms of KPI but shows notable disadvantages in security within sentiment and orthography tasks. In contrast, APE (Zhou et al., 2023) presents strong security results, yet its KPI scores are significantly lower for taxonomy tasks. PromptBreeder (Fernando et al., 2023) performs well in both sentiment and taxonomy tasks; however, it lags behind SoS in security, despite posting excellent KPI results. Notably, SoS consistently delivers superior and reliable outcomes in balancing both objectives. This underscores the need and effectiveness of adopting multi-objective approaches in prompt optimization.\nTable 2 shows the testing performance across various datasets, displaying results for the top 5 candidate prompts along with their corresponding performance on KPI and Security objectives. Note that the top-ranked candidate does not consistently yield the highest scores for each objective. Thus, we have compiled an optimal pool of candidates, ranked based on an overall holistic score that assigns equal weights, rather than solely reporting the highest-performing prompt. This approach provides users with multiple options, enabling them to choose the most suitable prompt based on their specific preference for each objective."}, {"title": "4.3 Analysis", "content": "Effects of LLM Models. To assess the general applicability of the SoS framework, we conducted end-to-end optimization tasks on various LLMs: GPT-3.5-turbo, Llama3-8B, and Mistral-7B. As detailed in Table 3, GPT-3.5-turbo achieves the highest performance in KPI and security objectives. Even though Llama3-8B and Mistral-7B display competitive security performance, their KPI outcomes remain slightly weak to those of GPT-3.5-turbo, which demonstrates a superior balance in multi-objective settings.\nEffect of Evolution Strategies. Table 4 provides empirical comparisons of various evolution strategies, namely exhaustive, parallel, and sequential. w\u2081 represents the weight allocated to the KPI objective, while 1 w\u2081 indicates the weight assigned to the security objective. We vary the weight settings from 1.0 to 0.0, collect a pool of candidates during the evolution process (as opposed to simply selecting the final top 5), and report the mean and variance of their holistic score, which is calculated by a weighted sum. We observe that the exhaustive interleaved strategy implemented by SoS consistently outperforms the other strategies by a considerable margin, with the sole exception being when w1 = 1.0. Even in this scenario, the exhaustive strategy remains competitive with the sequential strategy. Despite a drop in the holistic score as w\u2081 increases, the exhaustive strategy maintains greater stability, whereas both the parallel and sequential strategies exhibit a significant decline.\nComputational Cost. Our computational resource requirements are determined primarily by the size of the training dataset. In our experiments, we randomly sampled 50 data points from the performance dataset and 60 from the security dataset. The security dataset, sourced from the SALAD-Bench by Li et al. (2024), includes 6 classes and contributes 10 samples per class. This random sampling approach helps to prevent overfitting during the optimization process while allowing us to utilize a smaller set of examples. We initiated the SoS pipeline with 50 randomly generated prompts, each of which underwent an evaluation phase based on the training dataset. Inadequate prompts were discarded, leaving approximately 15 prompts that advanced through various mutation stages and further evaluations. This procedure resulted in an estimated 12,000 LLM calls."}, {"title": "5 Related Work", "content": "Prompt Optimization. Recent studies on prompt optimization, including works by (Fernando et al., 2023; Guo et al., 2023; Hsieh et al., 2023), have focused on exploiting LLMs to utilize evolutionary strategies for prompt exploration. These methods predominantly target single-objective optimization. However, very few studies have explored leveraging Pareto fronts to handle multi-objective optimization (Yang and Li, 2023a; Baumann and Kram, 2024). Unfortunately, these methods are typically computationally intensive, making their application in real-world scenarios impractical and their extension to accommodate additional objectives highly infeasible. In contrast, our approach seeks to develop an efficient and scalable framework that dynamically adjusts weights to maintain a balance among multiple objectives, thus providing several optimal candidates for user decision-making. Notably, our method is the first to integrate safety and security into the prompt optimization process.\nLLM Safety and Security. Recent efforts have been focused on two primary objectives: developing advanced attack methods and enhancing safety techniques (Wei et al., 2024; Yao et al., 2024; Rebedea et al., 2023; Zhang et al., 2023). Notable contributions in the field include the efficient generation of adversarial prompts through an automated red-teaming method proposed by Paulus et al. (2024) and SALAD-Bench, a benchmark for evaluating the safety of LLMs proposed by Li et al. (2024). Meanwhile, defensive strategies, such as those proposed in RPO (Zhou et al., 2024) and RigorLLM (Yuan et al., 2024), aim to incorporate adversaries into training or optimize safe suffixes. Our work takes a different approach by emphasizing a balanced optimization of safety and performance us-"}, {"title": "6 Industrial Deployment", "content": "SoS is an efficient framework that can optimize the performance and security of LLMs simultaneously in a flexible manner. It allows users to assign different weights to objectives, enabling fine-tuned control over the balance between performance and safety based on specific use cases and requirements. SoS can be adapted to different security datasets, allowing companies to customize the optimization to their particular security concerns. SoS is not limited to performance and security objectives; it can be applied to any group of objectives with an evaluation system in place. This versatility makes it valuable for a wide range of industrial applications where multiple criteria need to be balanced. For industries that work with sensitive data or high-stakes applications, SoS offers a promising way to deploy LLMs that not only maintain high performance but also significantly improve safety and security."}, {"title": "7 Conclusion", "content": "We introduce SoS, a novel framework that simultaneously enhances both performance and security in LLMs. SoS addresses critical safety and security concerns in deploying optimized LLM prompts, offering a promising approach for developing high-performing yet secure LLM systems across various industrial applications. Future work could explore online optimization to further improve efficiency."}, {"title": "8 Limitation", "content": "Despite having such achievements, SoS still needs thousands of inference calls in several iterations, which might be insufficient for supporting large-scale applications. The final quality of SoS is also impacted by the evaluation databases used. Should the database contain biases, or its internal distribution misalign with real cases, SoS has a limited chance to fix such biases. Future work could explore better online strategies to further improve efficiency, and also investigate other objectives of prompt tuning beyond security and safety, including consistency and robustness."}, {"title": "A Additional Experiment Setup", "content": "Implementation Details. We utilized GPT-3.5 to develop LLM agents capable of performing various mutation operators. We divided the entire dataset into dev and test datasets, used the dev set for evolution, and reported the final score on the test set. The prompt selection identifies locally optimal prompts using a threshold 8 of 1E-5 and the stopping threshold of is taken to be 0.01. We compared the performance of different LLM agent models, including Llama3-8B and Mistral-7B."}, {"title": "B Additional Experiment Results", "content": "Table 4 shows the ablation studies and results from the initial variations of the algorithm we experimented with, which ultimately led to the development of the final Exhaustive Evo algorithm. We add additional results here for the sentiment analysis task, as shown in Table 5. Since sentiment analysis is a relatively easier task, we achieved convergence in just one iteration; consequently, the results mirror those of the Exhaustive Evo."}, {"title": "C Meta Prompts for LLM agents", "content": "You are a mutator who is familiar with the concept of crossover in genetic algorithms, namely combining the genetic information of two parents to generate new offspring. Given two parent prompts, you will perform a crossover to generate an offspring prompt that covers the same semantic meaning as both parents.\n# Example\nParent prompt 1: Now you are a categorizer, your mission is to ascertain the sentiment of the provided text, either favorable or unfavorable.\nParent prompt 2: Assign a sentiment label to the given the sentence from ['negative', 'positive'] and return only the label without any other text.\nOffspring prompt: Your mission is to ascertain the sentiment of the provided text and assign a sentiment label from ['negative', 'positive'].\n# Given {passed in data}\nYou are a quick improver. Given an existing prompt and feedback on how it should improve. Create an improved version based on the feedback.\n# Given:\n{passed in data}\n## Improved Prompt ##"}]}