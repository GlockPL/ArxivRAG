{"title": "MULTI-NEURON UNLEASHES EXPRESSIVITY OF RELU NETWORKS UNDER CONVEX RELAXATION", "authors": ["Yuhao Mao", "Yani Zhang", "Martin Vechev"], "abstract": "Neural work certification has established itself as a crucial tool for ensuring the robustness of neural networks. Certification methods typically rely on convex relaxations of the feasible output set to provide sound bounds. However, complete certification requires exact bounds, which strongly limits the expressivity of ReLU networks: even for the simple \u201cmax\u201d function in R2, there does not exist a ReLU network that expresses this function and can be exactly bounded by single-neuron relaxation methods. This raises the question whether there exists a convex relaxation that can provide exact bounds for general continuous piecewise linear functions in Rn. In this work, we answer this question affirmatively by showing that (layer-wise) multi-neuron relaxation provides complete certification for general ReLU networks. Based on this novel result, we show that the expressivity of ReLU networks is no longer limited under multi-neuron relaxation. To the best of our knowledge, this is the first positive result on the completeness of convex relaxations, shedding light on the practice of certified robustness.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural networks have been shown vulnerable to adversarial attacks (Szegedy et al., 2014), where a small perturbation to the input can lead to a misclassification. The area of adversarial robustness, which measures the robustness of a model with respect to adversarial perturbations, has received much research attention in recent years, reflecting a major concern in the application of neural networks, especially in safety-critical domains such as autonomous driving and medical diagnosis. However, computing the exact adversarial robustness of a neural network is generally NP-hard (Katz et al., 2017), while adversarial attacks which try to construct an adversarial perturbation can only provide an upper bound on the robustness of the model. To tackle this issue, neural network certification (Singh et al., 2018; Wang et al., 2018; Bunel et al., 2020) has been proposed to provide robustness guarantees. Complete certification methods (Katz et al., 2017; Tjeng et al., 2019) that can provide exact bounds for all ReLU networks are computationally expensive due to the inherent hardness of the problem, and thus incomplete methods (Wong & Kolter, 2018; Singh et al., 2018; Weng et al., 2018; Gehr et al., 2018; Xu et al., 2020) have been widely investigated, typically focus-ing on convex relaxations, which can provide efficient and scalable certification at the cost of losing precision. Beyond certification, all existing algorithms for training certifiable models (Shi et al., 2021; M\u00fcller et al., 2023; Mao et al., 2023; 2024a; Palma et al., 2023; Balauca et al., 2024) are also based on convex relaxations. Due to their central role in certified robustness, it is critical to under-stand the trade-off between the efficiency and precision of convex relaxations.\nExpressivity of ReLU networks under convex relaxations In this work we focus on studying the expressivity of ReLU networks when convex relaxations are used. It has been previously shown that ReLU networks are expressive: they can precisely express every continuous piecewise linear func-tion (Hertrich et al., 2021) and thus can approximate every continuous function within an arbitrary error rate. A key question here is: do existing convex relaxation methods limit this expressive power?"}, {"title": "2 RELATED WORK", "content": "We now briefly review related work most closely related to ours.\nNeural Network Certification. Existing methods for neural network certification can be catego-rized into complete methods and incomplete methods. Complete methods provide exact bounds for the output of a network, usually relying on solving a mixed-integer program (Tjeng et al., 2019) or a satisfiability modulo theory problem (Katz et al., 2017). These methods are naturally computa-tionally expensive and do not scale well. Incomplete methods, on the other hand, provide sound but inexact bounds, based on convex relaxations of the feasible output set of a network. Xu et al. (2020) characterizes widely-recognized convex relaxations (Mirman et al., 2018; Wong et al., 2018; Zhang et al., 2018; 2022; Ferrari et al., 2022) as linear constraints, equivalent to linear program-ming in the corresponding linear systems. We distinguish three concrete convex relaxation meth-ods typically considered by theoretical work: Interval Bound Propagation (IBP) (Mirman et al., 2018; Gowal et al., 2018), which ignores the interdependency between neurons and use interval {$\\lbrack a, b\\rbrack | a, b \\in R$} as the convex relaxation; Triangle relaxation (Wong & Kolter, 2018), which ap-proximates the ReLU function by a triangle in the input-output space; and multi-neuron relaxations (Singh et al., 2018) which considers a group of ReLU neurons jointly in the linear system.\nConvex Relaxation Theories. Baader et al. (2020) first show the universal approximation theo-rem for certified models, stating that for every continuous piecewise linear function $f : R^n \\rightarrow R$ and any error rate $\\epsilon > 0$, there exists a ReLU network that expresses f and its IBP analysis can provide bounds with error at most $\\epsilon$. This result is generalized to other activations by Wang et al. (2022). However, Mirman et al. (2022) show that there exists a continuous piecewise linear function for which IBP analysis of any finite ReLU network expressing this function cannot provide exact bounds. This means that even for continuous piecewise linear functions, IBP requires a network with infinitely many parameters to provide exact bounds. Further, Mao et al. (2024b) show that IBP introduces a strong regularization on the parameter signs to provide good bounds, severely limiting the network capability. Beyond IBP, Baader et al. (2024) show that even Triangle, the most precise single-neuron relaxation, cannot precisely express the \u201cmax\u201d function in R2 with a finite ReLU net-"}, {"title": "3 BACKGROUND", "content": "We now start with a brief review of the required background. We first introduce convex relaxations for network certification and then present single-neuron and multi-neuron relaxation methods.\nConvex Relaxations for Certification. Given a function $f : R^{d_{in}} \\rightarrow R^{d_{out}}$ and a compact domain $X \\subset R^{d_{in}}$, we denote the graph of the function {$(x, f(x)) \\in R^{d_{in} + d_{out}} : x \\in X$} by $f[X]$. The certification task boils down to computing the up-per and lower bounds of the range $f(X)$, in order to verify that these bounds meet certain requirements, e.g., adversarial robust-ness. To this end, convex relaxations approximate $f[X]$ by a convex set $S \\subset R^{d_{in} + d_{out}}$ satisfying $S \\supseteq f[X]$. We then take the upper and lower bounds of $S$ (projected into $R^{d_{out}}$) which are usually much easier to compute compared to those of $f(X)$ due to the convexity of $S$\u2014as an over-approximation of the bounds on $f(X)$. We assume the do-main $X$ to be a convex polytope, because this is the common practice in certification, e.g., $L_\\infty$ neighborhoods of a reference point. Such convex polytopes can be represented by a set of linear constraints $C(x, f(x)) \\leq 0$. For example, consider the ReLU function $y = max(0, x)$ on the domain $X = \\lbrack -1,1 \\rbrack$. One possible convex relaxation is the Triangle relaxation (Wong & Kolter, 2018), represented by the set of linear constraints $(y \\geq x) \\wedge (y \\geq 0) \\wedge [y \\leq (x+1)]$. \nReLU Network Analysis with Layer-wise Convex Relaxations. Computing $f[X]$ of a ReLU network is generally NP-hard. To ease the computation, convex relaxations are applied in a layer-wise manner. Specifically, consider a ReLU network $f = W_L \\circ \\rho \\circ \\cdots \\circ \\rho \\circ W_1$ and an input convex polytope $X$. Denote the variable of the input layer by $x^{(0)}$, the first layer by $x^{(1)} = W_1(x^{(0)})$, the second layer by $x^{(2)} = \\rho(x^{(1)})$, and so on. Assume the input polytope is defined by the linear constraint set $C_0(x^{(0)}) \\leq 0$. We apply convex relaxations to the first layer $x^{(1)} = W_1(x^{(0)})$ to obtain a set of linear constraints $C_1(x^{(0)}, x^{(1)}) \\leq 0$. Proceeding by layers, we obtain linear constraint sets $C_{\\ell+1}(x^{(\\ell)}, x^{(\\ell+1)}) \\leq 0$, for $\\ell = 0, ..., 2L - 2$. Note that no explicit constraint across layers is considered, e.g., $C(x^{(0)}, x^{(2L-1)}) \\leq 0$ would not appear explicitly in the above procedure. Finally, we take the union of all constraint sets, $C = C_0(x^{(0)}) \\cup C_1 (x^{(0)}, x^{(1)}) \\cup ... \\cup C_{2L-1}(x^{(2L-2)}, x^{(2L-1)})$ and solve $C \\leq 0$ by by linear programming to obtain the upper and lower bounds of the output variable $x^{(2L-1)}$. As we perform the relaxation on $W_{\\ell}(\\cdot)$ or $\\rho(\\cdot)$ for every layer, the set $C$ represents a convex relaxation of the overall composed function $f = W_L \\circ \\rho \\circ \\cdots \\circ \\rho \\circ W_1$ on domain $X$. Note that we can choose to further neglect part of the linear constraints to reduce the computational complexity, yielding a more loose relaxation.\nSingle-Neuron and Multi-Neuron Relaxations. Within the framework of layer-wise convex relax-ations, the constraint set of an affine layer $y = Ax + b$ is always $C(x, y) = {Ax + b - y, - Ax - b + y} \\leq 0$, which translates to the equality $y = Ax + b$. No loss of precision, therefore, is intro-duced in affine layers. The core difference between different relaxation methods is how they handle the ReLU function. Single-neuron relaxation methods relax each ReLU neuron separately and dis-regard the interdependence between neurons, while multi-neuron relaxations consider a group of ReLU neurons jointly. Concretely, for the ReLU layer $y = \\rho(x)$ with $x \\in R^d$, the constraint sets computed by single-neuron relaxations are of the form $C(x_i, y_i)$ with $i \\in \\lbrack d \\rbrack$. In contrast, multi-neuron relaxations produce constraint sets of the form $C(x_{I_1}, y_{I_2})$ with $I_1, I_2 \\subset \\lbrack d \\rbrack$.\nSingh et al. (2019) propose the first multi-neuron relaxation called k-ReLU. For each ReLU layer, it considers at most k unstable neurons jointly, i.e., $C(x, y)$ is of the form $C(x_I, y_I)$, with $I \\subseteq \\lbrack d \\rbrack, |I| \\leq k$. However, k-ReLU is not complete for general ReLU networks (see \u00a77), thus we consider a stronger multi-neuron relaxation which only restrict the number of output variables in the constraints, allowing $C(x, y)$ to be of the form $C(x, y_I)$ with $I \\subseteq \\lbrack d \\rbrack, |I| \\leq k$. We denote this"}, {"title": "4 FULL EXPRESSIVITY OF RELU NETWORK UNDER MULTI-NEURON RELAXATIONS", "content": "We now present our main result. We combine an existing result on the representation capability of ReLU networks with our novel results, which we prove in detail in \u00a75 and \u00a76, to answer the question posed in \u00a71.\nWe establish in \u00a75 that $M_k^\\circ$ returns exact bounds for ReLU networks of width no more than k. In \u00a76, we prove that if a ReLU network has at most k unstable neurons in each layer\u2014this number could be far smaller than the network width-then $M_k$ provides exact output bounds. As a final step towards Theorem 1 below, Lemma 1 (Hanin, 2019, Theorem 2) states that any continuous piecewise linear function $f : \\lbrack 0, 1 \\rbrack^{d_{in}} \\rightarrow R$ can be expressed by a ReLU network of width $d_{in} + 3$ which has at most 3 unstable neurons per layer.\nTheorem 1. Let $d_{in} \\in N$ and let $X \\subseteq \\lbrack 0, 1 \\rbrack^{d_{in}}$ be a convex polytope in $R^{d_{in}}$. For every continuous piecewise linear function $f : \\lbrack 0, 1 \\rbrack^{d_{in}} \\rightarrow R$, denote the lower and upper bound of the range $f(X)$ by $\\ell := min_{x \\in X} f(x)$ and $u := max_{x \\in X} f(x)$. Then there exists a ReLU network $\\Phi$ satisfying $\\Phi(x) = f(x), \\forall x \\in X$, and applying $M_3$ and $M_{d_{in} + 3}$ to $(\\Phi, X)$ both return $\\ell$ and $u$."}, {"title": "5 MULTI-NEURON EXPRESSIVITY WITH BOUNDED WIDTH", "content": "We now develop the first central result behind our main theorem on the expressivity, which shows that the output-only multi-neuron relaxation $M^\\circ$ introduced in \u00a73 solves the exact output bound for ReLU networks of width at most k. This result is formally presented in Theorem 2.\nTheorem 2 (Precise $M^\\circ$ with Bounded Width). Let $L, k, d_{in}, d_{out} \\in N$. Consider a ReLU network $f : R^{d_{in}} \\rightarrow R^{d_{out}}$ of depth $L$ and width $\\leq k$. Let $X \\subset R^{d_{in}}$ be a convex polytope. Applying $M^\\circ$ to f on domain X returns the exact output set which is also a convex polytope, i.e.,\n$M^\\circ(f, X) = f(X).$\nProof. We prove by induction on the network depth L that $M^\\circ(f, X) = f(X)$. By Lemma 3 below, $f(X)$ is a convex polytope for every ReLU network f.\nWe start with the base case L = 1, when f is an affine function $f(x) = Ax + b$. By definition, $M^\\circ(f, X) = {Ax + b | x \\in X} = f(X)$. To prove the induction step, we assume that (1) holds for all ReLU networks of depth $\\leq L -1$ and width $\\leq k$. The subnetwork $f' = W_{L-1} \\circ \\rho \\circ \\cdots \\circ W_1$ consisting of the first L \u2013 1 affine and ReLU layers of f, clearly, has depth L - 1 and width $\\leq k$. By induction hypothesis, $M^\\circ(f', X) = f'(X)$. The resting subnetwork $f'' = W_1 \\circ \\rho$ which consists of the last affine and ReLU layer of f, or equivalently $f'' = W_1 \\circ \\rho \\circ Identity$, has depth 2 and width $\\leq k$. By induction hypothesis, again, we have $M^\\circ(f'', f'(X)) = f''(f'(X))$. Therefore,\n$M^\\circ(f, X) = M^\\circ(f'' \\circ f', X) = M^\\circ(f'', f'(X)) = f''(f'(X)) = f(X)$.\nThis concludes the proof of the induction step and hence establishes the claim.\nTheorem 2 is mainly based on two observations. First, the convex hull of a convex polytope is the polytope itself; in other words, $M^\\circ$ does not introduce any relaxation error for a single layer when the feasible output set under consideration is a convex polytope. Second, ReLU networks transform convex polytopes into convex polytopes. This convex polytope preserving property is proved in Lemma 3.\nLemma 3. Let $d_{in}, d_{out} \\in N, f : R^{d_{in}} \\rightarrow R^{d_{out}}$ be a ReLU network ended with either affine or ReLU layer, and X be a convex polytope in $R^{d_{in}}$. Then $f(X)$ is a convex polytope in $R^{d_{out}}$.\nProof. We first show that every affine and ReLU layer transforms a convex polytope into a convex polytope. Then, we prove the statement by induction on the network depth.\nAssume the input convex polytope X is represented by linear constraint set $C(x) \\leq 0$. Consider an affine transformation $y = Ax + b$. The functional graph {$(x, y) : y = Ax + b, x \\in X$} is defined by the constraints {$C(x), y \u2013 Ax \u2013 b, -y + Ax + b$} $\\leq 0$. Eliminating the variable x using the Fourier-Motzkin algorithm (Fourier, 1827), the resulting constraints are affine inequalities of y, thus define a convex polytope for y. We proceed to show the same property holds for the ReLU function"}, {"title": "6 MULTI-NEURON EXPRESSIVITY WITH BOUNDED UNSTABLE NEURONS", "content": "We have shown in \u00a75 that the output-only multi-neuron relaxation $M^\\circ$ returns the exact output set for ReLU networks of width at most k. This result essentially relies on the fact that in a feedforward ReLU network, $M^\\circ$ does not lose precision for layers with at most k neurons, although it discards the dependency between input variable and output variable in each layer after processing. However, this result does not directly apply to ReLU networks with skip connections, where neurons between non-adjacent layers might be connected by a skip-connection. While it is also possible to convert a ReLU network with skip connections into a feedforward network by introducing additional neurons in those layers, the width of the resulting feedforward network becomes unnecessarily large, thus k also needs to be as large which leads to significant computational overhead.\nIn this section, we tackle this problem by developing a general result with $M_k$ that applies to all ReLU networks, including those with skip connections. Specifically, we show that $M_k$ is precise for ReLU networks with at most k unstable neurons in each hidden layer. Since the number of unstable neurons in each layer will not increase when converting a network with skip connections to a feedforward network, this result generalizes to ReLU networks with skip connections as well.\nWe begin by formally defining stable and unstable neurons in Definition 4 and 5. Intuitively, intrin-sically unstable neurons are those that switch their activation pattern in the input set, while bounded unstable neurons are those that are not guaranteed to be stable by a convex relaxation, i.e., they have a positive upper bound and a negative lower bound under the given relaxation.\nDefinition 4 (Intrinsically Stable and Unstable Neuron). For a ReLU network $\\Phi$ and an input set X, a ReLU neuron is called intrinsically unstable on X if there exists $x_1, x_2 \\in X$ such that $x_1$ activates this neuron and $x_2$ does not activate it. Otherwise, it is called intrinsically stable on X."}, {"title": "7 CASE STUDY: THE MAX FUNCTION", "content": "Baader et al. (2024) prove that there does not exist a ReLU network that can express the \"max\" function in the compact domain $[0, 1]^2 \\subset R^2$ such that the network outputs can be bounded exactly by single-neuron relaxations. In this section, we take the \u201cmax\u201d function in $R^d$, d > 2, on domain $[0, 1]^d$, as an example to show that a multi-neuron relaxation easily resolve such impossibility results, as a confirmation of our main result.\nFirst, consider d = 2. In this case, we can represent the \"max\" function with the ReLU network $f = x_2 + \\rho(x_1 - x_2)$, illustrated in Figure 6. This network has width equals two (node c and d) and maximum unstable neurons per layer equals one (node c). We will thus show that $M_2$ and $M_1$ can return the exact bounds of the functional range, i.e., [0, 1].\nThe input constraints are $(x_1 \\geq 0) \\wedge (x_1 \\leq 1) \\wedge (x_2\\geq 0) \\wedge (x_2 \\leq 1)$. Besides, the constraints for affine layers are $(a = x_1 - x_2) \\wedge (b = x_2) \\wedge (f = c + d)$. With these constraints, we can compute the bounds of the output of the first affine layer with linear programming, yielding a \u2208 [-1,1] and b\u2208 [0, 1]. Therefore, a is bounded unstable and b is bounded stable.\nWe now show that $M_1$ computes the exact bounds of f. For the bounded stable node b, the constraint is d = b. For the bounded unstable node c, the constraint is $(c \\geq 0) \\wedge (c \\geq a) \\wedge (c \\leq 1 - b)$. Therefore, we have $f = c + d = c + x_2 \\geq 0 + x_2 > 0$ and $f = c + d = c + x_2 \\leq 1 - x_2 + x_2 = 1$"}, {"title": "8 DISCUSSION", "content": "Complete Certification with Multi-neuron Relaxations. This work establishes that two partic-ular multi-neuron relaxations are complete verifiers for ReLU networks. Despite their theoretical power, there is currently no algorithmic implementation of these relaxations. In particular, their al-gorithmic complexity is unknown. Developing efficient algorithms for these relaxations is important for future work, and we suggest a few possible directions below.\nThe first question is how to compute the convex hull. While this might be exactly computed (e.g., for affine layers), an approximate convex hull might be sufficient for practical purposes (M\u00fcller et al., 2022). Therefore, one may rely on \u201cconstraint mining\", i.e., finding valid constraints sequentially. Since the convex hull is the intersection of all valid constraints, one can iteratively add constraints to the linear system until the convex hull is fully covered. While effective constraint mining is non-trivial, we remark that due to the completeness of multi-neuron relaxations, the expensive branch-and-bound as deployed by M\u00fcller et al. (2022) is no longer required to find the exact bounds. In addition, similar constraint mining approaches are deployed by Zhang et al. (2022), but they con-sider all constraints possibly involving different layers, which is a much larger constraint space than that for a single layer.\nThe second question is how to solve the linear system efficiently, especially in the process of con-straint mining where multiple strongly overlapping linear programming problems need to be solved. This question might be relatively easy, because we can expect the optimal solution of the previous linear programming to be a good initial guess for the next linear programming. In particular, the simplex algorithm might be a good choice for this task because the new optimum must lie on the vertices introduced by new constraints.\nThe last question is how to check whether we have reached the exact bounds. We suggest two possible approaches. The first approach essentially relies on the effectiveness of constraint mining:"}, {"title": "9 CONCLUSION", "content": "We proved the first positive result on the completeness of convex relaxations and the expressivity of ReLU networks under convex relaxations. While single-neuron relaxations that relax each neuron separately are incomplete, we proved that (layer-wise) multi-neuron methods, where multiple neu-rons in the same layer are processed jointly, are complete. Specifically, for networks of width no more than k, one computes the convex hull of the range of each layer, proceeding in a layer-wise manner. Then, the resulting set of linear constraints induces exact upper and lower bounds on the output set of the network. In addition, when the network width is unbounded, but the number of unstable neurons is at most k in each layer, we can retain the exact bounds by jointly considering the input-output set of those k neurons. Our results demonstrate that the expressivity of ReLU networks is no longer limited under multi-neuron relaxations, in contrast to single-neuron relaxations which have previously been shown to severely limit the expressivity of networks they can certify exactly."}]}