{"title": "HUDEX: INTEGRATING HALLUCINATION DETECTION AND\nEXPLAINABILITY FOR ENHANCING THE RELIABILITY OF LLM\nRESPONSES", "authors": ["Sujeong Lee", "Hayoung Lee", "Wonik Choi", "Seongsoo Heo"], "abstract": "Recent advances in large language models (LLMs) have shown promising improvements, often\nsurpassing existing methods across a wide range of downstream tasks in natural language processing.\nHowever, these models still face challenges, which may hinder their practical applicability. For\nexample, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially\nin fields that demand high factual precision. Current benchmarks primarily focus on hallucination\ndetection and factuality evaluation but do not extend beyond identification. This paper proposes\nan explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing\nthe reliability of LLM-generated responses by both detecting hallucinations and providing detailed\nexplanations. The proposed model provides a novel approach to integrate detection with explanations,\nand enable both users and the LLM itself to understand and reduce errors. Our measurement results\ndemonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed\nmodel performs well in both zero-shot and other test environments, showcasing its adaptability across\ndiverse benchmark datasets. The proposed approach further enhances the hallucination detection\nresearch by introducing a novel approach to integrating interpretability with hallucination detection,\nwhich further enhances the performance and reliability of evaluating hallucinations in language\nmodels.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have showcased their potential in natural language processing\n(NLP) [1]. While LLMs can generate effective responses across diverse tasks, they are also limited by certain critical\nissues. One such limitation is hallucination, where the model produces information that is factually incorrect or\ngenerates content not requested or instructed by the user. This problem can lead to the spread of incorrect information,\nparticularly problematic in fields where accuracy and reliability are crucial, thereby limiting the applicability of LLMs\nin various industries. Consequently, hallucination is a major issue undermining the reliability of LLMs, prompting\nsignificant research into solutions.\nRecent studies have focused on developing benchmarks to detect and evaluate hallucinations and methods for mitigating\nthem. For example, FELM [2] provides a benchmark for assessing the factuality of LLMs by identifying factual errors\nin response segments through text-segment-based annotations. TruthfulQA [3] evaluates whether language models"}, {"title": "2 Related Work", "content": "A Large Language Model is an artificial intelligence model based on the Transformer architecture [5]. It refers to\na pre-trained language model (PLM) with a parameter size exceeding a certain threshold [6]. LLMs are trained on\nmassive datasets and typically have billions to hundreds of billions of parameters. Due to the extensive data used in\ntheir training, LLMs exhibit exceptional performance across various NLP tasks, including text generation, translation,\nand summarization.\nNotably, LLMs that surpass a certain parameter scale demonstrate emergent abilities not found in smaller models.\nExamples of these abilities include in-context learning, instruction following, and chain-of-thought (CoT) reasoning\n[7]. These capabilities enable LLMs to handle more complex tasks, such as advanced reasoning, problem-solving, and\ngenerating multi-turn responses."}, {"title": "2.1 Definitions of Large Language Models", "content": "A Large Language Model is an artificial intelligence model based on the Transformer architecture [5]. It refers to\na pre-trained language model (PLM) with a parameter size exceeding a certain threshold [6]. LLMs are trained on\nmassive datasets and typically have billions to hundreds of billions of parameters. Due to the extensive data used in\ntheir training, LLMs exhibit exceptional performance across various NLP tasks, including text generation, translation,\nand summarization.\nNotably, LLMs that surpass a certain parameter scale demonstrate emergent abilities not found in smaller models.\nExamples of these abilities include in-context learning, instruction following, and chain-of-thought (CoT) reasoning\n[7]. These capabilities enable LLMs to handle more complex tasks, such as advanced reasoning, problem-solving, and\ngenerating multi-turn responses."}, {"title": "2.2 Definitions of Hallucination", "content": "In NLP, hallucination refers to content that is unreliable or illogical compared to the provided source material [10], [11].\nPrevious studies categorize hallucinations into two broad two types: intrinsic and extrinsic [10], [11],[12], [13].\nIntrinsic hallucination occurs when the generated output contradicts the source content. For example, this happens\nwhen a model produces information that conflicts with the given data in response to a factual question. In contrast,\nextrinsic hallucinations involve outputs that include unverifiable or nonexistent information. This often occurs when the\nmodel generates content that cannot be corroborated by the source material.\nIn the context of LLMs, hallucination can be defined more specifically. LLM hallucinations, which prioritize user\ninstructions and interactions, can be categorized based on factuality and faithfulness [14]. Factual hallucinations arise\nwhen a model generates outputs that are based on real-world information but are either incorrect or unverifiable. For\ninstance, if the model inaccurately presents well-known facts or mentions nonexistent information, it is considered a\nfactual hallucination. Faithfulness-related hallucinations occur when the model generates responses unrelated to user\ninstructions or the provided content, or when it produces internally inconsistent answers. This type of hallucination is\nparticularly important in conversational models.\nThe issue of hallucination may stem from several factors, including the use of outdated data during the data collection\nprocess [15] or biased data [16] used for model training [14],[17], [18]. Furthermore, the risk of hallucinations tends to\nincrease with the size and complexity of the models."}, {"title": "2.3 LLM-Based Evaluation of LLMS", "content": "One of the key challenges discussed alongside the development of LLMs is the difficulty in accurately evaluating\nthe context and meaning of generated responses using traditional quantitative metrics. While human evaluation has\nbeen employed to address this limitation, it has considerable drawbacks, particularly in terms of time and resource\nconsumption [1],[19].\nTo overcome these challenges, the use of LLMs as evaluation tools, or \u201cLLM judges,\u201d has gained attention. [20]\npioneered an LLM-based evaluation framework, showing that strong LLMs achieved over 80% agreement with human\nexperts in evaluations. Subsequent studies by [21], [22], and [23] have expanded on this approach, further validating\nthe utility of LLM judges.\nThe introduction of LLM judges provides an efficient solution for evaluating large-scale data, where human evaluation\nmay be impractical. In addition to quantitative assessments, LLM judges offer qualitative evaluations based on their\nunderstanding of context and adherence to user instructions, making them versatile tools for comprehensive evaluation."}, {"title": "3 Data Construction", "content": "For training, we utilized the HaluEval, FactCHD, and FaithDial datasets, as summarized in Table 1.\nThe HaluEval dataset [24] is a hallucination evaluation benchmark designed to assess the likelihood of hallucinations\nbased on content type. It consists of 30,000 examples across three tasks: question answering, knowledge-based\ndialogues, and text summarization, along with 5,000 general user queries that include ChatGPT responses. In this study,\nwe used the question-answering and knowledge-based dialogue subsets as training data. Both subsets focus on detecting\nhallucinations based on provided knowledge, allowing the model to learn how to identify intrinsic hallucinations.\nThe FactCHD dataset [25] is a benchmark specifically designed to detect hallucinations that conflict with factual\ninformation in LLMs. It evaluates factual accuracy in the context of a wide range of queries and responses, facilitating\nfactual reasoning during evaluation. Unlike HaluEval, the FactCHD dataset does not include a pre-existing knowledge\nbase, enabling the model to learn to detect hallucinations in scenarios with limited reference material.\nThe FaithDial dataset [26] is designed to minimize hallucinations and improve the accuracy of information-seeking\ndialogues. It was built by modifying the Wizard of Wikipedia (WOW) benchmark to include hallucinated responses.\nThe dataset includes a BEGIN [27] label that categorizes responses based on their relationship to the knowledge source"}, {"title": "3.1 Datasets", "content": "For training, we utilized the HaluEval, FactCHD, and FaithDial datasets, as summarized in Table 1.\nThe HaluEval dataset [24] is a hallucination evaluation benchmark designed to assess the likelihood of hallucinations\nbased on content type. It consists of 30,000 examples across three tasks: question answering, knowledge-based\ndialogues, and text summarization, along with 5,000 general user queries that include ChatGPT responses. In this study,\nwe used the question-answering and knowledge-based dialogue subsets as training data. Both subsets focus on detecting\nhallucinations based on provided knowledge, allowing the model to learn how to identify intrinsic hallucinations.\nThe FactCHD dataset [25] is a benchmark specifically designed to detect hallucinations that conflict with factual\ninformation in LLMs. It evaluates factual accuracy in the context of a wide range of queries and responses, facilitating\nfactual reasoning during evaluation. Unlike HaluEval, the FactCHD dataset does not include a pre-existing knowledge\nbase, enabling the model to learn to detect hallucinations in scenarios with limited reference material.\nThe FaithDial dataset [26] is designed to minimize hallucinations and improve the accuracy of information-seeking\ndialogues. It was built by modifying the Wizard of Wikipedia (WOW) benchmark to include hallucinated responses.\nThe dataset includes a BEGIN [27] label that categorizes responses based on their relationship to the knowledge source"}, {"title": "3.2 Explanation Generation", "content": "The primary goal of our model is not only to detect hallucinations in generated responses but also to provide explanations\nfor the reasoning behind these judgments. A simple example of this process is illustrated in Figure 1. To achieve this,\nthe model must be trained on explanation data. While the FactCHD dataset includes explanations, the HaluEval and\nFaithDial datasets do not. Therefore, we used the Llama3 70B [28] model to generate explanation data for hallucination\ndetection in the HaluEval and FaithDial datasets.\nDuring the explanation generation process, we also generated answers corresponding to the hallucination labels. This\nstep ensured that the hallucination labels predicted by the model during explanation generation aligned with the existing\nhallucination labels in HaluEval and FaithDial datasets.\nUpon analyzing the model's predictions, we found that 0.5% of the responses failed to understand the prompt and\nasked for clarification, and 4.2% were classified as anomalies. Excluding these cases, 95.3% of the responses adhered\nto the expected format. As shown in Table 2, the accuracy of valid responses was 98.3%. Ultimately, 93.7% of the\nhallucination labels from HaluEval and FaithDial matched the model's predicted answers, and only the verified matching\ndata were used for training.\nTo further assess the quality of the generated explanations, we conducted statistical sampling. We defined the population\nas the set of generated explanations, with a confidence level of 99%, a conservatively estimated defect rate of p = 0.5,\nand a margin of error set at 2%. Through human evaluation of the selected sample, we validated the explanations to\nensure the accuracy and relevance of the reasoning provided."}, {"title": "4 Model Training and Inference", "content": "We used the Llama 3.1 8B model [28] for training and applied low-rank adaptation (LoRA) [29], a method under\nparameter efficient tuning (PEFT). The task prompts for training were divided into two main categories: hallucination\ndetection and hallucination explanation. The model was trained on both tasks using the same dataset."}, {"title": "4.1 Training", "content": "We used the Llama 3.1 8B model [28] for training and applied low-rank adaptation (LoRA) [29], a method under\nparameter efficient tuning (PEFT). The task prompts for training were divided into two main categories: hallucination\ndetection and hallucination explanation. The model was trained on both tasks using the same dataset."}, {"title": "4.2 Inference", "content": "The prompt structure for inference focuses on two key elements: persona provision and task stage provision. Persona\nprovision ensures that the model understands the specific task's goal before generating responses, encouraging deeper\nanalysis of the given information. By defining the task's context and role in advance, we aim for more consistent\noutputs. To generate a persona, we provided ChatGPT with task details and received recommendations for suitable\npersona candidates. After a human filtering process, we selected a hallucination expert persona to detect hallucinations.\nTask stage provision guides the model to approach complex problems systematically when generating responses. The\nprompt stages are structured adaptively based on the task and data characteristics. If background knowledge is available,"}, {"title": "5 Experiments", "content": "For the detection and explanation generation experiments, we used the test sets from HaluEval dialogue, HaluEval QA,\nFaithDial and FactCHD, which were also used during training. The HaluEval datasets, both for dialogue and QA tasks,\nprovide background knowledge, so we applied inference prompts designed to incorporate this information. FaithDial\nalso utilized the same inference prompt. For the FactCHD dataset, which does not include background knowledge,\nwe used the inference prompt stages suited for tasks without background knowledge. The persona was consistently\nprovided across all tasks, regardless of the presence or absence of background knowledge.\nFor zero-shot detection, we conducted experiments on HaluEval subsets not used during training, specifically HaluEval\nsummarization and HaluEval general. The HaluEval summarization dataset focuses on detecting hallucinations in\ndocument summaries, while the HaluEval general dataset evaluates hallucination detection in ChatGPT responses to\nuser queries. Since both datasets lack background knowledge, we used inference prompts designed for tasks without\nbackground knowledge."}, {"title": "5.1 Datasets", "content": "For the detection and explanation generation experiments, we used the test sets from HaluEval dialogue, HaluEval QA,\nFaithDial and FactCHD, which were also used during training. The HaluEval datasets, both for dialogue and QA tasks,\nprovide background knowledge, so we applied inference prompts designed to incorporate this information. FaithDial\nalso utilized the same inference prompt. For the FactCHD dataset, which does not include background knowledge,\nwe used the inference prompt stages suited for tasks without background knowledge. The persona was consistently\nprovided across all tasks, regardless of the presence or absence of background knowledge.\nFor zero-shot detection, we conducted experiments on HaluEval subsets not used during training, specifically HaluEval\nsummarization and HaluEval general. The HaluEval summarization dataset focuses on detecting hallucinations in\ndocument summaries, while the HaluEval general dataset evaluates hallucination detection in ChatGPT responses to\nuser queries. Since both datasets lack background knowledge, we used inference prompts designed for tasks without\nbackground knowledge."}, {"title": "5.2 Test Setting", "content": "For the detection experiments, we compared our HuDEx to two LLMs, GPT-4 [30] and Llama 3 70B. These models\nreceived the same inference prompts as our model and were tasked with classifying whether the responses contained\nhallucinations."}, {"title": "5.2.1 Detection Experiments", "content": "For the detection experiments, we compared our HuDEx to two LLMs, GPT-4 [30] and Llama 3 70B. These models\nreceived the same inference prompts as our model and were tasked with classifying whether the responses contained\nhallucinations."}, {"title": "5.2.2 Explanation Generation Experiments", "content": "To evaluate the explanations generated by each model, we used an LLM judge and conducted main experiment. The\nexperiment followed a single-answer grading approach, where each model's response was individually scored.\nIn the single-answer grading experiment, we divided the evaluation into two categories: factuality and clarity. Factuality\nassessed whether the explanation contained hallucinations, contradictions, or accurately reflected the given information.\nClarity evaluated how clearly and thoroughly the reason was articulated. Each criterion was scored on a 3-point scale,\nwith a maximum total score of 6 points."}, {"title": "6 Results", "content": ""}, {"title": "6.1 Detection Results", "content": ""}, {"title": "6.1.1 Test Data Detection", "content": "In this experiment, binary classification was performed to distinguish hallucinations from non-hallucinations using the\ntest sets from the training data, with accuracy as the evaluation metric. Table 3 compares the performance of Llama3\n70B, GPT4o, and our model across benchmark datasets such as HaluEval dialogue, HaluEval QA, FactCHD, and\nFaithDial.\nThe experimental results show that our HuDEx outperformed the larger models, Llama3 70B and GPT4o, across all\nbenchmarks. Specifically, it achieved an accuracy of 80.6% on the HaluEval dialogue dataset, surpassing Llama3 70B\n(71.8%) and GPT40 (72.5%), indicating superior performance in detecting hallucinations in conversational response.\nIn the HaluEval QA dataset, our model again achieved the highest accuracy of 89.6%, outperforming GPT40 (86.6%)\nand Llama3 70B (82.7%). This demonstrates its refined ability to detect hallucinations in QA tasks.\nOn the FactCHD and FaithDial datasets, HuDEx recorded accuracies of 70.3% and 58.8%, respectively, continuing\nto show strong performance on both benchmarks. On the FactCHD dataset, HuDEx outperformed Llama3 70B by\n11%, confirming its effectiveness in hallucination detection even when background knowledge is unavailable. On the\nFaithDial dataset, our HuDEx also significantly outperformed GPT40 (50.6%), achieving 58.8%, which highlights its\nconsistent performance on a different type of conversation-based dataset compared to HaluEval dialogue.\nThese results demonstrate that our model consistently delivers superior performance in hallucination detection across\nvarious benchmark datasets, outperforming larger models."}, {"title": "6.1.2 Zero-Shot Detection", "content": "Table 4 presents the results of the binary classification experiment on hallucination vs. non-hallucination in a zero-shot\nsetting. This experiment evaluated the model's hallucination detection performance on unseen data using the HaluEval\nsummarization and HaluEval general datasets, which were not included in the training data. Accuracy was used as the\nevaluation metric, consistent with the methodology in previous experiments.\nOn the HaluEval summarization dataset, HuDEx achieved an accuracy of 77.9%, outperforming Llama3 70B (69.55%)\nand GPT40 (61.9%). This demonstrates the model's ability to effectively detect hallucinations in summary texts of\noriginal content.\nThe HaluEval general dataset consists of queries posed by real users to GPT models, often containing complex responses\nthat go beyond typical conversational text. This complexity makes hallucination detection more challenging and serves\nas an important benchmark for evaluating model reliability on unstructured data. On this dataset, GPT40 recorded\nthe highest accuracy at 78.0%, while our model achieved 72.6%. These results suggest that while HuDEx delivers\nconsistent performance on complex responses, there is still room for improvement."}, {"title": "6.2 Explanation Generation Results", "content": ""}, {"title": "6.2.1 Single-Answer Grading", "content": "This experiment presents the evaluation of hallucination explanations generated by Llama3 70B and our model, as\nassessed by the LLM judge. The results, shown in Table 5, were obtained from the HaluEval dialogue, HaluEval QA,\nand FaithDial datasets. Explanations were evaluated based on two criteria: factuality and clarity, each scored out of 3\npoints, for a maximum combined score of 6 points.\nWhen comparing the performance of Llama3 70B and our HuDEx in terms of factuality, Llama3 70B scored lower on\nthe HaluEval dialogue dataset with 1.932 points but achieved relatively higher scores on HaluEval QA and FaithDial,\nwith 2.416 and 2.587 points, respectively. In contrast, our model outperformed Llama3 70B on factuality for the\nHaluEval dialogue dataset, though it scored slightly lower on HaluEval QA (2.299) and FaithDial (2.216). Despite the\nvariations in scores across datasets, HuDEx demonstrated consistent factual accuracy, indicating its ability to provide\nreliable information.\nIn terms of clarity, Llama3 70B achieved the highest score on the FaithDial dataset with 2.451 points, while our model\nclosely followed with 2.417 points. On the HaluEval dialogue and HaluEval QA datasets, our model outperformed\nLlama3 70B, scoring 2.413 and 2.523 points, respectively. This indicates that HuDEx provides clearer and more easily\nunderstandable explanations for hallucinations.\nOverall, our HuDEx demonstrated competitive performance in terms of factuality, clarity, and overall scores compared\nto Llama3 70B. These results support that our model consistently delivers reliable and clear hallucination explanations.\nThe next experiment evaluated the original explanations from the FactCHD dataset against those generated by our\nmodel, with results shown in Table 6. The conversion ratio was used to compare the performance of our HuDEx as a\npercentage, with the FactCHD score serving as the maximum (100%).\nFor factuality, FactCHD recorded a score of 2.2549, while our model scored slightly lower at 2.236. The conversion\nratio for factuality was 99%, indicating that although FactCHD's original explanations had slightly higher factual\naccuracy, HuDEx performed very closely to this benchmark."}, {"title": "7 Conclusion", "content": "The hallucination phenomenon in large language models (LLMs) presents a significant challenge that needs to be\naddressed in practical applications. This study proposes a model called HuDEx specifically designed to detect\nhallucinations in LLM-generated responses and provide explanations for them. By offering such feedback, the model\ncontributes to both user understanding and the improvement of LLM, fostering the generation and evaluation of more\nreliable responses.\nHowever, a key limitation of the model is its reliance on the LLM's inherent knowledge when sufficient source content\nis unavailable for detecting and explaining hallucinations. This dependency can reduce the clarity of the explanations\nand, in some cases, introduce hallucinations into the explanations themselves.\nDespite this limitation, the study demonstrates strong potential for detecting and explaining hallucinations. Future\nresearch should focus on overcoming these challenges and exploring methods to improve the model's performance. For\nexample, integrating external knowledge retrieval systems could reduce the model's reliance on its internal knowledge,\nwhile enhancing reasoning-based validation could lead to more reliable explanations.\nAdditionally, we aim to develop an automated feedback loop in future work. This system would allow for continuous\ncorrection and improvement of hallucinations, contributing to greater reliability and consistency in LLMs over time."}]}