{"title": "Improving training time and GPU utilization\nin geo-distributed language model training", "authors": ["Palak", "Rohan Gandhi", "Karan Tandon", "Debopam Bhattacherjee", "Venkata N. Padmanabhan"], "abstract": "The widespread adoption of language models (LMs) has\ncaused a huge surge in demand for GPUs. Training LMs re-\nquires tens of thousands of GPUs and housing them in the\nsame datacenter (DC) is a challenge. We focus on training\nsuch models across multiple DCs connected via the Wide-\nArea-Network (WAN). We built ATLAS that speeds up the\ntraining time using novel workload-aware temporal band-\nwidth sharing and other design choices. While ATLAS im-\nproves the training time, it does not completely eliminate the\nbubbles (idle GPU cycles). We built BUBBLETEA that runs\nprefill-as-a-service (part of LM inference) during the bubbles\nthus improving the GPU utilization without any impact on\ntraining. Together, ATLAS and BUBBLETEA improve training\ntime by up to 17\u00d7 and achieve GPU utilization of up to 94%.", "sections": [{"title": "1 Introduction", "content": "The advent of Generative AI and Language Models (LMs)\nsuch as GPT [4], Llama [10], and Mistral [11] has ushered in\nan era of widespread AI adoption across many industries from\nhealthcare to finance to entertainment. As more sectors realize\nthis newly unleashed potential, AI training is considered one\nof the larger cloud workloads of today.\nTraining such LMs requires a significant number of GPUs.\nThese LMs have seen a substantial increase in the number of\nparameters to improve the accuracy [42] and support larger\nnumber of tokens. For instance, the largest variant of Llama\nmodel, Llama 3.1, now boasts of 405 billion parameters, a\nsignificant jump from its predecessor's 70 billion parameters.\nConsequently, it is reported that GPT models with (rumoured)\ntrillions of parameters [5] require 10s of thousands of GPUs\n[2], and Llama 3 models with billions of parameters require\nthousands of GPUs for months to train [3].\nIt is desired that all GPUs catering to a training workload\nare housed in the same data center (DC) and leverage the fast\nintra-DC interconnects to finish the training quickly [28,41].\nHowever, it is a non-trivial engineering challenge to deploy a\nlarge number of GPUs in the same DC due to space, power,\npower density, and cooling requirements [7,8,58,60,65]. Such\nconstraints, coupled with the increasing AI inferencing de-\nmand consuming up to 90% of AI compute [52], are already\ngetting reflected in user reports on not being able to provision\n(book) even a few GPUs [66] per site. This hints at an emerg-\ning systems challenge \u2013 getting access to thousands of GPUs\nat a single DC site for training LMs is hard. We here explore\nthe intuitive yet non-trivial solution to this problem \u2013 geo-\ndistributing the training jobs across GPUs that span multiple\nDCs connected via the WAN.\nIn this paper, we focus on the problem of training individ-\nual LMs on GPUs housed across multiple DCs connected\nvia Wide-Area-Network (WAN). Training jobs use multiple\nforms of parallelism such as Data Parallelism (DP), Pipeline\nParallelism (PP) and Tensor Parallelism (TP) that all have\nsubstantial communication on the critical path (\u00a72). Commu-\nnication, needed during activation updates, gradient updates,\nand synchronization, incurs a significantly higher latency over\ninter-DC WAN than in intra-DC networks. Our experiments\nfor doing PP across DCs show how state-of-the-art sched-\nulers such as Varuna [19] face the following limitations: (a)\nThey end up creating bubbles (idle time; undesired) not only\nbetween the forward and backward passes in one training\niteration, but also between microbatches in the same mini-\nbatch (details in \u00a73.2). (b) Similarly, in PP, the DCs running\nlater pipeline stages are idle (bubble) before activations are\ntransferred from the preceding stages. These bubbles are am-\nplified due to slow WAN communication. Consequently, such\nsystems achieve only < 5% GPU utilization and each training\niteration is severely elongated. (c) Compared to a hypotheti-\ncal baseline of housing all such GPUs in a single DC, such\ngeo-distributed training can result in an order or magnitude\nslower training time. More details are in \u00a73.\nWe present ATLAS and BUBBLETEA- ATLAS addresses\nthe key limitations in geo-distributed training to substantially\nimprove the training time. However, it does not eliminate all\nbubbles. We built BUBBLETEA that offers prefill-as-a-service,\nscheduling the prefill phase of eligible inference requests\nto further reduce the bubbles. ATLAS and BUBBLETEA are\nindependent systems that complement each other and improve\nthe training time by up to 17\u00d7 and achieve GPU utilization\nof up to 94% in cross-DC LM training."}, {"title": "1.1 ATLAS design choices", "content": "ATLAS addresses above limitations using the following\ndesign choices:\nTurbo-charging communication: While WAN offers lower\nbandwidth than intra-DC networks, prior works [34, 46, 75]\npaint a bleak picture of WAN bandwidth. E.g., [34] could\nuse only 100 Mbps bandwidth between nodes in Virginia and\nSao Paulo on Amazon EC2. Similarly, [46] found an average\nof 193 Mbps between nodes in Utah and Wisconsin. Such\nlow bandwidth is true for a single TCP connection that these\nworks might have used. PyTorch framework also uses only\none TCP connection for communication between two nodes\n(\u00a73). However, two communicating nodes can easily spawn\nmultiple TCP connections to scale the bandwidth. With Azure\nDCs, we found that multiple TCP connections between same\npair of nodes get 5 Gbps irrespective of the DC locations.\nThis simple, intuitive idea of using multiple TCP connections\nsubstantially improves the training time.\nFinding the right parallelism: We revisit the first order ques-\ntion \u2013 which form of parallelism (DP, PP, TP) to use within\nand across DCs? A substantial amount of prior work focuses\non running DP across DCs [20,46,71,75]. However, we argue\nfor PP across DCs, and DP and TP within the DCs for LM\ntraining. Doing all-reduce over intra-DC network and PP over\nWAN improves overall latency.\nCoordinating pipelines: to reduce the bubbles in later stages\nof PP, ATLAS intelligently shares the WAN bandwidth across\nmultiple pipelines and start the processing of microbatches at\nthe later (in the pipeline) DCs sooner. In doing so: (a) ATLAS\nimproves the WAN utilization and improves the training time,\nand (b) such scheduling eliminates the bubbles that arise\nduring the microbatches.\nStriking the right balance: While the above ideas substan-\ntially improve the training time, we find that it is not always\nbeneficial to use all GPUs. For example, imagine availability\nof 1000 GPUs in one DC and 10 GPUs in a different DC. It is\nbetter to not use the 10 GPUs for the training, as the additional\nnetwork overhead (slower WAN) results in more bubbles and\nelongates the training time. Conversely, in a more balanced\ndistribution (1000 GPUs\u00d72), the training time could improve\nby distributing the job across both DCs. ATLAS uses a heuris-\ntic that computes the optimal split of GPUs across DCs for\nLM training, and enables 'what-if' analysis to explore the cost\nversus training time trade-off, and determine DC locations\nand number of GPUs needed in each DC."}, {"title": "1.2 BUBBLETEA", "content": "While ATLAS improves the training time and eliminates\nbubbles between the microbatches, it does not eliminate bub-\nbles around forward and backward passes (\u00a74.3). Eliminating\nthese bubbles requires either increasing the compute (by pro-\ncessing more layers at a GPU node) or reducing the communi-\ncation overhead. However, we find that both such approaches\ndo not work well in cross-DC training (details in \u00a75).\nTo mitigate the wastage of compute during bubbles, we\nschedule independent workloads during the bubbles. Specifi-\ncally, we find that prefill phase from the LM inference work-\nloads is well suited to consume such bubbles. Inference re-\nquests compose of distinct prefill (digesting the prompt before\nauto-regression or decode starts) and decode phases and re-\ncent works have proposed decoupling between such phases\nduring execution [53, 78]. The completion time of the pre-\nfill phase is highly deterministic due to known input prompt,\nwhich helps BUBBLETEA schedule them effectively during\nbubbles in training.\nBUBBLETEA builds prefill-as-a-service: (a) BUBBLETEA\ncontroller receives prefill requests from the inference con-\ntroller (that receives the requests from the users) and places\nthem to consume bubbles in the training pipeline. (b) BUB-\nBLETEA controller determines the GPU (in the same DC)\nwhere such prefill phase runs and then transfers the KV cache\nto a different GPU in the same DC (outside BUBBLETEA)\nfor decode phase. (c) BUBBLETEA reduces the number of\nGPUs provisioned for inference with a minor penalty (<10%\nof Time-To-First-Token (TTFT))."}, {"title": "1.3 Putting it together", "content": "We implemented ATLAS and BUBBLETEA that work hand-\nin-hand to simultaneously reduce the training time and im-\nprove GPU utilization in cross-DC LM training. Through\ntestbed experiments and large-scale simulations, we show\nthat: (a) ATLAS can reduce the training time up to 17x. (b)\nWhile significant benefits can be attributed to the intuitive\nidea of using multiple TCP connections, the other idea of in-\ntelligently sharing the WAN bandwidth improves the training\ntime by up to 1.82\u00d7 compared to state-of-art schedulers. (c)\nATLAS architecture is effective in using PP across DCs. Such\nan architecture can scale the throughput with more DCs. (d)\nAlgorithm in ATLAS to calculate the optimal number of GPUs\nper DC is effective and fast. (e) ATLAS and BUBBLETEA \u0441\u043e-\nordinate effectively, and BUBBLETEA is able to achieve the\nGPU utilization up to 94%."}, {"title": "2 Background", "content": "In this section, we present a background on training LMs.\nMost of the LMs such as GPT [4], LLama [10], OPT [76],\nMistral [11] use transformer based models [69] with many lay-\ners. Each transformer block comprises various components,\nincluding attention mechanisms and feedforward neural net-\nworks (FFNs), each containing its own neural network. A\ntraining job learns the parameters of neural networks.\nIn each training iteration, a model takes a few samples of\na data called minibatch and performs a forward pass that\ncomputes the loss values for the data samples followed by a\nbackward pass that computes the gradients. Finally, the model\nparameters are learnt by applying the negative of the gradients.\nRecent works save GPU memory by re-running the forward\npass just before the backward pass [19], called recomputation.\nDue to the massive sizes of the models (billions to trillions"}, {"title": "3 Limitations of existing designs", "content": "It is critical to understand the impact of WAN latency and\ncapacity constraints on LM training workloads that span mul-\ntiple DC sites. In this section, we quantify the WAN commu-\nnication overhead on the different modes of parallelism.\nSetup: We spawn 6 A100 GPU nodes across 3 DCs (\u00d72\nGPU nodes each). Each node has a single A100 GPU. For\ncontrolled experiments, we emulate the WAN latency and\nthroughput (set to 10 Gbps) using tc. Fig.1 shows the exper-\nmental setup. We use PyTorch framework for training. We\nmeasure the training time of iterations as we vary the WAN\nlatencies. Lower (higher) WAN latencies indicate that the\nDCs are closer (farther).\nBaseline models: We use two baseline models: (a) GPT-A\n(similar to GPT-3) with context length (L) of 4K and hidden\ndimension (H) of 4K, (c) GPT-B (bigger model than GPT-3)\nwith L,H = 6K,8K. We limit the number of layers to fit on 6\nGPUs. Both GPT-A and GPT-B follow the same architecture\nas GPT-2 [55] and GPT-3 [22]."}, {"title": "3.1 Impact of WAN latency on DP", "content": "In this experiment, all the nodes form a single DP ring, i.e.,\nthe model is replicated across all the nodes in the ring. Indi-\nvidual nodes are fed different minibatches, and the gradients\nare averaged out during the all-reduce communication phase.\nWe measure the impact on the training time due to in-\ncreasing WAN latencies. Fig.2 shows the training time for\nindividual iterations. While compute time stays the same,"}, {"title": "3.2 Impact of WAN latency on PP", "content": "Next, we evaluate the impact of WAN latencies on the\ntraining time when the nodes run PP. In this experiment, we\nsplit the number of layers across all 6 nodes. The nodes in the\nsame DC get adjoining layers to minimize the cross-DC traffic\n(and improve training time). Note that the existing schedulers\nincluding GPipe [37] and Varuna [19] try to overlap compute\nwith communication to reduce the job completion time. Fig.3\nshows the increase in the training time as we increase the\nWAN latencies when using Varuna scheduler that optimizes\nthe PP scheduling over GPipe and Megatron [49].\nLike DP, PP also shows significant slowdown in the training\ntime as WAN latency increases resulting in a reduction in the\nachievable bandwidth. We also observe that the slowdown in\nPP is smaller than the slowdown in DP (y-range different for\nFig. 2 & 3) at the same WAN latency.\nFig.4 shows the timeline of execution of PP for different\nmicrobatches for their forward pass, backward pass, and re-\ncomputation in Varuna for WAN latency = 40 msec. The\ntraining job starts in GPU G-1 and all microbatches execute\nback-to-back. The activations are sent to G-2, which is very\nfast as both the GPUs are in the same DC. However, as G-\n3 is in a different DC, the activations from G-2 to G-3 are\nsent over the low-bandwidth WAN. The size of activations\n(and gradients) is $B \\cdot L \\cdot H$ (B = batch size, L = seq. length =\n6K, H = hidden size = 8K) amounting to 96 MB with fp16\ncomputation. As a result, the activations take significant time\n(around 2.5 seconds) to transfer from G-2 to G-3. The delay\ncontinues to be observed for subsequent microbatches as well\nas for the next GPUs (G-3 to G-6). GPU G-6 processes the\nbackward pass at the end of each forward pass and starts send-\ning the gradients for the other GPUs in reverse order (G-6\nto G-1). Like activations, gradients also suffer due to lower\nWAN bandwidth.\nWe make the following observations: (a) even with a state-\nof-the art scheduler like Varuna, we see significant bubbles\nbetween the forward and backward passes and also between\nmicrobatches, (b) nodes G-5 and G-6 are idle for substantial\ntime in the beginning when the activations are transferred\nbetween G-2 to G-4, (c) like DP, PP also used one TCP con-\nnection between individual pairs of nodes when run with Py-\nTorch, and (b) bandwidth is not shared across activations and\ngradients in PyTorch. When activations for one microbatch\nare transferred between two GPUs, activations for subsequent\nmicrobatches are queued (instead of parallel transfers). This\nhelps finish transfer of existing microbatches and unblock the\nnext GPUs for further processing. Lastly, as activations and\ngradients are sent in opposite directions, they do not compete\nfor the same WAN bandwidth."}, {"title": "3.3 Impact of WAN latency on TP", "content": "Tensor Parallelism (TP) requires high performant networks\nwith low latency and very high bandwidth due to all-reduce\nphase on the critical path. It is mostly advised to run TP within\na node (with multiple GPUs) [30,70] where GPUs are con-\nnected via NVLink offering more than 100 GBps bandwidth.\nWAN networks do not provide such high bandwidths and we\ndo not run TP over WAN. E.g., as shown in Table 1, even with\n40 msec WAN latency, the bandwidth is just 293 Mbps (many\norders of magnitude smaller than NVLink bandwidth)."}, {"title": "4 ATLAS", "content": "We quantified in the previous section how lower bandwidth\non the WAN substantially elongates training time for different\nmodes of parallelism. We built ATLAS toward improving\nthe training time. We touched upon the summary of design\nchoices in \u00a71.1. We expand on them next."}, {"title": "4.1 Using multiple TCP connections", "content": "As mentioned in the previous section, PyTorch [16] simply\nuses one TCP connection for communication. As shown in Ta-\nble 1, as latency between the nodes increases, the achievable\nTCP bandwidth between the nodes shrinks. ATLAS enables\nthe use of multiple parallel TCP connections between VMs in\ndifferent DCs to scale the bandwidth. Fig.5 shows the achiev-\nable bandwidth for single and multiple TCP connections as\nwe vary the distance (hence, WAN latency) between DCs.\nWe have a server VM hosted in Azure US-East DC. To vary\nthe WAN latency, we select client VMs in different DCs in\nUS and Asia. We measure the bandwidth using iperf3 and\nCubic as TCP transport protocol. The x-axis shows the DC\nlocations as we select clients in different DCs. All the VMs\nuse 32 cores. We also tested with BBR [24] (delay-based\ncongestion control) flows and observed similar trends, as dis-\ncussed below, with individual BBR flows offering somewhat\nhigher bandwidth than Cubic.\nAs we spawn more concurrent TCP connections, the cumu-\nlative bandwidth grows. Interestingly, we observe that such"}, {"title": "4.2 3D parallelism across DCs", "content": "As mentioned in \u00a73, TP requires significant bandwidth.\nThus, ATLAS resorts to using TP only across intra-DC nodes\nconnected via NVLink (in the same node) or Infiniband. This\nleaves ATLAS with deciding the division of GPUs across DCs\nin favor of PP and/or DP.\nPP across DCs is beneficial: Prior works including [66]\nhave shown that doing PP across DCs and DP within DCs is\nbeneficial in geo-distributed training. Recall that PP tries to\noverlap compute with communication. Bubbles in compute\nare formed when communication takes longer than compute.\nFor context length L, hidden size H, and batch size B, com-\nmunication time is $O(BLH)$ (linear with L and H), while the\ncompute time is $O(BLH^2)$ (for MLP layers) + $O(BHL^2)$ (for\nattention layers) \u2013 quadratic with L and H [49]. Thus, the\ngap between compute and communication shrinks as model\nsize grows (increasing L, H, or both). On the other hand, we\ncan do DP across DCs where nodes in the same all-reduce\nring are in different DCs. However, the all-reduce runs on\ncritical path and its time depends on number of parameters of"}, {"title": "4.3 Improving training time through\nco-ordination", "content": "When running 3D parallelism according to the above archi-\ntecture, the existing schedulers such as Varuna and Gpipe as-\nsume there is no co-ordination between different DP pipelines.\nAs a result, each DP pipeline gets its own share of bandwidth\n(e.g., 5 Gbps) and roughly follow the same schedule across\ndifferent pipelines as shown in Fig.6(a). In essence, such shar-\ning is spatial, as each pipeline has its own share of bandwidth.\nNote that the nodes higher in the PP pipeline (e.g., node G-\n11 and G-12 in Fig.6(a) are idle early in the execution as they\nhave not received their microbatches. We observe that there\nis higher aggregate bandwidth available by using multiple\nnodes across DP pipelines. In ATLAS, different DP pipelines\nwork in tandem we provide higher bandwidth to one DP\npipeline by splitting the activations across multiple nodes (at\nan expense of lower bandwidth to other DP pipelines) so that\nDP pipeline can speedup sending its activations to the next\nnodes. We call it temporal bandwidth sharing.\nWe illustrate this key idea in Fig.6(b). Imagine ratio of\ncommunication to compute latency (C) to 2x. Imagine there\nare 2 DP pipelines each leveraging 5 Gbps WAN bandwidth\nbetween two nodes in different DCs. With schedulers like\nVaruna, as in Fig.6(a), downstream GPUs in the pipeline (PP)\nin a different DC (node G-11) need to wait for longer for\nactivation transfer to happen using 5 Gbps WAN bandwidth.\nwith ATLAS, the entire 2 \u00d7 5 = 10 Gbps bandwidth (across\nboth DP instances) is available to each PP thus speeding up\nactivation transfers to just 1 time-slot instead of 2. As a result,\nfirst DP instance progresses faster and makes the microbatches\navailable to node G-11 and G-12 in DC-3 quickly. These\nnodes then start their backward pass sooner too. Cumulatively,"}, {"title": "4.4 Schedule for temporal bandwidth sharing", "content": "ATLAS uses a heuristic that calculates the schedule for\nforward and backward passes, as detailed next. The schedule\nis pre-computed by ATLAS before the training starts, and\nadjusts to any stragglers.\n(1) ATLAS groups DP instances into DP-cells during the ini-\ntialization phase. DP instances within a DP-cell coordinates\nusage of the aggregate WAN bandwidth. DP-cells operate\nindependent of each other. In Fig.6(b), DP-1 and DP-2 form\na single DP-cell. Each DP instance in a DP-cell is assigned\na LocalDPRank and aggregate WAN bandwidth is shared\ntemporally between these DP instances based on their Lo-\ncalDPRanks. The bubbles between microbatches are because\nWAN communication is slower than compute. To eliminate\nbubbles, we set number of DP pipelines in a DP cell as C,\nwhere C is communication to compute ratio.\n(2) While scheduling forward passes for a microbatch on\nany DP pipeline, ATLAS filters only those for which activa-\ntions/gradients in memory at any point of time at any stage in\nthe pipeline is within the peak memory limit. As a result: (a)\nwe are always within the peak memory limit, unlike Varuna,\nwhich may exceed memory limits, and (b) we don't block\ncomputation and communication phases on other DP pipeline\nbecause of unnecessary utilization of the aggregate WAN\nbandwidth for transmitting activations/gradient that would"}, {"title": "4.5 DC selection", "content": "Above sections assume a given topology of GPUs and opti-\nmize for such topology. However, it is not always beneficial\nto use all the GPUs. For example, if 1,000 GPUs are in the\nsame DC, and 10 GPUs are in a different DC, then it's better\nto forgo the 10 GPUs and avoid slowdown in training time\ndue to poor WAN conditions.\nNow we describe how ATLAS assists in finding the right"}, {"title": "5 BUBBLETEA", "content": "As seen already, ATLAS reduces the training time using\ntemporal bandwidth sharing. Additionally, it eliminates the\nbubbles between microbatches and collates them together.\nHowever, it does not eliminate the GPU compute bubbles\nwhere GPU is idle. We find that despite ATLAS finishing\ntraining faster (and improving GPU utilization), it results in\nup to 60% time spent in bubbles.\nThere are two strawman's approaches to reduce such resid-\nual bubbles, as follows. (a) Increasing compute: as compute\nis shorter than communication in PP, one way is to increase\ncompute to match communication. This can be accomplished\nby each GPU processing more layers. As GPU memory is\nlimited, the layers could be loaded from CPU (like [57]). How-\never, we find that such a design is bottlenecked by PCIe band-\nwidth, where the time to load a 1 billion parameter layer from\nCPU is at least 100 msec even after using pinned-memory [6]\n(larger than processing the layer itself). (b) Decreasing com-\nmunication: we tried reducing communication through com-\npression that resulted in 2\u00d7 slowdown in training latency\nwhile achieving similar loss rates (\u00a76.7).\nWe take a novel approach to reduce bubbles. We build BUB-\nBLETEA to schedule a different workload during the bubbles.\nSpecifically, we observe that the prefill phase of the infer-\nence workload is well suited for such scheduling. Inference\nrequests have distinct prefill and decode phases [18,53] \u2013 the\nprefill phase processes the input prompt and the decode phase\ngenerates the output tokens in an autoregressive fashion. The\nexecution time of decode phase is unknown at the start as it\ndepends on the number of tokens generated. In contrast, the\nexecution time of prefill phase is highly predictable as it only\ndepends on the input prompt (size) that is known when the re-\nquest arrives. Leveraging this insight, we build BUBBLETEA\nthat provides prefill-as-a-service that we detail next."}, {"title": "5.1 BUBBLETEA design", "content": "The key goal for BUBBLETEA is to schedule the prefill\nstages of inference requests without interfering with the train-\ning job. To do so, we do not simultaneously schedule training\nand prefill. Rather, the prefill kernels are issued only when the\ntraining job observes (predictable) bubbles. Similarly, based\non the prefill estimated latency, we choose the GPU so that\nprefill ends (compute becomes available again) before the\ntraining job resumes. This approach significantly improves"}, {"title": "6 Evaluation", "content": "We evaluate ATLAS and BUBBLETEA using testbed exper-\niments and large-scale simulations. The key observations are:\n(a) ATLAS, with its optimizations, improves the cross-DC\ntraining time by up to 17\u00d7 compared to baselines like Varuna,\nGPipe and Megatron. (b) Even when the baselines leverage\nmultiple TCP connections, ATLAS improves training time by\nup to 1.82x than baselines. (c) ATLAS is able to scale the\nthroughput as we add more DCs. This is in stark contrast with\nfindings in \u00a73 where throughput reduced when using WAN.\n(d) BUBBLETEA could schedule the prefill requests during\nbubbles and achieve a GPU utilization of up to 94% in the\ntraining cluster, up from 45% when using only ATLAS."}, {"title": "6.1 Setup", "content": "Setup for testbed experiments: Our evaluation uses a setup\nsimilar to Fig.1 consisting of 12 GPUs in 3 DCs (4 \u00d7 3). We\nvary the latency between DCs using tc command. We set\nthe inter-DC bandwidth to be 20\u00d7 smaller than intra-DC\nbandwidth (e.g., Infiniband bandwidth is 100 Gbps between\ntwo nodes within same DC; inter-DC bandwidth is 5 Gbps).\nAll GPUs are A100 80 GB running PyTorch. We compare\nthe training time using ATLAS versus baselines including\nGPipe [37], Megatron [49] and Varuna [19]. For the baselines,\nwe divide the GPUs among 3 DP pipelines with 4 PP stages.\nFor ATLAS, we have 1 DP-cell with 3 DP pipelines with 4\nPP stages as above. While we do not leverage TP for the\ndemonstrations, TP could be used intra-DC. We use GPT-A\nand GPT-B models (\u00a73) for the evaluation. We set the number\nof microbatches to 4 (inline with the 4 PP stages) and 16."}, {"title": "6.2 ATLAS improves training time", "content": "We show that ATLAS is able to improve the training time\nby up to 17\u00d7 compared to the baselines using the 2 key ideas\nof using multiple TCP connections and temporal bandwidth\nsharing. We compare ATLAS against default baselines (that do\nnot use multiple connections; we give the benefit of multiple\nTCP connections to the baselines in the next section). We\nvary the WAN latency from 10 msec to 40 msec. Note that,\nirrespective of the latency, ATLAS gets same bandwidth due\nto multiple TCP connections. In contrast, the bandwidth for\nthe baselines reduces as WAN latency increases (Table 1).\nAs shown in Fig.8, ATLAS is able to reduce the training\ntime substantially. We find: (a) Compared to Gpipe, Megatron\nand Varuna, ATLAS is able to reduce the training time by up\nto 17x, 13x, and 12\u00d7 resp. across different WAN latencies\nand number of microbatches (M). (b) As expected, ATLAS\ngains increase as WAN latencies increase. This is because"}, {"title": "6.3 Efficient use of cross-DC GPUs", "content": "We demonstrate now through simulations how ATLAS im-\nproves throughput over Varuna by up to 24% (potentially\nsaving 10s of millions of dollars at scale) by using GPUs\nacross different DCs. In the previous experiments, we used\n12 GPUs. In this section, we use simulations to evaluate the\nbenefits on ATLAS over Varuna for bigger clusters.\nIn this experiment, we consider two DC-sets (simulated)."}, {"title": "6.4 Cross-DC GPU balancing", "content": "In the last experiment, we used all the GPUs available. In\nthis experiment, we show that it's not always better to use\nall available GPUs across DCs, and Algorithm 1 is needed\nto calculate the smallest number of GPUs required for the\nhighest throughput. In this experiment, we pick two DCs \u2013 the\nfirst DC has a fixed number (600) of GPUs and we vary the\nnumber of GPUs in the second DC from 0 to 600 in steps of\n10% (denoted by F). Through simulations (same setup as last\nexperiment with C= 2), we calculate the training throughput.\nWe use Algorithm. 1 (\u00a74.5) to calculate the optimal number\nof GPUs in individual DCs for highest throughput.\nFig.11 shows the improvement in training throughput nor-\nmalized to using GPUs in a single DC. It can be seen that\nwhen F increases to 10%, there is no improvement in through-\nput. Algorithm. 1, in this case, falls back to only first DC\n(no GPUs from second DC). This is because, for such F, we\nneed to send the activations from first DC to second DC over\nthe WAN. This inflates the latency to complete one iteration\nby roughly 11% and, hence, erases any throughput gains by\nadding the additional 10% GPU compute. We observe similar\nbehavior when increasing F from 20% to 30%. This reinstates"}, {"title": "6.5 BUBBLETEA boosts GPU utilization", "content": "In this section, we show how BUBBLETEA can significantly\nimprove the GPU utilization by scheduling prefill stages of\ninference requests during bubbles in ATLAS. We use the 12\nGPU setup and schedule prefills as mentioned in \u00a75.1 with 5\nmicrobatches running GPT-A. As there is only one DP-cell,\nwe set the PP depth of the inference model to 1. Recall that\nthe BUBBLETEA controller combines (1) the scheduling plan\nfrom ATLAS controller and (2) signals from the GPUs when\nthey finish processing training microbatches. Accordingly,\nBUBBLETEA issues the inference kernels.\nFig.12 shows the GPU utilization for two of the GPUs in\na single (training) pipeline. We make the following observa-\ntions: (a) BUBBLETEA is able to schedule the prefill stages\nperfectly during the training bubbles. This shows that BUB-\nBLETEA controller can accurately detect the bubbles (through\ncombining scheduling plan and GPU signals). (b) In doing\nso, BUBBLETEA boosts the GPU utilization to around 94%\nleaving very small residual bubbles. (c) There is no impact on\nthe training job. We found BUBBLETEA leaves some bubbles\nbetween prefill and training microbatches that helps resume\ntraining without delay. (d) BUBBLETEA does not schedule\nprefill if the bubble is not big enough.\nImpact on GPU utilization: Fig. 12 shows BUBBLETEA goes\none step further than ATLAS and improves the GPU utilization\nfrom 45% (ATLAS-only) to 94% using inference prefills."}, {"title": "6.6 BUBBLETEA PP overhead", "content": "In the last section, due to limited GPUs, we could not eval-\nuate the impact of PP for inference model. Using PP for infer-\nence model, different layers are assigned to different GPUs\nas memory available for the model is constrained (\u00a75). In this\nsection, we evaluate the overhead of such pipelining on TTFT\nfor Llama3 8-billion model. We use an analytical model.\nThe overhead in terms of the latency of PP is $O_1 = BLH$\n(\u00a74.2) between two layers. The total overhead is $O"}]}