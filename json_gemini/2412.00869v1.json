{"title": "Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting", "authors": ["Thilini Wijesiriwardene", "Ruwan Wickramarachchi", "Sreeram Vennam", "Vinija Jain", "Aman Chadha", "Amitava Das", "Ponnurangam Kumaraguru", "Amit Sheth"], "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like \"Oxygen is to Gas as <blank> is to <blank>\" requires identifying the semantic relationship (e.g., \"type of\") between the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair that shares the same relationship (e.g., \"Aluminum\u201d and \u201cMetal\u201d). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.", "sections": [{"title": "1 Introduction", "content": "The ability to form analogies enables humans to transfer knowledge from one domain to another, making it a core component of human cognition (Hofstadter, 2001; Holyoak et al., 2001; Minsky, 1988). Specifically, in analogy-making, the emphasis is on the relations among objects, as it is the system of relations that is compared across domains rather than the specific objects and their attributes (Gentner, 1983). Researchers have identified several types of analogies within the domain of NLP, such as proportional analogies (analogies among word/term pairs) (Brown, 1989; Chen et al., 2022; Ushio et al., 2021; Szymanski, 2017; Drozd et al., 2016), sentence-analogies (Jiayang et al., 2023; Afantenos et al., 2021; Zhu and de Melo, 2020; Wang and Lepage, 2020) and analogies of longer text (Sultan and Shahaf, 2022; Sultan et al., 2024). Proportional analogies, which is the focus of this paper, are presented in the form A:B::C:D, meaning A is to B as C is to D. These analogies involve four terms, where the relationship between the first pair of terms (A and B) is similar to the relationship between the second pair of terms (C and D).\nGenerative Artificial Intelligence (GenAI) models, particularly those recognized for their capacity to generate high-quality textual outputs, have emerged as a focal point of research in contemporary Natural Language Processing. The capabilities of these models are typically evaluated through a range of tasks, including question answering (Arora et al., 2022; Kasai et al., 2023), reasoning (Zhang et al., 2024), paraphrasing (Witteveen and Andrews, 2019), sentiment analysis (Kheiri and Karimi, 2023) and, more recently, analogical reasoning (Bhavya et al., 2024; Wijesiriwardene et al., 2023). Notably, Wijesiriwardene et al. (2023) have demonstrated that SAT-style Proportional analogies pose significant challenges for LLMs, particularly when solved using intrinsic distance-based similarity measures. Conversely, Webb et al. (2023) have shown that GPT-3 can surpass human performance in solving proportional analogies, though these findings were based on a dataset with limited size (774 data points) and a narrow range of distinct semantic relations among term pairs (seven semantic relation types). Motivated by the need"}, {"title": "2 Related Work", "content": "In this section, we introduce related literature on the main topics of our paper: proportional analogies and LLMs, prompting techniques, and knowledge-enhancement in LLM prompting."}, {"title": "2.1 Proportional Analogies and LLMs", "content": "One of the earliest methods for solving proportional analogies was Latent Relational Analysis (LRA), introduced by Turney (2005). LRA determines analogy by measuring the similarity in semantic relationships shared between word pairs, considering them analogous if they exhibit a high degree of relational similarity. With the advent of neural networks, vector difference-based methods (Vylomova et al., 2016; Allen and Hospedales, 2019; Mikolov et al., 2013) were used to address proportional analogies. As LLMs based on the Transformer architecture (Vaswani et al., 2017a) gained prominence, researchers began investigating the potential of LLMs, particularly Generative"}, {"title": "2.2 Prompting and Knowledge-enhanced Prompting", "content": "GenAI models are built on LLMs that are trained on extensive datasets and optimized for various tasks, including question-answering. This training implies that these models encapsulate the knowledge in the data, allowing them to effectively answer natural language queries (Roberts et al., 2020; Zhu and Li, 2023). Prompting involves transforming an input query into a structured natural language statement (prompt) and presenting it to the model, which then guides the output generation process of the model. (Schulhoff et al., 2024; Hadi et al., 2023; Liu et al., 2023). Generating outputs through prompting requires only forward passes during inference time, without any weight updates. Prompts can be created either manually (Wei et al., 2022; Schulhoff et al., 2024) or automatically (Ye et al., 2023; Reynolds and McDonell, 2021; Deng et al., 2022); in this work, we employ the more intuitive manual approach.\nPrompts can be categorized based on the context they provide. Zero-shot prompts (Brown, 2020) contain only instructions related to solving a specific task, whereas Few-shot prompts (Brown, 2020) include both the instructions and one or more examples. Providing examples when querying models is a paradigm broadly known as In-context Learning (ICL) (Brown, 2020). Chain-of-Thought (CoT) Prompting is designed to guide models through the reasoning process required to solve a task by presenting an exemplar that includes the question, reasoning path, and correct answer (Wei et al., 2022) or by just incorporating a thought-inducing phrase such as \u201cLet's think step by step\" (Kojima et al., 2022) (Zero-shot-CoT). Unlike conventional CoT prompting, which often includes an exemplar, our adaptation termed TKP does not pro-"}, {"title": "3 Approach", "content": "As illustrated in Figure 1, given a proportional analogy MCQ where the question consists of a single term pair (e.g., \u201cOxygen\u201d and \u201cGas\u201d), the GenAI model is required to provide the correct answer choice from five, four or three choices. Zero-shot Prompting, only include the MCQ and a simple instruction on how to produce the output without any knowledge enhancement added to the prompt. Next, we enhance the Zero-shot Prompt with exemplars of solved MCQs from the dataset. We consider this approach as enhancing the prompt with \u201cexemplar knowledge\u201d and refer to this prompting technique as Few-shot Prompting. We experiment with one exemplar (One-shot Prompting) and five exemplars (Five-shot Prompting). Then a combination of lexical, commonsense, and world knowledge from structured sources-WordNet, ConceptNet, and Wikidata, respectively - is added to the Zero-shot Prompts for knowledge enhancement, resulting in what we call SKP. Finally, the zero-shot prompt"}, {"title": "3.1 Dataset Creation", "content": "We introduce a 15K dataset of proportional analogies containing 5-way, 4-way and 3-way MCQs. Table 1 presents the dataset statistics along with examples from the dataset. We generate 14K questions out of the 15K based on the work by (Yuan et al., 2023). Yuan et al. (2023) introduced a an automatically generated million-scale analogy knowledge base with diverse relational structures among the analogous term pairs. We adopt this resource to develop n-way (n=[3, 4, 5]) MCQs as follows. A single n-way MCQ consist of a pair of terms representing the question and five term pairs representing the answer choices, among which only one term pair is the correct answer. The semantic relationship between the term pair in the question is the same as the semantic relationship shared between the term pair which is the correct answer. The rest of the incorrect answer choices consist of term pairs with different semantic relationships among them.\nThousand data points out of the 15K are borrowed from work by Ushio et al. (2021); Turney and Littman (2003); Boteanu and Chernova (2015) and contain 5-way, 4-way and 3-way MCQs. We highlight that, compared to previous proportional analogy MCQ datasets used for research (Webb et al., 2023; Ushio et al., 2021), the current dataset provides a significant increase in question quantity (~15-times) and diversity (with regard to diversity of semantic relations among terms). Our dataset also includes the semantic relationship shared by the question term pair compared to other datasets that do not include this information (Ushio et al., 2021; Turney and Littman, 2003; Boteanu and Chernova, 2015)."}, {"title": "3.2 Model Details", "content": "GenAI models are designed to generate content that are often indistinguishable from human-produced output. Current state-of-the-art GenAI models are largely based on the Transformer architecture (Vaswani et al., 2017b). In this work we compare the following popular open-source and proprietary GenAI models for their ability to solve proportional word analogy MCQs by incorporating variety of knowledge: (i) Falcon, a causal decoder-only model (Almazrouei et al., 2023), (ii) FlanT5 (Longpre et al., 2023), a T5 (Raffel et al., 2020a) based model trained on the Flan collection of datasets, (iii) GPT2 (Radford et al., 2019a), the first series of models to popularize in-context instructions, (vi) Mistral (Jiang et al., 2023), leveraging transformers architecture (Vaswani et al., 2017b) with several new introductions such as sliding window attention and pre-fill chunking, (v) Orca (Mukherjee et al., 2023), based on LLaMA model family (Touvron et al., 2023) and fine-tuned on complex explanation traces obtained from GPT-4 (Achiam et al., 2023), (vi) Zephyr (Tunstall et al., 2023), a fine-tuned version of Mistral trained on public datasets and optimized with knowledge distillation techniques. (vii) CodeT5 (Wang et al., 2021c), a unified pretrained encoder-decoder transformer model leveraging code semantics and finally (viii) CodeParrot (Jain, 2023), a model based on GPT-2 and trained to generate python code (ix) GPT-3.5-Turbo. Further details of the models used are presented in Appendix A."}, {"title": "3.3 Prompting Techniques", "content": "Currently, the most popular approach to Multiple Choice Question Answering (MCQA) is via cloze-style prompting (Brown, 2020; Robinson et al., 2023) where each answer choice is concatenated to the question separately and scored independently by the language model (LM). This style of prompting is problematic since it prevents the LM from comparing and contrasting all available options simultaneously. Additionally, it is computationally expensive, as it requires multiple forward passes through the LM to identify the correct answer (Robinson et al., 2023). To address these limitations, we adopt the prompt phrasing introduced by Robinson et al. (2023) with task-specific modifications. Specifically, the question and its symbol-enumerated candidate answers are provided to the model as a single prompt. Robinson et al. (2023) do not include specific instructions in the prompt for the model to output only the choice symbol. But"}, {"title": "3.3.1 Zero-shot Prompting", "content": "In Zero-shot Prompting, the question, all multiple choice answers and the instructions are provided in natural language (no knowledge is provided)."}, {"title": "3.3.2 Few-shot Prompting", "content": "We demonstrate the task to the model by providing several exemplars in the form of question, answer choices and the correct answer choice. Then the actual question and answer choices are provided requiring the model to choose the correct answer choice. We employ one-shot and five-shot prompting under the few-shot prompting strategy where one example and five examples are provided respectively. We select these quantities of exemplars to strike a balance between the models' maximum accepted context length and the computational resources required. To obtain the exemplars, we employ a semantic similarity based filtering mechanism as follows. We encode each proportional analogy MCQ in the dataset using a SOTA sentence encoding transformer model, and identify the most semantically similar single example/ five examples based on Cosine similarity."}, {"title": "3.3.3 Structured Knowledge Prompting (SKP)", "content": "We retrieve knowledge from structured sources, filter it, and then integrate the resulting refined knowledge into the prompts. We detail this process in the subsequent sections.\nKnowledge Retrieval. We leverage the following widely-used large knowledge sources to obtain three types of knowledge: (i) Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), which provides world knowledge in the form of explicit information about specific instances, encompassing billions of nodes and edges (Wang et al., 2021a); (ii) ConceptNet (Speer et al., 2017), a general-domain commonsense knowledge graph with 799,273 nodes and 2,487,810 edges; and (iii) WordNet (McCrae et al., 2019), a lexical database for the English language containing 161,338 words, 120,135 synsets, and 415,905 semantic relations.\nWe retrieve knowledge from above sources as follows. Since analogies focus on relations oppose to entities or entity attributes (Gentner, 1983), when retrieving knowledge from knowledge sources we focus on path finding approaches oppose to subgraph extraction approaches. To extract both world and commonsense knowledge, we utilize the pathfinding approach by Lin et al. (2019) that identifies connections between each term pair (in both the question and answer choices). Specifically, we extract paths of length $k$ from ConceptNet and Wikidata. When retrieving lexical knowledge from WordNet, we extract the shortest path between term pairs.\nKnowledge Filtering. For each term pair in the question and answer choices, multiple knowledge paths may be retrieved. To ensure the prompts stay within the maximum context length limit of the evaluated language models, we filter the retrieved paths and retain a single path for Wikidata and ConceptNet (See Figure 2). Filtering is not performed on WordNet since a single path (shortest) is always retrieved.\nThe filtering mechanisms we employ are as follows: (i) Random Filtering, where one path is randomly selected from the list of available paths; and (ii) Semantic Filtering, which selects the path most semantically similar to the term"}, {"title": "3.3.4 Targeted Knowledge Prompting (TKP)", "content": "When solving proportional analogies, humans typically examine the question term pair, identify the semantic relationship between the two terms, and select the answer pair that shares the same or a similar relationship. Inspired by this cognitive process, we modify the traditional Chain-of-Thought (CoT)"}, {"title": "4 Experimental Setting", "content": "We have conducted a comprehensive set of experiments across nine GenAI models over six prompt variants on a 15K dataset, totalling to 54 (9X6) experiments."}, {"title": "4.1 Implementation Details", "content": "We use API requests for GPT-3.5-Turbo and checkpoints from Hugging face for open-source models. The models are evaluated with following hyper parameter settings, temperature = 0.1, top_p=0.1 and repetition_penalty=1.2 to elicit more concrete answers for the MCQs. We use Sentence Transformers to identify semantically similar exemplars and to perform semantic knowledge filtering. We utilize Wikidata knowledge from (Wang et al., 2021b), ConceptNet knowledge from conceptnet and WordNet knowledge from Open English WordNet (2023)."}, {"title": "5 Results and Discussion", "content": "Proportional analogy multiple-choice questions (MCQs) are presented to each GenAI model using the previously described prompts. The model's response is extracted from the generated output, and accuracy is measured using Exact Match Accuracy (EMA) (Rajpurkar et al., 2016). While more flexible evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are commonly used to assess GenAI-generated outputs, we employ EMA because MCQs are inherently evaluated in a binary manner, where partial correctness is not rewarded. We report EMA as a percentage for each model and prompt variant. The results are presented in Table 2."}, {"title": "5.1 Model Performance and Prompting Techniques", "content": "The highest overall performance was attained by GPT-3.5-Turbo, achieving an EMA of 55.25%.\nThis result underscores the challenge that proportional analogies pose for current state-of-the-art GenAI models. This accuracy was obtained through Targeted Knowledge Prompting where the prompt was enhanced with targeted knowledge. Interestingly, the same model, when enhanced with structured knowledge, underperformed with an accuracy of 38% (EMA for SKP[random] is 38.29% and SKP[semantic] is 38.79%), compared to Zero-shot prompting (EMA 45.7%). This suggests that simply adding knowledge, even from diverse sources, may not be beneficial for cognitively demanding tasks such as proportional analogy completion. Out of the nine models four (Falcon, Flan, Mistral and GPT-3.5-Turbo) performs the best when prompted with Targeted Knowledge Prompts and two (GPT2 and Orca) performs the best with Zero-shot prompts with no knowledge enhancement. CodeT5 performs the best with one-shot prompts and Zephyr and CodeParrot performs the best with five-shot prompts. We also observe that models trained specifically on code generation such as CodeT5 and CodeParrot (specially Code-Parrot) perform at the lower end of the spectrum despite the demonstrated abilities of them to perform well on other MCQ datasets Robinson et al. (2023). We believe this is due to the challenging nature of the proportional analogy completion task."}, {"title": "5.2 Role of Structured Knowledge in Model Performance", "content": "Although enhancing prompts with structured knowledge does not consistently improve model performance compared to other prompting techniques, SKP[semantic] leads to slight increases in EMA values (ranging from 0.01% to 1.32%) compared to SKP[random], across all models except GPT-2 and Mistral (see Table 4). We identified a subset of MCQs (19.96%) where all three types of knowledge were available and conducted"}, {"title": "5.3 Exemplar Quantity vs. Model Performance", "content": "Brown (2020) demonstrated that the accuracy of large language models improves with an increase in the number of exemplars. However, Liu et al. (2022) found that the benefits diminish beyond 20 exemplars in certain cases. Similarly, in our study, increasing exemplars from one to five decreases EMA in six out of nine models (see Table 2), leading us to limit exemplars to a maximum of five."}, {"title": "5.4 Cost of Knowledge Acquisition vs. Model Performance", "content": "In this study, we utilize three types of knowledge to enhance prompts: exemplar knowledge, structured knowledge, and targeted knowledge. Among these, exemplar knowledge has the least acquisition cost since it is readily available from the dataset itself requiring no additional resources. Structured knowledge, on the other hand, is more expensive to acquire because it necessitates accessing external knowledge bases or graphs and filtering knowledge, which incurs computational overhead. Targeted knowledge is the costliest to acquire, as it involves identifying the specific semantic relationship between the question term pairs. This semantic relationship is not always readily available, requiring human annotation (for instance, in our dataset of 15K data points, 1K data points lacked this semantic information, necessitating human annotation). As shown in Table 2, targeted knowledge, being the most expensive to acquire, led to the best performance in four models (Falcon, FlanT5, Mistral and GPT-3.5-Turbo) including the peak performance (55% EMA) from GPT-3.5-Turbo. In contrast, structured knowledge, the second most costly, did not result in any model's best performance. Although exemplar knowledge is the least expensive, three models performed best with it (Zephyr and CodeParrot in Five-shot; CodeT5 in One-shot)."}, {"title": "6 Conclusion and Future Work", "content": "We explore the ability of nine LLMs to solve proportional analogies via different knowledge-enhanced prompting techniques on a 15K MCQ dataset that we introduce. Our extensive experiments show that most LLMs perform their best when targeted knowledge is incorporated into prompts as opposed to exemplar knowledge and"}, {"title": "7 Limitations", "content": "When incorporating knowledge paths in SKP, the exact semantic relationship between the question term pair may sometimes be retrieved from the knowledge sources. We define this specific information as targeted knowledge. As a result, these instances can be categorized as SKP with targeted knowledge. However, we do not currently verify or adjust for them.\nIn this work, we employed manual prompt cre-"}, {"title": "A Model Details", "content": "Falcon (Almazrouei et al., 2023): The Falcon model used in this work is the Falcon-7B-Instruct"}]}