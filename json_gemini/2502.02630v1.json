{"title": "scBIT: Integrating Single-cell Transcriptomic Data into fMRI-based Prediction for\nAlzheimer's Disease Diagnosis", "authors": ["Yu-An Huang", "Yao Hu", "Yue-Chao Li", "Xiyue Cao", "Xinyuan Li", "Kay Chen Tan", "Zhu-Hong You", "Zhi-An Huang"], "abstract": "Functional MRI (fMRI) and single-cell transcriptomics are pivotal in Alzheimer's disease (AD) research,\neach providing unique insights into neural function and molecular mechanisms. However, integrating\nthese complementary modalities remains largely unexplored. Here, we introduce scBIT, a novel method\nfor enhancing AD prediction by combining fMRI with single-nucleus RNA (snRNA). scBIT leverages\nsnRNA as an auxiliary modality, significantly improving fMRI-based prediction models and providing\ncomprehensive interpretability. It employs a sampling strategy to segment snRNA data into cell-type-\nspecific gene networks and utilizes a self-explainable graph neural network to extract critical subgraphs.\nAdditionally, we use demographic and genetic similarities to pair snRNA and fMRI data across\nindividuals, enabling robust cross-modal learning. Extensive experiments validate scBIT's effectiveness\nin revealing intricate brain region-gene associations and enhancing diagnostic prediction accuracy. By\nadvancing brain imaging transcriptomics to the single-cell level, scBIT sheds new light on biomarker", "sections": [{"title": "1. Introduction", "content": "Alzheimer's disease, a progressive neurological disorder impacting cognitive function and\ndaily life, affects an estimated 50 million people worldwide and is diagnosed using genetic\nsequencing and medical imaging techniques [1]. Genetic sequencing provides detailed\ninformation about an individual's genome at a microscopic level, including specific genetic\nvariations and genetic risk-related data [2, 3]. Meanwhile, medical imaging techniques offer\nmacroscopic insights, providing detailed images of brain structure, metabolic activity, and\nidentifiable areas of abnormality, aiding in the identification of lesions, atrophy, and other\nvisible brain changes [4, 5]. Both imaging and genetic information are vital markers for\ndiagnosing AD [6, 7]. Their combined use has led to the emergence of studies such as imaging\ngenomics and imaging transcriptomics, significantly contributing to a more comprehensive\nunderstanding of the disease's underlying mechanisms at both genetic and neuroimaging levels\n[8, 9].\nImaging genomics and transcriptomics represent advanced interdisciplinary methodologies\nthat integrate genetic and neuroimaging datasets, aiming to elucidate the correlations between\ngenomic variations, transcriptomic profiles, and neuroimaging phenotypes [10, 11]. The\nfoundational hypothesis posits that distinct gene expression signatures are associated with\nspecific neuroimaging characteristics, thereby enhancing the pathophysiological understanding\nof neuropsychiatric disorders and interindividual variability. This goes beyond what can be\nderived from imaging alone, which typically reveals structural or functional alterations without\ndirect molecular context. In comparison with gene sequencing-based diagnostic methods, these\napproaches confer a notable advantage over traditional gene sequencing-based diagnostic\nmethods by potentially obviating the necessity for invasive biopsy procedures, thereby reducing\nthe attendant morbidity and mortality risks [12, 13]. Furthermore, they facilitate a\ncomprehensive spatiotemporal analysis of tumor heterogeneity, an endeavor that is unattainable\nvia conventional serial biopsy methods.\nGenome-wide association studies (GWASs) now involve over a million participants and\nanalyze millions of genetic variants throughout the genome, identifying dozens to thousands of\ngenetic variants statistically linked to various diseases including AD. However, owing to the\n'missing heritability' phenomenon, which is partly attributed to pleiotropy, polygenicity, and\ncoarse phenotype resolution, these GWAS variants often do not correspond to functional\ngenetic variants [14]. With the use of imaging data, imaging transcriptomics is able to delineate\nAD imaging-derived phenotypes, presenting new opportunities for identifying functional\nvariants associated with more precise and fine-grained phenotypes. While imaging genomics\ndelves into the genetic variants related to imaging data, the recent advent of imaging\ntranscriptomics explores gene expression patterns across the brain and relate these patterns to\nvarious structural and functional properties as quantified by neuroimaging techniques,\nproviding a powerful tool for understanding the molecular mechanisms underlying AD from an\nalternative perspective [15, 16].\nUsing atlases of whole-brain gene expression data, research in this field aims to uncover\nspatial gene expression patterns that correlate with brain structure and function, thereby\nproviding insights into the molecular mechanisms underlying brain organization, and\nfacilitating the diagnosis of neurological disorders such as AD. However, as emphasized by\nMandal et al. [17], several significant limitations within current imaging transcriptomics studies\nare impeding their application. Firstly, while gene set enrichment is a routine practice in current\nimaging transcriptomic studies, its analysis results lack interpretation partly because the\nmolecular pathways they reveal are derived from post hoc enrichment analyses rather than\nbeing directly computed from the expression data. Secondly, current analyses in imaging\ntranscriptomics face challenges in fully integrating cell type information, which is pivotal as it\ntypically represents the primary source of variation across bulk brain transcriptomic datasets.\nFinally, existing datasets encounter the challenge of limited sample sizes, exemplified by the\nAllen Human Brain Atlas (AHBA) [18], the most comprehensive anatomical expression atlas,\nwhich is derived from data from merely 6 individuals, predominantly from the left hemisphere.\nAs the accumulation of data from single-cell molecular profiling technologies related to AD\nincreases [19], it offers a hopeful prospect by providing crucial supplementary information for\nimaging transcriptomics, to some extent alleviating issues related to data sources. It introduces\na new level of single-cell expression data, providing a foundational basis for comprehensive\nmolecular pathway characterization, including gene regulatory networks. Additionally, it\ninherently preserves differential expression across cell types and expand the sample size of\npatient data. The integration of snRNA sequencing and neuroimaging data effectively addresses\nthe current challenges encountered by imaging transcriptomics from a data-sourcing\nperspective, while simultaneously presenting significant computational challenges that warrant\ncareful consideration [17]. The foremost challenges lie in how to match the two modalities of\ndata from different patients and how to coarsen the expression information from individual\ngenes at the single-cell level to a meaningful feature representation at an appropriate granularity,\nand subsequently align it with neuroimaging features.\nIn this article, to address these challenges, we introduce the scBIT(acronym for single-cell\nBrain Imaging Transcriptomics) model, which innovatively combines single-cell\ntranscriptomics data with rs-fMRI data for the diagnosis of AD. Based on a cell-type-scale gene\ninteraction subgraph representation method, the scBIT model can learn the attention between\nbrain region and gene subgraphs through contrastive learning and subsequently predict the\nprobability of AD using any fMRI data without any prior hypothesis. The proposed approach\nunveils a pioneering perspective for exploring brain region and gene expression patterns on a\nsingle-cell or single-nucleus scale, while introducing an innovative framework that harnesses\nthe power of snRNA-fMRI cross-modal data for the diagnosis of AD. The main contributions\nof this work are summarized as follows:\n(1) We introduce scBIT, the first computational model to integrate single-cell RNA data with\nMRI data for AD prediction, providing a new approach for computational models in\nneurodegenerative disease research.\n(2) We employ cross-modal contrastive learning for unpaired individuals, which not only\nimproves diagnostic precision but also deepens the interpretability of pathogenic gene networks\nand their connections with brain regions and cell types.\n(3) We propose a new feature extraction method for snRNA-seq data, which is based on a\nbagging strategy to obtain coarse-grained features that preserve cell type and gene relationship\ninformation in single-cell data for attention computation with brain region features.\n(4) We implement similarity computations for patient data across snRNA-seq and fMRI data\ntypes, enabling the identification of relevant cases across diverse datasets."}, {"title": "2. Related works", "content": "Since there are currently no studies combining single-cell data with MRI data for predicting AD, this\nsection focuses on methods that exclusively utilize MRI data for AD classification or prediction. Recent\nresearch in AD prediction using neuroimaging data incorporates advanced methodologies that include\nnetwork techniques and machine learning algorithms. Mao et al. [20] leveraged rs-fMRI to extract ALFF"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1 Problem formulation", "content": "In this article, the prediction task of AD diagnosis using snRNA and fMRI multimodal data is designed\nwith snRNA as the helper modality. Through contrastive learning, correlations across different samples\nfrom the datasets are constructed, and these correlations are then integrated with fMRI data, which serves\nas the primary modality, to predict clinical state from imaging data. Specifically, for a set of single-cell\ntranscriptomic data from N\u2081 individuals, denoted as {T\u1d62}\u2081\u1d3a\u00b9, each T\u1d62 includes a gene expression matrix\nof dimensions determined by the number of nuclei and the number of genes, with each nucleus labeled\nby its cell type. Accompanying each individual's data are demographic and clinical labels such as age,\nN\ngender, and disease state. Given an fMRI dataset consisting of N individuals, {X\u1d62}\u2081\u1d3a, the first objective\nis to train separate encoders for two modalities, represented as T\u00ed = E\u209b\u2099(Ti) for snRNA data and X =E\u1d63\u2098(Xj) for fMRI data. An attention mechanism is then employed to learn attention scores a\u2c7c that\nquantify the relevance of the entire snRNA dataset to each fMRI image X\u2c7c. Subsequently, a classification\nfunction f is constructed to use the fMRI data and the derived attention scores to compute the disease\nprobability for each image: s\u2c7c = f(X\u2c7c, a\u2c7c) . This approach aims to harness the complementary\ninformation from both data modalities through a sophisticated learning scheme, emphasizing cross-\nmodal correlations to enhance the accuracy of AD prediction."}, {"title": "3.2 Overview of scBIT", "content": "scBIT is an end-to-end and hypotheses-free framework designed to predict the probability of AD using\nany fMRI data, supplemented with snRNA data as an auxiliary modality. It facilitates interpretative\nanalysis by delineating the intricate relationships between macroscopic brain regions and microscopic\ngenetic networks. There are three steps in scBIT framework . (i) Each snRNA\ndataset is initially processed through a bagging strategy to transform into 'cell bags', where each bag\ncontains cellular gene expression data to form individual gene networks. A self-explainable graph neural\nnetwork is then employed to predict cell types for each gene network and extract key gene subgraph\nrepresentations that significantly determine cell type classification. Each cell type is represented by an\nequal number of embeddings. Through this approach, the snRNA data matrix for each individual can be\nconverted into subgraph-level embeddings. (ii) Given that snRNA datasets and fMRI datasets are\ncollected from different individuals, associations between unpaired individuals across these datasets are\nconstructed using a variety of matching strategies. These strategies build on similarities based on age,\ngender, genetics, and clinical state to enable cross-modal contrastive learning. For each type of similarity,\na specific encoder is trained on the fMRI data to derive embeddings at the brain region ROI level. This\ntraining facilitates the computation of four distinct types of attention scores between the fMRI data and\neach snRNA dataset. A mix-of-experts [29] (MoE) model is then utilized to integrate these four attention\nscores into a unified score, representing the relevance of the fMRI data to each gene expression matrix\nin the snRNA dataset. (iii) An encoder is trained on the fMRI training dataset to generate embeddings\nthat encapsulate the functional attributes of brain regions. These embeddings are integrated with cross-\nmodal individual-level attention scores, previously computed in step 2, to form a joint representation.\nThis representation is then analyzed using a classifier to derive predictive outcomes for AD diagnosis."}, {"title": "3.3 scBIT architecture", "content": "The overall model architecture is depicted which contains three stages: (1) representation\nlearning for generated gene-gene interaction subgraph; (2) contrastive learning for cross-modal attention;\n(3) diagnosis prediction with pre-trained encoders. The specific implementation details of this model in\nthis work are presented in Fig. 2.\nStage 1: Representation learning for generated gene-gene interaction subgraph. scBIT utilizes a\nbagging strategy to construct sets of gene subgraphs from snRNA expression matrices, and it employs a\nself-explainable graph neural network framework (Fig. 2(a)) for model training, using cell types in\nsnRNA as labels. This method takes the interpretable sets of subgraphs as representations for each cell\ntype in snRNA to achieve coarse-grained feature representation. Given a set of single-cell transcriptomic\ndata from N\u2081 individuals, {T\u1d62}\u2081\u1d3a\u00b9, this stage transforms each T\u1d62 into a collection of cell type level gene\nnetwork subgraph embeddings, {p\u1d62}\u2081\u1d3a\u1d57\u2c7d\u1d9c, where N\ud835\udcb8 is the number of cell types (which equals 15 in this\nwork) and N\u209b is the predefined number of gene subgraphs used to represent each cell type (set to 6 in this\nwork). Here, each embedding p\u1d62 is interpretably mapped to a gene subgraph, with which it corresponds\none-to-one.\nCell bagging for gene-gene network construction. To characterize the distinct gene interaction\npatterns inherent to each cell type within snRNA, scBIT converts snRNA expression data into a collection\nof gene networks. Given an snRNA matrix T\u1d62, scBIT randomly selects a fixed number of cell nuclei (set\nto 20 in this work) from the same cell type to place into the same bag. Within this bag, the PCC value of\neach gene-gene pair across the 20 cell nuclei is calculated, and the top 20% pairs with the highest PCC\nvalue are retained to construct a gene-gene network. This process is repeated until all cells of the same\ntype have been processed. Consequently, the matrix T\u1d62 (comprising N\u1d64 cell nuclei) is converted into a set\nof gene networks {g\u2c7c}\u2081\u207d\u1d50\u207d\u1d3a\u1d58\u207e\u207e/\u2082\u2080, wherein each node within g\u1d62 has node features, represented by the\nexpression values of the respective gene across the 20 cell nuclei within the j-th bag. Since the cell nuclei\nin the same bag are sampled from the same cell type, the label of network they constitute also corresponds\nto the shared cell type.\nSelf-explainable GNN for gene subgraph extraction. As depicted in Fig. 2(a), each gene interaction\nbook is introduced into a self-explainable Graph Neural Network (GNN) as input. The network is\nstructured into two main components. The first component uses a GNN encoder to acquire a graph-wise\nembedding of the entire gene network. This embedding is then integrated with prototypes, generated in\nthe second component of the network using the interpretability strategy of the ProtGNN method [30].\nThese prototypes aid in computing the similarities between the gene network and all cell types, which\nare subsequently processed through an MLP (Multilayer Perceptron) coupled with a softmax function to\npredict the cell type of the gene network. The second component generates six prototypes for each cell\ntype and utilizes Monte Carlo Tree Search (MCTS) [31] to identify the subgraphs that correspond to\nthese prototypes. We let G = {G\u1d62}\u2081\u1d50\u1d62 be a collection of m gene networks from an snRNA-seq matrix,\nwhere each G\u1d62 consists of {g\ua7b1}\u2081\u1d50\u1d62, a set of gene networks specific to its cell type. The equations for the\narchitecture of the first component are\nh\ua7b1 = AvgPool(GNN\u2091\u2099c(g\ua7b1)) (1)\nsim(p\u1d62, h\ua7b1) = ||h\ua7b1 \u2013 p\u1d62||\u2082 (2)\nd\ua7b1 = [sim(p\u1d62, h\ua7b1)|k = 1 ... 90] (3)\nY = softmax(MLP(d\ua7b1)) (4)\nloss\ud835\udcb8\u2097\u209b = \u2081/\u2098 \u03a3 \u2098 q=1 min i:p\u1d62\u2208 P\u0443\u1d62 ||hq - p\u1d62||\u00b2 (5)\nloss\u209b\u2091\u209a = \u2081/\u2098 \u03a3 \u2098 q=1 min i:p\u1d62\u2209 P\u0443\u1d62 ||hq - p\u1d62||\u00b2 (6)\nLOSS = \u03a3\u1d64\u2097 \u2098 q=1- CrsEnt(yq, yq) + \u03bb\u2081 losscls + \u03bb\u2082losssep (7)\nGNN\u2091\u2099c is a graph-level encoder shared by the networks in both components, with detailed specifications\nprovided in Fig. 2(c); p\u2081 is derived from a set of graph prototypes P maintained by the second component,\ncomprising 90 prototypes distributed among 15 cell types, with six prototypes per type; losscls and losss e p\nrespectively ensure that each gene network embedding is close to at least one prototype of its own cell\ntype and distant from prototypes of other cell types; \u03bb\u2081 and \u03bb\u2082 were set to 0.1 and 0.05 in this work.\nTo generate and maintain the set of prototypesP = {P\u2081, ..., P\u1d62 ..., P\u2081\u2085} where each Pi =\\{P\u2086\u00d7\u1d62\u208b\u2085,... P\u2086\u00d7\u1d62}, the second component utilizes the MCTS algorithm to optimize the search for\nsubgraphs and to project these prototypes onto the most representative subgraphs that correspond to their\nrespective cell types (Fig. 2(a)). To enhance training time efficiency, such subgraph search and prototype\nprojection are performed at fixed intervals during the training process. P is initially randomized. The\nequations for the architecture of the second component are\nG = MCTS(G\u2081) (8)\ne\u209a\u1d63\u2032 = GNN\u2091\u2099c(g\u2c7d) where g\u2c7d \u2208 G\u2032 (9)\nsim(p\u1d62, e\u209a\u1d63\u2032) = log(||e\u209a\u1d63\u2032||\u00b2+\u00b9/||Pi-ep'\u1d63+\u2208||\u2082\u00b2) where p\u1d62 \u2208 Pi (10)\np\u2081 \u2190 arg min sim(pi, e\u209a\u1d63\u2032) (11)\ne\u209a\u1d63'\nMCTS guides the iterative search for the nearest subgraphs through multiple iterations, with each\niteration involving a forward phase for path selection and a backward phase for updating statistics,\ninitially favoring the exploration of less-visited nodes to optimize pruning actions, and ultimately\nselecting the subgraph with the highest similarity to the prototype as the new projected prototype. By\nusing Equation 9, we can conceptually map each prototype to a subgraph, thus enabling interpretative\ntraceability from the prototype to the subgraph. e was set to le-4. In summary, by integrating the two\ncomponents of this self-explainable GNN model, each snRNA data T; can be transformed into a set P,\nwhich consists of 90 prototypes representing 15 different cell types.\nStage 2: Contrastive learning for cross-modal attention. scBIT builds different ROI pre-trained\nencoders to extract brain ROI-level embeddings, and it calculates the cross-modal attentions among\nunpaired individuals. Then, contrastive learning is employed to train the ROI pre-trained encoders with\nfour different cross-modal individual similarities as labels, which optimizes the similarities between ROI-\nlevel and gene subgraph embeddings from unpaired samples to realize cross-modal matching. Given the\ncalculated gene subgraph embedding sets of entire snRNA dataset Pand a set of fMRI data from N\nindividuals, {X\u1d62}\u2081\u1d3a, this stage first embeds each X\u1d62 into brain ROI-level embeddings, {X\u1d62\u2032}\u2081\u1d3a, and\ncalculates similarity between Pand {X\u1d62\u2032}\u2081\u1d3a as the cross-modal attention a\u1d62.\nSimilarity calculation for individual matching in cross-modal datasets. For each pair of individuals\nacross datasets, we calculate similarity, s \u2208 {age, sex, state, gene}, based on four types of information:\nage, gender, clinical state, and genetic information. For gender, we assign a value of \"1\" to males and \"-\n1\" to females, then calculate the absolute value of the difference. For age, individuals in the snRNA\ndataset recorded as \"90+\" are simply considered as 90, and the difference is then calculated. For clinical\nstate, we assign values of 0, 0.25, 0.5, 0.75 to individuals in the snRNA dataset labeled as \"Not AD\",\n\"Low\", \"Intermediate\", \"High\", and values of 0, 0.25, 0.5, 0.75, 1 to those in the fMRI dataset labeled as\n\"CN\", \"EMCI\", \"MCI\", \"LMCI\", \"AD\", respectively, then calculate the difference. For the three types of\nsimilarities mentioned above, we performed a normalization process at the end.\nIn contrast to the gene expression information present in snRNA data, the fMRI dataset solely records\nindividuals' SNP genomic information, necessitating distinct processing approaches for each data type.\nFor the fMRI dataset's SNP data, we used PLINK [32] for preprocessing: filtering out SNPs with high\nmissing rates and those not meeting Hardy-Weinberg equilibrium, removing SNPs with a minor allele\nfrequency under 5%, and pruning linked SNPs. We then annotated SNPs via the NCBI database,\nsumming genes for SNPs linked to multiple genes, and calculated AUCell scores [33] from the KEGG\npathway database to assess pathway activities. For the snRNA data, we compute the AUCell scores for\neach cell across the KEGG pathway dataset and subsequently average these scores. Ultimately, we\nemploy cosine similarity to compare the AUCell scores derived from the two modalities, utilizing this\nmetric as a measure of individual similarity within the genetic information.\nROI pre-trained encoder for fMRI. As depicted in Fig. 2(b), to calculate similarity among unpaired\nindividuals within cross-modal datasets, scBIT first need to build pre-trained encoders (E\u1d63\u2098\u02e2) for fMRI\ndata to extract brain ROI-level embeddings. In this study, the fMRI feature pre-trained encoder is\nstructured into two components, as presented in Fig. 2(c). The first component utilizes two consecutive\nTransformer encoder layer (TEL) to embed the brain ROI sequence by capturing the long-term\ndependence via the self-attention mechanism. Then, the output embedding features are fed into one MLP\nlayer in the second component of the encoder network to compile the abstract features for getting the\nbrain ROI-level embeddings. For s-th type of similarity, a specific encoder is built on the fMRI data as\nfollows:\nX\u1d62\u209b\u2032 = E\u1d63\u2098\u02e2(X\u1d62) = MLP (TEL(TEL(X\u1d62))) where s \u2208 {age, sex, state, gene} (12)\nSubsequently, we devise four different types of attention scores to establish associations among\nindividuals across the datasets based on X\u1d62\u209b\u2032.\nContrastive learning for cross-modal matching. scBIT measures the cosine similarity, Csim,\nbetween Pj (Pj\u2208P) and X\u1d62\u2032 as cross-modal attentions and employs different optimize strategies for\nmatching unpaired snRNA-fMRI samples. Specifically, for the attention scores for discrete cross-modal\nindividual similarities, i.e., the sex and state, scBIT formulates the training of E\u1d63\u2098\u02e2 where s\u2208{sex, state} as classification problem by transforming the individual similarity labels into one-hot format.\nThen, supervised contrastive learning, L\u1d64\u1d63, is leveraged to train E\u1d63\u2098\u02e2 due to its capability of pulling\nthe leaned embeddings with identical labels together while pushing those from different classes apart, so\nas to learn more generalizable representations. First, to avoid overfitting, a weak augmentation method\nwith Gaussian noise is performed on Xi, resulting in an augmented dataset X\u1d62\u2032. By leveraging label\ninformation, the L\u1d64\u1d63 for E\u1d63\u2098 \u02e2 can be formulated as follows:\nL = - \u00b9/\u2082N \u03a3\u1d62=\u2081\u00b2\u1d3a log(exp( f(X\u1d62')\u1d40f(X\u2c7c')/\u03c4 )/\u03a3\u2096\u2208D(i) exp( f(X\u1d62')\u1d40f(X\u2096')/\u03c4 )) (13)\nwhere f(X\u1d62') denotes the embedding extracted by E\u1d63\u2098\u02e2 from X\u1d62' and f(\u00b7) denotes normalizing the\nembeddings into a unit hypersphere. D(i) represents the set of indices of the augmented samples sharing\nthe same label with the i-th sample and \u03c4\u2208 R+ is the scalar temperature parameter and set to 0.1 in this\nwork. As for the attention scores for continuous similarity values, scBIT take the training of E\u1d63\u2098 \u02e2 where s \u2208 {age, gene} as regression tasks by directly using the cross-modal individual similarities as labels.\nThen, the mean square error between the calculated cross-modal attentions and individual similarities is\nused to train E\u1d63\u2098 \u02e2 where s\u2208 {age, gene}.\nSubsequently, scBIT calculates cosine similarity between P and X\u1d62\u2032 that are inferred by well pre-\ntrained E\u1d63\u2098 \u02e2 as different types of cross-modal attentions, a\u015f, as follows:\na\u1d62 = C\u209b\u1d62\u2098(P,X\u1d62\u2032) = C\u209b\u1d62\u2098(P, E\u1d63\u2098 \u02e2(Xi)) (14)\nThe attention scores can quantify the relevance of the entire snRNA dataset to each fMRI image Xi,\nwhich are taken as intermodal information to aid the fMRI-based AD diagnosis.\nStage 3: Diagnosis prediction with pre-trained encoders. As shown in scBIT first builds\nan ROI encoder, E\u1d63\u2098, on the fMRI dataset to generate brain ROI-level embeddings, X\u1d62', that encapsulate\nthe functional attributes of brain regions. To integrate the inter-modal information, scBIT employs a\nmixture of experts (MoE) to synthesize the cross-modal attentions into a unified score to form a robust\nrepresentation that captures inter-modal relationships. This method takes the fMRI data and gene\nsubgraph embeddings as inputs and generates attention-specific weights for the inferred cross-modal\nattentions af to output the fused cross-modal attentions Ai as follows:\nA\u1d62 = \u03a3\u209b=\u2081\u2074 Softmax(X\u1d62WMOE)scsim(P, E\u1d63\u2098 \u02e2(Xi)) (15)\nwhere WMOE denotes the learned parameters of MoE model. Then, the fused cross-modal attentions are\nintegrated with the learned fMRI embeddings Xi as the joint representations. This representation is fed\ninto a classifier, f, to derive predictive outcomes for AD diagnosis, \u0177i, as follows:\n\u0177\u1d62 = Sigmoid(f((X, A\u1d62))) (16)\nwhere (\u2022) denote a concatenation operator."}, {"title": "4. Results", "content": ""}, {"title": "4.1 Data and materlals", "content": "All datasets used in this work are publicly available. In our study, we utilized multiple datasets to provide\na comprehensive analysis of AD. Functional MRI data were sourced from the ADNI, which includes\nimaging data along with demographic information such as age, sex, and disease status of the participants.\nThis data can be accessed through the\nADNI Image Collections\n(https://ida.loni.usc.edu/pages/access/search.jsp). Additionally, we used single-nucleus RNA sequencing\ndata from the Seattle Alzheimer's Disease Brain Cell Atlas, which provides detailed cellular-level\ninformation, including demographic and clinical metadata. This data, derived from the MTG and DLPFC,\nis available at Seattle Alzheimer's Disease Brain Cell Atlas (https://portal.brain-map.org/explore/seattle-\nalzheimers-disease/seattle-alzheimers-disease-brain-cell-atlas-download?edit&language=en).\nTo enrich our analysis, we also incorporated genetic information from the ADNI project, which\nincludes data on various genetic markers associated with AD, demographic details such as age and sex,\nand clinical information. This genetic data can be accessed through the ADNI Genetic Data\n(https://ida.loni.usc.edu/pages/access/geneticData.jsp?project=ADNI&page=DOWNLOADS&subPage\n=GENETIC_DATA). These datasets were used to analyze the progression and biological underpinnings\nof AD, providing a multi-faceted view of the condition through imaging, cellular, and genetic\nperspectives.\nGene pathway datasets used in this work include KEGG, Reactome, WikiPathways, HumanCyc,\nPathBank, and Panther. The first three databases can be accessed through the following URLs: KEGG\n(http://www.kegg.jp/),\nReactome\n(https://www.reactome.org),\nand WikiPathways\n(https://www.wikipathways.org). The latter three databases, HumanCyc, PathBank, and Panther, are\navailable via Pathway Commons (https://www.pathwaycommons.org/archives/PC2/v12/). These\nresources provide comprehensive pathway information that was utilized in our analyses."}, {"title": "4.2 scBIT achieves superior performance in Alzheimer's diagnosis with the aid of\nsnRNA data", "content": "To evaluate the predictive performance of scBIT, we curated cross-modal datasets from two databases:\nthe Seattle Alzheimer's Disease Cell Atlas (SEA-AD) [34] and the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) [35, 36], with their statistical data presented in Fig. 3(e) and Fig. 3(f). Considering that\nscBIT's cross-modal contrastive learning framework requires genetic information from fMRI data\nproviders, we exclusively utilized fMRI data from the ADNI dataset for performance testing, despite the\navailability of other Alzheimer's-related fMRI datasets that do not provide individual genetic information.\nWe employed a ten-fold cross-validation experimental framework to benchmark the prediction\nperformance on the ADNI dataset. This involved using nine folds alternately as the training set, with the\nremaining fold serving as the test set. To avoid the data leakage problem, we mask the disease state\nsimilarity corresponding to the testing set during the training of the pre-trained encoder. We designed\ntwo types of prediction tasks: a binary classification task to predict whether an individual has AD and a\nfive-class classification task to predict the severity of the disease. For the binary classification task, we\nused Accuracy (ACC), Area Under the Curve (AUC), Sensitivity (SEN), and Specificity (SPE) as\nperformance metrics. For the five-class severity assessment, we utilized ACC, F1-Score (F1), Precision\n(PRE), Recall (REC), and SPE as the evaluation criteria. The assessment was quantified by calculating\nthe mean and standard deviation across the ten folds.\nFig. 3(a) illustrates the predictive performance of scBIT on binary and five-class classification tasks\ndemonstrating the impact of incorporating versus omitting snRNA data. The architecture of the encoder\nand predictor in the scBIT model, when not incorporating snRNA data, aligns with that of the cross-\nmodal version of scBIT. Experimental results indicate that introducing snRNA data into the scBIT model\nsignificantly enhances predictive performance, with improvements in the binary classification task of\n3.39% in ACC, 5.63% in AUC, 2.74% in SEN, and 9.71% in SPE, and in the five-class classification\ntask of 26.59% in ACC, 7.44% in SPE, 24.98% in REC, 28.97% in PRE, and 26.89% in F1. The\nenhancements observed in the five-class classification task are substantially greater compared to those\nin the binary classification task. The reduction in standard deviation in the prediction results also\nindicates that the inclusion of snRNA data significantly enhances the stability of scBIT's predictions. We\nconducted an evaluation to determine the impact of integrating different proportions of snRNA data on\nthe predictive performance of scBIT. This assessment involved a systematic introduction of snRNA data\nat 20% intervals, ranging from 20% to 80%. The results (Fig. 3(d)) clearly demonstrated that the\ninclusion of incremental amounts of snRNA data significantly improved the accuracy of scBIT in binary\nclassification task. Notably, the average accuracy increased from 0.9431 with 20% snRNA data to 0.9569\nwhen 80% was included. Additionally, we evaluated the performance of scBIT across different\ndemographic groups (gender and age). The results (Fig. 3(b) and Fig. 3(c)) indicate that scBIT achieves\nbetter predictive accuracy in the male group and the younger age group (ages 50-70), with an average\nACC of 98.86 for the male group and 97.65 for the age 50-70 group.\nIn order to further assess the predictive capabilities of the scBIT model, we compared it against other\nexisting methods specifically designed for fMRI data analysis. The comparison encompassed not only\nmethodologies developed for AD utilizing the same dataset (ADNI) as employed in this work, but also\napproaches designed for the diagnosis of other neurological disorders. For the former, some methods\nshow variable performance due to the integration of different fMRI representation strategies, therefore\nwe selected the best results reported in the literature. For the latter, we have made adjustments to these\nmethods to enable them to process the same fMRI dataset as ours. The comparison results presented in\nTable 1 show that our scBIT method achieves the highest scores, significantly surpassing other\ncompeting models, including those designed for different diseases. Specifically, scBIT achieves an\naccuracy of 0.958, which is 11.66% higher than the average accuracy of methods targeting AD and 6.63%\nhigher than those developed for other diseases. This superior performance emphasizes the effectiveness\nof scBIT in enhancing diagnostic accuracy for AD."}, {"title": "4.3 scBIT enables cross-modal contrastive learning for unpaired individuals", "content": "Due to the use of snRNA data from living individuals and fMRI data from a donor cohort for AD research,\nit is inherently unfeasible to collect cross-modal data from the same individual, resulting in unpaired\nsampling issues when constructing similarity measures for contrastive learning model frameworks. To\naddress this challenge, scBIT harnesses the demographic and clinical information of data contributors by\nintegrating four distinct attributes (i.e., gender, age, clinical condition, and genetics) to match datasets\nacross two different modalities through the computation of similarity metrics. These metrics enable the\ntraining of two pre-trained regression models (for age and genetic similarities) and two pre-trained\nclassification models (for clinical condition and gender), each incorporating an encoder framework. To\nassess the effectiveness of different individual similarities, we tested all combinations of embeddings\nfrom their respective encoders, employing a MoE model for integration. The results (Fig. 3(g)) show that\ngenetic similarity achieved the highest average accuracy in single-encoder comparisons, reaching 95.05.\nFurthermore, the results reveal that incorporating more individual similarities leads to higher accuracy\nfor scBIT. Average accuracies improve with the number of encoders used, ranging from 94.53 with a\nsingle encoder, 94.73 with two encoders, 95.19 with three encoders, to 95.75 with four encoders. Fig.\n3(h) shows the proportion of weights for each encoder"}]}