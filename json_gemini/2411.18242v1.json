{"title": "Thai Financial Domain Adaptation of THaLLE \u2013 Technical Report", "authors": ["NLP-Voice Research Lab", "KBTG Labs", "KASIKORN Business\u2014Technology Group"], "abstract": "Large Language Models (LLMs) excel in general tasks but struggle with domain-specific challenges, such as specialized terminology and localized regulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack support for the Thai financial domain. We developed a Thai Financial LLM using the Investment Consultant (IC) exam dataset from the Stock Exchange of Thailand. To address dataset limitations, we applied data augmentation, ReLORA for efficient training, Continued Pretraining (CPT) for domain knowledge, and Rank-Stabilized LORA (rsLORA) for fine-tuning. Supervised Fine-Tuning (SFT) simulated exam scenarios, while Direct Preference Optimization (DPO) refined the model using feedback. The model achieved scores of 72%, 72%, and 84% on IC exam levels P1, P2, and P3, respectively, demonstrating its effectiveness in Thai financial advisory tasks and its potential for specialized applications.", "sections": [{"title": "Introduction", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable improvements across a variety of tasks, particularly in conversational system. These models are designed to understand and learn general knowledge from their training data, which often includes diverse and extensive text sources. Notable among them is Llama 3.1 [1], which has achieved significant performance in conversational tasks, including providing accurate answers even without additional context. Similarly, numerous other base models, trained on varied datasets and fine-tuned with different methodologies, have been introduced, showcasing impressive capabilities to follow instructions [2, 3, 4].\nDespite these achievements, LLMs face notable challenges when dealing with domain-specific knowledge. They often struggle with specialized terminology, abbreviations, and nuanced concepts unique to particular fields. Furthermore, their grounding and reasoning capabilities are typically built on general context and are not customized for specific scenarios. Financial use cases demands specialized knowledge and reasoning skills to address domain-specific queries. This includes understanding technical terms and abbreviations related to financial products (e.g., Retirement Mutual Fund (RMF)). Moreover, managing complex calculations and addressing localized contexts, such as country-specific regulatory frameworks, poses a considerable challenge.\nRecent developments in financial language models have introduced significant advancements. Among the most notable are FinGPT, an open-source financial LLM designed for enhanced data accessibility and lightweight adaptation [5], and BloombergGPT, a proprietary model trained on vast amounts of financial data to deliver exceptional performance in finance-specific tasks [6]. However, these advances do not adequately transfer to financial applications in the Thai context. The market currently lacks"}, {"title": "Background", "content": "a model that combines financial expertise, Thai language proficiency, and domain-specific financial knowledge tailored to Thailand's unique needs.\nBuilding upon this gap, our work focuses on developing a language model specifically tailored for the Thai financial domain. To achieve this, we utilized the Investment Consultant (IC) exam issued by the Stock Exchange of Thailand (SET) as a foundational dataset. Given the dataset's limited size, data augmentation techniques were employed to expand its scope and diversity. To further enhance the model's learning capabilities, we adopted ReLoRA, a method that applies low-rank updates to train high-rank networks, improving both training efficiency and performance [7]. The training process also incorporated CPT to establish robust foundational knowledge [8], while Rank-Stabilized LoRA (rsLORA) was used for efficient fine-tuning [9].\nMoreover, to emulate realistic examination and learning scenarios, we implemented two fine-tuning objectives: SFT simulating exam conditions, and DPO refining the model by leveraging feedback from incorrect responses [10]. These methodologies collectively ensure the model's alignment with the unique requirements of the Thai financial domain while maintaining high training efficiency and adaptability.\nAs a result, our contributions are summarized as follows:\n\u2022 Development of a Thai Financial LLM: Created a language model specifically tailored for the Thai financial domain using the IC exam dataset provided by SET.\n\u2022 Data Augmentation for Limited Datasets: Applied data augmentation techniques to address the constraints of a limited dataset, expanding the training corpus for better model performance.\n\u2022 Effective Training and Fine-Tuning Framework: Employed ReLORA for efficient and scalable training by applying low-rank updates to high-rank networks [7], utilized CPT to build foundational domain knowledge [8], and integrated rsLoRA for computationally efficient fine-tuning [9].\n\u2022 Simulation of Examination and Feedback-Driven Refinement: Applied SFT for exam scenario simulation and DPO to refine the model through feedback on incorrect responses [10].\nBy integrating these advanced techniques and methodologies, we successfully developed an LLM specifically designed to navigate the complexities of financial advisory. The model demonstrates its effectiveness by passing all levels of the IC exam, achieving scores of 72%, 72%, and 84% for P1, P2, and P3, respectively."}, {"title": "2.1 Financial Domain LLMs", "content": "LLMs provide robust foundations for financial applications through their pre-trained capabilities in diverse linguistic tasks [1, 11, 12, 13, 14, 15, 16]. Each model brings unique strengths, including multilingual support, computational efficiency, and optimized architectures. However, their general-purpose design often necessitates domain-specific adaptation to address the specialized requirements of financial datasets and terminology.\nTailored financial LLMs have been developed to meet these challenges. FinBERT [17] specializes in sentiment analysis within financial texts, leveraging a rich domain corpus to achieve high accuracy. FLUE and its derivative FLANG-BERT [18] serve as benchmarks for financial language understanding, outperforming general-purpose models. BloombergGPT [6] integrates proprietary financial datasets,"}, {"title": "2.2 The Investment Consultant License Exam", "content": "The IC license exam [20], issued by SET, is a mandatory certification for individuals seeking to professionally provide investment advice across a range of financial products. The exam is structured into three levels: P1, P2, and P3. P1 serves as the foundational level, while P2 and P3 progressively expand on the knowledge and skills introduced in P1. The details of each level are outlined as follows."}, {"title": "2.2.1 Plain Product (P1)", "content": "The Plain Product exam evaluates candidates on three core areas: Fundamental Knowledge, which includes investment environments, financial markets, risk and return, diversification, and various types of analyses; Related Rules and Regulations and Suitable Investment Consulting, focusing on professional conduct, regulatory compliance, and tailored investment advice; and Product Knowledge, covering equities, fixed income, mutual funds, and their associated valuation, risks, and trading mechanisms. Together, these modules ensure a comprehensive understanding of the investment landscape and the skills required for effective consulting.\n\u2022 Format: 100 multiple-choice questions with four options\n\u2022 Duration: 2 hours and 30 minutes\n\u2022 Passing Criteria: 70% of the total score and 70% in Related Rules and Regulations and Suitable Investment Consulting module"}, {"title": "2.2.2 Complex Product 1 (P2)", "content": "The Complex Products: Bond and Mutual Fund exam assesses candidates on their understanding of high-risk and complex financial products. It includes knowledge of Complex Bonds, such as structured debt securities, non-investment-grade bonds, and unrated securities, focusing on their characteristics, yield calculations, and associated risks. Additionally, it covers Complex Mutual Funds, emphasizing the ability to differentiate between fund types, calculate returns, and evaluate risks. The exam also highlights Investment Consulting for Complex Bonds and Mutual Funds, ensuring candidates can provide informed advice through proper consulting practices, leveraging investment channels, and utilizing reliable information sources.\n\u2022 Format: 25 multiple-choice questions with four options\n\u2022 Duration: 40 minutes\n\u2022 Passing Criteria: 70% of the total score"}, {"title": "2.2.3 Complex Product 2 (P3)", "content": "The Complex Products: Derivatives exam evaluates candidates on their understanding of derivatives through key areas, including the fundamentals of derivatives and their types, underlying assets, and market participants. It covers knowledge of futures contracts, focusing on pricing, valuation, and strategies for hedging, speculation, and arbitrage, as well as options contracts, emphasizing their definitions, pricing factors, and strategic uses. The exam also addresses the trading mechanisms of the Thailand Futures Exchange (TFEX), including trading procedures, clearing, and settlement processes, alongside detailed knowledge of the contract specifications for derivative products traded on TFEX.\n\u2022 Format: 50 multiple-choice questions with four options\n\u2022 Duration: 1 hour and 20 minutes\n\u2022 Passing Criteria: 70% of the total score"}, {"title": "2.3 ReLoRA: High-Rank Training Through Low-Rank Updates", "content": "ReLORA [7] is a parameter-efficient training technique designed to train large neural networks by employing low-rank updates to achieve high-rank performance. This method builds upon the Low-Rank Adaptation (LoRA) approach, which introduces low-rank matrices to fine-tune pre-trained models with reduced computational overhead. While LoRA is effective for fine-tuning, ReLORA extends its applicability to training models from scratch.\nThe core idea of ReLORA is to perform a series of low-rank updates that, when aggregated, approximate a high-rank update. This is based on the mathematical property that the rank of the sum of two matrices is less than or equal to the sum of their ranks. By iteratively applying low-rank updates and merging them into the original model parameters, ReLoRA incrementally increases the effective rank of the model's weight matrices. This process involves several key components:\nInitial Full-Rank Training: A brief phase of full-rank training to \"warm start\" the model, establishing a solid foundation for subsequent low-rank updates. LoRA Training with Restarts: Implementing LORA-based low-rank updates, followed by periodic restarts where the low-rank matrices are merged into the main model parameters. Jagged Learning Rate Schedule: Utilizing a learning rate schedule that resets to zero after each restart, followed by a quick warm-up, to stabilize training dynamics. Partial Optimizer Resets: Resetting parts of the optimizer state during restarts to prevent the optimizer's momentum from directing new updates toward previous low-rank subspaces. By integrating these components, ReLoRA effectively trains large-scale transformer language models with up to 1.3 billion parameters, achieving performance comparable to traditional full-rank training methods. Notably, ReLORA offers significant resource savings, reducing memory usage by up to 5.5 GB per GPU and enhancing training speed by 9-4%, depending on model size and hardware configuration. These advantages make ReLoRA a promising approach for efficient large-scale model training."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Data Preparation", "content": "To process large markdown documents, like the study materials used for training in the IC exam, we employ a technique called Dynamic Markdown Chunking. This technique involves splitting markdown files into smaller, contextually coherent sections by leveraging the structure provided by the document's headers. While keeping the chunk size within predefined token limits, we think that this method would help us retains relevant information grouped under a common topic."}, {"title": "3.2 Data Augmentation", "content": "The Dynamic Markdown Chunking method follows a two-step approach to ensure each chunk is meaningful and remains within the model's token limit:\n1. Initial Chunking by Header Level:\n\u2022 The document is initially chunked based on the hierarchy of headers, starting with level 2 headers (##). Each ## header begins a new chunk, and all subsequent content is included until the next ## header. Higher-level headers (#) are retained to provide context, while deeper-level headers (###, ####) are included within the chunk.\n\u2022 For example, a chunk beginning with a ## header would include all preceding # headers and subsequent content under deeper headers (e.g., ###) until the next ## header.\n2. Further Splitting of Large Chunks:\n\u2022 If a chunk exceeds the token limit, it is further split based on deeper header levels (e.g., ###, ####) or logical content divisions such as paragraphs or bullet points. Large paragraphs and lists are broken down carefully to preserve context.\n\u2022 For example, if the content under a header is too long, it may be split into smaller sub-chunks by breaking paragraphs or bullet points while retaining their logical flow.\nThis method enables the model to handle long documents like the IC exam study materials by splitting them into manageable chunks. The main goal is to ensure that the model can processes the given chunk of documents, while can also understand the relevance between common chunks."}, {"title": "3.2 Data Augmentation", "content": "Our training dataset includes a limited number of mock exams and over 1.3 million tokens of study materials. However, this is relatively small compared to the extensive range of topics and scenarios covered in the IC exam. To bridge this gap and ensure the model's performance remains robust, we implemented several data augmentation techniques. These methods enhance how knowledge is connected and applied, thereby improving the model's ability to generalize effectively. The following subsections detail the various augmentation techniques employed."}, {"title": "3.2.1 Self-Supervised Data Augmentation using Biased Zero-Shot", "content": "To generate reasoning data for exam questions with non-descriptive answers, we configure the model to produce each answer choice along with a supporting reason explaining why that choice could be correct. The reasoning for the correct answer is utilized during SFT, while the reasons for incorrect answers serve as negative examples in DPO [10]."}, {"title": "3.2.2 Multiple System Prompts Augmentation", "content": "To diversify the training data, we employed multiple system prompts, each designed to simulate the varied ways users might interact with the language model. By presenting the same IC exam content across different contexts, these prompts expose the model to a broader spectrum of scenarios. This approach enhances the model's ability to generalize and adapt, ensuring it can effectively respond across diverse financial domains."}, {"title": "3.2.3 Multiple-Choice Shuffling", "content": "To mitigate positional bias in the model's learning, we implemented Multiple-Choice Shuffling. This technique randomizes the order of answer options. The objective is to ensure that the model concentrates on the content of the question rather than learning the choice patterns. This approach aligns with findings from the study \u201cLeveraging Large Language Models for Multiple Choice Question Answering\" [21], which emphasizes the significance of how answer choices are presented to models and suggests that varying their order can serve as an effective augmentation strategy."}, {"title": "3.2.4 Multi-LLM Response Generation and Validation for DPO Training", "content": "To enhance response diversity and accuracy, we utilized multiple LLMs, including our base model, to generate responses for each question using both zero-shot and Chain-of-Thought (CoT) prompts [22]. Each prompt type generated distinct reasoning patterns, which were validated separately. Correct responses were then used in DPO training, and incorrect responses were labeled as rejected.\nUsing multiple prompt types introduces a variety of responses for each question, enriching the dataset and improving the model's performance. The prompt types employed include zero-shot prompts, which generate immediate answers followed by post-answer reasoning, and CoT prompts, which guide the model to break down reasoning steps before producing an answer. After generating multiple responses from various LLMs for each prompt type and question, we pair the responses and validate them. The response that adheres to the correct format and provides the correct answer is labeled as accepted, while the other is labeled as rejected."}, {"title": "3.2.5 Question-Answer Generation from Markdown", "content": "We augmented the dataset by generating question-answer pairs directly from markdown documents. This technique leverages the structured nature of markdown files, particularly headers, to create relevant questions and answers. Headers (#, ##,###) represent key sections or concepts, making them ideal for generating questions. The corresponding content under each header serves as the answer, resulting in a large set of meaningful question-answer pairs.\nThis process enhances the dataset by mimicking real-world question styles and helping the model establish better connections between topics and explanations. By focusing on question-answer pairs derived from the markdown's structure, the model gains a deeper understanding of hierarchical information and improves its performance in tasks such as IC exam preparation."}, {"title": "3.3 Model Optimization Techniques", "content": ""}, {"title": "3.3.1 Continual Pre-Training", "content": "We applied CPT [8] to train the model on a subset of the study materials using the Chunked Markdown data. The goal was to help the model gain foundational knowledge in finance by exposing it to structured content related to financial products, regulations, and complex financial instruments."}, {"title": "3.3.2 Supervised Fine-Tuning", "content": "We employed two different approaches for SFT to improve reasoning and question-answering capabilities:\n\u2022 CoT on Bias-Generated Reasoning: This fine-tuning helped the model improve its reasoning abilities by generating explanations for correct answers in a CoT format."}, {"title": "Direct Preference Optimization", "content": "\u2022 Question-Answer Fine-Tuning: The model was fine-tuned on a variety of question-answer pairs, which included markdown-generated Q&A, bias-generated CoT and zero-shot reasoning, and multiple-choice shuffling. This enabled the model to generalize across different question structures and domains, resulting in improved performance."}, {"title": "3.3.3 Direct Preference Optimization", "content": "Two variations of DPO [10] were applied to refine the model's decision-making and reasoning capabilities:\n\u2022 CoT on Bias-Generated Reasoning: This technique trained the model to generate and prioritize correct explanations for biased answers, discarding incorrect explanations.\n\u2022 Zero-shot Multiple-Choice Shuffling and Multi-LLM Response Generation: Using multiple-choice shuffling, the model was trained to prioritize correct answers based on content rather than the position of the answer. Additionally, using responses from various LLMs, the model was trained to prioritize correct answers with correct answer template over incorrect ones."}, {"title": "3.4 Parameter-Efficient Fine-Tuning Techniques", "content": ""}, {"title": "3.4.1 Rank-stabilized LORA", "content": "For CPT [8], we utilized RsLORA [9], which stabilized rank during low-rank adaptations, ensuring efficient fine-tuning of the model's foundational financial knowledge."}, {"title": "3.4.2 ReLORA", "content": "We applied ReLoRA [7], an iterative approach to training low-rank LoRA modules. Our training objective for each LoRA includes:\n\u2022 CPT on Chunked Markdown for foundational knowledge.\n\u2022 CPT focusing on P2 materials for better handling of complex products.\n\u2022 SFT with CoT Bias-Generated Reasoning.\n\u2022 SFT on markdown-generated Q&A, CoT and Zero-shot Bias-Generated Reasoning, and multiple-choice shuffling.\n\u2022 DPO on CoT Bias-Generated Reasoning.\n\u2022 DPO on Multiple-Choice Shuffling and Multi-LLM Response Generation."}, {"title": "4 Experimental Setup", "content": "We conducted a preliminary evaluation of foundational instruction-tuned generalist models on the public IC exams. This evaluation enabled us to assess the model's performance across various sections of the IC exam and to design targeted fine-tuning routines aimed at addressing areas that require further improvement.\nWe benchmarked various models, including commercially available APIs (i.e., gemini-1.5-flash-002, gemini-1.5-pro-002 [3], gpt-4-turbo-2024-04-09, and gpt-40-2024-11-20 [4]) and instruction-tuned foundational LLMs (i.e., Llama3.1-BB-Instruct [1], Qwen2-7B-Instruct [11], SeaLLMs-v3-7B-Chat [12], and our previously released model, THaLLE-0.1-7B-fa [23]), against the public IC practice exams. Our"}, {"title": "4.1 Dataset", "content": "in-house model is fine-tuned based on the Qwen2-7B-Instruct [11] open-source model, selected for its general performance and instruction-following capabilities. The training involved specialized data augmentation and optimization techniques to adapt the model to domain-specific tasks in financial consulting, as detailed in Sections 3.2 and 4.1.1."}, {"title": "4.1.1 Training Dataset", "content": "Our training dataset consists of two primary components: mock exams and study materials.\n\u2022 Mock Exams: The dataset includes a limited number of mock exams, each composed of four-choice multiple-choice questions. These exams closely simulate the actual IC exam format and cover content required for all three levels.\n\u2022 Study Materials: In addition to the mock exams, the dataset includes 1,365,594 tokens of study materials in markdown format. These materials encompass a wide range of topics pertinent to each level of the IC exam. The number of tokens of each level of IC exam is shown in Table 1. The markdown format ensures the model can parse and understand key sections like definitions, bullet points, and regulations, improving knowledge retrieval and context-aware responses."}, {"title": "4.2 Public Investment Consultant Practice Exam", "content": "For the test data, we utilized publicly available practice exams provided directly by SET. These exams are specifically designed to prepare candidates for the IC exams and are accessible on SET official website. The practice exams selected for benchmarking include:\n\u2022 P1: Basic investment products such as equities and bonds [24].\n\u2022 P2: Advanced instruments such as structured bonds and complex mutual funds [25].\n\u2022 P3: Focuses on derivatives like futures, options, and other financial derivatives [26].\nThe exams cover three key levels, as shown in Table 2, each focusing on distinct aspects of financial product knowledge. We utilized these exams as the test dataset to facilitate direct comparisons across models and ensure easy reproducibility of the benchmarking process. The evaluation was conducted to assess the model's understanding and performance on each task."}, {"title": "5 Experimental Results", "content": "The evaluation results, presented in Table 3, reveal a competitive performance landscape among the evaluated models on the public IC practice exams (P1, P2, and P3). Among the commercial APIs, gpt-40-2024-11-20 [4] consistently achieves the highest scores across all three tests, tying with gemini-1.5-pro-002 [3] in P1 and P2, while matching gpt-4-turbo-2024-04-09 in P3. This suggests that these models, particularly gpt-40-2024-11-20 [4], exhibit strong generalization and adaptability across diverse exam scenarios.\nIn the category of open instruction-tuned foundational LLMs, performance is notably more varied. Models such as Llama3.1-8B-Instruct [1] and THaLLE-0.1-7B-fa [23] demonstrate moderate proficiency, with THaLLE-0.1-7B-fa outperforming others in this category. However, the standout performance among open models comes from THaLLE-0.1-7B-ic, which achieves parity with top-performing commercial models, scoring 72% on P1 and P2 and 84% on P3. This underscores the efficacy of domain-specific fine-tuning (as indicated by the \u201cIC\u201d variant) in narrowing the performance gap between open-source and commercial models.\nA notable trend in the results is the performance disparity between commercial APIs and open foundational models. Commercial models generally display greater robustness and consistency, reflecting their extensive training on diverse and high-quality data. However, the success of THaLLE-0.1-7B-ic illustrates the potential of targeted instruction-tuning strategies in enabling open models to rival their commercial counterparts.\nAnother observation is the variability in results across different exam sections (P1, P2, and P3). Models such as gpt-4-turbo-2024-04-09 and THaLLE-0.1-7B-ic excel in P3, a section that may demand higher reasoning or specialized knowledge, whereas other models show relative declines in performance. This suggests that some models may be better optimized for specific task types, highlighting the importance of aligning model training objectives with the domain-specific requirements of the evaluation.\nIn conclusion, the results emphasize the continued dominance of commercial APIs in general-purpose tasks but also showcase the promise of fine-tuned, open-source solutions like THaLLE-0.1-7B-ic in achieving competitive performance within specific domains. This finding encourages further exploration of instruction-tuning and targeted fine-tuning approaches to bridge the performance gap in cost-effective and open-access settings."}, {"title": "6 Conclusion", "content": "In this work, we present THaLLE-IC, an extended version of the IC variants in the THaLLE project, specifically fine-tuned for the IC exam. The model leverages a combination of CPT, SFT, and DPO to adapt effectively to the demands of financial advisory tasks. To address the limited size of the training dataset, advanced data augmentation techniques, such as multiple-choice shuffling and multi-LLM response generation, were employed. These methods played a crucial role in enhancing the model's performance, enabling it to generalize across diverse financial scenarios."}, {"title": "Appendix", "content": ""}, {"title": "A Dynamic Markdown Chunking Example", "content": "Consider a markdown document with the following structure:\n# Example Heading 1\n## Example Heading 1.1\nText under example heading 1.1.\n### Example Heading 1.1.1\nDetails under example heading 1.1.1.\n### Example Heading 1.1.2\nDetails under example heading 1.1.2.\n## Example Heading 1.2\nText under example heading 1.2.\n### Example Heading 1.2.1\nDetails under example heading 1.2.1.\n### Example Heading 1.2.2\nDetails under example heading 1.2.2.\nStep 1: Initial Chunking by ## Headers:\n\u2022 Chunk 1:\n# Example Heading 1\n## Example Heading 1.1\nText under example heading 1.1.\n### Example Heading 1.1.1\nDetails under example heading 1.1.1.\n### Example Heading 1.1.2\nDetails under example heading 1.1.2."}, {"title": "\u2022 Chunk 2:", "content": "# Example Heading 1\n## Example Heading 1.2\nText under example heading 1.2.\n### Example Heading 1.2.1\nDetails under example heading 1.2.1.\n### Example Heading 1.2.2\nDetails under example heading 1.2.2.\nEach chunk includes all subsections under its main ## header, along with the top-level # header\nfor context.\nStep 2: Further Splitting if Chunk Exceeds Token Limit:\n\u2022 If a chunk, such as \"Example Heading 1.2,\" exceeds the token limit, it is further divided into\nsmaller chunks. For example:\nSub-chunk 1:\n# Example Heading 1\n## Example Heading 1.2\nText under example heading 1.2.\nSub-chunk 2:\n# Example Heading 1\n## Example Heading 1.2\n### Example Heading 1.2.1\nDetails under example heading 1.2.1.\nSub-chunk 3:\n# Example Heading 1\n## Example Heading 1.2\n### Example Heading 1.2.2\nDetails under example heading 1.2.2."}, {"title": "B System Prompt", "content": ""}, {"title": "C Model Output Examples", "content": ""}]}