{"title": "Integrative CAM: Adaptive Layer Fusion for Comprehensive Interpretation of CNNs", "authors": ["Aniket K Singha", "Debasis Chaudhuria", "Manish P Singh", "Samiran Chattopadhyaya"], "abstract": "With the growing demand for interpretable deep learning models, this paper introduces Integrative CAM,\nan advanced Class Activation Mapping (CAM) technique aimed at providing a holistic view of feature\nimportance across Convolutional Neural Networks (CNNs). Traditional gradient-based CAM methods,\nsuch as Grad-CAM and Grad-CAM++, primarily use final layer activations to highlight regions of\ninterest, often neglecting critical features derived from intermediate layers. Integrative CAM addresses\nthis limitation by fusing insights across all network layers, leveraging both gradient and activation scores\nto adaptively weight layer contributions, thus yielding a comprehensive interpretation of the model's\ninternal representation. Our approach includes a novel bias term in the saliency map calculation, a factor\nfrequently omitted in existing CAM techniques, but essential for capturing a more complete feature\nimportance landscape, as modern CNNs rely on both weighted activations and biases to make predictions.\nAdditionally, we generalize the alpha term from Grad-CAM++ to apply to any smooth function,\nexpanding CAM applicability across a wider range of models. Through extensive experiments on diverse\nand complex datasets, Integrative CAM demonstrates superior fidelity in feature importance mapping,\neffectively enhancing interpretability for intricate fusion scenarios and complex decision-making tasks.\nBy advancing interpretability methods to capture multi-layered model insights, Integrative CAM provides\na valuable tool for fusion-driven applications, promoting the trustworthy and insightful deployment of\ndeep learning models.", "sections": [{"title": "1. Introduction", "content": "The rapid development of machine learning, particularly through deep neural networks, has\nrevolutionized artificial intelligence (AI), enabling advancements across a wide array of real-world\napplications, from autonomous systems to complex decision support systems [1]. Despite the exceptional\ncapabilities of deep learning models in domains like object detection, speech recognition, and machine\ntranslation, these models have shifted away from the explainable, logic-driven approaches of traditional\nAI. In contrast to earlier systems, which could articulate their decision-making processes through\ntransparent, traceable inference steps, modern Al systems often function as \"black boxes,\" making it\ndifficult to discern the reasoning behind their outputs [2, 3]. This lack of transparency is especially\nproblematic in critical fields such as security, healthcare, and autonomous navigation, where\nunderstanding the basis for Al decisions is essential to ensure trust and facilitate human collaboration [4].\nIn response to this challenge, researchers have explored a variety of methods to enhance the\nexplainability of deep learning models. One prominent strategy involves training secondary models to\nprovide rationales for the predictions of the primary models, thereby explaining the underlying decision-\nmaking processes [5, 6] Another approach involves manipulating the input data to examine the resulting\nchanges in the model's outputs, to identify which features drive the model's decisions [7, 8]. Despite these\npromising directions, achieving full transparency and interpretability remains a complex and ongoing\nchallenge, with solutions varying greatly across different problem domains [9, 10]. In areas where Al\nperformance does not yet surpass human capabilities, transparency can help identify limitations and guide\nimprovements [11]. As Al systems reach human performance, fostering user trust through clear,\nunderstandable explanations becomes critical [12]. Furthermore, in scenarios where AI outpaces human\ncapabilities, explanations assume the role of machine teaching, empowering humans to make better\ndecisions through AI-guided insights [13]. Ultimately, as AI systems become more integrated into society,\nit is essential to facilitate their adoption through greater interpretability, ensuring their meaningful use in\ndiverse real-world contexts.\nThe trade-off between model accuracy and interpretability has long been a defining issue in Al\ndevelopment. While traditional rule-based systems are highly interpretable, they often lack accuracy or\nrobustness [14]. Conversely, decomposable pipelines, where each stage is meticulously crafted, afford\ninterpretability through intuitive explanations [15]. However, with deep learning models, interpretability\nis often sacrificed for performance gains achieved through abstraction (more layers) and integration (end-\nto-end training). Notably, recent advancements in Convolutional Neural Networks (CNNs) demonstrate\nstate-of-the-art performance across diverse tasks but present challenges in interpretability due to their\ncomplexity [16, 17, 18]. Consequently, the field of deep learning is increasingly exploring the delicate\nbalance between interpretability and accuracy, aiming to reconcile the benefits of sophisticated models\nwith the need for transparency and comprehension. This balance is crucial not only for fostering trust in\nAl systems but also for facilitating the practical integration of these models into diverse sectors.\nCounterfactual-based methods for generating visual explanations [19, 20] have provided valuable\ninsights into the decision-making process of CNNs by identifying the most important features for\nclassification tasks. However, these methods are often computationally expensive and sensitive to slight\nchanges in input data. Visual explanation techniques, such as Class Activation Mapping (CAM) [21] and\nits extensions like Grad-CAM [22], Grad-CAM++ [23], and Layer-CAM [24], have become widely used\nfor visualizing CNNs' decision-making processes. Each method offers a different approach to feature\nvisualization, but they often fall short in certain applications [25], particularly when dealing with complex,\nhigh-dimensional data where interpretability is critical.\nSeveral factors contribute to these limitations. Many techniques, excluding Layer-CAM, rely solely\non the final convolutional layer to produce class activation maps. This approach may overlook valuable\ninformation from earlier layers that play a role in a model's perception. Moreover, the spatial resolution\nof the final layer's output is often low, which results in coarse activation maps that miss finer details. In\ncontrast, shallow layers capture higher-resolution features, making them better suited for intricate detail\ndetection.\nEven when incorporating multiple layers, it is essential to recognize that not all layers hold the same\nsignificance within the model. While many CAM methods, including Grad-CAM, offer the flexibility to\nuse layers beyond the final convolutional layer, they do not provide guidance on which layers are most\ninformative for visualization. Furthermore, these methods do not provide insights into whether a specific\nlayer is crucial for interpretation. This leaves layer selection to trial and error, adding complexity and\ninconsistency to the interpretation process [26]."}, {"title": "For CNNs employing Global Average Pooling (GAP), prior CAM approaches represent the final\nclassification score $Y^C$ for a class $C$ as a linear combination of the global average pooled final layer feature\nmap $A^k$:", "content": "$Y^C = \\sum_k w_k \\sum_{i,j} A_{ij}^k\\ \\ \\ (1)$\nwhere $w_k^C$ is the weight for each feature map $A^k$. However, this approach overlooks an essential aspect\nof CNNs: the presence of bias terms in fully connected (fc) layers. CNN models inherently rely on both\nweights and biases, with each contributing to the overall structure and accuracy of the network's\npredictions. Simply condensing feature maps into a weighted linear combination and neglecting the bias\nterm can potentially limit the overall accuracy of these techniques, as they may miss subtle but significant\nbiases present in the model. The presence of different bias terms for different channels can significantly\nimpact the final class activation map for a layer, underscoring the importance of accounting for bias terms\nin CNN classifiers.\nTo capture this complexity more accurately, the final classification score $Y^C$ can be expressed as:\n$Y^C = \\sum_k w_k^C (\\sum_{i,j} A_{ij}^k) + b_k^C\\ \\ \\ (2)$\nwhere $b_k^C$ represents the bias term associated with each channel in the fc layer. Including this bias\nterm refines the generated class activation maps, as it accounts for the network's full architecture,\nimproving interpretability by incorporating both weights and biases\u2014a fundamental part of CNN\nclassifiers.\nTo address these limitations, this paper introduces a novel method called Integrative CAM (I-CAM),\ndesigned to enhance interpretability by leveraging key layers throughout the model for a comprehensive\nview of feature activations. I-CAM introduces a new layer score function that assesses each layer's\nrelevance, removing ambiguity in layer selection. Additionally, I-CAM incorporates bias terms for each\nfeature map $A^k$ to refine the classification score, as outlined in Equation 2, ensuring a more accurate\nreflection of the model's decision-making process.\nBeyond these innovations, I-CAM improves generalizability through a simplified adaptation of the\nalpha term from Grad-CAM++. This modification replaces the complex nth order partial derivatives with\na simpler expression involving the nth power of the first-order derivative, broadening applicability to any\nsmooth function beyond exponentials. This enhancement not only streamlines I-CAM's implementation\nbut also expands its utility across diverse functions."}, {"title": "2. Integrative CAM (I-CAM)", "content": "I-CAM is an innovative gradient-based Class Activation Mapping (CAM) method that redefines core\nprinciples in deep learning interpretability. It introduces an automated, dynamically weighted layer\nselection mechanism, moving beyond the conventional manual selection approach. By incorporating a\nbias term in saliency map computations and presenting a generalized alpha term from Grad-CAM++ , I-\nCAM offers a more robust framework for representing feature importance in model decision-making [27].\nThis approach sets a new benchmark in AI interpretability, enhancing the understanding of model\nbehavior and promoting the credibility of AI systems."}, {"title": "2.1. Perturbation-based layer importance score", "content": "The Integrative Class Activation Mapping (I-CAM) method employs a unique strategy to evaluate\nthe relative importance of CNN layers. Each layer receives an importance score based on an analysis of\nthe activation patterns and gradient information. However, it is essential to note that the determination of\nthis layer-specific score cannot be solely reliant on a single input image and the associated output class\nprobability. To provide a reliable layer scoring, I-CAM evaluates n perturbed variations of the original\ninput image $I$, yielding a set of n + 1 inputs. This multi-input framework enhances robustness by\nincorporating diverse perspectives from each perturbation, allowing for a more nuanced understanding of\nfeature activations [28, 29].\nThe importance score $S_l$ for layer $l$ is calculated as:\n$S_l = \\sum_{i=1}^n w_i \\sum_{k \\in F} (\\sum_{i,j} ||\\frac{\\partial O}{\\partial I_i}||_F - ||g_{ijl}||_F )^2\\ \\ \\ (3)$\nHere, $w_i$ is the weight for each perturbed input $I_i$, $\\frac{\\partial O}{\\partial I_i}$ represents the gradient of the output $O$\nconcerning the original input image $I$, $||\\cdot||_F$ denotes Frobenius norm [30] along the channel dimension\nand $g_{ijl}$ is the gradient-weighted activation for $I^i$ at layer $l$. If $A$ and $\\frac{\\partial O}{\\partial A}$ are the activation and gradient,\nrespectively, of layer $l$, then $g_{ijl} = relu(A \\frac{\\partial O}{\\partial A})$."}, {"title": "2.1.1. Image perturbations", "content": "To generate the n perturbed variations of the original image $I$, we apply a two-step perturbation\nprocess. First, random Gaussian noise $N_i$ is added to the image, where $N_i \\sim N(0,1)$ and scaled by a factor\n$\\alpha$. This introduces variability into the image. Then, a random pixel mask $M_i$ is applied, which is sampled\nfrom a Bernoulli distribution $M_i \\sim Bernoulli(1 - \\alpha)$. This mask removes a proportion $1 - \\alpha$ of the\npixels uniformly across all channels.\nThe perturbed image $I^i$ ($i = 1,2,..., n$) from the original image $I$ is given by:\n$I^i = (I + \\alpha N_i) \\odot M_i\\ \\ \\ (4)$\nwhere $\\odot$ denotes element-wise multiplication. The parameter $\\alpha$ controls the intensity of the noise\naddition and pixel masking, making the perturbation process highly flexible. This dual operation,\ncombining noise with random masking, ensures spatial consistency while allowing for meaningful\nperturbations of the image."}, {"title": "2.1.2. Perturbation weights", "content": "The generation and utilization of perturbations derived from an original image $I$ is crucial in the\nevaluation and robustness assessment of CNN classifiers. To acknowledge the varying importance and\ninteractions of different perturbed images with the model, as not all perturbations will yield equally\nvaluable insights, a more sophisticated and academically sound method is to employ a weighted mean.\nThe proposed methodology involves assigning a weighting factor $w_i$ to each perturbed input image\n$I^i$, which is calculated based on two key considerations. Firstly, the SVIM metric between the original\nimage $I$ and the perturbed image $I^i$ captures the extent of the perturbation. Secondly, the similarity\nbetween the output class probabilities $O$ of the model for the original image $I$ and the output class\nprobabilities $O^i$ for the perturbed image $I^i$, as quantified by the MDS metric, prioritizes perturbations that\ndo not significantly alter the model's output.\nThe final weighting factor $w_i$ is calculated as the geometric mean of the SVIM and MDS values and\nensures that the weighting factors are on a comparable scale, allowing for a meaningful aggregation of\nthe perturbations. Thus, the perturbation weight $w_i$ is calculated as follows:\n$w_i = \\sqrt{SVIM(I,I^i) * MDS(O,O^i)}\\ \\ \\ (5)$"}, {"title": "2.1.3. Structural Variability Index Metric (SVIM)", "content": "The SVIM (Structural Variability Index Metric) function is a Gaussian-based metric derived from\nthe well-established SSIM (Structural Similarity Index Metric) [32]. While the SSIM metric theoretically\nranges from -1 to 1, with 1 indicating a perfect match, in practical cases, it is bound to the range [0, 1].\nThe SVIM metric focuses on quantifying the variability of the structures between two input images,\nproviding a measure of how much the structural components of the perturbed image differ from those of\nthe original image. A value of 0 indicates that the perturbed image is either identical to or completely\ndissimilar from the original, while a value of 1 signifies that the perturbed image presents a meaningful\nvariation, maintaining structural similarity but with some degree of difference. The SVIM function is\ndefined as:\n$SVIM(x, y) = e^{\\frac{(SSIM(x,y)-0.5)^2}{2\\sigma^2}}\\ \\ \\ (6)$\nwhere SSIM is computed as:\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}\\ \\ \\ (7)$\nHere, $\\mu_x, \\mu_y$ are the means of the images, $\\sigma_x^2, \\sigma_y^2$ are the variances, $\\sigma_{xy}$ is the covariance, and $C_1, C_2$\nare small positive constants used for numerical stability.\nBy capturing both structural similarity and variability, the SVIM metric provides a more nuanced\nmeasure of image differences compared to traditional difference-based metrics. This makes it highly\neffective in assessing how meaningful a perturbation is, where a high SVIM score indicates a good\nperturbation that introduces useful variation without straying too far from the original structure."}, {"title": "2.1.4. Mean Difference Similarity (MDS)", "content": "The MDS (Mean Difference Similarity) function introduces an innovative approach for quantifying\nsimilarity between two probability distributions, $X$ and $Y$, and diverges from traditional metrics such as\nJensen-Shannon Divergence (JSD) [33]. While JSD relies on computing the average Kullback-Leibler\n(KL) divergence [34] between input distributions and an intermediary distribution, the MDS function\nbypasses this by employing a direct comparison between the distributions. At its core, MDS calculates\nsimilarity based on the Mean Difference Divergence (MDD). a weighted average of element-wise ratios\nbetween $X$ and $Y$.\nThe MDD function is formulated by taking the weighted mean of the differences between\ncorresponding elements $X_k$ and $Y_k$, with each weight $D_k = \\frac{X_k - Y_k}{2}$. This results in a weighted divergence\nscore that is subtracted from 1 to yield the MDS similarity score. Mathematically, MDD and MDS are\ndefined as follows:\n$MDD(X,Y) = \\sum_{k=1}^C D_k log(\\frac{X_k}{Y_k})\\ \\ \\ (8)$\n$MDS(X,Y) = 1 - MDD(X,Y)\\ \\ \\ (9)$\nwhere $X$ and $Y$ are the input probability distributions, $C$ is the number of elements in each distribution,\nand $X_k$ and $Y_k$ are the kth elements of each distribution.\nBy comparing distribution elements directly, the MDS function avoids dependence on a mid-point\ndistribution, thus simplifying the calculation and eliminating assumptions about intermediary forms that\nmay impact the resulting similarity measure. Alternatively, MDD(X, Y) can also be conceptualized as a\nform of symmetric Jensen-Shannon Divergence, computed as the average KL divergence in both\ndirections:\n$MDD(X,Y) = \\frac{KL(X,Y) + KL(Y,X)}{2}\\ \\ \\ (10)$\nThe symmetric property of MDS, where MDS(X,Y) = MDS(Y, X), aligns with the intuitive concept\nof similarity, as the similarity degree between two distributions should be consistent regardless of order.\nAs a whole, the MDS function offers a compelling alternative to traditional metrics like JSD, providing a\ndirect, interpretable measure of similarity between probability distributions that is applicable across a\nwide range of research domains."}, {"title": "2.2. Redundant layer filtering and layer weightage", "content": "Consider a CNN model with L layers. To prioritize meaningful feature extraction across these layers,\nwe assign an importance score $S_l$ to each layer $l$ ($l \\in \\{1,2, ..., L\\}$), resulting in a list of layer scores $S =$\n$\\{S_1, S_2, ..., S_L\\}$. Some of these layers may contribute minimal information and can be considered redundant\nfor the image at hand with respect to the model's prediction [35, 36, 37]. To address this, we rank the\nlayers in descending order of their importance scores, forming a new ordered list $S' = \\{S_1', S_2', ..., S_L'\\}$ such\nthat $S_1' \\geq S_2' \\geq ... \\geq S_L'$. We then compute the cumulative sum of these importance scores as follows:\n$C_k = \\sum_{l=1}^k S_l\\ \\ \\ (11)$\nwhere $C_k$ represents the cumulative importance score up to the $k^{th}$ layer. To ensure that only the most\ninformative layers are retained, we set a threshold T, representing the percentage of the overall importance\nscore to be preserved. In this study, we use a threshold of 0.95 (or 95%), which captures the majority of\ncritical features for effective feature map fusion for model perception visualization. We determine the\nindex k by satisfying the following conditions:\n$C_k \\geq T * \\sum_{l=1}^L S_l\\ \\ \\ (12)$\n$C_{k-1} < T * \\sum_{l=1}^L S_l\\ \\ \\ (13)$\nThis means that the cumulative score up to the kth layer meets or exceeds the threshold, while the\nscore up to the (k \u2013 1)th layer remains below it. Once we identify the top k layers to retain, we allocate\nweights to these selected layers based on their relative importance scores. The weight $W_l$ for the lth layer\nis defined as follows:\n$W_l = \\frac{S_l}{\\sum_{j \\in L'} S_j}\\ \\ \\ (14)$\nwhere L' represents the subset of the k selected layers from the original L. This process enables\nfocused integration of the most informative layers, enhancing the effectiveness of feature fusion by\nemphasizing layers that significantly contribute to the model's interpretability and performance."}, {"title": "2.3. Gradient-based CAM approaches", "content": "Gradient-based CAM methods such as Grad-CAM and Grad-CAM++ help localize target objects in\nimages by computing weighted sums of feature maps from the final convolutional layer, where weights\nrepresent each map's importance for the target class. In Grad-CAM, these weights, $w_k^c$, are determined by\naveraging gradients over spatial locations:\n$w_k^c = \\frac{1}{N} \\sum_{i,j} g_{ij}^{kc}\\ \\ \\ (15)$\nwhere N is the number of locations, $Y^c$ is the model output, and $g_{ij}^{kc} = \\frac{\\partial Y^c}{\\partial A_{ij}^k}$ is the gradient at location\n(i, j). Grad-CAM++ enhances this by incorporating higher-order gradients, refining $w_k^c$ with spatially-\ndependent terms:\n$w_k^c = \\sum_{i,j} \\alpha_{ij}^k relu(g_{ij}^{kc})\\ \\ \\ (16)$\nHere, $\\alpha_{ij}^k$ is a factor derived from second- and third-order gradients:\n$\\alpha_{ij}^k = \\frac{\\frac{\\partial^2 Y^c}{(\\partial A_{ij}^k)^2}}{2 \\cdot (\\frac{\\partial^2 Y^c}{(\\partial A_{ij}^k)^2} + \\sum_{i,j} A_{ij}^k \\frac{\\partial^3 Y^c}{(\\partial A_{ij}^k)^3})}\\ \\ \\ (17)$\nFollowing the computation of the channel-wise weights $w_k^c$ for each feature map $A^k$, a rectified linear\nunit (ReLU) operation is employed to eliminate negative responses to get the resulting class activation\nmap $M^c$, defined as:\n$M^c = relu(\\sum_k w_k^c A^k)\\ \\ \\ (18)$\nWhile these methods can be applied to both shallow and deep layers, they use a global weighting\nscheme across spatial locations, which works well for deeper layers where feature maps are smaller and\nspatial variance is low. However, applying these global weights to shallow layers, where feature maps are\nlarger and spatial variance is high, can lead to suboptimal results. Shallow layers capture more fine-\ngrained and localized details, making it important to account for spatially varying contributions rather"}, {"title": "than a single global weight. LayerCAM mitigates this by applying localized, element-wise gradients as\nweights at each spatial location:", "content": "$w_{ij}^c = relu(g_{ij}^{kc})\\ \\ \\ (19)$\nThis element-wise weighting allows LayerCAM to emphasize important local features within each\nfeature map, making it better suited for use on shallow layers that capture detailed patterns. However,\nLayerCAM's method of averaging CAMs from all layers to obtain the final map may dilute the\ncontribution of layers with varying relevance, potentially limiting interpretability.\nTo address this limitation, our proposed Integrative CAM (I-CAM) method extends Grad-CAM++\nby combining LayerCAM's element-wise approach with adaptive layer selection. I-CAM computes\nCAMs for the top 95% of layers based on their importance scores and weights each layer's CAM\naccording to its specific relevance, avoiding a simple average across all layers. Additionally, I-CAM\nintroduces a bias term, b, to capture more nuanced aspects of the model's behavior. The class activation\nmap $L^c$ for a selected layer using the I-CAM method is defined as:\n$L^c = \\sum_k w_{ij}^{kc} A_{ij}^k + b_k^c\\ \\ \\ (20)$\nThe final I-CAM, $L_{I-CAM}^c$, aggregates these layer-specific CAMs, weighted by the importance $W_l$ of\neach layer l:\n$L_{I-CAM}^c = \\sum_{l \\in L'} W_l L^c\\ \\ \\ (21)$\nwhere $W_l$ represents the weight for layer l among the selected layers L'. By integrating spatial\nimportance with adaptive layer-based weighting, I-CAM provides a more comprehensive view of the\nmodel's decision-making process, enhancing interpretability across the CNN and improving the quality\nof class activation maps for more accurate visualization."}, {"title": "2.4. Alpha term simplification", "content": "In classification models, the penultimate score for class c, denoted as $S^c$, is a fundamental element\nthat influences the final class prediction. The general form of a classification head, outlined in Eq. 22,\nmay include linear or ReLU activations to compute $S^c$. The parameters $w$ and $\\beta$ represent the activation\nweight and bias of the classifier, respectively, while $h(A^l)$ refers to the transformation applied to the\nactivation $A^l$ from layer $l$ before passing through the classifier. If I is the final layer, then $h(A^l) = A^l$.\n$S^c = g(A^l) = \\sum_{k \\in F} w \\sum_{i,j} h(A_{ij}^l) + \\beta_k\\ \\ \\ (22)$\nGradients, such as $\\frac{\\partial S^c}{\\partial A_{kl}^{ij}} = h'(A_{ij}^l)$, can be computed with automatic differentiation tools like\nPyTorch. If $h(A_{ij}^l)$ is a linear function of $A_{ij}^l$, potentially including ReLU activations, the derivative\n$h'(A_{ij}^l)$ is constant. Consequently, higher-order derivatives $\\frac{\\partial^2 S^c}{(\\partial A_{kl}^{ij})^2}$ and $\\frac{\\partial^3 S^c}{(\\partial A_{kl}^{ij})(\\partial A_{kl}^{ij})}$ will be zero.\nGrad-CAM directly uses this $S^c$ value as the class score $Y^c$ without further transformation. However,\nin most cases, the predicted output $Y^c$ is not taken directly from $S^c$; instead, it undergoes a smoothing\ntransformation, such as an exponential or softmax function, to ensure a stable and differentiable output.\nGrad-CAM++, for instance, applies an exponential transformation to $S^c$ to make $Y^c$ infinitely\ndifferentiable, which is beneficial for obtaining consistent higher-order derivatives necessary for easier\nimplementation of their alpha expression. Expanding on this idea, we derive a general approach to express"}, {"title": "higher-order derivatives as functions of the first-order derivative for any smooth function $f$, allowing\nconsistent application across interpretability methods that rely on multi-order derivatives. If $Y^c = f(S^c)$,\nis the final score after passing model output $S^c$ through a smooth function $f$, then:", "content": "$\\frac{\\partial Y^c}{\\partial A_{kl}^{ij}} = \\frac{\\partial Y^c}{\\partial S^c} \\frac{\\partial S^c}{\\partial A_{kl}^{ij}} = f'(S^c) \\frac{\\partial S^c}{\\partial A_{kl}^{ij}}\\ \\ \\ (23)$\n$\\frac{\\partial^2 Y^c}{\\partial (A_{kl}^{ij})^2} = \\frac{\\partial}{\\partial (A_{kl}^{ij})}(\\frac{\\partial Y^c}{\\partial A_{kl}^{ij}}) = \\frac{\\partial}{\\partial (A_{kl}^{ij})} (f'(S^c) \\frac{\\partial S^c}{\\partial A_{kl}^{ij}}) = \\frac{\\partial S^c}{\\partial A_{kl}^{ij}} \\frac{\\partial f'(S^c)}{\\partial A_{kl}^{ij}} + f'(S^c) \\frac{\\partial^2 S^c}{\\partial (A_{kl}^{ij})^2}$\n$= \\frac{\\partial S^c}{\\partial A_{kl}^{ij}} \\frac{\\partial f'(S^c)}{\\partial S^c} \\frac{\\partial S^c}{\\partial A_{kl}^{ij}} = f''(S^c) (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^2 + f'(S^c) \\frac{\\partial^2 S^c}{\\partial (A_{kl}^{ij})^2}$\n$\\frac{\\partial^3 Y^c}{\\partial (A_{kl}^{ij})^3} = \\frac{\\partial}{\\partial (A_{kl}^{ij})}(\\frac{\\partial^2 Y^c}{\\partial (A_{kl}^{ij})^2}) = \\frac{\\partial}{\\partial (A_{kl}^{ij})} [ f''(S^c) (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^2 + f'(S^c) \\frac{\\partial^2 S^c}{\\partial (A_{kl}^{ij})^2}]$\n$= f'''(S^c) (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^3 + 3 f''(S^c) \\frac{\\partial S^c}{\\partial A_{kl}^{ij}} \\frac{\\partial^2 S^c}{\\partial (A_{kl}^{ij})^2} + f'(S^c) \\frac{\\partial^3 S^c}{\\partial (A_{kl}^{ij})^3}$\nThus ultimately, for $Y^c = f(S^c)$,\n$\\frac{\\partial^n Y^c}{\\partial (A_{kl}^{ij})^n} = f^n(S^c) (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^n\\ \\ \\ (24)$\nHence, Eq. 16 is reduced to the form:\n$\\alpha_{ij}^k = \\frac{\\frac{f''(S^c) \\cdot (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^2}{2}}{\\frac{f''(S^c) \\cdot (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^2}{2} + \\sum_{i,j} A_{ij}^k \\frac{f'''(S^c) \\cdot (\\frac{\\partial S^c}{\\partial A_{kl}^{ij}})^3}{6}}\\ \\ \\ (25)$\nThis simplification is only feasible under the assumption that the partial derivatives of the objective\nfunction $S^c$ for the layer activation $A_{ij}^k$ exhibit a specific behavior. Specifically, the assumption is that\n$\\frac{\\partial^n S^c}{\\partial (A_{kl}^{ij})^n} = 0$ for n > 1, meaning that the first-order partial derivative $\\frac{\\partial S^c}{\\partial A_{kl}^{ij}}$ is a constant and not a function\nof the $A_{ij}^k$. This holds when $h(A_{ij}^l)$ is a linear function. When this assumption is held, simplification can\nbe conducted. However, if $h(A_{ij}^l)$ is a non-linear function, it can still be approximated by a piecewise\nlinear function $F(A_{ij}^l)$, such that $h(A_{ij}^l) = F(A_{ij}^l) + \\epsilon$ (where $\\epsilon$ represents the approximation error). The\nsimplification then becomes valid upon neglecting the approximation error. Fortunately, many common\nnon-linear activation functions, such as tanh and sigmoid, can already be expressed in piecewise linear\nforms hard-tanh and hard-sigmoid, respectively. This allows for the valid application of the simplification\nto the alpha term in Eq. 25, as the required assumptions are met.\nFor some other non-linear functions that involve more complicated behavior, like $GeLU(x)$ we can\nstill break it to a piece-wise linear form as any function can be approximated into piecewise linear\nfunctions. Thus, any non-linear function can be converted to a piece-wise linear form and the\nsimplification of the alpha term becomes valid if approximation error ($\\epsilon$) is low."}, {"title": "2.5. Weight and bias calculation", "content": "To determine the gradient-based weights for a layer", "term": "n$w_{ij"}, {"as": "n$S^c = \\sum_k w_k \\sum_{i", "term": "as a uniform value applied across\nan entire feature channel or as an element-wise term specific to individual spatial locations. These\napproaches yield the following expressions for the bias term:\n$b_k^c = S^c - \\sum_k w_k \\sum_{i,j} A_{ij}^k\\ \\ \\ (27)$\n$b_k^c = S^c - w_{ij}^{kc} \\sum_{i,j} A_{ij}^k\\ \\ \\ (28)$\nEq. 27 represents a uniform bias factor applied to an entire channel, functioning as a generalized bias\nterm, which is more in line with how actual bias works in a model. In contrast, Eq. 28 treats the bias as"}]}