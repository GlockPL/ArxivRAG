{"title": "Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality", "authors": ["Xin Kang", "Veronika Shteingardt", "Yuhan Wang", "Dov Dori"], "abstract": "Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.", "sections": [{"title": "1 Introduction", "content": "Integrating neural and symbolic approaches in AI seeks to combine the learning capabilities of neural networks with the interpretability of symbolic reasoning (Besold et al., 2017; Garcez and Lamb, 2023). However, traditional knowledge representations, such as triplet-based knowledge graphs, are limited in capturing complex processes, state changes, and hierarchical relationships inherent in dynamic systems (Wang et al., 2017; Heinzerling and Inui, 2021; Shi et al., 2021). Additionally, neural networks are often viewed as a black box due to their opaque decision-making processes, which poses significant challenges in domains requiring transparent reasoning, such as healthcare and finance (Lipton, 2018; Rudin, 2019; Doshi-Velez and Kim, 2017; Tjoa and Guan, 2020).\nRecent advancements have focused on enhancing AI reasoning capabilities by integrating language models with external knowledge sources."}, {"title": "2 Related Work", "content": "Neuro-Symbolic AI Approaches Neuro-symbolic AI integrates neural networks with symbolic reasoning to harness the strengths of both paradigms (Besold et al., 2017; Garcez and Lamb, 2023). Challenges in achieving reasoning transparency and interpretability persist, with approaches such as symbolic knowledge distillation (West et al., 2022) and factual knowledge editing (De Cao et al., 2021) addressing these issues. Frameworks like TransferNet (Shi et al., 2021) and interpretable reasoning models for dialogue generation (Yang et al., 2022) aim to provide clear reasoning paths. In sentiment analysis and mental health, neuro-symbolic frameworks like TAM-SenticNet (Dou and Kang, 2024) and causal inference models (Ding et al., 2024b,a) enhance explainability and logical inference. Specifically, in aspect-based sentiment analysis (ABSA), models such as the Multi-Agent Collaboration (MAC) (Kang et al., 2024) and approaches to improve AI transparency using generative agents (Kang, 2024) demonstrate the potential of neuro-symbolic AI in providing transparent and rational sentiment analysis.\nInterpretability and Transparency in Language Models Ensuring transparency and interpretability in AI decision-making is critical, particularly in complex systems (Lipton, 2018; Rudin, 2019). Various methods have been developed to enhance the interpretability of language models, including representation dissimilarity measures (Brown et al., 2023), SHAP-based explanation techniques (Mosca et al., 2022), and prompt-based explainers like PromptExplainer (Feng et al., 2024). Evaluation benchmarks for interpretability (Wang et al., 2022) and approaches to improve faithfulness and robustness (El Zini and Awad, 2022; Horovicz and Goldshmidt, 2024; Zhao et al., 2024) further contribute to making language models more transparent. Despite these advancements, achieving full transparency remains challenging, especially in applications requiring a clear understanding of the reasoning process.\nLanguage Models and Knowledge Graphs for Question Answering Integrating language models with knowledge graphs has been a significant focus to enhance QA capabilities. Approaches like QA-GNN (Yasunaga et al., 2021), DRLK (Zhang et al., 2022), and UniK-QA (Oguz et al., 2022) combine language models with graph neural networks and dynamic interactions to improve reasoning in QA tasks. Frameworks such as CIKQA (Zhang et al., 2023) and Triple-R (Kanaani et al., 2024)"}, {"title": "3 NCAI Framework", "content": "OPM unifies objects and processes within a single model, representing structural and behavioral aspects in both graphical and textual forms (Dori, 2002; Dori et al., 2016). OPM's bimodal property provides Object-Process Diagram (OPD) and Object-Process Language (OPL), enhancing understanding and reasoning transparency.\nTo illustrate OPM's capabilities, we use a running example based on natural language text describing the evolution of a Heuristic from a rule of thumb to a principle. This text serves as the input to the NCAI framework, as shown in Figure 1, and is provided in Appendix A.\nUsing this text, we constructed OPDs representing the processes and state changes of a Heuristic. The diagrams can be created and visualized using the OPCloud software (Dori et al., 2018; Kohen and Dori, 2021)."}, {"title": "3.1 Object-Process Methodology for NCAI", "content": "The corresponding OPL for the System Diagram (SD) and the In-Zoomed Diagram (SD1) are presented in Appendix B . These OPLs provide a textual representation that details the processes and state changes of the evolution of a heuristic from a rule of thumb to a principle.\nOPM's bimodal property, combining graphical OPD and textual OPL, facilitates a comprehensive representation of complex processes and state changes. The in-zooming mechanism allows for hierarchical decomposition, where processes can be detailed further in subsequent diagrams, enhancing understanding of intricate systems."}, {"title": "3.2 Converting Natural Language to OPM using In-Context Learning", "content": "We employ in-context learning to guide the LLM in converting natural language text into OPM models. The process involves providing the LLM with a carefully crafted prompt that includes OPM syntax, semantics, and examples. The prompt details can be found in (Dori and Shteingardt, 2025).\nLet $T_{NL}$ be the natural language text (Appendix A) and $P_{OPM}$ the prompt containing OPM instructions and examples. The input to the LLM is:\n$I = P_{OPM} \\circ T_{NL}$,\nwhere $\\circ$ denotes concatenation. The LLM generates the OPL representation:\n$T_{OPL} = LLM(I)$.\nThis process leverages the LLM's ability to generate structured textual OPL representations from unstructured text, utilizing in-context learning to guide the model's output toward the desired OPL format. The OPL generated by the LLM is presented in Appendix C.\nWhile the preliminary results are encouraging, designing prompts that yield accurate and syntactically correct OPM models from free-form text introduces several challenges. These include pinpointing the primary process, focusing on essential conceptual elements, and clarifying ambiguous relationships in the natural language source. To address these issues, we iteratively refine prompts, adjust instructions, and incorporate carefully chosen examples. Through this iterative approach, the LLM learns to better navigate textual ambiguities and produce more coherent OPM models, thus reducing the need for extensive manual refinement and enabling more reliable neuro-symbolic reasoning pipelines."}, {"title": "3.3 OPM Knowledge-Based Question-Answering System", "content": "We developed OPM-QA, an OPM knowledge-based Question-Answering system, that integrates OPM knowledge with the LLM to enhance answer accuracy and reasoning transparency. This system is a core component of NCAI, leveraging the structured knowledge representation of OPM to improve the reasoning capabilities of the LLM.\nOPM-QA employs in-context learning by providing the LLM with OPL as the OPM knowledge, a set of example question-answer pairs, and the test questions as context. The knowledge $K_{OPL}$ is derived from the constructed OPM (see Appendix B) and provides a structured and formalized representation. This structured knowledge allows the LLM to reason more effectively when generating answers.\nFor each test question $q_i$ in the set of test questions $Q_{test}$, the input to the LLM is formulated as:\n$I_i = K_{OPL} \\circ E_{QA} \\circ q_i$,\nwhere $E_{QA}$ is the set of example question-answer pairs, and $\\circ$ denotes concatenation. The LLM processes this input and generates an answer:\n$a_i = LLM(I_i)$.\nTo assess the impact of using structured OPM knowledge on the QA performance, we compare the OPM-QA with a baseline QA system using natural language knowledge (NL-QA). In NL-QA, we replace $K_{OPL}$ with the natural language knowledge $K_{NL}$, which corresponds to the text provided in Appendix A. This allows us to compare the effectiveness of the structured OPM knowledge against unstructured natural language knowledge in the QA task."}, {"title": "4 Experiments", "content": "The purpose of our experiment is to evaluate the effectiveness of the NCAI framework in performing multi-hop reasoning tasks and enhancing reasoning transparency. We aim to compare the performance of OPM-QA with the baseline NL-QA.\nData: We manually developed a dataset of 50 multi-hop reasoning question-answer pairs, following the FanOutQA benchmark (Zhu et al., 2024). These questions are based on the knowledge of the process that transforms informal rules of thumb into well-established principles. The questions require the model to integrate information from multiple statements to arrive at an answer, testing both answer accuracy and reasoning transparency. Examples of the QA pairs are provided in Appendix E, Table 3.\nKnowledge Sources: The OPL knowledge $K_{OPL}$ is the OPL generated from the constructed OPM model in Appendix B. The natural language knowledge $K_{NL}$ is the text provided in Appendix A. The QA systems use either $K_{OPL}$ or $K_{NL}$, along with 5"}, {"title": "4.1 Experiment Setup", "content": "example QA pairs $E_{QA}$ as context, to answer the 50 test questions $Q_{test}$.\nQA Systems: The QA systems employ in-context learning by providing the LLM with the respective knowledge source, a set of example QA pairs, and the test questions. The LLM used for both systems is GPT-40 (version 01-preview-2024-09-12), with parameters set to temperature = 0 and top_p = 1 to ensure deterministic output. By using the same LLM and parameter settings, we ensure a fair comparison between OPM-QA and NL-QA. The prompt used in these QA systems is shown in Appendix D. It has been carefully designed to be general enough for QA tasks across various domains and yet instructive enough to guide the model to answer with explicit reference to the OPM elements\u2014processes, objects, and states, thereby increasing reasoning transparency and answer accuracy.\nEvaluation Metrics: We evaluate system outputs using a combination of metrics that capture different aspects of answer quality and reasoning transparency. To assess how well the generated answers align with the ground truth in terms of content, we use Loose Accuracy and Strict Accuracy. Loose Accuracy measures the fraction of reference tokens that also appear in the predicted answer after lemmatization, removing stop words, and stripping punctuation, providing a relatively lenient measure of correctness. Strict Accuracy applies a non-linear weighting (with a parameter $k = 1.5$) to penalize partial matches more severely, thus enforcing a stricter standard of correctness.\nWhile Loose Accuracy and Strict Accuracy focus on token-level overlap, ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) (Lin, 2004) quantify lexical overlap through n-gram and sequence-based comparisons, capturing syntactic similarity between the generated answer and the reference. BLEURT (BT) (Sellam et al., 2020) complements these metrics by providing a more semantic-oriented evaluation, as it uses a learned model to judge the meaning and quality of the generated text. The GPT Judge Score (GPT) (Zhu et al., 2024) further evaluates factual consistency and logical coherence, reflecting how well the answer maintains internal logical structure and correctness from a large language model's perspective.\nTo address the need for a quantitative measure of reasoning transparency, we propose Transparency Precision ($P_T$), Transparency Recall ($R_T$), and Transparency F1 ($F1_T$). Let $E_P$ be the set of OPM elements-processes, objects, and states-identified in the prediction, and $E_G$ the set of OPM elements in the ground truth. Let $E_{P \\cap G}$ be the intersection of these sets, representing correctly matched OPM elements. We define:\n$P_T = \\frac{|E_{P \\cap G}|}{|E_P|}$,\n$R_T = \\frac{|E_{P \\cap G}|}{|E_G|}$,\n$F1_T = \\frac{2 \\cdot P_T \\cdot R_T}{P_T + R_T}$.\nHere, $P_T$ measures how accurately the predicted reasoning structure identifies the correct OPM elements, $R_T$ gauges how completely it recovers them, and $F1_T$ balances both. Together, these transparency metrics provide a statistical measure of how faithfully the system's reasoning aligns with the conceptual logic defined by OPM, offering a principled, quantitative response to calls for more objective assessments of reasoning transparency."}, {"title": "4.2 Results", "content": "Table 1 presents the results of our evaluation. For Loose Accuracy, OPM-QA achieves 0.858 \u00b10.162, greatly exceeding NL-QA's 0.638 \u00b1 0.212. This indicates that OPM-QA captures a significantly larger fraction of reference tokens under a lenient matching criterion. The difference is statistically significant (P < 0.001). Strict Accuracy, which imposes a harsher penalty on partial matches, shows OPM-QA at 0.806 \u00b1 0.213 compared to NL-QA's 0.530 \u00b1 0.252. This improvement is also statistically significant (P < 0.001), demonstrating that OPM-QA provides answers that are both more complete and more precisely aligned with the ground truth.\nRegarding syntactic overlap measures, OPM-QA significantly outperforms NL-QA in all ROUGE metrics. The ROUGE-1 score for OPM-QA is 0.772 \u00b1 0.159 versus NL-QA's 0.558 \u00b1 0.195, ROUGE-2 is 0.607 \u00b1 0.201 compared to 0.373 \u00b10.198, and ROUGE-L is 0.715 \u00b1 0.155 compared to 0.504 \u00b1 0.174. All these differences are highly statistically significant (P < 0.001). These results confirm that OPM-QA's generated answers exhibit considerably more lexical and subsequence-level similarity to the reference answers, adhering better to the structural and phrasing patterns of the ground truth.\nIn terms of semantic quality, the BLEURT score for OPM-QA is 0.596 \u00b1 0.165, which surpasses NL-QA's 0.474 \u00b1 0.111. This difference is statistically significant (P < 0.001). This suggests that OPM-QA not only matches lexically but also maintains closer semantic fidelity to the intended meanings of the ground truth answers.\nFactual consistency and logical coherence are further evidenced by the GPT Judge Score of 0.920 \u00b1 0.274 for OPM-QA compared to NL-QA's 0.800 \u00b1 0.404. This difference is not statistically significant (P = 0.086), although it still indicates a notable improvement in maintaining factual and logical integrity within the answers.\nMost notably, the transparency metrics reveal OPM-QA's substantial advantage in conceptual alignment. OPM-QA achieves a Transparency Precision of 0.917 \u00b1 0.161 and Transparency Recall of 0.953 \u00b1 0.143, whereas NL-QA scores 0.759 \u00b1 0.417 and 0.455 \u00b1 0.329, respectively. The Precision difference is statistically significant (P = 0.015), while Recall remains highly significant (P < 0.001). Consequently, Transparency F1 for OPM-QA is 0.922 \u00b1 0.136 compared to NL-QA's 0.546 \u00b1 0.342, with a P-value of P < 0.001. This metric, which balances Transparency Precision and Transparency Recall, underscores the overall superior performance of OPM-QA in aligning with the ground truth both accurately and comprehensively.\nOverall, the majority of these metrics demonstrate statistically significant improvements, affirming the superior performance of OPM-QA over NL-QA. Additionally, the enhancements in Transparency Precision metrics, despite being less statistically significant, further highlight OPM-QA's effectiveness in achieving greater factual consistency and precision in answers. Detailed evaluation results for 10 representative QA examples and additional evaluation tables are provided in Appendix E, including Tables 3, 4, and 5, which further confirm these findings."}, {"title": "4.3 Discussion", "content": "The experimental results confirm that grounding the reasoning process in a conceptual model leads to both improved accuracy and clearer interpretability. Compared to its counterpart, the OPM-QA system consistently aligns its reasoning with the well-defined ontology provided by the OPM model. While the NL-QA system may occasionally produce correct or partially correct answers, it often does so without revealing the underlying conceptual structure. In contrast, OPM-QA not only identifies the correct OPM elements-processes, objects, and states-required to transform the heuristic from one state to another but also presents a reasoning chain that is faithful to the conceptual logic defined by OPM."}, {"title": "5 Conclusion", "content": "We propose Neuro-Conceptual Artificial Intelligence (NCAI), a neuro-symbolic approach that integrates OPM conceptual modeling with deep learning to overcome limitations in traditional knowledge representation and reasoning. By embedding OPM-based conceptual logic into a QA system, NCAI captures complex processes and state changes that conventional triplet-based representations and black box neural models struggle to address. Through this structured, bimodal OPM representation, NCAI provides not only improved answer accuracy but also a demonstrably transparent and interpretable reasoning pathway. The introduction of transparency metrics ($P_T$, $R_T$, $F1_T$) offers quantitative support for the alignment with OPM-defined conceptual structures, moving beyond purely qualitative assessments of interpretability.\nOur experimental results demonstrate that NCAI substantially outperforms traditional methods on both standard accuracy-based measures and transparency-focused metrics. By leveraging OPM as a symbolic backbone and employing the LLM under structured guidance, NCAI brings neuro-symbolic AI closer to genuine explainability. Although this work focuses on QA, our conceptual modeling approach may generalize to other tasks requiring robust and interpretable reasoning. Future research will examine scalability to larger, more complex domains, refine prompt designs to handle richer conceptual structures, and integrate our approach with emerging prompting and agentic frameworks. The code and dataset are available on https://github.com/kangxin/NCAI.\nOne limitation of our study is that it relies on a relatively small, self-constructed dataset of 50 question-answer pairs. While sufficient for an initial proof of concept, the generalizability and scalability of NCAI to larger and more complex real-world scenarios remain to be explored. In future work, we intend to evaluate NCAI on larger publicly available benchmarks and more intricate conceptual domains, potentially requiring more efficient prompt designs or incremental model updates to handle extensive OPM knowledge.\nAdditionally, although QA serves as a proof-of-concept task to demonstrate the feasibility of integrating OPM with LLM, applying this approach to other downstream tasks, such as predictive modeling and real-time decision-making in dynamic environments, would require additional domain-specific adaptations and possibly integration with external data sources. While the OPM-based reasoning structure holds promise beyond QA, confirming its utility in these broader contexts remains an area for future investigation.\nMoreover, while our method improves transparency through OPM-driven conceptual alignment, certain ambiguities in the source text can still challenge the strict adherence of the LLM to OPM syntax and conventions. The generated OPM representations might require subsequent refinement by human modelers or more specialized training to ensure full syntactic correctness. Developing standardized benchmarks and further metrics for reasoning transparency, as well as exploring more advanced prompting and agentic design patterns, can help refine the approach, but these steps also remain as future endeavors."}, {"title": "Limitations", "content": "One limitation of our study is that it relies on a relatively small, self-constructed dataset of 50 question-answer pairs. While sufficient for an initial proof of concept, the generalizability and scalability of NCAI to larger and more complex real-world scenarios remain to be explored. In future work, we intend to evaluate NCAI on larger publicly available benchmarks and more intricate conceptual domains, potentially requiring more efficient prompt designs or incremental model updates to handle extensive OPM knowledge.\nAdditionally, although QA serves as a proof-of-concept task to demonstrate the feasibility of integrating OPM with LLM, applying this approach to other downstream tasks, such as predictive modeling and real-time decision-making in dynamic environments, would require additional domain-specific adaptations and possibly integration with external data sources. While the OPM-based reasoning structure holds promise beyond QA, confirming its utility in these broader contexts remains an area for future investigation.\nMoreover, while our method improves transparency through OPM-driven conceptual alignment, certain ambiguities in the source text can still challenge the strict adherence of the LLM to OPM syntax and conventions. The generated OPM representations might require subsequent refinement by human modelers or more specialized training to ensure full syntactic correctness. Developing standardized benchmarks and further metrics for reasoning transparency, as well as exploring more advanced prompting and agentic design patterns, can help refine the approach, but these steps also remain as future endeavors."}]}