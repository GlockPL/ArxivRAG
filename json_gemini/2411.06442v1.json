{"title": "Local Implicit Wavelet Transformer for Arbitrary-Scale Super-Resolution", "authors": ["Minghong Duan", "Linhao Qu", "Shaolei Liu", "Manning Wang"], "abstract": "Implicit neural representations have recently demonstrated promising potential in arbitrary-scale Super-Resolution (SR) of images. Most existing methods predict the pixel in the SR image based on the queried coordinate and ensemble nearby features, overlooking the importance of incorporating high-frequency prior information in images, which results in limited performance in reconstructing high-frequency texture details in images. To address this issue, we propose the Local Implicit Wavelet Transformer (LIWT) to enhance the restoration of high-frequency texture details. Specifically, we decompose the features extracted by an encoder into four sub-bands containing different frequency information using Discrete Wavelet Transform (DWT). We then introduce the Wavelet Enhanced Residual Module (WERM) to transform these four sub-bands into high-frequency priors, followed by utilizing the Wavelet Mutual Projected Fusion (WMPF) and the Wavelet-aware Implicit Attention (WIA) to fully exploit the high-frequency prior information for recovering high-frequency details in images. We conducted extensive experiments on benchmark datasets to validate the effectiveness of LIWT. Both qualitative and quantitative results demonstrate that LIWT achieves promising performance in arbitrary-scale SR tasks, outperforming other state-of-the-art methods. The code is available at https://github.com/dmhdmhdmh/LIWT", "sections": [{"title": "Introduction", "content": "Single Image Super-Resolution (SISR) refers to the process of recovering a high-resolution (HR) image from a single low-resolution (LR) image and has been widely applied across various fields [9, 10, 24, 26, 35]. Most existing SISR models comprise a deep neural network (DNN) with an upsampling module like learnable deconvolutions or pixel shuffling [8, 25] and can only deal with integer scaling factors, and these models necessitate retraining when encountering new scaling factors. Recent work has achieved arbitrary-scale super-resolution (SR) by replacing the upsampling layer typically used in previous methods with a local implicit image function and has demonstrated exemplary performance [5, 16]. These methods based on local implicit functions first extract features from LR images through a DNN-based encoder and then employ multi-layer perceptrons (MLPs) to map the 2D query coordinates of the HR image and the aggregated representation of the corresponding local region features (called latent code) to RGB values. There are two limitations to these existing methods. Firstly, the coordinate-based local ensemble technique [5, 16] used for querying RGB values fails to consider the relevance of features within local regions. Ensemble weights are typically computed based on the rectangular area between the query point and each nearest point (Figure 1(a)). These weights are solely dependent on the positional relationship between the query point and its nearest coordinates of local features and do not account for the features themselves, thus limiting the reconstruction performance of the model. Secondly, only the four nearest latent codes to the query point are used when querying RGB values based on coordinates. We argue that the representational capacity of LR features directly obtained from the encoder is limited, especially in large-scale SR, which may lead to blurry results lacking texture details. Introducing high-frequency prior information of image features into the local implicit functions is therefore necessary.\nMany existing methods have shown that high-frequency prior information obtained from discrete wavelet transform (DWT) can improve the performance of SR models based on deep learning [12, 28, 34]. However, the scale transformation rate of the DWT when applied to features or images is limited to powers of 2, and most DWT-based methods rely on this property for inverse transformation to achieve upsampling, thereby not achieving arbitrary-scale SR. To better restore high-frequency details while achieving arbitrary resolution upscaling, we propose the Local Implicit Wavelet Transformer (LIWT), which leverages cross-attention to exploit the high-frequency information obtained from DWT fully and accounts for the relevance of the features within a local region. As shown in Figure 1(b), LIWT consists of a Wavelet Mutual Projected Fusion (WMPF), a Wavelet-aware Implicit Attention (WIA), and a decoder. We first extract features from the LR image using an encoder and then decompose the features obtained into low-frequency components $LL$ and high-frequency components $LH$, $HL$, and $HH$ using DWT. To enhance LIWT's capability"}, {"title": "Related work", "content": "Single image super-resolution. SRCNN [7] pioneered the implementation of SISR using a convolutional neural network (CNN) in an end-to-end manner, subsequently leading to a series of methods that utilized other CNN modules designed to enhance SR performance further. Examples include methods employing residual modules such as EDSR [18] or dense connection modules such as RDN [33]. In addition, methods based on attention mechanisms [3, 6, 17, 22, 23, 27, 32] have been introduced into SISR, including channel attention [6, 23, 32], self-attention [3, 17], and non-local attention [22, 27].\nDWT based image super-resolution. DWT has been widely used in SISR due to its ability to express different frequency components [11, 12, 19, 28, 30, 34]. To leverage the high-frequency representation capability of DWT, WDRN [28] guides the feature extraction process to preserve high-frequency features in the wavelet domain, reconstructing HR images with more precise details; JWSGN [34] utilizes DWT to transform input features into the frequency domain and generate edge feature maps to correct high-frequency components, further recovering high-frequency details. Despite their impressive results, these SISR models require training different models for each upscaling factor, limiting their applicability.\nArbitrary-scale super-resolution. Several methods have been proposed to train a unified model capable of handling arbitrary upscaling factors [4, 5, 13, 16, 20, 29]. LIIF [5] employs an MLP as a local implicit function, which predicts the RGB values at any query coordinate by acquiring the HR image coordinates and the features around the coordinates. LTE [16] further introduces a local texture estimator that enriches the representation capability of local implicit functions by predicting Fourier information. Unlike the above methods, we introduce the high-frequency prior obtained from the wavelet transform into the local implicit function, further improving the recovery of high-frequency details."}, {"title": "Methodology", "content": "The overall framework of the proposed model, which integrates the encoder, the Wavelet Enhancement Residual Module (WERM), and the LIWT, is given in Figure 2(a). For a given LR image $I^{LR} \\in R^{H \\times W \\times 3}$ at the 2D LR coordinates $X^{LR} \\in X$ and any magnification $s = {sh, Sw}$, the model outputs an HR image $I^{HR} \\in R^{ShH \\times swW \\times 3}$ at the 2D HR coordinates $X^{HR} \\in X$, where x is the 2D coordinate space used to express the continuous 2D image domain. First, we employ the encoder $E_{\\theta}$ to extract features from the LR image, obtaining $Z \\in R^{H \\times W \\times C}$. Subsequently, we decompose Z using discrete wavelet transform (DWT) into low-frequency component $LL$ and high-frequency components $LH$, $HL$, and $HH$. The low-frequency component $LL$ is denoted as $F^{L}$, while the high-frequency components $LH$, $HL$, and $HH$ are concatenated along the channel axis and represented as $F^{H}$. We further process $F^{L}$ and $F^{H}$ using WERM to isolate high-frequency information, yielding enhanced texture features $Fw \\in R^{H/2 \\times W/2 \\times C}$ as high-frequency priors. We perform bicubic upsampling of $Fw$ to obtain $\\hat{Fw} \\in R^{H \\times W \\times C}$ and input it with the feature Z and the 2D HR coordinate $X_{HR}$ into the LIWT to generate the RGB values of the residual image $I^{HR} \\in R^{shH \\times swW \\times 3}$. Finally, the residual image $I^{HR} \\in R^{shHxswW \\times 3}$ is summed via element-wise addition with the bilinearly upsampled image $\\hat{I}^{HR} \\in R^{ShH \\times swW \\times 3}$ to generate the final predicted HR image $I^{HR}$. LIWT consists of the Wavelet Mutual Projected Fusion (WMPF), the Wavelet-aware Implicit Attention (WIA), and a decoder, collectively utilizing the high-frequency prior $Fw$ to recover image details. We train the entire framework using the L1 loss and utilize the Haar wavelet transform to perform the DWT."}, {"title": "Wavelet Enhancement Residual Module", "content": "To extract more high-frequency information conducive to reconstruction from the different components obtained through DWT decomposition, we devised WERM to process $F^{L}$ and $F^{H}$, which comprises several wavelet enhancement residual blocks (WERBs). As illustrated in Figure 2(d), the input to the kth WERB is denoted as $F^{L(k)}_{in}$ and $F^{H(k)}_{in}$, and the output is denoted as $F^{L(k)}_{out}$ and $F^{H(k)}_{out}$. Initially, we preprocess $F^{L}$ and $F^{H}$ separately using a 3 \u00d7 3 convolutional layer to obtain $F^{L}_{conv}$ and $F^{H}_{conv}$, which are then fed into the first WERB. We further designed a wavelet high-frequency enhancement residual block (WHFERB) to separate more sharp edge components from the smoothed surface of $F^{L(k)}_{in}$ and assist in reconstruction at each WERB. Figure 3 shows WHFERB consists of a Wavelet Local Feature Extraction (WLFE) branch and a Wavelet High-Frequency Enhancement (WHFE) branch. For the WLFE branch, we employ a 3 \u00d7 3 convolutional layer followed by a ReLU activation function to extract local features. For the WHFE branch, we employ a max-pooling layer to extract high-frequency information from the input features and use a 3 \u00d7 3 convolutional layer followed by a ReLU activation function to enhance the high-frequency features. Subsequently, we concatenate the outputs of the two branches and input them into a 1 \u00d7 1 convolutional layer for information fusion. We refine the high-frequency feature $F^{H}_{in}$ through two consecutive 3 \u00d7 3 convolutional layers to obtain $F^{H(k)}_{out}$. Then, the outputs of the two branches are concatenated and input into a 3 \u00d7 3 convolutional layer for information fusion to obtain $F^{H(k)}_{fusion}$. To effectively utilize high-frequency information and maintain training stability, we introduce skip-connections to transmit the fused information $F^{H(k)}_{fusion}$ back to the two branches separately, allowing the frequency information of the two branches to complement each other. For the WERM containing n WERBS, we concatenate the outputs $F^{L(n)}_{out}$ and $F^{H(n)}_{out}$ of the nth WERB and fuse them using a 3 \u00d7 3 convolutional layer. We also introduce skip connections to add the fused result to $F^{H}_{conv}$ to obtain the final high-frequency prior feature representation $Fw$. In this paper, we set n to 4."}, {"title": "Local Implicit Wavelet Transformer", "content": "Overview. LIWT processes the high-frequency prior feature $Fw$ and features Z from the encoder, and it outputs the RGB values of the corresponding residual image $I^{HR} \\in R^{ShH \\times SwW \\times 3}$ based on the input 2D HR coordinates $x^{HR}$. Figure 2(a) illustrates that it consists of the WMPF, the WIA, and a decoder $D_{\\theta}$ parameterized by $\\theta$. To leverage the useful texture information from the high-frequency prior $Fw$, we perform bicubic upsampling on $Fw$ to obtain $\\hat{Fw}$ firstly, which has the same size as feature Z. Subsequently, we assign weight coefficients to each channel of $\\hat{Fw}$ and feature Z and then fuse them to yield $F_{WMPF}$ through WMPF. This operation aims to direct the LIWT to focus more on the high-frequency components in $\\hat{Fr}$ that are beneficial for detail recovery. Simultaneously, we use convolutional layers to project $\\hat{Fw}$ and derive the latent embedding corresponding to the query q. Additionally, using two separate convolutional layers, we project Z to obtain latent embeddings for key k and value v, respectively. WIA estimates the local latent embedding $\\tilde{z} \\in R^{9 \\times C}$ for HR coordinates within"}, {"title": "Wavelet Mutual Projected Fusion", "content": "Fusion. LIWT utilizes WMPF to fuse the high-frequency details in $Fw$ with the features Z from the encoder. As illustrated in Figure 3, WMPF first concatenates $\\hat{Fw}$ and feature Z along the channel dimension to form the input $F^{in}_{WMPF}$. Subsequently, the entire input is represented through global average pooling, followed by learning the non-linear interactions within the concatenated features $F^{in}_{WMPF}$ across channels using FC-ReLU-FC layers, where FC represents a fully connected layer. Finally, attention coefficients are obtained via a sigmoid activation function, which is then multiplied with the features $F^{in}_{WMPF}$ at the channel level, enabling effective focus on the high-frequency components and contextual information of the features. Additionally, skip-connections are introduced to maintain training stability, and the fused features $FwMPF$ are outputted through a 1 \u00d7 1 convolutional layer."}, {"title": "Wavelet-aware Implicit Attention", "content": "As shown in Figure 2(c), LIWT utilizes WIA to perform cross-attention on the local grid, generating local latent embeddings $\\tilde{z} \\in R^{9 \\times C}$ for each HR coordinate. To effectively utilize the fused features $F_{WMPF}$ in the attention mechanism and avoid generating incorrect textures, we further fuse $\\hat{F}_{WMPF}$ and value v to obtain vf:\n$v_{f} = FC \\left( \\text{Concat } (v, \\hat{F}_{WMPF})\\right) + v$ (4)\nWe define $q \\in R^{1 \\times C}$, $k \\in R^{9 \\times C}$, and $v \\in R^{9 \\times C}$ on the local grid as sets $q = {q_{\\tau} | \\tau = 1, 2, ..., H}$, $k = {k_{r}^{\\tau} | r = 1, 2, ..., 9, \\tau = 1, 2, ..., H}$, and $v_{f} = {v_{f}^{r\\tau} | r = 1, 2, ..., 9, \\tau = 1, 2, ..., H}$, where r represents the latent code index on local grid, H represents the number of attention heads, and we set H to 8. WIA first calculates the inner product of q and k, then adds the relative position deviation B to the calculation result to obtain the attention matrix. Subsequently, the attention matrix is normalized through Softmax operation to generate a local attention map. Finally, WIA performs element-wise multiplication on vf and the local attention map and concatenates the operation results of different attention heads to obtain $\\tilde{Z}$:\n$\\tilde{z} = \\text{Concat } \\left( \\sum_{\\tau=1}^{9} \\frac{\\exp \\left( \\frac{q_{\\tau}^{\\top} k_{r}^{\\tau}}{\\sqrt{C / H}} + B\\right)}{\\sum_{i=1}^{9} \\exp \\left( \\frac{q_{\\tau}^{\\top} k_{i}^{\\tau}}{\\sqrt{C / H}} + B\\right)} \\times v_{f}^{\\tau} \\right)_{\\tau=1,2,...,H}$ (5)"}, {"title": "Experimental Results", "content": "Following [5, 16], we train our model on the training set of the DIV2K dataset [1], which comprises 800 images with a resolution of 2K. We evaluate the performance on the DIV2K validation dataset and benchmark datasets, including Set5 [2], Set14 [31], B100 [21], and Urban100 [14]. Consistent with previous SR methods based on implicit neural representation [5, 13, 16, 20, 29], we employed PSNR as the evaluation metric. We also provide SSIM and LPIPS evaluation metrics in the appendix.\nCurriculum Learning and Training details. Consistent with prior work [4, 16], we employed EDSR [18], RDN [33], and SwinIR [17] without upsampling modules to serve as encoders. For the synthesis of training data in each batch, we cropped patches of size 48s \u00d7 48s from each HR image, then downsampled them using bicubic interpolation to produce LR images of size 48 \u00d7 48 as inputs to the model. Subsequently, we sampled 482 pixels from the corresponding cropped patches to form RGB-coordinate pairs as ground truth. To enhance the model's ability to recover high-frequency details in large-scale SR, we proposed a curriculum learning training strategy. We trained the model for 1000 epochs. Within the first 250 epochs, we sampled the upsampling factor s from a uniform distribution $U(1,4)$. In the range of epochs from 250 to 500, we sampled the upsampling factor s from a uniform distribution $U (1,6)$. In the range of epochs from 500 to 1000, we sampled the upsampling factor s from a uniform distribution $U(1,8)$. By gradually expanding the range of scale factor sampling during training, the model can progressively learn how to handle large scaling factors. We set the batch size to 32 and utilized the Adam optimizer. The learning rate was initially set to 1e-4 and decayed by a factor of 0.5 every 200 epochs. Please refer to the appendix for our analysis of the training strategy."}, {"title": "Comparison with Previous Methods", "content": "We compare the proposed LIWT with existing arbitrary-scale SR methods [5, 13, 16, 20, 29]. Table 1 presents the PSNR metric results of all comparing methods on the DIV2K validation dataset. As shown in Table 1, our LIWT achieves the best performance across different scaling factors, which indicates that our method can be integrated with different encoders to improve performance. Table 2 depicts the results of LIWT on the four benchmark datasets. Our LIWT achieves the best performance across scaling factors of \u00d76, \u00d78, and \u00d712. At low scaling factors of \u00d72, \u00d73, and \u00d74, our method also achieves the performance in most scenarios. We have also provided a quantitative comparison under non-integer scaling factors in the appendix. Figure 4 shows the visual results of different methods. Benefiting from the high-frequency prior introduced by wavelet transform in the attention mechanism and the curriculum learning strategy, our method achieves accurate structural restoration."}, {"title": "Ablation Study", "content": "Effectiveness of WERM. We conduct extensive experiments using EDSR as the encoder to evaluate the effectiveness of WERM, as shown in Table 3. Upon separately removing the branches for processing $F^{H}$ and $F^{L}$, we observe a decrease in performance, indicating the impact of different frequency bands on reconstruction. Similarly, performance degradation is observed upon removing WERM and WHFERB. Figure 5(a) visualizes the features at various stages of WERM. The output Fw of WERM significantly enhances the representation of sharp edge textures (petal edges), with clear distinctions between edge and smooth regions, achieving more prominent discriminability than any feature at previous stages."}, {"title": "Effectiveness of WMPF and WIA", "content": "WIA. We conduct extensive experiments on different datasets to assess the effectiveness of WMPF and WIA. Table 4 demonstrates a performance decrease at most scaling factors after removing WMPF, indicating the role of high-frequency priors introduced by WMPF. To investigate the effectiveness of the cross-attention mechanism in WIA, we design the following two variant networks based on the self-attention mechanism: (1) LIWT(SA): replacing the query q with the latent embedding obtained by projecting the features Z from the encoder; (2) LIWT(WSA): replacing both the query q and key k with the latent embedding obtained by projecting $\\hat{Fw}$. Table 5 shows that LIWT achieves the best results across various scaling factors except for \u00d74 on Urban100 only. We provide visualizations of the cross-attention maps in Figure 5(b) to visually illustrate the effectiveness of WIA. The direction of attention map distribution aligns with the direction of these high-frequency details. This indicates that WIA successfully captures sharp-edge detail features. Besides, we observe a performance drop by removing the positional encoding and cell in Table 6, highlighting the importance of position and pixel size information for LIWT."}, {"title": "Complexity analysis", "content": "We investigate the computational consumption on an NVIDIA RTX 3090 24GB device. We use LR images of 192 \u00d7 192 size as input and evaluated them in the \u00d74 SR task. As shown in Table 7, although our model is slightly inferior to LTE [16] and LIIF [5] in terms of model size, memory consumption, and inference time, it can bring a PSNR improvement of more than 0.1 dB."}, {"title": "Conclusion", "content": "In this paper, we propose a novel Local Implicit Wavelet Transformer for arbitrary-scale SR. Specifically, we introduce high-frequency prior information of the features via discrete wavelet transform and a Wavelet Enhancement Residual Module and then effectively utilize high-frequency priori information to improve the reconstruction performance by utilizing the proposed Wavelet Mutual Projected Fusion and Wavelet-aware Implicit Attention module. Extensive experiments have shown that LIWT has superior performance to other methods."}, {"title": "Haar Wavelet Transform", "content": "The Haar wavelet transform is a simple and computationally efficient method for decomposing input signals into low-frequency and high-frequency sub-bands, widely employed in the field of computer vision [11, 12, 19, 28, 30, 34]. In this paper, we utilize the Haar wavelet transform to perform the discrete wavelet transform (DWT) on the features Z obtained from the encoder. The Haar wavelet transform typically involves processing the input signal with high-pass filter HT and low-pass filter $L^{T}$ to obtain different sub-bands. Specifically, the low-pass and high-pass filters are:\n$L^{T} = \\frac{1}{\\sqrt{2}} [1  1], H^{T} = \\frac{1}{\\sqrt{2}} [1  -1]$ (7)\nSimilarly, the filters of the Haar wavelet transform consist of four 2\u00d72 kernels, including $LL^{T}, LH^{T}, HL^{T}$ and $HH^{T}$. In this paper, we use $LL^{T}$ to process the feature Z to obtain the low-frequency component LL, and respectively use $LH^{T}, HL^{T}$ and $HH^{T}$ to process the feature Z to obtain the high-frequency components LH, HL and HH. Following the wavelet transform, the low-frequency component exhibits smooth surface and texture information, while the high-frequency components capture more complex texture details. We denote the low-frequency component LL as $F^{L}$ and concatenate the high-frequency components LH, HL, and HH along the channel axis, represented as $F^{H}$."}, {"title": "Analysis on the training strategy", "content": "We analyzed the impact of training strategies on the DIV2K dataset [1]. As shown in Table 8, training within a small scale sampling range of $s \\sim U(1,4)$ enables the model to achieve good performance at small upscaling scales but sacrifices reconstruction accuracy at higher scaling factors. Expanding the scale sampling range to $s \\sim U(1,8)$ during training can enhance the model's reconstruction performance at larger upscaling scales but decrease performance at smaller upscaling scales. Our proposed curriculum learning strategy gradually expands the sampling range during training. Although the performance at scaling factors of \u00d72 and \u00d73 is not as good as training within the small-scale sampling range of $s \\sim U(1,4)$, overall, it achieves the best balance across different scaling factors. Our training approach ensures effective reconstruction at large sampling scales and achieves the most optimal or suboptimal results across various scaling factors. To validate the generality of the proposed curriculum learning training strategy, we applied the same training setup to LIIF [5] and LTE [16]. As shown in Table 9, we observed performance improvements, indicating the effectiveness and generalizability of our training strategy."}, {"title": "More evaluation metrics", "content": "We employ two metrics, SSIM and LPIPS, to further demonstrate the effectiveness of LIWT compared to other arbitrary-scale SR methods. We compare the performance of LIWT, LTE [16], LIIF [5], and MetaSR [13] using SwinIR as the encoder on Set14 [31] and Urban100 [14] datasets. Typically, higher SSIM and lower LPIPS correspond to better performance. From the results in Table 10, it can be observed that except for \u00d72 scaling, our method achieves the highest SSIM and the lowest LPIPS. This indicates that our approach can recover more structural information and has better perceptual quality."}, {"title": "Comparison with DWT-based SR methods", "content": "We compare LIWT with other DWT-based SR methods [11, 15, 19, 28, 30, 34] using PSNR and SSIM metrics on Set14 [31] and Urban100 [14], where LIWT utilizes SwinIR[17] as the encoder. As shown in Table 11, our LIWT achieves the best results at various scaling factors."}, {"title": "Comparison of different arbitrary-scale SR methods at non-integer scale", "content": "To further assess the advantages of our method over other arbitrary-scale SR methods, we present comparative results of PSNR and SSIM metrics at non-integer scaling factors on Set14 [31] and Urban100 [14]. As shown in Table 12, Our LIWT achieves optimal results at various scaling factors."}, {"title": "The analysis of large-scale SR", "content": "We analyzed the advantages of our LIWT on the benchmark dataset for large-scale SR scenarios. We define scales larger than \u00d76 as large-scale. As shown in Table 13, our method achieves optimal results for large-scale at \u00d76, x8, and \u00d712. As shown in Figure 6, LIWT with RDN [33] as the encoder can even outperform other methods with SwinIR [17] as the encoder at \u00d78 SR on Set5 [2]."}]}