{"title": "BROADENING ACCESS TO SIMULATIONS FOR END-USERS VIA LARGE LANGUAGE\nMODELS: CHALLENGES AND OPPORTUNITIES", "authors": ["Philippe J. Giabbanelli", "Jose J. Padilla", "Ameeta Agrawal"], "abstract": "Large Language Models (LLMs) are becoming ubiquitous to create intelligent virtual assistants that assist\nusers in interacting with a system, as exemplified in marketing. Although LLMs have been discussed\nin Modeling & Simulation (M&S), the community has focused on generating code or explaining results.\nWe examine the possibility of using LLMs to broaden access to simulations, by enabling non-simulation\nend-users to ask what-if questions in everyday language. Specifically, we discuss the opportunities and\nchallenges in designing such an end-to-end system, divided into three broad phases. First, assuming the\ngeneral case in which several simulation models are available, textual queries are mapped to the most\nrelevant model. Second, if a mapping cannot be found, the query can be automatically reformulated and\nclarifying questions can be generated. Finally, simulation results are produced and contextualized for\ndecision-making. Our vision for such system articulates long-term research opportunities spanning M&S,\nLLMs, information retrieval, and ethics.", "sections": [{"title": "1 INTRODUCTION", "content": "As modelers, we are not usually our own end-users. Rather, models may be commissioned to address specific\norganizational and societal needs, co-designed with subject matter experts or individuals who provide their\nlived experiences, and employed as decision-support tools that eventually impact communities (Loeffler\nand Loeffler 2021; Oldfield and Haig 2022). End-users such as model commissioners, stakeholders, and\ncommunity members may not have technical expertise in a modeling language or simulation environment.\nYet, they need to interact with simulation models. When engagements are limited in scale, modelers may\nact as facilitators to translate the questions of end-users onto queries for a simulation engine, and translate\nsimulation results into a contextualized and accessible response. However, a reliance on modelers can\ncreate a bottleneck in the process, since it depends on staffing limitations and cannot easily scale-up.\nFor example, simulation models for welfare allocation, obesity management, or suicide prevention would\npotentially impact the lives of millions of individuals: sufficiently staffing a 'modeling helpdesk' to help\nindividuals ask questions and operate a model could be cost-prohibitive. In addition, when end-users either\nneed help to interpret a model or start to dedicate their own time to learning about modeling, they are less\nable to translate simulation insights into practices (Zellner et al. 2022).\nEnvironments such as Net Logo and ClouDES have been developed to make it easier for non-modelers\nend-users to interact with simulations (Ramli et al. 2015; Padilla et al. 2014). Several such environments\nhave been presented at the Winter Simulation Conference (\u2018WinterSim'), such as Sim4edu.com (Wagner\n2017). These environments often provide web-based interfaces and they emphasize low-code/no-code\ninteractions (Hewage et al. 2024; Bocciarelli and D'Ambrogio 2023) by allowing users to modify elements\nsuch as sliders or text fields as a means to input a new scenario. When existing interactive elements are not\nsufficient to input a scenario, drag-and-drop features may allow users to build more complex alternatives,\nas shown in commercial simulators (Chong et al. 2022). However, these practices still face two challenges:\na lack of usability, and limited potential for reuse. First, although interactions are intended to be 'user-"}, {"title": "2 CONCEPTUAL FRAMEWORK", "content": "Our framework is summarized in Figure 1. It is designed to allow users to express questions naturally,\nwhich would be answered thanks to a simulation model. We assume that the purpose of the query is a"}, {"title": "3 WHAT IS A QUESTION? ALIGNING CONSTRUCTS WITH MODEL PARAMETERS", "content": "Let's start with an example based on Tolk et al. (2013), where users ask questions regarding the consequences\nof sea level rise and its effect on flooding in city areas. Users want to know whether to vacate, invest, or\nmaintain (VIM). Accordingly, users can ask \u201cwhat VIM decision should I make in the downtown area if\nsea level will rise by six inches?\u201d To formulate this question requires known statements that establish what\nvacating, maintaining, and investing are. It also requires statements (potentially unknown at the time) of\nthe needed factors, and their combination, that may lead to a VIM decision. A question may vary depending\non the purpose of the answer (intent). The question here was formulated from the perspective of a city's\ndecision makers that need to assess where to vacate, maintain, or invest. This example previews major\nchallenges (Zeigler, Muzy, and Kofman 2019): users may be unfamiliar with modeling practices and the\nlevel of details that a model needs (i.e., scope and resolution); models may answer a question partially;\nwhat-if scenarios can have different effects in different models.\nGiven these challenges, we need to operate under clear definitions. According to Tolk et al. (2013), a\nmodeling question is \u201ca collection of sentences to which truth values needs to be assigned\" and akin to a\nquery directed to a referent, or in our context, a reference model. A reference model captures everything that\nis known or assumed about a problem domain. Depending on social processes dynamics and experiences\nof a team, the reference model may be reduced into a few options as represented by different conceptual\nmodels. There can also be multiple ways of implementing a simulation given a conceptual model, for\nexample based on data availability (Freund and Giabbanelli 2021). As a result, there is not necessarily\n'one' valid model to answer a user's question. Rather, the system needs to identify a candidate simulation\nmodel to answer the question at the scope and resolution needed to meet the intent of the final user. We\nfocus on identifying one candidate model, while noting that several algorithms can be applied if the system\""}, {"title": "4 CAN (A)I HAVE A WORD WITH YOU? CLARIFYING QUESTIONS FROM THE LLM", "content": "The user's question will be answered by retrieving, running, and explaining a simulation model. We expect\na gap between the user's query and a model, given that they may use different words. One of the primary\nconcerns in the field of Information Retrieval (IR) is to resolve the lexical gap between a user's query\nand the identification of a suitable 'document' (i.e., a simulation model in our case). As summarized\nby Anand et al. (2023), multiple IR techniques seek to \u201creconcile the difference between user intent and the\nintent of the [modeled] system\u201d. Query Rewriting/Reformulation (QR) is particularly common to address\nunderspecified and ambiguous queries, prior to (or instead of) asking a user for clarifications. Queries can\nbe expanded automatically by prompting LLMs to recommend useful expansion terms and appending them\nto the user's original question (Wang et al. 2023). It is even possible to request LLMs to create different\nprompts and answer them, thus using the variability of outputs to yield a greater variety of additional\nterms (Dhole and Agichtein 2024). However, adding terms may drift from the user's intent and increase\ncomputing time when retrieving a simulation model. An alternative is to rephrase the query by replacing"}, {"title": "5 RESPONSIBLE RESPONSES: CONTEXTUALIZING ANSWERS AND PROMOTING ETHICS", "content": "At this stage, the user asked a (potentially reworded and clarified) what-if question that calls for exper-\nimentation by simulation and a relevant model has been retrieved. We thus need to run the simulation\nand explain its results (Figure 4). The alignment between the user's question and the simulation model\ninforms us on which parameters need to be modified from their default values. The question must be parsed\nfurther to determine the direction and level of change on each parameter. The direction of effect may be\ndetected by the LLM. The level of change can be obtained by using fuzzy logic to transform the qualitative\nmodifiers expressed by the user (e.g., \u2018way more' after-school programs, \u2018modest increase' in minimum\nwage) into values within the operating domain of each parameter (Giabbanelli and Crutzen 2014). If a\nmodel is stochastic, multiple simulation runs will be needed. Several methods such as desired Confidence\nIntervals can determine the number of runs (see section 9.7.1 in Robinson 2014). However, if the proposed\nsystem operates as an open public platform where any number of individuals can ask questions, then the\ndelivery of compute resources in the context of citizen science may become a determining factor."}, {"title": "6 WHAT IS A GOOD SYSTEM? MEASURING FAITHFULNESS, BIASES, AND EXPLANATIONS", "content": "While our framework would create a combination of simulation models and LLMs that is accessible to a\nbroad audience, evaluating the system is a critical challenge, compounded by the limitations of the two\nparts as both simulations and LLMs can produce outputs that are not always accurate, reliable, or unbiased,\nor answers may simply get \u2018lost in translation'. To address these challenges, a multifaceted evaluation\napproach should be adopted by focusing on three main phases defined in the previous sections: question\nprocessing, query clarification/reformulation, integration of simulated output and response generation. As\npart of this evaluation, it is important to first create a carefully annotated dataset to provide a benchmark.\nThis data should consist of user questions, mapped queries, and simulated outputs. Given the vast space of\npossible user questions, automatically generating user questions based on parameters can be considered.\nSince the LLMs are essentially serving as the bridge between the user and the simulation model, the\nfirst stage of evaluation should focus on whether the LLM accurately understands the user's question and\nmaps it to a set of parameters that can be understood by the simulation model. LLMs are capable of\nprocessing natural language, but how well do they interpret the user's intentions/constructs and formulate\na query that can be used as input to a simulation model? Evaluation at this stage would thus identify when\nsimulation model parameters are not compatible/complete/found with the user's question. This evaluation\nis a necessary part of our system, as discussed in section 4 where the LLM asks clarification questions to\nthe user until satisfactory queries can be formulated. The most visible part for the user is the generation\nof the final response, hence we focus on its evaluation.\nMetrics of faithfulness assess how well the LLM's response reflects the content of the simulation\nresults, avoiding any misinterpretations or injections. Standard metrics (e.g., ROUGE, BERTScore) are not\neffective in capturing nuanced faithfulness or lack thereof, as they focus on the overall semantic similarity.\nInstead, one should rely on semantic inference related measures such as textual entailment which has been\nfound effective in identifying faithfulness (Maynez et al. 2020). Natural Language Inference (NLI) is a\ntask in natural language processing where given two pieces of text a premise (LLM's response) and a\nhypothesis (simulated output) \u2013 the goal is to determine whether the hypothesis can be inferred from the\npremise, with common relationships including entailment, contradiction, and neutral. An evaluator can\nleverage existing NLI models or train one by fine-tuning a pretrained language model such as BERT on\na new NLI dataset of simulated outputs. Specific factuality metrics based on textual entailment include\nAdversarial Natural Language Inference (Nie et al. 2019), Summary Consistency (Laban et al. 2022) and\nFactCC (Kryscinski and McCann 2021) to determine whether a given text is consistent with the source.\nBecause assessing faithfulness requires logical inference over factual information, several question-\nanswering (Q&A) based models can also be used such as QAGS (Wang, Cho, and Lewis 2020) and\nFEQA (Durmus, He, and Diab 2020). Faithfulness is only one aspect of evaluation. Another important\ndimension is that of biases, which can happen in LLMs due to issues at various stages such as dataset\nselection, annotation, and instruction tuning (Agiza, Mostagir, and Reda 2024). While several definitions\nof fairness and biases exist, two commonly used ones focus on demographic parity and equal opportunity.\nThe former compares the distribution of sensitive demographic attributes (e.g., race, gender) in the LLM's\noutputs to the distribution in the real world or the training data, with ideal outputs not favoring any\nparticular demographic group. The latter assesses whether the LLM provides similar response for different\ndemographic groups. For example, if you ask an LLM to suggest salary increments for a person, it\nshould offer similar options regardless of the person's gender. Recently introduced metrics such as Large"}, {"title": "7 DISCUSSION AND CONCLUSIONS", "content": "We articulated challenges and opportunities to design an end-to-end system that supports a broad set of\nend-users in interacting with simulations via textual queries. With the right resources, many of the steps that\nwe outlined can be addressed by the research community in the short- to medium-term. However, there are\nalso five long-term research challenges that warrant particular considerations. First, what if no single model\ncan be retrieved by the LLM to answer a user? For example, there may be two models: one that computes\nthe effect of A on B, and another that has parameter B and outcome C. A user may ask how a small increase\nin A impact C, which is not directly answerable by a single model but could be addressed by composing\nmodels. Creating a combination of models on the fly is complicated, as models may be intertwined rather\nthan neatly separable. At least, we should ensure that the combined model is consistent (i.e., without\ncontradicting statements). Second, user questions may not be as independent as they seem. For example, in\na validated model for suicide prevention, users could ask about the effect of increasing training for doctors:\nlittle to no impact. They could then ask about the effect of adding mental health education programs: little\nto no impact. However, when combined, these two evidence-based interventions could increase suicidal\nbehavior by creating an imbalance between service capacity and demand for services (Atkinson et al. 2020).\nThis is a problem for ethical decision-making, as users may not inform the platform that they intend to\nuse two interventions jointly, hence joined effects are never simulated.\nThird, an end-to-end solution requires many sub-systems, including retrieval (to align a query with a\nrelevant model), query rewriting, and running a simulation multiple times. The computational cost may\nbecome a concern, particularly if there is a broad set of potential users. While the problem may be alleviated\nby queuing simulations and informing users when results are ready, this may reduce the quality of service\nand it only distributes computations over time instead of fully addressing their costs. Alternatives to\nreduce the processing costs and associated energy consumption (i.e., green computing) have been studied\nboth in the Information Retrieval community (Scells, Zhuang, and Zuccon 2022) and by the simulation\ncommunity (Feng and Staum 2017). For instance, the system may answer a related question that has\nalready been computed. A fourth related challenge is that LLMs may already be able to generate simulation\noutputs instead of running the models. Indeed, LLMs have been trained on enormous amounts of data,\nsome of which includes simulation models. For instance, GPT can answer questions about the behavior of\nmodels that are part of the standard library of NetLogo. It would be of particular interest to assess to what\nextent can LLMs complement or replace the outputs of a simulation model, for instance by checking for\npossible alignment between the responses of the LLM generated with and without the simulation model.\nFinally, the response of an LLM is a mixture of the underlying information derived from the simulated\noutputs and its own parametric knowledge. Evaluating this mixture is of particular interest. Is the response\ngrounded in the human/domain knowledge embedded in a validated simulation model, or is it produced\nby the knowledge embedded in the LLM (coming from various and sometimes unknown datasets)? Does"}]}