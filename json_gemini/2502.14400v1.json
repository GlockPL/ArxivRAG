{"title": "HPS: Hard Preference Sampling for Human Preference Alignment", "authors": ["Xiandong Zou", "Wanyu Lin", "Yuchen Li", "Pan Zhou"], "abstract": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes \"hard\u201d dispreferred responses\u2014those closely resembling preferred ones to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Anil et al., 2023; GLM et al., 2024) have demonstrated exceptional capabilities across diverse user applications by leveraging the extensive global knowledge and behavioral patterns embedded in their massive pretraining corpora. However, the presence of misleading, toxic, and harmful content in these corpora poses significant risks, as LLMs can inadvertently propagate undesirable information (Bai et al., 2022b; Yao et al., 2024). Consequently, selecting and aligning the model's responses and behaviors with desired human values is crucial to developing safe, effective, and controllable AI systems (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Dai et al., 2023).\nTo achieve this alignment, several human preference alignment methods have been proposed. For example, Reinforcement Learning from Human Feedback (RLHF) (Schulman et al., 2017; Christiano et al., 2017) optimizes LLMs by training a reward model on human preference rankings and maximizing the reward of generated outputs. Recognizing the complexity and sensitivity of RLHF, recent works, e.g., Direct Preference Optimization (DPO) (Rafailov et al., 2024), Identity Preference Optimization (IPO) (Azar et al., 2024) and Self-Play Preference Optimization (SPPO) (Wu et al., 2024), bypass the reward model by directly optimizing preferences, and have shown promising performance.\nDespite their successes, existing methods for preference alignment often rely on underlying ranking models, such as the Plackett-Luce (PL) model (Luce, 1959; Plackett, 1975) or its simplified counterpart, the Bradley-Terry (BT) model (Bradley & Terry, 1952). The PL model ranks multiple responses to a prompt to align with human preferences, while the BT model focuses on pairwise comparisons. These models enable the derivation of training losses for alignment tasks. However, both PL- and BT-induced losses exhibit critical shortcomings when handling harmful responses.\nFirstly, both PL- and BT-based losses fail to handle harmful responses effectively. The PL loss (e.g., DPO (Rafailov et al., 2024) and PRO (Song et al., 2024)) encourages ranking less harmful responses above more malicious ones, inadvertently treating harmful outputs as \"preferred\" alternatives. This compromises the model's ability to robustly reject inappropriate or offensive content\u2014essential in tasks requiring strict safeguards. The BT loss (e.g., DPO (Rafailov et al., 2024), R-DPO (Park et al., 2024), Online DPO (Dong et al., 2024), and KTO (Ethayarajh et al., 2024)) focuses only on rejecting the most dispreferred response in a pair, leaving other problematic responses unaddressed. Secondly, these losses overlook nuanced differences among dispreferred responses. The PL loss treats all dispreferred responses equally, ignoring their varying informativeness, which could"}, {"title": "HPS: Hard Preference Sampling for Human Preference Alignment", "content": "guide better alignment learning. Similarly, the BT loss reduces rankings to pairwise comparisons, discarding macro-level distinctions that are crucial for capturing nuanced preferences (Sun et al., 2024; Song et al., 2024). Finally, computational inefficiency poses a significant challenge. Training with the PL loss requires processing and backpropagating through all responses in a ranked set, leading to substantial memory and computational overhead especially for long prompts or responses (Oosterhuis, 2021; Maystre & Grossglauser, 2015; Sakhi et al., 2023). While the BT loss is more efficient, its simplifications sacrifice critical preference information. These limitations underscore the need for an improved preference alignment framework-one that robustly rejects harmful content, captures nuanced preferences, leverages the varying informativeness of responses, and achieves computational efficiency without compromising alignment quality.\nContributions. We address these limitations by introducing a provably effective and efficient Hard Preference Sampling framework(HPS) for human preference alignment. Our key contributions are highlighted below.\nFirstly, we introduce the HPS framework to enhance human preference alignment. Specifically, we first propose a training loss that fine-tunes LLMs to robustly prefer the most desired response while rejecting all dispreferred and potentially harmful ones. Moreover, HPS leverages insights from supervised, metric, and contrastive learning (Schroff et al., 2015; Oh Song et al., 2016; Robinson et al., 2020), emphasizing the importance of \u201chard\u201d examples-dispreferred responses closely resembling the preferred ones. Accordingly, HPS develops a hard preference sampling strategy to prioritize such hard examples, enabling the model to distinguish between preferred and highly similar dispreferred responses more effectively. To ensure efficiency, HPS is then reformulated into a sampling approach, using a single Monte Carlo sampling to select a single dispreferred response per training iteration. This innovation significantly reduces computational overhead compared to PL which requires all dispreferred responses for each prompt.\nSecondly, HPS provably improves sample complexity over the vanilla PL loss. For a dataset D with m prompts and n responses per prompt, the distance between the optimum of the PL loss and the optimal human preference policy is bounded by $O(\\frac{n}{m})$ which is improved to $O(\\frac{1}{m})$ by using our HPS loss. This improvement ensures better preference alignment with fewer training samples, making HPS particularly advantageous in data-limited scenarios or when faster convergence is required.\nThirdly, we further prove that optimizing the HPS loss maximizes the reward margin \u2013 the gap between the most preferred response and the closest dispreferred one \u2013 for any given prompt. A high reward margin means less dispre-"}, {"title": "HPS: Hard Preference Sampling for Human Preference Alignment", "content": "ferred or unethical generation. So this maximization ensures the LLM learns a robust distinction between preferred and dispreferred responses, leading to superior alignment with human preferences.\nFinally, experimental results demonstrate that HPS outperforms state-of-the-arts (SoTAs) in both fine-tuning and transfer learning settings. On the HH-RLHF dataset (Bai et al., 2022a), HPS achieves comparable BLEU and reward performance but improves the average reward margin by 89% over DPO, IPO and other preference alignment methods. A higher reward margin reflects fewer dispreferred or harmful generations. When transferring fine-tuned LLMs on HH-RLHF to the PKU-Safety dataset (Ji et al., 2024b), HPS maintains comparable BLEU and reward scores while achieving an average reward margin improvement of 83% over SoTAs, further highlighting its robustness and generalizability."}, {"title": "2. Related Work", "content": "Fine-tuning large language models (LLMs) to align with human preferences is a critical research challenge (Stiennon et al., 2020; Ouyang et al., 2022). This task requires models to learn from contexts and corresponding responses scored by human annotators to replicate human preferences.\nReinforcement Learning from Human Feedback (RLHF) is a common approach, where an agent iteratively refines itself using supervision signals from reward models acting as human proxies (Ziegler et al., 2019; Ouyang et al., 2022; Dai et al., 2023; Christiano et al., 2017; Stiennon et al., 2020; Zhu et al., 2023; Lee et al., 2021; Nakano et al., 2021; Snell et al., 2022). This cyclic process has led to continuous performance improvements, enabling LLMs like ChatGPT (Achiam et al., 2023; Dubey et al., 2024) to excel.\nHowever, RLHF's on-policy nature introduces challenges. It requires learning a reward model from data as a preliminary step, leading to a complex two-stage optimization process. Recent advancements in preference alignment techniques have sought to simplify this process by enabling direct alignment through a single loss function (Rafailov et al., 2024; Park et al., 2024; Azar et al., 2024; Wu et al., 2024; Meng et al., 2024; Ethayarajh et al., 2024; Ji et al., 2024a; Chen et al., 2024; Song et al., 2024). While these techniques streamline optimization, they face limitations such as poor handling of harmful content, inefficient utilization of dispreferred responses, and high computational costs.\nTo address these limitations, we propose HPS, a novel framework for robust and efficient human preference alignment. HPS prioritizes the most preferred response while explicitly rejecting dispreferred and harmful ones. By emphasizing \"hard\" dispreferred responses\u2014those closely resembling preferred ones\u2014it improves rejection capabilities. Additionally,"}, {"title": "3. Preliminaries", "content": "Alignment methods typically contain three phases below.\nSupervised Fine-Tuning (SFT). This phase fine-tunes a pretrained LLM on a labeled dataset, producing #SFT, a model that achieves a strong baseline.\nPreference Modeling (PM). This phase builds a model to evaluate text sequences and assign scalar rewards reflecting human preference. Given a prompt x, the supervised fine-tuned model #SFT generates n candidate responses {yi}=1\u00b7 A common approach involves human labelers ranking responses to produce an ordering \u03c4:\n$y_{\\tau(1)} > y_{\\tau(2)} > \\dots > y_{\\tau(n)},$ (1)\nwhere y > y' indicates y is preferred over y'. But ranking becomes challenging as n increases (Lambert et al., 2022).\nThis preference ranking can be modeled probabilistically. While the ideal reward function r* (x, y) is inaccessible, it is often estimated by models like Bradley-Terry (BT) (Bradley & Terry, 1952) or Plackett-Luce (PL) (Luce, 1959; Plackett, 1975). Under PL, the preference distribution is:\n$P_{PL} (y_{\\tau(1)} > y_{\\tau(2)} > \\dots > y_{\\tau(n)} | x) = \\prod_{j=1}^n \\frac{e^{r*(x, y_{\\tau(j)} )}}{\\sum_{k=j}^n e^{r*(x, y_{\\tau(k)} )}}$ (2)\nWhen n = 2, Eqn. (2) degenerates to the BT model.\nFinally, by sampling from the preference model, one can construct a prompt-response dataset D = {di} 1, where each instance d\u2081 = (xi, Yr\u2081(1), YT\u2081(2),\u2026\u2026, Yr\u2081(n)) contains one prompt xi and the ranked responses {Yri(k)}=1\nPreference Fine-Tuning (PFT). This phase further aligns the language model with human preferences using the dataset D, employing explicit or implicit reward methods.\nFor explicit methods, Reinforcement Learning from Human Feedback (RLHF) is widely used. RLHF trains a reward model re to learn response rankings in D, then fine-tunes LLM #SFT using policy-gradient algorithms like PPO (Schulman et al., 2017) to generate higher-preference responses. Refer to previous works (Ziegler et al., 2019; Ouyang et al., 2022) for further details.\nHowever, RLHF is often complex and hyperparameter-sensitive, limiting its usability. Implicit reward methods like DPO (Rafailov et al., 2024) offer a simpler alternative by directly parameterizing the reward function:\n$r_{\\theta}(x, y) = \\beta \\log \\frac{\\pi_{\\theta}(y | x)}{\\pi_{ref}(y | x)} + \\beta \\log Z(x),$ (3)"}, {"title": "4. Methodology", "content": "where \u03c0\u03b8 is the policy model, Tref is the reference policy, \u1e9e is a scaling factor, and Z(x) is the partition function. Additional implicit reward parametrizations are discussed in Appendix A, including KTO (Ethayarajh et al., 2024) and SimPo (Meng et al., 2024). The KTO reward is given by: r\u043a\u0442\u043e(x,y)=l(y) logo (yz) (y), where l(y) \u2208 R+ is a normalizing factor, and SimPo reward is defined as:\n$r_{SimPo}(x, y) = \\log \\pi_{\\theta}(y|x) = \\sum_{i=1}^{|y|} \\log \\pi_{\\theta}(y_i|x, y_{<i}),$\nwhere y is the length of the response y and y<i is the set of tokens in the sentence y before the token yi. By incorporating the reward into the PL model, one can derive the corresponding training loss:\n$\\mathcal{L}_{PL} = \\mathbb{E}_{d \\sim D} \\sum_{i=1}^{|\\mathcal{M}|} \\mathcal{L}_i(d),$ (4)\nwhere\n$\\mathcal{L}_j(d) = -log( \\frac{e^{r_{\\theta}(x, y_{\\tau(j)} )}}{\\sum_{k=j}^n e^{r_{\\theta}(x, y_{\\tau(k)} )}}).$ (5)\nHere, Lj (d) encourages predicting the preferred response Y\u012b(j) over more dispreferred ones {Y7(k)}k=j+1\u00b7 For n = 2, Eqn. (5) reduces to the BT loss. Moreover, when multiple dispreferred responses exist, BT selects the most and least preferred to construct loss. See Appendix A.1 for details.\nTo begin with, we define the task of interest in this work.\nTask Definition. This work tackles a critical challenge in AI development: ensuring models generate helpful and harmless responses while strictly avoiding harmful or dispreferred outputs. Formally, for a given prompt x from the training dataset D, as illustrated in Fig. 1, there exists a most preferred response Y7(1), which is both harmless and highly desirable. The prompt may also elicit a set of dispreferred responses {YT(i)}=2, such as Y+(2) and Y7(3), some of which may contain varying degrees of harmful content. The goal is to align the model to consistently generate harmless and preferred responses like Y\u30f6(1) while strictly avoiding dispreferred or potentially malicious ones like Yr(i) (i \u2265 2).\nThis task is critical for applications requiring high-quality and safe content generation. For example, in healthcare or e-commerce, LLMs handle complex queries where harmful outputs, such as biased or offensive language, can lead to dissatisfaction, reputational harm, or legal liability. Similarly, in educational platforms, harmful responses referencing violence, drugs, or other inappropriate topics could mislead students or expose them to dangerous ideas. In such scenarios, increasing the rejection rate of unethical or dispreferred responses while maintaining acceptance of helpful ones is essential for safety, reliability, and user trust.\nIn the following, we first analyze the PL alignment objective and discuss its limitations in addressing this task. Then we elaborate on our proposed novel and effective approach."}, {"title": "4.1. Motivation: Analysis of PL & BT Training Losses", "content": "The PL training loss LPL in Eqn. (4) consists of n sub-losses {Lj(d)}=1 defined in (5). Each sub-loss Lj(d) encourages the model to rank the j-th preferred response Y+(j) above a set of less preferred responses {Y+(k)}k=j+1, following the order Yr(j) \u2013 Yr(j+1) > \u2026 > Yr(n \u2013 1). While this recursive ranking objective explores relative preferences among dispreferred responses, it falls short in helping the LLM reject harmful dispreferred samples while suffering from high training costs.\nInadequacy for Rejecting Harmful Responses. Given a prompt x and its ranked responses {Y+(j)}=1, the first response Y7(1) is always the preferred and helpful output, while subsequent responses {Y7(j)}=2 are potentially harmful or purely dispreferred. Ideally, the training loss should prioritize producing response Y\u30f6(1) and strictly avoid generating any harmful outputs. However, the recursive nature of Lj (d) inadvertently encourages the model to rank potentially harmful responses Y\u30f6(j) as \u201cpreferred\u201d compared to even less preferred alternatives. This misalignment limits the model's ability to robustly reject potentially harmful content like Yr(j) (j > 2), making the PL objective insufficient for addressing tasks where the strict rejection of inappropriate outputs is paramount. The BT loss focuses only on rejecting the most dispreferred response in a pair, leaving other problematic responses unaddressed. Accordingly, the PL and BT losses inadequately address the real-world need to prohibit harmful and dispreferred responses, which is critical in many high-stakes applications as discussed earlier.\nIndiscriminate Handling of Dispreferred Responses. Given a prompt x and a set of response responses {Y7(j)}=1, the PL loss treats all dispreferred responses {Y7(j)}=2 equally as shown in the denominator in Eqn. (5) when training the model to prioritize the most preferred response without considering the inter-ranking relationship"}, {"title": "4.2. Hard Preference Sampling for Alignment", "content": "among dispreferred responses. This overlooks the varying degrees of informativeness among dispreferred responses, which could otherwise guide more effective alignment learning. The BT loss reduces rankings to pairwise comparisons, directly discarding other dispreferred responses let alone their macro-level distinctions that are crucial for capturing nuanced preferences (Sun et al., 2024; Song et al., 2024).\nTraining Inefficiency. For each prompt x, the PL loss LPL requires forwarding all n candidate responses {Y+(\u0456)}=1 through the model to compute their rewards, followed by constructing n sub-losses {Lj(d)}=1 for back-propagation. Considering the big size of LLM, this leads to high GPU memory and computational costs, especially when dealing with long prompts or long responses. Indeed, training costs even scale linearly with the number of response candidates n, further severe large-scale training scenarios where computational resources and efficiency are critical considerations. While the BT loss is more efficient, its simplifications sacrifice critical preference information.\nGiven the limitations of the PL and BT objective in rejecting harmful responses and its high training cost, it is imperative to explore alternative strategies for alignment: robustly preventing harmful content generation while reducing training overhead. Below, we offer a more practical and effective method for aligning LLMs with real-world requirements.\nTo solve the task of interests, we propose a hard preference sampling framework (HPS). The target of the task is to train the model to reject all dispreferred and potentially harmful responses {YT(i)}=2, ensuring it generates only the most preferred response Y7(1) for a given prompt x. To this end, for a training sample d = (x, Y\u315c(1), \u0423\u0442(2),\u2026\u2026, \u0423\u0442(n)) ~ D, HPS can use the training loss\n$\\mathcal{L}_e = \\mathbb{E}_{d \\sim D} [-log( \\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{\\sum_{i=2}^n e^{r_{\\theta}(x, y_{\\tau(i)} )}})].$ (6)\nwhere the model is encouraged to rank Y7(1) above all dispreferred and potentially harmful responses {Y+(i)}=2. We use the DPO implicit reward as mentioned in Eqn. (3) here.\nHowever, this loss treats all dispreferred responses {YT(i)}=2 equally, ignoring their varying levels of informativeness. Previous works in supervised, metric, and contrastive learning (Schroff et al., 2015; Oh Song et al., 2016; Robinson et al., 2020) demonstrate that \"hard\" examples-those closely resembling the correct output but still incorrect-are particularly useful for learning. In our context, hard dispreferred responses are those that are highly similar to Y7(1) yet dispreferred or harmful. Training the model to distinguish Y+(1) from the hardest dispreferred response Y7(2) enables it to reject less preferred responses {Y7(i)}=3 more effectively. Thus, harder dispreferred responses should be penalized more heavily during training."}, {"title": "Hard Preference Sampling Framework (HPS).", "content": "Our HPS builds a distribution over the dispreferred responses as\n$q(x, y) = \\frac{e^{\\gamma r_{est}(x, y)} \\cdot p(y)}{Z},$ (7)\nwhere r*(x, y) is the inaccessible optimal reward model defined in Sec. 3 and can provide the ground-truth rewards, p(y) is the probability density of dispreferred response y, and Z is the partition function for normalization. For each ranked response Yr(i), we can either directly access its reward rest if available in the dataset D or estimate it using a pretrained human preference-aligned reward model, rest(X, Yr(i)) \u2248 r*(x, Yr(i)). Without loss of generality, we first formulate the Eqn. (6) in the expectation form:\n$\\mathcal{L}_e = \\mathbb{E}_{d \\sim D} [-log( \\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{e^{r_{\\theta}(x, y_{\\tau(1)} )} + N \\cdot \\mathbb{E}_{y \\sim q(x, y)}[e^{r_{\\theta}(x, y)}]} )],$ (8)\nwhere N = n - 1. Using the Monte Carlo importance sampling technique, Eqn. (8) becomes:\n$\\mathcal{L}_e = \\mathbb{E}_{d \\sim D} [-log( \\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{e^{r_{\\theta}(x, y_{\\tau(1)} )} + N \\cdot \\mathbb{E}_{y \\sim p(y)}[e^{r_{\\theta}(x, y)} e^{\\gamma r_{est}(x, y)} / Z]} )].$\nNext, we can empirically estimate the distribution q(x, y):\n$q(x, y) = \\frac{e^{\\gamma r_{est}(x,y)}}{\\sum_{i=2}^n e^{\\gamma r_{est}(x,y)}}.$\nHere for flexibility, we introduce a hyperparameter \u03b3 > 1 to control penalty strength in q(x, y). Thus, the empirical training loss function becomes:\n$\\mathcal{L}_e = \\mathbb{E}_{d \\sim D} [-log( \\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{e^{r_{\\theta}(x, y_{\\tau(1)} )} + N \\cdot \\mathbb{E}_{y \\sim p(y)}[e^{r_{\\theta}(x, y)} q(x, y)]} )],$ (9)\nHere, harder dispreferred responses whose hardness is reflected by their bigger rewards rest(x, y) contribute more to the loss due to their higher weights q(x, y). For instance, larger y sharpens the distribution, emphasizing harder dispreferred responses and enabling the model to better distinguish closely-ranked preferred and dispreferred responses.\nReducing Training Costs with Flexible Sampling. Although this approach improves alignment, computing rewards for all n responses and backpropagating through them can be computationally expensive. To address this, we propose to sample only a single importance-weighted dispreferred response y ~ q(y) and incorporate it into the loss function Eqn. (9) for each prompt x in practice, which works very well as shown in Sec. 6 and also significantly reduces computational and memory overhead. By focusing more on the hard dispreferred responses, our method retains robust alignment while greatly improving training efficiency."}, {"title": "5. Theoretical Analysis", "content": "Here we first analyze the sample efficiency of our HPS approach and the PL method, and then theoretically justify how HPS can maximize the reward margin between the most preferred response and other hard dispreferred responses, ensuring less dispreferred or harmful generation."}, {"title": "5.1. Sample Complexity Analysis", "content": "To analyze the sample complexity of our HPS and the vanilla PL in Eqn. (5), assume 0* denotes the optimal human preference policy, i.e., the inaccessible reward model r* (x, y). Then given the training dataset D containing m training samples {di}_1={(xi, {Ytz(j)}}=1)}=1, define\n$\\Theta_{HPS} = arg \\underset{\\theta}{\\text{min}} L_e, \\ \\Theta_{PL} = arg \\underset{\\theta}{\\text{min}} L_{PL},$ (10)\nwhere Le and LPL respectively denote our HPS loss in Eqn. (9), and the PL loss in Eqn. (5). Then we pose necessary assumptions widely used in network and RLHF analysis (Zhu et al., 2023; Li et al., 2024; Ozay, 2019).\nAssumption 1. a) Assume re is bounded, Lipschitz and also smooth, i.e., |re(x,y)| \u2264 ao, ||\u2207re(x,y)||2 \u2264 \u03b11, ||\u2207\u00b2re(x, y)||2 \u2264 a2 with three constants 00, a1 and 02. b) Assume 0* \u2208 \u04e9\u0432, where \u04e8\u0432 = {0 \u2208 Rd | ||0||2 \u2264 B}.\nAssumption 1 a) and b) pose the boundness on reward function re and the optimum 0*. These boundness assumptions are often held empirically since after training network parameters are often bounded (Zhu et al., 2023).\nBased on these assumptions, we can derive the following sample complexity bounds. See its proof in Appendix B.1.\nTheorem 1. With Assumption 1, with probability at least 1-8, the distance between the optimum solution $\\Theta_{HPS}$ of our HPS loss and the ground-truth optimum 0* can be bounded:\n$|| \\Theta_{HPS} - \\Theta^* ||_{\\Sigma_D} \\leq \\Psi_1 = C_1 \\sqrt{\\frac{d+log (1/\\delta)}{m} + \\frac{1}{16\\alpha_0^2 \\zeta(\\mathcal{N})} - \\frac{4 \\alpha_2}{\\mathcal{M} \\zeta}}$\nwhere\n$\\Sigma_P = \\frac{2+exp(2a_0+ln(n-1))+exp(-2a_0)}{mn}$\n$=\\frac{16\\alpha_0^2 \\zeta(\\mathcal{N})}{\\mathcal{M}^2}$ where $\\zeta = 2+exp(2a_0+ln(n-1))+exp(-2a_0), \\ \\ \\ \\sum_{i=1}^{\\mathcal{M}} \\sum_{j=1}^{\\mathcal{M}} \\sum_{k=j+1}^{\\mathcal{N}} Z_{ijk}^T Z_{ijk}$ in which $Z_{ijk}$\n$=\\triangledown r_{\\theta} (x_i, y_{\\tau(j)}) - \\triangledown r_{\\theta} (x_i, y_{\\tau(k)})$. Similarly, the distance between the optimum solution $\\Theta_{PL}$ of PL loss and the ground-truth optimum 0* can be bounded:\n$|| \\Theta_{PL} - \\Theta^* ||_{\\Sigma_D} < \\Psi_2=C_2 \\sqrt{\\frac{n^4e^{8a_0} (d+log (1/\\delta))}{m}}$\nTheorem 1 shows the bounded distance between the optimum solution OHPS of our HPS loss and the ground-truth optimum * and indicates its good approximation.\nSimilarly, Theorem 1 also indicates the distance between the optimum solution OPL of PL loss and the ground-truth * is bounded by 2. Now we compare the optimums of our HPS and PL by comparing the bounded distances \u03a81 and 2 to the ground-truth 0*. \u03a8\u2081 represents an asymptotic error bound of $O(\\frac{1}{m})$, while 2 represents an asymptotic error bound of $O(\\frac{n^2}{m})$. This indicates that, given the same"}, {"title": "5.2. Reward Margin Analysis", "content": "amount of training data, our HPS achieves better preference alignment performance compared to PL. Specifically, the optimum solution OHPS derived from HPS loss is closer to the desired ground-truth 0* than the solution obtained from PL loss. This suggests that HPS improves sample efficiency, making it advantageous in scenarios with limited data or when faster convergence to the true parameter is desired.\nFor a model, we analyze its reward margin between a preferred response and the dispreferred responses under the same prompt. Intuitively, given a fixed reward for the preferred response, a larger reward margin means the lower generation ability of the dispreferred responses, aligning with the target of human preference alignment. For this analysis, we first define the min-max loss:\n$\\underset{\\theta}{inf}\\underset{p\\in\\Pi} sup  \\mathcal{L}_{\\rho}= \\mathbb{E}_{d \\sim D} -log (\\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{e^{r_{\\theta}(x, y_{\\tau(1)} )} +N \\cdot \\mathbb{E}_{y \\sim p} [e^{r_{\\theta}(x, y)}]};)$ (11)\nwhere II = {p(x) : supp (p(x,\u00b7)) \u2286 {y \u2208y:1 < \u0442\u00af\u00b9(y) \u2264 |\u0442|}. Here, I represents a family of probability distributions whose support is restricted to elements with ranks lower than that of the sample Y7(1) given the prompt x, where |T| denotes the number of ranking classes.\nTheorem 2. Let L\u2081\u2081 = supp\u2208n L\u2161. Then it holds the convergence: Lo\u2192 L\u2081\u2081 as y\u2192\u221e where Le is our HPS loss.\nTheorem 2 establishes that when y\u2192\u221e, then our HPS training loss Le in (6) would converge to L\u2081\u2081 which is the loss under the hardest dispreferred distribution. Since L\u2081 samples the hardest dispreferred responses, optimizing L\u2081 encourages the model to identify the preferred response and hardest dispreferred responses, which is the desired training. This is because as discussed before, the works in supervised, metric, and contrastive learning (Schroff et al., 2015; Oh Song et al., 2016; Robinson et al., 2020) demonstrate that \"hard\" examples-those closely resembling the correct output but still incorrect\u2014are particularly useful for learning. In our context, training the model to distinguish the preferred response from the hardest dispreferred response enables it to reject less preferred responses more effectively.\nFurthermore, to examine the global minimizer of our HPS training loss Le, we analyze the optima of the training loss L in Eqn. (11), since we have proved their good approximation in Theorem 2. Without loss of generality, when the number of dispreferred response samples N = 1 \u2192 \u221e, we can remove the log N from the HPS training loss Le as it does not change the minimizers and the geometry of the loss surface, and obtain a limiting objective:\n$\\mathcal{L}_0 = \\mathbb{E}_{d \\sim D} [-log (\\frac{e^{r_{\\theta}(x, y_{\\tau(1)} )}}{\\mathbb{E}_{y \\sim p} [e^{r_{\\theta}(x, y)}]})]$ (12)\nNow we are ready to give our results in Theorem 3 whose"}, {"title": "6. Experiments", "content": "proof is in Appendix B.2.2.\nTheorem 3. Assume the ranking set \u315c is a finite set. Let Lo* = Suppen L\u1ef9 and 0* = arg mine Lo*. Then 0* is also the solution to the following problem:\n$\\theta^* = \\underset{\\theta}{arg \\underset{x}{max}} (r_{\\theta} (x, y_{\\tau(1)}) - \\underset{1<j<|\\tau|}{max} r_{\\theta} (x, y_{\\tau(j)}))$ (13)\nTheorem 3 implies that the minimizer 0* = arg min\u0259 Lo is equivalent to the one that maximizes the margin between the reward of the most preferred response, represented by r\u0259 (x, Y(1)), and the reward of the hardest dispreferred responses, represented by max1<j<|-| r\u04e9 (x, Y+(j)). So the optimum * of our HPS loss aims to maximize the reward margin between the preferred response and its closest dispreferred response. This guarantees that the model learns a robust distinction between preferred and dispreferred responses, and enjoys a better alignment performance with much less dispreferred or harmful generation.\nBaselines. In experiments, we use a supervised fine-tuned LLaMA3-8B checkpoint (Dong et al., 2024; Dubey et al., 2024) as both baseline SFT and reference model. We test the SFT model and integrate our HPS into several preference optimization methods, including DPO (Rafailov et al., 2024) (BT&PL), EXO (Ji et al., 2024a) (BT&PL), IPO (Azar et al., 2024), SPPO (Wu et al., 2024), and NCA (Chen et al., 2024).\nDatasets. We use two popular datasets, HH-RLHF (Bai et al., 2022a) and PKU-SafeRLHF (Ji et al., 2024b), focusing on helpfulness and safety (Lambert et al., 2024; Fourrier et al., 2024). HH-RLHF is multi-turn, while PKU-SafeRLHF contains single question-answer pairs. Each prompt in datasets includes two responses with human-rated preferences. To test different methods, for each prompt, we adopt LLaMA3 (Dong et al., 2024) to generate 100 responses, and generate their rankings and rewards using a top-10 safety-ranked reward model (Liu et al., 2024) from the RewardBench benchmark (Lambert et al., 2024).\nEvaluation Metrics. We evaluate response quality and harmful content rejection. We use BLEU (Papineni et al., 2002) to assess the text quality by comparing responses to ground-truth preferred answers and adopt a strong reward model (Wang et al., 2024) to measure the level of human preference gained."}]}