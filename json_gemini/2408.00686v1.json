{"title": "Can Developers Prompt? A Controlled Experiment for Code Documentation Generation", "authors": ["Hans-Alexander Kruse", "Tim Puhlf\u00fcr\u00df", "Walid Maalej"], "abstract": "Large language models (LLMs) bear great potential for automating tedious development tasks such as creating and maintaining code documentation. However, it is unclear to what extent developers can effectively prompt LLMs to create concise and useful documentation. We report on a controlled experiment with 20 professionals and 30 computer science students tasked with code documentation generation for two Python functions. The experimental group freely entered ad-hoc prompts in a ChatGPT-like extension of Visual Studio Code, while the control group executed a predefined few-shot prompt. Our results reveal that professionals and students were unaware of or unable to apply prompt engineering techniques. Especially students perceived the documentation produced from ad-hoc prompts as significantly less readable, less concise, and less helpful than documentation from prepared prompts. Some professionals produced higher quality documentation by just including the keyword Docstring in their ad-hoc prompts. While students desired more support in formulating prompts, professionals appreciated the flexibility of ad-hoc prompting. Participants in both groups rarely assessed the output as perfect. Instead, they understood the tools as support to iteratively refine the documentation. Further research is needed to understand which prompting skills and preferences developers have and which support they need for certain tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Developers often overlook or ignore software documenta-tion and generally assign it a low priority [1]. Yet, carefully documenting code is an essential task in software engineering. Up-to-date and high-quality documentation facilitates program comprehension [2], accelerates developer onboarding [3], [4], and mitigates technical debt [5]. Software documentation is also central to software maintenance, as documentation often requires updates and should evolve with software [6].\nResearchers and tool vendors have thus investigated differ-ent ways to automate documentation [7]. In particular, LLMs designed to model and generate human language [8] hold great potential in automating documentation tasks [9]. Among popular LLM use cases, code summarization has recently gained much attention [10]. However, the output of LLMs usually depends on the user input, called prompt. Slightly changing the prompt can lead to different results regarding conciseness, language style, and content of generated text [11].\nSeveral prompt engineering techniques have emerged to optimize interactions with LLMs [12]\u2013[15]. Few-shot prompt-ing is one of the most prominent techniques and comprises adding example outputs to a predefined prompt to specify output requirements [16], [17]. This technique allows users to optimize LLM responses while minimizing the number of input messages sent to the model [10], [14].\nPrevious research has primarily focused on benchmarking the performance of various models and prompt engineering techniques for generating source code documentation [7]. Recently, Ahmed and Devanbu concluded that common prompt engineering techniques perform better than ad-hoc prompting according to popular benchmark metrics [18]. However, studies focusing on the perspectives of developers when using LLMs, e.g., to generate documentation, are still sparse. In fact, a recent study showed that common metrics to evaluate LLMs do not align with human evaluation to assess the quality of generated documentation [19]. Hence, it remains uncertain whether prompt engineering techniques meet developers' requirements for generating and using code documentation. Moreover, it is unclear whether developers prefer a flexible, iterative interaction using chatbot-like ad-hoc prompting or a rather transaction-like execution of predefined prompts.\nThis study takes a first step towards filling this gap, focusing on two research questions:\nRQ1: How well can developers prompt LLMs to generate code documentation compared to a predefined prompt?\nRQ2: How is the developer experience for ad-hoc prompting compared to executing predefined prompts?\nTo answer these questions, we conducted a randomized controlled experiment with 20 professional developers and 30 computer science students. The experiment involved gener-ating code documentation using an LLM-powered integrated development environment (IDE). The first group utilized a Visual Studio Code (VS Code) extension based on GPT-4, enabling participants to enter ad-hoc prompts for selected code. The second group used a similar VS Code extension that executed a predefined few-shot prompt to generate code documentation via GPT-4 with a single click.\nOverall, we observed that developers with less experience require more guidance in phrasing an ad-hoc prompt, whereas more experienced developers can generate fairly good doc-umentation by including specific keywords like \"Docstring\" in the prompts. Overall, participants preferred transactional or guided LLM interactions to create code documentation while enjoying the flexibility of ad-hoc and iterative prompting. Our results show that predefined prompts deliver code documen-"}, {"title": "II. METHODOLOGY", "content": "A. Experimental Design\nWe designed a controlled experiment in a laboratory setting to compare developer usage of and experience with two VS Code extensions employing different prompts for code documentation generation. We followed a between-subject design, with one participant testing only one of both tools to avoid learning effects [21]. The ad-hoc prompt group created prompts in an already available GPT-4-powered IDE exten-sion, while the predefined prompt group interacted with our extension. We randomly assigned participants to the groups, with adjustments made to balance the occupations. Participants were unaware of their group.\nWe offered offline and online versions of the experiment. In the offline version, participants used our computer with VS Code and the digital questionnaire displayed on two screens. In the online version, we utilized Zoom for video conferencing, sharing a questionnaire hyperlink, and screen-sharing our VS Code window with external input control. Participants employed Thinking Aloud to express thoughts during the task [22]. To ensure a calm environment, we put a participant in a room empty of people. The questionnaire included questions about the code and the generated documentation. Additionally, we collected the ad-hoc prompts for a later comparison with the predefined prompt.\nThe experiment comprised three parts, as shown in Figure 1. In the first part, participants answered questions about occupation, as well as Python and VS Code experience to assess their ability to understand complex functions and use the IDE. Furthermore, we could check whether these variables have any effect on the observed behavior.\nThe second part focused on code comprehension and documentation generation. It consisted of two rounds, each involving a pre-selected Python function (Listing 1). Per round, participants (1) first attempted to comprehend the code without any documentation. This enabled us to analyze the effect of the LLM-generated documentation on understanding the code functions. Subsequently, (2) participants rated their comprehension on a scale from 1 (very low) to 5 (very high), and answered True/False questions about the code to further check their understanding (Table III). Participants could also state that they had insufficient information to answer a ques-tion. Furthermore, (3) participants manually created a function comment to externalize their understanding and better assess the quality of the comment subsequently generated by the tool. To guide the participants and shorten the completion time, we provided the participants with a comment template adhering to guidelines for clean Python code [23] and instructed them to add only the function description without the explanations of parameters. Participants then used the respective LLM-powered IDE extension to generate code documentation. Afterward, (4) as documentation should serve to facilitate code comprehension, they studied the generated comment and revis-ited the True/False questions. This step provided insights if the documentation supported or even hindered the comprehension. Finally, (5) they rated the generated documentation on six quality dimensions based on the human evaluation metrics by Hu et al. [19].\nThe documentation quality dimensions each provided a question and a five-point answer scale, addressing gram-matical correctness, readability, missing relevant information, unnecessary information presence, usefulness for developers, and helpfulness for code comprehension. We made minor adjustments to the original answers of two dimensions as we found them overloaded for our goal. In particular, the answers for grammatical correctness originally also included a rating of fluency, and the original readability answers also concerned the comment's understandability. The lowest point of each scale represents the negative pole, like \"very low readability\", while the highest point represents the positive pole, like \u201cvery low amount of unnecessary information\u201d.\nIn the third part of the experiment, participants assessed the usability of the respective IDE extension. They completed the standardized User Experience Questionnaire (UEQ) with 26 items, each associated with a seven-point answer scale and one of six usability categories [24]. Finally, participants provided comments on tool strengths and areas for improvement in two free-text fields."}, {"title": "B. Ethical Considerations", "content": "We followed the standard procedure of the ethics committee of our department for assessing and mitigating risks regarding ethics and data processing. Upon welcoming participants, we explained the privacy policy, study purpose, upcoming tasks, and data processing procedures. We designed the study to last for approx. 30 minutes per participant to prevent exhaustion and instructed participants not to seek perfect solutions to reduce stress. We offered clarifications on tasks and questionnaire content without providing solutions. We minimized observational bias by facing away while participants answered the questionnaire and also encouraged negative feedback [25]. To express our gratitude for the participation, we raffled vouchers among participants. We pseudonymized all published data as far as possible while maintaining our study objective."}, {"title": "C. Technical Setup", "content": "1) Selection of Experiment Tasks: We selected the Python source code for the experiment tasks from the open-source control system Karabo [26]. Karabo is maintained by the scientific facility European XFEL (EuXFEL), whose devel-opers partially participated in the experiment. By choosing this project and involving project insiders, we could assess how their programming and domain experience supports their code comprehension and documentation rating, especially in comparison to outsiders.\nTwo authors conducted a thorough screening of available Karabo functions and identified six candidates. All six were utility functions, which we deemed comprehensible for exper-iment participants not associated with EuXFEL. Our selection criteria also included code documentation quality aspects such as conciseness, completeness, and usefulness especially relevant for the few-shot examples [27]. We independently analyzed the candidates and categorized them into the com-plexity levels easy, medium, and hard. Afterward, we resolved categorization conflicts by discussion.\nWe selected one easy-, and one medium-rated function for the tasks of the experiment (Listing 1). The first function converts a boolean vector into a comma-separated string.\nWe considered the code easy to understand as only standard Python operations were used within this short function. The second function converts a given date to the ISO format. The complexity of this function lies in the usage of multiple classes and functions with abbreviated identifiers, and the lack of inline comments. Hence, participants must make assumptions based on identifier names and usage.\nInitially, we also selected a third function rated hard but removed it after a pilot study with three software engineering researchers, who reported difficulties with this function and far exceeded the time limit. While we acknowledge that LLMs might require more sophisticated prompts and context information to generate appropriate documentation for more complex code, our user study focused on how differently skilled developers can and prefer to interact with an LLM for the creative process of documentation generation. Therefore, within the experiment's time limit, all participants should be able to phrase ad-hoc prompts that generate appropriate comments for these functions.\n2) LLM Selection and Prompt Construction: Research and industry have introduced multiple LLMs with individual strengths and limitations. For our study, we focused solely on the GPT-4 model by OpenAI (version gpt-4-0613). We did not aim to benchmark different models but rather to study de-velopers prompting skills particularly for code documentation generation. OpenAI's models have been prominent in recent research on documentation generation [28]. Furthermore, dur-"}, {"title": "3) Tool of the Ad-hoc Prompt Group", "content": "We focused on IDE extensions as developers usually use IDEs for generating code documentation. We chose VS Code for its extensive cus-tomization capabilities through various extensions maintained by an open-source community. During our study, the exten-sion marketplace already offered multiple ChatGPT-like tools that we examined. Additionally, VS Code supports multiple programming languages, including Python.\nWe selected the ad-hoc prompt tool based on two criteria. First, it needed to provide a user interface similar to the ChatGPT interface, allowing users to engage in a conversa-tion with the software. This approach involves at least one self-written message to the model and copying and pasting the generated comment from the chat interface to the code. This criterion was crucial as we aimed to compare the user experience for this classic Q&A-based interaction with the one for the predefined prompt tool, which generates and inserts code documentation with a button click. Second, the tool had to support and behave like the original GPT-4 application programing interface (API) to handle any confounding factors related to the use of different LLM models.\nAfter a thorough testing of available extensions, we opted for ChatGPT by zhang-renyang [30] in version 1.6.62. This extension met all specified requirements. It offers a small prompt field (Figure 2a) and a larger ChatGPT-like chat window within the IDE. Our test showed that it mimics GPT-4. It was one of the most popular GPT extensions in the marketplace during our study period."}, {"title": "4) Tool of the Predefined Prompt Group", "content": "Our design objec-tive for the predefined prompt group was to minimize the steps required for developers to generate a high-quality Docstring comment for a specific code section. We determined that mark-ing the code within the IDE's code editor and selecting the \"Generate a comment\u201d action from the context menu should suffice, requiring three clicks to generate documentation (Figure 2b). Alternatively, the developer can choose this action through VS Code's command field. Hence, inexperienced and experienced VS Code users can use the primary feature of the tool with their preferred workflow. A secondary feature allows developers to customize the predefined prompt via the \u201cEdit default prompt\" action, enabling them to tailor the tool output to their preferred format.\nWe followed the VS Code guidelines to create the Type-Script-based extension [31] and utilized the axios package to implement calls to the OpenAI API. Based on the input prompt, the API returns a code comment generated by GPT-4. The extension automatically inserts this documentation above the previously selected code. To set up the extension, devel-opers must enter their OpenAI API token into a configuration field. This token is stored locally on the developer's machine. For the experiment, we pre-configured the tool with our token. We used GPT-4 version gpt-4-0613 for both tools. Gener-ating code documentation with this model took, on average, four seconds. The VS Code version was 1.85. The predefined prompt tool is included in our replication package [20] and also available in the VS Code marketplace [32]."}, {"title": "D. Experiment Execution and Participants", "content": "The experiment took place in October and November 2023, involving 50 participants: 25 per ad-hoc and predefined prompt group. 20 participants were professionals, including 12 soft-ware engineers, four data scientists, and four with other soft-ware development roles. 17 worked in the domains of scientific computing and three in finance. These participants took the expert perspective due to their professional experience with Python programming and, eventually, the application domain. The remaining 30 participants were students in computer sci-ence or related subjects, taking the newcomer perspective. We recruited participants through personal contacts in academia and industry to ensure that the participants had at least some experience with Python and VS Code. We conducted 33 experiments online and 17 offline at the EuXFEL campus in Schenefeld, Germany, with the first author leading the experiment and the second author assisting.\nOn a scale from 1 (very low) to 5 (very high), the mean value and standard deviation of the Python experience among professionals were 3.7 (1.06) in the ad-hoc and 4.5 (0.71) in the predefined prompt group. The values among students were notably lower, with 2.53 (0.83) in the ad-hoc and 2.47 (1.19) in the predefined prompt group. This difference confirms the expert/newcomer setting.\nAll participants were familiar with VS Code, which was important to mitigate biases due to the tool setup. Mean values and standard deviations in the ad-hoc/predefined prompt group among professionals were 2.9 (0.99) and 3.5 (1.18); among students they were 3.07 (1.03) and 3.07 (1.28).\nAt the beginning of each session, we trained each participant in using the respective tool to mitigate learning bias. We introduced them to the tool features, generated documentation for an example function, and answered tool-related questions. Afterward, we started the experiment. Due to manually writing prompts, ad-hoc prompt group participants completed the experiment after 23:53 minutes on average, while predefined prompt group participants required only 19:58 minutes."}, {"title": "E. Data Analysis", "content": "Our experiment comprised quantitative and qualitative data analyses. We report totals, mean values, and standard devia-tions where applicable. Furthermore, we employed inferential statistics to test for significant differences between the groups' ratings of the six documentation quality dimensions. Depend-ing on the statistical assumptions fulfilled by the respective variables, we utilized the parametric Welch's t test or the non-parametric Mann-Whitney U (MWU) test [33], [34].\nThe assumptions for both tests were that the collected data points were quantitative and independent. The first assumption was fulfilled as we provided a five-point answer scale for each question related to a quality dimension. The second assumption was fulfilled as we conducted the experiment with each participant individually, and we asked participants to not share information with others to avoid external influences. The third assumption of the Welch's t test is a normal distribution of data points, which we tested with the Shapiro-Wilk test [35]. If a normal distribution was not met for a specific quality dimension, we applied the MWU test instead. Besides reporting the p-values for statistical significance, we also report the effect sizes to indicate practical significance. We particularly applied Cliff's $\\delta$ [36], which describes the overlap of two groups of ordinal numbers. Values between 0.0 and 0.8 indicate a low to medium effect size, whereas values from 0.8 represent a large effect size, meaning a large difference between both groups.\nTo quantify the experience of developers using the ex-periment tools, we utilized the 26 seven-point scales of the UEQ [24] and its official data analysis tool [37]. This tool provides descriptive (mean values and standard deviations) and inferential statistics (t test) to compare user experience ratings for the two studied IDE extensions.\nEach of the 26 scales is related to one of six quality categories. The first category, Attractiveness, represents the pure valence of and overall satisfaction with a tool. Efficiency, Perspicuity, and Dependability are the pragmatic, goal-directed categories, and Stimulation and Novelty are hedonistic, not goal-directed categories. We chose Efficiency, Perspicuity, Dependability, and Novelty to measure tool-related experience, as these categories focus on tool interactions.\nTo evaluate participants' assessment of the quality of gen-erated documentation, we used the Stimulation category as it focuses on the output of a tool. We enhanced this analysis with qualitative insights made by manually analyzing participants' free-text answers to the questions \u201cWhat did you like most about the tool?\" and \"What would you change about the tool?\", placed at the end of the questionnaire. Two authors independently conducted Open Coding for all answers and discussed their codes to achieve consensus [38]. Furthermore, we report relevant remarks expressed by participants during the experiment.\""}, {"title": "III. PROMPTING FOR CODE DOCUMENTATION (RQ1)", "content": "A. Patterns in Ad-hoc Prompts\nThe prompts entered by the ad-hoc prompt group varied in detail and keywords. About half of the participants had to re-enter a different prompt after their first attempt to generate a code comment. Among the failed attempts were prompts such as \"Explain the function\" and \"Describe what this function does\", which summarized the code in longer prose text.\nWe categorized the 50 prompts that led to a successful generation of code comments for both code functions into five prompt patterns, listed in Table I. These patterns commonly in-cluded documentation-specific keywords, i.e., comment (34x), Docstring (12), and Python-conform description (1). Also, slightly longer prompts that included the comprehension-focused terms explain (2) and summarize (1) led to appropriate function comments. Professionals used the Docstring term more often (9x) than students (3x), which is in line with the higher Python experience of most participating professionals.\nSuccessful prompts were generally short, with a mean length and standard deviation of 7.1 (2.09) words among profession-als and 6.7 (2.62) words among students. The shortest prompt was \u201cGenerate docstring\u201d. None of the participants enhanced their prompts with examples or templates.\nB. Perceived Quality of Generated Documentation\nParticipants rated the documentation generated by the ad-hoc and predefined prompt tools based on six five-point-scale questions. All participants were aware of the expected format of a Python Docstring, as they were familiar with Python. Moreover, before using the tool, they manually created a func-tion comment based on a Docstring template. Table II displays the ratings per group, subgroup (student vs. professional), code function, and quality dimension, along with the p-values [33], [34] and effect sizes [36].\nWe performed several trials to determine the appropriate statistical test (see section II-E for details). We rejected the null hypothesis of the Shapiro-Wilk test for all groups and dimensions, indicating a non-normal data distribution, likely due to small sample sizes. We chose the MWU test for all com-parisons due to its suitability for non-normally distributed data [34]. The null hypothesis of this test indicates no statistically significant difference between the mean values of both groups for a specific dimension. We rejected this null hypothesis if the p-value was lower than 0.05. We noted that in all cases when the null hypothesis was rejected, the effect size $\\delta$ was large (0.8 or higher), indicating practical significance [36].\nComparing all participants in the ad-hoc and predefined prompt groups, we observed several statistically significant differences. For function 1, we found significant differences in the dimensions of Readability (4.12/4.72) and Unnecessary Information (3.28/4.32). The predefined prompt tool consis-tently provided concise Docstrings that participants perceived as more readable and contained less non-informative content compared to the output of the ad-hoc prompt tool. For function 2, significant differences were found in four dimensions, with the predefined prompt tool consistently receiving higher ratings than the ad-hoc prompt tool. The differences in Read-ability (3.52/4.68) and Unnecessary Information (2.84/4.68) reinforced the conclusions drawn from function 1.\nWe attribute the ratings of Usefulness (3.52/4.6) to the often excessive output length of the ad-hoc prompt tool. Participants noted that longer, prose documentation could impede development workflows, as these comments require more time to read while providing little additional information. Helpfulness was rated high in both groups (4.0/4.6), with some participants suggesting that the explanations provided by both tools were either too complicated or lacked the required level of detail. Across both groups, participants rated Grammatical Correctness and Missing Information very high.\nWhen we compared documentation ratings between students and professionals, we observed similar statistically significant differences in the ad-hoc and predefined prompt groups for students, while professionals did not exhibit significant differ-ences. Among students, we observed significant differences for function 1 in the dimensions Readability (4.0/4.66), Unnec-essary Information (3.33/4.86), and Usefulness (3.93/4.6). The different expectations regarding the documentation content contributed to the disparity in Usefulness, with some students preferring the longer summarizations by the ad-hoc prompt tool, while most others expected a structured comment as taught in university."}, {"title": "C. Code Comprehension", "content": "We analyzed how the LLM-generated documentation influ-enced the comprehension of the code. Initially, participants rated their perceived comprehension of the respective code function without available documentation, on a scale from 1 (\"not at all\") to 5 (\"very well\"). For function 1, the mean and standard deviation for these ratings were 3.6 (0.96) by the ad-hoc and 3.84 (1.11) by the predefined prompt group, indicating a similar good understanding. The assessment for function 2 was lower, with 2.4 (0.96) by the ad-hoc and 2.92 (0.81) by the predefined prompt group. Hence, the predefined prompt group expressed slightly higher confidence in the comprehension than the ad-hoc prompt group, despite similar stated Python experience. Across both groups and functions, professionals stated a higher comprehension than students, which aligns with their higher Python experience.\nWe assessed participants' actual comprehension through True/False questions about the code (Table III). We counted the number of participants who correctly answered these ques-tions before and after generating the documentation. Partici-pants could also choose the option \"Not enough information available to answer the question\", which we treated as an incorrect answer.\nBefore using the tools, most participants in both groups an-swered the two questions for function 1 correctly (Q1: 19+23; Q2: 23+24), as expected for this short and easy-categorized function. After using the tool, the correct answers for question 1 increased (Q1: 22+24), especially among students, while the ones for question 2 slightly decreased (Q2: 21+20) for both students and professionals. Thinking-aloud observations revealed that many participants were confused by the less technical documentation generated by both tools regarding this question, as it did not include the term iterable.\nThe number of correct answers to function 2 before using the tool was lower (Q1: 10+14; Q2: 15+13), reflecting its higher complexity. In both groups, correct answers increased after using the tools (Q1: 22+18; Q2: 21+21). Students had the highest increases: in the ad-hoc prompt group for Q1 from 5 to 13, and in the predefined prompt group for Q2 from 5 to 14. The increase in comprehension among professionals was marginal, except for the correct Q1 answers in the ad-hoc prompt group, which increased from 5 to 9. This data shows that the availability of documentation helped inexperienced developers to better comprehend the code, while the effect on professionals was mixed."}, {"title": "Answering RQ1", "content": "we conclude that developers were able to generate code documentation using ad-hoc prompts without prompt engineering techniques, often by providing relevant documentation keywords. How-ever, particularly less experienced students initially did not use such keywords and generated lengthy, less readable, and less useful code explanations instead. This is reflected in participants rating the quality of generated documentation. We observed statistically significant differences exclusively among students, who rated the readability, conciseness, usefulness, and helpfulness of comments generated via the predefined few-shot prompt higher than those created via ad-hoc prompts. Although we observed large effect sizes for some quality dimensions among professionals, this subgroup was generally less critical as they often generated documentation of the required quality and format by using specific keywords.\nWe also conclude that documentation generated by both tools helped (particularly students) comprehend more complex code. The generated documentation could also lead to confusion, even for professionals."}, {"title": "IV. DEVELOPER EXPERIENCES WITH PROMPTING (RQ2)", "content": "A. Overall Experience\nParticipants rated both tools positively in all six categories of the standardized UEQ [24], with values ranging from -3 to +3 (Table IV). However, students found the predefined prompt tool more positive in all user experience categories than the ad-hoc prompt tool, with statistically significant differences and large effect sizes in five categories. Professionals also rated the predefined prompt tool more positively in five categories, without significant differences but a large effect size in one category. For the pure valence category Attractiveness (1.01 ad-hoc / 1.99 predefined prompt group), the predefined prompt tool received higher ratings than the ad-hoc prompt tool, with a high difference among students (0.64/1.94). This indicates an overall better developer experience with the predefined prompt tool.\nB. Tool-Related Experience\nThe most significant difference between the groups was in the category Efficiency (0.7/1.98), with the predefined prompt tool receiving higher ratings, especially among stu-dents (0.42/2.12). Participants praised the ad-hoc prompt tool for its practicality of not having to open ChatGPT in the browser (two students, one professional), flexibility in select-ing relevant code (one professional), and custom prompts (one professional). They noted issues with copy-pasting the tool output to the code panel (six students, four professionals), multiple tries sometimes required for sufficient results (one student), the need to phrase a prompt (one professional), miss-ing keyboard shortcuts (one professional), and the slow output generation, which is primarily caused by the performance of GPT-4 (one student). Participants who interacted with the predefined prompt tool also noted the slow response (one student, two professionals), but expressed no further positive or negative comments.\nThe ratings for Perspicuity (1.25/1.87) were higher for the predefined prompt tool, with large effect sizes for students and professionals. While some participants of the ad-hoc prompt group participants found their tool easy to use (four students, one professional), others reported that they would find it easier to enter the prompt in a separate IDE panel instead of the pop-up at the top of the screen (one professional) and that the right-click context menu, which listed the tool features besides non-related IDE features, was obscure to them (one professional). Multiple predefined prompt group partic-ipants also found their tool easy to use (eight students, four professionals), highlighting the ease of generating comments without the need for further prompt input besides the code and a short command (one professional). Nevertheless, they assessed that the selection of relevant code could be facilitated (one professional). Overall, this assessment aligns with our observation that participants quickly understood both IDE extensions but tended to prefer the simplicity of the predefined prompt tool without having to create an effective prompt.\nRatings for Dependability (1.05/1.84) showed strongly di-verging views among students (0.85/1.95). Participants praised the ad-hoc prompt tool for supporting the comment creation workflow (six students, two professionals) and code compre-hension (three students, four professionals). They criticized the lack of prompt templates (three students), that the tool did not always generate code comments (two students, one profes-sional), and the missing opportunity for iterative improvements caused by the limitations of the OpenAI API at the time of our study (one student, one professional). The predefined prompt tool was also noted for facilitating the workflow (three students, one professional) and code comprehension (four students), as well as for its reliability (two profession-als). However, participants suggested adding a progress bar (one professional), previewing generated comments within the function (one professional), enabling the configuration of the required comment content (one student), adding a disclaimer that the output might be incorrect (one student), implementing a linter that alerts when generated comments became outdated (one professional), and using a locally deployed API for data security (one professional). Besides these improvement comments, the predefined prompt group was overall satis-fied regarding the Dependability aspects, whereas the ad-hoc prompt group, and especially the students, required more enhanced tool support.\nBoth tools achieved similar positive results concerning Novelty (1.1/1.18), without significant differences. Interest-ingly, the ad-hoc prompt tool received a higher rating from professionals (1.38/1.18) for its chat-like interaction within an IDE, which they considered more novel than clicking a button for documentation generation. Participants expressed no further comments regarding this UEQ category."}, {"title": "C. Documentation-Related Experience", "content": "For Stimulation (0.82/1.62), predefined prompt tool ratings were higher, especially among students (0.6/1.6). Concerning the output of the ad-hoc prompt tool, participants noted the detailed explanations (two students, two professionals), the mitigation of personal biases (one student), inspirational value (one professional), and overall high quality (three profession-als). Other participants criticized the output for containing un-necessary information (five students, three professionals), lack of detail regarding complex code (three students), and incon-sistent format due to using prompts of varying qualities (two students, one professional). The predefined prompt tool output received positive ratings for its conciseness (four students, one professional), overall high quality (three professionals), and similar structure across all generated comments (one student, one professional). Improvements were deemed necessary for the parameter descriptions (one student, one professional), line breaks (one professional), consistent punctuation (one student), and lack of detail for complex code (one student, one professional). These insights show that students were especially unsatisfied with the ad-hoc prompt tool output, while professionals were overall satisfied with both tools."}, {"title": "Answering RQ2", "content": "we conclude that the participants overall preferred a tool that automates code docu-mentation generation with a few clicks while offering options to configure this process. On average, pro-fessionals and students preferred executing predefined prompts over creating ad-hoc prompts. The simplicity and efficiency of a single button click to receive consistently high-quality documentation were key fac-tors for this assessment. While the predefined prompt group praised their tool, some missed the flexibility to adjust the documentation depth. Users of the ad-hoc prompt tool appreciated this flexibility, which often unintentionally resulted in longer and more explanatory comments that may assist code comprehension."}, {"title": "V. DISCUSSION", "content": "We summarize the results of our experiment and discuss potential implications for research and practice.\nA. Developers are not Necessarily Prompting Experts\nOur study indicates that developers are generally neither skilled (RQ1) nor inclined (RQ2) to effectively prompt an LLM in a way that it generates concise, readable code doc-umentation. This partly aligns with previous studies showing that optimized, predefined prompts outperform ad-hoc prompts [18", "39": ".", "40": "."}, {"40": [42]}, {"40": "and knowledge needs of code users. This research can help vendors of artificial intelligence (AI)-powered code documentation tools align the generated com-ments with the requirements of their users. Thus, vendors can focus on applying and optimizing predefined prompts, which participants in our experiment preferred, while also offering additional iterations to customize the output based on preferences"}]}