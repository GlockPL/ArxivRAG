{"title": "Improving Value-based Process Verifier via Structural Prior Injection", "authors": ["Zetian sun", "Dongfang Li", "Baotian Hu", "Jun Yu", "Min Zhang"], "abstract": "In the Large Language Model(LLM) reasoning scenario, people often estimate state value via Monte Carlo sampling. Though Monte Carlo estimation is an elegant method with less inductive bias, noise and errors are inevitably introduced due to the limited sampling. To handle the problem, we inject the structural prior into the value representation and transfer the scalar value into the expectation of a pre-defined categorical distribution, representing the noise and errors from a distribution perspective. Specifically, by treating the result of Monte Carlo sampling as a single sample from the prior ground-truth Binomial distribution, we quantify the sampling error as the mismatch between posterior estimated distribution and ground-truth distribution, which is thus optimized via distribution selection optimization. We test the performance of value-based process verifiers on Best-of-N task and Beam search task. Compared with the scalar value representation, we show that reasonable structural prior injection induced by different objective functions or optimization methods can improve the performance of value-based process verifiers for about 1~2 points at little-to-no cost. We also show that under different structural prior, the verifiers' performances vary greatly despite having the same optimal solution, indicating the importance of reasonable structural prior injection.", "sections": [{"title": "1. Introduction", "content": "The process verifier plays a crucial role in Large Language Model(LLM) reasoning scenario, which requires numerous intermediate steps and the quality of the intermediate reasoning step is pivotal(Lightman et al., 2024). Compared with outcome verifiers solely focusing on outcome correctness(Cobbe et al., 2021), process verifiers provide more detailed signals throughout the multi-step decision-making process including the final outcome, which can be an elegant replacement to outcome verifiers at little-to-no cost(Lightman et al., 2024). In the LLM reasoning scenario, only the outcome binary reward is accurate and has no inductive bias. We thus focus on the value-based process verifier, where the process signal is the value of the current state(Lu et al., 2024; Luo et al., 2024; Snell et al., 2024). The value-based process verifier incorporates limited inductive bias: the Markov Decision Environment has only the outcome binary reward, and the discount factor is equal to 1. In such an environment, the value signal can be effectively annotated through Monte Carlo Sampling, as introduced in Wang et al. (2024).\nThough Monte Carlo estimation is an unbiased estimation of the ground-truth value(i.e. the success rate of the current state to outcome), it's still inaccurate especially when the sampling quantity is limited. The first problem of the limited sampling quantity is that it can only represent limited different values. As the Monte Carlo estimation method tries to estimate the state value in a discrete sampling space, the differences in representation granularity between the estimated value and the ground-truth value will be unacceptable. The second problem of the limited sampling quantity is that the variance of Monte Carlo estimation is related to the ground-truth value p. When p is close to a marginal value, the variance of the Monte Carlo estimation result is small and the estimation can be accurate. When p is close to the middle value, the estimation will be inaccurate except for increasing the sampling quantity.\nTo address the first problem, people often use the mean-square error objective function and map the discrete estimated value into the contiguous space(Lu et al., 2024; Luo et al., 2024; Snell et al., 2024). The mean-square error objective function introduces the distance prior and shows that the distance between two different values can be measured by the square of the difference between the two values, which extends the discrete estimated values into the contiguous space. To address the second problem, people often use the temporal difference method to update the value of the current state using values of subsequent states iteratively, greatly reducing the sampling variance at the cost of a larger sampling quantity(Tsitsiklis & Roy, 1997; Gallici et al., 2024). However, in the LLM reasoning scenario, it's expensive to extend the sampling quantity as each sample requires a completed rollout to the final state, which limits the application of Monte Carlo estimation as the annotation of the state value.\nIn this paper, we propose the structural prior injection method. To clarify, motivated by the distance prior introduced by the mean-square error objective function that is used to bridge the gap between discrete labels and contiguous target space, we inject the structural prior into the na\u00efve scalar Monte Carlo state value expression, representing the scalar value as the expectation of some pre-defined categorical distribution. Under a suitable prior definition, the Monte Carlo estimation result can be treated as a one-step sampling of the ground-truth target distribution that is derived from the ground-truth state value p. The transformation allows us to interpret the Monte Carlo sampling error and the coarse-grained signal as the mismatch between the ground-truth target distribution and the posterior distribution conditioned on a sample taken from the ground-truth target distribution, and the error is derived from the difficulty of recovering the ground-truth distribution given limited sampling data.\nTo handle the problem, we propose the distance metric called Statistics-based Distance, which is used to measure the distance between two categorical distributions conditioned on the probability of sampling the corresponding category from the ground-truth target distribution. The distance metric can be used to judge the reasonableness of the prior structural information to be injected, which can therefore guide the optimization approach. We optimize the categorical distribution via different objective functions, including the mean-square error and the cross-entropy(HL) loss(Imani & White, 2018), showing that the reasonable structural prior can improve the performance of value-based process verifiers on different objective functions and different tasks for about 1~2 points at little-to-no cost. Through ablation studies, we also show that different structural prior can result in very different performances.\nFinally, we summarize our contributions in this paper:\n\u2022 We propose the structural prior injection method, transferring the Monte Carlo sampling error into the distribution mismatch problem, showing that the error is due to the difficulty of recovering the ground-truth distribution using limited sampling data.\n\u2022 We propose the Statictics-based Distance metric, which can guide the posterior distribution selection and handle the mismatch problem and is supported by the experiments.\n\u2022 We show that the improvements derive from the reasonable structural prior injection, which is a promising and meaningful direction for future work."}, {"title": "2. Preliminaries", "content": "We first formally review the concepts and formulations of the Markov decision process(MDP) and the Bellman Equation in the Large Language Model(LLM) reasoning scenario. Then, we review the Monte Carlo estimation method, which is used to estimate the state action value. Finally, we introduce the value-based process verifier."}, {"title": "2.1. Markov decision process and Bellman Equation", "content": "The Markov decision process can be represented as a 4-tuple (S, A, P, R), where $S = \\{s\\}$ is the state space, $A = \\{a\\}$ is the action space, $P = P(s_t, a_t, s_{t+1})$ is the transition probability measuring the probability that action $a_t$ in state $s_t$ at time t will lead to state $s_{t+1}$ at time t+1. In the deterministic scenario, the transition probability will be restricted to 1, showing that given $s_t$ and $a_t$, the next state will be $s_{t+1}$. $R = R(s_t, a_t, s_{t+1})$ is the reward function, measuring the reward received after transitioning from state $s_t$ to $s_{t+1}$ due to action $a_t$. The policy $\\pi$ is a mapping from the state space S to the action space A. A completed MDP consists of multiple states $\\{s_1, s_2, ..., s_t\\}$, where each state $s_i$ first samples the next action $a_i$ using the policy $\\pi$, then samples the next state $s_{i+1}$ according to the transition probability distribution given the current state $s_i$ and action $a_i$.\nThe Bellman Equation defines the relationship between neighboring values in an MDP. Given the state value V defined as\n$$V^{\\pi}(s) = E_{\\pi}[G_t | S_t = s]$$ (1)\nwhere $\\pi$ being the current policy and s being the current state, and return $G_t$ defined as\n$$G_t = \\sum_{k=0}^{\\infty}\\gamma^kR_{t+k}$$ (2)\nwhere $R_{t+k}$ is the abbreviation of $R(S_{t+k}, a_{t+k}, S_{t+k+1})$ and $\\gamma$ being the discount factor measuring the rate of reward decay, the expectation version of Bellman Equation is defined as\n$$V^{\\pi}(s) = E_{\\pi}[R_t|S_t = s] + \\gamma\\sum_{s'\\in S}P(s'|s)V^{\\pi}(s')$$\n$$= r(s) + \\gamma\\sum_{s'\\in S}P(s'|s)V^{\\pi}(s')$$ (3)\nwhere r(s) is defined as the expectation of immediate reward at state s.\nIn the LLM reasoning scenario, a completed reasoning path is split by a rule-based splitter like \"\\n\", forming a few non-overlapping reasoning steps. The action is defined as a single reasoning step, and the state is defined as the cumulative reasoning steps. Given current state $s_t$ and action $a_t$, the next state $s_{t+1}$ is uniquely determined, which shows that"}, {"title": "2.2. Monte Carlo Estimation", "content": "Monte Carlo estimation is the method that estimates state value function given policy $\\pi$ using Monte Carlo sampling in MDP. Rewriting Eqn.1, we have:\n$$V^{\\pi}(s) = E[G_t|S_t = s] \\approx \\frac{1}{N}\\sum_{i=1}^{N}[G_t^{\\pi}|S_t = s]$$ (6)\nwhere each $G_t^{\\pi}$ is the return of a completed trajectory sampled from trajectories that $S_t = s$ using policy $\\pi$. Monte Carlo estimation is an unbiased and effective estimation method.\nIn the LLM reasoning scenario, the return $G_t$ can be simplified as:\n$$G_t = \\sum_{k=0}^{\\infty}R_{t+k} = r$$ (7)\nwhere r is defined after Eqn.4. Then, the Monte Carlo estimation of state value V can be simplified as:\n$$V^{\\pi}(s) \\approx \\frac{1}{N}\\sum_{i=1}^{N}[G_t^{\\pi}|S_t = s] = \\frac{1}{N}\\sum_{i=1}^{N}[r(s)|S_t = s]$$ (8)\nwhich shows that we can estimate the intermediate state value using the outcome reward."}, {"title": "2.3. Value-based Process Verifier", "content": "The value-based process verifier aims at estimating the state value during the reasoning process. Given the initial question q and previous reasoning steps $\\{a_1, a_2,..., a_t\\}$, the value-based process verifier is asked to generate a scalar value that represents the state value of the current state $s_t = \\{a_1, a_2,..., a_t\\}$. Under the environment settings shown in \u00a72.1, given the ground truth answer, it's able to perform an automatic annotation approach to obtain process state value signals using the Monte Carlo method, and the state value is interpreted as the success rate of the outcome starting from the current state. We can thus formulate the value-base process verifier as a mapping from binary function to probabilities using parameter $\\theta$:\n$$f_{\\theta}(q, s_t) \\rightarrow [0, 1]$$ (9)"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Regression: from Scalar to Categorical Distribution", "content": "As we perform k individual rollouts from the current state to calculate the Monte Carlo return and estimate the state value, each of the rollouts will return a binary value which is the outcome reward. There are totally k + 1 different kinds of estimated state values, evenly ranging from 0 to 1 with step size being $\\frac{1}{k}$. We can thus construct a classification objective and train a classifier to handle the problem. To clarify, we map the k + 1 different estimated values into k + 1 different bins, each bin represents one of the estimated values. Each time, given the question q and the current state $s_t$, the value-based process verifier, which is a classifier, chooses one of the k + 1 different bins and maps the selected bin to the value. We can optimize the value-based process verifier through cross-entropy loss as follows:\n$$\\min_{\\theta}\\sum_{i=1}^{N}log f_{\\theta}(b_i|q, s_t),$$ (10)\nwhere $f_{\\theta}$ is the function mapping from state space $\\{q\\}\\bigcup s_t \\in S$ to one of the bins. $b_i \\in N^+$ is the bin index of estimated state value ranging from 1 to k + 1. The i-th bin will then be mapped to the state values, for example, $\\frac{i-1}{k}$. In such case, the predicted state value $f_{\\theta}(\\cdot | q, s_t)$ is defined in the discrete space while $V^{\\pi}(s)$ is defined in the contiguous space, which poses a mismatch between the objective function and target value.\nIn order to bridge the gap between the discrete estimated state value space and the contiguous target value space, researchers treat the value prediction problem as a regression task that includes the distance prior. Specifically, for two estimated values $\\hat{y}$ and $\\tilde{y}$, the distance between them is the square of the difference. The distance prior allows us to optimize the discrete estimated state value in the contiguous value space, ensuring the predicted value makes sense beyond the discrete values. To clarify, for the given dataset $\\{q^i, s_t^i, y^i\\}_{i=1}^N$, people use the mean-squared error (MSE) loss to optimize value-based process verifier as follows:\n$$\\min_{\\theta} \\sum_{i=1}^{N}(f_{\\theta}(q, s_t) - y^i)^2$$ (11)"}, {"title": "3.2. Structural Prior Injection via Categorical Distribution Modeling", "content": "By treating the k-times Monte Carlo sampling as a singular sample of the Monte Carlo estimation, the estimation distribution then follows the Binomial distribution:\n$$V^{\\pi}(s) \\sim Bin(k, p),$$ (15)"}, {"title": "3.3. Posterior Distribution Selection via Statistics-based Distance", "content": "As the estimated state value $V^{\\pi}(s)$ follows the Binomial distribution as described in Eqn.15, the ground-truth target distribution of categorical distribution is the Binomial distribution with ground-truth success rate p and number of Monte Carlo rollouts k. However, it is difficult to estimate the distribution with limited sampling instances in an unbiased way, especially when we sample only once from the categorical distribution. Given limited sampling results, we propose the metric to calculate the distance between posterior distribution and ground-truth target distribution at the statistical level as follows:\nDefinition 3.1. Statistical Distribution Distance. Given p as the posterior distribution, q as the target distribution, d as one of the pre-defined distance metrics like Kullback-Leibler(KL) divergence, Wasserstein distance, or else. The distance between distributions at the statistical level is defined via statistics-based expectation as follows:\n$$DT(p, q) = E_{x\\sim q}[d(p(\\cdot | x), q)].$$ (17)\nThe KL divergence is useful to measure the distance between distributions. However, as the categories in the categorical distribution have sequence characteristics(i.e. have different weights), it's not suitable to measure the distance between categorical distributions by KL divergence. We use the Wasserstein distance instead. It's reasonable that the smaller the Statistical Distribution Distance is, the more accurately we estimate the target distribution, which will help improve the performance of the value-based process verifier.\nAfter all, the value-based process verifier will be optimized by the Histogram Loss to provide distribution shaping supervise signal as follows:\n$$\\mathcal{L}^{CHL} = \\min_{\\theta} -\\sum_{i=1}^{N_z}\\sum_{j=1}^{Z} f_{\\theta} (z_j|q, s_t) log p(z_j|q, s_t)$$ (18)"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": "Datasets and metrics. Following previous research (Wang et al., 2024; Lightman et al., 2024), we evaluate value-based process verifiers based on their verification ability through best-of-N sampling. Specifically, given a problem p, we sample N candidate solutions from a generator. Then, the candidates are re-ranked according to the score generated by the given verifier. The candidate with the highest score will be treated as the best candidate and selected as the final solution. Finally, we compare the consistency of the final solution and ground-truth answer to determine whether the solution is correct. The statistical success rate will be reported. Following Snell et al. (2024), we also include the beam-search metric to evaluate the performance of value-based process verifiers at test time. Specifically, the number of beams N and beam size M will be set. Given a problem p, the verifier is required to score every step during the generation process. For each iteration, N beams will generate M next-step candidates individually, the verifier will select the top N candidates from the N * M beams as the beams for the next iteration, and then continue the iteration until finished. Like Best-of-N, we compare the consistency of the final solution and ground-truth answer. The statistical success rate will be reported.\nWe conduct our experiments on the challenging MATH datasetHendrycks et al. (2021) for verification. We use the test split following Lightman et al. (2024) as our test set, which consists of 500 randomly selected questions from MATH. As described by Wang et al. (2024), the subset evaluation produces similar results to the full-set evaluation. We uses MetaMATH(Yu et al., 2024) as the fine-tuning dataset as used in Wang et al. (2024).\nBaselines and implementation details. The generator in our experiments is based on LLemma-7b. We train it on MetaMATH for 3 epochs to get the generator. Based on the train split of MATH dataset, we construct the training dataset of the process verifier. To clarify, We use the generator to sample 15 solutions per problem. Following previous works(Lightman et al., 2024; Wang et al., 2024), we split each solution into steps by the pre-defined rule-based strategies (e.g. newline as the delimiters). For each step, we combine it with its previous steps to form an incomplete solution, then sample 8 rollouts to perform Monte Carlo estimation and annotate the state value. In general, we sample 15 * 8 = 120 samples for each problem and the training dataset has 180k samples in total."}, {"title": "5. Analysis and Discussion", "content": "In this section, we will discuss the influence of the Statistics-based Distance metric and the structural prior. Though related to each other, we discuss the distance metric and structural prior in different sections for the sake of clarity. The former is based on the cross-entropy loss that can be directly supervised by specific types of categorical distribution definitions. The latter is based on the mean-square error objective function where the training objective is only supervised on the expectation of the categorical distribution, which allows us to define the distribution more freely."}, {"title": "5.1. Posterior Distribution Selection.", "content": "In this section, we analyze the difference in distribution selection and their corresponding performance. To clarify, we use the following distributions:\n\u2022 One-hot distribution. A one-hot distribution that the category which sampled state value points to is set to be 1, other categories are set to 0.\n\u2022 Guassian distribution(dynamic). Truncated Gaussian distribution with dynamic variance. $\\mu$ is the sampled state value, $\\sigma^2$ is $\\frac{\\mu(1-\\mu)}{k}$.\n\u2022 Gaussian distribution(static). Truncated Gaussian distribution with fixed variance as described in Fare-brother et al. (2024). We set three standard deviations to be two bin widths in our experiments.\nWe report the statistics-based distance for different posterior distributions in Figure 1a. Specifically, we calculate the statistics-based distance varying on different ground-truth probability p from 0 to 1. We use the Wasserstein distance to measure the distance between categorical distributions. The distance between the i-th category and j-th category is $\\frac{|i-j|}{k}$, following the definition of our categorical distribution. As shown in Figure 1a, compared with Gaussian distributions, one-hot distribution has a much greater distance varying on"}, {"title": "5.2. Prior Categorical Distribution Selection.", "content": ""}, {"title": "6. Limitation and Future work", "content": "We believe that the structural prior injection method, as well as the categorical distribution modeling method, provides a meaningful aspect to improve the performance of the value-based process verifier and analyze the Monte Carlo estimation error. However, we are certain that research about the structural prior still has a long way to go. For instance, when optimizing the categorical distribution via Histogram Loss, as we use the statistics-based distance to measure the distance between the posterior distribution and ground-truth distribution, the distribution modeling can only be performed in a prior and non-differentiable way due to the non-differentiable nature of Wasserstein distance. One future direction is to soften the distance metric and transform the distribution modeling approach into a tractable and dynamic way.\nThe other thing is that we use the Histogram Loss to optimize the categorical distribution representation, which is not a perfect objective function due to the absence of distance prior. To clarify, as the categorical distribution is defined under the Dirac delta function of unsteady values, the categories then follow a natural order since our goal is the expectation of the categorical distribution. When optimizing the verifier via Histogram Loss, the distance between different categories will not be taken into account. Like other injected prior information, the data prior will not change the optimal solution which is the estimated posterior distribution, but it should result in a better performance for reasons like faster convergence or more accurate objective.\nFinally, while our work discusses the mean-square error and cross-entropy loss from the unified perspective, we find it hard to combine the two different objective functions together. When training verifiers using both the mean-square error and cross-entropy loss by their linear combinations, we find there's a performance drop compared with training on each objective function individually. The possible reason may be the different gradient directions during the optimization process or the different assumptions from the perspective of maximum likelihood optimization. Further work can perform more analysis of the potential and scalability of using both objective functions together."}, {"title": "7. Conclusion", "content": "In this paper, we introduce the structural prior, transforming the scalar value into the expectation of a pre-defined categorical distribution, which allows us to improve the value representation and address the Monte Carlo estimation error from the perspective of distribution estimation. Under suitable structural prior, we show that the error is derived from the mismatch between the estimated posterior distribution and the ground-truth distribution, which is intractable to estimate as we have only limited samples. We then provide the Statistics-based Distance as the metric to measure the distance between ground truth distribution and posterior distribution. Under the vanilla mean-square error objective function and Histogram Loss objective function, our experiments show that the transformation can yield consistent improvements in performance on different tasks in the LLM reasoning scenario, showing that the structural prior can be of great benefit to the optimization process of the value-based process verifier. Finally, we compare the effectiveness varyin on structural priors. By performing experiments on each objective function, we show that prior selection can greatly influence the performance of value-based process verifiers."}, {"title": "Impact Statements", "content": "This paper presents a method whose goal is to advance the large language model reasoning. This endeavor, while technically challenging, carries significant implications for the ethical use and societal impact of artificial intelligence. The goal of model's reasoning ability is to provide a more reasonable and useful tool for the human's benefit. It enhances Al's utility in various sectors to make reliable decisions that are in line with organizational goals and ethical standards."}, {"title": "A. Related Work", "content": ""}, {"title": "A.1. Test-time Scaling for LLM Reasoning.", "content": "The test-time scaling technique requires models to generate long Chain-of-Thought(CoT) explicitly or implicitly as its thinking steps or reasoning steps, which can effectively improve the reasoning capabilities of large language models. Some works provide relative reasoning demonstrations or thinking paradigms on the input, using few-shot prompting techniques like CoT prompting(Wei et al., 2022) or in-context learning(Zhou et al., 2022) to achieve test-time scaling. These methods can improve the reasoning capabilities of large language models to some extent, but the performance is highly sensitive to the quality and quantity of few-shot demonstrations(Qin et al., 2024). Some researchers focus on incorporating searching algorithm into the LLM reasoning scenario by explicitly performing searching algorithm during the reasoning process(Yao et al., 2023; Liu et al., 2023; Wang et al., 2023; Zhao et al., 2023). The methods are shown to be effective at the cost of large token consumptions, and the human-crafted searching algorithm can only fit specific tasks(Sprague et al., 2024), which makes the methods restricted. Another line of implementation of the test-time scaling technique is reinforcement learning. By providing suitable signals or feedback, the LLMs can learn to reason from scratch(DeepSeek-AI et al., 2025) or the instruct-tuned checkpoint(Zelikman et al., 2022)."}, {"title": "A.2. Process-supervised verifier.", "content": "Researchers have found that the process-supervised verifiers that are trained on fine-grained signals are effective for LLM reasoning and reinforcement learning, compared with outcome-supervised verifiers(Lightman et al., 2024). However, The definition of fine-grained signals is vague. Lightman et al. (2024) define the fine-grained signal as the stepwise calculate correctness and the signal is -1 if the reasoning step is incorrect, 1 if the reasoning step is correct, 0 if the reasoning step is neural. Wang et al. (2024) define the fine-grained signal as the binary signal. Specifically, they map the Monte Carlo return to binary labels. If all outcomes are wrong, the signal of the current step is labeled to be 0. If any outcome is correct, the current step's signal is labeled 1. Some other works define the signal in a rule-based version, consisting of calculating error, formatting error, or else(Xi et al., 2024; Zheng et al., 2024). In the LLM reasoning scenario, as only the final outcome has specific and accurate labels, inductive bias is inevitably introduced when defining the fine-grained signal, which may result in reward hacking due to the definition flaw(DeepSeek-AI et al., 2025). Following previous works(Luo et al., 2024; Snell et al., 2024; Lu et al., 2024), we define the fine-grained signal as the state value of the current step, which is calculated using the labels of final outcome only to reduce the potential human-crafted inductive bias."}, {"title": "A.3. Regression as Classification.", "content": "Several works have replaced regression with classification to improve performance in different domains(Weiss & Indurkhya, 1995; Zhang et al., 2023). Notably, Imani & White (2018) proposed the HL-Gauss cross-entropy loss as the drop-in replacement of MSE loss, showing that by incorporating prior distribution information, the classification loss can be effective in regression tasks. The authors then performed experiments in several different distributions and found that the HL-Gaussian distribution can greatly improve the performance, while other distribution-learning approaches can result in even worse performance. Farebrother et al. (2024) further extended the classification optimization method to several more complex domains, showing the general benefits of performing cross-entropy on the categorical distribution for regression tasks. However, how to perform better categorical distribution modeling is still unclear. In the LLM reasoning scenario, we endow the categorical distribution with fixed bins the specific definition, explaining the difference between regression and classification from the perspective of prior structural information injection. We show that the regression loss can also benefit from suitable categorical distribution modeling, indicating that the categorical distribution modeling can achieve the general benefits compared with scalar regression methods."}, {"title": "B. More Implementation Details", "content": "We provide the hyper-parameters when training the generator and value-based process verifiers in Table 2."}, {"title": "C. Reasoning process transit from non-deterministic to deterministic.", "content": "Finally, we further analyze the error and noise when estimating state value via Monte Carlo Sampling. First of all, we argue that the reasoning process transits from non-deterministic to deterministic. We measure the process as deterministic or"}]}