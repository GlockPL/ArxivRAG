{"title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "authors": ["Shuqi Liu", "Han Wu", "Bowei He", "Xiongwei Han", "Mingxuan Yuan", "Linqi Song"], "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023; Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024; Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022; Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023; Yu et al., 2024).\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023; Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-and-play enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing task-vector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks.\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted fine-tuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)\u2014derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to code-specific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models.\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling approach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance."}, {"title": "Related Work", "content": "Modeling merging (Yang et al., 2024; Goddard et al., 2024), as a complementary approach to training-based methods, has the capability to integrate multiple task-specialized models into a unified one (Wortsman et al., 2022; Stoica et al., 2024; Ainsworth et al., 2023; Yu et al., 2024), to improve model performance on individual tasks by merging checkpoints without requiring additional training (Yadav et al., 2023; Ilharco et al., 2023b), and to alleviates the issue of catastrophic forgetting (Alexandrov et al., 2024). According to whether the based models are in same architecture, the model merging methods can be divided into heterogeneous model merging and homogeneous model merging.\nHeterogeneous Model Merging. A brunch of work (Avrahami et al., 2022; Nguyen et al., 2023) attempts to perform architecture transformation before merging, aiming to transform multiple models with different architectures into a unified one. However, these approaches often rely on the learning process to align the models, which can potentially degrade their performance. Recent research in this direction often builds upon the concept of mode connectivity (Freeman and Bruna, 2017; Frankle et al., 2020; Tatro et al., 2020), which suggests the existence of a connected path between multiple local minima of models, along which the loss remains nearly constant. Furthermore, Entezari et al. (2022) revealed that models permuted to the same loss basin can be merged by averaging their weights. Following these intuitions, more recent works (Ainsworth et al., 2023; Jordan et al., 2023; Stoica et al., 2024) focus on permutation strategies to achieve better heterogeneous model merging.\nHomogeneous Model Merging. Task-specific models initialized from the same pre-trained model can often be merged without considering permutation symmetry (Wortsman et al., 2022; Ilharco et al., 2023b). One of the most straightforward approaches to model merging is to directly weighted average the parameters of base models (Shoemake, 1985; Wortsman et al., 2022). However, the performance of simple average merging is often sub-optimal, as task-specific features are typically not uniformly distributed. Task Arithmetic (Ilharco et al., 2023b) enhances the merging process by introducing task vectors, suggesting that simple arithmetic operations on these vectors can effectively edit models and produce a merged model. Building on the concept of task vectors, both DARE (Yu et al., 2024) and Ties (Yadav et al., 2023) employ pruning-then-scaling methods to merge task vectors, based on the assumption that not all parameters contribute equally to the final performance.\nAnother line of research on model merging leverages information derived from the activations of training data. For example, Matena and Raffel (2022) suggested a probability-space approach, which uses the Fisher information matrix to identify the importance of model parameters and proposed Fisher Merging. Jin et al. (2023) introduced RegMean, a data-less merging method that merges models in parameter space by solving a linear system constructed from data and model parameters. We posit that activations play a crucial role in identifying the key parameters within task vectors relevant to downstream tasks. To this end, we propose a novel sensitivity-guided activation method to facilitate more effective merging of key features."}, {"title": "Methodology", "content": "Our Sens-Merging method combines two levels of sensitivity analysis: layer-wise analysis within individual models and cross-task analysis across different models to achieve a balanced parameter distribution. For layer-wise analysis, we compute sensitivity scores using gradient information from calibration datasets. For cross-task analysis, we evaluate model alignment through logit comparison. These two components determine the final merging coefficients used to merge corresponding layers into a unified model, as shown in Figure XX."}, {"title": "Preliminary", "content": "Considering K task-specialized fine-tuned models {$\\theta_{SFT}^1$,..., $\\theta_{SFT}^K$} derived from a common pre-trained backbone $\\theta_{PRE}$, model merging aims to merge them into a single model $\\theta_{M}$ that can effectively handle all tasks simultaneously. The task-specific capabilities of each fine-tuned model are captured by task vectors, defined as the difference between the fine-tuned parameters and the pre-trained backbone:\n$\\delta_{tk} = \\theta_{SFT}^k - \\theta_{PRE}$, for k \u2208 {1, ..., K}.\nTask vector-based merging aggregates these task vectors to construct a single, static merged model:\n$\\theta_{M} = \\theta_{PRE} + \\sum_{k=1}^{K} \\lambda * \\delta_{tk}$\nwhere the coefficient $\\lambda$ represents the importance of each merged task vector."}, {"title": "Task-Specific Scaling", "content": "To accurately balance the parameters within individual task models, we conduct layer-wise sensitivity analysis by measuring each layer's contribution to model performance through aggregating parameter sensitivities within that layer.\nParameter Sensitivity. We define parameter sensitivity as the change in loss when setting that parameter to zero. A parameter is considered highly sensitive if zeroing it results in a significant loss increase. For a fine-tuned model with parameters $\\theta_{SFT} = [\\theta_1, ..., \\theta_N]$, where N represents the total number of parameters, the j-th parameter can be expressed as $\\theta_{SFT}^j = [\\theta_1, ..., 0_j, ..., \\theta_N]$. With gradients of the loss relative to $\\theta_{SFT}$ represented as $\\nabla_{\\theta_{SFT}} L$, the sensitivity of the j-th parameter for a specific sample $x_k$ from task ti is determined as:\n$S_{j,k}^{t_i} = |(\\theta_{SFT}^j)^T \\nabla_{\\theta_{SFT}} L(x_k)|$ (1)\nThe rationale behind this sensitivity definition stems from the first-order Taylor expansion of L(xk) relative to $\\theta_j$. In essence, $S_{j,k}^{t_i}$ provides an approximation for how the loss might change in the absence of $\\theta_j$:\n$(\\theta_{SFT}^j)^T \\nabla_{\\theta_{SFT}} L(x_k) \\approx L(\\theta_{SFT}) - L(\\theta_{SFT} - \\theta_j)$ (2)\nTo estimate the parameter sensitivity $S_{j}^{t_i}$ for task ti, we randomly sample m instances from the task training set as calibration samples. The final sensitivity score $S_{j}^{t_i}$ aggregates the individual sensitivities across all sampled instances: $S_{j}^{t_i} = \\sum_{k=1}^{m}S_{j,k}^{t_i}$\nLayer-Wise Sensitivity and Normalization. The layer-wise sensitivity $s_l^i$ is then calculated by summing the sensitivities of all parameters within each layer, thereby reflecting each layer's overall contribution to the model's performance. To allow for meaningful comparisons of these importance scores across different models, we apply L2 normalization to the sensitivities of all layers. Consequently, the task-specific sensitivity scaling factors $\\alpha_l^i$ are defined as:\n$s_l^i = \\sum_{j \\in P_l} S_j^{t_i}, \\quad \\alpha_l^i = \\frac{s_l^i}{||S^i||_2}$ (3)\nwhere Pl denotes the set of parameters in layer l, and L is the total number of layers in the model."}, {"title": "Cross-Task Scaling", "content": "While task-specific sensitivity focuses on the importance of layers within individual tasks, it is equally essential to evaluate how each task-specific model influences other tasks during the merging process. Cross-task sensitivity captures the interdependencies and shared representations between different tasks, ensuring that the merged model benefits from common features and decision-making processes.\nThe measurement of cross-task influence begins with evaluating logits alignment between different task-specific models. Specifically, for calibration samples from task tj, we compute the alignment score between model $\\theta_{SFT}^{t_i}$ and the expert model for task tj, $\\theta_{SFT}^{t_j}$, using the L2 distance between their output logits:\n$g_{i,j} = ||f_{\\theta_{SFT}^{t_i}}(x) - f_{\\theta_{SFT}^{t_j}}(x)||_2$ (4)\nwhere $f_{\\theta}(x)$ denotes the output logits of model $\\theta$ for input $x$, and $||\\cdot||_2$ represents L2 distance. This alignment score quantifies how closely the predictions of model $\\theta_{SFT}^{t_i}$ match those of the expert model for task $SFT_{t_j}$, providing insight into the degree of shared knowledge and representational similarity between tasks. To obtain a comprehensive measure of cross-task sensitivity for a specific task model $\\theta_{SFT}^{t_i}$, we aggregate the alignment scores across all other tasks. This aggregation process involves computing the normalized alignment:\n$\\tau_i = \\frac{\\sum_{i=1,i \\neq j}^{K} g_{i,j}}{\\frac{\\tau_i}{||\\tau||_1}}$ (5)\nThe resulting cross-task scaling factor $\\tau_i$ serves as a crucial metric that quantifies model $\\theta_{SFT}^{t_i}$'s ability to transfer knowledge across tasks. Higher values of $\\tau_i$ indicate superior cross-task generalization capabilities, suggesting that the model has learned robust representations that are valuable across multiple tasks. Conversely, lower values of $\\tau_i$ reflect greater task-specific specialization, indicating that the model's features are more narrowly focused on its primary task."}, {"title": "Integration with Merging Methods", "content": "Our Sens-Merging method combines task specific scaling factor $\\alpha_l^i$ and the cross-task scaling factor $\\tau_i$ into a plug-and-play module, which can be seamlessly integrated with existing task vector-based model merging methods. To effectively combine these sensitivity factors, we employ a two-step process. First, we multiply the task-specific scaling factor $\\alpha_l^i$ with the cross-task scaling factor $\\tau_i$ to capture both task-specific and cross-task importance. Then, we apply a softmax function with temperature T to normalize these products and obtain the final scaling coefficients:\n$\\sigma_l^i = Softmax(\\tau_i \\cdot \\alpha_l^i, T)$ (6)\nThe final step involves computing the merged model parameters $\\theta_M^l$ for each layer l. We start with the base model parameters $\\theta_{base}$ and incorporate weighted contributions from all K fine-tuned models. The contribution of each task-specific model is scaled by its normalized coefficient $\\sigma_l^i$ and multiplied by K to preserve the magnitude of updates:\n$\\theta_M^l = \\theta_{base} + \\sum_{i=1}^{K} K \\cdot \\sigma_l^i  (\\theta_{SFT}^i - \\theta_{base})$ (7)"}, {"title": "Experiments", "content": "Baselines. We evaluate the effectiveness of our Sens-Merging method by comparing it against both individual task-specific models and several established model-merging techniques, including Task Arithmetic, Ties-Merging, and DARE-Merging. Task Arithmetic (Ilharco et al., 2023b) enhances the merging process by introducing task vectors, suggesting that simple arithmetic operations on these vectors can effectively edit models and produce a merged model. Building on the concept of task vectors, both DARE (Yu et al., 2024) and Ties (Yadav et al., 2023) employ pruning-then-scaling methods to merge task vectors, based on the assumption that not all parameters contribute equally to the final performance.\nHyperparameters. Both baselines and our Sens-Merging enhanced baselines use the same hyperparameters for fair comparison. For Task Arithmetic, we use a default scaling coefficient of $\\lambda = 1$, which maintains the original magnitude of task vector when adding the pretrained backbone. However, the DARE method has been observed to be more sensitive to variations in both the scaling coefficient $\\lambda$ and the drop rate parameter r. To achieve a balanced performance, we set the scaling coefficient to $\\lambda = 0.5$ and establish a default drop rate of r = 0.5 for DARE. Similarly, for Ties-Merging, which requires the specification of a masking ratio, we set the default mask ratio to r = 0.7 across all experiments.\nBenchmark. Our experimental evaluation encompasses three families of models: LLaMA-2 7B series (Touvron et al., 2023), Mistral 7B series (Jiang et al., 2023), and LLaMA-2 13B series (Touvron et al., 2023), each specialized in distinct domains: general knowledge, mathematical reasoning, and code generation. For comprehensive evaluation, we utilize seven benchmark datasets spanning three key domains: MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019) and TruthfulQA (Lin et al., 2022) for assessing general knowledge and reasoning capabilities; GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for testing mathematical reasoning proficiency; and HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for evaluating code generation abilities. To ensure consistent and unbiased assessment, model performance is evaluated using zero-shot accuracy, with pass@1 rate specifically measuring code generation correctness."}, {"title": "Main Results", "content": "Merging Models with Sense-Merging. We first evaluate the effectiveness of our Sens-Merging method by utilizing it as a plug-and-play module to enhance existing task-vector-based baselines. Table 1 presents the performance of the baseline methods alongside their Sens-Merging enhanced counterparts across seven datasets. Specifically, when merging fine-tuned models specialized in general knowledge (Chat\u00b9), mathematical reasoning (Math\u00b2), and code generation (Code\u00b3), all derived from LLaMA2-7B4, Sens-Merging demonstrates a consistent improvement in the average performance across all domains. Specifically, when comparing the average scores of each method with and without Sens-Merging, we find that:\n(1) Superior Improvement in Task Arithmetic: Task Arithmetic exhibits a particularly notable increase from an average score of 29.03 without Sens-Merging to 34.78 with Sens-Merging, achieving a 19.22% relative improvement of 5.58 points. As both Ties-Merging and DARE have implemented drop strategies to mitigate parameter interference, the integration of scaling coefficient adjustments through Sens-Merging does not achieve as substantial an enhancement as seen with Task Arithmetic. Nevertheless, Sens-Merging still contributes to performance improvements in these methods, with Ties-Merging increasing from an average score of 34.73 to 35.13, and DARE improving from 34.86 to 35.13. (2) Domain-Specific Improvement: Within the general knowledge domain, Sens-Merging significantly enhances performance on both the MMLU and HellaSwag datasets across all merging methods. In mathematical reasoning, combining Sens-Merging with the Ties-Merging baseline achieves the highest scores on both GSM8K (47.69) and MATH (7.80), surpassing their respective baselines. In code generation, Task Arithmetic shows substantial improvements, increasing from 13.5 to 33.1 on MBPP and from 7.3 to 18.9 on HumanEval. (3) Enhanced Performance than Individual Fine-tuned Models: Sens-Merging enables the combined models to achieve higher performance on general knowledge and code generation tasks, even surpassing the original code fine-tuned model. For example, when integrating the Chat, Math, and Code models using Sens-Merging, performance on the MBPP and HumanEval datasets increases significantly. Specifically, accuracy improves from 26.8 to 32.3 on the MBPP dataset and from 12.8 to 19.5 on the HumanEval dataset. This demonstrates that model merging can overcome the challenges associated with training a single model for complex tasks by effectively integrating capabilities from other specialized fine-tuned models. Notably, when a Code model is merged with Math and Chat models, it achieves superior performance on coding tasks compared to code-specific fine-tuning alone.\nUsing Different Model Architecture. To verify the generalizability of our method across architectures, we conduct experiments using Mistral-7B models. Using task-specific models derived from the base Mistral-7B model5 - specifically Chat6, Math7, and Code8 our method demonstrates consistent performance improvements despite the architectural differences from LLaMA-based models. As shown in Table 2, when combined with Task Arithmetic and DARE, Sens-Merging demonstrated remarkable performance gains, surpassing the original baselines by 10.34 and 3.22 points respectively across all evaluated datasets. With Task Arithmetic, our method shows impressive gains across domains: 11.58 points in general knowledge, 4.86 points in mathematical reasoning, and 8.45 points in code generation. When combined with DARE, Sens-Merging particularly excelled in code generation, achieving a 5-point improvement over the original DARE and even outperforming task-specialized fine-tuned models. This superiority is evidenced by higher scores on coding benchmarks: 55.1 versus 50.9 on MBPP and 43.3 versus 40.0 on HumanEval.\nScaling to Larger Model Size. We further evaluate the scalability of our method using the LLaMA-2 13B9 models by merging Chat10, Math11, and Code12 fine-tuned models. As presented in Table 3, our approach maintains consistent performance gains at larger scales. Sens-Merging with Task Arithmetic demonstrates particularly strong improvements, outperforming the baseline by 5.68 points across all datasets, with notably impressive gains in code generation (14.75 points). When combined with Ties-Merging, Sens-Merging excels in mathematical reasoning tasks. Specifically, it achieves a 3.77% relative improvement (1.98 points) on the GSM8K dataset and a 3.03% relative improvement on the MATH dataset."}, {"title": "Ablation Studies", "content": "To understand the contribution of each component in our framework, we conduct ablation studies by incorporating either task-specific scaling factors or cross-task scaling factors into the Task Arithmetic method. As shown in Table 4, different scaling approaches exhibit task-dependent effectiveness. For mathematical reasoning, task-specific sensitivity scaling yields notable gains (a 21.36% relative improvement and an increase of 1.38 points on the MATH dataset) while having limited impact on other tasks. Conversely, cross-task scaling delivers significant improvements in general knowledge and code generation tasks (4.28 and 14.8 points respectively) but decreases mathematical reasoning performance by 4.8 points. Overall, cross-task scaling provides stronger aggregate performance enhancements, achieving a total gain of 5.37 points. Therefore, each scaling method involves a trade-off: task-specific scaling excels at enhancing specialized capabilities (particularly mathematical reasoning) but with limited broader impact, while cross-task scaling offers stronger overall performance improvements at the cost of sacrificing some task-specific excellence."}, {"title": "In-depth Analysis", "content": ""}, {"title": "Scaling Factors Analysis", "content": "Layerwise Sensitivity Distribution. Figure 3 reveals distinct layer-wise sensitivity patterns across model specializations: the chat model peaks at layer 10, leveraging lower layers for language processing; the math model shows maximum sensitivity around layer 15, emphasizing middle layers for mathematical reasoning; and the code model exhibits a unique dual-peak pattern, reflecting its need for both linguistic processing in lower layers and logical reasoning in middle layers. Thus, by leveraging layer-wise sensitivity, we enhance the weights of the layers that are most critical to performance, thereby ensuring that specialized functions are optimally preserved.\nCross-Task Sensitivity Scaling. In Table 6, we observe consistent variations in cross-task scaling factors across task vectors. Math models show the highest scaling coefficients (0.51-0.64), followed by Chat models (0.25-0.31), and Code models (0.12-0.18). These coefficients reveal that: mathematical reasoning provides strong transferable skills across tasks, chat abilities facilitate general language understanding, while coding skills are more specialized and less transferable."}, {"title": "Merge Two Fine-tuned Models", "content": "In addition to merging three models, we also evaluate the performance of combining two models: Chat & Math and Math & Code. We exclude the Chat & Code combination as its performance is significantly lower than that of the three-model merging. As shown in Table 5, when applied to two-model combinations, our Sens-merging method also outperforms baseline approaches, showing substantial improvements in Task Arithmetic method (3 points). In code generation, Sens-merging significantly improved performance over existing methods, achieving relative gains of 3.99% (1.2 points) over Ties-Merging and 4.29% (1.3 points) over DARE on the MBPP dataset. Notably, for both Ties-Merging and DARE methods, combining Chat and Math models yields better performance than merging all three models across all tasks, with Ties-Merging showing scores of 34.98 versus 34.73, and DARE showing 35.16 versus 34.86. This indicates that simply adding more models does not guarantee better performance in merging methods."}, {"title": "Conclusion", "content": "We introduce Sens-Merging, a novel method that determines model merging coefficients by analyzing parameter sensitivity both within specific tasks and across multiple tasks. Through extensive evaluation on Mistral 7B and LLaMA-2 7B/13B model families, we demonstrate that Sens-Merging enhances model merging performance across multiple domains, consistently outperforming both existing merging techniques and individually fine-tuned models. This improvement is particularly pronounced in code generation tasks, where merged models achieve superior results compared to specialized fine-tuning."}, {"title": "Limitations", "content": "While Sens-Merging demonstrates remarkable performance in model merging, achieving consistent improvements across various benchmarks, it shares fundamental limitations with existing task arithmetic-based methods. For example, our current implementation primarily addresses homogeneous model merging where base models share identical architectures. While this focus allows us to achieve state-of-the-art results in such scenarios, extending Sens-Merging to heterogeneous architectures remains an exciting direction for future research. Moreover, our method is specifically designed for large language models and has been primarily validated with LoRA fine-tuned models, where weight differences between specialized models are relatively constrained. For smaller-scale models or fully fine-tuned models with larger weight divergences, our approach may require adaptations."}, {"title": "Ethics Statement", "content": "This study utilizes publicly available datasets for our models. Prior research endeavors have generally taken ethical considerations into account. We have manually inspected a subset of samples and found no explicit ethical concerns, including violent or offensive content. Nonetheless, it is crucial to highlight that the output generated by large language models lacks the degree of control we might assume. Consequently, we are prepared to implement measures to mitigate any unforeseen outputs."}]}