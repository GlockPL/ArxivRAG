{"title": "DATA MOVEMENT LIMITS TO FRONTIER MODEL TRAINING", "authors": ["Ege Erdil", "David Schneider-Joseph"], "abstract": "We present a theoretical model of distributed training, and use it to analyze how far dense\nand sparse training runs can be scaled. Under our baseline assumptions, given a three\nmonth training duration, data movement bottlenecks begin to significantly lower hardware\nutilization for training runs exceeding about $10^{28}$ FLOP, two orders of magnitude above\nthe largest training run to date, suggesting the arrival of fundamental barriers to scaling\nin three years given recent rates of growth. A training run exceeding about $10^{31}$ FLOP\nis infeasible even at low utilization. However, more aggressive batch size scaling and/or\nshorter and fatter model shapes, if achievable, have the potential to permit much larger\ntraining runs. An interactive version of our model will shortly be accessible here.", "sections": [{"title": "INTRODUCTION", "content": "Scaling up neural networks and training them on more examples is crucial for good task performance (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022; Bi et al., 2024), with state-of-the-art models\nrequiring tens of thousands of GPUs\u00b9 to train in a reasonable duration. Previous work (Huang et al., 2019;\nRajbhandari et al., 2020; Lepikhin et al., 2020; Narayanan et al., 2021; Jiang et al., 2024b) has developed\npractical techniques enabling the rapid scaling of the past decade (Sevilla et al., 2022).\nIn this work, we address unexamined fundamental questions about limits to scaling in the future:\nQ1 Given present-day algorithms, GPUs, and interconnects, what is the biggest training run that can be\nperformed within a fixed duration, before intra- and inter-GPU data movement starts to seriously\nworsen utilization or even render it impossible?\nQ2 How far might this limit be extended, and what algorithmic or hardware progress can achieve that?\nAnswering these questions empirically would require millions of GPUs and large-scale engineering efforts,\nso we instead approach them theoretically. In doing so, we develop a simulator that can find optimal training\nrun configurations accounting for the factors that we identify as fundamental. This gives us the answers:\nA1 With most current technology, GPU utilization starts to fall at \u2248 $10^{28}$ floating point operations\n(FLOP), about three years away at recent trends (Epoch AI, 2024) of 4.2\u00d7 growth per year.\nCurrently-available specialized high-bandwidth inter-node interconnects can permit training runs\nabout two orders of magnitude larger (\u2248 $10^{30}$ FLOP), at which point latencies begin to worsen\nutilization, until reaching an absolute latency barrier at \u2248 $10^{31}$ FLOP, about seven years away."}, {"title": "\u0391 \u03a4\u039f\u03a5 MODEL: STACKED SPARSE LINEAR MLP BLOCKS", "content": "We first carefully define the class of models to be considered. Since the Transformer (Vaswani et al., 2017),\nincluding its sparse varieties (Fedus et al., 2022), is the dominant architecture today for frontier models, it\nseems a natural baseline. As we will show momentarily, the great majority of computation when training a\nTransformer occurs in its linear layers, so we adopt a simplified model consisting of these elements. This\napproach has the advantage that such layers also constitute the bulk of computation in many alternative\narchitectures (Peng et al. (2023) and Gu & Dao (2023) among others), so our model retains applicability\nto a wide variety of potential future algorithmic developments. However, the huge space of possible unex-\npected algorithmic developments precludes any watertight guarantees, should state-of-the-art architectures\nor learning algorithms change drastically.\nThe formal definitions we make are as follows: a sparse linear multi-layer perceptron (MLP) block\n(\u201cMLP block\u201d for short) consists of learnable weight matrices $W_f \\in \\mathbb{R}^{d_{ff}\\times d_{model}}$ and $W_i\\in \\mathbb{R}^{d_{model}\\times d_{ff}}$ for\nexpert indices $e$ in the range $1 < e < E$, where $E$ is the sparsity factor of the model.\nGiven a matrix of input vectors $X_{in} \\in \\mathbb{R}^{d_{model}\\times b}$, where $b$ is the batch size in units of tokens, a router\n$p: \\mathbb{R}^{d_{model}} \\to \\{1, . ., E\\}$ (assumed to be computationally inexpensive) chooses some expert index $p(X_k)$\nfor each token index $k < b$. The sparse linear MLP $f$ then yields an output matrix $X_{out} = f(X_{in}) \\in \\mathbb{R}^{d_{model}\\times b}$\nby mapping each token's input column $X_k$ to the corresponding output column $X_{out}$ via the weight matrices\ncorresponding to the chosen expert. If $\\index(e)$ indexes the set of tokens routed to expert $e$, then:\n$X_{out} = W_i (W_f X_{in}))$.\nWe optimistically assume balanced routing (Section 3.2.2): the same number $b/E$ of tokens is routed to\neach of the $E$ experts, so that during the forward pass every expert performs matrix multiplications of\nshapes $(d_{ff}, d_{model}) \\times (d_{model},b/E) \\to (d_{ff},b/E)$ and then $(d_{model}, d_{ff}) \\times (d_{ff},b/E) \\to (d_{model},b/E)$, each\nrequiring $d_{model}d_{ff}b/E$ multiply-accumulate (MAC) operations. Across both linear layers of all $E$ experts,\nthe MLP block's total arithmetic cost is $2d_{model}d_{ff}b$ MAC. We then assume $L$ MLP blocks in total, so that\nthe model's forward pass is their composition:\n$F(X) = f_L(f_{L-1}\u00b7\u00b7\u00b7(f_1(X)))).\nAcross these $L$ blocks, the model has\n$N_p = 2LEd_{model}d_{ff}$\nparameters in total and a forward pass on a batch size of $b$ requires $N_pb/E = 2Ld_{model}d_{ff}b$ MAC.\nDuring the backward pass, gradients of the loss $L$ must be computed for the inputs (activations and weights)\nof each matrix multiplication. A matrix multiplication $C = AB$ of shape $(I, K) \\times (K, J) \\to (I, J)$ on the\nforward pass becomes two matrix multiplications,\n$\\partial L/\\partial A = (\\partial L/\\partial C)B^T,$\n$\\partial L/\\partial B = A^T (\\partial L/\\partial C),$"}, {"title": "METHODS OF PARALLELISM", "content": "The workload of a training run can be distributed through a variety of methods (Weng & Brockman, 2022):\ndata parallelism (DP), tensor parallelism (TP), pipeline parallelism (PP), and expert parallelism (EP), corre-"}, {"title": "TENSOR SLICING AND ALL-REDUCE: DATA AND TENSOR PARALLELISM", "content": "Consider a forward pass matrix multiplication $C = AB$ of shape $(I, K) \\times (K, J) \\to (I, J)$ and its backward\npass computations $\\partial L/\\partial A = (\\partial L/\\partial C)B^T$ and $\\partial L/\\partial B = A^T(\\partial L/\\partial C)$ as described in Section 2. For\nexample, $A$ could be the weight matrix $W_f$ for an expert's first layer, and $B$ its input activations $X_{in}^{(e)}$, in\nwhich case the forward pass multiplication shape is $(d_{ff}, d_{model}) \\times (d_{model}, b/E)$, as discussed earlier."}, {"title": "ONE-DIMENSIONAL SLICING", "content": "A natural idea is to partition the data (and associated work) along one of the dimensions of size $I, J$, or $K$,\nwhile replicating the other dimensions. When this is the expert's batch dimension of size $b/E$, we call this\ndata parallelism, and when it's one of the layer shape dimensions $d_{model}$ or $d_{ff}$, we call it tensor parallelism.\nFundamentally they require the same communication pattern, which we now describe in general terms.\nIf the data is partitioned along the \u201cinternal\u201d $K$-sized dimension into $K'$-sized chunks across $N_K = K/K'$\nGPUs, and the computation partitioned accordingly, then the $n^{th}$ GPU performs a matrix multiplication of\nshape $(I, K') \\times (K', J) \\to (I, J)$, defined by:\n$C_{ij} = \\sum_{k=nK'}^{(n+1)K'-1} a_{ik}b_{kj}$\nEach GPU thus has only a partially-reduced value for each component of $C$, but requires the fully-reduced\ncomponent $C_{ij} = c_1 + c_2 + ... + c_{N_K -1}$ so that it may proceed with the next computation step. This\nnecessitates an all-reduce collective communication across the $N_K$ GPUs. (Kumar et al., 1994)\nWe can derive the minimal inter-GPU data movement volume for this all-reduce, assuming that messages\ncontain the single-word partial sum accumulated so far by the transmitting GPU. Given any total ordering\nof messages consistent with their causal partial ordering, the $(N_K \u2013 1)^{th}$ message is the first whose receiver\ncan possibly lie causally downstream of all other GPUs and therefore contain the fully-reduced component\n$C_{ij}$. The other $N_K \u2013 1$ GPUs must then also receive at least one additional message, so there must be at least\n$2(N_K \u2013 1)$ messages, and hence words, received in total.\nThat is, to all-reduce a single word across $N_K$ GPUs, each GPU must receive\n$2(N_K - 1)/N_K \\approx 2$"}, {"title": "MULTI-DIMENSIONAL SLICING", "content": "In general, we may partition the data across $N = N_IN_JN_K$ GPUs, with each performing a forward pass\nmultiplication of shape $(I', K') \\times (K', J') \\to (I', J')$, where as before we define $I' = I/N_I$, etc.\nBecause communication in the forward pass happens only along the \"inner\" $K$-sized dimension, this\ncan be treated as $N_IN_J$ independent matrix multiplications, entailing (as derived above) $2I'J'(N_K \u2212 1)$\nwords of inter-GPU data movement each. Across all independent multiplications, this works out to\n$2(N_II')(N_JJ')(N_K \u2212 1) = 2IJ(N_K \u2212 1)$ words of inter-GPU data movement in the forward pass, ex-\nactly as in the $K$-only parallelism case.\nWe find similarly for backward pass communication across the $I$ and $J$ dimensions, so that:\ntotal inter-GPU data movement = $2[IJ(N_K \u2212 1) + KJ(N_I \u2212 1) + IK(N_J \u2212 1)]$ words."}, {"title": "DATA PARALLELISM", "content": "In this case, each expert's activations $X_{in}^{(e)}$ and $X_{out}^{(e)}$ (as well as intermediate states $W_iX_{in}^{(e)}$) are sliced\n$N_{DP}$-way along the $b/E$-sized batch dimension. This becomes a reduction dimension in the backward pass\nwhen computing gradients with respect to the (replicated) weight matrices $W_f$ and $W_i$ (via multiplications\nof shapes $(d_{ff}, b/E) \\times (b/E, d_{model})$ and $(d_{model}, b/E) \\times (b/E, d_{ff}))$, so data parallelism necessitates an all-\nreduce of weight gradients across the batch dimension, with inter-GPU data movement per weight matrix of\n$2d_{ff}d_{model} (N_{DP} - 1)$ words per batch, as can be seen by applying Eq. 3 to this case."}, {"title": "TENSOR PARALLELISM", "content": "In this case, an expert's activations and weights are sliced into $N_{TP} = N_{TP, ff} \\times N_{TP, model}$ partitions,\nalong the $d_{ff}$ and $d_{model}$ dimensions. The $d_{model}$ dimension is internal in the first layer's forward pass of\nshape $(d_{ff}, d_{model}) \\times (d_{model}, b/E)$, as well as the second layer's activation gradient computation of shape\n$(d_{ff}, d_{model}) \\times (d_{model}, b/E)$. Symmetrically, the $d_{ff}$ dimension is internal in the second layer's forward pass\nand the first layer's activation gradient computation. Applying Eq. 3, we see $2(b/E)[d_{ff}(N_{TP, model} - 1) +$\n$d_{model} (N_{TP, ff} -1)]$ words of inter-GPU data movement for each of an expert's two layers. Aggregating across\nthe $L$ MLP blocks, $E$ experts per block, and two layers per expert, we have total inter-GPU data movement\nof\n$4Lb[d_{ff} (N_{TP, model} - 1) + d_{model} (N_{TP, ff} - 1)]/N_{TP}$\nwords per batch per tensor-parallel worker.\nFrom our general analysis above, we see that when $N_{TP}$ is small (the norm today), the smaller dimension\n$d_{model}$ goes un-partitioned ($N_{TP, model} = 1$), yielding 1D tensor parallelism, but that otherwise (as for the\nmuch larger training runs which are our focus), 2D tensor parallelism becomes optimal, with $d_{ff}/N_{TP, ff} \u2248$\n$d_{model}/N_{TP, model}$ (i.e. roughly square weight partitions). Solving this yields approximately\n$8Lbd_{model}d_{ff}/N_{TP}$"}, {"title": "POINT-TO-POINT: PIPELINE AND EXPERT PARALLELISM", "content": "We have so far examined partitioning the work along the per-expert batch dimension of size $b/E$ (data\nparallelism) and the two weight matrix dimensions of size $d_{model}$ and $d_{ff}$ (tensor parallelism). Two problem\ndimensions remain: we can also partition depth-wise across the $L$ MLP blocks with pipeline parallelism,\nand/or across the $E$ experts with expert parallelism. In this case, a given token at a given layer is processed\nby the GPU (or GPUs, if combined with tensor parallelism) corresponding to that layer's pipeline stage and\nthat token's routed expert. As the token moves through the layers during the forward and backward passes,\nits activations are transferred in a simple point-to-point communication of $d_{model}$ words whenever crossing\npipeline- or expert-parallel ranks. Because of the disjunctive nature of this condition, the point-to-point\ncommunication cannot always be attributed solely to one or the other method of parallelism, and a joint\nanalysis is warranted."}, {"title": "PIPELINE PARALLELISM", "content": "In a naive pipeline-parallel configuration, the first pipeline stage is assigned the first $L/N_{pp}$ layers, the\nsecond is assigned the next $L/N_{pp}$ layers, et cetera. Consequently, in each forward and backward pass, a\nvector of size $d_{model}$ has to be communicated a total of $N_{pp} - 1$ times. The total data movement cost across\nall GPUs is simply $d_{model}(N_{PP} \u2013 1)$ activations per forward or backward pass per token.\nBecause model layers are inherently serial, pipeline parallelism imposes a unique challenge not encountered\nby the other forms \u2013 data, tensor, and expert \u2013 of parallelism: minimizing the so-called pipeline bubble,\nthe time GPUs spend idle waiting for activations from other pipeline stages. If the entire batch were sent\nthrough the pipeline all together, then at any given moment only one of the $N_{pp}$ pipeline stages would ever\nbe active, and the bubble fraction (proportion of time spent idle) would equal $1 - 1/N_{pp}$. So instead, the\nbatch must be sliced further than whatever is imposed by data parallelism alone, into smaller pieces called\nmicrobatches, which travel individually through the pipeline according to a pipeline schedule in which,\nideally, the large majority of pipeline stages are active most of the time. Narayanan et al. (2021) notes that\na naive pipelining strategy (Fig. 4) with $m$ microbatches and $N_{pp}$-way pipeline parallelism incurs a bubble"}, {"title": "5D PARALLELISM", "content": "5D parallelism involves combining all of the parallelism methods we've discussed so far: data, tensor (both\ndimensions), pipeline, and expert parallelism. Table 1 summarizes the network communication costs of\nthese methods on an equal footing: how much communication they require per gradient step taken.\nTo see why combining multiple parallelism methods is more efficient than using any one method of paral-\nlelism by itself, consider the isolated problem of minimizing the total network bandwidth cost for a cluster\ngiven a fixed cluster size $N_{GPU} = N_{TP} N_{PP} N_{DP}$ for a dense training run (so that $E, N_{EP} = 1$). The total\nbandwidth cost for $i \\$\\neq$ 1 can be expressed as\n$\\approx 2N_p(N_{DP} - 1) + 2bd_{model}(N_{PP} \u2212 1) + 8bL\\sqrt{d_{ff}d_{model}}N_{TP} = C_{DP}N_{DP} + C_{PP}N_{PP} + C_{TP}\\sqrt{N_{TP}} - d, $"}, {"title": "WHAT CONSTRAINS DISTRIBUTED TRAINING?", "content": "Having discussed the available parallelism methods, we now turn to the main subject of our paper: the\nobstacles we face if we try to scale some combination of these methods arbitrarily far. Because all these\nmethods incur data movement costs, the arithmetic utilization of the GPUs can fall if a training run becomes\nbottlenecked by this data movement. We reiterate our key questions:\nQ1 Given present-day algorithms, GPUs, and interconnects, what is the biggest training run that can be\nperformed within a fixed duration, before intra- and inter-GPU data movement starts to seriously\nworsen utilization or even render it impossible?\nQ2 How far might this limit be extended, and what algorithmic or hardware progress can achieve that?"}, {"title": "DATA MOVEMENT", "content": "Distributed training can run up against data movement limits for two reasons: because we're moving too\nmuch data inside an individual GPU or between different GPUs. We consider each in turn."}, {"title": "INTRA-GPU DATA MOVEMENT", "content": "A typical GPU with tensor cores can compute much faster than it can move data to and from DRAM: for\nexample, an NVIDIA H100 SXM (NVIDIA, 2022) can perform a theoretical maximum of 2 \u00d7 $10^{15}$ FLOP/s\n(1 \u00d7 $10^{15}$ multiply-accumulates (MAC)/s) at 8-bit precision during dense matrix multiplications, but has a\nDRAM memory bandwidth of only 3.35 TB/s, for an arithmetic intensity (the ratio of the two) of \u2248 299\nMAC/byte. An 8-bit matrix multiplication of shape $(M, K) \u00d7 (K, N) \\to (M \u00d7 N)$ must perform MKN\nMAC and, under ideal caching, read the two input matrices and write the output matrix once each, for\nMK + KN + MN bytes accessed total. For the H100's computational resources to be balanced, we thus\nmust have 1/299 \u2248 (MK + KN + MN)/MNK = 1/M + 1/N + 1/K. In the square case,\n$M = N = K \\approx 896$.\nIf the dimensions are substantially smaller, then the tensor cores must go underutilized as they will be\nbottlenecked on data movement to and from DRAM. Similar considerations apply at lower levels of the\nmemory hierarchy as well. We discuss this further in Section 5.1 and Appendix 8.1.1.\nDistributed training across more GPUs or experts requires splitting the problem and hence reducing at least\none of $M, N, K$, worsening arithmetic intensity.\nThe main way to counter the degradation in arithmetic intensity is by increasing the dimensions of the\nmatrix multiplications: at least one of $d_{model}, d_{ff}, b$ (Section 2), either making the model fatter and shorter,\nor increasing the batch size. Scaling $b$ runs into the critical batch size limit, and it's not clear how far scaling\nmodel width $d_{model}$ and $d_{ff}$ at the expense of model depth $L$ can go while remaining near the compute-optimal\nfrontier. We discuss these matters in greater detail in Sections 4.2 and 4.4. The broad conclusion is there\nmay be no free lunch for arithmetic intensity."}, {"title": "INTER-GPU DATA MOVEMENT", "content": "In addition to data movement inside individual GPUs, distributed training requires the movement of data\nbetween them. We quantified this cost in Section 3.\nEach GPU has an upper bound to the rate of information it can receive or transmit per unit time. This means\nthat asymptotically, the scaling of the total network bandwidth of a cluster can at most be linear in cluster\nsize, even if the interconnect switches, wires, et cetera are costless. In practice, scaling is often sublinear.\nFor instance, a high-bandwidth NVLink region may be limited to a single node: GPT-4 (OpenAI (2023))\nused clusters of A100s (Patel & Wong, 2023) with an NVLink node size of 8. Increasing the tensor-parallel\ndegree $N_{TP}$ for such a cluster thus demands some fraction of bandwidth-hungry all-reduce communication\nover slower interconnects such as InfiniBand."}, {"title": "THE CRITICAL BATCH SIZE", "content": "Because data and pipeline parallelism both involve slicing the data matrix along the batch dimension", "noise scale": "t which the dependence of $\\Delta L$ on\n$b$ goes from linear to sub-linear. In this model", "truth\ndistribution": "a model with a smaller Kullback-Leibler divergence with the ground truth distribution can\nonly be distinguished from ground truth by drawing more samples. Knowing which direction to go in for\na useful gradient step certainly requires detecting that the model is not already optimal, so models with a\nsmaller reducible loss per token can accommodate larger batch sizes (in units of tokens). This information-\ntheoretic argument also makes the quantitative prediction that the noise scale $b_{noise}$ should vary inversely"}]}