{"title": "ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal Federated Learning in Brain Imaging Analysis", "authors": ["Xinpeng Wang", "Rong Zhou", "Han Xie", "Xiaoying Tang", "Lifang He", "Carl Yang"], "abstract": "Multimodal Federated Learning (MFL) has emerged as a promising approach for collaboratively training multimodal models across distributed clients, particularly in healthcare domains. In the context of brain imaging analysis, modality incompleteness presents a significant challenge, where some institutions may lack specific imaging modalities (e.g., PET, MRI, or CT) due to privacy concerns, device limitations, or data availability issues. While existing work typically assumes modality completeness or oversimplifies missing-modality scenarios, we simulate a more realistic setting by considering both client-level and instance-level modality incompleteness in this study. Building on this realistic simulation, we propose ClusMFL, a novel MFL framework that leverages feature clustering for cross-institutional brain imaging analysis under modality incompleteness. Specifically, ClusMFL utilizes the FINCH algorithm to construct a pool of cluster centers for the feature embeddings of each modality-label pair, effectively capturing fine-grained data distributions. These cluster centers are then used for feature alignment within each modality through supervised contrastive learning, while also acting as proxies for missing modalities, allowing cross-modal knowledge transfer. Furthermore, ClusMFL employs a modality-aware aggregation strategy, further enhancing the model's performance in scenarios with severe modality incompleteness. We evaluate the proposed framework on the ADNI dataset, utilizing structural MRI and PET scans. Extensive experimental results demonstrate that ClusMFL achieves state-of-the-art performance compared to various baseline methods across varying levels of modality incompleteness, providing a scalable solution for cross-institutional brain imaging analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal Federated Learning (MFL) has emerged as a transformative approach for collaboratively training machine learning models across distributed clients with multimodal data, especially in privacy-sensitive domains like healthcare [1]\u2013[3]. By integrating diverse data modalities, MFL facilitates the development of robust and accurate models for intelligent clinical decision support systems. However, real-world healthcare applications, particularly in cross-institutional brain imaging analysis, often face the challenge of modality incompleteness [4]. Specifically, some institutions may have access solely to PET imaging data, while others may have access only to MRI data. This disparity in available modalities significantly undermines the performance of traditional federated learning frameworks, which typically assume that all clients have access to the same set of modalities.\nTo address this challenge, researchers have proposed various methods to mitigate the impact of missing modalities in MFL. These methods can be broadly categorized into prototype-based approaches and generative approaches, each with distinct strengths and limitations. Prototype-based methods [4]\u2013[6] leverage the average of feature embeddings within the same class to serve as prototypes for feature alignment or modality completion. While this approach is effective in capturing general class-level characteristics, it struggles to encapsulate the inherent diversity and complexity of individual data points [7]\u2013[9]. As a result, the simplistic averaging process often produces prototypes that inadequately represent the underlying data distribution, leading to suboptimal alignment of modality-specific feature embeddings and ultimately degrading the global model's performance.\nOn the other hand, generative approaches attempt to reconstruct missing modalities using generative models, which hold promise for improving data completeness [10]\u2013[12]. However, these generative methods typically require modality-complete instances for effective training, yet such instances are often scarce in real-world settings of modality-incomplete MFL [11]. This limitation restricts their applicability to incomplete data and increases the likelihood of introducing noise or inaccuracies during the reconstruction process. Moreover, methods based on Generative Adversarial Networks (GANs) [13] are particularly susceptible to mode collapse, where the model fails to capture the full diversity of the data, leading to the generation of highly similar, non-representative samples [14], [15]. These limitations render generative approaches less robust and reliable in practical applications of modality-incomplete MFL.\nBeyond the limitations of prototype-based and generative approaches in addressing modality incompleteness in MFL, existing studies on modality-incomplete MFL share several common shortcomings from the perspective of federated learning. One such limitation is the unrealistic simulation of MFL scenarios, which assumes uniform modality availability or consistent missing patterns across all clients. These over-simplified simulations fail to accurately represent real-world modality incompleteness scenarios, thereby limiting their practical applicability, particularly in cross-institutional brain imaging analysis [16]. Furthermore, much of the current work on MFL with missing modalities employs uniform"}, {"title": "II. MODALITY INCOMPLETENESS SETTING", "content": "aggregation weights for different modules within the model during the federated learning process [17]. This uniform approach overlooks the varying modality distributions across clients, resulting in suboptimal global model performance.\nTo more accurately reflect the modality incompleteness in real-world scenarios of cross-institutional brain imaging analysis, this paper introduces a realistic and comprehensive setting for modality distribution. Unlike prior studies that assume uniform modality availability or consistent missing modalities across clients, our setting simulates both client-level and instance-level modality incompleteness, as illustrated in Fig. 1, capturing the diversity and complexity of modality distribution in real-world applications.\nBuilding on this simulation, we propose ClusMFL, a novel framework designed to address the challenges of modality incompleteness in MFL. Concretely, we apply the FINCH clustering algorithm [18] to cluster feature embeddings and construct a pool of cluster centers for each pair of modality and label across clients, providing a finer-grained representation of data distributions compared to traditional prototype-based methods. To ensure that modality-specific encoders accurately extract modality-specific features related to the labels, we perform feature alignment by employing supervised contrastive loss [19] over local feature embeddings and the pool of cluster centers. Furthermore, to mitigate the impact of severe modality incompleteness, we use these cluster centers as proxies for the feature embeddings of the missing modalities during the training process, enabling cross-modality knowledge transfer. In addition, we utilize a modality-aware aggregation method that assigns different aggregation weights to various modules of the model based on the modality distributions, effectively balancing the contributions from each client to different modules.\nTo evaluate the effectiveness of the proposed framework, we conduct extensive experiments under various settings of modality incompleteness and compare ClusMFL with several traditional federated learning algorithms, as well as algorithms specifically designed for modality-incomplete MFL. Specifically, we use brain imaging data from the real-world ADNI dataset, which includes structural MRI and PET scans for 915 participants stratified into three diagnostic categories: healthy controls, mild cognitive impairment (MCI), and Alzheimer's disease (AD) patients. By simulating realistic settings of modality incompleteness on this dataset, we aim to evaluate our framework under conditions that reflect real-world clinical challenges. The results show that ClusMFL outperforms existing approaches, particularly in scenarios with substantial modality incompleteness, underscoring its capability to address real-world challenges in MFL.\nIn this study, we focus on two representative brain imaging modalities: PET and MRI, and simulate a more realistic setting by considering both client-level and instance-level modality incompleteness. Each instance is represented as a triplet $(x_P, x_M, y)$, where $x_P$ corresponds to data from the PET modality, $x_M$ corresponds to data from the MRI"}, {"title": "III. METHOD", "content": "modality, and $y$ represents the associated label. Based on the availability of modalities, data instances are categorized into the following three types, as shown in Fig. 1:\n1) PET-only instances: These instances contain data\nsolely from the PET modality, with no corresponding\nMRI data. Such instances are formally denoted as\n$d_P = (x_P, \\O, y)$.\n2) MRI-only instances: These instances exclusively con-\ntain data from the MRI modality, with no accom-\npanying PET data. These are represented as $d_M =$\n$(\\O, x_M, y)$.\n3) Multimodal instances: These instances include data\nfrom both modalities, and are represented as $d_B =$\n$(x_P, x_M, y)$.\nAt the client level, we categorize clients into three groups\nbased on the composition of their instances:\n1) PET-only clients: These clients exclusively host PET-\nonly instances. Their datasets are denoted as $D_P =$\n$\\left\\{d_P^i\\right\\}_{i=1}^{n}$ , where $n$ is the number of instances.\n2) MRI-only clients: These clients solely host MRI-only\ninstances, denoted as $D_M = \\left\\{d_M^i\\right\\}_{i=1}^{n}$\n3) Multimodal clients: These clients contain a mix of all\nthree types of instances. Their datasets are represented\nas:\n$D_B = \\left\\{d_P^i\\right\\}_{i=1}^{\\beta_1 n} \\cup \\left\\{d_M^i\\right\\}_{i=1}^{\\beta_2 n} \\cup \\left\\{d_B^i\\right\\}_{i=1}^{(1-\\beta_1-\\beta_2)n}$,\nwhere $\\beta_1$ and $\\beta_2$ indicate the proportions of PET-\nonly and MRI-only instances on multimodal clients,\nrespectively, while $1 - \\beta_1 - \\beta_2$ denotes the proportion\nof multimodal instances on multimodal clients.\nFor client $i$, we denote the number of instances on this\nclient as $n_i$. The overall dataset is defined as follows:\n$\\mathbb{D} = \\bigcup_{i=1}^{\\alpha_1 N} \\mathbb{D}_P^i \\cup \\bigcup_{i=1}^{\\alpha_2 N} \\mathbb{D}_M^i \\cup \\bigcup_{i=1}^{(1-\\alpha_1-\\alpha_2)N} \\mathbb{D}_B^i,$\nwhere $\\alpha_1$, $\\alpha_2$, and $1 - \\alpha_1 - \\alpha_2$ represent the proportions\nof PET-only, MRI-only, and multimodal clients, respectively,\nand $N$ denotes the total number of clients participating in the\nfederated learning framework.", "subsections": [{"title": "A. Preliminary", "content": "In this study, we adopt a typical architecture for multimodal models, which includes two encoders-one for each modality-and a classifier. Let $f_P$ and $f_M$ denote the encoders for PET modality and MRI modality, respectively. The encoders $f_P$ and $f_M$ are responsible for extracting the relevant features from each modality. The model also includes a classifier, denoted as $g$, which concatenates the embeddings from the encoders and performs the final prediction.\nDuring the inference stage, for instances containing both modalities, the input data $x_P$ and $x_M$ are processed through their respective encoders $f_P$ and $f_M$. Specifically, the feature embeddings are obtained as $z_P = f_P(x_P)$ and $z_M = f_M(x_M)$, which are subsequently concatenated and passed to the classifier $g$ for the final prediction, $\\hat{y} = g(z_P, z_M)$. For instances with only a single modality, the feature embedding of the missing modality is replaced with a tensor of zeros, denoted as $0$.\nThe federated training process repeats the construction of the pool of cluster centers and cluster sizes (i.e., $C^{\\text{global}}$ and $S^{\\text{global}}$), followed by local training and aggregation in each round, which are explained in detail in the following sections. We provide an overview of the construction of $C^{\\text{global}}$ and $S^{\\text{global}}$ and local training in Fig. 2."}, {"title": "B. Constructing The Pool of Cluster Centers", "content": "Unlike traditional prototype learning methods, which use the mean of features as the prototype and result in a shift between individual samples and the prototype, this study adopts the FINCH [18] algorithm to construct a pool of cluster centers, thereby more effectively representing the clients' data distribution.\nOn each client, we begin by applying the FINCH clustering algorithm to calculate the local cluster centers for feature embeddings of each pair of modality and label. The resulting cluster centers, along with cluster sizes, are then uploaded to the server. For instance, consider the modality PET. The feature embeddings associated with modality PET are extracted as $Z_P = f_P(X_P)$, where $X_P$ represents all PET data on this client. The FINCH algorithm is then applied to $Z_P$ for clustering, yielding the corresponding cluster centers and cluster sizes. Specifically, for label $j$ in modality PET, FINCH identifies the cluster centers as:\n$(C_{P,j}, S_{P,j}) = \\text{FINCH}(Z_{P,j}),$\nwhere $Z_{P,j}$ denotes the feature embeddings with modality PET corresponding to label $j$, and $C_{P,j} = \\{c_1^{P,j}, c_2^{P,j}, ..., c_{K_{P,j}}^{P,j}\\}$ represents the $K_{P,j}$ cluster centers obtained from the FINCH algorithm for label $j$. The set $S_{P,j} = \\{s_1^{P,j}, s_2^{P,j}, ..., s_{K_{P,j}}^{P,j}\\}$ represents the sizes of the corresponding clusters, where each $s_k^{P,j}$ denotes the number of feature embeddings assigned to the $k$-th cluster for label $j$, with $c_k^{P,j}$ being the cluster center. $K_{P,j}$ represents the number of clusters, which is determined automatically by FINCH.\nFor client $i$, the cluster centers and sizes associated with modality PET and label $j$ are denoted as $C_{P,j}^i$ and $S_{P,j}^i$, respectively. If client $i$ lacks data for modality PET, we set $C_{P,j}^i = \\O$ and $S_{P,j}^i = \\O$.\nOnce the cluster centers and cluster sizes for each client are computed, they are sent to the server, where they are collected to form a global pool of cluster centers. Specifically, the global pool of cluster centers for label $j$ in modality PET, denoted as $C_{P,j}^{\\text{global}}$, is constructed by concatenating the cluster centers from all clients:"}, {"title": "C. Feature Alignment", "content": "$C_{P,j}^{\\text{global}} = \\bigcup_{i=1}^{N} C_{P,j}^i$.\nSimilarly, the corresponding cluster sizes for label $j$ are concatenated into a global pool, denoted as $S_{P,j}^{\\text{global}}$.\n$S_{P,j}^{\\text{global}} = \\bigcup_{i=1}^{N} S_{P,j}^i$.\nThe global pools $C_{P,j}^{\\text{global}}$ and $S_{P,j}^{\\text{global}}$ encapsulate the aggregated cluster centers and their respective sizes for each label $j$ across all clients, enabling the model to leverage a comprehensive representation of the data distribution. These global pools are then distributed back to each client, allowing them to utilize the global information during local training.\nThe same process is performed for MRI modality. Let $C_{M,j}^i$ and $S_{M,j}^i$ represent the cluster centers and cluster sizes, respectively, for MRI and label $j$ on client $i$. The global pools for MRI are constructed as:\n$C_{M,j}^{\\text{global}} = \\bigcup_{i=1}^{N} C_{M,j}^i, S_{M,j}^{\\text{global}} = \\bigcup_{i=1}^{N} S_{M,j}^i$.\nThese global pools for MRI are also distributed back to the clients to ensure that the global knowledge from both modalities is available for further training and optimization.\nTo ensure that the encoders $f_M$ and $f_P$ extract the correct modality-specific features and mitigate overfitting under severe modality incompleteness, we combine global cluster centers and local feature embeddings and apply supervised contrastive loss for alignment.\nFor any multimodal client with dataset $\\mathbb{D}_B$, let $X_P$ and $X_M$ represent all PET and MRI data on this client, respectively, along with their corresponding labels $y_P$ and $y_M$. The feature embeddings for PET modality and MRI modality are computed as $Z_P = f_P(X_P)$ and $Z_M = f_M(X_M)$ respectively. To incorporate global information, the feature embeddings $Z_P$ are concatenated with the global cluster centers $C_{P,j}^{\\text{global}}$.\n$Z_{P,G} = Z_P \\bigcup_{j=1}^{J} C_{P,j}^{\\text{global}}$,\nwhere $J$ is the total number of unique labels. Similarly, the label $y_P$ is extended as:\n$Y_{P,G} = \\underbrace{y_P \\bigcup_{j=1}^{J} \\{j\\} C_{P,j}^{\\text{global}}}_{C_{P,j}^{\\text{global}} \\text{ times}}$,\nwhere $\\{j\\}$ denotes a list containing label $j$ repeated $C_{P,j}^{\\text{global}}$ times.\nWe compute the supervised contrastive loss over $Z_{P,G}$. Let $\\mathbb{I} = \\{1, 2, ..., |y_{P,G}|\\}$ denote the index set, and define $A(i) = \\mathbb{I} \\setminus \\{i\\}$ as the set of all indices excluding $i$. The supervised contrastive loss for PET modality is defined as:\n$\\mathcal{L}_{ctr}^{\\text{P}} (X_P) = \\mathbb{E}_{i \\in \\mathbb{I}} [-\\frac{1}{|P(i)|} \\sum_{p \\in P(i)} \\log \\frac{v_{i,p}}{\\sum_{a \\in A(i)} v_{i,a}}]$,\nwhere:\n$\\bullet$ $P(i) = \\{p \\in A(i) | Y_i = Y_{P,G}\\}, \\text{represents the set of}$ indices corresponding to positive examples for $Z_{P,G}^i$.\n$\\bullet$ $v_{i,p} = \\exp(\\text{sim}(Z_{P,G}^i, Z_{P,G}^p) / \\tau)$, where $\\text{sim}(\\cdot, \\cdot)$ denotes the cosine similarity between two embeddings, and $\\tau$ is a temperature parameter.\nThe similar process is applied to calculate $\\mathcal{L}_{ctr}^{\\text{M}} (X_M)$ and the overall supervised contrastive loss for $\\mathbb{D}_B$ is then computed as:\n$\\mathcal{L}_{CTR}(\\mathbb{D}_B) = \\frac{|y_P| \\cdot \\mathcal{L}_{ctr}^{\\text{P}} (X_P) + |y_M| \\cdot \\mathcal{L}_{ctr}^{\\text{M}} (X_M)}{|y_P| + |y_M|}$\nFor clients with data from only one modality, the loss is computed solely for the corresponding modality, without involving the other modality."}, {"title": "D. Modality Completion Loss", "content": "To further enhance the model's classification performance in the presence of missing modalities, we use $C^{\\text{global}}$ as approximations for feature embeddings of the missing modality. Specifically, for a PET-only instance with data $d_P = (x_P, \\O, y)$ or an MRI-only instance with data $d_M = (\\O, x_M, y)$, we first pass the available modality through the corresponding encoder to obtain the feature embedding $z_P$ and $z_M$. We then approximate the missing modality using the $i$-th cluster center from the corresponding global cluster centers. The prediction for each instance is given by:\n$\\hat{Y} = \\begin{cases} g(z_P, C_{M,y}^{\\text{global, (i)}}) & \\text{for PET-only instance}, \\\\ g(C_{P,y}^{\\text{global, (i)}}, z_M) & \\text{for MRI-only instance}, \\end{cases}$\nwhere $C_{P,y}^{\\text{global, (i)}}$ and $C_{M,y}^{\\text{global, (i)}}$ represents the $i$-th cluster center from $C_{P,y}^{\\text{global}}$ and $C_{M,y}^{\\text{global}}$ respectively. The loss for a PET-only instance is then computed as:\n$\\mathcal{L}_{mc}(d_P) = - \\frac{1}{\\mathcal{T}_{M,y}} \\sum_{i=1}^{|S_{M,y}^{\\text{global}}| \\\\} S_{M,y}^{\\text{global, (i)}} \\cdot \\mathcal{L}_{CE} (Y, \\hat{Y}),$\nwhere $\\mathcal{L}_{CE}$ denotes the cross-entropy loss function, and $\\mathcal{T}_{M,y} = \\sum_{i=1}^{|S_{M,y}^{\\text{global}}|} S_{M,y}^{\\text{global, (i)}}$. Similarly, for an MRI-only instance, the loss is computed as:\n$\\mathcal{L}_{mc}(d_M) = - \\frac{1}{\\mathcal{T}_{P,y}} \\sum_{i=1}^{|S_{P,y}^{\\text{global}}| \\\\} S_{P,y}^{\\text{global, (i)}} \\cdot \\mathcal{L}_{CE} (Y, \\hat{Y}),$\nwhere $\\mathcal{T}_{P,y} = \\sum_{i=1}^{|S_{P,y}^{\\text{global}}|} S_{P,y}^{\\text{global, (i)}}.$"}, {"title": "E. Overall Loss Function", "content": "For any client $i$, let $\\mathbb{D}^{\\text{single}} = \\{d | d \\in \\mathbb{D}_i, d \\text{ is a single modality instance}\\}$ be the subset of instances with a single modality (either PET-only or MRI-only). The overall modality completion loss is calculated as:\n$\\mathcal{L}_{MC}(\\mathbb{D}_i) = \\mathbb{E}_{d \\in \\mathbb{D}^{\\text{single}}} [\\mathcal{L}_{mc}(d)].$\nIn this manner, we use the global cluster centers of the available modality to approximate the missing modality, allowing the model to still make meaningful predictions in the presence of incomplete data.\nFor client $i$ with dataset $\\mathbb{D}_i$, the prediction $\\hat{y}_r$ is generated as described in Section III-A for each instance $d_i$. Let $\\hat{y} = (\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_{n_i})$ denote the vector of predicted labels for the $n_i$ instances in the client's dataset, and $y = (y_1, y_2, ..., y_{n_i})$ denote the corresponding true labels. The overall loss is computed as:\n$\\mathcal{L}(\\mathbb{D}_i) = \\mathcal{L}_{CE}(\\hat{y}, y) + \\lambda_1 \\mathcal{L}_{CTR}(\\mathbb{D}_i) + \\lambda_2 \\mathcal{L}_{MC}(\\mathbb{D}_i),$\nwhere $\\lambda_1$ and $\\lambda_2$ are the regularization coefficients that balance the contributions of the contrastive loss and the modality completion loss, respectively."}, {"title": "F. Modality-Aware Aggregation", "content": "In this section, we describe the modality-aware aggregation (MAA) method adopted in our framework. Specifically, we use different aggregation weights for different modules within the model, based on the number of instances for each modality at each client.\nLet $n_p^i$ represent the number of instances with the PET modality at client $i$, and $n_m^i$ represent the number of instances with the MRI modality at client $i$. The total numbers of instances with PET and MRI modalities across all clients are denoted as $n_{\\text{total}}^p$ and $n_{\\text{total}}^m$, and are computed as:\n$n_{\\text{total}}^p = \\sum_{i=1}^{N} n_p^i, \\quad n_{\\text{total}}^m = \\sum_{i=1}^{N} n_m^i.$\nFor client $i$, the aggregation weights for the encoders are computed as:\n$w_p^i = \\frac{n_p^i}{n_{\\text{total}}^p}, \\quad w_m^i = \\frac{n_m^i}{n_{\\text{total}}^m},$\nwhere $w_p^i$ and $w_m^i$ are the aggregation weights for encoder $f_p$ and encoder $f_m$, respectively.\nThe aggregation processes for encoders $f_p$ and $f_m$ are as follows:\n$f_p^{\\text{global}} = \\sum_{i=1}^{N} w_p^i f_p^i, \\quad f_m^{\\text{global}} = \\sum_{i=1}^{N} w_m^i f_m^i,$\nwhere $f_p^i, f_m^i$ are the model parameters of the PET encoder and MRI encoder, respectively.\nFor the classifier $g$, the aggregation process is given by:\n$g^{\\text{global}} = \\frac{1}{\\sum_{i=1}^{N} n_i} \\sum_{i=1}^{N} n_i g^i,$\nwhere $g^i$ denotes the model parameters for classifier $g$."}]}, {"title": "IV. EXPERIMENT", "content": "A. Experimental Setup\nThe brain imaging dataset used in this study is sourced from the Alzheimer's Disease Neuroimaging Initiative (ADNI) public repository [20] and comprises 915 participants stratified into three diagnostic categories: 297 healthy controls (HC), 451 mild cognitive impairment (MCI), and 167 Alzheimer's disease (AD) patients. Multimodal neuroimaging acquisitions encompass structural Magnetic Resonance Imaging (VBM-MRI) and 18 F-florbetapir PET (AV45-PET), enabling examination of brain structure and amyloid plaque deposition, respectively.\nConsistent with established neuroimaging processing pipelines [21], [22], we preprocess neuroimaging data to region-of-interest (ROI) features from each participant's images. First, the multi-modality imaging scans are aligned to each participant's same visit. All imaging scans are aligned to a T1-weighted template image. Subsequently, the images are segmented into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) maps. They are normalized to the standard Montreal Neurological Institute (MNI) space as 2 \u00d7 2 \u00d7 2 mm\u00b3 voxels, being smoothed with an 8 mm full-width at half-maximum (FWHM) Gaussian kernel. We preprocess the structural MRI scans with voxel-based morphometry (VBM) by using the SPM software [23], and register the AV45-PET scans to the MNI space by SPM. For both MRI and PET scans, we parcellate the brain into 90 ROIs (excluding the cerebellum and vermis) based on the AAL-90 atlas [24], and computed ROI-level measures by averaging voxel-wise values within each region.\nIn the original dataset, each instance contains both modalities. We first split the dataset into a training set and a test set with a ratio of 1:4. For the test set, we ensure that the proportions of the three types of instances are equal, i.e., each type accounts for 1/3 of the total. In the federated learning setup, we distribute the training instances equally across clients while preserving label distribution. Each instance is then assigned a type (multimodal or single-modality) based on the MFL settings controlled by \u03b11, \u03b12, \u03b21, and \u03b22 as described in Section II. Single-modality instances drop the corresponding modality. For simplicity, we set \u03b11 = \u03b12 = \u03b1 and \u03b21 = \u03b22 = \u03b2.", "subsections": [{"title": "B. Main Results", "content": "To evaluate the effectiveness of our proposed method, we compare it against several baseline approaches, including FedAvg [25], FedProx [26], FedMed-GAN [10], FedMI [4], MFCPL [5], and PmcmFL [6]. FedAvg and FedProx are traditional federated learning algorithms, with FedAvg employing simple parameter averaging, while FedProx introduces a proximal term to the objective to mitigate client heterogeneity. FedMed-GAN employs CycleGAN [27] to complete missing modalities, enhancing diagnosis accuracy. FedMI, MFCPL, and PmcmFL leverage prototype learning to model class distributions and align features, effectively addressing incomplete modalities and heterogeneous data across clients. All methods are evaluated under the same experimental setup to ensure a fair comparison.\nTable I presents a comprehensive comparison of the performance of our proposed method against several baseline approaches. The experiments were conducted under varying configurations of \u03b2 and \u03b1, which denote modality incompleteness and client diversity, respectively.\nAs indicated in Table I, our method consistently outperforms the baseline approaches across different settings of modality incompleteness, achieving superior results in terms of precision, recall, F1 score, accuracy, and AUC. Notably, certain algorithms specifically designed for modality-incomplete MFL fail to outperform traditional federated learning methods, such as FedProx, in some scenarios.\nMoreover, as the values of \u03b1 and \u03b2 increase\u2014corresponding to a higher proportion of instances with only a single modality\u2014most of the baseline algorithms exhibit a noticeable decline in performance. In contrast, our proposed method demonstrates stable and robust performance, highlighting its effectiveness and resilience in handling varying degrees of modality incompleteness."}, {"title": "C. Ablation Study", "content": "In order to assess the contributions of each component, we conduct an ablation study under the setting of \u03b1 = 0.4 and \u03b2 = 0.2, as shown in Table II. The results demonstrate that applying modality-aware aggregation (MAA) alone yields lower performance across all metrics. Incorporating the contrastive loss ($\\mathcal{L}_{CTR}$) improves the results significantly, while the modality completion loss ($\\mathcal{L}_{MC}$) also leads to moderate gains. Combining both losses without MAA further enhances performance. The full method, which includes both MAA and the two loss functions, achieves the highest performance, with precision, recall, F1-score, accuracy, and AUC all showing notable improvements. These findings highlight the effectiveness of combining modality-aware aggregation with the contrastive and modality completion losses in addressing modality incompleteness in MFL."}, {"title": "D. Convergence Efficiency", "content": "To better analyze the convergence behavior of different algorithms in modality-incompleteness scenarios, we conduct experiments with four different random seeds in the setting of the first fold, with \u03b1 = 0.4 and \u03b2 = 0.2. The training curves, displayed in Fig. 3, represent the mean values across these"}]}, {"title": "V. CONCLUSION", "content": "random seeds, while the shaded areas indicate the standard deviation. From Fig. 3, we make the following observations:\n\u2022 F1 Score and AUC: Our method exhibits a rapid and consistent increase in both F1 score and AUC during the initial communication rounds, and subsequently stabilizes at higher values compared to the baseline methods. This indicates its superior ability to quickly capture relevant patterns. Furthermore, the consistent improvement in both metrics suggests that our approach enhances class discrimination more effectively as training progresses.\n\u2022 Test Loss: Our method achieves convergence with fewer communication rounds compared to the baseline methods. Specifically, it converges around the 11th communication round, whereas most baseline methods require approximately 20 rounds to reach convergence, highlighting the superior efficiency of our approach.\n\u2022 Time: Our method achieves significant performance improvements without a substantial increase in training time. Furthermore, the training time of our method remains significantly lower than that of FedMed-GAN, which requires adversarial training for GANs.\nIn this study, we propose ClusMFL, a novel framework designed to address modality incompleteness in multimodal federated learning. ClusMFL enhances local data representation with the FINCH clustering algorithm, mitigates missing modality effects through supervised contrastive loss and modality completion loss, and employs a modality-aware aggregation strategy to adaptively integrate client contributions. Extensive experiments on the ADNI dataset demonstrate that ClusMFL outperforms state-of-the-art methods, particularly in scenarios with severe modality incompleteness, though at a slightly higher computational cost. Future research will explore the incorporation of additional modalities to further enhance the proposed framework's applicability."}]}