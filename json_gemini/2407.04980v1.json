{"title": "Enabling Causal Discovery in Post-Nonlinear Models with Normalizing Flows", "authors": ["Nu Hoang", "Bao Duong", "Thin Nguyen"], "abstract": "Post-nonlinear (PNL) causal models stand out as a versatile and adaptable framework for modeling intricate causal relationships. However, accurately capturing the invertibility constraint required in PNL models remains challenging in existing studies. To address this problem, we introduce CAF-PoNo (Causal discovery via Normalizing Flows for Post-Nonlinear models), harnessing the power of the normalizing flows architecture to enforce the crucial invertibility constraint in PNL models. Through normalizing flows, our method precisely reconstructs the hidden noise, which plays a vital role in cause-effect identification through statistical independence testing. Furthermore, the proposed approach exhibits remarkable extensibility, as it can be seamlessly expanded to facilitate multivariate causal discovery via causal order identification, empowering us to efficiently unravel complex causal relationships. Extensive experimental evaluations on both simulated and real datasets consistently demonstrate that the proposed method outperforms several state-of-the-art approaches in both bivariate and multivariate causal discovery tasks.", "sections": [{"title": "1 Introduction", "content": "The need to uncover causal relationships from solely observational data where randomized controlled trials are impractical has led to the emergence of causal discovery methodologies as a crucial field of study. A key challenge in causal discovery stems from the non-uniqueness of causal models that can induce the same data distribution [31]. In other words, recovering the causal structure becomes impossible without making additional assumptions about the causal model. To overcome this, various functional causal models (FCMs) have been proposed with ensured identifiabilities, such as the linear non-Gaussian acyclic model (LiNGAM) [30], additive noise model (ANM) [11], and post-nonlinear (PNL) model [39]. While LiNGAM is limited to linear relationships, ANM adheres to the assumption of the additive noise with non-linear causal mechanisms.\nAmong them, the PNL model stands out for its generality in modeling complex non-linear causal systems. Specifically, under PNL models, the effect Y is generated from its cause X via the structural equation $Y := g (h(X) + e_X)$, where $e_X$ is an exogenous noise independent of the cause. Here, $h(.)$ can be any function but $g(.)$ is constrained to be invertible, which has consequently introduced critical challenges in modeling and estimation. Several approaches have been investigated to effectively estimate PNL models [39, 34, 14], yet representing the exact invertibility required by the model still poses a major difficulty. For example, AbPNL [34] adopts auto-encoders"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Causal Discovery under PNL Models", "content": "While various identifiable Structural Equation Models (SEMs) and estimation methods have been proposed to address the identifiability issue in causal discovery [30, 11, 39, 15, 13], our study focuses on the PNL model, which is one of the most flexible model representing complicated causal relationships. Nevertheless, causal discovery under PNL models is very challenging and under-studied due to the invertibility constraint.\nSeveral approaches have been proposed to address the challenge, yet they own certain limitations that hinder their ability to achieve higher performance. For instance, in [39], the PNL model is modeled by neural networks and estimated by minimizing the mutual information between the cause and noise. However, this approach ignores the invertibility constraint during the optimization process, which potentially results in invalid estimates that violate the invertibility requirement. To address this problem, AbPNL [34] leverages auto-encoders to represent \"approximately invertible\" functions, which are estimated by minimizing both the reconstruction loss and mutual information of the noise and the presumed cause. However, AbPNL suffers from two key limitations. Firstly, it lacks a guarantee of learning a truly invertible function, leading to failures in reconstructing the underlying noise in the effect variable, inevitably causing incorrect causal direction identification. Secondly, the loss function necessitates computing mutual information during training, resulting in significant computational costs, especially when dealing with multiple variables.\nMeanwhile, instead of jointly learning both $g(.)$ and $h(.)$, Keropyan et al. [14] employs ranked-based regression methods to separately learn the two transformations. Nevertheless, the study mainly focuses on linear settings for the inner function."}, {"title": "2.2 Multivariate Causal Discovery", "content": "Traditional causal discovery methods can be divided into three groups: constraint-based methods [31, 25, 3], score-based methods [5, 6, 10], and hybrid methods [33, 20]. Most of these methods search for the causal directed acyclic graph (DAG) within the combinatorial space of graph structure, which is known to be challenging due to the super-exponential explosion in complexity with respect to the number of variables [26]. This has changed in 2018 when Zheng et al. [41] introduced a novel approach known as NOTEARS that maps the discrete space to a continuous one, enabling the utilization of various gradient-based optimization techniques. This breakthrough opened up new possibilities for causal discovery by incorporating deep learning and continuous optimization methodologies [38, 18, 17].\nAlternatively, instead of searching the vast space of all possible causal structures, another line of research focuses on finding a causal topological order [27, 29, 37], accomplished by searching over the space of permutations, which is orders of magnitude smaller than the space of DAGs. By fixing a topological order, the acyclicity constraint is naturally ensured, eliminating the need for additional checks. After obtaining a causal topological order, an additional pruning step is necessary to remove spurious edges. This pruning process helps refine the inferred causal relationships by discarding unnecessary connections between variables."}, {"title": "2.3 Normalizing Flows in Causal Discovery", "content": "Normalizing flows [21] have emerged as a class of expressive generative models within deep learning, capable of effectively modeling complex probability distributions by learning invertible transformations. In the context of causal discovery, many studies have applied normalizing flows to encapsulate intricate data distributions that may arise in practice [24, 4, 16]. For instance, a recent study [15] have employed autogressive flows in the context of location-scale (LS) models. More specifically, the causal mechanism for LS models is defined as $y = g (x) e+h(x)$, which deviates significantly from PNL models. Based on the formulations of PNL and LS models, they cannot generalize over each other, meaning that the techniques used in [15] are not applicable to PNL models, as empirically confirmed in Section 5. To the best of our knowledge, normalizing flows have yet been studied for causal discovery under PNL models, where the invertible causal mechanism is of central importance, which highlights the significance of our study."}, {"title": "3 Preliminary", "content": "Suppose we observe an empirical dataset of n i.i.d. samples of the random vector $X = [x_1,x_2, ..., x_d]^\\mathsf{T}$ of d variables. We assume that the data is generated under the PNL model as follows\n$X_i := g_i (h_i (X_{Pa(i)}) + \\epsilon_i), i = 1, 2, ..., d$ (1)\nwhere $Pa(i)$ is a set of direct causes (or parents) of $x_i$. The noise variables {$\\epsilon_i$} are mutually independent and therefore $\\epsilon_i \\perp X_{pa(i)}$ for each i = 1, 2, ..., d. The corresponding causal graph, denoted as G, has directed edges $(j \\rightarrow i) : j \\in Pa(i)$ representing the causal relationships between variables. Similar to previous studies [14, 18, 27], this work also relies on assumptions of acyclicity and sufficiency. That means the causal graph G is a directed acyclic graph (DAG) and there is no hidden confounder. The goal of causal discovery is to infer the true DAG G based on the empirical data of X."}, {"title": "4 CAF-PoNo: Causal Discovery via Normalizing Flows for Post-Nonlinear Models", "content": "In this section, we first elaborate how to incorporate normalizing flows to design an effective estimator of PNL causal models. Then, we explain how to leverage this estimator for distinguishing cause and effect. Lastly, the framework is generalized to unravel the causal structure among multiple variables under PNL data."}, {"title": "4.1 Normalizing Flows for the PNL model", "content": "Normalizing flows is a powerful tool to express complex probability distributions from a simple base distribution through several invertible and differentiable transformations by exploiting the change of variables rule. The primary objective of normalizing flows research is to devise efficient transformations that possess the desirable properties of invertibility and differentiability with a tractable derivative/Jacobian for effectively applying the change of variables rule. For more details regarding normalizing flows, we refer to [21].\nHere, consider an empirical dataset of two variables (X, Y) generated by Eq. (2), we are interested in estimating the functions $g(.)$ and $h(.)$, so that we can recover the noise as\n$\\epsilon_y = g^{-1}(y) \u2013 h(x).$ (3)\nNormalizing flows are used in this study specifically to model the invertible function $g(\u00b7)$. In particular, we employ the cumulative distribution function (CDF) flow [21], which is a simple, yet efficient invertible transformation for one dimensional (scalar) variables. A CDF flow is a positively weighted combination of a set of CDFs of any arbitrarily positive density function. For example, the following mixture is a CDF, and is thus invertible:\n$f(x) = \\sum_{i=1}^{k} W_i\\Phi_i (x, \\mu_i, \\sigma_i),$ (4)\nwhere k is the number of components, $w_i \\geq 0, \\sum_{i=1}^{k} W_i = 1$ and $(\\mu_i, \\sigma_i)$ are the weights and parameters for each component, respectively, whereas $\\Phi_i$ is the cumulative distribution function of the i-th component. For simplicity, in our implementation, we consider a Gaussian distribution with mean $\\mu_i$ and variance $\\sigma_i$ for each component, yet it should be noted that our method is not restricted to Gaussian data but can be applied to more generic settings. Thanks to the universal approximation capability of Gaussian mixture models, the Gaussian mixture CDF flow can express any strictly monotonic $\\mathbb{R} \\rightarrow (0, 1)$ map with arbitrary precision [12].\nTo apply the CDF flow for PNL estimation, let $z = h(x) + \\epsilon_y$, then we have $y = g(z)$ and $z = g^{-1}(y)$. We use neural networks to parametrize the inner function h, while CDF flows are used to model $g^{-1}$ instead of g since the noise directly relates to $g^{-1}$ as shown in Eq. (3). To model $g^{-1}$, which is a $\\mathbb{R} \\rightarrow \\mathbb{R}$ map, we start with a CDF"}, {"title": "4.2 Cause-Effect Inference", "content": "When the PNL model is identifiable, by definition, there exists no reverse model $x = g_x(h_x(y) + e_x)$ such that $g_x$ is invertible and $Y \\perp e_x$ [39], which implies that the best model found by MLE in the anti-causal direction will have $e_x \\perp Y$, since invertibility is already guaranteed. This insight enables us to design an independence-based cause-effect identification method.\nMore specifically, to decide the causal direction, we fit a model corresponding to each direction $X \\rightarrow Y$ and $Y \\rightarrow X$ to the data, after which we obtain the estimated noises $\\hat{\\epsilon}_y$ and $\\hat{\\epsilon}_x$, respectively. Subsequently, the model that exhibits independence between the estimated noise and the putative cause is chosen as the more likely causal explanation. To quantify independence, we employ the Hilbert-Schmidt independence criterion (HSIC) [9] which is a popular kernel-based dependence measure that requires no parameter estimation. A higher HSIC value indicates a stronger dependence, while a lower value indicates a lower level of dependence. Consequently, the independence score is the negative value of the HSIC. The details of the proposed bivariate causal discovery algorithm is illustrated in Algorithm 1."}, {"title": "4.3 Extension to Multivariate Causal Discovery", "content": "An important aspect of our approach is that it is readily extendable to multiple variables, thus effectively addressing the challenges of the multivariate causal discovery problem.\nTowards this end, we decompose the causal structure learning task into two stages, where we first identify the causal ordering among the variables, then the edges are recovered with respect to said ordering. This approach ensures the acyclicity of the resultant graph, while attaining an efficient polynomial runtime."}, {"title": "4.3.1 Causal Ordering Identification", "content": "Definition 1. A causal ordering of a graph G is a non-unique permutation \u03c0 of d nodes such that a given node always precedes its descendants in \u03c0, i.e., $i <_\\pi j \\forall j\\in Des_G (i)$.\nEach permutation \u03c0 corresponds to a unique, fully connected DAG $G^\u03c0$ where every pair of nodes i j defines directed edges $(i \\rightarrow j)$ in $G^\u03c0$. Therefore, the graph G that we seek for is a subgraph of $G^\u03c0$ if \u03c0 is among the topological sorts of G. As a consequence, a pruning procedure needs to be performed to eliminate redundant edges from G.\nFollowing [35], we utilize the two following propositions to find one of causal orders of the underlying causal structure, by recursively detecting and excluding the sink node."}, {"title": "4.3.2 Pruning Method", "content": "To eliminate spurious edges and refine the causal structure, we employ an array of conditional independence tests based on a given permutation. In particular, we remove edges $(i\\rightarrow j)$ if $X_i \\perp X_j | X_{pre_{\\pi}(j)\\{i}}$ where $pre_{\\pi}(j)$ is a set of preceding variables in the causal ordering \u03c0 [36]. This pruning method does not depend on the SEM assumption, and yet is more flexible than the CAM pruning, a popular pruning method in other ordering-based studies [27, 29, 37], which is however only designed for generalized additive models. That being said, we also investigate both approaches comparatively in the Appendix.\nTo perform the conditional independence tests, we employ a recent state-of-the-art latent representation learning based conditional independence testing (LCIT) method [7], which does not make any parametric assumption and is shown to scale linearly with the sample size. It is worth mentioning that alternative conditional independence testing methods can also be employed during this pruning process. We summarize the pruning method in Algorithm 3. Together, Algorithm 2 and Algorithm 3 highlights the key steps for the CAF-PoNo method in multivariate causal discovery."}, {"title": "4.3.3 Complexity Analysis", "content": "The causal ordering identification in Algorithm 2 requires $O(d^2)$ iterations, each spending $O(nd)$ for the noise estimation using CAF-PoNo and $O(n^2d)$ for the sink score calculation using HSIC, leading to a total complexity of $O(n^2d^3)$. Similarly, the pruning step in Algorithm 3 performs $O(d^2)$ conditional independence tests, each costing O(nd), resulting to the final complexity of $O(nd^3)$."}, {"title": "5 Numerical Evaluations", "content": "To assess the quality of the proposed method, we compare it with state-of-the-art causal discovery models on two scenarios: bivariate and multivariate cause-effect inference, using both synthetic and real data, where the ground truth causal directions/structures are available.\nRegarding parameter selection in our method, for each dataset, the hyperparameters are chosen to maximize the AUC score, with values among the following sets:\n\u2022 The number of Gaussian components k: {4, 8, 10, 12}.\n\u2022 The hidden size of a single layer neural network: {16, 32, 64, 128}.\n\u2022 The number of epochs: {200, 300, 500, 700, 1000}.\nAll parameters are trained using the Adam optimizer with early stopping, learning rate of 0.001, and batch size of 128. We randomly"}, {"title": "5.1 Bivariate Causal Discovery", "content": ""}, {"title": "5.1.1 Baselines and the Evaluation Metric", "content": "We compare the bivariate causal discovery performance of the proposed method against several popular baselines as described below:\n\u2022 Additive noise model (ANM) [11] estimates the causal mechanism via Gaussian Process regressions and utilizes HSIC to assess the independence between the cause and residuals.\n\u2022 Post nonlinear (PNL) [39] reconstructs the noise using two neural networks trained to minimize the mutual information between the putative cause and the noise, and utilizes HSIC test for independence test between the cause and the estimated noise.\n\u2022 Causal Autoregressive Flow (CAREFL) [15] utilizes affine normalizing flows to estimate the affine causal model introduced in the paper.\n\u2022 Autoencoder-based post nonlinear (AbPNL) [34] models PNL using an auto-encoder trained with a combination of independence and reconstruction losses."}, {"title": "5.1.2 Synthetic data", "content": "We generate datasets that follow the PNL model in Eq. (2), considering both simple and complex functional relationships. Following [39, 34], simple functional relationships are captured in the Simple-PNL dataset, which is constructed using various function choices as outlined in Table 4 of the Appendix. We consider three different noise distributions: Gaussian, Uniform, and Laplace noises. For each distribution, we generate 500 pairs of (X, Y), each pair consisting of 1,000 i.i.d. samples. Furthermore, we introduce PNL-GP dataset with more intricate functional relationships within the PNL model where h (.) and g (.) are the weighted sum of Gaussian Processes and sigmoid functions, respectively. To assess CAF-PoNo's effectiveness beyond the PNL assumption, we also utilize synthetic datasets generated from non-linear additive noise (AN) and location-scale (LS) noise models, as provided by [32].\nThe results are presented in Table 1, demonstrating that CAF-PoNo outperforms other methods on all PNL datasets, achieving the highest AUC score overall. Notably, the results highlight the robustness of CAF-PoNo across various types of noise even though the base noise type is solely set as Gaussian, implying CAF-PoNo's effectiveness in addressing noise mis-specification. Interestingly, the results also reveal CAF-PoNo's capability in handling model mis-specification. In more details, CAF-PoNo attains an AUC of approximately 90% under LS data, demonstrating a comparable performance to LOCI, a method specifically designed for the LS scenario."}, {"title": "5.1.3 Real world data", "content": "To demonstrate the effectiveness of CAF-PoNo in real world scenarios, we utilize the T\u00fcbingen dataset [19], a common benchmark in bivariate causal discovery comprising of 99 cause-effect pairs from a variety of domains, e.g., abalone measurements, census income, and liver disorders. The number of samples for each dataset varies from 94 to 16,382.\nTable 4 illustrates the comparative performance in terms of AUC and Accuracy scores. The results clearly demonstrate that CAF-PoNo outperforms other baselines on both metrics when applied to real-world scenarios. Specifically, CAF-PoNo achieves the highest AUC, surpassing the runner-up RECI model by a significant margin of around 5%. Following prior studies [15, 13], we also provide the accuracy which is a common metric in bivariate causal discovery. As evidenced by the table, CAF-PoNo again accomplishes the highest accuracy of 71.71%, leaving the second-best method LOCI by a large margin of over 8%."}, {"title": "5.2 Multivariate Causal Discovery", "content": ""}, {"title": "5.2.1 Baselines and Evaluation Metrics", "content": "We consider the multivariate version of AbPNL [35] which is the first work demonstrating multivariate causal discovery for PNL model. In addition, we also include three ordering-based methods designed for ANM models including RESIT [23], NPVar [8] and SCORE [27]. The causal orders obtained from these methods are applied with the same pruning procedure as described in Section 4.3.2 with a commonly adopted significance level of 0.001.\nWe consider two common structural metrics for multivariate causal discovery evaluation: the structural Hamming distance (SHD) and structural intervention distance (SID) [22]. These metrics enable us to quantify the error between the predicted DAG with the true DAG. In particular, the SHD measures the total number of missing, extra, and reverse edges between the estimate and the true DAG, while the SID measures the minimum number of interventions required to transform the output to the true DAG. Hence, lower values for both SHD and SID indicate a better fit of the output DAG to the given data. In addition, we also consider the order divergence ($\\mathcal{D}_{order}$) [27] as a measure of the quality of the causal ordering. More specifically, the order divergence reflects the number of directed edges in the true DAG that disagree with the causal ordering."}, {"title": "5.2.2 Synthetic data", "content": "We generate Erd\u00f6s-R\u00e9nyi causal graphs [2] with d nodes with an expected in-degree of 2. The data is generated following the PNL model (1), where $h_i$ represents the weighted sums of Gaussian processes and $g_i$ represents sigmoid functions. For each causal structure, we generate 1,000 samples from uniform noises $U(0, 1)$.\nEffect of dimensionality. To study the performance of the proposed method across different numbers of nodes, we vary d from 4 to 11 variables. The results are shown in Figure 2, showing that our proposed method consistently outperforms other baselines in most cases. Remarkably, the proposed method shows a significantly low $\\mathcal{D}_{order}$ compared with competitors when the graph size grows. Surprisingly, AbPNLMulti exhibits the poorest performance although the method is specifically designed for the PNL model.\nEffect of sample size. In order to assess the impact of sample sizes on the performance of different models, we vary the sample size from 100 to 1,000 while fixing d = 4. The results in Figure 3 reveal that"}, {"title": "6 Conclusion", "content": "In this paper, we introduce CAF-PoNo, an innovative and efficient flow-based approach for cause-effect identification under the PNL model. Moreover, CAF-PoNo surpasses existing methods in parameter estimation for the PNL model by adhering to the invertibility constraint, resulting in the highest AUC on synthetic and real datasets. We also extend CAF-PoNo for multivariate causal discovery and show its superiority over state-of-the-art methods."}]}