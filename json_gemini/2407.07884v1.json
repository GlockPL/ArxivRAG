{"title": "Vegetable Peeling: A Case Study in Constrained\nDexterous Manipulation", "authors": ["Tao Chen", "Eric Cousineau", "Naveen Kuppuswamy", "Pulkit Agrawal"], "abstract": "Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of developed\ndexterous manipulation controllers for downstream tasks. In this study, we focus\non constrained dexterous manipulation for food peeling. Food peeling presents\nvarious constraints on the reorientation controller, such as the requirement for the\nhand to securely hold the object after reorientation for peeling. We propose a\nsimple system for learning a reorientation controller that facilitates the subsequent\npeeling task.", "sections": [{"title": "1 Introduction", "content": "Having robots perform food preparation tasks has been of great interest in robotics. Imagine the\nscenario of making mashed potatoes, where a critical step is to peel potatoes. Humans peel potatoes\nby grasping the potato in one hand and using the second hand to actuate a peeler to remove the\npotato's skin. After a part of the potato is peeled, it is rotated while being held in the hand (i.e.,\nin-hand manipulation) and peeled again. The sequence of rotating and peeling continues until all of\nthe potato's skin is removed. In this work, we present a robotic system that can re-orient different\nvegetables using an Allegro hand in a way that their skin can be peeled using another manipulator.\nOur setup is shown in Figure 1 and Figure 2.\nIn-hand rotation of vegetables is an instance of dexterous manipulation problem [1], a family of\ntasks that involves continuously controlling the force on an object while it is moving with respect\nto the fingertips [2, 3]. The challenges in dexterous manipulation stem from the frequent making\nand breaking of contact, issues in contact modeling, high-dimensional control space, perception\nchallenges due to severe occlusions, etc. A body of work made simplifying assumptions such as\nmanipulating convex objects [4, 5, 1, 6], small finger motions[7, 8, 9], slow or quasi-static motion or"}, {"title": "2 Related Work", "content": "In-hand Object Reorientation: Dexterous manipulation involves the use of high degrees-of-freedom\n(DoF) manipulators for object manipulation [19]. Its requirement for high-dimensional real-time\ncontrol and its nature of frequent contact-making and breaking present grand challenges to roboticists.\nRecently, there has been a growth of interest in a particular instance of dexterous manipulation\nproblems: in-hand object reorientation. This problem is of particular interest as it is a necessary\nstep in many tool-use scenarios. For example, to use a screwdriver for tightening a screw, one has\nto reorient the screwdriver to align it with the screw. We can cluster the works in in-hand object"}, {"title": "3 Method", "content": "Peeling requires a reorientation controller that can stop its motion and firmly hold objects after\nreorientation. The first step in stopping is to decide when re-orientation should be stopped. One\npossibility is to have a perception system predict the desired rotation angle after which the next round\nof peeling would be performed. To accomplish the goal, the robot would need to track changes in\nobject pose and compare it with the target rotation angle. However, accurately estimating object pose\nis challenging, especially when generalization to new objects is necessary [37, 16, 13, 31].\nOne of our insights is that instead of training a predictor for desired rotation angle and object pose\nestimation, it can be easier and sufficient to train a binary vision classifier that detects in real-time\nwhen the peeled part has been turned over. With such a classifier, the reorientation controller's job is\nsimply to keep reorienting the object until it receives a stop signal. In this formulation, unlike prior\nworks [11, 12], the reorientation controller is not conditioned on target orientation but rather on a\nstop signal. Formally, the policy takes as input a binary variable $I_{stop} \\in \\{0, 1\\}$ representing the stop\nsignal. If $I_{stop} = 1$, the policy should stop immediately and ensure the fingers stably and firmly hold\nthe object. Otherwise, the policy should continue reorienting the object. Note that in this work, we\nfocus on learning the reorientation controller, leaving integration of a vision classifier to future work.\nThe next question is how to train such a policy. Using RL to train the policy from scratch can be\nchallenging and requires extensive reward shaping because $I_{stop} = 1$ is a rare event in an episode,"}, {"title": "3.1 Teacher Policy Learning: Reorient and Stop", "content": "We train the teacher policy to re-orient the object along a pre-defined axis and stop (see Figure 3a).\nThe teacher is formulated as a goal-conditioned policy $a_{t} = \\pi_{\\Theta}(o_{t}, a_{t-1}, g)$, where $\\Theta$ represents\nvariables for the teacher policy, $o_{t}$ is the observation, $a_{t}$ is the action command, $g$ is the goal\nrepresenting the amount by which the object needs to be re-oriented. $g$ is randomly and uniformly\nsampled from $[1.57, 4.0]$rad during training.\nWhile the teacher policy's formulation is similar to that in prior works [11, 12], we propose (i) a\nmuch simpler reward function, (ii) new success criteria that effectively encourages the policy to stop\nthe object and firmly hold it, and (iii) an interpolation scheme that enables smoother policy actions in\nthe real world."}, {"title": "3.1.1 Reward Function", "content": "A common approach to designing the reward function is to create multiple terms that make it easier\nfor the manipulator to discover the desired behavior (i.e., reward shaping). For instance, to facilitate\nexploration, we can devise a reward term that reduces the distance between the fingertips and the\ncenter of mass (CoM) of the object. To discourage excessive translational motion of the object during\nrotation, we can create a reward term that penalizes the displacement of the CoM. To discourage the\nobject from rotating with undesired motion along other axes, we can add another reward term that\nreduces the distance between the tip of the thumb and the centerline of the palm. This ensures that\nthe thumb applies force close to the object's CoM, rather than to one side of the object. Additionally,\nwe need to design a reward term that discourages the fingers from covering the top surface of the\nobject, which affects peeling. Hence, designing multiple reward terms is necessary to regulate the\nbehavior under specific constraints. Balancing these terms requires extensive hyper-parameter tuning.\nFor the task of in-hand re-orientation, we found that the reward function can be substantially\nsimplified by using a task demonstration. However, unlike prior works that rely on trajectory-\nlevel demonstrations [40, 41], our method only requires a one-step demonstration (a keyframe),\nwhich is much easier to collect. Specifically, we manually move the real Allegro hand to a good pose\nwhere the constraints mentioned above are satisfied (e.g., the fingers do not cover the food item),\nand the fingers touch the object and are ready to reorient it. We record the joint positions as $q^{demo}$.\nDuring training in simulation, we encourage the joint positions at any time step to be close to $q^{demo}$.\nOverall, our reward function is as follows:\n$r_{t} = c_{1}\\mathbb{1}(\\text{Task successful}) + c_{2}\\frac{1}{\\Delta \\theta_{t} + \\epsilon_{\\theta}} + c_{3} ||q_{t} - q^{demo} ||^{2}$\nwhere $c_{1} = 800, c_{2} = 1.5, c_{3} = -0.6$ are coefficients. $\\mathbb{1}(\\text{Task successful})$ is 1 when the task is\nsuccessfully completed, and 0 otherwise. $\\Delta \\theta_{t}$ is the distance between the object's current and goal\norientation. The first two terms are task rewards for object reorientation. The last term is to regulate\nhand behavior."}, {"title": "3.1.2 Success Criteria", "content": "In a goal-conditioned object reorientation, a common way to claim the task successful is by checking\nif the distance between the object's current and the goal orientation is smaller than a threshold value\n(orientation criterion $C_{ori} = \\Delta \\theta < \\overline{\\theta}$) [16, 13]. Another criterion is that all the fingertips should\nmake contact with the object (contact criterion $C_{contact}$), a pre-requisite for firmly holding the object\nafter reorientation. However, only checking these two criteria is insufficient to ensure the policy\nlearns to stop the motion and hold the object firmly around the goal orientation, as discussed in [12].\nThe policy can oscillate around the goal state due to observation and control delay and noise.\nTo further encourage the policy to stop robot motion when the goal is reached and firmly hold the\nobject, we propose adding time constraints to the success criteria: both $C_{ori}$ and $C_{contact}$ should be\ncontinuously satisfied for $T_{succ}$ time steps. Adding this criterion makes the MDP partially observable\nsince the policy's observation lacks the knowledge of time. Therefore, to facilitate policy learning,\nwe augment the observation space with a scalar indicator variable $I_{succ} = \\frac{t_{succ}}{T_{succ}} | I_{succ} \\in [0, 1]$,\nwhere $t_{succ}$ is the number of consecutive steps satisfying $C_{ori}$ and $C_{contact}$. The observation space\nbecomes $o^{E} := o^{O} \\oplus I_{succ}$. In this work, $\\overline{\\theta} = 0.2rad$, $T_{succ}=8$."}, {"title": "3.1.3 Reset Constraints", "content": "As mentioned earlier, a reorientation policy for peeling needs to meet several constraints, such as\nin-place and fixed-axis reorientation (Figure 3b). While one could design individual reward terms\nto satisfy these constraints, tuning these reward terms to achieve the desired result can be difficult.\nInstead, it is much simpler to formulate the constraints as reset conditions. In other words, if the\nconstraints are violated, the episode is reset immediately. This incentivizes the policy to explore\nonly in space where the constraints are satisfied. Similar techniques were also used in some prior\nworks [11, 12, 14]."}, {"title": "3.1.4 Interpolation and Reference for Action Commands", "content": "Our neural network controller operates at a relatively low control frequency of 12Hz. To track the joint\nposition command, a low-level PD controller runs at 300Hz. To ensure smoother joint motion, we\ninterpolate the low-frequency joint position commands. While more complex interpolation schemes\nsuch as spline interpolation are possible, we found that simple linear interpolation is sufficient to\ngenerate smooth higher-frequency (60Hz) joint position commands. To do this, we linearly interpolate\nbetween the current reference joint positions ($q^{ref}_{t}$) and the desired joint positions ($q^{cmd}_{t}$) for the next\npolicy control time step. We then send the interpolated joint position commands to the PD controllers.\nMathematically, $q^{cmd,n}_{t+1} = q^{ref}_{t} + a_{t}$, where $n \\in [1, N] (N=5)$ and $q^{cmd,n}_{t+1}$ represents the $n^{th}$\ninterpolated joint position command for the next policy control time step.\nWhen the action space is chosen as the change in joint position, the target joint position for the PD\ncontroller is calculated as follows: $q^{cmd}_{t} = q_{t} + a_{t}$ [12, 11, 16]. Here, $q_{t}$ is the current joint position,\nand $a_{t} = \\Delta q_{t}$ is the desired change in joint positions, as described earlier. In this case, the reference"}, {"title": "3.2 Student Policy Learning: Imitate and Stop", "content": "After learning a goal-conditional teacher policy $a^{E}_{t} = \\pi_{\\Theta}(o^{E}_{t}, a_{t-1}, g)$, the next question is how\nto train a real-world deployable student policy that can rotate the object in hand and hold it stably\nafter reorientation. We propose conditioning the student policy on a stop signal $I_{stop} \\in \\{0, 1\\}$:\n$a^{S}_{t} = \\pi_{S}(o^{S}_{t}, a_{t-1}, I_{stop})$. In other words, the student policy should continue reorienting the object\nwhen $I_{stop} = 0$, but stably hold the object when $I_{stop} = 1$. This design choice provides flexibility in\nhow we control the policy to stop the reorientation. For example, the policy could rotate the object\nfor a pre-specified amount of time (i.e., set $I_{stop} = 1$ after t seconds). Alternatively, an external\nperception module could detect when the peeled part has fully turned over, triggering $I_{stop} = 1$ and\nthe policy to stop the motion and hold the object immediately.\nHow can we use the learned goal-conditioned teacher policy to train a student policy that is conditioned\non the stop signal? We can set the value for $I_{stop}$ automatically during policy rollout based on the\norientation distance $\\Delta \\theta_{t}$.\n$I_{stop} = \\begin{cases}\n1 & \\text{if } \\Delta \\theta > \\delta,\n0 & \\text{otherwise}\n\\end{cases}$\nDetails about the observation space and the policy architecture are in Section A.3 in the appendix."}, {"title": "3.3 Peeling", "content": "In this section, we demonstrate that our reorientation controller can be used for downstream peeling\ntasks. We use the dexterous robot hand to do the reorientation and then control another Franka Panda\nrobot arm to do the peeling as shown in Figure 2. To control the robot arm, we experimented with\nboth using a teleoperation system and an automatic vision-based peeling system."}, {"title": "3.3.1 Teleoperation-based peeling", "content": "We used a leader-follower teleoperation system in which a human operator controls a leader system,\nand the Franka arm follows the motion of the leader in real-time. A 200 Hz operational space"}, {"title": "3.3.2 Vision-based peeling", "content": "While teleoperation provides effective peeling commands for the Franka arm and demonstrates\nthat our reorientation controller can firmly grasp objects after reorientation, automating the peeling\nprocess would be ideal. One approach to achieve this is by computing the peeler's motion trajectory\nbased on RGB and depth vision data. The trajectory can be determined through the following steps\n(see Figure 5): (1) We utilize Grounded SAM [44] to segment the target vegetable given an image\nand vegetable name input. (2) Using the segmentation mask and depth data, we reconstruct the 3D\npoint cloud representing the vegetable's top surface. (3) We identify the vegetable's longest axis (the\npeeling direction) by applying principal component analysis. (4) We slice the point cloud into a 2cm\nthick segment along the central plane that crosses the center point and aligns with the longest axis.\nWe then project all the points within the slice onto the plane. (5) We fit a spline curve to the projected\npoints to obtain a smooth trajectory for the peeler tip. Finally, cartesian-space position control moves\nthe peeler along this trajectory while keeping the peeler orientation fixed."}, {"title": "4 Results", "content": "To quantitatively evaluate the real-world policy transfer performance, we tested the controller on four\nvegetables (Figure B.2a): a pumpkin (mass: 827g), a melon (623g), a radish(727g), a papaya(848g)."}, {"title": "4.1 Traveling distance for a fixed amount of commanded motion time", "content": "The first question we want to answer is whether the learned policy can successfully reorient vegetables\nin the real world. In peeling, the width of the peeled part depends on the peeler's width. Thus, it is\nmore informative to measure how much the reorientation controller rotates an object by the traveling\ndistance of a surface point, rather than the absolute rotation angle. Specifically, we mark a reference\npoint $P^{ref}$ on the object surface near the mid-point of its rotational axis. At the start, we ensure $P^{ref}$\nis centered and facing upward when held. After reorientation, we record the new point $P^{new}$ that is\nnow centered and facing upward. We then measure the contour length from $P^{new}$ to $P^{ref}$ along the\nsurface (Figure B.2b).\nTo demonstrate the capability of our controller to reorient real objects, we conducted two rounds of\ntesting. Our controller is trained to stop motion when it receives a stop signal. In the first round, we\nsent the stop signal 3.5 seconds after the controller started rotating. In the second round, we sent the\nstop signal 7 seconds after start. We repeated each test 10 times. As shown in Figure 6a, the controller\nsuccessfully reoriented all four food items by a sufficient amount for peeling. When commanded to"}, {"title": "4.2 How well does the controller track the commanded motion time?", "content": "As discussed in Section 3, if our controller can quickly respond to a stop signal at any time step, it\ncan be combined with a perception system that tracks peeling progress. Hence, we measured how\nlong it takes to stop the hand and object motion after receiving the stop signal. As shown in Figure 6b,\nthe motion stops after 0.4s on average after the controller receives the stop signal."}, {"title": "4.3 Firm grasp after reorientation", "content": "To enable downstream peeling, the reorientation controller must learn to firmly grasp the object after\nstopping finger motion. We tested this by checking if the Allegro hand and object could be lifted\nin the air for 3s by only lifting the object with a single human hand. Table B.1 in the appendix\nshows that across objects and commanded times, the controller firmly grasped objects in 90% of tests.\nMoreover, our controller possesses the capability of performing consecutive reorientations. It can\nrepetitively execute the sequence of peeling and reorientation multiple times in succession."}, {"title": "4.4 Real-world Peeling", "content": "We evaluated whether the reorientation controller could reorient food items to facilitate peeling\n(Figure 1). We tested using an Allegro hand and a Leap hand [45]. Testing showed that peeling\napplied substantial pulling forces on objects. However, in most cases, both hands maintained a firm\nenough grasp to enable successful peeling. Failures often occur when holding small objects, as some\nfingertips may fail to establish secure contact with the surface."}, {"title": "5 Discussions", "content": "The reorientation controller presented in this study is a blind controller that relies solely on propri-\noceptive sensory information. While it has demonstrated the ability to successfully reorient heavy\nobjects and securely hold them in place, its performance could potentially be enhanced by incorpo-\nrating visual and tactile feedback. The current system has a few failure modes. Firstly, the object\nmight slip out of the hand since the controller does not utilize any vision information. Secondly, the\ncontroller might fail if the vegetables are small, as the fingers cannot effectively make contact with the\nobject. When using a vision-based peeling approach to peel the vegetables, the segmentation network\n(Grounded SAM) might fail to correctly identify and segment the target vegetable in the image.\nSometimes, the segmentation mask would incorrectly include the robot hand. Some fine-tuning of\nthe pre-trained Grounded SAM model would be necessary to mitigate such issues. Future work\ncould involve learning a peeling policy via behavior cloning on data collected via teleoperation to\nachieve better autonomy of the system. Additionally, incorporating visual and tactile feedback into\nthe reorientation controller could potentially enhance its performance"}, {"title": "Appendix A Training", "content": "A.1 Training setup\nRobot: We use an Allegro Hand that is controlled via a PD controller at 300Hz. Our control policy\nsets joint position commands and runs at a lower frequency at 12Hz.\nSimulation: We trained the policies in Isaac Gym simulation [46]. To set dynamics-related robot\nparameters in the simulation, we followed a prior approach [12], which uses a gradient-free search\nmethod to find the dynamics parameters for each joint (joint friction, damping, maximum joint\nvelocity, and maximum effort) in simulation that generates the motor response that is closest to the\nreal motors.\nObject Dataset: We collected 23 object meshes (potatoes, squash, cucumber, etc.) from Obja-\nverse [47]. 10 variants for each mesh were created by varying the size. The mass of the object was\nrandomly sampled in the range of [80, 960]g. Note that we aim to reorient much heavier objects than\nprior works [16, 12, 11, 13]."}, {"title": "A.2 Teacher Policy Learning", "content": "A.2.1 Observation and Action Space\nof includes joint positions and velocities, the fingertip poses and velocities, object pose and velocity,\nthe distance between the current object orientation and the goal orientation, and whether any of the\nfingertips touch the object. $a_{t}$ is the delta joint position command. The neural network policy runs at\n12Hz.\nA.2.2 Domain randomization and Perturbation during training\nDuring training, we apply domain randomization on the joint stiffness and damping, friction, and\nrestitution. Additionally, we randomly apply a perturbation force on the object's CoM. We randomly\nsample the direction of the perturbation force and set its magnitude to 10$m_{o}$, where $m_{o}$ is the object\nmass."}, {"title": "A.3 Student Policy Learning", "content": "A.3.1 Observation Space\nIn this work, we only use proprioceptive sensory information (joint positions $q_{t}$ and velocities $\\dot{q_{t}}$)\nas the observation input ($o_{t}^{S}$). Our findings indicate that relying solely on proprioceptive sensory\ninformation results in strong performance. Future research could investigate incorporating visual data\nto further enhance the system's capabilities, such as preventing objects from slipping out of the grasp.\nA.3.2 Policy Architecture\nAs the student policy only has access to a limited amount of sensory information (a POMDP setting),\nit is important to incorporate history information, as has been done in previous works [16, 13, 12].\nWhile [16, 13, 12] utilized RNNs to process history information, Transformers [48] have gained\nsignificant attention due to their improved performance and faster training in domains such as"}, {"title": "Appendix B Testing", "content": "B.1 Testing setup\nFigure B.2a show the objects used for evaluation. Figure B.2b illustrates how we measure the\ntraveling distance of the rotation motion.\nB.2 Firm grasp after reorientation\nTable B.1 shows the success rate of the lifting action after the reorientation. It shows that our\nreorientation controller can control the fingers to firmly hold the object after the reorientation.\nB.3 Ablation study\nDemo term in Reward function We proposed using a keyframe demonstration to ease reward\nshaping. To evaluate its effectiveness, we compared learning curves of the teacher policies trained\nwith and without the $c_{3} ||q_{t} - q^{demo} ||^{2}$ reward term. As shown in Figure B.3a, adding the keyframe\nsubstantially improved learning. Additionally, it demonstrates that mimicking the keyframe pose via\na single reward term effectively reduces the reward-shaping burden.\nNecessity of having joint velocity information in $\\pi_{S}$ The student policy's sensory input included\njoint positions and velocities. We investigated whether including joint velocity information in the\ninput is beneficial. Figure B.3b shows that adding joint velocities to the input improved performance.\nTransformer vs RNN Different from prior works [16, 13, 11, 12], our student policy uses a\nTransformer architecture instead of an RNN architecture. We compared the learning performance\nof a Transformer-based policy and an RNN-based policy. Figure B.4a and Figure B.4b show that\na Transformer-based policy learns much faster and gets better performance at convergence than an\nRNN-based policy."}]}