{"title": "Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation", "authors": ["Tao Chen", "Eric Cousineau", "Naveen Kuppuswamy", "Pulkit Agrawal"], "abstract": "Recent studies have made significant progress in addressing dexterous manipulation problems, particularly in in-hand object reorientation. However, there are few existing works that explore the potential utilization of developed dexterous manipulation controllers for downstream tasks. In this study, we focus on constrained dexterous manipulation for food peeling. Food peeling presents various constraints on the reorientation controller, such as the requirement for the hand to securely hold the object after reorientation for peeling. We propose a simple system for learning a reorientation controller that facilitates the subsequent peeling task.", "sections": [{"title": "1 Introduction", "content": "Having robots perform food preparation tasks has been of great interest in robotics. Imagine the scenario of making mashed potatoes, where a critical step is to peel potatoes. Humans peel potatoes by grasping the potato in one hand and using the second hand to actuate a peeler to remove the potato's skin. After a part of the potato is peeled, it is rotated while being held in the hand (i.e., in-hand manipulation) and peeled again. The sequence of rotating and peeling continues until all of the potato's skin is removed. In this work, we present a robotic system that can re-orient different vegetables using an Allegro hand in a way that their skin can be peeled using another manipulator. Our setup is shown in Figure 1 and Figure 2.\nIn-hand rotation of vegetables is an instance of dexterous manipulation problem [1], a family of tasks that involves continuously controlling the force on an object while it is moving with respect to the fingertips [2, 3]. The challenges in dexterous manipulation stem from the frequent making and breaking of contact, issues in contact modeling, high-dimensional control space, perception challenges due to severe occlusions, etc. A body of work made simplifying assumptions such as manipulating convex objects [4, 5, 1, 6], small finger motions[7, 8, 9], slow or quasi-static motion or manipulating a few specific objects [10, 7, 8] to leverage trajectory optimization or planning-based methods to achieve in-hand object re-orientation [1, 7, 8, 9, 6, 4, 5, 10]. Another line of work has used reinforcement learning for in-hand re-orientation [11, 12, 13, 14, 15] and recent works have leveraged simulation to train policies capable of dynamically re-orienting a diverse set of new objects in real-time and in the real world [11, 12].\nThere are several challenges in adapting re-orientation controllers for a downstream task such as peeling vegetables. These challenges stem from the fact that controllers optimized for re-orientation [16, 13, 14, 15, 12] are only optimized to continuously reorient the object and not to satisfy numerous constraints arising from task-specific requirements. For instance, peeling vegetables requires the hand to first stop re-orienting the object and then for the peeler to peel the vegetable. Many prior works solve a version of the re-orientation problem where the object is continuously rotated [17, 16, 13] or otherwise perform quasistatic re-orientation [8]. Stopping and re- starting dynamic re-orientation is difficult due to the challenge of dealing with the object's inertia. Second, the hand needs to hold the object firmly enough to resist forces applied by the peeler. The closest work that attempts to hold the object at a target configuration [12] is only able to loosely hold the object which is insufficient for resisting forces. Third, the hand needs to reorient the vegetable along a specific axis in place. Here, the specific axis refers to the rotational axis on the object that is parallel to the peeling direction. Similar to how humans reorient vegetables for peeling, it is desirable for the hand to reorient the object in place so that multiple consecutive cycles of reorientation and peeling can be performed. If the object substantially shifts its position during reorientation, the controller will struggle to reorient and hold the object at future time steps. Fourth, when the vegetable is held stationary the fingers should not obstruct the top surface of the vegetable to ensure that the peeler can peel the vegetable.\nWhile in-hand object reorientation has been widely studied [11, 12, 16, 18, 13, 17], no prior works can satisfy the constraints mentioned above. Yet, these constraints become critical for downstream dexterous manipulation beyond object re-orientation. We use vegetable peeling as a case study to investigate the challenges and solutions for building a dexterous manipulation system that can operate under constraints. We develop a framework where we leverage reinforcement learning in simulation to train a policy that can perform object re-orientation under constraints. For the peeling task, we explored two approaches - a teleoperation-based method leveraging human guidance as well as an autonomous vision-based technique. Our contributions are as follows:\n1.  A framework for solving dexterous manipulation problems under the aforementioned con-straints.\n2.  We propose a method that can make RL policy learn to stop its motion and hold objects firmly in hand a critical behavior for many downstream dexterous manipulation problems.\n3.  We present a step towards a robotic system capable of peeling diverse vegetables with differ-ent shapes, masses, and material properties while holding and manipulating the vegetables in hand."}, {"title": "2 Related Work", "content": "In-hand Object Reorientation: Dexterous manipulation involves the use of high degrees-of-freedom (DoF) manipulators for object manipulation [19]. Its requirement for high-dimensional real-time control and its nature of frequent contact-making and breaking present grand challenges to roboticists. Recently, there has been a growth of interest in a particular instance of dexterous manipulation problems: in-hand object reorientation. This problem is of particular interest as it is a necessary step in many tool-use scenarios. For example, to use a screwdriver for tightening a screw, one has to reorient the screwdriver to align it with the screw. We can cluster the works in in-hand object reorientation from many aspects. For example, from the perspective of sensory information, [20] studies open-loop cube reorientation without using any sensors, [21, 5, 16, 10, 22] use motion capture system or special tracking markers for object reorientation, [17] uses proprioceptive sensors such as joint encoders, [23, 24, 15, 14] use tactile sensors and [25, 16, 12, 18] utilize vision sensors. In terms of the dynamics of the system, [7, 8, 9] achieved object reorientation under the assumption of quasi-static motion where object moves slowly and its inertia effect can be ignored, while [15, 16, 12, 14, 26] focuses on dynamic object reorientation where object is manipulated in a fast and dynamic way. To make in-hand object manipulation useful for downstream tool use tasks, one important aspect of the skill is the ability of stably and firmly holding the object in end of the policy rollout. While many prior works on dynamic manipulation such as [16, 10, 14, 15, 17] only consider endlessly rotating the object in hand and cannot stop the object stably when the object reaches the goal orientation, some works such as [12, 26] try to develop controllers that can reorient objects in hand and also hold the object in the goal orientation. Our work studies dynamic in-hand object manipulation with the capability of stopping objects stably in hand.\nReinforcement Learning for Contact-rich Tasks: Contact-rich tasks are particularly challenging due to the difficulty in modeling the system dynamics, especially when the tasks are performed in the wild, outside of a constrained and controlled setting. Examples of such tasks include quadruped robots hiking in mountains and robot hands reorienting various everyday objects. There have been many works using reinforcement learning to learn controllers for solving contact-rich tasks [27, 16, 13, 28, 29, 30, 31]. In the real world, robots typically only have access to a limited amount of state information of the system due to the lack of sensors or the challenges in setting up the sensors. Using reinforcement learning to learn controllers from scratch with limited sensory information tends to be data-inefficient. One way to speed up policy learning is to provide asymmetric information to the policy and value function, where the value function observes much more privileged information [16, 13, 27, 32]. Another method is to decouple policy learning into two stages: a reinforcement learning stage where agents (teacher) observe privileged fully-observable state information, and an imitation learning stage where the policy with limited sensory observation input (student) learns to imitate the policy with fully-observable state information. This approach has been successfully applied to various contact- rich problems such as locomotion [33, 34, 30, 35, 36] and dexterous manipulation [11, 12, 17]. Our pipeline is built upon the idea of teacher-student policy learning and has made several key improvements, which we will detail below."}, {"title": "3 Method", "content": "Peeling requires a reorientation controller that can stop its motion and firmly hold objects after reorientation. The first step in stopping is to decide when re-orientation should be stopped. One possibility is to have a perception system predict the desired rotation angle after which the next round of peeling would be performed. To accomplish the goal, the robot would need to track changes in object pose and compare it with the target rotation angle. However, accurately estimating object pose is challenging, especially when generalization to new objects is necessary [37, 16, 13, 31].\nOne of our insights is that instead of training a predictor for desired rotation angle and object pose estimation, it can be easier and sufficient to train a binary vision classifier that detects in real-time when the peeled part has been turned over. With such a classifier, the reorientation controller's job is simply to keep reorienting the object until it receives a stop signal. In this formulation, unlike prior works [11, 12], the reorientation controller is not conditioned on target orientation but rather on a stop signal. Formally, the policy takes as input a binary variable $I_{stop} \\in \\{0, 1\\}$ representing the stop signal. If $I_{stop} = 1$, the policy should stop immediately and ensure the fingers stably and firmly hold the object. Otherwise, the policy should continue reorienting the object. Note that in this work, we focus on learning the reorientation controller, leaving integration of a vision classifier to future work.\nThe next question is how to train such a policy. Using RL to train the policy from scratch can be challenging and requires extensive reward shaping because $I_{stop} = 1$ is a rare event in an episode, and when the $I_{stop}$ is flipped to one from zero, the policy needs to quickly stop the motion posing a hard-exploration challenge.\nPrior works [11, 12] show success in training a goal-conditioned object reorientation controller. Can we leverage a goal-conditioned reorientation controller to train a controller that reacts to a stop signal? It turns out we can formulate this using the teacher-student learning framework [11, 12, 38, 35, 34]. Specifically, we can use RL to train a goal-conditioned controller that reorients an object by random goal angles along its rotational axis. This acts as the teacher. Next, we can use imitation learning (specifically DAGGER [39]) to train a controller conditioned on the stop signal to imitate the teacher. The stop signal can be generated during training by checking if the orientation distance to the goal is below a threshold. Using imitation learning bypasses the hard exploration challenge."}, {"title": "3.1 Teacher Policy Learning: Reorient and Stop", "content": "We train the teacher policy to re-orient the object along a pre-defined axis and stop (see Figure 3a). The teacher is formulated as a goal-conditioned policy $a_t = \\pi_{\\Theta}(o_t, a_{t-1}, g)$, where $\\Theta$ represents variables for the teacher policy, $o_t$ is the observation, $a_t$ is the action command, $g$ is the goal representing the amount by which the object needs to be re-oriented. $g$ is randomly and uniformly sampled from $[1.57, 4.0]$rad during training.\nWhile the teacher policy's formulation is similar to that in prior works [11, 12], we propose (i) a much simpler reward function, (ii) new success criteria that effectively encourages the policy to stop the object and firmly hold it, and (iii) an interpolation scheme that enables smoother policy actions in the real world."}, {"title": "3.1.1 Reward Function", "content": "A common approach to designing the reward function is to create multiple terms that make it easier for the manipulator to discover the desired behavior (i.e., reward shaping). For instance, to facilitate exploration, we can devise a reward term that reduces the distance between the fingertips and the center of mass (CoM) of the object. To discourage excessive translational motion of the object during rotation, we can create a reward term that penalizes the displacement of the CoM. To discourage the object from rotating with undesired motion along other axes, we can add another reward term that reduces the distance between the tip of the thumb and the centerline of the palm. This ensures that the thumb applies force close to the object's CoM, rather than to one side of the object. Additionally, we need to design a reward term that discourages the fingers from covering the top surface of the object, which affects peeling. Hence, designing multiple reward terms is necessary to regulate the behavior under specific constraints. Balancing these terms requires extensive hyper-parameter tuning.\nFor the task of in-hand re-orientation, we found that the reward function can be substantially simplified by using a task demonstration. However, unlike prior works that rely on trajectory- level demonstrations [40, 41], our method only requires a one-step demonstration (a keyframe), which is much easier to collect. Specifically, we manually move the real Allegro hand to a good pose where the constraints mentioned above are satisfied (e.g., the fingers do not cover the food item), and the fingers touch the object and are ready to reorient it. We record the joint positions as $q^{demo}$. During training in simulation, we encourage the joint positions at any time step to be close to $q^{demo}$.\nOverall, our reward function is as follows:\n$r_t = c_1\\mathbb{1}(\\text{Task successful}) + c_2 \\frac{1}{\\Delta \\theta_t + \\epsilon_{\\theta}} + c_3 ||q_t - q^{demo} ||^2$ (1)\nwhere $c_1 = 800, c_2 = 1.5, c_3 = -0.6$ are coefficients. $\\mathbb{1}(\\text{Task successful})$ is 1 when the task is successfully completed, and 0 otherwise. $\\Delta \\theta_t$ is the distance between the object's current and goal orientation. The first two terms are task rewards for object reorientation. The last term is to regulate hand behavior."}, {"title": "3.1.2 Success Criteria", "content": "In a goal-conditioned object reorientation, a common way to claim the task successful is by checking if the distance between the object's current and the goal orientation is smaller than a threshold value (orientation criterion $C_{ori} = \\Delta\\theta < \\overline{\\theta}$) [16, 13]. Another criterion is that all the fingertips should make contact with the object (contact criterion $C_{contact}$), a pre-requisite for firmly holding the object after reorientation. However, only checking these two criteria is insufficient to ensure the policy learns to stop the motion and hold the object firmly around the goal orientation, as discussed in [12]. The policy can oscillate around the goal state due to observation and control delay and noise.\nTo further encourage the policy to stop robot motion when the goal is reached and firmly hold the object, we propose adding time constraints to the success criteria: both $C_{ori}$ and $C_{contact}$ should be continuously satisfied for $T_{succ}$ time steps. Adding this criterion makes the MDP partially observable since the policy's observation lacks the knowledge of time. Therefore, to facilitate policy learning, we augment the observation space with a scalar indicator variable $I_{succ} = \\frac{t_{succ}}{T_{succ}}|I_{succ} \\in [0,1]$, where $t_{succ}$ is the number of consecutive steps satisfying $C_{ori}$ and $C_{contact}$. The observation space becomes $o^{\\Theta} := o^{\\Theta} \\oplus I_{succ}$. In this work, $\\overline{\\theta} = 0.2 rad, T_{succ} = 8$."}, {"title": "3.1.3 Reset Constraints", "content": "As mentioned earlier, a reorientation policy for peeling needs to meet several constraints, such as in-place and fixed-axis reorientation (Figure 3b). While one could design individual reward terms to satisfy these constraints, tuning these reward terms to achieve the desired result can be difficult. Instead, it is much simpler to formulate the constraints as reset conditions. In other words, if the constraints are violated, the episode is reset immediately. This incentivizes the policy to explore only in space where the constraints are satisfied. Similar techniques were also used in some prior works [11, 12, 14]."}, {"title": "3.1.4 Interpolation and Reference for Action Commands", "content": "Our neural network controller operates at a relatively low control frequency of 12Hz. To track the joint position command, a low-level PD controller runs at 300Hz. To ensure smoother joint motion, we interpolate the low-frequency joint position commands. While more complex interpolation schemes such as spline interpolation are possible, we found that simple linear interpolation is sufficient to generate smooth higher-frequency (60Hz) joint position commands. To do this, we linearly interpolate between the current reference joint positions ($q^{ref}_t$) and the desired joint positions ($q^{cmd}_{t+1}$) for the next policy control time step. We then send the interpolated joint position commands to the PD controllers. Mathematically, $q^{cmd,n}_{t+1} = q^{ref}_t + a_t$, where $n \\in [1, N]$ (N = 5) and $q^{cmd,n}_{t+1}$ represents the $n^{th}$ interpolated joint position command for the next policy control time step.\nWhen the action space is chosen as the change in joint position, the target joint position for the PD controller is calculated as follows: $q^{cmd} = q_t + a_t$ [12, 11, 16]. Here, $q_t$ is the current joint position, and $a_t = \\Delta q_t$ is the desired change in joint positions, as described earlier. In this case, the reference is chosen to be the current joint positions, i.e., $q^{ref} = q_t$. However, we found that this scheme results in significant jerky motion when combined with action interpolation. To illustrate this, consider a simplified example of one joint, as shown in Figure 4a. Since we are using a PD controller only to control the joint position, there is usually an error in tracking the joint position command, as shown by the difference between $q^{cmd}_t$ and $q_t$. If we set $q^{ref} = q_t$, when we interpolate between $q^{ref}_t$ and $q^{cmd}_{t+1}$, it tends to cause a sudden change in the PD controller's set point, as shown in Figure 4a. A sudden change in the set point can cause a sudden change in the joint torque command and hence cause jerky motion. To resolve this issue, we use the previous joint position command as the reference, as shown in Figure 4b. In other words, $q^{ref} = q^{cmd}_{t-1}$, and $q^{cmd}_{t+1} = q^{cmd}_{t} + a_t$."}, {"title": "3.2 Student Policy Learning: Imitate and Stop", "content": "After learning a goal-conditional teacher policy $a^\\Theta_t = \\pi_{\\Theta}(o^\\Theta_t, a^\\Theta_{t-1},g)$, the next question is how to train a real-world deployable student policy that can rotate the object in hand and hold it stably after reorientation. We propose conditioning the student policy on a stop signal $I_{stop} \\in \\{0,1\\}$: $a_t = \\pi_{S} (o_t, a_{t-1}, I_{stop})$. In other words, the student policy should continue reorienting the object when $I_{stop} = 0$, but stably hold the object when $I_{stop} = 1$. This design choice provides flexibility in how we control the policy to stop the reorientation. For example, the policy could rotate the object for a pre-specified amount of time (i.e., set $I_{stop} = 1$ after t seconds). Alternatively, an external perception module could detect when the peeled part has fully turned over, triggering $I_{stop} = 1$ and the policy to stop the motion and hold the object immediately.\nHow can we use the learned goal-conditioned teacher policy to train a student policy that is conditioned on the stop signal? We can set the value for $I_{stop}$ automatically during policy rollout based on the orientation distance $\\Delta \\theta_t$.\n$I_{stop} = \\begin{cases} 1 & \\text{if } \\Delta\\theta_t > \\overline{\\theta} \\\\ 0 & \\text{otherwise} \\end{cases}$\nDetails about the observation space and the policy architecture are in Section A.3 in the appendix."}, {"title": "3.3 Peeling", "content": "In this section, we demonstrate that our reorientation controller can be used for downstream peeling tasks. We use the dexterous robot hand to do the reorientation and then control another Franka Panda robot arm to do the peeling as shown in Figure 2. To control the robot arm, we experimented with both using a teleoperation system and an automatic vision-based peeling system."}, {"title": "3.3.1 Teleoperation-based peeling", "content": "We used a leader-follower teleoperation system in which a human operator controls a leader system, and the Franka arm follows the motion of the leader in real-time. A 200 Hz operational space impedance controller [42] runs on the Panda arm, controlling for pose via torque, and an operator interacts with a Haption Virtuose\u2122 6D HF TAO\u00b9 device. Bilateral position-position haptic coupling is done between the two devices. The controllers and haptic coupling are implemented using Drake [43]."}, {"title": "3.3.2 Vision-based peeling", "content": "While teleoperation provides effective peeling commands for the Franka arm and demonstrates that our reorientation controller can firmly grasp objects after reorientation, automating the peeling process would be ideal. One approach to achieve this is by computing the peeler's motion trajectory based on RGB and depth vision data. The trajectory can be determined through the following steps (see Figure 5): (1) We utilize Grounded SAM [44] to segment the target vegetable given an image and vegetable name input. (2) Using the segmentation mask and depth data, we reconstruct the 3D point cloud representing the vegetable's top surface. (3) We identify the vegetable's longest axis (the peeling direction) by applying principal component analysis. (4) We slice the point cloud into a 2cm thick segment along the central plane that crosses the center point and aligns with the longest axis. We then project all the points within the slice onto the plane. (5) We fit a spline curve to the projected points to obtain a smooth trajectory for the peeler tip. Finally, cartesian-space position control moves the peeler along this trajectory while keeping the peeler orientation fixed."}, {"title": "4 Results", "content": "To quantitatively evaluate the real-world policy transfer performance, we tested the controller on four vegetables (Figure B.2a): a pumpkin (mass: 827g), a melon (623g), a radish(727g), a papaya(848g)."}, {"title": "4.1 Traveling distance for a fixed amount of commanded motion time", "content": "The first question we want to answer is whether the learned policy can successfully reorient vegetables in the real world. In peeling, the width of the peeled part depends on the peeler's width. Thus, it is more informative to measure how much the reorientation controller rotates an object by the traveling distance of a surface point, rather than the absolute rotation angle. Specifically, we mark a reference point $P^{ref}$ on the object surface near the mid-point of its rotational axis. At the start, we ensure $P^{ref}$ is centered and facing upward when held. After reorientation, we record the new point $P^{new}$ that is now centered and facing upward. We then measure the contour length from $P^{new}$ to $P^{ref}$ along the surface (Figure B.2b).\nTo demonstrate the capability of our controller to reorient real objects, we conducted two rounds of testing. Our controller is trained to stop motion when it receives a stop signal. In the first round, we sent the stop signal 3.5 seconds after the controller started rotating. In the second round, we sent the stop signal 7 seconds after start. We repeated each test 10 times. As shown in Figure 6a, the controller successfully reoriented all four food items by a sufficient amount for peeling. When commanded to reorient for 3.5s, 90% of tests reoriented the objects by at least 4cm. With 7s, 90% of tests reoriented objects by at least 7.3cm. Given more time, the controller reoriented objects by a larger amount."}, {"title": "4.2 How well does the controller track the commanded motion time?", "content": "As discussed in Section 3, if our controller can quickly respond to a stop signal at any time step, it can be combined with a perception system that tracks peeling progress. Hence, we measured how long it takes to stop the hand and object motion after receiving the stop signal. As shown in Figure 6b, the motion stops after 0.4s on average after the controller receives the stop signal."}, {"title": "4.3 Firm grasp after reorientation", "content": "To enable downstream peeling, the reorientation controller must learn to firmly grasp the object after stopping finger motion. We tested this by checking if the Allegro hand and object could be lifted in the air for 3s by only lifting the object with a single human hand. Table B.1 in the appendix shows that across objects and commanded times, the controller firmly grasped objects in 90% of tests. Moreover, our controller possesses the capability of performing consecutive reorientations. It can repetitively execute the sequence of peeling and reorientation multiple times in succession."}, {"title": "4.4 Real-world Peeling", "content": "We evaluated whether the reorientation controller could reorient food items to facilitate peeling (Figure 1). We tested using an Allegro hand and a Leap hand [45]. Testing showed that peeling applied substantial pulling forces on objects. However, in most cases, both hands maintained a firm enough grasp to enable successful peeling. Failures often occur when holding small objects, as some fingertips may fail to establish secure contact with the surface."}, {"title": "5 Discussions", "content": "The reorientation controller presented in this study is a blind controller that relies solely on propri-oceptive sensory information. While it has demonstrated the ability to successfully reorient heavy objects and securely hold them in place, its performance could potentially be enhanced by incorpo- rating visual and tactile feedback. The current system has a few failure modes. Firstly, the object might slip out of the hand since the controller does not utilize any vision information. Secondly, the controller might fail if the vegetables are small, as the fingers cannot effectively make contact with the object. When using a vision-based peeling approach to peel the vegetables, the segmentation network (Grounded SAM) might fail to correctly identify and segment the target vegetable in the image. Sometimes, the segmentation mask would incorrectly include the robot hand. Some fine-tuning of the pre-trained Grounded SAM model would be necessary to mitigate such issues. Future work could involve learning a peeling policy via behavior cloning on data collected via teleoperation to achieve better autonomy of the system. Additionally, incorporating visual and tactile feedback into the reorientation controller could potentially enhance its performance"}, {"title": "Appendix A Training", "content": "Robot: We use an Allegro Hand that is controlled via a PD controller at 300Hz. Our control policy sets joint position commands and runs at a lower frequency at 12Hz.\nSimulation: We trained the policies in Isaac Gym simulation [46]. To set dynamics-related robot parameters in the simulation, we followed a prior approach [12], which uses a gradient-free search method to find the dynamics parameters for each joint (joint friction, damping, maximum joint velocity, and maximum effort) in simulation that generates the motor response that is closest to the real motors.\nObject Dataset: We collected 23 object meshes (potatoes, squash, cucumber, etc.) from Obja- verse [47]. 10 variants for each mesh were created by varying the size. The mass of the object was randomly sampled in the range of [80, 960]g. Note that we aim to reorient much heavier objects than prior works [16, 12, 11, 13]."}, {"title": "A.2 Teacher Policy Learning", "content": "."}, {"title": "A.2.1 Observation and Action Space", "content": "of includes joint positions and velocities, the fingertip poses and velocities, object pose and velocity, the distance between the current object orientation and the goal orientation, and whether any of the fingertips touch the object. $a_t$ is the delta joint position command. The neural network policy runs at 12Hz."}, {"title": "A.2.2 Domain randomization and Perturbation during training", "content": "During training, we apply domain randomization on the joint stiffness and damping, friction, and restitution. Additionally, we randomly apply a perturbation force on the object's CoM. We randomly sample the direction of the perturbation force and set its magnitude to 10mo, where mo is the object mass."}, {"title": "A.3 Student Policy Learning", "content": "."}, {"title": "A.3.1 Observation Space", "content": "In this work, we only use proprioceptive sensory information (joint positions $q_t$ and velocities $\\dot{q}_t$) as the observation input ($o_t$). Our findings indicate that relying solely on proprioceptive sensory information results in strong performance. Future research could investigate incorporating visual data to further enhance the system's capabilities, such as preventing objects from slipping out of the grasp."}, {"title": "A.3.2 Policy Architecture", "content": "As the student policy only has access to a limited amount of sensory information (a POMDP setting), it is important to incorporate history information, as has been done in previous works [16, 13, 12]. While [16, 13, 12] utilized RNNs to process history information, Transformers [48] have gained significant attention due to their improved performance and faster training in domains such as natural language processing. Therefore, in this work, we employ a Transformer-based policy architecture. $\\alpha_t = \\pi_{S}(o_t^l, a_0, I_{stop}, ..., o_t^l, a_{t-1}, I_{stop})$. The policy is a decoder-only attention network (Figure 3c) with three self-attention layers. The hidden size is 256, the intermediate size is 512, and the number of attention heads is 8. The policy is trained using DAGGER [39]."}, {"title": "Appendix B Testing", "content": "."}, {"title": "B.1 Testing setup", "content": "Figure B.2a show the objects used for evaluation. Figure B.2b illustrates how we measure the traveling distance of the rotation motion."}, {"title": "B.2 Firm grasp after reorientation", "content": "Table B.1 shows the success rate of the lifting action after the reorientation. It shows that our reorientation controller can control the fingers to firmly hold the object after the reorientation."}, {"title": "B.3 Ablation study", "content": "Demo term in Reward function We proposed using a keyframe demonstration to ease reward shaping. To evaluate its effectiveness, we compared learning curves of the teacher policies trained with and without the $c_3 ||q_t - q^{demo} ||^2$ reward term. As shown in Figure B.3a, adding the keyframe substantially improved learning. Additionally, it demonstrates that mimicking the keyframe pose via a single reward term effectively reduces the reward-shaping burden.\nNecessity of having joint velocity information in $ \\pi_{S}$ The student policy's sensory input included joint positions and velocities. We investigated whether including joint velocity information in the input is beneficial. Figure B.3b shows that adding joint velocities to the input improved performance.\nTransformer vs RNN Different from prior works [16, 13, 11, 12], our student policy uses a Transformer architecture instead of an RNN architecture. We compared the learning performance of a Transformer-based policy and an RNN-based policy. Figure B.4a and Figure B.4b show that a Transformer-based policy learns much faster and gets better performance at convergence than an RNN-based policy."}]}