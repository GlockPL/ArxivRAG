{"title": "Noise-Free Explanation for Driving Action Prediction", "authors": ["Hongbo Zhu", "Theodor Wulff", "Rahul Singh Maharjan", "Jinpei Han", "Angelo Cangelosi"], "abstract": "Although attention mechanisms have achieved consider-able progress in Transformer-based architectures across various Ar-tificial Intelligence (AI) domains, their inner workings remain to beexplored. Existing explainable methods have different emphases butare rather one-sided. They primarily analyse the attention mecha-nisms or gradient-based attribution while neglecting the magnitudesof input feature values or the skip-connection module. Moreover,they inevitably bring spurious noisy pixel attributions unrelated to themodel's decision, hindering humans' trust in the spotted visualizationresult. Hence, we propose an easy-to-implement but effective way toremedy this flaw: Smooth Noise Norm Attention (SNNA). We weightthe attention by the norm of the transformed value vector and guidethe label-specific signal with the attention gradient, then randomlysample the input perturbations and average the corresponding gradi-ents to produce noise-free attribution. Instead of evaluating the expla-nation method on the binary or multi-class classification tasks like inprevious works, we explore the more complex multi-label classifica-tion scenario in this work, i.e., the driving action prediction task, andtrained a model for it specifically. Both qualitative and quantitativeevaluation results show the superiority of SNNA compared to otherSOTA attention-based explainable methods in generating a clearervisual explanation map and ranking the input pixel importance.", "sections": [{"title": "1 Introduction", "content": "The Vision Transformer (ViT) is becoming a prominent architecturein computer vision (CV) tasks [27] like image classification, segmen-tation and object detection. ViT is effective at processing spatial andtemporal data, outperforming traditional convolutional neural net-works (CNNs) by removing the assumption imposed by kernel size,and makes enormous progress in autonomous driving tasks. How-ever, The inner workings of Deep Learning (DL) models, includingTransformers, remain opaque [38], necessitating the development ofeXplainable AI (XAI) methods to make their decision-making pro-cesses more transparent. XAI aims to make deep neural networks(DNNs) more understandable and reliable by providing insights intowhy the model makes specific decisions. This transparency enhances-trust in AI systems, enables the detection of biases or errors, and fa-cilitates alignment with ethical and legal standards [47]. Visualizingthe decision process of DL models is crucial for debugging down-stream tasks, ultimately enhancing their applicability and trustwor-thiness in real-world scenarios [39].\nThe self-attention mechanism in transformers assigns weights be-tween input tokens, indicating their relative importance. This enables"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Explainable Autonomous Driving", "content": "Understanding intelligent decision-making processes in autonomousvehicles remains challenging, impeding widespread societal accep-tance of this technology [35]. Therefore, in addition to ensuring safereal-time decision-making, autonomous vehicle systems should alsoexplain how their decisions are formulated to comply with regulatoryrequirements across various jurisdictions [40]. Kim and Canny [28]utilize a causal attention model integrated with saliency filtering toidentify input regions influencing the output, particularly in steeringcontrol. Building upon this work, Kim et al. [29] introduces textualexplanations through an attention-based video-to-text mechanism,producing \"explanations\" for the decisive actions of self-driving ve-hicles. Xu et al. [55] propose a paradigm for explainable reasoningof driving action-inducing objects. Most recently, Jing et al. [25] de-veloped an action decision-making model with explicit and implicittexplanations. Though the above-mentioned methods provide someexplanation with a saliency map, all these methods are designedfor CNN-based models, not Transformer-based models. Addition-ally, these methods tend to highlight large areas of unrelated pixels,which confuses users a lot."}, {"title": "2.2 Self-Supervised Learning & Knowledge distillation", "content": "SSL [32] is one of the main success factors in current NLP, whichemploys pretext tasks that leverage contextual information withinsentences to provide a robust learning signal [43]. In CV, prevalentSSL strategies include pretext methods [37] and contrastive learning[23]; the former exploits inherent properties or tasks within the inputdata to learn representations, e.g., MAE [18]; the latter encouragesdiscriminative representations by maximizing dissimilarity betweenfeatures extracted from different samples, e.g., CLIP [42]. For au-tonomous driving, SSL has been used for motion estimation [3], andsegmentation [36]. Knowledge distillation (KD) [19] compresses theknowledge learned by the teacher network into the student network,utilising soft labels (probability distributions). For the self-drivingdomain, KD has been used for driving scene semantic segmentation[34], object detection [31], and motion planning [53]. However, tothe best of our knowledge, neither SSL nor KD has yet been used fordriving action prediction tasks among the works mentioned above."}, {"title": "2.3 Explainable Methods for Transformers", "content": "Attention-based RawAtt [22] is the earliest proposed method thatuses the last layer's attention weights as the salience of each input"}, {"title": "3 Methodology", "content": "As shown in Figure 1, our framework consists of two components,i.e., the trained predictor and the explainer SNNA. We first train theViT in a self-supervised manner on a large unlabelled dataset, thenfine-tune the classifier in a supervised manner on a relatively smallhuman-labelled driving action dataset. Lastly, SNNA generates a vi-sual explanation of the trained classifier."}, {"title": "3.1 Driving Action Predictor", "content": ""}, {"title": "3.1.1 Self-Supervised Learning", "content": "We adapted DINO for the self-supervised training with ViT as thebackbone network. In this setting, student and teacher models aretrained simultaneously. The teacher model serves as a source of su-pervision for the student model. The student model learns to mimicthe teacher model's behaviour using the teacher model's predictionsas targets. As is illustrated in Figure 2, we define the input imagex, student network ges parameterized by s and teacher network getparameterized by \u03b8t, while x\u03b9 refers to the small local crops as inputfor ges and xg represents the bigger global crops as input for get. Thestudent ges is trained by simply matching the output distribution ofgo over different views (crops) of the same input image x. The neu-ral network g comprises a backbone f (ViT) and a projection headh, i.e., g = h o f. pi and pg represent the student and teacher net-work's output probability distribution; the probability p is obtainedby normalizing the network's output g with a softmax function. Bothstudent and teacher networks share the same backbone architecture gbut with different parameters Os and Ot. The parameter Os is learnedfrom Ot with stochastic gradient descent. As teacher network Ot is notpriori known here, we build it from the last round of the student net-works using the \"exponential moving average\" (ema) and freezingthe teacher network over an epoch. The teacher network performsbetter than the student throughout the training [6], guiding the stu-dent's training by providing target features of higher quality.\nViT As is shown in Figure 3, an image is split into non-overlappingcontiguous p\u00d7 p pixels square patches in the ViT architecture. Thesepatches are then passed through a linear transformation to form patchembeddings. This can be achieved by using a Conv2d with the ker-nel size and stride set to patch-size p. Then, an extra learnable token[CLS] is prepended to the embedded sequence, which serves as arepresentation of the entire image to perform classification, and weattach the projection head h at its output. The patch tokens and the[CLS] token are fed to a standard Transformer network with a \"pre-norm\" layer normalization. The Transformer consists of a sequenceof self-attention and feed-forward layers, paralleled with skip con-nections [16], which directly concatenate/merge features from dif-ferent layers and enables gradients to flow better. The self-attentionlayers update the token representations by looking at other token rep-resentations with an attention mechanism.\nAttention Mechanism Self-attention layers are the core compo-nent of Vision Transformers, which assign a pairwise attention valuebetween every two tokens [52]. Each patch embedding builds its rep-resentation by \"attending\" to the other patches. By connecting otherpotentially distant patches of the image, the network builds a rich,high-level semantic understanding of the scene from the distributedpatches in different space locations. Each attention head gathers rel-evant information from the transformed input vectors in differentsemantic spaces. Also, by visualizing the averaged attention mapsamong the heads of the last attention layer, we can see that the global"}, {"title": "3.1.2 Supervised Multi-label Finetuning", "content": "As the teacher network performs better than the student through-out the training, we extract the teacher network after the self-supervised pretraining is finished and add a linear classifier headfor multi-label classification. Instead of using softmax activation-based Cross Entropy Loss, we chose the sigmoid activation-basedBinary Cross Entropy With Logits Loss as our loss function formulti-label classification. We calculate the per-instance accuracy(Acc =FP+FN+TP+TN) by summing the number of true posi-tives and true negatives divided by the number of dataset labels andaveraging the final accuracy across the whole testing dataset."}, {"title": "3.2 Explainer: Smooth Noise Norm Attention (SNNA)", "content": "Mathematically, Attention computes output vector y \u2208 R(n+1)xdfrom a sequence of transformed vector Zl\u22121 = {zl\u22121,..., zn\u22121}\u2286Rd, where n is the number of patches, l \u2208 {1, ..., L}, L is the numberof layers, and d is the model embedding dimensions.\nAlj := softmax\n(q (zl\u22121)Tk (zl\u22121))\n\u221adk\u2208 Rh\u00d7(n+1)\u00d7(n+1),(1)\nAj is the attention weight token zl\u22121 assigned to token zl\u22121.\nAl\u2208 Rh\u00d7(n+1)\u00d7(n+1), where h is the number of heads."}, {"title": "Norm Weighted Attention", "content": "From Equation 4, the space com-plexity of Alvl (Al\u2208 Rh\u00d7(n+1)\u00d7(n+1),vl\u2208 R(n+1)\u00d7d) isO(h \u00d7 (n + 1)2 \u00d7 d). This operation is computationally expensiveas the embedding dimension d is usually large (d = 384 in ViT-S). Herein, we propose using the norm weighted attention Al||vl||2(||v||2 \u2208 Rn+1) to measure the magnitude of the input vector for theoutput, with the space complexity reduced to O(h \u00d7 (n + 1)2)."}, {"title": "Class Activation Map", "content": "[57] To achieve class discriminative expla-nations, we define VAl :=f\u2202fcdA as a mask and dot product it withAl||vl||2, where fc is the model's output prediction on class c, and\u2202fcdA is the raw attention of layer l. As we are more interested in posirelevance, so only the positive values of the \"gradients-relevancemultiplication\" were considered when computing the relevance ofweighted attention (denoted as \"superscript +\"). We treat the multi-ple heads equally, i.e., averaging multiple heads evenly. Consideringthe skip connections in the Transformer block, an identity matrix isadded at the end of Equation 6 to account for residual connections[1].\nAl = Eh((Al||vl||2 \u2299 \u2207Al)+) + I\n(6)\nWhere Al \u2208 R(n+1)\u00d7(n+1),is the Hadamard product. Eh is theexpectation across the head dimension."}, {"title": "Smooth Noise", "content": "To avoid highlighting irrelevant pixels andsmoothen the attribution maps, we draw inspiration from Smooth-Grad [48]: \"removing noise by adding noise\", which uses a randomsampling strategy around the input by averaging the obtained attri-butions to produce visually sharper attribution map, as illustratedin Figure 5. The smoothing process effectively retains the relevantparts and reduces the gradient self-induced noise [12], which refersto the instability introduced during the backpropagation, includingnumerical instability and vanishing or exploding gradients. Whenthese sources of noise or instability affect the gradients computedduring backpropagation, they can lead to inaccurate or unreliable ex-planations. We smooth the raw gradients over the input space, whichcomputes an attribution map by averaging multiple attribution mapswith m permutation inputs."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "The Berkeley Deep Drive dataset3(BDD-100K)[56] contains100,000 images with 2D bounding boxes, lane markings, and full-frame instance segmentation. As an extension of BDD-100K, BDD-OIA4[55] collected images of complicated scenes (#pedestrians > 5or #vehicles > 5) from the original BDD-100K dataset and then annotated them with four action categories: move forward, slow down,turn left and turn right. The total 22k annotated images are split intosubsets of 16k for the training, 2k for the validation and 4k for thetest set. To our knowledge, BDD-OIA is the only publicly availableannotated driving action dataset with binary labelling: potentially ex-ecutable actions are marked as 1, while the rest are marked as 0, e.g.,[1,1,0,0], which means you can move forward or slow down. Samplesof the images and their annotation can be seen in Appendix Figure 8.Both the aforementioned datasets are used in this work. The size ofthe RGB image in both datasets equals 720 x 1280.\nThe model training details can be found in Appendix A.1."}, {"title": "4.2 Baseline explainable methods", "content": "We chose the four currently most representative attention-based ex-plainable methods plus one self-proposed method, i.e., AttIG, andcompared SNNA with them through qualitative and quantitative ex-periments. RawAtt[22] utilize the raw attention weights from the lastlayer Al, but can only get class-agnostic attribution. AttGrad[10]regard the partial derivative of the output to the input as a measureof the network's sensitivity for each input dimension and element-wise product of the attention matrix with Attention Gradient as the"}, {"title": "4.3 Qualitative evaluation", "content": "The qualitative evaluation is based on the inspection of the producedattribution maps. However, this introduces a strong bias in the eval-uation. Humans judge methods more favourably that produce expla-nations closer to their expectations at the cost of penalizing methodsthat might more closely reflect the network behaviour. There are lim-itations to the qualitative evaluation of attribution maps due to biasesin human intuition towards simplicity. Therefore, apart from qualita-tive comparison, we use quantitative metrics defined in Section 4.4as a complementary for more trustworthy evaluation."}, {"title": "4.4 Quantitative evaluation", "content": "A common method in model interpretability involves systematicallyremoving or altering input features and observing the impact on themodel's output. The significance of input features for the final de-cision can be measured by the change in model performance with-out them. Removing features with high attribution scores should de-crease the performance, while discarding features with low attribu-tion scores has less effect on the model's performance [54].\nFaithfulness quantifies the fidelity of an explanation techniqueby measuring if the identified tokens impact the output. We regardthe attribution value as a relevance score; the larger the value, thestronger the correlation, and perform positive perturbation tests: to-kens are removed from the highest relevance to the lowest, to eval-uate the explanation faithfulness by Area-under-the-perturbation-curve (AUPC) [14] and Log-odds scores [41] metrics. We gradually\"remove\" the input features of a given input and measure the ac-curacy of the network. In positive perturbation, We expect a steepdecrease in performance, indicating that the removed tokens are im-portant to the classification score."}, {"title": "4.5 Replacement Methods", "content": "Pixel Mask For ViT, the input image will undergo normalization aspart of pre-processing. This means that ViT does not actually \"see\"the raw pixel values; instead, it sees normalized values, where themean pixel value of the image dataset might not be 0. Therefore,setting the pixel value to 0 (black) could introduce a significant sta-tistical outlier for the model rather than a neutral or \"uninformative\"input. It can introduce biases and disrupt the context or texture con-tinuity of the surrounding area, potentially leading the model to fo-cus on the edges of the masked area rather than ignoring it. So, wereplace the corresponding patch pixel with the per channel-wise av-eraged value of the input image.\nToken Mask Setting the token value of a specific patch to the zerovector means essentially removing the information associated withthat patch from the input representation [33]. This forces the modelto redistribute its attention to the remaining non-zero tokens. Usinga zero vector to replace a patch's embedding is straightforward anddoes not disrupt the input's expected shape or the subsequent pro-cessing pipeline. As a result, we can maintain the original dimen-sional structure of the input and ensure that the model architecturedoes not need any modification to accommodate the masked input.Hence, we replace the image patch tokens with the zero vector.\nAttention Mask Sets the attention weights for the masked tokensto 0, so the masked parts will not be forwarded in the network any-more [44]. This approach is grounded by the attention mechanism,where attention weights determine the importance of each tokenwhen computing the output of the attention layers. By setting the at-tention weights of certain tokens to zero, we effectively remove theirinfluence on the subsequent layers, allowing us to assess how criticalthey are for the model's decision."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Qualitative", "content": "Figure 6 presents a visual comparison between SNNA and variousother baselines. Where RawAtt create a very scattered saliency map,many unrelated areas are marked as significant, like the indicatorlights on the far left of the picture, as well as the flower beds andthe traffic sign in the distance. AttGrad and AttIN have similar re-sults, but strangely, two small areas at the bottom of the picture arehighlighted to the left and right front of the car. Overall, the resultsof GenericAtt and AttIG are the best among baselines, but they stillhave some puzzling noise. From Figure 6, we can see that our methodprovides clearer and more intuitive visualization with respect to thebaselines, which produce more or less noise attribution. More con-vincing visual results can be found in Appendix Figure 10."}, {"title": "5.2 Quantitative", "content": "Due to computational costs, all the experiment results are aver-aged on 1000 samples randomly chosen from the BDD-OIA test-ing dataset. As the saliency pixel in driving scenarios normally ac-counts for a relatively small area, we focus our ablation experiment"}, {"title": "6 Conclusion", "content": "This study addresses significant challenges in generating clear andconfident explanations for Transformer-based models by introducinga novel approach termed Smooth Noise Norm Attention (SNNA). Itutilizes attention weights, value vectors, and gradients to calculateattribution scores and quantify input features' influence on modeloutputs. These attribution scores provide the significance of the in-put features for the model's decision. Our experimental results re-veal that SNNA can produce clear attribution maps that are more\nrelevance-sensitive and with much less noise than baseline methods.We also demonstrate that SNNA outperforms baseline methods inranking the importance of the input pixels regarding the quantitativemetrics. Although our current SNNA method is specifically imple-mented on ViT for image classification tasks, it can be naturally ap-plied to any Transformer-based architectures in future research. Asthere is no ground truth annotation for the \"noisy\" pixel, the investi-gation regarding the faithfulness of the salient pixels to the model'sdecision remains to be investigated in future research. Due to the lackof credible evaluation metrics, how to fairly and effectively assess theeffectiveness of XAI methods remains to be explored."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation details", "content": "We train the classification model on the previously introduced two datasets. We chose ViT-S/8 with L = 12 Transformer encoder layers for thiswork. We first initialize our model from the pretrained checkpoint (ViT-S/8) on ImageNet and train it on the unlabelled BDD-100k trainingdataset with a batch size of 16 for 200 epochs on two Nvidia A100 (80GB) GPUs. Then, we freeze the backbone network and fine-tune theclassification head with the labelled data for 100 epochs (Suggested by DINO). During pretraining, we follow the suggestion of DINO torandomly crop and then resize the original image to two 224 \u00d7 224 bigger crops and eight 96 \u00d7 96 small crops for the teacher and studentnetwork separately. Considering the computational resource limits, we resized the input image to 360 \u00d7 640 during supervised fine-tuning. Ourfinal trained driving-action prediction model gets 74.6% multi-label classification accuracy on the BDD-OIA testing dataset, which is higherthan the baseline [55] accuracy of 73.4%."}]}