{"title": "Noise-Free Explanation for Driving Action Prediction", "authors": ["Hongbo Zhu", "Theodor Wulff", "Rahul Singh Maharjan", "Jinpei Han", "Angelo Cangelosi"], "abstract": "Although attention mechanisms have achieved considerable progress in Transformer-based architectures across various Artificial Intelligence (AI) domains, their inner workings remain to be explored. Existing explainable methods have different emphases but are rather one-sided. They primarily analyse the attention mechanisms or gradient-based attribution while neglecting the magnitudes of input feature values or the skip-connection module. Moreover, they inevitably bring spurious noisy pixel attributions unrelated to the model's decision, hindering humans' trust in the spotted visualization result. Hence, we propose an easy-to-implement but effective way to remedy this flaw: Smooth Noise Norm Attention (SNNA). We weigh the attention by the norm of the transformed value vector and guide the label-specific signal with the attention gradient, then randomly sample the input perturbations and average the corresponding gradients to produce noise-free attribution. Instead of evaluating the explanation method on the binary or multi-class classification tasks like in previous works, we explore the more complex multi-label classification scenario in this work, i.e., the driving action prediction task, and trained a model for it specifically. Both qualitative and quantitative evaluation results show the superiority of SNNA compared to other SOTA attention-based explainable methods in generating a clearer visual explanation map and ranking the input pixel importance.", "sections": [{"title": "1 Introduction", "content": "The Vision Transformer (ViT) is becoming a prominent architecture in computer vision (CV) tasks [27] like image classification, segmentation and object detection. ViT is effective at processing spatial and temporal data, outperforming traditional convolutional neural networks (CNNs) by removing the assumption imposed by kernel size, and makes enormous progress in autonomous driving tasks. However, The inner workings of Deep Learning (DL) models, including Transformers, remain opaque [38], necessitating the development of eXplainable AI (XAI) methods to make their decision-making processes more transparent. XAI aims to make deep neural networks (DNNs) more understandable and reliable by providing insights into why the model makes specific decisions. This transparency enhances trust in AI systems, enables the detection of biases or errors, and facilitates alignment with ethical and legal standards [47]. Visualizing the decision process of DL models is crucial for debugging downstream tasks, ultimately enhancing their applicability and trustworthiness in real-world scenarios [39].\nThe self-attention mechanism in transformers assigns weights between input tokens, indicating their relative importance. This enables the use of attention weights in the analysis of model outputs by revealing an importance distribution over the input space [7]. However, attention score alone does not provide a comprehensive understanding of model behaviour, e.g., latent activations. Also, attention lacks class-specificity and primarily indicates the softmax output (in attention layers), neglecting other model components, e.g., the magnitudes of input feature values or the skip-connection module. Studies by Serrano and Smith [44] demonstrate that removing representations with high attention weights does not consistently result in performance degradation. Jain and Wallace [22] found that attention scores may not serve as reliable explanations, as they often conflict with other indicators of feature importance, such as gradient-based measures. Abnar and Zuidema [1] suggested that contextual information among tokens becomes more homogeneous as the model depth increases, undermining the reliability of explanations derived solely from raw attention weights. Kobayashi et al. [30] took the effect of input features into account but ignored the skip connection. Chefer et al. [8] considered the skip connection module, but no input features. Moreover, regardless of whether the attention-based or gradient-based method was used, none of them tried to solve the noise attribution problem, i.e., the presence of irrelevant or misleading visual highlights in the heatmap that do not correspond to the actual model's decision, which is non-trivial and cannot be ignored.\nThe necessity to explain driving action prediction arises from var-"}, {"title": "2 Related Work", "content": "The need to explain driving action prediction arises from various factors. Firstly, societal expectations demand reliability assurances given the high-stakes and safety-critical nature of autonomous driving. Secondly, explanations are invaluable tools for enhancing system performance, allowing engineers and researchers to address deficiencies and glean insights into failure modes [51]. Thirdly, explanations foster trust among users, facilitating broader acceptance of the technology by providing transparency into the decision-making processes [45]. Driving is a complex task that involves various interacting factors. We may have multiple action choices in many driving scenarios, as shown in Appendix Figure 8. So, driving action prediction involves more than simply assigning a single action label to each observed environment. Instead, it requires identifying all possible actions the system might take. This makes driving action prediction a multi-label classification problem. Training deep learning models in a supervised way requires large amounts of labelled data, which is expensive and time-consuming to collect [13]. Driving environment images from the cameras are easy to collect, while the driving action labels (i.e., move forward, slow down, turn left and turn right) are hard to get and are unavailable in most open-source autonomous driving datasets such as KITTI [11], ApolloScape [21], and Waymo [49], or only partially available in BDD-100k [55]. It makes training a driving action prediction model in a purely supervised way extremely expensive since data needs to be annotated. Self-Supervised Learning (SSL) [24], emerges as a compelling and flexible approach for training AI models, as it can leverage a vast amount of unlabelled data by learning a surrogate label or pre-text task created by the modeller, hence does not require extensive labelled data.\nRecent works have shown that we can learn unsupervised features without labels of the downstream task using SSL, which has seen huge interest in various Natural Language Processing (NLP) (BERT [26], GPT [4] and T5 [43]) and CV (SimCLR [9], MoCo [17] and SwAV [5]) models. These models utilize the widely available data without annotation for representation learning. Compared to supervised learning in CV, which simplifies the intricate visual details of an image to a single predefined category [37], SSL aims to assist the model in learning more transferable, generalizable and robust representations from pseudo-labels. To take advantage of the vast amount of unlabeled self-driving images, we adapt DINO [6] (Self-DIstillation with NO Labels) to train a driving action prediction model (ViT) using self-supervised learning on a large unlabelled dataset, followed by supervised learning on a smaller labelled dataset. Our pipeline is illustrated in Figure 1. DINO employs self-distillation, which trains a student model on small local crops and a teacher model on bigger crops of the input image, and then calculates the loss between both representations. It works by interpreting self-supervision as a particular case of self-distillation, where no labels are used. The teacher network will push the student network to learn global representations by seeing only the small local crops. DINO is related to co-distillation [2] where student and teacher have the same network architecture. Meanwhile, the teacher network in co-distillation also distils from the student part.\nTo overcome the noisy attribution problem, we introduce Smooth Noise Norm Attention (SNNA) 2, a concise and considered explainable method: we first weigh the attention weights with the norm of the transformed value vector, then mask with the attention gradient with respect to the prediction. Finally, the noise is filtered by sampling the instance around the input with Gaussian noise. By evaluating its performance in the driving action prediction task, the ex-"}, {"title": "2.1 Explainable Autonomous Driving", "content": "Understanding intelligent decision-making processes in autonomous vehicles remains challenging, impeding widespread societal acceptance of this technology [35]. Therefore, in addition to ensuring safe real-time decision-making, autonomous vehicle systems should also explain how their decisions are formulated to comply with regulatory requirements across various jurisdictions [40]. Kim and Canny [28] utilize a causal attention model integrated with saliency filtering to identify input regions influencing the output, particularly in steering control. Building upon this work, Kim et al. [29] introduces textual explanations through an attention-based video-to-text mechanism, producing \"explanations\" for the decisive actions of self-driving vehicles. Xu et al. [55] propose a paradigm for explainable reasoning of driving action-inducing objects. Most recently, Jing et al. [25] developed an action decision-making model with explicit and implicit explanations. Though the above-mentioned methods provide some explanation with a saliency map, all these methods are designed for CNN-based models, not Transformer-based models. Additionally, these methods tend to highlight large areas of unrelated pixels, which confuses users a lot."}, {"title": "2.2 Self-Supervised Learning & Knowledge distillation", "content": "SSL [32] is one of the main success factors in current NLP, which employs pretext tasks that leverage contextual information within sentences to provide a robust learning signal [43]. In CV, prevalent SSL strategies include pretext methods [37] and contrastive learning [23]; the former exploits inherent properties or tasks within the input data to learn representations, e.g., MAE [18], the latter encourages discriminative representations by maximizing dissimilarity between features extracted from different samples, e.g., CLIP [42]. For autonomous driving, SSL has been used for motion estimation [3], and segmentation [36]. Knowledge distillation (KD) [19] compresses the knowledge learned by the teacher network into the student network, utilising soft labels (probability distributions). For the self-driving domain, KD has been used for driving scene semantic segmentation [34], object detection [31], and motion planning [53]. However, to the best of our knowledge, neither SSL nor KD has yet been used for driving action prediction tasks among the works mentioned above."}, {"title": "2.3 Explainable Methods for Transformers", "content": "Attention-based RawAtt [22] is the earliest proposed method that uses the last layer's attention weights as the salience of each input"}, {"title": "3 Methodology", "content": "As shown in Figure 1, our framework consists of two components, i.e., the trained predictor and the explainer SNNA. We first train the ViT in a self-supervised manner on a large unlabelled dataset, then fine-tune the classifier in a supervised manner on a relatively small human-labelled driving action dataset. Lastly, SNNA generates a visual explanation of the trained classifier."}, {"title": "3.1 Driving Action Predictor", "content": ""}, {"title": "3.1.1 Self-Supervised Learning", "content": "We adapted DINO for the self-supervised training with ViT as the backbone network. In this setting, student and teacher models are trained simultaneously. The teacher model serves as a source of supervision for the student model. The student model learns to mimic the teacher model's behaviour using the teacher model's predictions as targets. As is illustrated in Figure 2, we define the input image x, student network \\(g_{\\theta_s}\\) parameterized by \\(\\theta_s\\) and teacher network \\(g_{\\theta_t}\\) parameterized by \\(\\theta_t\\), while \\(x_l\\) refers to the small local crops as input for \\(g_{\\theta_s}\\) and \\(x_g\\) represents the bigger global crops as input for \\(g_{\\theta_t}\\). The student \\(g_{\\theta_s}\\) is trained by simply matching the output distribution of \\(g_{\\theta_s}\\) over different views (crops) of the same input image x. The neural network g comprises a backbone f (ViT) and a projection head h, i.e., \\(g = h \\circ f\\). \\(p_i\\) and \\(p_g\\) represent the student and teacher network's output probability distribution; the probability p is obtained by normalizing the network's output g with a softmax function. Both student and teacher networks share the same backbone architecture g but with different parameters \\(\\theta_s\\) and \\(\\theta_t\\). The parameter \\(\\theta_s\\) is learned from \\(\\theta_t\\) with stochastic gradient descent. As teacher network \\(\\theta_t\\) is not priori known here, we build it from the last round of the student network s using the \"exponential moving average\" (ema) and freezing the teacher network over an epoch. The teacher network performs better than the student throughout the training [6], guiding the student's training by providing target features of higher quality.\nViT As is shown in Figure 3, an image is split into non-overlapping contiguous \\(p\\times p\\) pixels square patches in the ViT architecture. These patches are then passed through a linear transformation to form patch embeddings. This can be achieved by using a Conv2d with the kernel size and stride set to patch-size p. Then, an extra learnable token [CLS] is prepended to the embedded sequence, which serves as a representation of the entire image to perform classification, and we attach the projection head h at its output. The patch tokens and the [CLS] token are fed to a standard Transformer network with a \"pre-norm\" layer normalization. The Transformer consists of a sequence of self-attention and feed-forward layers, paralleled with skip connections [16], which directly concatenate/merge features from different layers and enables gradients to flow better. The self-attention layers update the token representations by looking at other token representations with an attention mechanism.\nAttention Mechanism Self-attention layers are the core component of Vision Transformers, which assign a pairwise attention value between every two tokens [52]. Each patch embedding builds its representation by \"attending\" to the other patches. By connecting other potentially distant patches of the image, the network builds a rich, high-level semantic understanding of the scene from the distributed patches in different space locations. Each attention head gathers relevant information from the transformed input vectors in different semantic spaces. Also, by visualizing the averaged attention maps among the heads of the last attention layer, we can see that the global"}, {"title": "3.1.2 Supervised Multi-label Finetuning", "content": "As the teacher network performs better than the student throughout the training, we extract the teacher network after the self-supervised pretraining is finished and add a linear classifier head for multi-label classification. Instead of using softmax activation-based Cross Entropy Loss, we chose the sigmoid activation-based Binary Cross Entropy With Logits Loss as our loss function for multi-label classification. We calculate the per-instance accuracy \\(\\text{Acc} = \\frac{FP+FN}{TP+TN}\\) by summing the number of true positives and true negatives divided by the number of dataset labels and averaging the final accuracy across the whole testing dataset."}, {"title": "3.2 Explainer: Smooth Noise Norm Attention (SNNA)", "content": "Mathematically, Attention computes output vector \\(y \\in \\mathbb{R}^{(n+1)\\times d}\\) from a sequence of transformed vector \\(Z^{l-1} = \\{z_0^{l-1},..., z_n^{l-1}\\} \\subseteq \\mathbb{R}^d\\), where n is the number of patches, \\(l \\in \\{1, ..., L\\}\\), L is the number of layers, and d is the model embedding dimensions.\n\\[A_j^l := \\text{softmax}(\\frac{q(z_i^{l-1})^T k (z_j^{l-1})}{\\sqrt{d_k}}) \\in \\mathbb{R}\\]\n\\(A_j\\) is the attention weight token \\(z_i^{l-1}\\) assigned to token \\(z_j^{l-1}\\). \\(A^l \\in \\mathbb{R}^{h\\times (n+1)\\times (n+1)}\\), where h is the number of heads."}, {"title": "Norm Weighted Attention", "content": "From Equation 4, the space complexity of \\(A^l v^l\\) \\((A^l \\in \\mathbb{R}^{h\\times (n+1)\\times (n+1)}, v^l \\in \\mathbb{R}^{(n+1)\\times d}\\) is \\(O(h \\times (n + 1)^2 \\times d)\\). This operation is computationally expensive as the embedding dimension d is usually large (d = 384 in ViT-S). Herein, we propose using the norm weighted attention \\(A^l ||v^l||_2\\) (\\(||v||_2 \\in \\mathbb{R}^{n+1}\\)) to measure the magnitude of the input vector for the output, with the space complexity reduced to \\(O(h \\times (n + 1)^2)\\).\nClass Activation Map [57] To achieve class discriminative explanations, we define \\(\\mathcal{V}^l := \\frac{\\partial f_c}{\\partial A^l}\\) as a mask and dot product it with \\(A^l||v||_2\\), where \\(f_c\\) is the model's output prediction on class c, and \\(A^l\\) is the raw attention of layer l. As we are more interested in positive relevance, so only the positive values of the \"gradients-relevance multiplication\" were considered when computing the relevance of weighted attention (denoted as \"superscript +\"). We treat the multiple heads equally, i.e., averaging multiple heads evenly. Considering the skip connections in the Transformer block, an identity matrix is added at the end of Equation 6 to account for residual connections [1].\n\\[\\bar{A}^l = E_h((A^l||v||_2 \\odot \\nabla \\mathcal{V}^l)^+) + I\\]\nWhere \\(\\bar{A}^l \\in \\mathbb{R}^{(n+1)\\times (n+1)}\\), is the Hadamard product. \\(E_h\\) is the expectation across the head dimension."}, {"title": "Smooth Noise", "content": "To avoid highlighting irrelevant pixels and smoothen the attribution maps, we draw inspiration from SmoothGrad [48]: \"removing noise by adding noise\", which uses a random sampling strategy around the input by averaging the obtained attributions to produce visually sharper attribution map, as illustrated in Figure 5. The smoothing process effectively retains the relevant parts and reduces the gradient self-induced noise [12], which refers to the instability introduced during the backpropagation, including numerical instability and vanishing or exploding gradients. When these sources of noise or instability affect the gradients computed during backpropagation, they can lead to inaccurate or unreliable explanations. We smooth the raw gradients over the input space, which computes an attribution map by averaging multiple attribution maps with m permutation inputs.\n\\[SG(x_e) = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial f_c(x_e^i)}{\\partial A^l}\\]\nWhere \\(x_e^i = x + \\mathcal{N}(0, \\sigma^2)\\), x is the input image, and \\(A^l\\) is the last layer raw attention. Gaussian noise \\(\\mathcal{N}\\) is used to smoothen the input space and construct visually sharper attribution maps. It is briefly discussed in [48] that standard deviation \\(\\sigma\\) needs to be carefully selected to get the best result. If too small, the attribution maps are still noisy; if too large, the maps become irrelevant. We empirically found that m = 5,\\(\\sigma = 0.15(\\text{max}(x) - \\text{min}(x))\\) is sufficient in this work. Then, we multiply the per layer norm weighted attention and element-wise product of the SmoothGrad to get the relevance matrix \\(\\mathcal{R}(x)\\). By weighting the attention, noise in the model amplified/caused by applying the multi-layer transition process is reduced or eliminated.\n\\[\\mathcal{R}(x) = (\\bar{A}^1 \\cdot \\bar{A}^2 \\cdot \\ldots \\ldots \\bar{A}^L) \\odot SG(x_e)\\]\nIn ViT, as shown in Figure 4 the [CLS] token serves as a class token, aggregating information from the entire input sequence that encapsulates global context information of the image. To retrieve per-token relevance for classification tasks, we take the row corresponding to [CLS] token, namely \\(\\mathcal{R}[0]\\), then skip the first [CLS] token to get the final attribution vectors \\(\\mathcal{R}[0, 1:]\\) corresponding to the input patch sequences. We generate the visualization map by reshaping the attribution vectors to a matrix, then up-sampling it back to the original input image size by bilinear interpolation with the scale factor of patch size p, as is illustrated in the Appendix Algorithm 1."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "The Berkeley Deep Drive dataset 3(BDD-100K)[56] contains 100,000 images with 2D bounding boxes, lane markings, and full-frame instance segmentation. As an extension of BDD-100K, BDD-OIA 4 [55] collected images of complicated scenes (#pedestrians > 5 or #vehicles > 5) from the original BDD-100K dataset and then annotated them with four action categories: move forward, slow down, turn left and turn right. The total 22k annotated images are split into subsets of 16k for the training, 2k for the validation and 4k for the test set. To our knowledge, BDD-OIA is the only publicly available annotated driving action dataset with binary labelling: potentially executable actions are marked as 1, while the rest are marked as 0, e.g., [1,1,0,0], which means you can move forward or slow down. Samples of the images and their annotation can be seen in Appendix Figure 8. Both the aforementioned datasets are used in this work. The size of the RGB image in both datasets equals 720 x 1280.\nThe model training details can be found in Appendix A.1."}, {"title": "4.2 Baseline explainable methods", "content": "We chose the four currently most representative attention-based explainable methods plus one self-proposed method, i.e., AttIG, and compared SNNA with them through qualitative and quantitative experiments. RawAtt[22] utilize the raw attention weights from the last layer \\(A^l\\), but can only get class-agnostic attribution. AttGrad[10] regard the partial derivative of the output to the input as a measure of the network's sensitivity for each input dimension and element-wise product of the attention matrix with Attention Gradient as the"}, {"title": "4.3 Qualitative evaluation", "content": "The qualitative evaluation is based on the inspection of the produced attribution maps. However, this introduces a strong bias in the evaluation. Humans judge methods more favourably that produce explanations closer to their expectations at the cost of penalizing methods that might more closely reflect the network behaviour. There are limitations to the qualitative evaluation of attribution maps due to biases in human intuition towards simplicity. Therefore, apart from qualitative comparison, we use quantitative metrics defined in Section 4.4 as a complementary for more trustworthy evaluation."}, {"title": "4.4 Quantitative evaluation", "content": "A common method in model interpretability involves systematically removing or altering input features and observing the impact on the model's output. The significance of input features for the final decision can be measured by the change in model performance without them. Removing features with high attribution scores should decrease the performance, while discarding features with low attribution scores has less effect on the model's performance [54].\nFaithfulness quantifies the fidelity of an explanation technique by measuring if the identified tokens impact the output. We regard the attribution value as a relevance score; the larger the value, the stronger the correlation, and perform positive perturbation tests: tokens are removed from the highest relevance to the lowest, to evaluate the explanation faithfulness by Area-under-the-perturbation-curve (AUPC) [14] and Log-odds scores [41] metrics. We gradually \"remove\" the input features of a given input and measure the accuracy of the network. In positive perturbation, We expect a steep decrease in performance, indicating that the removed tokens are important to the classification score.\nAUPC measures the total area under the accuracy perturbation curve. Here, we calculate the average perturbation accuracy overall test examples under masking of the top k% pixels/token/attention, then approximate the AUPC by summing over the candidates k set.\n\\[AUPC = \\sum_{k}\\frac{1}{N} \\sum_{i=1}^{N} Acc(\\mathcal{F}(x_i))\\]\nLogOdd calculates the average difference of logarithmic probabilities after and before masking the top k% pixels/token/attention over all test examples, then sum over the candidates k set.\n\\[LogOdd = \\sum_{k}\\frac{1}{N} \\sum_{i=1}^{N} log\\frac{Acc(\\mathcal{F}^k(x_i))}{Acc(\\mathcal{F}(x_i))}\\]\nN is the number of test samples, \\(x_i\\) is the test sample, and \\(\\mathcal{F}^k\\) is the masked sample after masking the top k% pixels/token/attention. Acc is the multi-label classification accuracy. \\(\\sum_k\\) means sum over candidates k set. Directly removing specified tokens from the original"}, {"title": "4.5 Replacement Methods", "content": "Pixel Mask For ViT, the input image will undergo normalization as part of pre-processing. This means that ViT does not actually \"see\" the raw pixel values; instead, it sees normalized values, where the mean pixel value of the image dataset might not be 0. Therefore, setting the pixel value to 0 (black) could introduce a significant statistical outlier for the model rather than a neutral or \"uninformative\" input. It can introduce biases and disrupt the context or texture continuity of the surrounding area, potentially leading the model to focus on the edges of the masked area rather than ignoring it. So, we replace the corresponding patch pixel with the per channel-wise averaged value of the input image.\nToken Mask Setting the token value of a specific patch to the zero vector means essentially removing the information associated with that patch from the input representation [33]. This forces the model to redistribute its attention to the remaining non-zero tokens. Using a zero vector to replace a patch's embedding is straightforward and does not disrupt the input's expected shape or the subsequent processing pipeline. As a result, we can maintain the original dimensional structure of the input and ensure that the model architecture does not need any modification to accommodate the masked input. Hence, we replace the image patch tokens with the zero vector.\nAttention Mask Sets the attention weights for the masked tokens to 0, so the masked parts will not be forwarded in the network any more [44]. This approach is grounded by the attention mechanism, where attention weights determine the importance of each token when computing the output of the attention layers. By setting the attention weights of certain tokens to zero, we effectively remove their influence on the subsequent layers, allowing us to assess how critical they are for the model's decision."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Qualitative", "content": "Figure 6 presents a visual comparison between SNNA and various other baselines. Where RawAtt create a very scattered saliency map, many unrelated areas are marked as significant, like the indicator lights on the far left of the picture, as well as the flower beds and the traffic sign in the distance. AttGrad and AttIN have similar results, but strangely, two small areas at the bottom of the picture are highlighted to the left and right front of the car. Overall, the results of GenericAtt and AttIG are the best among baselines, but they still have some puzzling noise. From Figure 6, we can see that our method provides clearer and more intuitive visualization with respect to the baselines, which produce more or less noise attribution. More convincing visual results can be found in Appendix Figure 10."}, {"title": "5.2 Quantitative", "content": "Due to computational costs, all the experiment results are averaged on 1000 samples randomly chosen from the BDD-OIA testing dataset. As the saliency pixel in driving scenarios normally accounts for a relatively small area, we focus our ablation experiment"}, {"title": "6 Conclusion", "content": "This study addresses significant challenges in generating clear and confident explanations for Transformer-based models by introducing a novel approach termed Smooth Noise Norm Attention (SNNA). It utilizes attention weights, value vectors, and gradients to calculate attribution scores and quantify input features' influence on model outputs. These attribution scores provide the significance of the input features for the model's decision. Our experimental results reveal that SNNA can produce clear attribution maps that are more relevance-sensitive and with much less noise than baseline methods. We also demonstrate that SNNA outperforms baseline methods in ranking the importance of the input pixels regarding the quantitative metrics. Although our current SNNA method is specifically implemented on ViT for image classification tasks, it can be naturally applied to any Transformer-based architectures in future research. As there is no ground truth annotation for the \"noisy\" pixel, the investigation regarding the faithfulness of the salient pixels to the model's decision remains to be investigated in future research. Due to the lack of credible evaluation metrics, how to fairly and effectively assess the effectiveness of XAI methods remains to be explored."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation details", "content": "We train the classification model on the previously introduced two datasets. We chose ViT-S/8 with L = 12 Transformer encoder layers for this work. We first initialize our model from the pretrained checkpoint (ViT-S/8) on ImageNet and train it on the unlabelled BDD-100k training dataset with a batch size of 16 for 200 epochs on two Nvidia A100 (80GB) GPUs. Then, we freeze the backbone network and fine-tune the classification head with the labelled data for 100 epochs (Suggested by DINO). During pretraining, we follow the suggestion of DINO to randomly crop and then resize the original image to two 224 \u00d7 224 bigger crops and eight 96 \u00d7 96 small crops for the teacher and student network separately. Considering the computational resource limits, we resized the input image to 360 \u00d7 640 during supervised fine-tuning. Our final trained driving-action prediction model gets 74.6% multi-label classification accuracy on the BDD-OIA testing dataset, which is higher than the baseline [55] accuracy of 73.4%."}]}