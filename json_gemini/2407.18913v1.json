{"title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments", "authors": ["Shu Ishida", "Jo\u00e3o F. Henriques"], "abstract": "This work compares ways of extending Reinforcement Learning algorithms to Partially Observed Markov Decision Processes (POMDPs) with options. One view of options is as temporally extended action, which can be realized as a memory that allows the agent to retain historical information beyond the policy's context window. While option assignment could be handled using heuristics and hand-crafted objectives, learning temporally consistent options and associated sub-policies without explicit supervision is a challenge. Two algorithms, PPOEM and SOAP, are proposed and studied in depth to address this problem. PPOEM applies the forward-backward algorithm (for Hidden Markov Models) to optimize the expected returns for an option-augmented policy. However, this learning approach is unstable during on-policy rollouts. It is also unsuited for learning causal policies without the knowledge of future trajectories, since option assignments are optimized for offline sequences where the entire episode is available. As an alternative approach, SOAP evaluates the policy gradient for an optimal option assignment. It extends the concept of the generalized advantage estimation (GAE) to propagate option advantages through time, which is an analytical equivalent to performing temporal back-propagation of option policy gradients. This option policy is only conditional on the history of the agent, not future actions. Evaluated against competing baselines, SOAP exhibited the most robust performance, correctly discovering options for POMDP corridor environments, as well as on standard benchmarks including Atari and MuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The open-sourced code is available at https://github.com/shuishida/SoapRL.", "sections": [{"title": "1 Introduction", "content": "While deep Reinforcement Learning (RL) has seen rapid advancements in recent years, with numerous real-world applications such as robotics (Gu et al., 2017; Akkaya et al., 2019; Haarnoja et al., 2024), gaming (Van Hasselt et al., 2016; Arulkumaran et al., 2019; Baker et al., 2022), and autonomous vehicles (Kendall et al., 2019; Lu et al., 2023), many algorithms are limited by the amount of observation history they plan on. Developing learnable embodied agents that plan over a wide spatial and temporal horizon has been a longstanding challenge in RL.\nWith a simple Markovian policy \\( \\pi(a_t|s_t) \\), the agent's ability to make decisions is limited by only having access to the current state as input. Early advances in RL were made on tasks that either adhere to the Markov assumption that the policy and state transitions only depend on the current state, or those can be solved by frame stacking (Mnih et al., 2015) that grants the policy access to a short history. However, many real-world tasks are better modeled as Partially Observable Markov Decision Processes (POMDPs) (\u00c5str\u00f6m, 1965), and necessitate solutions that use working memory. The history of the agent's trajectory also contains signals to inform the agent to make a more optimal decision. This is due to the reward and next state distribution \\( p(r_t, s_{t+1}|s_{0:t}, a_{0:t}) \\) being conditional on the past states and actions, not just on the current state and action."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Partially Observable Markov Decision Process", "content": "POMDP is a special case of an Markov Decision Process (MDP) where the observation available to the agent only contains partial information of the underlying state. In this work, s is used to denote the (partial) state given to the agent, which may or may not contain the full information of the environment (which shall be distinguished from state \\( \\bar{s} \\) as the underlying state \\( \\bar{s} \\).\u00b9 This implies that the \u201cstate\u201d transitions are no longer fully Markovian in a POMDP setting, and may be correlated with past observations and actions. Hence, \\( p(r_t, s_{t+1}|s_{0:t}, a_{0:t}) \\) describes the full state and reward dynamics in the case of POMDPs, where \\( s_{0:t} \\) is a shorthand for \\( {s_t|t_1 \\leq t \\leq t_2} \\), and similarly with \\( a_{0:t} \\)."}, {"title": "2.2 The Options Framework", "content": "Options (Sutton et al., 1999; Precup & Sutton, 2000) are temporally extended actions that allow the agent to make high-level decisions in the environment. Each option corresponds to a specialized low-level policy that the agent can use to achieve a specific subtask. In the Options Framework, the inter-option policy \\( \\pi(z_t|s_t) \\) and an option termination probability \\( w(s_{t+1}, z_t) \\) govern the transition of options, where \\( z_t \\) is the current option, and are chosen from an n number of discrete options \\( {Z_1, ..., Z_n} \\). Options are especially valuable when there are multiple stages in a task that must be taken sequentially (e.g. following a recipe) and the agent must obey different policies given similar observations, depending on the stage of the task.\nEarlier works have built upon the Options Framework by either learning optimal option selection over a pre-determined set of options (Peng et al., 2019) or using heuristics for option segmentation (Kulkarni et al., 2016; Nachum et al., 2018), rather than a fully end-to-end approach. While effective, such approaches constrain the agent's ability to discover useful skills automatically. The Option-Critic architecture (Bacon et al., 2017) proposed end-to-end trainable systems which learn option assignment. It formulates the problem such that inter-option policies and termination conditions are learned jointly in the process of maximizing the expected returns."}, {"title": "2.3 Option-Critic architecture", "content": "As mentioned in Section 2.2, the options framework (Sutton et al., 1999; Precup & Sutton, 2000) formalizes the idea of temporally extended actions that allow agents to make high-level decisions. Let there be n discrete options \\( {Z_1, ..., Z_n} \\) from which \\( z_t \\) is chosen and assigned at every time step t. Each option corresponds to a specialized sub-policy \\( \\pi_{\\theta}(a_t|s_t, z_t) \\) that the agent can use to achieve a specific subtask. At t = 0, the agent chooses an option according to its inter-option policy \\( \\pi_{\\varphi}(z_t|s_t) \\) (policy over options), then follows the option sub-policy until termination, which is dictated by the termination probability function \\( w_{\\psi}(s_t, z_{t-1}) \\). Once the option is terminated, a new option \\( z_t \\) is sampled from the inter-option policy and the procedure is repeated."}, {"title": "2.4 Expectation Maximization algorithm", "content": "The EM algorithm (Dempster et al., 1977) is a well-known method for learning the assignment of latent variables, often used for unsupervised clustering and segmentation. The k-means clustering algorithm (Forgy, 1965) can be considered a special case of EM. The following explanation in this section is a partial summary of Chapter 9 of Bishop's book (Bishop, 2006)."}, {"title": "2.5 Forward-backward algorithm", "content": "The EM algorithm can also be applied in an HMM setting for sequential data, resulting in the forward-backward algorithm, also known as the Baum-Welch algorithm (Baum, 1972)."}, {"title": "3 Option assignment formulation", "content": "The aim is to learn a diverse set of options with corresponding policy and value estimates, such that each option is responsible for accomplishing a well-defined subtask, such as reaching a certain state region. At every time step t, the agent chooses an option \\( z_t \\) out of n number of discrete options \\( {Z_1, ..., Z_n} \\)."}, {"title": "3.1 Option policy and sub-policy", "content": "The goal is to learn a sub-policy \\( \\pi_{\\theta}(a|s, z) \\) conditional to a latent option variable z, and an option policy \\( \\pi_{\\psi}(z'|s, a, z) \\) used to iteratively assign options at each time step, to model the joint option policy\n\\[p_{\\Theta}(a_t, z_{t+1} | s_t, z_t) = \\pi_{\\theta}(a_t | s_t, z_t) \\pi_{\\psi}(z_{t+1} | s_t, a_t, z_t).\\]\nHere, the learnable parameter set of the policy is denoted as \\( \\Theta = {\\theta, \\psi} \\)."}, {"title": "3.2 Evaluating the probability of latents", "content": "Let us define an auto-regressive action probability \\( a_t := p(a_t | s_{0:t}, a_{0:t-1}) \\), an auto-regressive option forward distribution \\( \\zeta(z_t) := p(z_t | s_{0:t}, a_{0:t-1}) \\), and an option backward feedback \\( \\beta(z_t) := p(s_{t:T}, a_{t:T-1} | s_{t-1}, a_{t-1}, z_t) \\). Notice that the definitions of action probability a, option forward \\( \\zeta(z_t) \\), and option backward \\( \\beta(z_t) \\) resemble \\( c_t \\), \\( \\alpha(z_t) \\) and \\( \\beta(z_t) \\) defined in Section 2.5, respectively. While it is common practice to denote the forward and backward quantities as \\( \\alpha \\) and \\( \\beta \\) in the forward-backward algorithm (also known as the \\( \\alpha-\\beta \\) algorithm), here \\( a_t \\) is redefined to denote the action probability (corresponding to the normalizing constant \\( c_t \\)), and \\( \\zeta(z_t) \\) for the option forward distribution, to draw attention to the fact that these are probabilities of option \\( z_t \\) and action \\( a_t \\), respectively."}, {"title": "4 Proximal Policy Optimization via Expectation Maximization", "content": "In this section, PPOEM is introduced, an algorithm that extends PPO for option discovery with an EM objective. The expectation of the returns is taken over the joint probability distribution of states, actions and options, sampled by the policy. This objective gives a tractable objective to maximize, which has a close resemblance to the forward-backward algorithm."}, {"title": "4.1 Expected return maximization objective with options", "content": "The objective is to maximize the expectation of returns \\( R(\\tau) \\) for an agent policy \\( \\pi \\) over a trajectory \\( \\tau \\) with latent option \\( z_t \\) at each time step t. The definition of a trajectory \\( \\tau \\) is a set of states, actions and rewards visited by the agent policy in an episode. The objective \\( J[\\pi] \\) can be written as:\n\\[J[\\pi_{\\theta}] = \\mathbb{E}_{\\tau, Z \\sim \\pi} [R(\\tau)] = \\int_{T, Z} R(\\tau) p(\\tau, Z | \\Theta).\\]\nTaking the gradient of the maximization objective,\n\\[\\nabla_{\\theta} J[\\pi_{\\theta}] = \\int_{T, Z} R(\\tau) \\nabla_{\\theta} p(\\tau, Z | \\Theta) d\\tau = \\int_{T, Z} R(\\tau) \\frac{\\nabla_{\\theta} p(\\tau, Z | \\Theta)}{p(\\tau, Z | \\Theta)} p(\\tau, Z | \\Theta) d\\tau = \\mathbb{E}[R(\\tau) \\nabla_{\\theta} \\log p(\\tau, Z | \\Theta)].\\]"}, {"title": "4.1.1 Relationship with Expectation Maximization", "content": "The objective derived in Equation (23) closely resembles the objective of the EM algorithm applied to the HMM with options as latent variables. The expectation of the marginal log-likelihood \\( Q(\\Theta; \\Theta^{old}) \\), which gives the lower-bound of the marginal log-likelihood \\( \\log p(T|\\Theta) \\), is given by\n\\[Q(\\Theta; \\Theta^{old}) = \\mathbb{E}_{Z \\sim p(T, \\Theta^{old})} [\\ln p(\\tau, Z | \\Theta)] = \\int \\frac{p(T, Z | \\Theta)}{\\int_Z \\tau, \\Theta^{old})} \\ln p(\\tau, Z | \\Theta) dZ\\]\n\\[= \\sum_{T} \\sum_{z_t, z_{t+1}} [p(z_t, z_{t+1} | \\tau, \\Theta^{old}) \\log p_{\\theta}(a_t, z_{t+1} | s_t, z_t)] + const.\\]\nThe difference is that the expected return maximization objective in Equation (23) weights the log probabilities of the policy according to the returns, whereas the objective of Equation (25) is to find a parameter set \\( \\Theta \\) that maximizes the probability that the states and actions that appeared in the trajectory are visited by the joint option policy \\( p_{\\Theta} \\)."}, {"title": "4.2 PPO objective with Generalized Advantage Estimation", "content": "A standard optimization technique for neural networks using gradient descent can be applied to optimize the policy network. Noticing that the optimization objective in Equation (23) resembles the policy gradient algorithm, the joint option policy can be optimized using the PPO algorithm instead to prevent the updated policy \\( p_{\\theta} (a_t, z_{t+1} | s_t, z_t) \\) from deviating from the original policy too much.\nSeveral changes have to be made to adapt the training objective to PPO. Firstly, \\( \\nabla \\log p_{\\theta} \\) is replaced by \\( \\frac{V\\pi_{\\theta}}{\\piold} \\), its first order approximation, to easily introduce clipping constraints to the policy ratios. Secondly, the return \\( R(T) \\) is replaced with the GAE, \\( A^{GAE} \\), as described in Equation (3).\nExtending the definition of GAE to work with options,\n\\[A^{GAE}_{t}(z_{t}, z_{t+1} | \\tau) = r_{t} + \\gamma V(s_{t+1}, z_{t+1}) - V(s_{t}, z_{t}) + \\lambda \\gamma (1 - d_t) A^{GAE}_{t+1}(z_{t+1} | \\tau)\\]\n\\[A^{GAE}_{t}(z_t | \\tau) = \\sum_{z_{t+1}} p(z_{t+1} | z_t, \\tau) A^{GAE}_{t}(z_t, z_{t+1} | \\tau).\\]\nThe GAE could be evaluated backwards iteratively, starting from t = T with the initial condition \\( A^{GAE}(z_{t+1} | \\tau) = 0 \\). The option transition function \\( p(z_{t+1} | z_t, \\tau) \\) can be evaluated using \\( p(z_t, z_{t+1} | \\tau) \\) (Equation (24)) as:\n\\[p(z_{t+1} | z_t, \\tau) = \\frac{p(z_t, z_{t+1} | \\tau)}{\\sum_{z_{t+1}} p(z_t, z_{t+1} | \\tau)}.\\]\nThe target value \\( V_{target}(s_t, z_t) \\) to regress the estimated value function towards can be defined in terms of the GAE and the current value estimate as:\n\\[V_{target}(s_t, z_t) = V^{\\pi}(s_t, z_t) + A^{GAE}(z_t | \\tau).\\]"}, {"title": "5 Sequential Option Advantage Propagation", "content": "In the previous section, assignments of the latent option variables Z were determined by maximizing the expected return for complete trajectories. The derived algorithm resembles the forward-backward algorithm closely, and requires the backward pass of \\( \\beta(z_t) \\) in order to fully evaluate the option probability \\( p(Z|\\tau) \\). During rollouts of the agent policy, however, knowing the optimal assignment of latents \\( p(z_t|t) \\) in advance is not possible, since the trajectory is incomplete and the backward pass has not been initiated. Therefore, the policy must rely on the current best estimate of the options given its available past trajectory \\( {s_{0:t}, a_{0:t}} \\) during its rollout. This option distribution conditional only on its past is equivalent to the auto-regressive option forward distribution \\( \\zeta(z_t) := p(z_t | s_{0:t}, a_{0:t-1}) \\).\nSince the optimal option assignment can only be achieved in hindsight once the trajectory is complete, this information is not helpful for the agent policy upon making its decisions. A more useful source of information for the agent, therefore, is the current best estimate of the option assignment \\( \\zeta(z_t) \\). It is sensible, therefore, to directly optimize for the expected returns evaluated over the option assignments \\( \\zeta(z_t) \\) to find an optimal option policy, rather than optimizing the expected returns for an option assignment \\( p(Z|\\tau) \\), which can only be known in hindsight.\nThe following section proposes a new option optimization objective that does not involve the backward pass of the EM algorithm. Instead, the option policy gradient for an optimal forward option assignment is evaluated analytically. This results in a temporal gradient propagation, which corresponds to a backward pass, but with a slightly different outcome. Notably, this improved algorithm, SOAP, applies a normalization of the option advantages in every back-propagation step through time.\nAs far as the authors are aware, this work is the first to derive the back propagation of policy gradients in the context of option discovery."}, {"title": "5.1 Policy Gradient objective with options", "content": "Let us start by deriving the policy gradient objective assuming options. The maximization objective \\( J[\\pi] \\) for the agent can be defined as:\n\\[J[\\pi_{\\theta}] = \\mathbb{E} [R(T)] = \\int R(\\tau) p(\\tau) d\\tau.\\]\nTaking the gradient of the maximization objective,\n\\[\\nabla_{\\theta} J[\\pi_{\\theta}] = \\int R(\\tau) \\nabla_{\\theta} p(\\tau) d\\tau = \\int R(\\tau) \\frac{\\nabla_{\\theta} p(\\tau)}{p(\\tau)} p(\\tau) d\\tau = \\mathbb{E}[R(r) \\nabla_{\\theta} \\log p(\\tau|\\Theta)].\\]\nSo far, the above derivation is the same as the normal policy gradient objective without options. Next, the likelihood for the trajectory T is given by:\n\\[p(\\tau|\\Theta) = p(s_{0:T}, a_{0:T-1}|\\Theta) = p(s_0) \\Pi_{t=0}^{T-1} [p(a_t|s_{0:t}, a_{0:t-1}, \\Theta) P(s_{t+1}|s_{0:t}, a_{0:t})].\\]\nThis is where options become relevant, as the standard formulation assumes that the policy \\( \\pi(a|s) \\) is only dependent on the current state without history, and similarly that the state transition environment dynamics \\( P(s'|s, a) \\) is Markovian given the current state and action. In many applications, however, the states that are observed do not contain the entire information about the underlying dynamics of the environment\u00b2, and therefore, conditioning on the history yields a different distribution of future states compared to conditioning on just the current state. To capture this, the policy and state transitions are now denoted to be \\( p(a_t | s_{0:t}, a_{0:t-1}) \\) and \\( P(s_{t+1}| s_{0:t}, a_{0:t}) \\), respectively. Here, the probabilities are conditional on the historical observations (\\( s_{0:t} \\)) and historical actions (e.g. \\( a_{0:t} \\)), rather than just the immediate state \\( s_t \\) and action \\( a_t \\). Note that \\( p(a_t | s_{0:t}, a_{0:t-1}) \\) is a quantity \\( a_t \\) that has already been evaluated in Section 3.2.\nEvaluating \\( \\nabla_{\\Theta} \\log p(\\tau|\\Theta) \\), the log converts the products into sums, and the terms that are constant with respect to \\( \\Theta \\) are eliminated upon taking the gradient, leaving\n\\[\\nabla_{\\theta} \\log p(\\tau|\\Theta) = \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log p(a_t | s_{0:t}, a_{0:t-1}, \\Theta) = \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\log a_t = \\sum_{t=0}^{T-1} \\frac{\\nabla_{\\theta} a_t}{a_t},\\]\nwhere \\( a_t \\) is substituted following its definition in Section 3.2.\nSubstituting Equation (33) into Equation (31),\n\\[\\nabla_{\\theta} J[\\theta] = \\mathbb{E}[R(T) \\nabla_{\\theta} \\log p(\\tau|\\Theta)] = \\mathbb{E}\\left[ \\sum_{t=0}^{T-1} R(T) \\frac{\\nabla_{\\theta} a_t}{a_t} \\right].\\]"}, {"title": "5.2 Analytic back-propagation of the policy gradient", "content": "If a forward pass of the policy can be made in one step over the entire trajectory, a gradient optimization on the objective can be performed directly. However, this would require storing the entire trajectory in GPU memory, which is highly computationally intensive. Instead, this section analytically evaluates the back-propagation of gradients of the objective so that the model can be trained on single time-step rollout samples during training.\nGradient terms appearing in Equation (38) are either \\( \\nabla \\zeta(z_t) \\) or \\( \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\) for \\( 0 \\leq t \\leq T - 1 \\). While \\( p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\) is approximated by neural networks and can be differentiated directly, \\( \\nabla \\zeta(z_{t+1}) \\) has to be further expanded to evaluate the gradient in recursive form as:\n\\[\\nabla \\zeta(z_{t+1}) = \\nabla \\frac{\\sum_{z_t} \\zeta(z_t) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)}{a_t} = \\sum_{z_t} \\nabla \\frac{\\zeta(z_t) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)}{a_t} - \\zeta(z_{t+1}) \\sum_{z_t} \\nabla \\frac{\\zeta(z_t) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)}{a_t}.\\]\nUsing Equation (39), it is possible to rewrite the \\( \\nabla \\zeta(z_{t+1}) \\) terms appearing in Equation (38) in terms of \\( \\nabla \\zeta(z_t) \\) and \\( \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\). Defining the coefficients of \\( \\nabla \\zeta(z_{t+1}) \\) in Equation (38) as option utility \\( U(z_{t+1}) \\),\n\\[\\sum U(z_{t+1}) \\nabla \\zeta(z_{t+1}) = \\frac{1}{A_t} \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} \\left[ \\nabla ( \\zeta(z_t) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)) - \\zeta(z_{t+1}) \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} U(z_{t+1}) \\zeta(z_{t+1}) \\nabla ( \\zeta(z_t) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)) \\right]\\]\n\\[= \\frac{1}{A_t} \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} \\left[ U(z_{t+1}) \\zeta(z_{t+1}) p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\nabla \\zeta(z_t) + U(z_{t+1}) \\zeta(z_{t+1}) \\zeta(z_t) \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\right].\\]\nThus, the occurrences of gradients \\( \\nabla \\zeta(z_{t+1}) \\) have been reduced to terms with \\( \\nabla \\zeta(z_t) \\) and \\( \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\).\nApplying this iteratively to Equation (38), starting with t = T - 1 in reverse order, Equation (38) could be expressed solely in terms of gradients \\( \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\). Defining the coefficients of \\( \\nabla p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\) as policy gradient weighting \\( W_t (z_t, z_{t+1}) \\),\n\\[A^{GOA}_{t}(z_{t+1}) = \\sum_{z_t} A^{GAE}(z_t) \\zeta(z_t) + (1 - d_t) \\frac{\\sum U (z_{t+1}) - \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} U(z_{t+1}) \\zeta(z_{t+1})}{\\zeta(z_{t+1})},\\]\n\\[U(z_{t}) = \\frac{\\sum_{z_{t+1}} A^{GOA}_{t}(z_{t+1}) p_{\\theta}(a_t, z_{t+1} | s_t, z_t)}{A_t},\\]\n\\[W(z_t, z_{t+1}) = \\frac{A^{GOA}_{t}(z_{t+1}) \\zeta(z_t)}{A_t},\\]\nwhere \\( A^{GOA}_{t}(z_{t+1}) \\) is a new quantity derived and introduced in this work as Generalized Option Advantage (GOA), which is a term that appears in evaluating \\( U(z_t) \\) and \\( W(z_t, z_{t+1}) \\).\nRewriting the policy gradient objective in Equation (38) with the policy gradient weighting,\n\\[\\nabla_{\\theta} J[\\pi_{\\theta}] = \\mathbb{E}\\left[ \\frac{1}{A_t} \\sum_{t=0}^{T-1} \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} A^{GOA}_{t}(z_{t+1}) \\zeta(z_t) \\nabla_{\\theta} p_{\\theta}(a_t, z_{t+1} | s_t, z_t) \\right].\\]"}, {"title": "5.3 Learning objective for option-specific policies and values", "content": "The training objective given in Equation (42) is modified so that it could be optimized with PPO. Unlike in Section 4.2, the training objective is written in terms of \\( p_{\\theta} \\) and not \\( \\nabla \\log p_{\\theta} \\). Therefore, the clipping constraints are applied to \\( p_{\\theta} \\) directly, limiting it to the range of \\( (1 - \\epsilon)p_{\\theta}^{old} \\) and \\( (1 + \\epsilon)p_{\\theta}^{old} \\). The resulting PPO objective is:\n\\[J_{\\theta} = \\mathbb{E}_{s_t, a_t \\sim \\pi} \\left[ \\sum_{\\begin{subarray}{c} z_t, z_{t+1} \\end{subarray}} \\min \\left( \\frac{\\pi_{\\theta} (a_t, z_{t+1} | s_t, z_t)}{\\piold (a_t, z_{t+1} | s_t, z_t)} A^{GOA}(z_{t+1}), clip \\left( \\frac{\\pi_{\\theta} (a_t, z_{t+1} | s_t, z_t)}{\\piold (a_t, z_{t+1} | s_t, z_t)}, (1 - \\epsilon) , (1 + \\epsilon) \\right) A^{GOA} (z_{t+1}) \\right) \\right].\\]\nThe option-specific value function \\( V_{\\phi} (s_t, z_t) \\) parameterized by \\( \\phi \\) can be learned by regressing towards the target values \\( V_{target}(s_t, z_t) \\) evaluated in Equation (37) for each state \\( s_t \\) and option \\( z_t \\) sampled from the policy and option-forward probability, respectively. Defining the objective function for the value regression as \\( J_\\phi \\),\n\\[J_{\\phi} = \\mathbb{E}_{s_t \\sim \\pi, z_t \\sim \\zeta} [V_{target} (s_t, z_t) - V_{\\phi}(s_t, z_t)]^2.\\]\nThe final training objective is to maximize the following:\n\\[J_{SOAP} = J_{\\theta} + J_{\\phi}.\\]"}, {"title": "6 Experiments", "content": "Experiments were conducted on a variety of RL agents: PPO, PPOC, Proximal Policy Optimization with Long Short-Term Memory (PPO-LSTM), PPOEM (ours), and SOAP (ours). PPO (Schulman et al., 2017) is a baseline without memory, PPOC (Klissarov et al., 2017) implements the Option-Critic algorithm, PPO-LSTM implements a recurrent policy with latent states using an LSTM (Hochreiter & Schmidhuber, 1997), PPOEM is the algorithm developed in the first half of this work that optimizes the expected returns using the forward-backward algorithm, and SOAP is the final algorithm proposed in this work that uses an option advantage derived by analytically evaluating the temporal propagation of the option policy gradients. SOAP mitigates the deficiency of PPOEM that the training objective optimizes the option assignments over a full trajectory which is typically only available in hindsight; SOAP optimizes the option assignments given only the history of the trajectory instead, making the optimization objective better aligned with the task objective.\nThe aim is to (a) show and compare the option learning capability of the newly developed algorithms, and (b) assess the stability of the algorithms on standard RL environments. All algorithms use PPO as the base policy optimizer, and share the same backbone and hyperparameters, making it a fair comparison. All algorithms use Stable Baselines 3 (Raffin et al., 2021) as a base implementation with the recommended tuned hyperparameters for each environment. In the following experiments, the number of options was set to 4."}, {"title": "6.1 Option learning in corridor environments", "content": "A simple environment of a corridor with a fork at the end is designed as a minimalistic and concrete example where making effective use of latent variables to retain information over a sequence is necessary to achieve the agent's goal."}, {"title": "6.2 Stability of the algorithms on CartPole, LunarLander, Atari, and MuJoCo environments", "content": "Experiments were also conducted on standard RL environments to evaluate the stability of the algorithms with options. Results for CartPole-v1 and LunarLander-v2 are shown in Figure 6, and results on 10 Atari environments (Bellemare et al., 2013) and 6 MuJoCo environments (Todorov et al., 2012) are shown in Figure 7 and Figure 8, respectively. There was no significant difference in performances amongst the algorithms for simpler environments like CartPole and LunarLander, with PPO-LSTM slightly outperforming others. For the Atari and MuJoCo environments, however, there was a consistent trend that SOAP achieves similar performances (slightly better in some cases, slightly worse in others) to the vanilla PPO, while PPOEM, PPO-LSTM and PPOC were significantly less stable to train. It could be hypothesized that, similarly to Section 6.1, the policy of PPOC disregarded the information of past options when choosing the next option, which is why the performance was unstable with larger environments. Another point of consideration is that, with N number of options, there are N number of sub-policies to train, which becomes increas-ingly computationally expensive and requires many visits to the state-option pair in the training data, especially when using a Monte Carlo estimate by sampling the next option as is done in PPOC instead of maintaining a distribution of the option \\( \\zeta(z_t) \\) as in PPOEM and SOAP. As for"}]}