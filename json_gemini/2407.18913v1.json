[{"title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments", "authors": ["Shu Ishida", "Jo\u00e3o F. Henriques"], "abstract": "This work compares ways of extending Reinforcement Learning algorithms to Partially Observed Markov Decision Processes (POMDPs) with options. One view of options is as temporally extended action, which can be realized as a memory that allows the agent to retain historical information beyond the policy's context window. While option assignment could be handled using heuristics and hand-crafted objectives, learning temporally consistent options and associated sub-policies without explicit supervision is a challenge. Two algorithms, PPOEM and SOAP, are proposed and studied in depth to address this problem. PPOEM applies the forward-backward algorithm (for Hidden Markov Models) to optimize the expected returns for an option-augmented policy. However, this learning approach is unstable during on-policy rollouts. It is also unsuited for learning causal policies without the knowledge of future trajectories, since option assignments are optimized for offline sequences where the entire episode is available. As an alternative approach, SOAP evaluates the policy gradient for an optimal option assignment. It extends the concept of the generalized advantage estimation (GAE) to propagate option advantages through time, which is an analytical equivalent to performing temporal back-propagation of option policy gradients. This option policy is only conditional on the history of the agent, not future actions. Evaluated against competing baselines, SOAP exhibited the most robust performance, correctly discovering options for POMDP corridor environments, as well as on standard benchmarks including Atari and MuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The open-sourced code is available at https://github.com/shuishida/SoapRL.", "sections": [{"title": "1 Introduction", "content": "While deep Reinforcement Learning (RL) has seen rapid advancements in recent years, with numerous real-world applications such as robotics (Gu et al., 2017; Akkaya et al., 2019; Haarnoja et al., 2024), gaming (Van Hasselt et al., 2016; Arulkumaran et al., 2019; Baker et al., 2022), and autonomous vehicles (Kendall et al., 2019; Lu et al., 2023), many algorithms are limited by the amount of observation history they plan on. Developing learnable embodied agents that plan over a wide spatial and temporal horizon has been a longstanding challenge in RL.\nWith a simple Markovian policy \u03c0(at|st), the agent's ability to make decisions is limited by only having access to the current state as input. Early advances in RL were made on tasks that either adhere to the Markov assumption that the policy and state transitions only depend on the current state, or those can be solved by frame stacking (Mnih et al., 2015) that grants the policy access to a short history. However, many real-world tasks are better modeled as Partially Observable Markov Decision Processes (POMDPs) (\u00c5str\u00f6m, 1965), and necessitate solutions that use working memory. The history of the agent's trajectory also contains signals to inform the agent to make a more optimal decision. This is due to the reward and next state distribution p(rt, st+1|80:t, a0:t) being conditional on the past states and actions, not just on the current state and action."}, {"title": "", "content": "A common approach of accommodating POMDPs is to learn a latent representation using sequential policies, typically using a Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Unit (GRU) (Cho et al., 2014) or Transformer (Vaswani et al., 2017). This will allow the policy to gain access to signals from the past. Differentiable planners (Tamar et al., 2016; Lee et al., 2018; Ishida & Henriques, 2022) are another line of work that incorporate a learnable working memory into the system. However, these approaches have an inherent trade-off between the duration of history it can retain (defined by the policy's context window size) and the compute and training data required to learn the policy. This is because the entire history of observations within the context window have to be included in the forward pass at training time to propagate useful gradients back to the sequential policy. Another caveat is that, with larger context windows, the input space is less constrained and it becomes increasingly unlikely that the agent will revisit the same combination of states, which makes learning the policy and value function sample-expensive, and potentially unstable at inference time if the policy distribution has changed during training.\nTraining RL agents to work with longer working memory is a non-trivial task, especially when the content of the memory is not pre-determined and the agent also has to learn to store information relevant to each task. With the tasks that the RL algorithms are expected to handle becoming increasingly complex (Dulac-Arnold et al., 2021; Milani et al., 2023; Chen et al., 2023), there is a vital need to develop algorithms that learn policies and skills that generalise to dynamic and novel environments. Many real-world tasks are performed over long time horizons, which makes it crucial that the algorithm can be efficiently trained and quickly adapted to changes in the environment.\nThe aim of this work is to develop an algorithm that (a) can solve problems modeled as POMDP using memory, (b) has a constrained input for the policy and value function so that they are more trainable, (c) only requires the current observation to be forward-passed through a neural network at a time to reduce the Graphical Processing Unit (GPU) memory and computational requirements.\nAcquiring transferable skills and composing them to execute plans, even in novel environments, are remarkable human capabilities that are instrumental in performing complex tasks with long-term objectives. Whenever one encounters a novel situation, one can still strategize by applying prior knowledge with a limited budget of additional trial and error. One way of achieving this is by abstracting away the complexity of long-term planning by delegating short- term decisions to a set of specialized low-level policies, while the high-level policy focuses on achieving the ultimate objective by orchestrating these low-level policies.\nThere has been considerable effort in making RL more generalizable and efficient. Relevant research fields include Hierarchical Reinforcement Learning (HRL) (Vezhnevets et al., 2017; Nachum et al., 2018; Pateria et al., 2021; Zhang et al., 2021), skill learning (Pertsch et al., 2020; Nam et al., 2022; Peng et al., 2022; Shi et al., 2023), Meta Rein- forcement Learning (Meta-RL) (Wang et al., 2016; Duan et al., 2016; Rakelly et al., 2019; Beck et al., 2023) and the options framework (Sutton et al., 1999; Precup & Sutton, 2000), with a shared focus on learning reusable policies. In particular, this research focuses on the options framework, which extends the RL paradigm with a Hidden Markov Model (HMM) that uses options to execute long-term behavior.\nThe Option-Critic architecture (Bacon et al., 2017) presents a well-formulated solution for end-to-end option discovery. The authors showed that once the option policies are learned, the Option-Critic agent can quickly adapt when the environment dynamics are changed, whereas other algorithms suffer from the changes in reward distributions.\nHowever, there are challenges with regard to automatically learning options. A common issue is that the agent may converge to a single option that approximates the optimal policy under a Markov assumption. Additionally, learning options from scratch can be sample-inefficient due to the need to learn multiple option policies.\nIn the following sections, two training objectives are proposed and derived to learn an optimal option assignment.\nThe first approach, Proximal Policy Optimization via Expectation Maximization (PPOEM), applies Expectation Max- imization (EM) to a HMM describing a POMDP for the options framework. The method is an extension of the forward-backward algorithm, also known as the Baum-Welch algorithm (Baum, 1972), applied to options. While this approach has previously been explored (Daniel et al., 2016; Fox et al., 2017; Zhang & Paschalidis, 2020; Giammarino & Paschalidis, 2021), these applications were limited to 1-step Temporal Difference (TD) learning. In addition, the learned options have limited expressivity due to how the option transitions are defined. In contrast, PPOEM augments the forward-backward algorithm with Generalized Advantage Estimate (GAE) (Schulman et al., 2016), which is a temporal generalization of TD learning, and extends the Proximal Policy Optimization (PPO) (Schulman et al., 2017) to work with options. While this approach was shown to be effective in a limited setting of a corridor environment"}, {"title": "", "content": "requiring memory, the performance degraded with longer corridors. It could be hypothesized that this is due to the learning objective being misaligned with the true RL objective, as the approach assumes access to the full trajectory of the agent for the optimal assignment of options, even though the agent only has access to its past trajectory (and not its future) at inference time.\nAs an alternative approach, Sequential Option Advantage Propagation (SOAP) evaluates and maximizes the policy gradient for an optimal option assignment directly. With this approach, the option policy is only conditional on the history of the agent. The derived objective has a surprising resemblance to the forward-backward algorithm, but showed more robustness when tested in longer corridor environments. The algorithms were also evaluated on the Atari (Bellemare et al., 2013) and MuJoCo (Todorov et al., 2012) benchmarks. Results demonstrated that using SOAP for option learning is more effective and robust than using the standard approach for learning options, proposed by the Option-Critic architecture.\nThe proposed approach can improve the efficiency of skill discovery in skill-based RL algorithms, allowing them to adapt efficiently to complex novel environments."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Partially Observable Markov Decision Process", "content": "POMDP is a special case of an Markov Decision Process (MDP) where the observation available to the agent only contains partial information of the underlying state. In this work, s is used to denote the (partial) state given to the agent, which may or may not contain the full information of the environment (which shall be distinguished from state s as the underlying state s).\u00b9 This implies that the \u201cstate\u201d transitions are no longer fully Markovian in a POMDP setting, and may be correlated with past observations and actions. Hence, p(rt, st+1|80:t, a0:t) describes the full state and reward dynamics in the case of POMDPs, where so:t is a shorthand for {st|t1 \u2264 t \u2264 t2}, and similarly with ao:t."}, {"title": "2.2 The Options Framework", "content": "Options (Sutton et al., 1999; Precup & Sutton, 2000) are temporally extended actions that allow the agent to make high-level decisions in the environment. Each option corresponds to a specialized low-level policy that the agent can use to achieve a specific subtask. In the Options Framework, the inter-option policy \u03c0(zt|st) and an option termination probability (st+1, zt) govern the transition of options, where zt is the current option, and are chosen from an n number of discrete options {Z1, ..., Zn}. Options are especially valuable when there are multiple stages in a task that must be taken sequentially (e.g. following a recipe) and the agent must obey different policies given similar observations, depending on the stage of the task.\nEarlier works have built upon the Options Framework by either learning optimal option selection over a pre-determined set of options (Peng et al., 2019) or using heuristics for option segmentation (Kulkarni et al., 2016; Nachum et al., 2018), rather than a fully end-to-end approach. While effective, such approaches constrain the agent's ability to discover useful skills automatically. The Option-Critic architecture (Bacon et al., 2017) proposed end-to-end trainable systems which learn option assignment. It formulates the problem such that inter-option policies and termination conditions are learned jointly in the process of maximizing the expected returns."}, {"title": "2.3 Option-Critic architecture", "content": "As mentioned in Section 2.2, the options framework (Sutton et al., 1999; Precup & Sutton, 2000) formalizes the idea of temporally extended actions that allow agents to make high-level decisions. Let there be n discrete options {Z1, ..., Zn} from which z\u0142 is chosen and assigned at every time step t. Each option corresponds to a specialized sub-policy \u03c0\u03b8(at|St, 2t) that the agent can use to achieve a specific subtask. Att = 0, the agent chooses an option according to its inter-option policy \u03c0\u03c6(zt|st) (policy over options), then follows the option sub-policy until termination, which is dictated by the termination probability function wy(st, Zt\u22121). Once the option is terminated, a new option zt is sampled from the inter-option policy and the procedure is repeated.\nThe Option-Critic architecture (Bacon et al., 2017) learns option assignments end-to-end. It formulates the problem such that the option sub-policies \u03c0\u03bf(at|St, 2t) and termination function W\u2084(st+1, 2+) are learned jointly in the process of maximizing the expected returns. The inter-option policy \u03c0\u03c6(2t|st) is an e-greedy policy that takes an argmax z of the option value function Q$(s, z) with 1 \u2013 \u0454 probability, and uniformly randomly samples options with e probability. In every step of the Option-Critic algorithm, the following updates are performed for a current state s, option z, reward r, episode termination indicator d \u2208 {0, 1}, next state s', and discount factor \u03b3\u2208 [0,1):\n$\\delta\\leftarrow r + \\gamma (1 - d) \\left[(1 - w_{\\psi}(s', z)) Q_{\\phi}(s', z) + w_{\\psi}(s', z) \\max_{z} Q_{\\phi}(s', z)\\right] - Q_{\\phi}(s, z),$\\n$Q_{\\phi}(s, z) \\leftarrow Q_{\\phi}(s, z) + \\alpha_{\\phi} \\delta,$\\n$\\theta\\leftarrow \\theta + \\alpha_{\\theta} \\frac{\\partial \\log \\pi_{\\theta}(a/s, z)}{\\partial \\theta} [r + \\gamma Q_{\\phi}(s', z)],$\\n$\\psi\\leftarrow \\psi - \\alpha_{\\psi} \\frac{\\partial w_{\\psi}(s', z)}{\\partial \\psi} [Q_{\\phi}(s', z) - \\max_{z} Q_{\\phi}(s', z)].$\nHere, \u03b1\u03c6, \u03b1\u03b8 and ay are learning rates for Q$(s, z), \u03c0\u03c1(a|s, z), and 14(s, z), respectively.\nProximal Policy Option-Critic (PPOC) (Klissarov et al., 2017) builds on top of the Option-Critic architecture (Bacon et al., 2017), replacing the e-greedy policy over the option-values with a policy network \u03c0\u03c6(z|s) parametrized by \u03c6 with corresponding learning rate \u03b1\u03c6, substituting the policy gradient algorithm with PPO (Schulman et al., 2017) to optimize the sub-policies \u03c0\u03bf(\u03b1|s, z).\nA standard PPO's objective is:\n$L_{PPO}(\\theta) = \\mathbb{E}_{s, a \\sim \\pi_{\\theta_{old}}} \\left[\\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)} A^{GAE}, clip\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon\\right) A^{GAE}\\right)\\right],$\\nwhere $\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$ is a ratio of probabilities of taking action a at state s with the new policy against that with the old policy, and $A^{GAE}$ is the GAE (Schulman et al., 2016) at time step t. GAE provides a robust and low-variance estimate of the advantage function. It can be expressed as a sum of exponentially weighted multi-step TD errors:\n$A^{GAE} = \\sum_{t'=t}^{T}(\\gamma \\lambda)^{t'-t} \\delta_{t'},$\\nwhere dt = rt + y(1 \u2013 dt)V(st+1) \u2013 V(st) is the TD error at time t, and \u03bb is a hyperparameter that controls the trade-off of bias and variance. Extending the definition of GAE to work with options, the update formula for PPOC can be expressed as:\n$A^{GAE}(s, z) \\leftarrow r + \\gamma V(s', z') - V (s, z) + \\lambda \\gamma (1 - d) A^{GAE} (s', z'),$\\n$Q_{\\phi}(s, z) \\leftarrow Q_{\\phi}(s, z) + \\alpha_{\\phi} A^{GAE}(s, z),$\\n$\\theta\\leftarrow \\theta + \\alpha_{\\theta} \\frac{\\partial J_{LPO}(\\theta)}{\\partial \\theta} A^{GAE} (s, z),$\\n$\\psi\\leftarrow \\psi - \\alpha_{\\psi} \\frac{\\partial \\log \\pi_{\\phi}(z|s)}{\\partial \\phi} A^{GAE}(s, z).$\nPPOC is used as one of the baselines in this work."}, {"title": "2.4 Expectation Maximization algorithm", "content": "The EM algorithm (Dempster et al., 1977) is a well-known method for learning the assignment of latent variables, often used for unsupervised clustering and segmentation. The k-means clustering algorithm (Forgy, 1965) can be considered a special case of EM. The following explanation in this section is a partial summary of Chapter 9 of Bishop's book (Bishop, 2006).\nThe objective of EM is to find a maximum likelihood solution for models with latent variables. Denoting the set of all observed data as X, the set of all latent variables as Z, and the set of all model parameters as \u0398, the log-likelihood function is given by:\n$\\log p(X|\\Theta) = \\log \\sum_{Z} p(X, Z|\\Theta).$\\nHowever, evaluating the above summation (or integral for a continuous Z) over all possible latents is intractable. The EM algorithm is a way to strictly increase the likelihood function by alternating between the E-step that evaluates the expectation of a joint log-likelihood log p(X, Z|\u0398), and the M-step that maximizes this expectation.\nIn the E-step, the current parameter estimate Oold (using random initialization in the first iteration, or the most recent updated parameters in subsequent iterations) is used to determine the posterior of the latents p(Z|X, Oold). The joint log-likelihood is obtained under this prior. The expectation, denoted as Q(\u0398; \u0398old), is given by:\n$Q(\\Theta; \\Theta_{old}) = \\mathbb{E}_{Z \\sim p(Z|X, \\Theta_{old})} [\\log p(X, Z|\\Theta)] = \\sum_{Z} p(Z|X, \\Theta_{old}) \\log p(X, Z|\\Theta).$\\nIn the M-step, an updated parameter estimate Onew is obtained by maximizing the expectation:\n$\\Theta_{new} = \\arg \\max_{\\Theta} Q(\\Theta, \\Theta_{old}).$\\nThe E-step and the M-step are performed alternately until a convergence criterion is satisfied. The EM algorithm makes obtaining a maximum likelihood solution tractable (Bishop, 2006)."}, {"title": "2.5 Forward-backward algorithm", "content": "The EM algorithm can also be applied in an HMM setting for sequential data, resulting in the forward-backward algorithm, also known as the Baum-Welch algorithm (Baum, 1972).\nFor this HMM, the joint likelihood function for the observed sequence X = {xo, ..., XT} and latent variables Z = {20, ..., z} is given by:\n$p(X, Z|\\Theta) = p(z_{0}|\\Theta) \\prod_{t=0}^{T} p(x_{t}|z_{t}, \\Theta) \\prod_{t=1}^{T} p(z_{t}|z_{t-1}, \\Theta).$"}, {"title": "", "content": "Using the above, EM objective can be simplified as:\n$Q(\\Theta; \\Theta_{old}) = \\sum_{Z} p(Z\\X, \\Theta_{old}) \\log p(X, Z|\\Theta) =$\\n$\\sum_{z_{0}} p(z_{0}|\\Theta_{old}) \\log p(z_{0}|\\Theta) + \\sum_{t=0}^{T} \\sum_{z_{t}} p(z_{t}|X, \\Theta_{old}) \\log p(x_{t}|z_{t}, \\Theta) +$\\n$\\sum_{t=1}^{T} \\sum_{z_{t-1}, z_{t}} p(z_{t-1}, z_{t}|X, \\Theta_{old}) \\log p(x_{t}, z_{t}|z_{t-1}, \\Theta).$"}, {"title": "2.5.1 E-step", "content": "In the E-step, p(zt|X) and p(zt\u22121, 2t|X) are evaluated. Note that in the following derivation, it is assumed that the probability distributions are conditioned on \u0398. Defining a(zt) := p(zt|xo:t), \u03b2(zt) := p(xt+1:Tzt) and normalising constant ct := p(xt|x0:t-1),\n$p(z_{t}|X) = \\frac{p(x_{t+1:T}|x_{0:t})}{p(x_{0:T})} = \\frac{p(x_{0:T}, z_{t}) p(x_{0:t}, z_{t}) p(x_{t+1:T}|z_{t})}{p(x_{0:T})} = \\alpha(z_{t})\\beta(z_{t}),$\n$p(z_{t-1}, z_{t}|X) = \\frac{p(x_{0:T}, z_{t-1}, z_{t})}{p(x_{0:T})} = \\frac{p(x_{0:t-1}, z_{t-1}) p(x_{t}|z_{t}) p(z_{t}|z_{t-1}) p(x_{t+1:T}|z_{t})}{p(x_{0:T})} = \\frac{p(x_{t}|z_{t}) p(z_{t}|z_{t+1}) \\alpha(z_{t})\\beta(z_{t})}{c_{t}}.$\nRecursively evaluating a(zt), \u03b2(zt) and ct,\n$\\alpha(z_{t}) = \\frac{p(x_{0:t}, z_{t})}{p(x_{0:t})} = \\frac{p(x_{t}, z_{t}|x_{0:t-1})}{p(x_{t}|x_{0:t-1})} = \\frac{\\sum_{z_{t-1}} [p(z_{t-1}|x_{0:t-1}) p(x_{t}|z_{t}) p(z_{t}|z_{t-1})]}{p(x_{t}|x_{0:t-1})} = \\frac{p(x_{t}|z_{t}) \\sum_{z_{t-1}} [\\alpha(z_{t-1}) p(z_{t}|z_{t-1})]}{c_{t}},$\n$\\beta(z_{t}) = \\frac{p(x_{t+1:T}|z_{t})}{p(x_{t+1:T}|x_{0:t})} = \\frac{\\sum_{z_{t+1}} [p(x_{t+2:T}|z_{t+1}) p(x_{t+1}|z_{t+1}) p(z_{t+1}|z_{t})]}{p(x_{t+1:T}|x_{0:t})} = \\frac{\\sum_{z_{t+1}} [\\beta(z_{t+1}) p(x_{t+1}|z_{t+1}) p(z_{t+1}|z_{t})]}{c_{t+1}},$\n$c_{t} = p(x_{t}|x_{0:t-1}) = \\sum_{z_{t-1}, z_{t}} [p(z_{t-1}|x_{0:t-1}) p(x_{t}|z_{t}) p(z_{t}|z_{t-1})] = \\sum_{z_{t-1}, z_{t}} [\\alpha(z_{t-1}) p(x_{t}|z_{t}) p(z_{t}|z_{t-1})].$\nInitial conditions are \u03b1(zo) = $\\frac{p(x_{0}|z_{0}) p(z_{0})}{\\sum_{z_{0}}[p(x_{0}|z_{0}) p(z_{0})]}, \u03b2(z_{T}) = 1.$"}, {"title": "2.5.2 M-step", "content": "In the M-step, the parameter set is updated by maximizing Q(\u0398; \u0398old), which can be rewritten by substituting p(zt X) and p(zt\u22121, 2t|X) in Equation (9) with a(z) and \u1e9e(z) (ignoring the constants) as derived in Section 2.5.1."}, {"title": "2.5.3 Option discovery via the forward-backward algorithm", "content": "The idea of applying the forward-backward algorithm to learn option assignments is first introduced in Daniel et al. (2016), and has later been applied in both Imitation Learning (IL) settings (Zhang & Paschalidis, 2020; Giammarino & Paschalidis, 2021) and RL settings (Fox et al., 2017). However, in previous literature, the option policy is decoupled into an option termination probability w(st, zt\u22121), and an inter-option policy \u03c0(zt|st). Due to the inter-option policy being unconditional on the previous option zt\u22121, the choice of a new option zt will be uninformed of the previous option 2t-1. This may be problematic for learning POMDP tasks as demonstrated in Section 6.1, because if a task consists of a sequence of options, then knowing which one was executed before is important to decide to move on to"}, {"title": "3 Option assignment formulation", "content": "The aim is to learn a diverse set of options with corresponding policy and value estimates, such that each option is responsible for accomplishing a well-defined subtask, such as reaching a certain state region. At every time step t, the agent chooses an option z\u0142 out of n number of discrete options {Z1, ..., Zn }."}, {"title": "3.1 Option policy and sub-policy", "content": "The goal is to learn a sub-policy \u03c0\u03bf(a|s, z) conditional to a latent option variable z, and an option policy \u03c0\u03c5(z'|s, a, z) used to iteratively assign options at each time step, to model the joint option policy\n$p_{\\Theta}(a_{t}, z_{t+1}| s_{t}, z_{t}) = \\pi_{\\theta}(a_{t}| s_{t}, z_{t}) \\pi_{\\psi}(z_{t+1}| s_{t}, a_{t}, z_{t}).$\nHere, the learnable parameter set of the policy is denoted as O = {0,\u03c8}.\nA comparison of the option policy used in this work and the standard options framework is shown in Figure 2. Unlike the options framework, which further decouples the option policy \u03c0\u03c8 into an option termination probability w(st, zt-1), and an unconditional inter-option policy \u03c0(zt|st), in this work the option policy is modeled \u03c0\u03c8 with one network so that the inter-option policy is informed by the previous option z\u0142 upon choosing the next 2t+1."}, {"title": "3.2 Evaluating the probability of latents", "content": "Let us define an auto-regressive action probability at := $p(a_{t}| s_{0:t}, a_{0:t-1})$, an auto-regressive option forward distribution (zt) := $p(z_{t}| s_{0:t}, a_{0:t-1})$, and an option backward feedback \u1e9e(zt) := $p(s_{t:T}, a_{t:T-1}| s_{t-1}, a_{t-1}, z_{t})$. Notice that the definitions of action probability a, option forward ((zt), and option backward \u1e9e(zt) resemble ct, a(zt) and \u1e9e(zt) defined in Section 2.5, respectively. While it is common practice to denote the forward and backward quantities as a and \u1e9e in the forward-backward algorithm (also known as the a-\u1e9e algorithm), here at is redefined to denote the action probability (corresponding to the normalizing constant ct), and ((zt) for the option forward distribution, to draw attention to the fact that these are probabilities of option z\u0142 and action at, respectively."}, {"title": "4 Proximal Policy Optimization via Expectation Maximization", "content": "In this section, PPOEM is introduced, an algorithm that extends PPO for option discovery with an EM objective. The expectation of the returns is taken over the joint probability distribution of states, actions and options, sampled by the policy. This objective gives a tractable objective to maximize, which has a close resemblance to the forward-backward algorithm."}, {"title": "4.1 Expected return maximization objective with options", "content": "The objective is to maximize the expectation of returns R(7) for an agent policy \u03c0over a trajectory \u0442 with latent option zt at each time step t. The definition of a trajectory 7 is a set of states, actions and rewards visited by the agent policy in an episode. The objective J[\u03c0] can be written as:\n$J[\\pi_{\\theta}] = \\mathbb{E}_{\\tau, Z \\sim \\pi} [R(\\tau)] = \\int_{T, Z} R(\\tau) p(\\tau, Z|\\Theta) d\\tau.$\nTaking the gradient of the maximization objective,\n$\\nabla_{\\theta} J[\\pi_{\\theta}] = \\int_{\\tau, Z} R(\\tau) \\nabla_{\\theta} p(\\tau, Z|\\Theta) d\\tau = \\int_{\\tau, Z} R(\\tau) \\frac{\\nabla_{\\theta} p(\\tau)}{p(\\tau, Z|\\Theta)} p(\\tau, Z|\\Theta) = \\mathbb{E}[R(\\tau) \\nabla_{\\theta} \\log p(\\tau, Z|\\Theta)].$"}, {"title": "", "content": "To simplify the derivation", "by": "n$p(\\tau", "p(s_{0": "T"}, "a_{0:T-1}, z_{0:T}|\\Theta) = p(s_{0}, z_{0}) \\prod_{t=0}^{T-1} [p_{\\theta}(a_{t}, z_{t+1}|s_{t}, z_{t}) p(s_{t+1}|s_{0:t}, a_{0:t}"], "at": "p(a_{t"}, {"0": "t"}, {"0": "t-1"}, {"s_{0": "t"}, {"0": "t-1"}, {"t": "T"}, {"t": "T-1"}, {"frac{p(s_{0": "T"}, {"0": "T-1"}, {"z_{t+1})}{p(s_{0": "T"}, {"0": "T-1"}, {"p(s_{t": "T"}, {"t": "T-1"}, {"0": "t-1"}, {"0": "t-1"}, {"p(s_{0": "T"}, {"0": "T-1"}, {"p(s_{0": "t"}, {"0": "t-1"}, {"p(s_{t+1": "T"}, {"t+1": "T-1"}, {"z_{t+1})}{p(s_{0": "t"}, {"0": "t-1"}, {"t}|s_{0": "t"}, {"0": "t-1"}, {"t+1": "T"}, {"t+1": "T-1"}, {"0": "t"}, {"0": ""}]