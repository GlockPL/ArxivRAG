{"title": "NATURAL LANGUAGE COUNTERFACTUAL EXPLANATIONS FOR GRAPHS USING LARGE LANGUAGE MODELS", "authors": ["Flavio Giorgi", "Cesare Campagnano", "Fabrizio Silvestri", "Gabriele Tolomei"], "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical area of research to unravel the opaque inner logic of (deep) machine learning models. Among the various XAI techniques proposed in the literature, counterfactual explanations stand out as one of the most promising approaches. However, these \u201cwhat-if\u201d explanations are frequently complex and technical, making them difficult for non-experts to understand and, more broadly, challenging for humans to interpret. To bridge this gap, in this work, we exploit the power of open-source Large Language Models to generate natural language explanations when prompted with valid counterfactual instances produced by state-of-the-art explainers for graph-based models. Experiments across several graph datasets and counterfactual explainers show that our approach effectively produces accurate natural language representations of counterfactual instances, as demonstrated by key performance metrics.", "sections": [{"title": "1 Introduction", "content": "In recent years, Machine Learning have become deeply embedded in various facets of our daily lives, influencing decisions ranging from personalized recommendations to critical judgments in healthcare and finance. This pervasive integration has raised significant concerns regarding transparency and accountability, leading to legislative actions such as the European Union's General Data Protection Regulation (GDPR) [Voigt and Von dem Bussche, 2017], which emphasizes the right to explanation for automated decision-making processes. Similarly, the upcoming EU Artificial Intelligence Act aims to regulate AI systems, mandating explainability and interpretability in high-risk applications [European Commission, 2021].\nIn the realm of eXplainable Artificial Intelligence (XAI), numerous techniques have been proposed to address this need. Among them, counterfactual explanations [Wachter et al., 2017] stand out as one of the most promising post-hoc methods. The core idea is to explain a model's prediction by identifying a counterfactual example \u2013 i.e., the minimal change to the input that leads to a different prediction. Counterfactual explanations have been successfully applied to unveil the inner workings of predictive models having various degrees of complexity, ranging from ensembles of decision trees [Tolomei et al., 2017] and multi-layer perceptrons [Montavon et al., 2018] to more sophisticated models like transformers [Chefer et al., 2020].\nDespite these advancements, a significant challenge remains: translating algorithmically generated explanations into a \"format\" that is accessible and comprehensible to end-users who may not possess technical expertise. Fredes and Vitria [2024] takes a first step in this direction by leveraging the generative capabilities of Large Language Models (LLMs) to produce user-friendly counterfactual explanations in natural language."}, {"title": null, "content": "Building on the work above, we tackle the more ambitious challenge of generating natural language explanations from counterfactual examples tailored for graph neural networks (GNNs). Indeed, counterfactual examples derived from graph inputs are inherently more complex than those from tabular data, given the intricate relationships and structures they capture (e.g., nodes, edges, and their dependencies). This complexity presents unique challenges that demand a specialized approach to translate them into universally comprehensible natural language explanations.\nMoreover, graph data is prevalent across various domains such as financial decision-making [Cao et al., 2020], fraud detection [Wang et al., 2021], and biology [Fout et al., 2017], where GNN-based models have shown remarkable predictive performance by encoding high-order structural relationships within their learned graph representations.\nSpecifically, we consider the counterfactual examples output by a generic graph counterfactual explainer designed for node classification tasks using GNNs. Then, we instruct several open-source LLMs to translate these \"raw\" counterfactual examples into coherent natural language explanations that are accessible also to non-expert users. To evaluate the quality of the generated explanations, we introduce a set of novel metrics that measure how accurately our method maps the counterfactual examples to their corresponding natural language descriptions. As openly generated text might be hard to evaluate using automatic evaluation metrics [Chiang and Lee, 2023], we also validate this evaluation through human judgment. Extensive experiments conducted using two graph counterfactual explainers for node classification \u2013 CF-GNNExplainer [Lucic et al., 2022] and its extension for node features, CF-GNNFeatures [Giorgi et al., 2024] \u2013 across several graph datasets and multiple open-source LLMs demonstrate that our method can effectively support decision-making processes in critical domains through the generation of natural language explanations.\nTo summarize, our main contributions are as follows:\n\u2022 We present a method for translating counterfactual explanations for graphs into natural language using state-of- the-art open-source LLMs. This is, to our knowledge, the first work proposing the use of LLMs as means for converting the output of a GNN model to a human-readable format.\n\u2022 We define novel metrics to properly assess the effectiveness of these explanations.\n\u2022 We perform an extensive evaluation of our approach, which includes varying the sizes of LLMs, datasets, and explanation methods.\nThis paper is structured as follows: in Section 2, we summarize related work; In Section 3, we describe our method, which is validated through extensive experiments in Section 4. We discuss the practical implications of our method in Section 5. Finally, Section 6 concludes our work and proposes future directions.\nThe code to reproduce our experiments is available at https://github.com/flaat/llm-graph-cf."}, {"title": "2 Related Work", "content": "Counterfactual explainability has emerged as a pivotal approach for interpreting complex machine learning models by illustrating how changes in input variables can lead to different outcomes. Despite this, a significant challenge persists: making these explanations accessible and understandable to a broad audience, particularly non-technical users who may struggle with abstract mathematical concepts and technical jargon. The intricacy of counterfactual explanations often hinders their comprehension among laypersons, limiting their practical utility in real-world applications where user trust and transparency are paramount.\nThe advent of large language models (LLMs), such as GPT-4 and its successors, has revolutionized the field of natural language processing. These models possess an unparalleled capacity for understanding and generating human-like text, making them invaluable tools for translating complex technical information into plain language. Their widespread availability and ease of integration into various platforms further enhance their appeal for tasks requiring sophisticated language generation and interpretation.\nLeveraging the immense capabilities of LLMs presents a promising solution to the accessibility problem in counterfactual explainability. By converting intricate data-driven explanations into natural language narratives, LLMs can make the insights derived from machine learning models more digestible for non-technical users. This translation not only aids in user comprehension but also fosters greater trust in automated systems by promoting transparency.\nIn this context, the pioneering work done by Fredes and Vitria [2024] marks a significant milestone. Their research laid the foundations for utilizing LLMs to transform data-based counterfactual explanations into coherent, user-friendly language. After generating the counterfactual examples, the researchers aimed to identify the primary causal factors deduced from these examples that led to the user's differing classification. To achieve this, they input both the set of counterfactual examples and the original user data into a Large Language Model (LLM), instructing it to produce a"}, {"title": null, "content": "list of the main reasons why the user was classified differently. Once the LLM generated this list, they meticulously verified its accuracy and identified the most relevant causes contributing to the explanation.\nSubsequently, they synthesized all the information produced and leveraged the LLM once more to generate a final explanation articulated in plain language. This explanation was crafted to emphasize actionable steps that the user could take to alter their input data or behavior, thereby changing their classification to the desired category. By doing so, they not only provided a transparent rationale behind the classification but also offered practical guidance for the user to achieve a favorable outcome. However, their work focuses solely on the relatively straightforward case of translating counterfactual examples derived from a single, well-known tabular dataset. Furthermore, they used a proprietary LLM model (GPT-40) making it hard to replicate their approach.\nTo the best of the authors' knowledge, there are no papers in the State-of-the-Art that address the problem of translating the explanations generated via counterfactual explainers into natural language explanations; for this reason, we firmly believe that our work can be useful to the community."}, {"title": "3 From Counterfactual Examples to Natural Language Explanations", "content": null}, {"title": "3.1 The Counterfactual Explanation Problem", "content": "The counterfactual explanation problem in a classification task is described as follows. Given a sample $x$ and a predictive model $f_\\theta$ parametrized by $\\theta$ \u2013 hereinafter referred to as oracle \u2013 the goal is to find a sample $x' \\neq x$ such that $f_\\theta(x) \\neq f_\\theta (x')$. This $x'$ is called a counterfactual example for $x$. Among all potential counterfactual examples (if any), we assume the existence of a counterfactual example generator $g$, which takes as input the original instance $x$ and returns a counterfactual $x^*$, where the distance $d(x, x^*)$ is minimized, or $\\varnothing$ if no valid counterfactual example exists. The distance function $d(\\cdot,\\cdot)$ ensures that the counterfactual sample $x^*$ remains as close as possible to the original factual sample $x$.\nNote that, in this work, we focus on graph inputs. Specifically, each sample $x$ and its counterfactual(s) $x'$ can be represented as $G(V, E)$ and $G'(V', E')$, respectively. Therefore, a counterfactual example for an input graph $G$ will be a new graph $G'$, where either the node features, the structural links, or both differ from the original, while still satisfying the counterfactual criterion of altering the model's prediction. This introduces additional challenges as the modifications can affect both the node-level properties and the overall graph topology."}, {"title": "3.2 Proposed Method", "content": "We assume to have an oracle $f_\\theta$ for a node classification task, specifically a graph neural network trained on a given input graph. For any node instance $x$, we know both its predicted label $\\hat{y}$, such that $f_\\theta(x) = \\hat{y}$, and its optimal counterfactual example $x^* = g(x)$, which is generated by the counterfactual explainer $g$.\nTo generate the natural language explanation ($e(x^*)$) associated with the generic counterfactual example ($x^*$), we feed a pre-trained LLM $m$ with the factual ($x$) and counterfactual ($x^*$) instances along with a specific prompt $p$, i.e., $e(x^*) = m(p, x, x^*)$.\nOne of the critical challenges of this approach is how to validate the quality of the generated natural language explanations through the LLM. In the following section, we introduce the new evaluation framework proposed in this work."}, {"title": "3.3 A New Evaluation Framework", "content": "Given the novelty of the problem we are tackling, to the best of our knowledge, no established quality metrics exist in the literature to rigorously evaluate explanations generated by LLMs for graph counterfactuals. The only commonly adopted approach is based on subjective human judgment, which may introduce variability and bias in the assessment process. To address this gap and ensure a more systematic evaluation, we have developed a suite of novel metrics that provide a quantitative and objective assessment of the language model's capability to articulate pairs of factual and counterfactual graphs into coherent and informative natural language explanations.\nFor clarity, we denote the factual graph as $G$ and the corresponding counterfactual graph as $G'$. To compute these metrics effectively, we structured our prompts to include a request for the language model to populate a predefined dictionary with essential graph information. This dictionary encompasses the target node of the classification task, its original class in the factual graph, its modified class in the counterfactual graph, the neighborhood of the target node in both $G$ and $G'$, and the set of features associated with the target node in each scenario."}, {"title": null, "content": "By doing so, we aim to evaluate the language model's ability to discern and convey the critical elements of the graph structure and its transformations, thereby assessing the model's comprehension of how the changes in the graph influence the classification outcome for the target node. This framework allows us to objectively measure the performance of the language model in translating structural and attribute-based differences between $G$ and $G'$ into precise and meaningful natural language descriptions.\nTarget Node Identification (TNI) This metric assesses the ability of the LLM to accurately identify and reference the target node within the graph structure. Given the importance of the target node as the focal point of the classification task, correctly pinpointing it is crucial for generating valid explanations. Formally, given a graph $G(V, E)$, the target node $t \\in V$, and a node $v \\in V$ predicted by the LLM, we define the Target Node Identification (TNI) metric as:\n$TNI(v) = \\begin{cases}\n1 & \\text{if } v = t, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nTarget Node Neighborhood Understanding (TNNU) This metric measures the LLM's proficiency in identifying and articulating the nodes that constitute the neighborhood of the target node within the factual graph $G$. An accurate understanding of the neighborhood is fundamental, as these nodes and their connections often have a significant influence on the classification outcome. Let $G(V, E)$ be a graph, let $t \\in V$ be the target node in the graph $G$, and let $N(t) = \\{u \\in V | (t, u) \\in E\\}$ denote the set of neighbors of the target node $t$. Given a set of neighbors $N_{LLM}(t)$ predicted by the LLM for the target node $t$, we define the Target Node Neighborhood Understanding (TNNU) metric as:\n$TNNU(N_{LLM}(t)) = \\begin{cases}\n1 & \\text{if } N_{LLM}(t) = N(t), \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nCounterfactual Class Identification (CCI) This metric evaluates the capacity of the LLM to correctly comprehend and express the change in class assignment of the target node from the factual graph $G$ to the counterfactual graph $G'$. Let $G' = (V, E')$ be the factual, such that $f_\\theta (G') = c'$ where $c'$ is the counterfactual class of the target node $t$ such that $f_\\theta(G') \\neq f_\\theta(G)$, and let $C_{LLM}$ be the counterfactual class predicted by the LLM, CCI is defined as:\n$CCI(C_{LLM}) = \\begin{cases}\n1 & \\text{if } C_{LLM} = c', \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nFactual Target Node Feature (FTNF) This metric examines the LLM's ability to accurately recognize and describe the features associated with the target node in the factual graph $G$. Correctly identifying these features is essential, as they are pivotal in determining the node's initial classification. Let $G = (V, E)$ be the factual graphs. Let $t \\in V$ be the target node, and $x_t \\in \\mathbb{R}^d$ denote the feature vectors of node $t$ in the factual graph $G$. We also define the vector of features predicted by the LLM for the target node $t$ as $X_{LLM}$, the metric is computed as:\n$FTNF(X_{LLM}) = \\begin{cases}\n1 & \\text{if } X_{LLM} = x_t, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nCounterfactual Target Node Feature (CFTNF) Similar to FTNF, this metric evaluates the LLM's capability to identify and articulate the features associated with the target node in the counterfactual graph $G'$. This metric is critical for assessing whether the LLM captures the differences in node attributes that lead to a shift in classification. Let $G' = (V', E')$ be the counterfactual graphs. Let $t \\in V'$ be the target node, and $x'_t \\in \\mathbb{R}^d$ denote the feature vectors of node $t$ in the counterfactual graph $G'$. We also define the vector of features predicted by the LLM for the target node $t$ as $X_{LLM}$, the metric is computed as:\n$CFTNF(X_{LLM}) = \\begin{cases}\n1 & \\text{if } X_{LLM} = x'_t, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nCounterfactual Target Node Neighbors (CFTNN) This metric measures the LLM's capacity to correctly identify the set of neighboring nodes for the target node in the counterfactual graph $G'$. Understanding these neighbors in the counterfactual context is essential, as changes in the neighborhood structure may directly influence the target node's classification shift. Accurately capturing the neighbors in $G'$ helps the LLM articulate how the local connectivity of the target node has been modified and how this alteration affects the node's role and classification within the network. Let $G'(V', E')$ be a counterfactual graph, let $t' \\in V'$ be the target node in the graph $G'$, and let $N(t') =$"}, {"title": null, "content": "$\\begin{align*} \\{u \\in V' | (t', u) \\in E'\\} \\end{align*}$ denote the set of neighbors of the target node $t$. Given a set of neighbors $N_{LLM}(t')$ predicted by the LLM for the target node $t'$, we define the Counterfactual Target Node Neighbors (CFTNN) metric as:\n$CFTNN(N_{LLM}(t')) = \\begin{cases}\n1 & \\text{if } N_{LLM}(t') = N(t'), \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nOverall, these metrics provide a comprehensive framework for evaluating the language model's capacity to interpret and describe the structural and feature-based transformations between factual and counterfactual graphs. By assessing these distinct aspects of graph understanding, we can quantitatively measure the model's ability to generate coherent and insightful explanations that accurately reflect the graph dynamics and their implications on classification outcomes. We decided to select the explanations that score 1 on at least 5 out of 6 metrics and test them to human judgment."}, {"title": "4 Experiments", "content": "Given the general framework we described in Section 3, this section delves into the problem of counterfactual translation into natural language using LLM in graph neural networks."}, {"title": "4.1 Graph Counterfactual Explainers", "content": "As outlined in Section 3, our proposed pipeline requires the integration of a counterfactual explainer for node clas- sification, denoted as $g$. In this study, we employed two state-of-the-art explainers, namely, CF-GNNExplainer and CF-GNNFeatures, allowing us to investigate the impact of distinct graph modifications on the LLM's ability to generate natural language explanations. Specifically, CF-GNNExplainer modifies the graph structure by perturbing the adjacency matrix, while CF-GNNFeatures focuses on altering the node attributes. This dual approach enables us to assess how variations in both structural and feature-based properties influence the quality and coherence of the generated explanations.\nCF-GNNExplainer is a perturbation-based counterfactual explainer. It defines $A_v = P \\odot A_v$, where $P$ is a binary perturbation matrix that sparsifies $A_v$. The goal is to find $P$ for a given node $v$ such that $f_\\theta (A_v, x) \\neq f_\\theta(P \\odot A_v, x)$. To find P, CF-GNNExplainer exploits a technique to train sparse neural networks to zero out entries in the adjacency matrix (i.e., removing edges). This results in the deletion of the edge between node $i$ and node $j$.\nCF-GNNFeatures is a node features perturbation-based counterfactual explainer. Given a graph $G(V, E)$, two matrices are defined, namely: $V_x$, the node features matrix representing the features for every node in G and $P_x$, the feature perturbation matrix. Initially, the matrix $P_x$ is filled with ones to maintain the original sets of attributes. Given an oracle $f_\\theta$, parameterized by $\\theta$, we fix all the weights and train $P_x$ to change the attribute matrix $V_x$ that is fed into the oracle multiplying the current feature matrix with the perturbation matrix."}, {"title": "4.2 Large Language Models", "content": "In this study, we utilized a family of state-of-the-art LLMs, namely Qwen2.5 [Qwen Team, 2024], to generate the natural language explanation. We experiment using different model sizes in terms of parameters; in particular we employ the 0.5B, 1.5B, 3B, 7B, and 14B variants.\nThe model configuration used in this work includes several hyperparameters that influence its behavior during text generation:\n\u2022 temperature: 0.1,\n\u2022 top-p: 0.8,\n\u2022 top-k: 30,\n\u2022 repetition penalty: 1.05,\n\u2022 max output tokens: 2048,\n\u2022 top-k: 10.\nIn order to reduce the memory footprint of the models, we used GPTQ [Frantar et al., 2023] quantization with 4-bit integer precision (int4).\nOverall, the chosen configuration allows the model to generate meaningful counterfactual explanations for complex graph structures while maintaining computational efficiency. The setup leverages the power of modern LLMs along with advanced quantization techniques to produce high-quality outputs, making it an ideal choice for applications requiring detailed textual descriptions of graph perturbations.\nPrompting The design and structure of the prompt is critical, as the performances of LLMs are highly influenced by how the graph data is presented. To this end, we adopted the incident representation framework proposed by Fatemi et al. [2024], with some adjustments to better suit the requirements of our task. To provide all the information needed for the LLM, we use an initial system prompt providing the essential background information on the challenges of counterfactual explainability, particularly in the context of graph-based data. Subsequently, the counterfactual prompt is introduced (Figure 2), which instructs the LLM to generate a coherent and contextually appropriate natural language explanation based on both the factual and counterfactual graph samples."}, {"title": "4.3 Experimental Setup", "content": "The experiments have been carried out on a machine equipped with 64GB of RAM, an Nvidia RTX 4090, and a AMD Ryzen 9 7900 processor. As oracle we used a 2-layer GCN trained for 500 epochs with a learning rate of 0.001."}, {"title": "4.4 Datasets", "content": "To ensure a comprehensive evaluation, we employed two well-known citation network datasets: Cora and CiteSeer. The CiteSeer dataset comprises 3,312 scientific publications categorized into six distinct classes, connected by 4,732 citation"}, {"title": "4.5 Results", "content": "To provide an appropriate evaluation, we tested multiple counterfactual methods and LLMs on different graph datasets. As shown in Tables from 1 to 4 the results for the graph understanding as defined in Section 3 are generally good for models with more parameters. In particular, a clear trend emerges across all tables: as the number of parameters in the LLM increases, performance improves significantly across all metrics, as expected. The smallest model, with 0.5 billion parameters, consistently scores zero across all metrics and datasets, indicating its inability to generate meaningful explanations. With an increase to 1.5 billion parameters, there is a minimal improvement, but performance remains substantially low.\nNotable improvements are observed with the 3-billion-parameter model, especially in metrics like Target Node Identification (TNU) and Counterfactual Class Identification (CCI). However, it's the largest model, with 14 billion parameters, that achieves the highest scores across all metrics and datasets. This demonstrates the importance of model size when dealing with complex tasks such as generating natural language explanations from graph counterfactuals.\nIn Table 1, results show that using CF-GNNFeatures on the Cora dataset, the TNU metric increases from 0.000 with the smallest model to 0.720 with the largest model. Similarly, in Table 3, using the CiteSeer dataset, the TNU metric reaches 0.879 with the 14-billion-parameter model. These improvements indicate that larger models can understand and generate accurate descriptions of complex graph structures.\nComparing performances across datasets The LLMs generally perform better on the CiteSeer dataset, especially when using the CF-GNNFeatures explainer. For example, in Table 3 (CF-GNNFeatures on CiteSeer), the 14-billion- parameter model achieves a TNU of 0.879, higher than the 0.720 achieved on the Cora dataset in Table 1.\nSeveral factors contribute to this difference in performance. The datasets have different levels of complexity, differences in feature distributions, or inherent properties that make one more amenable to the LLM's processing capabilities. Cite- Seer have more straightforward or more distinctive features that the LLM can more readily associate with classification outcomes, aiding in generating coherent explanations."}, {"title": "Comparison between explainers", "content": "The two counterfactual explainers used in the study, CF-GNNFeatures and CF-GNNExplainer, focus on different aspects of the graph. CF-GNNFeatures modifies node features, while CF- GNNExplainer modifies the graph structure.\nFor instance, in Table 1 (CF-GNNFeatures on Cora), the FTNF (Factual Target Node Feature) and CFTNF (Counter- factual Target Node Feature) scores at 14 billion parameters are 0.668 and 0.565, respectively. In contrast, in Table 2 (CF-GNNExplainer on Cora), these scores are lower, at 0.579 for both metrics at the same model size.\nThis suggests that LLMs find it easier to generate explanations when the modifications involve changes in node features rather than structural changes in the graph. Explaining structural changes may require a deeper understanding of the graph's topology and how it influences the classification, which appears more challenging for the LLMs."}, {"title": "Human Evaluation", "content": "In order to assess appropriately the explanations, we built a questionnaire asking to answer the following questions using a scale from 1 to 5:\n\u2022 Q1: Is the terminology and language used in the explanation appropriate and easy to understand?\n\u2022 Q2: How clear and easy to understand is the provided counterfactual explanation?\n\u2022 Q3: How clearly does the explanation describe the changes in node connections (graph structure) that led to the counterfactual outcome?\n\u2022 Q4: Are the changes in features and structure easy to interpret and make sense in the context of the original graph?\n\u2022 Q5: What is your overall assessment of the clarity and coherence of the counterfactual explanation?\nThe results of the human evaluation can be seen in Table 5. The human evaluation shows that explanations generated by the LLM that focus on node features are more effective and meaningful to human users than those focusing on adjacency matrix perturbations: participants found feature-based explanations to be more accessible in terms of language, clearer in conveying the reasoning, and more interpretable within the context of the graph."}, {"title": "5 Practical Implications", "content": "The proposed framework for generating natural language counterfactual explanations using LLMs holds significant promise for enhancing interpretability and transparency in graph-based machine learning models. This approach has several practical implications across domains where complex graph structures are used, such as financial analysis, healthcare, cybersecurity, and social network analysis. By translating complex counterfactual explanations into natural language, our method makes these explanations more accessible to non-technical stakeholders, including business managers, policymakers, and end-users. Moreover, with the European Union's General Data Protection Regulation (GDPR) and the EU Artificial Intelligence Act, explainability and transparency of AI systems are becoming mandatory, particularly in high-stakes applications. Our method addresses this requirement by ensuring that counterfactual explanations are technically sound and easily interpretable, thereby aiding compliance with legal and ethical standards.\nIn summary, our framework provides a comprehensive solution for bridging the gap between technical counterfactual explanations and human comprehension, enabling broader adoption of AI technologies in high-stakes, complex domains. The method promotes transparency and trust and enhances the utility of counterfactual explanations as a tool for model evaluation, debugging, and refinement."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we presented a novel framework for generating natural language counterfactual explanations for graph- based models using state-of-the-art LLMs. Our approach leverages the inherent generative capabilities of LLMs to transform complex and technical counterfactual examples into coherent and accessible natural language descriptions. By doing so, we address a critical gap in the literature: the lack of intuitive, user-friendly counterfactual explanations for GNNs. We validated our method across several GNN-based counterfactual explainers and multiple graph datasets, demonstrating its effectiveness through a suite of newly proposed metrics and human evaluation. Our findings show that our framework not only captures the underlying structural and attribute-based transformations within the graphs but also produces explanations that are easily interpretable by non-expert users, thereby enhancing the transparency and usability of graph-based machine learning models.\nOur framework does not depend upon any particular counterfactual explainer, so it can be used with any approach. We developed additional metrics that quantify the quality of the natural language output-such as graph understanding. Finally, our experiments were conducted using open-source LLMs without task-specific fine-tuning. Future research could explore the impact of fine-tuning the LLMs on graph-specific tasks or developing domain-specific LLMs to further enhance the quality and relevance of the generated explanations.\nOverall, we believe that our framework represents a significant step toward bridging the gap between complex algorithmic explanations and human comprehension in the domain of graph-based machine learning."}]}