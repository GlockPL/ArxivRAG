{"title": "What Did My Car Say? Autonomous Vehicle Explanation Errors, Context, and Personal Traits Impact Comfort, Reliance, Satisfaction, and Driving Confidence", "authors": ["ROBERT KAUFMAN", "AARON BROUKHIM", "DAVID KIRSH", "NADIR WEIBEL"], "abstract": "Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger's comfort in relying on an AV, preference for control, confidence in the AV's ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV's driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.", "sections": [{"title": "1 Introduction", "content": "Autonomous vehicles offer a wide range of potential societal benefits, including reduced driving infractions, traffic volume, environmental impact, and passenger stress [21]. Despite these benefits, it is a well-documented problem that adoption is limited by a lack of trust in how vehicles make decisions [38]. This is not a problem specifically with autonomous vehicles, but a widespread issue across many types of AI-based systems [5]. System transparency via explainable AI (XAI) has been proposed as a means to mitigate concerns with AI-based systems like AVs, offering users a look \"under the hood\" of black-box AI models so they understand what the system is doing and why [25, 51].\nHowever, although potentially increasing trust, explanation of AI interactions in the real world can contain errors. Prior work has shown that when autonomous vehicles exhibit driving errors, passenger trust and willingness to rely on the vehicle can deteriorate quickly [33, 63]. Far less is known about the impact of explanation errors [8, 39]."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 AV Trust and Explainability", "content": "Human-Centered Explainable AI \u2013 Continuous development of explainable AI (XAI) has brought major progress in the quest for trust through AI transparency [51]. Approaches to XAI vary, often limited by the availability of the model [58, 65]. Recent work found that even when an explanation is given \u2013 trust and engagement with the system may not improve unless the right information is given in the right way, at the right time [43, 68]. Several studies and theory pieces have demonstrated the value of human-interpretable explanations on user understanding and trust [28, 66]. In particular, explanations that are modeled off those given by human experts have suggested as a means to increase understanding for experts and novices alike [37, 57]. There have likewise been calls for the need for explanations to be sensitive to particular user characteristics (such as personal traits or experiences) [18, 19, 35] or context of use (including the specific use environment and goals of a user) [36, 60]. Recent discussion on XAI usefulness had proposed that explanations should move beyond simply conveying algorithmic mechanisms for decision-making and instead supporting user hypothesis generation [52]. Despite the large amount of research on explanation design, little has been done to understand what happens when these explanations fail. The study we present here is a first step towards filling this knowledge gap, using autonomous vehicles as a specific domain of interest.\nInterface Modalities \u2013 Research on in-vehicle interfaces for explanation, such as visual heads-up-displays (HUDs) [10, 13, 59], audio interaction [32, 47, 53], and even haptic feedback for drivers [16] has shown that there are a variety of ways explanations can support users, each dependent on the use-case and context of interest. The benefits and drawbacks of different modalities often relate to the complex process of transferring sufficient knowledge to accomplish a task (such as to build understanding through system transparency) without creating too much cognitive load [12, 34, 40]. In the present study, we leverage video and audio explanations for our study on the impact of errors, as these are common and efficient methods to transfer information to a user without overwhelm. We present both modalities of information at the same time to increase the accessibility of our study and ensure there is successful transfer of information.\nExplanation Content \u2013 Choosing the appropriate content for an AI explanation is crucial for enhancing rider trust and reliance [51]. Recent work as focused on providing a description of what a vehicle is doing and why a vehicle is doing it, as these enable a user to create a momentary evaluation of the AV's behavior in terms of reliance [27]. For example, Koo et al. [41] present 'how', 'why', and a combination of both in various simulated autonomous driving scenarios to assess driver attitudes and safety performance. They found improved safety when both 'how' and 'why' were presented to drivers, but a preference for 'why' explanations alone. Kaufman et al. [34] leveraged auditory and visual explanations to teach humans to be better drivers via an 'AI coach', highlighting the additive value of what and why-type information for transferring knowledge, adding that too much information can cause overwhelm. They emphasize the need for explanations to strike a balance between complexity and comprehensiveness. The impact of what (similar to Koo's how) and why-type explanation errors explored in the present study - remains unknown.\nFactors Impacting Trust \u2013 Knowing what factors may impact a person's trust and reliance decisions is vitally important to designing XAI explanations to support them. Hoff and Bashir's theoretical model of trust in automation highlights the importance of three distinct yet interdependent facets of trust: dispositional (e.g. personality or cultural attitudes), situational (e.g. based on a particular context of use), and learned trust (e.g. based on a present evaluation of system performance) [27]. Additionally, Kaufman et al. [36] developed a framework to understand situational awareness in joint action between humans and AV, a key focus of explainable AI transparency in safety-critical situations like driving. They describe how communications like AV explanations can enable human-AV teamwork to achieve particular"}, {"title": "2.2 Al Errors", "content": "Widespread integration of AI systems into everyday tasks has demonstrated huge benefits to productivity and optimiza-tion in many domains [22]. However, concerns over Al errors remain a major point of contention, particularly as systems proliferate. Examples of errors with real-world consequences include language models' propensity to hallucinate [69] and algorithmic bias that favors men over women used in the hiring process [14].\nGeneral AV Errors \u2013 Autonomous vehicle driving errors have important real-world consequences, which may include physical harm or even fatalities [23]. Even in cases where AV driving outperforms humans [62], concerns over vehicle errors can be a major hindrance to adoption and use [11]. Luo et al. [48] shows that errors caused by an AV had a more significant negative impact on user trust than external errors, such as those caused by other drivers or road conditions. Declines in trust may be difficult to recover from and have long-lasting effect [63]. Explanations are no panacea: trust is difficult to achieve when the system itself performs poorly, even when explanations meant to elicit trust are presented [33]. In this paper, we seek to connect prior work showing the dire impact of driving errors [48, 70] to broader XAI literature investigating user perceptions of AI explanations [8, 42].\nExplanation Errors - Some prior work has investigated the impact of explanation errors for autonomous systems. In a study on \"white box\u201d XAI, Cabitza et al. [8] found that non-expert users tend to not catch explanations errors and believe the system even when explanations were wrong, attributing the phenomena to the Halo Effect found in social psychology where people assumed correctness of the system without verifying accuracy. Over- or under-reliance is a problem as people learn to calibrate their interactions with Al systems [20]. Conflicting results suggest that explanations may help reduce over-reliance in some cases [67], but increase over-reliance in others [39]. Other research has shown that the influence of explanations on reliance may be based on the systems' performance itself [56].\nKnowledge Gap - Though several prior studies have investigated the consequences of accurate AV explanations, the impact of explanation errors on reliance behavior and related outcomes remains unexplored. Of particular interest to us are cases when the autonomous vehicle's driving performs properly, as this allows us to separate the impact of driving performance from the impact of explanation performance. This has yest to be explored, and we address this important knowledge gap."}, {"title": "3 Method", "content": "We conducted an online experiment using realistic, simulated driving scenarios of an autonomous vehicle driving through various environments. These ranged from rural to urban driving and from routine navigational challenges like driving around construction cones to challenging situations like avoiding collisions with erratic drivers. During the experiment, participants viewed videos of the scenarios and, after each video, provided ratings related to trust, reliance, satisfaction, and evaluation of the AV.\nTo test the impact of explanation errors, participants were shown three versions of each scenario. Those with an \"accurate\" explanation were correctly told 'what' the AV was doing and 'why' it was doing it. Errors were introduced via a \"low\" error condition, where the AV correctly explained 'what' it was doing but incorrectly explained 'why', and a \"high\" error condition, where both 'what' and 'why' explanations were incorrect. In all three versions of each scenario, the AV drove identically, so only the explanations differed. The AV drove accurately and lawfully in all cases. Participants were randomly shown videos from 9 of 27 possible scenarios, providing ratings after each video, making 27 total video ratings per participant (9 scenarios X 3 accuracy levels). Scenarios were presented in random order."}, {"title": "3.3 AV Explanations and Errors", "content": "During all driving scenario videos, explanations of the AV's behavior were provided to participants at the time of AV action. Explanations were presented simultaneously in visual and auditory form to reflect current trends in AV explanation research [34], as well as to provide better accessibility to participants. Specifically, explanations were presented visually in written English on the vehicle's dashboard display and in auditory fashion via spoken English produced by Amazon Polly Text-To-Speech [30]. For an example of the visual presentation of explanations, see Figure 1.\nExplanations provide information on what' action the AV is doing (e.g. braking) as well as a local explanation for 'why' the vehicle is doing it (e.g. to avoid a collision with a merging vehicle). The importance of 'what' and 'why'-type information has been studied in past experimental work on explanations of driving behavior by Kaufman et al. [34] and Koo et al. [41]. Frameworks by Wang et al. [68] and Lim et al. [45] in cognitive science and Miller [51] in philosophy emphasize that both 'what' and 'why' information are important for transparency, trust, and user understanding when interacting with AI-based systems like AVs. The work presented here examines the impact of both 'why' errors alone, and 'why and what' errors combined, respectively.\nExplanation errors were presented via three conditions differing in the accuracy of the explanation provided. Comparing the \"accurate\" to \"low\" group shows the impact of errors related to the AV's rationale for behavior (i.e. 'why'). Comparing the \u201clow\u201d to \u201chigh\u201d group shows the impact of adding errors related to the AV's description of its own action (i.e. adding 'what' to the 'why' error). It is important to note that the AV's actual driving behavior remained consistent, accurate, and lawful across conditions; only the explanation itself changed. Participants rated videos for 9 of 27 possible scenarios \u2013 seeing one video for each of the three conditions per scenario - presented in random order. This makes a total of 27 total video ratings per participant (9 scenarios X 3 accuracy levels).\nAs part of a secondary analysis, we further categorized mistakes in the \"high\" error condition - where the AV is providing an incorrect description of what it is doing - based on the potential harm that would result should the AV have acted on the mistaken 'what' description. For example, if the AV mistakenly says it is going \"left\" when really it is going straight, we hypothesized that the impact of this error would be greater if going left would result in an accident, as opposed to simply a wrong turn. To explore if this difference in pragmatics does impact our results, we run a secondary analysis within just the 'high' condition to test for impact of potential harm."}, {"title": "3.4 Measures", "content": "3.4.1 Main Outcomes: Scenario Ratings. These measures were taken after each video to provide insight into the impact of the explanation errors.\n\u2022 Comfort relying on AV (proxy for trust). \"How comfortable would you feel relying on this AV in this specific situation?\" (0-10 rating scale)\n\u2022 Reliance on AV (preference to take control). \"If this specific situation were to happen in the real world, would you prefer to rely on an AV or take control yourself?\" (Binary choice: 'Rely on AV' or 'Take control myself'). To aid comparison, reliance data was transformed from 0-1 to 0-10 to be on the same scale as the other variables. This does not impact how the data is interpreted.\n\u2022 Satisfaction with explanation. \"How satisfied are you with the AV's explanation?\" (0-10 rating scale)\n\u2022 Confidence in AV driving ability. \u201cPlease rate your confidence in the AV's driving ability.\" (0-10 rating scale). Note: the actual driving performance never changed.\n3.4.2 Context Descriptors. These measures were taken for each scenario after the \"accurate\" condition video only to provide insight into the impact of driving context on main outcomes.\n\u2022 Harm of Driving Situation. \"In the real world, how would you rate the risk of harm of this specific driving situation?\" (0-10 rating scale)\n\u2022 Difficulty of Driving. \u201cIn the real world, how would you rate the difficulty of driving in this specific situation?\" (0-10 rating scale)\n3.4.3 Additional Outcomes. Additional outcomes further contextualize our main experimental findings. These were measured before and/or after the rating task.\n\u2022 Expertise. Expertise was measured before the rating task via three self-rated questions related to a person's knowledge and understanding of AVs. (5-point likert scale from Strongly Disagree to Strongly Agree)\n\u2022 Trust. Trust was measured before and after the rating task via four questions related to adaptability, safety, overt trust, and willing to recommend a friend to ride in an AV. (5-point likert scale from Strongly Disagree to Strongly Agree)\n\u2022 Explicit Factors Contributing to Reliance Decisions. After the task, participants rated the relative impact of seven aspects of the driving and explanation experience on their reliance decision-making. (5-point likert scale from Not At All to Very Much). Factors included: potential harm, driving difficulty, accuracy of 'what' and 'why' information, AV driving performance, prior knowledge, and previous scenarios viewed."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Impact of Explanation Errors: Comfort, Reliance, Satisfaction, and Confidence", "content": ""}, {"title": "4.1.1 Summary of Main Outcomes", "content": "Across our four main outcomes, segmenting the data by explanation error condition level gives us an initial impression on the impact of our experimental manipulation. We find the highest scores for comfort relying on the AV, reliance preference, satisfaction with the AV's explanation, and confidence in the AV's driving ability in the Accurate condition, followed by the Low condition and then the High condition. Visualizing main outcomes by scenario, we find that the effect of condition was very consistent across scenarios.\nEven when explanations were accurate, participants' overall comfort relying on the AV (comfort), and their preference to rely on the AV (reliance), were middling to low, reflecting the overall reluctance to trust AVs found in past human-AV interaction research by Kenesei et al. [38]. Participants were more positive about the AV explanations provided to them in the study, however, this satisfaction deteriorated quickly when errors were introduced. Despite the AV's driving performance - and therefore, demonstrated ability - remaining consistent across all conditions, impressions of the AV's driving ability worsened as AV explanation errors were introduced."}, {"title": "4.1.2 Overall Impact of Errors on Main Outcomes", "content": "Linear mixed-effects (LME) models were used to measure the effect of errors on each major outcome: comfort relying on the AV, reliance decision, satisfaction with explanation, and confidence in the AV's driving ability. Though LME models produce similar results to mixed-model ANOVAs, they offer greater flexibility for repeated measures experiments. They also incorporate random effects, which help reduce the likelihood of a Type 1 error [7]. Models were made with fixed effects for error condition and random effects for the specific scenario and individual participant. The fixed effects allow us to compare differences between study conditions, while the random effects allow us to control for differences by scenario and individual participant.\nIndividual comparisons between each condition (Accurate-Low, Accurate-High, Low-High) show highly significant effects (p < 0.001) across all four outcome measures, implying error condition significantly impacted outcomes. The results for comfort relying on the AV, reliance decision, and satisfaction with the explanation follow the expected trend: we find that outcome scores decrease as error level increases. Surprisingly, we found the same effect on a person's evaluation of the AV's driving ability, where confidence scores also decrease as error level increases. This is unexpected, given that the driving performance shown in the videos were identical and only the explanations changed. The implication is that there is a cross-over effect between a person's evaluation of the explanation and the person's evaluation of the vehicle's driving, despite evidence suggesting that the driving performance is consistently high quality."}, {"title": "4.1.3 Potential Harm of 'What'-type Errors", "content": "In the high error condition group, what-type errors are incorrect descriptions of what the AV is doing. To understand the impact of the potential harm of these errors, we conducted a secondary analysis comparing errors that would result in an accident if acted upon by the AV versus those that would not. To test this case, we use LME models with fixed effects for the categorization of potential harm (0 or 1) and random effects for driving scenario and individual differences by participant. This analysis is conducted only on data from the \"high\" condition in isolation."}, {"title": "4.2 Impact of Driving Context: Perceived Harm and Difficulty of Driving", "content": ""}, {"title": "4.2.1 Relationship Between Harm and Difficulty", "content": "By examining the impact of harm and difficulty on outcome ratings, we can derive an understanding of how these contextual factors influenced our results. Unsurprisingly, we found a strong, positive relationship between the perceived harm and the difficulty of driving in a particular scenario (r = 0.69, p < .001; Adj R2 = .48)."}, {"title": "4.2.2 Impact of Harm and Difficulty On Main Outcome Ratings", "content": "We assess the impact of harm and difficulty on main outcome ratings using LME models with added fixed effects for the average harm and difficulty of scenarios. We maintain the fixed effect of outcome condition as well as random effects for scenario and participant. Using this model, main effects for difficulty and harm give us an understanding of how these factors impacted outcome judgments independent"}, {"title": "4.2.3 Impact Of Harm and Difficulty On Differences Between Error Conditions (Interaction Effects)", "content": "Using a similar LME model, we can assess if difficulty or harm impacted the relationship between error level and outcomes by looking at interaction effects. Specifically, we examine if the differences found between each error condition are predicted by a scenario's average difficulty or harm. In this case, we use the accurate group as a common reference (model intercept) and then compare if the changes of each main outcome from accurate to low and accurate to high vary with respect to difficulty and harm. We find significant interaction effects for several of our outcomes, implying that contextual characteristics like harm and difficulty may be moderating how much of an impact an error may have.\nComfort Relying on AV \u2013 Starting with comfort relying on the AV, we find that scenario difficulty has more of an impact on comfort in the low error condition than in the accurate condition. Specifically, when difficulty increased, comfort decreased significantly more in the low condition compared to the accurate condition. A similar but opposite effect was found with harm: in the low error condition, comfort increases significantly more with higher harm than in the accurate condition. The implication is that comfort judgments may be more sensitive to the difficulty and harm of the scenario when there are some errors (low condition) compared to when there are no errors (accurate). We do not see difficulty or harm as more impactful on comfort judgments in the high error condition compared to the accurate condition. This implies that these judgments of comfort are likely based on the high magnitude of error (main effect) for the high group, as opposed to being influenced by the difficulty or harm of the driving context (interaction effect) in these cases.\nReliance Decision - The case is similar for reliance decision, though the effects are trending towards significance as opposed to being statistically significant. We find that scenario difficulty has more of an impact on reliance in the low error condition than it did in the accurate condition. Specifically, when difficulty increased, reliance decreased more in the low condition compared to the accurate condition. For harm in the low error condition, higher harm increases reliance significantly more than in the accurate condition. The implication is that reliance judgments may be more sensitive to the difficulty and harm of the scenario when there are some errors (low condition) compared to when there are no errors (accurate). As with comfort, we do not find difficulty or harm as more impactful on reliance in the high error condition compared to the accurate condition. This implies that judgments of reliance when errors are high are likely based on the error itself rather than difficulty or harm.\nSatisfaction w/ Expl. - The interaction effects change for satisfaction with an explanation. We do not see difficulty or harm as more impactful on satisfaction in the low error condition compared to the accurate condition, implying that differences in judgments of the explanation are likely based on the explanation quality itself without being influenced by the difficulty or harm of the situation in these cases. The same effect is seen when comparing differences between the"}, {"title": "4.3 Trust and Expertise", "content": ""}, {"title": "4.3.1 Impact Of Exposure To Errors On Trust", "content": "We measured participant trust before and after the experiment was completed to see if exposure to repeated AV explanation mistakes affected AV trust levels as a disposition. In general, using a paired-samples t-test, we did not find that exposure to errors in the experiment impacted trust. Only one"}, {"title": "4.3.2 Correlations of Initial Trust and Subjective Expertise with Outcomes", "content": "Calculating correlations between initial trust and self-reported expertise level and a participants' average outcome ratings allows us to assess if trust or expertise can predict how a person will evaluate the AV. We found a strong positive correlation between initial trust and expertise, (r = 0.58, p < 0.001), implying that those who know more about AVs trust them more.\nWe found moderate positive correlations between expertise and comfort (r = 0.38, p < 0.001), satisfaction (r = 0.35, p < 0.001), and confidence (r = 0.36, p < 0.001), meaning that those with more expertise reported more positively to these outcomes on average. We found high correlations between initial trust and comfort (r = 0.52, p < 0.001) as well as confidence (r = 0.51, p < 0.001). We found moderate correlations between initial trust and reliance (r = 0.30, p < 0.001) as well as satisfaction (r = 0.49, p < 0.001). These effects are consistent with linear mixed-effects models looking at the main effects of expertise and initial trust on outcomes. Together, these results imply that participants with higher initial trust or expertise tended to rate outcomes higher on average."}, {"title": "4.4 Self-Reported Rationale For Trust and Reliance Decisions", "content": ""}, {"title": "4.4.1 Factors Contributing to Rating Decisions", "content": "We explicitly inquired upon the basis of reliance decisions by asking participants to rate the relative importance of several factors on their reliance ratings. This was done after the rating task concluded. As a whole, we found that the accuracy of the explanation on 'what' the AV is doing was the most important consideration, followed by the explanation accuracy of 'why' the AV is doing it, the harm of the situation, the AV's driving ability, and the difficulty of the driving situation. Prior scenes viewed and prior knowledge did not have much reported impact, though this is unsurprising, as these implicit judgments may be difficult for participants to self-describe. Many of these differences were significant based on ANOVA. Notably, harm was significantly more important than difficulty, while explanation accuracy (both 'what' and 'why' components) were significantly more important than AV driving ability. This latter effect may be in part due to the salience of these factors based on the study's manipulation. We did not see significant differences between the importance of 'what' and 'why' accuracy."}, {"title": "4.4.2 Feedback from Participants", "content": "Participants were given the opportunity to express their general views on autonomous vehicles on how explanation errors impacted their decision-making specifically. Unsurprisingly, many participants reflected general concerns with AVs, including doubt that they can \"adapt to any circumstance [causing] a bigger accident.\" One participant commented that \"after a lifetime of driving myself, I'm not sure if I feel comfortable giving over control to a computer.\" Desire for control was brought up multiple times. The common sentiment was that giving control to AVs is considered risky.\nRegarding exposure to explanation errors, one participant commented that they were \"less confident in the reliability of [autonomous] vehicles\" after exposure to errors, while another commented that when explanations \"were wrong, [it made] my confidence in the system shaky.\u201d These comments reiterate our general study findings on the negative impact of errors on reliance, as well as the holistic judgment of an AV's ability in general. Some participants broke their decision-making down in terms of individual factor priority. For instance, \u201cif the AV described the action it was taking incorrectly I was a lot less confident and consistently chose to take control myself... if the reason was incorrect, it still impacted my confidence level but not as much.\" This reflects the general trend found in our individual factor analysis suggesting 'what' information may be more important than 'why' rationale, even if this trend was not found to be statically significant."}, {"title": "5 Discussion", "content": "This study aimed to assess the impact of autonomous vehicle (AV) explanation errors and driving context characteristics on participant comfort relying on an AV, preference to rely on the AV instead of taking control themselves, satisfaction with the explanation, and confidence in the AV's driving ability. Using a mixed-methods approach with a heterogeneous sample of participants (n = 232), we found that explanation errors and contextual characteristics like driving difficulty and perceived harm have a large impact on how a person may think, feel, and behave towards AVs. Echoing prior work by Nourani et al. [55], important consideration must also be given to personal factors like initial trust and expertise, which may further impact how a person interacts with the system. Our results provide insight into how to design effective human-AV interactions and interactions with AI-based systems more generally. We discuss key findings and their implications for future AV design in these contexts.\nAutonomous vehicle (AV) explanation errors had a detrimental effect on all outcomes. Our results suggest that errors significantly reduced participant comfort relying on an AV, preference to rely on the AV instead of taking control themselves, and satisfaction with the explanation. These effects are unsurprising, as we would expect outcomes related to a person's reliance decision or satisfaction with an explanation to reflect the system's performance [27]. We were surprised, however, to find crossover effects between the explanatory performance of the AV and a person's confidence in the AV's driving ability, given the AV's actual demonstrated driving performance remained consistent across all conditions. This crossover effect alludes to the mental model of potential AV users, where evaluation of the"}, {"title": "5.1 Implications For Autonomous Vehicle Design and Research", "content": "Our results have important implications for future AV design and research. Understanding the consequences of AV explanation errors and contextual characteristics like driving difficulty and perceived harm are necessary first steps towards designing vehicles which may be more trustworthy, reliable, and satisfactory for people interacting with the AV. In this section, we will discuss how our study's insights can be used to design more trustworthy AVs.\nThe foremost implication of this work is to emphasize the importance of designing systems that can produce accurate explanations for AV decisions. This implication is not too surprising, as why would designers ever intentionally produce inaccurate explanations? It becomes more meaningful, however, when testing explanatory systems before deployment. Our results indicate that even with a well-functioning driving system, if explanations are not of high quality, people still won't want to use the AV.\nThe crossover between a person's evaluation of explanation performance and driving performance may be of particular interest to AV designers. If this crossover is a concern as it should be in the case of deployment - the most obvious solution would be to work on improving explanations so that they are on par with driving ability. In the meantime, specific UI features may be implemented to help users separate their evaluations of explanations from driving"}, {"title": "5.2 Limitations and Future Study", "content": "As with any study, this research is not without limitations. First, though study findings were generally robust and fit within logical narratives, there is the possibility that findings resulting from online data collection and based on simulations may not generalize to real-world attitudes or behavior. A large effort was put into making driving simulations as realistic as possible, however, we could not test the effect of explanation errors on real roads out of concern for participant safety. Future work should seek to test the impact of errors in a real-world environment. A similar concern may be found for study outcomes: though results on reported reliance or comfort may be clear in a research context, there is no guarantee that explicit declarations of thought or behavior will remain consistent when immersed in a real-world driving context with real consequences of bodily or financial harm. This is a concern for all studies which rely on survey measures or explicit participant statements as a proxy for real-world behavior. It is possible that seeing each scenario three times (with different explanations) may have impacted the ratings provided. This could be mitigated in the future by showing participants only one video per scenario, randomized by error condition. Finally, we examined proximal explanations for action (\u201cbraking\u201d) and cause of action (\u201c... a pedestrian is crossing the road.\u201d) presented in written fashion and auditory fashion. Future work can examine explanations of distal causes (\u201cpedestrian in the road... because there is an obstacle on the sidewalk\") which could provide additional context for why a driving situation is happening in the first place. These could be explained using potentially different modalities of presentation."}, {"title": "6 Conclusion", "content": "In a simulated driving study with 232 participants we tested how autonomous vehicle (AV) explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) impact four driving perception and behavior-related outcomes: comfort relying on an AV, preference to rely on the AV instead of taking control themselves, satisfaction with the explanation, and confidence in the AV's driving ability.\nOur results indicate that explanation errors, contextual characteristics, and personal traits have a large impact on how a person may think, feel, and behave towards AVs. Explanation errors negatively affected all outcomes. Surprisingly, this included reduced ratings of the AV's driving ability, despite driving performance remaining constant. The negative impact of errors increased with error magnitude and the potential harm of the error, providing evidence that implications of an error matter in addition to the mere presence of an error. Harm and driving difficulty directly impacted outcomes as well as moderated the relationship between errors and outcomes. In general, harm was associated with lower comfort, reliance, and confidence ratings, while driving difficulty was associated with higher reliance ratings. Overall harm was the more important contextual factor of consideration. In terms of a decision function for AV reliance, perceived harm - influenced by driving difficulty - may underlie the evaluation between a person's confidence in the AV's performance compared to their own ability. We found that individuals with higher expertise tended to trust AVs more, and these each correlated with more positive outcome ratings in turn.\nOverall, our results emphasize the need for accurate and contextually adaptive AV explanations to foster trust, reliance, satisfaction, and confidence. Understanding the ramifications of explanation errors can help future AV research and design teams prioritize design and better understand the impacts of their design choices. They also provide a foundation for context-aware design, personalized explanation interfaces, and potential ethical or regulatory guidelines for the deployment of explainable AI (XAI) systems for autonomous vehicles that are safe and trustworthy."}]}