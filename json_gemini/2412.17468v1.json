{"title": "Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning", "authors": ["Jaesun Shin", "Eunjoo Jeon", "Taewon Cho", "Namkyeong Cho", "Youngjune Gwon"], "abstract": "While message passing graph neural networks result in informative node embeddings, they may suffer from describing the topological properties of graphs. To this end, node filtration has been widely used as an attempt to obtain the topological information of a graph using persistence diagrams. However, these attempts have faced the problem of losing node embedding information, which in turn prevents them from providing a more expressive graph representation. To tackle this issue, we shift our focus to edge filtration and introduce a novel edge filtration-based persistence diagram, named Topological Edge Diagram (TED), which is mathematically proven to preserve node embedding information as well as contain additional topological information. To implement TED, we propose a neural network based algorithm, named Line Graph Vietoris-Rips (LGVR) Persistence Diagram, that extracts edge information by transforming a graph into its line graph. Through LGVR, we propose two model frameworks that can be applied to any message passing GNNs, and prove that they are strictly more powerful than Weisfeiler-Lehman type colorings. Finally we empirically validate superior performance of our models on several graph classification and regression benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recently, message passing graph neural networks and its variants have emerged as an effective method to learn graph representations (Morris et al. (2019); Chen et al. (2019); Balcilar et al. (2020); Kondor et al. (2018); Cai et al. (2021); Hamilton et al. (2017); Gilmer et al. (2017); Wu et al. (2020)). Since message passing GNNs are designed to learn node representations, they can extract informative node embeddings by capturing localized information. However, they can hardly capture topological information of the entire graph (Chen et al."}, {"title": "2 Preliminaries", "content": "In this section, we briefly recall Weisfeiler-Lehman test, graph neural network, and some basic knowledge in algebraic topology related to persistence homology."}, {"title": "2.1 (Higher) Weisfeiler-Lehman Test", "content": "The Weisfeiler-Lehman test (WL test) is an algorithm which determines the graph isomorphism problem according to the histogram of colors on the vertices where the colors are iteratively aggregated by those of the neighborhood vertices. Precisely, for a finite graph G = (V, E) with the initial coloring $\\mathcal{X}: V \\rightarrow \\mathbb{Z}, v \\rightarrow 1$, the n-th coloring $\\mathcal{X}_{n} \\in \\operatorname{Hom}(V, \\mathbb{Z})$ is given by $\\mathcal{X}_{n}(v) := \\sum_{u \\in \\mathcal{N}(v)} \\mathcal{X}_{n-1}(u)$, where $\\mathcal{N}(v)$ is a neighbor of $v$ in G. Then two graphs G and G' are isomorphic only if their associated n-th colorings $\\mathcal{X}_{n}$ and $\\mathcal{X}'_{n}$ coincide for all n \u2265 1. To distinguish two graphs of the same order m = |V|, it suffices to terminate the algorithm in the n-th iteration for some n = O(mk) Douglas (2011), and it is known that the algorithm is effective for a broad class of graphs Babai and Kucera (1979).\nHigher-order variant of WL test, named k-WL, has been proposed to improve expressive power and apply color refinements iteratively on vertex tuples instead of single vertices. For"}, {"title": "2.2 Graph Neural Network", "content": "Graph neural network (GNN) computes the structure of a graph and its node features to learn a representation vector $\\mathbf{h}_{v}$ of a vertex v. Modern GNNs use spatial methods based on a message passing scheme (Kipf and Welling (2016)). In short, the learning process of GNNs iteratively updates the node features from those of the neighboring nodes, which formally associates (1) the representation vectors $\\mathbf{h}_{v}^{(k)} \\in \\mathbb{R}^{d}$, and (2) the aggregation procedure $\\mathbf{h}_{v}^{(k)} = \\phi^{k}(\\mathbf{h}_{v}^{(k-1)}, f^{k}(\\{\\mathbf{h}_{u}^{(k-1)} : u \\in V | (v, u) \\in E\\}\\}))$, given by an aggregation function $f^{k}$ that operates on multisets and a combine function $\\phi^{k}$. To extract the graph-level representation $\\mathbf{h}_{G}$, various pooling methods, also called readout operations, have been proposed to summarize the representation vectors $\\mathbf{h}_{v}^{(k)}$ of nodes $v \\in V$ (Zhang et al. (2018); Ying et al. (2018); Ranjan et al. (2020); Hofer et al. (2020)). In terms of the expressive power of $\\mathbf{h}_{G}$, it is proven in Xu et al. (2018a) that GNNs are as powerful as the WL test under the assumption on the injectivity of $f^{k}$ and $\\phi^{k}$ for each k."}, {"title": "2.3 Simplicial Complexes, Persistence Homology, and Vietoris-Rips Filtration", "content": "In this section, we will introduce some basic knowledge in algebraic topology. Readers who are already familiar with algebraic topology may skip this section without hesitation."}, {"title": "2.3.1 SIMPLICIAL COMPLEXES", "content": "In this subsection, we recall some basics of simplicial complexes. Briefly speaking, a simplex is the simplest geometric object, such as points, line segments, triangles, and their higher-dimensional analogs. Moreover, a simplicial complex is a set of simplices satisfying certain rules. Both are central topological concepts in algebraic topology in order to understand the shape and structure of complex spaces. Formal definitions of both objects are as follows:\nDefinition 1 A k-simplex is a k-dimensional polytope which is the convex hull of affinely independent k+1 vertices. Moreover, a simplicial complex K is a set of simplices satisfying the following: (1) every face of a simplex in K is also in K, and (2) for any $\\sigma_{1}, \\sigma_{2} \\in K$ such that $\\sigma_{1} \\cap \\sigma_{2} \\neq \\emptyset, \\sigma_{1} \\cap \\sigma_{2}$ is a face of both $\\sigma_{1}$ and $\\sigma_{2}$. Finally, the d-skeleton of a simplicial complex K is the simplicial complex consisting of the set of all simplices in K of dimension d or less."}, {"title": "2.3.2 \u041d\u043eMOLOGY AND BETTI NUMBERS", "content": "Homology is an abstract way of associating topological or algebraic spaces with a sequence of algebraic objects. In algebraic topology, this allows to encode the topological information of a space through a chain of vector spaces and linear maps. We refer to Hatcher (2002) for interested readers. In general, homology can be defined over an arbitrary field, but for simplicity, we restricted our attention to $\\mathbb{Z}_{2}$. Furthermore, we will only deal with the homology classes whose algebraic objects are vector spaces.\nLet $C_{0}, C_{1},...$ be vector spaces over $\\mathbb{Z}_{2}$, and let $\\partial_{n}: C_{n} \\rightarrow C_{n-1}$ be linear maps satisfying $\\partial_{n+1} \\circ \\partial_{n} = 0$ for all $n \\geq 0$, which we call boundary operators. A chain complex refers to the sequence\n$C_{\\bullet}: ... \\rightarrow C_{n+1} \\stackrel{\\partial_{n+1}}{\\longrightarrow} C_{n} \\stackrel{\\partial_{n}}{\\longrightarrow} C_{n-1} \\stackrel{\\partial_{1}}{\\longrightarrow} ... C_{0}\\rightarrow 0$.\nLet $\\operatorname{ker}(\\partial_{n}) = \\{x \\in C_{n} | \\partial_{n}(x) = 0\\}$ be the kernel of $\\partial_{n}$, which we call cycles, and let $\\operatorname{im}(\\partial_{n}) = \\{y \\in C_{n-1} | \\operatorname{there} \\operatorname{exists} x \\in C_{n} \\operatorname{such} \\operatorname{that} \\partial_{n}(x) = y\\}$ be the image of $\\partial_{n}$, which we call boundaries. Since $\\partial_{n+1} \\circ \\partial_{n} = 0$ holds for all n, it is clear that $\\operatorname{im}(\\partial_{n+1}) \\subseteq \\operatorname{ker}(\\partial_{n})$ for all n. Since both $\\operatorname{im}(\\partial_{n+1})$ and $\\operatorname{ker}(\\partial_{n})$ are vector spaces, we may form the quotient vector space\n$H_{n}(C_{\\bullet}) := \\operatorname{ker}(\\partial_{n}) / \\operatorname{im}(\\partial_{n+1})$\nfor all n \u2265 0. We call $H_{n}(C_{\\bullet})$ the n-th homology (group) of $C_{\\bullet}$, and the elements of $H_{n}(C_{\\bullet})$ are called homology classes. Moreover, the dimension of $H_{n}(C_{\\bullet})$ as a vector space, denoted by $\\beta_{k}(C_{\\bullet})$, is called the n-th Betti number of $C_{\\bullet}$."}, {"title": "2.3.3 SIMPLICIAL HOMOLOGY", "content": "We will introduce simplicial homology, a type of homology that is widely used in algebraic topology and tailored to our purposes. As the name suggests, simplicial homology is derived from simplicial complexes.\nGiven a simplicial complex K, we define a chain complex $C_{\\bullet}(K)$ of $\\mathbb{Z}_{2}$-vector spaces in the following way: Let $K_{n} = \\{\\sigma_{1},...,\\sigma_{k}\\}$ be the n-skeleton of K. Then we define $C_{n}(K)$ to be the vector space over $\\mathbb{Z}_{2}$ with $K_{n}$ as a basis. Moreover, for each n-simplex $\\sigma_{e} = [v_{0},..., v_{n}] \\in C_{n}(K)$, the boundary operator $\\partial_{n}: C_{n}(K) \\rightarrow C_{n-1}(K)$ is defined by\n$\\partial_{n}(\\sigma_{e}) := \\sum_{j=0}^{n} [v_{0}, ..., \\hat{v}_{j}, ..., v_{n}]$,"}, {"title": "2.3.4 PERSISTENCE HOMOLOGY AND DIAGRAMS", "content": "Given a point cloud P sampled from the unknown manifold M, how can we determine the topological characteristics of M from P? The simplest way is to generate a suitable manifold K from P and compute its homology. However, homology is very sensitive to small changes, so there is a problem that the topological characteristics of K can be very different from those of M. Persistence homology (Oudot (2017)) addresses this problem by incorporating the scale \u03b5, which varies from 0 to \u221e into homology computations.\nAs \u03b5 increases, the topological characteristics of a manifold induced by \u03b5 can vary: some topological information born at some $\\varepsilon_{0}$ and die $\\varepsilon_{1}$. Informally, the idea of persistence homology is to track all the birth and death of topological information with scale \u03b5.\nA persistence homology essentially tracks the evolution of homology classes in a filtration of simplicial complexes K. Once a simplicial complex K admits a filtration\n$\\mathcal{K}^{\\bullet}: \\emptyset = \\mathcal{K}^{-\\infty} \\subseteq ... \\subseteq \\mathcal{K}^{\\varepsilon} \\subseteq ... \\subseteq \\mathcal{K}^{\\delta} \\subseteq ... \\subseteq \\mathcal{K}^{\\infty} = \\mathcal{K}$,\nthen each inclusion $f_{i,j}: \\mathcal{K}^{i} \\rightarrow \\mathcal{K}^{j}$ is a simplicial map so that it induces a linear map $H_{n}(f_{i,j})$ between the homologies of $H_{n}(\\mathcal{K}^{i})$ and $H_{n}(\\mathcal{K}^{j})$. Such indices i and j are referred to as 'time' in persistence homology theory. Fix a non-negative integer n \u2265 0. For any i < j, one can see whether a homology class in $H_{n}(\\mathcal{K}^{i})$ are mapped to the same homology class in $H_{n}(\\mathcal{K}^{j})$ by $H_{n}(f_{i,j})$. If this happens, such a homology class is said to persist from time i to j. If not, such a class is said to have died at some time between i and j. If a homology class first appears at time i and disappears at time j, then we say that this class is born at time i and dies at time j, and appends (i, j) as an element of n-th persistence homology. By tracking all homology classes of $\\{H_{n}(\\mathcal{K}^{t})\\}_{t \\in \\mathbb{R}}$ and appending them as elements of n-th persistence homology, the n-th persistence homology can be seen as a multi-set of birth and death tuples. Now, regard the n-th persistence homology as a multi-set of points in $\\mathbb{R}^{2}$. Then we call such a multi-set n-th persistence diagram.\nInformally, n-th persistence homology (or diagram) tracks different topological features depending on n. For example, 0-th persistence homology tracks the birth and death of connected components, while 1-th persistence homology tracks those of circular holes. The general n-th persistence homology has information about the birth and death of n-dimensional holes. Through persistence homology information for each n, we can understand the characteristics of a given topological object."}, {"title": "2.3.5 VIETORIS-RIPS COMPLEX AND FILTRATION", "content": "Given a point cloud P with a distance matrix M and a scale \u03b5, the Vietoris-Rips complex of $\\mathcal{P}$ is a type of simplicial complex constructed from P and M whose simplices are formed by connecting points in P that are within a certain distance \u03b5 of each other. In particular, given two scales $\\varepsilon_{1} < \\varepsilon_{2}$, the Vietoris-Rips complex of $\\varepsilon_{1}$ is contained in that of $\\varepsilon_{2}$. Thus,"}, {"title": "3 Notations and conventions", "content": "In this section, we summarize some notations used throughout this paper. First of all, we use the following notations, which are commonly used, to distinguish between a set and a multi-set: we denote a set by $\\{...\\}$ and a multi-set by $\\{\\{...\\}\\}$. Moreover, the WL test refers to the 1-WL test, otherwise specified.\nLet $(\\mathcal{G},\\mathcal{C})$ be a space of graphs with node coloring $\\mathcal{C}$, and let $\\mathcal{X}_{G} \\subseteq \\mathbb{R}^{N}$ (or simply x) be a space of node features of $(\\mathcal{G},\\mathcal{C})$ containing (0,...,0), where $N \\in \\mathbb{N}$. For a graph $G\\in \\mathcal{G}$, $\\mathcal{V}(G)$ denotes a set of nodes in G, $\\mathcal{E}(G)$ denotes a set of edges in G, and $\\mathcal{EC}(G)$ denotes a multi-set of colored edges in G, that is, $\\mathcal{EC}(G) = \\{\\{\\{\\mathcal{C}(u), \\mathcal{C}(v)\\}\\} | \\{\\{u, v\\}\\} \\in \\mathcal{E}(G)\\}\\}$.\nFinally, since we frequently use the notation in Definition 3, we summarize it here again. Given a finite point cloud P and a non-negative symmetric matrix M of size $|P|\u00d7 |P|$ with zero diagonals, we denote the k-skeleton of $\\operatorname{VR}(\\mathcal{P}, M)$ as $\\operatorname{VR}_{k}(\\mathcal{P}, M)$, and put $\\operatorname{VR}_{k}(\\mathcal{P}, M) = \\{\\operatorname{VR}_{k}(\\mathcal{P}, M)\\}_{\\varepsilon \\in \\mathbb{R}}$ for $k \\in \\mathbb{Z}_{\\geq 0}$."}, {"title": "4 Theoretical Framework: Topological Edge Diagram", "content": "In this section, we introduce a novel edge filtration-based persistence diagram, which we call Topological Edge Diagram. To begin with, we introduce the edge filtration map, and then define the topological edge diagram by leveraging it. In the end, we will wrap up this section by analyzing its theoretical expressive power."}, {"title": "4.1 Definition of Topological Edge Diagram (TED)", "content": "To introduce our persistence diagram, we first introduce the edge filtration (map) which is essential in developing our theoretical framework. To grasp the distinction between node (Hofer et al. (2020); Horn et al. (2021)) and edge filtration, see Figure 2.\nDefinition 4 Let $G \\in \\mathcal{G}$ be a graph, and let $\\mathcal{C}$ be a node coloring of G.\n1. An edge filtration (map) $e f_{\\mathcal{C}}$ of G with respect to $\\mathcal{C}$ is defined to be a positive real-valued function $e f_{\\mathcal{C}}: \\bigsqcup_{G \\in \\mathcal{G}} \\mathcal{EC}(G) \\rightarrow \\mathbb{R}_{>0}$ such that $\\operatorname{sup}\\{e f(x) | x \\in \\bigsqcup_{G \\in \\mathcal{G}} \\mathcal{EC}(G)\\} < \\infty$, where $\\mathcal{EC}(G) = \\{\\{\\{\\{\\mathcal{C}(u), \\mathcal{C}(v)\\}\\} | \\{\\{u,v\\}\\} \\in \\mathcal{E}(G)\\}\\}\\operatorname{is} a multi-set of colored edges.\n2. For an edge filtration $e f_{\\mathcal{C}}$ and $i \\geq 0$, $p h_{\\operatorname{VR}}^{i}(G, e f_{\\mathcal{C}})$ is the i-th persistence diagram of 1-skeleton of Vietoris-Rips filtration with point cloud $\\mathcal{V}(G)$ whose distance matrix $\\mathcal{M}(G)$ is defined as follows: for any $i, j = 1, ..., |\\mathcal{V}(G)|$ corresponding to nodes $u_{i}, u_{j} \\in \\mathcal{V}(G)$,\n$\\mathcal{M}(G)_{i,j} = \\begin{cases}e f_{\\mathcal{C}}(\\{\\{\\mathcal{C}(u_{i}), \\mathcal{C}(u_{j})\\}\\}), & \\operatorname{if} \\{\\{\\mathcal{C}(u_{i}), \\mathcal{C}(u_{j})\\}\\} \\in \\mathcal{EC}(G), \\\\ \\infty & \\operatorname{otherwise}\\end{cases}$\nRemark 5 In this remark, we will explain why the 1-skeleton of the Vietoris-Rips complex of $\\mathcal{V}(G)$ is a natural choice when defining the persistent homology $p h_{\\operatorname{VR}}(G, e f_{\\mathcal{C}})$ of the graph G (Definition 4-2). Before going on, we briefly explain the conditions that the filtration $\\{\\mathcal{X}_{\\varepsilon}\\}_{\\varepsilon>0}$ of a simplicial complex X must satisfy to define persistent homology: given a simplicial complex X, the filtration $\\{\\mathcal{X}_{\\varepsilon}\\}_{\\varepsilon>0}$ to define the persistence homology of X should satisfy the following condition: for any \u03b5 > 0, $\\mathcal{X}_{\\varepsilon} \\rightarrow X$ as simplicial complexes and $\\lim_{\\varepsilon \\rightarrow \\infty} \\mathcal{X}_{\\varepsilon} = X$.\nIn this perspective, we see how the dimension of the clique complex of G is related when defining its Vietoris-Rips filtration. More precisely, let $\\operatorname{VR}_{d}(\\mathcal{V}(G), \\mathcal{M}(G))$ be the d-skeleton of Vietoris-Rips complex of scale $\\varepsilon > 0$ with point cloud $\\mathcal{V}(G)$ whose distance matrix is $\\mathcal{M}(G)$ (Definition 4). Since a graph $G \\in \\mathcal{G}$ is 1-simplicial complex, it is easy to see the following:"}, {"title": "4.2 Theoretical Expressivity of TED", "content": "It is clear that an injective edge filtration $e f_{\\mathcal{C}}$ of the node coloring $\\mathcal{C}$ includes all the pairwise node coloring information. Hence our question is whether TED also contains all of the node coloring information. For theoretical clarity, we first propose a weak assumption about the node coloring $\\mathcal{C}$, called the Degree Assumption.\nDefinition 7 (Degree Assumption) Let $\\mathcal{C}$ be a node coloring of G, that is, $\\mathcal{C}: \\bigsqcup_{G \\in \\mathcal{g}} \\mathcal{V}(G) \\rightarrow \\mathcal{X}$. We say that $\\mathcal{C}$ satisfies the degree assumption if the following holds: for any $u, v \\in \\bigsqcup_{G \\in \\mathcal{g}} \\mathcal{V}(G)$, if $\\mathcal{C}(u) = \\mathcal{C}(v)$, then deg(u) = deg(v).\nWe remark that the degree assumption is a very weak assumption related to node coloring, and almost all node colorings we know satisfy this assumption. For example, any node colorings of all message passing GNNs with arbitrary initial node colorings satisfy the Degree Assumption. Furthermore, the WL test, which is a classical graph isomorphism test, satisfies the Degree Assumption as demonstrated in the following example.\nNow, we are ready to show the strong expressive power of TED. Specifically, our next result shows that TED can incorporate all of the node coloring information if any graph $G \\in \\mathcal{G}$ has no isolated nodes.\nLemma 8 Let $G, H \\in \\mathcal{G}$ be graphs with no isolated nodes, and let $\\mathcal{C}$ be a node coloring of $G$ satisfying the degree assumption. If $\\{\\{\\mathcal{C}(u) | u \\in \\mathcal{V}(G)\\}\\} \\neq \\{\\{\\mathcal{C}(v) | v \\in \\mathcal{V}(H)\\}\\}$, then TED(G, $e f_{\\mathcal{C}}$) \u2260 TED(H, $e f_{\\mathcal{C}}$) for any injective edge filtration $e f_{\\mathcal{C}}$ and any $k \\geq 1$.\nProof Since $\\{\\{\\mathcal{C}(u) | u \\in \\mathcal{V}(G)\\}\\} \\neq \\{\\{\\mathcal{C}(v) | v \\in \\mathcal{V}(H)\\}\\}$, there are three cases:"}, {"title": "5 Algorithm: Line Graph Vietoris-Rips Persistence Diagram", "content": "In this section, we propose a novel neural-network-based algorithm, named Line Graph Vietoris-Rips Persistence Diagram, to implement TED. First, we construct the map $\\operatorname{t}_{\\phi}$ that transforms a colored graph into a colored line graph (Definition 11) in Section 5.1. Next, through $\\operatorname{t}_{\\phi}$, we propose an algorithm to construct an injective edge filtration, which is the core of our Line Graph Vietoris-Rips Persistence Diagram, in Section 5.2. Finally, we analyze its theoretical expressivity in Section 5.3."}, {"title": "5.1 Construction of the map $\\operatorname{t}_{\\phi}: (\\mathcal{G},\\mathcal{C}) \\rightarrow (\\mathcal{L}_{G},\\mathcal{C})$", "content": "First of all, we briefly recall line graph $L_{G}$ of a graph $G\\in \\mathcal{G}$. In graph theory, a line graph is a type of graph where the vertices correspond to the edges of a given graph G, and two vertices in the line graph are connected by an edge if and only if their corresponding edges in G share a common endpoint. Formally, it is defined as follows:\nDefinition 10 Let $G = (\\mathcal{V}(G), \\mathcal{E}(G)) \\in \\mathcal{G}$ be a graph. Its line graph $L_{G} = (\\mathcal{V}(L_{G}), \\mathcal{E}(L_{G}))$ is a graph such that (1)) each node in $\\mathcal{V}(L_{G})$ represents an edge in $\\mathcal{E}(G)$, and (2) for any $\\{\\{u_{1}, u_{2}\\}\\} \\neq \\{\\{v_{1}, v_{2}\\}\\} \\in \\mathcal{E}(G)$ with $u_{1}, u_{2}, v_{1}, v_{2} \\in \\mathcal{V}(G)$, their corresponding nodes in $\\mathcal{V}(L_{G})$ are adjacent if and only if $u_{i} = v_{j}$ for some i = 1,2 and j = 1,2.\nThroughout this paper, we will write the node of the line graph $L_{G}$ corresponding to $\\{\\{u,v\\}\\} \\in \\mathcal{E}(G)$ as $l_{\\{\\{u,v\\}\\}}$. Moreover, we denote $\\mathcal{L}_{G} := \\{L_{G} | G \\in \\mathcal{G}\\}$. Now we first propose a colored line graph, which is a coloring version of a line graph (Definition 10). This is defined using the node coloring $\\mathcal{C}$ of a given colored graph $(\\mathcal{G},\\mathcal{C})$.\nDefinition 11 Let $(\\mathcal{G},\\mathcal{C})$ be a graph with node coloring $\\mathcal{C}$. A colored line graph $(\\mathcal{L}_{G}, \\mathcal{C}_{h})$ of $(\\mathcal{G},\\mathcal{C})$ with respect to h is the line graph $L_{G}$ of G with the node coloring $\\mathcal{C}_{h}$ such that for any $l_{\\{\\{u,v\\}\\} } \\in \\mathcal{V}(L_{G}), \\mathcal{C}_{h}(l_{\\{\\{u,v\\}\\}}) = h(\\{\\{\\mathcal{C}(u),\\mathcal{C}(v)\\}\\})$, where $l_{\\{\\{u,v\\}\\}}$ is a node in $L_{G}$ corresponding to $\\{\\{u, v\\}\\} \\in \\mathcal{E}(G)$ and h is a hash map on $\\{\\{\\{\\mathcal{C}(u),\\mathcal{C}(v)\\}\\} \\in \\mathcal{EC}(G) | u, v \\in \\mathcal{V}(G), G\\in \\mathcal{G}\\}.\nIn order to elaborate our algorithm, Line Graph Vietoris-Rips (LGVR) Persistence Diagram, we first construct a map $\\operatorname{t}_{\\phi}$ that transforms $(\\mathcal{G}, \\mathcal{C}) \\in (\\mathcal{G}, \\mathcal{C})$ into a colored line graph $(\\mathcal{L}_{G}, \\mathcal{C}^{\\phi}) \\in (\\mathcal{L}_{G}, \\mathcal{C})$. Recall that $\\mathcal{X}$ denotes a space of node features of $(\\mathcal{G},\\mathcal{C})$ containing (0,...,0), where $N\\in \\mathbb{N}$. Let $\\mathcal{M}_{\\mathcal{X}}(2) = \\{\\{\\{x,y\\}\\} | x,y \\in \\mathcal{X}\\}$, and let m be a multi-layer perceptron with learnable parameters. Now, define a map\n$\\Phi: \\mathcal{M}_{\\mathcal{X}}(2) \\rightarrow \\mathbb{R}^{2N}, \\{\\{x,y\\}\\} \\rightarrow (x + \\eta \\cdot m(x) + y + \\eta \\cdot m(y), |x + \\eta \\cdot m(x) - y - \\eta \\cdot m(y)|)$,\nwhere \u03b7 is a learnable scalar parameter. Then we can define the map $\\operatorname{t}_{\\phi}: (\\mathcal{G}, \\mathcal{C}) \\rightarrow (\\mathcal{L}_{G}, \\mathcal{C}^{\\phi})$ as follows: for a given $(\\mathcal{G}, \\mathcal{C}) \\in (\\mathcal{G}, \\mathcal{C})$, let $l_{\\{\\{u,v\\}\\}}$ be the node in $L_{G}$ corresponding to"}, {"title": "5.2 Construction of Line Graph Vietoris-Rips (LGVR) Persistence Diagram", "content": "In this section, we will elaborate on a neural network-based algorithm, named Line Graph Vietoris-Rips (LGVR) Persistence Diagram, with the same expressivity as TED. Pseudocode for the construction of LGVR is described in Algorithm 1 (See Figure 4 for the overall framework). Here, we briefly explain it.\nThe core of LGVR is to construct an injective edge filtration matrix $A_{\\mathcal{C}}$ that contains distance information between nodes in a colored graph $(\\mathcal{G},\\mathcal{C})$ (See orange box in Figure 4). To do this, we first convert $(\\mathcal{G},\\mathcal{C})$ into a colored line graph $(\\mathcal{L}_{G},\\mathcal{C}^{\\phi})$ through a map $\\operatorname{t}_{\\phi}$. Now that the edge information of G has been converted into the node information of LG, we perform the binary node classification task (actual vs virtual) so that the nodes of $L_{G}$ corresponding to the actual edges of G have a value of 0. However, since all nodes in $L_{G}$ correspond to actual edges in G, they are all trained to have values sufficiently close to 0 during node classification, which can hinder the acquisition of rich information. To address this, we extend $(\\mathcal{L}_{G}, \\mathcal{C}^{\\phi})$ into a colored complete line graph $(L_{K_{G}}, \\mathcal{C}^{\\phi})$, and perform the task on $(L_{K_{G}}, \\mathcal{C}^{\\phi})$. In this way, we extract meaningful edge information by adding virtual edges to G and training to distinguish actual edges from virtual ones (Appendix A). Based on the extracted edge information of G, we construct the matrix $A_{\\mathcal{C}}$. Finally, LGVR(G,C) is defined as the persistence diagram of $\\{\\operatorname{VR}_{1}(\\mathcal{V}(G), A_{\\mathcal{C}})\\}_{\\varepsilon \\in[0,0.5]}$ (See Appendix B for $\\varepsilon \\in [0, 0.5]$). Similar to conventional GNNs, LGVR is trained in an end-to-end fashion with task-specific loss $\\mathcal{L}_{\\text{task}}$ (for example, cross entropy) and the LGVR loss $\\mathcal{L}_{\\text{LGVR}}$ in Algorithm 1.\nWe conclude this section with the complexity analysis of LGVR. Since the complexity of LGVR is dominated by the calculation of persistence homology, we will focus on explaining the computational complexity of dimensions 0 and 1. In short, they can be computed efficiently with a worst-case complexity of $O(m\\alpha(m)))$ for a graph with m sorted edges"}, {"title": "5.3 Theoretical Expressivity of LGVR", "content": "The most crucial point in LGVR is whether the edge filtration in Algorithm 1 can be injective. In this regard, we will first state and prove the most essential lemma that demonstrates the injectivity of the edge filtration used in LGVR as follows."}, {"title": "6 Model Framework", "content": "From Theorem 13, we can infer two important results regarding LGVR: (1) LGVR can preserve arbitrary node coloring information, and (2) especially for WL type coloring, LGVR has stronger expressive powers, which enables the construction of more powerful topological GNNs. In this section, we will delve deeper into the construction of topological GNNs. Before providing a framework for applying LGVR to GNN, we first remark one important aspect of LGVR. From a theoretical perspective, LGVR can guarantee the expressive powers of node colorings due to the injectivity of edge filtration. However, from a practical perspective, if specific node information plays a crucial role in determining the characteristics of a graph due to its rich node coloring information, it is likely that LGVR may not extract a graph representation that properly reflects this since LGVR can only indirectly use the node coloring information by converting it into other (topological) information."}, {"title": "6.1 Integration Technique", "content": "In this subsection", "f_{i}": "X_{i"}, "rightarrow \\mathbb{R}^{M}\\}_{i \\in I}$ so that\n$\\bigcap_{i \\in I} \\{f_{i}(x_{i}) | X_{i} \\in X_{i}\\} = \\emptyset$.\nProof First, we prove the case when |I| = 2. We claim that given any functions $f_{1}: X_{1} \\rightarrow \\mathbb{R}^{m}$ and $f_{2}: X_{2} \\rightarrow \\mathbb{R}^{m}$, there exists $\\varepsilon \\in \\mathbb{R}^{m}$ such that $\\{f_{1}(x_{1}) | x_{1} \\in X_{1}\\} \\cap \\{f_{2}(x_{2}) + \\varepsilon | x_{2} \\in X_{2}\\} = \\emptyset$. Choose any two functions $f_{1}: X_{1} \\rightarrow \\mathbb{R}^{m}$ and $f_{2}: X_{2} \\rightarrow \\mathbb{R}^{m}$, and consider the set\n$\\mathcal{D}_{F} := \\{f_{1}(x_{1}) - f_{2}(x_{2}) | x_{1} \\in X_{1} \\operatorname{and} x_{2} \\in X_{2}\\}$.\nSince $X_{1}$ and $X_{2}$ are countable, so is $\\mathcal{D}_{F}$. Hence there exists infinitely many $\\varepsilon \\in \\mathbb{R}^{m}$ such that $\\varepsilon \\notin \\mathcal{D}_{F}$, which proves the claim. Now, by replacing $f_{2}$ by $f_{2} + \\varepsilon$ for some $\\varepsilon \\notin \\mathcal{D}_{F}$, it is clear that $f_{1}$ and $f_{2}$ satisfy the desired property.\nFor general I with $|I| < \\infty$, the same arguments implies the existence of infinitely many $\\varepsilon_{i} \\in \\mathbb{R}^{M}, i"]}