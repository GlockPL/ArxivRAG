{"title": "Prompting Large Language Models with Rationale Heuristics for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hua", "Peng Yang", "Bing Li", "Fengyuan Liu"], "abstract": "Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question\nAnswering (VQA). Despite the encouraging results of previous studies, prior methods prompt\nLLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior\nmethods do not sufficiently activate the capacities of LLMs. We propose a framework called\nPLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate\nthought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers.\nExperiments show that our approach outperforms the existing baselines by more than 2.2 and\n2.1 on OK-VQA and A-OKVQA, respectively.", "sections": [{"title": "1. Introduction", "content": "Visual Question Answering (VQA) is a popular task in the artificial intelligence. Knowledge-based VQA poses\neven higher demands, requiring not only the understanding of image information but also the introduction of external\nknowledge to answer questions. Recently, open-domain VQA benchmarks [15, 21] have been established, allowing the\nuse of any external knowledge to answer questions. This paper focuses on the VQA with open-domain knowledge. Early\nmethods for knowledge-based VQA involve retrieving information from knowledge bases. However, these methods\nhave limitations, such as the potential inability to retrieve the required knowledge.\nRecently, methods based on Large Language Models (LLMs), such as GPT-3 [2], have achieved encouraging\nresults. PICa [29] applies GPT-3's in-context learning to knowledge-based VQA. They use a captioning model to\nconvert images into captions, thereby textualizing the VQA task. The caption, question, and some in-context examples\nare integrated into a textual prompt to guide GPT-3 in predicting the answer. PICa suffers from limitations such as\ninsufficient prompt information and the choice of in-context examples to be improved. Prophet [22] further explores\nthe research based on PICa, using the vanilla VQA model to inspire GPT-3. Prophet uses a vanilla VQA model to\nselect in-context examples and generate candidate answers. Then, the caption, question, candidate answers, and in-\ncontext examples are integrated into a textual prompt to guide GPT-3 in predicting the answer directly. Existing works\nhave yielded exciting results, but often they have directly prompted LLMs to predict answers, neglecting intermediate\nthought process. Previous studies demonstrate that intermediate thought process can effectively improve the reasoning\nabilities of LLMs.\nIn this paper, we propose PLRH-a novel framework that Prompts LLMs with Rationale Heuristics for knowledge-\nbased VQA. Unlike previous works, instead of predicting the answer directly, we first prompt the LLM with Chain\nof Thought (CoT) to generate the rationale heuristics. Next, in the inference stage, rationale is added to the formatted\nprompt to inspire the LLM to predict the answer. Figure 1 illustrates the comparison of our PLRH with PICa and\nProphet. To our knowledge, this is the first attempt to prompt LLMs with rationale heuristics for knowledge-based\nVQA. Table 1 lists the main notations. The main contributions are as follows:\n1. We propose a novel framework consisting of three stages. Firstly rationales will be generated for all training\nsamples, then rationale heuristics will be obtained for testing samples, and finally the LLM will be prompted to\npredict the answers."}, {"title": "2. Related Work", "content": "Visual Question Answering (VQA).\nInterest in VQA [6] has significantly increased in recent years. Recent\nworks in this domain can generally be classified into several categories: improved visual features [23, 32], enhanced\nmodel architectures [30, 11], and better learning strategies [10, 35]. Many of the state-of-the-art methods adopt the\nTransformer architecture [26]. In addition to studies focused on general VQA, there is a growing interest in exploring\nmore specific VQA tasks that require specialized reasoning skills, such as knowledge integration [18].\nKnowledge-based VQA. Knowledge-based VQA often requires information beyond the image itself to accurately\nanswer questions. Early approaches typically rely on external knowledge resources. With the rise of LLMs [34, 3],\nwhich have demonstrated remarkable capabilities, researchers have begun exploring the use of LLMs in knowledge-\nbased VQA. PICa [29] applies the GPT-3's in-context learning to knowledge-based VQA. They convert images into\ncaptions, integrating input and in-context examples into formatted prompts that inspire the GPT-3 to predict answers.\nPromptCap [8] further improves the captioning method by adding questions to the prompts, making the generated\ncaptions helpful in answering the questions. Prophet [22] inherits PICa and uses a vanilla VQA model to inspire the"}, {"title": "3. Methodology", "content": "Figure 2 shows an overview of our framework. Our framework consists of three stages. In the first stage, we\nmanually craft several in-context examples and generate rationales for all training samples. In the second stage, we\nuse a vanilla VQA model to select in-context examples and generate rationales for the test inputs. In the third stage,\nwe employ the rationale heuristics to prompt the LLM to predict the answers."}, {"title": "3.1. Preliminaries", "content": "LLMs have demonstrated powerful in-context few-shot learning capabilities. Given the input x, target y is predicted\nconditioned on the prompt p(h, e, x). At each decoding step s:\n$y^* = arg max P_{LLM} (y | p(h, \\varepsilon, x), y_{<s}),$\nwhere h denotes the prompt head, $\\varepsilon = \\{(x_i, y_i)\\}_{i=1}^I$ denotes the in-context examples.\nPICa [29] applies in-context learning paradigm of GPT-3 [2] to knowledge-based VQA. To enable LLMs to\nunderstand images, they convert the images into captions using a captioning model. PICa formats the in-context\nexamples as follows:\nContext: c\u1d62 \\n Question: q\u1d62 \\n Answer: a\u1d62\nwhere context refers to the caption of the image, \\n indicates the line break, and (c\u1d62, q\u1d62, a\u1d62) denote the image-\nquestion-answer triplet in the training set. PICa formats the test input as follows:\nContext: c \\n Question: q \\n Answer:\nThe test input format is similar to the in-context examples, except the answer is left blank for the LLM to predict.\nProphet [22] further leverages a vanilla VQA model to inspire GPT-3. By incorporating the candidate answers\ngenerated by the vanilla model into the prompt, they expand PICa's triplet Context-Question-Answer into a quadruplet\nContext-Question-Candidates-Answer. The in-context examples of Prophet are formatted as follows:\nContext: c\u1d62 \\n Question: q\u1d62 \\n Candidates: w\u1d62 \\n Answer: a\u1d62\nwhere w\u1d62 denotes the candidate answers generated by vanilla VQA model.\nUnderstanding PICa and Prophet will facilitate the comprehension of our approach. We introduce the concept of\nchain-of-thought and incorporate rationale heuristics into the prompt to better inspire the LLM."}, {"title": "3.2. Stage 1: Generating rationales for the training samples", "content": "We first generate corresponding rationales for all training samples to ensure they are available for use during\ninference. We manually crafted several in-context examples for prompting the LLM.\nContext: c\u1d62 \\n Question: q\u1d62 \\n Answer: a\u1d62 \\n Rationale: r\u1d62\nwhere c\u1d62 corresponds to the caption of image, c\u1d62, q\u1d62, a\u1d62 correspond to the image-question-answer, and r\u1d62 is the\nrationale manually crafted by us. The input is formatted as follows:\nContext: c \\n Question: q \\n Answer: a \\n Rationale:\nAs shown in Figure 2(a), we prompt the LLM to generate the corresponding rationale based on context, question,\nand answer. This allows the LLM to generate rationales corresponding to the answers.\n$Y_r = LLM(H, E(c_i, q_i, a_i, r_i), X(c, q, a))$\nwhere Y\u1d63 denotes the rationale generated by LLM, H denotes the prompt head, E(c\u1d62, q\u1d62, a\u1d62, r\u1d62) denotes the in-context\nexamples, and X(c, q, a) denotes the input."}, {"title": "3.3. Stage 2: Rationale heuristics generation for the test input", "content": "At this stage, we generate the rationale for the test input. We first select in-context examples from the training\nsamples for the test input and then prompt the LLM to generate the rationale. Define the VQA dataset as D =\n{(v\u1d62, q\u1d62, a\u1d62)} M\u1d62\u208c\u2081 , and the vanilla VQA model M is trained on D."}, {"title": "3.4. Stage 3: Prompting the LLM with rationale heuristics", "content": "At this stage, we have obtained rationales for both the training samples and the test input. We will use the rationale\nheuristics to prompt the LLM to predict the answer. We adopt the same in-context example selection strategy as in the\nprevious stage. The in-context examples can be defined as:\n$E = \\{(c_i, q_i, r_i, a_i) | i \\in I_E \\}$\nWe construct the prompt to inspire the LLM to predict the answer. The prompt format is as follows:\nPrompt head\nContext: c\u1d62 \\n Question: q\u1d62 \\n Rationale: r\u1d62 \\n Answer: a\u1d62\nContext: c \\n Question: q \\n Rationale: r \\n Answer:"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "OK-VQA [15] and A-OKVQA [21] are commonly used knowledge-based VQA datasets. The OK-VQA dataset\ncontains 9K/5K image-question pairs for the training/test sets. Each sample is annotated with ten open-ended answers.\nThe A-OKVQA dataset includes 17K/1K/7K for the training/validation/test sets. Each sample is annotated with ten\nopen-ended answers for direct answer evaluation. We adopt the direct answer evaluation on validation set for A-\nOKVQA. For evaluation metrics, a generated answer is deemed 100% accurate if at least three humans annotated\nthat correct answer. The accuracy metric is: min($"}, {"title": "4.2. Baselines and Implementation", "content": ""}, {"title": "4.2.1. Baselines", "content": "We compare our approach with a range of existing baselines:\n\u2022 Methods with external knowledge resources: MUTAN [1], Mucko [36], ConceptBert [5], KRISP [14], MAVEx\n[28], Visual Retriever-Reader [13], TRiG [4], UnifER [7], GPV-2 [9], VLC-BERT [20].\n\u2022 Methods with other multimodal models: ClipCap [16], ViLBERT [12], LXMERT [24].\n\u2022 Methods with GPT-3/LLMs: PICa [29], PromptCap [8], Prophet [22].\nTo ensure a fair comparison, we replace the GPT-3 in PromptCap and Prophet with LLaMA."}, {"title": "4.2.2. Implementation", "content": "For the vanilla VQA model, we follow Prophet [22] and use the pretrained MCAN-large [31] model. We adopt\nthe PromptCap [8] as the captioning model. For the LLM, we choose LLaMA2-Chat [25]. LLaMA is an excellent,\nopen-source LLM with powerful capabilities and is free of charge. We use the 7B version, which can run on a single\nNVIDIA V100 GPU. Considering the context length limitations, we set the number of in-context examples to 8."}, {"title": "4.3. Main Results", "content": "Tables 2 and 3 demonstrate the results. Our method outperforms the existing baselines by more than 2.2 and 2.1\non OK-VQA and A-OKVQA, respectively. Overall, our approach outperforms both LLM-based approaches and other\nbaselines. Specifically, LLM-based methods tend to outperform other baselines that are not LLM-based. Among all\nthe baselines, PICa, PromptCap, and Prophet are LLM-based methods, and they outperform the other baselines. This\nis because LLMs are pre-trained models with massive parameters and have powerful capabilities. Therefore LLM-\nbased methods tend to have better results. Our approach is also based on LLM and outperforms PICa, PromptCap as\nwell as Prophet to achieve the best results. This is because our approach not only combines PromptCap's captioning\nmethod, and Prophet's in-context example selection, but also employs a novel rationale heuristic that further activate\nthe capability of LLM."}, {"title": "4.4. Ablation Study", "content": "We add our rationale heuristic on separate setups to further validate our approach. Table 4 shows the results.\n\"Prophet & PromptCap\" denotes combining Prophet's in-context example selection with PromptCap's captions.\n\"Prophet\" denotes setup based on Prophet baseline, \"PromptCap\" denotes setup based on PromptCap baseline. \"+\nRationale\" denotes a further addition to the setups with our rationale heuristic. Observation of the results shows that\nour rationale method achieves the better results on both datasets. Our method improves the performance under different\nbaseline setups. This suggests that CoT can further activate the capacity of LLM, so the rationale heuristics can obtain\nbetter results."}, {"title": "4.5. Parameter Sensitivity Study", "content": "We also investigate how performance varies with the number of in-context examples, as shown in Figure 3. The\nsettings for the number of in-context examples include {1,2,4,6,8}. \"Ours\" denotes our method and \"Ours (- Rationale)\"\ndenotes that our method removes the rationale heuristic. Similar trends are observed on both datasets. Overall,\nperformance improves as the number of in-context examples increases. Specifically, performance is worst when the\nnumber of in-context examples is 1. As the number of in-context examples gradually increases, the performance\ngradually improves. When the number of in-context examples reaches 4, the performance improvement gradually\nslows down. When the number of in-context examples reaches 8, the performance reaches the highest."}, {"title": "4.6. Case Study", "content": "Figure 4 illustrates the case study. We show success cases and failure cases, as well as formatted prompts for\nprediction."}, {"title": "4.6.1. Presentation of cases", "content": "Figure 4(a) shows the cases, containing both successes and failures. We show the impact of our rationale heuristics\non the prediction results of the LLM. The three cases on the left are success cases, i.e., case 1, case 2, and case 3. The\nthree cases on the right are failures, i.e., case 4, case 5, and case 6. As can be seen from the case study, the rationale\nheuristic can inspire the LLM to make correct predictions, and in some cases it can also mislead the LLM."}, {"title": "4.6.2. Demonstration of prompts", "content": "Figure 4(b) shows the presentation of the prompt for the prediction. We show the prompt containing two in-\ncontext examples for better visualization. The format of the prompt is Context-Question-Rationale-Answer. The prompt\nconsists of three parts: the prompt head for describing the task, the in-context examples, and the last test input. The\ntest input leaves the answer blank to allow the LLM to predict it."}, {"title": "4.7. More details of prompts", "content": "To better explain our framework, we have also provided the prompts for stage 1 and stage 2. Figure 5 shows more\ndetails of prompts. The prompts consist of three parts, i.e., the prompt head is used to describe the task, the in-context\nexamples, and the input. The format of the prompts in the first stage is Context-Question-Answer-Rationale, which we\nuse to generate rationales for all training samples. The format of the prompts in the second stage is Context-Question-\nRationale, which we use to generate rationales for all test samples. The prompts in the second stage do not have answer,\nthis is because the test samples do not have answer. So in order to have a consistent format for in-context learning, we\nconstruct it this way."}, {"title": "5. Conclusion", "content": "We propose a novel framework called PLRH, which prompts LLMs with rationale heuristics for knowledge-based\nVQA. The framework is divided into three stages: first, the LLM is prompted to generate rationales for all training\nsamples, then the vanilla VQA model is used to obtain in-context examples and the LLM is prompted to generate\nrationale heuristics for the test sample, and finally, the rationale heuristics are integrated into the prompt along with\nthe in-context examples to inspire the LLM to predict the answer. In the future, we will continue to study LLM-based\nmethods for knowledge-based VQA."}]}