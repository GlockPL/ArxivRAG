{"title": "Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification", "authors": ["Yijia Chang", "Hanrui Jiang", "Chao Lin", "Xinyi Huang", "Jian Weng"], "abstract": "The great economic values of deep neural networks (DNNs) urge AI enterprises to protect their intellectual property (IP) for these models. Recently, proof-of-training (PoT) has been proposed as a promising solution to DNN IP protection, through which AI enterprises can utilize the record of DNN training process as their ownership proof. To prevent attackers from forging ownership proof, a secure PoT scheme should be able to distinguish honest training records from those forged by attackers. Although existing PoT schemes provide various distinction criteria, these criteria are based on intuitions or observations. The effectiveness of these criteria lacks clear and comprehensive analysis, resulting in existing schemes initially deemed secure being swiftly compromised by simple ideas. In this paper, we make the first move to identify distinction criteria in the style of formal methods, so that their effectiveness can be explicitly demonstrated. Specifically, we conduct systematic modeling to cover a wide range of attacks and then theoretically analyze the distinctions between honest and forged training records. The analysis results not only induce a universal distinction criterion, but also provide detailed reasoning to demonstrate its effectiveness in defending against attacks covered by our model. Guided by the criterion, we propose a generic PoT construction that can be instantiated into concrete schemes. This construction sheds light on the realization that trajectory matching algorithms, previously employed in data distillation, possess significant advantages in PoT construction. Experimental results demonstrate that our scheme can resist attacks that have compromised existing PoT schemes, which corroborates its superiority in security.", "sections": [{"title": "1 Introduction", "content": "Driven by the ever-increasing economic value of deep neural networks (DNNs), how to protect the intellectual property (IP) of DNN models has been a great concern for artificial intelligence (AI) enterprises [1]. This concern becomes par- ticularly strong since various model stealing (extraction) at- tack methods are proposed [2\u20134]. To address this concern, a common solution is to devise DNN model ownership verifica- tion schemes, by which a verifier (e.g., patent office or court) can distinguish the legal model owner from attackers. These schemes are expected to be secure so that attackers can not cheat the verifier to obtain model ownership.\nRecently, proof-of-training (PoT) [5\u20138] has been proposed as a promising solution for model ownership verification, also known as proof-of-learning or provenance-of-training. Differ- ing from other schemes that verify the features of the target model (e.g., watermarking [9-13] and fingerprinting [14-17]), PoT schemes examine the knowledge of the training record to distinguish the model owner from attackers. Typically, a train- ing record consists of training data, training algorithms, and a training trajectory from the initial model state to the final model state. If a party has a valid training record for a partic- ular model, then it is recognized as the model owner. Note that attackers can steal the model but does not perform its training process. Hence, compared with manipulating model features, attackers face more challenges to forge their training records and may even need to pay much more effort than hon- est training [5]. This greatly reduces the profits of attackers from stealing the model. Therefore, in this paper, we focus on PoT schemes and consider their design for DNN model ownership verification.\nTo ascertain the authenticity of training records as either honest or forged, researchers have proposed multiple PoT schemes. These schemes employ various criteria to examine the distinctions between honest and forged training records. The different types of examined distinctions divide existing PoT schemes into two categories. One is retraining-based cat- egory [5-7] that examines whether the training trajectory can be generated by retraining from training data and algorithms. The other is statistics-based category. Since training data may be private, this category does not examine training data but only assesses whether the statistical metrics of the training trajectory satisfy pre-specified requirements [8]. These works"}, {"title": "2 Technical Overview", "content": "When a party claims its ownership of a DNN model, PoT aims to examine the ownership by verifying whether the party is the real trainer of this DNN model. As illustrated in Figure 1, this verification involves three types of stakeholders: verifier is an honest official institution (e.g., patent office or court) who is responsible for examining model ownership; honest prover performs the training of a DNN model and generates a record of the training process; and malicious prover wishes to illegally claim model ownership after stealing a model from the honest prover.\nTo verify model ownership, the idea behind PoT is that the honest prover does have the record of the training process, while malicious provers do not. Typically, a training record consists of training algorithms as well as their inputs (i.e., training data) and outputs (i.e., a training trajectory from the initial model to the final model). Based on this idea, a secure PoT scheme should distinguish an honest prover from malicious provers according to their training records. By the above description, the central questions in designing a secure PoT scheme include: 1) What are the key distinctions between honest and malicious training records? and 2) How to detect these distinctions?\nFor the first question, the two PoT categories mentioned above [5-8] explore the answers from different perspectives. In the retraining-based PoT schemes [5-7], the training tra- jectory can be retrained from the training algorithms over training data in an honest record but cannot do so in a mali- cious record. In the statistics-based PoT schemes [8], through observation and experimental verification, several statistical characteristics are identified to distinguish between honest and malicious training trajectories.\nAlthough existing PoT schemes are shown to be effective against some particular attacks, they may be insecure against more tricky attacks. For the retraining-based category, exist- ing works have found successful attacks against it [18, 19]. The statistics-based category, proposed recently, remains un- breached yet. Nonetheless, by slightly modifying a known attack, we successfully attack the statistics-based category in a more realistic setting. The details of this attack and its effect are presented in Section 3.3.4.\nBuilding on the above investigations, we apply formal meth- ods to deepen our understanding of attack methods and guide secure PoT scheme design. Specifically, to answer the first question, we take the following three steps.\n\u2022 First, we formalize the training record and the threat model of malicious provers, with a focus on the goal, capabilities, and incapabilities of malicious provers. Par- ticularly, these incapabilities play the role of security assumption in our modeling.\n\u2022 Second, we model those \"non-trivial\" attack methods that are feasible under the capabilities of malicious provers but can not be detected by existing PoT schemes.\n\u2022 Third, we analyze the key distinctions between honest training records and malicious training records forged by those non-trivial attacks.\nThrough these three steps, we provide a thorough classi- fication of potential attacks and obtain some useful analysis results to answer the first question. Particularly, attacks are divided into three categories: 1) weak attacks that can be de- tected by existing PoT schemes, 2) strong attacks that are out of the range of attackers' capabilities, and 3) non-trivial at- tacks. Particularly, we put the focus on non-trivial attacks and analyze the key distinctions between honest and forged train- ing records under each type of non-trivial attacks. One type is algorithm manipulation attacks, under which we find that the training trajectory and training data from the honest train- ing record are more dependent on each other. More formally, we use mutual information \\(I\\) to measure the dependence and prove the following theorem.\nTheorem 1 (Informal) Given two datasets D and D(M) sam- pled from the same distribution, suppose that Tr is trajectory output by honest training algorithms and \\(T_{r}^{TM}\\) is trajectory output by forged training algorithms, then we have\n\\[I(D; TT) > I(D^{(M)}; T^{(M)})\\]\nThe other type is data fabrication attacks, under which the key distinction between training records is the data distribution. The forged training data have a different distribution from the honest training data. With these analysis results, we can design a secure PoT scheme by checking these distinctions.\nNonetheless, even though we have the above guidelines, answering the second question (i.e., detection of these dis- tinctions) still faces two challenges. The first challenge is"}, {"title": "3 Modeling of Attacks against PoT", "content": "In this section, we provide a comprehensive modeling of attacks against PoT. To this end, we formalize the training record in Section 3.1, model the malicious provers in Section 3.2, and characterize the methods of forging training records in Section 3.3.\n3.1 Formalization of Training Record\nTo formalize the record of training process, we review the training process and study those contents that can be saved as records. On a high level, DNN training usually starts from an initial model and produces a final model via gradient descent algorithms. The algorithm for model initialization, denoted by \\(I_A\\), is either to sample a random value from a particular distribution or to choose an existing model that has shown good accuracy. After the model is initialized, DNN training will update models in a number of epochs following the gradi- ent descent algorithm. During the multiple epochs of training process, it is common practice to save model states at some checkpoints. We call the sequence of these saved model states training trajectory, which is formally defined as follows.\nDefinition 1 Training trajectory TT is a sequence of n model states {M1,...,Mn}, where M\u2081 is the initial model and Mi is the saved model state in the i-th checkpoint for 2 \u2264 i \u2264 n. Particularly, the final model state Mn is called the tail of training trajectory, and the value of n is called the length of training trajectory.\nTo update models from one checkpoint to the next check- point, DNN training needs to compute a model update by"}, {"title": "3.2 Modeling of Malicious Provers", "content": "3.2.1 Attack Goal\nThe goal of malicious provers is to forge a training record on some target model so that the verifier can not distinguish the honest training record from the forged training record. Formally, let \\(T_{R,n}^{T(H)}\\) (resp. \\(T_{R,n}^{T(M)}\\)) denote the training record from the honest (resp. malicious) prover. The malicious prover succeeds if the verifier approves \\(T_{R,n}^{T(M)}\\) rather than \\(T_{R,n}^{T(H)}\\).\nHowever, if the malicious prover is willing to invest unlim- ited efforts in the attack, there exist trivial attack methods that are guaranteed to succeed. The malicious prover simply needs"}, {"title": "3.3 Modeling of Attack Methods", "content": "3.3.1 Taxonomy of Potential Attacks\nNext, we model the attack methods for forging training records. Our modeling aims to focus on those non-trivial attack methods that 1) can not be easily detected by existing defense methods and 2) are feasible within the capabilities of attackers. To this end, we first exclude two types of \"weak\u201d attacks in the subsequent modeling of non-trivial attacks.\nThe first type of weak attacks outputs a forged training record with incorrect structure. The correct structure of a length-n training record should consist of a length-n trajec- tory, an initialization algorithm, a sequence of n - 1 training algorithms and/or datasets. Each training algorithm should be in the form of an executable program. Its input is the preced- ing model and a training dataset, and its output is the model update. The violation of these requirements will be marked as \"incorrect structure\" and excluded from verification.\nThe second type of weak attacks forges a dishonest ini- tialized model M\u2081. To detect this type of attacks, we restrict the honestly initialized model to two possible cases. In one case, the initialization algorithm samples M\u2081 from a random distribution and a forged M\u2081 can be detected by statistical tests. In the other case, the initialization algorithm adopts an existing well-trained model as the initialized model. To detect a forged M\u2081 in this case, a common practice is to check whether this well-trained model has an ownership proof [5]. If the initialization algorithm does not fall into these two cases"}, {"title": "3.3.2 Forward Direction Attacks", "content": "As illustrated in Figure 3(b), the forward direction attacks consist of two sequential steps. The first step is to determine the sequence of training algorithms \\(T_A^{(M)}\\) and data \\(D^{(M)}\\) in the training record. The second step is to generate the train- ing trajectory \\(T_T^{(M)}\\) by running the training algorithms over training data. Hence, the effort of the malicious prover is the cost (e.g., running time) of the initialization algorithm \\(I_A^{(M)}\\) and the training algorithms \\(T_A^{(M)}\\). Recall that the goal of the malicious prover is to make its effort less than that of the honest prover. Since the initialization algorithm is assumed to be honest, the common idea of forward direction attacks is to manipulate \\(T_A^{(M)}\\) so that \\(Cost(T_A^{(M)},{T^{(M)},D^{(M)}})\\) < \\(Cost(T_A^{(H)},{T^{(H)},D^{(H)}})\\). On the contrary, the malicious prover has few motivations to fabricate the training data, be- cause there is a higher risk of being detected but no benefit in doing so. Therefore, along the forward direction, we focus on algorithm manipulation attacks.\nBy Assumption 1, the malicious prover cannot find ma- nipulated algorithms with less cost unless the forged training algorithms utilize the target model for model updating. Hence, the forward direction algorithm manipulation attacks mainly consider how to integrate the target model into training algo- rithms to reduce the cost. We provide an example below.\nAttack 1 (forward direction) [24]: Given a target model M, the malicious prover honestly samples an initialized model M\u2081 and training data. Then it generates a training trajectory \\(T_T^{(M)} = {M_1,...,M_m}\\). In particular, the malicious prover generates Mi from Mi\u22121 (2 < i < m) by minimizing the following loss function:\n\\[L' = L(D_i) + \\alpha\\cdot d(M_i,M),\\]\nwhere L() is an honest loss function for DNN training (e.g., cross-entropy loss or mean square error loss) over dataset Di, d(Mi, M) is the distance between M\u2081 and M, and \u03b1 is a weight coefficient of loss function.\nThrough this algorithm design, the models will converge to M more quickly with a larger value of \u03b1 and the honest training algorithm is in fact a special case with \u03b1 0. Let E and t (resp. E' and t') denote the number of epochs and the running time of each epoch for honest (resp. forged) training algorithms. By setting a large value of \u03b1, E' can be far less than E. Although the running time of each epoch is slightly higher (i.e., t' > t) due to the extra term d(Mi,M) in the loss function, the total cost of forged algorithms E't' can be lower than the total cost of honest algorithms Et, as long as the value of \u03b1 is large enough.\nWe note that existing PoT scheme [5] treats training al- gorithms that utilize the target model M as structure incor- rect algorithms and thus excludes them from consideration. Nonetheless, since we model each algorithm as an executable program (see Section 3.1), it is hard to detect its internal mech- anism and realize its abnormality. Although the verifier can also ask the prover to submit training algorithms in the form of pseudo-code, then the verification of training algorithms requires to implement all of these algorithms by programming. Due to the reproducibility hardness of training algorithms, this way would put too much burden on the verifier and may be infeasible in practice. Therefore, this paper assumes the algorithms are submitted in the form of executable programs and takes Attack 1 into consideration.\nNext, we model the key feature of (forward direction) al- gorithm manipulation attacks. Since the manipulated training algorithms must use the target model for model updating, the"}, {"title": "3.3.3 Reverse Direction Attacks", "content": "As illustrated in Figure 3(c), the reverse direction attacks consist of the same two sequential steps as forward direction attacks but in a different order. Specifically, the first step is to forge the training trajectory \\(T_T^{(M)}\\), while the second step is to forge the sequence of training algorithms \\(T_A^{(M)}\\) and data \\(D^{(M)}\\). We use fr and f\u0104 to denote two forging algorithms in these two steps, respectively. Notably, the trajectory is generated by fr rather than the training algorithms claimed in training record, i.e., fr \u2260 \\(T_A^{(M)}\\), which is a key difference between forward and reverse direction attacks. Next, we discuss two types of reverse attack methods that take training algorithms and training data as forgery objects, respectively.\nReverse Algorithm Manipulation Attacks. This type of at- tacks manipulate training algorithms \\(T_A^{(M)}\\) rather than forging"}, {"title": "3.3.4 A Successful Attack against Statistics-Based PoT", "content": "Next, we show a successful attack against the existing statistics-based PoT scheme [24] in a more realistic setting. On one hand, the existing scheme assumes that the malicious prover only has a small dataset and is not aware of the initial model of honest training trajectory, which is different from our threat model (see Section 3.2). On the other hand, the ex- isting scheme assumes that the prover will save model states in every epoch, which may result in excessive storage cost. Hence, in practice, the model owner usually save model states in some checkpoints, and there are usually several epochs between these two checkpoints. We set the number of epochs between two checkpoints to be 5 in the following attack.\nIn the new setting, we find that the existing scheme [24] may fail to distinguish honest training records from forged ones. Specifically, existing scheme examines six properties of training trajectory (labeled from 1 to 6) to distinguish be- tween honest and forged training records. We perform honest training and Attack 4 to output an honest trajectory and a forged trajectory upon CIFAR10 dataset and ResNet18 model architecture. Then we run the existing scheme to examine properties 1-6 on these two trajectories. The results demon- strate that the honest trajectory and the forged trajectory show the same value on every properties except property 2. The property 2 measures the maximum distance between two model states in successive checkpoints. They claim that the honest training record should have a smaller value on this metric. However, in our experimental results, the metric of the honest training record (0.021) is greater than that of mali- cious training record (0.019). Therefore, the existing scheme fail to distinguish between the honest prover and malicious provers in this case."}, {"title": "4 Criterion for Attack Detection", "content": "Based on the above modeling, we identify two types of attack methods (namely algorithm manipulation and data fabrication"}, {"title": "5 Our Generic PoT Construction", "content": "In this section, we show how the universal criterion can guide us to design secure PoT schemes. Before presenting our PoT construction, we first describe the basic settings on verifier capability that follows the existing PoT scheme [24]. On one hand, we assume that the verifier is honest, i.e., it will faithfully execute the verifying algorithm. Still, considering existing laws and regulations about data privacy, our POT scheme will not send training data to the verifier. On the other hand, we assume that the verifier has a test dataset Dtest with a similar distribution as the training data. In practice, the test dataset can be obtained from public dataset or be selected from those data on which the target model has high confidence to output correct inference results.\nNext, we present our PoT construction under the above setting. Generally speaking, a PoT scheme is a pair (P, V), where P is a proving algorithm that takes a training record as input and outputs a proof, and V is a verifying algorithm that takes multiple proofs as input and selects a proof as the one from honest provers. In our PoT construction, the prov- ing algorithm is to simply put training record except training data (i.e., {IA, TA, TT }) into the proof. For the verifying algo- rithm, based on the modeling of attack methods, the verifying algorithm needs to check the following items.\n1. Is the proof structurally correct?\n2. Is the model initialization honest?\n3. Does the prover conduct algorithm manipulation or data fabrication attacks?\nAmong these items, the former two items can be checked by following existing PoT schemes [5,6], while the third item can be examined based on the aforementioned universal criterion. Correspondingly, we decompose the verifying algorithm into three stages. Stage 1 is used to check the former two items, while Stages 2 and 3 are performed to detect attacks according to the criterion. More specifically, Stage 2 synthesizes the"}, {"title": "6 Experiment Evaluation", "content": "In this section, we implement a prototype of our PoT scheme design and evaluate its security through experiments. The experiment setup is described in Section 6.1 and the results are shown in Section 6.2.\n6.1 Experiment Setup\nDatasets and Models. To observe the impacts of datasets and model architectures on the security of our PoT scheme, we evaluate our PoT scheme across two classical CV datasets (i.e., CIFAR10 with ten classes of images and CIFAR100 with one hundred classes of images) and three model architectures (i.e., a three-layer CNN model, a ResNet18 model, and a ResNet34 model). The training datasets and test datasets are used by provers and the verifier, respectively.\nImplementation of Attacks. After training these models fol- lowing the honest way, we simulate Attacks 1-4 as described in Section 3.3. For Attacks 1, 3, and 4, they have tunable parameters denoted by \u03b1 or \u03b1i in their descriptions, which will affect the length of output trajectory. Specifically, for Attacks 1 and 4, the larger the value of \u03b1, the more quickly the model converges and thus the shorter the trajectory. In contrast, for Attack 3, the larger value of \u03b1i implies more intermediate model states in the trajectory and hence a longer trajectory. To observe the impact of trajectory length, we forge two training records for each of Attacks 1, 3, and 4 with dif- ferent values of \u03b1 or \u03b1i. One record has a shorter trajectory and the other has a longer trajectory. In total, we forge seven training records by executing Attacks 1-4.\nImplementation of PoT Scheme. We implement our PoT scheme with Python. The stage of data distillation is imple- mented based on existing trajectory matching algorithm [22]. To guarantee the convergence of synthetic data, this stage executes 300 iterations so that the synthetic data are barely updated. In addition, all related parameters follow the default setting. Particularly, a critical parameter is the synthetic data size (SDS), i.e., the number of images in the synthetic data. For CIFAR10 (resp. CIFAR100) dataset, the value of SDS is set to be 10, 100, or 500 (resp. 100, 500, or 1000). For the data evaluation stage, we train three DNNs (i.e., t = 3) using the synthetic data and observe their average accuracy."}, {"title": "6.2 Experiment Results", "content": "6.2.1 Overall Results\nWe depict the evaluation results upon various model architec- tures and datasets in Figure 5. In each sub-figure, we show"}, {"title": "7 Related Work", "content": "In this section, we review existing PoT schemes, which can be divided into the following two categories.\nIn retraining-based PoT schemes, if the training trajectory can be generated by retraining with the same algorithm and data, the training record passes test and the ownership claim is approved. The most classical PoT scheme [5] belongs to this type. However, in this scheme, the model owner has to send the training data to the verifier. Considering that training data often contain sensitive information, this scheme can not be applied when data privacy is a key concern [24]. To solve this problem, some recent works propose to adopt cryptographic primitives (e.g., zero-knowledge proofs) to conduct the re- training test without leaking the training data [6, 7]. Nonethe- less, the adoption of cryptographic primitives leads to high communication and computation complexities with respect to the number of model parameters and the size of training data. As a result, these schemes can be used only in simple models (e.g., linear regression model) or small datasets.\nIn statistics-based PoT schemes, the verifier only collects training trajectory (without training data) and checks its co- herence by analyzing several statistics metrics [24]. If these metrics all fall within the pre-specified range, the ownership claim is approved. Since the statistics-based category does not need to disclose training data to the verifier, it is thought to have a natural superiority on preserving data privacy. However, regardless of the category of schemes, existing PoT schemes fail to defend against more tricky attacks. For example, existing works develop adaptive attacks against the retraining-based PoT schemes [18, 19]. These ever-emerging attacks indicate an insufficient considerations of possible at- tacks. Particularly, even though existing works consider sev- eral attacks and discuss whether their PoT schemes can de- fend against these attacks, they fail to exclude all reasonable attacks from success. In this work, we provide a comprehen- sive modeling of attack methods and analyze their common features. Such modeling and analysis enable a goal-oriented PoT design by revealing the range of attacks that PoT should defend against."}, {"title": "8 Conclusion and Discussion", "content": "In this paper, we adopt the formal methods in the area of PoT security to devise PoT schemes with stronger security. Unlike existing works relying on intuitions or observations, we con- duct theoretical modeling and analysis to identify distinctions between honest and forged training records. Following this way, we discover a universal criterion for attack detection and further propose a generic PoT construction. The empirical experiments validate the security of our PoT scheme against various attacks. We expect this work can establish a good foundation towards understanding and enhancing the security of PoT schemes, but future work is still required. Specifi- cally, this work may leave the following problems that require further investigation:\n\u2022 Implementation with strict security proof. Although our universal criterion provides an idea to detect all attacks covered by our model, whether the verifier will choose the correct honest prover relies on elaborate implemen- tation, such as suitable parameter setting. Considering that the wrong choice in the real world may incur seri- ous legal consequences, to guarantee the correctness of verifier's output, it may require to provide a strict proof of the implementation's security via formal verification.\n\u2022 Application to real-world large language models (LLMs). Recall that our PoT construction relies on a trajectory matching algorithm to distill synthetic data from the training record. Although our PoT does not limit the range of target AI models, applying it to LLMs such as ChatGPT [28] may pose additional challenges due to LLMs' unique features. One is that LLMs perform natu- ral language processing (NLP) tasks, whereas most of existing trajectory matching algorithms are designed for computer vision (CV) tasks. Still, recent work [29] has identified a candidate for NLP tasks that can help tackle this challenge. The other is that LLMs have an extremely large number of parameters, potentially leading to a high computing workload for verifiers. Existing works ex- plore lightweight trajectory matching algorithms [30], which can help to tackle this challenge."}, {"title": "A Trajectory Matching Algorithm", "content": "Our PoT construction relies on a trajectory matching algo- rithm to generate the synthetic data S from prover's trajectory and training algorithms. Although the details of different tra- jectory matching algorithms may vary, all of these algorithms follow the same paradigm. Specifically, this paradigm con- sists of T iterations. In each iteration, the verifier samples a fragment with length K from prover's trajectory, say, from \\(M_i\\) to \\(M_{i+k}\\). Then a synthetic model M' is updated from Mi using the same training algorithms in \\(T_A\\) but different training data (i.e., using synthetic data S instead of prover's training data). After the ending synthetic model M'k is generated, the verifier computes some loss function between it and the correspond- ing model in prover's trajectory \\(M_{i+k}\\). By minimizing such"}, {"title": "B Proof of Theorem 1", "content": "Theorem 1 (Restated) Suppose that the malicious prover conducts algorithm manipulation attacks with a lower cost than honest training. Let (IA,TA,TT,D) and (IA,T(M), T(M), D(M)) denote the training records of the honest prover and the malicious prover, respectively. When Assumptions 1-2 hold, we have\n\\[I(D;T_{T,[i:i+k]}) > I(D^{(M)}; T^{(M)}_{T,[i:i+k]}),\\]\nwhere TT,[i:i+k] and TM)\nT,[i:i+k] are two length-k fragments from TT and Tr TM), respectively.\nProof: We conduct this proof by 1) computing the mutual information I(TT,[i:i+k]; D) and 2) comparing the values of mutual information between honest prover and manipulated prover. Before conducting the proof, we provide several basic knowledge on information theory that will be used in the subsequent proof. The information theory has established that for random variables x, y, z, the following equations hold [25]:\n\\[I(x;y) = H(x) \u2013 H(x|y),\\]\n\\[I({x_1,x_2,...,x_n};y) = \\sum_{i=1}^n I(x_i;y | {x_1,x_2,..., x_{i-1}}).\\]\nParticularly, if z is independent with x conditioning on y (i.e., I(x,z | y) = 0), then we have:\n\\[H(x | y,z) = H(x | y).\\]\n1) Mutual Information Computation. By Equations (11) and (12), we have the following equations for training traj\u0435\u0441- tory Tr and datasets D.\n\\[I(T_{T,[i:i+k]}; D) =I({M_i,..., M_{i+k}};D)\\]\\[= \\sum_{j=i}^{i+k-1}I(M_j;D | {M_i,...,M_{j-1}})\\]\n\\[= \\sum_{j=i}^{i+k-1}H(M_j |{M_1,...,M_{j-1}}) -H(M_j | {D,M_1,...,M_{j-1}}).\\]"}]}