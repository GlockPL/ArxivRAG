{"title": "Advancing Agentic Systems: Dynamic Task\nDecomposition, Tool Integration and Evaluation using\nNovel Metrics and Dataset", "authors": ["Adrian Garrett Gabriel", "Alaa Alameer Ahmad", "Shankar Kumar Jeyakumar"], "abstract": "The rapid advancements in Large Language Models (LLMs) and their enhanced\nreasoning capabilities are opening new avenues for dynamic, context-aware task\ndecomposition, and automated tool selection. These developments lay the ground-\nwork for sophisticated autonomous agentic systems powered by LLMs, which\nhold significant potential for process automation across various industries. These\nsystems demonstrate remarkable abilities in performing complex tasks, interact-\ning with external systems to augment LLMs' knowledge, and executing actions\nautonomously. To address the challenges and harness the opportunities presented\nby these advances, this paper makes three key contributions.\n\u2022 We propose an advanced agentic framework designed to autonomously pro-\ncess multi-hop user queries by dynamically generating and executing task\ngraphs, selecting appropriate tools, and adapting to real-time changes in task\nrequirements or tool availability.\n\u2022 We introduce novel evaluation metrics tailored for assessing agentic frame-\nworks across diverse domains and tasks, namely Node F1 Score, Structural\nSimilarity Index, and Tool F1 Score.\n\u2022 We develop a specialized dataset based on the AsyncHow dataset to enable\nin-depth analysis of agentic behavior across varying task complexities.\nOur findings demonstrate that asynchronous and dynamic task graph decomposi-\ntion significantly improves system responsiveness and scalability, particularly in\nhandling complex, multi-step tasks. Through detailed analysis, we observe that\nstructural and node-level metrics are more critical in sequential tasks, whereas\ntool-related metrics dominate in parallel tasks. In particular, the Structural Sim-\nilarity Index (SSI) emerged as the most significant predictor of performance in\nsequential tasks, while Tool F1 Score proved essential in parallel tasks. These\nfindings highlight the need for balanced evaluation methods that capture both\nstructural and operational aspects of agentic systems. Our specialized dataset\nenables comprehensive evaluation of these behaviors, providing valuable insights\ninto improving overall system performance, with the importance of both structural\nand tool-related metrics validated through empirical analysis and statistical testing.\nThe evaluation of agentic systems presents unique challenges due to the intricate\nrelationships between task execution, tool usage, and goal achievement. Our\nevaluation framework, validated through empirical analysis, offers valuable in-\nsights for improving the adaptability and reliability of agentic systems in dynamic\nenvironments.", "sections": [{"title": "1 Introduction", "content": "Recent advances in LLMs have catalyzed the development of sophisticated agentic systems capable\nof automating multistep tasks, interacting with external systems, and adapting to changing contexts\n[1, 25]. These systems are promising in industries requiring autonomous workflow processing and\ntool integration. Despite their potential, LLM-based systems face limitations in industrial settings due\nto lack of training on proprietary data and the challenges of fine-tuning. Fine-tuning LLMs for each\nbusiness use case requires costly, labor-intensive dataset collection and processing. In such contexts,\nLLMs are limited in their ability to manage real-time decision making in dynamic environments.\nA scalable alternative to fine-tuning LLMs for each use case is the development of agentic systems\nthat dynamically integrate external tools. Augmenting LLMs with tools allows these systems to\nhandle complex queries and adapt without constant retraining. Agentic frameworks enable LLMs\nto decompose tasks into smaller sub-tasks, whose significance is highlighted in [16], select the\nappropriate tools for each task, and adjust to real-time changes in tool availability.\nOne of the first frameworks that allowed LLMs to interact with external tools is LangChain Project,\nwhich paved the way for more sophisticated agentic platforms such as BabyAGI Project and AutoGen\n[29]. These systems represent important steps toward the realization of autonomous AI agents, but\nare often constrained by high latency, limited adaptability, and insufficient support for dynamic tool\nintegration. Moreover, current systems lack comprehensive evaluation methods that fully capture the\ncomplexity of task graph generation and tool selection, limiting their scalability and reliability in\nindustrial applications.\nTo address these challenges, our work presents a framework\u00b9 that advances the capabilities of\ntraditional agentic systems. Our framework integrates real-time tool selection, dynamic task graph\ngeneration, and an evaluation mechanism to assess agentic behavior across diverse tasks and domains.\nThe proposed architecture consists of five core components: the Orchestrator that generates task\ngraphs based on user queries, the Delegator that manages task distribution, ensuring seamless\ncommunication between tasks, the agents that autonomously execute tasks using LLMs, the tools\nthat provide predefined functions necessary for task completion, and the Executor that handles the\nexecution sequence, optimizing for both parallel and sequential task execution.\nA significant aspect of this work is the development of a comprehensive evaluation framework\u00b2.\nExisting agentic systems often lack domain-specific metrics to rigorously assess their performance in\nhandling task decomposition and tool integration. To fill this gap, we propose these novel metrics:\nNode and Tool F1 Scores: These metrics assess the system's precision and recall in matching\ntask nodes to the expected task graph, ensuring accurate task decomposition, and in selecting the\nappropriate tools for each task within the graph.\nStructural Similarity Index (SSI): A metric that assesses the overall fidelity of the task graph\ngenerated by the system compared to the expected graph, capturing both node and edge similarities\nto ensure the system preserves the logical structure of tasks.\nAdditionally, we introduce a specialized dataset\u00b3 designed to evaluate agentic systems, which allows\na detailed analysis of the interdependencies between task decomposition, tool selection, and system\nperformance, providing a foundation for evaluating agentic.\nIn summary, this paper makes the following contributions:\n\u2022 An agentic framework for dynamic task decomposition, tool integration, and autonomous\ntask execution, designed for complex, multi-hop queries.\n\u2022 Novel evaluation metrics, including the Node F1 score, the Structural Similarity Index, and\nthe Tool F1 score, for detailed task-specific assessment of agentic systems.\n\u2022 A specialized dataset to evaluate agentic behavior across, supporting analysis of task graph\ngeneration, tool selection, and system performance."}, {"title": "2 Related Work", "content": "The remainder of this paper is structured as follows - Section 2 discusses related work in agentic\nsystems, task graph generation, and evaluation of agentic systems. Section 3 provides a detailed\nexplanation of the architecture of our proposed framework. Section 4 presents our evaluation metrics\nand dataset, followed by empirical results in Section 5. Finally, Section 6 concludes with future\ndirections and implications for agentic systems in real-world industrial settings."}, {"title": "2.1 Agentic Frameworks for Task Graph Generation and Tool Selection", "content": "The field of AI agent systems powered by LLMs has seen substantial development, with various\nframeworks proposed to enhance collaboration, task automation, and system scalability. Recent\nsurveys by Wang et al., Guo et al., Masterman et al., and Xi et al. have explored these advancements\nin detail, highlighting the evolving landscape of LLM-powered AI agents [26, 11, 21, 31, 8]. In [5],\nCrawford et al. thoroughly explored a flexible agent framework with a focus on the planning and\nexecution components of an Al agent. [16] particularly explores the importance of planning and task\ngraph generation.\nExisting frameworks, such as LangGraph, AutoGen, and BabyAGI, provide various approaches\nto task generation, tool selection, and handling multi-hop user queries [15, 3, 4]. For instance,\nLangGraph enables stateful workflows with tasks managed cyclically or sequentially, but it does not\ndynamically adjust the task graph during execution as our proposed framework does [6]. AutoGen\nand BabyAGI offer dynamic task generation but are limited by predefined toolsets and scalability\nissues, respectively [30, 23]. Our framework improves upon these methods by introducing real-\ntime adaptability with a Task-Aware Semantic Tool Filtering mechanism, allowing for on-the-fly\nintegration of new tools and dynamic task graph adjustments to handle increased task complexity\n[13]. This capability ensures continuous and efficient task execution, enhancing existing approaches.\nDespite these advancements, several limitations and shortcomings exist in current frameworks:\nLack of testability: Existing frameworks often do not support rigorous unit testing of each component,\nmaking it challenging to ensure reliability and correctness in complex tasks [23]. Our framework\narchitecture is highly modular, enabling each agent to be tested individually, addressing this gap.\nHigh latency: The absence of efficient task parallelization mechanisms leads to increased latency,\nhindering real-time performance [12, 32]. Our system decomposes user queries into sub-tasks and\nexecutes them in parallel wherever possible, reducing latency.\nLimited customizability: Many frameworks offer limited flexibility for customization, restricting\ntheir applicability across diverse domains and specific use cases [23, 30]. Our customizable design\nensures applicability across various domains."}, {"title": "2.2 Evaluation Metrics and Datasets for Agentic Frameworks", "content": "Despite advancements in agentic frameworks, there remains a notable gap in comprehensive eval-\nuation metrics and datasets that accurately assess the performance of these systems across diverse\ntasks and domains. Existing benchmarks, such as AgentBench and VisualAgentBench, focus on LLM\nperformance across diverse environments and visual task automation but fall short in evaluating task\ngraph structures in depth [19, 20]. Similarly, TASKBENCH introduces \"Tool Graphs\" to measure\ntask automation processes but provides limited analysis of intermediate steps [24], while AgentQuest\nemphasizes multi-step reasoning with little focus on task graph fidelity [10]. Although VillagerAgent\nexplores multi-agent task dependencies in simulated environments, its focus is more on coordination\nthan on detailed task decomposition [7].\nOur proposed evaluation framework fills these gaps by introducing detailed metrics\u2014such as Node F1\nScore, Structural Similarity Index, and Tool F1 Score-paired with specialized datasets to assess task\ndecomposition, tool selection, and execution through task graph metrics. These metrics and datasets\noffer a granular analysis of agent performance, addressing the complexity of multi-step reasoning,\ntask automation, and the interdependencies between evaluation metrics. This approach addresses\ngaps in current benchmarks by focusing on intermediate steps and structural fidelity, offering a more\nnuanced assessment applicable to real-world scenarios."}, {"title": "3 Agentic Framework Architecture", "content": "The framework consists of the following major components, as illustrated in Figure 1. These\ncomponents include the Orchestrator, the Delegator, Tools, Agents, and the Executor. The diagram\noutlines the flow of a user query through these components, showing how each element interacts to\nproduce a final response.\nThe Orchestrator as shown in Figure 1 analyzes the user's input to produce a Directed Acyclic\nGraph (DAG) with task nodes and dependency edges. Asynchronous task graph decomposition, as\nexplored by recent work on graph-enhanced LLMs, allows parallel task execution, enabling real-time\nadaptation to task changes and dependencies. This reduces execution time by shortening critical\npaths, which is crucial for handling complex, dynamic queries [16]. Hence, borrowing from this\nwork and Graph Theory, the Orchestrator can be instructed to produce a task graph that is optimized\nfor one or more of the following concepts:\n\u2022 Coarse Grained Task Decomposition: A strategy in which the user query is decomposed\ninto a relatively small number of large tasks, each representing a substantial amount of\nwork. This approach minimizes the overhead associated with task management, such as\nscheduling, synchronization, and inter-task communication, by focusing on larger, more\nindependent units of work [9], [22]. While this can reduce the potential for parallelism, it\nsimplifies execution and can be more efficient in scenarios where the overhead of managing\nnumerous small tasks is prohibitive.\n\u2022 Fine Grained Task Decomposition: A strategy where a user query is broken down into\na large number of small, granular tasks. Each task represents a minimal unit of work,\nallowing for a high degree of parallelism as many tasks can be executed simultaneously.\nHowever, this approach often requires more sophisticated management of task dependencies,\ncommunication, and synchronization, which can introduce significant overhead [9], [22].\n\u2022 Critical Path Optimization: A strategy that focuses on identifying and shortening the\ncritical path-the longest sequence of dependent tasks that determines the minimum time\nrequired to obtain a complete response to a User Query. By optimizing tasks in this path,\nthe overall latency can be reduced [14].\nAs depicted in Figure 1, the Delegator receives the task graph from the Orchestrator and is responsible\nfor assigning tasks to the appropriate agents or tools. It plays a critical role in managing intra-task\nand inter-task communication by utilizing inter-task memory buffers, ensuring that each task has the\nnecessary context and data from its dependent predecessors."}, {"title": "4 Evaluation Framework", "content": "Evaluating agentic systems necessitates a multidimensional analysis of all the individual components\nas well as the final output. Our approach focuses on evaluating intermediate steps as well as final\noutcomes, using a robust set of metrics designed to assess the system's ability to decompose tasks,\nselect appropriate tools, and execute tasks effectively. This evaluation framework is grounded in\nexisting literature, including the gaps identified by Gioacchini et al.[10], Shen et al.[24], and Liu\net al.[19], which highlight the need for a more granular evaluation of agentic behaviors in LLMs.\nTo address these gaps, we developed a comprehensive evaluation dataset inspired by Lin et al.[17],\nintegrating task graphs, tool usage and final outputs. This dataset supports a more nuanced analysis\nof agentic systems by capturing both intermediate processes and final results, ensuring a holistic\nevaluation of system performance."}, {"title": "4.1 Dataset Creation", "content": "Our dataset is specifically designed to evaluate the agentic behavior of LLM-driven systems across\nvarious domains and tasks. The dataset creation process was structured to ensure representativeness,\nand relevance to real-world scenarios based on the AsyncHow [17] dataset. The AsyncHow dataset\nwas particularly suited for this study due to its unique structure that covers parallel, and sequential\ntask graphs. This comprehensive coverage allows for a thorough evaluation of agentic systems, which\nneed to handle different types of task relationships and dependencies. The dataset's design, which\nincludes a mix of simple linear workflows and more intricate interdependent tasks, supports the\ncomprehensive evaluation of the system's ability to manage these varying complexities effectively.\nThe AsyncHow dataset's validation for each scenario within the parallel and sequential categories of\ntask graphs ensures that it is a robust foundation for evaluating the proposed agentic systems. As this\nis one of the most important steps in an agentic system we found it ideal to use it as a foundational\ndataset. Below is a detailed breakdown of the steps involved.\nTask Graph Construction: We randomly sampled 50 task graphs from the AsyncHow dataset,\nensuring diversity while maintaining empirical manageability for the evaluation of agentic systems.\nBy selecting 50 scenarios for the parallel and sequential task graph category, the study ensured that\nthe dataset remains robust yet feasible for in-depth analysis. The empirical soundness of selecting\n50 scenarios, resulting in more than 250 tools, was determined to be sufficient for evaluating the\ncorrectness of tool selection and the overall performance of the agentic systems.\nTool Function Generation: Tool descriptions were parsed and translated into synthetic Python\nfunctions. These functions were designed to replicate the behavior of real-world tools, ensuring that\nthe agent's interactions with these tools are realistic and contextually appropriate. These functions\ncontribute to creating a realistic and challenging environment for the agentic system as they span\na variety of tasks, such as simulating the return of data as JSON from API calls or the data from a\ncandidate for a potential job interview. The functions were then executed to generate final responses\nfor each scenario, providing a benchmark for evaluating the system's performance.\nFinal Dataset Composition: The final dataset includes a comprehensive set of components: sce-\nnario names, task graphs, tool functions, expected tool call sequences, gold standard responses,\nand complexity categories for each scenario. This structure facilitates a detailed evaluation of\nboth intermediate steps and final outcomes, enabling a more nuanced assessment of the agent's\nperformance.\nWith this data, we can evaluate each component of our framework:\n\u2022 Task graph composition: We can compare the task graph from the evaluation dataset to the\ntask graph of our agentic system and validate it according to several similarity metrics (4.2).\n\u2022 Tool selection: We can match the list of expected tools selected against the list of tools our\nagentic system chose.\n\u2022 Answer generation: We can judge the gold standard answer against the answer the agentic\nsystem created."}, {"title": "4.2 Evaluation Metrics", "content": "To rigorously evaluate the agentic system's performance, we employ a set of metrics that measure the\naccuracy and effectiveness of task decomposition, tool selection, and task execution. These metrics\nprovide a comprehensive assessment of the system's capabilities, ensuring a thorough evaluation of\ntask graphs, tool identification, and answer generation accuracy.\nPrecision, Recall, and F1 Score are core metrics used to evaluate the accuracy and completeness\nof tool identification. The precision score reflects the system's ability to correctly identify relevant\ntools without including false positives, while recall measures its success in identifying all relevant\nelements. The F1 Score balances precision and recall, providing a comprehensive view of the system's\nperformance in identifying tools.\nPrecisiontool = \\frac{TP_{tool}}{TP_{tool} + FP_{tool}} \nRecalltool = \\frac{TP_{tool}}{TP_{tool} + FN_{tool}}\nF1 Scoretool = \\frac{2 x Precision_{tool} \u00d7 Recall_{tool}}{Precision_{tool} + Recall_{tool}}\nPrecisionnode = \\frac{TP_{node}}{TP_{node} + FP_{node}}\nRecallnode = \\frac{TP_{node}}{TP_{node} + FN_{node}}\nF1 Scorenode = \\frac{2 x Precision_{node} Recall_{node}}{Precision_{node} + Recall_{node}}\nPrecisionedge = \\frac{TP_{edge}}{TP_{edge} + FP_{edge}}\nRecalledge = \\frac{TP_{edge}}{TP_{edge} + FN_{edge}}\nF1 Scoreedge = \\frac{2 x Precision_{edge} \u00d7 Recalledge}{Precision_{edge} + Recalledge}\nThese formulas are applicable for tool identification, node and edge matching, as accurate identifica-\ntion is crucial for the successful decomposition and execution of tasks by the agent.\nNode and Edge Matching: We also assess how well the system matches nodes (tasks) and edges\n(dependencies) in the task graph to the expected structure. These metrics are key to evaluating the\nstructural integrity and correctness of task decomposition.\nNode Label Similarity: To assess how semantically similar the nodes in the actual task graph are to\nthose in the expected graph, we use a cosine similarity measure based on node labels:\nNode Label Similarity = \\frac{1}{|N_{expected}|} \\sum_{i=1}^{N_{expected}} max_{j \\in N_{actual}} CosineSim(L_i, L_j)\n(1)\nGraph Evaluation: Beyond node and edge matching, we employ metrics that evaluate the entire\nstructure of the task graph, providing a more holistic assessment of its accuracy.\n\u2022 Graph Edit Distance (GED): GED quantifies the dissimilarity between the actual task graph\nand the expected graph by calculating the minimum number of edit operations (insertions,\ndeletions, substitutions) needed to transform one graph into the other. It provides a more\ngranular view of graph differences than simple node or edge matching.\n\u2022 Structural Similarity Index (SSI): This metric combines node and edge similarities into a\nsingle score, offering a comprehensive evaluation of the task graph's structural fidelity:\nSSI = \\frac{Node Label Similarity + Edge F1 Score}{2}\n(2)\n\u2022 Path Length Similarity: This metric compares the lengths of paths between corresponding\nnodes in both the actual and expected task graphs, providing insight into how well the system\ncaptures the broader structure of task dependencies:\nSPL = \\frac{1}{|V|^2} \\sum_{(u,v) \\in V_1 \\times V_2} exp(-a |d_{G_1}(u, v) \u2013 d_{G_2}(u, v) |)\n(3)\nComplexity Score: The complexity score as defined by [17] as the total number of vertices and\nedges within a task graph: |V| + |E|, where V is the number of nodes and E is the number of edges.\nThis metric provides a measure of the graph's structural complexity and helps compare task graphs of\nvarying sizes."}, {"title": "5 Results and Discussion", "content": "Upon investigating our agentic framework we noticed that explicitly mentioning decomposition\nstrategies (coarse grained vs fine grained) allowed the system to adapt based on the complexity and\ninterdependence of tasks extracted from a user query and saw a significant improvement in task\naccuracy and reduction in inefficiency - lesser amount of redundant tasks were produced. This is\nin agreement with [2, 18], which highlight the importance of multi-granularity approaches in AI\nframeworks, particularly in complex domains like multi-hop question answering.\nWe conducted an in-depth analysis of the proposed metrics on our agentic system to understand their\nimpact on the system's performance for both sequential and parallel tasks. The analysis revealed\nthat structural and node-level metrics are more critical in sequential tasks, while tool-related metrics\nare prominent in parallel tasks. Our choice of evaluation metrics is validated by prior research on\nLLMs and their performance in graph-based tasks. Studies have shown that combining precision and\nrecall, such as in the Node F1 Score, is essential for accurately evaluating systems that interact with\ncomplex graph structures, as it minimizes false positives and negatives [27]. The Structural Similarity\nIndex (SSI) has also been validated as a reliable metric for assessing the preservation of both the\nfunctional and structural aspects of task graphs [28].\nThe Structural Similarity Index (SSI) emerged as the most significant predictor of Answer Score,\nwith a strong positive correlation (r = 0.470, p < 0.001). Plot can be seen in the Appendix in\nFigure 3. Node Label Similarity also showed a substantial positive correlation (r = 0.447, p <\n0.01). Interestingly, Expected Task Complexity exhibited a moderate negative correlation (r =\n0.293, p < 0.05), suggesting that as tasks become more complex, performance tends to decrease.\nThese correlations were further reflected in real-world applications of the agentic system, where\nscenarios requiring fine-grained task decomposition revealed high Node Precision and Recall, but also\nhighlighted challenges with managing complex dependencies, as indicated by lower Edge F1 Scores.\nWhen considering the absolute coefficients from our linear regression model, SSI again emerged as\nthe most important feature, followed by Edge F1 Score and Node Label Similarity. This aligns with\nour correlation analysis, emphasizing the importance of structural accuracy in sequential tasks. The\nmodel achieved an R-squared value of 0.3631, indicating that these features explain approximately\n36.31% of the variance in Answer Score for sequential tasks.\nFor parallel tasks, we observed a shift in the importance of metrics, with tool-related features gaining\nprominence. Plots can be seen in Appendix A in Figure 4. Tool F1 Score showed the strongest\ncorrelation with Answer Score (r = 0.476, p < 0.001), closely followed by Tool Recall (r = 0.474, p <\n0.01) and Tool Precision (r = 0.414, p < 0.01). SSI and Node Label Similarity both demonstrated\nmoderate positive correlations (r = 0.380, p < 0.05 for both). Interestingly, our regression analysis\nrevealed that SSI and Node Label Similarity had the highest importance scores, despite not having\nthe strongest correlations. This suggests a complex interplay between these structural features and\ntool-related metrics in parallel tasks. The model for parallel tasks achieved an R-squared value of\n0.3933, explaining 39.33% of the variance in Answer Score.\nOur findings highlight differences in the factors influencing agentic system performance between se-\nquential and parallel tasks. Structural metrics (SSI and Node Label Similarity) are important across\nboth task types, but their relative importance is higher in sequential tasks. Practical application of our\nevaluation framework also supports these findings, where task decomposition in real-world scenarios\ndisplayed strong structural performance but revealed weaknesses in dependency management (as\nseen in lower Edge F1 Scores). Tool-related metrics (Tool F1 Score, Recall, and Precision) are\nmore strongly correlated with performance in parallel tasks, suggesting that effective tool selection\nand usage become critical in more complex, non-linear task structures. Edge-related metrics (Edge\nPrecision and Edge F1 Score) show some importance in sequential tasks but are not significant in\nparallel tasks, possibly due to the more complex relationships between nodes in parallel structures."}, {"title": "6 Limitations", "content": "While the proposed agentic framework offers significant advancements in evaluating agentic systems,\nit faces several limitations. A primary issue is the lack of support for multi-agent communication,\nwhich limits the framework's effectiveness in scenarios requiring task coordination among multiple\nagents. The system is only suited for single-agent environments or cases where agents operate"}, {"title": "7 Future Work", "content": "independently. Expanding this capability would significantly enhance its versatility in complex\nsettings.\nPrecision, Recall, and F1 Score-are also sensitive to dataset imbalances. When there are very few\nor many expected tools, these metrics may not capture performance nuances effectively, leading\nto skewed results. Another limitation is that these metrics do not account for the context in which\nthe tools are applied. A tool may be identified correctly, but if used in an inappropriate context,\nthis error is not reflected in the metrics, potentially leading to inaccurate performance assessments.\nCalculating Graph Edit Distance (GED) is computationally intensive, especially for large graphs, and\nthis sensitivity of GED to cost, along with the ambiguity in its interpretation, reduces its effectiveness\nin evaluating complex task graphs. The matching of nodes and edges can suffer when multiple\nnodes or edges possess similar labels, which can require manual intervention or the implementation\nof additional rules to resolve conflicts. This increases the complexity of the task and reduces the\nreliability of fully automated methods. Moreover, edge matching is heavily dependent on the accuracy\nof node matching, which means that any errors in node similarity evaluations can propagate and\nnegatively affect the assessment of task dependencies. The Structural Similarity Index (SSI) also\npresents certain drawbacks. First, it is highly sensitive to node similarity scores, which can be\nproblematic when nodes - representing tasks are described in significantly different ways. This\ncan distort the overall similarity measure. Additionally, SSI assigns equal weight to node and\nedge similarities, which may not be appropriate in all cases. In some situations, task dependencies,\nrepresented by edges, may be more important than the individual tasks themselves, and equal\nweighting could overlook this context. Finally, SSI computation, particularly for large graphs can be\ncomputationally expensive, further complicating its application in large-scale evaluations.\nBuilding on the limitations identified, future research should focus on the following. First, the\nintegration of multi-agent communication protocols would allow for dynamic collaboration, enabling\nthe system to handle more complex, real-time scenarios that require the collaboration of multiple\nagents. Additionally, introducing causal inference methods to the evaluation framework would offer\ndeeper insights into system performance by moving beyond correlation-based metrics. Optimizing\nscalability remains a critical challenge, especially for large-scale real-time environments, where\nreducing latency and computational overhead is essential. Finally, developing more sophisticated\ndatasets that reflect real-world randomness and multi-agent interactions is crucial for testing and\nimproving the scalability of agentic systems. Refer to Appendix C for detailed concrete future\nresearch directions to extend this work."}, {"title": "8 Conclusion", "content": "Analysis of our agentic framework revealed that task graph based decomposition [17] and explicitly\nintegrating coarse-grained and fine-grained decomposition strategies led to improved task accuracy\nand a reduction in inefficiencies by minimizing redundant tasks. This adaptability, driven by the\ncomplexity and interdependence of user query tasks, highlights the critical role of multi-granularity\napproaches in agentic systems. Moreover, while these strategies enhanced system performance,\nchallenges in managing complex dependencies, especially in fine-grained tasks, became evident.\nOur work also presents a comprehensive framework for evaluating agentic systems driven by LLMs.\nWe introduce proper metrics, including the Node F1 Score, Structural Similarity Index, and Tool\nF1 Score, to assess the performance of these systems in task decomposition and tool integration\nscenarios. Additionally, our specialized dataset created for this study enables an in-depth analysis\nof agentic behavior across various task complexities. Our findings emphasize the importance of\nboth structural and tool-related metrics in determining overall system performance, with notable\ndifferences between sequential and parallel tasks. While structural metrics are more critical in\nsequential tasks, tool usage becomes more significant in parallel tasks, highlighting the need for\nbalanced evaluation methods that capture both the structural and operational aspects of agentic\nsystems. The proposed framework and evaluation methodology provide a strong foundation for\nfurther research. Addressing the outlined limitations and pursuing the proposed future work will be\ncritical in advancing agentic systems capable of handling more complex, real-time, and collaborative\nenvironments."}]}