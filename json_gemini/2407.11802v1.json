{"title": "Invariant Consistency for Knowledge Distillation", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "abstract": "Knowledge distillation (KD) involves transferring the knowledge from one neural network to another, often from a larger, well-trained model (teacher) to a smaller, more efficient model (student). Traditional KD methods minimize the Kullback-Leibler (KL) divergence between the probabilistic outputs of the teacher and student networks. However, this approach often overlooks crucial structural knowledge embedded within the teacher's network. In this paper, we introduce Invariant Consistency Distillation (ICD), a novel methodology designed to enhance KD by ensuring that the student model's representations are consistent with those of the teacher. Our approach combines contrastive learning with an explicit invariance penalty, capturing significantly more information from the teacher's representation of the data. Our results on CIFAR-100 demonstrate that ICD outperforms traditional KD techniques and surpasses 13 state-of-the-art methods. In some cases, the student model even exceeds the teacher model in terms of accuracy. Furthermore, we successfully transfer our method to other datasets, including Tiny ImageNet and STL-10. The code will be made public soon.", "sections": [{"title": "1. Introduction", "content": "Knowledge distillation (KD) is a method where knowledge from a larger, well-trained model (the teacher) is transferred to a smaller, more efficient model (the student) by minimizing the Kullback-Leibler (KL) divergence between their outputs. This approach allows the student model to emulate the performance of the teacher model while maintaining reduced computational complexity, making it highly suitable for deployment in resource-constrained environments. Representation learning is integral to KD as it enables the student model to learn meaningful feature representations that capture the underlying data distribution. Effective representation learning through KD can significantly enhance the performance of the student model across various tasks, including natural language processing, computer vision, and speech recognition [2, 13, 32].\nDespite advancements in KD, efficiently transferring complex knowledge from the teacher to the student model remains a significant challenge. The primary difficulty lies in ensuring that the student model captures the same level of abstract features and subtle information as the teacher model without requiring similar computational capacity. The KD objective introduced by [13] assumes that all dimensions are independent, given the input [29]. This traditional KD objective employs a factored form that does not effectively transfer the structural knowledge or dependencies between different output dimensions. This limitation is similar to the issue in image generation where using an l2 objective leads to blurry results due to independence assumptions between the output dimensions. Various techniques have been proposed to address this challenge, such as the use of intermediate representations [25], attention mechanisms [35], similarity-preserving methods [30], and correlation congruence [23]. Additionally, methods like variational information distillation [1] and relational knowledge distillation [21] have also contributed to enhancing the knowledge transfer process.\nIn this paper, we introduce a novel approach called Invariant Consistency Distillation (ICD), which aims to improve the efficiency and accuracy of KD by enhancing the alignment between the teacher and student models. Our method ensures that the representations learned by the student are consistent with those of the teacher through a combination of contrastive learning and an explicit invariance penalty. By incorporating an instance discrimination task, ICD effectively transfers knowledge from the teacher model to the student model, resulting in improved performance and robustness. In various knowledge transfer tasks, we observe improved performance. Remarkably, in some cases, our objective allows the student model to surpass the teacher in terms of accuracy and robustness. We attribute this success to our objective's ability to transfer all the information in the teacher's representation, rather than just knowledge about independent output class probabilities, thereby enhancing representation learning. Our contributions are threefold:\n1. We propose a new objective that combines contrastive"}, {"title": "2. Related Work", "content": "The seminal work on KD introduced the concept of transferring knowledge from large, cumbersome models to smaller, faster models while maintaining generalization power [13]. This method utilizes temperature scaling in the softmax outputs to better capture and transfer the knowledge of the teacher model. Various extensions and improvements to this approach have been proposed. For instance, using intermediate representations, or \"hints\", to guide the learning process has been suggested [25]. Another approach focuses on aligning the attention maps of the teacher and student models to ensure they focus on similar regions during training [35]. There are methods that preserve relational knowledge between samples [30] and those that align the correlation structures between teacher and student models [23]. Additionally, some methods employ variational inference techniques to enhance the knowledge transfer process [1]. Further advancements include focusing on the structural relationships between data points to ensure the student model learns relational information [21], and preserving the internal dynamics of neural networks during distillation [12, 22]. Other notable techniques involve compressing networks via factor transfer [16], optimizing and minimizing networks for enhanced transfer learning [34], promoting selectivity in the distillation process [15], and employing contrastive learning objectives to improve representation learning [29].\nContrastive methods in self-supervised learning have proven effective for learning robust representations by maximizing mutual information. Techniques such as Contrastive Predictive Coding (CPC) [31], Deep InfoMax (DIM) [14], and Augmented Multiscale DIM (AMDIM) [3] use noise contrastive estimation (NCE) to create objectives that serve as bounds on mutual information. These approaches demonstrate that their objectives can maximize a lower bound on mutual information, which is crucial for learning meaningful representations. Recently, some methods have successfully combined contrastive learning with standard architectures, utilizing strong augmentations and memory banks to improve performance [5, 10]. Additionally, non-contrastive methods like BYOL have emerged, achieving remarkable results without explicitly maximizing mutual information [9].\nOur method, Invariant Consistency Distillation (ICD), builds on these foundations by integrating contrastive learning with an explicit invariance penalty. This approach ensures that the representations learned by the student model are consistent with those of the teacher model, leading to improved performance and robustness. By addressing the limitations of previous methods and introducing a rigorous framework for invariant consistency, ICD offers a significant advancement in the field of KD."}, {"title": "3. Methodology", "content": "This section presents our methodology to improve the efficiency and accuracy of KD. Our method, Invariant Consistency Distillation (ICD), focuses on enhancing the alignment between the teacher and student models by ensuring that the representations learned by the student are consistent with those of the teacher. By introducing a novel objective that combines contrastive learning with an explicit invariance penalty, ICD effectively transfers knowledge from the teacher model to the student model, leading to improved performance and robustness."}, {"title": "3.1. Preliminary", "content": "KD involves transferring knowledge from a high-capacity teacher neural network, denoted as $f_T$, to a more compact student neural network, $f_S$. Consider $x_i$ as the input to these networks, typically an image. We represent the outputs at the penultimate layer (just before the final classification layer, or logits) as $z_T^i = f_T(x_i)$ and $z_S^i = f_S(x_i)$ for the teacher and student models, respectively. The primary objective of KD is to enable the student model to approximate the performance of the teacher model while leveraging the student's computational efficiency. The overall distillation process can be mathematically expressed as:\n\n$\\mathcal{L} = \\mathcal{L}_{sup}(y_i, z_S^i) + \\lambda \\cdot \\mathcal{L}_{distill} (z_T^i, z_S^i)$\n\nwhere $y_i$ represents the true label for the input $x_i$ and $\\lambda$ is a hyperparameter that balances the supervised loss and the distillation loss. The supervised loss $\\mathcal{L}_{sup}$ is the alignment error between the network prediction and the annotation. For"}, {"title": "3.2. Invariant consistency", "content": "Our method aims to enhance the alignment between the teacher $f_T$ and the student model $f_S$ by ensuring that their learned representations are consistent under varying conditions. We develop an objective function that introduces a structured approach to enforce this consistency between the teacher's output $z_T^i$ and the student's output $z_S^i$. The objective combines contrastive learning with an explicit invariance penalty to ensure that the representations from both models are aligned. The objective function for ICD is formulated as:\n\n$\\mathcal{L}_{kd}(z_T^i, z_S^i) = \\mathcal{L}_{contrast} (z_T^i, z_S^i) + \\alpha \\cdot \\mathcal{L}_{invariance} (z_T^i, z_S^i)$\n\nwhere $\\alpha$ is a hyperparameter that balances the contrastive loss $\\mathcal{L}_{contrast}$ and the invariance penalty $\\mathcal{L}_{invariance}$.\nThe contrastive loss encourages the representations from the teacher and student models for the same input data to be similar, while simultaneously pushing apart representations from different data inputs:\n\n$\\mathcal{L}_{contrast}(z_T^i, z_S^i) = - \\log \\frac{\\exp(\\phi(z_T^i, z_S^i) / \\tau)}{\\sum_{j=1}^{M} \\exp(\\phi(z_T^i, z_S^j) / \\tau)}$\n\nwhere $\\phi$ is a similarity function, $\\tau$ is a temperature parameter, and M is the number of negative samples. In our framework, positives are obtained by pairing the outputs $z_T^i$ and $z_S^i$ from the teacher and student models for the same input instance. Negatives are sampled from other instances within the mini batch, following the instance discrimination principle to differentiate between distinct data points [33]. The similarity function $\\phi$ is chosen as the cosine similarity:\n\n$\\phi(z_T^i, z_S^i) = \\frac{z_T^i \\cdot z_S^i}{\\|z_T^i\\| \\|z_S^i\\|}$\n\nTo enforce invariance, we minimize the discrepancy between the distributions of representations $z_T^i$ and $z_S^i$. This penalty ensures that the student model learns to produce representations that are invariant to the teacher model's transformations:\n\n$\\mathcal{L}_{invariance} (z_T^i, z_S^i) = KL(p(z_T^i) || p(z_S^i))$\n\nwhere KL denotes the KL divergence between the distributions $p(z_T^i)$ and $p(z_S^i)$, ensuring that the student model's outputs faithfully replicate the teacher model's transformations across different inputs."}, {"title": "4. Experiments", "content": "We evaluate our ICD framework in the KD task for model compression of a large network to a smaller one.\nDatasets. (1) CIFAR-100 [17] contains 50,000 training images with 500 images per class and 10,000 test images. (2) STL-10 [7] consists of a training set of 5,000 labeled images from 10 classes and 100,000 unlabeled images, and a test set of 8,000 images. (3) Tiny ImageNet (TIN-200) [8] comprises 200 classes, each with 500 training images and 50 validaton images.\nSetup. We experiment on CIFAR-100 with student-teacher combinations of different capacities, such as ResNet [11] or Wide ResNet (WRN) [36], VGG [28], MobileNet [26], and ShuffleNet [19,37] (more details are described in the supplementary material). We set the temperature parameter to $\\tau = 0.07$. The hyperparameters are empirically set to $\\alpha = 0.5$ and $\\beta = 1$. Both the student and teacher outputs are projected to a 128-dimensional space using a projection head consisting of a single linear layer, followed by $l_2$ normalization. We train for a total of 240 epochs. More details on the training procedures can be found in the supplementary material."}, {"title": "4.1. Results on CIFAR-100", "content": "Table 1 and Table 2 present a comparison of top-1 accuracies for various distillation objectives. Table 1 focuses on student and teacher models with identical architectures, while Table 2 examines models with differing architectures. Our method consistently outperforms other distillation objectives, including the original KD, demonstrating an average relative improvement of 24.66% for ICD and 76.42% for ICD combined with KD. Significantly, when applying our method to distill from WRN-40-2 to WRN-16-2 (same architecture) or from WRN-40-2 to ShuffleNet-v1 (different architecture), it not only matches but surpasses the performance of the teacher network. Specifically, it achieves accuracy improvements of 0.45% and 0.78% over the teacher, respectively. ICD combined with KD performs exceptionally well even when student and teacher models have different architectures, achieving top accuracies in several configurations, such as MobileNet-v2 distilled from VGG-13 and ShuffleNet-v1 distilled from WRN-40-2. While CRD and CRD combined with KD also perform well, ICD and ICD combined with KD frequently achieve higher accuracies. We also observe that KD alone performs robustly, as no other methods except CRD and ICD consistently surpass traditional KD (as indicated by green arrows \u2191 in Table 1 and Table 2). Both CRD and ICD utilize contrastive objectives to ensure consistency between the student and teacher models. Additionally, methods distilling from intermediate representations tend to underperform compared to those focusing on the last several layers when switching from same to different architectures. For instance, the Attention Transfer (AT) and FitNet methods underperform the vanilla student model, whereas PKT, SP, CRD, and ICD, which focus on the last layers, perform well. This discrepancy can be attributed to the different solution paths mapping input to output across architectures, leading to conflicts when mimicking intermediate representations."}, {"title": "4.2. Capturing inter-class correlations", "content": "Cross-entropy loss ignores the correlations among class logits in a teacher network, often leading to suboptimal knowledge transfer. By employing \"soft targets\", distillation methods such as those described by [13] have successfully captured these correlations, enhancing student learning.\nOur results demonstrate that ICD achieves close correlation alignment between teacher and student logits, as evidenced by the minimal differences in their correlation matrices. Moreover, our method also enhances representation learning as it employs a contrastive objective."}, {"title": "4.3. Transferability of representations", "content": "Our research focuses on transferring knowledge from a teacher network to a student network while learning representations that embody general knowledge applicable across various tasks and datasets. To investigate this, we employ a distillation process where a WRN-40-2 teacher network transfers its learned representations to a WRN-16-2 student network, which can either be trained directly from the CIFAR-100 dataset or through distillation. In our experiments, the student network functions as a fixed feature extractor, processing images from STL-10 and TIN-200, both resized to 32 \u00d7 32. To assess the generalizability of these representations, we train a linear classifier on top of the last feature layer to perform 10-way classification for STL-10 and 200-way classification for TIN-200. Our results indicate that most distillation methods, significantly enhance the transferability of the learned representations on both target datasets. Interestingly, the teacher network, while performing best on CIFAR-100, offers representations that transfer the least effectively, possibly due to a bias towards the training dataset. In contrast, the student network employing a combination of ICD and KD distillation not only matches the teacher's performance on CIFAR-100 but also exhibits superior transferability, with notable improvements of 3.9% and 4.7% in STL-10 and TIN-200, respectively."}, {"title": "5. Conclusions", "content": "We have developed a new technique for neural network distillation that leverages invariant consistency to improve the traditional KD process. Our method, demonstrated for model compression, can also be applied to cross-modal knowledge transfer and ensemble distillation from a group of teachers to a single student network. It combines contrastive learning with an invariance penalty to better align teacher and student models. Unlike CRD [29], our approach requires no memory bank. Instead, it uses a trainable temperature parameter and a bias to scale and shift the logits. By leveraging the contrastive nature, ICD not only aligns the student with the teacher's output but also improves the quality of the learned embeddings, leading to better generalization and performance. Our experiments demonstrate that this approach consistently outperforms standard KD methods, enhancing both accuracy and robustness. Notably, our method has shown excellent performance on CIFAR-100 and has been successfully applied to other datasets such as TIN-200 and STL-10. In some cases, student models using our KD technique even exceeded the performance of their teacher models, suggesting significant potential for this approach in optimizing neural network training and deployment in various settings."}]}