{"title": "Improving Thompson Sampling via Information Relaxation for Budgeted Multi-armed Bandits", "authors": ["Woojin Jeong", "Seungki Min"], "abstract": "We consider a Bayesian budgeted multi-armed bandit problem, in which each arm consumes a different amount of resources when selected and there is a budget constraint on the total amount of resources that can be used. Budgeted Thompson Sampling (BTS) offers a very effective heuristic to this problem, but its arm-selection rule does not take into account the remaining budget information. We adopt Information Relaxation Sampling framework that generalizes Thompson Sampling for classical K-armed bandit problems, and propose a series of algorithms that are randomized like BTS but more carefully optimize their decisions with respect to the budget constraint. In a one-to-one correspondence with these algorithms, a series of performance benchmarks that improve the conventional benchmark are also suggested. Our theoretical analysis and simulation results show that our algorithms (and our benchmarks) make incremental improvements over BTS (respectively, the conventional benchmark) across various settings including a real-world example.", "sections": [{"title": "Introduction", "content": "As an intuitive and efficient heuristic algorithm for sequential decision-making tasks in unknown environments, Thompson Sampling (TS) (Thompson, 1933) has been enjoying a huge success in practice and adopted in recommendation systems (Chapelle & Li, 2011), A/B testing (Graepel et al., 2010), the online advertisement (Graepel et al., 2010; Agarwal, 2013), reinforcement learning (Osband et al., 2013), etc. Built upon online Bayesian inference framework, TS takes an action optimized to model parameters randomly drawn from the posterior distribution at each decision epoch. This simple procedure, called posterior sampling, finds a surprisingly proper balance between exploitation and exploration, and is proven to achieve optimality (Agrawal & Goyal, 2012; Russo & Van Roy, 2014).\nHowever, the posterior sampling procedure only considers the current level of model uncertainty, not considering the future consequences of individual actions. This often critically affects the performance of TS, particularly when the value of exploration needs to be taken into account carefully for example, when there are an excessive number of arms (Russo & Van Roy, 2022) when the arms have different noise variances (Kirschner & Krause, 2018; Min et al., 2020), or when the exploration is restricted due to a budget constraint, the situation formulated as a budgeted multi-armed bandit (MAB) (Ding et al., 2013; Xia et al., 2015).\nIn the budgeted MAB, playing an arm yields a random reward and incurs a deterministic/random cost at the same time, and no more play can be made once the playe runs out of budget. This setting has been introduced to model online bidding optimization in sponsored search (Amin et al., 2012; Tran-Thanh et al., 2014), and on-spot instance bidding in cloud computing (Agmon Ben-Yehuda et al., 2013). The algorithms such as KUBE (Tran-Thanh et al., 2012), UCB-BV1/BV2 (Ding et al., 2013), PD-BwK (Badanidiyuru et al., 2013), i/c/m-UCB, b-Greedy (Xia et al., 2017), and BTS\n(Xia et al., 2015) have been proposed and analyzed. Budgeted Thompson Sampling (BTS), as an immediate extension of TS for budgeted MAB, is considered as a baseline algorithm to be fixed in this work. Although it significantly outperforms the other algorithms, it still does not consider the remaining budget information when making a decision, and hence suffers from the aforementioned issue.\nTo overcome this shortcoming, we adopt the Information Relaxation Sampling (IRS) framework, recently suggested by Min et al. (2019) for classical Bayesian K-armed bandit problems. Generalizing the concept of posterior sampling, the IRS framework suggests a class of algorithms which optimize their actions to a randomly generated future scenario (not just model parameters) in a careful consideration of the belief dynamics of Bayesian learners.\nOur contributions are threefold: First, by applying the IRS framework to the budgeted MAB setting, we develope a series of algorithms that can exploit the specific details of the problem instance such as budget information. Without introducing any auxiliary parameter, they easily achieve the state-of-the-art performance. In our numerical experiment, the improvement over BTS can be as large as 75% in terms of reduction in regret.\nSecond, we obtain as byproducts a series of upper bounds on the maximal performance that can be achieved in the given problem instance. This series of upper bounds also improve the conventional one commonly used in the definition of Bayesian regret, and turn out to be useful to see how much additional improvement can be made.\nFinally, we extend IRS to random cost settings by making two levels of extensions. As a relatively simpler extension, we allow IRS policies to sample the mean cost values from their posterior distributions and then solve inner problems as if these sampled values are the ground truth, i.e., the idea of IRS is applied only to rewards but not to costs. As a more complicated extension, we can make IRS policies to sample all future cost realizations and then solve more complex inner problems that additionally consider how much the decision maker will learn about the cost distributions, i.e., the idea of IRS is applied to both rewards and costs. Our numerical experiment shows that these extensions of IRS policies indeed offer sequential improvements over BTS as expected. And it show that the more complicated extension outperforms the simple extension.\nThroughout this paper, we will focus on explaining two specific algorithms, namely, IRS.FH and IRS.V-Zero, instead of describing the general framework."}, {"title": "Problem Formulation and Preliminaries", "content": "We consider a Bayesian budgeted MAB problem with K arms and a resource budget B. A problem instance can be specified by a tuple $(K, B, (c_a, R_a, \\Theta_a, P_a, \\mathcal{Y}_a, \\mathcal{Y}_{a,0})_{a\\in[K]})$ which will be described in a greater detail below.\nRewards and costs. Let $\\mathcal{A} = [K]$ be the set of arms, among which the decision maker (DM) can play one in each time period. The stochastic reward that the DM earns from the $n$th pull of arm $a$ is represented with a nonnegative random variable $R_{a,n}$, and we assume that its distribution is given by $R_a(\\theta_a)$:\n$R_{a,n}\\sim R_a(\\theta_a), \\forall n = 1,2,...,$\nwhere $\\theta_a \\in \\Theta_a$ is the unknown parameter that the DM aims to learn. Given $\\theta_a$, the rewards $R_{a,1}, R_{a,2},...$ are independent.\nWhenever arm $a$ is played, it also incurs a deterministic cost,\u00b9 denoted by $c_a \\in \\mathbb{N}$ (i.e., consumes $c_a$\nunits of resources deterministically). The total amount of resources that the DM can use is limited by $B\\in \\mathbb{N}$, and the DM's goal is to maximize the expected total reward within this budget constraint."}, {"title": "Bayesian framework.", "content": "In the Bayesian framework, the unknown parameter $\\theta_a$ is treated as a random variable and we assume that its prior distribution is given by $P_a(y_{a,0})$, i.e.,\n$\\theta_a \\sim P_a(y_{a,0}),$\nwhere the hyperparameter $y_{a,0} \\in \\mathcal{Y}_a$, which we call (initial) belief, specifies the prior distribution.\nAs a Bayesian learner, the DM's belief about $\\theta_a$ will be updated according to the Bayes' rule whenever a new reward realization from the arm $a$ is observed. To describe the belief dynamics explicitly, we introduce a Bayesian update function $\\mathcal{U}_a : \\mathcal{Y}_a \\times \\mathbb{R}_+ \\rightarrow \\mathcal{Y}_a$. That is, after playing the arm $a$ for the first time, the belief is updated from $y_{a,0}$ to $y_{a,1} \\equiv \\mathcal{U}_a(y_{a,0}, R_{a,1})$ and then the posterior distribution of $\\theta_a$ can be written as $P_a(y_{a,1})$. We accordingly define $y_{a,n}$ be the belief that the DM will have after playing the arm $n$ times, i.e., $y_{a,n} = \\mathcal{U}_a(y_{a,n-1}, R_{a,n})$ for $n = 1, 2, . . ..$\nMean reward estimates. We denote the unknown mean reward of arm $a$ by $\\mu_a(\\theta_a)$ as a real-valued function of parameter $\\theta_a$:\n$\\mu_a(\\theta_a) \\triangleq \\mathbb{E}[R_{a,n}|\\theta_a]$.\nLet us denote its $n$-sample (Bayesian) estimate by $\\hat{\\mu}_{a,n}(\\cdot)$ as a real-valued function of first $n$ reward realizations: abbreviating $(R_{a,1},..., R_{a,n})$ as $R_{a,1:n}$, we define\n$\\hat{\\mu}_{a,n}(R_{a,1:n}; y_{a,0}) \\triangleq \\mathbb{E}_{\\theta_a\\sim P_a(y_{a,0})}[\\mu_a(\\theta_a)|R_{a,1:n}],$\nwhich represents the expected performance of arm $a$ inferred from its first $n$ reward realizations, or equivalently, the predictive mean reward of arm $a$ that the DM would believe after playing the arm $n$ times."}, {"title": "Remark 1 By Strong Law of Large Numbers", "content": "$\\\\lim_{n\\rightarrow \\infty} \\hat{\\mu}_{a,n}(R_{a,1:n}; y_{a,0}) = \\mu_a(\\theta_a), a.s.,$\nwhich says that, in terms of mean-reward estimation, knowing the parameter is equivalent to having an infinite number of observations. Also, for any $n$ and $k$, it holds that\n$\\hat{\\mu}_{a,n+k}(R_{a,1:n+k}; y_{a,0}) = \\hat{\\mu}_{a,k}(R_{a,n+1:n+k}; y_{a,n}),$\nwhich says that making an inference using $n+k$ samples given an initial belief is equivalent to making an inference using the later $k$ samples after updating the belief using the former $n$ samples.\nPolicy and performance. Let $\\pi$ be the DM's policy, and $A_t$ be the arm played by at time $t$. The reward that the DM earns at time $t$ can be written as\n$r_t \\triangleq R_{A_t,n_{A_t,t}}$ where $n_{a,t} = \\sum_{s=1}^t \\mathbb{1}\\{A_s = a\\}.$\nHere, $n_{a,t}$ counts the number of times that arm $a$ has been played up to time $t$. An admissible policy $\\pi$ should decide $A_t$ based only on the information revealed prior to time $t$, $(A_s, r_s)_{s=1}^{t-1}$.\nBesides, playing the arm $A_t$ consumes $c_{A_t}$ units of resources. To describe the budget constraint explicitly, we introduce a stopping time $\\tau$ representing the first time that the cumulative cost exceeds the given budget, i.e.,\n$\\tau \\triangleq \\min\\{t : \\sum_{s=1}^t c_{A_s} > B\\}$"}, {"title": "Performance bound and regret.", "content": "A quantity $W$ is said to be a performance bound if $W \\geq V(\\pi)$ for any policy $\\pi$.\nAs a performance bound commonly used in the MAB literature, $W^{\\text{BTS}}$ is defined as\u00b2\n$W^{\\text{BTS}} \\triangleq B\\times \\max_{a\\in\\mathcal{A}}\\frac{\\mathbb{E}[\\mu_a(\\theta_a)]}{c_a}$\nThis quantity represents the expected performance of the clairvoyant fractional solution: when the player knows the parameters $\\theta_{1:K}$ in advance, it is optimal for him to play the arm $a^*$ with the largest reward-to-cost ratio $\\mu_{a^*}(\\theta_{a^*})/c_{a^*}$, (fractionally) $B/c_{a^*}$ times in a row, which will yield the total reward of $\\mathbb{E} [\\mu_{a^*}(\\theta_{a^*}) \\times B/c_{a^*}] (= W^{\\text{BTS}})$ in average. Clearly, no policy can perform better than this clairvoyant player, and therefore, $W^{\\text{BTS}}$ is an upper bound on the maximal achievable performance for the given MAB instance.\nA performance bound $W$ is said to be tighter than the other $W'$ if $W < W'$. A tighter bound provides a more precise quantification of the hardness of a particular MAB instance, and can better serve as a performance benchmark.\nOn the other hand, we will later utilize the Bayesian regret to visualize and compare the performance of policies, which is defined as\n$\\text{REGRET}(\\pi) \\triangleq W^{\\text{BTS}} - V(\\pi).$\nThe regret quantifies the suboptimality of a policy, and is non-negative since $W^{\\text{BTS}}$ is a performance bound. Once we have a performance bound $W$ tighter than $W^{\\text{BTS}}$, the gap $W^{\\text{BTS}} - W$ will provide a lower bound on the minimal achievable regret (i.e., $\\text{REGRET}(\\pi) > W^{\\text{BTS}} - W$ for any $\\pi$).\nBayesian optimal policy. In the Bayesian setting, there exists a policy that achieves the maximal performance $V^*$:\n$V^* \\triangleq \\sup_{\\pi} V(\\pi).$\nSuch a Bayes-optimal policy and its performance $V^*$, in theory, can be obtained by solving the Bellman equation (corresponding to an MDP with a state space $\\mathcal{Y}_1 \\times ... \\times \\mathcal{Y}_K$ and an action space $\\mathcal{A}$. See Appendix A for the detail), but they are intractable in most cases.\nAs motivated in the introduction, our primary goal is to improve the BTS policy in terms of performance, where the Bayes-optimal policy will be our ideal target. Another goal is to improve the performance bound $W^{\\text{BTS}}$ in terms of tightness, where $V^*$ will be our ideal target."}, {"title": "Algorithms", "content": "In this section, we propose a series of policies that improve Budgeted Thompson Sampling (BTS) toward the Bayes-optimal policy by leveraging the idea of information relaxation sampling. In parallel,\nBudgeted Thompson Sampling\nAs an immediate extension of Thompson Sampling to the budgeted MAB setting, BTS (Xia et al., 2015) utilizes the posterior sampling of the parameters. As described in Algorithm 1, the policy $\\pi^{\\text{BTS}}$ at each time $t$ draws a random sample of the parameters from the posterior distribution (i.e., $\\theta_a^{(t)} \\sim P_a(y_{a,n_{a,t-1}})$ in line 4), and plays the arm with the largest reward-to-cost ratio given the sampled parameters (i.e., $\\arg \\max_{a} \\mu_a(\\theta_a^{(t)})/c_a$ in line 6). After observing the result of the play, it updates the belief about the arm according to the Bayes' rule (line 11), and repeats this procedure until the budget is exhausted.\nAlgorithm 1 BTS\nInput: $K, B, (c_a, R_a, \\Theta_a, P_a, \\mathcal{Y}_a, \\mathcal{Y}_{a,0})_{a\\in[K]}$\nProcedure:\n1: Initialize $t\\leftarrow 1$, $B_1 \\leftarrow B$, $n_{a,0} \\leftarrow 0$ for each $a \\in \\mathcal{A}$\n2: while $B_t > 0$ do\n3:   for each arm $a \\in \\mathcal{A}$ do\n4:     Sample $\\theta_a^{(t)} \\sim P_a(y_{a,n_{a,t-1}})$\n5:   end for\n6:   $A_t \\leftarrow \\arg \\max_{a\\in\\mathcal{A}}\\{\\mu_a(\\theta_a^{(t)})/c_a\\}$\n7:   if $B_t < c_a$ then\n8:     break\n9:   else\n10:     Play $A_t$, receive $r_t$, pay $c_{A_t}$ ($B_{t+1} \\leftarrow B_t - c_{A_t}$)\n11:     Update $y_{a,n_{a,t-1}+1} \\leftarrow \\mathcal{U}_a(y_{a,n_{a,t-1}}, r_t)$, and $n_{a,t} \\leftarrow \\begin{cases}\n12:         n_{a,t-1}+1 & \\text{ for } a=A_t \\\n13:         n_{a,t-1} & \\text{ for } a\\neq A_t\n14:     \\end{cases}$\n15:   end if\n16:   $t\\leftarrow t+1$.\n17: end while\nOne can immediately relate this arm-selection rule with the performance bound $W^{\\text{BTS}}$, defined in (1). As motivated earlier, the arm $a^* = \\arg \\max_{a\\in\\mathcal{A}} \\{\\mu_a(\\theta_a)/c_a\\}$ is the optimal one to play if the parameters are known and the fractional solution is allowed. The policy $\\pi^{\\text{BTS}}$ mimics such a clairvoyant player's decision by replacing the unknown components $\\mu_a(\\theta_a)$'s with their randomly generated counterparts $\\mu_a(\\theta_a^{(t)})$'s. Note that the randomness in this sampling procedure enforces $\\pi^{\\text{BTS}}$ to deviate from the myopic decision, resulting in explorations.\nAlthough BTS is simple and computationally efficient ($O(K)$ computations per decision), its arm-selection rule does not incorporate the remaining budget information. As an extreme example, if the remaining budget is so small that each arm can be play at most once, it is Bayes-optimal to make the myopic decision, i.e., $A_t \\leftarrow \\arg \\max_{a\\in\\mathcal{A}}\\{\\mathbb{E}_{\\theta_a\\sim P_a(y_{a,n_{a,t-1}})} [\\mu_a(\\theta_a)]/c_a\\}$. For this reason, BTS often performs unnecessary explorations, particularly near the end of horizon, which motivates next algorithm IRS.FH.\nIRS.FH\nOur first proposed algorithm IRS.FH\u00b3 is very similar to BTS but additionally incorporates how many times each arm can be played in the future within the remaining budget. While the belief\nAlgorithm 2 IRS.FH\nInput: $K, B, (c_a, R_a, \\Theta_a, P_a, \\mathcal{Y}_a, \\mathcal{Y}_{a,0})_{a\\in[K]}$\nProcedure:\n1: Initialize $t\\leftarrow 1$, $B_1 \\leftarrow B$, $n_{a,0} \\leftarrow 0$ for each $a \\in \\mathcal{A}$\n2: while $B_t > 0$ do\n3:   for each arm $a \\in \\mathcal{A}$ do\n4:     Sample $\\theta_a^{(t)} \\sim P_a(y_{a,n_{a,t-1}})$ and $\\hat{R}_{a,i}^{(t)} \\sim R_a(\\theta_a^{(t)})$ for $i = 1, ..., [B_t/c_a]$\n5:     $\\{\\mu_{a,[B_t/c_a]}^{(t)} \\leftarrow \\mu_{a,[B_t/c_a]}(R_{a,1:[B_t/c_a]}; y_{a,n_{a,t-1}})$\n6:   end for\n7:   $A_t \\leftarrow \\arg \\max_{a \\in \\mathcal{A}}\\{\\frac{\\mu_{a,[B_t/c_a]}^{(t)}}{c_a}\\}$\n8:   Play $A_t$ and update variables (Algorithm 1 lines 7-13)\n9: end while\nPolicy $\\pi^{\\text{IRS.FH}}$. More specifically, the policy $\\pi^{\\text{IRS.FH}}$ at each time samples not only the parameters $\\theta_a$'s but also all future rewards $\\hat{R}_{a,i}$'s (line 4). Here, $\\hat{R}_{a,i}$ represents the sampled reward realization associated with the future $i$th play of arm $a$, where $i < [B_t/c_a] - 1$ since the arm $a$ can be updated at most $[B_t/c_a] - 1$ times when the remaining budget is $B_t$. Given these sampled future rewards, it computes the future $([B_t/c_a] - 1)$-sample mean-reward estimate $\\mu_{a,[B_t/c_a] - 1}^{(t)}$, i.e., the belief that we would have if we allocate all remaining budget to the arm $a$ and the sampled future rewards indeed realize. Finally, the arm with the largest reward-to-cost $\\mu_{a,[B_t/c_a] - 1}^{(t)}/c_a$ is selected: this is almost identical to the arm-selection rule of BTS except that $\\mu_{a,[B_t/c_a] - 1}^{(t)}$ is used instead of $\\mu_a(\\theta_a^{(t)})$.\nIn other words, $\\pi^{\\text{IRS.FH}}$ finds the best arm given a finite-number of randomly synthesized future observations. By simulating the future belief changes using the sampled future rewards, it naturally takes into account how much we can learn in the future: when a smaller amount of budget is remaining, fewer future rewards will be sampled, and thus the future belief will less deviate from the current belief, which makes $\\pi^{\\text{IRS.FH}}$ more myopic, desirably.\nLet us examine the Beta-Bernoulli case for example: when the current belief is $y_a = (\\alpha_a, \\beta_a)$ and the remaining budget is $B$, $\\hat{\\mu}_{a,[B/c_a]-1}$ can be expressed as\n$\\hat{\\mu}_{a,[B/c_a]} = \\frac{\\alpha_a + \\sum_{i=1}^{[B/c_a]-1} \\hat{R}_{a,i}}{\\alpha_a + \\beta_a + [B/c_a]-1}$.\nNote that, when $B$ is small, $\\hat{\\mu}_{a,[B/c_a]} \\approx \\frac{\\alpha_a}{\\alpha_a+\\beta_a} = \\mathbb{E}_{\\theta_a\\sim Beta(\\alpha_a,\\beta_a)} [\\mu_a(\\theta_a)]$ which leads to the myopic decision (i.e., exploitation), and when $B$ is large, $\\hat{\\mu}_{a,[B/c_a]} \\approx \\frac{\\sum_{i=1}^{[B/c_a]-1} \\hat{R}_{a,i}}{[B/c_a]-1} \\approx \\mu_a(\\theta)$. Like this, the degree of exploration is naturally adjusted depending on the amount of remaining budget, mitigating the over-exploration issue that BTS suffers from.\nWe also remark that IRS.FH can be computationally efficient as much as BTS. Observe that in the above example $\\sum_{i=1}^{[B/c_a]-1} \\hat{R}_{a,i}$ is distributed with Binomial($[B/c_a]-1, \\theta_a$), and therefore $\\hat{\\mu}_{a,[B/c_a]}$ can be computed via a single random number generation without sampling $\\hat{R}_{a,i}$'s one by one. Such a trick is applicable to more general situations where the reward distribution belongs to natural exponential family, and both IRS.FH and BTS requires $O(K)$ computations per decision.\nBound $W^{\\text{IRS.FH}}$. We can motivate a new performance bound $W^{\\text{IRS.FH}}$ that is associated with $\\pi^{\\text{IRS.FH}}$. Analogously to the way that we relate BTS with $W^{\\text{BTS}}$, we define\n$W^{\\text{IRS.FH}} \\triangleq B\\times \\max_{a\\in\\mathcal{A}}\\frac{\\mathbb{E}[\\mu_{a,[B/c_a]-1}(R_{a,1:[B/c_a]-1};y_{a,0})]}{c_a}$"}, {"title": "IRS.V-Zero", "content": "We consequently propose our next algorithm, IRS.V-Zero\u2074, that further improves IRS.FH by solving a more complicated optimization problem in each time period. It takes into account not only how many times each arm can be played, but also how the belief changes over the course of future plays.\nAlgorithm 3 IRS.V-Zero\nInput: $K, B, (c_a, R_a, \\Theta_a, P_a, \\mathcal{Y}_a, \\mathcal{Y}_{a,0})_{a\\in[K]}$\nProcedure:\n1: Initialize $t\\leftarrow 1$, $B_1 \\leftarrow B$, $n_{a,0} \\leftarrow 0$ for each $a \\in \\mathcal{A}$\n2:\n3: while $B_t > 0$ do\n4:   for each arm $a \\in \\mathcal{A}$ do\n5:     Sample $\\theta_a^{(t)} \\sim P_a(y_{a,n_{a,t-1}})$ and $\\hat{R}_{a,i}^{(t)} \\sim R_a(\\theta_a^{(t)})$ for $i = 1, ..., [B_t/c_a]$\n6:     for $i = 1, ..., [B_t/c_a]$ do\n7:       $\\hat{\\mu}_{a,i}^{(t)} \\leftarrow \\mu_{a,i}(\\hat{R}_{a,1:i}^{(t)}; y_{a,n_{a,t-1}})$\n8:     end for\n9:   end for\n10:\n11:  Solve $(\\hat{n}_1,..., \\hat{n}_K) \\leftarrow \\arg \\max_{\\tilde{n}_{1:K}\\in \\mathcal{N}(B_t)} \\sum_{a=1}^K \\sum_{i=1}^{\\tilde{n}_a} \\hat{\\mu}_{a,i-1}^{(t)}$, where $\\mathcal{N}(B_t) \\triangleq \\{(\\tilde{n}_1,........,\\tilde{n}_K); \\sum_{a=1}^K c_a\\tilde{n}_a \\leq B_t\\}$\n12:  $A_t \\leftarrow \\arg \\max_{a\\in\\mathcal{A}} \\{\\hat{n}_a\\}$\n13:  Play $A_t$ and update variables (Algorithm 1 lines 7-13)\n14: end while\nPolicy $\\pi^{\\text{IRS.V-Zero}}$. The pseudo-code is given in Algorithm 3. The policy $\\pi^{\\text{IRS.V-Zero}}$ samples the entire future reward realizations just like $\\pi^{\\text{IRS.FH}}$ does, and computes all future finite-sample estimates $\\hat{\\mu}_{a,i}^{(t)}$ for $i = 1,2,..., [B_t/c_a]$ sequentially. And then it solves a knapsack-like optimization problem (line 10) so as to determine how many times each arm should be played in the sampled future: with the nonnegative decision variables $\\tilde{n}_1,..., \\tilde{n}_K$, it solves\n$\\begin{aligned}\n\\text{maximize} &\\sum_{a=1}^K \\sum_{i=1}^{\\tilde{n}_a} \\hat{\\mu}_{a,i-1}^{(t)} \\\\\n\\text{subject to} & \\sum_{a=1}^K \\tilde{n}_ac_a \\leq B_t.\n\\end{aligned}$\nGiven the optimal solution $(\\hat{n}_1,..., \\hat{n}_K) \\in \\mathcal{N}_K$, it actually plays the arm with the largest $\\hat{n}_a$ (line 11) with an arbitrary tie-breaking rule.\nThe optimization problem (2) is to find the \"optimal allocation of the remaining budget across the arms\". In its objective, the term $\\hat{\\mu}_{a,i-1}^{(t)}$ represents the predictive mean reward of the future $i$th play"}, {"title": "Bound", "content": "WIRS.V-Zero. Focusing on the optimization problem (2), we immediately obtain the following performance bound:\n$W^{\\text{IRS.V-Zero}} \\triangleq \\mathbb{E} \\left[ \\max_{\\tilde{n}_{1:K} \\in \\mathcal{N}_B} \\sum_{a=1}^K \\sum_{i=1}^{\\tilde{n}_a} \\hat{\\mu}_{a,i-1} \\right],$\nwhere $\\mathcal{N}_B \\triangleq \\{(\\eta_1,..., \\eta_K); \\sum_{a=1}^K c_a\\eta_a \\leq B\\}$, and $\\hat{\\mu}_{a,i-1}$ hides its dependency on $\\hat{R}_{a,1:i-1}$ and $y_{a,0}$ for better presentation. In Theorem 1, we show that $W^{\\text{IRS.V-Zero}}$ further improves $W^{\\text{IRS.FH}}$.\nGeneralization\nNote that all of three policies, $\\pi^{\\text{BTS}}$, $\\pi^{\\text{IRS.FH}}$, and $\\pi^{\\text{IRS.V-Zero}}$, share the following structure in common: they in each time period (i) randomly generate future information via posterior sampling, (ii) optimize their decision to this randomly generated future via solving a deterministic optimization problem (referred to as inner problem), (iii) play an arm according to the optimized decision, and update the belief according to Bayes' rule. Their corresponding performance bounds, $W^{\\text{BTS}}$, $W^{\\text{IRS.FH}}$, and $W^{\\text{IRS.V-Zero}}$, can be obtained by solving the same inner problems, not with the sampled future realizations, but with the true future realizations.\nThe information relaxation sampling (IRS) framework formally generalizes this structure with the notion of information relaxation penalties. Deferring its detailed description to Appendix A, we briefly remark that IRS unifies BTS and the Bayesian optimal policy (OPT) into a single framework, and also includes IRS.FH, IRS.V-Zero, and IRS.V-EMax as special cases that interpolate between BTS and OPT.\nEach policy-bound pair is characterized by inner optimization problem: from BTS to OPT, they introduce increasingly complicated optimization problems, becoming more considerate but more computationally costly. We indeed observe and (partly) prove that these policies achieve increasingly better performance and these performance bounds achieve increasingly better tightness.\nIn addition, we also implement and evaluate IRS.INDEX policy, which, strictly speaking, does not belong to IRS framework (it does not have a corresponding performance bound). It internally utilizes IRS.V-EMax to obtain a random approximation of the Gittins index. See Appendix A for the detail.\nExtension to Random Cost\nWe have so far developed our framework for deterministic cost setting. In this section, we extend IRS framework to random cost setting, in which each arm consumes a random amount of resource whenever played and this random cost is drawn from an unknown distribution that we also aim to learn. More specifically, the stochastic cost that the DM pays for the $n$th pull of arm $a$ is represented with a nonnegative random variable $C_{a,n}$. Every notation is analogously defined for costs, while we use superscript (or \") to represent the parameters/variables related to costs (or rewards, respectively): e.g., the distribution of $C_{a,n}$ is given by $C_a(\\theta_a^c)$, where $\\theta_a^c$ is the unknown parameter for which we have a prior $P_a(y_{a,o}^c)$.\""}, {"title": "Simple extension", "content": "As described in Xia et al. (2015), BTS applied to the random cost setting draws the parameters $\\theta$'s from the posterior, and selects the arm with the largest mean-reward-to-mean-cost ratio: i.e., $\\arg \\max_a \\frac{\\mu_a(\\theta_a^r)}{\\mu_a(\\theta_a^c)}$. Analogously, we motivate simple extensions of IRS policies that solve the same inner problems to the deterministic cost setting but use $\\mu_a(\\theta_a^c)$ instead of $c_a$.\nExtension with additional penalties In the deterministic cost setting, we have motivated IRS polices by relaxing the information constraint imposed on reward realizations. Similarly, we can consider to relax the information constraint imposed on cost realizations. That is, we can let a policy to sample the future cost realizations in addition to the future reward realizations and solve some deterministic optimization problem with respect to this sampled future but in the presence of penalties for letting the DM exploit the future information. A penalty function suitable for IRS.V-Zero can be designed as follows.\u2075\nThe penalty function of IRS.V-Zero is given by\n$\\mathcal{Z}^{\\text{IRS.V-Zero}} (a_{1:t}, \\omega) \\triangleq r_t(a_{1:t},\\omega) - \\mathbb{E}_y[r_t(a_{1:t}, \\omega)|\\mathcal{H}_{t-1}(\\theta^r_{1:t-1})].$\nThis penalizes the DM for knowing the future reward realizations, and similarly, we can add an extra term that penalizes the DM for knowing the future cost realizations:\n$\\mathcal{Z}^{\\text{IRS.V-Zero}} (a_{1:t}, \\omega) \\equiv r_t(a_{1:t},\\omega)-\\mathbb{E}_y[r_t(a_{1:t}, \\omega)|\\mathcal{H}_{t-1}(\\theta^r_{1:t-1})]$\n$-\\lambda \\{c_t(a_{1:t}, \\omega) -  \\mathbb{E}_y[c_t(a_{1:t}, \\omega)|\\mathcal{H}_{t-1}(\\theta^c_{1:t-1})] \\}$"}]}