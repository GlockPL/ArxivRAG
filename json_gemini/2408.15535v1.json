{"title": "Improving Thompson Sampling via Information Relaxation for Budgeted Multi-armed Bandits", "authors": ["Woojin Jeong", "Seungki Min"], "abstract": "We consider a Bayesian budgeted multi-armed bandit problem, in which each arm consumes a different amount of resources when selected and there is a budget constraint on the total amount of resources that can be used. Budgeted Thompson Sampling (BTS) offers a very effective heuristic to this problem, but its arm-selection rule does not take into account the remaining budget information. We adopt Information Relaxation Sampling framework that generalizes Thompson Sampling for classical K-armed bandit problems, and propose a series of algorithms that are randomized like BTS but more carefully optimize their decisions with respect to the budget constraint. In a one-to-one correspondence with these algorithms, a series of performance benchmarks that improve the conventional benchmark are also suggested. Our theoretical analysis and simulation results show that our algorithms (and our benchmarks) make incremental improvements over BTS (respectively, the conventional benchmark) across various settings including a real-world example.", "sections": [{"title": "1 Introduction", "content": "As an intuitive and efficient heuristic algorithm for sequential decision-making tasks in unknown environments, Thompson Sampling (TS) (Thompson, 1933) has been enjoying a huge success in practice and adopted in recommendation systems (Chapelle & Li, 2011), A/B testing (Graepel et al., 2010), the online advertisement (Graepel et al., 2010; Agarwal, 2013), reinforcement learning (Osband et al., 2013), etc. Built upon online Bayesian inference framework, TS takes an action optimized to model parameters randomly drawn from the posterior distribution at each decision epoch. This simple procedure, called posterior sampling, finds a surprisingly proper balance between exploitation and exploration, and is proven to achieve optimality (Agrawal & Goyal, 2012; Russo &\nVan Roy, 2014).\nHowever, the posterior sampling procedure only considers the current level of model uncertainty, not considering the future consequences of individual actions. This often critically affects the performance of TS, particularly when the value of exploration needs to be taken into account carefully for example, when there are an excessive number of arms (Russo & Van Roy, 2022) when the arms have different noise variances (Kirschner & Krause, 2018; Min et al., 2020), or when the exploration is restricted due to a budget constraint, the situation formulated as a budgeted multi-armed bandit (MAB) (Ding et al., 2013; Xia et al., 2015).\nIn the budgeted MAB, playing an arm yields a random reward and incurs a deterministic/random cost at the same time, and no more play can be made once the playe runs out of budget. This setting has been introduced to model online bidding optimization in sponsored search (Amin et al., 2012;\nTran-Thanh et al., 2014), and on-spot instance bidding in cloud computing (Agmon Ben-Yehuda\net al., 2013). The algorithms such as KUBE (Tran-Thanh et al., 2012), UCB-BV1/BV2 (Ding et al.,\n2013), PD-BwK (Badanidiyuru et al., 2013), i/c/m-UCB, b-Greedy (Xia et al., 2017), and BTS"}, {"title": null, "content": "(Xia et al., 2015) have been proposed and analyzed. Budgeted Thompson Sampling (BTS), as an immediate extension of TS for budgeted MAB, is considered as a baseline algorithm to be fixed in this work. Although it significantly outperforms the other algorithms, it still does not consider the remaining budget information when making a decision, and hence suffers from the aforementioned issue.\nTo overcome this shortcoming, we adopt the Information Relaxation Sampling (IRS) framework, recently suggested by Min et al. (2019) for classical Bayesian K-armed bandit problems. Generalizing the concept of posterior sampling, the IRS framework suggests a class of algorithms which optimize their actions to a randomly generated future scenario (not just model parameters) in a careful consideration of the belief dynamics of Bayesian learners.\nOur contributions are threefold: First, by applying the IRS framework to the budgeted MAB setting, we develope a series of algorithms that can exploit the specific details of the problem instance such as budget information. Without introducing any auxiliary parameter, they easily achieve the state-of-the-art performance. In our numerical experiment, the improvement over BTS can be as large as 75% in terms of reduction in regret.\nSecond, we obtain as byproducts a series of upper bounds on the maximal performance that can be achieved in the given problem instance. This series of upper bounds also improve the conventional one commonly used in the definition of Bayesian regret, and turn out to be useful to see how much additional improvement can be made.\nFinally, we extend IRS to random cost settings by making two levels of extensions. As a relatively simpler extension, we allow IRS policies to sample the mean cost values from their posterior distributions and then solve inner problems as if these sampled values are the ground truth, i.e., the idea of IRS is applied only to rewards but not to costs. As a more complicated extension, we can make IRS policies to sample all future cost realizations and then solve more complex inner problems that additionally consider how much the decision maker will learn about the cost distributions, i.e., the idea of IRS is applied to both rewards and costs. Our numerical experiment shows that these extensions of IRS policies indeed offer sequential improvements over BTS as expected. And it show that the more complicated extension outperforms the simple extension.\nThroughout this paper, we will focus on explaining two specific algorithms, namely, IRS.FH and IRS.V-Zero, instead of describing the general framework."}, {"title": "2 Problem Formulation and Preliminaries", "content": "We consider a Bayesian budgeted MAB problem with K arms and a resource budget B. A problem instance can be specified by a tuple $(K, B, (c_a, R_a, \\Theta_a, P_a, \\mathcal{Y}_a, \\mathcal{Y}_{a,0})_{a\\in[K]})$ which will be described in a greater detail below.\nRewards and costs. Let $A = [K]$ be the set of arms, among which the decision maker (DM) can play one in each time period. The stochastic reward that the DM earns from the $n$th pull of arm a is represented with a nonnegative random variable $R_{a,n}$, and we assume that its distribution is given by $R_a(\\theta_a)$:\n$R_{a,n}\\sim R_a(\\theta_a), \\forall n = 1,2,...,$\nwhere $\\theta_a \\in \\Theta_a$ is the unknown parameter that the DM aims to learn. Given $\\theta_a$, the rewards $R_{a,1}, R_{a,2},...$ are independent.\nWhenever arm a is played, it also incurs a deterministic cost,\u00b9 denoted by $c_a \\in \\mathbb{N}$ (i.e., consumes $c_a$\nunits of resources deterministically). The total amount of resources that the DM can use is limited\nby $B\\in \\mathbb{N}$, and the DM's goal is to maximize the expected total reward within this budget constraint.\n\u00b9In development and analyses of our suggested algorithms, we primarily focus on the deterministic cost setting.\nThe main ideas naturally extend to random cost setting. See \u00a7 4."}, {"title": null, "content": "Bayesian framework. In the Bayesian framework, the unknown parameter $\u03b8_a$ is treated as a random variable and we assume that its prior distribution is given by $P_a(ya,0)$, i.e.,\n$\u03b8_a \\sim P_a(ya,0)$,\nwhere the hyperparameter $y_{a,0} \u2208 \\mathcal{Y}_a$, which we call (initial) belief, specifies the prior distribution.\nAs a Bayesian learner, the DM's belief about $\u03b8_a$ will be updated according to the Bayes' rule whenever a new reward realization from the arm a is observed. To describe the belief dynamics explicitly, we introduce a Bayesian update function $\\mathcal{U}_a : \\mathcal{Y}_a \u00d7 \\mathbb{R}_+ \u2192 \\mathcal{Y}_a$. That is, after playing the arm a for the first time, the belief is updated from $y_{a,0}$ to $y_{a,1} \u2261 \\mathcal{U}_a(y_{a,0}, R_{a,1})$ and then the posterior distribution of $\u03b8_a$ can be written as $P_a(y_{a,1})$. We accordingly define $y_{a,n}$ be the belief that the DM will have after playing the arm n times, i.e., $y_{a,n} = \\mathcal{U}_a(y_{a,n-1}, R_{a,n})$ for $n = 1, 2, . . ..$\nMean reward estimates. We denote the unknown mean reward of arm a by $\u03bc_a(\u03b8_a)$ as a real-valued function of parameter $\u03b8_a$:\n$\u03bc_a(\u03b8_a) = E[R_{a,n}|\u03b8_a]$.\nLet us denote its n-sample (Bayesian) estimate by $\\hat{\u03bc}_{a,n}(\u00b7)$ as a real-valued function of first n reward realizations: abbreviating $(R_{a,1},..., R_{a,n})$ as $R_{a,1:n}$, we define\n$\\hat{\u03bc}_{a,n}(R_{a,1:n}; y_{a,0}) \u2261 E_{\u03b8_a\\sim P_a(y_{a,0})}[\u03bc_a(\u03b8_a)|R_{a,1:n}]$,\nwhich represents the expected performance of arm a inferred from its first n reward realizations, or\nequivalently, the predictive mean reward of arm a that the DM would believe after playing the arm\nn times.\nThese mean-reward metrics $\u03bc_a$ and $\\hat{\u03bc}_{a,n}$ will be repeatedly used throughout the paper. The reason\nwhy we define $\u03bc_a$ and $\\hat{\u03bc}_{a,n}$ as functions is to clarify their dependencies on the random variables and\nto utilize their functional form when developing algorithms later. To help understanding, we make\nthe following remark.\nRemark 1 By Strong Law of Large Numbers, we have\n$\\lim_{n\u2192\u221e} \\hat{\u03bc}_{a,n} (R_{a,1:n} ; y_{a,0}) = \u03bc_a(\u03b8_a)$, a.s.,\nwhich says that, in terms of mean-reward estimation, knowing the parameter is equivalent to having\nan infinite number of observations. Also, for any n and k, it holds that\n$\\hat{\u03bc}_{a,n+k}(R_{a,1:n+k}; y_{a,0}) = \u03bc_{a,k}(R_{a,n+1:n+k}; y_{a,n})$,\nwhich says that making an inference using n+k samples given an initial belief is equivalent to making\nan inference using the later k samples after updating the belief using the former n samples.\nPolicy and performance. Let $\u03c0$ be the DM's policy, and $A_t$ be the arm played by $\u03c0$ at time t.\nThe reward that the DM earns at time t can be written as\n$r_t = R_{A_t, n_{A_t,t}} where n_{a,t} = \\sum_{s=1}^t 1\\{A_s = a\\}.$\nHere, $n_{a,t}$ counts the number of times that arm a has been played up to time t. An admissible policy should decide $A_t$ based only on the information revealed prior to time t, $(A_s, r_s)_{s=1}^{t-1}$.\nBesides, playing the arm $A_t$ consumes $c_{A_t}$ units of resources. To describe the budget constraint\nexplicitly, we introduce a stopping time $\u03c4$ representing the first time that the cumulative cost exceeds the given budget, i.e.,\n$\u03c4 \\coloneqq min\\{t: \\sum_{s=1}^t c_{A_s} > B \\}$"}, {"title": null, "content": "Only the rewards realized before time $\u03c4$ are counted, so the total reward collected by the DM can be written as $\\sum_{t=1}^{\u03c4-1} r_t$. As a trivial upper bound on $\u03c4$, we introduce $T_{max} \u2261 max_{a\\in A}\\{\\lfloor B/c_a \\rfloor + 1\\}$.\nWe denote by $V(\u03c0)$ the expected performance of policy $\u03c0$in a given MAB instance:\n$V(\u03c0) \\coloneqq E \\bigg[\\sum_{t=1}^{\u03c4-1} r_t\\bigg] = E \\bigg[\\sum_{t=1}^{\u03c4-1} \u03bc_{A_t}(\u0398_{A_t})\\bigg].$\nHere, the expectation operator takes into account the randomness of the policy (if randomized\nlike BTS), the reward realizations $R_{1:K,1:T_{max}}$, and the parameter realizations $\u0398_{1:K}$. Note that\n$V(\u03c0)$ can be alternatively represented as $E \\bigg[\\sum_{t=1}^{\u03c4-1} \u03bc_{A_t}(\u0398_{A_t})\\bigg]$ by the law of total expectation, since\n$E[r_t|A_t, \u0398_{1:K}] = \u03bc_{A_t}(\u03b8_{A_t})$.\nPerformance bound and regret. A quantity W is said to be a performance bound if $W \u2265 V(\u03c0)$\nfor any policy $\u03c0$.\nAs a performance bound commonly used in the MAB literature, $W^{BTS}$ is defined as\u00b2\n$W^{BTS} \u2261 B\u00d7max_{a\\in A} \\bigg\\{ \\frac{E[\u03bc_a(\u03b8_a)]}{c_a} \\bigg\\} = B\u00d7max_{a\\in A} \\bigg\\{ \\frac{E[\u03bc_a(\u03b8_a)]}{c_a} \\bigg\\} $\nThis quantity represents the expected performance of the clairvoyant fractional solution: when the player knows the parameters $\u0398_{1:K}$ in advance, it is optimal for him to play the arm $a^*$ with the largest reward-to-cost ratio $\u03bc_a(\u03b8_a)/c_a$, (fractionally) $B/c_{a^*}$ times in a row, which will yield the total reward of $E[\u03bc_{a^*}(\u03b8_{a^*}) \u00d7 B/c_{a^*}]$ (= $W^{BTS}$) in average. Clearly, no policy can perform better than this clairvoyant player, and therefore, $W^{BTS}$ is an upper bound on the maximal achievable performance for the given MAB instance.\nA performance bound W is said to be tighter than the other $W'$ if $W < W'$. A tighter bound provides a more precise quantification of the hardness of a particular MAB instance, and can better serve as a performance benchmark.\nOn the other hand, we will later utilize the Bayesian regret to visualize and compare the performance of policies, which is defined as\n$REGRET(\u03c0) \u2261 W^{BTS} \u2212 V(\u03c0)$.\nThe regret quantifies the suboptimality of a policy, and is non-negative since $W^{BTS}$ is a performance bound. Once we have a performance bound W tighter than $W^{BTS}$, the gap $W^{BTS} \u2212 W$ will provide a lower bound on the minimal achievable regret (i.e., $REGRET(\u03c0) > W^{BTS} \u2212 W$ for any $\u03c0$).\nBayesian optimal policy. In the Bayesian setting, there exists a policy that achieves the maximal performance $V^*$:\n$V^* \\coloneqq sup_\u03c0 V(\u03c0)$.\nSuch a Bayes-optimal policy and its performance $V^*$, in theory, can be obtained by solving the Bellman equation (corresponding to an MDP with a state space $\\mathcal{Y}_1 \u00d7 ... \u00d7 \\mathcal{Y}_K$ and an action space A. See Appendix A for the detail), but they are intractable in most cases.\nAs motivated in the introduction, our primary goal is to improve the BTS policy in terms of per- formance, where the Bayes-optimal policy will be our ideal target. Another goal is to improve the performance bound $W^{BTS}$ in terms of tightness, where $V^*$ will be our ideal target.\n\u00b2The naming $W^{BTS}$ is not common in the literature. The motivation for this choice is explained in \u00a73.1."}, {"title": "3 Algorithms", "content": "In this section, we propose a series of policies that improve Budgeted Thompson Sampling (BTS) to- ward the Bayes-optimal policy by leveraging the idea of information relaxation sampling. In parallel,"}, {"title": "3.1 Budgeted Thompson Sampling", "content": "As an immediate extension of Thompson Sampling to the budgeted MAB setting, BTS (Xia et al., 2015) utilizes the posterior sampling of the parameters. As described in Algorithm 1, the policy $\u03c0^{BTS}$ at each time t draws a random sample of the parameters from the posterior distribution (i.e., $\\theta_a^{(t)} \\sim P_a(y_{a,n_{a,t-1}})$ in line 4), and plays the arm with the largest reward-to-cost ratio given the sampled parameters (i.e., $argmax_a \u03bc_a(\u03b8_a^{(t)})/c_a$ in line 6). After observing the result of the play, it updates the belief about the arm according to the Bayes' rule (line 11), and repeats this procedure until the budget is exhausted."}, {"title": "3.2 IRS.FH", "content": "Our first proposed algorithm IRS.FH\u00b3 is very similar to BTS but additionally incorporates how many times each arm can be played in the future within the remaining budget. While the belief\n\u00b3IRS stands for Information Relaxation Sampling, and FH stands for Finite Horizon."}, {"title": "3.3 IRS.V-Zero", "content": "We consequently propose our next algorithm, IRS.V-Zero\u2074, that further improves IRS.FH by solving a more complicated optimization problem in each time period. It takes into account not only how many times each arm can be played, but also how the belief changes over the course of future plays."}, {"title": "3.4 Generalization", "content": "Note that all of three policies, $\u03c0^{BTS}$, $\u03c0^{IRS.FH}$, and $\u03c0^{IRS.V-Zero}$, share the following structure in com- mon: they in each time period (i) randomly generate future information via posterior sampling, (ii) optimize their decision to this randomly generated future via solving a deterministic optimization problem (referred to as inner problem), (iii) play an arm according to the optimized decision, and up- date the belief according to Bayes' rule. Their corresponding performance bounds, $W^{BTS}$, $W^{IRS.FH}$, and $W^{IRS.V-Zero}$, can be obtained by solving the same inner problems, not with the sampled future realizations, but with the true future realizations.\nThe information relaxation sampling (IRS) framework formally generalizes this structure with the notion of information relaxation penalties. Deferring its detailed description to Appendix A, we briefly remark that IRS unifies BTS and the Bayesian optimal policy (OPT) into a single framework, and also includes IRS.FH, IRS.V-Zero, and IRS.V-EMax as special cases that interpolate between BTS and OPT.\nEach policy-bound pair is characterized by inner optimization problem: from BTS to OPT, they introduce increasingly complicated optimization problems, becoming more considerate but more computationally costly. We indeed observe and (partly) prove that these policies achieve increasingly better performance and these performance bounds achieve increasingly better tightness.\nIn addition, we also implement and evaluate IRS.INDEX policy, which, strictly speaking, does not belong to IRS framework (it does not have a corresponding performance bound). It internally utilizes IRS.V-EMax to obtain a random approximation of the Gittins index. See Appendix A for the detail."}, {"title": "4 Extension to Random Cost", "content": "We have so far developed our framework for deterministic cost setting. In this section, we extend IRS framework to random cost setting, in which each arm consumes a random amount of resource whenever played and this random cost is drawn from an unknown distribution that we also aim to learn. More specifically, the stochastic cost that the DM pays for the nth pull of arm a is represented with a nonnegative random variable $C_{a,n}$. Every notation is analogously defined for costs, while we use superscript $`c$ (or $^r$) to represent the parameters/variables related to costs (or rewards, respectively): e.g., the distribution of $C_{a,n}$ is given by $C_a(\\theta_a^c)$, where $\\theta_a^c$ is the unknown parameter for which we have a prior $P_a(ya,o)."}, {"title": null, "content": "IRS algorithms can be extended to random cost in multiples ways. We here explore two ideas - a simple extension that uses the sampled mean cost, and a bit more complicated extension that uses the sampled future cost realizations and introduces additional penalties.\nSimple extension As described in Xia et al. (2015), BTS applied to the random cost setting draws the parameters \u03b8's from the posterior, and selects the arm with the largest mean-reward-to- mean-cost ratio: i.e., $\\arg \\max \\frac{\u03bc_a(\\theta_a^r)}{\u03bc_a(\\theta_a^c)}$. Analogously, we motivate simple extensions of IRS policies that solve the same inner problems to the deterministic cost setting but use $\u03bc_a(\\theta_a^c)$ instead of $c_a$.\nExtension with additional penalties In the deterministic cost setting, we have motivated IRS polices by relaxing the information constraint imposed on reward realizations. Similarly, we can consider to relax the information constraint imposed on cost realizations. That is, we can let a policy to sample the future cost realizations in addition to the future reward realizations and solve some deterministic optimization problem with respect to this sampled future but in the presence of penalties for letting the DM exploit the future information. A penalty function suitable for IRS.V-Zero can be designed as follows.\u2075\nThe penalty function of IRS.V-Zero is given by\n$Z^{IRS.V-Zero}(a_{1:t}, \u03c9) \\coloneqq r_t(a_{1:t},\u03c9) \u2212 E_y[r_t(a_{1:t}, \u03c9)|\\mathcal{H}_{t\u22121}(\u03b8_{1:t\u22121})].$\nThis penalizes the DM for knowing the future reward realizations, and similarly, we can add an extra term that penalizes the DM for knowing the future cost realizations:\n$Z^{IRS.V-Zero}(a_{1:t}, \u03c9) \\coloneqq r_t(a_{1:t},\u03c9) \u2212 E_y[r_t(a_{1:t}, \u03c9)|\\mathcal{H}_{t\u22121}(\u03b8_{1:t\u22121})]\u2212 \u03bb(c_t(a_{1:t}, \u03c9) \u2212 E_y[c_t(a_{1:t}, \u03c9)|\\mathcal{H}_{t\u22121}(\u03b8_{1:t\u22121})])$.\nHere, $\u03bb \u2208 \\mathbb{R}$ supposedly captures the additional benefit that the DM can earn by knowing the\nactual cost realization at time t instead of its expected value. A natural choice of $\u03bb$ will be the dual variable associated with the budget constraint of the inner problem that IRS.V-Zero solves, i.e.,\n$\u03bb = max_a \u03bc(\u03b8^r_a)/\u03bc(\u03b8^c_a)$, the quantity reflects the additional benefit that the DM can earn when one\nunit of resource is additionally given. We consider an extended version of IRS.V-Zero policy that uses its sampled value, i.e., $\u03bb = \\max_a \u03bc(\u03b8^r_a)/\u03bc(\u03b8^c_a)$, resulting in the following inner problem:\n$\\max_{\u03b7_1,..., \u03b7_K} \\sum_{a=1}^K\\sum_{i=1}^{N_a} \\{\\hat\u03bc^r_{a,i-1} + \u03bb(c_{a,i} - \\hat\u03bc^c_{a,i-1})\\} \\text{s.t.} \\sum_{a=1}^K\\sum_{i=1}^{N_a} \\hat{C}_{a,i} \\le B.$\n\u2075We extended the penalty functions not only for IRS.V-Zero but also to IRS.V-EMax and IRS.INDEX policy. The detailed procedure of two extensions is implemented in Appendix B."}, {"title": "5 Analysis", "content": "We first provide a theoretical result showing that the performance bounds $W^{BTS}$ and $W^{IRS.FH}$ proposed in \u00a73 are valid upper bounds on the maximal achievable performance and incrementally tighter than the conventional benchmark."}, {"title": "Theorem 1 (Monotonicity of performance bounds)", "content": "For any Bayesian budgeted MAB, we have\n$W^{BTS} > W^{IRS.FH} > W^{IRS.V-Zero} > V^*$.\nThe formal proof of Theorem 1 is given in Appendix C. We briefly sketch the main idea as follows.\nRecall that each of these bounds represents the maximal performance that can be achieved by a clairvoyant player who has an access to some additional information that is supposed to be unknown, and therefore, it should be greater than $V^*$, the maximal performance of the non-clairvoyant player."}, {"title": null, "content": "In this line of thought, the gap $W \u2212 V^*$ can be understood as a quantity that measures how much additional benefit can be extracted by exploiting the additional information, which should decrease when less useful information is additionally given. This explains the monotonicity $W^{BTS} > W^{IRS.FH} > W^{IRS.V-Zero}$, which is formally proven via Jensen's inequality.\nOn the other hand, the improvements in the performance bounds ($W^{BTS} \u2192 W^{IRS.FH} \u2192 W^{IRS.V-Zero}$) naturally imply the improvements in their corresponding policies ($\u03c0^{BTS} \u2192 \u03c0^{IRS.FH} \u2192 \u03c0^{IRS.V-Zero}$). Recall that each of these policies mimics the behavior of the clairvoyant player using the self- generated future information, i.e., it plays an arm that would have been selected by the one who optimistically believes that the sampled future information is the ground truth. The gap $W \u2212 V^*$ now can be translated as a quantity that measures how overly optimistic the corresponding policy will behave. Hence, the policy associated with a tighter performance bound is less likely to make a decision that is overly optimized to a particular realization of future information, and avoids over-explorations more effectively.\nWe indeed observe in all our numerical experiments that the suggested policies monotonically im- prove BTS in terms of performance, i.e., $V(\u03c0^{BTS}) < V(\u03c0^{IRS.FH}) < V(\u03c0^{IRS.V-Zero})$. However, proving this monotonicity is very challenging, so we instead investigate the gaps between the performance of these policies and their corresponding performance bounds, and establish upper bounds on these gaps."}, {"title": "Theorem 2 (Suboptimality gap)", "content": "Consider a Bayesian budgeted MAB such that $R_a$ is a natural exponential family distribution specified by a log-partition function $A_a(\u03b8_a)$ and $P_a$ is given by its conjugate prior whose density function is of the form $\\exp(\\xi_a \u03b8_a \u2212 \u03bd_a A_a(\u03b8))$. Suppose that all the log-partition functions are L-smooth, i.e., $d\u03b8 A_a(\u03b8) \u2264 L, \u2200\u03b8_a \u2208 \u0398_a$, and $\u03bd_a = \u03bd, \u2200 a \u2208 A$. Then, for any B \u2265 2 max{c\u2081,..., cK}, we have\n$W^{BTS} \u2212 V (\u03c0^{BTS}) < \u22642\u221aL/\u221a\u03bd + \u221a2 log T_{max}\\bigg(\\frac{K}{\\nu}+2KT_{max}\\bigg)^{\\frac{1}{2}}\\frac{1}{\\sqrt{K}}$\n$W^{IRS.FH} \u2212 V (\u03c0^{IRS.FH}) < 2\u221aL/\u221a\u03bd + \u221a2 log T_{max}\\bigg(\\frac{K}{\\nu}+2KT_{max}\\bigg)^{\\frac{1}{2}}\\frac{1}{3\\sqrt{K}}$\n$W^{IRS.V-Zero} \u2212 V(\u03c0^{IRS.V-Zero}) < \u221aL/\u221a\u03bd + \u221a2 log T_{max}\\bigg(\\frac{K}{\\nu}+2\\sqrt{KT_{max}}\\bigg)^{\\frac{1}{2}}\\frac{1}{3\\sqrt{K}}$\nwhere $T_{max} \\coloneqq max_{a\\in A} \\{\\lfloor B/c_a \\rfloor + 1\\}$.\nTheorem 2 considers Bayesian budgeted MABs with natural exponential family distributions, which\ninclude the Beta-Bernoulli case (L = 1/2, $\u03bd$ = $\u03b1+\u03b2$) and the Beta-Binomial case (L = m/2, $\u03bd$ = ($\u03b1+ \u03b2$)/m). While all these suboptimality gaps have the same asymptotic order of $O(\\sqrt{KT_{max} log T_{max}})$, this result shows that IRS.FH and IRS.V-Zero make incremental improvements over BTS in the additional term and in the leading coefficient. The proof is given in Appendix D.\nNote that our analysis aligns closely with the regret lower bound analysis and the algorithm's regret upper bound analysis typically conducted in the MAB literature. Theorem 1 provides a tighter lower bound $V^*$ compared to the lower bound $W^{BTS}$ presented in other budgeted MAB-related studies. Theorem 2 highlights improvements in the suboptimality gap, distinct from the regret upper bound $W^{BTS} \u2212 V(\u03c0)$ noted in other budgeted MAB literature. The observed reduction in the suboptimality gap may be due to enhancements in $V(\u03c0)$, although it remains somewhat ambiguous whether these improvements are predominantly due to $W^\u03c0$ or $V(\u03c0)$. This ambiguity makes direct comparisons of $V(\u03c0)$ values challenging and renders the result less robust. Nevertheless, experimental evidence substantiates that notable improvements are also achieved in $V(\u03c0)$."}, {"title": "6 Numerical Experiments", "content": "We demonstrate the effectiveness of our proposed policies and performance bounds through nu- merical simulations. We consider deterministic cost setting with three MAB instances \u2013 (a) the Beta-Bernuolli MAB with two arms, (b) the Beta-Bernuolli MAB with five arms, and (c) the Beta- Binomial MAB with six arms as a real-world example arising in the online advertisement business. In each setting, we evaluate the empirical performance of IRS policies as well as their correspond- ing performance bounds, and also provide a comparison with KUBE (Tran-Thanh et al., 2012), UCB-BV1 (Ding et al., 2013), i/c/m-UCB (Xia et al., 2017), and a modified version of PD-BwK (Badanidiyuru et al., 2013) as competing benchmarks.\nFigure 1 visualizes the simulation results in these three settings where the x-axes represent the budget B. The solid-line curves report the regret of the policies (WBTS \u2212 V(\u03c0)), and the dashed- line curves report the regret lower bounds obtained with the performance bounds (WBTS \u2212 W). The run time of each policy is reported in the legend, representing the average time to complete a single run of simulation.\nBeta-Bernoulli MABs. We first examine a Beta-Bernoulli MAB instance with K = 2, (c\u2081, c\u2082) = (10,20), and \u03b1a = \u03b2a = 1,\u2200a \u2208 A, and report the result of 50,000 runs of simulation in Figure 1(a). When B = 2,000, BTS outperform all competing benchmarks by a large margin, from 32% (BTS's regret vs. PD-BwK's regret) up to 228% (BTS's regret vs. i/c/m-UCB & UCB-BV1's regret). Our proposed policies even further improve BTS: IRS.FH, IRS.V-Zero, IRS.V-EMax, and IRS.INDEX policies, respectively, achieve 8%, 18%, 44%, and 51% improvement over BTS in terms of reduction in regret. Furthermore, we can infer from the regret lower bound WBTS \u2013 WIRS.V-EMax (brown dashed-line curve) that no policy can achieve an improvement more than 74%, highlighting that IRS.INDEX policy is near optimal.\nWe next examine a Beta-Bernoulli MAB instance with K = 5, c1:5 = (2, 3, 10, 19, 20), and \u03b1a = \u03b2\u03b1 = 1, \u2200a \u2208 A, and report the result of 20,000 runs of simulation in Figure 1(b). IRS.V-EMax is excluded due to its computational inefficiency. The gaps between BTS and other benchmarks are even larger, and IRS.FH, IRS.V-Zero, and IRS.INDEX policies, respectively, achieve 6%, 15%, and 48% improvement over BTS.\nApplication to online advertisement budget allocation. We examine a Beta-Binomial MAB instance that represents a bandit task encountered by a company who wants to optimally allocate his marketing budget across a number of ad campaigns with unknown click-through-rates (CTRs). More specifically, the arms represent the campaigns available to this company, and playing an arm a means that the company decides to spend ca dollars on the campaign a on the next day which will create\n\u2076We also show that IRS algorithms are sufficiently scalable for random cost setting through numerical simulation.\nSee Appendix E for the detail."}, {"title": "7 Conclusion", "content": "We have proposed a series of algorithms for budgeted MAB that improve Thompson sampling uti- lizing the information relaxation. In their arm-selection procedure, they simulate Bayesian learner's belief dynamics with respect to the sampled future realizations, and by doing so they can take into account how much the decision maker can learn within the remaining budget constraint. As a byproduct, our framework produces performance bounds that provide better quantifications of possible improvement. While the main ideas are mostly adopted from Min et al. (2019), this paper highlights that the information relaxation technique is particularly effective for budgeted bandit tasks, in which finding an optimal balance between exploration and exploitation is critical.\nOur contribution may seem obvious, but it is far from trivial. Existing literature on Budgeted MAB did not consider the use of the remaining budget information at all, and its extension in the context of the IRS framework presented its application in more realistic and appropriate settings. Unlike classic MAB problems, the termination time (the total number of pulls, denoted by stopping time T in our proof) depends on the sequence of actions, which introduces additional challenges requiring careful theoretical analysis and complicates algorithm implementation.\nWe further extend the framework to the random cost setting. The adoption of the IRS framework naturally necessitates the inclusion of cost sampling. However, a challenge arises regarding the imposition of an information relaxation penalty on cost in this context. address this challenge, we propose introducing a dual variable for the budget constraint, algorithmically simplifying it to the posterior mean reward-cost ratio. This dual variable concept holds promise for extending the imposition of additional penalties beyond budget constraints, potentially encompassing scenarios such as bandit problems with multiple constraints."}, {"title": "A Information Relaxation Sampling (IRS) Framework", "content": "Additional notation. To describe IRS framework explicitly, we introduction some additional notation and redefine some notation defined in"}]}