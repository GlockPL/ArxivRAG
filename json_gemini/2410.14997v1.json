{"title": "Improving Pronunciation and Accent Conversion through Knowledge Distillation And Synthetic Ground-Truth from Native TTS", "authors": ["Tuan Nam Nguyen", "Seymanur Akti", "Ngoc Quan Pham", "Alexander Waibel"], "abstract": "Previous approaches on accent conversion (AC) mainly aimed at making non-native speech sound more native while maintaining the original content and speaker identity. However, non-native speakers sometimes have pronunciation issues, which can make it difficult for listeners to understand them. Hence, we developed a new AC approach that not only focuses on accent conversion but also improves pronunciation of non-native accented speaker. By providing the non-native audio and the corresponding transcript, we generate the ideal ground-truth audio with native-like pronunciation with original duration and prosody. This ground-truth data aids the model in learning a direct mapping between accented and native speech. We utilize the end-to-end VITS framework to achieve high-quality waveform reconstruction for the AC task. As a result, our system not only produces audio that closely resembles native accents and while retaining the original speaker's identity but also improve pronunciation, as demonstrated by evaluation results.", "sections": [{"title": "I. INTRODUCTION", "content": "Second-language (L2) English learners typically present accents and mispronunciations, which can greatly impact their communication proficiency. AC basically aims to change the accent of speech while retaining the information of content and speaker identity. However, this task remains particularly challenging due to the extreme lack of parallel corpora and the large variations in different speakers. This research focuses on investigating techniques to enhance AC models' capacity to improve pronunciation made by L2 speakers and devising appropriate evaluation measures for this purpose.\nConventional AC [1], [2] methods require reference utterances in the target accent during synthesis, limiting their applicability due to the challenge of obtaining linguistically identical references in different accents. This research, like recent studies [3], [4], focuses on reference-free AC, which converts accents without needing reference utterances during inference. Reference-free AC methods generally fall into two categories: autoregressive and non-autoregressive architectures.\nAutoregressive AC models, typically based on sequence-to-sequence (seq2seq) architectures, rely mostly on parallel data for training. However, such data-utterances from the same speakers in different accents is scarce, making it difficult to obtain. To address this, data augmentation techniques like text-to-speech (TTS) [5] or voice conversion [6] are used to generate ground-truth audio for non-native speakers. These ground-truth audios, originating from different speakers, often differ in duration and prosody compared to the original non-native audio. While the encoder-decoder model with attention mechanisms can help aligning inputs and outputs, the differences in length make these models less suitable for applications requiring precise time synchronization, such as video dubbing. Additionally, there are some autoregressive models [7], [8] that trained with non-parallel data, but still share similar drawbacks, including slow inference speeds and unstable attention mechanism.\nIn contrary, non-autoregressive AC leverages non-parallel data, which is abundant and diverse. To effectively utilize this data, these models are built on a disentangle-resynthesis framework. Here, speech is disentangled into separate features such as speaker identity, content, prosody, and accent, which are then recombined to generate the original waveform. Content features are often represented by bottleneck features (BNFs), extracted from self-supervised pre-trained models [9] (e.g., WavLM [10], Wav2vec [11], HuBERT [12]), ASR bottlenecks [3], or ASR output in logits form [13]. However, BNFs also capture accent-related feature and non-native pronunciation issues. While methods like adversarial training [13]or Pseudo-Siamese networks [3] attempt to disentangle accent information, they still struggle to improve pronunciations due to the absence of ground-truth.\nBuilding on the strengths and limitations of existing AC models, we propose a novel framework for training a non-autoregressive AC model using generated parallel data. We hypothesize that a TTS system trained solely on native speech will produce accent-independent linguistic representations. Additionally, this native TTS system is able to generate ideal ground-truth data for non-native speakers, ensuring native pronunciation, same speaker identity, duration, prosody, and precise alignment with the original non-native audio. The accent-independent linguistic representations learned by the native TTS model not only facilitate ground-truth generation but also distill knowledge into our AC model, aiding in the learning of accent-independent features.\nPrevious studies [7], [14] have used native TTS models to guide AC models, often by sharing the same decoder and distilling knowledge from the TTS text encoder to the AC encoder. A key challenge in these approaches is aligning the outputs of the TTS and AC encoders due to their differing lengths. In [7], both models use the autoregressive Tacotron [15] architecture, necessitating an external unstable attention mechanism for alignment. Another approach [14] employs the non-autoregressive FastSpeech 2 [16] TTS model. In this approach, alignment is achieved through an external length regulator based on Montreal Force Alignment, which uses a GMM-HMM ASR model to upsample the TTS encoder's output, enabling both encoders to produce outputs of similar length. This latter method closely aligns with our proposed approach. These approaches use a separate vocoder to convert mel-spectrograms into waveforms.\nThe non-autoregressive VITS framework has proven successful in both TTS [17] and Voice Conversion [9]. Inspired by this success, we adopt a modified VITS framework for our AC system due to several advantages. VITS uses internal monotonic alignment, simplifying audio-text alignment and improving training efficiency. Additionally, as an end-to-end architecture, VITS eliminates the need for a separate vocoder to convert mel-spectrograms into waveforms, setting it apart from previous AC methods.\nOverall, to further enhance the AC model's training process, we introduced several key improvements. In the first stage, we simultaneously pre-train the AC model while training the VITS-based native TTS, allowing the AC model to learn the distribution of native audio data comprehensively. After training the native VITS-based TTS, we leverage it to generate ideal ground-truth data for non-native audio, as described in the next section. In the second stage, we fine-tune the pre-trained AC model using the non-native input alongside the generated ground-truth output and also distillate knowledge from the native TTS. In summary, we train the VITS-based TTS model to assist in initializing the weights, generating ideal ground-truth data, and facilitating knowledge distillation for the AC model."}, {"title": "II. METHODOLOGY", "content": "In the first stage, we pre-train the AC model and train the native TTS model using the VITS framework. VITS [17] is a conditional variational autoencoder architecture augmented with normalizing flow [18]. It consists of three main components: a posterior encoder, a prior encoder, and a waveform generator. These modules encode the distributions $q_{\\phi}(z|x)$, $p_{\\theta}(z|c)$, and $p_\\psi(y|z)$, respectively. Here, $q_{\\phi}(z|x)$ and $p_\\psi(y|z)$ represent the posterior and data distributions, parameterized by the posterior encoder $\\phi$ and HiFi-GAN [19] waveform generator $\\psi$, where $x$ is the speech input,$z$ is the latent variable and $y$ is the waveform output. The prior distribution of $z$ is defined as $p_{\\theta}(z|c)$, parameterized by the prior encoder $\\theta$ and refined through a normalizing flow $f$, where the latent variables are conditioned on the input $c$, which can be either audio or text.\nOur AC and TTS models share the HiFi-GAN decoder, posterior encoder, speaker encoder, F0 encoder and normalizing flow, as shown in Fig. 1a. The posterior encoder takes linear spectrograms $X_{lin}$ as input to sample a latent variable $z$. This latent $z$, along with speaker embedding $g$ from speaker encoder and F0 sequence embeddings $F0$ from F0 encoder, are fed into the HiFi-GAN vocoder to generate the waveform. The AC and TTS models each have separate prior encoders. The AC model's prior encoder audio includes a content encoder which extracts speech features and a bottleneck extractor to generate a normal distribution out of BNFs. Previous works have utilized self-supervised pre-trained representations [9] or ASR features (bottleneck or logits) [13] as content encoder outputs. These features contain information related to content, speaker, and accent. To filter out unwanted information, the BNFs are processed by the bottleneck extractor. The TTS model's prior encoder consists of a text encoder $\\theta_{text}$, which takes the transcript as input. Monotonic Alignment Search (MAS) aligns the text encoder output with the normalizing flow output, to match the differing lengths for text representations and audio representations.\nThe training loss is split into Variational Auto-encoder (VAE) loss and Generative Adversarial Network (GAN) loss. GAN loss includes adversarial loss $L_{adv}(D)$ and $L_{adv}(G)$ for the discriminator and generator, along with feature matching loss $L_{fm}(D)$ for the generator $G$ . VAE loss consists of reconstruction loss $L_{rec}$ (L1 distance between target and predicted mel-spectrogram) and $L_{kl}$ (KL divergence between the prior and the posterior). To address training-inference mismatch of VITS, we integrate the full end-to-end inference during training to improve voice quality. Specifically, L1 distance between ground-truth $x^{mel}$ and inference output $x^{mel}_{e2e}$ generated by $p_{\\psi}(x|f^{-1}(z'), g, F0)$, where the latent $z'$ is sampled from $P_{\\theta_{audio}}(z|C_{audio})$ or $P_{\\theta_{text}}(z|C_{text})$, and both $g$ and $F0$ are extracted from speaker encoder and F0 encoder respectively, is added to reconstruction loss. KL loss $L_{kl}$, which sums the KL divergence between the prior distributions $P_{\\theta_{text}}(z|C_{text})$ and $P_{\\theta_{audio}}(z|C_{audio})$, and the posterior distribution $q_{\\phi}(z|x)$. The overall training loss for pre-training phase can be expressed:\n$L_{kl} = KL(q(z|x)||P_{\\theta_{text}}(z|C_{text}))+KL(q_{\\phi}(z|x)||P_{\\theta_{audio}}(z|C_{audio}))$\\\n$L_{recon} = ||x^{mel} - \\hat{x}^{mel}|| + ||x^{mel} - x^{mel}_{e2e}||$\n$L(G) = L_{rec} + L_{kl} + L_{adv}(G) + L_{fm}(G)$\n$L(D) = L_{adv} (D)$"}, {"title": "B. Generating ideal ground-truth", "content": "The native TTS model trained earlier is used to generate ground-truth audio for each non-native input. We start by sampling the latent variable $z$ from the posterior distribution of the non-native audio $x$. We use MAS to find the alignment $A$ to maximize the log-likelihood of $f(z)$ on the distribution $P_{\\theta_{text}}(z'|C_{text}, A)$, which is used to upsample the text input.\n$z~ q_{\\phi}(z|X_{non-native}) = N(z; \\mu_{\\phi}(x), \\sigma_{\\phi}(x))$\n$A = argmax log N(f(z); \\mu_{\\theta_{text}}(C_{text}, A), O_{\\theta_{text}} (C_{text}, A))$\n$C_{upsample} = upsampling(C_{text}, A)$"}, {"title": "C. Finetuning AC model with ideal ground-truth and knowledge distillation from native TTS", "content": "In the second stage, we continue fine-tuning the AC model using synthetic ground-truth data. The primary objective is to enable the model to generate ideal native output from non-native input. Specifically, the content encoder processes the non-native input, while other components take the synthetic ground-truth, as illustrated in Figure 1b. During this process, we freeze the parameters of all components except the bottleneck extractor and HiFi-GAN decoder. The bottleneck extractor is fine-tuned to capture accent-independent content representations from non-native inputs. To facilitate this, we introduce a distillation loss into the training, which is the KL divergence between the two prior distributions $P_{\\theta_{text}}(z|C_{text})$ and $P_{\\theta_{audio}}(z|C_{audio})$. The training loss during the fine-tuning stage is defined as:\n$L_{distill} = KL(p_{\\theta_{audio}}(Z|C_{audio})||P_{\\theta_{text}}(Z|C_{text}))$\n$L(G) = L_{rec} + L_{kl} + L_{distill} + L_{adv}(G) + L_{fm}(G)$"}, {"title": "D. Overall model architecture detail", "content": "Our posterior encoder, HiFi-GAN decoder, normalizing flow, and text prior encoder follow the original VITS framework [17]. For the content encoder, we fine-tuned a pretrained Wav2vec 2.0 model on the Librispeech and L2Arctic dataset using CMU-dict phoneme labels, utilizing the bottleneck of the final layer as the content representation. Afterward, the content encoder's parameters are frozen. To get better text-audio alignment, we input phoneme labels instead of text into the text encoder. Both bottleneck extractor and text encoder are the same as the VITS Transformer-based text encoder. We retain Wav2vec 2.0's downsampling rate of 320 for calculating mel-spectrograms, F0, and linear spectrograms, while the HiFi-GAN decoder uses a corresponding upsample rate of 320. F0 sequences are derived using the YAAPT algorithm with the same downsampling rate, while the F0 encoder is based on [20]. We employ the pre-trained speaker encoder from [21]."}, {"title": "E. Inference", "content": "Our framework supports inference with or without a transcript. Without a transcript, the model extracts content information from non-native audio using content encoder and the bottleneck extractor in the prior encoder, then generates a waveform from the latent variable sampled from the prior distribution, conditioned on the speaker and prosody embedding. When a transcript is provided during inference, the system can utilize the prior text encoder instead of the audio encoder, as described in Section II-B, to genarate output with native pronunciation. This dual inference approach enhances inference quality, particularly when a transcript is available."}, {"title": "III. EXPERIMENT AND RESULT", "content": "We use the LJspeech dataset [22], which features consistent pronunciation from a single native speaker. Using FreeVC [9], a voice conversion model, we generate multispeaker native-accented utterances from the original voices. This augmented multi-speaker dataset is utilized, enabling the system to be trained in a multi-speaker setting during the first stage. In the second stage, we fine-tune the AC model on the L2ARCTIC dataset [23], featuring 24 accented speakers across 6 accents. For each accent, we select 3 speakers for training and the remaining speakers for testing. Each speaker's utterances are split into a training set of 1,032 non overlap utterances, a validation set of 50, and a test set of 50. The test set, chosen using our competive ASR model [24], focuses on utterances with the high average WER (larger than 10) across all speakers, with assumption that the higher WER rates mean stronger accents."}, {"title": "B. Evaluation metrics", "content": "1) Subjective tests: Nativeness and Speaker Similarity Tests: Ten participants conducted two evaluations using a 5-point scale (1-bad, 2-poor, 3-fair, 4-good, 5-excellent). In the Nativeness test, they rated how closely the converted audios resembled native speech. In the speaker similarity test, participants assessed the similarity between the voice identity of the original input and the converted audio, providing a score out of 5 for each.\n2) Objective tests: Word error rates (WER), Accent classifier accuracy (ACC) and Speaker Embedding Cosine Similarity (SECS): To evaluate the improvement in pronunciation, we use the Word Error Rate (WER) from our competitive seq2seq Transformer ASR model [24]. A lower WER indicates better intelligibility in the conversion process. SECS is employed to measure speaker similarity by computing the cosine similarity between speaker embeddings of the original speech and the converted speech. We utilize the state-of-the-art speaker verification model from [10] to extract speaker embeddings, which suggests that a cosine similarity greater than 0.85 indicates that both audios likely originate from the same speaker.\nAdditionally, we train an accent classifier to determine whether an input audio is native or non-native. The classifier architecture and training setting is similar to [6]. We compute two accuracy scores, one for original non-native audio set and one for converted native audio set. A better AC is expected to show a larger gap between these two ACCs."}, {"title": "C. Experimental setup", "content": "Trained on both native and non-native audio with ASR loss, the content encoder's final layer yields representations closely aligned with linguistic content. This makes it more accent- and speaker-independent, facilitating accurate mapping from non-native pronunciation to the correct linguistic information. Therefore, we use the model which is pre-trained in the first stage as the baseline. The proposed model is the model fine-tuned in the second stage. To assess the effectiveness of fine-tuning on the synthetic ground-truth data, we train a variant that only uses knowledge distillation loss $L_{distill}$ between the text and audio encoders in the second stage. Additionally, the audio quality of the Synthetic ground-truth is also evaluated, allowing us to assess the system's performance when the correct transcript is provided. The training hyperparameters are set similarly to those in the original VITS, with the system trained for 600,000 steps in the first stage and 200,000 steps in the second stage. Sample evaluation audios are available at a github repository 1"}, {"title": "D. Result", "content": "The objective metric evaluation, shown in Table II, indicates that the proposed method outperforms others in terms of WER. This demonstrates that the method successfully improves pronunciation, making it closer to native speech. Even without synthetic ground-truth, the model still benefits from knowledge distillation from the native TTS, although the pronunciation improvement is smaller. The synthetic ground -truth, generated from transcripts, has a WER of 5.1, confirming the high quality of these ground-truth audios. Additionally, SECS remains stable between 0.82 and 0.84 across all settings, verifying that the speaker identity is well-preserved in all methods. In terms of ACC, the results are not as strong as other metrics. Since we aim to preserve the prosody of the original audio, which can sometimes indicate the accent of the speaker, the model occasionally identifies the converted audio as non-native. This prosody retention may lead to less effective accent conversion in some cases. The subjective metric evaluation, also shown in Table I, indicates that our proposed method performs best in the nativeness test, while the Sim-MOS results, like in the objective evaluation, remain similar across all setups."}, {"title": "IV. CONCLUSION", "content": "In this work, our work presents a significant advancement in accent conversion and native-like pronunciation correction. Through the introduction of a two-stage training process that leverages VITS framework, we enable the AC model to better capture the accent-independent feature of speech while maintaining the speaker's identity. The use of generated ground-truth data, alongside knowledge distillation from native TTS, further enhances the system's ability to correct non-native pronunciations effectively. Objective and subjective evaluations demonstrate that our approach not only improves accent conversion but also addresses pronunciation issues, providing a more natural and comprehensible output. Future work may explore further refinement in emotion, prosody and accent feature to build upon these promising results."}]}