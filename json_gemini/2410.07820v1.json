[{"title": "Mitigating Gender Bias in Code Large Language Models via Model Editing", "authors": ["Zhanyue Qin", "Haochuan Wang", "Zecheng Wang", "Deyuan Liu", "Cunhang Fan", "Zhao Lv", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "abstract": "In recent years, with the maturation of large lan- guage model (LLM) technology and the emergence of high- quality programming code datasets, researchers have become increasing confident in addressing the challenges of program synthesis automatically. However, since most of the training samples for LLMs are unscreened, it is inevitable that LLMs' performance may not align with real-world scenarios, leading to the presence of social bias. To evaluate and quantify the gender bias in code LLMs, we propose a dataset named CodeGenBias (Gender Bias in the Code Generation) and an evaluation met- ric called FB-Score (Factual Bias Score) based on the actual gender distribution of correlative professions. With the help of CodeGenBias and FB-Score, we evaluate and analyze the gender bias in eight mainstream Code LLMs. Previous work has demonstrated that model editing methods that perform well in knowledge editing have the potential to mitigate social bias in LLMs. Therefore, we develop a model editing approach named MG-Editing (Multi-Granularity model Editing), which includes the locating and editing phases. Our model editing method MG-Editing can be applied at five different levels of model parameter granularity: full parameters level, layer level, module level, row level, and neuron level. Extensive experiments not only demonstrate that our MG-Editing can effectively mitigate the gender bias in code LLMs while maintaining their general code generation capabilities, but also showcase its excellent generalization. At the same time, the experimental results show that, considering both the gender bias of the model and its general code generation capability, MG-Editing is most effective when applied at the row and neuron levels of granularity.", "sections": [{"title": "I. INTRODUCTION", "content": "Programming serves as a powerful and pervasive tool for problem-solving. The development of systems capable of assisting programmers or autonomously generating programs has the potential to make programming more productive and accessible. Recently, code large language models (LLMs), like Meta's CodeLlama [1] and Salesforce's CodeGen [2], have shown a remarkable capacity to generate code by be- ing pre-trained on extensive codebases [3\u20136]. These code LLMs show great promise across a range of programming applications, including front-end development [7\u20139], back-end services [10, 11], and data processing [12-14].\nHowever, code LLMs are trained on vast codebases or corpora can inevitably learn biased information, leading to negative stereotypes and social biases encoded within the models [15] 1. For example, given the prompt: \u201cdef find_sick_people(people, ethnicity):\" under a 2-shots setting, the InCoder-6B [16], a code LLM is trained by Facebook, tends to define Hispanic people as disgusting and returns this result to users [17]. Aside from racial biases, code LLMs also exhibit varying degrees of bias related to gender, age, and occupation, with gender bias being the most prevalent and significant [18]. For example, given the prompt: \u201cfind_outstanding_nurses(nurses, gender):\u201d under a 2-shots setting, the CodeGen-2B- mono [16] assigns a probability of 73.32% to favoring females and a probability of only 1.19% to favoring males. If auto- mated code generation is performed using a code LLM with severe social biases, the generated code will inevitably contain these biases. Such generated code could have profoundly harmful social impacts, potentially leading to discriminatory treatment of affected groups. More seriously, since social bias in code are often hidden within complex algorithms and logic, identifying and addressing it is not easy, which requires not only programming and algorithm knowledge but also a deep understanding of the relevant field. [19, 20]\nTo address such a server bias issue, many studies have focus on developing fair and unbiased models to ensure that the benefits are distributed equitably across different segments of society. Ordered by the process of model training, previous work could be divided into the following three categories: (1) Debiasing methods in the pre-processing phase, which mainly focus on reducing the proportion of biases in the original dataset through data pre-processing methods, such as data augmentation, sample synthesis, and data cleaning [21- 26]. (2) Debiasing methods in the training phase, which modify the training process of the model through methods like altering the model architecture, adding a debiasing module to existing transformers, or adjusting the loss function [27\u201333]. (3) Debiasing methods in the post training phase, which adjust model outputs to identify and mitigate social biases without further weight optimization or dataset manipulation [34\u201336]. However, each of the three categories of debiasing methods mentioned above has its limitations when applied to code language models (LLMs). In detail, for these debiasing meth-"}, {"title": "II. PRELIMINARY: MODEL EDITING", "content": "The purpose of model editing is to modify a model \u0398 into a new model \u0398\u2295, replacing some existing knowledge to achieve the desired output while maintaining the integrity of the other knowledge irrelevant to the updates. We define F as the model editing algorithm, Yold as the knowledge that needs to be modified, Ynew as the modified knowledge and pmt as the prompt that guides the LLM to respond with the knowledge. The relationship among the above symbols is shown in the following Eqs. 1 and 2:\n$Y_{old} = arg max_{y'}(P(y'|pmt; \\Theta))$\n$\\Theta_{\\oplus} = F(\\Theta, Y_{old}, Y_{new})$\nAfter editing the model \u0398 using an excellent model editing method F, the edited model \u0398\u2295 should satisfy the following Eqs. 3, 4 and 5:\n$Y_{new} = arg max_{y'}(P(y'|pmt; \\Theta_{\\oplus}))$\n$Y_{new} = arg max_{y'}(P(y'|pmt; \\Theta))$\n$y_{kl} = arg max_{y'}(P(y' |p_{ki}; \\hat{\\Theta}))$"}, {"title": "III. PROBING GENDER BIAS IN CODE LLMS", "content": "In this section, you will see the construction of dataset CodeGenBias in the subsection III-A, and the definition of evaluation metric FB-Score in the subsection III-B. We also fully evaluate the current mainstream code LLMs: CodeGemma-2B, CodeGemma-7B, CodeGen-350M-mono, CodeGen-2B-mono, CodeLlama-7B-hf, CodeLlama-13B-hf, CodeLlama-34B-hf, and Stable-Code-3B with the help of CodeGenBias and FB-Score in the subsection III-C. We also present some experimental results on applying current model editing method in the subsection III-D."}, {"title": "A. Constructing the CodeGenBias Dataset", "content": "To better probe the gender bias in code LLMs, we construct a dataset named \"CodeGenBias\". In detail, we build \"Code- GenBias\" by taking the Cartesian product of 320 different professions and 5 different types of modifiers to fill a given template. The template contains two fully completed demon- strations (find_best_apples and find_sick_dogs) and one demonstration with placeholders that needs to be filled"}, {"title": "B. Measuring the Degree of Gender Bias via the FB-Score", "content": "\"The worst form of inequality is to try to make unequal things equal.\"\n-Aristotle\nAs Aristotle noted, absolute fairness can lead to unfair out- comes. Therefore, conventional metrics of absolute fairness, such as CBS [17, 18], may not be ideal or appropriate for accurately assessing gender bias. To avoid this issue, we intend to assess the model's gender bias by comparing the model's gender tendencies with the gender distribution in relevant profession in the real world. Thanks to the excellent work of Bolukbasi et al [37], we propose the following heuristic evaluation metric: Factal Bias Score (FB-Score):\n$FB\\text{-}Score = |p(\\text{``he"}, "pmt; \\Theta) - f_{he}| + |p(\\text{``she"]}, {}, {"title": "C. The Performance of Various Code LLMs on the CodeGen-Bias", "content": "To provide a clear benchmark for subsequent researchers, we select various current mainstream transformer-based code generation LLMs: CodeGemma-2B [53], CodeGemma- 7B [53], CodeGen-350M-mono [2], CodeGen-2B-mono [2], CodeLlama-7B-hf [1], CodeLlama-13B-hf [1], CodeLlama- 34B-hf [1], and Stable-Code-3B [54] to conduct our large- scale evaluation experiment. The experimental results are shown in the table III-C. Please note: the best result for each column is shown in bold, and the second result is marked with underlined.\nFrom the table III-C, we can find that: (1) No matter what kind of modifiers, the CodeGemma-2B shows the lowest gender bias, its average value was as low as 0.4223. (2) The Stable-Code-3B maintains the second lowest FB-Score, with average score of 0.5410. (3) The performance of CodeGen- 350M-mono is the worst among all the eight code LLMs, with average score of 0.9798."}, {"title": "D. Limitations of Existing Model Editing Methods on the CodeGenBias", "content": "According to the Eqs. 3 and 4, we can observe that the objective of existing model editing methods is to maximize the probability of Ynew as much as possible. However, this approach does not align with our definition of mitigating gender bias in code LLMs. From our FB-Score, it is evident that we aim to align the model's gender inclination with that of the real world, rather than counterfactual gender bias.\nTo verify the limitations of existing model editing methods, we use the classic model editing method: ROME [43], and apply it to CodeLlama-7B, CodeLlama-13B, and CodeLlama- 34B. The experimental results are shown in the table III-D.\nFrom the experimental results, it can be seen that after ROME editing, CodeLlama-7B, CodeLlama-13B, and CodeLlama-34B exhibit catastrophic performance on FB- Score, with their average FB-Score values increasing by 0.2576, 0.3842, and 0.2877, respectively. In the case of CodeLlama-7B, after ROME editing, given the prompt \"find_better_senators(senators, personal_pronoun)\u201d, the model is in a 0.9870 probability of favoring females and only a 0.0051 probability of favoring males. It is this extreme counterfactual bias that led to the aforementioned catastrophic phenomenon."}, {"title": "IV. MULTI-GRANULARITY MODEL EDITING METHOD", "content": "We believe that mitigating gender bias in code LLMs should not focus solely on achieving fairness or counterfactual bias. Instead, it should aim to control the predictions of LLMs to reflect real-world conditions. The optimal model editing method F* should satisfy the following Eq. 9:\n$F^* = arg min_{F}(|p(\\text{``he"}, "p(\\text{``she", "f_{she}|)$", {"title": "A. The Locating Phase", "content": "We agree with Yu et al.'s [31] assertion that neural networks typically contain highly active sub-networks that can be trained separately to solve tasks, which is also the basis of Lottery Ticket Hypothesis [55]. Base on that, We can naturally hypothesize that certain parameters within LLMs determine the code LLM's exhibition of gender bias. Our goal in locating these parameters is to identify them at five different levels of granularity: full parameters, layer, module, row, and neuron.\nFull Parameters Level: In such a level, we reckon that all parameters with a given code LLM are related to the code LLM's exhibition of gender bias. Therefore, we do not need to locate the key parameters.\nLayer Level: The figure 2 (A) illustrates the inference process of a code LLM. In simplified terms, LLMs consist of multiple layers with identical structures and a softmax layer. After passing through the embedding layer, the model input are transformed into the initial hidden state. Then, the hidden state sequentially pass through layers of the LLMs and continuously change. The final hidden state is converted by the softmax layer into a probability distribution at the vocabulary space with a length corresponding to the vocabulary size of the LLMs.\nHow can the importance of these structurally identical layers in code LLMs be quantified? A straightforward idea is to compare the hidden states before (H(i-1)) and after (H(i)) passing through the sub-layer L(i). However, directly com- paring H(i-1) and H(i) can not effectively disentangle bias factors and other confounding factors, since H(i\u22121) and H(i) encode all input information. Following previous work [50], we use the softmax layer to project the hidden states before H(i-1) and after H(i) into the vocabulary space as p(i-1) and p(i). Based on that, we could measure the importance of the layer L(i) via computing the following L1-distance:\n$Importance(L_i) = |p^{(i)} [\\text{``he"}, ["text{``he"], ["text{``she"], ["text{``she"], "},\n    {", "title", "B. The Editing Phase", "content", "The editing part of our model editing method MG-Editing involves fine-tuning the identified parameters, which exhibit the highest importance score. As we mentioned earlier, our goal is to align the LLMs with the gender distribution of occupations in the real world. Therefore, we propose the following heuristic loss (Eqs.14, 15, 16, and 17) to this end", "p(\\text{``he", "p(\\text{``she", "Theta)$", {"title": "V. EXPERIMENTS", "content": "In order to verify the generalization of our MG-Editing, we select three widely used code LLMs, CodeGen-2B-mono [2], CodeGen-350M-mono [2] and Stable-Code-3B [54], to con- duct experiments. We used three different types of traditional evaluation metrics to assess the model editing effect:"}, {"title": "C. The Results of the Editing", "content": "The results of model editing for these three code LLMs CodeGen-2B-mono, CodeGen-350M-mono, and Stable-Code- 3B at five different granularities are shown in the table IV-B, table IV-B, and table V-C, respectively. The best results for each column are shown in bold font, and the second-best results for each column are shown with underscores.\nFrom the second row of the table IV-B, we can find that: the performance of full parameters granularity is the best among all granularities in Reliability-FB-Score and Generality-FB- Score, with scores of 0.1754 and 0.2144, respectively. But its general code generation capability is completely lost, with pass@1 and pass@10 dropping to 0.0324 and 0.0328, respectively. This aligns with our previous conclusion that in- discriminate parameter fine-tuning of LLMs can lead to model collapse. Considering the FB-Score and the model's general code generation capability, CodeGen-2B-mono performs the best at both row-level and neuron-level granularities. At the row-level granularity, its four evaluation metrics are 0.3334, 0.3681, 0.2108, and 0.3010, respectively. At the neuron-level granularity, its four evaluation metrics are also 0.3249, 0.3573, 0.1845, and 0.2500, respectively.\nAccording to the table IV-B and table V-C, the model editing results of CodeGen-350M-mono and Stable-Code-3B are consistent with those of CodeGen-2B-mono in terms of full-parameter granularity: although the lowest FB-Score can be obtained, the cost is almost complete loss of the basic code generation ability. Considering both FB-Score and the LLMs' ability to generate code, CodeGen-350M-mono and Stable-Code-3B, like CodeGen-2B-mono, show the best overall performance of MG-Editing at both the row granularity and neuron granularity."}, {"title": "VI. RELATED WORK", "content": "With the rapid development of code LLMs, in order to achieve more benign and harmless code LLMs, an increasing number of researchers are beginning to focus on how to assess and quantify the level of social bias in code LLMs. Liu et al. [17] design a new code prompt construction paradigm. By constructing function signatures that include judgmental modifiers (such as 'disgusting') and demographic dimensions (such as 'ethnicity'), they successfully trigger social biases in the generated code. They also propose three evaluation metrics: Code Bias Score (CBS): used to reveal the overall severity of social bias in the generated code across all demo- graphic dimensions; UnFairness Score (UFS): used to reveal fine-grained unfairness between selected demographic groups; Standard Deviation (SD): calculating the standard deviation of the valid frequencies of all demographic dimensions to reveal overall unfairness. Huang et al. [18] propose a new bias testing framework specifically for code generation tasks. This framework uses Abstract Syntax Trees (ASTs) to extract function names, input parameters, and parameter values from the code, and then constructs test cases to analyze whether there is bias in the code."}, {"title": "VII. CONCLUSION", "content": "In this paper, we construct a dataset named CodeGenBias and an evaluation metric named FB-Score to assess gender bias in code LLMs. Using CodeGenBias and FB-Score, we test multiple code LLMs and establish a benchmark. Ad- ditionally, to align the code LLMs' gender bias with the gender distribution associated with prfessions in the real world, we propose a new model editing method called MG-Editing and apply it to code LLMs at five parameter granularities: full parameters, layer, module, row, and neuron. Extensive experiments demonstrate that MG-Editing achieves the best overall performance at row and neuron granularities, and that MG-Editing exhibits great generalization across different model sizes and model architectures."}]