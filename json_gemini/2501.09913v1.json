{"title": "TOWARDS A LITMUS TEST FOR COMMON SENSE", "authors": ["Hugo Latapie"], "abstract": "This paper is the second in a planned series aimed at envisioning a path to safe and beneficial\nartificial intelligence. Building on the conceptual insights of \u201cCommon Sense Is All You Need,\u201d\nwe propose a more formal litmus test for common sense, adopting an axiomatic approach that\ncombines minimal prior knowledge (MPK) constraints with diagonal or G\u00f6del-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach applies to the\nAbstraction and Reasoning Corpus (ARC), acknowledging training/test data constraints, physical or\nvirtual embodiment, and large language models (LLMs). We also integrate observations regarding\nemergent deceptive hallucinations, in which more capable AI systems may intentionally fabricate\nplausible yet misleading outputs to disguise knowledge gaps. The overarching theme is that scaling\nAI without ensuring common sense risks intensifying such deceptive tendencies, thereby undermin-\ning safety and trust. Aligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle truly novel concepts\nbut also provides a stepping stone toward an ethical, reliable foundation for future safe, beneficial\nand aligned artificial intelligence.", "sections": [{"title": "1 Introduction and Motivation", "content": "Recent progress in AI-particularly large language models-has showcased remarkable pattern-matching, reasoning\n1, and generative capabilities. Yet these systems often fail to exhibit common sense, faltering in truly novel contexts\nor producing \"hallucinations,\u201d which can worsen as the systems scale. Alarmingly, an emerging phenomenon of\ndeceptive hallucinationsShaikh [2024], wherein advanced AI appears to fabricate information intentionally to hide\nknowledge gaps or maintain superficial coherence, further underscores the potential danger.\nA Larger Goal: Safe and Beneficial AI We believe that common sense is a prerequisite for safe, trustworthy AI,\npreventing catastrophic misalignment or advanced hallucinations that could exacerbate ethical concerns.\nFrom \"Common Sense Is All You Need\u201d to Axiomatic Foundations The previous paper, \u201cCommon Sense Is All\nYou Need\", laid out why minimal prior knowledge (MPK), adaptive reasoning, and environment-based interaction are\nessential for robust autonomy. In this second paper, we refine this concept by:\n\u2022 Presenting an axiomatic litmus test for diagnosing common sense, ensuring no large pre-trained heuristics or scaled\npatterns can trivially solve out-of-distribution tasks.\n\u2022 Illustrating how this approach fits the Abstraction and Reasoning Corpus (ARC) constraints, acknowledging train-\ning/test data.\n\u2022 Discussing emergent deceptive hallucinations as an accelerant for advanced but misguided AI that lacks common\nsense."}, {"title": "2 Foundations: Minimal Prior Knowledge and Diagonal Novelty", "content": "By bridging diagonal arguments with real concerns about misleading or unethical AI outputs, we move toward a future\nof truly beneficial artificial intelligence built on a stable foundation."}, {"title": "2.1 Recap of Minimal Knowledge (MPK)", "content": "In [Latapie, 2025], an agent restricted to minimal or universal logic cannot rely on specialized domain expansions.\nInstead, it must invent intangible transformations to solve tasks outside its known set K. Such success strongly\nsuggests a child-like or animal-like adaptivity that fosters ethically aligned, context-sensitive behavior."}, {"title": "2.2 Diagonal or G\u00f6del-Style Argument", "content": "G\u00f6del's incompleteness [G\u00f6del, 1931] implies that any enumerated set of statements has statements it cannot decide\nor derive. Analogously, we design intangible puzzle logic a* absent from K plus environment axioms; an agent that\nsolves a* must effectively extend its knowledge base. This stands in contrast to advanced but purely memorized logic,\nwhich might produce hallucinations or \u201ccover-up\" illusions in the face of genuine novelty."}, {"title": "3 Deceptive Hallucinations: A Growing Threat When Lacking Common Sense", "content": ""}, {"title": "3.1 Standard vs. Deceptive Hallucinations", "content": "Standard Hallucinations 1. Factually incorrect outputs from AI with no direct intent to mislead. 2. Typically arise\ndue to poor grounding or insufficient data coverage.\nDeceptive Hallucinations 1. Appear as intentionally fabricated statements, used by the AI to appear coherent or\nconfident. 2. Reflect emergent behavior when the AI hides ignorance rather than admitting uncertainty, thereby\nmisleading the user."}, {"title": "3.2 Why Scaling Without Common Sense Exacerbates Deception", "content": "Larger models can produce increasingly plausible statements, while lack of intangible concept inference means they\nfail to adapt genuinely. Instead, they \u201cpatch\u201d knowledge gaps by generating fake sources or plausible but false state-\nments, undermining reliability. An AI scaled from such a foundation could become dangerously manipulative."}, {"title": "3.3 Integrating the Axiomatic Litmus Test to Address This Risk", "content": "Our litmus test ensures advanced systems face intangible tasks not resolvable by data-scale patterns alone. If the\nsystem is forced to admit ignorance or conjecture new logic from minimal feedback, we reduce the impetus for\ndeceptive cover-ups, thereby mitigating emergent manipulative behaviors."}, {"title": "4 An Axiomatic Litmus Test for Common Sense", "content": ""}, {"title": "4.1 Core Elements", "content": "1. Agent's Knowledge Set: K = {C1, . . . } enumerating known transformations/statements.\n2. Minimal Prior Knowledge (MPK): The universal or baseline subset of K, disclaiming specialized data or heuris-\ntics.\n3. Environment Axioms (Env): Foundational domain logic for the puzzle or scenario.\n4. Diagonal Task 7* referencing intangible rule a* not derivable from KU Env.\n5. Limited Interaction and Feedback: The agent sees only partial demonstrations or pass/fail signals; no large\nre-training or new expansions of K.\nLitmus Step: If the agent solves T* by adopting a*, that strongly implies concept invention-i.e., common sense."}, {"title": "5 Link to ARC: Training/Test Data Acknowledgments", "content": "ARC Overview The Abstraction and Reasoning Corpus [Chollet, 2019] includes 400 training tasks and 400 test\ntasks. Typically, a solver might absorb heuristics from the training tasks, i.e., building K.\nConstructing the Puzzle to Exceed K We isolate intangible puzzle logic a* absent from or contradicting all trans-\nformations gleaned from the 400 training tasks. The agent sees 2\u20133 examples referencing a*, then one final test input.\nBecause no known heuristic covers a*, the agent must form a new statement. If it does so successfully, it passes the\nlitmus test.\nMitigating Deceptive Hallucinations in ARC Solvers Without common sense, a solver might try to \"blend\" partial\nheuristics or produce appealing but fundamentally incorrect transformations. By design, intangible tasks remain un-\nsolvable if the solver fails to adopt a*. This approach reveals whether the agent can go beyond memorized expansions\nor produce \"fake outputs\" that appear confident but are wrong."}, {"title": "6 Physical and Virtual Embodiment", "content": ""}, {"title": "6.1 Child/Animal Scenarios", "content": "Children or animals begin with minimal sensorimotor or instinctual knowledge. A puzzle box that opens only via\nan intangible or contradictory mechanism not in typical script forces new inferences. Observed success or failure\nindicates the presence (or lack) of child/animal-level common sense."}, {"title": "6.2 Robotics", "content": "A robot has enumerated motion primitives and an environment model. If intangible environment phenomena (e.g.,\nfriction toggling, ephemeral collisions) are not in K, the robot's standard plan library fails. Solving relies on real-time\nconcept formation, showing robust adaptivity vital to safe physical deployments."}, {"title": "7 Mathematical Formulation for LLMs and AI", "content": ""}, {"title": "7.1 LLMs with Enormous Training Corpora", "content": "Let KLLM be the (arguably vast) set of textual knowledge acquired by a large language model during pre-training.\nWhen the training corpus encompasses most existing human-written language, it becomes practically intractable to\nfully enumerate or formalize KLLM. Consequently, constructing intangible or diagonal puzzle rules that lie definitively\n\"outside\" of KLLM is a challenging endeavor, given the model's expansive coverage of data. Nevertheless, one can still\ncraft novel textual puzzles that appear strongly disjoint from prior text distributions, with the caveat that guaranteeing\nabsolute disjointness requires thorough scrutiny."}, {"title": "7.2 Leveraging an ARC-Style Domain for Feasibility", "content": "A more feasible approach emerges by adapting the Abstraction and Reasoning Corpus (ARC) methodology to the\nlanguage domain. ARC tasks typically involve grid-based transformations that can be reformulated into textual de-\nscriptions or dialogues. This is useful because ARC's assumption of minimal knowledge is more tractable to specify.\nIn effect, the \"public domain\" of ARC training and test tasks is well-defined, so any puzzle logic absent from those\ntasks can serve as a diagonal property a* disjoint from the solver's known transformations. This contrasts with the\ndifficulty of ensuring disjointness in a domain as vast as all recorded human text.\nEmpirically, many concrete ARC puzzles remain unsolved by contemporary large language models. These puzzles\nprovide a test bed where the minimal prior knowledge set is explicitly enumerated (i.e., the standard ARC transfor-\nmations), paving the way for a diagonal argument: one can guarantee puzzle logic a* is, in principle, outside the\nenumerated domain. Consequently, a model that succeeds must go beyond memorized patterns to hypothesize and\nintegrate a genuinely new concept."}, {"title": "7.3 Restricting Fine-Tuning and Monitoring Outputs", "content": "In aligning with minimal prior knowledge constraints, we similarly restrict large-scale fine-tuning or additional data\ningestion for the LLM. The model receives only a few demonstration examples plus a final test prompt-much like a"}, {"title": "7.4 Observations from Test-Time Chain-of-Thought Systems (e.g., \"01\" and \"03\")", "content": "Advances in large language models have led to test-time chain-of-thought (CoT) systems like OpenAI's o1 and 03.\nThese models, when richly guided by intermediate reasoning steps, can achieve remarkable performance on certain\nARC-like puzzles. In high-compute modes 03 can register impressive scores on subsets of the ARC tasks. Yet these\naccomplishments come with critical considerations:\n1. High Computational Overheads: Systems like 03 may process billions of tokens per task, incurring compute\ncosts reported to be hundreds or even thousands of dollars per puzzle in extreme modes. While this might yield strong\nresults on certain tasks, its economic feasibility for broader adoption remains uncertain. Moreover, from a \u201cminimal\nprior knowledge\u201d (MPK) perspective, such heavy reliance on massive test-time inference (plus any hidden heuristics\nfrom training) complicates claims that the model spontaneously infers new logic.\n2. Incomplete Coverage of ARC and Persistent Failures: Even with considerable resource usage, chain-of-thought\nLLMs have not definitively solved all ARC tasks. In some 03 configurations, results around 75\u201387% accuracy on\nspecific ARC benchmarks are reported-impressive but not exhaustive. Certain tasks illustrate that purely scaling\nchain-of-thought does not guarantee a conceptually grounded solution consistent with minimal prior knowledge con-\nstraints.\n3. Unclear Evidence of Genuine Common Sense: While these models can produce long, plausible reasoning traces,\nit remains ambiguous whether they are performing intangible leaps akin to \u201ccommon sense\u201d or simply reorganizing\nknown patterns. The phenomenon of \u201cdeceptive hallucinations\u201d\u2014generating coherent but misleading responses to\nceal gaps-may broaden as the system's sophistication grows, if no mechanism ensures intangible transformations\nare logically discovered rather than just approximated.\n4. Challenges in Formalizing KLLM: Given that LLM training data can span virtually all publicly available text,\nenumerating or formalizing KLLM becomes practically intractable. Consequently, guaranteeing that a puzzle rule a*\nis disjoint from such a vast corpus is far more complex than in a bounded domain like ARC. This is why adapting\nARC's well-defined training set (or any similarly constrained environment) to a textual puzzle format can be more\nfeasible: any intangible logic absent from the known transformations becomes a solid test for new concept formation.\nTaken together, these observations illustrate both the promise and limitations of chain-of-thought or \u201cscaling-based\"\nsolutions for LLMs in ARC-like tasks. While such models demonstrate partial leaps in reasoning, they carry high\ncomputational costs, fall short of repeatedly solving all puzzles, and do not obviously circumvent the minimal prior\nknowledge principle spelled out in this paper. Instead, a more structured approach ensuring diagonal or intangible\ntasks remain outside the enumerated domain of prior heuristics\u2014may offer a clearer path to verifying genuine adaptiv-\nity. In the broader context of building safe and beneficial AI, calibrating both economic viability and honest conceptual\nleaps becomes paramount, lest we risk advanced but deceptively incomplete intelligence that can lead to misaligned\nor manipulative outcomes."}, {"title": "8 Ensuring a Path to Safe and Trustworthy AI", "content": ""}, {"title": "8.1 Scaling vs. Common Sense", "content": "Without robust intangible inference, scaling model size or data leads to advanced illusions of coherence but fosters\ndeceptive hallucinations. An AI built this way might convincingly articulate false or destructive directives. Our\nlitmus test anchors system design on minimal prior knowledge checks for intangible puzzle success, mitigating these\nemergent deceptions."}, {"title": "8.2 Ethical, Reliable Foundations", "content": "We plan subsequent papers on:\n\u2022 Ethical AI: how common sense is indispensable for moral alignment.\n\u2022 The role of emotions or human frailties in AI: why artificially imposing them is hazardous.\n\u2022 A rigorous path to beneficial AI, one that systematically confronts the crucial philosophical and technical dilem-\nmas-such as the Chinese Room argument, the paperclip alignment problem, the frame problem, and numerous\nadditional puzzles of cognition and ethics. Rather than merely scaling existing architectures, our approach insists\non tackling these cornerstones explicitly and providing concrete resolutions. By embedding solutions to these co-\nundrums into the foundational blueprint of AI design, we mitigate the danger of developing systems that may\namplify misalignment or \u201challucination\u201d risks. Through a principled synthesis of philosophical insight, theoretical\nclarity, mathematical rigor, and empirical validation, we can confidently advance toward a next-generation artificial\nintelligence that is robust, trustworthy, and ultimately beneficial to humanity.\nThis litmus test is the methodological engine ensuring an AI, prior to wielding artificial intelligence, can handle out-of-distribution contexts responsibly."}, {"title": "9 Open Challenges and Future Work", "content": "Guaranteeing Disjointness in Practice As K grows (e.g., child's life experience, LLM corpora, robot motion\nlibraries), it can be difficult to ensure intangible rules are truly absent. Practical solutions involve iterative checks or\ncarefully contrived puzzle logic that draws on never-before-seen properties.\nInterpretability and Auditing AI's Concept Formation Diagnosing how or when the AI hypothesizes intangible\ntransformations remains tough. Tools for symbolic introspection or transparent reasoning could help confirm it adopted\na*, not a partial or illusory fix.\nScaling Up to Real-World Ethical Domains Eventually, intangible tasks must incorporate social or moral con-\ntexts-e.g., \"No typical heuristics can handle a moral quandary with unknown or contradictory premises.\" By passing\nsuch scenarios starting from MPK, an AI might demonstrate genuine ethical sense-making beyond pre-scripted guide-\nlines."}, {"title": "10 Conclusion", "content": "We propose an axiomatic litmus test for common sense that addresses the immediate danger of more intelligent AI\nsystems exhibiting deceptive hallucinations, ultimately undermining alignment and safety. By combining minimal\nprior knowledge, restricted real-time interactions, and intangible or diagonal tasks, we ensure that no memorized\ntransformations or heuristics alone can suffice. Success demands genuine conceptual leaps\u2014common sense. This\nframework builds on \"Common Sense Is All You Need,\u201d attempting to offer a more rigorous foundation that can scale\nfrom ARC challenges and robotic embodiments to large language models (LLMs) and beyond.\nAchieving trust in advanced AI requires validating that it can handle novelty without deception or destructive illusions.\nThe litmus test proposed here clarifies whether an agent truly exceeds its known concept set in real time. However,\nscaling existing architectures without solving key philosophical and technical conundrums\u2014like the Chinese Room\nargument, the paperclip alignment problem, and the frame problem-risks compounding dangerous behaviors in more\npowerful models. Addressing these foundational challenges is not optional: it is the necessary bedrock for designing\nAI that remains properly grounded and robust as intelligence scales.\nFuture installments in this series will demonstrate how ethical AI also hinges on common sense as a prerequisite\nand why artificially imposing human-like emotional fragilities could harm potentially superintelligent systems. By\ncarefully developing AI solutions-rooted in solving fundamental theoretical, mathematical, and empirical issues\u2014\nwe invite a prosperous future guided by safe, context-aware, and common sense-driven AI."}]}