{"title": "Artificial Neural Networks for Photonic Applications: From Algorithms to Implementation", "authors": ["PEDRO FREIRE", "EGOR MANUYLOVICH", "JAROSLAW E. PRILEPSKY", "SERGEI K. TURITSYN"], "abstract": "This tutorial-review on applications of artificial neural networks in photonics targets a broad audience, ranging from optical research and engineering communities to computer science and applied mathematics. We focus here on the research areas at the interface between these disciplines, attempting to find the right balance between technical details specific to each domain and overall clarity. First, we briefly recall key properties and peculiarities of some core neural network types, which we believe are the most relevant to photonics, also linking the layer's theoretical design to some photonics hardware realizations. After that, we elucidate the question of how to fine-tune the selected model's design to perform the required task with optimized accuracy. Then, in the review part, we discuss recent developments and progress for several selected applications of neural networks in photonics, including multiple aspects relevant to optical communications, imaging, sensing, and the design of new materials and lasers. In the following section, we put a special emphasis on how to accurately evaluate the complexity of neural networks in the context of the transition from algorithms to hardware implementation. The introduced complexity characteristics are used to analyze the applications of neural networks in optical communications, as a specific, albeit highly important example, comparing those with some benchmark signal processing methods. We combine the description of the well-known model compression strategies used in machine learning, with some novel techniques introduced recently in optical applications of neural networks. It is important to stress that although our focus in this tutorial-review is on photonics, we believe that the methods and techniques presented here can be handy in a much wider range of scientific and engineering applications.", "sections": [{"title": "1. Introduction", "content": "Machine learning has a tremendous number of definitions, which often reflect the specific interests of the researchers who formulate them. Here, we use the definition of machine learning as a bevy of algorithms that \u201c... allows computer programs to automatically improve through experience and that automatically infer some general laws from specific data\", taken from the classical Tom Mitchell's monograph [1]. In this tutorial-review, we will discuss blending machine learning with various photonics technologies and applications. The mixture of these two complementary disciplines enables the development of new scientific and engineering techniques that benefit both from the speed and parallelism inherent to optical systems and the ability of machine learning to infer from data and automatically improve system performance. Nonlinear photonics often features complex light dynamics and deals with systems that cannot be easily comprehended or controlled. Therefore, another attractive feature of machine learning in photonics applications is its capability to deal with complex nonlinear systems, whilst staying flexible and re-adaptable. Additionally, photonic devices and systems operating at high speed can quickly generate a vast amount of data. This makes them well-suited to the application of various data-based machine learning algorithms that improve performance with increasing available data sets. Therefore, photonics and machine learning look like a perfect fit for each other, and their combination can naturally bring forth new ideas, theories, and devices, as well as novel concepts for understanding the description of light-related phenomena.\nArtificial neural networks, which we will henceforth call simple neural network (NNs), are computational machine learning frameworks that attempt to mimic some brain operations. The attractive features of biological NNs, which we would like to keep when using their artificial analogs, are robustness and fault tolerance; flexibility and easiness of re-adaptation to the changing conditions; ability to deal with a variety of data, meaning that the network can make do with information that is fuzzy, probabilistic, noisy, and even inconsistent; collective computation, i.e., the network can process the data in parallel and in a distributed manner [2]. Whilst the NNs are frequently attributed to supervised learning thanks to numerous widely-known successful examples in, e.g., image recognition [3, 4], they are also applicable to unsupervised learning [5], semi-supervised learning [6,7], and reinforcement learning [8\u201311], to mention the most noticeable directions. Of course, in this tutorial-review, we cannot address each specific item from the list above. Instead, we will focus on some particular examples of using the NNs in photonics, trying to explain why the particular combination of a machine learning method with a photonics application has turned out to be successful.\nHere, it is pertinent to note that ultra-fast photonic applications can bring about conditions and requirements (in terms of accuracy, speed, and complexity), which differ from those in more \"traditional\" use cases of NNs. For example, in optical communications, the typical bit-error-rate (the probability of error occurrence in the dataset, speaking in \u201cmachine learning\" language) before forward error correction, is of the order $10^{-2}$, which is, for instance, much lower than we have in typical image recognition tasks [12]. Therefore, the solutions developed in deep learning applications for image recognition and language processing often require adaptation and/or substantial modifications when we deal with, e.g., an equalization task in optical communications. We specifically notice that the real-time operation of NNs in ultra-fast photonics inevitably sets a limit on the acceptable level of NN's complexity and processing latency (inference). Thus, in this review, we pay special attention to the NNs with reduced complexity, and this, in turn, emanates into the reduction of the energy consumption used for signal processing, the sought-for feature in almost every application nowadays.\nThere are numerous recently-emerged and still developing areas at the interface of machine learning and photonics: general neuromorphic photonics, unconventional optical computing, photonic neural networks, optical communications, imaging, and sensing, to mention a few important examples where the cross-fertilization of the fields has already proven to be fruitful. Typically, the NNs' application in photonics is related to the processing of large data sets, which is the case in optical communications, ultra-fast photonics, optical imaging and sensing, lasers, optical metrology, design of new photonic materials, and so on. However, we would like to stress that this tutorial-review is not aimed to be a comprehensive overview of all applications of NNs (or, in more general terms, of the machine learning methods) in photonics, as this goal would be too large and general to fit into any review paper or even in a monograph. More information, details, methods, and examples of merging the photonics and artificial intelligence solutions can be found in other recent review papers covering different aspects of the subject and presenting various view-points [13-29]. How is then this tutorial-review different from numerous other review papers in the field? In this paper, we aim to improve some photonic techniques and technologies by using NNs for signal or data processing, providing analysis of the complexity and hardware implementation. We do not provide a comprehensive survey of optical reservoir computing or photonic NNs, which form a huge, rapidly expanding, and utterly fascinating area; we refer the reader to recent works and reviews on the subject [29-45], including critical opinions [46]. In particular, a good exposition of the known and potential benefits of using neuromorphic devices in place of their \u201cvon Neumann\u201d counterparts, including estimates of"}, {"title": "2. Basics of artificial neural networks for photonics community", "content": "In an artificial NN, several neurons are connected together in a nonlinear manner. The network learns by adjusting the weights and biases, according to the feedback (typically provided by the so-called back-propagation technique) based on the evaluation of the accuracy of the NN's prediction, which is quantified by a cost (loss) function. The number of neurons in the input layer corresponds to the input characteristics, whereas the number of output neurons is linked to the batch of classes of interest for classification; or it can be just a single neuron when we do with a single-class regression. In the deep NN structures, the layers between the input and output layer are referred to as hidden layers; the number of neurons per layer is arbitrary, and the choice of NN's hyperparameters (the number of neurons in the hidden layers and the number of hidden layers) requires designer's expertise in adjusting the NN structure to the task in hand; the choice of hyperparameters also depends on the complexity of the system to be modeled, as these parameters ultimately define the representation capability of an NN. For convenience of presentation, in this section, we briefly revisit some basic types and features of artificial NNs that will be discussed throughout the paper.\nHowever, we note that in spite of the (deceptive) simplicity of the short description of NNS given above, there are a plethora of unresolved puzzles and problems in the theory of deep NN, which typically relate to the three fundamental deep NN challenges: expressibility, optimizability, and generalizability. At the moment, we do not seem to have a good universal theory that would give us persuasive answers to all the problems itemized above, while the works shedding light on some of the NNs' properties, features, and peculiarities emerge continuously."}, {"title": "2.1. Dense Layer", "content": "We start from the basic feedforward NN, the so-called multi-layer perceptron (MLP). The simplest variant of the perceptron idea was first developed in 1943 by McCulloch and Pitts [48], but this concept drew the essential attention of scientific society only after Frank Rosenblatt's implementing it as a machine built in 1958 [49]. While Rosenblatt used just a single layer of neurons for binary prediction, nowadays, the perceptron's original idea has been largely generalized, such that it evolved into a (deep) feed-forward densely-connected multilayer structure that we call the MLP.\nA dense layer, also known as a fully connected layer, is a layer in which each neuron (labeled as i) is connected with all the neurons (labeled as j) from the previous layer with a specific weight $W_{ij}$. The input vector is mapped to the output vector in a nonlinear manner by the dense layer, due to the presence of a non-linear activation function. Dense layers can be combined to form an MLP, which is a class of a feed-forward deep NN. Fig. 1 illustrates the working operation of a single neuron in such a dense layer.\nThe output vector y of a dense layer, given x as an input vector, is written as:\n$y = \\phi(Wx + b),$ (1)\nwhere y is the output vector, $\\phi$ is a nonlinear activation function, W is the weight matrix, and b is the bias vector.\nNow, let us turn to the hardware implementation aspect of this most prolific NN structure, where we first mention the electronic implementation. The traditional matrix multiplier-and-accumulator (MAC) is used for the implementation of such layers in the digital domain [50]. More recently, the electrical analog implementation of a dense layer was demonstrated using a CMOS with transistors and resistors [51,52], or using an operational transconductance amplifier [53]. As a drawback, the analog NNs' implementation typically renders a lower accuracy and is more sensitive to noise compared to their digital counterparts [54].\nNow, we mention that there are two different elements of the NN processing that are addressed in the photonic feed-forward NN implementation: the matrix-vector multiplications, and the activation function. First, we address the differences in the activation function. The first widely adopted approach for the activation of photonic NNs, which can be called a \"fully-analog\" implementation, entails utilizing silicon photonic meshes comprising the networks of Mach- Zehnder interferometers and programmable phase shifters (electro-optic activations). However, lately, a novel approach for the activations coined \"hybrid\" photonic programmable NNs has emerged, demonstrating remarkable features in terms of low latency and energy efficiency for"}, {"title": "2.2. Convolutional Neural Networks", "content": "In a convolutional NN (CNN), we apply the convolutions with different filters to extract the features and convert them into a lower-dimensional feature set, The CNNs can be used in 1D, 2D, or 3D network arrangements depending on the applications. Here we focus on 1D-CNNs, which are applicable to, e.g., processing sequential data [64]. The 1D-CNN processing with padding equal to 0, dilation equal to 1, and stride equal to 1, can be summarized as the following transformation:\n$y = \\Phi(\\sum_{n=1}^{n_i} \\sum_{j=1}^{n_k} x_{i+j-1,n} K_{j,n} + b^f),$ (4)\nwhere $y^f_i$ denotes the output, known as a feature map, of a convolutional layer built by the filter f in the i-th input element, $n_k$ is the kernel size, $n_i$ is the size of the input vector, $x_{i,n}$ represents the raw input data, $k^f_j$ denotes the j-th trainable convolution kernel of the filter f and $b^f$ is the bias of the filter f.\nIn the general case, the additional parameters, such as padding, dilation, and stride, also affect the output size of the CNN. The padding adds information (often zeros) to the empty points around the edges of an input signal so that its size stays the same after the convolution operation. The dilation and stride affect how the kernel operation will behave in the convolution. The dilation \"inflates\" the kernel by adding holes between the kernel elements, and the stride controls"}, {"title": "2.3. Vanilla Recurrent Neural Networks", "content": "Vanilla RNN is different from MLP and CNN in terms of its ability to handle memory, which is quite beneficial for time series analysis and prediction. Here, we note that the feedforward models (e.g., those described above) can be reckoned, according to J. L. Elman [67], as an \"... attempt to \"parallelize time\" by giving it a spatial representation... However, there are problems with this approach, and it is ultimately not a good solution. A better approach would be to represent time implicitly rather than explicitly.\u201d The recurrent structures described in the following subsections do that implicit representation, Fig. 5: RNNs take into account the current input and the output that the network has learned from the prior input. The propagation step for the vanilla RNN at the time step t, can be described as follows:\n$h_t = \\phi(Wx_t + Uh_{t-1} + b),$ (9)\nwhere $\\phi$ is again the nonlinear activation function, $x_t \\in \\mathbb{R}^{n_i}$ is the $n_i$-dimensional input vector at time t, $h_t \\in \\mathbb{R}^{n_h}$ is a hidden layer vector of the current state with size $n_h$, $W \\in \\mathbb{R}^{n_h \\times n_i}$ and $U \\in \\mathbb{R}^{n_h \\times n_h}$ represent the trainable weight matrices, and b is the bias vector. For more explanations on the vanilla RNN operation, see, e.g., Ref. [68]. Even though the RNNs were tailored for efficient memory handling, they still suffer from the inability to capture the long-term dependencies because of the infamous vanishing gradient issue [69]."}, {"title": "2.4. Long Short-Term Memory Neural Networks", "content": "LSTM is an advanced type of RNN. While RNNs suffer from short-term memory issues, the LSTM network has the ability to learn long-term dependencies between time steps (t), insofar as it was specifically designed to address the gradient problems encountered in RNNs [92,93]. LSTM networks are made up of LSTM cells, which are units that contain a series of gates that can control the flow of information into and out of the cell, as shown in Fig. 7. The gates can learn to keep relevant information and discard irrelevant information, allowing the LSTM cell to remember important information for long periods of time. More specifically, there are three types of gates in an LSTM cell: an input gate ($i_t$), a forget gate ($f_t$), and an output gate ($o_t$). More importantly, the cell state vector ($C_t$) was proposed as a long-term memory to aggregate relevant information throughout the time steps.\nThe LSTM equation, as shown in Eq. (13), describes the computations involved in a single time step of an LSTM model. The input at time step t, $x_t \\in \\mathbb{R}^{n_i}$, is processed by the LSTM model to produce an output at time step t, $h_t \\in (-1,1)^{n_h}$. The subscript t denotes the current time step, while t \u2212 1 denotes the previous time step.\n$i_t = \\sigma(W^i x_t + U^i h_{t-1} + b^i)$,\n$f_t = \\sigma(W^f x_t + U^f h_{t-1} + b^f)$,\n$o_t = \\sigma(W^o x_t + U^o h_{t-1} + b^o)$,\n$C_t = f_t \\odot C_{t-1} + i_t \\odot \\phi(W^C x_t + U^C h_{t-1} + b^C)$,\n$h_t = o_t \\odot \\phi(C_t),$ (13)\nwith $\\odot$ being the element-wise (Hadamard) multiplication, where $\\phi$ is usually the \"tanh\u201d activation functions, $\\sigma$ is usually the sigmoid activation function, the sizes of each variable are $x \\in \\mathbb{R}^{n_i}, f_t, i_t, o_t \\in (0,1)^{n_h}, C_t \\in \\mathbb{R}^{n_h}$ and $h_t \\in (-1,1)^{n_h}$.\nTo explain further, the LSTM equation above is divided into 5 stages. First, the input gate controls the flow of information into the memory cell. It takes the input $x_t$ and the previous hidden state $h_{t-1}$ as inputs, and produces an output $i_t \\in (0, 1)^{n_h}$ that represents the degree to which the input should be written to the memory cell. Second, the forget gate controls the flow of information out of the memory cell. It takes the input $x_t$ and the previous hidden state $h_{t-1}$ as inputs, and produces an output $f_t \\in (0,1)^{n_h}$ that represents the degree to which the previous cell state $C_{t-1}$ should be retained. Next, the output gate controls the flow of information out of the memory cell. It takes the input $x_t$ and the previous hidden state $h_{t-1}$ as inputs, and produces an output $o_t \\in (0,1)^{n_h}$ that represents the degree to which the current cell state $C_t$ should be outputted. Then, the memory cell $C_t$ is responsible for storing and updating information over time. It takes the input $x_t$, the previous hidden state $h_{t-1}$, and the previous cell state $C_{t-1}$ as inputs, and produces a new cell state $C_t \\in \\mathbb{R}^{n_h}$ that integrates the current input and the previous memory. Finally, the hidden state $h_t$ is the output of the LSTM model at time step t. It takes the current cell state $C_t$ and the output gate $o_t$ as inputs, and produces an output $h_t \\in (-1,1)^{n_h}$ that represents the current hidden state of the LSTM model."}, {"title": "2.5. Gated Recurrent Units", "content": "Introduced in 2014 [94], the GRU network, similar to the LSTM, was designed to overcome the short-term memory issues of RNNs. However, the GRU is less complex than the LSTM7, as it has only two types of gates: the reset ($r_t$) and update ($z_t$) gates, as shown in Fig. 8. The reset gate is used to handle short-term memory, whereas the update gate is responsible for long-term memory [96]. In addition, the candidate hidden state ($\\tilde{h}$) is also introduced to measure how relevant the previous hidden state is to the candidate state. The GRU for a time step t can be formalized as:\n$z_t = \\sigma(W^z x_t + U^z h_{t-1} + b^z)$,\n$r_t = \\sigma(W^r x_t + U^r h_{t-1} + b^r)$,\n$\\tilde{h}_t = \\phi(W^h x_t + r_t \\odot U^h h_{t-1} + b^h)$,\n$h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t,$ (14)\nwhere $\\phi$ is typically the \"tanh\u201d activation function and the rest of the designations are the same as in Eq. (13).\nIn addition to (14), defining the so-called fully gated unit, the simpler GRU architecture variants called minimal gated unit are also sometimes used [97]: in these types, the reset, and update gates are merged. Some other GRU variants are described and compared in Ref. [96]."}, {"title": "2.6. Echo State Networks", "content": "Echo state networks (ESNs) belong to the class of recurrent structures, more specifically, to the reservoir computing category [98]. The ESN was proposed to simplify the training process while staying efficient and simple to implement. The ESN comprises three layers: an input layer, a recurrent layer, known as a reservoir, and an output layer, which is the only layer that is trainable. The reservoir with random weights assignment is used to replace back-propagation in traditional NNs to reduce the computational complexity of training [99]. We notice that the reservoir of the ESNs can be implemented in two domains: digital and optical [100]. With the optical implementation of the reservoir, the computational complexity dramatically falls; however, the degradation of the performance due to the change of domain can be non-negligible [101]. In this work, we only examine the digital domain implementation. Moreover, we focus on the leaky-ESN, as it is believed to often outperform the \"standard\" ESNs and is more flexible due to time-scale phenomena [102, 103]. The equations of the leaky-ESN for a certain time step t are given as:\n$a_t = \\phi (W^r s_{t-1} + W^{in}x_t + W^{back}y_{t-1}),$ (15)\n$s_t = (1 - \\mu)s_{t-1} + \\mu a_t,$ (16)\n$y_t = W^o s_t + b^o,$ (17)\nwhere $s_t$ represents the state of the reservoir at time t, $W^r$ denotes the weight of the reservoir with the sparsity parameter $s_p$, $W^{in}$ is the weight matrix that shows the connection between the input layer and the hidden layer, $\\mu$ is the leaky rate, $W^o$ denotes the trained output weight matrix, and $y_t$ is the output vector.\nThe schematics of an ESN are shown in Fig. 9. The crucial point in the ESN or reservoir computing concept is that despite the complex structure of these networks, only the weights of the output (readout) layer are trainable. One can see that the multiple interconnections described by matrices $W^{in}, W^r$, and $W^{back}$, constitute a complex recurrent structure with rich internal dynamics. Training of a classical MLP or RNN with a comparable number of neurons would be time-consuming. However, the concept of ESN speeds up the training process drastically and reduces it to linear regression on the output layer. The important feature of this type of NNs is that it can be easily implemented in the physical domain. Many dynamical systems with large internal phase space and exhibiting nonlinear properties can be employed as a reservoir. There are various experimental implementations of ESNs, including fiber-cavity-based schemes [104].\nFinally, we would like to highlight some potential drawbacks of using ESN which include: i) Difficulty in training: ESN can be difficult to train, as they require careful tuning of the network's hyperparameters in order to achieve a good performance; ii) Limited ability to model long-term dependencies: An ESN is not able to effectively model long-term dependencies in the data, as they have a fixed-size reservoir and do not allow information to flow through the network over many timesteps."}, {"title": "2.7. Attention Layers", "content": "Attention is an NN mechanism that observes a whole collection of data and selectively focuses on a subset of the collection. In other words, attention mechanisms are a way to allow a model to focus on specific parts of its input when processing it, rather than using the entire input equally. The attention unit is schematically represented in Fig. 11. It was first applied to sequence-to-sequence learning in [105] and was used mostly to further exploit the importance of each subset among the input data. In other words, attention is one add-on component of a network's architecture, in charge of managing and quantifying the interdependence between the data of interest. General attention investigates the interdependence between input and output elements, whilst self-attention deals with finding correlations among input elements [106-108].\nLet us turn to the case of general attention to account for the interdependence between the final predicted symbol and both the input symbols and the output hidden states. By adding such an attention mechanism, we expect to find the contribution of the input symbols and their hidden representations to the final received symbol prediction. Therefore, we can identify the essential part of the input sequence for training that could lower the computational complexity.\nThe attention is generally a single- or multi-layer feed-forward NN with trainable weights and biases, which are applied to the output hidden states of the RNN layer.\nIn the original attention mechanism [105], an input sequence {$x_1, ..., x_{Tx}$ } targets an output sequence {$y_1, ..., y_{T_y}$ }. The conditional probability for a certain target output $y_i$, is defined as:\n$P(y_i|y_1,...y_{i-1}, x) = g(y_{i-1}, s_i, c_i),$ (18)\nwhere g is a nonlinear, potentially multi-layered, function that outputs the probability of $y_i$; $s_i$ is an RNN's hidden state for time i computed through $s_i = f(s_{i\u22121}, y_{i\u22121}, c_i)$. $C_i$ is a context vector conditioned for each target $y_i$, i.e., a vector generated from the sequence of the hidden states for predicting the current target output $y_i$; it is computed as a weighted sum of the hidden states {$h_1,..., h_{Tx}$}:\n$C_i = \\sum_{j=1}^{T_x} a_{i,j}h_j,$ (19)\nwhere the weight $a_{i,j}$ of each $h_j$ is computed by\n$a_{i,j} = \\frac{exp e_{ij}}{\\sum_{k=1}^{T_x} exp e_{ik}},$ (20)\nwhere $e_{ij} = a(s_{i\u22121}, h_j)$ is an alignment model which scores how well the inputs around position j and the output at position i match.\nInstead of predicting the conditional probability of each target $y_i$ from a sequence of targets, we focus only on the received symbol $y_i$:\n$y_i = g(c), \\quad where \\quad c = a * h = [a_1h_1, ..., a_{Tx}h_{Tx}].$ (21)\nThe weight $\\alpha_i$ of each $h_i$ is calculated by\n$\\alpha_i = \\frac{exp e_i}{\\sum_{j+1}^{2k+1} exp e_j}$ (22)\nwhere $e_i = a(h_i)$ is the adapted alignment model and indicates the matching score between the output symbol $y_i$ and the hidden representations h of the input sequence x. According to [105], we can define the activation function f of the RNN and the alignment model a by choice. A single-layer perceptron (SLP) is selected as our alignment model. Matrix multiplication is first"}, {"title": "2.8. Transformers", "content": "The vanilla transformer is a deep learning architecture that was introduced in Ref. [109]. Its architecture is shown in Fig. 12. The transformer is a sequence-to-sequence model that operates on sequences of vectors, where the goal is to learn a mapping from one sequence to another. The key innovation of the transformer is the use of the previously mentioned self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when generating the output sequence. In a nutshell, the transformer consists of an encoder and a decoder. The encoder takes the input sequence and produces a sequence of hidden representations, which are then used by the decoder to generate the output sequence. The self-attention mechanism is used in both the encoder and the decoder, allowing the model to attend to different parts of the input sequence when generating each element of the output sequence. The vanilla transformer can be expressed mathematically as follows:\nLet $X = {x_1,x_2, ..., x_n}$ be the input sequence, where $x_i$ is a vector of dimension $d_{model}$, so the shape of X is [n \u00d7 $d_{model}$]. Similarly, let $Y = {y_1, y_2, ..., y_m}$ be the output sequence, where $y_j$ is a vector of dimension m \u00d7 $d_{model}$.\nThe encoder consists of N identical layers, where each layer has three sub-layers: a multi-head self-attention mechanism, an Add&Norm layer, and a position-wise fully connected feed-forward network. The output of the ith layer of the encoder is denoted as $H_i = {h_{i,1}, h_{i,2}, ..., h_{i,n}}$, where $h_{i, j}$ is a vector of dimension n \u00d7 $d_{model}$.\nThe multi-head self-attention mechanism can be expressed as:"}, {"title": "2.9. Residual Neural Networks", "content": "An artificial neural network becomes a Residual Neural Network (ResNet) [110] if the input of a specific layer is also passed (or skipped) to another deeper layer in the network; this connection is called a residual connection. The utilization of skip connections or shortcuts, visually illustrated in Fig. 13, is a distinctive feature of ResNets. These connections facilitate the bypassing of specific layers, thereby addressing challenges like vanishing gradients and promoting more efficient training within deep architectures.\nAnother famous architecture that uses residual connections is the HighwayNet [113], The HighwayNet preserves the shortcuts introduced in the ResNet, but augments them with a learnable parameter to determine to what extent each layer should be a skip connection or a nonlinear connection. It is noteworthy that HighwayNets possess the capacity to autonomously learn the skip weights through an additional weight matrix governing their gates. In contrast, ResNet models are conventionally characterized by double or triple-layer skips, incorporating non-linear activation functions such as ReLU and batch normalization, which enhance the expressiveness and convergence capabilities of the models.\nAdditionally, DenseNets [114] serve as a relevant descriptive reference for models incorporating multiple parallel skip connections, underscoring the adaptability and versatility of residual connections in contemporary neural network designs.\nLet us now define what is the feed-forward equations for such a type of NN layer. Given the weight matrix $W^{l-1,l}$ for the connection weights from layer l \u2212 1 to l, and the weight matrix $W^{l-2,l}$ for the connection weights from layer l \u2212 2 to l, then the forward propagation through"}, {"title": "2.10. Radial basis function neural network", "content": "A radial basis function (RBF) network is an artificial NN that uses the RBFs as activation functions. Its schematics and a comparison of RBF function and sigmoid function are given in Fig. 15. The network output is a linear combination of input RBFs and neuron parameters. The concept itself was introduced by Broomhead and Lowe in 1988 [117]. There are numerous applications for RBF networks, including function approximation, time series prediction, classification, and system control. Even though the RBF concept is considerably old and familiar, and, often, the other NN types are preferred nowadays, it still attracts the attention of data scientists [118].\nThe RBF networks typically have three layers: an input layer, a hidden layer with a non-linear RBF activation function and a linear output layer [119]. The input can be modeled as a vector of real numbers $x \\in R^n$. The output of the network is then a scalar function of the input vector, $\\varphi: R^n \\rightarrow R$, and is given by:\n$\\varphi(x) = \\sum_{i=1}^N a_i p (||x \u2212 c_i||),$ (N is the number of neurons in the hidden layer, ci is the center vector for neuron i, and ai is the weight of neuron i in the linear output neuron.)\nwhere N is the number of neurons in the hidden layer, $c_i$ is the center vector for neuron i, and $a_i$ is the weight of neuron i in the linear output neuron. Functions that depend only on the distance from a center vector are radially symmetric about that vector, hence the name radial basis function. In the basic form, all inputs are connected to each hidden neuron. The norm is typically taken to be the Euclidean distance (although the Mahalanobis distance [120] appears to perform better with pattern recognition) and the radial basis function is commonly taken to be a Gaussian function:\n$p (||x \u2212 c_i||) = exp [\u2212\u03b2_i ||x \u2212 c_i||^2] .$ \nThe Gaussian basis functions are local to the center vector in the sense that\n$\\lim_{||x||\\rightarrow \\infty} p (||x - c_i||) = 0,$\ni.e., changing the parameters of one neuron has only a small effect on input values that are far away from the center of that neuron.\nThe RBF networks are the universal approximators on a compact subset of $R^n$ under certain modest restrictions regarding the activation function shape. This implies that an RBF network with sufficient hidden neurons can approximate any continuous function on a closed, constrained set with arbitrary accuracy [121].\nIn addition to the unnormalized architecture mentioned, the RBF networks can be normalized. In this case, the mapping is\n$\\varphi(x) \\stackrel{def}{=} \\frac{\\sum_{i=1}^N a_i p (||x - c_i||)}{\\sum_{i=1}^N p (||x \u2013 c_i ||)} = \\sum a_i u (||x \u2212 c_i ||),"}, {"title": "2.11. Autoencoders", "content": "Autoencoders are the NN architectures that are trained to reconstruct their inputs. A basic architecture of an autoencoder is shown in Fig. 16. In a more formal description", "bottleneck\". In this sense, this first part can be summarized by an encoder NN $g_\\phi(.).$ The bottleneck output is decoded terminating with an output layer with the same dimensionality as the encoder's input layer (X), a reconstruction of X. Here, the decoder part can be described as $f_\\theta(.)$. It is important to highlight that without the bottleneck, the encoder, and decoder would copy their input to the output, and by having a bottleneck, the encoder compresses the data to a latent representation that is more robust. The bottleneck appears in many autoencoder variations [122-124": ".", "MSE": "n$L_{MSE}(x, \\tilde{x}) = \\frac"}]}