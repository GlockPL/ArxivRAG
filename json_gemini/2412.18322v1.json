{"title": "Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning", "authors": ["SAFA BEN ATITALLAH", "CHAIMA BEN RABAH", "MAHA DRISS", "WADII BOULILA", "ANIS KOUBAA"], "abstract": "Graph Mamba, a powerful graph embedding technique, has emerged as a cornerstone in various domains, including bioinformatics, social networks, and recommendation systems. This survey represents the first comprehensive study devoted to Graph Mamba, to address the critical gaps in understanding its applications, challenges, and future potential. We start by offering a detailed explanation of the original Graph Mamba architecture, highlighting its key components and underlying mechanisms. Subsequently, we explore the most recent modifications and enhancements proposed to improve its performance and applicability. To demonstrate the versatility of Graph Mamba, we examine its applications across diverse domains. A comparative analysis of Graph Mamba and its variants is conducted to shed light on their unique characteristics and potential use cases. Furthermore, we identify potential areas where Graph Mamba can be applied in the future, highlighting its potential to revolutionize data analysis in these fields. Finally, we address the current limitations and open research questions associated with Graph Mamba. By acknowledging these challenges, we aim to stimulate further research and development in this promising area. This survey serves as a valuable resource for both newcomers and experienced researchers seeking to understand and leverage the power of Graph Mamba.", "sections": [{"title": "1 Introduction", "content": "Graph-based learning models, particularly Graph Neural Networks (GNNs), have gained significant traction in recent years due to their ability to effectively capture and process complex relational data. These models have proven advantageous in many different fields where graphs are the typical way to represent data [1]. The increasing significance of GNNs can be attributed to various factors. Graph-structured data has been raised in many real-world systems, such as social networks, molecular structures, and citation networks [2, 3]. GNNs have a solid ability to leverage relational information and the connections between entities. In addition, different advanced GNN architectures have been proposed with high scalability to handle large-scale graphs, making them suitable for big data applications. This type of learning can be applied to various tasks, including node classification, link prediction, and graph classification.\nHowever, they face several significant challenges that limit their effectiveness in specific scenarios. Most GNNs are restricted in their ability to effectively capture long-range dependencies. They typically rely on message passing between neighboring nodes, which can lead to information dilution over multiple hops. This constraint is particularly problematic in graphs with complex hierarchical structures. In addition, many GNN architectures require multiple rounds of neighborhood aggregation, which is computationally expensive, especially for large-scale graphs. The computational cost grows significantly as the number of layers increases to capture more complex patterns. Furthermore, GNNs usually face memory constraints and increased training time when applied to large graphs [4]. The issue is heightened for dynamic graphs, where the structure changes over time and requires frequent updates to node representations. Sampling techniques have been proposed to address this but can lead to information loss. GNN variants have quadratic complexity regarding the number of nodes or tokens. Similar issues arise in GNNs when computing full graph attention or when dealing with dense graphs. This quadratic scaling significantly impacts performance and limits the application of these models to huge graphs or long sequences.\nIndeed, addressing the limitations of current graph-based learning models is crucial for their broader applicability. One promising direction in this effort is the adaptation of State-Space Models (SSMs) to graph learning, which has led to the development of Graph Mamba. SSMs are mathematical models initially designed for sequence modeling in control theory and signal processing. They represent a system's behavior using a set of input, output, and state variables related by first-order differential equations. In the context of ML, SSMs can efficiently model long-range dependencies in sequential data. They offer a continuous-time perspective on sequence modeling, which can benefit specific data types.\nRecently, Mamba has emerged as a groundbreaking approach in Artificial Intelligence (AI), specifically designed as a specialized form of the SSM to address the computational limitations of traditional Deep Learning (DL) models. Standard models, such as Convolutional Neural Networks (CNNs) and Transformers, face a significant challenge related to computational inefficiency, particularly in tasks involving long-sequence modeling. Mamba's primary goal is to enhance computational efficiency by reducing time complexity from quadratic, as seen in transformers, to linear. Inspired by advancements in structured SSMs, Mamba is presented to boost performance in areas requiring long-range dependency modeling and large-scale data processing.\nGraph Mamba emerges as a specialized variant of SSMs designed specifically for graph learning. Its primary goal is to address the limitations of traditional GNNs by leveraging the unique strengths of state-space models. The core concept of Graph Mamba is its state-space modeling approach, which employs selective scanning, a powerful mechanism for efficiently processing graph information by dynamically focusing on the most relevant parts of the graph structure. This allows Graph Mamba to manage large-scale and complex graphs with superior computational performance.\nRecently, there has been an increasing interest in Graph Mamba, as shown by the growing number of articles. This survey aims to investigate the potential of integrating graph structures with Mamba frameworks to enhance representation learning and scalability. Through a comparative analysis of existing literature and empirical studies, this survey evaluates the performance of Graph Mamba against traditional Machine Learning (ML) methods."}, {"title": "1.1 Related Surveys", "content": "This section provides a thorough summary of essential survey studies from two research fields; GNN architectures and Mamba framework."}, {"title": "1.1.1 Surveys on Graph Neural Networks: Advancements and Applications", "content": "GNNs have found applications in a variety of domains, including computer vision, recommendation systems, fraud detection, and healthcare. Several comprehensive surveys have been elaborated on GNNs. In [5], the authors presented a comprehensive review of GNNs, emphasizing their evolution, essential concepts, and numerous potential applications of this cutting-edge technology. GNNs transformed ML by effectively modeling relationships in graph-structured data, overcoming the constraints of conventional neural networks. The study described major GNN architectures such as Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Graph Sample and"}, {"title": "1.1.2 Surveys on Mamba: Trends, Techniques, and Applications", "content": "Since its introduction in late 2023, Mamba has received a lot of attention in the DL community because it offers compelling benefits that encourage adoption and exploration across multiple domains. Nnumerous surveys have been elaborated to investigate MAmba potential and its applications. For example, The Patro et al. in [7] investigated the use of SSMs as efficient alternatives to transformers for sequence modeling applications. It classified SSMs into three paradigms: gating, structural, and recurrent, and discussed key models like S4, HiPPO, and Mamba. This survey emphasized the use of SSMs in a variety of domains, including natural language processing, vision, audio, and medical diagnostics. It compared SSMs and transformers based on computational efficiency and benchmark performance. The paper emphasized the need for additional research to improve SSMs' ability to handle extended sequences while maintaining high performance across multiple applications.\nQu et al. [8] gave a thorough explanation of Mamba. They positioned Mamba as a viable alternative to transformer topologies, particularly for tasks involving extended sequences. The survey presents the fundamentals of Mamba, highlighting its incorporation of features from RNNs, Transformers, and SSMs. It examined improvements in Mamba design, including the creation of Mamba-1 and Mamba-2, which featured breakthroughs such as selective state space modeling, HiPPO-based memory initialization, and hardware-aware computation optimization methods. The authors also looked into Mamba's applications in a variety of domains, including natural language processing, computer vision, time-series analysis, and speech processing, demonstrating its versatility in tasks such as large language modeling, video analysis, and medical imaging. The study identified many problems related to Mamba use, including limitations in context-aware modeling and trade-offs between efficiency and generalization. They also suggested improvements for Mamba's generalization capabilities, computational efficiency, and discussed its applicability in new research areas.\nIn their recent study, Wang et al. in [9] conducted a comprehensive survey that emphasized the changing landscape of DL technologies. This survey focused primarily on the theoretical foundations and applications of SSMs in fields such as natural language processing, computer vision, and multi-modal learning, with the goal of addressing the computational inefficiencies of conventional models. Experimental comparisons revealed that, while SSMs showed promise in terms of efficiency, they frequently fell short of the performance of cutting-edge transformer models. Despite this, the findings in this study revealed that SSMs could reduce memory usage and provide insights into future research to improve their performance. This study provided valuable insights into DL architectures, showing that SSMs could play a crucial role in their development.\nOn the other hand, recent studies have explored Mamba Vision techniques, emphasizing its rapid growth and rising importance in computer vision. They highlight Mamba's ability to address the limitations of CNNs and Vision Transformers, particularly in capturing long-range dependencies with linear computational complexity. Rahman et al. [10] investigated the Mamba model, this revolutionary computer vision approach that addressed the constraints of CNNs and Vision Transformers (ViTs). With CNNs, local feature extraction is more efficient, but with ViTs, long-range dependencies are more difficult due to their quadratic self-attention mechanism. Mamba used Selective Structured State Space Models (S4) to handle long-range dependencies with"}, {"title": "1.1.3 Discussion", "content": "While the surveys discussed above provide essential insights into a variety of cutting- edge fields, they do have significant limitations. Many surveys on GNNs concentrate on the theoretical foundations and architecture of these networks, paying little attention to practical problems and model scalability in dynamic scenarios. In addition, while these surveys highlight GNN's relevance in research fields like healthcare and recommendation systems, they often ignore practical challenges such as computational complexity, scalability in large networks, and limited generalization across heterogeneous datasets. Besides, while many surveys discuss Mamba frameworks' potential to overcome transformer limitations, they tend to focus on theoretical advancements and model efficiency rather than providing an in-depth analysis of real-world limitations, such as trade-offs between computational efficiency and performance across various domains. The available studies on GNNs and Mamba models highlight their distinct improvements but remain limited in scope. GNN surveys investigate graph-based learning but do not explore how graph structures may be incorporated into Mamba frameworks. Mamba-related surveys, on the other hand, concentrate on sequential modeling and computing efficiency without investigating the possibility of combining graph-based methods. This discrepancy creates a huge research gap. Integrating graph structures into Mamba presents transformative capabilities that need a comprehensive review."}, {"title": "1.2 Contributions of the Proposed Survey", "content": "There has been a rapid surge in research exploring Graph Mamba's architecture, improvements, and applications across various domains. However, the insights remain distributed across various studies, and there is currently no thorough review that brings these findings together. As the field advances rapidly, a well- structured overview of the latest developments is increasingly valuable. The main contributions of this survey paper are illustrated in the following points:\n\u2022 This survey offers a comprehensive explanation of the fundamental principles of Graph Mamba and offers a strong theoretical foundation for both researchers and practitioners.\n\u2022 It examines the most recent enhancements to the original Graph Mamba architecture and evaluates the performance implications of various proposed modifications.\n\u2022 A comparison of various Graph Mamba variants is presented to emphasize their unique characteristics.\n\u2022 The survey examines a variety of disciplines in which Graph Mamba has been implemented, such as computer vision, healthcare, and biosignals.\n\u2022 Additionally, it identifies potential fields for future implementations of Graph Mamba and addresses the current limitations and open research questions in this context."}, {"title": "1.3 Paper Organization", "content": "This survey provides a comprehensive overview of Graph Mamba state space models, including their architectures, applications, challenges, and potential future directions. We explore the advantages and disadvantages of existing Graph Mamba models and discuss their prospects for future development. The paper is organized as follows: Section 2 discusses the preliminaries and key terms related to Graph Neural Networks, State Space Models, and Mamba. In Section 3, we delve into various Graph Mamba architectures. Section 4 highlights recent applications of Graph Mamba. Sections 5 and 6 present benchmarks and a comparative analysis of results demonstrating Graph Mamba's performance across different tasks. Section 7 outlines the limitations of applying Graph Mamba. Section 8 explores emerging areas and future research directions. Finally, we conclude the work in Section 9."}, {"title": "2 Preliminaries", "content": "This section reviews the foundation of GNNs and SSMs and how they are integrated in the Graph Mamba framework."}, {"title": "2.1 Graph Neural Networks (GNNs)", "content": "GNNs have developed as a strong class of DL models built for graph-structured data. Unlike standard ML models, which often operate on fixed-sized inputs such as pictures or sequences, GNNs are specially designed to handle non-Euclidean data, represented as nodes and edges [1]. This makes GNNs ideal for tasks that need complicated relational data, such as social networks, knowledge graphs, chemical structures, and recommendation systems. Graphs are inherently adaptable and can represent a broad range of data formats. Standard DL models, such as CNNs, perform well with structured data like grids or sequences but fail to generalize to graph data. GNNs address this drawback by learning representations of nodes, edges, and graphs in a way that captures both the local neighborhood information and the global structure of the graph. Indeed, GNNs are based on the idea of message forwarding, in which each node in the network gathers information from its neighbors to update its representation. This method enables GNNs to effectively capture both local patterns and long-range relationships throughout the graph by propagating information through a set of layers. In the following subsections, we present an overview about some popular GNN architectures proposed in the literature."}, {"title": "2.1.1 Graph Convolutional Networks (GCNs)", "content": "GCNs, introduced by Kipf et al. [12], are a specialized type of GNN created to work with graph-based data. The core idea is to take the concept of convolution, which is so effective in image processing with grids of pixels, and adapt it to the irregular structure of graphs. In contrast to conventional CNNs that depend on static grids, GCNs execute localized convolutions at each node, aggregating information from adjacent nodes. This enables GCNs to understand the links and patterns inside the graph structure in a manner that conventional CNNs can't. The propagation rule for a GCN layer is represented as:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in N(i)} C_{ij} W^{(l)} h_j^{(l)})$"}, {"title": "2.1.2 Graph Attention Networks(GATs)", "content": "In [13], GATs have been proposed by Velikovi et al., which are a type of GNN designed to address limitations in traditional GNNs. They are specially designed for complex connections and irregular graph structures. Their key innovation is an attention mechanism that selectively aggregates information from neighboring nodes, allowing them to focus on the most relevant inputs. This method assigns different weights to each neighbor, emphasizing the importance of specific nodes during aggregation and improving the model's ability to capture meaningful relationships. The computations made in the GAT layer are presented in the following Equation 2:\n$h_i^{(l+1)} = \\sigma(\\sum_{j \\in N(i)} \\alpha_{ij} W^{(l)} h_j^{(l)})$"}, {"title": "2.1.3 Graph Sample and Aggregation (GraphSAGE)", "content": "GraphSAGE, introduced by Hamilton et al. in [13], is a scalable GNN architecture for large graphs. It learns node embeddings by sampling and aggregating information from local neighbors, allowing inductive learning to generalize to unseen nodes. GraphSAGE consists of two main parts: embedding generation (forward propagation) and parameter learning. The model iteratively traverses neighborhood layers and enables nodes to gather information from their surroundings. The representation for a node v at depth k is updated as follows:\n$h_v^{(k)} = \\sigma (W^k CONCAT (h_v^{(k-1)}, AGGREGATE (\\{h_u^{(k-1)}, \\forall u \\in N(v)\\})))$"}, {"title": "2.2 State Space Models (SSMs)", "content": "DL has seen a notable transformation with the emergence of Transformer models, which have attained dominance in both computer vision and natural language processing. Their success is attributed to the self- attention mechanism, an effective strategy that enhances model understanding by producing an attention matrix based on query, key, and value vectors [14]. This methodology has transformed how models analyze and comprehend data. However, the Transformer architecture faces a notable challenge. Its self-attention mechanism operates with quadratic time complexity. As the input sequence length grows, the computational requirements increase exponentially and create a significant bottleneck, especially when dealing with very long sequences or large datasets. This limitation has pushed research to develop more efficient architectures that can maintain the benefits of self-attention while scaling more effectively to more significant inputs.\nIn this context, Mamba was proposed by Gu et al. [15] based on SSMs. It has gained much interest in recent years due to its effectiveness in providing good performance as transformers while reducing the overall complexity. SSMs are widely used to represent dynamic systems [16]. They convert a one-dimensional input sequence (u(t)) into an N-dimensional continuous latent state (x(t)) and project it into a one-dimensional output signal (y(t)). Equations 4 and 5 describe this transformation process:\n$x'(t) = Ax(t) + Bu(t)$\n$y(t) = Cx(t) + Du(t)$"}, {"title": "2.2.1 Structured State Space Sequence Models (S4)", "content": "S4 was introduced to address the inefficiencies of traditional transformers in sequence modeling [17]. It leverages structured matrices that allow for the fast and efficient modeling of long sequences. S4 transforms the input sequences into latent states that evolve over time using a continuous-state model. This method is particularly suited for handling very long-range dependencies, as it avoids the quadratic complexity of self-attention mechanisms."}, {"title": "2.2.2 Simplified State Space Layers for Sequence Modeling (S5)", "content": "S5 builds on the foundation of S4 but simplifies the model architecture for more efficient sequence modeling [18]. It reduces the complexity of the latent state transformation process, making it faster and more scalable while retaining the ability to model long-range dependencies."}, {"title": "2.2.3 Selective State Space Models (S6)", "content": "S6 is the most recent development in SSMs and has been widely adopted, particularly in graph-based learning tasks like Graph Mamba [17]. The introduction of a selective scanning mechanism improves its capacity to efficiently manage both complex graph structures and long sequences. In this survey, we provide a comprehensive overview of S6, detailing its architecture and the contributions of the selective scanning mechanism.\nThe selective scanning mechanism in S6 is designed to overcome the limitations faced by standard sequence models. In contrast to conventional models that treat all segments of the input sequence equally, S6 selectively identifies and emphasizes the most relevant portions of the input. This allows the model to concentrate on relevant information and minimize the cost of computing linked to processing irrelevant data points. The selective scanning approach significantly decreases temporal complexity, making S6 scalable for practical applications. The equations below present the foundation of the S6 model.\n\u2022 State Transition with Selection:\n$B(L, N) \\leftarrow SB(x)$\n\u2022 Output Mapping with Selection:\n$C(L, N) \\leftarrow s_C(x)$\nWhere sc(x) acts as a filtering mechanism in the output mapping process, directing attention to particular components of x that contribute most meaningfully to the final output. By selectively mapping relevant features, the model reduces noise and enhances the clarity of information passed to subsequent layers.\n\u2022 Parameter Adjustment with Selection:\n$\\Delta(L, D) \\leftarrow \\tau(Parameter + s_\\Delta(x))$\nIn this step, $s_\\Delta(x)$ dynamically adjusts the parameters within A in response to variations in the input x. The function t then consolidates these adjustments. This adaptability enhances the model's capability to generalize across diverse inputs\n\u2022 Discrete State Evolution:\n$\\bar{A}, B \\leftarrow discretize(A, \\Delta, B)$\nThe discretization technique converts continuous model dynamics into a discrete representation. This makes them easier to process linearly and saves a lot of computing power. By updating the matrices \u0394, A, and B, this step enables the model to preserve essential temporal relationships within the data while optimizing for discrete operations.\n\u2022 Final Output:\n$y \\leftarrow SSM(\\bar{A}, B, C) (x)$\nThe final output y is generated by the SSM, which uses the selectively updated matrices \u0100, B, and C. This process takes the selectively processed input data and turns it into an improved output that captures the structure and relationships of the input data properly.\nIn the context of graph-based learning, S6 has proven particularly useful. Graph data often includes complex relationships between nodes and edges, and processing every node in a large graph can become computationally prohibitive. S6's selective scanning mechanism allows the model to concentrate on the most critical substructures of the graph. This is vital for applications that require long-range dependencies or deep graph traversal."}, {"title": "2.3 Transition from Graph Learning and SSMs to Graph Mamba", "content": "While standard SSMs were built to handle sequence data, many real-world tasks need graph-structured data, in which nodes represent entities and edges describe relationships. Unlike sequences, graphs are non-Euclidean, which means they lack a natural order. This raises the challenge of the complex modeling of long-range connections and multi-hop interactions between nodes. To expand the capabilities of SSMs to graphs, Graph Mamba was designed as a specialized variation of SSMs for graph learning. This design uses state-space modeling approaches to address the specific problems of graph-based tasks. By leveraging the linear complexity of SSMs, Graph Mamba provides a more efficient alternative to GNNs. The main elements of this transition include:\n\u2022 Linear Complexity:\nOne of the standout features of Graph Mamba is its inherited linear time complexity from SSMs, making it significantly more efficient compared to standard GNNs. Graph Mamba can analyze long-range relationships via linear scaling, which improves speed on big networks.\n\u2022 Unified Spatial and Temporal Processing:\nGraph Mamba is designed to handle both spatial (node and edge relationships) and temporal (changes over time) information in a unified framework. This makes it especially useful for dynamic graph workloads like growing social networks or temporal knowledge graphs.\n\u2022 Selective Scanning Mechanisms:\nGraph Mamba, like SSMs, uses selective scanning techniques to effectively process graph information and use the structure's most important components. This enables it to capture key information while minimizing unnecessary computations."}, {"title": "3 Graph Mamba", "content": "In this section, we provide an overview of Graph Mamba architecture. In addition, we discuss the graph structures used, the selective scanning methods designed, and the different training strategies followed for Graph Mamba."}, {"title": "3.1 Graph Mamba Architecture", "content": "Graph Mamba is a subset of SSMs created particularly for graph-based learning tasks. Its architecture, as depicted in Figure 1, is designed to solve the constraints of classic GNNs, such as computational inefficiency and the difficulty in representing long-range relationships in big networks. Graph Mamba's architecture is based on the capacity to represent graphs with linear time complexity, which is more efficient than the quadratic complexity described in many typical GNNs. This is achieved by using SSM concepts that include state updates and latent representations. Graph Mamba recurrently treats data and enables the capture of long-term interactions between nodes in the network without relying on expensive attention techniques like Transformers.\nThe basic architecture of Graph Mamba consists of three main blocks:\n(1) State-Space-Based Message Passing: Instead of standard message-passing as in GNNs, Graph Mamba uses state-space modeling approaches to transfer information among nodes effectively.\n(2) Selective Scanning: Graph Mamba employs a selective mechanism to focus on relevant nodes and edges within the graph. This technique makes it highly efficient when dealing with large graphs where complete message passing is too expensive.\n(3) Spatial-Temporal Integration: The model can also handle temporal components of dynamic graphs, making it ideal for tasks requiring both node connections and changes over time."}, {"title": "3.2 Graph Structure in Mamba Models", "content": "The underlying graph structure in the Graph Mamba framework is crucial in shaping the model's data processing and learning potential. Different graph structures capture distinct relationships and dynamics. To maximize its adaptability and effectiveness across diverse applications, we classify these graph structures into three main categories based on the nature of the data they manage; dynamic graph, heterogeneous graph, and spatio-temporal graph. This classification, as presented in Figure 2, allows Graph Mamba to efficiently harness"}, {"title": "3.2.1 Dynamic Graphs", "content": "Dynamic graphs enhance the Graph Mamba framework's functionality by enabling it to model and learn from developing data structures in which both node attributes and edge connections fluctuate over time. These graphs play an important role in capturing temporal dependencies and adjusting to real-world contexts, where interactions between entities are dynamic and time-sensitive [19]."}, {"title": "3.2.2 Heterogeneous Graph", "content": "Heterogeneous graphs, as used in the Graph Mamba framework, incorporate multiple types of nodes and edges that capture various relationships between entities [23]. Unlike dynamic graphs, which focus on temporal changes, heterogeneous graphs highlight the complexity of node and edge heterogeneity. This graph type allows Graph Mamba to model intricate interactions between different types of entities effectively. In [24], Pan et al. introduced the HeteGraph-Mamba, a novel approach to learning with heterogeneous graphs. They use a selective SSM that integrates with the graph learning process. The SSSM improves the handling of heterogeneous data by selectively focusing on important features within the graph's structure. By leveraging this heterogeneous graph structure, Graph Mamba excels in modeling complicated real-world systems. The ability to learn from multiple entities and relationships makes it ideal for applications that need understanding multi-faceted interactions, such as biological system pattern detection."}, {"title": "3.2.3 Spatio-Temporal Graphs", "content": "Spatio-temporal graphs represent a sophisticated category of dynamic graphs that develop over time and simultaneously enclose spatial relationships among nodes. The graphs illustrate that both nodes and edges, along with their attributes, are affected by spatial proximity and temporal variations [20]. For example, SpoT-Mamba [25] further advances the graph construction by capturing long-range spatial and temporal dependencies using selective state-space modeling. This enables the model to concurrently learn from the spatial structure and the temporal dynamics of the graph. These graphs are particularly useful for tasks that require understanding both spatial and temporal dependencies, such as traffic flow prediction, social network analysis, and weather forecasting."}, {"title": "3.3 Selective Scanning Mechanisms for Graph Mamba", "content": "In Graph Mamba, selective scanning is a technique designed to handle graph-structured data in a parallel and recurrent manner. This technique preserves node and edge states that has information from previous stages of the graph traversal. This selective scanning extends the concept of SSMs for graph domains. The main components of this method include:\n\u2022 Node and Edge State Update: The selective scan mechanism changes the state of each node or edge in the graph according to the current input (node/edge characteristics) and the preceding state (data from adjacent nodes/edges). The graph topology determines the flow of information between nodes during this update.\n\u2022 Selective Propagation: The selective scan mechanism determines which components of the node/edge states to transmit across the graph and which components to forget or reduce in significance. This selectivity guarantees that only the most useful data from interconnected nodes and edges is retained, while less significant elements are excluded. This is essential for capturing long-range relationships in complex graph architectures.\n\u2022 Output Generation: This mechanism produces output for each node and edge after the processing of the chosen and modified states, which are used for further tasks such as node classification, link prediction, or graph classification.\nGraph Mamba models use various scanning mechanisms to rapidly explore and analyze graph structures, especially in dynamic and spatiotemporal contexts. These techniques help to optimize node and edge selection and traversal, improve computing performance, and ensure accurate graph dependency learning. The following are the primary scanning methods used in Graph Mamba models:\n(1) Graph Selective Scan:\nThis method selectively incorporates dynamic graph information to optimize which node or edge states are updated and transmitted throughout the graph. It prioritizes the most critical elements of the graph to provide efficient state updates while minimizing computational overhead [26] [27]. It is used mainly"}, {"title": "3.4 Training strategies", "content": "In the ML paradigm, various approaches are employed to train models, including supervised, semi-supervised, unsupervised, and self-supervised learning. Each of these strategies contributes significantly to the model's performance, generalization capabilities, and scalability. The learning method has a big effect on how well the model fits the data. This is particularly relevant for graph-based learning, where the complexity of the graph structures and the amount of labeled data available change significantly across different domains [35]. Graph-based learning introduces unique challenges, as graphs often encode intricate relationships between nodes and edges. The availability of labeled data is usually a challenge, which leads to exploring more flexible learning approaches such as semi-supervised, unsupervised, or self-supervised learning [36]. These approaches allow models to learn effectively from both labeled and unlabeled data or even from the structure of the graph itself. In the following sections, we provide a comprehensive review of the learning methods utilized in Graph Mamba models, highlighting how each approach is tailored to address the specific challenges of graph-based learning across various applications."}, {"title": "3.4.1 Supervised Learning Learning", "content": "Supervised Learning is the most used approach in many graph-based applications, especially in scenarios where high-quality labeled data is available [37]. In this method, models learn to predict the correct output based on the input using an annotated dataset. The supervised learning method allows Graph Mamba models to learn directly from labeled data, where the correct outputs are provided. This lets the model make more accurate predictions because it can change its internal representations repeatedly"}, {"title": "3.4.2 Semi-Supervised Learning", "content": "In semi-supervised learning, both labeled and unlabeled data are used to train the model [39]. This method of learning is especially useful in scenarios where labeled data is scarce but unlabeled data is abundant, a common situation in real-world graph datasets [40]. By combining the advantages of supervised and unsupervised learning, semi-supervised learning enables Graph Mamba models to leverage the structure of the graph and improve predictions even with limited labeled data. Semi-supervised learning is especially effective for dynamic or heterogeneous networks where the interconnections among nodes and edges change over time and identifying all data points could be challenging [33, 41]. This strategy helps the model learn better generalization by using data from both labeled and unlabeled parts of the graph. This makes it better at predicting results for nodes that aren't labeled."}, {"title": "3.4.3 Self-Supervised Learning", "content": "Self-supervised learning (SSL) is an emerging paradigm in which models learn from the data's inherent structure. In the context of graph-based learning, SSL has gained significant attention as it allows models to uncover meaningful patterns from the graph's topology and node features [42-45]. To do this, pretext tasks are added to the model to help it understand the data before the main task is conducted. In Graph Mamba models, SSL enables the model to capture long-range dependencies within the graph without needing labeled data. This makes it especially useful for dynamic systems, where labeled data is often sparse or unavailable. SSL also enhances the scalability of Graph Mamba models, as they can train on large, real-world graph datasets without being limited by the availability of labels [46, 47]."}, {"title": "3.4.4 Unsupervised Learning", "content": "Unsupervised learning is a powerful method for discovering hidden patterns or structures within data without requiring labels [48]. In the context of Graph Mamba models, unsupervised learning is utilized to derive significant representations of nodes and edges. These representations can subsequently be employed for downstream tasks, including graph generation or classification. Following this method, Graph Mamba models are very flexible in areas with less supervision because they can adapt to several applications using just the graph's structure [3, 49]. The main advantage of this method is the elimination of the requirement for human-annotated data.\nTo conclude, training strategies play a crucial role in the success and adaptability of Graph Mamba models. Supervised learning remains the go-to approach when labeled data is abundant, providing precise and rapid model convergence. Semi-supervised learning bridges the gap between labeled and unlabeled data, particularly effective for real-world scenarios where labeled data is scarce. Self-supervised learning stands out for its ability to leverage the graph's inherent structure, enabling models to uncover meaningful patterns and scale efficiently to large datasets. Lastly, unsupervised learning empowers Graph Mamba models to discover hidden relationships and structures within graph data."}, {"title": "4 Review of Recent Applications of Graph Mamba", "content": "By incorporating SSMs, Graph Mamba enables the representation of complex spatial and temporal dependencies in graph structures. These models excel in general-purpose applications like social networks, in addition to other specialized fields such as healthcare, biosignal analysis, and heterogeneous graph learning. This section reviews the diverse applications of Graph Mamba, highlights the innovations introduced in each domain, and examines the critical research publications that showcase its effectiveness."}, {"title": "4.1 General-Purpose Graph Learning and Benchmarking", "content": "The convergence of ML with graph data has led to groundbreaking advances across different domains. However, traditional graph learning methods have been limited in their ability to capture long-range dependencies and handle dynamic graph structures efficiently. To address these challenges, SSMs have emerged as a robust framework, enhancing graph learning by integrating temporal modeling and better management of complex relationships within graph data. This method extends the capabilities of graph-based methods by providing a mechanism to capture both spatial and temporal dynamics."}, {"title": "4.2 Knowledge and Heterogeneous Graphs", "content": "The development of ML models for heterogeneous and knowledge graphs has opened new avenues for handling multi-relational data structures. These graphs are commonly used in fields such as social networks, recommendation systems, and knowledge representation, where diverse entities and connections need to be effectively modeled. SSSMs offer a solid way to improve graph learning by including temporal dynamics and accurately capturing the complex relationships that exist in different types of graphs.\nTo address the challenges posed by heterogeneous graphs, Pang et al. [24] developed HeteGraph-Mamba, a model that uses SSMs to manage varied data structures with multiple node and edge types. It records different sorts of nodes and their connections using different edge types. The model includes a graph-to-sequence conversion approach that reduces complicated graph data while keeping structural subtleties. This strategy is"}, {"title": "4.3 Traffic and Environmental Forecasting", "content": "Traffic and environmental forecasting pose significant challenges that demand accurate modeling of complex spatiotemporal dynamics [51]. These fields require systems that can capture evolving patterns, such as fluctuating traffic flows, weather changes, and environmental shifts, across vast networks of interconnected nodes. Traditional methods often fall short in handling long-range dependencies and non-linear relationships"}, {"title": "4.4 Healthcare and Biosignals", "content": "Healthcare and biosignal analysis has recently emerged as critical areas for applying advanced ML models, especially those using graph-based approaches [55", "56": "."}]}