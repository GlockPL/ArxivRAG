{"title": "Implementing Derivations of Definite Logic Programs with Self-Attention Networks", "authors": ["Phan Thi Thanh Thuy", "Akihiro Yamamoto"], "abstract": "In this paper we propose that a restricted version of logical inference can be implemented with self-attention networks. We are aiming at showing that LLMs (Large Language Models) constructed with transformer networks can make logical inferences. We would reveal the potential of LLMs by analyzing self-attention networks, which are main components of transformer networks. Our approach is not based on semantics of natural languages but operations of logical inference. We show that hierarchical constructions of self-attention networks with feed forward networks (FFNs) can implement top-down derivations for a class of logical formulae. We also show bottom-up derivations are also implemented for the same class. We believe that our results show that LLMs implicitly have the power of logical inference.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) are giving strong impacts to our everyday life. Many people begin to make use of them in various manners and to expect that more power is given to them. An example of such power is logical inference. Some say that LLMs can make logical inference, and make discussions on the semantical correctness of outputs made by LLMs, where semantics are meanings of sentences in natural languages. Referring the theory of mathematical logic, the correctness of logical inference should be supported not only semantical manners but also operational. Operations of logical inference are methods for deriving conclusions from assumptions and showing the truth of sentences based on them. We take an operational approach towards showing the potential of logical inference in LLMs. More precisely we analyze the transformer networks, which are known as the fundamental mechanism of major LLMs, in particular, self-attention networks which are main components of transformers (Vaswani et al. 2017).\nAs logical inference mechanisms we employ top-down derivations for definite logic programs and queries. On the relation between bottom-up derivations of definite logic programs and neural networks, a stream of research starting with (Sakama, Inoue, and Sato 2017) has been made (Nguyen et al. 2018) (Aspis, Broda, and Russo 2018) (Nguyen et al. 2021) (Sakama, Inoue, and Sato 2021). Each of these presents a method to represent a definite logic program with a matrix so that matrix multiplication plus some additional operation corresponds to bottom-up derivation. We first show that top-down derivations can be implemented by a type of self-attention networks. Also we show that the bottom-up derivation treated in the previous research can be implemented by another type of self-attention networks.\nA self-attention network takes three inputs: a vector representing a query, a matrix representing a set of keys, and a matrix representing a set of values. These inputs remind us the operations of derivation made of a query, the head of a definite clause, and its body. Our fundamental idea is to make correspondence between the inputs of self-attention networks and the three operations of derivation. We also employ the hardmax function instead of the softmax function used in self-attention networks. This is from the previous research on the analysis of self-attention networks on the viewpoints of acceptors of formal languages (Hahn 2020) (Yao et al. 2021) (P\u00e9rez, Barcel\u00f3, and Marinkovic 2021)."}, {"title": "Preliminaries", "content": "Following (Vaswani et al. 2017) the encoder part of a transformer is constructed of a position encoder of the inputs and N layers of neural networks following the position encoder. Each layer consists of two components, a self-attention network and a feed-forward network (FFN). In this paper we do not use the position encoder. The self-attention network in (Vaswani et al. 2017) takes a set of queries, a set of keys, and a set of values as its inputs and outputs a new set of queries. All queries, keys, and values are vectors. Following the original paper, let $q_k$ be a raw-vector for a query, and let\n$K =\n\\begin{pmatrix}\nk_1\\\\k_2\\\\:\\\\k_m\n\\end{pmatrix}$\nand $V =\n\\begin{pmatrix}\nV_1\\\\V_2\\\\:\\\\V_m\n\\end{pmatrix}$\nbe respectively arrays for a set of keys and a set of values. The function of the k-th layer of a self-attention network is\n$Attention(q_k, K, V) = softmax(\\frac{q_kK^T}{\\sqrt{d}})V,$\nwhere d is the dimension of vectors for queries and keys. In our discussion we omit the normalization with $\\sqrt{d}$ and replace the softmax function with the hardmax function as in the analysis of self-attention networks with traditional theories of formal languages (Hahn 2020; Yao et al. 2021). The hardmax function is defined as\n$hardmax(x_1,...,x_m) = (y_1,..., y_m)$,\nwhere\n$y_i =\n\\begin{cases}\n\\frac{1}{M} & \\text{ if } x_i \\text{ is a maximum of } (x_1, ..., x_m), \\\\\n0 & \\text{ otherwise}\n\\end{cases}$\nand M is the number of maximums in $(x_1,..., x_m)$. Then the self-attention function (1) is represented as\n$a^k = Attention(q^*, K, V) = \\sum_{j=1}^{m} s_jV_j$, where\n$(s_1,...,s_m) = hardmax(((q^k, k_1), ..., (q^k, k_m)))$.\nLet the FFN following the self-attention implement a function f. The output of the k-th layer is\n$q^{k+1} = f(a^k)$,\nand this is passed to the $(k + 1)$-th layer as its input."}, {"title": "Self-Attention Networks"}, {"title": "Definite Logic Programs and Derivations", "content": "Every formula in porpositional logic consist of propositional variables and logical connectives. In our case we use two logical connectives: $\\wedge$ meaning \"AND\" and $\\leftarrow$ meaning \"IF\". For example, $p \\leftarrow q\\wedge r$ is interpreted as \"A proposition p holds if both q and r hold.\" In this paper we call every propositional variable a propositional symbol, or simply, a symbol.\nWe give a simple example of logical formulae which we treat in our discussion. Let the set of propositional symbols be p, q, r, s, t, u, and w. We prepare a special symbol $\\top$, which means \"TRUE\", and $\\bot$, which means \"FALSE\". We say a logical formula is a conjunction or a query if it contains a single symbol or multiple symbols connected with $\\wedge$. For example,\n$p\\wedge q\\wedge r$, p, and $\\top$\nare conjunctions, and also called queries. We say a logical formula is a definite clause if it contains a connective $\\leftarrow$ with a single symbol on the left-hand side and a conjunction on the right-hand side. For example,\n$p\\leftarrow q\\wedge r$\n$q\\leftarrow s$\n$r\\leftarrow s\\wedge t$\n$s\\leftarrow u$\n$t\\leftarrow \\top$\n$u\\leftarrow \\top$\n$w\\leftarrow \\bot$\nare definite clauses. The lhs of a definite clause is called its head and the conjunction of its rhs is called its body.\nWe explain top-down derivations with a simple example. Let P denote the set of definite clauses above. A top-down derivation starts with a query. Assume that a query consisting of one symbol p is given. Then a definite clause in P whose head matches with p is searched. In this case p \u2190 q\u0245r is found. Then p in the query is replaced with the body of the definite clause, and a goal clause\nq\u0245r\nis derived. Next a definite clause whose head matches q and a clause whose head matches r are searched, and a query\ns\u0245st\nis derived. By the idempotent property of \u0245, the query is simplified into\ns\u0245t.\nIn repetition of the same operation we eventually obtain a derivation sequence illustrated in Fig. 1. The last query is simplified into T, which means \"The first query p is proved.\"\nWe give formal definitions. Let \u03a0 be a finite set of propositional symbols. A conjunction is a formula of the form\n$q_1 \\wedge ... q_n  \\quad (n \\geq 1)$,\nwhere $q_1, ..., q_n \\in \\Pi \\cup{\\top, \\bot}$. A conjunction is also called a query. Since we employing propositional logic, we assume that $q_1,...,$ and $q_n$ are mutually different. A defnite clause C is a formula of the form\n$p\\leftarrow q_1\\wedge q_2 \\wedge ... \\wedge q_n \\quad (n \\geq 1)$\nsuch that $p, q_1,..., q_n \\in \\Pi$,\nor\n$p\\leftarrow \\top$\n$p\\leftarrow \\bot$\nwith $p\\in \\Pi$. The proposition p is called the head of the clause and denoted by $head(C)$. The conjunction $q_1 \\wedge q_2 \\wedge ... \\wedge q_n$, $\\top$, or $\\bot$ is called its body and denoted by $body(C)$. A finite set of definite clauses is called a definite program. We assume that every program must satisfy the restriction that no pair of definite clauses in the program share their head. Such a program is called an SD-program (Nguyen et al. 2018).\nGiven an SD-program P, a one-step top-down derivation of a conjunction as a query (2) is to find, for each i (0 < i < m), a definite clause in P of the form\n$p_i\\leftarrow q_{i1} \\wedge ... \\wedge q_{in_i}$,\nPrecisely speaking, formulae of the form (3) are not allowed in (Nguyen et al. 2018)"}, {"title": "Implementing Top-Down Derivations with Self-Attention Networks", "content": "First we give an illustration with the previous example. From each definite clause C in P we make two vectors $\\mathbf{h_c}$ and $\\mathbf{b_c}$ with a vector of 9 dimensions. Each dimension corresponds to p, q, r, s, t, u, w, T, and \u22a5. The vector $\\mathbf{h}$ shows the symbol appearing in the head, and $\\mathbf{b}$ appearing in the body. Clearly every $\\mathbf{h}$ is a unit vector. From the first definite clause in P we get two vectors\n$\\mathbf{h_c} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix} = (1,0,0,0,0,0,0,0,0)$ and\n$\\mathbf{b_c} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix}= (0, 1, 1, 0, 0, 0, 0, 0, 0)$.\nSince P is an SD-program, we can indicate these vectors with p = head(C) and write $\\mathbf{h_p}$ and $\\mathbf{b_p}$ instead of $\\mathbf{h_c}$ and $\\mathbf{b_c}$. Additionally we prepare two vectors for each of T and \u22a5 as follows:\n$\\mathbf{h_{\\top}} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix} = (0,0,0,0,0,0,0,1,0)$,\n$\\mathbf{b_{\\top}} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix} = (0,0,0,0,0,0,0,1,0)$,\n$\\mathbf{h_{\\bot}} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix} = (0,0,0,0,0,0,0,0,1)$, and\n$\\mathbf{b_{\\bot}} =\n\\begin{pmatrix}\np\\\\q\\\\r\\\\s\\\\t\\\\u\\\\w\\\\T\\\\\\bot\n\\end{pmatrix} = (0,0,0,0,0,0,0,0,1)$.\nFrom vectors $\\mathbf{h_p}, \\mathbf{h_q},..., \\mathbf{h_{\\top}}$ we get an identity matrix $I_9$ of 9-dimension. In implementing a one-step top-down derivation with a self-attention network we substitute $I_9$ to the key matrix K in (1). To the value matrix V in (1), we substitute\n$B_P =\n\\begin{pmatrix}\n\\mathbf{b_p}\\\\ \\mathbf{b_q}\\\\ \\mathbf{b_r}\\\\ \\mathbf{b_s}\\\\ \\mathbf{b_t}\\\\ \\mathbf{b_u}\\\\ \\mathbf{b_w}\\\\ \\mathbf{b_{\\top}}\\\\ \\mathbf{b_{\\bot}}\n\\end{pmatrix} =\n\\begin{pmatrix}\np & q & r & s & t & u & w & T & \\bot\\\\\n0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix}$.\nAs the function implemented by the FFN following the self-attention network, we use the dimension-wise Heaviside function\n$\\mathbf{H}((v_1, v_2, ...)) = (H_0(v_1), H_0(v_2),...)$,\nwhere $H_0$ is a Heaviside function defined as\n$H_0(x) =\n\\begin{cases}\n1 & \\text{ if } x > 0, \\\\\n0 & \\text{ otherwise}\n\\end{cases}$.\nThe top-down derivation illustrated in the last section is represented as follows. First we represent the query consisting only of p as a vector\n$\\mathbf{q^1} = (1, 0, 0, 0, 0, 0, 0, 0, 0)$\nand put this into the first layer as its input. The computation of $(s_1,..., s_9)$ is\n$hardmax((\\mathbf{q^1}, \\mathbf{k_1}), ..., (\\mathbf{q^1}, \\mathbf{k_9})) = (1, 0, 0, 0, 0, 0, 0, 0, 0)$\nand so the output of the self-attention network is\n$\\mathbf{a^1} = (0, 1, 1, 0, 0, 0, 0, 0, 0)$.\nNext we put $\\mathbf{a^1}$ into the FFN following the self-attention network and put the result $\\mathbf{q^2} = H(\\mathbf{a^1})$ into the second layer as its input, which represents q\u0245 r. Then\n$hardmax((\\mathbf{q^2}, \\mathbf{k_1}), ..., (\\mathbf{q^2}, \\mathbf{k_9})) = (0, \\frac{1}{2}, \\frac{1}{2}, 0, 0, 0, 0, 0, 0)$\nand the result of the self-attention network is\n$\\mathbf{a^2} = (0, 0, 0, \\frac{1}{2}, \\frac{1}{2}, 0, 0, 0, 0) = (0, 0, 0, 0.5, 0.5, 0, 0, 0, 0)$.\nThe output of the layer is\n$\\mathbf{q^3} = H(\\mathbf{a^2}) = (0, 0, 0, 1, 1, 0, 0, 0, 0)$,\nwhich represents s \u0245 t. In the same manner, by letting $\\mathbf{q^{k+1}} = H(\\mathbf{a^k})$, we get\n$hardmax((\\mathbf{q^3}, \\mathbf{k_1}), ..., (\\mathbf{q^3}, \\mathbf{k_9})) = (0, 0, 0, \\frac{1}{2}, \\frac{1}{2}, 0, 0, 0, 0)$,\n$\\mathbf{a^3} = (0, 0, 0, 0, 0, \\frac{1}{2}, 0, \\frac{1}{2}, 0)$,\n$\\mathbf{q^4} = H(\\mathbf{a^3}) = (0, 0, 0, 0, 0, 1, 0, 1, 0)$,"}, {"title": "Implementing Bottom-Up Derivation with Self-Attention Network", "content": "In this section we explain the method of implementing the bottom-up derivation of an SD-program proposed in (Nguyen et al. 2018) with the self-attention netwroks. For an SD-program P a program matrix $M_P$ is defined in a slightly different manner of the body matrix $B_P$.\nFirst we have to note that they do not use the dimension of T or \u22a5 and therefore vectors in $\\mathbb{R}^N$ is used.\nFor a definite clause C we define a progam vector $m_C = (M_1, M_2,..., m_N)$ as\n$M_i =\n\\begin{cases}\n1 & \\text{if $p_i$ appears in body (C)}, \\\\\n\\frac{1}{\\mid body(C) \\mid} & \\text{if body(C) = $\\top$ and head(C) = $p_i$}, \\\\\n0 & \\text{otherwise},\n\\end{cases}$\nwhere $M$ is the number of symbols in the body of the definite clause. The program matrix $M_P$ is constructed by putting the program vectors vertically. For example, the program matrix of the SD-program presented in Section 2.2 is\n$M_P =\n\\begin{pmatrix}\np & q & r & s & t & u & w\\\\\nm_p\\\\ m_q\\\\ m_r\\\\ m_s\\\\ m_t\\\\ m_u\\\\ m_w\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{pmatrix}$.\nThe vector q represents such an interpretation that the proposition p is interpreted to be true if and only if the dimension of p of q is 1. Then the bottom-up derivation is represented with the attention function (1) by replacing the softmax function with the dimension-wise identity function and letting K = $M_P$ and V = $H_P$.\nProposition 1. Assume that the self-attention function employs the hardmax function and is followed by the dimension-wise Heaviside function H. Let P be an SD-program and q is a vector representing a query Q. Then the output H(Attention(q, Hp, Bp)) represents the query Q' obtained by one-step top-down derivation of Q."}, {"title": "Conclusion", "content": "In this paper we show that top-down derivations from a conjunction of propositions as queries with an SD-program are represented by self-attention networks followed by the FFNs representing the dimension-wise Heaviside function. The self-attention network is modified by replacing the softmax function with the hardmax function. We also show that bottom-up derivations from an interpretation with an SD-program are represented by the self-attention networks with the element-wise identity function as the substitution of the softmax function. We believe that our results show that LLMs implicitly have the power of logical inference.\nThe previous work listed in Section 1 proposed that bottom-up derivations for several extensions of definite logic programs can be represented by operations of tensors. We conjecture that our representation method could be modified for some of such extensions by using the multi-head attention networks (Vaswani et al. 2017).\nThe replacement of the softmax function with the hardmax is required by the point that we are based on the traditional binary-valued propositional logic. The original definition of the self-attention networks employs the softmax function because LLMs works in probabilistic manners. Our future work includes some extensions of our discussion to probabilistic propositional logic so that we could show more potentials of practical uses of LLMs."}]}