{"title": "NAVIGATION WITH QPHIL: QuanTIZING PLANNER\nFOR HIERARCHICAL IMPLICIT Q-LEARNING", "authors": ["Alexi Canesse", "Mathieu Petitbois", "Ludovic Denoyer", "Sylvain Lamprier", "R\u00e9my Portelas"], "abstract": "Offline Reinforcement Learning (RL) has emerged as a powerful alternative to\nimitation learning for behavior modeling in various domains, particularly in com-\nplex navigation tasks. An existing challenge with Offline RL is the signal-to-noise\nratio, i.e. how to mitigate incorrect policy updates due to errors in value estimates.\nTowards this, multiple works have demonstrated the advantage of hierarchical of-\nffline RL methods, which decouples high-level path planning from low-level path\nfollowing. In this work, we present a novel hierarchical transformer-based ap-\nproach leveraging a learned quantizer of the space. This quantization enables the\ntraining of a simpler zone-conditioned low-level policy and simplifies planning,\nwhich is reduced to discrete autoregressive prediction. Among other benefits,\nzone-level reasoning in planning enables explicit trajectory stitching rather than\nimplicit stitching based on noisy value function estimates. By combining this\ntransformer-based planner with recent advancements in offline RL, our proposed\napproach achieves state-of-the-art results in complex long-distance navigation en-\nvironments.", "sections": [{"title": "1 INTRODUCTION", "content": "Navigation and locomotion in complex, embodied environments is a long-standing challenge within\nMachine Learning (Kaelbling et al., 1996; Sutton & Barto, 2018). Operating non-trivial agents in\ncomplex spaces is critical in a wide range of real-world applications, such as in robotics or in the\nvideo game industry. A core difficulty of navigation lies in solving long-horizon tasks that require\nintricate path planning (Hoang et al., 2021; Park et al., 2024). In the Reinforcement Learning (RL)\nsetting (Sutton & Barto, 2018), traditional online Goal-Conditioned deep Reinforcement Learning\n(GCRL) methods often struggle with such long-horizon tasks because of the sparse nature of the\nreward signal, leading to hard exploration problems. Offline GCRL circumvents this exploration\nproblem by leveraging large amounts of unlabeled and diverse demonstration data to learn policies\nthrough passive learning (Prudencio et al., 2023). A core advantage of offline RL compared to\nother forms of behavior extraction from datasets \u2013 for instance imitation learning \u2013 is the ability to\nimprove over suboptimal datasets, e.g. by learning state value functions to bias the learned policy\ntowards rewarding actions (Kostrikov et al., 2021). However, offline RL is not trivial to apply for\nlong-horizon goal-reaching tasks, which often provide sparse reward signals, leading to a noisy value\nfunction which consequently hinders the performance of the policy. Part of this issue comes from\nlow \"signal-to-noise\u201d ratio to learn the value function (Park et al., 2024). Because a suboptimal\naction can be corrected quickly in subsequent steps of a trajectory, its impact on the real value\nfor faraway goals can be covered by the value prediction noise, which can lead to the learning of\nsuboptimal behaviors for the policy.\nHierarchical architectures have shown considerable advantages in goal-conditioned navigation to\nsolve such issues (Vezhnevets et al., 2017; Pertsch et al., 2021; Park et al., 2024). These approaches\neffectively decompose the problem into two distinct components: high-level path planning and low-\nlevel path following. Among these, Hierarchical Implicit Q-Learning (HIQL) (Park et al., 2024) has\nrecently emerged as the state-of-the-art method. HIQL leverages a hierarchical structure to learn"}, {"title": "2 RELATED WORK", "content": "VQ-VAEs for reinforcement learning Vector Quantized Variational Autoencoders (Van\nDen Oord et al., 2017) have demonstrated their utility in reinforcement learning, particularly for\noffline setups. Jiang et al. (2022) and Luo et al. (2023) use a VQ-VAE to discretize continuous ac-\ntion spaces, mitigating the curse of dimensionality. In contrast, QPHIL uses a VQ-VAE to quantize\nstates, simplifying waypoint generation in long-horizon tasks. These two approaches are comple-\nmentary: action quantization focuses on simplifying policy search, while state quantization helps in\ntask decomposition. Within goal-conditioned reinforcement learning, several works used VQ-VAE\nto encode observations into discrete (sub)goals. Lee et al. (2024a) learn quantized goals to simplify\ncurriculum learning. Islam et al. (2022) also use quantization to map (sub)goals into discrete and\nfactorized representations to efficiently handle novel goals at test time. Both these works focus on\nonline goal-conditioned reinforcement learning while QPHIL tackles the offline learning setting.\nKujanp\u00e4\u00e4 et al. (2023) also uses VQ-VAE to generate discrete subgoals, but while they rely on a\nfinite subgoal set derived from offline learning, QPHIL identifies subgoals directly through the VQ-\nVAE. To the best of our knowledge, VQ-VAE has never been applied for discrete planning in offline\nGCRL settings.\nHierarchical offline learning Leveraging a hierarchical structure to decompose and simplify se-\nquential decision-making problems is a long-standing idea and subject of research within machine\nlearning (Schmidhuber, 1991; Sutton et al., 1999). Recently, multiple successful works renewed\nthe interest of the research community to these models, both for online (Vezhnevets et al., 2017;\nPertsch et al., 2021; Kim et al., 2021; Fang et al., 2022) and offline reinforcement learning (Nachum\net al., 2018b; Ajay et al., 2020; Rao et al., 2021; Rosete-Beas et al., 2023; Yang et al., 2023; Shin\n& Kim, 2023). Among these, Hierarchical Implicit Q-Learning (HIQL) from Park et al. (2024) has\nemerged as the state-of-the-art method for offline goal-conditioned RL. Most methods differ in how"}, {"title": "3 PRELIMINARIES", "content": "Offline Goal Conditioned Reinforcement Learning We frame our work in the context of offline\ngoal-conditioned Reinforcement Learning (offline GCRL), which involves training an agent to in-\nteract with an environment to reach specific goals, without getting access to the environment itself\nat train time. It is typically modeled as a Markov Decision Process (MDP) defined by a tuple"}, {"title": "4 QUANTIZING PLANNER FOR HIERARCHICAL IMPLICIT LEARNING", "content": "In this paper, we propose a new hierarchical goal conditionned offline RL algorithm: Quantizing\nPlanner for Hierarchical Implicit Q-Learning (QPHIL). While current hierarchical methods rely\nheavily on continuous waypoint predictions, we propose to consider discrete subgoals, allowing\nto simplify the planning process. Instead of relying on precise coordinates, QPHIL identifies key\nlandmarks to guide trajectory planning, much like how a road trip is described by cities or highways\nrather than specific geographic points. The algorithm detects these landmarks, creates landmark-\nbased sequences, and formulates policies to navigate between them, ultimately reaching the target\ndestination."}, {"title": "4.1 OVERALL DESIGN", "content": "QPHIL operates through four components: a state quantizer $\\phi$, which divides the state set into a finite\nset of landmarks, a plan generator $\\pi^{plan}$, which acts as a high-level policy to generate a sequence\nof landmarks to be reached given the final goal, and two low-level policy modules: $\\pi_{landmark}$, which\ntargets state areas defined as landmarks, and $\\pi_{goal}$, which targets a specific state goal."}, {"title": "4.2 TOKENIZATION", "content": "As defined in the previous section, the state quantizer $\\phi$\nis used to map raw states into a set of k tokens. It is\ncomposed of a state encoder $g : S \\rightarrow R^{d}$, which maps the\nstate space to a latent space of dimension d, and a learned\ncode-book $z : \\Omega \\rightarrow R^{d}$, which associates each token to\na continuous embedding. From these, $\\phi$ is defined as:\n$\\phi(s) = arg min_{w \\in \\Omega} \\Delta(g(s), z(w))$, with $\\Delta$ the euclidean\ndistance in the latent space.\nAs depicted in Figure 7, three losses are considered to\ntrain these components. First, the VQ-VAE learns a\nmeaningful representation by considering a reconstruc-\ntion loss, which ensures that states can be decoded from\ntokens they are projected into:\n$L_{recon} = E_{s \\sim D}||f(z(\\phi(s)))) \u2013 s||^{2}$,\nwhere D is the dataset and $f : R^{d} \\rightarrow S$ is a decoder net-\nwork trained together with the code-book of token em-\nbeddings.\nAs the gradient flow stops in the previous loss due to the discrete projection performed by $\\phi$, we\nneed to consider an additional commitment loss to learn the encoder parameters:\n$L_{commit} = E_{s \\sim D}||g(s) \u2013 sg(z(\\phi(s)))||^{2}_{2} + \\beta||sg(g(s)) \u2013 z(\\phi(s))||^{2}_{2}$ ,\nwhere sg is the stop-gradient operator. The first term of this loss aims at attracting encoder outputs\nclose to the code of the token states project into. As explained by authors of the VQ-VAE archi-\ntecture (Van Den Oord et al., 2017), the embedding space grows arbitrarily if the token embedding\ndoes not train as fast as the encoder parameters. The second term, weighted by a hyperparameter $\\beta$,\naims to prevent this issue by forcing the encodings to commit to a code-book's embedding.\nTo introduce dynamics of the environment in the representation learned, we consider a third con-\ntrastive loss that incentivize temporally close states to be assigned the same tokens, while temporally\ndistant states receive different tokens. To achieve this, we use the triplet margin loss (Balntas et al.,\n2016) applied to our setting:\n$L_{contrastive} = E_{\\substack{st\\sim D\\k\\sim[-\\delta,\\delta]\\\\k'\\sim[-\\delta,\\delta]}} max {\\Delta(g(st), g(st+k)) \u2013 \\Delta(g(st), g(s_{t+k'})), 0}$ ,\nwhere d is the time window used to specify temporal closeness of states in demonstration trajectories.\nThis loss is of crucial importance in navigation, for instance in settings with thin walls, where two\nstates can be close in the input space while corresponding to very different situations.\nThe tokenizer loss, used in training is then a linear combination of the three previous losses:\n$L_{tokenizer} = L_{recon} + \\lambda_{1}L_{commit} + \\lambda_{2}L_{contrastive}$ ."}, {"title": "4.3 SEQUENCE GENERATION", "content": "Once the tokenizer has been trained, each state from dataset trajectories can be discretized leading to\nsequences of tokens. Temporal consistency induces sub-sequences of repeated tokens corresponding\nto positions within a given region. By applying a simple post-processing step $\\eta(\\tau)$ that removes\nconsecutive repetitions of tokens in a trajectory $\\tau$, we obtain more concise sequences that succinctly\nrepresent key zones to traverse in the correct order. For instance, a tokenized sequence such as \u201c1 1\n1 2 2 3 3 3 4 4\" is simplified to \"1 2 3 4\", reflecting the core structure of the trajectory. This process\nis illustrated in Figure 8.\nThen, the planner $\\pi^{plan}$ is trained following a teacher forcing approach on trajectories from D, con-\nsidering any future token of $\\tau \\in D$ as the associated training goal:\n$L_{plan} = -E_{\\mathcal{T} \\sim D} E_{t<|T|,  g \\in G , \\phi(T_{t})\\neq \\phi(T_{t+1})} \\mathbb{I} _{\\phi(g) \\in \\eta(\\phi(\\mathcal{T} < t))} log \\pi^{plan}(\\phi(T_{t+1}) | \\phi(g), \\eta(\\phi(\\mathcal{T} \\leq t)))$"}, {"title": "4.4 Low-LEVEL POLICIES", "content": "As described above, two low-level policies $\\pi_{landmark}$ and $\\pi_{goal}$ have to be trained to perform actions in\nthe environment. Both of these policies are trained via IQL, whose principle is described in section\n3. As $\\pi_{goal}$ works with initial (non tokenized) goals to allow final precise navigation in the final area\nof the task, we consider a classical V network $V_{goal}$ trained using equation 7 on raw states and goals,\nindependently from all other components, from which the policy is extracted using AWR:\n$\\mathcal{L}_{\\pi_{goal}} = E_{(s_t,s_{t+1},g)\\sim D}[exp(\\beta\\cdot(V_{\\theta}(s_{t+1},g) - V_{\\theta}(s_t,g))\\log\\pi_{goal}(s_{t+1}|s_t,g)]$ (2)\nOur policy $\\pi_{landmark}$ uses the same principle, but replaces the distant goal g by sub-goals defined as\nthe next landmark to be reached from quantized trajectories in the buffer of training transitions. That\nis, given any training trajectory $\\mathcal{T} = ((s_t, a_t, s_{t+1},g))_{t=0}^{|\\mathcal{T}|-1}$, we consider a sequence of relabelled\ntransitions $\\mathcal{T} = ((s_t, a_t, s_{t+1}, next(\\mathcal{T}, t)))_{t=0}^{|\\mathcal{T}|-1}$, where $next(\\mathcal{T}, t) = \\phi (s_{min({l\\in[t+1,|\\mathcal{T}|]:\\phi(s_l)\\neq\\phi(s_t)})})$ corresponds to the next different subgoal in the sequence of quantized states $(\\phi(s_t))_l$ of the\ntrajectory $\\mathcal{T}$. $\\pi_{landmark}$ is then trained on that set of relabelled transitions $\\mathcal{D}$. Then, the policy is\nextracted similarly as for $\\pi_{goal}$ using equation 2 on $\\mathcal{D}'$ rather than D, in order to obtain $\\pi_{landmark}$ via a\nrelabelled IQL from $V^{\\pi_{landmark}}$ ."}, {"title": "5 EXPERIMENTS", "content": "Our experiments aim to address the following questions:\n1. Does QPHIL architecture enable efficient long-term navigation?\n2. Can QPHIL handle sparse data scenarios using token-level stitching?\n3. Does QPHIL still performs in diverse state-target initialization?\n4. What is the impact of the contrastive loss used for landmark learning?"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "The following section provides details on environments, datasets and baselines used to study QPHIL."}, {"title": "5.2 DOES QPHIL ARCHITECTURE ENABLE EFFICIENT LONG-TERM NAVIGATION ?", "content": "We first analyze the performance in success rate of the different baselines on the state-based\nAntMaze-{Medium,Large,Ultra} settings. As usual in D4RL Antmaze benchmarks (Park et al.,\n2024), starting state and target goals are sampled near two reference points, forcing the agent to\nwalk across the entire maze. Table 1\nshows the results of our experiment on the set of AntMaze map where we denote \"w/aug.\" in the\ncase of stitching data augmentation and \"w/o aug\" otherwise. We see that our method is near the\nstate-of-the art on smaller maps,. QPHIL outper-\nforms all other methods on the larger maps, reaching 70% success-rate on AntMaze-ultra which is\nthe best to our knowledge on this benchmark, beating HIQL by at least 10% on average on top of\nhaving a lower standard deviation."}, {"title": "5.3 CAN QPHIL HANDLE SPARSE DATA SCENARIOS USING TOKEN-LEVEL STITCHING ?", "content": "To test QPHIL's scaling ability in more difficult settings, we created a larger AntMaze environment\ncalled AntMaze-Extreme along with two datasets variants \"diverse\" and \"play\". As\nshown in , in AntMaze-Extreme QPHIL attains up to 50% success rate, which is signif-\nicantly above baseline results. While QPHIL remains competitive in short-term settings, our approach is especially\nwell suited for long-distance goal reaching navigation, in which the explicit stitching strategy af-\nforded by our space discretization is crucial."}, {"title": "6 CONCLUSION", "content": "We proposed QPHIL, a hierarchical offline goal-conditioned reinforcement learning method that\nleverages pre-recorded demonstration to learn a discrete representation and temporally consistent\nrepresentation of the state space. QPHIL utilizes those discrete state representations to plan sub-\ngoals in a discretized space and guide a low policy towards its final goal in a human-inspired man-\nner. QPHIL reaches as a result top performance in challenging long-term navigation benchmarks,\nshowing promising next steps for discretization and planning in continous offline RL settings.\nLimitations and next directions QPHIL is a method aimed at navigation as it aims to solve tasks\nwhere a low amount of landmarks is sufficient. An interesting direction would be to analyse how to\nleverage such discrete representation in more intricate planning settings, such as multi-task robotics\nscenarios. Also, while leveraging the planning capabilities of QPHIL for model predictive control\nusecases could be an interesting follow up, enhancing the learning of the landmarks to adapt their\nsize or spread regrading the uncertainty or the importance that characterize each area also constitute\nvery promising next steps."}, {"title": "B.1 IMPLEMENTATION AND TRAININGS", "content": "Our implementation of QPHIL is based on the Pytorch machine learning\nlibrary. It is available in the supplementary materials as a .zip file and will be next available in a\npublic dedicated repository. We ran our experiments on a GPU cluster composed of Nvidia V100\nGPUs, each training taking approximately 10 hours in the right conditions."}, {"title": "B.2 HYPER-PARAMETERS", "content": "We present bellow the architectural choices and the hyper-parameters used to produce the results\npresented in Table 1 and Figure 6.\nVQ-VAE For the VQ-VAE, we based our implementation on the git repository. We utilize for all AntMaze\nvariants as the encoder and the decoder a simple 2-layer MLP with ReLU activations and hidden\nsize of 16 and latent dimension of 8. For antmaze, we add a gaussian noise to the input positions\nand we normalize the positions before feeding them to the encoder. Also, we vary the number of\nencodings in the codebook as the map grows. We provide bellow the complete list of used hyper-\nparameters in Table 2. Unless specified, the VQ-VAE hyper-parameters are the defaults ones from\nthe initial library's implementation.\nTransformer For the transformer, we used as described an encoder-decoder architecture inspired\nby the paper Vaswani et al. (2017) provided by the torch.nn library. We use a max sequence length\nof size 128, an embedding dimension of 128, a feed-forward dimension of 128, 4 layers of 4 heads\nand a dropout of 0.2. We train our model with 250 epochs when we apply stitching and 2500 epochs\notherwise. We perform validation computation with a 0.95 dataset split and perform sampling with\na temperature of 0.9. We optimize our model using the Adam optimizer with a learning rate of 1e-5.\nAll of those results are also presented in Table 3 bellow.\nLow subgoal policy For the low subgoal policy and its value function, we adapt the policy pro-\nposed by HIQL. For the value function, we use a MLP policy of 3 layers with hidden size of 512\nand GeLU activations. We apply layer normalization for each layers and initialize the weights with\na variance scaling initialization of scale 1. No dropout is used for the value function. For the pol-\nicy, we use a two layer MLP with hidden size of 256 and ReLU activations. We don't apply layer\nnorm and initialize the parameters though a variance scaling initialization of scale 0.01. Our policy"}, {"title": "D ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "D.1 WHAT IS THE IMPACT OF THE CONTRASTIVE LOSS USED FOR LANDMARK LEARNING ?", "content": "The use of a contrastive loss is essential in the context of high dimensional data, where the VQ-\nVAE reconstruction loss is not enough to learn temporally consistent latent encodings (meaning that\ntemporally nearby states share spatially nearby encodings).\nTo assess the impact of the contrastive loss on the learned landmarks, we compute, for each token,\nthe minimum and maximum distances between states position and their corresponding codebook's\ndecoded position. This is represented as histograms in . While the unconstrained VQ-VAE\ntends to allocate higher token density in areas of higher data density, we observe that the contrastive\nloss results in a smoother repartition of the tokens, which in consequence increases the performance\nof our model. This allows to stabilize conditonning in areas of high data density. Further analysis is\nprovided in appendix E."}, {"title": "D.2 DOES QPHIL STILL PERFORMS IN DIVERSE STATE-TARGET INITIALIZATIONS ?", "content": "Previous sections relied on evaluating performance\nin AntMaze environments by classically sampling\ninitial states so and goals g from narrow distribu-\ntions near two fixed points, requiring the agent to\ncross the entire maze. Regarding QPHIL, given such\nstate and goal distributions do not span across multi-\nple learned landmarks, each sampled navigation sce-\nnario leads to the similar landmark-based condition-\ning for our low-level policy, which is not convenient\nto conduct a comprehensive performance evaluation.\nConsequently, we designed Random-AntMaze eval-\nuation environments, which cycles through a diverse\nset of 50 couples of (50, g) allowing a more rigorous\ntest of the generalization capabilities of our model.\nTable 5 showcases the performance of HIQL and QPHIL on Random-AntMaze-Ultra and Random-\nAntMaze-Extreme. In the random initialization setting QPHIL still performs significantly better\nthan HIQL, reaching up to 20% improvement in success rate, which is the previous best method to\nour knowledge."}, {"title": "D.3 DOES RL MATTER FOR THE HIGH-POLICY IN ANTMAZE ?", "content": "As we use imitation learning in the our high-level policy, we tested the performance of HIQL with\na behavior cloning high level policy to look the potential impact of losing the RL weighting. As\nsuch, we tested HIQL with a classical Behavioral Cloning learning of the high level policy for\nthe AntMaze-Ultra variants.. We see no significative\ndifference in performance between the two approaches, meaning that RL in high levels doesn't\nnecessarily improve performance on AntMaze settings."}, {"title": "D.4 WHAT ABOUT REPLANNING AND A CLOSED LOOP FORMAT ?", "content": "QPHIL's high policy allows full planning in an open-loop manner and shows great performance by\ndoing so. In the experiments reported in the main body of this paper, we considered this open-loop\nversion that plans the sequence of subgoals at the start of the episode and\ndoesn't perform any further replanning then. The tokens from the initial plan are consumed each\nafter the other once they have been reached. However, if the low policy makes a mistake and goes\ninto an unexpected landmark, the initial can become obsolete, more optimal paths could be consider. This\narea is provided."}, {"title": "E WHAT IS THE IMPACT OF THE CONTRASTIVE LOSS USED FOR LANDMARK LEARNING ?", "content": "The use of a contrastive loss is paramount in the context of high dimensional data, where the VQ-\nVAE reconstruction loss is not sufficient to learn temporally consistent latent encodings (meaning\nthat temporally nearby states share spatially nearby encodings). In our specific case, as we decode\nthe positions, we observe experimentally an amount of consistency in our latent representations, as\nshown in Figure 14."}]}