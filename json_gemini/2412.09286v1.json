{"title": "Learning Novel Skills from Language-Generated Demonstrations", "authors": ["Ao-Qun Jin", "Tian-Yu Xiang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Yue Cao", "Sheng-Bin Duan", "Fu-Chao Xie", "Zeng-Guang Hou"], "abstract": "Current robot learning algorithms for acquiring novel skills often rely on demonstration datasets or environment interactions, resulting in high labor costs and potential safety risks. To address these challenges, this study proposes a skill-learning framework that enables robots to acquire novel skills from natural language instructions. The proposed pipeline leverages vision-language models to generate demonstration videos of novel skills, which are processed by an inverse dynamics model to extract actions from the unlabeled demonstrations. These actions are subsequently mapped to environmental contexts via imitation learning, enabling robots to learn new skills effectively. Experimental evaluations in the MetaWorld simulation environments demonstrate the pipeline's capability to generate high-fidelity and reliable demonstrations. Using the generated demonstrations, various skill learning algorithms achieve an accomplishment rate three times the original on novel tasks. These results highlight a novel approach to robot learning, offering a foundation for the intuitive and intelligent acquisition of novel robotic skills.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots have been deployed across various domains, including home assistance, healthcare, and industrial automation [1], [2]. In these dynamic environments, they are often required to perform diverse tasks that demand novel skills. Current approaches for skill learning, such as imitation learning (IL) and reinforcement learning (RL), rely on demonstration datasets or environment interactions. These limitations make autonomous novel skill learning in robotics still a challenging and unresolved problem.\nRL enables robots to acquire novel skills through environmental interaction and can be applied across environments. However, RL faces significant challenges due to its reliance on trial-and-error exploration, which is time-consuming and potentially dangerous. In both simulated and real-world settings [3], [4], RL requires extensive interactions between robots and their environments, demanding significant computational resources and time. Additionally, during the RL training phase, robots may exhibit uncontrolled behaviors, posing safety risks in sensitive domains such as home assistance and healthcare [5]\u2013[7]. These limitations make RL resource-intensive and challenging to scale for acquiring a wide range of novel skills.\nIL enbles robots acquire novel skills through learning the mapping between actions and states in expert demonstrations, bypassing the challenges of environment exploration [8]\u2013[11]. However, IL relies on the availability of high-quality expert demonstrations, which are often difficult and costly to obtain, particularly in real-world environments [12], [13]. Collecting such demonstrations requires skilled operators to carefully configure environments and perform tasks, making the process labor-intensive. Moreover, the unique settings for each novel task in simulated and real-world environments necessitate additional effort, further complicating data collection. In summary, IL is constrained by extensive human involvement,"}, {"title": "II. METHOD", "content": "The proposed pipeline comprises four modules that enable robots to learn novel tasks directly from language descriptions (see Fig. 2). First, a large vision-language model (VLM) enriches task descriptions. These enhanced descriptions are input to a demonstration video generator (DVG), which synthesizes task-specific video demonstrations. Next, an inverse dynamics model (IDM) extracts paired actions and states from these demonstrations. Finally, an end-to-end imitation learning model (ILM) maps environmental states to actions, enabling robots to acquire novel skills."}, {"title": "A. Demonstration Generation", "content": "1) Prompt Expansion with VLM: VLM is employed to expand concise task descriptions [24]. Compared to the single modal LLM (with only language as input), VLMs, trained with both language and visual modalities, offer a deeper understanding of real-world contexts. The prompt expansion process can be formalized as follows:\n$\\displaystyle P(Y|X) = \\prod_{t=1}^{T} P(Y_{t}|Y_{<t}, X)$ (1)"}, {"title": "B. Skill Learning", "content": "Once the state-action pairs are obtained, the final step involves establishing a mapping between states and actions through end-to-end imitation learning. This process can be framed as a markov decision process (MDP), which includes elements such as the set of possible states, the set of possible actions, and a transition model that predicts how the system moves between states based on actions. While traditional reinforcement learning relies on a reward function to guide the agent's behavior, imitation learning focuses on mimicking expert demonstrations without explicitly using rewards. The agent learns a policy by minimizing the difference between its predicted actions and those shown in the expert data. The policy is trained to minimize the behavioral cloning loss:\n$\\displaystyle L_{3} = E_{(s,a)\\sim D}[||\\pi(s) - a||^{2}]$ (6)\nwhere $D$ represents the demonstration dataset, $s$ denotes the state, $a$ is the expert action, and $\\pi(s)$ is the action predicted by the policy network.\nPolicy learning is achieved through a learning objective that reduces the error between the agent's actions and the expert's, effectively teaching the agent to replicate the expert's behavior as closely as possible."}, {"title": "C. Pipeline for Learning Novel Skills", "content": "The proposed pipeline enables zero-shot task learning by using prompts to generate demonstrations for previously unseen tasks, allowing robots to acquire novel skills without prior examples. The process is detailed in Algorithm 2. Upon receiving a prompt for a novel task, the pipeline employs the VLM to expand the task description, providing details about the environment and required actions. Based on the enriched description, the DVG generates demonstration videos. The IDM extracts motion intentions from these videos, constructing state-action pairs. Finally, an imitation learning technique maps states to actions, achieving novel skill learning."}, {"title": "III. EXPERIMENTS", "content": "To validate the efficiency of the proposed pipeline, 20 manipulation trials are collected for each of the 22 tasks from the MetaWorld [3] multi-task learning benchmark. Data are collected using a fixed camera setup with an elevation angle of -25\u00b0 and an azimuth of 145\u00b0. The collection frequency is set at 80 Hz, with trajectories capped at a maximum length of 500 steps. Visual frames are rendered at a resolution of 512x512. To enrich the diversity of state-action pairs, random action selection is incorporated during the data collection with an epsilon-greedy probability of 0.1."}, {"title": "B. Training Details", "content": "Experimental Setup: The 22 tasks are organized into 16 groups, which are further divided into two folds to define few-shot and zero-shot tasks. Performance evaluation of the proposed pipeline is conducted under the two-fold cross-validation (task compositions are detailed in Table II).\nPreprocessing: During the training stage of the DVG, video data are sampled with a skip step of 3, yielding up to 36 frames per video. In contrast, the IDM and ILM are trained with the original frame rate. The IDM applies edge extraction and noise injection as preprocessing steps, while the ILM uses normalization and Gaussian noise to improve robustness across varying states.\nModel Configuration: The VLM in the pipeline is the closed-source commercial model GLM-4-0520 [24], which supports direct few-shot inference. Three handwritten examples are provided for VLM to expand prompts. The DVG is based on the Tune-A-Video [28], initialized with Stable-Diffusion-1.4 and enhanced with randomly initialized Spatio-Temporal Attention and a T2I-Adapter. The IDM is based on ViT and incorporates a randomly initialized Transformer-Encoder to encode temporal dimensions.\nThe DVG is fine-tuned using 20 demonstration videos, expanded task descriptions, and corresponding poses for each few-shot task. Similarly, the IDM is trained with 20 demonstration videos and corresponding actions for each few-shot tasks. Input data for the IDM is segmented with a temporal window size of 12. The learning rate and batch size were optimized using the Ray Tune hyperparameter optimization framework.\nTwo skill learning models, LCBC [29] and RT-1 [21], are employed as ILM due to their strong generalization and task-learning ability. LCBC utilizes T5 [30] text embeddings to encode language instructions and Vision Transformer (ViT) [31] features to extract visual information. These multimodal encodings are processed by a transformer-encoder-based policy network to integrate linguistic and visual data for action prediction. RT-1 extends this approach by incorporating historical states and using a transformer-decoder with self-attention to process temporal sequences of visual and language features, enhancing action prediction across multiple timesteps.\nThe task learning baselines are trained on 20 demonstration samples for each few-shot task, with an additional 11 zero-shot tasks used to test zero-shot performance and use a discrete action space of 256 bins. In contrast, the proposed pipeline uses the generated demonstrations, covering both few-shot and zero-shot tasks. The learning rate and batch size were similarly optimized using Ray Tune."}, {"title": "C. Experimental Metrics", "content": "The quality of generated demonstrations is a critical factor in the proposed pipeline. Human evaluators are recruited to rate each generated demonstration based on the following criteria:\nPhysical laws: Adherence to physical laws and principles\nAccomplishment: Successful completion of the presented task\nConsistency: Consistency between the video content and its description.\nMetrics for Skill Learning: The proposed framework aims to enable robots to learn novel skills, with the primary metric being task accomplishment. This study reports task-level metrics for each fold and the overall accomplishment rate across tasks at the fold level."}, {"title": "D. Experimental Results", "content": "The experimental results are reported under two setups:\nFew-Shot Learning: The proposed pipeline is fine-tuned using a small number of manipulation trials, representing few-shot skill learning. In this setup, results are reported on the same tasks used during the fine-tuning stage.\nZero-Shot Learning: The pipeline is evaluated on tasks not seen during fine-tuning, directly testing its ability to learn novel tasks.\nQuality of the Generated Demonstration: The quality of the generated demonstrations is evaluated by four human evaluators, with average scores across three metrics reported under two experimental settings in Fig.4. In both settings, the generated demonstrations adhered to physical laws, successfully accomplished tasks, and aligned with descriptions in over 50% of cases. Although performance slightly decreased under the few-shot learning setting, the above-50% success rate demonstrates the pipeline's ability to generate reliable demonstrations, contributing to novel skill learning. Visualization results in Fig.3 illustrate that the generated demonstrations achieve both fidelity and diversity.\nRobot Skill Learning: Under the few-shot learning setting, different imitation algorithms achieved comparable performance using the generated demonstrations from the proposed pipeline and the collected expert manipulations. This indicates that the quality of the generated demonstrations is similar to the collected manipulations and that the state-action pairs constructed by the IDM are precise enough for the imitation learning pipeline.\nNotably, under the zero-shot learning setting, the generated demonstrations achieved nearly three times the task accomplishment rates of algorithms trained with fine-tuned expert-collected data (see Table III). This result underscores the proposed pipeline's superior capability to enable robots to learn novel, unseen tasks, which is challenging for the previous imitation learning methods. Moreover, LCBC demonstrated comparable performance under the zero-shot learning setting to that achieved in the few-shot setting, further confirming the effectiveness of the proposed pipeline in enabling robots to acquire novel skills."}, {"title": "IV. CONCLUSIONS", "content": "This study proposes a pipeline enabling robots to learn novel skills directly from natural language-based instructions, leveraging generative models, IDM and ILM. Specifically, an generative-models-based method organizes actions by utilizing the prior knowledge embedded in generative models, directly converting language instructions into demonstration videos. IDM and ILM algorithms are then applied to learn novel skills from these generated demonstrations. This approach addresses limitations in task data collection, which are constrained by labor-intensive manual construction and physical restrictions. Future work will focus on refining and optimizing the framework to enhance its accuracy and efficiency in robot skill learning, particularly for complex manipulation tasks. Additionally, the proposed pipeline will undergo rigorous validation in real-world environments to assess its scalability and practical applicability."}]}