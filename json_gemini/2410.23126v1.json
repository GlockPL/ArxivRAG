{"title": "Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes", "authors": ["Jerry Yao-Chieh Hu", "Dennis Wu", "Han Liu"], "abstract": "We study the optimal memorization capacity of modern Hopfield models and Kernelized Hopfield Models (KHMs), a transformer-compatible class of Dense Associative Memories. We present a tight analysis by establishing a connection between the memory configuration of KHMs and spherical codes from information theory. Specifically, we treat the stored memory set as a specialized spherical code. This enables us to cast the memorization problem in KHMs into a point arrangement problem on a hypersphere. We show that the optimal capacity of KHMS occurs when the feature space allows memories to form an optimal spherical code. This unique perspective leads to: (i) An analysis of how KHMs achieve optimal memory capacity, and identify corresponding necessary conditions. Importantly, we establish an upper capacity bound that matches the well-known exponential lower bound in the literature. This provides the first tight and optimal asymptotic memory capacity for modern Hopfield models. (ii) A sub-linear time algorithm U-Hop+ to reach KHMs' optimal capacity. (iii) An analysis of the scaling behavior of the required feature dimension relative to the number of stored memories. These efforts improve both the retrieval capability of KHMs and the representation learn-ing of corresponding transformers. Experimentally, we provide thorough numerical results to back up theoretical findings.", "sections": [{"title": "1 Introduction", "content": "We study the optimal memorization capacity of Kernelized modern Hopfield Models (KHMs) [Wu et al., 2024a], propose a sublinear-time algorithm to achieve it, and analyze parameter selection for these models. KHMs belong to a class of transformer-compatible Dense Associative Memory [Krotov and Hopfield, 2021, 2016] known as Modern Hopfield Models (MHMs) [Wu et al., 2024a,b, Hu et al., 2024a, 2023, Ramsauer et al., 2020]. The defining characteristics of these models include their super-linear memory capacity and strong connection to transformer attention mechanisms [Vaswani et al., 2017]. The former makes them interesting models for associative memory, and the latter makes them versatile transformer-compatible backbones with diverse empirical successes [Burns, 2024, Burns and Fukai, 2023, Hu et al., 2024a,c, Xu et al., 2024, Wu et al., 2024a,b, Hoover et al., 2023a, Seidl et al., 2022, F\u00fcrst et al., 2022]. However, one major limitation of MHMs is their reliance on the quality of memory distribution for effective pattern storage and retrieval [Wu et al., 2024a, Sec. 1].\nStudying this limitation in these models is fundamental and of practical importance. One one hand, it prevents MHMs from functioning as full-fledged content-addressable memory models. On the other hand, it implies that the representation learning ability of current transformer attention [Vaswani et al., 2017] is suboptimal [Wu et al., 2024a, Thm. 3.1]. Addressing this issue benefits both computational"}, {"title": "2 Main Theory", "content": "We provide a theoretical analysis on the optimal memory capacity of KHMs. First, we begin by comparing the memory capacity between MHM and KHM using the standard high-probability lower bound [Hu et al., 2023, Ramsauer et al., 2020]. Then, we present a spherical code perspective as a framework for depicting the optimal memory capacity of both MHMs and KHMs. In our analysis, we make the following pattern normalization assumption on memory patterns:"}, {"title": "2.1 High-Probability Capacity Lower Bound", "content": "We start by showing the memory capacity of KHM using the standard capacity lower bound introduced by Ramsauer et al. [2020]. This provides a direct comparison between KHMs and previous works. The definition of the generalized fixed point [Sriperumbudur and Lanckriet, 2009] is\nDefinition 2.1 (Generalized Fixed Point [Sriperumbudur and Lanckriet, 2009]). We say a set $S \\subseteq \\mathbb{R}^d$ is a generalized fixed point w.r.t. $T$ if $T(y) \\in S$ for every $y \\in S$.\nRemark 1. In contrast to Definition 2.1, a fixed point of $T$ is a point $y$ satisfying $T(y) = y$.\nLet $S_{\\Phi}^{\\mu}$ be a ball with radius $R_{\\Phi}^{\\mu}$ centered at every memory pattern in the feature space $\\Phi(\\xi_{\\mu})$:\n$S_{\\Phi}^{\\mu} = \\{y \\mid ||\\Phi(\\xi_{\\mu}) - y|| \\le R_{\\Phi}\\}$, where $R_{\\Phi} := \\frac{1}{2} \\min_{\\mu,\\nu \\in [M]} ||\\Phi(\\xi_{\\mu}) \u2013 \\Phi(\\xi_{\\nu})||$.\nFollowing [Wu et al., 2024a], we define the memory storage and retrieval as:\nDefinition 2.2 (Pattern Storage and Retrieval). We say a memory pattern $\\xi_{\\mu}$ is stored if $S_{\\Phi}^{\\mu}$ is a generalized fixed point of $T$, and there exists a fixed point $x \\in S_{\\Phi}^{\\mu}$. A memory pattern $\\xi_{\\mu}$ gets $\\epsilon$-retrieved by $T$ with an input query $x$ if $||T(x) \u2013 \\xi_{\\mu}|| \\le \\epsilon$.\nThis definition is compatible with both KHMs and MHMs (with identity feature map). Under Definition 2.2, KHM's memory capacity is lower bounded by the following lemma.\nLemma 2.1 (Memory Capacity of KHM). Let $1 - p$ be the probability of successfully storing and retrieving a pattern. Assuming the patterns are normalized, the number of patterns $M_{\\Phi}$ that can be stored and retrieved by the KHM, following the update rule (1.3), is lower-bounded by:\n$M_{\\Phi} \\ge \\sqrt{p}C^{(D_{\\Phi}-1)/4}$,\nwhere $C$ is the solution to $C = b/(W_0(\\exp\\{a+\\ln b\\}))$, with $W_0(\\cdot)$ being the principal branch of Lambert W function, $a := (4/(D_{\\Phi}-1)) (\\ln((2\\sqrt{p}-2)/R_{\\Phi}) + 1)$ and $b := 4\\beta/(5(D_{\\Phi}-1))$. For comparison, $M_0$ reduces to MHM's capacity lower bound by setting $\\Phi = Id$, with $D_{\\Phi} = d$.\nWith a fixed $D_{\\Phi}$, the highest lower bound of Lemma 2.1 corresponds to specific a $\\Phi$ that maximizes $R_{\\Phi}$. This provides an intuitive insight on the design of separation loss [Wu et al., 2024a, Defini-tion 2.2] for kernel learning in [Wu et al., 2024a, Algorithm 1]. With an additional feature space, KHM has an exponential memory capacity in $D_{\\Phi}$ that does not depend on $d$. When $D_{\\Phi} = d$, KHMs obtain a tighter lower bound than MHMs if $R_{\\Phi} > R$. This bound connects the storage capacities of KHMs and MHMs, showing that their capacities scale exponentially with respect to $D_{\\Phi}$ and $d$."}, {"title": "2.2 Memory Code: Memories as Spherical Code", "content": "There are two aspects the lower bound in Lemma 2.1 does not address: the maximal capacity of KHMs and the flexibility of choosing different \u03a6 in KHMs. Therefore, we present a new framework using spherical codes to take the above perspectives into consideration for further analysis. We begin by introducing the concepts of spherical code and optimal spherical code.\nDefinition 2.3 (Spherical Code). A $d$-dimensional spherical code on the unit sphere $\\mathbb{S}^{d-1}$ is a finite set $C_N = \\{c_1, ..., c_N \\}$ of $\\mathbb{S}^{d\u22121}$ with $N$ points, where $c_i \\in \\mathbb{R}^d$ for $i \\in [N]$ and $|C_N| = N$.\nDefinition 2.4 (Minimal Separation). The minimal separation $\\rho(C_N)$ of a spherical code $C_N$ is the maximal inner product between two distinct points in $C_N$:\n$\\rho(C_N) = \\max_{c_i, c_j \\in C_N \\atop i \\neq j} \\langle c_i, c_j \\rangle$, for every $i \\neq j$.\nDefinition 2.5 (Optimal Spherical Code). Let $C_N = \\{c_1,...,c_N\\} \\subseteq \\mathbb{S}^{d-1}$ be a $d$-dimensional spherical code with $N$ points. An optimal spherical code $C_{N}^{*}$ minimizes the maximal pairwise inner product, which corresponds to maximizing the minimal separation between points in the code. Formally, the optimal spherical code $C_{N}^{*}$ is defined as:\n$C_N^* = \\underset{C_N \\subset \\mathbb{S}^{d-1}}{\\text{argmin}} \\underset{i,j}{\\text{max}} \\langle c_i, c_j \\rangle$, for $i, j \\in [N]$.\nNext, we recall the function class $\\mathcal{H}$ of the linear feature map introduced by Wu et al. [2024a]:\nDefinition 2.6. The function class $\\mathcal{H}$ consists of linear maps that satisfy the following properties:\n1. For all $\\Phi \\in \\mathcal{H}$, $\\Phi : \\mathbb{S}^{d\u22121} \\rightarrow \\mathbb{S}^{D_{\\Phi}\u22121}$ is a linear map defined by a matrix $W \\in \\mathbb{R}^{d \\times D_{\\Phi}}$.\n2. The matrix $W$ has full column rank.\n3. When applying $\\Phi$ to different inputs:\n\u2022 For a vector $\\xi \\in \\mathbb{R}^d$, $\\Phi(\\xi) = W \\xi \\in \\mathbb{R}^{D_{\\Phi}}$.\n\u2022 For a matrix $\\Xi \\in \\mathbb{R}^{d \\times M}$, $\\Phi(\\Xi) = (\\Phi(\\xi_1), ..., \\Phi(\\xi_M)) \\in \\mathbb{R}^{D_{\\Phi} \\times M}$.\n\u2022 For a set of vectors $V = \\{v_1, ..., v_V \\}$, $\\Phi(V) = \\{\\Phi(v_1), ..., \\Phi(v_N)\\}$ with $|\\Phi(V)| = N$.\nDefinition 2.6 ensures KHMs with feature map $\\Phi(\\cdot) \\in \\mathcal{H}$ satisfying the defining characteristics of MHMs: accurate [Wu et al., 2024a, Lemma 2.1] and consistent [Wu et al., 2024a, Thm 2.1] retrieval according to Definition 2.2. Now, we combine the concept of spherical code and memory storage.\nDefinition 2.7 (Kernelized Well-Separation Condition [Wu et al., 2024a,b, Hu et al., 2023, Ramsauer et al., 2020]). Given a set of kernelized memory patterns $\\Phi(\\Xi) = \\{\\Phi(\\xi_{\\mu})\\}_{ \\mu=1 }^{M} \\subseteq \\mathbb{S}^{D_{\\Phi}-1}$, the kernelized memory pattern $\\Phi(\\xi_{\\mu})$ satisfies the well-separation condition if the following holds:\n$\\Delta_{\\Phi}^{\\mu} \\ge \\frac{1}{\\beta} \\ln \\Big( \\frac{2(M-1)}{R_{\\Phi}} \\Big)$,\nwhere the inverse temperature \u03b2 is given by (1.3) and $R_{\\Phi}$ is defined by (2.1).\nThe inequality (2.2) is a necessary condition for the \u03bc-th memory to have a well-defined attractor basin. Hence, the more memories satisfying (2.2) the greater the memory capacity of the model.\nDefinition 2.8 (Memory Code). Let $M \\in \\mathbb{N}^+, \\beta > 0, D_{\\Phi} > 1$ and $\\Phi \\in \\mathcal{H}$. For any finite set $\\Phi(\\Xi) = \\{\\Phi(\\xi_{\\mu})\\}_{ \\mu=1 }^{M} \\subseteq \\mathbb{S}^{D_{\\Phi}\u22121}$, we say the set $\\Phi(\\Xi)$ is a memory code if all points in $\\Phi(\\Xi)$ satisfies (2.2). Further, we denote $\\mathcal{A}_{D_{\\Phi}}$ as the set of all memory codes in $\\mathbb{S}^{D_{\\Phi}\u22121}$, including all possible $\\Xi, \\Phi$.\nNotably, $\\mathcal{A}_{D_{\\Phi}}$ includes all the possible pattern sets $\\{\\Phi(\\Xi)\\}$ that are able to be stored and retrieved by kernelized Hopfield models and modern Hopfield models. Naturally, the optimal memory capacity is the size of the largest memory code in $\\mathbb{S}^{D_{\\Phi}\u22121}$. This leads to our next definition:\nDefinition 2.9 (Optimal Memory Capacity). For $D_{\\Phi} > 1$ and $\\Phi \\in \\mathcal{H}$, the optimal capacity $M^*$ is the cardinality of the largest memory code in $\\mathcal{A}_{D_{\\Phi}}$, i.e., $M^* := \\underset{\\Phi(\\Xi) \\in \\mathcal{A}_{D_{\\Phi}}}{\\text{max}} |\\Phi(\\Xi)|$ for all possible $\\Xi, \\Phi$.\nDefinition 2.9 specifies the largest possible memory code in $\\mathcal{A}_{D_{\\Phi}}$ for a given $D_{\\Phi}$. Let $\\Xi^{\\ast}$ denote the memory set associated with $M^*$, such that $|\\Xi^{\\ast}|| = M^*$. To store all patterns in $\\Xi^{\\ast}$, we need to find a suitable feature map $\\Phi$ such that $\\Phi(\\Xi^{\\ast})$ is a valid memory code.\nFollowing by this definition, we present the next lemma for optimal capacity.\nLemma 2.2 (Capacity of Optimal Spherical Code). Given a fixed $D_{\\Phi} > 1$, and its corresponding $M^*$, if an optimal code $C_{opt}$ is in $\\mathbb{S}^{D_{\\Phi}-1}$ and has size $M^*$, then $C_{opt} \\in \\mathcal{A}_{D_{\\Phi}}$."}, {"title": "Proposition 2.1 (Optimal Memory Capacity).", "content": "Following Lemma 2.2, with $\u03b8 \u2208 (0, \u03c0/2)$, we have\n$M^* = c^{D_{\\Phi}}$,\nfor some c > 1. Here = indicates matching upper and lower bounds up to constant factors.\nProposition 2.1 indicates that the optimal capacity of MHMs and KHMs scales exponentially with $D_\u03a6$. This capacity bound is provably tight and optimal for large feature dimension $D_\u03a6$. It echos the exponential capacity lower bound in Lemma 2.1 and in prior works [Wu et al., 2024a,b, Hu et al., 2024a,b,c, 2023, Ramsauer et al., 2020]. Moreover, Lemma 2.2 shows that achieving the maximal capacity in any $D_\u03a6$ is equivalent to achieving optimal codes. Thus, for a given memory set \u039e of size M, the memory storage problem with KHMs divides into two sub-problems:\n(P1) Finding a sufficiently large $D_\u03a6$ (in Section 3.2), and\n(P2) Finding a \u03a6 such that \u03a6(\u039e) is an optimal spherical code (in Section 3.1).\nNext, we examine these two sub-problems and present a sub-linear time algorithm to solve them."}, {"title": "3 Sub-Linear Time Algorithm for Optimal Memory Capacity", "content": "In this section, we present an sub-linear time algorithm that achieves optimal capacity. Then, we analyze the scaling behavior of D\u03a6 for KHMs to store any desired amount of memories."}, {"title": "3.1 Learning to Achieve Optimal Memory Code", "content": "Here we present an asymptotic result showing that an algorithm exists to find the optimal \u03a6 for maximizing memory storage in dimension D\u03a6. Building on the results from the previous sections, we consider the following problem:\nProblem 1 (HardMax Problem). Given a memory set \u039e = {$\u03be_1,..., \u03be_M$}, and assuming that D\u03a6 is sufficiently large to satisfy (2.2), we define the HardMax problem as finding a \u03a6 such that \u03a6(\u039e) forms an optimal spherical code:\n$ \\underset{\\Phi \\in \\mathcal{H}}{\\text{min}} \\mathcal{L}_{HardMax}(\\Phi), \\text{ where } \\mathcal{L}_{HardMax}(\\Phi) := \\underset{\\nu,\\mu \\in [M], \\nu \\neq \\mu}{\\text{max}} \\langle \\Phi(\\xi_{\\mu}), \\Phi(\\xi_{\\nu}) \\rangle \\ge \\rho^*.$\nThis problem setup involves finding a \u03a6 such that \u03a6(\u039e) forms an optimal spherical code. Ideally, a more expressive function class $\\mathcal{H}$ would simplify finding such a \u03a6; exploring explicit forms of more powerful mappings is left for future work. Note that (3.1) represents a min-max optimization problem. Achieving the global optimum is notoriously challenging [Hsieh et al., 2021, Daskalakis et al., 2021, Shen et al., 2020]. Thus, we introduce a surrogate objective to solve (3.1):\nDefinition 3.1 (Average Separation Loss). For $\u03c4 > 0$, given a set of memory patterns \u039e and a feature map \u03a6, we define the average separation loss as\n$\\mathcal{L}(\\Xi, \u03c4, \\Phi) := \\frac{1}{M} \\sum_{\\mu=1}^{M} l_{\\mu}(\\Xi, \\Phi, \u03c4), \\text{ where } l_{\\mu} (\\Xi, \\Phi, \u03c4) = \\underset{\\nu \\neq \\mu}{\\frac{\\sum_{v=1}^{M} \\text{exp} \\big( \\frac{\\langle \\Phi(\\xi_{\\mu}), \\Phi(\\xi_{\\nu}) \\rangle}{\u03c4} \\big) }{ \\big) }} \\Bigg).$\nThe primary difference between (3.1) and (3.2) is that (3.2) calculates average separation, whereas (3.1) focuses on the maximum separation between a single pair. This surrogate loss alleviates the challenging optimization, as (3.2) is convex. Therefore, with vanishing temperature \u03c4, the next theorem shows that (3.2) converges to the HardMax problem asymptotically.\nTheorem 3.1. For any possible integer M, we have\n$\\underset{\u03c4 \\rightarrow 0}{\\text{lim sup}} ( \\underset{\\Phi \\in \\mathcal{H}}{\\text{argmin}} \\mathcal{L}(\\Xi, \\Phi, \u03c4) \\big) \\subseteq \\underset{\\Phi \\in \\mathcal{H}}{\\text{argmin}} \\mathcal{L}_{HardMax} (\\Phi).$"}, {"title": "U-Hop+: Sub-Linear Time Algorithm for Achieving Optimal Memory Capacity.", "content": "Next, we present Algorithm 1 for finding a \u03a6 such that \u03a6(\u039e) forms an optimal spherical code. To meet the conditions in Definition 2.6, we use projected gradient descent to convert this constrained optimization problem into an unconstrained one. Several methods satisfy the requirements in Definition 2.6; we discuss them in Appendix B.2. We denote the learning rate as \u03b3, the input matrix of the loss function as X, and the weight matrix as W. We define a single Projected Gradient Descent (PGD) step as\n$W_{t+1} = \\text{PGD}(W_t, \u03b3, X)$,\nWe defer the detailed formulation to Appendix B.2. Since the separation loss is convex and smooth, using projected gradient descent with a learning rate $\u03b3 \\le 1/G$, yields a sub-linear convergence rate of $O(1/N)$ [Iusem, 2003]. This provides an asymptotic solution to the first sub-problem ((P1)). Next, we examine the relationship between feature dimension D\u03a6 and the number of memories M."}, {"title": "3.2 Impact of D", "content": "This subsection analyzes the minimum D\u03a6 to store a given set of M memories. Based on the well-separation condition and the derivation in Appendix C.2, the required in to store M memories scales as $O(\\ln(M))$. With this insight, the following proposition shows the scaling behavior of required D\u03a6 with respect to M and $\u0394_{min}^\u03a6$.\nProposition 3.1. Let M* be the optimal memory capacity in $\\mathbb{S}^{D_{\\Phi}}$ and \u03a6 \u2208 H. For any optimal code $C^{\u2217}$ in $\\mathbb{S}^{D_{\\Phi}\u2217\u22121}$ of size $M^\u2217$, the minimal separation $\u03c1(C^\u2217)$ is bounded by:\n$\\frac{1}{\\sqrt{\u03c0}} \\sqrt{\\frac{\\Gamma (\\frac{D_{\\Phi}}{2} + 1)}{M^* \\Gamma(\\frac{D_{\\Phi}-1}{2}) }} < \\underset{\\Phi \\in \\mathcal{H}}{\\text{max}} \u0394_{min}^\u03a6 \\le 2 \\sqrt{\u03c0} \\sqrt{\\frac{\\Gamma (\\frac{D_{\\Phi}}{2} + 1)}{M^* \\Gamma(\\frac{D_{\\Phi}-1}{2}) }},$\nwhere \u0393(\u00b7) is the gamma function.\nRemark 2. By gamma function asymptotics, Proposition 3.1 is consistent with Proposition 2.1.\nProposition 3.1 establishes the separation value for memory codes that achieve optimal capacity in $D_\u03a6$-dimensional space. Using this bound, for a given separation value $\u0394_{min}^\u03a6$, the minimum D\u03a6 required to store M points scales as $\\log(M^2/\u0394_{min}^\u03a6)$. We conduct an experiment to demonstrate the bound's tightness and provide an example with $D_\u03a6 = 3$ in Figure 3."}, {"title": "4 Experimental Studies", "content": "We consider the distribution of metastable state size under standard MHM and KHM update rules. The results are in Table 1. In general, with more metastable state having the size of 1, meaning the Hopfield model stores more memories as the query converges to a single memory. For metastable state size larger than 1, it represents that the retrieved pattern converges near the mean of a subset of memories, violating the requirement of SS = 0.\nBaselines. We compare different variants of MHMs and KHMs. Santos et al. [2024], Wu et al. [2024b] provide comprehensive analyses of modern Hopfield models with various normalization functions. Here, we consider softmax, 1.5-entmax, and sparsemax for normalization. We equip these three baselines with U-Hop to compare against standard MHMs.\nSettings and Metrics. Let p = Softmax($\u03b2x\u039e$). We determine whether the update rule converges to either a single memory or a mixture of memories by observing the probability distribution p. The quantity ||p|| represents the size of the metastable state, which is the number of non-zero entries in the probability distribution. In the case of 1.5-entmax and sparsemax, we calculate ||P|| directly. For softmax, since it only generates non-zero entries, we use a threshold of 0.01 and consider the entries under the threshold as 0. We conduct experiments using both synthetic and MNIST datasets. For MNIST, we use the training set as memories and the test set as queries. For synthetic datasets, we randomly generate memories and queries with Gaussian initialization. To ensure the convergence to the fixed point, we perform multiple updates on the query. For more details, refer to Appendix D.1.\nResults. On both synthetic and MNIST datasets, it is evident that under separation maximization, the size of the metastable state dramatically decreases within just 20 iterations of Algorithm 1. This result demonstrates that, with Algorithm 1, KHMs are capable of storing patterns that MHMs cannot store. The significant percentage of size 1 metastable states in KHMs indicates that they circumvent the memory confusion problem in dense associative memory models [Krotov and Hopfield, 2016]. For the MNIST dataset, we see MHMs show close performance with KHMs under 1.5-entmax and sparsemax, showing that the methods in [Santos et al., 2024, Wu et al., 2024b, Hu et al., 2023] also circumvent the memory confusion problem. Notably, KHMs require only one-fourth of the dimensions to store memories while perfectly storing 60,000 MNIST patterns. These results suggest that KHMs with Algorithm 1 efficiently utilize feature dimensions for memory storage."}, {"title": "4.2 Energy Landscape under U-Hop+ Stores More Memories", "content": "Settings and Metrics. We visualize the energy landscape of KHMs at different stages of Algorithm 1 using contour plots. The results are presented in Figure 1. We consider two settings: 2 and 4 memories stored in a 2-dimensional space. Ideally, the energy landscape should position memories in multiple separated low-energy regions (valley), with each region isolated from others by high-energy regions. If multiple memories share the same valley, it leads to memory confusion and the presence of metastable states during energy minimization. For experiment details, refer to Appendix D.2.\nResults. The first row in Figure 1 shows the raw energy landscape without KHM and Algorithm 1, corresponding to the modern Hopfield energy landscape. On the right side of Figure 1, we observe that MHMs are only able to store 2 out of 4 points, but KHMs are able to further separate one point"}, {"title": "4.3 Multiple Instance Learning", "content": "We conduct multiple instance learning (MIL) on 4 real-world datasets using Hopfied-based models with and without our U-Hop+ algorithm. We follow the setup in [Hu et al., 2023, Wu et al., 2024a, Santos et al., 2024] by using a model with 1 embedding layer, 1 HopfieldPooling layer and a linear readout layer. We first utilize Algorithm 1 to \"pretrain\u201d the embedding and HopfieldPooling layer,"}, {"title": "5 Discussion and Conclusion", "content": "This work complements U-Hop [Wu et al., 2024a] by establishing the optimal capacity of kernelized modern Hopfield models (KHMs) and providing the first tight and optimal memory capacity bound for transformer-compatible dense associative memories We start by connecting stored memories in KHMs to spherical codes from information theory. We then prove that maximizing memory storage in KHMs requires arranging memories as an optimal spherical code in feature space. This allows us to matches the well-known exponential lower bound [Wu et al., 2024a,b, Hu et al., 2024a,b,c, 2023, Ramsauer et al., 2020] with an upper bound. This achievement is notable, as deriving such a tight bound is challenging due to the max-min structure of maximal separation among stored memories. Moreover, we introduce a sub-linear time algorithm to achieve this optimal capacity, U-Hop+ (Algorithm 1). U-Hop+ performs this rearrangement with a convergence rate of $O(1/n)$. Additionally, we analyze the minimum dimension $D_\u03a6$ required to store M memories. Numerically, we validate the effectiveness of KHMs and demonstrate how Algorithm 1 enhances memory storage in both KHM retrieval tasks and transformer representation learning tasks.\nCan U-Hop+ Preserve Semantic Meanings? In representation learning, it is crucial to preserve relationships in the feature space after encoding data [Wang et al., 2023, Neelakantan et al., 2022]. The primary strategy is to ensure the embeddings of similar instances share similar directions in Euclidean space. At first glance, the approach of pushing all memories away from each other in Equation (3.2) may seem counterintuitive. However, as detailed in Appendix D.5, we find that the learned feature map still encodes similar instances closely together (Figure 4), even without semantic information involved. This result indicates that U-Hop+ stores memories in a semantically coherent manner. A discussion of the separation capability of \u03a6 can be found in Figure 4.\nLimitations. One limitation of our work only considers linear affine functions as the feature map $\\Phi \u2208 \\mathcal{H}$. Additionally, standard spherical code analysis focuses only on normalized points on a hypersphere, ignoring memories with varying magnitudes. We leave them for future research."}, {"title": "Broader Impact", "content": "We expect no negative social impacts as this work mostly present theoretical results and numerical simulations. As discussed in our introduction, this paper develops a theoretical framework to study Kernelized Hopfield models, potentially benefit the area of computational associative (Hopfield) memory models, transformer networks and large foundation models."}]}