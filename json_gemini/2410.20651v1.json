{"title": "SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis", "authors": ["Huzaifa Pardawala", "Siddhant Sukhani", "Agam Shah", "Veer Kejriwal", "Abhishek Pillai", "Rohan Bhasin", "Andrew DiBiasio", "Tarun Mandapati", "Dhruv Adha", "Sudheer Chava"], "abstract": "Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in finance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49, 446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reflect the tone of the answers provided during QA sessions across different domain. Our findings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform significantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the financial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license\u00b9.", "sections": [{"title": "1 Introduction", "content": "Earnings Calls (ECs) and their linguistic nuances serve as a vital communication channel between company executives and investors, offering insights into a company's performance and future outlook [Sawhney et al., 2021]. The long-form Question and Answer (QA) sessions of these calls are particularly significant as they provide unscripted interactions that reveal executives' confidence and strategic clarity. Unlike the scripted presentations in the beginning of ECs, the dynamic nature of QA sessions invites real-time scrutiny [Matsumoto et al., 2011] and deeper analysis [AlhamzehFurthermore, our dataset and methodology extend beyond the financial domain, addressing the need for robust subjectivity and misinformation detection tools applicable in various domains such as elections, journalism, sports, and public policy. QA sessions are prevalent in these areas, where the quality and clarity of responses significantly impact decision-making and public perception. As shown in Appendix N, we applied our models to White House Press Briefings [The White House, 2024], a setting where transparency and caution are paramount. Our analysis of QA pairs from White House Press Briefings and Gaggles demonstrates the utility of our models in a political context. These findings underscore SubjECTive-QA's effectiveness in capturing the nuanced subjective information"}, {"title": "2 Features in SubjECTive-QA", "content": "SubjECTive-QA consists of six features for analyzing the quality of speech of the respondent. These features and their definitions are given in table 2. This process was initialised with an LLM-guided approach: passing each QA pair to the PaLM 2 API [Anil et al., 2023], to obtain the 10 most prevalent properties demonstrated by the answer for that particular question. This approach is elaborated in detail in Appendix I.The final 6 chosen features were also seen to have a significant impact on almost all QA pairs as per visual inspection over the corpus of our data and the reason for choosing each specific feature can be seen in 2. All the features were shown to be independent after annotation as seen in figure 4. This independence criterion is paramount as it ensures that potential classifiers could be fine-tuned to focus on any one of the 6 features without having to account for the others."}, {"title": "3 Methodology", "content": "The creation of SubjECTive-QA is depicted below in figure 2. Table 1 details the metrics of our dataset captured by this process.\n3.1 Dataset Construction\nIdentification and Selection We commenced with the identification of company tickers, sectors, and verification of EC dates from 2007 to 2021. A foundational dataset obtained from Chava et al.\nSampling and Data Collection We then proceeded with the randomized selection of companies and their respective earnings call dates from the 119, 978 records obtained from Chava et al.Sector Consolidation When consolidating the data, there were significant overlaps in QA structures and linguistic patterns in the ECTs across the original sectors in terms of the features chosen. To address this, the 13 unique columns in the original dataset as mentioned in Appendix H.1 were\n3.2 Annotations\nAnnotators The last step was the manual annotation of each QA pair across the 6 features mentioned\nAnnotation Guideline This paper employed Microsoft Excel for the annotation procedure and figure 3 illustrates the manual annotation process. The annotators were asked to strictly adhere to the following annotation guidelines:\nGive the answer a rating of:\n\u2022 2: If the answer positively demonstrates the chosen feature, with regards to the question.\n\u2022 1: If there is no evident relation between the question and the answer for the feature.\n\u2022 0: If the answer negatively demonstrates the chosen feature with regards to the question."}, {"title": "4 Dataset Analysis", "content": "4.1 Independence Criterion\nUpon looking at the correlation matrix in figure 4, a general independence of features can be seen with the range of the correlations being between -0.08 and 0.39. As stated before and ver-ified through this correlation matrix, no significant relationship exists be-tween any of the features, indicating that the chosen features uniquely clas-sify the behaviour of the respondent and therefore can be independently modelled in the future. It is impor-tant to note that this correlation matrix disregards sector-wise bias between different features.\n4.2 Rating Distribution\nIn order to measure the sector-wise bias and variable distributions of fea-tures across sectors, we utilized violin plots as seen in Appendix J as they allow for a compact representation of the kernel density and distribution of the data. Revealing specific asymme-tries and skewness in various features across industries, these plots aided in the identification of specific behaviors and distributions of features across sectors.\nWhen considering the spread of the ratings across the features, it can be seen that around 90% of answers were given a rating of 2 for Clear and Relevant, showing that most respondents answer questions in a cohesive manner that is contextually relevant. For answers with a rating of 0, the feature that had the highest number of zeroes was Specific with around a fifth of all QA pairs negatively demonstrating this feature, indicating the variance in the quality of answers as shown by its violin plots within Appendix J.\nFurther exploration into the distribution of the features Clear and Relevant supports the hypothesis that company representatives aim to be confident and on-topic with their violin plots being highly dense to the rating of 2; however, there is high variation in the Specific feature across industries, suggesting a lack of technical details possibly to simplify information for a broader audience or to protect the company's reputation."}, {"title": "5 Benchmarking", "content": "On the other hand, the violin plots demonstrate the telecommunications sector to be highly Optimistic with overwhelming positive responses and an overall buoyant industry sentiment. However, the respondents within this industry were highly Cautious as seen in its violin plots and this is apparent within all industries, displaying the conservative nature of the answers. Overall, a wide spectrum of densities across different features and industries demonstrate diversity in tones and attitudes, emphasising the multilayered complexity of the proposed dataset.\n5.1 Models\nPre-trained Language Models (PLMs) To establish a performance benchmark, our study en-compasses a range of transformer-based Pre-trained Language Models. We employ BERT base (uncased), FinBERT-tone [Huang et al., 2020], and RoBERTa-base. To avoid overfitting on financial text data, we refrain from pre-training any of the models before fine-tuning them. The task performed is sequence classification, minimizing cross-entropy loss. The experiments are conducted using PyTorch [Paszke et al., 2019] on an NVIDIA A40 GPU. Each model is initialized with the pre-trained version from the Transformers library provided by Huggingface [Wolf et al., 2020]. We use varying hyperparameters and conduct multiple runs for each model using three seeds (5768, 78516, 944601), three batch sizes (32, 16, 8), and three learning rates (1e \u2013 4, 1\u0435 \u2013 5, 1\u0435 \u2013 6). Following this, we utilize a grid search strategy to find the best model for each feature. The ethical considerations while using these models are outlined in Appendix F.\nLarge Language Models (LLMs) Our study also encompasses four popular open-source LLMs: Llama-3-70b-Chat [Dubey et al., 2024], LLama-3-8b-Chat [Dubey et al., 2024], Mixtral-8x22B Instruct (141B)[Jiang et al., 2024], and Mixtral-8x7B Instruct (46.7B), and one closed-source LLM:\n5.2 Results\nAs seen in Figure 5, all models had similar performance on the dataset. While Clear and Relevant features were identified correctly a larger proportion of the time, the models' evaluation of Assertive and Specific were not as accurate. For each feature, we observed different models performing better. Due to the independence of our features, we can use each model independently to evaluate a given feature. For Clear, BERT had the highest weighted F1 score of 80.93%. For Optimistic and Assertive, RoBERTa-base had the highest weighted F1 scores of 62.69% and 49.10%, respectively. For Relevant, the LLMs, Llama-3-70b-Chat and Mixtral-8x22B Instruct (141B), outperformed the Pre-trained Language Models (PLMs), with Llama-3-70b-Chat achieving the highest weighted F1 score of 82.75%. For Specific, FinBERT had the highest weighted F1 score. This can be attributed to the fact that the other models are general-purpose models, whereas FinBERT is a domain-specific model for finance. For Cautious, BERT outperformed the other models with a weighted F1 score of 60.66%. Across all six features, RoBERTa-base had the highest average weighted F1 score of 63.95%. Mixtral-8x22B Instruct (141B) had a higher average weighted F1 score than Llama-3-70b-Chat.\nThe features Clear and Relevant were the easiest for models to identify, with BERT achieving the highest weighted F1 score of 80.93% for Clear and Llama-3-70b-Chat scoring 82.75% for Relevant. These features are more straightforward to detect as they rely on linguistic cues like coherence and topic alignment, making them accessible for general-purpose models. In contrast, detecting Assertive and Specific was more challenging. RoBERTa-base led in detecting Assertive with a score of 49.10%, while FinBERT excelled in identifying Specific, which relies on domain-specific technical details. These lower scores reflect the difficulty models face in capturing nuanced aspects such as tone and technicality.\nAnalysis of Model Performance FinBERT's higher performance for Specific emphasizes the value of domain-specific pre-training, as general-purpose models struggled with specialized financial terminology. The better performance on Clear and Relevant stems from their objective nature, as they rely on straightforward criteria-whether an answer is understandable and relevant to the question. However, detecting Assertive and Specific requires models to interpret subtle cues, making them harder to identify.\nThe performance discrepancies highlighted in this analysis open up important avenues for further research and development of models capable of handling subjective features more effectively. The challenges faced by current models in identifying nuances like assertiveness, cautiousness, and specificity suggest that standard pre-training on large corpora may not be sufficient for capturing complex human communication in high-stakes environments. Additionally, constructing richer training datasets with more nuanced annotation guidelines could help models learn to distinguish subtle variations in tone, sentiment, and technical specificity. This opens up opportunities to explore new architectures or techniques, such as reinforcement learning or attention mechanisms, that focus on capturing the intent and subjectivity behind language, thereby enhancing the model's capacity to perform well in complex, subjective question and answer scenarios. Furthermore, a comparison of model latency, as explored by Shah and Chava [2023], could be an interesting direction for future work to assess the trade-offs between performance and efficiency.\n5.3 Transfer Learning Ablations\nThis study evaluates the transfer learning capabilities of the best-performing model, ROBERTa-base, originally trained and tested on the SubjECTive-QA dataset. Specifically, we investigate its performance when fine-tuned on the SubjECTive-QA dataset, followed by testing on 65 question-answer pairs from White House Press Briefings and Gaggles as outlined in Section N with the outcomes of these transfer learning experiments. The model achieved a mean weighted F1 score of 65.97% across all the features, performing the best on Clear and the worst on Cautious. All the individual features' weighted F1 scores are outlined in Appendix N. This shows the broader applicability of the dataset across different significant domains such as Politics where clarity and transparency are of utmost importance."}, {"title": "6 Related Works", "content": "Subjective Datasets Recent advancements in sentiment analysis such as Sy et al. [2023] have led to tailored tools and the creation of subjective datasets that have high potential within the financial domain. Many studies emphasize the importance of emotional information [Chen et al., 2023a] and linguistic extremity [Bochkay et al., 2020] on stock returns and investor opinions. The FinArg dataset curated by Alhamzeh et al. [2022] delves into argumentative sentiment while the General Numeral Attachment dataset generated by Shi et al. [2023] enhances numeral interpretation in ECTs, improving volatility forecasting. However, these datasets remain focused on a singular field, limiting their applicability across financial tasks. To optimize model performance, diverse data is crucial [Liang, 2016, Shah et al., 2022]. While the mentioned datasets take a unidimensional approach, our method leverages the multidimensional nature of ECTs to better capture sentiment.\nSentiment Analysis and Annotations Previous studies on the role of language in corporate reporting only take into account the tone of negative or positive words [TETLOCK, 2007, Loughran and McDonald, 2010] whilst our dataset focuses on a multidimensional analysis of 6 features. Most prior datasets also annotate single turn QA systems [Zhu et al., 2021, Qu et al., 2019, Li et al., 2022] without taking account the context of the question being asked [Deng et al., 2022]. Furthermore general sentiment datasets such as Malo et al. [2014] and Sinha and Khandait [2020] lose accuracy because they annotate over large text [Tang et al., 2023]. Our dataset aims to utilize the context of both questions and answers to augment our manual annotation process and incorporate a more nuanced annotation style to not lose accuracy.\nEarning Calls Based on their availability and the vast amount of information prevalent within them, ECTs proved to be a viable data source for our research. ECTs, hosted by publicly traded companies to discuss aspects of their earnings reports [Givoly and Lakonishok, 1980, Keith and Stent, 2019], remain to be a major form of communication that help investors to review their price targets and trade decisions [Frankel et al., 1999, Kimbrough, 2005, Matsumoto et al., 2011]. Recently, Shah et al. [2024] proposed a novel framework for fine-tuning LLMs on earnings call transcripts, integrating both sentiment and financial performance features, a method that enhances predictive power for earnings surprises. Secondly, sentiment analysis within ECTs has historically been proven to possess a correlation to earning surprises [Price et al., 2012, Bowen et al., 2002, Doyle et al., 2012], providing quantitative value of analyzing subjectivity in ECTs. Finally, sentiment analysis, fine tuning of LLMs, and deep learning tactics on ECTs offer valuable insight to predict companies' future earnings surprises [Koval et al., 2023, Larcker and Zakolyukina, 2012] and emotional reaction [Bochkay et al., 2020, Chava et al., 2022] with reasonable accuracy. Our dataset allows for NLP models to be fine tuned on the subjectivity of ECTs with the goal to be generally used on QA pairs in various fields of research.\nDatasets in similar domains Appendix K provides a brief comparison of SubjECTive-QA with other datasets in the financial domain, focusing on the following attributes: size, number of features, list of labels, and license used. The datasets include TAT-QA [Zhu et al., 2021], a question-answering benchmark based on a hybrid of tabular and textual content in finance; FinQA [Chen et al., 2022a], a dataset designed for numerical reasoning over financial data; FinArg [Alhamzeh et al., 2022], which annotates argument structures in earnings calls; TruthfulQA [Lin et al., 2022], which measures how models mimic human falsehoods; Trillion Dollar Words [Shah et al., 2023], which evaluates the meeting minute sentences of the federal reserve of the United States; ConvFinQA [Chen et al., 2022b], which explores chains of numerical reasoning in conversational financial question answering; and MathQA [Amini et al., 2019], a dataset focused on interpretable math word problems."}, {"title": "7 Limitations and Future Work", "content": "Earnings Calls Sampled Our dataset of Earnings Calls only encompasses companies listed in the New York Stock Exchange from 2007 to 2021 balanced across the 6 major industries defined in Appendix H.1. Insights from this dataset may not be applicable for Earnings Calls of companies from other countries or years. We plan to extend our research to other years and countries and test the broader applicability of our models."}, {"title": "8 Discussion", "content": "SubjECTive-QA offers the first dataset of long form QA pairs annotated across six features. The dataset consists of 2, 747 QA pairs taken from 120 Earnings Call Transcripts annotated on six features: Clear, Assertive, Cautious, Optimistic, Specific, and Relevant. The goal of SubjECTive-QA is to serve as a resource for further research into the intersection between language and financial markets. Rather than solely focusing on the quantitative information within the Earnings Calls, measuring the various features present within QA pairs provides another dimension to analyze the effect of ECs on market dynamics. This paper defines the creation of SubjECTive-QA and examines introductory analysis into the distribution of our manual annotations. We believe that SubjECTive-QA can be a valuable resource for further exploration into the impact of ECs on financial markets and the FinNLP domain at large.\nBroader Impact: By capturing the intricate nuances of speech, our subjective dataset also lays the foundation for a new approach to identifying disinformation and misinformation. Conventional detection methods often fail to recognize its subtle linguistic cues, so our findings will prove vital. SubjECTive-QA therefore has applications beyond the field of FinNLP in various fields such as sports, news and politics to identify misinformation and disinformation. Through systematic analysis and refinement of the specifics of this dataset, researchers can develop algorithms capable of discerning various forms of disinformation, thereby advancing the field's ability to combat deceptive narratives effectively."}]}