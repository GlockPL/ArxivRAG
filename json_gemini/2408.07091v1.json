{"title": "Node Level Graph Autoencoder: Unified Pretraining for Textual Graph Learning", "authors": ["Wenbin Hu", "Huihao Jing", "Qi Hu", "Haoran Li", "Yangqiu Song"], "abstract": "Textual graphs are ubiquitous in real-world applications, featuring rich text information with complex relationships, which enables advanced research across various fields. Textual graph representation learning aims to generate low-dimensional feature embeddings from textual graphs that can improve the performance of downstream tasks. A high-quality feature embedding should effectively capture both the structural and the textual information in a textual graph. However, most textual graph dataset benchmarks rely on word2vec techniques to generate feature embeddings, which inherently limits their capabilities. Recent works on textual graph representation learning can be categorized into two folds: supervised and unsupervised methods. Supervised methods finetune a language model on labeled nodes, which have limited capabilities when labeled data is scarce. Unsupervised methods, on the other hand, extract feature embeddings by developing complex training pipelines. To address these limitations, we propose a novel unified unsupervised learning autoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ language models as the backbone of the autoencoder, with pretraining on text reconstruction. Additionally, we add an auxiliary loss term to make the feature embeddings aware of the local graph structure. Our method maintains simplicity in the training process and demonstrates generalizability across diverse textual graphs and downstream tasks. We evaluate our method on two core graph representation learning downstream tasks: node classification and link prediction. Comprehensive experiments demonstrate that our approach substantially enhances the performance of diverse graph neural networks (GNNs) across multiple textual graph datasets. Remarkably, a two-layer GNN can achieve a testing accuracy of 77.10% on the ogbn-arxiv dataset. Furthermore, by ensembling with GNNs from existing SOTA methods, our method achieves a new SOTA of 78.34%.", "sections": [{"title": "1 Introduction", "content": "Textual graphs are graph-based data that incorporate textual attributes such as phrases, sentences, or documents, where each entity contains a segment of text. The rich information from textual attributes in textual graphs significantly enhances the per- formance across a diverse range of real-world applications, such as citation graphs [12, 41], social networks [2, 22], knowledge graphs [32, 34, 43], and recommendation systems [9, 23].\nUnlike traditional Natural Language Processing (NLP) tasks, the text on the nodes in a textual graph is correlated with each other, which is important for downstream training and inference. For example, the ogbn-arxiv dataset [12] is a citation network for aca- demic articles, where the nodes contain the title and abstract of the corresponding article, and the edges represent citations between the articles. As shown in Figure 1, textual graph learning typically involves two stages: 1) Extracting features from the text of the nodes, and 2) Training graph neural networks (GNNs) on the ex- tracted node features. The second stage has been well-studied, and there are powerful GNN models available to solve it [6, 18, 20]. The former stage, however, still requires more effective feature extrac- tors, which is an active area of research in the field of textual graph representation learning."}, {"title": "2 Related Work", "content": "2.1 Utilizing LMs for Textual Graph\nFor textual graph learning, language models (LMs) are an essential component for capturing text information. The LM+GNN paradigm has become the mainstream approach for textual graph-related tasks. Existing LM+GNN methods can be divided into two types: one-step and two-step methods. For one-step methods, the most"}, {"title": "2.2 Graph Pretraining Frameworks", "content": "Significant progress has been made in developing graph pretraining to learn expressive representations for GNNs. Several GNN pre- training frameworks have been proposed: 1) Graph Autoregressive Modeling: An autoregressive framework to perform iterative graph reconstruction. GPT-GNN [14] predicts one masked node and its edges at a time given a graph with randomly masked nodes and edges. MGSSL [48] generates molecular graphs in an autoregres- sive manner. 2) Masked Components Modeling: Masking out some components in a graph and training a GNN to predict them. Hu et al. [13] propose attribute masking where some attributes on nodes or edges are masked out for prediction. GROVER [31] masks out some subgraphs in a molecular graph to capture the contextual information. 3) Graph Contrastive Learning: Constructing a self- supervised learning objective to learn representations that capture the structural and semantic similarities between graph components. DGI [40] and InfoGraph [36] enhance the representation of a graph by maximizing the mutual information between the graph-level structure and subgraph-level structure. MVGRL [8] uses node diffu- sion to generate augmented nodes and maximizes the mutual infor- mation between the augmented and original nodes. GRACE [51] and its variants [42, 52] maximize the agreement of the different two augmented views of the node representation. GraphCL [45] and its variants [37, 44] propose new contrastive strategies for graph pretraining. Unfortunately, these existing graph pretraining frameworks cannot be trivially adapted to textual graphs, and the modified frameworks still may not fully effectively capture both the textual and structural information. NodeGAE proposes a novel pretraining framework for textual graphs, which can capture the textual and structural information simultaneously."}, {"title": "2.3 Autoencoders for Feature Extraction", "content": "In computer vision, the encoded features from an autoencoder can capture important visual characteristics of the input images, such as edges, textures, and shapes [16]. Similarly, in natural language processing, the encoded features from an autoencoder can cap- ture semantic and syntactic information from the input text [33].In graph-based learning, Graph Autoencoder (GAE) reconstructs the"}, {"title": "3 Preliminary", "content": "In this section, we will formally define the problem we are address- ing and introduce key concepts related to our proposed method."}, {"title": "3.1 Problem Formulation", "content": "We denote the textual graph as $G = (V,E)$, where V and $E \\subset V \\times V$ represent the set of nodes and the set of edges respectively. We convert the edge set $E$ to an adjacency matrix $A \\in {0, 1}^{|V|\\times|V|}$. For each node $v \\in V$, the node attribute is a text sequence $t \\in T$, where T is the set of textual attributes, aligning with the set of nodes V. We focus on two fundamental problems in textual graphs: node classification and link prediction. For node classification, our goal is to build a model $\\Phi: V \\rightarrow Y$ to predict the label $y \\in Y$ of the node, where $Y \\in R^C$ and C is the number of classes. For link prediction, we aim to construct a model $\\Phi: V \\times V \\rightarrow {0, 1}$ to predict the linkage between two nodes, where $\\Phi(v_i, v_j) = 1$ if there exists a link between node $(v_i, v_j)$, otherwise $\\Phi(v_i, v_j) = 0$."}, {"title": "3.2 GNN for Textual Graph Learning", "content": "GNN provides a unified framework to make predictions on graphs, which recently dominates the field of graph learning. Generally, GNN recursively aggregates the neighbour feature embeddings to predict the properties of nodes. For simplicity, we take the vanilla Graph Convolutional Network (GCN) [18] for formulation. A GCN layer can be formulated as: $\\sigma(\\~{A}HW)$, where $\\~{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$, $D_{ii} = \\sum_{j\\in|V|} A_{ij},\\forall i \\in [V]$, $H\\in R^{|V|\\times d}$ is the feature embed- ding matrix with the embedding dimension d, $W \\in R^{d\\times d_w}$ is the model weight with the output dimension $d_w$, and $\\sigma(\\cdot)$ is the acti- vation function. For node classification, a classifier can be directly appended to the final layer of the GNN to predict the class of the node; while for link prediction, a similarity function is adopted to compute the similarity score between two node embeddings. Unlike traditional graph-based tasks, the features of textual graphs do not directly construct feature embeddings for GNN training. Instead, the features are text sequences, which poses a challenge for creat- ing feature embeddings that can capture both the text information and the graph structure information, which can be formulated as: $H = \\Psi(T, V, A)$. In most graph benchmarks [12, 43], the feature embeddings are generated using techniques such as skip-gram or bag-of-word [24]. Additionally, recent works have proposed new approaches for more effective feature extraction in textual graph settings [4, 5, 10]."}, {"title": "3.3 InfoNCE Loss", "content": "InfoNCE loss [39] is a commonly used objective function for self- supervised representation learning. The core idea behind InfoNCE is to encourage the model to learn similar representations for data"}, {"title": "4 Node Level Graph Autoencoder", "content": "We propose a novel pretraining framework for improving textual graph learning: the Node Level Graph Autoencoder (NodeGAE). The architecture of NodeGAE is shown in Figure 2, and the whole training pipeline is illustrated in Figure 3. Our method follows a two- stage training pipeline: 1) First, we train an autoencoder in a self- supervised manner to reconstruct the text attributes on the nodes. The encoder takes a text sequence $t_{1:M}$ with a sequence length of M as the input and outputs the extracted feature embedding h:\n$h = LM_{encoder}(t_{1:M})$.\nThen, the feature embedding is fed into the decoder to reconstruct text sequence $\\^t_{1:L}$ with a sequence length of L:\n$\\^t_{1:L} = LM_{decoder}(h)$.\nWe take the language modeling loss as the text reconstruction loss:\n$L_{LM}(t_{1:M}) = -\\sum_{i=1}^{M}logp(t_i|t_{<i})$.\nUnder the self-supervised learning framework, the text reconstruc- tion objective trains the encoder and decoder simultaneously. Dur- ing this training process, the autoencoder effectively learns a map- ping between the latent embeddings and the corresponding textual data. As a result, the latent embeddings can serve as a powerful representation of the data within the textual graph. 2) Then, we take the frozen encoder of the trained autoencoder as a feature extractor to obtain feature embeddings for the downstream GNN training. For the downstream tasks, we focus on node classification and link prediction.\nThe pseudo-code in PyTorch-style [28] for the whole training process is demonstrated in Algorithm 1. For the limited space of the paper, the algorithm is only written for the task of node classi- fication."}, {"title": "4.1 Graph Structure Learning", "content": "Textual graph learning combines text learning with graph struc- ture learning. Through text reconstruction, the autoencoder can effectively capture the semantic information of the text attributes. We also want the autoencoder to learn the local structure of each node in order to generate improved node embeddings. For graph structure learning, we leverage the InfoNCE loss [39] to learn the structural information. Positive samples are drawn from the node's neighbors, while negative samples come from other data points in"}, {"title": "4.2 Variational Framework", "content": "Our proposed approach, NodeGAE, can be formulated into a varia- tional framework. Let $p_\\theta (T|V, E)$ represent the distribution over the text T of the corresponding node V with its edge E and $q_\\phi (H|T, V, E)$ represent the estimated posterior distribution over the latent representation H for data in the textual graph, where H,T,V, E are random variables, and $\\theta, \\phi$ represent the parameters for the encoder and the decoder. The goal is to maximize the dis- tribution $p_\\theta (T|V, E)$. From the original Variational Autoencoder Encoder (VAE) framework [16], we know that the distribution is intractable, yet it has an Evidence Lower Bound (ELBO) that can be used for optimization. We have also derived the ELBO for Node- GAE:\n$\\log p_\\theta (T|V,E) \\geq E_{h~q_\\phi (H|T,V,E)}[\\log p_\\theta (T|H,V,E)] - D_{KL}(q_\\phi (H|T,V,E) || p_\\theta (H|V, E)),$"}, {"title": "4.3 Parameterization", "content": "The encoder and decoder in our autoencoder architecture are param- eterized as a sentence embedding model [25, 30] and a T5-like [29] language model, i.e. an encoder transformer model and an encoder- decoder transformer model, respectively. The feature embedding takes the average of the encoder outputs across all input tokens. To enhance the reconstruction performance, the embedding from the encoder is projected to a larger size embedding: $W_2\\sigma(W_1h)$, where $W_1 \\in R^{d_{enc}\\times d_{enc}}$, $W_2 \\in R^{s_{dec}\\times d_{enc}}$, s is the sequence length, and $d_{enc}, d_{dec}$ are the embedding dimension of the encoder and decoder. The projected encoder embedding is then reshaped into a sequence of input-sized embeddings and fed as the input to the decoder: Decoder($W_2\\sigma(W_1h)$). This reshaping step allows the decoder to process the encoded text representation in a sequence-to-sequence manner, generating the reconstructed text output. The decoder can more effectively learn the semantic information from an input sequence than from just an input embedding alone.\nThe parameterization design for NodeGAE allows the autoen- coder to leverage the powerful text comprehension capabilities of language models for the text reconstruction task. By projecting the encoder embedding to a larger size, the model can better capture the nuanced semantic information in the input text, leading to im- proved reconstruction quality."}, {"title": "5 Experiments", "content": "We have performed extensive experiments to evaluate the effec- tiveness of our proposed method NodeGAE, by showing the perfor- mance on downstream tasks: node classification and link prediction. In Section 5.1, we describe the experimental setup, including the datasets, models, and evaluation metrics used. In Section 5.2, we present the main results of NodeGAE accross diverse models and textual graph datasets. In Section 5.3, we conduct ablation studies to check the effectiveness of InfoNCE loss. Besides, in Section 5.4 we further investigate NodeGAE, demonstrating a faster convergence rate. Finally, in Section 5.5, we demonstrate the text reconstruction process during the pretraining phrase."}, {"title": "5.1 Experimental Setup", "content": "Datasets. For node classification, we evaluate our method on two textual graph datasets: ogbn-arxiv and ogbn-products [12], where the details of the datasets are shown in Table 3. We keep the origi- nal split for the datasets, and the raw text data is provided by the officials. For link prediction, we create a link prediction dataset based on ogbn-arxiv: we random sample the links in ogbn-arxiv for training, validating, and testing with a ratio of 7:2:1.\nAutoencoder Models. For the encoder, we use a sentence-T5-base model [25], which is a T5-base encoder pretrained for text retrieval with 110M parameters. For the decoder, we use a T5-base model [29] with 223M parameters. The projection layer appended to the out- put layer of the encoder is a 2-layer MLP. The sequence length for the projection is s = 16. We train the autoencoder using the Adam optimizer [15] with a learning rate of 1e-4 and 10,000 linear warm-up steps. The token sequence length is 256. For the InfoNCE loss, we sample the neighbors from 1-hop and 2-hop neighbors and set $\\alpha_1 = 1, \\alpha_2 = 0.1, \\tau = 0.5$ in Equation (5).\nClassifiers. We evaluate feature embeddings with commonly used baseline classifiers: MLP, GCN [3] and GraphSage [6] to perform node classification and link prediction. Due to the vast scale of the ogbn-products dataset, training GCN and GraphSAGE becomes impractical. Therefore, we take ClusterGCN [3] as a substitution. Additionally, to achieve higher accuracy, we utilize SOTA GNN backbones: RevGAT [20], GAMLP [47] and SAGN+SCR [35, 46]. We take the Adam optimizer [15] to train all the classifiers. For node classification, the learning rate is set to 1e-2 for MLP and GraphSAGE, 5e-3 for GAMLP and SAGN+SCR, 2e-3 for RevGAT; while for link prediction, the learning rate is set to 1e-4 for both MLP and GraphSAGE.\nEvaluation Metrics. For evaluation metrics, we use accuracy and the Area Under the ROC Curve (ROC-AUC) [1] for node classi- fication and link prediction respectively. Accuracy measures the fraction of nodes that are correctly classified; ROC-AUC captures the model's ability to rank true links higher than false links. Both accuracy and ROC-AUC range from 0 to 1. We report the mean result + one standard deviation over 10 repeated runs. The best- performing methods are highlighted in bold."}, {"title": "5.2 Main Result", "content": "Following the settings described in 5.1, we evaluate NodeGAE by demonstrating the quality of the generated feature embeddings. The results are shown in Table 1 for node classification and in Table 2 for link prediction. To assess the quality, we evaluate the downstream performance on the feature embeddings, i.e., accuracy for node classification and ROC-AUC for link prediction. We com- pare the quality of our feature embeddings hNodeGAE with those generated by commonly used methods: hshallow, shallow feature embeddings created by skip-gram [24]; hsent-emb, feature embed- dings from a sentence-T5-base model [25], a pretrained information retrieval model; hlm-finetune, feature embeddings from a T5-base model finetuned on the text of labeled nodes; and hgiant, feature embeddings from a self-supervised learning method utilizing graph structure [4]. The results in the tables show that our approach con- sistently outperforms the other methods across all datasets and models, demonstrating its effectiveness in improving textual graph learning.\nFurthermore, on ogbn-arxiv dataset for node classification, our best model (RevGAT) achieve an accuracy of 77.10%, which is com- parable to the performance of existing SOTA methods: SimTeG [5], TAPE [10], and GLEM [49], which achieve an accuracy of 77.01%, 77.50%, and 76.94%; for link prediction, our best model (MLP) reach"}, {"title": "5.3 Ablation Study", "content": "We conduct ablation studies to show the extent to which the qual- ity of the feature embeddings can be improved by using the In- foNCE loss [39]. The results are shown in Table 5. The experiment is performed on the node classification and link prediction using ogbn-arxiv dataset. A represents the margin of improved perfor- mance when using the InfoNCE loss. As demonstrated in the table, InfoNCE can generally enhance the performance of NodaGAE. Par- ticularly for MLP, there is a 4.34% improvement in accuracy for node classification and an 8.74% enhancement in ROC-AUC for link prediction."}, {"title": "5.4 Convergence Analysis", "content": "We compare the convergence speed of the GNN trained on the feature embeddings of hNodeGAE, hshallow, and hsent-emb, which is presented in Figure 4. We illustrate the evolution of validation performance, test performance, and training loss over the course"}, {"title": "5.5 Text Reconstruction", "content": "To test whether the autoencoder can successfully reconstruct the text, we show the BLEU [27], ROUGE [21], and F1 scores of the generated text produced by the autoencoder during the pretraining stage, as illustrated in Figure 6. The curve in the figure demon- strates that the autoencoder can reconstruct text with high quality. Specifically, the model can achieve BLEU, ROUGE, and F1 scores of 21.98%, 61.20%, and 59.36%, respectively. These results indicate that the feature embeddings generated by the model contain rich textual information."}, {"title": "6 Conclusion", "content": "In this work, we have proposed NodeGAE, a novel node-level graph autoencoder framework for textual graph representation learning. Our simple and general approach leverages unsupervised learning through text reconstruction, which allows the encoder to effectively capture the valuable textual information from the graph nodes. To further enhance the embeddings, we incorporate InfoNCE loss to capture the graph structure. By taking advantage of unsupervised learning and leveraging the synergy between the text and the graph structure, our model is able to generate high-quality node embed- dings that lead to superior performance on downstream tasks. Our comprehensive experimental evaluation demonstrates that Node- GAE achieves promising performance across diverse datasets and downstream tasks. We believe that the insights gained from this work will inspire further research in this important and growing area."}]}