{"title": "FoPru: Focal Pruning for Efficient Large Vision-Language Models", "authors": ["Lei Jiang", "Weizhe Huang", "Tongxuan Liu", "Yuting Zeng", "Jing Li", "Lechao Cheng", "Xiaohua Xu"], "abstract": "Large Vision-Language Models (LVLMs) represent a significant advancement toward achieving superior multimodal capabilities by enabling powerful Large Language Models (LLMs) to understand visual input. Typically, LVLMs utilize visual encoders, such as CLIP, to transform images into visual tokens, which are then aligned with textual tokens through projection layers before being input into the LLM for inference. Although existing LVLMs have achieved significant success, their inference efficiency is still limited by the substantial number of visual tokens and the potential redundancy among them. To mitigate this issue, we propose Focal Pruning (FoPru), a training-free method that prunes visual tokens based on the attention-based token significance derived from the vision encoder. Specifically, we introduce two alternative pruning strategies: 1) the rank strategy, which leverages all token significance scores to retain more critical tokens in a global view; 2) the row strategy, which focuses on preserving continuous key information in images from a local perspective. Finally, the selected tokens are reordered to maintain their original positional relationships. Extensive experiments across various LVLMs and multimodal datasets demonstrate that our method can prune a large number of redundant tokens while maintaining high accuracy, leading to significant improvements in inference efficiency.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Vision Language Models (LVLMs) [2, 15, 20, 26, 35] have exhibited remarkable capabilities in diverse multimodal scenarios, propelling advancements in intricate tasks such as image and language comprehension. These models typically involve a substantial number of visual tokens, often ranging from hundreds to thousands [5]. The large quantity of visual tokens significantly amplifies the training and inference costs of LVLMs [7].\nTo alleviate the issues of excessive visual tokens in LVLMs, researchers have proposed a series of visual token compression methods [4, 7, 24]. For instance, Q-Former [15] and Resampler [2] utilize cross-attention and a set of learnable queries to extract the most relevant visual tokens and manage their quantity. Abstractor [6] and LDP [8, 9] employ convolutional layers to aggregate visual features, thereby generating compressed visual tokens. DenseConnector [30] regulates the number of visual tokens through learnable MLP layers. In DocKylin [33], redundant regions in images are identified and removed using image gradient information, while a k-means clustering method is employed to extract relevant tokens from a vast pool of visual tokens. However, these compression methods generally require retraining LVLMs, rendering them unsuitable for direct application to pre-existing general-purpose LVLMs.\nWe observe that the deep layers in the visual encoder exhibit an imbalance in attention distribution, where attention is concentrated on a limited number of tokens. This suggests that during the visual encoding stage, a small subset of visual tokens already captures critical visual information, while a significant proportion of tokens are likely unimportant and redundant. Motivated by this observation, we propose Focal Pruning (FoPru), a training-free token pruning approach that can be seamlessly applied to various LVLMs. Specifically, FoPru consists of three stages. First, the Token Significance stage leverages attention scores derived from the visual encoder to calculate the significance of each token. The Token Pruning stage then prunes visual tokens based on these significance scores. In the Token Reordering stage, tokens are reordered according to their original positions, maintaining their relative positional relationships. Within the Token Pruning stage, we further introduce two alternative pruning strategies: Rank Pruning, which retains the most critical tokens from a global perspective, and Row Pruning, which preserves local continuous tokens row by row.\nTo validate the effectiveness of FoPru, we conduct experiments on diverse models and datasets. The results demonstrate that the FoPru approach significantly reduces visual tokens while achieving remarkable performance across multiple datasets. Remarkably, even at an extreme token retention ratio of 0.2% (retaining as few as 5 tokens), FoPru maintains approximately 60% accuracy on the Ai2D[13] and SQA[22] datasets. Additionally, using only 25% of visual tokens FoPru yields accuracy within a 1% margin on MMMU [32], SQA, and POPE [17] datasets, with a maximum improvement of 2.52X in Time To First Token (TTFT) and 1.24X in Time Per Output Token (TPOP), thereby substantially enhancing inference efficiency for LVLMs.\nThe core contributions of this paper are as follows:\n1. Proposing a General Visual Token Pruning Method: We introduce Focal Pruning (FoPru), a training-free approach that achieves pruning based on the attention distribution provided by the visual encoder itself, which is applicable to various LVLMs.\n2. Developing a Framework Supporting Multiple Token Pruning Strategies: We construct a framework that implements various pruning strategies based on the distinct characteristics of images.\n3. Validating the Effectiveness of the Method Across Multiple Datasets and Models: Extensive experiments on diverse benchmark datasets and LVLMs demonstrate that our FoPru can significantly reduce the number of visual tokens and achieve efficient inference while maintaining accuracy."}, {"title": "2. Related Work", "content": "2.1. Large Vision-Language Models (LVLMs)\nLarge Language Models (LLMs), such as GPT-4 [1] and Llama [27], have achieved remarkable progress and demonstrated excellent capabilities in a wide range of natural language understanding tasks. In light of the great advantages of LLMs, recent Large Vision-Language Models (LVLMs) [31] transform image information into visual tokens and align them with textual tokens as inputs to the LLMs, resulting in significant advancements in multimodal capabilities. First, BLIP-2 [15] is a pioneering model that employs a learnable and lightweight Q-Former to bridge a vision encoder and a LLM. This model freezes the two components separately and performs two-stage pre-training, thereby achieving cross-modal alignment. Through improving BLIP-2, many researchers produce various excellent open-source LVLMs [3, 28, 35]. MiniGPT-4 [35] simplifies alignment between a visual encoder and the LLM by using a single linear projector. LLava [18, 20] collects instruction data generated by GPT4 to fine-tune both the LLM and the visual-to-text projection matrix through end-to-end visual instruction tuning. There are also emerging powerful proprietary models, such as GPT-4V [29], Qwen-VL-Max [3], and Gemini [23], which show top-tier multimodal capabilities across a variety of visual-language tasks.\n2.2. Token Reduction in LVLMs\nAlthough current LVLMs have achieved remarkable vision-language understanding abilities, concerns remain that their inference efficiency is limited by their auto-regressive generation paradigm and potential token redundancy, especially when dealing with a large number of visual and textual tokens. In the literature, some researchers have attempted to employ various methods to mitigate this issue. Q-Former [15] and Resampler [2] utilize cross-attention and a set of learnable queries to obtain the most relevant tokens to control their quantity. Abstractor [6] and LDP [8, 9] employ convolutional layers to aggregate visual features, generating compressed tokens. DenseConnector controls token quantity through learnable MLP layers. DocKylin [33] leverages Adaptive Pixel Slimming (APS) and Dynamic Token Slimming (DTS) to compress visual content at the pixel and token levels. TokenPacker [16] proposes a novel visual projector, which injects enriched high-resolution characteristics into a coarse low-resolution one to generate the condensed visual tokens. The aforementioned training-based methods require substantial resources to train specific LVLMs and lack generality. Instead, FastV [7] is a recent training-free method that performs layer-level pruning to discard tokens with low attention scores in the LLM backbone. Similarly, PruMerge and PruMerge+ [24] are training-free methods that cluster visual tokens, updating key tokens through k-nearest neighbor weighted averaging. However, these approaches overlook the importance of encoder's internal attention map and the mode collapse we have discovered in its deeper layers, thereby limiting their ability to guide token redundancy identification and leading to suboptimal pruning performance."}, {"title": "3. Preliminary", "content": "LVLMs are aimed at generating textual responses based on input images and instructions [31]. A typical LVLM consists of three key modules: a vision encoder, an advanced LLM, and a projector, which serves as a bridge for modality alignment. First, the vision encoder transforms the input image into visual embeddings Ev, often utilizing the ViT architecture [10]. Next, the projector converts these visual embeddings into visual tokens Tv by mapping them into the text space, making them understandable to the LLM. Given the generated visual tokens Tv and instructions' textual tokens Tt, the LLM then produces the L-length output response Y in an auto-regressive manner based on the following probability distribution:\n$P(Y|T_t, T_v) = \\prod_{i=1}^{L} P(Y_i|T_t, T_v, Y_{<i}).$\nAs shown in the formula, the inference efficiency and memory requirements of LVLMs heavily depend on the length of the input tokens that the LLM needs process, which consist of both textual and visual tokens. In fact, due to the auto-regressive nature of LLM decoding, the computational complexity of the LLM is proportional to the square of the input token length. This indicates that reducing the input tokens is crucial for improving the inference efficiency of LVLMs.\n3.1. Token Redundancy Analysis\nIn this subsection, we present important data analysis on the redundancy of visual tokens in LVLMs. First, we analyze the high proportion of visual tokens among the input tokens to the LLM. Then, we observe the imbalanced attention distribution in the vision encoder, which indicates the presence of numerous unimportant and redundant tokens in its output.\nHigh Proportion of Visual Tokens. We randomly select 10 samples from each of seven multimodal datasets and count the number and proportion of visual tokens and textual tokens using LLaVA-NeXT-8B [14]. The average results are presented in Figure 3. This statistic suggests that visual tokens dominate the input tokens to the LLM, which aligns with findings from other research on other LVLMs [7]. This high proportion of visual tokens affects inference efficiency, suggesting that some visual tokens might be unimportant and could be pruned to improve processing speed."}, {"title": "4. Methodology", "content": "4.1. Overview\nFigure 2 illustrates the overall architecture of FoPru for LVLMs. FoPru identifies and prunes redundant visual tokens before they reach the LLM. First, we leverage the attention maps from the vision encoder to calculate the significance score of each token in the Token Significance stage. Then we introduce two Token Pruning Strategies: rank and row strategy, which focus on retaining global key visual information and local continuous information, respectively. Next, in the Token Reordering stage, the selected tokens are reordered to restore their relative positional information. An overview of this algorithm is provided in Algorithm 1.\n4.2. Token Significance\nTo prune redundant tokens for accelerating inference, it is crucial to identify the significance scores for each token. The pruning process is guided by multi-head attention map $A_k \\in \\mathbb{R}^{N \\times N}$ for $k \\in \\{1,...,H\\}$, extracted from the penultimate layer of the encoder, captures spatial dependencies among tokens. Here, N represents the number of visual tokens, and H denotes the number of attention heads. This layer is chosen because, in LLaVA, its image feature output provides the primary visual representation, which is subsequently aligned with text by projecting it into the visual token space. First, we average the attention weights $A_k$ across all heads as follows:\n$A = \\frac{1}{H} \\sum_{k=1}^{H} A_k$ \nNext, we take the average along the last two dimensions of A to get the average attention score for each dimension:\n$s_1 = \\frac{1}{N} \\sum_{j=1}^{N} A[:, j], s_2 = \\frac{1}{N} \\sum_{i=1}^{N} A[i, :]$\nwhere $s_1$ represents the average attention across columns, and $s_2$ represents the average across rows. Then, we compute the variance of these two vectors, $Var_1$ and $Var_2$, to identify the direction with greater data dispersion: $Var_1 = Var(s_1), Var_2 = Var(s_2)$. Intuitively, the dimension with higher variance has a more dispersed data distribution, making important tokens stand out more prominently. We therefore select the vector with the larger variance as the final token significance score Sig:\n$Sig = \\begin{cases} s_1, & \\text{if } Var_1 > Var_2 \\\\ s_2, & \\text{otherwise} \\end{cases}$\n4.3. Token Pruning\nWe propose two alternative strategies to implement token pruning in FoPru. We denote r as the token retention ratio, a predefined hyperparameter.\nRank strategy To capture the core global visual information, we first calculate each visual token's significance score $Sig \\in \\mathbb{R}^{1 \\times N}$ using Eq. 4. Visual tokens are then globally ranked by these significance scores, and only the top $N \\times r\\%$ tokens are retained while the less significant tokens are discarded. This rank strategy enables the model to concentrate on the most globally informative visual features, reducing potential interference from redundant tokens.\nRow Strategy To implement a structured token pruning mechanism, we begin by reshaping the one-dimensional visual token significance score vector Sig, into a two-dimensional layout $Sig_{grid} \\in \\mathbb{R}^{n \\times n}$ that reflects the relative spatial positions within the image, where N = n x n. Considering that textual information is predominantly organized horizontally, we compute an aggregate significance score for each of the n rows. Based on their accumulated significance scores, we select n \u00d7 r% rows, denoted as $Sig_{grid}[R, :]$, and reshape them into a one-dimensional sequence. Here R is the index set of the most significant rows. The row strategy approach preserves spatial coherence and essential horizontal features, maintaining the structural integrity of the original image.\n4.4. Token Reordering\nTo preserve the relative positional relationships among the selected visual tokens, a reordering process is applied. The indices of selected tokens, determined by the rank or row strategy, are denoted as $Z_{idx}$ with shape (1, N \u00d7 r%). Sorting $Z_{idx}$ in ascending order yields $I_{sorted}$, which maintains the tokens' original spatial positions. Using $I_{sorted}$, we obtain the pruned and spatially ordered set of visual tokens, $T_{pruned} = \\{t_i | i \\in Z_{sorted}\\}$. This reordering is crucial for preserving the contextual integrity and continuity necessary for accurate inference by LVLMs.\nAfter reordering, the pruned visual tokens are first projected to align with the textual tokens in terms of modality. Then the visual tokens are combined with the textual tokens and jointly input into the LLM. This integrated input enables the LLM to utilize only the most relevant visual information to perform inference. As a result, the LVLMs can achieve greater computational efficiency while maintaining or even enhancing the final accuracy performance."}, {"title": "5. Experiments", "content": "5.1. Experimental Setting\nDatasets We utilize 7 widely used multimodal datasets to evaluate the performance, including POPE [17], MMMU [32], SQA [22], Ai2D [13], GQA [12], TextVQA [25] and Ocrbench [21]. POPE is utilized to evaluate the model's ability to identify and correct errors in images within multimodal scenarios. MMMU is a multimodal benchmark that covers multiple academic tasks, requiring university-level subject knowledge and reasoning skills. SQA (i.e., ScienceQA) focuses on answering questions in the science domain, covering a wide range of topics from basic science to advanced research. Ai2D is used to evaluate the model's ability to interpret and understand complex scientific and educational diagrams. GQA focuses on visual question answering tasks, testing the model's ability to understand and answer questions about image content. TextVQA involves processing and answering questions related to text found in images, requiring the model to recognize and understand the text within the images. Ocrbench concentrates on optical character recognition (OCR), evaluating the model's ability to recognize and interpret text in images of varying quality and text."}, {"title": "6. Conclusion and Limitations", "content": "This paper proposes a training-free and general inference optimization method for various LVLMs, called Focal Pruning (FoPru), which aims to address the issue of inefficient inference caused by a large amount of redundancy in visual tokens. Specifically, FoPru leverages the attention scores from the visual encoder to determine the significance score of visual tokens. Then, we provide multiple alternative attention-based token pruning strategies before the tokens are input into the projector to significantly reduce the number of visual tokens that the LLM needs to process. Finally, a token reordering method is employed to ensure that the relative positional information between tokens is preserved. Extensive experiments on three widely used LVLMs and seven multimodal datasets demonstrate that our FoPru can significantly reduce the number of visual tokens and improve inference speed while keeping the accuracy loss minimal."}]}