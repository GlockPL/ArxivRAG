{"title": "Dealing with Inconsistency for Reasoning over Knowledge Graphs: A Survey", "authors": ["Anastasios Nentidis", "Charilaos Akasiadis", "Angelos Charalambidis", "Alexander Artikis"], "abstract": "In Knowledge Graphs (KGs), where the schema of the data is usually defined by particular ontologies, reasoning is a necessity to perform a range of tasks, such as retrieval of information, question answering, and the derivation of new knowledge. However, information to populate KGs is often extracted (semi-) automatically from natural language resources, or by integrating datasets that follow different semantic schemas, resulting in KG inconsistency. This, however, hinders the process of reasoning. In this survey, we focus on how to perform reasoning on inconsistent KGs, by analyzing the state of the art towards three complementary directions: a) the detection of the parts of the KG that cause the inconsistency, b) the fixing of an inconsistent KG to render it consistent, and c) the inconsistency-tolerant reasoning. We discuss existing work from a range of relevant fields focusing on how, and in which cases they are related to the above directions. We also highlight persisting challenges and future directions.", "sections": [{"title": "1. Introduction", "content": "Knowledge Graphs (KGs) are widely adopted by world-leading organizations in a variety of fields, including technology, banking, and media, supporting a range of applications in the areas of knowledge acquisition, integration, and maintenance (Abu-Salih, 2021). One of the key benefits of KGs is the direct integration of formal reasoning. However, in practice, this is often hindered by inconsistencies (Pensel & Turhan, 2018). KGs can be seen as special cases of Knowledge Bases (KBs) where extensional knowledge, called the ABox, is organized in the form of a graph. Implicit knowledge is derived from the KG through a set of axioms, called the TBox, typically expressed as first-order formulas. Knowledge Graphs, a special kind of KBs, can be considered to be augmented versions of ontologies, while in some cases the terms are used interchangeably (Ehrlinger & W\u00f6\u00df, 2016; Mitchell et al., 2018; Heist et al., 2020). Other works use the term ontology when referring to solely a KG's TBox (Wang et al., 2018; Tran et al., 2020). Hogan et al. (2022) provide a comprehensive introduction to KGs and related research directions.\nKGs are often developed through (semi-) automated knowledge extraction processes, and/or by integrating knowledge from different resources, which often lead to errors and inconsistencies (Paulheim, 2016). Inconsistencies, in particular, arise when parts of the TBox and/or ABox contradict. As inconsistencies hinder the process of reasoning, dealing with them is an important problem. We provide an overview of the state-of-the-art of reasoning on inconsistent KGs, organized into three subtasks: a) the detection of inconsistencies, b) the fixing of inconsistencies, and c) reasoning in the presence of inconsistencies.\nThe scope of this survey is an overview of works that originate from different domains such as the Semantic Web, data quality, and query answering, highlighting why and how they are relevant to formal reasoning on inconsistent KGs. Xue and Zou (2022) provide a survey on the broader field of quality management in KGs and Paulheim (2016) provides a survey of approaches that infer missing knowledge (KG completion) or identify erroneous assertions (error detection). In general, many works that focus on KG completion and error detection aim at improving the KG quality and some of them employ reasoning to add missing information or to detect errors. However, quite often they do not lead to a fully complete, error-free, and globally consistent KG. Conversely, our focus here is on formal reasoning in inconsistent KGs. In this context, some of the above approaches can be proven useful to move towards a consistent KG, for example just by removing the errors that exist; but they are not necessarily enough to achieve consistency, where inconsistency detection and fixing methods come into play. Lambrix (2023) provide a recent survey on the debugging and completion of ontologies, focusing on the task of repairing the TBox of a KB. We, on the other hand, pay more focus on repairing the ABox, which is much larger and error-prone than TBox for several real-world KGs, such as Bpedia (Auer et al., 2007) and YAGO (Pellissier Tanon et al., 2020). In addition, for cases when KG consistency is not achievable, we also consider inconsistency-tolerant reasoning approaches (Bienvenu, 2020). Alternative types of non-classical KG reasoning have been proposed, such as reasoning based on distributed representations and Neural Networks (Hohenecker & Lukasiewicz, 2020; Chen, Jia, & Xiang, 2020), which do not necessarily require consistency and lie beyond the scope of this survey.\nThe remainder of the article is organized as follows: First, we introduce some basic notions and terms about inconsistent KGs and provide some examples in Section 2. Then we focus on checking whether a KG is consistent or not, and what parts of it are involved in an inconsistency. This is a challenging task, in particular for large-scale KG, as we discuss in Section 3. In Section 4 we investigate the fixing of an inconsistent KG, by altering its inconsistent parts, aiming for a consistent version on which reasoning is applicable. However, altering the KG to achieve consistency may not always be possible. In this direction, in Section 5 we consider paraconsistent approaches that focus on tolerating the presence of inconsistencies during the reasoning process. Finally, Section 6 presents a summary as well as the open challenges."}, {"title": "2. Inconsistent Knowledge Graphs", "content": "AKG K = TUA comprises a set of assertions A called the ABox which expresses the extensional knowledge of the graph, and a set of axioms T called the TBox which is the intensional knowledge of the graph. An assertion is a triple (s1, l, s2) representing a labeled edge of the KG between nodes s\u2081 and 82 with label l. We will say that s\u2081 is the subject,\ns2 is the object and I is the predicate of the triple. The TBox is expressed in a language\nL that is a fragment of first-order logic. In particular, it is usual to express the TBox in a Description Logic (DL). DLs is a family of logics that are less expressive than first-order logic and are used to model concepts (or classes), roles (or relations) and their relationships. Concepts are used to model sets of nodes of the knowledge graph and roles model sets of edges and therefore correspond to unary and binary predicates of first-order logic, respectively. In DLs, axioms are logical statements relating concepts and roles. The form of the axioms depends on the specific DL, and as a result different DLs have different expressive power. The most basic language is AL (Attributive Language), and can be expanded e.g., with C for complements to form ALC, or SR for property chains, characteristics, and role hierarchies, O for nominals, I for inverse properties, and Q for qualified cardinality constraints to form the more expressive SROIQ language (Baader, Calvanese, McGuiness, & Nardi, 2007).\nExample 1. Consider the knowledge graph depicted in Figure 1. The graph consists of nodes like Bob and Robby. An edge labeled belongsTo connects Bob to Robby. Similarly, an edge labeled hasName connects Robby and 2000. The nodes Person and Robot also appear in the graph and represent classes. The node Bob is linked to Person, indicating that Bob is an instance of Person.\nThe TBox of the KG defines relationships between the classes. For example, the axiom Person Equipment I expresses that Person and Equipment are disjoint classes, and the axiom Robot Equipment states that Robot is a subclass of Equipment. Additionally, the relation belongsTo has domain and range restrictions for the classes Equipment and Person, respectively. These restrictions are expressed by the axioms Person = belongTo.T and Equipment = \u2200belongTo\u00af\u00b9.T. Data such as 2000 are treated as nodes in the knowledge graph. Data types are usually defined as classes that contain the corresponding values. For example, the node 2000 is an instance of Integer and Integer is disjoint with String.\nA crucial feature of a KG is the ability to reason about the knowledge it represents. We consider the usual definition of an interpretation of first-order logical statements. Given a domain of individuals D, an interpretation assigns a meaning to the non-logical symbols of the KG. For example, in the case of a KG expressed in a DL, an interpretation assigns to each concept Ca subset CD of D and to each role R a subset RD of D \u00d7 D. An interpretation is a model of K if and only if it satisfies all axioms and assertions in K. If K admits a model we will say that K is satisfiable. We will say that K is unsatisfiable if and only if no model exists. We also use the term consistent if and only if K is satisfiable and the term inconsistent otherwise.\nDefinition 1 (Entailment). A KG K entails a formula \u00a2 (K = $), if and only if every model of K is also a model of 6.\nIf a KG is inconsistent then any formula is entailed. In other words, we cannot distinguish between true and false assertions, and therefore reasoning is useless.\nExample 2. Consider again the KG in Figure 1 and assume that due to some error in knowledge extraction, an assertion is generated stating that Bob belongsTo Robby. This leads to the following inconsistencies:\n(i) Bob \u2208 Person, while the domain of belongsTo should be Robot and Robot is disjoint with Person;\n(ii) Robby \u2208 Robot and Robot Equipment, which is disjoint with Person that is defined as the range of belongsTo.\nMoreover, another TBox axiom may restrict a data property hasName to hold only values of type xsd:string. If the ABox contained, e.g., that Robby hasName \"2000\"^^xsd:int, then this would be the cause of an additional inconsistency, since xsd:string and xsd:int are different data types.\nThere can be several reasons for an inconsistent KG. The TBox may be problematic regardless of the ABox. In this case, the set of axioms are ill-formed and produce contradictory conclusions. Furthermore, the ABox may be inconsistent regardless of the TBox. Finally, both ABox and TBox may be consistent by themselves but inconsistent if combined. Inconsistencies may be a result of ill-defined TBoxes, or when definitions from multiple sources are mixed, e.g., in cases of knowledge migration or KG merging (Huang, van Harmelen, & ten Teije, 2005).\nDefinition 2 (Conflict set, Explanation). A conflict set is a subset S\u2286 K such that S is inconsistent. If S is minimal then S is an inconsistency explanation.\nIn order to re-establish consistency in a KG we need to alter the TBox, the ABox or both. In general, we can define a distance function A over pairs of KGs and a partially order set (Q, \u2264) that quantifies the degree of alteration among different KGs. Given two KGs K and K', \u0394(K, K') maps to an element of Q. Given KGs K, K\u2081 and K2 we will say that K\u2081 is at least as close as K2 is to K, if and only if \u2206(K, K\u2081) \u2264 \u2206(K, K2). Also, we will say that K\u2081 is A-minimally altered from K if and only if there is no KG K' such that\n\u25b3(K, K') \u2264 \u2206(\u039a, \u039a1)."}, {"title": "3. Inconsistency Detection", "content": "To detect inconsistencies, i.e. to come up with the inconsistency explanations (cf. Def. 2), we could rely on exact or approximate approaches. Exact methods would generate a set of explanations, which e.g., in the case of the example of Figure 1, would be the following:\nExplanation 1. (1) Equipment DisjointWith Person, (2) belongsTo Domain Equipment,\n(3) Bob belongsTo Robby, (4) Bob Type Person.\nExplanation 2. (1) Equipment DisjointWith Person, (2) Robot SubClassOf Equipment,\n(3) belongs To Range Person, (4) Robby Type Robot, (5) Bob belongsTo Robby.\nExplanation 3. (1) hasName Range: xsd:string\n(2) Robby hasName \"2000\"^^xsd:int.\nConsider the case of Explanation 1 where the size of the conflict set S is four. If we removed any of the triples, resulting to an S' \u2286 S, then no inconsistency would be present. If we removed (1), due to the open world assumption (Baader et al., 2007) under which most DL reasoners operate, there would be no conflict with Bob being a Person; if we removed (2), then there would be no restriction for (3), and so on.\nTable 1 outlines the most relevant methods for KG inconsistency detection, which we now describe. Horridge et al. (2009) propose sound and complete methods which exhaus-tively search for explanations by recursively employing the hitting set tree algorithm (Reiter, 1987) on subsets of the initial KG. Unfortunately, such approaches do not scale to large KGs. To address this issue, modularization techniques may be adopted. Modularization has been introduced in the scope of reusing and extending ontologies, and aim to maintain only the triples that are effectively used for the inference of new knowledge, this way reducing the size of the utilized KG. For example, Suntisrivaraporn et al. (2008) extract locality-based modules and employ hitting set trees to discover explanations of SHOIQ entailments, though this approach does not focus solely in detecting inconsistencies.\nTran et al. (2020) decompose KGs into modules of individuals, i.e., subsets of triples of the original KG that include a reference to a particular individual. Each module along with the TBox\u2014generally being of significantly smaller size than the original large KG\u2014is analyzed independently and in parallel to obtain the inconsistency explanations. Considering the example of Figure 1, the module of Bob would include the TBox and two ABox assertions, Bob \u2208 Person and Bob belongsTo Robby, thus allowing the detection of the inconsistency of Explanation 1. Similarly, the module of Robby will contain all ABox assertions apart from Bob \u2208 Person, thus allowing the detection of Explanations 2 and 3.\nTran et al. employ an additional step that groups explanations into abstractions, which describe high-level types of inconsistency. This way, we may present to the human KG developer a smaller number of abstractions, instead of a larger number of explanations that may be overwhelming. Although scalable, this approach works on DL-Lite+ eboot, which is only a fragment of OWL 2 (Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009). For instance in Example 1, if instead of the Bob belongsTo Robby assertion we had Bobby belongs To Robby and Bob sameAs Bobby, then Explanation 1 would also include (5) Bob sameAs Bobby, but would nevertheless not be detected at any module of the corresponding individuals by Tran et al. (2020); the module of Bob would not contain Bobby belongsTo Robby, and the module of Bobby would not contain Bob Type Person.\nAnother way to achieve tractability is to reduce the expressiveness of the supported language. Towards this, Meilicke et al. (2017) utilize clash queries for DL-Litea that allow the detection of inconsistencies that lie beyond the expressivity restrictions of this language for a given TBox and a sequence of different ABoxes. Clash queries are pre-compiled combinations of classes, relationships and attributes that are capable of inducing inconsistency in DL-Litea KGs. Consider Example 1 again, on which we could create an appropriate query to retrieve the triples that satisfy T = p(U) \u2286 T and U(a,v) \u2208 A and v & T\u00b9, where p(U) is an attribute range of values, T a value domain, U(\u03b1,\u03c5) the assignment of an attribute value v for an individual a, and v\u00b9 and T\u00b9 interpretation functions. Then, the result would be Explanation 3. Different types of inconsistencies can be captured by other clash patterns, similarly to the above. Likewise in Meilicke et al. (2017), with the help of a classical reasoner, the signature elements that are entailed by a clash pattern are matched to those that induce inconsistency.\u00b9 This task is performed incrementally as more ABoxes are examined, and for signature element combinations that have already been recognized as inconsistent, reasoning is skipped. Thus, the amount of computationally expensive invocations of the reasoner decreases over time.\nThe other category of approaches that achieve scalability generally sacrifice completeness, and thus rely on approximate reasoning. For instance, an approximate approach that is similar to Tran et al. (2020) that we already described, is that of de Groot et al. (2021). The difference, however, is that instead of starting from the individuals to extract their modules, this work extracts subgraphs, based on all subjects that are included in the ABox assertions. Each such subject constitutes the root node for the construction of each respective subgraph. Then, according to breadth-first search, all the triples with a subject that corresponds to the root node are included. Next, the objects and predicates are expanded accordingly, up to a maximum number Gmax defined by the user. Gmax tunes the tradeoff between completeness and scalability. Then, for each subgraph an off-the-shelf reasoner is invoked to obtain inconsistency explanations, which are finally grouped into the so-called \"anti-patterns\" by substituting subjects, objects and, in some cases, predicates as well, with variables. Intuitively, an anti-pattern is a smaller, inconsistent subgraph with variables instead of values that serves as an abstraction template for categorizing the inconsistency types that are discovered in the KG. The concept of Gmax could also be incorporated in the approach of Tran et al. (2020) for combining modules of individuals up to a number of 'hops', and thus cover a more expressive language fragment. Another difference with that work is that the root node of the subgraph can be any entity, instead of merely individuals.\nPaulheim and Stuckenschmidt (2016) present an approximate method for predicting if a set of ABoxes are inconsistent with respect to a TBox. This is performed by formulating the problem as binary classification, i.e., trying to determine whether a given ABox is consistent or not, judging by a set of feature values. The ABoxes are translated into feature vectors using path kernels, and then off-the-shelf classification algorithms are applied. According to the empirical analysis, this method can scale even for very large KGs. Moreover, key factors affecting performance are the feature translation and classification, while accuracy is shown to reach up to 96% of correct class label predictions for some dataset cases. Paulheim and Bizer (2014) consider the statistical distributions of types in the subject and object position linked with a property in an assertion, to calculate a confidence score and highlight those surpassing a given threshold as erroneous.\nSenaratne (2023) proposes an approximate method for detecting anomalous\u2014and thus, possibly erroneous\u2014assertions in KGs, which relies on unsupervised learning for anomaly detection. Following the Corroborative Path algorithm that is a variant of the Path Rank algorithm (Lao & Cohen, 2010), this method identifies alternative paths between entities in a triple, i.e., the existence or not of other property relations (or chains of such) between the subject and the object. This way, the method constructs a binary feature matrix with the entities as rows, and the properties (or property chains) as columns. The KG entities are also assessed regarding their correctness by considering other kinds of features, such as graph structure characteristics, frequency of appearance, etc. Finally, a one-class support vector machine identifies the anomalous parts, which subsequently should be removed or corrected.\nOther approaches rely on neural networks as in Chen et al. (2021). TBox axioms and ABox assertions are transformed to embeddings, namely vectors with numerical values that result from TransE (Bordes, Usunier, Garcia-Duran, Weston, & Yakhnenko, 2013) transformations of the original KG data. The values of the vector coefficients are such that their distance indicates the degree of relationship between the original data (Ji, Pan, Cambria, Marttinen, & Yu, 2022). After the KG transformation to embeddings, a fitness score is assigned to each element according to its degree of expectancy according to the transformation mechanism. Also, additional attention coefficients are taken into account i.e., according to if the subject satisfies different types of axioms, such as domain, range, disjointness, irreflexive, and asymmetric axioms. All these scores are combined into a cross-entropy loss function which is minimized for training the neural network. However, such methods are not guaranteed to detect every possible inconsistency in a KG.\nThere are also relevant approaches from other domains, such the QuickXplain algorithm and its variants (Junker, 2004; Shchekotykhin, Friedrich, Rodler, & Fleiss, 2014) that seek to explain constraint violations in constraint satisfaction problems by dividing and pruning the initial input graphs. However, since these mainly rely on setting preference orderings on the constraints\u2014a procedure that is not straightforward for real-world large KGs\u2014we consider them out of the scope of our survey. Overall, there are multiple algorithms that can detect inconsistencies in KGs, which are based on different principles and aim to strike a balance between completeness and scalability. Having analyzed their functionality, we now proceed with the next step on how to deal with reasoning over inconsistent KGs i.e., to fix or tolerate the inconsistencies."}, {"title": "4. KG Fixing", "content": "We focus on the task of 'fixing' an inconsistent KG, i.e. selecting and applying a set of actions that change the KG in order to resolve any inconsistency and lead to a repair, as defined in Definition 3.\nDefinition 4 (fix). A fix F is a set of alterations on K that lead to a A-repair R.\nIn the example of Figure 1, a potential fix could be, for instance, to update the ABox as presented in the fix of Example 3:\nExample 3. Add Robby belongs To Bob in place of Bob belongs To Robby, and add Robby hasName \"2000\"^^xsd:string in place of Robby hasName \"2000\"^^xsd:int.\nIn addition to the above, there are alternative fixes that lead to a consistent KG, such as removing the assertions involved in the inconsistency from the ABox and/or the TBox. In the remaining part of this section, we provide an overview of how related works address or can be used to address the KG fixing task, categorized according to the part of the KG that is considered for updating.\n4.1 Unreliable TBox\nAn inconsistency in a KG can be the result of an ill-defined TBox. For instance, adding that class Robot is a subclass of Person (Robot C Person) in the TBox of the example of Figure 2 would lead to an inconsistent KG. This is because the class Robot is also a subclass of Equipment which is disjoint with Person, hence it contradicts the assertion that Robby isA robot. In such cases, fixing the KG should be achieved by changing the TBox to render it free of such contradictions. A reliable source for such resolution of inconsistencies is domain experts. However, humans need to be guided throughout the elements of the TBox that are problematic, given that the number of such elements is manageable.\nSeveral alternative methods are available in this direction, aiming to minimize the input required by the domain experts in order to resolve the faults in the ontology, make it more complete, or revise it (Lambrix, 2023; Qi & Yang, 2008). In this direction, some methods rank the ontology elements based on the number of inconsistencies they are involved in (Heyvaert et al., 2019), belief-revision approaches considering the number of other ax-ioms that can be automatically evaluated upon the evaluation of each axiom (Nikitina et al., 2012), or considering parts of the ontology that can be trusted in combination with conse-quences of alternative update actions (Pe\u00f1aloza, 2019). Other methods develop query-based fault-localization strategies that rely on active learning (Rodler, 2016), axiom likelihood es-timation (Shchekotykhin et al., 2012), or consider different query-answering behaviors by the experts (Rodler, 2022). Finally, a tool, named OntoDebug (Schekotihin, Rodler, & Schmid, 2018), is also available for guiding experts to resolve errors in ontologies, as a plug-in for the popular open-source ontology editor Prot\u00e9g\u00e9 (Musen, 2015).\nIn the more general case, inconsistency is caused by errors in any of the axioms involved and the fix may require altering the TBox, the ABox, or both. For example, instead of updating the value \"2000\"^^xsd:int into \"2000\"^^xsd:string to comply with the T\u0432\u043e\u0445, as suggested in the fix of Example 3, one could remove the restriction itself. This depends on whether an integer should be acceptable as an equipment name or not. Apart from removing elements from the TBox, adding and/or altering elements may also be possible. A method towards this direction was proposed by T\u00f6pper et al. (2012), where the TBox may be updated with new restrictions during the KG development process to detect as many inconsistencies as possible, identifying errors. During this process, altering the elements of both the ABox and the TBox is supported.\n4.2 Reliable TBox\nAlthough errors in the TBox are possible, a lot of research focuses on inconsistencies caused by erroneous ABox assertions. In such cases, the TBox is considered reliable and the fix consists of updating the ABox, as performed in the fix of Example 3. In many KGs, such as DBpedia (Auer et al., 2007) and YAGO (Pellissier Tanon et al., 2020), this is a reasonable scenario as the ABox can often be very large rendering its manual development or validation less feasible than that of the TBox, which can be typically smaller. A discussion on this topic is provided by Paulheim (2016). In the remainder of this section, we present some methods for fixing a KG with a reliable TBox. We focus on how alternative fixes are generated and chosen, and what types of constraints are considered, as summarized in Table 2.\n4.2.1 GENERATING FIXES\nA fix restores consistency in a KG, i.e. leads to a repair, as defined in Definition 4. As regards ABox fixing, most works focus on the deletion of some ABox assertions (Bonatti et al., 2011; Melo & Paulheim, 2020; Baader et al., 2022). KG fixing through both deletion and addition of assertions has also been investigated, for example in the context of satisfying constraints on minimum cardinality (Pellissier Tanon et al., 2019; Ahmetaj et al., 2022). Adding assertions combined with deletions can also support the special case of \u201cupdate-based repairing\" (Melo & Paulheim, 2020; Arnaout et al., 2022). Contrary to \"deletion-based\" repairing, in update-based approaches, only parts of an assertion are altered in an effort to retain as much original information as possible. In the example of Figure 1 for instance, an update-based fix could include changing the subject of the Bob belongs To Robby assertion into Robby, and the object into Bob, as presented in the fix of Example 3, retaining the belongsTo relation instead of removing it from the KG. In this regard, different alternative fixes may be available and the automated calculation of such alternative fixes is a first goal towards fixing. In the fix of Example 3 for instance, any Person could be the subject of the assertion without raising inconsistency, not only Bob. Some important challenges in this task include the identification of alternative fixes that can indeed lead to consistency, as well as the efficiency of this process, as the number of alternatives to be considered as potential fixes can be large.\nSeveral methods in the data-cleaning literature rely on variations of the chase algo-rithm (Benedikt et al., 2017) for reasoning which allows them to generate and apply update actions during the reasoning process, and develop a fix incrementally, without exhaustive consideration of all potential combinations of update actions (Geerts et al., 2020). Though such methods are usually not designed particularly for KGs, their application to KGs is often feasible, provided that an adequate transformation between the adopted formalizations is feasible. That is, the KG should be fully expressed in the language that each method supports. Arioua and Bonifati (2018), for example, consider a TBox consisting of specific types of relational database constraints, namely Tuple-generating and conflict detection de-pendencies, which are less expressive than OWL. In addition, graph-specific methods have also been proposed for the data-fixing task in the context of the graph-database literature. For example, Gfix defines the notion of graph quality rules and introduces a graph-specific variation of the chase algorithm which is used for deducing certain fixes on an inconsistent KG (Fan et al., 2019).\n4.2.2 CHOOSING AMONG FIXES\nWhen alternative fixes are available, the choice of which of them to adopt is the other goal of KG fixing. Some works rely exclusively and directly on human user input for such choices (Arioua & Bonifati, 2018) but others attempt to automate this process. For instance, this can be done by employing the idea of active integrity constraints in the TBox, which is extended to include policies for consistency restoring, as overviewed by Feuillade et al. (2020). Another way to automatically choose among fixes is by checking which parts of the ABox are more reliable, based on given reliability levels. Fan et al. (2019) for example, con-sider a part of the ABox as ground truth and incrementally resolve potential inconsistencies with fixes that can be entailed by this part, consulting human users only in cases where such a fix is not available. In the example of Figure 1 for instance, even if the assertion Robby belongs To Bob were part or could be entailed by a ground truth, still user input would be re-quired for choosing among alternative fixes for updating Robby hasName \"2000\"^^xsd:int, such as replacing the object with \"2000\"^^xsd:string or \"Robby\"^^xsd:string. Other works consider several reliability levels that prioritize different parts of the ABox, which may reflect, for example, the quality of their original resources. In this context, the fix that leads to a preferred repair can be chosen automatically, as the one that removes assertions of lower reliability level. To make the computation of such repairs feasible, Bonatti et al. (2011) consider the O2R\u00af subset of rules in the Rule Language profile of OWL 2 (OWL 2 RL2), which is selected to ensure linear growth of assertional inferences with regard to the ABox size. Benferhat et al. (2015a) focus on prioritized DL-Lite KBs (Calvanese et al., 2007).\nAnother way of choosing among alternative fixes is based on the minimality of update actions in a fix. Ahmetaj et al. (2022), for instance, inspired by related work on database repairing, propose a method that identifies a minimal set of update actions on an RDF KG to conform to a set of constraints, expressed in the Shapes Constraint Language\u00b3 (SHACL). This conformation can be full, or in cases that this is not possible, it can be maximal, fixing as many assertions as possible. In a similar point of view, Baader et al. (2022) focus on EL4 KBs with inconsistency that involves some entailed information and rely on the minimality of entailment loss. That is, they choose the fixes that lead to the so-called \"optimal repairs\", where no conflicting information is entailed anymore, but the non-conflicting entailments are retained as much as possible.\nMachine Learning (ML) models have also been applied for KG fixing, mostly in the context of KG completion and error detection. Consistency checking is employed by some methods, mainly considering automatically extracted constraints as a means towards KG completion and error detection. But leading to a consistent KG considering a pre-existing TBox is often not guaranteed. Pellissier Tanon et al. (2019, 2021) employ supervised ML approaches to learn specific correction rules from the history of resolving previous errors in an RDF KG by human experts. However, most approaches focus on unsupervised ML and learn to identify erroneous parts of the ABox based on frequent patterns. Paulheim and Bizer (2014), for example, consider type and property distributions in RDF KGs to identify missing types of entities and wrong property assertions that link entities of unexpected types. Cheng et al. (2020) propose mining rules that can capture and fix different types of errors considering graph patterns as well, by introducing Graph Repairing Rules and Flexible Graph Repairing Rules.\nMelo and Paulheim (2020) mine patterns, by developing a decision tree classifier, and then translate them into SHACL constraints. They propose the development of decision-tree models that rely on path and type features on a KG to score the relation assertions as potentially erroneous, i.e., as candidates for removal or correction. The correction is performed by updating the type of some entity with a type estimated by another ML model, or by replacing the entity with a \"similar\" one. The similarity between entities is estimated based on the Wikipedia disambiguation links and name string similarities. The resulting relation assertions are ranked according to the confidence of the ML model for them not being erroneous. In the same direction, other methods rely on neural networks to estimate the correctness of specific assertions for fixing KGs, particularly neural embedding representations. Abedini et al. (2022), for instance, propose a method that employs KG embedding approaches to learn a model for estimating whether an assertion is true or not. Based on these estimations, they rank the alternative assertions for unique RDF relations, where an entity is related at most to one other entity. More recently, Ye et al. (2023) combine both KG embedding representations and first-order logical rules with confidence weights for the same task.\nFinally, beyond direct human-user input and patterns extracted from the KG itself, external knowledge sources (EK) have also been considered. Melo and Paulheim (2020), for instance, exploit the Wikipedia disambiguation pages to estimate entity similarity, as already discussed above. Gad-Elrab et al. (2019) exploit textual web resources to collect supporting evidence for specific assertions and provide it to the human KG curators, based on rules expressed as Horn clauses, and Arnaout et al. (2022) propose the use of probes on pretrained language models for choosing the right entity to replace the object of an erroneous relation assertion.\nAnalytical approaches usually rely on specific logic formalizations and optimization strategies regarding the fixes generated. However, for choosing among alternative fixes they often require direct human user input or pre-defined priority/reliability levels which may not always be available in practice. ML-based approaches, on the other hand, usually rely on unsupervised techniques that exploit the KG itself without the need for special input. However, such approaches typically focus on the task of minimizing erroneous assertions, not necessarily leading to a consistent KG. The combination of the above directions is therefore very promising for the efficient fixing of inconsistent KGs. Still, for cases where choosing among the alternative fixes is not feasible or desired, there are approaches for reasoning while tolerating inconsistency, as discussed in the section that follows.\""}, {"title": "5. Inconsistency-Tolerant Reasoning", "content": "Inconsistent-tolerant reasoning emerges as a non-standard reasoning that refrains from fix-ing contradictions within the knowledge graph. Several approaches had defined different semantics that can tolerate inconsistent KGs. The approaches are summarized in Table 3. While our focus is in reasoning on KGs, the concept of inconsistency-tolerant reasoning is broader and has been applied in various other formalisms.\n5.1 Repair-based Reasoning\nA prominent concept involves performing reasoning as if the knowledge graph is repaired. In other words, the reasoning is happening in a repair of the knowledge graph. The notion of repair-based reasoning has been initially introduced by Arenas et al. (1999) for query answering over relational databases (actually, they called their approach consistent query answering). This idea was later adapted by Lembo et al. (2010) for ontological reason-ing, leading to the development of the so-called AR semantics for various DLs where AR stands for ABox repair. Most of the repair-based reasoning approaches operate under the assumption that the TBox is reliable and the only possible kind of repair is an"}]}