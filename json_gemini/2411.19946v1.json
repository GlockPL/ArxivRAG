{"title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation", "authors": ["Zhiqiang Shen", "Ammar Sherif", "Zeyuan Yin", "Shitong Shao"], "abstract": "Recent advances in dataset distillation have led to solutions in two main directions. The conventional batch-to-batch matching mechanism is ideal for small-scale datasets and includes bi-level optimization methods on models and syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution matching, gradient matching, and weight trajectory matching. Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets. This approach has garnered substantial interest within the community, as seen in SRe\u00b2L, G-VBSM, WMDD, and CDA. A primary challenge with the second approach is the lack of diversity among syntheses within each class since samples are optimized independently and the same global supervision signals are reused across different synthetic images. In this study, we propose a new Diversity-driven EarlyLate Training (DELT) scheme to enhance the diversity of images in batch-to-global matching with less computation. Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process. These distilled images from the subtasks demonstrate effective generalization when applied to the entire task. We conduct extensive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the previous state-of-the-art by 2~5% on average across different datasets and IPCs (images per class), increasing diversity per class by more than 5% while reducing synthesis time by up to 39.3% for enhancing the training efficiency.", "sections": [{"title": "1. Introduction", "content": "In the era of large models and large datasets, dataset distillation has emerged as a crucial strategy to enhance training efficiency and make advanced technologies more accessible and affordable for the general public. Previous approaches [3, 4, 6, 14, 18, 21, 36, 42, 43, 47] primarily employ a batch-to-batch matching technique, where information like features, gradients, and trajectories from a local original data batch are used to supervise and train a corresponding batch of generated data. The strength of this method lies in its ability to capture fine-grained information from the original data, as each batch's supervision signals vary. However, the downside is the necessity to repeatedly input both original and generated data for each training iteration, which significantly increases memory usage and computational costs. Recently, a new decoupled method [19, 39, 40] has been proposed to separate the model training and data synthesis, also it leverages the batch-to-global matching to avoid inputting original data during distilled data generation. This solution has demonstrated great advantage on large-scale datasets like ImageNet-1K [28, 40] and ImageNet-21K [39]. However, as shown in Fig. 2, a significant limitation of this approach is the lack of diversity caused by the mechanism of synthesizing each data point individually, where supervision is repetitively applied across various synthetic images. For instance, SRe2L [40] utilizes globally-counted layer-wise"}, {"title": "2. Related Work", "content": "Dataset Distillation. Dataset distillation or condensation [36] focuses on creating a compact yet representative subset from a large original dataset. This enables more efficient model training while maintaining the ability to evaluate on the original test data distribution and achieve satisfactory performance. Previous works [3, 4, 6, 14, 18, 21, 36, 42, 43, 47] mainly designed how to better match the distribution between original data and generated data in a batch-to-batch manner, such as the distribution of fea-"}, {"title": "3. Approach", "content": "Preliminaries. The objective of a regular dataset distillation task is to generate a compact synthetic dataset $S = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\hat{\\mathbf{x}}_{|S|}, \\hat{y}_{|S|})\\}$ as a student dataset that captures a substantial amount of the information from a larger labeled dataset $T = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_{|T|}, y_{|T|})\\}$, which serves as the teacher dataset. Here, $\\hat{y}$ represents the soft label for the synthetic sample $\\hat{x}$, and the size of $S$ is much smaller than $T$, yet it retains the essential information of the original dataset $T$. The learning goal using this distilled data is to train a post-validation model with parameters $\\theta$:\n$$\\theta_S = \\underset{\\theta}{\\operatorname{arg \\min}} L_S(\\theta),$$\n$$L_S(\\theta) = \\mathbb{E}_{(\\hat{x}, \\hat{y}) \\in S} [l(f_{\\theta}(\\hat{x}), \\hat{y}; \\theta)],$$\nwhere $l$ is a standard loss function such as soft cross-entropy and $f_{\\theta}$ represents the model.\nThe primary aim of dataset distillation is to produce synthetic data that ensures minimal performance difference between models trained on the synthetic dataset $S$ and those trained on the original dataset $T$ using validation data $V$. The optimization procedure for generating $S$ is given by:\n$$\\underset{S, \\hat{S}}{\\operatorname{arg \\min}} \\{ \\sup_{l(\\mathbf{x}_{val}, y_{val})} - l(f_{\\theta_\\hat{S}}(\\mathbf{x}_{val}), y_{val}))\\}  (\\mathbf{x}_{val}, y_{val}) \\sim V.$$\nwhere $(\\mathbf{x}_{val}, y_{val})$ are the sample and label pairs in the validation set of the real dataset $T$. The learning task then focuses on the pairs within $S, maintaining a balanced representation of distilled data across each class.\nInitialization. Previous dataset distillation methods [28, 39, 40] on large-scale datasets like ImageNet-1K and 21K employ Gaussian noise by default for data initialization in the synthesis phase. However, Gaussian noise is random and lacks any semantic information. Intuitively, using real images provide a more meaningful and structured starting point, and this structured start can lead to quicker convergence during optimization because the initial data already contains useful features and patterns that are closer to the target distribution, which further enhances realism, quality, and generalization of the synthesized images. As shown in Fig. 2 right subfigure, our generated images exhibit both diversity and a high degree of realism in some cases."}, {"title": "4. Computational Analysis", "content": "For image optimization-based methods like SRe2L and CDA, the total computational cost is calculated as $N \\times T$, where $N$ is the MI. In our scheme, the first batch images undergo $T_1$ iterations (where $T_1 = T$). Subsequent batches are processed with progressively fewer iterations, such as $T_2 (T_2 = T_1 - RI)$ for the next set, and so forth. The iterations for the final batch are reduced to $RI$ which is 1/j of the standard count (where $j = 4$ or 8 in our ablation), the total number of our optimization iterations required is $N \\times T - \\frac{1}{j*j}RI$, which is roughly 2/3 of prior batch-to-global matching methods. Our real time consumptions for data generation are shown in Table 6, note that the smaller the dataset like CIFAR, the more time is spent on loading and processing the data, rather than training."}, {"title": "5. Conclusion", "content": "We have introduced a new training strategy, $EarlyLate$, to improve image diversity in batch-to-global matching scenarios for dataset distillation. The proposed approach organizes predefined IPC samples into smaller, manageable subtasks and utilizes local optimizations. This strategy helps in refining each subset into distributions characteristic of different phases, thereby mitigating the homogeneity typically caused by a singular optimization process. The images refined through this method exhibit robust generalization across the entire task. We have extensively evaluated this approach on CIFAR, Tiny-ImageNet, ImageNet-1K, and its variants. Our empirical findings indicate that our approach significantly outperforms prior state-of-the-art methods across various IPC configurations."}]}