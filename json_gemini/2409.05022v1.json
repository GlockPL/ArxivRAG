{"title": "Sequential Recommendation via Adaptive Robust Attention with Multi-dimensional Embeddings", "authors": ["Linsey Pang", "Amir Hossein Raffiee", "Wei Liu", "Keld Lundgaard"], "abstract": "Sequential recommendation models have achieved state-of-the-art performance using self-attention mechanism. It has since been found that moving beyond only using item ID and positional embeddings leads to a significant accuracy boost when predicting the next item. In recent literature, it was reported that a multi-dimensional kernel embedding with temporal contextual kernels to capture users' diverse behavioural patterns results in a substantial performance improvement. In this study, we further improve the sequential recommender model's robustness and generalization by introducing a mix-attention mechanism with a layer-wise noise injection (LNI) regularization. We refer to our proposed model as adaptive robust sequential recommendation framework (ADRRec), and demonstrate through extensive experiments that our model outperforms existing self-attention architectures.", "sections": [{"title": "1 INTRODUCTION", "content": "Sequential recommendation plays a pivotal role in enhancing user experience and engagement in various e-commerce and social media platforms. Unlike traditional recommendations that focus solely on static preferences, sequential recommendations leverage the sequential nature of user interactions to provide personalized recommendations over time. By analyzing users' historical behaviors and interactions in chronological order, it is easy to effectively capture temporal dynamics, user preferences, and evolving interests. Sequential recommendation algorithms employ techniques such as recurrent neural networks, convolutional neural networks, and self-attention mechanisms to model sequential patterns and predict users' future actions [8, 11]. Recently, several variants of attention-based sequential recommendation models have emerged to enhance the performance of item ID-based models (i.e. SASRec) [11]. For example, BERT4Rec [16] adapts bidirectional self-attention from BERT via masked language modeling. TiSASRec [12] models user interaction sequences with time intervals, and MEANTIME [2] incorporates a mixture of attention mechanisms with multi-temporal embeddings, etc. While these models demonstrate improved performance, they often focus on specific aspects of user interaction sequence modeling, such as time-based or context-based features, attention architecture design, or robustness through techniques like data augmentation or denoising [1, 13], etc. To capture the complexity of the user behavior and provide highly personalized recommendations, it is important to encode each user's unique behavior and apply customized attention mechanisms to learn short-term and long-term user interest as well.\nIn this work, we aim to capture unique sequential patterns of user behavior and prioritize the robustness and generalization of model as well. Our key contributions are: (1) Our proposed method incorporates multi-dimensional kernels for item representation and user behaviors into the attention mechanism; (2) Instead of using traditional multi-head attention, we utilized absolute and relative mixture attention mechanism to learn unique patterns from different user behavior; (3) NIR (noise injection regularization) is applied as a layer-wise regularizer to enhance the robustness and generalization; (4) The experiments conducted on four popular datasets show our proposed model outperforms the baseline recommenders."}, {"title": "2 PRELIMINARIES", "content": "In this section, we provide preliminaries and related work.\nMix-Attention Mechanism: The multi-head attention plays a crucial role in sequential recommendations to effectively capture long-range dependencies and dynamic relationships within user interaction sequences [11]. Mix-Attention mechanism proposed in MEANTIME [2] is an extension of the classical multi-head attention mechanism [11]. Instead of keeping each head processing the split input embedding from the global input matrices, mix-head attention process information of the queries Q, keys K, and values V for each head from different embedding input scheme (globally or locally).\nMulti-dimensional Embeddings: In sequential recommendation, effectively modeling user interactions involves using absolute or relative time-series kernels, absolute or relative positional kernels [2, 7, 14]. For instance, absolute time stamps, such as day-of-week and seasonality embeddings, identify patterns in user behavior (Figure 1a and b). Additionally, embeddings of relative time intervals reveal how preferences change over different timescales (Figure 1c). Furthermore, relative positional-related embedding within a user sequence capture relative positions and transitions, aiding in understanding the progression of user interests.\nRobust Regularizer and Exploration: To enhance robustness and generalization in transformer related tasks, some works have focused on designing lightweight transformer architectures [6]. Others involve using learnable attention distributions [1, 3]. One more line of approach is adopting noise injection regularization (NIR) schemes, for instance, layer-wise noise stability regularization (LNSR) [10] is introduced in an unsupervised manner and provides performance benefits."}, {"title": "3 OUR LEARNING FRAMEWORK", "content": "In this section, we introduce our adaptive robust sequential recommendation (ADRRec) model."}, {"title": "3.1 Problem Formulation", "content": "Let U be a set of users, and V a set of items. For each user u \u2208 U, we have a sequence of item Ids Vu = [Vu1,..., Vuk\u203a\u00b7\u00b7\u00b7, UV] that the user previously interacted with in chronological order and the corresponding time sequence of the interaction Tu = [tu1, ..., tuk,\u2026\u2026\u2026, t|V|] that stores the absolute timestamp values. Our goal is to predict the next item vunext that the user u is likely to interact with at the target timestamp tunext based on the given history (Vu, Tu)."}, {"title": "3.2 Input Representation", "content": "Here we describe various types of embeddings leveraged for modeling sequential user behavior.\nAbsolute Time Embedding: There are two different approaches for computing absolute time embedding, one is embedding-based, and the other one is projection-based [14, 18]. For embedding-based, each timestamp t is decomposed into multiple components and each component ti(i \u2208 (1,.., k)) representing different time unit. For instance, ti can represent one of these units: year, month, day, or minute etc. Each ti is employed by a learnable embedding Et, e Rd. The multi-dimensional embedding can be given by: \u03c6(t) = Concat[w1Et\u2081 +b1, W2Et2 + b2, ..., wiEt; + bi], where {wi}=1 and {bi}d are learnable parameters. For projection-based embedding, it leverages the translation-invariant time kernel. The global continuous time is computed by subtracting the minimum time-stamp and is described by: \u03c6(t) = F (wit + bi), where F is a periodic activation function, can be referred to sinusoid function and {wi}=1 and {bi}=1 are learnable parameters.\nRelative Time Embedding Relative time embeddings encode the relationship between each interaction pair in the sequence by utilizing temporal difference information. Given a matrix of temporal differences D \u2208 RN\u00d7N defined as dij = (ti \u2013 tj)/\u03c4, where \u03c4 is an adjustable unit time difference and i \u2208 N, j\u2208 N in the given sequence with length N. The encoding functions on D are similar to [2]. We use three types of embeddings: \u03c6(dij) = F(widij + bi) is used to learn periodic occurrences, \u03c6(dij) = Exp(dij/freqh) learns the pattern with an downtrend quickly, and \u03c6(dij) = Log(dij/freqwhere is the adjustable parameter.\nAbsolute Positional Embedding Similar to the absolute time embedding, we use two types of absolute positional embeddings: (1) the fixed positional encoding (p), where p is the position index, commonly used for transformer [4, 17]; and (2) the learnable positional embedding same as SASRec [11].\nRelative Distance Embedding To mitigate self-attention modules failing to capture short-term user dynamics, multiple works [7, 15] apply gaussian prior or learnable weight to correct the importance of items aligning to the current central item. The positional distance matrix D \u2208 RN\u00d7N defined as dij = (pi \u2013 pj) and weight G is denoted as gij = exp (\u2212(dij \u2212 \u00b5)\u00b2/2\u03c3\u00b2) and element-wise multiply to attention score matrix: G\u2218 (Q\u22c5 KT/\u221adk), where Pi, pj \u2208 N are the position indices in the given sequence with length N, \u03bc and \u03c3 are used for initialization or the weight G can be learnable."}, {"title": "3.3 Attention Mechanism", "content": "Mix-attention: We adopt the mix-attention architecture as our backbone which is composed of absolute attention and relative attention. We employ multiple absolute and relative kernel embeddings to capture users' diverse patterns. The absolute embedding attention head is described as: heada = Attention(Q + Q\u00aa, K + K\u00aa, V), where Q and Q\u00aa are the query matrices composed of the common embedding (e.g., ItemID embedding Q) and one or more different absolute embeddings (e.g., Q\u00aa), respectively; K and Ka are the corresponding key matrices; V is the value matrix and the relative embedding attention head can be described as: headr = Attention(Q, K, V) + Attention(Q+b\u00b2, K\u00b9, V), where K' represents the relative encoding key matrix; b\u00b9 represents the learnable bias vector added to the queries.\nStacking Layer and Point-Wise Feed-Forward Network: Our model operates similar to [11]. We apply Position-wise Feed Forward Network (FFN) and stack L sublayers with residual connection including LayNorm() and Dropout() as well."}, {"title": "3.4 Robust Input and Output", "content": "Inspired by NIR [9], noise is injected into the transformer layers during training, and explicit layer regularization is applied. Specifically, given an input point x, a perturbed input x is generated by adding random noise e with a small magnitude to x. We enhance the strategy by creating a parameterized neural network for learnable injected noise. Considering a linear neural network with m inputs and n outputs, denoted by x = w. x + b, where x \u2208 Rm is the layer input, w \u2208 Rm\u00d7n is the weight matrix, and b \u2208 R\" is the noise. The corresponding neural layer with respect to the input x can be represented as: x = (\u03bc\u03c9 + \u03c3w \u2299ew) x + \u03bc\u03b5 + \u03c3\u03b5 e\u03bc The parameters \u03bcw \u2208 Rm\u00d7n, \u03bc\u03b5 \u2208 Rn, \u03c3w \u2208 Rm\u00d7n, and \u03c3\u03b5 \u2208 Rn are learnable, ew \u2208 Rm\u00d7n and eb \u2208 Rm are standard gaussian noise random variables, where \u03b6 =(\u03bc, \u03c3) is the set of learnable vectors. We apply layerwise noise stability regularizer (LNSR) on the training data set to enhance robustness and generalization following: \u0154(0) = Ee||f(x + e) \u2212 f(x)||\u00b2 = \u2211=1 di,j||.fi,j (x + \u025b) \u2212 fi,j(x)||\u00b2., where j is the layer where noise is injected, i, j are the layer index from 1 to L (the total number of layers), di,j are the regularization weights for each layer, x is the input and & is the injected noise vector. For \u0154, by using first-order and second-order Taylor expression to represent f (x + \u025b), it has: R(0) = \u2211{\u03a9j(f) + \u03a9\u3150(f)}, where\""}, {"title": "3.5 Learning Objective", "content": "Our model (ADRRec) training is composed of two components: (1) absolute and relative pattern training to learn user long-term and short-term preferences and (2) stability regularizer for generalization. The form of the cost function can be represented as: \u03b8\u2217 = arg min \u03b8E [L(f(x; \u03b8), y) + \u03bb\u0154(\u03b8)], where \u03bb\u2208 [0, 1] and L is the loss function measuring the discrepancy between the network's prediction f(x; \u03b8) and the true label y. \u0154(\u03b8) is the regularizer."}, {"title": "4 EXPERIMENTS", "content": "In the experiments, we aim to address several key questions: (1) Comparison with Baseline Models: How does ADRRec perform compared to baseline models in terms of accuracy? (2) Impact of Different Components: What is the effect of incorporating different components, such as absolute and relative embeddings, on the model's performance? (3) Robustness Analysis: How robust is ADRRec to variations in the input data? We evaluate our approach on four real-world datasets: MovieLens 1M and 20M, Amazon Beauty and Game [11]."}, {"title": "4.1 Comparison with Baseline Models", "content": "To validate the effectiveness of our proposed model, we compare it with popular baselines: SASRec [11], BERT4Rec [4], TISAS [12], and MEANTIME [2]. These models were selected due to their strong performance in sequential recommendation tasks, as they leverage self-attention mechanisms: SASRec using left-to-right attention, BERT4Rec using bidirectional attention, and TiSASRec accounting for temporal intervals and MEANTIME utilize mixture attention framework-making them well-suited for our comparative analysis. Among the baseline methods, our model ADRRec consistently outperforms the rest baselines (Table 3)."}, {"title": "4.2 Robustness Analysis", "content": "To evaluate robustness and generalization of our model, we performed robustness studies by: (1) evaluating the standard deviation of the model results run with 3 random seeds (Table 4). The results on mean, std show the stability and robustness. (2) In the test set, randomly masking some continuous portion (10%, 30%) in input test sequence (Table 5). We observe it does not degrade generalization performance."}, {"title": "4.3 Ablation Studies", "content": "To evaluate the impact of different components, we used Beauty and ML-1M datasets to compare the performance: (1) w vs. w/o absolute/relative kernel embedding, and (2) w vs. w/o noise regularizer. Table 6 shows the comparison of different combinations of kernel embedding plus noise injection regularizer. The results show noise injection regularizer have effectiveness in both datasets."}, {"title": "5 CONCLUSION", "content": "In this paper we presented ADRRec, an adaptive and robust sequential recommendation model, which uses multi-dimensional kernel encoding and mix-attention mechanism to learn each unique user behavior. We apply layer-wise noise injection regularization to enhance robustness and generalization. Experiments on four classical datasets show that our model outperforms the baselines in sequential recommendation."}]}