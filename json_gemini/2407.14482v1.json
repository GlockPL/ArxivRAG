{"title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities", "authors": ["Peng Xu", "Wei Ping", "Xianchao Wu", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro"], "abstract": "In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge the gap between open-access LLMs and leading proprietary models (e.g., GPT-4-Turbo) in long-context understanding and retrieval-augmented generation (RAG) capabilities. These two capabilities are essential for LLMs to process large volumes of information that cannot fit into a single prompt and are complementary to each other, depending on the downstream tasks and computational budgets. We present a detailed continued training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, along with a three-stage instruction tuning process to enhance the model's instruction-following, RAG performance, and long-context understanding capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model achieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context understanding tasks and surpasses it on the RAG benchmark. Interestingly, we find that the long-context retriever can alleviate the top-k context fragmentation issue in RAG, further improving RAG-based results for long-context understanding tasks. We also provide extensive comparisons between RAG and long-context solutions using state-of-the-art long-context LLMs.", "sections": [{"title": "Introduction", "content": "The open LLM community has made significant progress in advancing the capabilities of open-access large language models (LLMs), including Llama-3-70B-Instruct (Meta-AI, 2024), QWen2-72B-Instruct (Alibaba-QWen, 2024), Nemotron-4-340B-Instruct (Nvidia et al., 2024), and Mixtral-8x22B-Instruct-v0.1 (Mistral, 2024). However, performance gaps compared to frontier proprietary models, e.g., GPT-4-Turbo (OpenAI, 2023), still exist in many domains. Additionally, open-access models focused on key domains have been developed, such as DeepSeek-Coder-V2 (Zhu et al., 2024) for coding and math, ChatQA 1.5 (Liu et al., 2024) for conversational QA and retrieval-augmented generation (RAG), and InternVL 1.5 (Chen et al., 2024) for vision-language tasks, which can be on par with GPT-4-Turbo-2024-04-09 (OpenAI, 2023) in the certain domains.\nIn recent developments, the trend of extending the context window length in LLMs has gained remarkable traction within both the industrial and research communities. All leading proprietary LLMs support very large context window, allowing them to accommodate several hundred pages of text in a single prompt. For example, GPT-4 Turbo (OpenAI, 2023) and Claude 3.5 Sonnet offer a 128K and 200K context window, respectively. Meanwhile, Gemini 1.5 Pro (Gemini-Team, 2024) impressively supports up to a 10M context. Open-access LLMs have also made significant strides to keep up (01.AI et al., 2024; Alibaba-QWen, 2024). For instance, QWen2-72B-Instruct (Alibaba-QWen, 2024) and Yi-34B (01.AI et al., 2024) support 128K and 200K context windows, respectively. However, the training data and technical details for these models are missing, making reproduction challenging. In addition, these models have mostly been evaluated on synthetic tasks, like Needle in a Haystack (Kamradt, 2023) test, which does not accurately represent real-world downstream task performance. For example, previous studies shows a noticeable gap between open-access LLMs\nand leading proprietary models on real-world long context understanding tasks (Zhang et al., 2024a; Hsieh et al., 2024). In this work, we focus on bridging the gap between the open-access Llama-3 and proprietary GPT-4 Turbo on real-world long context understanding tasks.\nThe long-context capability of LLMs is sometimes considered a rival technique to retrieval-augmented generation (RAG). However, from a pragmatic perspective, these techniques complement each other. An LLM with a long context window can either process large volumes of text as a prompt or utilize retrieval methods to efficiently extract relevant information from the extensive text, depending on the downstream tasks and accuracy vs. efficiency trade-offs. RAG has efficiency advantages and can easily retrieve relevant contexts for query-based tasks (e.g, QA) from billions of tokens, a feat that long context models cannot achieve. Meanwhile, long context models are good at tasks such as summarizing entire documents, where RAG may not perform as well. As a result, the state-of-the-art LLM needs to excel at both capabilities, providing options for different downstream tasks based on accuracy and efficiency requirements. In a previous study, the open-source ChatQA 1.5 (Liu et al., 2024) model can surpass GPT-4-Turbo on RAG tasks. In this work, we present ChatQA 2, which possesses both GPT-4-Turbo level long context understanding capability and RAG performance.\nXu et al. (2024) extended the context window of Llama2 (Touvron et al., 2023b) to 16K and 32K tokens, and studied the interplay between RAG and long-context LLMs. It demonstrates that the RAG method can improve the generation accuracy and inference efficiency of a GPT-3.5-turbo-16k level long context model for QA and query-based summarization tasks. In this work, we extend this study by pushing the long context LLM to GPT-4-Turbo level with 128K context window and combining it with a state-of-the-art long-context retriever for RAG.\nSpecifically, we make the following contributions:\n1.  We present a two-step approach to establish the long context capability of Llama3-70B. First, we extend Llama3-70B base's context window from 8K to 128K by continually pretraining it on a mix of SlimPajama (Soboleva et al., 2023) with upsampled long sequences (Fu et al., 2024). Then, we apply a three-stage instruction tuning process on curated datasets to enhance the instruction-following, RAG, and long context understanding capabilities at each respective stage. We find that stage-wise instruction-tuning, by incorporating previous datasets, simplifies experimentation and hyperparameter tuning. This approach enhances long context capabilities while maintaining RAG performance.\n2.  We demonstrate that the resulting Llama3-ChatQA-2-70B-128K can be on par or slightly worse than GPT-4-Turbo-2024-04-09 on many real-world long context understanding tasks. In addition, it outperforms GPT-4-Turbo-2024-04-09 on RAG and conversational QA tasks.\n3.  The current RAG pipeline has limitations that can undermine downstream task accuracy: i) top-k chunk-wise retrieval introduces fragmentation of context; and ii) a small top-k leads to low recall, while a larger k introduces too much irrelevant context to the LLM (e.g., see the analysis in Figure 1 of Yu et al., 2024). We find that the state-of-the-art long-context retriever (Wang et al., 2023c; Lee et al., 2024) can largely alleviate these issues and further improve the RAG-based results for long-context understanding tasks.\n4.  In the comparison between RAG and long-context results, we find that the GPT-4-Turbo level long-context model (including our Llama3-ChatQA-2-70B) outperforms the RAG on 32K benchmarks but still underperforms compared to RAG methods on real-world 128K tasks.\nWe organize the rest of the paper as follows. We discuss related work in \u00a7 2. We introduce the continued pretraining for context window extension in \u00a7 3 and the three-stage instruction tuning in \u00a7 4. We report results in \u00a7 7 and conclude the paper in \u00a7 8."}, {"title": "Related Work", "content": "2.1 Long Context LLM\nThe trend of extending the context window in LLM starts by Claude with 100K token context (Anthropic, 2023). Although the underlying long context techniques behind proprietary models are unclear, the open LLM and research community has developed many methods to extend the context window of LLMs through continued training or fine-tuning (Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023a; Tworkowski et al., 2023; Chen et al., 2023b; Peng et al., 2023; Xiong et al., 2023;\nFu et al., 2024), especially for open-access LLMs (Touvron et al., 2023a,b) based on rotary position embedding (RoPE) (Su et al., 2024).\nThere are two popular approaches to adapt RoPE for long-context inputs: position interpolation (Chen et al., 2023a) and increasing the base frequency 0 of ROPE (Xiong et al., 2023; Liu et al., 2023b). Recently, Yi-34B (01.AI et al., 2024) was pretrained with a sequence length of 4K, and its context window was extended to 200K by increasing the ROPE @ from 10,000 to 5M during continued pretraining. Qwen2-72B-Instruct (Alibaba-QWen, 2024) was trained on 32K-length contexts and extrapolated to a 128K context length using YaRN (Peng et al., 2023). Instead of extending the context window of the base model then applying instruction tuning, GradientAI (2024) directly fine-tunes the Llama-3-Instruct, which uses NTK-aware interpolation (Peng et al., 2023) and the formula from Liu et al. (2023b) to scale up \u03b8.\n2.2 Retrieval-augmented Generation (RAG)\nRetrieval with a standalone retriever (e.g., Karpukhin et al., 2020; Wang et al., 2022; Lin et al., 2023; Lee et al., 2024) is a long-standing solution for handling long texts that cannot fit into the context window of language models. In previous work, various retrieval-augmented language models have been proposed (Nakano et al., 2021; Borgeaud et al., 2022; Wang et al., 2023b,a; Guu et al., 2020; Izacard & Grave, 2021; Izacard et al., 2022; Lewis et al., 2020; Huang et al., 2023; Khandelwal et al., 2019; Liu et al., 2024).\nPrevious dense-embedding-based retrievers only supported limited context windows (e.g., 512 tokens) (e.g., Karpukhin et al., 2020; Wang et al., 2022; Lin et al., 2023). In top-k chunk-wise retrieval, the short chunk size increases context fragmentation. As a result, extending the context window of retrievers has become popular. For example, Jina Embeddings 2(G\u00fcnther et al., 2023) and Nomic Embed (Nussbaum et al., 2024) support 8K tokens, while E5-mistral-7B (Wang et al., 2023c) and NV-Embed Lee et al. (2024) support 32K tokens."}, {"title": "Extending Context Window to 128K", "content": "In this section, we present the method to extend the context window from 8K to 128K for Llama3. We prepare our long context pretraining corpus from the Slimpajama (Soboleva et al., 2023) following Fu et al. (2024). We upsample long-context documents with the hyperparameter set as 0.1 to produce 10 billion tokens with sequence length of 128k. Since Llama3 is pretrained with a much higher RoPE base frequency of 500,000 compared to Llama2, we increased the RoPE base frequency to 150M accordingly. We set the batch size to 32 to fit 4 million tokens in a batch and use a learning rate of 3e-5 to train 2000 steps (8B tokens in total).\nInterestingly, we found it more effective to separate different documents using special characters, such as \"<s>\", rather than the reserved beginning and ending tokens <BOS> and <EOS>. We hypothesize that the <BOS> and <EOS> tokens in Llama3 signal the model to ignore previous chunks of text after pretraining, which is not helpful for the LLMs to adapt for longer context inputs."}, {"title": "Instruction-Tuning with Long Context Data", "content": "In this section, we present the instruction tuning method designed to enhance both long context understanding capability and RAG performance.\nSpecifically, we implement three stages of instruction-tuning. For the first two stages, we follow ChatQA 1.5 (Liu et al., 2024), where the model is initially trained on 128k high-quality instruction-following datasets, and then trained on a blend of conversational QA data with provided context. However, these two stages involve relatively short contexts, with a maximum sequence length of only 4K tokens. To enhance our model's capability to handle very long context sequences up to 128k tokens, we collect a long SFT dataset.\nThis dataset is collected through two categories: 1) For SFT data sequences less than 32k: We leverage existing long-context datasets from LongAlpaca12k, GPT-4 samples from Open Orca 1, and\nLong Data Collections 2. 2) For sequence lengths between 32k and 128k: Since it is challenging to collect such SFT samples, we rely on synthetic datasets. We utilize NarrativeQA, which contains both the ground truth summary and semantically related paragraphs. We assemble all the related paragraphs and randomly insert the ground truth summary to simulate a real long document for its QA pairs. Both the full long SFT dataset and the short SFT dataset from the first two stages are then blended for training. We set the learning rate at 3e-5 and the batch size at 32."}, {"title": "Long Context Retriever meets Long Context LLM", "content": "As we mentioned in previous section, the current RAG pipeline for LLM has the following issues: i) The top-k chunk-wise retrieval introduces non-negligible fragmentation of context for generating accurate answers. For example, previous state-of-the-art dense-embedding based retrievers (e.g., Li et al., 2023; Lin et al., 2023) only support 512 tokens. ii) Small top-k (e.g., 5 or 10) usually leads to relatively low recall, while much larger k (e.g., 100) can lead to worse generation (see Table 5 in Xu et al. (2024)) as the previous LLMs could not utilize too many chunked context very well (Liu et al., 2023a). To address the issue, we propose to use the most recent long-context retriever (Wang et al., 2023c; Lee et al., 2024), which can support thousands of tokens. In our setting, we use the E5-mistral embedding model (Wang et al., 2023c) as the retriever."}, {"title": "Evaluation Benchmarks", "content": "We compare our model against SOTA long context models: 1) GPT-4-Turbo-2024-04-09 (128K context window) (OpenAI, 2023), 2) Qwen2-72B-Instruct (128K context window) (Alibaba-QWen, 2024), and (3) Llama-3-70B-Instruct-Gradient-262k (GradientAI, 2024).\nTo give a comprehensive study of different context lengths, our evaluation benchmarks covers three categories, 1) long context benchmarks beyond 100K tokens, 2) medium-long context benchmarks within 32K tokens, and 3) short context benchmarks within 4K tokens. We also apply RAG when it is applicable to the downstream tasks.\n6.1 Long Context Benchmarks Beyond 100K Tokens\nInfiniteBench (Zhang et al., 2024b) is proposed to evaluate the long context capability of LLMs over 100K sequence length. As we focus on real-world english tasks, we only take the four related tasks from the InfiniteBench, i.e. longbook summarization (En.Sum), longbook qa (En.QA), longbook multiple choice (En.MC), and longbook dialogue (En.Dia). En.Sum is a task that requires models to generate a concise summary of the given novel and is evaluated using the ROUGE-L-Sum metric (Lin,\n2004). En.QA is annotated by a pipeline that ensures the questions' necessitating of long-range dependencies and reasoning, beyond simple short passage retrieval. Aggregation reasoning and filtering reasoning are the two primary reasoning categories. F1 score is used to evaluate the quality of the answer. En.MC is annotated with the same pipeline of En.QA except that four answer choices are provided and exact matching scores are reported. En.Dia leverages movie and drama scripts from a designated online database 3 with long, multi-role dialogues. In this task, random instances of character names within a script are masked and the objective is to correctly identify these masked names. Exact matching score is used again to evaluate the prediction accuracy.\n6.2 Medium-Long Context Benchmarks within 32K Tokens\nWe use the long context datasets (except NarrativeQA as it is included in our training) from Xu et al. (2024) as our benchmark for medium-long datasets within 32K. There are six datasets in total, where QMSum (QM), Qasper (QASP), QUALITY (QLTY) are token from SCROLLS and HotpotQA (HQA) MuSiQue (MSQ), MultiFieldQA-en (MFQA) are token from LongBench. Following the official metrics, we report the geometric mean of ROUGE scores (i.e., ROUGE1/2/L) (Lin, 2004) for QM, the exact matching (EM) score for QLTY, and F1 scores for the remaining four datasets QASP, MSQ, HQA and MFQA.\n6.3 Short Context within 4K Tokens\nWe use ChatRAG Bench (Liu et al., 2024) as our benchmark for short context within 4k. ChatRAG bench consists of 10 datasets and we exclude HDial as it is vasincluded in our training. Following the setup of Liu et al. (2024), for Doc2Dial (D2D), QuAC, and QReCC task with long documents, each document is divided into segments of roughly 300 words. The top 5 relevant chunks are then retrieved as context for each user question. For TopiOCQA and INSCIT, top-20 chunks were retrieved to obtain similar context length to the first three datasets. The other four datasets are CoQA, DOQA, ConvFinQA (CFQA), and SQA, which cover a wide range of domains like finance, children's stories, literature, mid/high school exams, news, Wikipedia and etc. We use F1 score as the metric to evaluate the generations and report the average score without HDial as it is a fair zero-shot comparisons over different models."}, {"title": "Results", "content": "In this section, we present the results and comparisons from extensive benchmark evaluations. We begin with the synthetic Needle in a Haystack test, then focus on real-world long context understanding and RAG tasks.\n7.1 Needle In A Haystack\nWe evaluate our Llama3-ChatQA-2-70B model on the Needle In A Haystack test (Kamradt, 2023). This synthetic task is popular for testing the long-context capability of LLMs, and can be considered as a threshold level evaluation. Figure 1 demonstrates the performance of our model with up to 128K tokens, showing that our model achieves 100% accuracy. This test confirms our model's perfect long-context retrieval capability.\n7.2 Long Context Evaluation Beyond 100K Tokens\nIn this subsection, we evaluate the long context capability beyond 100K tokens on the real-world tasks from InfiniteBench (Zhang et al., 2024a). Table 2 shows that our model (34.11) outperforms many existing state-of-the-art models, such as GPT4-Turbo-2024-04-09 (33.16), GPT4-1106 preview (28.23), Llama-3-70B-Instruct-Gradient-262k (32.57) and Claude 2 (33.96). Additionally, our model is very close to the highest score of 34.88 achieved by Qwen2-72B-Instruct, confirming the competitive long-context capability of our model.\n7.3 Medium-Long Context Evaluation within 32K Tokens\nIn this subsection, we evaluate the medium-long context capability within 32K tokens. Table 3 shows that GPT-4-Turbo-2024-04-09 achieves the highest score of 51.93 among all models. Our model scores 47.37, which is higher than Llama-3-70B-Instruct-Gradient-262k but is lower than Qwen2-72B-Instruct. This difference can be attributed to the extensive 32K pretraining implemented by Qwen2-72B-Instruct, while we used a much smaller continued pretraining corpus. Additionally, we found that all the RAG solutions perform worse than the long context solution, which suggest all these SOTA long context LLMs can really handle 32K tokens within their context window.\n7.4 CHATRAG BENCH: Short Context Evaluation within 4K Tokens\nIn this subsection, we evaluate the models on the short context tasks within 4K tokens from CHATRAG BENCH (Liu et al., 2024). Our model achieves the average score of 54.81. Even though it is worse than Llama3-ChatQA-1.5-70B, it still outperforms GPT-4-Turbo-2024-04-09 and Qwen2-72B-Instruct. This confirms that extending short context models to long context is not a free lunch. How to effectively extend the context window to even larger scale (e.g., million tokens in Gemini 1.5 Pro (Gemini-Team, 2024)) without any degradation on regular short context tasks is an exciting research direction.\n7.5 RAG vs. Long Context\nIn Table 5 and Table 6, we compare RAG vs. long context solutions under different context lengths. For sequence length beyond 100k, we only report the average score of En.QA and En.MC as the RAG setting is not directly applicable for En.Sum and En.Dia. We found that for downstream tasks within 32k sequence length, our long context solution is better than RAG. This means using RAG can save the cost but the accuracy will drop a bit. On the other hand, we found that for context lengths beyond 100K, RAG (using top-5 for our Llama3-ChatQA-2-70B, and top-20 for Qwen2-72B-Instruct) outperforms the full long-context solution. This indicates that even state-of-the-art long-context LLMs may struggle to effectively understand and reason over 128K tokens. In such scenarios, RAG is recommended for better accuracy and lower inference cost, provided it is applicable to the downstream tasks."}, {"title": "Conclusion", "content": "We introduce Llama3-ChatQA-2-70B, a long context model that possess GPT-4 Turbo-level capabilities for both understanding up to 128K long contexts and utilizing retrieved contexts for generation. This provides the flexible options for different downstream tasks with specific accuracy and efficiency requirements. We present a detailed and reproducible technical recipe for building and evaluating the model, including the methods, training data, and evaluation benchmarks. In particular, we evaluate ChatQA 2 on RAG and short-context benchmark (ChatRAG) (within 4K tokens), medium-context tasks from SCROLLS and LongBench (within 32K tokens), and long-context tasks from InfiniteBench (beyond 100K tokens). We demonstrate that the Llama3-ChatQA-2-70B can achieve GPT-4-Turbo-2024-0409 level accuracy on these benchmarks."}]}