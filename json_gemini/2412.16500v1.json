{"title": "Speech Retrieval-Augmented Generation without Automatic Speech Recognition", "authors": ["Do June Min", "Karel Mundnich", "Andy Lapastora", "Erfan Soltanmohammadi", "Srikanth Ronanki", "Kyu Han"], "abstract": "One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)-based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts.", "sections": [{"title": "I. INTRODUCTION", "content": "Retrieval-Augmented Generation (RAG) [1] has enabled Large Language Models (LLMs) to generate responses using data not available during any of their training stages. This increases the helpfulness of these models since they can be used as an interface to explore topics released after the training, or to cite sources of information to improve factuality. However, RAG remains mainly used for text-based sources, which may contain images and tables [2], [3].\nOver recent years, however, we have observed a surge in the creation of unstructured content and information such as audio recordings or videos containing spoken information. One tool that has the potential to enable efficient search through these expanding audio archives to structure the information is spoken content retrieval [4]. This method indexes and retrieves passages in audio format and offers a solution for searching large collections of speech data such as meeting recordings [5]. However, available spoken content retrieval systems treat the problem as a variant of text retrieval, where the source text is automatic speech recognition (ASR) transcriptions of audio produced by an ASR system [4], which usually contain errors. This system is often referred to as a cascaded model, where the output of the ASR step is fed to the text-based retriever as input. With the recent advancements in speech processing [6] and text retrieval [7], the cascaded model presents a robust baseline across many speech tasks, including spoken dialogue state tracking [8] and spoken language understanding (SLU) [9].\nDespite the robustness of the cascaded approach, there are down-sides in its use for retrieval from data in spoken form. For example, the errors from ASR can propagate downstream, and negatively impact retrieval and generation performance. This problem can be exacerbated in challenging topics for ASR systems such as named entity recognition [10], even more so considering that named entities are often central to accurate retrieval since often they are used to match queries to passages. In addition, applying ASR to speech results in the loss of paralinguistic information. To best represent the information contained in speech without loss, speech should be indexed and retrieved in its original form.\nIn this work, we propose a solution to overcome the limitations of the ASR-based cascaded retrieval systems and instead directly index and search audio passages in their original speech format. Specifically, we tackle the problem of text query to audio passage retrieval by using a text embedding model to encode audio passages in the same text embedding space, effectively allowing multimodal retrieval using a single embedding model.\nOur main contributions are:\n\u2022 We propose and implement an end-to-end speech retrieval system that embeds both text and audio in the same space, allowing retrieval of text and speech interchangeably,\n\u2022 Our method used a lightweight adapter between an LLM-based text embedding model and a speech encoder, making it data efficient during training and avoiding cross-modal contrastive learning,\n\u2022 We implement an end-to-end speech RAG framework that requires no ASR for open question answering from spoken passages."}, {"title": "II. BACKGROUND", "content": "Inspired by the success of CLIP [11] in the task of cross-modal retrieval, several audio embedding models have been proposed, leveraging large audio-caption pair datasets with contrastive learning techniques [12], [13], [14], [15]. Moreover, powerful audio and speech encoders have been made widely available, leading to better audio representations for a wide array of tasks [6], [16]. However, models like CLAP-LAION, CLAP-MS, and others are limited to matching audio to natural language descriptions of an audio event, instead of matching speech to its corresponding language content [17].\nOn the other hand, spoken content retrieval involves matching the linguistic and semantic content of speech contained in audio to queries [4], [18]. Recent models typically use contrastive learning [19], [20] to train speech embedding models that can either match spoken or text queries to audio passages. SpeechDPR tackles spoken query to spoken passage retrieval by training an end-to-end model with teacher distillation from dense text retrievers [21]. Our work tackles text query to audio passage retrieval and avoids expensive contrastive learning while achieving competitive performance with ground truth (GT) text retrieval."}, {"title": "III. RETRIEVAL FROM SPOKEN PASSAGES", "content": "Our framework for text-to-speech retrieval consists of a fine-tuned speech adapter and a frozen unimodal text retriever (shown in Fig. 1a). The speech adapter projects downsampled speech repre-sentations into the text embedding space of the text retriever model. This architecture allows us to leverage the retrieval capacity of state-of-the-art text retrievers without optimizing a cross-modal embedding model from scratch, which would require much larger datasets.\n\nA. Speech adapter for text-based retriever\nThe function of the speech adapter is to adapt the speech repre-sentations of the input audio into the text embedding space of the downstream text model. In this work, we follow a similar approach as [23]. Specifically, the adapter, which consists of an encoder and a downsampler, is trained using a cosine embedding similarity loss propagated in an end-to-end manner from the text model.\nSpeech encoder: We use HuBERT [24], a pre-trained speech encoder, and feed the last hidden layer representations to the speech adapter.\nSpeech adapter: Typically, the text token sequence length of a speech transcript is much shorter than the sequence length of the correspond-ing discretized speech. Therefore, we use an average pooling layer of the time dimension to downsample the speech representations, and a projection layer to upsample the speech representation to the LLM embedding dimension [25].\n\nB. Cross-modal retriever\nTo encode both modalities (text for queries and speech for pas-sages), we use the same frozen text-based retriever as our backbone [26], [27]. The main difference across modalities is the embedding module, which maps the raw data (text tokens, audio features) into the embedding space of the text retriever.\nEmbedding text: We use the original token embedding module of the text retriever model. Then, the input embedding sequences are processed by the retriever, and the final layer's representations are pooled to create et, the fixed-sized representation of the text.\nEmbedding speech: Our speech adapter embeds speech into a sequence of embeddings by first deriving frame-level speech features with the pre-trained speech encoder, and downsampling the feature sequences over the temporal dimension. The embeddings are then consumed by the retriever, with es as the final embedding output.\nTraining loss: Given et, the text embedding of a ground truth transcript of an audio passage and es, the audio embedding of the audio passage, we compute the distillation loss using the cosine embedding loss:\n$L(e_s, e_t) = 1 - cos(e_s, e_t) = 1 - \\frac{e_s \\cdot e_t}{||e_s||||e_t||}$ (1)\nC. Audio-conditioned generator\nFor an end-to-end speech RAG model that does not require ASR, we use a pre-trained multi-task SLM trained on various speech tasks such as speech recognition or question answering [28]. Similar to our cross-modal retriever, the SLM consists of a speech adapter and a frozen text model."}, {"title": "IV. EXPERIMENTS", "content": "A. Data\nWe use two speech datasets, SpokenSQUAD [29], and VoxPopuli [30] for our RAG experiments. SpokenSQUAD is a spoken version of the SQUAD dataset, where Wikipedia text are converted into speech using text-to-speech systems [31]. It is annotated with text queries about the spoken passages and ground truth answers. The audio passages are pre-chunked in passage-level, with the average spoken passage duration of ~60s.\nVoxPopuli is a large collection of speech from the European Parliament events, with utterances averaging ~10s. We use the English subset of VoxPopuli and create query and answer pairs from the speech using LLM-prompting. We first identify potential answer candidates by extracting named entities from each utterance using a fine-tuned BERT model [32], and use answer-aware generation to mine text queries. Specifically, we prompt Claude 3.5 Sonnet to generate a question that is answered by the extracted named entity, given the utterance as context (Table I). For both datasets, each query has only one relevant passage.\nB. Implementation details\nRetriever: For our speech encoder, we use HuBERT-large [24], which uses self-supervised learning to generate deep representations of speech sampled at 16kHz. Our speech adapter downsamples the output of HuBERT 4 times (for a final frame length of 80ms). Both the speech encoder and adapter are unfrozen. For our frozen retriever backbone, we use E5-Mistral-7B-Instruct LLM-based retriever [27]. We train the model for 20 epochs, with a stopping criteria of validation loss, with a patience of 3. We use the Adam optimizer with a learning rate of 5e-5, and \u03b2\u2081 = 0.9, \u03b22 = 0.999. We train with a batch size of 4 and a gradient accumulation step of 16.\nGenerator: After audios are retrieved from the vector database, each query is combined with the top-k retrieved audios and a task instruction prompt as a prompt an instruction-tuned LLM \u00b9 which generate answers. to an 7B-parameter SLM 2 to generate an answer [28]. We use the SLM as is, without fine-tuning its parameters to this specific task.\nC. Evaluation\nRetrieval: We use Recall@k as our evaluation metrics for the retrieval experiment:\n$Recall@k = \\frac{\\text{# of relevant passages in top-k}}{\\text{Total # of relevant items}}$ (2)\nwhere k = 5, 10, 100. Each query has exactly one relevant passage. For the relevance score, we use the cosine similarity between the text query embedding and the audio passage embedding.\nGeneration: For the generation experiment, we use top-5 retrieved passages as context provided along with the text query and the LLM instruction. To evaluate the correctness of the generated answer, we use\n\u2022 Exact Match (EM), which assigns 1 if the ground-truth is in the answer, otherwise 0, and\n\u2022 LLM Correctness, implemented as machine-based evaluation of match to cover minor spelling alterations or other edge cases [33].\nD. Baselines: fully-cascaded and semi-cascaded RAG\nWe implement two types of cascaded RAG baselines. The fully-cascaded baseline consists of a cascaded text retriever (an ASR module and a text retriever) and an LLM generator. This framework transcribes the audio passages and treats spoken content retrieval as a text retrieval problem. The semi-cascaded baseline uses our speech retriever to retrieve the audio passages, then uses the transcripts of the audios and uses them to condition generation. For our baselines, we use Qwen-7B-Chat as the text LLM generator.\nTo investigate the effect of transcription quality of the ASR step, we implement different versions of the baseline, each with varying levels of average WER. We use the transcriptions as ground truth text (where we assume that the WER is 0%), while the case uses the same audio encoder used for the speech adapter. To simulate a severe WER scenario, we use a small ASR module\u00b3 trained on 100hrs of Librispeech [34].\nBoth baselines use the same frozen text retriever as the speech retriever and the text-only version of the SLM (Qwen-7B-Chat)."}, {"title": "V. RESULTS", "content": "A. Retrieval results\nWe show our retrieval experiment results in Table II. We observe that across both datasets, our proposed speech retriever outperforms the cascaded baselines. This shows that our method offers a retrieval performance advantage even when the ASR step is done using a relatively high-performance ASR model. While the comparison between the cascaded model and the ground truth text baseline indicates that the text retrieval model can be robust against low WER error rates, the large performance drop in the High WER results shows that noisy transcriptions can lead to severe retrieval performance degradation. Our speech retrieval removes the possibility of ASR error propagation at the retrieval step by operating directly on speech.\nMoreover, our speech retriever achieves a retrieval performance that is on par with that of the ground truth transcript-based retrieval, even obtaining a slightly higher performance in one metric (Recall@10, VoxPopuli set), showing that our proposed speech retriever is a practical and powerful alternative to the cascading framework of spoken content retrieval.\nB. Retrieval performance under different noise levels\nTo study the effect of noise on retrieval performance, we plot the Recall@5 of the text baselines and our speech retriever at different noise levels, as shown in Fig. 2. We add Gaussian noise at different signal-to-noise (SNR) levels to the audio. We observe that across both datasets, our speech retriever is more robust to noise than the cascaded baseline that uses the same speech encoder as our end-to-end retriever, with the exception of a very high noise setting for the SpokenSQUAD dataset (which is TTS-based).\n\nC. Generation results\nTable III shows our comparison of SpeechRAG, fully-cascaded, and semi-cascaded RAG baselines. Promisingly, we find that the full SpeechRAG framework outperforms the High-WER cascaded baselines on both datasets, highlighting the potential of SpeechRAG. For example, Table 3 shows how SpeechRAG avoids the corruption of context information by an inaccurate transcription of named entities. However, we also find that it performs worse than the fully- and semi-cascaded baselines under settings, with a larger gap for the SpokenSQUAD dataset. This is possibly due to the difference in the durations of the retrieved audios, with an average SpokenSQUAD audio lasting 4 times as long as the average VoxPopuli utterance. This adversely affects performance since SLMs are typically not trained to handle multiple, long-context audios."}, {"title": "VI. CONCLUSION", "content": "In this work, we propose a first fully speech-based solution to question answering over speech. To achieve this, we implement a speech retriever consisting of a speech adapter and a frozen LLM-based text retriever and show that by indexing and retrieving speech directly, our framework outperforms cascaded retrieval in noisy ASR scenarios, and matches ground truth text retrieval. In the generation step, our framework bypasses ASR by using an SLM conditioned on retrieved audio. While the SpeechRAG generation outperforms cascaded baselines in high WER scenarios, we identify the potential for improvement in the performance gap between our framework and the low WER baseline generations."}]}