{"title": "Speech to Speech Translation with Translatotron: A State of the Art Review", "authors": ["Jules R. Kala", "Emmanuel Adetiba", "Abdultaofeek Abayomi", "Oluwatobi E. Dare", "Ayodele H. Ifijeh"], "abstract": "A cascade-based speech-to-speech translation has been considered a benchmark for a very long time, but it is plagued by many issues, like the time taken to translate a speech from one language to another and compound errors. These issues are because a cascade-based method uses a combination of methods such as speech recognition, speech-to-text translation, and finally, text-to-speech translation. Translatotron, a sequence-to-sequence direct speech-to-speech translation model was designed by Google to address the issues of compound errors associated with cascade model. Today there are 3 versions of the Translatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The first version was designed as a proof of concept to show that a direct speech-to-speech translation was possible, it was found to be less effective than the cascade model but was producing promising results. Translatotron2 was an improved version of Translatotron 1 with results similar to the cascade model. Translatotron 3 the latest version of the model is better than the cascade model at some points. In this paper, a complete review of speech-to-speech translation will be presented, with a particular focus on all the versions of Translatotron models. We will also show that Translatotron is the best model to bridge the language gap between African Languages and other well-formalized languages.", "sections": [{"title": "Introduction", "content": "In the World, we can identify approximately 7,151 languages [12]. Some are formalized and others are not (mostly African languages). The languages are sometimes not properly formalized or still going through a formalization process. To bridge the communication barrier it is important to design artificial intelligence (AI) models that are able to translate from one language to another. The translation model could help in language preservation.\nMany AI models have been designed to automate the translation process, we can identify cascade-based models and speech-to-speech (S2ST) (or direct speech transla-tion) models. Cascade-based translation models use a combination of many methods to achieve speech-to-speech translation [13]. On the other hand, the speech-to-speech model translates languages into another directly hence avoiding compound errors observed when using the cascade approach [1]. The goal of this paper is to review the speech-to-speech translation models. A particular focus will be on the translatotron models designed by Google.\nFor an in-depth understanding of this literature review, it is important to define some key terms. Automatic speech recognition is a technology that converts spoken language into written text [14]. Machine translation is a software used to translate text or speech from one language to another[15]. Text-to-speech synthesis is a technology used to convert written text into spoken language [16]. End-to-end speech translation is a system that directly translates speech from one language into another without an intermediate text representation[17]. Neural machine translation: is an artificial neural network-based translation model that uses neural networks to predict the likelihood of a sequence of words in a given target language [18]. Real-time translation: it is a speech translation done in real-time or with relatively low delays [19]. The multimodal translation is a translation system that includes many input modes: text, speech, and visual cue [20]. Low resource languages are languages with limited resources available to train AI models [21]. Zero-shot translation is the ability of a translation model to translate between two language pairs without being explicitly trained [22]. Paralinguistic features are nonlexical language elements such as tone, pitch, intonation, and emotion [23]. There are also translation model performance evaluations or metrics that are critical when comparing translation models [24]. The first metric is BLEU (Bilingual Evaluation Under Study) it is a value comparing the machine translation output to a reference translation. TER (Translation Edit Rate) represents the number of edits required to transform the machine translation into the reference translation. The MCD (Mel-Cepstral Distortion) is a metric used to evaluate the quality of a synthesized speech.\nThis review work will provide a clear answer to the following question: What are the differences between all translatotron models and which model is adequate for the design and implementation of a S2ST model?\nThe contributions of this work are:\n\u2022 A review of S2ST models.\n\u2022 A comparison of translatotron's models.\nThe rest of this paper is organized as follows: Section one: a review of speech-to-speech translation models followed by an in-depth description of the translatotron models from sections two to four and, in section five some corpora used for S2ST will be presented. In section six a comparative study of translatotron models model will be presented followed by the conclusion."}, {"title": "Speech to speech translation", "content": "Translatotron wasn't the first and not the only model to implement speech-to-speech translation. In this section, we will be exploring some models and approaches used for the implementation of speech-to-speech translation.\nCheng et al., [4], developed a speech-to-speech translation model (S2ST) for real-world unwritten language. Their focus was on the translation of languages that don't have a standard text-writing system. Translation from English to Taiwanese Hokkien was used as a case study. Data used to build the model were obtained from human-annotated data, mining of speech datasets, and the use of pseudo-labeling techniques. They took advantage of the advances in applying a self-supervised discrete representation model and improved their speech-to-speech model.\nIt is shown that S2ST models benefited from the advances made in the field of speech representation but are still facing issues such as acoustic multi modality and high latency. Huang et al., [5], also agreed with these observations and also stated that current S2ST uses autoregressive models that predict each unit based on the previously generated unit, and failed to take advantage of parallel computation. Thus the authors, designed TranSpeech, an S2ST with bilateral perturbation to help solve the acoustic modal issue. The proposed model learns only the linguistic information from speech to generate a more deterministic representation. TranSpeech is considered the first S2ST model that doesn't use autoregression. Overall the proposed model yields an improvement of 2.9 BLEU on average compared to the baseline. The parallel decoding nature significantly reduced the latency.\nAn early attempt into S2ST was proposed by Vidal, [6]. He designed a finite state S2ST which is a fully integrated approach to translating speech input language into an output language. The proposed model mapped the input to the output language in terms of the finite state translation model which is learned from input and output sentences. The model was also integrated with the standard acoustic phonetics model of the input language. The resulting global model supplied through Viterbi provides an optimal output sentence for each input. The model was used for an hotel front desk speech services and displays a 700-words dictionary.\nNakamura et al., [7], designed ATR(Advanced Telecommunications Research Institute International) Multilingual Speech-to-Speech Translation System, which is a multi-lingual S2ST system. The purpose of the proposed system was to translate from English to Asian languages (Chinese and Japanese). The system is composed of a large vocabulary, a con-tinuous speech recognition machine, text-to-text translation, and a text-to-speech synthe-sizer. All the components are multilingual. A statistical machine learning model trained on the corpus forms the basis of the proposed model. The dataset used to train the model is composed of 600000 sentences. The system was able to achieve the level of a person with a score of 750 on the TOEIC (Test of English for International Communication) test.\nLee et al., [8], proposed a direct S2ST model with discrete units. The translation model translates from one language to another without relying on intermediate text generation. The proposed model is based on a self-supervised discreet speech encoder on a target speech and a training of a sequence-to-sequence speech-to-speech unit translation to predict the representation of the target speech. The system was able to generate dual modality output (speech and text) in a single inference when target text transcription was available. Experiments on the Fisher Spanish to English dataset yield an improvement"}, {"title": "Translatotron 1", "content": "Translatotron 1 developed by Jia et al.,[1] is the first attempt at the design of an attention-based sequence to sequence neural network that was able to directly translate speech from one language into another language without intermediate text representation. The pro-posed model is trained by mapping speech spectrograms of the input language to the output language spectrograms corresponding to the translation. The author demon-strates the ability of the model to synthesize the speaker's voice during translation The tests conducted on two Spanish-to-English speech translation datasets were not good but promising because the model underperformed compared to the baseline cascade. The main goal of the model was to demonstrate that direct speech-to-speech translation could be achieved.\nFigures 1, describes translatotron 1 Architecture. The architecture is composed of the following elements:\n\u2022 A sequence-to-sequence attention-based neural network is used to generate a target spectrogram.\n\u2022 A vocoder to transform the target spectrogram to a time domain waveform.\n\u2022 An optional speaker encoder that is pre-trained and can be used to identify speakers and enable cross-language voice conversation with simultaneous translation."}, {"title": "Translatotron 2", "content": "Jia et al.,[2] designed Translatotron 2, which is a direct speech-to-speech translation model that can be trained end to end. The proposed model is composed of a speech\nThe overall process of translatotron 2 can be summarized with the following equation 1:\n$S_{t}'=D(E(S_{s}))$,\nWhere $S_{t}'$ is the predicted target spectrogram, $S_{s}$ is the spectrogram sequence of the source speech, D is the decoder containing the attention module finally & the encoder. Translatotron 2 model is trained in a supervised manner. $L_1$ and $L_2$ losses predicted between the target spectrogram $S_{t}'$ and the real spectrogram $S_{t}$ are combined to design the model loss function. The following equation 2 describes translatotron 2 loss function.\n$L_{spec}(S_{t},S_{t}')= \\frac{1}{TK} \\sum_{i=1}^{T} \\sum_{j=1}^{K} ||S_{t}^{i} - S_{t}'^{j}||^{2}$,\nWith $S_{t}^{i}$ representing the i-th frame of $S_t$, T is the number of frame in $S_t$, K the number of frequency bins in $S_t$, and $||\\cdot||$ the distance. The duration loss is the second loss linked to the total number of frames T and the total duration of phonemes coming from the acoustics synthesizer as described in equation 3.\n$L_{dur} = (T - \\sum_{i=1}^{k} d_{i})^{2}$,\nWith $d_i$ equals i \u2013 th phonem predicted duration. The auxiliary phoneme loss uses the sequence of predicted probabilities over target phonemes. The sequence is describe as $P^{t} = \\{P_{1}^{t},...,P_{i}^{t}\\}$, and the ground as $P^{t} = \\{P_{1}^{t},...,P_{i}^{t}\\}$, and the cross Entropy as CE(...). Equation 4 describes the auxiliary phoneme loss.\n$L_{phn}(P^{t}, P^{t}) = \\frac{1}{T} \\sum_{i=1}^{p} CE(\\tilde{P}_{i}^{t}, P_{i}^{t})$.\nFinally equation 5 presents the overall loss.\n$L = L_{spec}(S_{t},S_{t}') + \\lambda_{dur}L_{dur} + \\lambda_{phn}L_{phn}(P^{t}, P^{t})$.\nExperimental results show that translatotron 2 outperformed translatotron 1 with a large margin of approximately +15.5 BLEU and obtained results that are similar to the baseline cascade. The proposed model can preserve the voice of each speaker in the training dataset. The translatotron 2 architecture was designed to address 3 performance bottlenecks observed in translatotron 1 namely:\n\u2022 Limitations of the auxiliary textual supervision during training, because it doesn't impact the S2ST directly.\n\u2022 Modeling translation alignment between 2 very long spectrogram sequences poses a serious challenge when using the attention mechanism.\n\u2022 Overgeneration and undergeneration are the robustness issues linked to attention-based speech generation."}, {"title": "Translatotron 3", "content": "One of the techniques used to acquire multilingual word embeddings is Multilingual Un-supervised Embedding (MUSE) described in Conneau et al., [9]. The embedding can be represented as two matrices X and Y if it is assumed that the learning process starts from two sets of word embedding trained on monolingual corpora. The two matrices $X\\in R^{c\\times M}; Y \\in R^{c \\times M}$ can be used to represent words, where M and c are the embedding parametters. M is the number of the most frequent words used for the computation of embedding with a dimension c. Training a MUSE model is about mapping W*, knowing that\n$W^{*} = arg \\min_{W\\in R^{c\\times C}} ||WX - Y||_{F} = UV^{'}, withU\\Sigma V^{T} = SVD (YX^{T})$.\nWhere $||.||_{F}$ is the Frobinus norm.\nEquation 6 is solved using an adversarial method. The elements randomly sampled from WX and Y are classified using a discriminator. To avoid the discriminator from inferring the original input W is updated. The discriminator is a classifier with a loss defined using equation 7.\n$L_{w} = \\frac{1}{M} \\sum_{i=1}^{M} \\{log P_{D}(source = 0|Wx_{i}) + log P_{D}(source = 1|y_{i})\\}$\nWith $P_{D}(.)$ represents the discriminator.\nTranslatotron 3 [3] is a recent approach used to design an unsupervised direct S2ST model that uses a shared encoder and a separated decoder for the source and target languages. A reconstruction loss, a MUSE embedding loss, and a S2S back translation loss are used to train the proposed model. The proposed model has two decoders one for the target $D_t$ and the other for the source $D_s$. Each decoder contains an acoustic synthesizer, a linguistic decoder, and a singular attention model. It also has an encoder E, to encode both target and source languages. The encoder used in translatotron 3 has the same architecture as the one used in translatotron 2 [2]. Equation 8 describes the encoding process.\n$E(S_{in}) = [E_{m}^{t}(S_{in}), E_{m}^{s}(S_{in})]$\nWith $S_{in}$ the source or the target language. $E_{m}^{t}(S_{in})$ the first half of the output is trained to a MUSE, it uses the MUSE loss. $E_{m}^{s}(S_{in})$ does not need the MUSE loss to be updated. The decoder is described in equation 9.\n$S_{out} = D_{out} (E(S_{in}))$,\nWhere $S_{in}$ and $S_{out}$ are the input and output of the spectrogram sequences.\nThe training process of the model focused on the reduction of the following losses:\n\u2022 MUSE loss is described by equation 10.\n$L_{MUSE}(S_{in}) = \\frac{1}{n} \\sum_{i=1}^{n} ||E(S_{in})_{i} - E_{i}||^{2}$,\n\u2022 Reconstruction loss in equation 11.\n$L_{recon} = L_{recon-spec}^{Src}(S^{s}, \\tilde{S^{s}}) + L_{recon-spec}^{tgt}(S^{t}, \\tilde{S^{t}}) + L_{LS2TC}(P^{s}, \\tilde{P^{s}}) + L_{tgt2Src}(P^{t}, \\tilde{P^{t}})$,\n\u2022 A back translation loss in equation 12.\n$L_{back-translation} = L_{back-translation}^{Src2tgt} + L_{back-translation}^{tgt2src}$,\n\u2022 The overall loss summarizing all loss is presented in equation13.\n$L_{BT-phase} = L_{back-translation} + L_{recon-phase}$."}, {"title": "Comparative study", "content": "The comparative study of the three versions of the Translatotron model is summarized in the following table.\nThis table clearly shows that translatotron 3 is the best option when designing a sequence-to-sequence S2ST model. When designing an English-to-Yoruba translation model with a health environment as a context, we believe translatotron 3 to be the best option."}, {"title": "Conclusion", "content": "This literature review presents past and recent developments in the field of S2ST. A particular focus was on direct translation models. Translatotron evolved from a simple proof of concept to an unsupervised model that is able to produce results comparable to the basedline and even better than the unsupervised basedline model. This work is a preliminary step toward designing an S2ST model able to translate between English and Yoruba in the medical environment. The choice of translatotron 3 to design a translation model between English and Yoruba, is based on the properties displayed and the fact that not all variants of Yoruba are actually formalized and can be easily transcribed."}]}