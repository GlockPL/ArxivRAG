{"title": "Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application", "authors": ["Gonzalo I\u00f1aki Quintana", "Laurence Vancamberg", "Vincent Jugnon", "Agn\u00e8s Desolneux", "Mathilde Mougeot"], "abstract": "This work studies the relationship between Contrastive Learning and Domain Adaptation from a theoretical perspective. The two standard contrastive losses, NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely used for Domain Adaptation. Our work shows that minimizing the contrastive losses decreases the CMMD and simultaneously improves class-separability, laying the theoretical groundwork for the use of Contrastive Learning in the context of Domain Adaptation. Due to the relevance of Domain Adaptation in medical imaging, we focused the experiments on mammography images. Extensive experiments on three mammography datasets synthetic patches, clinical (real) patches, and clinical (real) images - show improved Domain Adaptation, class-separability, and classification performance, when minimizing the Supervised Contrastive loss.", "sections": [{"title": "1. Introduction", "content": "Given a source data distribution or domain, we are often interested in transferring the representation learned to a different, albeit related, target domain. This is crucial for leveraging models pre-trained on large annotated datasets, as well as for adapting test and training distributions, which are generally different (de Mathelin et al., 2021). In particular, Domain Adaptation (DA) methods seek to minimize the effects of the domain shift to enable more efficient transfer. This is especially relevant in the medical imaging domain, where high data variability and limited access to large datasets pose significant challenges to the development of Deep Learning (DL)-based solutions, often hindering model generalization and performance across diverse clinical settings (Garrucho et al., 2022).\nContrastive Learning (CL) is a learning paradigm where semantically similar data-points are close to one another in the feature space, enabling to learn representations that are invariant given certain transformations. Intuitively, mapping data points from different domains to the same region in the feature space mirrors the DA problem. In addition CL separates the representations of semantically different data-points, which has been found to be beneficial for downstream task performance, like classification, detection, or segmentation. Contrastive Learning has been widely applied in the medical imaging domain (Chaitanya et al., 2020; Dong & Voiculescu, 2021; Cao et al., 2021; Quintana et al., 2024).\nInspired by the similarity of the tasks that Domain Adaptation and Contrastive Learning pursue, as well as by the growing interest in CL for DA, we analyze both paradigms to provide theoretical justifications for applying CL to DA. Due the relevance of Domain Adaptation in medical imaging, we conduct experiments on mammography images for classification tasks, specifically determining the presence or absence of breast cancer."}, {"title": "1.1. Related work", "content": "Domain Adaptation. Let $D_s = \\{X_s \\times Y_s, \\pi_s \\}$ be a source domain and $D_t = \\{X_t \\times Y_t, \\pi_t\\}$ a target domain, where $X_*$ is an instance or covariate space, $Y_*$ is the label space, and $\\pi_*: X_* \\times Y_* \\to \\mathbb{R}$ a joint probability measure. The target domain $D_t$ is typically unlabeled, i.e., $Y_t = \\emptyset$, contains fewer labels than $D_s$, or has a smaller dataset. In Domain Adaptation, we seek to transfer the representations learned in the source domain for solving a source task $T_s$ to the target domain, while considering that source and target tasks are the same. Various DA strategies have been proposed based on the nature of the domain shift (e.g., covariate shift, prior probability shift, concept shift), the availability of labels in the target domain (supervised, unsupervised, semi-supervised), and the type of models employed (e.g., shallow or deep architectures). In this work we consider the hidden covariate shift (de Mathelin et al., 2023) or covariate observation shift (Kull & Flach, 2014), a subtype of concept shift where it is assumed that there exists a non-linear transformation of the covariates that eliminates the shift. One"}, {"title": "2. Contrastive Learning and dissimilarity measures", "content": "Consider a learning problem with data from two labeled domains $D_0 = \\{X \\times Y, \\pi_0\\}$ and $D_1 = \\{X \\times Y, \\pi_1\\}$, with $\\pi_d : X \\times Y \\to [0, 1]$ the joint probability measure of the instances $x \\in X \\subseteq \\mathbb{R}^{n_1\\times n_2}$ and labels $y \\in Y$ of the $d$-th domain. We denote by $\\pi_x^d$ and $\\pi_y^d$ the marginal probability measures on the instances and labels, and by $\\pi_{x|y=c}^d$, the conditional probability measure on the instances knowing the label is $c$. We also consider the mixture domain $D_p = \\{X \\times Y, \\pi_p\\}$ with joint probability measure $\\pi_p := p \\pi_1 + (1 - p) \\pi_0$, where $p$ is the mixture probability. In this work, we mostly consider the equiprobable domains case $p = 0.5$.\nLet $\\phi : X \\to Z = \\mathbb{R}^m$ be a feature map parametrized by a neural network, where $m$ is the embedding dimension. $\\phi$ defines a Reproducing Kernel Hilbert Space (RKHS) wih kernel $k$ such that $k(x,x') = (\\phi(x), \\phi(x'))_z$, where $(\\cdot, \\cdot)_z$ denotes the inner product in $Z$."}, {"title": "2.1. Contrastive Learning", "content": "We recall the definitions of the Normalized Temperature-scaled Cross Entropy (NT-Xent) loss and the Supervised Contrastive loss.\nDefinition 2.1 (NT-Xent loss). Consider a batch of instances $B$ and their feature representation $z$, which are assumed of unitary norm, i.e., $||z||= 1$. The NT-Xent loss defined as:\n$L_{NT-Xent} = \\frac{1}{|B|-1} \\sum_{i=0}^{|B|} \\log \\frac{e^{z_i \\cdot z_{j(i)} / \\tau}}{\\sum_{l \\in A(i)} e^{z_i z_l / \\tau}}$ (1)\nwhere $z_i = \\phi(x_i)$ is the feature representation of instance $x_i$, $z_{j(i)}$ is the positive counterpart of feature $z_i$, and $A(i) = \\{0,...|B|-1\\} \\setminus \\{i\\}$ is the set of the indices of all features with the exception of $z_i$, and $\\tau$ is a temperature parameter.\nDefinition 2.2 (Supervised Contrastive loss). Given a batch of instances $B$, the Supervised Contrastive loss is defined as:\n$L_{SupContr} = \\frac{1}{|B|} \\sum_{i \\in B} \\frac{1}{|P(i)|} \\log \\frac{e^{z_i \\cdot z_{j} / \\tau}}{\\sum_{l \\in A(i)} e^{z_l z_l / \\tau}}$ (2)\n$j\\in P(i)$\nwhere $z_i = \\phi(x_i)$ is the feature representation of instance $x_i$, and $P(i) = \\{j \\in A(i) : y_j = y_i\\}$ is the set of indices of the positive counterparts of feature $z_i$."}, {"title": "2.2. Contrastive Learning and Domain Adaptation", "content": "In the following, we revisit the definition of the CMMD and establish its connection to contrastive losses.\nDefinition 2.3 (CMMD). Given two labeled domains $D_0$ and $D_1$, and the mapping $\\phi : X \\to Z$. The CMMD is defined as:\n$CMMD^2(D_0, D_1, \\phi) = [||E_{x \\sim \\pi_{x|y}^0} [\\phi(X)] - E_{x \\sim \\pi_{x|y}^1} [\\phi(X)]||_Z^2]$  (3)\n$E_{y}E_{x \\sim \\pi_{x|y}^0} \\\\ E_{x \\sim \\pi_{x|y}^1}$\nThe CMMD calculates the difference between the expected embedding of instances in the two domains, for each class. If $\\phi$ is adjusted so as to minimize the CMDD, then the embeddings $\\phi(X)$ are similar regardless of the domain, and the conditional distributions of the embeddings of the two domains will be matched. It can thus be seen as a measure of Domain Adaptation. Definition 2.3 corresponds to the definition of the Weighted Class-wise MMD (WCMMD) and encompasses other definitions of the CMMD found in the literature (Wang et al., 2021) as a particular case when all the classes have the same prior probability. We propose the following lemma that relates Contrastive Learning to the minimization of the CMMD (proof in Appendix C).\nLemma 2.4. In a high temperature regime, both the Supervised Contrastive loss and the NT-Xent loss can be expressed in terms of the CMMD by the following equation:\n$\\tau L_{Contr} \\approx \\frac{i}{4} CMMD^2(D_0, D_1, \\phi) + E_{x,x'\\sim\\pi^{0.5}} [k(X, X')]$\n$- \\frac{1}{2} (E_{x,x'\\sim\\pi^0} [k(X, X')] + E_{x,x'\\sim\\pi^1} [k(X, X')]) - \\frac{1}{2\\tau^2} E_{x\\sim\\pi^{0.5}} [Var_{x'\\sim\\pi^{0.5}} [k(X, X')]]$ (4)\n$+ O (\\frac{E_{x\\sim\\pi^{0.5}} [Var_{x'\\sim\\pi^{0.5}} [k(X, X')]]^2}{\\tau^4}) + \\log(|B|-1)$\nLemma 2.4 suggests that decreasing $L_{Contr}$ decreases the CMMD, which improves Domain Adaptation. Equation (4) also includes other terms, which can be analyzed as follows: A represents the similarity between all pairs of features, while B denotes the similarity between pairs of features from the same class and domain. C is a variance term. The constant term $\\log (|B|-1)$, with $|B|$ and $\\tau$ the batch size and the temperature of the Contrastive losses, is irrelevant for the optimization. The last term of Equation (4) refers to the approximation error of the Taylor series used to obtain the equation. The term $A - B/2$ can be interpreted the difference of the similarity between all the features, and the similarity between features of the same class and domain. In a nutshell, the contrastive loss compute the contrast with"}, {"title": "2.3. Contrastive Learning and class-separability", "content": "Extending on the work of Li et al. (2021), we can relate the contrastive losses (Supervised and NT-Xent) to an Inter-class MMD (IMMD) through the following lemma.\nLemma 2.5. By assuming that the kernel k is bounded, i.e., $|k(x,x')| < k_{max}, \\forall x, x'$, and that the inner product on $Y$ satisfies $(y, y')_Y = \\Delta \\mathbb{1}\\{y=y'\\} + l_0$, then the Contrastive losses bound the IMMD:\n$-\\frac{1}{2} IMMD^2 + \\gamma HSIC(X, X) + O (\\sqrt{Var [k (X, X')]}) \\leq L_{contr},$ (5)\nwith\n$IMMD^2 = E_{C_1,C_2 \\sim \\pi^{0.5}} [||E_{x\\sim\\pi_{x|C_1}^{0.5}} [\\phi(X)] - E_{x\\sim\\pi_{x|C_2}^{0.5}} [\\phi(X)]||_Z^2],$ (6)\nwhere HSIC(X, X) is the Hilbert-Schmidt Independence Criterion (Gretton et al., 2005), $\\alpha$ is a proportionality constant which depends on problem parameters, and $\\gamma \\in \\mathbb{R}$ is a constant satisfying $\\max\\{2, 2k_{max}\\} = (1+\\sqrt{1-4\\gamma})/(2\\gamma)$. For the Supervised Contrastive loss, $A_l = K$ (the number of classes). For the NT-Xent loss, $A_l = N$ (the number of instances).\nThe IMMD computes the difference between embeddings in the mixture domain with different class, and is thus a measure of class-separability. The HSIC is equal to the covariance in the feature space $Z$, and it thus measures a non-linear covariance between the instances given the map $\\phi$. We remark that the condition on $(y, y')_y$ is satisfied when considering one-hot vectors and the Euclidean inner product on the label space. Li et al. (2021) proved Lemma 5 for the NT-Xent loss. In Appendix D we extend the proof to the Supervised Contrastive loss.\nTo measure class-separability in the feature space we define another MMD-based quantity, the Different-class MMD (DCMMD), which is more general than the inter-class MMD of Equation (5). The DCMMD measures the difference between the features of two different classes, in the same and different domains.\nDefinition 2.6 (DCMMD). Given two labeled domains $D_0$,"}, {"title": "3. Numerical experiments", "content": "This section describes the datasets, models, and training settings used for the numerical experiments."}, {"title": "3.1. Datasets", "content": "Three types of mammography datasets are considered in this work: a clinical mammography image dataset (GEHC image dataset), a clinical mammography patch dataset (GEHC patch dataset), and a synthetic mammography patch dataset (synthetic patch dataset). The two clinical datasets contain GEHC images from anonymized patients, collected from a single institution in France following the EU General Data Protection Regulation. Additionally, two publicly-available datasets, CBIS-DDSM (Lee et al., 2017) and InBreast (Moreira et al., 2012) are also used in this work.\nGEHC image dataset. It contains 1300 cases, of which 197 are biopsy-proven cancers and 313 contain benign biopsied lesions. The remaining 790 are normal cases, which are studies in which no suspicious lesion was found in the breasts, and are confirmed by a one-year follow-up exam. The dataset is split in training (936 cases), validation (167 cases), and test (197) subsets in a stratified fashion, which takes into account the case pathology (benign or malignant), the lesions contained in the image (mass and calcification), and the description or sub-type of the lesions (e.g., spiculated mass, oval mass, granular calcification, etc.).\nGEHC patch dataset. Ten normal patches, and at least ten lesion 512 \u00d7 512 pixel patches are extracted from each image that contains a lesion (mass or calcification), with two different strategies: \u201cfixed\u201d and \u201crandom\u201d extractions. For every lesion, a \u201cfixed\u201d patch centered in the lesion centered is extracted. If the lesion is too large to be entirely contained in the patch, the space covered by the lesion is divided into a grid of N \u00d7 M non-overlapping patches, which are incorporated to the patch dataset. This assures that every part of the lesion is represented in the dataset but may introduce an undesirable bias, as most patches coming from"}, {"title": "3.2. Image style heterogeneity", "content": "In this work, we focus on studying the effect of training a DL model with data with different image styles. In particular, we use the sigmoid Look-Up Table (LUT) function, a contrast enhancement technique commonly used in mammography (Hern\u00e1ndez-V\u00e1zquez et al., 2024; on the Evaluation of Cancer-Preventive Interventions, 2016), as proxy transformation to define the two data distributions or domains. However, the methodology developed in this work is applicable to any other image style or contrast transformation. Figure 2 shows an example of the LUT application on a full mammography image.\nTwo distinct types of datasets are created from the base datasets for training and validation purposes, by splitting mammograms and patches at the case level:\n\u2022 An augmented dataset, which includes two versions of each image: one with the LUT applied and the other without. This dataset can be seen as equivalent to applying a LUT-based data augmentation.\n\u2022 Mixed datasets, where the original dataset was divided into two groups. One group had images with the LUT applied, while the other did not.\nFor the clinical images and patches, four mixed datasets are constructed. For the synthetic patches, a single mixed dataset is constructed, due to the simplicity of the problem and the less variability observed in the results. Models are trained using each of these datasets, and evaluated in the same test set, which contained the two versions of each image (with and without LUT)."}, {"title": "3.3. Model architecture and training methodology", "content": "A patch-based model is used for classifying mammography images, which consists in first training a path-classifier and then extending it to a whole image classifier by appending additional residual blocks and re-training on complete images (see Figure 3). In this work, DenseNet-121 (Huang"}, {"title": "4. Results", "content": "The numerical results are organized into three sections: first, an illustration of Lemma 2.4, followed by a quantitative analysis, and finally, a qualitative analysis."}, {"title": "4.1. Illustration of Lemma 2.4", "content": "Figure 4 shows the evolution of the individual terms in Equation (4) during training with synthetic patches. We observe that the CMMD and the similarity between all pairs, given by term A, decrease while minimizing the Supervised Contrastive loss, following Equation (4). The similarity between pairs of features with the same class and domain increases, as predicted by Equation (4).\nTo quantify the trends observed, we analyze the correlation between the derivatives of each of the terms, and the derivative of the Supervised Contrastive loss for different temperature values $\\tau$. This enables to numerically assess if the quantities are moving in the same, or opposite direction"}, {"title": "4.2. Quantitative analysis", "content": "Table 1 shows the results for the mixed datasets. Domain Adaptation is measured in terms of the CMMD and class-separability is measured by the DCMMD. Classification performance is evaluated with the accuracy and AUC for the binary whole image classifier, and with the accuracy, One vs. One (OvO) AUC, and One vs. Rest (OvR) AUC for the patch classifiers. We observe that in all the classification problems, the models trained with the Supervised Contrastive loss (SupContr+LCP, SupContr+CE) achieve higher Domain Adaptation and class-separability than the CE models. This translates into a higher downstream clas-"}, {"title": "4.3. Qualitative analysis", "content": "We now perform a qualitative evaluation of the extracted feature space. Figure 6 shows the t-SNE plot of the extracted features for clinical patch-classifier, trained with the three losses (CE, SupContr+LCP, SupContr+CE), with weights initialized from ImageNet (Figure 6a) and CBIS-DDSM (Figure 6b). When Transfer Learning from ImageNet is used, the CE model features are more separated by domain than by class, while the SupContr+LCP and SupContr+CE models are domain-invariant (Figure 6a). When Transfer Learning from CBIS-DDSM is used (Figure 6b) it can be seen that the CE model has already some degree of domain invariance, especially for non-normal classes. We hypothesize that, in this case, the similarity of the CBIS-DDSM dataset to the GEHC images is leveraged by the DL-model, virtually increasing the training dataset and increasing the robustness of the learned features. As will be seen later, this decreases the impact of Domain Adaptation on classification performance. The SupContr+LCP and SupContr+CE models attain domain invariance for all the classes, including Normal patches.\nFigure 7 shows the features t-SNE plot for the whole image classifier. It can be seen that the features of the CE model can be easily separated by domain, despite the features of the CE patch-classifier being domain-invariant for most classes (we recall that the whole image classifier was obtained by extending the patch-classifier, pre-trained on CBIS-DDSM). We hypothesize that this is caused by the maladaptation of the normal patches for the CE model in Figure 6b, as every mammography image contains many normal regions. On the other hand, the features of the SupContr+LCP and SupContr+CE models are domain-invariant."}, {"title": "5. Conclusions", "content": "In this work, we mathematically showed that minimizing two standard contrastive losses - NT-Xent loss and Supervised Contrastive loss - decreases the CMMD and thus performs Domain Adaptation. Moreover, it improves class-separability in the feature space, which is often associated to higher downstream task performance. These findings"}]}