{"title": "Self-Supervised Learning for Identifying Maintenance Defects in Sewer Footage", "authors": ["Daniel Otero", "Rafael Mateus"], "abstract": "Sewerage infrastructure is among the most expensive modern investments requiring time-intensive manual inspections by qualified personnel. Our study addresses the need for automated solutions without relying on large amounts of labeled data. We propose a novel application of Self-Supervised Learning (SSL) for sewer inspection that offers a scalable and cost-effective solution for defect detection. We achieve competitive results with a model that is at least 5 times smaller than other approaches found in the literature and obtain competitive performance with 10% of the available data when training with a larger architecture. Our findings highlight the potential of SSL to revolutionize sewer maintenance in resource-limited settings.", "sections": [{"title": "1. Introduction", "content": "The high expenses and labor-intensive process of gathering labeled data have driven researchers to seek innovative methods to train neural networks without annotations or with minimal annotated data. Self-Supervised Learning (SSL) emerges as an unsupervised learning strategy in which models learn to understand and represent data using their structure as the supervision signal (Ozbulak et al., 2023). The application of SSL techniques to computer vision has revolutionized the field, not only pushing the boundaries of unsupervised pretraining performance on popular benchmarks, such as ImageNet (Deng et al., 2009), but also leading researchers to adapt these methods to effectively tackle domain-specific challenges.\nSewerage infrastructure is one of the most costly in modern society, with traditional manual inspections required to identify defects. This process is limited by the number of qualified personnel and the time it takes to inspect each pipe (Haurum & Moeslund, 2021). Given these limitations, adopting an automated approach is both practical and necessary. However, the success of these methods depends on the availability of large amounts of labeled data, which is difficult to collect due to the shortage of inspectors. We recognize the necessity to create automated solutions without the need for vast amounts of labeled data.\nWe are the first to propose applying self-supervised learning to the domain of sewer infrastructure inspection. We introduce a straightforward approach that uses the DINO methodology that achieves competitive results with state-of-the-art methods without the need for complex implementations. Our approach not only demonstrates the adaptability of SSL in a specialized field but also sets the groundwork for future innovations in maintaining critical urban infrastructure.\nWe evaluate our approach on the Sewer-ML dataset (Haurum & Moeslund, 2021), a multi-label dataset that contains 1.3 million images and 17 different types of defects. This study demonstrates strong results (50.05 F2CIW and 87.45 F1 Normal) when fine-tuning with only 10% of the available data, significantly reducing the need for annotations. Additionally, we successfully trained a much smaller model compared to state-of-the-art methods, making it ideal for deployment on small devices for live detection and enhancing scalability in resource-limited settings."}, {"title": "2. Related work", "content": "Self-supervised learning. SSL methods can be broadly categorized as contrastive or non-contrastive based on how they avoid representation collapse (Balestriero et al., 2023; Ozbulak et al., 2023). Contrastive methods use positive and negative pairs to help the model distinguish between different instances by comparing similar and dissimilar examples (Chen et al., 2020; He et al., 2020). On the other hand, non-contrastive methods avoid explicit negative pairs and use strategies like clustering (Caron et al., 2020), distillation (Caron et al., 2021), redundancy reduction (Bardes et al., 2022), or masked image modeling (Assran et al., 2022; 2023) to ensure rich feature extraction.\nAmong the non-contrastive distillation methods, we highlight DINO (Caron et al., 2021) as it is part of our methodology. Self-distillation involves a teacher network generating pseudo-labels that a student network aims to replicate, encouraging the student to learn robust representations. The student and teacher networks share the same architecture and the teacher parameters are updated using an exponential moving average of the student ones, providing stable targets and preventing the model from collapsing to trivial solutions. We explain in detail how DINO is used within our approach in Section 3.\nRecent research on the application of self-supervision to domain-specific tasks has shown encouraging results. For instance, SSL has achieved state-of-the-art performance in pixel-wise anomaly localization (Li et al., 2021). Moreover, SSL has matched and surpassed the performance of clinical experts in medical imaging (Zhang et al., 2023; Azizi et al., 2023), has demonstrated superior performance in 3D facial image texture reconstruction (Zeng et al., 2021), and has successfully addressed label deficiencies in training the backbone network for an RGB-D object tracking problem (Zhu et al., 2024).\nSewer-ML literature. The Sewer-ML benchmark introduced state-of-the-art graph-based models such as KSS-Net (Wang et al., 2020), as well as popular vanilla architectures like ResNet-101 (Wu et al., 2019) and TResNet (Ridnik et al., 2020) (see Table 1). Despite their different methodologies, these approaches achieve very similar performance.\nSeeking to improve the presented baseline, Haurum et al. (2022a) proposed using a hybrid vision transformer combined with a Sinkhorn tokenizer (HViT-Sk). This method enhances model efficiency and accuracy by using CNN-generated feature maps as inputs to the ViT (Dosovitskiy et al., 2020) and employing the Sinkhorn tokenizer to eliminate redundancies. Building on this, they later proposed a multi-task learning approach (CT-GAT), where a common backbone network is jointly optimized by multiple task-specific GNN heads, resulting in a more robust and versatile inspection system (Haurum et al., 2022b).\nMoreover, Tao et al. (2022) combine features extracted by a graph-based module and a CNN with block attention modules. The graph-based module is used to capture the correlation information between labels. Similarly, Hu et al. (2023) worked on maximizing the defect-relevant information. They proposed a Self-Purification Module (SPM) that splits the feature representation space into the sum of two spaces: defect-relevant and defect-irrelevant features. They optimized the network using three loss terms: one to purify defect-relevant features, one to decorrelate defect-irrelevant features, and one to prevent collapse. Furthermore, Zhao et al. (2022) used Bayesian techniques to train an \"uncertainty-aware\" neural network (TMSDC)."}, {"title": "3. Methodology", "content": "3.1. Standard approach to SSL\nIn computer vision, self-supervised learning teaches neural networks to understand images using unlabeled data. This is accomplished by generating multiple random augmentations of the same image and training the model to recognize that these different views all originate from the same source. This is referred to as the pretext task and aims to teach the model to generate similar embeddings for similar inputs and dissimilar embeddings for dissimilar ones.\nMathematical definition. Let $f_\\theta$ be an encoder backbone with parameters $\\theta$ that produces vector representations $r$ from augmented views $x_t$ of an image $x$ produced by a stochastic function $T(x) = x_t$. Representations $r$ can be mapped to projections $z$ and predictions $z'$ using projector $g_\\gamma$ and predictor $q_{\\pi}$ functions, where $g_\\gamma(f_\\theta(x)) = z$ and $q_{\\pi}(g_\\gamma(f_\\theta(x))) = z'$. In this context $g_\\gamma$ and $q_{\\pi}$ are MLPs.\nLike other popular self-supervised approaches (Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Bardes et al., 2022), DINO employs a projection head on top of the encoder backbone, with the loss being computed on the projector's output. The projector function acts as an informational bottleneck, ensuring that the backbone's representations are not overly biased to merely comply with the self-supervised learning objective (Chen et al., 2020).\nThis comprises the intuition behind self-supervised pre-"}, {"title": "3.2. Implementation details", "content": "Architecture. For the self-supervised pretraining, we used the DINO methodology. For the encoder backbones, we used the ViT Tiny (ViT-T/16) and ViT Small (ViT-S/16) models, which primarily differ in the number of parameters\u20145.5M and 21.6M respectively\u2014and computational complexity, with ViT-T/16 having 192 hidden layers and 3 heads, and ViT-S/16 having 384 hidden layers and 6 heads.\nThe projector of the models comprised an MLP with two hidden layers of size 2048 and an output layer of size 256. The loss was computed with respect to 32,768 prototypes. For other DINO hyperparameters, we adhered to the recommendations in the original paper (Caron et al., 2021). The training was performed using Pytorch 2.0.2 (Paszke et al., 2019) on 16 Tesla T4 GPUs, using the maximum batch size that could fit into memory for each model. Our code development was greatly inspired by the solo-learn library (da Costa et al., 2022).\nGlobal views instead of multi-crop. Sewer-ML is a multi-label dataset where defects vary in shape and size. To avoid matching local views with fewer defects (or none) to global views containing the full image, we did not perform multi-crop. This decision was made to prevent potential mismatches in embeddings and to avoid hindering the neural network optimization during pretraining.\nOptimization. The experiments for pretraining were conducted over 35 epochs using the AdamW optimizer. The base learning rate was set to 5\u00d710\u22125\u00d7batch_size/256. A linear warmup starting at 3 \u00d7 10-5 was applied for the first 10 epochs, followed by a cosine scheduler with no restarts. The base and final decay rates (T) were 0.996 and 0.999, respectively, with a minimum learning rate of 1 \u00d7 10-6.\nFor fine-tuning, we took the pretrained backbone and placed an untrained classifier head on top of it. The experiments were run for 45 epochs using the AdamW optimizer, with a base learning rate of 5 \u00d7 10-4 \u00d7 batch_size/256. A multistep scheduler with a gamma of 0.1 was used, with step milestones at epochs 15 and 35.\nLoss function and positive weights. Given the unbalanced nature of the dataset and the superior importance of recall over precision in the benchmark metrics, it is necessary to craft a custom-weighted loss to effectively address the task. We optimized the model with respect to a binary cross-entropy loss with positive weighting. The co-"}, {"title": "3.3. Sewer-ML benchmark metrics", "content": "To assess the performance of the multi-label benchmark, we use the proposed metrics. A weighted F2 metric (F2CIW) for defect prediction and a regular F1 score (F1Normal) for non-defect predictions (Haurum & Moeslund, 2021). The weights for the F2 metric are assigned to each defect class based on their economic impact. Moreover, the F2 score is employed to prioritize recall over precision since missing a defect has a greater economic impact than generating a false positive."}, {"title": "4. Results", "content": "We conducted several experiments to evaluate our models. These experiments include reporting metrics for the pretrained architectures by (i) training a linear classifier on top of the frozen backbone, (ii) fine-tuning the models using 10%, 50%, and 100% of the data, and (iii) pretraining the models using a hybrid approach that incorporates both self-supervised and supervised losses. For comparison purposes, we also trained the models in a fully supervised setting. All experiments were performed using the ViT-T/16 and ViT-S/16 architectures.\nPerformance. Our experiments with the ViT-S model demonstrate its robustness across varying data levels. When using 100% of the data for fine-tuning, its performance was on par with state-of-the-art methods. Using 50% of the data, ViT-S performed nearly as well as when using the full dataset. Even with just 10% of the data, the model showed solid baseline performance, proving effective in data-scarce scenarios (see Table 2). For both architectures, the hybrid approach enhanced non-defect detection but demonstrated limited performance for identifying defects. We hypothesize that the self-supervised signal enabled the model to encode richer representations of non-defective pipes. However, this also limited the feature exploitation of the supervised loss, affecting defect detection results."}, {"title": "Simplicity and effectiveness of the approach.", "content": "Current methods often require specialized knowledge and extensive labeled data. In contrast, our approach is straightforward, involving only pretraining and fine-tuning, which are standard practices in transfer learning, as well as requiring significantly fewer labels due to our use of self-supervision methodologies. This simplicity not only makes our method more accessible but also offers greater adaptability, allowing for effective performance with less labeled data while still achieving comparable results to more complex methods."}, {"title": "Informational content.", "content": "We employed the RankMe metric (Garrido et al., 2023) to monitor the informational content of representations during pretraining. A higher value"}, {"title": "5. Conclusions", "content": "Our research demonstrates the effective application of self-supervised learning to the domain of sewer infrastructure inspection, specifically in defect detection, a field traditionally reliant on labor-intensive and costly manual inspections. This approach not only achieves high-performance results with minimal labeled data but also provides a scalable and cost-effective solution for urban infrastructure maintenance.\nEven when fine-tuning with only 10% of the available data, our research achieves notable results. We propose deploying a smaller model in production\u2014approximately 20% the size of state-of-the-art models\u2014that delivers robust performance. This approach reduces the need for extensive labeling and optimizes model size for on-device scalability in live detection. Although not the primary focus of this study, we observed that the ViT-T/16 model performs well in a fully supervised setting, which is a promising result considering its compact architecture.\nFor future research, it is essential to investigate the potential of various self-supervised learning methods that have not yet been applied to sewer infrastructure inspection, particularly by assessing their performance in low-data, low-compute environments. While Sewer-ML is a curated dataset, it may not fully reflect the complexities of real sewer inspections, particularly the defect-to-non-defect ratio. Therefore, the proposed method might not be immediately applicable out-of-the-box and may require extensive experimentation with other self-supervised learning techniques. Nevertheless, training a foundational model on sewer pipes offers the novel potential for transferability to a broader range of tasks within this industry."}, {"title": "A. Image Augmentations", "content": "During self-supervised pretraining, we employed several augmentations to enhance the diversity of the training dataset. Specifically, we applied random crops and resized the images to 224x224, using a scale ranging from 0.5 to 1.0 and bicubic interpolation. We applied color jitter to adjust the brightness, contrast, saturation, and hue of the images. Additionally, we included random grayscaling with a probability of 0.15, also random Gaussian blurring with a probability of 0.3 and a sigma ranging from 0.1 to 1, and finally random equalization and solarization with a probability of 0.3. Horizontal flipping was performed randomly. Finally, all images were normalized. During validation, the images were only resized and normalized.\nWe used a slightly different image augmentation pipeline for fine-tuning. Instead of performing random crops, we used full image resizes. We keep augmentations like color jitter, random horizontal flip, and normalization, consistent with the pretrain augmentations. We replaced the remaining transformations with random equalizing and random autocontrasting. We also incorporated random affine augmentations with a rotation limit of 5 degrees and applied random erasing with a scale ranging from 0.01 to 0.05 and a ratio ranging from 0.1 to 1. Validation augmentations remained the same as for pretraining."}], "equations": ["pos_weightc =2\u00d71+\\frac{CIWC}{\\sum_{i=1}^{C}CIWC}(1."]}