{"title": "WHAT MATTERS FOR MODEL MERGING AT SCALE?", "authors": ["Prateek Yadav", "Tu Vu", "Jonathan Lai", "Alexandra Chronopoulou", "Manaal Faruqui", "Mohit Bansal", "Tsendsuren Munkhdalai"], "abstract": "Model merging aims to combine multiple expert models into a more capable single\nmodel, offering benefits such as reduced storage and serving costs, improved gen-\neralization, and support for decentralized model development. Despite its promise,\nprevious studies have primarily focused on merging a few small models. This\nleaves many unanswered questions about the effect of scaling model size and how\nit interplays with other key factors-like the base model quality and number of\nexpert models-, to affect the merged model's performance. This work system-\natically evaluates the utility of model merging at scale, examining the impact of\nthese different factors. We experiment with merging fully fine-tuned models us-\ning four popular merging methods\u2014Averaging, Task Arithmetic, Dare-TIES,\nand TIES-Merging\u2014across model sizes ranging from 1B to 64B parameters and\nmerging up to 8 different expert models. We evaluate the merged models on both\nheld-in tasks, i.e., the expert's training tasks, and zero-shot generalization to un-\nseen held-out tasks. Our wide range of experiments provide several new insights\nabout model merging at scale and the interplay between different factors. First,\nwe find that merging is more effective when experts are created from strong base\nmodels, i.e., models with good zero-shot performance, compared to pre-trained\nones. Second, larger models facilitate easier merging. Third merging consistently\nimproves generalization capabilities. Notably, when merging eight large expert\nmodels, the merged models often generalize better compared to the multitask\ntrained models. Fourth, we can better merge more expert models when working\nwith larger models. Fifth, different merging methods behave very similarly at larger\nscales. Overall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.", "sections": [{"title": "1 INTRODUCTION", "content": "Model merging (Raffel, 2021) refers to the process of combining two or more constituent (expert)\nmodels to produce a new, and potentially more powerful model. The appeal of this technique is\nrooted in several benefits it can confer: first, it dramatically reduces storage and serving costs by\nreusing a single model across tasks; second, it enables compositional combination of capabilities\nfrom expert models, which can improve generalization to novel tasks; and third, merging supports\ndecentralized and modular model development by allowing multiple contributors to independently\nbuild models and later combine them together.\nThese characteristics have led to a great deal of recent efforts in developing cost-effective model\nmerging methods (Matena & Raffel, 2022b; Ilharco et al., 2022; Jin et al., 2022; Yadav et al., 2024b;\nYang et al., 2023; Yu et al., 2024d; Shah et al., 2023; Tam et al., 2023; Zhao et al., 2024), often using\nsimple arithmetic operations, such as averaging the parameters of the constituent models. However,\nmost of these studies are limited to small-scale experiments with relatively small models (typically\n< 7B parameters) and merging 2 or 3 experts (Yu et al., 2024a;c), and mainly focus on improving\nbenchmark performance on held-in tasks that the expert models were trained on (Yu et al., 2024a;\nYadav et al., 2024b). Despite the promises that model merging holds, the research community still"}, {"title": "2 BACKGROUND", "content": "Model merging has emerged as a cost-effective method for developing improved models. Two\ncommon use cases of merging are: (1) combining model checkpoints from different data versions,\nhyperparameters, or training stages to enhance distributional robustness (Team et al., 2024; Dubey\net al., 2024), and (2) combining multiple expert models trained on different datasets to leverage\ntheir complementary capabilities. In both scenarios, the expert models generally share a common\narchitecture and a base model from which the expert models are created via fine-tuning.\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to\nenhance its capabilities. Each expert model is trained on distinct datasets covering different tasks,\ndomains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as\n\"held-in\", while those that are new and unseen are called \u201cheld-out\". Our goal is to create a unified\nmodel that retains the individual expert models' capabilities on held-in tasks while improving zero-\nshot generalization on held-out tasks. This merging approach provides a flexible, modular method\nfor post-training large language models, facilitating the addition of new features and capabilities to\ntop-performing models."}, {"title": "2.1 MODEL MERGING METHODS", "content": "We denote the set of N expert tasks as $t_1,..., t_N$ and the base model weights, representing the\ncommon ancestor of all expert models as $\\theta_{\\text{base}}$. The weights of the corresponding specialized\nexpert models, each obtained by fully fine-tuning the base model on a specific expert task, are\ndenoted as $\\theta_1, ..., \\theta_N$, respectively. We focus on \u201copen vocabulary\" models which utilize natural\nlanguage as input and output for both classification and generation tasks, eliminating the need for\ntask-specific classification heads making the merging process simpler. Given this, model merging\nmethods can be defined as a function M(.). This function takes as input the base model, the\nset of N expert models, and potentially additional information, denoted by \u03a6. This additional\ninformation may include activation statistics, Fisher matrices, or other method-specific data. The\noutput of the function is the merged model, represented by its parameters $\\theta_m$. Formally, $\\theta_m =$\nM($\\{\\theta_i\\}_{i=1}^N$, $\\theta_{\\text{base}}$, \u03a6), where \u03a6 is method specific data.\nGiven our focus on studying model merging with large models, we select four merging methods\nbased on their popularity and simplicity. We only study merging methods that can scale to tens\nof billions of model weight parameters and do not require any additional information to perform\nmerging, i.e., \u03a6 = {}, as these techniques are efficient for even larger models. Other more complex\nmethods that require computing fisher matrices (Matena & Raffel, 2022a), backward passes (Yang\net al., 2023), or additional information like model activation (Jin et al., 2023) are skipped because of\ntheir computational complexities for large scale model merging that we focus on in this work. Next,\nwe describe the four selected model merging methods in detail."}, {"title": "2.1.1 AVERAGING", "content": "Parameter averaging (Choshen et al., 2022b; Wortsman et al., 2022a) is a well-established technique\nin federated learning (McMahan et al., 2017) and recent applications extend its utility to merge\nmodels for enhancing model robustness against out-of-distribution data (Wortsman et al., 2022b;\nRam\u00e9 et al., 2022a), refine pre-trained models (Yu et al., 2024a), develop multimodal models (Sung\net al., 2023), and create multitask models by combining capabilities (Yadav et al., 2024b; Ilharco et al.,\n2022). Parameter averaging is achieved by taking a mean of all the expert model weights together\nwithout using the base model which can be formally described as, M($\\{\\theta_i\\}_{i=1}^N$, $\\theta_{\\text{base}}$) = $\\frac{1}{N}\\sum_{i=1}^N \\theta_i$."}, {"title": "2.1.2 TASK ARITHMETIC", "content": "Task Arithmetic (Ilharco et al., 2022) introduces a novel concept of \u201ctask vectors\" for model merging.\nFor task $t_i$, the task vector is denoted as $\\tau_i$ = $\\theta_i$ - $\\theta_{\\text{base}}$ which captures task-specific knowledge by\nquantifying the difference between the fine-tuned expert parameters ($\\theta_i$) and the original base model\nparameters ($\\theta_{\\text{base}}$). A scaling hyperparameter \u03bb controls the contribution of the aggregated task-\nspecific knowledge to the final model. The merged model is then constructed by linearly combining\nthe base model parameters with a scaled sum of all task vectors. Formally, task arithmetic can be\ndescribed as, M($\\{\\theta_i\\}_{i=1}^N$, $\\theta_{\\text{base}}$; \u03bb) = $\\theta_{\\text{base}}$ + \u03bb* $\\sum_{i=1}^N (\\theta_i$ - $\\theta_{\\text{base}})$."}, {"title": "2.1.3 TIES MERGING", "content": "TIES-Merging (Yadav et al., 2024b) identifies two main challenges with model merging: 1 during\nfinetuning expert models accumulate a lot of noise in the parameters, and 2 different experts might\nwant to change the same parameter in different directions leading to interference/conflict between\nthe expert models. They demonstrate that both of these factors hurt model merging and propose a\nthree steps process to remove redundant parameters, followed by resolving sign conflicts, and finally\naggregating only the parameters that are not conflicting. Specifically, in TIES Merging they first\nzero out the values in each task vector that have low magnitudes to obtain the trimmed task vector\n$\\hat{\\tau}_i$ for each task. Next, they chose the aggregate sign (m) for each parameter based on whether the\nparameter has a higher total magnitude in the positive or the negative direction across all trimmed\ntask vector, formally, m = sgn($\\sum_{i=1}^N \\hat{\\tau}_i$). Finally, for each parameters p the models whose sign\nmatches the aggregate sign are averaged to obtain the merged task vector. Finally, the merged model\nis obtained by scaling the merged task vector using a hyperparameter \u03bb and then added back to the\nbase model as, $\\theta_m = \\theta_{\\text{base}} + \u03bb* \\big[\\frac{1}{|A^P|}\\sum_{i\u2208A^P} \\hat{\\tau}_i \\big]$, where $A^P$ = $\\{i \u2208 [N] | \\sigma_i^P = \\sigma^P\\}$."}, {"title": "2.1.4 DARE MERGING", "content": "Dare (Yu et al., 2024a) extends the idea of TIES merging by proposing to use a dropout-like pruning\nstage to remove noise before merging. Specifically, a Bernoulli mask $M_i$ with drop probability p\nis applied to each task vector to obtain the pruned task vector $\\hat{\\tau}_i$ = $(1 - M_i) \\tau_i/(1-p)$. This\nstochastic process randomly zeroes out elements within the task vector while preserving its expected\nvalue. These pruned task vectors are then used along with either TIES Merging or Task Arithmetic.\nDue to the popularity of the Dare variant that uses TIES Merging, we use that to represent the Dare\nmethod and call it Dare-TIES."}, {"title": "2.2 CHALLENGES/LIMITATIONS", "content": "Model Merging has been utilized at a growing rate in practice as it has recently been applied to\nbuilding modern language models like Llama-3 (Dubey et al., 2024) and Gemma-2 (Team et al., 2024).\nHowever, most formal studies on model merging have been performed with relatively small models.\nThere are a few studies that look at larger models with 7B and 13B parameters. However, those\nstudies mostly focus on merging 2-3 models to improve benchmark numbers as opposed to better\nunderstanding how the size of the model affects the model merging process and the resultant model.\nTo motivate our work, we present some of the limitations of the existing studies and highlight their\ndifference with our work.\nMost Studies on Small Models (< 7B parameters): Almost all existing model merging papers\nrarely use large models (> 7B). For example past works (He et al., 2024; Daheim et al., 2023;\nOrtiz-Jimenez et al., 2024; Jang et al., 2024), including popular methods like ModelSoup (Wortsman\net al., 2022a), Task Arithmetic (Ilharco et al., 2023) and TIES-Merging (Yadav et al., 2024b),\nRegMean (Jin et al., 2023), Fisher-Merging (Matena & Raffel, 2022a) Ada-Merging (Yang et al.,\n2023), MatS (Tam et al., 2024) perform experiments with model families like CLIP (Radford\net al., 2021), ViT (Dosovitskiy et al., 2021), T5 (Raffel et al., 2020a), DeBERTa (He et al., 2021),\nRoberta (Liu et al., 2019), BERT (Devlin et al., 2018) with less than 1B parameters. Hence, it is\nunclear how well model merging works for large models, what factors play an important role, the\neffect of model size, number of tasks being merged, and its effect on both held-in performance and\ngeneralization of the model. Some studies hypothesize that bigger models might be easier to merge\nhowever there are no concrete large scale studies to thoroughly assess such claims at large scale.\nModel Merging Studies with Large Models are Shallow: Some recent works like DARE (Yu\net al., 2024a), WIDEN (Yu et al., 2024c), Chat-Vector (Huang et al., 2024b) demonstrate merging\nresults for larger models with up to 13B parameters, however these studies have a few limitations:\nThey primarily focus on using model merging to improve model quality and hence their experiments\ndo not provide concrete insights on how model size interplays with merging, They only merge\na maximum of two or three models at once, They primarily focus on held-in tasks and do not\nprovide any insights on the effect of merging on a model's generalization abilities. Other works like\nRewardSoup (Rame et al., 2024), WARM (Rame et al.), WARP (Ram\u00e9 et al., 2024), FuseLLM (Wan\net al., 2024a), FuseChat (Wan et al., 2024b) also work with ~ 7B sized models and focus on specific\napplications of model merging without providing any deeper insight about how merging performance\nchanges for large models.\nVaried Evaluation Setups: Most previous works rarely share their experimental setup where\nboth the expert datasets and the objective vary. For example, RegMean (Jin et al., 2023), Task\nArithmetic (Ilharco et al., 2023), TIES (Yadav et al., 2024b), MaTS (Tam et al., 2024) uses GLUE\ntasks (Wang et al., 2018), Vision tasks, TO held-out, and TO held-in (Sanh et al., 2021b) tasks\nrespectively. Moreover, different works evaluate for different use cases like intermediate task training\nin Fisher merging (Matena & Raffel, 2022a), robustness in modelsoups (Wortsman et al., 2022a),\nand held-in performance for Dare (Yu et al., 2024a), both held-in and held-out performance in TIES\nMerging (Yadav et al., 2024b). Given our focus on combining model capabilities in the post training\nphase, we focus on evaluating on both held-in tasks and generalization to unseen held-out tasks."}, {"title": "3 LARGE SCALE EVALUATION OF MODEL MERGING", "content": "In this work, we address the limitations mentioned above by systematically understanding the effect\nof various factors like model size, base model quality, merging method, and the number of models\nbeing merged on both the held-in and generalization performance of the final merged model. Next,\nwe describe our experimental design.\nData: Sanh et al. (2021a) found that explicit multitask training of T5 (Raffel et al., 2020b) on a\ncollection of prompted datasets produces a model with strong zero-shot performance on unseen tasks.\nThis has become a common experimental setting for benchmarking zero-shot generalization (e.g.\n(Longpre et al., 2023; Jang et al., 2023; Zhou et al., 2022; Chung et al., 2024; Muqeeth et al., 2024).\nHence, we adopt the experimental setting from the T0 mixture (Sanh et al., 2021a) which contains\n8 held-in and 4 held-out task categories. For each of these categories there are multiple datasets\nin the TO mixture (Sanh et al., 2021b) and hence to reduce evaluation costs, we select 2 datasets\nfrom each category based on the popularity and the train dataset size. Specifically, the 8 held-in task\ncategories (with a total of 16 datasets) include Multiple-choice QA, Extractive Qa, Closed-Book\nQA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase\nIdentification. Similary, the 4 held-out task categories (with a total of 7 datasets) are Sentence\nCompletion, Natural Language Inference, Coreference Resolution, and Word Sense Disambiguation.\nFor more details see Section A.\nExpert Model Creation: Recognizing the significance of post-training for LLMs where models\nare typically fully fine-tuned, we perform full fine-tuning to create our expert models to better mimic\nthe post-training setting. Moreover, in post-training phases it is common to first perform Instruction\nTuning (IT) on the model before moving on to other steps. Hence, we examine the effect of using\nstrong instruction-tuned base models on the process and outcome of model merging. Given this, we\nutilize the PaLM-2 models (Anil et al., 2023) with sizes 1B, 8B, 24B, and 64B as our base models\n($\\theta_{\\text{base}}$). To obtain the instruction tuned base model, we further fine-tuned the PaLM-2 models on the\nFLAN-v2 dataset (Longpre et al., 2023) while excluding the T0-mixture tasks (Sanh et al., 2021a).\nThese instruction-tuned variants are denoted as PaLM-2-IT. For each of the 2 base model types\n(non-IT vs IT) and 4 model sizes, we perform full fine-tuning on the 8 held-in task categories resulting\n64 specialized experts models which are then used further in our experiments. Comprehensive details\nregarding hyper parameters and computational requirements are provided in Appendix B.\nExperimental Setting: Given our collection of expert models, for each merging experiment we\nselect a subset of expert models which we call the constituent models. We create a large merging\nexperiment grid with 2 base models (PaLM-2 and PaLM-2-IT), four model sizes (1B, 8B, 24B, 64B),\nfour Merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the number of constituent\nmodels (2, 4, 6, 8), and 3 seeds to randomly select the constituent tasks for the experiment resulting\nin a total of 384 merging experiments. These seeds are shared across different experimental settings\nto ensure the same tasks are selected across base models, model sizes and merging methods to ensure\nfair comparison. For example, in an experiment we merged 2 expert models, derived from the 64B\nPaLM-2 base model with the constituent models being MCQ and Summarization experts while the\nsame experiment with a different seed resulted in Closed Book QA and Sentiment Analysis experts\nas the constituent models.\nEvaluation: For each of the experiments above, we assess the merged model's performance by\nevaluating it on both the held-in tasks \u2013 i.e., the training tasks of the constituent expert models \u2013 and\nall 4 held-out task categories. For example, if the constituent models are MCQ and Summarization\nexperts, then for held-in tasks we evaluate on the MCQ datasets (DREAM and Cosmos QA) and\nSummarization datasets (CNN Daily Mail and XSum) resulting a total of 4 held-in evaluation datasets.\nMoreover, all merging experiments are also evaluated on the 4 held-out tasks categories consisting of\n7 datasets listed in Appendix A. There we perform approximately ~ 9000 model evaluations across\nall of our experiments.\nMetric: Given that different datasets use different metrics, we normalize the performance metrics to\nmake them unitless so that they can be aggregated. For held-in tasks, the merged model's performance\nis normalized against the corresponding task expert model's performance. However, for held-out"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "In this section, we explore the interplay between model size and key factors such as base model quality,\nmerging method, and the number of constituent (expert) model, along with their effect on both held-in\nand zero-shot generalization (held-out) performance. Our findings are: Merging is more effective\nwhen the constituent models are derived from instruction-tuned base models rather than pretrained\nones (see \u00a74.1); Larger models facilitate easier merging (\u00a74.2); Merging significantly improves\nzero-shot generalization, with instruction-tuned models benefiting from increased constituent models,\nand larger model sizes allowing the merged model to match or exceed multi-task training (\u00a74.3);\nWe can merge more models effectively when using larger models (\u00a74.4); and Different merging\nmethods perform similarly when applied to large-scale instruction-tuned models. Below, we outline\nthe experimental setup and discuss these findings in detail."}, {"title": "4.1 INSTRUCTION-TUNED MODELS FACILITATE EASIER MERGING", "content": "Experimental Setup: Prior research suggests a connection between robust zero-shot models and\neffective model merging. Wortsman et al. (2022a) demonstrate that averaging strong zero-shot\nmodels improves out-of-distribution robustness. Ortiz-Jimenez et al. (2024) indicate that effective\npretraining allows for weight disentanglement, and thus enhancing merging. Other studies (Yadav\net al., 2024b; Ilharco et al., 2023) propose that strong base models could aid in model merging, though\nthis hypothesis remains largely untested.\nTo assess how base model quality affects the held-in performance of merged models, we perform\nmerging experiments with fully fine-tuned experts from PaLM-2 and PaLM-2-IT. We vary model\nsizes in {1B, 8B, 24B, 64B} and the number of constituent models in {2, 4, 6, 8}. Held-in performance\nis measured over three trials to minimize the impact of selected expert models and their data\ndistributions. A consistent seed is used across different base models, model sizes, and merging\nmethods to ensure fair task comparisons. We evaluate four merging methods: averaging, task\narithmetic, TIES, and Dare-TIES, and also compare against the performance of task-specific expert\nmodels.\nFindings: Our results, presented in Figure 3, indicate that PaLM-2-IT models denoted by green\ncolor (\u2022), consistently outperforms PaLM-2 models (\u2022) across various merging methods (\u2022, \u25b2, \u25c6, *),"}, {"title": "4.2 MODEL MERGING BECOMES EASIER WITH BIGGER MODELS", "content": "Experimental Setup: In this section, we explore the effect of model size on the held-in performance\nof merged models. We run experiments using different model sizes, base models, merging methods,\nand numbers of constituent models. As in the previous experiment, we report the average results over\nthree random seeds and compare the performance of the merged models to that of the task-specific\nexpert models.\nFindings: Figure 4 illustrates how increasing base model size impacts merging effectiveness. As\nmodel size grows (denoted by colors, \u25a0), merged model performance generally improves.\nThis positive trend is consistent across all base models (different subplots), merging methods (x-axis\n\u2192), and numbers of constituent models (subplots). For large instruction-tuned PaLM-2-IT models,\nthe merged models perform nearly as well as task-specific expert models denoted by dashed line.\nThese results demonstrate that larger models facilitate merging. This suggests a promising approach\nfor developing adaptive, modular post-training recipes. If the remaining performance gap can be\nfurther reduced, model merging could become a cost-effective alternative to multitask training. Our\nfull results across settings are available in the Appendix C."}, {"title": "4.3 MERGED MODELS AT SCALE GENERALIZE BETTER", "content": "Experimental Setup: Expert models are created by fine-tuning our base model on specialized\ntasks, which can lead to a decrease in its generalization capabilities. This raises the question: How\nwell, if at all, can the merged model generalize to held-out tasks? Ideally, the merged model should\nperform at least as well as the base model on these tasks. To explore this, we evaluate the merged\nmodel's performance on unseen tasks across various model sizes, merging methods, and numbers\nof constituent models. Additionally, we compare our merging approach to a traditional multitask\nbaseline, where a single model is trained on a mixture of all eight held-in task categories. As detailed\nin Section 3, we normalize the performance of both the merged and multitask model against the base\nmodel to assess relative gains or losses in generalization abilities.\nFindings: Figure 2 and Figure 5 show the zero-shot generalization performance of the merged\nmodel using PaLM-2-IT and PaLM-2, respectively. Overall, we find that: The merged models\noutperform their corresponding base models in zero-shot generalization to held-out tasks, as indicated\nby performance values greater than 1 in most cases; This improvement is consistent across\nvarious model sizes (denoted by subplot), base models (different figures), merging methods (different\ncolors,), and numbers of constituent models (on x-axis \u2192), suggesting that merging generally\nimproves generalization; \u2192 For weak base models (i.e., PaLM-2) illustrated in Figure 5, the number\nof constituent expert models had little effect on zero-shot generalization (Left and Center plots).\nHowever, increasing model size significantly improved the merged model's performance over the base\nmodel (Right plot); In contrast, strong base models (PaLM-2-IT) show a different trend, zero-shot\ngeneralization monotonically improves with the addition of more expert models as shown in Figure 2.\nWe hypothesize this positive correlation arises from reduced model noise through the inclusion of\nmultiple experts, resulting in better generalization; and \u2192 Notably, our merged model outperforms\nthe multitask baseline when combining more than 6 large instruction-tuned expert models (over\n24B). This indicates that models developed through merging can generalize even better than those\ntrained on a multitask mixture, offering a promising approach for developing highly capable language\nmodels. Our full results on other merging methods and model size are available in Appendix C."}, {"title": "4.4 BIGGER MODEL SIZES CAN MERGE MORE EXPERTS", "content": "Experimental Setup: When creating multitask models, datasets for different tasks or domains are\ntypically combined. In contrast, model merging involves developing separate expert models for each\ntask or domain before combining them. Previous work has shown that merging multiple models can\nreduce the quality of the resulting model (Yadav et al., 2024b; Ilharco et al., 2022). In this study, we\nexperiment with merging up to 8 expert models from various base models, model sizes, and merging\nmethods to assess their impact on successful merges.\nFindings: Figure 6 shows the held-in and held-out performance of the merged models using\nTask Arithmetic as the number of constituent models increases shown on x-axis. Results for other"}, {"title": "4.5 MERGING METHODS BECOME SIMILAR AT SCALE", "content": "We find that all merging methods exhibit similar\nperformance when merging large instruction-\ntuned models. This suggests that simpler meth-\nods, such as Averaging, can be sufficient for\nmerging powerful large expert models. Figure 7\nshows the held-in and held-out performance of\nthe 64B experts derived from PaLM-2-IT. All\nmerging methods yield comparable results on\nboth held-in and held-out tasks for any number\nof constituent models (shown on x-axis). We\nhypothesize that as model size increases, expert\nmodels are highly over-parameterized due to\nlimited training data. Consequently, the subtle\nadvantages of certain merging techniques \u2013 such\nas highlighting information via task vectors (Il-\nharco et al., 2022), resolving interference (Ya-\ndav et al., 2024b), or pruning (Yu et al., 2024a)\n- which benefit smaller models, become less rel-\nevant. This indicates a need for more practical\nand scalable methods to improve merging at scale."}, {"title": "4.6 DISCUSSION AND TAKEAWAYS", "content": "In this section, we summarize key insights from our study and provide practical recommendations\nfor model merging practitioners. Overall, we find that: Creating expert models from the best\navailable base model is always beneficial. The quality of the base model can be gauged by its\nzero-shot generalization capabilities. We hypothesize that better generalization leads to improved\nweight disentanglement (Ortiz-Jimenez et al., 2024) and a flatter loss landscape, enhancing linear\nmode connectivity and facilitating model merging; \u2192 Merged models often underperform compared\nto task-specific expert models, indicating a potential loss in performance. Despite this, specialized\nexpert models generally outperform general-purpose multitask models (Liu et al., 2022; Roziere et al.,\n2023; Luo et al., 2023), suggesting that the performance loss may not be significant when compared\nto multitask models trained on specific tasks; and \u2192 Our findings indicate that large-scale merging\ncan accommodate more models and significantly improve generalization, outperforming multitask\ntraining when a powerful zero-shot base model is employed. Surprisingly, we find that when\nworking with large instruction tuned models, different merging method perform very similary. This\nimplies that using simple merging methods like averaging will result in models that are comparable\nin quality with the models obtained from more advanced merging method. We hope our research\ninspires further fundamental studies on developing more practical and scalable merging methods."}, {"title": "5 RELATED WORK", "content": "5.1 LOSS LANDSCAPE AND WEIGHT INTERPOLATION\nWhile the loss function of a neural network is generally non-convex, recent work (Draxler et al.,\n2018; Freeman & Bruna, 2016; Garipov et al., 2018; Jordan et al., 2023; Gueta et al., 2023) has\ndemonstrated that the parameter values from different training runs can sometimes be interpolated\nwithout increasing the loss (i.e. they are mode-connected). Many methods (Kuditipudi et al., 2019;\nTatro et al., 2020; Benton et al., 2021) have explored finding these low-loss paths between models,\nfocusing on simple (not necessarily linear) interpolations. For example, Frankle et al. (2020) showed\nthat if a part of the optimization trajectory is shared between two neural networks then they can be\ninterpolated without lowering accuracy. On the other hand, Neyshabur et al. (2020) showed that\nnaively interpolating two neural networks with completely disjoint optimization trajectories can result\nin a catastrophic drop in their accuracies. Entezari et al. (2021) hypothesized that if we account\nfor the permutation symmetry of neural networks, then all neural networks of a given architecture\ntrained on the same dataset are linear mode connected. This assumption of the existence of a low-loss\n\"basin\" in parameter space encompassing the models is critical for model merging (Ilharco et al.,\n2023). Ainsworth et al. (2022); Singh & Jaggi (2020); Wang et al. (2020); Jordan et al. (2022); Pe\u00f1a\net al. (2023) therefore used techniques based on finding permutations (Wang et al., 2020; Ainsworth\net al., 2022) and optimal transport (Singh & Jaggi, 2020) to better align neural networks trained from\nscratch so that they can be merged or interpolated without increasing the loss.\n5.2 MODEL MERGING\nSection 2.1 discusses the merging methods that we use for our experiments, however, the popularity of\nmodel merging has led to a ever-growing number of methods and applications of model merging (He\net al., 2024; Daheim et al., 2023; Yadav et al., 2023a;b; 2024b; Matena & Raffel, 2022a; Jin et al.,\n2023). Next, we discuss some of these methods which were omitted due to large scale practical\nconsiderations. Tangent Task Arithmetic (Ortiz-Jimenez et al., 2024) fine-tune models in the tangent\nspace for better weight disentanglement when using Task Arithmetic. Akiba et al. (2024) explore\nusing evolutionary algorithms to choose which layers to merge. SLERP (Shoemake, 1985) and Model\nStock (Jang et al., 2024) consider the geometric properties in weight space where SLERP performs\nspherical interpolation of model weights while Model Stock approximates a center-close weight based\non several FT models, utilizing their backbone as an anchor point. Tang et al. (2023) train a mask that\nlearns which parameters are important for the merged model. Ye et al. (2023) train a gating network to\npredict a weight that is then used to compute a weighted average of examples during inference. Yadav\net al. (2"}]}