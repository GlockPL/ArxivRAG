{"title": "EGEAN: An Exposure-Guided Embedding Alignment Network for Post-Click Conversion Estimation", "authors": ["Guoxiao Zhang", "Huajian Feng", "Yadong Zhang", "Yi Wei", "Qiang Liu"], "abstract": "Accurate post-click conversion rate (CVR) estimation is crucial for online advertising systems. Despite significant advances in causal approaches designed to address the Sample Selection Bias problem, CVR estimation still faces challenges due to Covariate Shift. Given the intrinsic connection between the distribution of covariates in the click and non-click spaces, this study proposes an Exposure-Guided Embedding Alignment Network (EGEAN) to address estimation bias caused by covariate shift. Additionally, we propose a Parameter Varying Doubly Robust Estimator with steady-state control to handle small propensities better. Online A/B tests conducted on the Meituan advertising system demonstrate that our method significantly outperforms baseline models with respect to CVR and GMV, validating its effectiveness. Code is available: https://github.com/hydrogen-maker/EGEAN.", "sections": [{"title": "1 Introduction", "content": "The post-click conversion rate (CVR) is essential for revenue growth in online advertising systems[16]. The CVR task faces significant challenges because it is trained in the click space while predictions are made in the impression space, which inevitably encounters Sample Selection Bias(SSB) problem: the missing conversion labels follow a missing-not-at-random(MNAR) pattern, leading to a mis- alignment between the event distributions in the training space and the inference space. Figure 1 illustrates this bias and its formation process.\nCurrently, some causal approaches are proposed to adapt for the biased estimates from NMAR data [5, 9, 12-14, 16, 18]. The proof of the unbiasedness of these IPS and DR estimators is based on the assumption that the distribution of covariates is consistent, which has been pointed out to be false in AECM[16]. AECM[16] first introduces an adversarial method of Domain Adaptation(DA) to address the covariate shift problem between click and unclick spaces. However, the adversarial method does not always work well on small datasets and is relatively challenging to optimize [17]. Besides, the general method of DA ignores an intrinsic connection between the distribution of covariates in the click and non-click space.\nInspired by the fact that the exposure event is the shared part of click and non-click events, indicating an intrinsic connection between the distribution of covariates in the click and non-click spaces, we discard the conventional adversarial training solutions and propose an Exposure-Guided Embedding Alignment Network (EGEAN). Specifically, we first align the distribution of covariates to the exposure space through an exposure task. Then, we add a task-personalized network to enhance the efficiency of CTR and CVR estimation with a loss function that minimizes the distribu- tion gap. Besides, to handle small propensities better, we draw on the methods from StableDR[7] and propose a Parameter Varying DR(PVDR) Estimator with steady-state control. Our contributions include:\n\u2022 Proposing an exposure-guided embedding alignment net- work to address estimation bias caused by covariate shift.\n\u2022 Introducing a parameter varying doubly robust estimator with steady-state control to handle small propensities better.\n\u2022 Successfully deploying this method on the Meituan online advertising system, achieving 5.94% rise in CVR during on- line A/B tests."}, {"title": "2 PROPOSED METHOD", "content": "In this section, we describe our proposed EGEAN as shown in Figure 2 and PVDR estimator."}, {"title": "2.1 Exposure- Guided Embedding Alignment Network", "content": "EGEAN comprises three fundamental mechanisms: 1) an exposure probability prediction task. 2) a task personalized network. 3) a metric learning module.\n2.1.1 Exposure Probability Prediction Task. We propose an expo- sure probability prediction task to capture the common features of both clicked and unclicked events, as shown in Figure 2. For this task, positive samples are exposed samples, while negative samples are generated using in-batch negative sampling. The network concatenates the original embeddings of users and items, which are then processed through a Multi-Layer Perceptron (MLP):\n$\\hat{y}_{exp} = \\sigma(MLP(x_{u,i}))$\nwhere $\\hat{y}_{exp} \\in (0,1)$ is the predicted exposure probability of the i-th original item, $\\sigma$ is the sigmoid function, and $x_{u,i}$ indicates the concatenation of user features and item features. We use cross- entropy loss to optimize this network with the exposure label.\n2.1.2 Task Personalized Network. After the pretraining phase, our objective is to enhance the efficiency of multi-task learning by incor- porating a task-personalized network. This network is structured around three integral components:1) At the input layer, we incor- porate LoRA to fine-tune the embeddings.2)At the representation layer, we add an embedding personalized network.3)At the MLP layer, we introduce a parameter personalized network.\nLoRA.First, drawing inspiration from the Low-Rank Adapta- tion (LoRA)[4]. We finetune the pre-trained embeddings for CTR task and CVR task using Low-Rank adaptors. Specifically, for a pre-trained embeddings $W \\in R^{d\\times k}$, we constrain its update by rep- resenting the latter with a low-rank decomposition $W + \\Delta W = BA$, where $B \\in R^{d\\times r}$, $A \\in R^{r\\times k}$, and the rank $r < min(d,k)$. During training, $W$ is frozen and does not receive gradients, while the $A$ and $B$ contain trainable parameters.\nPNet.Following [1], We utilize task-side features $E(F_t) \\in R^{k}$ as the input of EPNet, including task ID. $U_{ep}$ is the Gate NU of EPNet in the embedding layer, and its output $S_{task} \\in R^{e}$ is given by:\n$S_{task} = U_{ep}(E(F_t) \\oplus (*(E)))$,\nwhere we concatenate the general embedding $E \\in R^{e}$ with the input, but without using gradient backpropagation, denoted as (*). Next, we employ the external Gate NU to perform the personalized transformation on embedding E without altering the original embedding layer. The transformed embedding is:\n$O_{ep} = S_{task} \\otimes E$,\nwhere $O_{ep} \\in R^{e}$, and $\\otimes$ denotes the element-wise product.\nPPNet Following [1], we propose PPNet to modify DNN param- eters in multi-task learning. We use task-side features ($F_t$) as priors for PPNet, such as task ID, etc. Specifically, the detailed structure of PPNet is as follows:\nWe concatenate the output of EPNet $O_{ep}$ with the personalized prior $O_{prior}$ as the input of $U_{pp}$, which is the Gate NU in PPNet. To avoid affecting the embedding updated in EPNet, we perform the operation of stop gradient on $O_{ep}$.\nFurthermore, we integrate PPNet into all DNN layers to fully per- sonalize DNN parameters, balancing targets with different sparsity for different users in multiple tasks, formulated as follows:\n$O = S_{task} \\otimes H^{(l)}$,\n$H^{(l+1)} = f(O_{pp}W^{(l)} +b^{(l)}), l \\in \\{1, ..., L\\}$,\nwhere L is the number of DNN layers of task towers and f is the activation function."}, {"title": "2.1.3 Metric learning", "content": "Finetuning for CVR task with the embedding personalized network may lead to discrepancies in the distribution of clicks and non-click spaces. To address this issue, we employ metric learning to compute the distributional distance. Specifically, we calculate the distance between the CVR and shared embedding, which we designate as the metric loss. This merit loss is then in- corporated into the final model loss. The model can better transfer knowledge learned from the non-click space to the click space by minimizing the metric loss. Following [2], we utilize Maximum Mean Discrepancy (MMD) to measure the distributional distance. The formula is typically expressed as:\n$MMD^2 (F, p, q) = ||E_p[f(x)] \u2013 E_q[\\phi(y)]||_H$\nwhere $F$ is a class of functions, $p$ and $q$ are two distributions, $\\phi$ is a feature mapping to a Reproducing Kernel Hilbert Space (RKHS), $E_p[f(x)]$ denotes the expectation of the feature mapping of sam- ples drawn from distribution $p$, and $E_q[\\phi(y)]$ denotes the expec- tation of the feature mapping of samples drawn from distribution $q$."}, {"title": "2.2 Parameter Varying DR Estimator", "content": "2.2.1 Problem Definition. In CVR prediction, the model takes user- item features $x_{u,i}$ and outputs a conversion probability $\\hat{r}_{u,i}$. We denote the prediction matrix by $\\hat{R} \\in R^{n\\times m}$, where $\\hat{r}_{u,i} \\in [0, 1]$. If R were fully observed in both click space O and non-click space N, the ideal loss function would be:\n$L_{ideal} = E(R, \\hat{R}) = \\frac{1}{|D|} \\sum_{(u,i) \\in D} d(r_{u,i}, \\hat{r}_{u,i})$,\nwhere $d(r_{u,i}, \\hat{r}_{u,i}) = -r_{u,i} log(\\hat{r}_{u,i}) - (1-r_{u,i}) log(1-\\hat{r}_{u,i})$. However, $L_{ideal}$ is theoretical, as non-click space labels are unobserved. The bias between a model M's loss $L_M$ and $L_{ideal}$ is:\n$Bias [L_M] = |L_M - L_{ideal}|$.\nConversions are only observed in O, leading to the naive estimator's loss:\n$L_{naive} = \\frac{1}{|O|} \\sum_{(u,i) \\in O} \\delta_{u,i}d(r_{u,i}, \\hat{r}_{u,i}).$\n2.2.2 PVDR Estimator. We present our PVDR Estimator, which incorporates techniques from StableDR[7]. This approach consists of three components: training an imputation model, implementing a steady-state control condition, and developing a PVDR Estimator.\nImputation model. Initially, we train an imputation model $\\hat{\\epsilon}_{ui} = \\theta(x_{u,i})$. The imputation model, parameterized by $\\theta$, aims to estimate the CVR prediction error $\\hat{\\epsilon}_{u,i}$ with $x_{u,i}$. The loss of im- putation model is given as\n$\\pounds = \\frac{1}{|D|} \\sum_{(u,i) \\in D} [\\hat{r}_{ui} - \\theta(x_{u,i})]^2$.\nSteady-state control condition. We propose an ingenious for- mula to determine that PVDR is unbiased, which we refer to as the steady-state control condition. Specifically,\n$\\lambda + (1 \u2013 \\lambda)A = B$\nwhere $\\lambda$ is a hyperparameter, $A = \\frac{1}{\\sum_{(u,i) \\in D} \\frac{o_{u,i}}{p_{u,i}}}$, $B = \\frac{\\sum_{(u,i) \\in D} \\frac{o_{u,i}}{p_{u,i}}}{\\sum_{(u,i) \\in D} o_{u,i}}$\nPVDR Estimator. The PVDR estimator is given as\n$L_{PVDR} = \\sum_{(u,i) \\in D} \\frac{o_{u,i}l_{u,i}}{p_{u,i}} + (1 \u2013 \\lambda) \\sum_{(u,i) \\in D} \\frac{o_{u,i}}{p_{u,i}}$\nWhen the steady-state condition is met, PVDR is a doubly robust estimator, and the CVR estimation is unbiased.\n$\\lambda + (1 -\\lambda) \\sum_{(u,i) \\in D} \\frac{o_{u,i}}{p_{u,i}}  = \\frac{\\sum_{(u,i) \\in D} \\hat{r}_{u,i}\\frac{o_{u,i}}{p_{u,i}}}{\\sum_{(u,i) \\in D} o_{u,i}}$\nObviously, StableDR[8]is a special case of PVDR when $\\lambda = 1$, and it degenerates to IPS when $\\lambda = 0$. Due to the introduction of parameter $\\lambda$, it is evident that the variance of PVDR outperforms SDR when the propensity score is relatively small."}, {"title": "3 Experiments", "content": "We conduct extensive experiments on the public datasets and one private industrial dataset. The statistics of the processed datasets are reported in Table 1.\n\u2022 Ali-CCP : The public dataset Ali-CCP [10] gathered from real-world traffic logs of the recommender system in Taobao. The Ali-CCP contains 400 thousand users and 4.3 million items, as well as over 80 million user-item interactions.\n\u2022 Meituan: Three months of search logs in recommend ad system in Meituan are to generate the examples and corre- sponding features, ranging from July. 2024 to Oct. 2024"}, {"title": "3.0.1 Evaluation Metric", "content": "We apply widely used AUC (Area Under ROC) as the primary metric to evaluate the performance."}, {"title": "3.0.2 Baseline Methods", "content": "We compare our proposed framework with the following baselines.\n\u2022 ESMM[10]: It learns CVR utilizing a CTR task and a CTCVR task which is a non-causal estimator.\n\u2022 MTL-DR[15]: It deploys the DR estimator within a multi- task framework to debias more robustly.\n\u2022 DDPO [12]: It incorporates the IPW estimator to regularize ESMM's CVR estimation.\n\u2022 DCMT [18]: It optimizes two opposite CVR towers in the click space and the non-click space.\n\u2022 AECM[16]: It optimizes two opposite CVR towers in the click space and the non-click space."}, {"title": "3.0.3 Implementation Details", "content": "For a fair comparison, we adopt the following settings for all methods: The batch size is set to 1024 and we employ the Adam[6] optimizer with a learning rate of $Ir = 1\\times10^{-3}$ and weight decay of $\\f = 1\\times10^{-3}$. The activation functions involved in two discriminators make use of LeakyReLU[11] with a negative slope set to 0.2. The embedding size is set to 5 for the Ali- CCP dataset and 8 for the Meituan dataset.Xavier initialization[3] is used here to initialize the parameters."}, {"title": "3.1 Overall Performance", "content": "In this section, we compare the performance of our proposed EGEAN with the baselines on the above two datasets. We consider two dis- tinct tasks, i.e., CVR and CTCVR, and record data in the entire impression space D with the metric AUC for evaluation. The over- all results are shown in Table 2."}, {"title": "3.2 Ablation Study", "content": "To analyze the contributions of each component in our model, we conduct the ablation studies presented in Table 3. The framework consists of three integral components: an exposure network, a task personalization network, and a metric learning module. As shown in Figure 2, three degraded versions suffer a decrease over the two datasets compared to the original EGEAN. This indicates the effectiveness of our well-designed user task personalization modeling and distribution alignment strategy."}, {"title": "3.3 Case Study", "content": "Fig 3(a) and Fig 3(b) display the characteristic distribution of embed- dings for the Meituan dataset. The visualizations in these figures vividly demonstrate that in the absence of the TSN module, there is a notable separation between the distributions of clicked events (represented by cherry blossom dots) and unclicked events (blue dots). Conversely, upon integrating the TSN module, the clicked and unclicked events are fully integrated into the same feature space, leading to their complete intermingling. These observations pro- vide a degree of confirmation for the effectiveness of our proposed framework and the validity of our experimental results."}, {"title": "3.4 Online A/B Test", "content": "From 2024-10-01 to 2024-10-07, we conducted online A/B testing on the online advertising system in Meituan to validate the pro- posed EGEAN. As shown in Table 3, compared to the DDPO model (our last product model), EGEAN achieves 5.94% CVR and 6.29% GMV improvement. Now, EGEAN serves the main traffic in the advertising system in Meituan."}, {"title": "4 Conclusions", "content": "In this paper, we introduce a novel model, EGEAN, that tackles the covariate shift challenges prevalent in CVR estimation. Through the seamless integration of a parameter varying doubly robust estimator, EGEAN has exhibited remarkable enhancements com- pared with existing baseline models across diverse datasets. Our model notably increases system GMV by 6.29% and CVR by 5.94% on Meituan online advertising systems. This approach opens new research possibilities in CVR estimation, and we hope our work will inspire further exploration in this area."}]}