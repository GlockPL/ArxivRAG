{"title": "Evolving Hard Maximum Cut Instances for Quantum Approximate Optimization Algorithms", "authors": ["Shuaiqun Pan", "Yash J. Patel", "Aneta Neumann", "Frank Neumann", "Thomas B\u00e4ck", "Hao Wang"], "abstract": "Variational quantum algorithms, such as the Recursive Quantum Approximate Optimization Algorithm (RQAOA), have become increasingly popular, offering promising avenues for employing Noisy Intermediate-Scale Quantum devices to address challenging combinatorial optimization tasks like the maximum cut problem. In this study, we utilize an evolutionary algorithm equipped with a unique fitness function. This approach targets hard maximum cut instances within the latent space of a Graph Autoencoder, identifying those that pose significant challenges or are particularly tractable for RQAOA, in contrast to the classic Goemans and Williamson algorithm. Our findings not only delineate the distinct capabilities and limitations of each algorithm but also expand our understanding of RQAOA's operational limits. Furthermore, the diverse set of graphs we have generated serves as a crucial benchmarking asset, emphasizing the need for more advanced algorithms to tackle combinatorial optimization challenges. Additionally, our results pave the way for new avenues in graph generation research, offering exciting opportunities for future explorations.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of noisy intermediate-scale quantum (NISQ) technology, the practical use of quantum computing is being increasingly pursued, despite challenges like the scarcity of quantum resources and the noise in quantum gates [50]. Key research areas such as combinatorial optimization [21], quantum machine learning [7], and quantum chemistry [41] are recognized as potential fields where NISQ devices might surpass best classical methods, thereby showcasing a quantum advantage.\nThe Recursive Quantum Approximate Optimization Algorithm (RQAOA) [12] was developed to improve upon the Quantum Approximate Optimization Algorithm (QAOA) for combinatorial optimization problems like maximum cut. While QAOA [21] was initially introduced as a quantum heuristic, research indicates it struggles to surpass the Goemans and Williamson (GW) algorithm [24], a classical method with strong approximation guarantees, at constant depth [5, 20, 31, 39]. In contrast, RQAOA employs a recursive strategy that iteratively refines solutions, demonstrating superior performance both numerically and analytically [2, 11, 13]. This makes RQAOA a promising advancement in quantum optimization.\nResearch has shown that certain algorithms underperform in specific scenarios, often labeled as hard or worst-case instances [36]. Yet, guidance on how to generate or identify these challenging instances of studies is typically lacking in the literature. For instance, the RQAOA paper highlights an infinite family of d-regular bipartite graphs where the QAOA lags behind the GW algorithm when the depth (number of layers) is O(logn), where n is the number of nodes [12]. However, the paper does not provide instructions for constructing these graphs. In our work, we seek to bridge this gap by generating instances that underscore the differences between quantum algorithms and classical optimization algorithms in tackling the maximum cut problem, with a particular focus on the RQAOA and GW algorithms. While various heuristics have been explored in [17], we selected the GW algorithm due to its well-established approximation bounds and extensive documentation of both its strengths and limitations, making it a suitable benchmark for comparison. We employ an evolutionary method using the classical Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [28, 30] paired with a fitness function specifically tailored for this purpose. This strategy generates a variety of instances in the latent space of a thoroughly trained Graph Autoencoder (GAE). These instances are designed to be easily solvable by one algorithm while posing substantial challenges to the other, thereby providing a clearer comparison of their relative strengths and weaknesses.\nWe deliver several significant contributions:\n\u2022 We have developed a method that effectively pinpoints maximum cut instances that either pose significant challenges or are considerably more manageable for the RQAOA in comparison to the GW algorithm.\n\u2022 Through an extensive analysis of these instances across various sizes of the maximum cut problem, our generated instances serve as valuable benchmarks for evaluating and contrasting the effectiveness of different algorithms in addressing maximum cut challenges. This benchmarking not only underscores performance discrepancies among algorithms but also aids in advancing quantum computing methodologies.\n\u2022 Our research brings a fresh perspective to graph generation studies within the network science community, spurring further investigation and innovation in this domain."}, {"title": "2 PRELIMINARIES", "content": "Maximum cut. The maximum cut problem in graphs is one of the classical and well-studied NP-hard combinatorial optimization problems. Given an undirected graph G = (V, E), the goal is to compute a partitioning into two sets S\u2282 V and V \\ S such that the number of edges\n$C(S) = |\\{e \u2208 E | e\u2229S \u2260 0 \u2227 e\u2229 (V \\ S) \u2260 0\\}|$ crossing the two sets is maximal. The maximum cut problem is APX-hard [47] and can be approximated using the GW algorithm [24]. Specifically, this approach involves solving a relaxed version of the original problem. We first tackle an optimization problem in the continuous domain [-1,1]N using a semidefinite program, which provides us with a real-valued solution associated with a cost denoted as $C_{relaxed}$. Then, a sampling process, guided by probabilities derived from the entries of the real-valued solution (via a random projection method), is used to produce a feasible solution in the form of a binary string. It is important to note that the cost of the generated binary string is guaranteed to be at most equal to $C_{relaxed}$, and it is possible that $C_{max} < C_{relaxed}$. However, the expected value obtained by the cost function using this method is proven to be less than $\u03b1C_{max}$, where \u03b1 \u2248 0.878, for any input graph. In other words, this is an \u03b1-approximation algorithm.\nThe maximum cut problem has received significant attention in the area of quantum optimization and different quantum algorithms have been designed. As discussed earlier, QAOA faces significant limitations due to its intrinsic locality and symmetry constraints, with substantial theoretical research having established no-go theorems in this area. To address these challenges, RQAOA [12] was introduced as an alternative quantum optimization algorithm specifically for combinatorial optimization problems. During each iteration of RQAOA, the problem size-represented as a graph or hypergraph-is reduced by one or more elements, based on correlations between variables derived from QAOA. This reduction process introduces non-local effects by creating new connections between nodes that were not previously linked, thus circumventing the locality constraints of QAOA. Finally, the iterative process stops once the number of variables is below a certain predefined threshold nc (nc\u2248 O(1) << |V|) such that a classical solver can be used to solve the remaining graph. In this study, we focus on depth-1 RQAOA, which can be classically simulated with a runtime of O(n4) [48]. Understanding the performance of these algorithms is challenging and pointing out instances where quantum approaches perform significantly better or worse than the classical GW algorithm provides an important understanding of their performance in comparison to state-of-the-art classical approximation algorithms."}, {"title": "3 RELATED WORK", "content": "In optimization, generating diverse problem instances is essential for effective benchmarking. The choice of instances significantly impacts algorithm evaluation, making it crucial to update benchmarks regularly to prevent overfitting specific cases. Assessing benchmark quality is challenging, as measuring diversity and representativeness is not straightforward. Typically, evaluation considers key dimensions such as feature space, performance space, and instance space [6].\nThe literature highlights various methods for generating maximum cut instances used in benchmarking. A notable example is the Balasundarm-Butenko problem generator [3], which utilizes a framework designed for creating test functions within global optimization. This involves applying continuous formulations to combinatorial optimization problems. Another key tool is the machine-independent graph generator known as Rudy, introduced by [32] which has been widely used in subsequent research to evaluate algorithms [8, 15, 16]. In 2018, a comprehensive library [17] containing 3,296 diverse instances was introduced, aggregating data from multiple sources, and further enriching the resources available for algorithm testing.\nEvolutionary algorithms (EAs) have proven effective in generating diverse problem instances across various domains. Bossek et al. [10] pioneered an EA-based approach for crafting the Travelling Salesperson Problem (TSP) instances using novel mutation operations. Neumann et al. [44] developed a method for evolving diverse TSP instances and images using multi-objective indicators. Gao et al. [22] introduced an EA-driven framework to generate problem instances classified as easy or hard based on distinct features. Addressing diversity in high-dimensional feature spaces, Marrero et al. [38] proposed a Knapsack problem instance generator combining Principal Component Analysis with novelty search. More recently, research has explored evolving instances for the chance-constrained maximum coverage problem [51]. This study departs from traditional approaches that rely on approximation ratios in the (1 + 1) EA, instead incorporating performance ratio variances for improved evaluation."}, {"title": "4 EVOLVING HARD MAXIMUM CUT INSTANCES", "content": "GNNs have proven effective for generating graphs with diverse attributes. In our research, we use tools like the GAE to explore and identify specific graph structures. Our goal is to identify hard maximum cut instances. We begin by training a GAE with graph generators that are rooted in network science. After the GAE is adequately trained, we proceed to pinpoint particular graph instances within its complex latent space. The identification process is facilitated by the classical CMA-ES algorithm. This strategy employs a fitness function to assess scenarios where the RQAOA either surpasses or falls short compared to the GW algorithm.\nProblem formulation. We aim to propose a method for evolving hard instances of the maximum cut problem, to challenge a recently proposed quantum search algorithm - RQAOA, against a well-studied classical algorithm - GW. We measure the largest number of cuts found by each algorithm on a graph instance I, denoted as $P_{RQAOA}(I)$, or $P_{GW}(I)$.\nGraph instance representation. We choose to base our research on Permutation-Invariant Variational Autoencoder (PIGVAE) [57] as it handles the graph isomorphism problem explicitly. The core principle of PIGVAE is that identical graphs should yield the same representation, regardless of the order of their nodes. It combines a variational autoencoder with a permuter model. This permuter model, trained in conjunction with the encoder and decoder, assigns a permutation matrix to each input graph. This matrix aligns the node order of the input graph with that of its reconstruction, ensuring consistent representation across different inputs.\nEvolving instances with CMA-ES. We aim to search hard graph instances for a maximum cut solver against another solver. Hence, we propose using the following objective function:\n$\\frac{P_B(I)}{P_A(I)}$,\nwhere A is the baseline/reference algorithm and B is the one of interest. In this paper, we try to answer two questions: is there a cluster/family of graphs (1) hard to solve by RQAOA w.r.t. the classical GW method; (2) vice versa? For those two questions, we maximize the objective functions, $P_{GW}(I)/P_{RQAOA}$ and $P_{RQAOA}(I)/P_{GW}$, respectively, in the latent space of the already-trained PIGVAE model. We utilize the widely applied CMA-ES for this maximization task. For each latent point sampled by the CMA-ES, we first decode the latent point to a graph instance I and then execute RQAOA and GW algorithms on instance I 100 times due to their stochasticity. We take the median value of each algorithm to compute the above ratio.\nImportantly, a randomly sampled latent point may be decoded into a disconnected graph. We might initiate the CMA-ES with connected graphs. Still, as the search progresses, CMA-ES may sample disconnected graphs again. To resolve this issue, we propose the following strategy: for any latent point that corresponds to a disconnected graph, we solve the maximum cut problems supported on each connected component and then take the sum of the number of cuts from all components. For RQAOA, if the number of nodes of a component is smaller than a preset value $n_c$, we solve the maximum cut problem on this component by a brute force method; Otherwise, we apply RQAOA on it. In the recursion step of RQAOA, disconnectedness can be created even if we start with a connected graph. In this case, we apply the above strategy to the connected components. In the experiments, we test our approach with two sizes: 20 nodes and 100 nodes, and we set nc to 10 and 20, respectively."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Experimental setup", "content": "GAE training. We sampled the training graph instance u.a.r. from six random graph models: Erd\u0151s-R\u00e9nyi [18], Watts-Strogatz small-world [56], Newman-Watts-Strogatz small-world [45], Random Regular [55], Barab\u00e1si-Albert [4], and Dual-Barab\u00e1si-Albert graphs [42]. We also took the parameter setting of those graph models and hyperparameters of the PIGVAE model from [58]. In each training epoch, we generate 6 000 graph instances from the above random models. We train the PIGVAE model with maximally 3000 epochs and a batch size of 128. An early stopping mechanism is implemented to terminate training if the validation loss shows no improvement over the past 60 epochs. To assess our approach on different graph sizes, we train two separate models: one for 20-node instances and the other for 100-node instances.\nCMA-ES setup. The CMA-ES algorithm is configured with a population size of 64, leveraging parallel evaluations to enhance computational efficiency. To further optimize performance, a restart mechanism is implemented, allowing up to 10 restarts, with each limited to a maximum of 2,000 function evaluations. The algorithm is based on the standard implementation provided by the pycma library [27].\nReproducibility. The full implementation is available in our Zenodo repository [1], along with supplementary materials for reproducibility and reference."}, {"title": "5.2 Results", "content": ""}, {"title": "5.2.1 Evaluation of the GAE.", "content": "During the training phase, we randomly save 60 graphs out of 6,000 in each epoch, accumulating a total of 6,000 graphs for in-depth evaluation. Our initial examination focuses on the 64-dimensional latent space structure, utilizing embeddings generated from these 6,000 graphs. Initially, we compute several key features, as detailed in Section 7.1 (see the supplementary material [1]), prioritizing those that significantly influence the complexity and difficulty of the maximum cut problem. Research indicates that certain graph characteristics, especially eigenvalues, provide substantial insight into a graph's connectivity and complexity concerning the maximum cut problem [17, 43]. We continue to evaluate our trained GAE through a link prediction task, leveraging learned embeddings to capture graph topology. These embeddings facilitate adjacency matrix reconstruction, enabling predictions of potential node connections. Performance is measured using the macro area under the receiver operating characteristics curve (ROC-AUC) (0 to 1 scale, where 0.5 indicates random guessing and 1 denotes perfect prediction). Our initial assessment focuses on a GAE trained on 10-node Erd\u0151s-R\u00e9nyi graphs (50% probability) using a single A100 GPU with a batch size of 32. This setup yields a macro ROC-AUC of 0.933 \u00b1 0.004 across 6,000 graphs training graphs. Building on this promising result, we extend our analysis to larger graphs containing all graph models, obtaining macro ROC-AUC scores of 0.709 \u00b1 0.001 for 20-node graphs and 0.611\u00b10.001 for 100-node graphs, using the same number of training samples."}, {"title": "5.2.2 Feature analysis through parallel coordinate plot.", "content": "To ensure diversity, each CMA-ES run begins from different latent points within the latent space. For 20-node maximum cut instances, we generate 1,519 instances where RQAOA outperformed GW and 1,231 instances where GW exceeded the performance of RQAOA. Similarly, for 100-node instances, both algorithms outperform each other in 850 cases. To further analyze these instances, we examine various features (see section 7.1 in the supplementary material [1]). Comparative results are visualized in Figure 3a for 20-node graphs and Figure 3b for 100-node graphs."}, {"title": "5.2.3 Feature analysis with machine learning pipeline.", "content": "To analyze the GW algorithm's role in generating hard maximum cut instances, we expand our feature set based on prior research [43]. Incorporating six additional GW-related features (see Section 7.2 in the supplementary material [1]), we increase our total to 18 features within our machine learning pipeline. For classification, we use the TPOP library [46], an AutoML tool for optimizing machine learning pipelines. Instances, where RQAOA outperforms GW, are labeled 0, while GW-dominant cases are labeled 1. We split the dataset 70:30 for training and testing. Due to class imbalance, particularly in 20-node experiments, we optimize for balanced accuracy [14], while standard accuracy [49] is used for the balanced 100-node dataset. Using TPOP, we conduct 10-fold cross-validation over 100 generations, evaluating 60 candidate solutions per generation.\nIn our 20-node experiments, we achieve an outstanding average balanced accuracy of 0.9988 \u00b1 0.0012 across ten runs, each with a unique random data split. This result is obtained using an ExtraTreesClassifier [23]. Key predictive features include EXPECTED_COSTGW_OVER_SDP_COST and SPECTRAL_GAP, where the former evaluates GW's effectiveness, and the latter reflects graph spectral properties. The relevance of the GW algorithm's output in our analysis aligns with our focus on instances that are particularly challenging for maximum cut problems, influenced by the GW performance. The importance of eigenvalues also plays a crucial role while GIRTH (shortest cycle length) contributes little to classifier performance. Further, the partial dependence plot visualizes the impact of two top features.\nTo assess the significance of features, we conduct a permutation important analysis. This involved randomly shuffling the values within columns of our dataset, focusing particularly on the top two features identified in both the 20-node and 100-node experiments. For each feature, we generate 10 different permutations of our original dataset. We then evaluate the impact of these permutations on the model's performance, which is initially trained on the unaltered dataset, using the same 10 seeds as the original setup. For the 20-node experiments, the permutation of the feature EXPECTED_COSTGW_OVER_SDP_COST results in a slight decrease in performance, with metrics changing by a -0.0006 \u00b1 0.0018. Interestingly, shuffling the SPECTRAL_GAP feature leads to a minor performance increase, indicated by 0.0001 \u00b1 0.0017. When both features are permuted simultaneously, the combined effect causes a more noticeable performance decrease of 0.0009 \u00b1 0.00177. In the 100-node experiments, permutating the EXPECTED_COSTGW_OVER_SDP_COST feature lead to a more pronounced decline in performance, measured at -0.0067\u00b10.0048. Permutation of the feature LOG_SMALLEST EIGVAL results in a smaller drop in performance, recorded as -0.0004 \u00b10.0008. Permuting both features together causes a decrease in performance of -0.0012 \u00b1 0.0013. Overall, we can conclude that the performance shifts upon permuting a single feature or two top features slightly affect the model's performance.\nTo determine whether classifier performance could be sustained without the top two most informative features identified in our 20-node and 100-node experiments, we re-initiate the entire automated machine learning process excluding these features. In the 20-node experiments, this exclusion results in a minor decrease in performance, showing a decline of 0.0015\u00b10.0022 from the original baseline. Interestingly, in the 100-node experiments, performance remained unchanged, suggesting that the other features in the dataset still carry significant predictive power and are capable of compensating for the absence of the top two features."}, {"title": "5.2.4 RQAOA as a high-performing heuristic with machine leaning pipeline.", "content": "After successfully applying our evolutionary approach to evolve hard maximum cut instances, a new question emerges: whether RQAOA consistently performs well? For our purposes, we seek to establish if RQAOA can achieve a performance level above 0.96, with a minimum advantage of 0.04 over the GW algorithm. In this study, we reassess the same dataset with a modified labelling strategy. Instances are labelled as 1 when the ratio of GW to RQAOA is 0.96 or less, indicating at least a 4% improvement over the GW algorithm. All other instances are labelled as 0. We develop classifiers on both 20-node and 100-node instances that incorporate all the features previously discussed with achieved a balanced accuracy of 0.9826 \u00b1 0.0042 on 20-node experiments and a 0.8343 \u00b1 0.0303 in the 100-node experiment, maintaining the same experimental and analysis procedures as before. The models, generated using the TPOP library are detailed further in section 7.3. The analysis of these plots reveals a clear pattern: low values in GW-related features are strongly associated with increased predicted probabilities of RQAOA achieving outstanding performance. Moreover, the significance of the DENSITY feature in influencing RQAOA's performance is evident. This analysis not only shows the impact of GW-related cost features but also emphasizes the critical role of DENSITY in determining the effectiveness of the RQAOA algorithm.\nTable 3 and Table 4 showcase the key features identified by the classifier for 20-node and 100-node maximum cut instances, respectively. Highlighted in these tables, the features emphasize the robust performance of RQAOA, validating its effectiveness as a high-performing heuristic through significant permutation importance.We evaluate the performance of RQAOA against different thresholds to better understand our generated instances. Initially, thresholds of 0.95 and 0.98 are also considered; however, they are discarded because there are different situations related to 20-node or 100-node instances that no instances meet the criteria to be labelled as 1. At a threshold of 0.97, the average balanced accuracy achieved is 0.9795\u00b10.0045 for 20-node instances. For 100-node instances, the performance improved notably at the 0.97 threshold, achieving an average balanced accuracy of 0.9214\u00b10.0049. This represents a substantial enhancement over the results obtained with the 0.96 threshold.\nOur study offers a fresh perspective on graph generation within network science, encouraging further exploration and advancements in this area. Future work can be explored from various perspectives. Our current experiments focus on graphs with 20 and 100 nodes, but we are actively working to scale up to larger instances, such as 400-node graphs. A major challenge in this expansion is the scalability of the PIGVAE model, which was originally developed for smaller graphs and faces significant memory constraints when applied to larger ones. Running the model on 400-node graphs leads to severe computational bottlenecks. To address this, we have reduced the model's size, but this comes at the potential cost of training performance. Moving forward, we aim to explore alternative strategies to improve scalability, enabling the model to efficiently process graphs with thousands of nodes while maintaining its effectiveness. Additionally, as part of our future work, we aim to integrate additional heuristics beyond GW to enhance the depth of our comparative analysis."}, {"title": "6 CONCLUSION", "content": "Given the current literature does not provide detailed guidelines for constructing hard maximum cut instances, our research aims to address this gap. We focus on developing instances that highlight the distinctions between the quantum method RQAOA and the classical GW algorithm by effectively identifying maximum cut instances that pose significant challenges or are notably manageable for the RQAOA compared to the performance of the GW algorithm. Specifically, our work extends beyond situations where RQAOA merely surpasses GW, exploring instances where RQAOA achieves exceptionally high-quality outcomes and significantly outperforms GW. We employ an evolutionary approach using the classical CMA-ES algorithm, enhanced with a fitness function specifically tailored within the latent space of a well-trained Graph Autoencoder."}, {"title": "7 APPENDIX", "content": ""}, {"title": "7.1 Appendix A", "content": "The features of the instances utilized in this study are detailed as follows:\n\u2022 LOG_NUM_EDGES: The logarithm of the number of edges in the graph.\n\u2022 DENSITY: The proportion of actual edges to the maximum possible number of edges (i.e., of a complete graph).\n\u2022 LOGRATIO_EDGETONODES: The natural logarithm of the edge-to-node ratio.\n\u2022 LOG_CHROMATIC_NUM: The logarithm of the minimum number of colours required to colour the graph such that no two adjacent nodes share the same colour.\n\u2022 NORM_MIS: The size of the largest independent node-set, adjusted relative to the total number of nodes, showcasing the proportion of non-adjacent nodes.\n\u2022 GRAPH_ASSORTATIVITY: Gauges node connection similarity based on node degree, with values near 1 suggesting similar degree connections.\n\u2022 SPECTRAL_GAP: The difference between the largest and second-largest eigenvalues of the graph's normalized Laplacian matrix.\n\u2022 LOG_LARGESTEIGVAL: The logarithm of the highest eigenvalue from the normalized Laplacian matrix.\n\u2022 LOG_SECONDLARGESTEIGVAL: The logarithm of the normalized Laplacian matrix's second-largest eigenvalue.\n\u2022 LOG_SMALLESTEIGVAL: The logarithm of the smallest non-trivial eigenvalue from the normalized Laplacian matrix.\n\u2022 GIRTH: The length of the shortest loop in the graph.\n\u2022 TRANSITIVITY: The likelihood ratio of nodes to form tightly connected clusters."}, {"title": "7.2 Appendix B", "content": "The features related to the GW algorithm are introduced as follows:\n\u2022 PERCENT_CUT: The ratio of the optimal cut value derived from the semidefinite programming (SDP) solution to the overall number of edges in the graph.\n\u2022 PERCENT_POSITIVE_LOWER_TRIANGULAR: Quantifies the share of positive entries in the lower triangular section of the factorized matrix.\n\u2022 PERCENT_CLOSE1_LOWER_TRIANGULAR: The proportion of elements in the lower triangular of the factorized matrix that have absolute values under 0.1.\n\u2022 PERCENT_CLOSE3_LOWER_TRIANGULAR: The percentage of elements in the lower triangular of the factorized matrix with absolute values smaller than 0.001.\n\u2022 EXPECTED_COSTGW_OVER_SDP_COST: The proportion of the GW approach's average cost (over 1,000 random trials) relative to the maximum theoretical cost as determined by the SDP method.\n\u2022 STD_COSTGW_OVER_SDP_COST: Quantifies the standard deviation of the outcomes from 1,000 random implementations of GW, normalized by the best possible cost determined through the SDP method."}, {"title": "7.3 Appendix C", "content": "The models with hyperparameters constructed in the section of feature analysis with machine learning pipeline are highlighted in the following:\n\u2022 The classifier developed using TPOP for instances involving 20 nodes uses an ExtraTreesClassifier, which comprises a collection of 100 decision trees. This ensemble employs the entropy criterion to determine the quality of splits within the trees. Each tree considers approximately 35% of the features for node splitting. It also ensures that each leaf node has at least one sample, while at least nine samples are necessary to initiate further node splitting. The random_state parameter is set to 42, ensuring reproducibility.\n\u2022 The classifier built with TPOP for 100-node instances is implemented as an ExtraTreesClassifier that features bootstrapping. The entropy criterion is used to evaluate split quality across the trees. It also requires 80% of the features to be considered for each tree split and a minimum of 19 samples for each leaf node. Additionally, a node split requires at least 5 samples to proceed. The ensemble includes 100 trees, and the random_state parameter is fixed at 42.\nThe models with hyperparameters constructed in the section of RQAOA as a high-performing heuristic with the machine learning pipeline are highlighted in the following:\n\u2022 The classifier configured for 20-node instances using the TPOP library employs a sequential machine-learning pipeline. It begins with a StackingEstimator that deploys an MLPClassifier. This is complemented by another StackingEstimator, which utilizes an ExtraTreesClassifier. The data is then normalized using a StandardScaler. Finally, the process culminates with a GaussianNB classifier.\n\u2022 The classifier configured using TPOP for 100-node instances also employs a sequential machine-learning pipeline. Initially, a StackingEstimator incorporating a GaussianNB classifier makes preliminary predictions that feed into later stages. Following that, a SelectFwe feature selector uses the f_classif with an alpha of 0.012 to selectively maintain features. Subsequently, another StackingEstimator featuring a Decision Tree Classifier is used. The process concludes with a GaussianNB classifier."}]}