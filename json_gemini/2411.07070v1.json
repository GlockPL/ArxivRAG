{"title": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models", "authors": ["Qian Sun", "Hanpeng Wu", "Xi Sheryl Zhang"], "abstract": "The pretraining and fine-tuning approach has become the leading technique for various NLP applications. However, recent studies reveal that fine-tuning data, due to their sensitive nature, domain-specific characteristics, and identifiability, pose significant privacy concerns. To help develop more privacy-resilient fine-tuning models, we introduce a novel active privacy auditing framework, dubbed PARSING, designed to identify and quantify privacy leakage risks during the supervised fine-tuning (SFT) of language models (LMs). The framework leverages improved white-box membership inference attacks (MIAs) as the core technology, utilizing novel learning objectives and a two-stage pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the exposure of privacy risks. Additionally, we have improved the effectiveness of MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our research aims to provide the SFT community of LMs with a reliable, ready-to-use privacy auditing tool, and to offer valuable insights into safeguarding privacy during the fine-tuning process. Experimental results confirm the framework's efficiency across various models and tasks, emphasizing notable privacy concerns in the fine-tuning process.", "sections": [{"title": "Introduction", "content": "Concerns regarding the privacy of training data pose a significant challenge in AI security, especially in sensitive domains like healthcare [34] and finance [55], where privacy issues are particularly pronounced. The study of privacy attacks and defense mechanisms for large language models (LLMs) is still in its early stages. In the past, research has primarily concentrated on extracting training data from pre-trained language models (PLMs) [5, 24, 27], leading to the development of novel theories regarding model memorization in LLMs [7] and mechanisms of data leakage [26]."}, {"title": "Related Work", "content": "existing privacy auditing methods primarily focus on the passive detection phase, where they assess privacy leakage after model training is completed. However, these methods often struggle to effectively address the dynamic changes occurring during the fine-tuning process, potentially overlooking critical privacy risks. To shed new light on developing more privacy-robust LMs, the following natural questions are explored as, Can the model fine-tuner actively identify privacy risks during the fine-tuning process? Can we quantify the privacy risks in this process using MIAs? What are the characteristics of privacy leakage during the process of LMs?\nAs a result, it is necessary to develop more thorough methods to fill this technological gap. We note that non-members and members show noticeable variations in numerical data such as loss and gradient norms at the final layer, as well as in implicit data such as intermediate module outputs and gradients during fine-tuning. We have developed an effective methodological framework PARSING (Privacy Auditing on Risk of Supervised fine-tunING) to tackle these distinctions, highlighting and quantifying the privacy risks associated with the fine-tuning of LMs."}, {"title": "Contributions", "content": "In this paper, we make the following contributions to the study of privacy in LMs.\n\u2022 We introduce a novel privacy auditing framework applied during the fine-tuning phase of LMs, which has the potential to be employed in the foundation model designs. The PARSING framework aims to identify and quantify the risks of privacy leakage inherent in the fine-tuning process and seeks to help achieve a balance between privacy and utility. We offer a detailed explanation of the structure and operational mechanisms of the framework.\n\u2022 We propose an active two-stage white-box MIA method targeting LMs, applicable to various models including the GPT series and Llama series. By introducing a new methodological architecture and learning objectives, the method first optimizes the membership representation of samples, thereby enhancing their feature representation capability. This improvement significantly increases the effectiveness of MIAs on complex models.\n\u2022 We evaluate PARSING on a range of models and diverse text tasks, and benchmarked it against existing studies. Empirical validation has shown that our framework is effective in detecting and quantifying privacy risks during the fine-tuning of LMs. Moreover, we also conduct a systematic analysis of the key factors leading to privacy vulnerabilities during the fine-tuning process, including task complexity, model size, text length, and so forth, in order to propose corresponding privacy protection strategies."}, {"title": "Related Work", "content": "Within a defined training environment and strategy for a particular model, the main objective of training data privacy audits [22, 43, 54] is to identify and evaluate possible data privacy risks associated with the model, such as data leakage, unauthorized data inference, and the feasibility of adversaries reconstructing the original training data. It is widely accepted that a model service is considered to Privacy-Guaranteed if it does not divulge training data information during the inference phase. Particularly in domains sensitive to privacy,"}, {"title": "Training Data Privacy Audits", "content": "the development of models that guarantee privacy is of paramount importance. The privacy audit framework for training data [38, 40] is generally established on the basis of privacy attack and defense methods. It identifies and measures potential privacy risks using various attack techniques, and mitigates these risks through corresponding defense techniques."}, {"title": "Privacy Attack", "content": "There are numerous privacy attack methodologies targeting model training data, notably including MIAs [10, 40, 50], training data extraction attacks [5, 7, 26], model inversion attacks [14, 21], and property inference attacks [2, 15]. (1) MIAs are widely-studied privacy attack methods on basic neural network models. It aims to determine whether a specific sample was used in training a given model. While MIAs can pose significant privacy threats to models trained on sensitive data, they are also used as a tool to assess the privacy boundaries of a model [6, 38, 43, 54]. (2) Training data extraction attacks focus on extracting or reconstructing the original data used for training a model. Particularly in the realm of large language models, these attacks generally adhere to the generating-then-ranking methodology framework, as proposed by Chrlini et al. [5]. (3) The primary objective of model inversion attacks is to infer aggregate details about the input data. These attacks are particularly effective when access to the model is limited, as they may reveal the model's internal structure or extract specific information about the training data. (4) Property Inference Attacks aim at deducing sensitive attributes from the training data that are not directly revealed or are irrelevant to the task. For responsible data users and neural network model designers, mitigating Property Inference Attacks is an important consideration."}, {"title": "Privacy Defense", "content": "To mitigate the risks introduced by these attack algorithms, researchers have developed various novel algorithmic techniques aimed at bolstering privacy safeguards. A commonly adopted approach is the incorporation of differential privacy [13] protocols during model training. This approach involves introducing noise at the data level [11] or algorithmic level [1], which helps shield personal information from precise identification. In addition to methods grounded in statistical theory with guaranteed robustness, practitioners also employ empirical and heuristic approaches, such as data anonymization [39], data augmentation [25] and adversarial privacy enhancement [23, 41]. Although mathematical techniques offer robust privacy guarantees, they frequently result in reduced model performance and higher computational demands. Heuristic methods, which are typically simpler to apply, may lack the theoretical robustness provided by differential privacy."}, {"title": "Privacy Measurement", "content": "Privacy audit frameworks are pivotal in the development of secure artificial intelligence systems. Presently, methods for measuring privacy are primarily divided into two categories: static and dynamic methods. Static methods, based solely on privacy attacks [6, 38, 40], evaluate a model's vulnerability to privacy leakage post-training through simulated attacks. Typical techniques for these attacks include membership inference and training data extraction. Conversely, dynamic privacy measurement methods proactively monitor and assess privacy risks throughout the model's training process by integrating privacy measurement mechanisms into the training methodology [4, 22]. Compared to static methods, dynamic methods offer a more comprehensive analysis of privacy issues, thus providing researchers with enhanced insights for developing robust privacy protection strategies."}, {"title": "Membership Inference Attacks", "content": "The aim of membership inference attacks is to determine if a specific target sample was included in the training dataset of a designated model [5, 10, 20, 40, 50]. Such attacks represent a notable risk to data privacy"}, {"title": "Approach", "content": "MIAs can be categorized based on the design approach of the attack model into two types: classifier-based techniques and metric-based techniques. Classifier-based methods aim to determine if a particular sample being analyzed is part of the training set of the target model by creating a binary classifier. Frequently employed methods include the use of shadow model training [50]. Conversely, metric-based approaches distinguish between member and non-member instances by examining differences in various specific metrics. These metrics encompass prediction loss [57], confidence vectors [49], among others [52]."}, {"title": "Adversarial Knowledge", "content": "Adversarial knowledge refers to the information an attacker has about a target model, which greatly impacts the effectiveness and complexity of the attack. In the majority of attacks, an attacker is typically restricted to the model's output data. However, in scenarios where the attacker is the model publisher, such as during privacy evaluations in the model training phase, the attacker may possess full knowledge of the model. Attacks are categorized based on the attacker's knowledge level into black-box and white-box attacks [20, 42]. In black-box attacks, the attacker can only access the model's confidence scores [50] or just the hard labels [10, 31, 48]. Conversely, in white-box attacks, the attacker has access to detailed computational data regarding a particular sample [40]. This encompasses examining the model's internal architecture and operations, intermediate computation, gradient details, model loss, as well as all the information accessible in black-box situations."}, {"title": "LLMs Fine-tuning", "content": "In recent years, the domain of NLP has experienced substantial advancements, the continuous enhancement of machine computational capabilities and data acquisition methods has heralded a new paradigm. This methodology is marked by comprehensive pre-training on general domain data, followed by precise fine-tuning for specific tasks [61]. This development has been instrumental in the emergence of highly acclaimed and effective pre-trained language models, most notably the GPT series [44, 46, 47] and the Llama series [53, 61].\nIn the early stages of LMs fine-tuning, the predominant approach was full-parameter fine-tuning (FFT), i.e., all parameters of the pre-trained model were adjusted for downstream tasks. However, this method often requires substantial data to tune a large number of parameters, and as the model size increases, so does the demand for computational resources. Consequently, parameter-efficient fine-tuning techniques [35] have emerged. Among these, popular methods include prompt tuning [28, 33] and prefix tuning [29], where only a subset of the model's parameters is fine-tuned for downstream tasks, or more targeted adjustments are made to specific model components.\nAdapter-based methods [18, 32, 45] represent another approach, in which adapter structures are designed and embedded in the transformer architecture, and only the newly added adapter structures are fine-tuned during training. However, the former can pose challenges in optimization, while the latter may introduce inference latency [19]. In response to these challenges, researchers have proposed more advanced low-rank adaptation methods. These methods aim to confine changes in model weights to a low-rank subspace [19]. By decomposing the weight matrices of the model, only a small subset of parameters needs to be fine-tuned. This approach not only minimizes computational consumption, but also retains sufficient model flexibility to learn new tasks. Notable updated methods include AdaLoRA [60], LoftQ [30], among others."}, {"title": "Preliminaries", "content": "At a colloquial level, MIAs provide a means of quantifying privacy leakage risks, forming the foundation for many other attacks and privacy audit frameworks. They are also often used as a benchmark for evaluating privacy protection measures. In our study, we focus more on employing the MIA method as the core component of the auditing framework to identify privacy risks during the model fine-tuning phase. The auditor is the entity executing the attack. In this section, we outline the basic settings of the MIA component."}, {"title": "Capability of the Auditor", "content": "In this scenario, we examine an attack launched by the entity engaged in fine-tuning, meaning the auditor is actively participating in the fine-tuning procedure. Consequently, the auditor can extensively utilize the model's internal details to deduce its training data. This capability enables an assessment of the highest possible privacy leakage from the fine-tuned dataset."}, {"title": "Objective of the Auditor", "content": "Definition 1. (Classifier-Based Membership Inference Attack) Consider a target model $M_{ft}$ trained on a fine-tuned training dataset $D_{train}^{ft}$ and an attacker possessing knowledge $K$. Given an instance sample $x$, the membership inference module $A$ is a function that maps the input tuple $(x, M_{ft}, K)$ to a binary decision, defined as:\n$A(x, M_{ft}, K) \\rightarrow \\{0,1\\}$,\nwhere 1 indicates that the sample $x$ is inferred to be a member of the training dataset $D_{train}^{ft}$ while 0 indicates otherwise.\nThe auditor's objective is to ascertain whether sample $x$ is present in the fine-tuning training dataset $D_{train}^{ft}$."}, {"title": "Quantitative Metrics", "content": "To thoroughly assess PARSING's effectiveness and achieve the goal of quantifying privacy, we utilize three quantitative metrics: balanced attack accuracy [40, 50], TPR at low FPR [6] and AUC. And report the auditing results using the ROC curve presented on a logarithmic scale.\nIn an effort to expose privacy leakage risks, the balance accuracy metric is sufficient. In this context, \"balanced\" means that the training and testing datasets for audit models contain an equal number of member"}, {"title": "Design of PARSING", "content": "The complex and dynamic nature of fine-tuning LMs makes simple information extraction methods insufficient for capturing the detailed patterns learned from the training data. We concentrate on learning high-quality property embeddings from the white-box information of diverse samples, allowing the inference module to more effectively differentiate between non-member and member samples. In this section, we present a detailed overview of the principles and workflow of PARSING."}, {"title": "Data Partitioning", "content": "The auditor seeks to conduct a privacy examination on the target model during its fine-tuning stage. To achieve this, it is essential to divide a dataset $D_{inf}$ from $D_{ft}$ for audit before starting the fine-tuning and assign labels to these data. The original training dataset for model fine-tuning is defined as $D_{train}^{ft}= \\{(x_i,y_i)|i = 1, 2, ..., N\\}$,"}, {"title": "Property Extraction", "content": "Property refers to the intermediate computation results of the LMs on specific data, which we consider as properties of the data for the model. During the fine-tuning phase, the model is dissected to extract intermediate computation, which is then compiled into a repository of candidate properties for optimization of property embeddings. We categorize the properties into two groups: forward properties and backward properties. In particular, the definition is given as,\nForward property includes the results from each intermediate module, such as the token embedding, attention, and task-specific blocks. Given an input sample x and its label y, for each intermediate module $M_l$, where $1 < l < L$ and $L$ denotes the total number of intermediate modules in the model, compute its output $x_{l+1}$ to be used as the input for the subsequent block, which can be expressed as:\n$x_{l+1} = M_l(x_l)$.\nFor the entire target model, all forward properties is formalized as:\n$I_f = \\{M_l(x_{l-1})\\}_{l=1}^L \\cup \\{x,y\\}$.\nBackward property permits gradient computation that includes the calculated gradients $\\nabla_{\\Theta}L(M_{ft}(x), y)$ and their norms $|\\|\\nabla_{\\Theta}L\\||_p$ for each model parameter, calculated from the loss $L(M_{ft}(x), y)$. Here, $\\Theta$ denotes the set of all the parameters of the target model. The backward property repository is defined as:\n$I_b = \\{\\nabla_{\\Theta}L, ||\\nabla_{\\Theta}L\\||_p, L(M_{ft}(x), y) \\}$.\nThe white-box candidate property repository is formalized as follows:\n$I = I_f \\cup I_b$."}, {"title": "Property Embedding", "content": "Property embedding step transforms the properties extracted from the target model into feature representations used for privacy evaluation. The property embedding module is the central element of PARSING.\nDefinition 2. (Embedding with Forward and Backward Properties) Let $I$ be a candidate property set. Forward properties $I_f$ and backward properties $I_b$ are selected from $I$, ensuring alignment through a"}, {"title": "Membership Inference", "content": "The inference module, acting as the classifier, is trained using sample property embeddings r and explicit membership labels $y_{0,1}$. This component is structured as a multilayer fully connected neural network: the input layer processes the embeddings as features; the hidden layers explore the intricate relationships between these features; and the output layer determines membership status of the samples. For each sample's embedded representation r(x), the classifier predicts its membership label \u00ce, and the cross-entropy loss is:\n$L_{ce}= -\\frac{1}{N}[l_i log(\\hat{l_i}) + (1-l_i)log(1 - \\hat{l_i})]$,\nwhere l is the true membership label, and \u00ce is the predicted label. Cross-entropy loss is used to train the classifier to accurately distinguish member samples from non-member samples. Combine the losses of the two components for joint optimization, The final optimization objective combines the aforementioned losses:\n$L_{total} = L + vL_{ce}$,\nwhere v represents the weight parameter, balance the influence of each part's loss.\nThe property embeddings generated in the previous steps have been fine-tuned to equip the classifier with essential information for differentiating between member and non-member samples, significantly enhancing the precision of membership inference. The classifier is trained by minimizing the cross-entropy loss between the predicted membership labels and the actual labels $y_{0,1}$."}, {"title": "Application and Experiments", "content": "In this section, we empirically evaluate the effectiveness of PARSING in privacy auditing during model tuning. Empirically demonstrated the privacy vulnerabilities of fine-tuning LMs and analyzed the nature of their privacy leakage."}, {"title": "Experimental Setup", "content": "We have selected GPT-2 [47], GPT-Neo [3], and Llama2 [53] as the focus of our study. The primary reason for the selection is their extensive use in various tasks, making them essential subjects for investigating vulnerabilities in privacy leakage. Moreover, these models vary in size, allowing us to explore how their architecture and capacity influence their susceptibility to attacks. For every text task, we enhance the model with an optional task-specific layer."}, {"title": "Results Analysis", "content": "Our aim is to reveal risks as thoroughly as possible and quantify the privacy risks."}, {"title": "Conclusion", "content": "Our research provides valuable insights and tools for enhancing privacy resilience in the fine-tuning of LMs. By introducing a robust privacy auditing framework PARSING, demonstrating its efficacy through rigorous experiments, and thoroughly analyzing the privacy characteristics of LMs fine-tuning, we contribute to the development of safer and more reliable NLP applications. Future work will focus on extending the framework to more realistic fine-tuning applications and dynamic learning environments, with the aim of further refining privacy protection mechanisms in the evolving landscape of LM applications."}]}