{"title": "SLaNC: Static LayerNorm Calibration", "authors": ["Mahsa Salmani", "Nikita Trukhanov", "Ilya Soloveychik"], "abstract": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of billions of parameters have generated enormous pressure on the manufacturers of dedicated hardware accelerators and made the innovative design of the latter one of the most rapidly expanding fields of the AI industry. Various approaches have been explored to enable efficient and accurate processing of LLMs on the available accelerators given their computational and storage limitations. Among these, various quantization techniques have become the main focus of the community as a means of reducing the compute, communication and storage requirements. Quantization to lower precision formats naturally poses a number of challenges caused by the limited range of the available value representations. When it comes to processing the popular Transformer models on hardware, one of the main issues becomes calculation of the LayerNorm simply because accumulation of the variance requires a much wider dynamic range than the hardware enables. In this article, we address this matter and propose a computationally-efficient scaling technique that can be easily applied to Transformer models during inference. Our method suggests a straightforward way of scaling the LayerNorm inputs based on the static weights of the immediately preceding linear layers. The scaling factors are computed offline, based solely on the linear layer weights, hence no latency or computational overhead is added during inference. Most importantly, our technique ensures that no numerical issues such as overflow or underflow could happen during the compute. This approach offers smooth, accurate and resource-effective inference across a wide range of hardware architectures. The article provides theoretical justification as well as supporting numerical simulations.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) based on Transformers [1] have recently become the dominant Deep Neural Network (DNN) architecture due to their unprecedented performance results in all language modeling [2, 3], text processing [4], image and video generation [5], and many other tasks. However, this success comes at a cost of enormous volumes of compute, storage, and data transfer. A whole new industry of dedicated hardware accelerators has emerged in the last few years to accommodate the needs of LLM training and inference [6, 7]. Another major initiative targeted at making the inference feasible and sustainable involves the development of lower precision formats [8, 9, 10], efficient quantization techniques [11], algorithmic solutions [12], accurate approximations [13], and other software optimizations [14, 15].\nEfficient quantization techniques such GPTQ [16], AWQ [17], SmoothQuant [18], KVQuant [19], K-sort [20], and numerous others enable storing and processing of LLMs in low-precision formats. Often, that would involve training the model in FP32 format and casting it to 4, 8 or 16-bit precision formats before deployment onto inference hardware [11, 21, 20]. The most popular approach is to compress the static weights to 4 or 8-bit integers or floats and reduce the activations to FP16 or BF16"}, {"title": "Problem Formulation", "content": "Quantization of an LLM to a low-precision format (e.g., 4, 8 or 16-bit) can lead to a significant degradation of the output quality, and thus has to be applied together with some advanced technique capable of restoring the accuracy [16, 17, 18, 19, 20, 27, 28]. However, an even bigger challenge caused by casting models into low-precision formats is the limited dynamic range of such formats, which can completely ruin the compute flow if applied blindly. The most prominent example is the computation of LayerNorm, which becomes impossible on FP16 accelerators due to the unavoidable overflows and underflows as demonstrated next."}, {"title": "LayerNorm Compute", "content": "Layer Normalization (LayerNorm) has become one of the most ubiquitous non-linear operations in modern DNNs since it prevents the gradients from decaying or exploding during training. Extensive literature has demonstrated that the current DNN architectures cannot be practically trained without frequent normalization of hidden states [29, 30, 31]. State of the art Transformer models include dozens or even hundreds of LayerNorm operators which are introduced to facilitate training but make inference troublesome due to the numerical problems introduced by the computation of their denominators.\nGiven a row input $x \\in R^d$ and fixed parameters $\\gamma, \\beta \\in R^d$, the LayerNorm output reads as\n$y(x) = (\\frac{x - \\mu 1}{\\sigma}) \\odot \\Gamma + \\beta \\in R^d$,\nwhere\n$\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$ and $\\sigma = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2}$ \nAs Eq. 2 suggests, the standard way of computing requires summing up the squares of the input vector elements.\nNote also that the Transformer architecture comes in two flavors based on the location of the residual branch-out. It can take off before the LayerNorm (pre-LN residual) or after (post-LN residual), Fig. 1. Originally, the post-LN option was suggested [1] but later the other one became quite popular since it was observed to speeds-up the training [34]. To be specific and for lack of space below we focus on the post-LN desugn, however, we emphasize that the derivations and conclusions equally apply to the pre-LN one."}, {"title": "LayerNorm Scaling", "content": "The natural way of addressing the problem of overflow or underflow during computation of Layer-Norm would be to appropriately scale its input. Determining the correct scaling factors appears to be challenging because while avoiding overflow we also do not want to excessively dump the input causing underflow and vice versa. As a consequence, any reasonable scaling algorithm must take into account the actual LayerNorm input values and cannot set the scaling parameters blindly.\nA common solution would be to calibrate the scaling factors. This involves passing a test dataset through the Transformer to gauge the range of the input vector norms and setting the scaling factor based on some first-order statistic of this range (e.g., mean or median norm). This technique requires extra calibration data and significant computational overhead even for such a basic operation as LayerNorm, making this approach impractical."}, {"title": "Analytical Static Scaling", "content": "In this work, we propose a different methodology that enables analytical offline computation of the desired scaling factors. The scales are determined solely based on the static weights of the liner layers immediately preceding the LayerNorm at hand. This way we calibrate all the LayerNorm operators of a model statically, without using a calibration dataset or additional runtime resources everything is computed preemptively during model compilation.\nThe idea of the method is based on a simple observation that LayerNorms inside a Transformer occur frequently and in a regular pattern since any large Transformer is a chain of dozens of identical decoders. Typically, two consecutive LayerNorms surround the attention or the Multi-Layer Perceptron (MLP) block of every decoder. Eq. 1 suggests that we can treat a LayerNorm as a Euclidean"}, {"title": "SLANC for Standard MLP Block", "content": "To illustrate the idea, let us consider the MLP block of a standard Transformer, Fig. 2a. Since we neglect the additive bias \u03b2, the output of the MLP block can be expressed as\n$y = F (x\\Gamma E) G + x\\Gamma$,\nwhere the addition comes from the residual connection, and is an element-wise non-linearity which is usually a contraction function (e.g. ReLU, GeLU, etc.) making the norm of its argument smaller. Since usually, the maximal partial derivative of is bounded by a constant close to one, we can approximate the norm of y as\n$\\Vert y\\Vert \\propto \\Vert xFEG + xF \\Vert F$.\nEventually, we conclude that\n$\\frac{\\Vert y\\Vert}{\\Vert x\\Vert} \\propto \\Vert \\Gamma(EG + I) \\Vert F$.\nRecall that x is the output of the normalization step of a LayerNorm (see Fig. 2a) and thus has unit norm. Therefore, it is natural to set the scaling factor of the following LayerNorm to the right-hand side of Eq. 5 and this should solve the overflow/underflow issue. In Section 4, we demonstrate by extensive simulations that this is actually the case. Note that the scale determined by Eq. 5 only involves static weights and can be computed offline."}, {"title": "SLANC for Llama MLP Block", "content": "Using the same methodology, we derive an analogous formula for the scaling factors of the Layer-Norm following the modified MLP block designed for the decoders of the Llama family of models, Fig. 2b. Here, in addition to the two linear layers of the standard MLP block, we have another linear layer whose output is multiplied with the output of the non-linearity in the element-wise manner. The non-liner function itself is usually chosen to be GeLU. The input of the post-MLP block LayerNorm y reads as\n$y = (F (x\\Gamma E)\\odot x\\Gamma B) G + x\\Gamma$.\nSimilar principles as above together with basic properties of matrix norms yield\n$\\Vert y\\Vert \\approx \\Vert \\Vert \\Gamma E\\Vert xFBG + xF\\Vert F$,\nwhere we used the fact that Finally, the scaling factor computes as\n$\\frac{\\Vert y\\Vert}{\\Vert x\\Vert} \\approx \\Vert \\Gamma (\\Vert \\Gamma E\\Vert BG + I) \\Vert F$."}, {"title": "SLANC for the Attention Block", "content": "Next, we derive a formula for the scaling factor of the LayerNorm following the standard attention block with h heads. As it can be seen in Fig. 2c, the most critical observation here is that the product of the Softmax output $S^i$ of head i with $V^i$ results in a convex combination of the rows of the latter. The outputs {$SiVi$}$_{i=1}^{h}$ are concatenated, hence, the norm of the concatenated vector can be approximated by the norm of the concatenation of {$xFW^i$}$_{i=1}^{h}$ which is precisely $xFWv$. We get\n$\\Vert y\\Vert \\propto \\Vert xFWvP + xF\\Vert F = \\Vert xF(WvP + I)\\Vert F$,\nand conclude that the following scale should be used in the post-attention LayerNorm operator\n$\\frac{\\Vert y\\Vert}{\\Vert x\\Vert} \\propto \\Vert \\Gamma (WvP + I) \\Vert F$."}, {"title": "Experiments", "content": "To demonstrate the power of our SLaNC scaling technique, we present simulation results for Llama models. Note that the Llama architecture replaces the LayerNorm by Root Mean Squared Layer Normalization (RMSNorm) [35], which differs from the former only by omitting the mean \u03bc subtraction in Eq. 2 and thus does not affect SLaNC scaling.\nIn our first experiment, we collected empirical statistics of the sums of squares in the denominators of the RMSNorm operators without scaling and with SLaNC scaling. To this end, we applied Llama-2-7b to Wikitext 2 dataset.\nNext, we compared the perplexities of Llama models on the same Wikitext 2 dataset with the default FP32 implementation of RMSNorm and with the sum of squared accumulated in FP16 (all other operations from the default setup intact).\nSince SLaNC scales are known ahead of time, we can easily apply them to the \\epsilon constants as well (in fact, we divide \\epsilon by the squared SLaNC scalings). As the bottom row of Table 1 demonstrates, now the FP16 SLaNC scaling can precisely reproduce the default FP32 values."}, {"title": "Conclusion", "content": "In this paper, we present a novel SLaNC technique that makes LLM inference possible on FP16 accelerators without the need to cast LayerNorm operators into FP32. This theoretically grounded approach provides easy-to-use formulae for an offline computation of scaling factors for the inputs of LayerNorms. The SLaNC scaling factors guarantee precise computation of the LayerNorm in FP16 and provably avoid overflows and underflows. By keeping all the compute in FP16, the SLaNC algorithm enables low latency accurate compute, which is demonstrated by our extensive numerical simulations."}]}