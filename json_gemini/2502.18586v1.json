{"title": "Autonomous Vision-Guided Resection of Central Airway Obstruction", "authors": ["Mariana E. Smith", "Nural Yilmaz", "Tanner Watts", "Paul Maria Scheikl", "Jiawei Ge", "Anton Deguet", "Alan Kuntz", "Axel Krieger"], "abstract": "Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.", "sections": [{"title": "1. Introduction", "content": "Central airway obstruction (CAO) refers to an occlusion of the trachea most commonly caused by tumor extension into the airway [1]. In the United States, malignant growths cause CAO in 80,000 new patients every year, posing significant morbidity [2]. Of all patients diagnosed with CAO, 20% will suffer from cough, shortness of breath, and obstructive pneumonia [3]. In acute cases, patients are unable to breathe due to full airway occlusion, requiring emergency resection and stabilization. Most patients diagnosed with CAO also present with advanced lung cancer. In this context, the primary purpose of addressing CAO is to provide palliative care aimed at enhancing quality of life by improving respiratory function [4].\nTo avoid the risks associated with open surgery, surgeons address CAO by deploying a rigid hand-held bronchoscope through the patient's mouth. A widely employed surgical technique is the 'core-out' technique, wherein the surgeon uses the bevel of the bronchoscope to scrape the CAO from the tracheal wall [5,6]. After the tumor is removed, instruments and stents can be deployed through the inner lumen of the bronchoscope [3]. However, the core-out technique requires frequent repositioning and tilting of the bronchoscope, using the patient's mouth as the fulcrum. This maneuver places significant stress on the patient's mouth and neck, leading to complications such as spinal injury in approximately one-third of cases [6].\nPrecise electrocautery is an alternative to the core-out technique, to minimize trauma to the trachea during CAO removal. This approach was successfully demonstrated by Gafford et al., who performed minimally-invasive CAO removal with a teleoperated robotic bronchoscope equipped with a monocular endoscope and two concentric tube manipulators for gripping and electrocautery [7]. The CAO removal was performed on cadaver tissue, showing a significant reduction in applied force to the patient compared to the core-out technique.\nHowever, the robotic system must still be operated by a human surgeon. To address the current shortage of surgeons, recent studies have introduced autonomy in robotic surgical procedures to reduce surgeon workload, particularly during time-consuming or repetitive tasks. Several studies have focused on autonomous robotic tumor resection in the past decade, including removal of foam tumor fragments [8], suction of gelatinous fluids [9], resection of a rubber tumor model [10], and electrosurgery of pseudo-tumors in porcine tissue [11]. Most recently, a vision-guided robotic system demonstrated the first supervised autonomous tongue tumor resection (i.e., glossectomy) using animal tissues [12-14]. Reported accuracy was on par with manual resections performed by an expert surgeon. However, no studies have yet shown an autonomous resection of CAO. This work is inspired by the demonstrated success of autonomous glossectomy and by the potential of minimally-invasive robots to perform precise resection within the trachea. This study represents the first demonstration of a supervised autonomous vision-guided workflow for the surgical resection of CAO.\nDevelopment of an autonomous system for CAO removal requires addressing challenges in perception, planning, robot control, and miniaturization. In this paper, we address the challenges of perception and planning by using a simplified setup. We use rigid-link robotic arms and an open-surgery environment to validate our vision-based autonomous workflow. Our workflow enables straightforward transfer to systems that address miniaturization and control challenges. In future work, the workflow in this paper may be adapted to minimally-invasive manipulators capable of operating within an enclosed trachea.\nThe contributions of this work are 1) development of an ex-vivo CAO animal tissue model designed for an open-surgery approach, which allows the use of large rigid-link robots while maintaining realistic tissue effects, and 2) open-surgery demonstration of the first vision-based supervised autonomous workflow for the resection of CAO in five consecutive models."}, {"title": "2. Methods", "content": "The full system (Fig. 1a) is comprised of a robot-mounted electrocautery tool, a robot-mounted RGBD camera, and a passive laparoscopic gripper. A tissue model is placed on an electrosurgical grounding platform between the robots. The passive gripper tensions the tumor while the robot-mounted electrocautery tool performs the autonomous cut with visual guidance from the mounted RGBD camera, enabling supervised fully autonomous resection of CAO."}, {"title": "2.1. Robotic Hardware", "content": "Two UR manipulators (Universal Robots, Odense, Denmark) are used in this study. The UR10e manipulator is equipped with an electrosurgical instrument, which was custom-built from a monopolar electrode (Bovie, Clearwater, FL) with a laparoscopic extension. The UR5 robot holds the RGBD camera (Zivid 2 Plus M60, Oslo, Norway) for imaging of the surgical scene. The Zivid 2 Plus M60 RGBD camera employs structured light 3D imaging and is factory-calibrated. A standard hand-eye calibration procedure was performed with the Zivid camera to obtain the necessary transformation for system registration. A fiducial-based calibration was conducted to obtain the transformation between the UR5 and the UR10e. As in the minimally-invasive case, the electrocautery tool in this work performs each cut while approaching from the distal end of the trachea, as if inserted through the mouth. However, due to the size of the rigid-link robots, the gripper and camera cannot also approach from the distal end; instead, we place the camera above the tumor and the gripper approaching from the proximal end. However, the vision-based resection workflow is not reliant on this configuration and would still be viable if it was transferred to smaller manipulators which all approach from the distal end. Also, since this study focuses on the automation of the resection task alone, the gripper holding the tumor in the other arm was not automated. For the experiments in this paper, a laparoscopic gripper (Snowden-Pencer SP90-6379, Tucker, GA) was fixed on a passive arm and manually translated incrementally along its primary axis to move the tumor before each cut took place. Automating this aspect is left to future work."}, {"title": "2.2. Surgical Process Modeling", "content": "Before designing the robotic control workflow, it is necessary to define the surgical subtasks which make up the complete procedure of CAO resection. We adopt a subtask-level Surgical Process Model (SPM) in our approach, inspired by [15]. Using SPM representations, the procedure can be split into three subtasks: Reach-in, Resect, and Retract. A finite state machine (FSM) approach naturally follows from these discrete subtasks. FSMs are widely used in autonomous processes due to their maintainable, intuitive structure [16]. In this work, the Reach-in, Resect, and Retract subtasks become finite states, where specific events are programmed to trigger state transitions."}, {"title": "2.3. Control Workflow", "content": "In our autonomous resection framework (Fig. 2), we aim to provide a systematic approach to supervised tumor resection with point cloud data and polynomial surface fitting. First, data acquisition is conducted as the system uses the Zivid RGBD camera to obtain a snapshot of the surgical scene (which contains a point cloud, color image, depth image, and camera intrinsics). This snapshot data is fed to the segmentation node, which autonomously generates bounding boxes around the tumor and trachea in the color image, and segmentations of the tumor and trachea in the point cloud. These bounding boxes and segmentation are plotted for visualization by the supervisor. The program then prompts the supervisor to either accept the autonomous segmentation or to reject it and hand-draw the bounding boxes, which are then used for autonomous segmentation. The segmented trachea and tumor are published as point clouds to independent ROS topics.\nNext, the trachea surface is fitted with a polynomial surface model. This fitted surface provides a foundation for trajectory planning, where the cutting paths are generated along said surface. After a cut path has been generated, it is displayed to the human supervisor in RViz, along with the RMSE between this cut and the cut which was predicted using the dataset (surface fit and point clouds) stored from the previous cut. The RMSE between cuts indicates how much the newly measured tumor shape and cut trajectory differs from the prior model's predictions (see Section 2.7 for a more detailed explanation of RMSE calculations). If the RMSE value exceeds a set threshold, the system self-supervises by displaying an error to the human operator; the human operator can then choose to re-image and re-plan the cut if desired.\nThis process is divided into three phases of execution using a finite-state approach: Reach-in, Resect, and Retract."}, {"title": "2.4. Tissue Models", "content": "Due to the size of our rigid-link end-effectors, which limits their operation within a confined trachea, we introduce an ex-vivo CAO tissue model for an open procedure, comprised of chicken tissue and a 'half-pipe' of porcine trachea (Fig. 1b, 1c). The usage of real animal tissue provides realistic effects from electrocautery and tissue deformation, as well as accurate color and texture information for segmentation, making our findings readily applicable to in-vivo scenarios."}, {"title": "2.5. Segmentation", "content": "The segmentation pipeline extracts the trachea point cloud from RGBD snapshots to generate the cut trajectory. As shown in Fig. 3, the pipeline combines a custom object detection model based on Faster R-CNN [17], Meta's Segment Anything model (SAM 1) [18], and 2D mask projection to generate labeled point clouds."}, {"title": "2.5.1. Object Detection Model", "content": "Based on the Faster R-CNN architecture [17], the custom object detection neural network produces labeled bounding boxes and their classification scores (CLS Score), for the trachea and tumor. The implementation leverages a pre-trained Faster R-CNN model with ResNet50 FPN backbone [19] (trained on the COCO dataset), modified to locate the trachea and tumor. Transfer learning enables a model fine-tuning by replacing the final prediction layer with a custom layer to classify three classes (background, trachea, and tumor). The model was trained using stochastic gradient descent (SGD) with momentum (0.9) and weight decay (0.0005) at a learning rate of 0.005 for 10 epochs. The training dataset was manually labeled by a human operator who drew the bounding boxes. The dataset comprised 137 labeled bounding boxes for tissue and trachea, processed in mini-batches of 4 images. Data augmentation was limited to tensor conversion to preserve spatial relationships between the trachea and the tumor. Classification was performed using a softmax function, which converts raw output scores into class scores through exponentiation and normalization. During inference, if the classification score is below 70% we ask the human to provide the bounding box."}, {"title": "2.5.2. Segmentation and Depth Projection", "content": "Our approach combines Faster R-CNN's automatic localization capabilities with SAM's prompt based foundation model architecture to create an end-to-end segmentation pipeline. Faster R-CNN first predicts the location of tumor and trachea regions, which then serve as learned prompts for SAM to generate precise segmentation masks, eliminating the need for manual bounding box annotations from human supervisors. The generated binary masks from SAM are applied to the depth image, effectively setting all non-masked pixels to zero depth. This operation isolates the region of interest in the depth map. Given a masked depth image and the camera intrinsics, the pinhole camera model projects the pixels to point clouds:\n$X = \\frac{(u - C_x) \\cdot Z}{f_x}$\n$Y = \\frac{(v - C_y) \\cdot Z}{f_y}$\n$Z = depth\\_value.$\nHere, the X and Y coordinates are calculated by subtracting the pixel values u, v from the principal points $C_x, C_y$, which are then divided by the x focal lengths $f_x, f_y$. After projecting the two binary masks to point clouds, the final trachea point cloud is refined by subtracting the tumor point cloud, producing clean, labeled point clouds suitable for surface model fitting."}, {"title": "2.6. Trachea Surface Modeling", "content": "Following segmentation, the trachea point cloud has a hole where the tumor is located due to occlusion by the pseudo tumor in the imaging process. We use a surface modeling process to \"fill in\" this hole, providing a continuous reference surface for trajectory planning.\nTo best fit the trachea, various surface fitting models were applied to point clouds taken from four discrete trachea models (Fig. 4). The objective was to determine the optimal fitting models by minimizing two criteria: computation time and root mean squared error (RMSE). A Pareto front analysis was performed for each trachea model to identify the best models based on the trade-off between these two objectives. To model the trachea surface accurately, we applied polynomial surface fitting models to point cloud data obtained from four different ex-vivo trachea models (captured in .pcd format). A series of polynomial surface fitting models, ranging from degree 1x1 (poly11) to 10x10 (poly1010), were applied to approximate the shape of the trachea. The fitting process involved solving a least-squares regression problem, where polynomial coefficients were determined from the trachea point cloud. Once the polynomial surfaces were fitted, we computed RMSE as a measure of how well each fitted surface matched the original trachea point cloud. This provided a quantitative assessment of how accurately each polynomial model approximated the real trachea geometry. After computing RMSE separately for all four trachea models, we took the mean RMSE across the four models for each polynomial. In Fig. 4, the subplot grid layout is organized by polynomial orders, where the columns represent the increasing polynomial degree in the Y direction and the rows represent the increasing polynomial degree in the X direction.\nIn addition to the error, the computation time required to fit each surface to the trachea data was recorded. This served to evaluate the computational efficiency of each model. After obtaining the RMSE and time values for all models, a Pareto front analysis was conducted, the results of which are shown in Fig. 5. In Fig. 5, each point represents a polynomial model plotted based on its RMSE and computation time. The models closer to the Pareto front (red dots) offer an optimal balance between accuracy and efficiency.\nEven the smallest differences in RMSE are of critical importance, particularly in the context of autonomous surgery. When examining the Pareto optimal polynomials with the mean RMSE values below 1 mm, it is observed that the mean RMSE does not significantly decrease beyond poly55, stabilizing around 0.6 mm. Among the tested models, the 5th-order polynomial (poly55) achieved the lowest computation time and was therefore chosen as the optimal model. Using poly55, the polynomial surface model is expressed as:\n$P(x,y) = \\sum_{i=0}^{5} \\sum_{j=0}^{5} a_{ij} x^i y^j  where  i + j \\le 5.$\nHere, $a_{ij}$ are the polynomial coefficients to be determined. This polynomial expression defines the surface by capturing the relationship between the x and y coordinates and the resulting z values (surface height). To obtain the coefficients, the known x and y values are substituted into P(x, y), and the resulting system of equations is solved by equating the polynomial output to the corresponding z values from the point cloud. This fitting process generates a polynomial surface that closely models the trachea, where i and j are non-negative integers up to 10, with each term $x^i y^j$ representing a combination of polynomial degrees in x and y."}, {"title": "2.7. Electrocautery Pitch Angle and Trajectories", "content": "To determine what pitch angle to set the electrocautery tool during the autonomous resection, four handheld surgical procedure demonstrations (with manual cautery and a laparoscopic gripper) were analyzed. In each demonstration, a point cloud snapshot of the surgical scene was taken during each of the first four cuts. When these point clouds were viewed in the x-z plane, the angle of the electrocautery tool could be determined by fitting a linear model to points along the tool, as shown in Fig. 6. From these demonstrations, a mean electrocautery pitch of 28.3\u00b0 \u00b1 4.6\u00b0 was calculated (Table 1). This informed the programmed pitch angle of the electrocautery tool during the autonomous resection.\nFor each trajectory generation, the generated surface fit of the trachea is utilized. The trajectory is comprised of six total cut paths, which are designed to be evenly spaced over the size of the tumor in the X-Y plane. The number of cut paths is currently fixed to the arbitrary value of 6 cuts; in the future this could be modified such that the number of cuts varies based on tumor size. For each cut path, the Z values are determined using the polynomial surface model of the trachea, with an added offset of 1 mm as not to puncture the trachea surface, thus defining the trajectory path 1 mm above the real trachea surface. Each cut path is traveled left-to-right by the electrocautery tool, and then retraced right-to-left, after which the electrocautery tool returns to a pulled-back home position. Fig. 7 illustrates the fitted surface and cut paths as simulated in RViz. In cycle i, the program receives the tumor cloud i and trachea surface i, and plans cut paths i and i+1. The robot then executes cut path i, and the tumor is then retracted backward. Then, in cycle i+1, the program receives new data for the tumor cloud i+1 and trachea surface i+1. It then plans cut paths i+1 and i+2. Before executing the cut path i+1, the program first compares the RMSE errors between the surface i+1 created in the first cycle i, to the surface i+1 created in the second cycle i+1. The RMSE check serves as a self-supervision mechanism to detect significant discrepancies in trajectory predictions. If RMSE is low, the next cut follows the predicted path without human intervention. If RMSE is high, this suggests segmentation errors, tissue deformation or unexpected tool interactions, prompting a human supervisor to validate the cut before execution. Sometimes, errors in surface modeling can occur due to inaccurate segmentation. In such cases, human supervision assesses the RMSE to determine whether the trajectory for cut i+1 is suitable. This process continues for all cuts."}, {"title": "3. Experiments", "content": "We tested our autonomous resection workflow by conducting consecutive procedures on five distinct tissue models. First, five CAO tissue models were fabricated to mimic a range of clinical cases such that they are somewhat varied in shape and size. Each tissue model was created by first sculpting a piece of chicken tissue (Giant Food, Carlisle, PA) such that the base measures approximately 50 mm by 75 mm, and the attached tumor protrusion is approximately 20 mm in diameter. The chicken tissues, which were stored in a freezer prior to the model fabrication, tended to release moisture over time, making them challenging to grasp securely with the standard gripper. To address this, fresh chicken tissues were used, and allowed to dry slightly before beginning the procedure. Ex-vivo porcine tracheas (Animal Technologies, Tyler, TX) were cut into \u2018half-pipes\u2019 and then into lengths of approximately 75 mm. A hole was cut in the center of each trachea. Each trachea was lowered down onto the chicken model, and secured into place with Loctite 415 (McMaster-Carr, Elmhurst, IL) and Insta-Set accelerator (Bob Smith Industries, Paso Robles, CA). The wide base made of chicken tissue provided ample contact area between each model and the electrosurgical grounding plate, enabling steady monopolar electrocautery. After all five models were fabricated, preliminary CT scans were taken of each model. The ground-truth segmentations of the tumors were then obtained from the CT (using 3D Slicer [20]), as shown in Fig. 11.\nTo begin each procedure, a tissue model was placed onto the electrosurgical grounding plate and secured in place with four stay sutures. Two pieces of black felt cloth were placed under the trachea to obfuscate the background from the camera view for ease of segmentation. The Zivid camera first captured a depth image, RGB image, camera intrinsics, and 3D point cloud of the surgical scene. Using this data, a custom Faster R-CNN model generated bounding boxes and SAM-based segmentation masks which were projected onto the depth data. A fifth-degree polynomial (poly55) was fitted to the segmented trachea to generate smooth cut trajectories. The robot then began the Reach-in stage, aligning with the trachea's centroid, followed by the Resect stage, executing cuts at 2 mm/s with 24 W electrosurgical power. A Smoke Shark (Bovie, Clearwater, FL) evacuated cautery smoke. After each cut, the robot entered the Retract stage to return to a safe position. A manually positioned laparoscopic gripper then tensioned the tumor to expose the cut boundary. The cut was then shifted by L/6 mm for subsequent resections, where L is the tumor length along the trachea. Process images taken during the experiment are shown in Fig. 8. The procedure was stopped when the tumor was detached, and a postoperative CT scan evaluated resection effectiveness."}, {"title": "3.1. Results", "content": "All five CAO models and their corresponding post-procedural outcomes are illustrated in Fig. 11. By qualitatively comparing the side views provided, it is evident that the tumor was successfully removed from the trachea in all five consecutive trials. Models 1 and 3 were over-resections (119.5% and 114.9% of tumor removed, respectively), while Models 2, 4, and 5 were under-resections (91.1%, 92.2%, and 92.0% of tumor removed, respectively). In this context, it is not necessary to resect a margin around the tumor, since CAO removal is a palliative and not curative procedure. Mudambi et al. defines successful CAO removal as a reopening of the airway lumen to over 50% of the nominal diameter [21]. By this definition, all five of our procedures can be deemed successful, marking feasibility demonstrations of autonomous vision-based CAO resection in 5 out of 5 trials.\nAdditionally, we analyzed the surfaces of the models after each procedure. After each resection was complete, a snapshot was taken of the tissue model, and the regular point cloud segmentation method was conducted. A surface model was fit to the trachea point cloud and then raised in Z by 1 mm to create the 'goal' surface (which, according to the programmed trajectories, should align closely with the charred top of the tumor). A point cloud of the charred area was also obtained. For each point in the charred area, the Z-value was compared to the corresponding Z-value of the 'goal' surface. In this way, RMSE could be calculated between the programmed cut surface and the real cut surface seen post-procedure. The tissue surfaces for the actual tissue (charred tissue post-cut) and fitted tissue (based on trachea surface) for all five models are shown in Fig. 9. The RMSE for each model is also provided, which range from a minimum of 1.23mm to a maximum of 2.65 mm.\nWe also analyzed the effectiveness of our segmentation pipeline through the five procedures. We found that the ability to extract accurate point clouds relied more on the ability to predict bounding boxes than on the resulting segmentation of the objects (which leverages the predicted bounding boxes). Our custom Faster R-CNN model outperformed simple CNNs in terms of both intersection over union (IoU) scores and the downstream task of producing an accurate trachea point cloud, as shown in Fig. 10. The faster R-CNN had an IoU of (0.762\u00b10.283) for the trachea and (0.592\u00b10.36) for the tumor. The simple CNN had IOU scores of (0.49\u00b10.2567) for the trachea and (0.150\u00b10.181) for the tumor (higher is better for IoU)."}, {"title": "4. Discussion", "content": "As mentioned in the Results, the clinical benchmark for successful resection of CAO is a reopening of the airway lumen to over 50% of its nominal diameter. Our resections were successful by this definition. Moreover, our results indicate a potentially superior clinical outcome of lumen reopening by over 90%. From a technical standpoint, we had set an initial target to leave a 1 mm cutting offset above the trachea. This targeted offset was intended to remove approximately 90% of the tumor, leaving the remaining 10% to avoid damaging the trachea. However, due to calibration limitations and minor bending of the electrocautery tool during the procedure, the system occasionally deviated from this target. It is also a possibility that those deviations were caused in part by the relative spatial inaccuracies of the UR robots (0.1mm repeatability for the UR10e). In some trials, the tumor was under-resected as intended, but in others, the tumor was over-resected because the electrocautery tool penetrated into the trachea. In future work, it will be key to conduct a more exact calibration, use a more accurate robotic system, or possibly switch to a more rigid electrocautery tool, to permit higher accuracy in resection.\nThe tool-tissue interactions between the chicken and gripper also posed challenges during the experiments. When the chicken was too wet, it easily slipped out of the gripper. The chicken also began to tear after being gripped for an extended period of time. In future work, we hope to employ an automated gripping strategy rather than the passive heuristic gripping used in this study. An autonomous gripper may be able to detect tissue slipping and prompt a re-gripping motion, and it could perhaps also modulate the applied force. As we look toward employing our workflow on minimally-invasive robots in the future, a possible solution for gripping could be to instead push and prod the tumor away from the cut, rather than biting it with a toothed gripper. This would be advantageous since it would likely cause less tissue damage, would require less applied force, and would allow the minimally-invasive gripper to approach from the distal (mouth) end of the trachea.\nConsidering the minimally-invasive translation, it is relevant to reconsider the position of the camera in our configuration. In this study, we placed the camera above the tissue, since the robotic components were too large to place the camera on the wrist of the electrocautery tool. However, in a minimally invasive case, the electrocautery tool, gripper, and camera would all approach together from the distal (mouth) end. While a camera approaching from the distal end would have a much different perspective than the camera in this study, we hypothesize that a distal camera would actually be advantageous for our resection task. A distal camera would be able to see directly into the cuts being made, and could perhaps provide a more accurate representation of the boundary between the tumor and the trachea. If translated to a minimally-invasive robot, the camera would likely take the form of some monocular endoscope, with depth information inferred from the endoscopic footage using techniques such as SLAM and SFM [22,23].\nThere were also some challenges present in our segmentation network. The intermediate stages of tumor removal were not significantly represented in the training data, leading to certain faults during segmentation. For example, tissue charring from electrocautery was occasionally detected as part of the tumor, necessitating the human supervisor to replace the predicted bounding boxes with manually drawn ones. During the procedures presented in this study, the human chose to intervene to draw manual boxes for 5 out of the 27 total segmentation steps. Also, the segmentation process occasionally included a part of the gripper in the trachea mask, leading to outliers in the trachea point cloud. This led to errors in the surface model and consequently in the generated trajectories, particularly along the Z-axis. This misassignment resulted in inaccurate depth estimations, affecting the precision of the planned cuts and increasing the possibility of unintended interaction with the trachea. In future work, it will be necessary to include a proper distribution of mid-procedure charred data in the training data for segmentation.\nMost importantly, our current resection architecture is designed as an open-loop system. As future work, we aim to develop a closed-loop framework that incorporates a feedback mechanism to adapt to tissue deformations in real time. Such a system would enable the resection process to dynamically respond to changes in the surgical environment.\nTo translate this research to minimally-invasive manipulators which can fit within the trachea, our workflow would remain viable, with the snapshot and segmentation processes replaced by endoscopic monocular SLAM [22,23]. To incorporate autonomous functions into surgical workflows, systems like the da Vinci robot could suggest cutting plans, similar to a \"park assist\" feature in cars. The robot would analyze the surgical site and propose actions, but the surgeon could easily intervene at any point. To ensure safety and effectiveness, the system would allow real-time overrides and provide transparent reasoning for its suggestions."}]}