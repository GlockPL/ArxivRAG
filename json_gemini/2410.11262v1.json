{"title": "UNVEILING OPTIONS WITH NEURAL DECOMPOSITION", "authors": ["Mahdi Alikhasi", "Levi H. S. Lelis"], "abstract": "In reinforcement learning, agents often learn policies for specific tasks without the\nability to generalize this knowledge to related tasks. This paper introduces an algo-\nrithm that attempts to address this limitation by decomposing neural networks en-\ncoding policies for Markov Decision Processes into reusable sub-policies, which\nare used to synthesize temporally extended actions, or options. We consider neural\nnetworks with piecewise linear activation functions, so that they can be mapped to\nan equivalent tree that is similar to oblique decision trees. Since each node in such\na tree serves as a function of the input of the tree, each sub-tree is a sub-policy\nof the main policy. We turn each of these sub-policies into options by wrapping\nit with while-loops of varied number of iterations. Given the large number of op-\ntions, we propose a selection mechanism based on minimizing the Levin loss for\na uniform policy on these options. Empirical results in two grid-world domains\nwhere exploration can be difficult confirm that our method can identify useful\noptions, thereby accelerating the learning process on similar but different tasks.", "sections": [{"title": "INTRODUCTION", "content": "A key feature of intelligent agents that learn by interacting with the environment is their ability to\ntransfer what is learned in one task to another (Andr\u00e9 & Markovitch, 2005; Taylor & Stone, 2009).\nIn this paper, we investigate extracting from neural networks \u201chelpful\u201d temporally extended actions,\nor options (Sutton et al., 1999), as a means of transferring knowledge across tasks. Given a neural\nnetwork encoding a policy for a task, where a policy is a function that returns an action for a given\nstate of the task, we decompose the network into sub-networks that are used to create options.\nWe assume that neural networks encoding policies use two-part piecewise-linear activation func-\ntions, such as ReLU (Nair & Hinton, 2010), which is standard in reinforcement learning (Mnih\net al., 2013). As shown in previous work (Lee & Jaakkola, 2020; Orfanos & Lelis, 2023), such\nnetworks can be mapped into oblique decision trees (Breiman et al., 1986), which is a type of tree\nwhere each node encodes a linear function of the input. In this paper, we map networks with piece-\nwise linear functions to a structure similar to an oblique decision tree, which we call a neural tree.\nLike an oblique decision tree, each internal node in the neural tree is a function of the input of the\nnetwork. Unlike an oblique tree, the leaf nodes of neural trees represent the entire output layer of\nthe network. Thus, each node in the neural tree, including leaf nodes, represents a sub-policy of the\nneural policy. We hypothesize that some of these sub-policies can generalize across tasks and we\nturn them into options by wrapping them in while-loops that perform different numbers of iterations.\nSince the number of sub-policies grows exponentially with the number of neurons in the network,\nwe introduce a procedure to select a subset of options that minimizes the Levin loss (Orseau et al.,\n2018) on a set of tasks that the agent has already mastered. We compute the Levin loss for the uni-\nform policy over the options because such a policy approximates the policy encoded in a randomly\ninitialized neural network representing an agent in the early stages of learning. If the tasks used to\ngenerate and select options are similar to the next task, minimizing the Levin loss can increase the\nchances that the agent visits promising states early in learning, thus guiding the agent's exploration.\nOur method of learning options has benefits compared to previous approaches. First, it enables the\nextraction of options from neural policies, even when learning options was not the original intention.\nThis implies the possibility of learning options from \"legacy agents\", provided that their networks\nuse piecewise-linear functions. Second, our method automatically learns from data when to start"}, {"title": "RELATED WORK", "content": "Options Temporally extended actions have a long history in reinforcement learning. Many pre-\nvious works rely on human knowledge to provide the options (Sutton et al., 1999) or components\nto enable them to be learned, such as the duration of the options (Frans et al., 2017; Tessler et al.,\n2017), the number of options to be learned (Bacon et al., 2017; Igl et al., 2020), or human supervi-\nsion (Andreas et al., 2017). Our decomposition-based method learns all options components from\ndata generated by the agent interacting with the environment. Other approaches use specific neu-\nral architectures for learning options (Achiam et al., 2018), while we show that options can \"occur\nnaturally\" in neural policies, even when it was not intended to learn them. Options have also been\nused to improve exploration in reinforcement learning (Machado et al., 2018; Jinnai et al., 2020;\nMachado et al., 2023). We minimize the Levin loss for the uniform policy with the goal of guiding\nexploration in the early stages of learning. However, instead of covering the space, we equip the\nagent with the ability to sample sequences of actions that led to promising states in previous tasks.\nTransfer Learning Transfer learning approaches are represented by different categories such as\nregularization-based, such as Elastic Weight Consolidation (Kirkpatrick et al., 2017), which pre-\nvent the agent from becoming too specialized in a given task. Others focused on adapting the\nneural architecture to allow for knowledge transfer across tasks, while retaining the agent's ability\nof learning new skills. Progressive Neural Networks (Rusu et al., 2016), Dynamically Expandable\nNetworks (Yoon et al., 2017), and Progress and Compress (Schwarz et al., 2018) are representative\nmethods of this approach. Previous work also stored past experiences as a way to allow the agent to\nlearn new skills while retaining old ones (Rolnick et al., 2019). Previous work has also transferred\nthe weights of one model to the next (Narvekar et al., 2020). One can transfer the weights of a\npolicy (Clegg et al., 2017), of a value network, or both (Shao et al., 2018). SupSup (Wortsman et al.,\n2020) and Modulating Masks (Ben-Iwhiwhu et al., 2022) also transfer knowledge by learning masks\nfor different tasks. The use of masks is particularly related to our approach because they also allow\nthe agent to use sub-networks of a network, but to learn policies instead of options.\nCompositional Methods Compositional method attempts to decompose the problem so that one\ncan train sub-policies for the decomposed sub-problems (Kirsch et al., 2018; Goyal et al., 2019;\nMendez et al., 2022). These methods assume that the problem can be decomposed and often rely on\ndomain-specific knowledge to perform such a decomposition. \u03c0-PRL learns sub-policies in earlier\ntasks, and these sub-policies are made available as actions to the agent (Qiu & Zhu, 2021). We also\nlearn policies in earlier tasks as a means of learning options, but we consider all sub-policies of a\npolicy to learn options, instead of considering only the final policy as an option, as in \u03c0-PRL.\nWe use representative baselines of these categories in our experiments, including Option-Critic (Ba-\ncon et al., 2017), ez-greedy (Dabney et al., 2021), and DCEO (Klissarov & Machado, 2023). We\nalso use Progressive Neural Networks, and a variant of our method that does not use decomposition\nand therefore resembles the skill learning process used in \u03c0-PRL and H-DRLN (Tessler et al., 2017)."}, {"title": "PROBLEM DEFINITION", "content": "We consider Markov decision processes (MDPs) $(S, A, p, r, \\gamma)$, where $S$ represents the set of states\nand $A$ is the set of actions. The function $p(s_{t+1}|s_t, a_t)$ encodes the transition model, since it gives\nthe probability of reaching state $s_{t+1}$ given that the agent is in $s_t$ and takes action $a_t$ at time step $t$.\nWhen moving from $s_t$ to $s_{t+1}$, the agent observes the reward value of $R_{t+1}$, which is returned by\nthe function $r$; $\\gamma$ in $[0, 1]$ is the discount factor. A policy $\\pi$ is a function that receives a state $s$ and an\naction $a$ and returns the probability in which $a$ is taken in $s$. The objective is to learn a policy $\\pi$ that\nmaximizes the expected sum of discounted rewards for $\\pi$ starting in $s_t$: $E_{\\pi,p}[\\sum_{k=0}^{\\infty} \\gamma^k R_{k+t+1}|S_t]$.\nLet $P = {P_1, P_2, \u2026, P_n }$ be a set of MDPs, which we refer to as tasks, for which the agent learns\nto maximize the expected sum of discounted rewards. After learning policies for $P$, we evaluate the\nagent while learning policies for a set of tasks $P'$ with $P' \\cap P = \\emptyset$. This is a simplified version of\nscenarios where agents learn continually; we focus on transferring knowledge from $P$ to $P'$ through\noptions (Konidaris & Barto, 2007)."}, {"title": "LEARNING OPTIONS WITH NEURAL NETWORK DECOMPOSITION", "content": "We use the knowledge that the agent generates while learning policies $\\pi$ for tasks in $P$ to learn\ntemporally extended actions that use \u201cparts\u201d of $\\pi$ that can be \u201chelpful\". We hypothesize that these\ntemporally extended actions can speed up the learning process of policies for $P'$. We consider the\noptions framework to define temporally extended actions (Sutton et al., 1999). An option $\\omega$ is a\ntuple $(I_{\\omega}, \\pi_{\\omega}, T_{\\omega})$, where $I_{\\omega}$ is the initiation set of states in which the option can be selected; $\\pi_{\\omega}$\nis the policy that the agent follows once the option starts; $T_{\\omega}$ is a function that receives a state $s_t$\nand returns the probability in which the option terminates in $s_t$. We consider the call-and-return\nexecution of options: once $\\omega$ is initiated, the agent follows $\\pi_{\\omega}$ until it terminates.\nHere is an overview of our algorithm for learning options.\n1. Learn a set of neural policies {$\\pi_{\\theta_1}, \\pi_{\\theta_2},\\cdots, \\pi_{\\theta_n } $}, one for each task in $P$.\n2. Decompose each neural network encoding $\\pi_{\\theta_i}$ into a set of sub-policies $U_i$ (Section 4.1).\n3. Select a subset from $\\cup U_i$ to form a set of options $\\Omega$ (Section 4.2).\n4. Use $A \\cup \\Omega$ as the set of actions that the agent has available to learn a policy for tasks in $P'$.\nWe can use any algorithm that learns a parameterized policy $\\pi_{\\theta}$, such as policy gradient (Williams,\n1992) and actor-critic algorithms (Konda & Tsitsiklis, 1999) in Step 1 above. In Step 4, we can use\nany algorithm to solve MDPs, because we augment the agent's action space with the options learned\nin Steps 1-3 (Kulkarni et al., 2016). The process of decomposing the policies into sub-policies\nis described in Section 4.1 (Step 2) and the process of defining and selecting a set of options is\ndescribed in Section 4.2 (Step 3). Since we use the set of options $\\Omega$ as part of the agent action space\nfor the tasks in $P'$, Step 3 only defines $\\pi_{\\omega}$ and $T_{\\omega}$ for all $\\omega$ in $\\Omega$, and $I_{\\omega}$ is set to be all states $S$. Due\nto its process of decomposing trained neural networks, we call DEC-OPTIONS both the algorithm\nand the options it learns."}, {"title": "DECOMPOSING NEURAL POLICIES INTO SUB-POLICIES", "content": "We consider fully connected neural networks with $m$ layers $(1, \\ldots, m)$, where the first layer is\ngiven by the input values $X$ and the $m$-th layer the output of the network. For example, $m = 3$\nfor the network shown in Figure 1. Each layer $j$ has $n_j$ neurons $(1, \\ldots, n_j)$ where $n_1 = X$.\nThe parameters between layers $i$ and $i + 1$ of the network are indicated by $W^i \\in \\mathbb{R}^{n_{i+1} \\times n_i}$ and\n$B^i \\in \\mathbb{R}^{n_{i+1} \\times 1}$. The $k$-th row vector of $W^i$ and $B^i$, denoted $W_k^i$ and $B_k^i$, represent the weights\nand the bias term of the $k$-th neuron of the $(i + 1)$-th layer. In Figure 1, $n_1 = 2$ and $n_2 = 2$. Let\n$A^i \\in \\mathbb{R}^{n_i \\times 1}$ be the values the $i$-th layer produces, where $A^1 = X$ and $A^m$ is the output of the\nmodel. A forward pass in the model computes the values of $A^i = g(Z^i)$, where $g(\\cdot)$ is an activation\nfunction and $Z^i = W^{i-1} \\cdot A^{i-1} + B^{i-1}$. In Figure 1, the neurons in the hidden layer use ReLU as\nthe activation function, and the output neuron uses a Sigmoid function."}, {"title": "SYNTHESIZING AND SELECTING OPTIONS", "content": "Let {$\\pi_{\\theta_1}, \\pi_{\\theta_2}, \\ldots, \\pi_{\\theta_n} $} be the set of policies that the agent learns for each task in $P$. Let $U_i$ be\nthe set of sub-policies obtained through the neural tree of $\\pi_{\\theta_i}$, as described in the previous section,\nand $U = {U_1, U_2, \\ldots, U_n}$. Let {$(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T)$ } be a sequence of state-action\npairs observed under $\\pi$ and a distribution of initial states $\\mu$ for a task $\\rho$, where $s_0$ is sampled from\n$\\mu$ and, for a given state $s_t$ in the sequence, the next state $s_{t+1}$ is sampled from $\\rho(\\cdot|s_t, a_t)$, where\n$a_t = \\arg \\max_a \\pi(s_t, a)$. The use of the $\\arg \\max$ operator over $\\pi$ reduces the noise in the selection\nprocess of the DEC-OPTIONS because the options also act greedily according to the sub-policies\nextracted from $\\pi$. If $\\rho$ is episodic, $s_{T+1}$ is a terminal state; $T + 1$ defines a maximum horizon for the\nsequence otherwise. $T_i$ is a set of such sequences for $\\pi_{\\theta_i}$ and $\\rho_i$'s $\\mu$. Finally, $T = {T_1, T_2, \\ldots, T_n}$.\nThe sub-policies $U$ do not offer temporal abstractions as they are executed only once. We turn\nthese sub-policies into options by wrapping each $\\pi$ in $U$ with a while-loop of $z$ iterations. Once\nthe resulting option $\\omega$ is initiated, it will run for $z$ steps before terminating. We denote as $\\omega_z$ the\n$z$-value of $\\omega$. In each iteration of the while loop, the agent will execute in the environment the\naction $\\arg \\max_a \\pi(s, a)$, where $\\pi$ is the sub-policy and $s$ is the agent's current state. The $\\arg \\max$\noperator ensures that the policy is deterministic within the loop. Let $T_{\\max}$ be the length of the\nlongest sequence in $T$, then we consider options with $z = 1, \\ldots, T_{\\max}$ for each sub-policy in $U$. Let\n$\\Omega = {\\Omega_1, \\Omega_2, \\ldots, \\Omega_n}$ be the set of all while-loop options obtained from $U$. Each $\\Omega_i$ has $T_{\\max} |U_i|$\noptions for $\\pi_i$, one for each $z$. Our task is to select a subset of \u201chelpful\u201d options from $\\Omega$.\nWe measure whether a set of options is helpful in terms of the Levin loss (Orseau et al., 2018) of\nthe set. The Levin loss measures the expected number of environmental steps (calls to the function\n$\\rho$) an agent needs to perform with a given policy to reach a target state. The Levin loss assumes\nthat $\\rho$ is deterministic and the initial state is fixed; the only source of variability comes from the\npolicy. The Levin loss for a sequence $T_i$ and policy $\\pi$ is $L(T_i, \\pi) = \\frac{|T_i|}{\\prod_{(s, a) \\in T_i} \\pi(s, a)}$.\nThe factor $\\frac{1}{\\prod_{(s, a) \\in T_i} \\pi(s, a)}$ is the expected number of sequences that the agent must sample with $\\pi$ to\nobserve $T_i$. We assume that the length of the sequence the agent must sample to observe $T_i$ is known\nto be $|T_i|$ and therefore is fixed, so the agent performs exactly $|T_i|$ steps in every sequence sampled.\nLet $\\pi^{\\omega}$ be the uniform policy for an MDP, that is, a policy that assigns equal probability to all actions\navailable in a given state. Furthermore, let $\\pi_{\\Omega}^{\\omega}$ be the uniform policy when we augment the MDP\nactions with a set of options $\\Omega$. There are two effects once we add a set of options to the set of\navailable actions. First, the Levin loss can increase because the probability of choosing each action\ndecreases, including the actions in the target sequence. Second, the Levin loss can decrease because\nthe number of decisions the agent needs to make can also decrease, potentially reducing the number\nof multiplications performed in the denominator of the loss. Our task is then to select a subset of\noptions from the set $\\Omega$ generated with decomposed policies such that we minimize the Levin loss.\n$\\arg \\min_{\\Omega' \\subseteq \\Omega_T} \\sum_{T_i \\in T} L(T_i, \\pi_{\\Omega'}^{\\omega}).$\nWe divide the set of tasks $P$ into disjoint training and validation sets to increase the chances of\nselecting options that generalize. For example, an option that encodes $\\pi_{\\theta_i}$ with a loop that iterates for\n$z$ steps, where $z$ is equal to the length of the sequences in $T_i$, is unlikely to generalize to other tasks,\nas it is highly specific to $\\rho_i$. In Equation 1, $\\Omega_T$ is the set of options extracted from the policies learned\nfor the tasks in the training set and $T_T$ are the sequences obtained by rolling out the policies learned\nfor the tasks in the validation set. We consider uniform policies in our formulation because they"}, {"title": "GREEDY APPROXIMATION TO SELECT OPTIONS", "content": "The greedy algorithm for approximating a solutions to Equation 1 initializes $\\Omega'$ as an empty set\nand, in every iteration, adds to $\\Omega'$ the option that represents the largest decrease in Levin loss. The\nprocess stops when adding another option does not decrease the loss, so it returns the subset $\\Omega'$.\nDue to the call-and-return model, we need to use a dynamic programming procedure to efficiently\ncompute the values of $L$ while selecting $\\Omega'$. This is because it is not clear which action/option the\nagent would use in each state of a sequence so that the probability $\\prod_{(s, a) \\in \\tau_i} \\pi(s, a)$ is maximized.\nFor example, an option $\\omega$ returns the correct action for $\\omega_z$ states in the sequence starting in $s_1$. While\n$\\omega'$ does not return $a_1$ in $s_1$, it returns the correct action in the sequence for $\\omega_z$ states from $s_2$. If\n$\\omega_z' < \\omega_z$ and using $\\omega$ in $s_1$ prevented us from using $\\omega'$ in $s_2$ because $\\omega$ would still be executing in\n$s_2$, then using $a_1$ in $A$ for $s_1$ and then starting $\\omega'$ in $s_2$ could maximize $\\prod_{(s, a) \\in \\tau_i} \\pi(s, a)$."}, {"title": "EXPERIMENTS", "content": "We conducted experiments to evaluate the hypothesis that neural networks encoding policies for\nMDPs may contain useful underlying options that can be extracted through neural decomposition.\nIn our experiments, we consider a set of tasks $P$, all of which share a common state representation\nand output structure, but may differ in terms of reward functions and dynamics. The primary ob-\njective is to evaluate an agent that uses an action space augmented with DEC-OPTIONS synthesized\nfrom $P$ on a new set of tasks $P'$. We use Proximal Policy Optimization (PPO) (Schulman et al.,"}, {"title": "PROBLEM DOMAINS", "content": "We use two domains where exploration can be difficult and it is easy to generate similar but different\ntasks: MiniGrid (Chevalier-Boisvert et al., 2023) and ComboGrid, which we introduce in this paper.\nMiniGrid The first domain is based on Minigrid environments. In MiniGrid, the agent is restricted\nto a partial observation of its surroundings, determined by a view size parameter. We select a set of\nsimple tasks as set $P$ and a set of more challenging tasks as set $P'$. In $P$, we use three instances of\nthe Simple Crossing Environment with 9 \u00d7 9 grids and one wall, as shown in Figure 2a. For the test\nset $P'$, we use three configurations of the Four Rooms Environment, as illustrated in Figure 2b. In\nFour Rooms, the agent navigates a 19 \u00d7 19 grid divided into four rooms. In the first task, the agent\nand the goal point are in the same room. In the second task, they are located in neighboring rooms,\nand in the third task, which is shown in Figure 2b, they are located in two non-neighboring rooms.\nComboGrid In this environment, the agent has full observational capabilities. The agent's move-\nments are determined by a combination of actions (combo). Four unique combinations, each corre-\nsponding to a move to a neighboring cell, dictate the agent's navigation. If the agent successfully\nexecutes the correct series of actions corresponding to a valid combo, it moves to the corresponding\nneighboring cell. Failing to execute the correct combo results in the agent remaining in its current\nposition and the history of previous combo actions being reset. The state contains not only the grid,\nbut also the agent's current sequence of actions. A reward of -1 is assigned in the environment,"}, {"title": "EMPIRICAL RESULTS", "content": "Figure 3 shows the learning curves of agents equipped with DEC-OPTIONS and of the baselines.\nThe first row shows the results for agents using PPO, while the second row shows the results for\nagents using DQN. Each plot shows the average return the agent receives over 24 independent runs\nof each system; the plots also show the 95% confidence interval of the runs. Since the domain is\nnot discounted, the maximum reward is 1.0. Each instance of Four Room: 1, 2, and 3, represents\na domain in which the agents using transfer learning attempt to learn after learning a policy for\ntasks in $P$. Similarly to Figure 3, Figure 4 shows the results for the ComboGrid domain. Here, the\nmaximum return the agent can obtain is 40, 10 for each marker the agent collects.\nIn Minigrid, Vanilla-RL fails to learn a good policy with the number of samples used in our experi-\nments. The dynamics of the environment, where the agent needs to turn and then move, contributes\nto this difficulty. Transfer-PPO and PNN also faced difficulties due to interference caused by dif-\nferences between tasks (Kessler et al., 2022). In contrast, DEC-OPTIONS, Dec-Options-Whole,\nNeural-Augmented, and Modulating-Mask performed well in Four Rooms 1. Regarding the DQN\nagents, Option-Critic, DCEO, and DEC-OPTIONS outperformed Vanilla-RL. As the complexity of\nthe task increases, most of the baselines do not converge to an optimal policy. In particular, only the\nDEC-OPTIONS agent learned optimal policies in Four Rooms 3.\nIn the smallest ComboGrid, most of the baselines yield similar performance. Similarly to Minigrid,\nas we increase the size of the grid, we start to notice differences among the methods. For grids of\nsize 4 \u00d7 4, DEC-OPTIONS and Dec-Options-Whole converge quicker to an optimal policy than the\nother methods. For grid of size 6 \u00d7 6, DEC-OPTIONS converges quicker than all other methods. For\nthe DQN experiments, we also observe similar results to those observed in MiniGrid, where the gap\nbetween DEC-OPTIONS and other methods increases as the grid size increases.\nAlthough we compare DEC-OPTIONS with other option-learning algorithms, we note that DEC-\nOPTIONS solves a fundamentally different problem than the other methods evaluated. For example,\nDCEO focuses on exploration while learning options for a target task. In contrast, we learn options\non a set of tasks and evaluate whether these options are helpful in downstream problems. Future\nresearch might investigate the use of some of these baselines in the context of transfer learning.\nExamples DEC-OPTIONS learned long options for MiniGrid. The sequence below shows the ac-\ntions from state to goal (left to right) of the DEC-OPTIONS agent in the Four Room 3 environment.\nHere, 0, 1, and 2 mean 'turn right', 'turn left', and 'move forward', respectively. The curly brack-\nets show what is covered by one of the options learned. The options reduce the number of agent\ndecisions from 39 to only 10 in this example.\n0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2\nWe show a sub-sequence of an episode of the DEC-OPTIONS agent in the 4 \u00d7 4 ComboGrid. Option\n2 learns sequences of actions that move the agent to another cell on the grid (e.g., \u201cDown\u201d and\n\u201cRight\u201d). Option 1 is shorter than Option 2 and it applies in many situations. For example, calling\nOption 1 twice can move the agent up, or even finish a left move to then move up.\n0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 1, 1, 0, 2, 2, 0"}, {"title": "CONCLUSIONS", "content": "In this paper, we showed that options can \"occur naturally\" in neural network encoding policies\nand that we can extract such options through neural decomposition. We called the resulting options\nDEC-OPTIONS. Our decomposition approach assumes a set of related tasks where the goal is to\ntransfer knowledge from one set of tasks to another. Since the number of options we can extract\nfrom neural networks grows exponentially with the number of neurons in the network, we use a\ngreedy procedure to select a subset of them that minimizes the Levin loss. By minimizing the Levin\nloss, we increase the chances that the agent will apply sequences of actions that led to high-reward\nstates in previous tasks. We evaluated our decomposition and selection approach on hard-exploration\ngrid-world problems. The results showed that DEC-OPTIONS accelerated the learning process in a\nset of tasks that are similar to those used to train the models from which the options were extracted."}, {"title": "EXAMPLE OF THE ALGORITHM FOR COMPUTING THE LEVIN Loss", "content": "Let $S = {s_0, s_1, s_2, s_3, s_4, s_5}$ be a sequence of states and $\\Omega = {\\omega_1, \\omega_2}$ be a set of options. $\\omega_1$ can\nstart in $s_0$ and it terminates in $s_2$; $\\omega_2$ can start in $s_1$ and it terminates in $s_4$. Next, we show how the\ntable M in Algorithm 1 changes after every iteration of the for-loop in line 2.\nThe value of 3 of M[5] at the end of the last iteration indicates that the state $s_5$ can be reached with\nthree actions: a primitive action from $s_0$ to $s_1$, $\\omega_2$ from $s_1$ to $s_4$, and another primitive action from\n$s_4$ to $s_5$. If $P_{u, \\Omega} = 0.25$, then the optimal Levin loss value returned in line 8 of Algorithm 1 for T\nand $\\Omega$ is $\\frac{6}{0.25^3}$ = 384."}, {"title": "EXPERIMENTS DETAILS", "content": "In our methodology, we employ an approach to select options based on a set of sequences denoted\n$T = {T_1, T_2,\\ldots, T_n}$, as described in Section 4.2. This selection process is driven by a greedy\nalgorithm, facilitating the identification of options that can be \u201chelpful\u201d. However, it is the case\nthat when decomposing the policy $\\pi_{\\theta_i}$, the resulting sub-policies will have a low Levin loss on\nsequence $T_i$. To mitigate this bias, we compute the Levin loss for sub-policies derived from the\nneural decomposition of $\\pi_{\\theta_i}$ on the set $T \\setminus {T_i}$."}, {"title": "PLOTS", "content": "Figures shown in Section 5 were generated using a standardized methodology. To ensure robustness,\nwe applied a systematic procedure to all our baselines. We began the process by performing a\nhyperparameter search, as outlined in B.3, to select the best hyperparameters for each baseline.\nSubsequently, we perform 30 independent runs (seeds) for each baseline. We discarded the 20% of\nthe independent runs with the poorest performance. We computed the mean and 95% confidence\nintervals over the remaining 24 seeds."}, {"title": "ARCHITECTURE AND PARAMETER SEARCH", "content": "For algorithms utilizing the Proximal Policy Optimization (PPO) framework", "https": "github.com/arcosin/Doric"}, {"https": "github.com/lweitkamp/option-critic-pytorch and the best parameters found for the learning\nrate and the number of options in a hyperparameter sweep process. For the DCEO baseline"}, {"https": "github.com/mklissa/dceo/).\nThe parameter search is then applied to the learning rate"}, {"https": "github.com/dlpbc/mask-lrl). In scenarios where the Deep\nQ-Network (DQN) was employed, such as Vanilla-RL DQN and Dec-Options DQN baselines, we\nadhered to the stable-baselines. Similarly to PPO, we performed a parameter search, this time tar-\ngeting Tau and the learning rate, while keeping other parameters fixed. All the parameter searches\nmentioned were performed using the grid search method (LaValle et al., 2004).\nFor tasks within the MiniGrid domain, we employed a feedforward network. The policy network\nstructure consisted of a single hidden layer comprising 6 nodes, while the value network used three\nhidden layers with 256 neurons each. As tasks transitioned to P', we expanded the policy network\nto encompass three hidden layers with 50 neurons in each layer. In the Transfer-PPO method,\nwe also used a policy network with three hidden layers with 50 neurons in each across all"}]}