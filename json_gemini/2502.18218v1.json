{"title": "FLARE: A Framework for Stellar Flare Forecasting using Stellar Physical Properties and Historical Records", "authors": ["Bingke Zhu", "Xiaoxiao Wang", "Minghui Jia", "Yihan Tao", "Xiao Kong", "Ali Luo", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "abstract": "Stellar flare events are critical observational samples for astronomical research; however, recorded flare events remain limited. Stellar flare forecasting can provide additional flare event samples to support research efforts. Despite this potential, no specialized models for stellar flare forecasting have been proposed to date. In this paper, we present extensive experimental evidence demonstrating that both stellar physical properties and historical flare records are valuable inputs for flare forecasting tasks. We then introduce FLARE (Forecasting Light-curve-based Astronomical Records via features Ensemble), the first-of-its-kind large model specifically designed for stellar flare forecasting. FLARE integrates stellar physical properties and historical flare records through a novel Soft Prompt Module and Residual Record Fusion Module. Our experiments on the publicly available Kepler light curve dataset demonstrate that FLARE achieves superior performance compared to other methods across all evaluation metrics. Finally, we validate the forecast capability of our model through a comprehensive case study.", "sections": [{"title": "1. Introduction", "content": "Stellar flares are defined as the rapid release of magnetic field energy stored in a star's atmosphere, as illustrated in Figure 1. These phenomena are crucial for understanding stellar structure, evolution, and magnetic activity, as well as exploring potentially habitable exoplanets and extraterrestrial life [38]. Flare records are currently obtained through continuous scanning of stars using survey telescopes in conjunction with manual analysis. Despite these efforts, the quantity of observed flare samples remains limited, rendering them inadequate for comprehensive research needs. Consequently, forecasting stellar flare timing holds significant importance for astronomical studies. However, to date, there has been no published research addressing this area.\nSolar flare prediction has garnered significant research attention [1, 9, 33], but stellar flare forecasting presents distinct challenges compared to solar flare prediction. Leveraging the solar proximity, researchers can easily obtain solar magnetograms and magnetic field parameters, facilitating accurate solar flare predictions. In contrast, stellar flare forecasting predominantly relies on light curves. As depicted in Figure 2, a light curve represents the chronological variation of a stellar luminosity, measured in flux using Julian Date as the time axis. This figure illustrates that light curves often have missing data points. Additionally, two key characteristics emerge from the analysis: (1) A single star exhibits varying trend patterns across different time periods (refer to Figure 2(b)). (2) The variation trends differ significantly among different stars (see Figure 2(c)). These complex variations in light curves pose challenges for flare forecasting. Observably, a flare event is characterized by a rapid flux increase followed by a gradual decline, resulting in sharp short-term flux changes. Conversely, non-flare regions do not display such characteristics (as shown in Figure 2(a)).\nThe stellar flare forecasting task focuses on using light curves to predict whether a specified star will experience a flare within the next 24 hours. This can be viewed as a multi-task framework that combines both forecasting and classification objectives. Deep learning methods [3, 20, 40, 43] have been widely applied to time series analysis with promising results in prior research. However, these methods still do not demonstrate superior feature extraction capabilities compared to pre-trained large multimodal models in stellar flare forecasting. The intrinsic characteristics of stars, their varying evolutionary stages, and external factors such as other celestial bodies and interstellar dust can lead to diverse patterns in light curves. These external influences make achieving high accuracy in stellar flare forecasting using only light curves particularly challenging. As a result, additional data sources are required to improve the reliability of predictions.\nIn this paper, we introduce a novel task of forecasting stellar flare events. To address this challenge, we propose the FLARE framework (Forecasting Light-curve-based Astronomical Records via feature Ensemble). Through empirical analysis, we observe that stellar flares exhibit strong correlations with various stellar physical properties. Consequently, FLARE incorporates these stellar features as auxiliary inputs to enhance light curve feature extraction and improve forecasting performance. Furthermore, our investigation reveals that frequent historical flare events are positively correlated with the likelihood of future flares. To leverage this temporal dependency, FLARE integrates historical flare records as additional auxiliary features for enhanced prediction accuracy. We also introduce two novel components: the Soft Prompt Module, which combines stellar physical feature names and values to facilitate star-specific feature detection, and the Residual Record Fusion Module, designed to integrate light curves with historical flare records for improved model robustness. Finally, we employ a large multi-modal model fine-tuned using LoRA [16] to extract features from the outputs of these modules, thereby enabling accurate stellar flare forecasting.\nThe main contributions are as follows: (1) We present the first attempt at developing a method for stellar flare forecasting, addressing a previously unexplored challenge in astrophysics. (2) Through rigorous experimental analysis, we demonstrate that both stellar physical properties and historical flare records play significant roles in flare forecasting. Then, we propose a large-scale model called FLARE, which has shown remarkable effectiveness in enhancing accuracy. (3) Extensive experimental results validate the superior performance of FLARE compared to other approaches."}, {"title": "2. Literature Review", "content": ""}, {"title": "2.1. Time Series Representation Learning", "content": "Time series representation learning methods can be categorized based on the backbone into five groups: MLPs, RNNs, CNNS, GNNs, and Transformers.\nInspired by the efficient performance of autoregressive models, MLPs such as DLinear [40] demonstrate excellent performance. However, these methods often require additional design to capture time-wise dependency effectively. RNNs [20, 29] are naturally suitable for modeling sequential data, while they suffer from issues such as gradient vanishing due to recurrent structure and struggle to learn relationships between multivariate variables. CNNs, unlike RNNs, are less prone to gradient vanishing and excel at capturing the local patterns in time series. However, they often require stacking multiple convolutional layers to learn global futures, as seen in TCN [3], which results in a significant training time cost. GNNs [21, 36] abstract variables as nodes and establish edges between multivariate variables, learning spatial dependencies through GCN [19]. However, this approach relies on message passing to capture global features, and shows less scalable than Transformers. Leveraging the self-attention mechanism, Transformers are particularly adept at learning long-term temporal dependencies and complex multivariate correlations. Transformers can be categorized into three categories according to different types of tokenization. Point-wise methods [34, 35] learn correlations between time steps but become computationally expensive for long sequences. Series-wise methods [25] pay attention to model multivariate dependencies by tokenization, but struggle with complex temporal patterns. Patch-wise methods [28, 42] adjust patch sizes for flexibility across different time series, making them more adaptable to different types of time series data. These methods have demonstrated certain advantages in specific tasks, and our work also adopts a patch-based approach in light curve processing."}, {"title": "2.2. Time Series Analysis based on PLMs", "content": "Among time series large models, aside from MOMENT [14] and Chronos [2] which are trained from scratch using big time series data, most approaches are adaptations of existing PLMs. According to modification methods, these approaches can be sorted into three types:\nFine-tuning. Studies like UniTime [24] and OFA [44] unfreeze a portion of parameters, while others, including TEMPO [5] and LLM4TS [6] leverage Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA [16], adapt to new data by increasing trainable parameters without disrupting the existing structure of the large model.\nAlignment. PLMs trained on text data need to be aligned with time series data in the same data space. Based on the object of modification, these approaches can be divided into two categories. The first approach fine-tunes PLMs with time series data to map the model parameters into time series data space, as seen in LLM4TS [6]. The other approach maps the time series vector into text space, as demonstrated in TIME-LLM [17], which employs multi-head attention mechanisms to achieve mapping.\nPrompt-learning. Studies such as UniTime [24] incorporate text prompts, while TEST [31] employs the combination of trainable vectors and textual token embeddings to improve performance.\nAlthough these methods generally perform well on specific tasks, they struggle to simultaneously handle multitask time series analysis and text feature extraction."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Problem Definition", "content": "The stellar flare forecasting task involves predicting whether a flare will occur in the future for a given star based on its light curve observations, physical properties, and historical flare records. For a star $i$, its physical properties are represented as $P_i = \\{(k_{im}, v_m) | m = 0,1,...,M\\}$, where $k_{im}$ and $v_m$ are the name and the value of the $m$-th physical property, respectively. Here, $k_{im}$ is a text, while $v_m \\in \\mathbb{R} \\cup \\{\\emptyset\\}$, where $\\emptyset$ indicates a missing value. The observed flux for star $i$ at timestamp $t$ is denoted as $x^i_t \\in \\mathbb{R} \\cup \\{0\\}$. The historical flare records are represented by $R^i = \\{(t^i_s, t^i_e) | n = 0,1,..., N\\}$, where $t^i_s$ and $t^i_e$ denote the start and end times of the $n$-th flare, respectively. A binary indicator variable is defined such that $y^i_t = 1$ if a flare occurs at timestamp $t$, i.e., if there exists a time $t'$ satisfying $t \\le t' \\le t + H - 1$; otherwise, $y^i_t = 0$.\nAdditionally, $y^i_{(t_0,t_1)} = 1$ indicates that at least one flare occurred within the interval $(t_0,t_1)$. Let $K$ and $H$ denote the observation window length and forecast horizon, respectively. The observed light curve is represented as $X^i = [x^i_{-K},...,x^i_{-1}] \\in \\mathbb{R}^K$, and the historical flare records are denoted by $Y^i = [y^i_{-K},..., y^i_{-1}]^T \\in \\mathbb{R}^K$. The forecast probability of a flare occurring between timestamps $t$ and $t' = t + H - 1$ is denoted by $\\hat{y}^i_{(t,t')} \\in [0, 1]$. The forecasting task can be formalized as follows:\n$\\hat{y}^i_{(t,t')} = F(X^i; Y^i; P^i; \\Phi),$   (1)\nwhere $\\Phi$ represents the model parameters of the model $F$."}, {"title": "3.2. Experimental Observations", "content": "In Table 1, we present an experimental analysis to investigate how stellar physical properties and historical flare records influence the accuracy of flare forecasting using Kepler dataset.\nSince stellar physical properties exhibit distinct characteristics compared to light curves, we first map these property values into a compatible dimensional space before concatenating them with the light curve data to derive forecast probabilities. For historical flare records, which share similar temporal characteristics with light curves, $Y_t$ and $Y^i$ are concatenated along the flux dimension prior to model input.\nThe experimental results reveal that both stellar physical properties and historical flare records contribute meaningfully to forecasting performance. Furthermore, their combined use yields superior predictive accuracy compared to using either type of data alone. Interestingly, we observe that the marginal gain in performance diminishes after incorporating additional supplementary data. This finding suggests that stellar flare forecasting should not be approached solely as a time series prediction task based on light curves. Instead, leveraging both stellar physical properties and historical flare records represents an effective strategy for enhancing forecast accuracy and achieving a more comprehensive understanding of the underlying phenomena."}, {"title": "4. Methodology", "content": "In this section, we present our proposed model, FLARE. The overall architecture of the model is illustrated in Figure 3, which comprises three key components. First, as detailed in Subsection 4.1, each light curve is decomposed into its trend and residual components, with historical flare records being integrated into the residual through a Residual Record Fusion Module to enhance robustness. Second, Subsection 4.2 introduces two prompt patterns based on tabular stellar physical properties and employs P-tuning [23] to distinguish between different stars effectively. Finally, Subsection 4.3 describes the fine-tuning of PLM to simultaneously process text and light curves."}, {"title": "4.1. Light Curve Embedding", "content": "Typically, a flare only exists for a short period and is independent of the overall flux variation trend of the light curve. Furthermore, the periodic occurrence of flares only exists when the magnetic field of the star is stable. Given this phenomenon, separately handling the trend and the flare of the light curve could help eliminate mutual interference and improve light curve embedding. Even though the effectiveness of historical flare records has been verified in Subsection 3.2, fusing them with real flare could help improve robustness and reduce the misleading effect of false positive records. However, not all stars exhibit clear periodic luminosity variations. Moreover, their periods vary significantly, ranging from shorter than one day to longer than the observation window. In such cases, real-time embedding can be beneficial. Based on these considerations, we divide this subsection into three components: (1) normalization and decomposition, (2) trend processing and residual record fusion, and (3) timestamp embedding. As the light curve embedding process is uniform across stars, in the following text, we omit the superscript and use $x_t$ to represent the flux at timestamp $t$, and similarly for $y_t$.\nNormalization and Decomposition. Due to the inherent limitations in the precision of the telescope, flux values exhibit substantial variations across different observation periods. This variation necessitates the normalization of the data by dividing the flux values by the median, in order to effectively mitigate the potential influence of systematic errors. Besides, frequent data omissions in light curves require us to perform a decomposition that distinguishes the overall flux trend from local abrupt flux variations, while minimizing the impact of missing values. Given that the time steps with missing data are usually non-consecutive, we employ a moving average to capture the trend of the light curve's variations, explicitly excluding the missing data from the computation to ensure that the trend is not unduly influenced by data gaps. This process can be represented as:\n$\\begin{aligned}\n\\hat{x}_t &= \\frac{x_t}{\\text{median}(x_t)},\\tag{2}\n\\tilde{x}_t &= \\frac{1}{d_W} \\sum_{j=t-[W]}^{t+[W]} \\hat{x}_j m_j, \\tag{3}\n\\nonumber \\\\\nm_j &= \\begin{cases}\n0 & \\text{if } j = \\emptyset, \\\\\n1 & \\text{otherwise},\n\\end{cases} \\tag{4}\n\\end{aligned}$\nwhere $\\hat{x}_j$ is the normalized $x_t$, $m_j$ is an indicator for missing data, $d_W$ is the length of the sliding window, and $\\tilde{x}_t$ represents the trend at timestamp $t$. $X^T = [\\tilde{x}_{t-K},...,\\tilde{x}_{t-1}] \\in \\mathbb{R}^K$ are utilized to represent the trend of the light curve, with the rest representing the residual $X^R = [x_{t-K},..., x_{t-1}] \\in \\mathbb{R}^K$, where $x^r_j = \\hat{x}_j - \\tilde{x}_j$, $j = t - K,...,t - 1$.\nTrend Processing and Residual Record Fusion. Since both the trend $X^T$ and the residual $X^R$ are univariate time series, the temporal context at each timestamp plays a crucial role in data embedding. We generate patches for both the trend and the residual separately to obtain $X^T \\in \\mathbb{R}^{L\\times P}$ and $X^R \\in \\mathbb{R}^{L\\times P}$, where $L = \\lfloor \\frac{K-P}{S} \\rfloor$ is the number of patches with length of $P$, and $S$ is the stride. The $X^T$ is passed through the MLP to obtain the $X^{T'} \\in \\mathbb{R}^{L\\times d}$. For the residual, the gating mechanism is applied in conjunction with the flare record $\\tilde{y}_t$ to process $X^R$. This process can be represented as:\n$\\begin{aligned}\n[X^{R'}; \\tilde{Y}_t] &= [X^R; \\tilde{Y}_t]W + b, \\tag{5}\ng &= \\sigma(X^R W_{gR} + \\tilde{Y}_tW_{gY} + b_g), \\tag{6}\nX^{R'} &= (g \\odot X^R + (1 - g) \\odot \\tilde{Y}_t)W + b, \\tag{7}\n\\end{aligned}$\nwhere $W \\in \\mathbb{R}^{P \\times d}$, \\{\\bf{W}_{gR},\\bf{W}_{gY},W\\} \\subset \\mathbb{R}^{d \\times d}$, $b \\in \\mathbb{R}^{2L}$, \\{\\bf{b}_{g},b\\} \\subset \\mathbb{R}^{L}$. $\\tilde{Y}_t$ is derived from $\\tilde{y}_t$ by generating patches. $X^{R'} \\in \\mathbb{R}^{L \\times d}$ is the embedding of the residual. $\\sigma$ denotes the sigmoid activation function and $\\odot$ represents Hadamard product, respectively.\nTimestamp Embedding. As stars exist beyond the solar system, the time formatting used on Earth is not suitable and is typically represented by Julian Date, which is a continuous floating-point number. We extract the numerical values of each digit from the hundredths place to the ten-thousands place in a decimal manner, derive embeddings from these values, and use their sum as the timestamp embedding. Since the light curve has been divided into multiple patches, each patch requires a timestamp embedding. Given that the time intervals between adjacent time steps within the same patch are of fixed length, we employ the average timestamp embedding for the patch, and the collection of all patches is denoted as $E \\in \\mathbb{R}^{L \\times d}$.\nFollowing the three steps outlined above, the final time series embedding is obtained as $X^{TS} = [X^{T'};X^{R'};E] \\in \\mathbb{R}^{2L \\times d}$."}, {"title": "4.2. Prompt Design", "content": "Motivation. Stellar flares are a prominent manifestation of stellar activity. As summarized in Yong [38]'s study, factors such as stellar age, rotation speed, and stellar mass are correlated with flare frequency, which further supports the use of stellar physical properties for flare forecast task. Stellar physical properties often exhibit frequent missing values and inconsistent numerical ranges. While interpolation and standardization can be used to address these issues separately, they may also introduce biases and lead to the loss of valuable physical information. Given that the physical properties of stars are presented as tabular data, and that Hegselmann [15] has experimentally shown that combining column names with values leads to better performance than using only values, we organize the physical property values of stars along with their corresponding names into a textual structure. Furthermore, inspired by P-tuning [23], we design the Soft Prompt Module to learn stellar physical properties for distinguishing stars.\nSoft Prompt Module. The textual description of stellar physical properties is shown in Figure 4(a), where any physical property is represented as a range of values or an exact value. Inspired by P-tuning, which optimizes a small number of prompt embeddings and demonstrates good scalability while saving computational resources, we propose replacing part of the word vectors in the text with trainable parameters, as shown in Figure 4(b). We design corresponding replacement patterns based on the types of the physical property. In both pattens, two vectors ($h_0$, $h_1$) are retained to represent the start and end of a the physical property description, with a vector $h_2$ separating the physical name from the value. Depending on the type of the physical property, an additional feature separator vector $h_2$ will be inserted. All words except the replaced ones are embedded by the text encoder. Additionally, trainable embedding is utilized to represent the ID of the star. Through this prompt design, the meaning of physical property names, as well as the physical significance of their numerical value ranges, are both preserved. Finally, we use $X^P \\in \\mathbb{R}^{S\\times d}$ to represent the embedding of the physical properties of the star $i$, and $S$ is the number of tokens in the segmented textual descriptions."}, {"title": "4.3. Pre-Trained Large Model Fine-tuning", "content": "Zhou [44] provides experimental evidence that training PLMs from scratch often hurts performance. However, freezing most of the parameters and only training a small subset can preserve the representational learning capability of PLMs. We freeze the majority of the parameters, particularly those in the multi-head attention mechanism and the feed forward layers, allowing the large model to fine-tune only the LayerNorm layers. To adapt to cross-modal inputs, we employ Low-Rank Adaptation (LoRA) [16] to introduce trainable low-rank matrices to the multi-head mechanism, which allows effective learning of the correlation between the physical property text vectors $X^P \\in \\mathbb{R}^{S\\times d}$ and the light curve patches $X^{TS}$ while introducing only a small number of trainable parameters. The final embedding $Z^i = PLM(X^P; X^{TS}) \\in \\mathbb{R}^{2L \\times d}$ is obtained at the end of this process."}, {"title": "4.4. Loss Function", "content": "Since a portion of the samples are false positives, label smoothing is applied. After computing the forecast flare probabilities with the MLP, we utilize the cross-entropy loss function incorporating label smoothing, which can be represented as:\n$\\hat{y}_{(t,t')} =POOLING(Z^i)W^e + b^e,$   (8)\n$\\begin{aligned}\nL_i &= - [(1 - \\epsilon)y_{(t,t')}\\log(\\hat{y}_{(t,t')}) \\\\\n&+ \\epsilon(1 - y_{(t,t')})\\log(1 - \\hat{y}_{(t,t')})], \\tag{9}\n\\end{aligned}$\n$\\mathcal{L} = \\frac{1}{N_s} \\sum_{i=1}^{N_s} \\frac{1}{N_i} \\sum_{t=1}^{N_i} L_i,$   (10)\nwhere $\\epsilon$ is the smoothing coefficient of the label, $N_s$ denotes the number of stars, $N_i$ represents the number of samples for star $i$, $W^e \\in \\mathbb{R}^{1 \\times 2}$, $b^e \\in \\mathbb{R}^{1}$, and $POOLING$ refers to the operation of dimensionally reduction of $Z^i$."}, {"title": "5. Experiments", "content": "In this section, extensive experiments are conducted to evaluate the effectiveness of FLARE and the indispensability of each module, and an analysis of several flare forecasting cases is presented."}, {"title": "5.1. Experimental Setup", "content": "Datasets. Kepler mission [4] monitored the luminosity variations of over 150,000 stars from 2009 to 2018. For our study, we select high-precision light curves of 7,160 stars with flare events from 2009 to 2013, sampled every halfhour intervals, forming the Kepler light curve dataset. Each observation window consists of 512 data points, and the object is to forecast whether a flare event will occur within the next 24 hours, corresponding to 48 data points. The light curves of each star are split into training and test sets in a 4:1 ratio based on chronological order, and the flare rate of the test set is controlled at 50% through random sampling.\nBaselines. We compare the proposed method with five type baselines: (1) PLMs (MOMENT [14], Chronos [2], OFA [44], and UniTime [24]) (2) MLPs (Dlinear [40], TiDE [8], and FreTS [37]), (2) RNNs (GRU [11] and LSTNet [20]), (3) CNNs (MICN [32], TCN [3], and SCINet [22]), (4) Transformers (PatchTST [28], iTransformer [25], Autoformer [35], Crossformer [42], ETSformer [34], and Informer [43]).\nEvaluation Metrics. To evaluate our forecasting model, we employ five evaluation metrics: AUC, Accuracy, Recall, F1 score and Precision. We prioritize high Recall and Accuracy while keeping adequate Precision for accurate forecasting.\nExperiment Settings. We use AdamW [26] with a learning rate of 1e-5 as the optimizer, train for 200 epochs, and apply early stopping with a patience of 15. The PLMS, LSTNet, and TCN use publicly available code from their original papers, while the MLPs, Transformers, MICN, and SCINet use the code provided by TSLib1. Both the text encoder and the PLM are BERT [10]."}, {"title": "5.2. Performance Comparison", "content": "We compare FLARE with various baselines, conducting at least three runs to compute the average performance, as depicted in Table 2. Here, FLARE clearly outperforms other methods and is the only one achieving an accuracy greater than 70%. The following are three key observations:\n(1) Among the five types of baselines, PLMs and RNNs typically perform well when neither historical flare records nor stellar physical properties are employed. The high effectiveness of MOMENT and Chrones can be attributed to the knowledge gleaned from pre-trained large time-series models. We further analyze the subpar performance of OFA, which results from its simplistic approach to light curve processing. In contrast, UniTime, based on the same PLM, performs commendably. Additionally, the strong performance of RNNs is ascribed to the temporal characteristics of light curves.\n(2) Among MLPs and Transformers, only TiDE and PatchTST show classification ability when restricted to using only light curves. An analysis of this phenomenon is presented. The robust performance of TiDE is credited to the Residual Block, which bolsters its resilience and enables it to manage a certain level of noisy samples in the dataset. Among Transformers, point-wise methods (e.g., Autoformer and ETSformer) perform poorly, presumably because flux values lack contextual information, rendering them inadequate for effective feature learning at each time step. Series-wise methods (e.g., iTransformer) have difficulty capturing complex temporal dependencies, while patch-wise methods, such as PatchTST, exhibit excellent performance. Although Crossformer, also a patch-wise method, shows poor metrics, our analysis indicates that this is due to the mismatch between the univariate light curve and the CrossDimension Attention Mechanism.\n(3) The inclusion of historical flare records and stellar physical properties improves the performance of all baselines, with only minor metric differences. This result underscores the significance of historical flare records and stellar physical properties in stellar flare prediction."}, {"title": "5.3. Ablation Study", "content": "Effectiveness of Each Module An ablation study is conducted to assess the effectiveness of each module within FLARE. The results are presented in Table 3, where modules are either removed or replaced individually. \"FLARE w/o Soft Prompt Module\" indicates replacing the vector generated by the Soft Prompt Module with a vector mapped from stellar physical property values. \u201cFLARE w/o Residual Record Fusion Module\u201d means the removal of this module, with historical flare records concatenated with the light curve along the flux dimension. \"FLARE w/o LoRA\" refers to the exclusion of LoRA during fine-tuning.\nOur evaluation reveals that the absence of any of the three modules leads to a decline in certain performance metrics, while FLARE exhibits robust performance across all metrics. Notably, the omission of the Soft Prompt Module results in a significant performance drop. This finding aligns with the conclusions of Hegselmann [15], which posits that textual headers contribute to the classification of tabular-form data. The removal of LoRA causes a slight decrease in all five metrics, demonstrating the utility of LORA in fine-tuning. Although the performance change before and after removing the Residual Record Fusion Module is minimal, retaining it leads to a higher Recall, thereby highlighting the effectiveness of this module.\nAblation Studies on Text Encoder and PLM. An ablation study of the text encoder and PLM within FLARE is presented in Table 4. The analysis reveals that when both components are GPT-2, the performance deteriorates. Conversely, upon the introduction of BERT, the performance improves. This improvement can be ascribed to the high-quality text embeddings that BERT provides for stellar physical properties. Employing BERT for both the text encoder and PLM yields the optimal performance, thereby highlighting the significance of the text encoder and PLM in FLARE.\nPerformance using only Light Curves. A performance comparison of models using only light curves is presented in Table 5. The baselines include SolarFlareNet [1], TS2Vec [39], CRT [41], LPT [12], VS-Loss [18], HIVECOTEV2 [27], TARNet [7], and SAnD [30]. The comparison demonstrates the limited effectiveness of these models when relying solely on light curves. The results further highlight the efficacy of FLARE in leveraging historical flare records and stellar physical properties for forecasting stellar flares.\nAblation Studies on Pre-trained Large Models. A performance comparison of pre-trained large models is presented in Table 5. The models included are ViT [13], MOMENT [14], and Chronos [2]. This comparison demonstrates the impact of stellar physical properties (SPPs) and historical flare records (HFRs) on the stellar flare forecasting task."}, {"title": "5.4. Case Study", "content": "To further elucidate the reasons underlying FLARE's flare forecasting capabilities, we conduct a case study to explore the working mechanism of FLARE. The forecast results of FLARE for selected samples are visualized in Figure 5. Evidently, FLARE can generate effective forecasts based on the observation area and can adapt to different stars and diverse flux variation patterns. Specifically, Figure 5(g) and Figure 5(h) demonstrate that FLARE can accurately predict flares on light curves with distinct flux variation patterns originating from the same star, thereby highlighting its robust forecasting ability."}, {"title": "5.5. Statistical Observations", "content": "The analysis shown in Figure 6 reveals that a positive correlation exists between the number of flare-related time steps within the observation region and the flare probability in the forecast horizon when the number of time steps corresponding to flares is relatively small. However, as the number of flare-associated time steps continues to increase, the quantity of eligible statistical samples decreases, resulting in a negligible correlation."}, {"title": "5.6. Robust Analysis", "content": "To validate the robustness of the Residual Record Fusion Module (RRFM), we substitute it with residual features combined with embedded stellar flare records (FLARE w/o RRFM) and visualize the feature embeddings of each patch for both FLARE and FLARE w/o RRFM. As shown in Figure 7(b), FLARE w/o RRFM can distinguish between strong flare and non-flare regions. However, it cannot differentiate between suspected mislabeling and weak flare regions. In Figure 7(c), the dots, arranged from left to right, represent non-flare, suspected mislabeling, weak flare, and strong flare regions. The similarity between the features of the suspected mislabeling and non-flare regions highlights the enhanced robustness of FLARE against mislabeling."}, {"title": "6. Conclusion", "content": "In this paper, we demonstrate that both stellar physical properties and historical flare records are beneficial for forecasting stellar flares. Motivated by these findings, we propose the FLARE model, which incorporates two specialized modules: the Soft Prompt Module and the Residual Record Fusion Module. The Soft Prompt Module enables the model to differentiate between various star types, facilitating effective feature extraction tailored to each star's characteristics. Complementing this, the Residual Record Fusion Module enhances model robustness by integrating historical flare records with light curve residuals. Our experiments on the Kepler light curve dataset underscore FLARE's superior performance compared to existing models. We expect that these empirical results will provide valuable insights for future advancements in stellar flare forecast research."}]}