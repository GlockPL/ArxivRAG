{"title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers", "authors": ["Zebin Yang", "Renze Chen", "Taiqiang Wu", "Ngai Wong", "Yun Liang", "Runsheng Wang", "Ru Huang", "Meng Li"], "abstract": "In this paper, we propose MCUBERT to enable language models like BERT on tiny microcontroller units (MCUs) through network and scheduling co-optimization. We observe the embedding table contributes to the major storage bottleneck for tiny BERT models. Hence, at the network level, we propose an MCU-aware two-stage neural architecture search algorithm based on clustered low-rank approximation for embedding compression. To reduce the inference memory requirements, we further propose a novel fine-grained MCU-friendly scheduling strategy. Through careful computation tiling and re-ordering as well as kernel design, we drastically increase the input sequence lengths supported on MCUs without any latency or accuracy penalty. MCUBERT reduces the parameter size of BERT-tiny and BERT-mini by 5.7\u00d7 and 3.0x and the execution memory by 3.5\u00d7 and 4.3\u00d7, respectively. MCUBERT also achieves 1.5x latency reduction. For the first time, MCUBERT enables lightweight BERT models on commodity MCUs and processing more than 512 tokens with less than 256KB of memory.", "sections": [{"title": "1 Introduction", "content": "IoT devices based on microcontroller units (MCUs) are ubiquitous, enabling a wide range of speech and language applications on the edge, including voice assistant [9, 38], real-time translation [40, 45], smart home [35], etc. Language models (LMs), e.g., BERT [13], are fundamental to these applications. While cloud off-loading is heavily employed for LM processing, it suffers from high latency overhead, privacy concerns, and a heavy reliance on WiFi or cellular networks [3, 16]. Hence, there is a growing demand for deploying BERT-like LMs on MCUs.\nThough appealing, enabling BERT on MCUs is very challenging. On one hand, MCUs only have a very simple memory hierarchy with highly constrained memory budgets [3, 5]. For example, a state-of-the-art (SOTA) ARM Cortex-M7 MCU only has 320 KB static random-access memory (SRAM) to store intermediate data, e.g., activation, and 1 MB Flash to store program and weights, directly limiting the peak execution memory and the total parameter size of LMs [3, 5, 23, 24]. As shown in Figure 1, even with 8-bit quantization, the BERT-tiny model [4, 36] exceeds the SRAM and Flash capacity by more than 2.3\u00d7 and 4.3\u00d7, respectively.\nOn the other hand, the computation graph of a Transformer block in BERT is more complex when compared with convolutional neural networks (CNNs): each multi-head attention (MHA) block not only comprises more MCU-unfriendly tensor layout transformation operators, e.g., reshape, transpose, etc, but also organizes them with linear operators in a more complex topology. Naively executing these operators can result in significant memory consumption and poor performance [5, 18].\nExisting works, however, cannot resolve these challenges. Most SOTA BERT designs [19, 21, 32] and network optimization designs"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Model Deployment on MCUs.", "content": "There are two main approaches to deploy models on MCUs, interpretation [8, 12] and code generation [5, 23, 24]. The interpretation-based methods embed an on-device runtime interpreter to support flexible model deployment but require extra memory to store meta-information and extra time for runtime interpretation. Code-generation-based methods directly compile the given model into target code to save memory and reduce inference latency. MCUBERT uses a code generation method that is more specialized for"}, {"title": "2.2 Network efficiency optimization.", "content": "Network efficiency is very important for the overall performance of deep learning systems and has been widely studied. We focus on reviewing the model optimizations targeting at MCUs and for LMs.\nDeep learning models need to meet the tight storage and memory constraints to run on MCUs. Previous works propose network and scheduling optimization as summarized in Table 1. [3, 24, 31] compress CNNs with NAS to meet the storage constraints. [5] deploys small Transformer models which already satisfy the memory constraints of MCUs. [22] deploys vision transformer (ViT) on MCUs mainly by compressing the MLP layers and searching the token numbers. However, there are new challenges for deploying BERT on MCUs. First, compared to tiny CNNs and small Transformer models, BERT models have more parameters. Second, the memory bottleneck of BERT inference mainly lies in the MHA block when sequence length is long, which is shown in Figure 1(c). The computation of MHA block is more complex compared with CNN blocks and MLP layers, bringing new challenges for scheduling optimization. We propose MCUBERT to first deploy BERT, the most representative encoder transformer model, on MCUs.\nThough no existing works deploy BERT models on MCUs, there are network optimization algorithms proposed for LMs on other platforms, e.g., GPUs, such as pruning [6, 10, 34, 41], quantization [2, 28, 33, 43], low-rank factorization [1, 7, 17, 21, 25], etc. As MCUs do not natively support low-bit quantization or sparse computation, low-rank factorization is usually more MCU-friendly. Existing works such as Distilled Embedding [25] and Albert [21] leverage low-rank factorization based on singular value decomposition (SVD) for embedding compression and prune singular values with small magnitudes. Adaptive embedding [1] compresses the embedding table with clustered low-rank approximation: it divides the tokens in an embedding table into clusters first and applies low-rank approximation with different ratios to each cluster based on the cluster importance. Adaptive embedding empirically leverages token frequency as the proxy metric for its importance, which leads to high accuracy degradation.\nThere are also scheduling optimization proposed for Transformer models on GPUs. FlashAttention [11] only computes the attention score tensor partially each time and repetitively update the accumulation of the partial sum to drastically reduce the execution memory. There are also previous works like [14, 30, 42] that use kernel fusion to reduce memory usage and accelerate inference. However, these methods usually do not consider quantization and targets at both training and inference. Our scheduling optimization is inspired by these methods but is more MCU-friendly.\nAs shown in Table 1, our proposed MCUBERT first deploys transformer model BERT on MCUs. MCUBERT compresses the embedding table, which account for most of the parameters and can't be stored in MCU. Besides MLP block, We also carefully optimize the MHA block, which comprises more MCU-unfriendly operators and can also be the memory bottleneck of BERT inference. Compared with GPU-based methods, MCUBERT conduct network and scheduling optimization in a more MCU-friendly mode."}, {"title": "3 MCUBERT: MCU-friendly Network/Scheduling Co-Optimization", "content": null}, {"title": "3.1 Motivations and Overview", "content": "We now discuss our observations on key challenges that prevent running BERTs directly on MCUs and introduce the overall flow of MCUBERT. The notations used in the section is summarized in Table 2.\nObservation 1: the tight MCU Flash storage limits model size and forces to use small BERT models, for which the embedding table becomes the major bottleneck. To satisfy the Flash storage constraints of MCUs, which is often less than 2 MB [3, 24], we are forced to consider only small BERT models, e.g., BERT-tiny and BERT-mini [13], which have lower embedding dimensions and fewer Transformer layers. However, there still exists a 4.3\u00d7 gap between the model size and the MCU Flash, even for BERT-tiny with 8-bit quantization. As shown in Figure 1(a), the embedding table contributes to more than 90% of the parameters of BERT-tiny and becomes the major bottleneck. Hence, embedding compression is required to enable BERT on MCUs.\nObservation 2: the MCU SRAM size limits the execution peak memory of both MHA and MLP, especially for long sequence lengths. During BERT inference, all the activations need to be stored in the MCU SRAM. Although Flash paging and re-materialization has been proposed [27] to reduce the SRAM requirements, the introduced Flash access and re-computation incur high power and latency overhead. As shown in Figure 1(c), with the increase of the sequence length, the peak memory of both MHA and MLP increases significantly and quickly exceed the MCU SRAM limit. Depending on the sequence length, both MLP and MHA can be the memory bottleneck. Meanwhile, as shown in Figure 1(b), for a long input sequence, the score matrix in MHA incurs highest memory consumption. Therefore, both MLP and MHA, especially the score matrix, needs to be optimized to enable processing long sequences.\nObservation 3: the naive design of computation kernels introduces non-negligible latency overhead. Both MHA and MLP involve many linear/batched matrix multiplication operators with large shapes. MHA further complicates the inference with extensive"}, {"title": "3.2 MCU-aware NAS for Embedding Compression", "content": "MCU-aware NAS Formulation. To reduce the parameter size and satisfy the tight Flash storage constraint, we propose embedding compression based on clustered low-rank approximation following [1], which is more MCU-friendly as MCUs do not natively support sub-8-bit quantization or sparse computation. Following [1], we set the number of clusters to 4 and show its impact in ablation study. Two important questions remain to be answered: 1) how to cluster the tokens and 2) how to determine the approximation"}, {"title": "3.3 MCU-friendly Scheduling Optimization", "content": "We now introduce our MCU-friendly scheduling optimization to reduce the inference memory and latency. We optimize the scheduling of both MHA and MLP since and also optimize the kernel implementation for MHA for better efficiency."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiment Setup", "content": "Dataset. We search and evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark [37], which is a collection of text classification tasks. For most evaluations and comparisons, we leverage the Multi-Genre Natural Language Inference Corpus (MNLI) dataset, which is the largest dataset in GLUE.\nSearching setting. We select lightweight BERT models, i.e., BERT-tiny and BERT-mini for our experiments, and the pre-trained models are adopted from [4, 36]. For NAS, we use AdamW optimizer with a zero weight decay. We use a batch size of 32 for training and set the learning rate to 5 \u00d7 10-5.\nModel deployment. We deploy our model on different MCUs, i.e., NUCLEO-F746 with 320KB SRAM and 1MB Flash, NUCLEO-F767 with 512KB SRAM and 2MB Flash as well as NUCLEO-H7A3ZI-Q with 1.4 MB SRAM and 2MB Flash, to measure the latency and the peak memory usage. The batch size is fixed to 1."}, {"title": "4.2 Importance of two-stage NAS", "content": "As described in Section 3, we propose a two-stage NAS strategy, searching token clustering in the first stage and approximation ratios in the second stage. To demonstrate the importance of such two-stage formulation, we compare MCUBERT with other baselines based on BERT-tiny and MNLI dataset, as shown in Figure 7, including 1) baseline adaptive embedding; 2) one-stage NAS that searches the token clustering and approximation ratios together (DARTS"}, {"title": "4.3 Accuracy Comparison on GLUE", "content": "Comparison on MNLI.. We compare MCUBERT with the baseline FWSVD [17] and adaptive embedding [1] on the MNLI dataset, as shown in Table 3. For both BERT-tiny and BERT-mini, MCUBERT can simultaneously achieve better accuracy and smaller parameter size compared to the baseline FWSVD and adaptive embedding. Specifically, for BERT-tiny, MCUBERT can achieve 1.4\u00d7 and 1.6x embedding parameter reduction with 1.3% and 7.5% better accuracy compared to adaptive embedding and FWSVD, respectively. With the same model size, MCUBERT improves the accuracy by 2.2% and 8.4%. For BERT-mini, MCUBERT achieves 3.3% better accuracy with a smaller parameter size compared to adaptive embedding. We also observe for BERT-tiny, MCUBERT tries to assign a higher approximation ratio for the second cluster but assign more tokens to the fourth cluster.\nComparison on Other Datasets. We then compare MCUBERT with adaptive embedding on other GLUE datasets. Note we focus on the MRPC, SST2, and QQP datasets as even the full BERT-tiny and BERT-mini suffers from accuracy issues on COLA and RTE datasets [13]. As shown in Table 4, MCUBERT consistently outperforms adaptive embedding on all three tasks by 3.43%, 0.69%, 0.88% better accuracy for BERT-tiny, respectively. For different datasets, the number of tokens and the approximation ratio of each cluster are different, indicating the necessity of task loss-guided compression."}, {"title": "4.4 Peak Memory and Latency Comparison", "content": "Peak memory comparison. We compare the inference latency and peak memory with the baseline CMSIS-NN [20], [5], and FlashAttention [11]. We re-implement [5] and [11] based on the original paper. When re-implement FlashAttention, we need to store the output matrix in high precision, which has been discussed in Section 3.3. As shown in Figure 8, both CMSIS-NN and [5] do not consider the execution memory in the scheduling, leading to a fast memory growth and out-of-memory issue for a long sequence length, i.e., 512. For BERT-tiny, MCUBERT achieves more than 1.9\u00d7 and 3.5x peak memory reduction compared to the baseline when the sequence lengths are 64 and 512, respectively. This indicates MCUBERT can support 3x, 2x, and 2\u00d7 longer input sequences compared to CMSIS-NN and [5] on MCUs with 512 KB, 320 KB and 128 KB SRAM, respectively. In FlashAttention, the output matrix needs iterative updates, demanding fp32 data format instead of int8 precision, causing substantial memory usage growth. Moreover, typical FlashAttention implementation keeps the entire query, key, and value matrices in memory, further increasing memory consumption compared to our approach. Compared with FlashAttention, MCUBERT also achieves more than 1.6\u00d7 and 1.9\u00d7 peak memory reduction in all sequence lengths for BERT-tiny and BERT-mini, respectively.\nLatency comparison. The latency comparison for different sequence lengths is shown in Table 5. MCUBERT achieves around 1.5x and 1.3\u00d7 latency reduction compared to CMSIS-NN and [5] consistently for different sequence lengths and different MCUs. We visualize the latency breakdown for [5] and MCUBERT. As shown in Figure 10, MCUBERT reduces the latency of both the linear and matrix multiplication operators and fuses all tensor shape transformation operators, demonstrating the high efficiency of our kernel design. Compared with other acceleration strategies such as FlashAttention, MCUBERT still achieves lower latency, although MCUBERT already shows lower peak memory usage than FlashAttention. With the model and scheduling optimization, MCUBERT enables to deploy BERT-tiny on NUCLEO-F746 with sequence length of 512 for the first time."}, {"title": "4.5 Ablation Study", "content": "Impact of the number of clusters. MCUBERT follows adaptive embedding and empirically fix the number of clusters to 4. We now evaluate its impact on the accuracy-parameter Pareto front. We use the BERT-tiny model and MNLI dataset for comparison. As shown in Figure 11, the Pareto front for c = 4 indeed outperforms that for c = 3 or c = 5. We observe c = 3 performs much worse when the model parameter size is small. We hypothesize this is because when c = 3, important tokens are forced to have a small dimension, leading to low accuracy.\nImpact of fine-grained scheduling. To evaluate the impact of fine-grained scheduling on both latency and memory, we change the number of tokens in each tile, i.e., t. The change in latency and peak memory is plotted in Figure 9. As we can observe, the peak execution memory reduces consistently with the decrease of t while the latency remains roughly the same for t \u2264 2. The high latency for t = 1 is because of hardware under-utilization."}, {"title": "5 Conclusion", "content": "This work proposes MCUBERT, a network/scheduling co-optimization framework enabling BERT on MCUs. For network optimization, MCUBERT proposes an MCU-aware two-stage NAS algorithm with clustered low-rank approximation for embedding compression. For scheduling optimization, we leverage tiling, in-place computation, and kernel optimization to simultaneously reduce peak memory and latency. MCUBERT overcomes all existing baselines and enables to run the lightweight BERT models on commodity MCUs for the first time."}, {"title": "Algorithm 1 Searching for Token Clustering", "content": "Input: Embedding matrix Emb, vocabulary size v, # clusters c, embedding dimension d, division value div, # training epochs epochs\nOutput: Clustering results cls\n1: aj,i=Vj, i\n2: U0 = Emb\n\u25ba The embedding table of first cluster is not factorized\n3: for i = 1,..., c - 1 do\n4: d U\u00a1, V = LowRankFactorization(Emb, )\n\u25ba Factorize the embedding table of other clusters except\nthe first cluster\n5: end for\n6: Define the embedding of j-th token: aj,0U0,j + \u2211i=1 aj,iUi,jV\n\u25ba Vo is excluded as the embedding table of the first cluster is\nnot factorized\n7: for z = 0, ..., epochs - 1 do\n8: Fix a and update weights w by descending \u2207wl(w, a)\n9: Fix w and update architecture parameters a by descending \u03bd\u03b1 (l(w, \u03b1) + \u03bb \u03a3\u00a1 Size(Embi))\n10: end for\n11: cls[j] = arg maxi (aj,i) Vj\n12: return cls"}, {"title": "Algorithm 2 LowRankFactorization", "content": "Input: Matrix to be factorized Matrix; Factorization ratio r\nOutput: Factorization Results U and VT\n1: U, Diag(2), VT = SVD(Matrix)\n\u25ba Diag(2) denotes the diagonal matrix filled with vector \u03a3\n2: \u03a3 = \u03a3[0 : r]\n3: U = U[:, 0 : r] Diag(21/2)\n4: VT = Diag(21/2) VT [0 : r, :]\n5: return U, VT"}, {"title": "Algorithm 3 Searching for Low-Rank Approximation Ratio", "content": "Input: Embedding matrix Emb, vocabulary size v, # clusters c, embedding dimension d, # training epochs epochs, importance threshold \u1e9e*, searched token clustering cls\nOutput: Low-rank approximation ratios ratios\n1: Bi,1 = B Vi, l\n2: Divide the Emb into c parts Embo,..., Embc-1 according to token clustering cls\n3: U0 = Embo\n\u25ba The embedding table of first cluster is not factorized\n4: for i = 1,..., c - 1 do\n5: Ui, Diag(\u2211i,0, . . ., \u03a3i,d\u22121), V = SVD(Embi)\n\u25ba Factorize the embedding table of other clusters except the\nfirst cluster\n6: end for\n7: Define embedding of j-th token in i-th cluster (for i \u2265 1): Ui,j \u00b7 Diag(\u03b2i,021,0, ..., Bi,d\u22121\u2211i,d\u22121)V\n8: For the first cluster, the embedding of j-th token is Uo, j\n9: for z = 0, ..., epochs - 1 do\n10: Fix \u1e9e and update weights w by descending \u2207wl(w, \u03b2)\n11: Fix w and update architecture parameters \u1e9e by descending \u25bc\u03b2(l(w, \u03b2) + \u03bb \u03a3\u00a1 Size(Embi))\n12: end for\n13: Initialize ratios[i] = 0 Vi\n14: for i = 1,..., c - 1 do\n15: for l = 0, ..., d- 1 do\n16: ratios[i]+ = 1\n17: if Bi,1 > B\n\u25b7 Only save singular values whose importance is larger\nthan threshold\n18: end for\n19: end for\n20: return ratios"}], "equations": [{"equation": "\\begin{equation}\naj,0Embo[j] + \\sum_{i=1}^{c-1}aj,iEmb_i[j],\n\\end{equation}", "context": "Then, the embedding for the j-th token becomes:"}, {"equation": "\\begin{equation}\nEmb_i = U_iDiag(\\beta_{i,0}\\Sigma_{i,0}, \\cdots, \\beta_{i,d-1}\\Sigma_{i,d-1})V.\n\\end{equation}", "context": "Then, we have"}, {"equation": "\\begin{equation}\nSize(Emb_i) = \\sum_l \\beta_{i,l}(d + \\sum_j a_{j,i}),\n\\end{equation}", "context": "Based on \u03b1 and \u03b2, the embedding size for the i-th cluster can be approximated as"}, {"equation": "\\begin{equation}\n\\min_{\\alpha,\\beta} \\min_W l_{w,\\alpha,\\beta} + \\lambda\\sum_i Size(Emb_i).\n\\end{equation}", "context": "min lw,a,\u03b2 + \u03bb\u2211 Size(Embi).\n\u03b1,\u03b2\nW"}]}