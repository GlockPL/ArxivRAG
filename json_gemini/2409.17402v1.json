{"title": "Enhancing Recommendation with Denoising Auxiliary Task", "authors": ["Pengsheng Liu", "Linan Zheng", "Jiale Chen", "Guangfa Zhang", "Yang Xu", "Jinyun Fang"], "abstract": "The historical interaction sequences of users plays a crucial role in training recommender systems that can accurately predict user preferences. However, due to the arbitrariness of user behavior, the presence of noise in these sequences poses a challenge to predicting their next actions in recommender systems. To address this issue, our motivation is based on the observation that training noisy sequences and clean sequences (sequences without noise) with equal weights can impact the performance of the model. We propose a novel self-supervised Auxiliary Task Joint Training (ATJT) method aimed at more accurately reweighting noisy sequences in recommender systems. Specifically, we strategically select subsets from users' original sequences and perform random replacements to generate artificially replaced noisy sequences. Subsequently, we perform joint training on these artificially replaced noisy sequences and the original sequences. Through effective reweighting, we incorporate the training results of the noise recognition model into the recommender model. We evaluate our method on three datasets using a consistent base model. Experimental results demonstrate the effectiveness of introducing self-supervised auxiliary task to enhance the base model's performance.", "sections": [{"title": "1 Introduction", "content": "Recommender systems play a crucial role in today's internet and e-commerce domains, offering users improved information retrieval and shopping experiences, while also yielding substantial economic benefits for businesses [1-3]. Click-through rate (CTR) prediction holds a significant role within personalized recommender systems [4-7]. By analyzing users' historical interaction sequences, these systems recommend products aligned with user interest and preferences, facilitating the discovery of potentially engaging content [8]. This method enhances user experience, fosters sales and propagates content [9,10]. In the context of sequence-based recommendation, the issue of noise present in sequences significantly impacts the establishment of accurate and reliable recommender models, forming a complex and pivotal challenge within the field. Sequence noise can arise from various sources, including user curiosity, data collection inaccuracies and environmental shifts, consequently leading to misjudgments of user interest and inaccurate recommender model outcomes [7,11-17]. Models trained on clean sequences significantly outperform those trained on original, noise-containing sequences. This underscores the imperative of exploring denoising strategies in recommender systems [18].\nTo address the challenges mentioned above, denois-"}, {"title": "2 Related Work", "content": "In this section, we introduce the CTR Models and provide a comprehensive overview of the methods related to sequence denoising in CTR Models."}, {"title": "2.1 CTR Models", "content": "In recent years, deep learning based models have gained significant traction in CTR prediction [15]. These models exhibit strong representation learning capabilities, enabling them to capture more intricate and challenging patterns and features. Existing deep learning based recommender models can be broadly categorized into two types: sequence-based [7,26\u201334] and graph-based [35-37]. We propose a sequence-based denoising method in this paper. Consequently, this subsection focuses on sequence-based recommender models. Wide & Deep [11] and DCN [14] leverage the memory and generalization capabilities of feature interactions by combining traditional generalized linear models with deep neural networks. DIN [7] uses self-attention mechanisms to enhance the representation of user interest. SASRec [38] and S3Rec [39] utilize multi-head self-attention mechanism to model relationships within sequences. PSSA [40] employs a learnable progressive sampling strategy to identify the most valuable items. FEARec [41] enhances recommendation by converting user historical behavior sequences into frequency domain representations and combining them with a self-attention mechanism.\nCTR models leverage self-supervised learning [42] methods to improve data utilization and learn feature representations. For instance, DuoRec [43] and MPT [44] enhance item embedding distributions through contrastive learning. ICL [45] and simple CL method [46] address data sparsity and popularity bias by learning user intent representations. Pre-training GNN [47], multi-channel hypergraph convolutional network [48], DHCN [49] and self-supervised tri-training [50] integrate self-supervised learning with other relevant techniques to enhance the performance of recommender systems."}, {"title": "2.2 Denoising Methods", "content": "Identifying noisy sequences is an essential step in sequence denoising. DROP [51] and three instance selection methods [52] discuss how to reduce the number of sequences in the training set without affecting classification accuracy. AutoDenoise [25] deletes sequences that have a counteractive effect on the model through rewards. Hierarchical Reinforcement Learning for Course Recommendation in MOOCs [53] removes noisy courses by jointly training of a hierarchical reinforcement learning-based modifier and a basic recommender model. DeCA [24] determines noisy sequences by analyzing the discrepancies in user preferences predicted by two recommender models. MMInfoRec [54] and ContrastVAE [55] address issues such as sparsity and uncertainty in recommender systems by leveraging contrastive learning techniques. DT4SR [56] effectively resolves the problem of neglecting user dynamic preferences and item relationships in traditional methods by introducing uncertainty into sequential modeling.\nSDK framework [57] deals with the challenges of Knowledge Graphs (KGs) in knowledge-aware recommendation by modeling hyper-relational facts and using self-supervised learning mechanisms. SGL [58] improves the recommendation performance of long-tail items and the robustness against interaction noises by using an auxiliary self-supervised learning task. We propose a denoising auxiliary task neither requires considering the impact on the model nor adds excessive additional training steps. We define a model capable of recognizing noise, thereby enhancing the model's performance.\nAfter recognizing the noisy sequences, we need to handle these sequences to improve the performance of the recommender model. Existing methods for handling noisy sequences can be classified into two categories: truncated denoising [18,19,21] and reweighted denoising [18]. WBPR [19] and T-CE [18] define thresholds for samples, truncating sequences with loss values higher than the threshold at each iteration. IR [21] modifies labels to train downstream modules for recommendation tasks. In R-CE [18], smaller weights are assigned to high-loss sequences to prevent the model from fitting them too quickly. However, truncated denoising risks filtering out many clean sequences, while reweighted denoising suffers from limited transferability. We propose an ATJT method similar to reweighted denoising, but it addresses limitations by adaptively adjusting the weighting degree."}, {"title": "3 Methodology", "content": "In this section, we will introduce the preliminary work and discuss the training processes for both the noise recognition model and the recommender model. We will also provide a detailed explanation of how to implement the ATJT method."}, {"title": "3.1 Preliminary", "content": "In this paper, we use batch b composed of training sequences as the input for both the noise recognition model and the recommender model. Each batch has a size M, and the sequences have a length N.\nAll batches are divided into two groups, BR and BD. The batch in the first group, denoted as bR = {si,1,\u2026, si,m,\u2026\u2026,si,M} \u2208 BR, undergoes obtaining the weights of historical interaction sequences through the noise recognition model. We then use the reweighted sequences to train the recommender model. We use si to represent the sequences of the i-th batch that are used for training the recommender model. And BR consists of a total of I batches.\nThe batch in the second group, denoted as bD = {sj,1,\u2026\u2026, sj,m, \u2026\u2026\u2026, sj,M} \u2208 BD, is used to train the noise recognition model capable of accurately recognizing noisy sequences. We use sj to represent the sequences of the j-th batch that are used for training the noise recognition model. And BD consists of a total of J batches. In summary, BR\u016aBD = B and BR\u2229BD = \u00a2.\nWe further divide the batch bD into two batches, b(+)\nD and b(\u2212)\nD. b(+)\nD represents clean batch within bD consisting of original sequences. b(\u2212)\nD = {sj,1,\u2026, sj,m\u2026, sj,M} represents noisy batch consisting of randomly replaced sequences from bD, where sj,m = {v1, ..., vn..., vN} represents the m-th noisy sequence that has undergone random replacement in the j-th batch of the noise recognition model. Within the sequence sj,m, vn represents the n-th interaction item that has been randomly replaced. At this point, the second batch transforms into bD ="}, {"title": "3.2 Recommender Model", "content": "In the training process of the recommender model, as shown in Fig.1, given a batch bR\u2208 BR, the sequences si in the batch initially pass through the noise recognition model to obtain weights wi. Subsequently, the reweighted sequences are used to train the parameters of the recommender model. It is worth noting that the recommender model can be chosen based on specific requirements, such as DIN or DCN. Its training process aligns with these base models.\nThe CTR prediction of the recommender model can be viewed as a supervised binary classification task. Therefore, we optimize the recommender model using a binary cross-entropy loss function. Additionally, considering the impact of noisy sequences on the training of the recommender model, it is essential to recognize and assign smaller weights to mitigate the influence of noisy sequences. Consequently, we define the loss function for the recommender model as follows:\n$C_R = \\frac{1}{|b_R|} \\sum_{s_i \\in b_R} \\sum (w_i(y_i \\log f(u, s_i; \\Theta_R) + (1-y_i) \\log(1 - f(u, s_i; \\Theta_R))))$, (1)\nwhere yi and f(u, si; OR) represent the labels for clicks and the predicted probabilities of clicks for the sequences si in br, respectively. wi represents the weights of the sequences si. Typically, noisy sequences have smaller weights w compared with clean sequences in model training (as shown in our experiments in Subsection 4.2.4). This approach reduces the impact of noisy sequences on model performance [18,24]. We will elaborate on how to determine the sequence weights wi that improve the performance of the recommender model in Subsection 3.3.2 and Subsection 3.4.2."}, {"title": "3.3 Noise Recognition Model", "content": "To build a noise recognition model capable of accurately distinguishing noisy sequences from clean sequences and weighting the sequences for the recommender model, we opt for a self-supervised training"}, {"title": "3.3.1 Data Replacement", "content": "As shown in Fig.2(a), we use the batch bD = b(+)\nD \u222a b(\u2212)\nD as the input for the noise recognition model, where b(+)\nD represents clean batch consisting of original sequences, labeled as 1. And b(\u2212)\nD represents noisy batch composed of randomly replaced noisy sequences, labeled as 0. While the selection of b(\u2212)\nD from bD is not fixed, it should not be too scant. Specifically, we assume that there are very few noisy sequences in b(+)\nD. If we select too few sequences in b(\u2212)\nD, it may lead to a situation where the extremely few noisy sequences in b(+)\nD outnumber the sequences in b(\u2212)\nD, meaning that the number of sequences in b(+)\nD labeled as 1 while actually being 0 is greater than the number of sequences in b(\u2212)\nD labeled as 0. This situation could lead the noise recognition model to incorrectly learn noisy sequences as positive (labeled 1). Hence, it is essential to ensure an adequate number of sequences in b(\u2212)\nD to avoid an unstable situation that could lead the noise recognition model to learn in the wrong direction. Up to this point, we have discussed the training method for the recommender model and how input sequences for the noise recognition model are generated."}, {"title": "3.3.2 Weight Generation", "content": "In this subsection, we will describe the method for generating weights wi. As depicted in Fig.1, the noise recognition model is a sequence-to-value model. The model takes bD \u2208 BD as input. For sj\u2208bD, where sj consists of items with length N and a target item to be predicted. sj first passes through the neural network's embedding layer, it is transformed into the sequences of embeddings [e1,\u2026\u2026,eN,\u2026\u2026,eN,eT], in which en represents the embedding of the n-th item in the sequences sj after passes through the neural network's embedding layer. Then, we pass it through the attention network to obtain user hidden representation of the sequences sj:\n$h = \\sum_{n=1}^{N} a_n e_n$,\nwhere\n$a_n = \\frac{MLP(e_n || e_T)}{\\sum_{n'=1}^N MLP (e_{n'} || e_T)}$\n|| represents the concatenation of embeddings. Subsequently, we concat h with the embedding eT of the target item, and then pass the results through a MLP to produce the weights wj. Given that our noise recognition method can be viewed as a self-supervised binary classification task, we use the binary cross-entropy loss function for optimization:\n$\\pounds_D^P = \\frac{1}{|b_D|} \\sum_{s_j \\in b_D} (y_j \\log g(u, s_j; \\Theta_D) + (1 - y_j) \\log(1 - g(u, s_j; \\Theta_D)))$, (2)\nwhere yj and g(u, sj; OD) = wj represent the labels for noise and the predicted probabilities of noise for the sequences sj in bD, respectively.\nThe noise recognition model similarly uses training set bR \u2208 BR, which is used in training the recommender model, as input. This set of sequences is non-overlapping with the training sequences used for the noise recognition model, which will be explained in detail in Subsection 3.4.1. At this point, we can use the results g(u, si; OD) output by the noise recognition"}, {"title": "3.4 ATJT Method", "content": "To fully use the data, after fitting the parameters of both the recommender model and the noise recognition model, we can reverse the training data for BR and BD. This means that BD optimizes the recommender model while BR optimizes the noise recognition model. It is worth noting that the recommender model continues to use the original model in the subsequent training, while the noise recognition model is trained using a duplicate model. The purpose of this is to ensure that all data can be used to train the recommender model, while the noise recognition model does not fit all the data. Throughout the training process, the reweighted recommender model and the noise recognition model are trained together. This ensures that the noise recognition model can accurately recognize noisy sequences while optimizing the results of sequence reweighting.\nTwo important points to note are: first, we only use the original sequences to train the recommender model, and the noise recognition model is trained with original sequences and randomly replaced noisy sequences. Second, if there is a need to make more extensive use of the data, the training set sequences can be divided into"}, {"title": "3.4.1 Data Partition.", "content": "N groups instead of two groups. As shown Fig.2(b), we demonstrate a training method where the training set is divided into four groups. In extreme cases, only one sequence receives the best reweighting output by the noise recognition model and trains the recommender model, while the rest of the sequences are input into the noise recognition model to achieve the best recognition performance in training. However, this method increases the number of duplicate models for training the noise recognition model. Therefore, the minimum grouping is two groups, and the maximum grouping is the size of the training set sequences | BR \u222a BD|. The specific grouping can be chosen based on available resources and performance considerations."}, {"title": "3.4.2 Loss Function", "content": "In this subsection, we focus on how to jointly train the recommender model with the noise recognition model by computing the loss value. When we partition the training set sequences into N groups, due to |BR| : |BD| = 1 : N - 1, the batches used for training the recommender model should be in a ratio of 1: N-1 compared with those used for training the noise recognition model, as illustrated in Fig.3. Therefore, the loss function for the noise recognition model should be:\n$C_D^P = \\frac{1}{N-1} \\sum_{j=i(N-1)}^{(i+1)(N-1)-1} \\pounds_D^P$,\nwhere i represents the index of the batch used by the recommender model. N - 1 represents the number of batches used by the noise recognition model corresponding to one batch of data used for training the recommender model, and C represents the binary cross-entropy loss function when training the noise recognition"}, {"title": "3.4.3 Overall Optimization Algorithm of Model Training", "content": "The joint training process is illustrated in Algorithm 1. The joint training consists of two parts: the calcu-"}, {"title": "4 Experiments", "content": "We conduct extensive experiments to address the following three questions:\n\u2022 RQ1: How does the performance of the ATJT method compare with the base model?\n\u2022 RQ2: What is the impact of different types of noisy sequences generation on performance?\n\u2022 RQ3: How does different sequence weighting training methods affect performance?"}, {"title": "4.1 Experimental Setting", "content": "We evaluate our method using the MovieLens20M\u2460, Amazon (Electro) and Yelp datasets. We select these three datasets for two reasons: 1) They represent diverse scenarios, namely an online movie platform and an e-commerce platform, with varying levels of product diversity. 2) They differ in size and characteristics. The statistical data for MovieLens20M, Amazon (Electro) and Yelp are shown in Table 1."}, {"title": "4.1.2 Evaluation Protocol", "content": "To accurately assess the performance of the recommender model, we first divide users' historical interaction sequences into training and testing sets in a 4:1 ratio. In this setup, we use the training set to train both the noise recognition model and the recommender model, while the testing set is used to evaluate the performance of the recommender model. Notably, we need to ensure that users' historical interaction sequences in the training and testing sets are non-overlapping. Additionally, to avoid the issue described in Subsection 3.4.1, where the noise recognition model fits the training data, we also need to ensure that the historical interaction sequences used for training the recommender model and the noise recognition model are non-overlapping.\nWe evaluate the testing set using standard AUC (Relative Improvement Area Under the ROC Curve) scores, HR@5 and NDCG@5. These three metrics are widely used in click prediction tasks [7,8]. Higher values for all three metrics indicate superior model perfor-"}, {"title": "4.1.3 Implementation Details", "content": "The construction of the ATJT method is based on the PyTorch framework. We encapsulate the noise recognition model into a class, allowing it to be integrated as a plugin with most recommender models. The implementation of the noise recognition model follows a unified structure when integrated with different underlying recommender models. The implementation of the noise recognition model relies on an attention mechanism. Specifically, we implement it as an attention model with embedding and output layers. The attention part consists of two layers of MLPs. For MLP (1), we set the linear layers as (64, 32), and for MLP (2), we set the linear layers as (32, 1). Each MLP consists of a linear layer, a PReLU activation function and a dropout operation (rate=0.5). The output part after SUM pooling consists of three layers of MLPs. For MLP (1), we set the linear layers as (40, 256), for MLP (2), we set the linear layers as (256, 64), and for MLP (3), we set the linear layers as (64, 1). The structure of MLPs is identical to the attention part. The output layer is implemented with a sigmoid function, with a dimension of 1, in order to obtain different weights for training the recommender model with noisy and clean sequences. The noise recognition model is uniformly optimized using the Adagrad optimizer, and a learning rate search is conducted from {0.1, 0.01, 0.001, 0.0001}.\nWhen training recommender models, each method follows the following steps: 1) When training DCN and Wide & Deep models, we treat historical interactions as item features. The DNN architectures for DCN and Wide & Deep are set as (128, 128) and (256, 128), respectively. 2) For DIN model, we use user ID, historical interaction item IDs and their corresponding categories as input features, following [7]. 3) For SASRec and S3Rec models, we use historical interaction item IDs as input features, following [38] and [39]. 4) When training the FEARec model, our input features are the same as those used in the DIN model. Additionally, we use the default hyperparameter configurations provided by the original author of the model on GitHub."}, {"title": "4.2 Experimental Results", "content": "We propose an ATJT method based on six fundamental recommender models and compare their per-"}, {"title": "4.2.1 Overall Performance (RQ1)", "content": "formance with the base recommender models on three different datasets. Experimental results show that the ATJT method outperforms base models in terms of AUC, HR@5 and NDCG@5, as shown in Table 2.\nWe find that the ATJT method yields better improvements in DCN and Wide & Deep models compared with DIN, SASRec, S3Rec and FEARec models. This phenomenon can be attributed to the attention mechanism possessed by DIN, SASRec, S3Rec and FEARec, which adaptively learns users' interest representations from the historical interaction sequences, thus mitigates the impact of behaviors unrelated to users' interest representations [7]. Furthermore, the enhancement of the ATJT method is more pronounced in the Wide & Deep model than in the DCN model. The reason may lie in that when we process the input features of the DCN model, we regard historical interactions as item features. Therefore, the cross network captures feature interactions, mitigates the impact of irrelevant features on model performance during training [14]. The Wide & Deep model lacks attention mechanisms like DIN, multi-head attention mechanism like SASRec, S3Rec and FEARec or the cross network like the DCN model, which can filter out irrelevant or negative behaviors on model optimization.\nThe ATJT method compensates for the Wide & Deep model's inability to filter out irrelevant or negative behaviors within user actions, resulting in a more noticeable performance improvement.\nThe ATJT method demonstrates superior performance on the Yelp dataset compared with the MovieLens20M and Amazon (Electro) datasets. This observation may be attributed to the relatively larger size of"}, {"title": "4.2.2 Noise Generation Analysis (RQ2)", "content": "In this subsection, we analyze the effect of selecting how many sequences in the noise recognition model training data to replace, and the effect of replacing a different number of historical click items in the artificially replaced noisy sequences. Fig.4 shows the impact of selecting 0.1, 0.3, 0.5, 0.7 and 0.9 of the data in the noise recognition model training as noisy sequences on the performance of the recommender model. Table 3 shows the impact of replacing 1, 2, 3, 5 and 10 historical click items in the artificially replaced noisy sequences on the performance of the recommender model."}, {"title": "4.2.3 Sequence Weighting Ablation Experiments (RQ3)", "content": "To investigate the impact of different noisy sequence weighting methods on the performance of the recommender model, we compare three weighting training methods: Without Auxiliary Task (WAT), Direct Noise Recognition (DNR) and ATJT. The sequence weights for three methods are obtained through a consistent model structure as described in Subsection 4.1.3. In the DNR method, the noise recognition model is first"}, {"title": "4.2.4 Impact of Noise on Sequence Weights (wi)", "content": "To demonstrate that noisy sequences have smaller weights compared with clean sequences, we use the DCN model as the base model and conduct analyses on three datasets. As shown in Table 5, the weights of the noisy sequences are approximately 19% on average smaller compared with the weights of the clean sequences. This aligns with our expectation that assigning smaller weights to noisy sequences can enhance the base model's performance."}, {"title": "5 Conclusion and Future Work", "content": "In this study, we proposed a novel self-supervised ATJT method. This method leverages the training outcomes of a noise recognition model to reweight sequences for training the recommender model. Additionally, we conducted joint training of the recommender model and noise recognition model to acquire more appropriate weights, further enhancing the performance of the recommender model. We then evaluated our method on three datasets and six base models, demonstrating its effectiveness. Finally, we validated the impact of different noisy sequences and training methods on recommender model performance through Noise Generation Analysis and Sequence Weighting Ablation experiments.\nIn the context of future prospects, through adversarial networks, the well-trained noise recognition model can discriminate between artificially replaced noisy sequences, is used as a discriminator to learn a generator that makes it unable to recognize whether the sequences has been artificially replaced with noise. At this point, the generator can create noisier sequences than those replaced by humans, which may be more similar to the noisy sequences in the original sequences. Therefore, using these generated sequences as noisy sequences might yield better results."}]}