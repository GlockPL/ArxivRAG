{"title": "PHYSICS-INFORMED DEEP INVERSE OPERATOR NETWORKS FOR SOLVING PDE INVERSE PROBLEMS", "authors": ["Sung Woong Cho", "Hwijae Son"], "abstract": "Inverse problems involving partial differential equations (PDEs) can be seen as discovering a mapping from measurement data to unknown quantities, often framed within an operator learning approach. However, existing methods typically rely on large amounts of labeled training data, which is impractical for most real-world applications. Moreover, these supervised models may fail to capture the underlying physical principles accurately. To address these limitations, we propose a novel architecture called Physics-Informed Deep Inverse Operator Networks (PI-DIONs), which can learn the solution operator of PDE-based inverse problems without labeled training data. We extend the stability estimates established in the inverse problem literature to the operator learning framework, thereby providing a robust theoretical foundation for our method. These estimates guarantee that the proposed model, trained on a finite sample and grid, generalizes effectively across the entire domain and function space. Extensive experiments are conducted to demonstrate that PI-DIONs can effectively and accurately learn the solution operators of the inverse problems without the need for labeled data.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has revolutionized numerous fields, from natural language processing to computer vision, due to its ability to model complex patterns in large datasets (LeCun et al., 2015). In the domain of scientific computing, deep learning offers a promising approach to solving problems traditionally addressed by numerical methods, particularly when dealing with high-dimensional or nonlinear problems (Carleo et al., 2019; Karniadakis et al., 2021). However, direct applications of neural networks in scientific fields often encounter challenges such as the need for large datasets and difficulties in enforcing known physical laws within the learning process. These challenges have led to the development of specialized machine learning approaches that can integrate the governing principles of scientific problems directly into the learning framework, improving both performance and generalizability.\n\nPhysics-Informed Machine Learning (PIML) has emerged as a powerful paradigm to address these challenges by embedding physical laws, typically expressed as partial differential equations (PDEs), into the structure of neural networks (Raissi et al., 2019; Sirignano & Spiliopoulos, 2018). Rather than solely relying on large labeled datasets, PIML incorporates the governing equations of physical systems into the learning process, allowing for data-efficient models that respect the underlying physics. This paradigm is particularly useful for scenarios where the amount of available data is limited, or where it is essential to maintain the consistency of predictions with known physical principles. PIML has been applied to a wide range of problems, including fluid dynamics, heat transfer, and electromagnetics, demonstrating the ability of neural networks to capture the behavior of complex systems while obeying their physical constraints (Mao et al., 2020; Cai et al., 2021a;b; Cuomo et al., 2022).\n\nAt the early stage, PIML methods focused on solving a single problem instance, leading to a growing need for real-time inference to multiple cases. Recent advances in operator learning have aimed to"}, {"title": "2 PHYSICS-INFORMED DEEP INVERSE OPERATOR NETWORKS", "content": "Let $\\Omega \\subset \\mathbb{R}^d$ be a bounded open set with a smooth boundary, and let $\\Omega_T = \\Omega \\times [0, T]$. Throughout this paper, we denote the domain by $\\Omega$ and $\\Omega_T$ depending on whether the problem is time-dependent. We consider a generic PDE defined by:\n\n\n$\\begin{aligned}\n\\mathcal{N}(u, s) &= 0, & x \\in \\Omega, \\\\\n\\mathcal{B}(u) &= 0, & x \\in \\partial \\Omega.\n\\end{aligned}$\n\n\nA large class of PDE-based inverse problems involves identifying an inverse operator,\n\n\n$\\mathcal{F}^{-1}: \\Omega_m \\rightarrow \\mathcal{S},$\n\n\nwhere $\\Omega_m \\subset \\Omega$ represents the measurement domain and $s$ is an unknown quantity within the system of interest. Recently, research has focused on directly parameterizing the inverse operator using neural networks $\\mathcal{F}_{\\epsilon}^{-1}$ supervised by paired input-output dataset $\\{(u^{(i)}|_{\\Omega_m}, s^{(i)})\\}_{i=1}^N$. However, from a practical perspective, such labeled pairs are often difficult to obtain. Furthermore, this type of supervised training tends to overlook important physical constraints unless explicitly addressed (Karniadakis et al., 2021; Wang et al., 2021b). Existing operator learning frameworks do not parameterize"}, {"title": "3 EXTENDING STABILITY ESTIMATES TO THE OPERATOR LEARNING FRAMEWORK", "content": "The theoretical foundation for the proposed PI-DIONs is threefold. First, we introduce the stability estimates for the inverse problems we considered. This stability estimate is a crucial component that connects the prediction error with the loss functions used for training PI-DIONs. Second, we demonstrate that this stability estimate can be extended to the operator learning framework demonstrating that small enough $\\mathcal{L}$ implies a small prediction error. Finally, we present a universal approximation theorem for PI-DIONs, which guarantees that the loss function can be reduced to an arbitrarily small value."}, {"title": "3.1 STABILITY ESTIMATES FOR INVERSE PROBLEMS", "content": "We start with a brief introduction to the stability estimates for inverse problems, beginning with an informal discussion of the connection between these stability estimates and the physics-informed loss functions.\n\nInformal statements. Let $u$ be the solution to equation 1 for a given $s$, with partial measurements taken over the subdomain $\\Omega_m \\subset \\Omega$. For any approximations $u^*$ and $s^*$, the loss functions $\\mathcal{L}_{physics}$ and $\\mathcal{L}_{data}$ are computed with $N = 1$ (i.e., the single-sample case) at the collocation points $\\{x_k\\}_{k=1}^K$, $\\{x_j\\}_{j=1}^M$."}, {"title": "3.2 STABILITY ESTIMATES IN THE OPERATOR LEARNING FRAMEWORK", "content": "We introduce a continuous version $\\bar{\\mathcal{L}}_{data}$ of $\\mathcal{L}_{data}$ on $\\Omega_m$ as follows:\n\n\n$\\bar{\\mathcal{L}}_{data} = \\int_{\\Omega_m} [u_{\\eta,\\theta}(x) - u(x)]^2 dx d\\mu(U),$\n\n\nwhere $u_{\\eta,\\theta}$ is produced by the input $u$ and $\\mu(U)$ represents the probability measure on the function space $U$. Similarly, let $\\nu(S)$ denote the probability measure on the function space $S$ which can be induced by the inverse operator $\\mathcal{F}^{-1}$ from $\\mu(U)$. We assume the dataset $\\{(u^{(i)}|_{\\Omega_m}, s^{(i)})\\}_{i=1}^N$ is sampled from $\\mu(U)$ and $\\nu(S)$ by using this probability measure.\n\nWe now state a theorem that implies the difference between $\\mathcal{L}_{data}$ and $\\bar{\\mathcal{L}}_{data}$ becomes small when both the number of sampled functions $N$ and the number of grid points $L$ for $\\mathcal{L}_{data}$ are sufficiently large.\n\nTheorem 1. Suppose that $\\sup_{u \\in U} ||u||_{L^{\\infty}(\\Omega_m)} \\leq R$ and $\\sup_{\\eta,\\theta} ||u_{\\eta,\\theta}||_{L^{\\infty}(\\Omega_m)} \\leq R$ for some $R > 0$. Consider an input-output dataset $\\{(u^{(i)}|_{\\Omega_m}, s^{(i)})\\}_{i=1}^N$ generated through the following process. First, sample $N$ functions $\\{s^{(i)}\\}_{i=1}^N$ from the probability measure $\\nu(S)$ and compute the numerical solutions $\\{u^{(i)}\\}_{i=1}^N$. Second, for all $\\{u^{(i)}\\}$, sample the grid points for partial measurements from the uniform distribution on $\\Omega_m$, i.e., the grid points are shared across all functions. If the number of sampled functions $N$ and the number of grid points $L$ satisfy:\n\n\n$N \\geq \\frac{8 \\log(8N_c / \\delta)}{\\log 2}, \\quad L \\geq \\frac{256 R^4 |\\Omega_m|^2}{\\epsilon^2} \\log \\frac{2}{\\delta},$\n\n\nthen,\n\n$\\mathbb{E}_{data} \\leq \\bar{\\mathcal{L}}_{data} + \\epsilon,$\n\nholds with probability at least $1 - \\delta$, where $N_c$ is a constant depending on $\\epsilon$.\n\nA similar property can be derived for $\\mathcal{L}_{physics}$, where arbitrary $u, s$ sampled from $\\mu(U)$ and $\\nu(S)$ satisfy the governing PDE, equation 1, given a sufficiently large number of grid points and samples. As before, we define the continuous version $\\mathbb{E}_{physics}$ of $\\mathcal{L}_{physics}$ as follows:\n\n\n$\\mathbb{E}_{physics} = \\int_{\\Omega} [\\mathcal{N}(u_{\\eta,\\theta}(x), s_{\\zeta,\\theta}(x))]^2 dx d\\mu(U) + \\int_{\\partial \\Omega} [\\mathcal{B}(u_{\\eta,\\theta}(x))]^2 dx d\\mu(U),$\n\n\nwhere $u_{\\eta,\\theta}$ and $s_{\\zeta,\\theta}$ are produced by the input $u|_{\\Omega_m}$. By applying a similar technique as above, we can prove the following theorem, which implies that $\\mathcal{L}_{physics}$ converges to $\\mathbb{E}_{physics}$ as the number of grid points $K, M$ and the number of samples $N$ becomes sufficiently large.\n\nTheorem 2. Consider the same sampling process for $u^{(i)}$ as described in Theorem 1 holds. Suppose that $\\sup_{u \\in U} ||\\mathcal{N}u||_{L^{\\infty}(\\Omega_m)} \\leq R_{\\mathcal{N}}$, $\\sup_{\\eta,\\theta} ||\\mathcal{N}u_{\\eta,\\theta}||_{L^{\\infty}(\\Omega_m)} \\leq R_{\\mathcal{N}}$, and $\\sup_{u \\in U} ||\\mathcal{N}u||_{L^{\\infty}(\\Omega_m)} \\leq R_{\\mathcal{B}}$, $\\sup_{\\eta,\\theta} ||\\mathcal{N}u_{\\eta,\\theta}||_{L^{\\infty}(\\Omega_m)} \\leq R_{\\mathcal{B}}$. Additionally, let $K$ and $M$ denote the number of grid points for the interior $\\Omega$ and the boundary $\\partial \\Omega$, respectively, and shared across all $u^{(i)}$. If the number of sampled functions $N$ and the number of grid points $K, M$ satisfy:\n\n\n$N > \\max \\left( \\frac{8 \\log(16N_{\\mathcal{N}} / \\delta)}{\\log 2}, \\frac{8 \\log(16N_{\\mathcal{B}} / \\delta)}{\\log 2} \\right), \\quad K > \\frac{64 R_{\\mathcal{N}}^2 |\\Omega|}{\\epsilon^2} \\log \\frac{2}{\\delta}, \\quad M \\geq \\frac{64 R_{\\mathcal{B}}^2 |\\partial \\Omega|}{\\epsilon^2} \\log \\frac{2}{\\delta},$\n\n\nthen with probability at least $1 - \\delta$, the loss functions will satisfy\n\n$\\mathcal{L}_{physics} < \\mathbb{E}_{physics} + \\epsilon,$\n\nwhere $N_{\\mathcal{N}}$ and $N_{\\mathcal{B}}$ are constants depending on $\\epsilon$.\n\nBy applying Theorems 1 and 2, we can now derive the following theorem, which states that the output functions $u_{\\eta,\\theta}$ and $s_{\\zeta,\\theta}$ of PI-DIONs can approximate the true $u$ and $s$ with high probability over $\\mu(U)$ and $\\nu(S)$. Notably, previous results from Zhang et al. (2023); Zhang & Liu (2023a) established a similar stability estimate, but they considered only the special case where the supports of $\\mu(U)$ and $\\nu(S)$ consist of a single element within $U$ and $S$, respectively. In contrast, this paper aims to extend the result by considering more general distributions $\\mu(U)$ and $\\nu(S)$. Consequently, the theorem below implies that PI-DIONs can accurately approximate $u$ and $s$ even with the partial measurement of unseen data $u$ in $U$, which represents our main theoretical result for PI-DIONs.\n\nTheorem 3. Suppose the same sampling process holds as in Theorem 2. Additionally, assume that the number of sampled functions $N$ and the number of grid points $L, K, M$ satisfy the conditions in Theorem 2. Then, for any $u \\in U, s \\in S$, and $\\epsilon > 0$, the following inequality holds with probability at least $(1 - 2\\delta)(1 - 2 \\sqrt{\\epsilon - \\mathcal{L}_{physics} + \\mathcal{L}_{data}})$,", "equation": null}, {"title": "3.3 UNIVERSAL APPROXIMATION THEOREM FOR PI-DIONS", "content": "Finally, we demonstrate that PI-DIONs can achieve small values for both $\\mathcal{L}_{data}$ and $\\mathcal{L}_{physics}$. This result builds upon the universal approximation property of DeepONet, which asserts that DeepONet"}, {"title": "4 EXPERIMENTS", "content": "We empirically evaluate the proposed PI-DIONs on three benchmark inverse problems of different types. We present both the unsupervised PI-DIONs and supervised PI-DIONs, where the supervised one is trained with an additional loss function\n\n\n$\\mathcal{L}_s = \\frac{1}{N K} \\sum_{i=1}^{N,K} [s_{\\zeta}^{(i)}(x_k) - s_k^{(i)}]^2,$\n\n\nwhere $s_k^{(i)}$ represents the label, so that the loss function becomes\n\n\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{physics} + \\lambda_2 (\\mathcal{L}_{data} + \\mathcal{L}_s),$\n\n\nfor some $\\lambda_1$ and $\\lambda_2$.\n\nFor the comparative analysis, we choose the supervised DeepONets and FNOs as baselines. The details of the training process, including the architecture and hyperparameters, are presented in Appendix B. Additional experiments, including a sensitivity analysis, an ablation study, and a comparison with PINNs, are provided in Appendix C. The $\\lambda$-values are selected based on insights gained from training PINNs for each example. It is worth noting that across all three experiments, we observe that a larger $\\mathcal{L}_{data}$ results in a smaller test error. This aligns with our intuition that, during the early stages of training, a large $\\mathcal{L}_{data}$ will push $s_{\\zeta,\\theta}$ in the wrong direction, as $u_{\\eta,\\theta}$ differs from the true solution, leading to an inaccurate $\\mathcal{L}_{physics}$. Finally, in Appendix C.3, we provide a detailed description and additional experiments on integrating the variable-input operator network(Prasthofer et al., 2022) into PI-DIONs to address the case of irregular measurements (sensor points)."}, {"title": "4.1 REACTION-DIFFUSION EQUATION: INVERSE SOURCE PROBLEM USING BOUNDARY MEASUREMENT", "content": "We start by considering the reaction-diffusion equation\n\n\n$\\begin{aligned}\n\\partial_t u + \\Delta u &= F(x, t), & (x, t) \\in \\Omega_T := \\Omega \\times [0, T], \\\\\nu(x, t) &= 0, & (x, t) \\in \\partial \\Omega \\times [0, T], \\\\\nu(x, 0) &= u_0(x), & x \\in \\Omega,\n\\end{aligned}$\n\n\nwhere $\\Omega = [0, 1]$ and $T = 1$. As it is hard to measure the internal source in many engineering applications, discovering unknown source function $F(x, t)$ from the temperature distribution $u(x, t)$ is an important inverse problem. The inverse problem amounts to discovering the source function $f(x)$ from the initial data $u(x, 0)$, final data $u(x, T)$, and boundary data $u|_{\\partial\\Omega \\times [0, 1]}$. It is well known that if the source function $F(x, t) = f(x)g(t)$ is separable and $g(t)$ is given in advance, then this problem attains a unique solution. Recently, Zhang et al. (2023) proposed a physics-informed neural network for this problem, but it requires retraining when new data is introduced. In contrast, we demonstrate that our method successfully learns the inverse operator, enabling real-time inference in arbitrary resolution without the need for retraining on the same problem.\n\nWe randomly sampled the initial condition $u_0(x)$ and the unknown source function $f(x)$ from the Gaussian random field. Using the central Finite Difference Method (FDM) on a 30 \u00d7 30 grid, we computed the numerical solution and extracted the partial measurement dataset $\\{u^{(i)}|_{\\partial \\Omega_T}\\}_{i=1}^N$, precisely the boundary data in the space-time domain. Since the measurement data can be treated as a 1-dimensional function, we used an MLP for both the reconstruction and inverse branch networks. As the solution and target functions have different domains, i.e., $u_{\\eta,\\theta} : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ and $s_{\\zeta,\\theta} : \\mathbb{R} \\rightarrow \\mathbb{R}$, we made reconstruction trunk network and inverse trunk network separately. The reconstruction branch, together with the reconstruction trunk network learns a mapping from the partial measurement $u|_{\\partial\\Omega_T}$ to the complete solution profile $u|_{\\Omega_T}$. Simultaneously, the inverse branch, also paired with the inverse trunk network, is trained to approximate the inverse operator $\\mathcal{F}^{-1} : u_{\\partial \\Omega_T} \\leftrightarrow f$.\n\nWe trained a PI-DION with 1000 unsupervised samples, where the data consists solely of partial measurements $u|_{\\partial\\Omega_T}$. We then compared the relative $L_2$ errors against benchmark supervised models, including DeepONets. Furthermore, we trained PI-DIONs with labeled training data $\\{(u^{(i)}|_{\\partial\\Omega_T}, f^{(i)})\\}$ to assess the performance of the unsupervised PI-DION. All models were evaluated on a test dataset of 1000 samples. As shown in Table 1, the unsupervised PI-DION outperforms the supervised models and achieves a test error comparable to that of PI-DION trained with fully supervised samples. It is important to note that FNO is not applicable to this problem, as it requires the input and output functions to be defined on the same domain. In this problem, input function $u|_{\\partial\\Omega_T}$ is defined on the boundary of $\\partial\\Omega_T$, while the source term $f$ is defined within the domain $\\Omega$. This discrepancy between the domains requires training a nontrivial mapping between these two different spaces. We present a test sample of the true solution, partial measurement, predicted solution, true source function and the predicted source function in Figure 2."}, {"title": "4.2 HELMHOLTZ EQUATION: INVERSE SOURCE PROBLEM USING INTERNAL MEASUREMENT", "content": "We consider the following Helmholtz equation:\n\n\n$\\begin{aligned}\n-\\nabla \\cdot (\\sigma(x) \\nabla u(x)) + c(x)u(x) &= f(x), & x \\in \\Omega, \\\\\n\\sigma(x) \\frac{\\partial u}{\\partial \\nu}(x) &= g(x), & x \\in \\partial \\Omega,\n\\end{aligned}$\n\n\nwhere $\\Omega = [0, 1]^2$ and $\\nu$ represents the outward normal direction on $\\partial \\Omega$. The objective of this inverse problem is to reconstruct the source function $f(x)$ for $x \\in \\Omega$ using internal measurement $u|_{\\Omega_m}$, where $\\Omega_m = [0.2, 0.8]^2$. For simplicity, we set $\\sigma = c = 1$ and $g = 0$. This problem has been extensively studied from a Physics-Informed Neural Networks (PINNs) perspective in Zhang & Liu (2023b;a). Here, we extend the problem into the operator learning framework and demonstrate that PI-DION effectively solves the inverse problem by approximating the inverse operator without labeled data."}, {"title": "4.3 DARCY FLOW: UNKNOWN PERMEABILITY", "content": "Next, we consider the 2D steady state Darcy flow equation:\n\n\n$\\begin{aligned}\n-\\nabla \\cdot (\\sigma(x) \\nabla u(x)) &= f(x), & x \\in \\Omega, \\\\\nu(x) &= 0, & x \\in \\partial \\Omega.\n\\end{aligned}$\n\n\nIn this example, $u(x)$ represents the pressure field in a porous medium, defined by a positive permeability field $\\sigma(x)$. The inverse problem involves determining the unknown permeability field $\\sigma(x)$ from the full measurement $u(x)$. We randomly sampled the permeability from a Gaussian random field followed by a min-max scaling, and computed the numerical solution $u$, using the fixed source term $f(x, y) = 100x(1 - x)y(1 - y)$ and FDM on a 30 \u00d7 30 grid.\n\nIn this setup, the reconstruction branch, paired with the trunk network, simply learns the identity mapping from the measurement to the solution. In contrast, the inverse branch, along with the trunk network, learns the inverse operator $\\mathcal{F}^{-1} : u \\rightarrow \\sigma$. Both branch networks are modeled using convolutional neural networks (CNN), while the trunk network is modeled using an MLP. Given that the permeability field is always positive, we impose a hard constraint on the inverse operator to"}, {"title": "5 DISCUSSION", "content": "We explored inverse problems that can be framed through the identification of an inverse operator $\\mathcal{F}^{-1}$ between suitable function spaces. In this context, we proposed a novel architecture called PI-DIONs, which eliminates the reliance on costly labeled data typically required for training machine learning models. By adopting a physics-informed approach, PI-DIONs directly parameterize the functions of interest\u2014specifically, the solution $u_{\\eta,\\theta}$ and the target function $s_{\\zeta,\\theta}$\u2014thereby integrating the underlying physical principles into the model.\n\nAdditionally, we provided a robust theoretical foundation for our proposed method by extending stability estimates for inverse problems to the operator learning framework. Our theoretical results demonstrate that PI-DIONs can accurately predict both the solution $u$ and the unknown quantity $s$ based on partial measurement data $u|_{\\Omega_m}$. This ability to work with incomplete data highlights the practicality and effectiveness of our approach. Additionally, we believe that empirically verifying the theoretical bounds on $N, K, M$, and $L$ will be an interesting direction for future work.\n\nTo substantiate our claims, we conducted a series of numerical studies that showcased the superior performance of unsupervised PI-DIONs. Remarkably, these models achieved test errors comparable to those of traditional supervised baselines, thereby indicating that unsupervised training can be as effective as its supervised counterpart in certain contexts.\n\nWhile the inference time of PI-DION is significantly faster than that of a single PINN, it is important to note that the unsupervised training of PI-DION entails substantial computational costs (see 5). Therefore, accelerating the convergence of PI-DION presents a promising avenue for future research, potentially enabling more efficient training processes and broader applicability to complex inverse problems. Furthermore, the integration of PI-DIONs with recent advancements in"}, {"title": "6 REPRODUCIBILITY STATEMENT", "content": "We provide detailed experimental setups for each PDE in Appendix B. Additionally, the proofs are included in Appendix D, and the source code is submitted as supplementary material."}, {"title": "A PI-DIONS-V0", "content": "In this section, we introduce an initial version of PI-DIONs, termed PI-DION-v0, which was considered during the early stages of development. PI-DION-v0 consists of the inverse branch network, the forward branch network, and the trunk network, similar to the final PI-DIONs architecture. However, PI-DION-v0 first produces the target function solution $s_{\\zeta,\\theta}(x)$ by combining the outputs of the inverse branch and the trunk network, then uses this prediction as input to the forward branch network to produce the final target function $u_{\\eta,\\zeta,\\theta}(x)$. The key difference lies in the input to the forward(reconstruction) branch: while PI-DIONs use the partial measurement, PI-DION-v0 uses the predicted target function. The overall architecture is illustrated in Figure 5.\n\nPI-DION-v0 also achieved promising relative test errors for the inverse source problem in the reaction-diffusion equation. However, PI-DION consistently outperformed PI-DION-v0 in both supervised and unsupervised settings (see Table 2). Moreover, PI-DION proved to be computationally more efficient than PI-DION-v0. Despite this, we believe it is worth discussing the architecture of PI-DION-v0 in this paper, as it represents a more natural approach to solving the inverse problem. The first part, consisting of the inverse branch and trunk network, can be viewed as an inverse operator $\\mathcal{F}^{-1}$, while the second part, the forward branch paired with the trunk network, acts as a forward operator $\\mathcal{F}$. Thus, PI-DION-v0 can be interpreted as learning an identity operator $\\mathcal{I} = \\mathcal{F}^{-1} \\circ \\mathcal{F}$, mapping partial measurements to the solution. We believe there are situations where PI-DION-v0 outperforms, primarily due to its more natural architecture."}, {"title": "B ARCHITECTURE AND TRAINING DETAILS", "content": "All experiments were conducted on a single RTX 3090 GPU, with the batch size determined based on available memory. For the three experiments, we used either 1,000 or 500 samples per batch. We used Adam optimizer for training, with a learning rate 1e-3. We trained PI-DIONs and the baseline models for a sufficient number of epochs until convergence was achieved, as is typical with physics-informed training, which tends to have a slow yet reliable convergence process. We summarize the number of trainable parameters in Table 3."}, {"title": "B.1 REACTION-DIFFUSION EQUATION: INVERSE SOURCE PROBLEM USING BOUNDARY MEASUREMENT.", "content": "Both the reconstruction and inverse branch networks were simple Multi-Layer Perceptrons (MLP) each consisting of layers with 60 (input), 32, 32, and 32 (output) A ReLU activation was applied to each layer. Since the solution and target functions have different domains, i.e., $u_{\\eta,\\theta} : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ and $s_{\\zeta,\\theta} : \\mathbb{R} \\rightarrow \\mathbb{R}$, we designed separate trunk networks for reconstruction and inversion. Both trunk networks were also MLPs, with the reconstruction trunk consisting of layers with 2 (input), 32, 32, and 32 (output) nodes, and the inverse trunk consisting of layers with 1 (input), 32, 32, and 32 (output) nodes where a Tanh activation function was applied. The baseline DeepONet consists of the same MLP branch and MLP trunk networks and we did not consider the baseline FNO for this example.\n\nThe numerical solutions were computed on a 30 \u00d7 30 grid, and the same grid was used as input to the trunk networks for training PI-DIONs. The model was trained in a full-batch setting, where the input to the branch networks has a size of (1000, 60), the input to the reconstruction trunk is (900, 2), and the input to the inverse trunk is (30, 1)."}, {"title": "B.2 HELMHOLTZ EQUATION: INVERSE SOURCE PROBLEM USING INTERNAL MEASUREMENT", "content": "For the Helmholtz equation, we employed a convolutional neural network (CNN) composed of three Conv2d layers with 3 \u00d7 3 filters, each followed by the GeLU activation and max-pooling layers with a 3 \u00d7 3 kernel. Additionally, an MLP with 128 output nodes and GeLU activation was applied for both the reconstruction and inverse branch networks. The trunk network was an MLP with a structure consisting of 2 (input), 128, 128, and 128 (output) nodes where a Tanh activation was applied. The baseline DeepONet consists of the same CNN branch and MLP trunk networks and we adopted the implementation of FNO from Li et al. (2020a).\n\nThe numerical solutions were computed on a 50 \u00d7 50 grid, and the internal 40 \u00d7 40 grid was used as input for the branch networks. PI-DION was trained with a mini-batch size of 500, meaning the input to the branch networks had a size of (500, 1600), and the input to the trunk network had a size of (2500, 2)."}, {"title": "B.3 DARCY FLOW: UNKNOWN PERMEABILITY", "content": "For the Darcy flow, we employed a CNN and MLP with the same architecture as used for the Helmholtz equation. The trunk network is an MLP consisting of layers with 2 (input), 128, 128, and 128 (output) nodes. To ensure the predicted permeability lies within the range [0, 1], we applied a sigmoid activation function to the prediction $s_{\\zeta,\\theta}$. The baseline DeepONet consists of the same CNN branch and MLP trunk networks and we adopted the implementation of FNO from Li et al. (2020a).\n\nThe numerical solutions were computed on a 100 \u00d7 100 grid and then downsampled to a 30 \u00d7 30 scale. PI-DION was trained with a mini-batch of size 500, meaning the input to the branch networks had a size of (500, 900) and the input to the trunk network had a size of (900, 2)."}, {"title": "C ADDITIONAL EXPERIMENTS", "content": "In this section, we present additional experiments to further demonstrate the robustness and effectiveness of PI-DIONS."}, {"title": "C.1 COMPARISON TO PHYSICS-INFORMED NEURAL NETWORKS(PINNS)", "content": "We select the weights $\\lambda_1$ and $\\lambda_2$ for PI-DIONs based on insights gained from training PINNs. The inverse PINNs discussed in this section are unsupervised, meaning no supervision is provided for the target function $s$. For each problem, we utilize two neural networks, each comprising three hidden layers with 64 neurons per layer, to approximate the solution $u$ and the target function $s$. The loss functions are defined similarly to those in PI-DIONS:\n\n\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{physics} + \\lambda_2 \\mathcal{L}_{data}.$\n\n\nAll experiments were conducted on a single RTX 3090 GPU. We used Adam optimizer for training, with a learning rate 1e-3."}, {"title": "C.2 SENSITIVITY ANALYSIS", "content": "The loss function comprising $\\mathcal{L}_{physics}$ and $\\mathcal{L}_{data}$ is widely used in the Physics-Informed Machine Learning(PIML) literature. Additionally, numerous studies have introduced loss-balancing algorithms for these two components (e.g., (Wang et al., 2021a; Son et al., 2023)) by multiplying weights $\\lambda_1$ and $\\lambda_2$ to construct an objective function:\n\n\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{physics} + \\lambda_2 \\mathcal{L}_{data}.$\n\n\nHere, we investigate how the performance of the proposed PI-DIONs can be further improved by incorporating such techniques. Specifically, we conduct a sensitivity analysis on the $\\lambda$-values. For this analysis, we train seven PI-DION models with the following weight combinations:\n\n$(\\lambda_1, \\lambda_2) = (100, 1), (10, 1), (1, 1), (1, 0.1), (0.1, 1), (1, 10), (1, 100).$\n\nThe experiments are conducted to solve the inverse source problem for the reaction-diffusion equation described in Section 4.1."}, {"title": "C.3 VARIABLE-INPUT PI-DIONS", "content": "Vanilla DeepONet requires that the sensor points (locations where the input function is sampled) remain consistent across all samples. However, in the context of inverse problems, this constraint can be problematic, as sensor points may vary from sample to sample. To address this limitation, Prasthofer et al. (2022) introduced variable-input deep operator networks, which allow for flexibility in sensor point locations. This variable-input architecture can be seamlessly incorporated into our PI-DION framework. To evaluate this approach, we conducted experiments on a reaction-diffusion equation. We generated a dataset $\\{(u^{(i)}, s^{(i)})\\}_{i=1}^{1000}$ using a fine computational grid $\\{(\\tilde{t_k}, \\tilde{x_l})\\}_{k=1,l=1}^{100,101}$, considering the CFL condition (De Moura & Kubrusly (2013)). For each sample $(u^{(i)}|_{\\Omega_m}, s^{(i)})$, we randomly selected 30 collocation points in the spatial domain, resulting in irregularly sampled data. Consequently, the set of collocation points varies across samples.\n\nAlthough Prasthofer et al. (2022) originally proposed an attention-based mechanism for handling variable input, we opted for a simplified architecture. The overall structure is depicted in Figure 6. Both the sensor embedding and position embedding are implemented using simple multilayer perceptrons (MLPs). The final embedding is obtained by computing the inner product of their outputs. For the weights, we used $(\\lambda_1, \\lambda_"}]}