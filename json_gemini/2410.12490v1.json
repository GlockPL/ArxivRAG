{"title": "Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective", "authors": ["Yongxin Zhu", "Bocheng Li", "Hang Zhang", "Xin Li", "Linli Xu", "Lidong Bing"], "abstract": "Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models.", "sections": [{"title": "1 Introduction", "content": "In recent years, remarkable advancements have been achieved in the field of image generation, principally propelled by the development of latent-based generative models, such as Latent Diffusion Models (LDMs) [34, 30] and Mask Image Models (MIMs) [7, 26]. By employing reconstructive autoencoders such as VQGAN [15] or VAE [23] to compress images into a manageable low dimensional latent space, these models can generate highly realistic and imaginative samples. In light of the transformative impact of autoregressive (AR) generative models, such as Large Language Models [31, 32, 5, 27] in NLP, it becomes compelling to investigate the feasibility of similar paradigms to images. Despite the advances in image autoregressive pre-training, exemplified by models such"}, {"title": "2 Problem Formulation", "content": "In section 2.1, we formalize the latent space requirements for generative models and categorize current latent-based generative models. Furthermore, in section 2.2, we analyze the stability of different induced latent spaces and propose to stabilize the latent space for autoregressive generative models instead of stabilizing the generation process with an iterative decoding strategy like LDMs."}, {"title": "2.1 Latent Space for Generative Models", "content": "Drawing inspiration from the complexity perspective of latent space induced from autoencoders in Hu et al. [21], we delve into the latent space for generative models. Generative models aim at learning a distribution to approximate the data distribution Px. Given an tractable prior distribution Pz and a distance metric D(\u00b7,\u00b7) between distributions, the purpose of a generative model g \u2208 G is to minimize the distance between data distribution Px and the distribution generated by g(Z):\n\nmin D(Pg(z), Px).\n\nFor example, GANs [16] employ the Gaussian distribution as their prior and utilize a discriminator network as the distance metric. However, the optimal strategy for representing data in generative models is still under-explored. Recent studies on latent diffusion models [34] have identified that direct learning in the pixel space of images is suboptimal. They propose to learn in a latent space induced by a constrained autoencoder model such as VAE [23] or VQGAN [15], which has been demonstrated to improve the perceptual quality.\nA simple method to construct the latent space is using an encoder f \u2208 F : Rd \u2192 Rdz to map raw data samples x \u2208 Rd into a latent space f(X) of dimension d\u2082. Consequently, the goal of latent-based generative models is to learn the distribution as per the following formula:\n\nmin D(Pg(z), Pf(x)),\n\nwhere Pf(x) denotes the data distribution in the latent space induced by the encoder f. However, identifying the optimal latent space configuration for generative models remains unresolved."}, {"title": "Distance between distributions in different spaces.", "content": "Given the ultimate goal of the generative model is to produce image pixels, a decoder model h\u2208H : Rdz \u2192 Rd, paired with the encoder model f, is necessary to convert latent representations back into pixels. We define a generalized distance between different spaces associated with a decoder h \u2208 Has:\n\nDH(Pf(x), Px) := inf D(Ph(f(x)), Px).\n\nBy employing DH to compare distributions across different spaces, we can define the ideal latent space f(X) as the one that minimizes DH(Pf(x), Px). This implies selecting a latent representation that minimizes empirical objective L(h|Pf(x)) with the same family H of decoder models. Such a latent configuration depends on both the data and the decoder training methodology. We can formalize it with an autoencoder framework by looking at the encoder and decoder together, and the primary goal becomes the reconstruction of the input sample x, formulated as minf,\u0127 L(h(f(x)), x). Once a generative model g successfully approximates the latent distribution Pf(x), the generated sample can be efficiently transformed back into pixels using the decoder h.\nSimilarly, we can define the distance between distributions in the latent space f(X) \u2208 Rdz and data space X \u2208 Rd conditioned on the latent-based generative model g\u2208 G,\n\nDG (Pf(x), Px) := inf D(Pg(z), Pf(x)),\n\nwhich aims to minimize the empirical objective with the generative model family G."}, {"title": "Optimal Latent Space for Generative Models.", "content": "Now that we have characterized the ideal latent distribution given the family of generative models and the data, the next step is to determine how to find the optimal latent space. At the population level, the objective for the latent-based generative models with a decoder is:\n\nmin D(Ph(g(Z)), Px) = min D(Ph(f(x)), Px) + D(Pg(z), Pf(x)),\n\nwhere the first term focuses on optimizing the decoder to enhance the reconstruction quality and the second one is directed towards optimizing the generative model to more accurately approximate the latent space distribution. Inspired by this observation, we can characterize the optimal latent distribu-tion P(x) for a given Px from the perspective of minimizing the distance between distributions in different spaces by defining f* as\n\nargmin Dlatent (Pf(x), Px) := f*(X) = argmin D\u00ba (Pf(x), Px) + DH(Pf(x), Px)\n\nNotice that Pf*(x) depends on multiple factors, including Px, the distance metric D, and the constructed model families G and H. By integrating D\u00ba and DH, we arrive at a comprehensive measure of the distance between the distribution in the latent space and the original data distribution as Dlatent (Pf(x), Px). The second term, DH(Pf(x), Px), is exactly the objective of reconstructive autoencoders. Ultimately, from examining the learning objective pertinent to identifying the optimal latent space for generative models, it becomes evident that:\nA reconstructive autoencoder does not necessarily establish an advantageous latent space for generative models."}, {"title": "Two Pathways of the Latent Space Construction.", "content": "Although we theoretically analyze the optimization of the optimal latent space for generative models, it is challenging to implement in practice because optimizing (f, g, h) simultaneously is computationally complex. A practical solution is to optimize D\u00ba and DH separately, allowing for tractable training.\n\nWhen DH (Pf(x), Px) is not a target for optimization, it implies that the optimization of the decoder within the generative model framework is bypassed. The encoder independently forms a latent space, aligning with self-supervised learning (SSL) strategies aimed at un-covering lower-dimensional features from unlabeled data. However, learning the generative models in the latent space induced by SSL models remains relatively unexplored.\nOn the other hand, when D\u00ba(Pf(x), Px) remains fixed, the primary objective becomes optimizing the encoder and decoder to effectively learn and represent the latent space,"}, {"title": "2.2 Stability of the Latent space", "content": "We first describe the decoding mechanism of various latent generative models. Both LDM [34] and MaskGIT [7] can be depicted as an iterative sampling procedure given by:\n\np(x) = \\prod_{i=1}^{T}p(x^{i}|x^{i-1}).\n\nThe intermediate states x\u00b2 in LDMs represent images infused with Gaussian noise of varying variance, whereas for MaskGIT, they denote discretely tokenized images augmented with masks. In contrast, the autoregressive framework of VQGAN (AR) is described as:\n\np(x) = p(x_{1},...,x_{n}) = \\prod_{i=1}^{N}p(x_{i}|x_{<i}),\n\nwhere x is the i-th patch in the image sequence. Notice that x\u00b2 denotes the entire image while xi means the local patch tokens. In the autoregressive decoding process, if the previously sampled tokens are incorrect, the accuracy of subsequent tokens would be affected due to error aggregation. In contrast, the iterative decoding approach allows for the revision of earlier misjudged tokens. When the latent space is unstable that small perturbation in pixels can change the latent distribution significantly, the iterative decoding mechanism employed by LDM and MaskGIT can alleviate the error aggregation problem by allowing for the revision of earlier misjudged tokens while autoregressive models cannot. Consequently, a stable latent space is required to reduce the error introduced in the generation process of autoregressive models.\nThis principle forms the foundation of our methodology for developing a metric to evaluate latent spaces with an emphasis on the stability of the latent representations. We examine two primary types of latent spaces: (1) autoencoder induced by minimizing DH(Pf(x), Px) and (2) self-supervised learning (SSL) model induced by minimizing D\u00ba (Pf(x), Px). By analyzing the network parameters of these models in a linear regime, we derive the following propositions."}, {"title": "3 Stabilize the Latent Space with Self-supervised Learning Model", "content": "In this section, we present a simple but effective image tokenizer that discretizes the feature rep-resentations of discriminative SSL models to form discrete tokens for autoregressive models. The architecture of our model is illustrated in Figure 2.\nDiscrete Image Discriminative Tokenizer Drawing inspiration from the VQGAN tokenizer [15], which employs an implicit K-Means clustering algorithm within the latent space to generate discrete tokens for autoregressive modeling, we propose a straightforward approach to perform K-Means clustering on the feature space of discriminative SSL models to obtain discrete tokens. To process a given dataset, our initial step involves gathering the features of image patches, akin to the hidden states produced by SSL models. Then we employ a clustering algorithm to group these patches, resulting in a collection of K clustering centers. These centers constitute the codebook for the discrete tokenizer. To determine the discrete tokens for an image patch at inference, we identify its nearest neighbor in the codebook, which is then designated as the discrete token for the respective patch.\nImage Autoregressive Modeling After converting images into discrete tokens with the discrimina-tive tokenizer, we treat each image as a sequence by flattening the discrete tokens from images into a 1D sequence in raster order. We train a causal Transformer [37] model with the next token prediction objective, which is the same as the standard approach for language models."}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nWe take the discriminative SSL model DINOv2 [28] as the encoder for all experiments. The K-Means model is trained on the randomly selected 10% subset of ImageNet [11] training set. We use the autoregressive model with the same architecture as GPT-2 [32]. We train the DiGIT models with the base and large sizes. The vocabulary size of the tokenizer for the base is 8192 and 16000 for the large size. More implementation details and hyper-parameters are provided in Appendix A.3.\n4.2 Image Understanding\nThe GPT model is famous for learning the semantic feature by a generative training objective of next token prediction. We compare the image understanding ability of different image autoregressive models with linear probe as described in iGPT [8]. We train a linear classifier on top of the frozen features average from each layer on the ImageNet training set. We report the Top-1 accuracy compared with other image autoregressive models in Table 2. Remarkably, with only 219M parameters, DiGIT achieves a Top-1 accuracy of 71.7%, surpassing both iGPT and VIM-Base, which have a greater number of parameters and operate at more visual tokens. Despite representing images with a smaller token grid size (16 \u00d7 16 as opposed to 32 \u00d7 32), DiGIT still delivers superior top-1 accuracy, demonstrating the effectiveness of our tokenizer. Moreover, when we scale DiGIT's parameters from 219M to 732M, the Top-1 accuracy shows an additional increase of 8.6% and reaches 80% for the first time. The improvement indicates that DiGIT with the proposed discriminative tokenizer has the potential for the development of large vision models.\n4.3 Image Generation\nSince the SSL models do not have a paired decoder to recover pixels from latent space, the generative models trained with our discriminative tokenizer require an auxiliary image decoder to render pixels. The discriminative tokenizer can be seamlessly integrated with any existing image generative models trained with a tokenizer induced from a reconstructive autoencoder. In our experiment, we train an autoregressive model VQGAN, and a MIM model MaskGIT as the pixel decoder respectively. The results are presented in Table 3 and Table 4. The autoregressive model equipped with our discriminative tokenizer achieves the SOTA performance with FID reaching 3 for the first time. Furthermore, the performance significantly improves as the model size increases, demonstrating the potential of a large vision model with next token prediction. Interestingly, when utilizing the DiGIT as the conditioning factor, the performance of both the autoregressive and MaskGIT decoders becomes close (4.62 and 4.79). This observation suggests that stabilizing the latent space produces effects analogous to the iterative stabilization decoding mechanism.\n4.4 Ablation Study\nWe conduct the ablation study to present a comprehensive analysis of the proposed discriminative tokenizer in image generation and understanding. The results are illustrated in Table 3(a) and"}, {"title": "4.5 Comparison of Discrete Tokenizers", "content": "We conduct an experiment to investigate the effect of different kinds of SSL models on latent space. We generally categorize the SSL models into two types according to their pre-training objectives: (1) Global level (MoCo) and patch level (MAE,iBOT), (2) reconstructive (MAE) and discriminative (\u041c\u043e\u0421\u043e, \u0456\u0412\u041e\u0422). At the global level, the loss function is computed using an aggregate output such as [CLS] token or mean pooling. In contrast, patch-level models involve patches directly in loss computation. Reconstructive models, such as MAE, aim to recover image pixels in a manner akin to autoencoders, while discriminative models are optimized to learn the distinguishable features. As demonstrated in Table 4(a), the discriminative objective plays a pivotal role in image generation in that it can stabilize the latent space. Furthermore, because generative models need to predict patches, the inclusion of a patch-level loss function can enhance performance.\nTo assess the stability of latent space induced by our discriminative tokenizer and reconstructive tok-enizer. We pre-train two auto-regressive generative models on the ImageNet dataset [11], employing the proposed discriminative tokenizer and VQGAN tokenizer respectively. We provide each model with the upper half of the target image as a conditional prompt for generation, challenging them to complete the lower half of the image. A stable latent space should be able to help the autoregressive model generate the lower half more robustly, maintaining thematic and aesthetic coherence. As shown in Figure 4(b), the FID decreases for both models when given a longer prefix context. However, when the prefix length is reduced from 75% to only 12.5% of the image, the model trained with the VQGAN tokenizer encounters difficulties in producing images that adhere to the specified prompt. In contrast, the model utilizing the discriminative tokenizer effectively continues to produce congruent visual tokens, maintaining low FID scores even with a significantly truncated prefix."}, {"title": "5 Related Work", "content": "Image Tokenizer The image tokenizer [36, 33, 15] is essential in converting pixels into discrete tokens for autoregressive generative modeling. VQVAE [36] first proposes to assign latent features learned by an encoder to the nearest entry in the learnable codebook embeddings, followed by a decoder to reconstruct the original image pixels. VQGAN [15] further incorporates adversarial loss and perceptual loss to improve the image synthesis quality. RQ-Transformer [24] extends the single-layer quantizer to the multi-layer residual quantizer to augment the visual tokenizer's ability to capture fine-grained details. ViT-VQGAN [40] incorporates the modern Vision-Transformer [13] into VQGAN to enhance the reconstruction quality. MAGVIT-v2 [41] substitute the online update codebook in VQGAN with a lookup-free quantizer to enable a larger vocabulary for generative language models.\nImage Autoregressive Modeling Inspired by the success of autoregressive Transformer [37] in text generation tasks, there have been several efforts to replicate it in image generation tasks. One of the pioneering works is iGPT [8], which pre-trains an autoregressive model on pixels with the same architecture as GPT2 [32], achieving promising results in unsupervised visual representation learning. LVM [1] proposes a large-scale vision dataset composed of images and videos, based on which a large vision model is trained. Empirical observations indicate that the model scales effectively across various tasks with in-context learning [39]. Similarly, AIM [14] follows ViT [13] and represents images in the patch. It is observed that the performance of image recognition continues to increase as the model size scales up. VAR [35] proposes a next-scale prediction to generate images from coarse to fine in a hybrid of autoregressive and nonautoregressive manner.\nSelf-supervised Learning Models Self-supervised learning (SSL) [9, 17] plays an important role in learning fundamental visual representations for downstream tasks. Among them, SimCLR [9], MoCo [9, 18, 10] compute losses at the image level through [CLS] token aggregation or pooling operations with contrastive learning. iBOT-style models [42, 6, 28] extend the loss to the patch level, achieving improved performance in dense prediction tasks. BEiT [3] uses VQGAN tokenized sequences as the training target. MAE [19] randomly masks some patches of images and reconstructs the pixels with unmasked patches as the condition."}, {"title": "6 Conclusion", "content": "In this paper, we make an exploration in the latent space for generative modeling. We introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Subsequently, we propose a simple but effective discriminative image tokenizer, followed by an image autoregressive generative model DIGIT. Empirical results indicate that our tokenizer achieves superior performance across both image understanding and image generation tasks. Notably, when DiGIT is scaled up in model size, it exhibits even greater enhancements, indicating the potential for the development of large vision models. Our findings challenge the conventional wisdom that proficiency in reconstruction equates to an effective latent space for auto-regressive generation. Through this work, we aim to rekindle interest in the generative pre-training of image auto-regressive models and encourage a reevaluation of the fundamental components that define latent space for generative models."}, {"title": "A Appendix", "content": "A.1 Limitation and Broader Impact\nOur proposed discriminative tokenizer exhibits remarkable capabilities in both the image under-standing and the image generation tasks, which is challenging for a single model. However, the discriminative tokenizer induced by the SSL model cannot directly render pixels. Consequently, we need to train another decoder model to convert tokens from discriminative tokenizers to VQ-GAN tokens. The potential for a direct generation of RGB pixels from the tokens produced by the discriminative tokenizer remains an uncharted avenue, which we leave for future work.\nImage generative models possess a dichotomous nature, particularly within the realm of visual media. On the positive side, they foster a myriad of creative endeavors, and methodologies aim to minimize the costs of training and inference, holding the potential to broaden access and democratize the use of this technology. On the negative side, the simplicity with which manipulated content can be crafted and propagated raises serious concerns. This includes the proliferation of misinformation and spam. Furthermore, generative models may inadvertently unveil aspects of their training data, a particularly troubling issue when that data includes sensitive or personal information collected without explicit consent.\nA.2 Proof of claims in Section 2"}, {"title": "Proposition A.1.", "content": "Consider the optimization problem for X \u2208 Rn:\n\nmin ||X \u2013 W2W1X||}},\nW1\u2208Rmxn, W2\u2208Rnxm\n\nwhich is a linear autoencoder. W2 is a minimizer of the problem if and only if its column space is spanned by the first m loading vectors of X.\nProof. First, we derive the condition for the optimal W\u2081 in the context of this optimization problem. Setting the gradient of the objective function with respect to W\u2081 to zero leads to W\u2081 being the left Moore-Penrose pseudoinverse of W2 [2]. Similarly, setting the gradient with respect to W2 to zero would identify W2 as the right pseudoinverse of W1:\n\nW\u2081 = W = (WW2)\u00af\u00b9W.\n\nThis finding indicates that the optimization can be simplified to focus on a single matrix (either W\u2081 or W2), hence removing the redundancy in the parameters:\n\nmin ||X \u2013 W2WX||2\nW2\u2208Rnxm\n\nThe term W2W = W2(WW2)\u00af\u00b9W is recognized as the matrix form of the orthogonal projection operator onto the column space of W2. This property holds true even when the column vectors of W2 are not orthonormal.\nBy performing QR decomposition on W2, W2 = QR where Q is an orthogonal matrix (QTQ = I) and R is an upper triangular matrix, we effectively transform the problem into optimizing over orthogonal matrices. The objective can thus be restated as:\n\nmin ||X-WTWX||, subject to WWT = Inxn.\nW\u2208Rmxn\n\nThis revelation explicitly demonstrates that minimizing the reconstruction error in the space Rmxn demands that W (equivalent to W2 in our context) projects X onto a space spanned by its most significant structural components (in terms of variance), which are precisely the first m loading vectors, or principal components, of X."}, {"title": "Proposition A.2.", "content": "The discriminative self-supervised model learns to separate data distributions in latent space as LDA in principle.\nProof. We first consider the objective of Fisher LDA for two-class classification. LDA seeks to find a linear projection that maximizes the separation between the two classes:"}, {"title": "Proposition A.1.", "content": "Consider the optimization problem for X \u2208 Rn:\n\nmin ||X \u2013 W2W1X||}},\nW1\u2208Rmxn, W2\u2208Rnxm\n\nwhich is a linear autoencoder. W2 is a minimizer of the problem if and only if its column space is spanned by the first m loading vectors of X.\nProof. First, we derive the condition for the optimal W\u2081 in the context of this optimization problem. Setting the gradient of the objective function with respect to W\u2081 to zero leads to W\u2081 being the left Moore-Penrose pseudoinverse of W2 [2]. Similarly, setting the gradient with respect to W2 to zero would identify W2 as the right pseudoinverse of W1:\n\nW\u2081 = W = (WW2)\u00af\u00b9W.\n\nThis finding indicates that the optimization can be simplified to focus on a single matrix (either W\u2081 or W2), hence removing the redundancy in the parameters:\n\nmin ||X \u2013 W2WX||2\nW2\u2208Rnxm\n\nThe term W2W = W2(WW2)\u00af\u00b9W is recognized as the matrix form of the orthogonal projection operator onto the column space of W2. This property holds true even when the column vectors of W2 are not orthonormal.\nBy performing QR decomposition on W2, W2 = QR where Q is an orthogonal matrix (QTQ = I) and R is an upper triangular matrix, we effectively transform the problem into optimizing over orthogonal matrices. The objective can thus be restated as:\n\nmin ||X-WTWX||, subject to WWT = Inxn.\nW\u2208Rmxn\n\nThis revelation explicitly demonstrates that minimizing the reconstruction error in the space Rmxn demands that W (equivalent to W2 in our context) projects X onto a space spanned by its most significant structural components (in terms of variance), which are precisely the first m loading vectors, or principal components, of X."}, {"title": "Proposition A.2.", "content": "The discriminative self-supervised model learns to separate data distributions in latent space as LDA in principle.\nProof. We first consider the objective of Fisher LDA for two-class classification. LDA seeks to find a linear projection that maximizes the separation between the two classes:"}]}