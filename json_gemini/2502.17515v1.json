{"title": "Towards User-level Private Reinforcement Learning with Human Feedback", "authors": ["Jiaming Zhang", "Mingxi Lei", "Meng Ding", "Mengdi Li", "Zihang Xiang", "Difei Xu", "Jinhui Xu", "Di Wang"], "abstract": "Reinforcement Learning with Human Feedback (RLHF) has emerged as an influential technique, enabling the alignment of large language models (LLMs) with human preferences. Despite the promising potential of RLHF, how to protect user preference privacy has become a crucial issue. Most previous work has focused on using differential privacy (DP) to protect the privacy of individual data. However, they have concentrated primarily on item-level privacy protection and have unsatisfactory performance for user-level privacy, which is more common in RLHF. This study proposes a novel framework, AUP-RLHF, which integrates user-level label DP into RLHF. We first show that the classical random response algorithm, which achieves an acceptable performance in item-level privacy, leads to suboptimal utility when in the user-level settings. We then establish a lower bound for the user-level label DP-RLHF and develop the AUP-RLHF algorithm, which guarantees $(\\epsilon, \\delta)$ user-level privacy and achieves an improved estimation error. Experimental results show that AUP-RLHF outperforms existing baseline methods in sentiment generation and summarization tasks, achieving a better privacy-utility trade-off.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has significantly transformed the field of artificial intelligence, leading to widespread adoption and application in diverse domains (Hong et al., 2024; Yang et al.; Cheng et al.; Zhang et al., 2024), such as legal (Chalkidis and Kementchedjhieva, 2023; Wu et al., 2023b), medical (Thirunavukarasu et al., 2023; Garc\u00eda-Ferrero et al., 2024; Yuan et al., 2024), and coding assistant (Ross et al., 2023; Nam et al., 2024; Kazemitabaar et al., 2024). A notable advancement in this area is the incorporation of Reinforcement Learning from Human Feedback (RLHF), a paradigm that improves the performance and alignment of language models by integrating human preferences and feedback into the training process (Ouyang et al., 2022).\nCurrently, there is a substantial body of research on RLHF, with methods broadly categorized into reward model-based and reward model-free approaches. Although SOTA methods in the academic domain typically lean toward reward model-free approaches, such as DPO (Rafailov et al., 2024), systems such as ChatGPT (OpenAI, 2022) and Claude (Antropic, 2023) employ a reward model combined with reinforcement learning algorithms (e.g. PPO). This distinction arises because DPO tends to produce out-of-distribution biased responses, and its performance is susceptible to distribution shifts between model output and preference data set (Xu et al., 2024). In experimental evaluations, PPO consistently outperforms DPO in tasks such as dialogue generation and code generation (Xu et al., 2024).\nAlthough RLHF has shown considerable promise, it also introduces substantial privacy concerns, particularly with regard to the identity and preferences of human labelers. For example, in the financial domain, investment advisors can annotate and provide feedback on LLM-generated investment recommendations. These preference labels may encompass sensitive information such as individual investment strategies, risk preferences, and financial objectives. In addition, LLMs have the potential to infer individual user preferences, interests, or personal attributes by analyzing queries and interactions posed. Such inferences may lead to the generation of targeted advertisements, personalized suggestions, or other customized content, which in turn could inadvertently expose sensitive aspects of the user's private life (Yan et al., 2024; Lyu et al., 2023; Harte et al., 2023; Hu et al., 2024; Huang et al., 2024).\nTo address the aforementioned privacy concerns, Differential Privacy (DP) (Dwork et al., 2006),"}, {"title": "", "content": "which quantifies privacy protection by adding noise to ensure the anonymity of individual data in statistical analysis, has become a common approach, effectively protecting user privacy by introducing noise into the data. Recently, there have been an increasing number of studies focusing on DP-RLHF (Chowdhury et al., 2024; Chua et al., 2024; Wu et al., 2023a; Teku et al., 2025), demonstrating the feasibility of DP to RLHF. However, traditional DP algorithms in RLHF focus on item-level privacy, where they ensure that the inclusion or exclusion of any single data point does not significantly affect the model's output. In practical scenarios, a user typically contributes multiple pieces of data, rendering item-level privacy insufficient. Specifically, existing methods offer privacy guarantees that deteriorate as user participation increases or blindly introduce excessive noise, leveraging the group property of differential privacy, which severely impacts the performance of the model in deployment. (Levy et al., 2021). For example, in Section 4, we will show that the Randomized Response mechanism, which is the canonical way of protecting label privacy and achieves the nearly optimal rate in the item-level case (Chowdhury et al., 2024), will significantly degrade its utility in the user-level privacy scenario. Thus, there is a pressing need for developing user-level label DP-RLHF with a better privacy-utility tradeoff.\nTo tackle the challenges outlined above, motivated by the current developments of user-level DP supervised learning (Geyer et al., 2017; Bassily and Sun, 2023a; Levy et al., 2021; Liu and Asi, 2024a; Ghazi et al., 2024; Zhao et al., 2024), we propose a novel framework, namely AUP-RLHF, which satisfies user-level label DP and achieves a smaller estimation error. The key intuition is the use of the average gradient of the loss with respect to each user's preferences for parameter updates in the DP-SGD algorithm (Abadi et al., 2016; Su et al., 2024; Shen et al., 2023; Xiao et al., 2023; Hu et al., 2022). However, unlike the existing DP-SGD designed for item level, we leverage outlier removal and adaptive sampling processes to handle the high sensitivity to ensure that the gradient exhibits concentrated properties. Then, at each step of the SGD update, we add noise related to the concentrated parameters only to ensure privacy, which is significantly small. Our contributions are as follows:\n1. We first demonstrate that under canonical as-"}, {"title": "", "content": "sumptions, the classical randomized response method, which outperforms well in the item-level setting, exhibits a large estimation error $O(\\frac{d}{m})$ in the user-level setting. Specifically, when the contribution $m$ for each user is large, randomized response sacrifices utility to provide uniform privacy protection in the privacy-utility trade-off, rendering the algorithm unsuitable. We also establish a lower bound $\\Omega(\\frac{d}{\\sqrt{n} m} + \\frac{m n e}{\\sqrt{m}})$ for user-level DP-RLHF.\n2. To close the gap between the upper and lower bounds, we develop AUP-RLHF that satisfies $(\\epsilon, \\delta)$-user-level label privacy, with an upper bound of $O(\\frac{d\\sqrt{d}}{\\sqrt{m} n e})$ for the estimation error. Additionally, we extend the algorithm to the $K$-wise case, where the upper bound of the estimation error remains $O(K \\frac{2 d \\sqrt{d}}{\\sqrt{m} n e})$.\n3. We empirically validate AUP-RLHF through experiments on commonly studied tasks. We conducted controlled sentiment generation and summarization experiments. We show that, across base models of varying sizes and different privacy parameter configurations, AUP-RLHF consistently outperforms the other user-level DP baselines."}, {"title": "2 Related Work", "content": "User-level DP learning. User-level DP has garnered increasing attention due to its privacy protection, which aligns more closely with real-world scenarios. Several studies have explored various tasks of user-level DP, such as mean estimation (Levy et al., 2021), empirical risk minimization (Levy et al., 2021), and stochastic convex optimization (Bassily and Sun, 2023b; Liu and Asi, 2024b). Levy et al. 2021 first introduced user-level DP, providing stricter privacy protection by safeguarding the entire contribution of users, based on a novel private mean estimation algorithm. Bassily and Sun 2023b further investigated how carefully selected first-order optimization methods can achieve optimal utility under local DP conditions. Liu and Asi 2024b, on the other hand, ensured query concentration through above-threshold and adaptive sampling techniques, thereby releasing the minimal number of user assumptions while still achieving optimal utility. However, none of them has considered RLHF. In this paper, we build the"}, {"title": "", "content": "first theoretical results on user-level (label) DP-RLHF.\nPrivacy-preserving in RLHF. Zhu et al. 2023 provided a theoretical framework for RLHF under clean data conditions, demonstrating the properties of MLE and pessimistic MLE. However, due to privacy leakage issues (Li et al., 2023), MLE cannot be applied directly. Building on this, Chowdhury et al. 2024 introduced a theoretical framework for incorporating DP into RLHF, designing an unbiased loss function based on random responses to achieve DP. Teku et al. 2025 proposed a novel multi-stage mechanism that privately aligns the model with labels from previous stages combined with random responses. However, as we will show later, random responses cannot be easily extended to more practical user-level settings, which results in poor utility. Meanwhile, Wu et al. 2023a utilized DP-SGD across the three stages of RLHF\u2014Supervised Fine-Tuning, Reward Model Learning, and Alignment with PPO\u2014to ensure privacy preservation. Chua et al. 2024 explored the application of Group Privacy and User-wise DP-SGD to achieve user-level DP in RLHF. However, they provide only experimental results and lack rigorous theoretical guarantees. Our experiments will show that our proposal method has better performance than these methods."}, {"title": "3 Preliminaries", "content": "3.1 RLHF\nLet $\\mathcal{D} = \\{(s_i, a_i^0, a_i^1, y_i)\\}_{i=1}^n$ be a dataset of $n$ samples, where each sample has a state $s_i \\in \\mathcal{S}$ (e.g., prompt given to a language model), two actions $a_i^0, a_i^1 \\in \\mathcal{A}$ (e.g., two responses from the language model), and label $y_i \\in \\{0, 1\\}$ indicating which action is preferred by human experts. We assume that the state $s_i$ is first sampled from some fixed distribution $p$. The pair of actions $(a_i^0, a_i^1)$ are then sampled from some joint distribution (i.e., a behavior policy) $\\mu$ conditioned on $s_i$. Finally, the label $y_i$ is sampled from a Bernoulli distribution conditioned on $(s_i, a_i^0, a_i^1)$, i.e., for $l \\in \\{0, 1\\}$,\n$\\mathbb{P}_{\\theta^*} [Y_i = 1 | s_i, a_i^0, a_i^1] = \\sigma((\\phi(s_i, a_i^1) - \\phi(s_i, a_i^0))^T \\theta^*),$ where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ refers to the sigmoid function.\nHere, $r_{\\theta^*}(\\cdot, \\cdot)$ denotes the reward model parameterized by an unknown parameter $\\theta^*$, which we aim to estimate from the dataset $\\mathcal{D}$. This model is"}, {"title": "", "content": "known as the Bradley-Terry-Luce (BTL) model. In this work, we specifically focus on a linear reward model as follows:\n$r_{\\theta^*}(s, a) = \\phi(s, a)^T \\theta^*,$ where $\\phi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^d$ is some known and fixed feature map. For instance, such a $\\phi$ can be constructed by removing the last layer of a pre-trained language model, and in that case, $\\theta^*$ corresponds to the weights of the last layer. With this model, one can equivalently write the probability of sampling $Y_i = 1$ given $(s_i, a_i^0, a_i^1)$ as\n$\\mathbb{P}_{\\theta^*} [y_i = 1 | s_i, a_i^0, a_i^1] = \\sigma((\\phi(s_i, a_i^1) - \\phi(s_i, a_i^0))^T \\theta^*),$ where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ refers to the sigmoid function. Let $x_i = \\phi(s_i, a_i^1) - \\phi(s_i, a_i^0)$ denote the differential feature of actions $a_i^1$ and $a_i^0$ at state $s_i$. For any $\\theta \\in \\mathbb{R}^d$, we use this differential feature to define the predicted probability of the label $y_i$ given $x_i$ as follows (we omit dependence on $\\theta$ for brevity):\n$\\begin{aligned}\nP_{i,1} := \\mathbb{P}_{\\theta} [Y_i = 1 | x_i] = \\sigma(x_i^T \\theta), \\\\\nP_{i,0} := 1 - P_{i,1}.\n\\end{aligned}$\n3.2 Differential Privacy\nWe use the notation $\\mathcal{Z}_i := \\{(s_{i,j}, a_{i,j}^0, a_{i,j}^1, Y_{i,j})\\}_{j=1}^m$ to represent user $i$'s contributions, and $z_{i,j} := (s_{i,j}, a_{i,j}^0, a_{i,j}^1, Y_{i,j})$ to represent user $i$'s $j$th contributions. We use capital $Z$ to denote one user and $z$ to denote one item. We consider user-level DP, which protects all of a user's contributions, meaning that the output of an algorithm $\\mathcal{M}$ operating on a dataset with $n$ users (thus, $mn$ samples in total) $\\mathcal{D} = (\\mathcal{Z}_i)_{i=1}^n$ is \u2018indistinguishable\u2019 when a single user's contributions in $\\mathcal{D}$ are altered. A formal definition is provided below.\nDefinition 1. (User-Level Differential Privacy). A mechanism $\\mathcal{M}: (\\mathcal{Z}^m)^n \\rightarrow \\mathbb{R}^d$ is $(\\epsilon, \\delta)$ user-level differentially private, if for any neighboring datasets $\\mathcal{D}, \\mathcal{D}' \\in (\\mathcal{Z}^m)^n$ that differ in one user, and for any event $\\mathcal{O}$ in the range of $\\mathcal{M}$, we have\n$\\mathbb{P}r[\\mathcal{M}(\\mathcal{D}) \\in \\mathcal{O}] \\leq e^{\\epsilon} \\mathbb{P}r[\\mathcal{M}(\\mathcal{D}') \\in \\mathcal{O}] + \\delta.$\nThe original definition of DP (Dwork et al., 2006) assumes that the whole dataset is private, which is quite strong for many scenarios in RLHF. As in RLHF, a user's preferences are only associated with the label of the"}, {"title": "", "content": "(prompt, response, label) tuple, while the prompt itself is not sensitive, as it is sampled from pre-collected datasets that are already considered public knowledge (Chowdhury et al., 2024; Teku et al., 2025). Thus, in this paper, we consider the label DP (Ghazi et al., 2021). We assume that $(s, a^0, a^1)$ is publicly accessible, while user preference data $y \\in \\{0, 1\\}$ are private. Therefore, the focus is on protecting the user's preference information $y$. The definition of label DP at the user level is provided below.\nDefinition 2. (User-Level Label Differential Privacy). A mechanism $\\mathcal{M}: (\\mathcal{Z}^m)^n \\rightarrow \\mathbb{R}^d$ is $(\\epsilon, \\delta)$ user-level label differentially private, if for any neighboring datasets $\\mathcal{D}, \\mathcal{D}' \\in (\\mathcal{Z}^m)^n$ that differ in the labels of one user, and for any event $\\mathcal{O}$ in the range of $\\mathcal{M}$, we have\n$\\mathbb{P}r[\\mathcal{M}(\\mathcal{D}) \\in \\mathcal{O}] \\leq e^{\\epsilon} \\mathbb{P}r[\\mathcal{M}(\\mathcal{D}') \\in \\mathcal{O}] + \\delta.$\nNote that item-level label differential privacy is a specific case of this definition with $m = 1$.\n4 Sub-optimality of Random Response\nTo achieve label-level DP, a natural approach is to employ the random response (RR) (Warner, 1965) mechanism to each label, which achieves privacy by randomly flipping labels. In fact, such a simple idea has been shown to achieve satisfactory performance in the item-level DP-RLHF both theoretically and practically (Teku et al., 2025; Chowdhury et al., 2024). However, the theoretical behavior of RR in the user-level setting remains unclear. In the following, we first investigate the theoretical behavior of RR in the user-level setting. We will then show how the upper bound of the estimation error degrades when each user contributes a large amount of data $m$.\n4.1 User-level Random Response\nLet $\\epsilon \\geq 0$, $m$ be the sample numbers of a single user, and $y \\in \\{0, 1\\}$ be the true label. By the group privacy property, to extend the classical RR to the user level, we need to flip each label to make it satisfy $\\epsilon$-DP. In detail, the outputs of the RR mechanism $\\tilde{y}$ follow the following probability distribution\n$\\begin{aligned}\n&\\mathbb{P}[\\tilde{y} = y] = \\frac{e^{\\epsilon/m}}{1 + e^{\\epsilon/m}} = \\sigma(\\epsilon/m), \\\\\n&\\mathbb{P}[\\tilde{y} \\neq y] = 1 - \\sigma(\\epsilon/m).\n\\end{aligned}$"}, {"title": "", "content": "It is easy to show that User-level RR is $\\epsilon$-user-level label DP. Then, we consider the following loss on the perturbed data, which is unbiased to the original loss in (non-private) RLHF. We will present the design of this loss function in Appendix B.2.\n$\\hat{\\mathcal{L}}_{\\mathcal{D},\\epsilon} (\\theta) = - \\sum_{i=1}^n \\sum_{j=1}^m [\\mathbb{I} (Y_{i,j} = 1) \\log \\hat{p}_{i,j} + \\mathbb{I} (Y_{i,j} = 0) \\log \\hat{p}_{i,j}],$ where the predicted scores of each randomized label $Y_{i,j}$ given $x_{i,j}$ are defined as:\n$\\begin{aligned}\n&\\hat{p}_{i,j} = \\frac{\\sigma(x_i^T \\theta) \\sigma(\\epsilon/m)}{\\sigma(x_i^T \\theta) \\sigma(\\epsilon/m) + (1 - \\sigma(x_i^T \\theta)) (1-\\sigma(\\epsilon/m))}, \\\\\n&\\hat{p}_{i,j} = \\frac{(1 - \\sigma(x_i^T \\theta)) \\sigma(\\epsilon/m)}{\\sigma(x_i^T \\theta) \\sigma(\\epsilon/m) + (1 - \\sigma(x_i^T \\theta)) (1-\\sigma(\\epsilon/m))}.\n\\end{aligned}$\nThus, our private model is defined as\n$\\hat{\\theta}_{RR} \\in \\underset{\\theta \\in \\Theta_{RR}}{\\operatorname{argmin}} \\hat{\\mathcal{L}}_{\\mathcal{D},\\epsilon} (\\theta),$ where $\\Theta_{RR}$ satisfies $\\epsilon$-user-level label DP due to RR and the post-processing property of DP. To get the estimation error of $\\hat{\\theta}_{RR}$, we make the following assumption, which is standard in the existing literature (Shah et al., 2016; Shin et al., 2023; Chowdhury et al., 2024).\nAssumption 1 (Boundedness). (a) $\\theta^*$ lies in the set $\\Theta_B = {\\theta \\in \\mathbb{R}^d | (1, \\theta) = 0, ||\\theta|| \\leq B}$ with some constant $B$. Here the condition $(1, \\theta) = 0$ ensures identifiability of $\\theta^*$. (b) Features are bounded, i.e., for all $(s, a)$ we have $|\\phi(s,a)| \\leq L$ for some constant $B$.\nLet $\\Sigma_{\\mathcal{D}} := \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T$ denote the sample covariance matrix of differential features $x_i = \\phi(s_i, a^1) - \\phi(s_i, a^0)$.\nTheorem 1. For any $\\epsilon > 0$, the private model $\\hat{\\theta}_{RR}$ is $\\epsilon$-DP. Moreover, under Assumption 1, for any $\\alpha > 0$, with probability at least $1 - \\alpha$, we have\n$||\\hat{\\theta}_{RR} - \\theta^*|| \\leq O\\left( \\sqrt{\\frac{\\frac{1}{nm} \\cdot \\frac{e^{\\epsilon/m} + 1}{\\gamma \\kappa \\frac{e^{\\epsilon/m} - 1}{} \\frac{1 + \\log(1/\\alpha)}{V}}\\right),$ where $\\gamma = \\frac{2 + e^{-2LB} + e^{2LB}}{\\lambda_{min}(\\Sigma_{\\mathcal{D}})}$.\nNote that, the bound is non-trivial only when the sample covariance matrix is positive definite. However, this assumption is too strong due to the high dimensionality of the feature vector. We can relax it by imposing that the population covariance matrix is positive definite by imposing a coverage assumption on the state-action feature space, which is commonly encountered in offline bandit and re-inforcement learning settings (Yin et al., 2022)."}, {"title": "", "content": "Assumption 2 (Coverage of feature space). Defining the differential state-action features $x = \\phi(s, a^1) - \\phi(s, a^0)$, and population covariance matrix $\\Sigma = \\mathbb{E}_{s \\sim p(\\cdot), (a^0, a^1) \\sim \\mu(\\cdot | s)} [x x^T]$. We assume that $\\lambda_{min}(\\Sigma) \\geq \\kappa$ for some $\\kappa > 0$.\nCorollary 1. Under the same assumption in Theorem 1 and Assumption 2, with probability at least $1 - \\alpha$, we have\n$||\\hat{\\theta}_{RR} - \\theta^*|| \\leq O\\left( \\frac{1}{\\sqrt{nm}} \\cdot \\frac{L^2}{\\kappa} \\sqrt{\\frac{e^{\\epsilon/m} + 1}{\\gamma \\frac{e^{\\epsilon/m} - 1}{}} \\frac{1 + \\log(1/\\alpha)}{V}}\\right)$\nRemark 1. The estimation error in Corollary 1 is influenced by the convergence parameter $\\kappa$, which implicitly depends on the dimensionality $d$ (Wang et al., 2020). Since $||x|| \\leq L$, it follows that $\\kappa \\leq L^2$ by Assumption 1. Thus, to implement user label level DP, the estimation error of estimator $\\hat{\\theta}_{RR}$ is $O(\\frac{d^2}{\\frac{e^{\\epsilon/m}-1}{\\sqrt{mn}}})$. When $m = 1$, i.e., when in the item-level case, our bound matches the nearly-optimal rate shown in (Chowdhury et al., 2024). However, when $m$ is large, $\\frac{1}{2}$ becomes a vanishingly small quantity. Consequently, the estimation error comes to $O(\\frac{d}{\\sqrt{mn}})$, which can be a large magnitude, rendering the estimator inefficient.\n4.2 Lower Bound of User-level DP-RLHF\nOn the other hand, we introduce the lower bound of estimation error for user-level DP-RLHF, which characterizes the worst-case performance of any user-level DP algorithms. A detailed discussion is deferred to the Appendix B.4.\nTheorem 2 (Informal Statement). For any $(\\epsilon, \\delta)$-user-level DP algorithm with output $\\theta_{priv}$, there exists an instance of the BTL model with the underlying parameter $\\theta^*$ such that\n$\\mathbb{E}||\\theta_{priv} - \\theta^*||^2 > \\Omega\\left(\\frac{d}{\\sqrt{n}m} + \\frac{\\sqrt{d}}{\\sqrt{mn} e}\\right)$\n5 Main Method\nIn the previous section, we have shown that although the RR-based mechanism for DP-RLHF in the user-level setting is simple, there are several issues. First, when the contribution of each user $m$ is large, we can see the privacy-utility trade-off in Corollary 1 is bad. This is because, intuitively, larger $m$ indicates we have more data, so the estimation error should be lower. However, a larger"}, {"title": "", "content": "Algorithm 1 AUP-RLHF\n1: Input: Dataset $\\mathcal{D} = (\\mathcal{Z}_1, ..., \\mathcal{Z}_n) \\in (\\mathcal{Z}^m)^n$, privacy parameters $(\\epsilon, \\delta)$, initial point $\\theta_0$\n2: Based on users, partition $\\mathcal{D}$ into $k$ disjoint datasets ${\\mathcal{D}_i}_{i \\in [k]}$, where $\\mathcal{D}_i$ is of size $n_i := n/2^{k+1-i}$\n3: for $i = 1, ..., k$ do\n4: \\quad Run Algorithm 2 with $(\\theta_{i-1}, \\mathcal{D}_i, n_i, \\mathcal{T}_i, \\eta_i, \\epsilon, \\delta, \\tau_i)$ and get its output $\\theta_i$.\n5: end for\n6: Output: $\\theta = \\theta_k$\n$m$ will introduce a larger error for the RR mechanism, which contradicts our expectations. Second, we can see there is a large gap between the lower bound and upper bound. Thus, a natural question is whether we can further fill in the gap. Finally, the private model $\\hat{\\theta}_{RR}$ needs to be the exact minimizer of the loss in (2), which is impossible to get in practice due to the non-convexity of the loss. Thus, a practical and efficient DP algorithm is needed.\nIn this section, we propose our AUP-RLHF method, which is based on DP-SGD (Abadi et al., 2016; Wang et al., 2017; Wang and Xu, 2019; Wang et al., 2019). Instead of flipping labels, here we consider the original loss function in the BTL model with linear reward:\n$\\ell(\\theta; z) := [\\mathbb{I}(y_z = 1) \\log p_{z,1} + \\mathbb{I}(y_z = 0) \\log p_{z,0}],$ where $p_{z,1} = \\sigma(x^T \\theta), p_{z,0} = (1 - \\sigma(x^T \\theta))$.\nIn AUP-RLHF (Algorithm 1), we first partition the data into several disjoint sets, and for each set, we will use a DP-SGD-based update, namely AdapUserPriv-SGD (Algorithm 2). In detail, during each iteration of parameter updates, the framework operates in three stages. First, a subset of users is selected via user-wise sampling, and the average gradient of the selected subset is computed as a query (Step 4-8). Second, the query is passed through a user-level private mean estimation oracle, which outputs a gradient satisfying user-level DP (Step 9-15). Third, the gradient obtained from the previous step is used to perform gradient descent for parameter updates.\nThe main difference between AUP-RLHF and DP-SGD is the private mean estimation oracle for aggregated gradients. If we directly ex-tend DP-SGD to the user-level (Appendix Algo-"}, {"title": "", "content": "Algorithm 2 AdapUserPriv-SGD\n1: Input: Initial model weights $\\theta_0$", "tau$\n2": "sigma \\leftarrow$ Privacy Account($\\epsilon, \\delta, n, \\tilde{n}, T$) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\"}]}