{"title": "Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality", "authors": ["Yiwen Xu", "Qinyang Hou", "Hongyu Wan", "Mirjana Prpa"], "abstract": "In this paper, we present Safe Guard, an LLM-agent for the detection of hate speech in voice-based interactions in social VR (VR-Chat). Our system leverages Open AI GPT and audio feature extraction for real-time voice interactions. We contribute a system design and evaluation of the system that demonstrates the capability of our approach in detecting hate speech, and reducing false positives compared to currently available approaches. Our results indicate the potential of LLM-based agents in creating safer virtual environments and set the groundwork for further advancements in LLM-driven moderation approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Social Virtual Reality (VR) platforms such as VRChat, Rec Room, Bigscreen, AltspaceVR, and Meta Horizon Worlds [47] have gone through a substantial rise in popularity in recent years. These platforms provide integrated features such as customizable avatars and real-time voice chat, offering users immersive first-person online social experience where they can communicate, interact, and engage in a more immersive and embodied way using head-mounted displays (HMDs) compared to traditional online platforms [51]. In social VR, real-time voice interactions are fundamental. Users engage in conversations using their real voices and communicate with others in real time (RT) within the virtual environment. This form of interaction enhances users' feeling of presence and connection, making the experience resemble real-life face-to-face communication compared to text-based interactions [47].\nHowever, as the increasing number of users engage in voice interactions within these virtual environments, new challenges and risks continue to arise in ensuring safe and trustworthy communication in social VR settings. Hate speech, as one of the main harassment forms on social VR platforms, poses significant threats to user well-being and the overall health of the VR communities. People who encounter online harassment often report significant disruptions to their offline lives [7], encompassing emotional and physical distress, shifts in their future technology usage, and raised concerns about safety and privacy [18]. Zheng et al. [51] discussed that online harassment within immersive environments like social VR settings could potentially cause more serious and negative effects on the target individuals' well-being than traditional online attacks. Consequently, effective hate speech detection mechanisms are indispensable for social VR to mitigate the risks and maintain positive user experiences.\nCombating hate speech in social VR faces several challenges. Social VR safety risks are often unpredictable, highly personal, and real-time, making documenting and data collecting difficult [51]. The immersive nature of social VR and the immediacy of voice interactions make it difficult for the platforms to detect and moderate toxic behaviors effectively [51]. To this date, the most accurate method for combating hate speech is human moderation conducted by the community members, yet with the rise of the number of events and attendees, moderators are presented with scalability challenges due to the limited number of moderators and the burden that moderation poses on community members [47].\nTo overcome the challenges, we draw on the work on AI-moderation of harassment in social VR by Schulenberg et al. [47] and a study conducted by Fiani et al. [14] which proposed the design direction for embodied AI moderators in social VR to mitigate harassment towards children. These studies highlighted the potential of AI agents in improving the moderation process by providing timely and context-aware interventions, which is crucial in dynamic and immersive settings like social VR. Building on the insights, our study focuses on the development of an LLM agent named \"Safe Guard\" to detect real-time voice-based hate speech in social VR environments (see Figure 1). Recent advancements in Large Language Models (LLMs) demonstrate under-explored potential for the use of LLMs in the context of real-time voice-based moderation in VR. To that end, we leveraged GPT-3.5 into our LLM-agent design for hate speech detection in VRChat. The proposed agent has two modes: (1) conversational mode with a single user, where it conducts conversation while simultaneously detecting and alerting for hate speech (2) observational mode when multiple users are present, where it monitors interactions to detect and alert for hate speech.\nLLMs present promising solutions for enhancing moderation capabilities in VR settings, by exhibiting the capabilities to effectively identify and classify hate speech based on context and textual content. Kolla et al. [28] highlighted that LLMs perform well on various natural language tasks, including sentiment analysis and the detection of slurs or derogatory remarks, demonstrating their ability to identify explicit hate and offensive speech.\nHowever, LLMs can be limited by their inability to process audio patterns and audio cues directly, which may result in challenges such as misidentification and false positives, especially when dealing with edge cases. Kumar et al. [29] discovered that GPT is likely to produce false positives in identifying hate speech due to triggering on poor language (e.g., profanity, slurs) and stereotypes (34%), even in instances of neutral or positive connotation. Given the consequences that false positives may have on online communities such as resulting in bans and suspended accounts, addressing the scalability of hate speech detection approaches in real time presents multi-faceted challenges. In addition, most previous studies focused on batch detection of harassment in text-based posts or comments [7, 17, 24, 25], a noticeable gap remains in exploring how to detect real-time verbal hate speech in VR setting that is efficient, accurate, and scalable.\nTo address these deficiencies and reduce false positives, in the study of Safe Guard, we aim to bridge the gap by deploying audio feature analysis to enhance LLMs' ability to distinguish hateful content from benign content. The audio model can capture audio features such as tone, pitch, and emotion which LLMs might miss. This capability is valuable for detecting voice-based hate speech where the emotional context and speaker's intent are crucial.\nWe envision the future role of Safe Guard as a first step in detecting and assisting human moderators in combating hate speech in real-time voice-based interactions. To that end, in this work we explore the first step that is concerned with the development of an LLM agent and answering the following research question (RQ):\nHow to detect hate speech in social VR by leveraging audio features and LLM text analysis in LLM-agents in real time?\nThe main contributions of this work include: (1) an embodied LLM-based agent to detect hate speech in real-time voice interactions in social VR, (2) a high-accuracy, fast-speed prompting method for GPT 3.5 to detect real-time voice-based hate speech, (3) a CNN classifier for extraction and analysis of audio features to assist hate speech detection, (4) a system that integrates LLM detection and audio features analysis with the LLM agent system in VRChat for real-time voice-based hate speech detection, and (5) manually collected and annotated video datasets, then transferred into audio format, for validating voice-based hate speech detection.\nIn the following paper, we present Literature Review, Methodology, System Design, System Evaluation Results, Discussion of the Results, Limitations, Future Work, and Conclusion."}, {"title": "2 LITERATURE REVIEW", "content": ""}, {"title": "2.1 Defining Harassment and Hate Speech in the Context of Social VR", "content": "The concept of online harassment remains highly contextual and often involves personal interpretation [12]. Prior studies pointed at the lack of consistent consensus on the definition of harassment across different online social communities. Pater et al. [41] analyzed policy documents from fifteen social media platforms and pointed out that none of those platforms explicitly define what constitutes harassment. By cross-comparison of activities and behaviors that co-occur with harassment in these documents, the most common harassment types include abuse, bullying, harm, hate, stalking, and threats [41]. General online harassment was also categorized by using six distinct behaviors: offensive name-calling, purposeful embarrassment, stalking, physical threats, harassment over a sustained time, and sexual harassment [12]. Alternatively, Blackwell et al. [7] categorized toxic online harassment into four categories, namely flaming, doxing, impersonation, and public shaming. Flaming means hostile and insulting interactions between users. Doxing refers to the act of publicly revealing private information about an individual without their consent. Impersonation involves pretending to be someone else to deceive or harm, and public shaming is the act of humiliating someone in public [7].\nIn the context of social VR, Freeman et al. [18] examined which specific behaviors or interactions and in which context in social VR should be qualified as harassment, and concluded that any discriminatory conduct in social VR based on identity features such as gender and race (e.g., racism, misogyny, and homophobia) and with a specific target or victim are considered harassment. Moreover, Freeman et al. [18] suggested that embodiment and immersion in social VR can simulate real-life offline physical harassment, introducing new forms of online harassment that resemble offline behaviors. Besides movement and gesture-based harassment which is unique in the social VR context, most verbal harassment on social VR platforms share similarities to those found in other online social communities, including hate speech [39]. Verbal harassment in both contexts often involves using various types of detrimental user behaviors involving abusive communications directed towards other users [51].\nHate speech has been defined by United Nations [38] as \"any kind of communication in speech, writing or behaviour, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor.\" Guimar\u00e3es, et al. [20] examined well-known datasets through Web and social network crawling and and labeled the messages as either hate speech or non-hate speech.By conducting experiments using four distinct datasets containing messages labeled as hate and non-hate speech, mainly from Twitter many studies such as [50, 11, 16, 36, 35] agreed on the hate speech definition as attacks on a person or a group based on race, religion, ethnic origin, sexual orientation, disability, or gender. Similarly, Arango, et al. [3] defined hate speech as communications of animosity or disparagement of an individual or a group on account of a group characteristic such as race, color, national origin, sex, disability, religion, or sexual orientation."}, {"title": "2.2 Moderation of Harassment in Social VR", "content": "Existing research on social VR moderation emphasize the complexity of managing harassment due to its embodiment nature, identity-related threats, presence of minors, and lack of consensus on definitions [51, 15]. The immersive and real-time nature of voice-based interactions in social VR makes it difficult for social VR platforms to accurately and effectively detect harassment due to the absence of a persistent, written record [24]. Bad actors can deliver insults and abusive comments in real time and cause immediate and direct harm. Schulenberg et al. [47] claimed that social VR might lead to more severe forms of harassment compared to other contexts. Therefore, hate speech misconducts such as racial slurs in voice channels faced harsher punishments than in text channels [24].\nIn the context of social VR, Schulenberg et al. [47] provided an overview of moderation efforts including Community Guidelines and Punishments, and Moderation Pipelines that are mainly based on human moderation, and introduced the potential of AI moderation in social VR. They concluded that the awareness of the presence of an Al moderation system that can detect the large-scale embodied and immersive multi-user virtual environments and act in the moment can effectively prevent harassment in social VR before it occurs [47]. Existing safety-enhancing features on social VR platforms such as blocking, personal space bubble, muting, reporting players or trust systems [15] to keep users safe from problematic users were found to have limitations [34, 13]. First, they place the burden of moderation on users, who might be ill-equipped or unaware of effective strategies [22]. Second, platforms rely on volunteer moderators to manage behavior in public virtual rooms [8], but they cannot cover most incidents. Sabri et al. [46] showed that only 24% of incidents in social VR are addressed by moderators, highlighting the need for better moderation tools.\nAutomated embodied moderation has been investigated as an alternative solution [13] to create safer spaces in social VR by providing a protective figure that takes action to mitigate harmful interactions. Previous studies [13, 14] explored the use of VR-embodied AI agents to maintain safe and respectful interactions in social VR. Fiani et al. [14] introduced \"Big Buddy\" as a Wizard-of-Oz automated agent in a simulated social VR game and intervenes when a fictional player disrupts the child's game. Unlike traditional moderation tools(e.g., reporting and blocking) that operate in the background and would have flaws in the process and lack trust and feelings of unfair treatment discouraging users from using them [14], embodied agents are visually present within the virtual environment, allowing for real-time interaction and enforcement of community guidelines. However, these AI-based moderation systems have key limitations in their ability to detect problematic events, parse ambiguity, and identify false positives [14, 30]. False positives are produced by mistakenly flagging benign content as harmful [28]. Misidentifying content as harmful or inappropriate can lead to unjust removals or sanctions, which could harm users who may be unfairly targeted [44].\nDealing with uncertainty and missing contextual factors (e.g., tones and pitch in voice) remains a key challenge for automated embodied moderation [13]. Recent advances in AI, particularly in LLMs, along with immersive VR and avatar interfaces, allow for the creation of embodied conversational agents that can engage in natural dialogue with humans [43]. As multiple experimentations [9, 48, 4, 2] have explored incorporating LLMs into LLM-agent conversations, LLM-based agents are becoming more and more prevalent. LLMs can quickly generate dynamic and high-quality responses that adjust to a player's actions, choices, and the overall state of the game world [9, 43]. Mehta et al. [37] highlighted that conversational Als and LLMs in LLM-agent interactions as 'extremely viable' and a potential successor of traditional pre-scripted dialogues. Therefore, the emergence of VR-embodied LLM-based agents in social VR opens up underexplored opportunities to leverage LLM's potential for not only generating conversations but also capturing and detecting real-time voice-based hate speech."}, {"title": "2.3 LLMs for RT Voice-based Hate Speech Detection", "content": "LLMs have shown promise in real-time (RT) voice-based hate speech detection due to their capability for identifying the violating content and providing the reasoning on why the content violated rules [28]. LLMs showed good contextual understanding, which allows them to provide correct reasoning when handling rule violations in online communities [28]. LLMs also have demonstrated state-of-the-art performance in natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data [21], enabling them to grasp intricate contextual details. This feature enables LLMs to detect RT verbal hate speech in complex or ambiguous scenarios, where traditional approaches fall short for the existing supervised learning-based detection methods cannot fully capture context to make accurate predictions [21].\nThe robustness of LLMs to understand and interpret text right after conversion from audio, even when it contains typographical errors, slang, or informal language may contribute to faster detection times in the real-time hate speech detection. Traditional machine learning models and deep learning approaches require extensive text preprocessing to normalize the input data, including tokenization, stop words removal, stemming, and lemmatization [26]. These steps are essential for ensuring that the input data is consistent and interpretable. On the contrary, LLMs can comprehend the meaning and context of raw text inputs without requiring preprocessing owing to their extensive pre-training, which could translate into a more streamlined and efficient pipeline.\nAdditionally, LLMs require minimal feature engineering, streamlining the development process and allowing for faster deployment. This is particularly advantageous in real-time voice moderation, where rapid and accurate detection is important for maintaining a safe and inclusive social VR environment. In comparison, traditional and deep learning Machine Learning methods such as Recurrent Neural Networks, Deep Neural networks, and long short-term memory networks require extensive feature engineering and large labeled datasets [19]. Fine-tune pre-trained transformers such as BERT, ROBERTa and ALBERT [45] are resource-intensive during both training and inference.\nLimitations of LLMs in Hate Speech Detection LLMs are not able to detect tones and emotions within verbal interactions, particularly in edge cases where tone and emotions are crucial for differentiating the nature of verbal speech [29]. Rana et al. [45] stated that the most essential features in classifying hate speech would be the speaker's emotional state and its influence on the spoken words. Tones, emotions, and vocal intonations play crucial roles in conveying the intent behind words. The same sentence can have different meanings based on the speaker's tone, whether to be playful, sarcastic, angry, calm. For example, Rana et al. [45] asserted that a political leader calmly discussing immigration policies at a conference is less harmful than delivering the same speech with extreme anger and disgust towards a specific targeted user, as the latter incites hostility against immigrants in the country. Another example is the different interpretations of certain words depending on the emotion and context [45]. When a friend jokingly calls someone a name in a playful tone and the conversation remains calm or civil, it should not be classified as hate speech. However, if the same words were delivered with a harsh tone and intent to attack specific individuals, they should be detected as offensive or hurtful and classified as hate speech. Since LLMs rely solely on plain text, they are unable to capture the subtleties of spoken language. This limitation results in misclassifications and inaccuracies, especially false positives, of verbal harassment in hate speech detection. False positives often due to LLMs' difficulty in accurately interpreting human emotions [29], as important contextual cues present in tone are not conveyed through text. For example, LLM-based moderator might take the user's exaggerated language as disrespectful to others [29]."}, {"title": "2.4 Improving the Accuracy of LLM Using Audio Feature Analysis for Hate Speech Detection", "content": "Audio features can be analyzed and applied to address the shortcomings of LLMs in detecting hate speech. Rana et al. [45] suggested that the classification of hate speech is significantly influenced by the speaker's emotional state and its impact on their spoken words. The study claimed that incorporating emotion into hate speech detection can help reduce false positives in systems that rely solely on text data as input. Patrick et al. [42] also asserted that there is a close link between hate speech and the emotional and psychological state of the speaker and evident in the emotional tone of their language.\nKumar et al. [29] showed that incorporating context in LLM rule-based moderation corrected 35% of errors with minimal changes to the prompt. Integrating audio feature analysis with LLMs can enhance hate speech detection by providing context that text alone cannot capture. Barakat et al. [5] demonstrated that MFCC audio features significantly improved the accuracy of independent keyword spotting (KWS) for detecting offensive language in video blogs, greatly outperforming the existing speech-to-text methods, which indicates the potential to use audio features for detecting audio-based hate speech.\nDas et al. [10] identified that hate speech demonstrated a certain pattern in temporal features including zero crossing rate and root mean square energy. This pattern can be used to assist hate speech detection by capturing distinctive acoustic characteristics. Analyzing elements like pitch, volume, and speech rate gives insights into the speaker's emotional state and intent, thereby improving classification accuracy. Khan et al. [27] highlighted the effectiveness of combining audio features such as pitch and Mel-Frequency Cepstral Coefficients (MFCC) with models like K-Nearest Neighbors (KNN) and Naive Bayes to enhance emotion classification accuracy. Hu et al. [23] found that using MFCC features with a Gaussian Mixture Model (GMM) combined with a Support Vector Machine (SVM) outperformed traditional GMM methods, achieving improvements in emotion classification accuracy. Similarly, Zhou et al. [52] introduced GMM supervector with SVM using MFCC features achieving an 88% accuracy rate in classifying emotions.\nFinally, recent work by Roblox [6] highlights the advancement of combining a proprietary text filter and audio features analysis for detecting hate speech. They demonstrated the efficacy of a CNN model to create classifiers on MFCC audio features. The model can detect and categorize verbal hate speech into different categories including profanity, dating and sexual, racism, and bullying. This combined method achieved an average precision of 94.48% [6], demonstrating the effectiveness of integrating audio features with text analysis to enhance detection capacity. We are extending this work by leveraging publicly available GPT model and aim at reducing false positives."}, {"title": "3 METHODOLOGY", "content": "Our approach involves the following key steps:\n\u2022 Defining voice-based hate speech criteria in social VR,\n\u2022 Designing and implementing an LLM-based agent Safe Guard for hate speech detection,\n\u2022 Exploring suitable LLMs prompting method for real-time hate speech detection,\n\u2022 Processing voice messages through text conversion and applying real-time LLM-based detection on transcripts. In parallel, extract and analyze the key features of audio using a CNN classifier to assist detection,\n\u2022 Building and integrating the LLM-based detection system into the Safe Guard agent architecture to enhance moderation capabilities,\n\u2022 Collecting and analyzing evaluation metrics to iteratively refine and optimize the proposed system."}, {"title": "3.1 Hate Speech Training and Testing Datasets", "content": "To the best of our knowledge, there are no publicly available audio datasets for hate speech detection. To meet the purpose of our real-time voice-based hate speech detection, we used the HATEMM dataset [10], which was sourced from the BitChute platform and consists of 1083 videos annotated for hate speech. This dataset includes a ground truth annotation file, with 39.8% of the samples labeled as hate speech. We extracted audio from these videos and transcribed it into text using the OpenAI Whisper. The dataset was split into 80% for training purposes, and 20% for testing. The processing approaches involved extracting audio from the videos using FFmpeg API. The audio was then converted into text transcripts with OpenAI Whisper API, and the transcripts were stored in files. Further processing was conducted by combining the transcripts with extracted audio features to improve detection accuracy."}, {"title": "3.2 LLM Set Up and Prompt with Hate Speech Moderation Rules", "content": "In this study, we employed ChatGPT 3.5 as our LLM model for hate speech detection in social VR. Our choice of this model was due to several key considerations. Firstly, GPT 3.5 is capable of handling complex contextual nuances. Secondly, GPT 3.5 is more practical in real-time detection because it requires less computational resources and is more efficient than GPT 4. Overall, it offers a good balance between performance and resource usage.\nBefore using LLMs for real-time detection, we provided information to the models on datasets labeled for hate speech. Prompt-based strategies have been found to effectively guide LLMs in leveraging the context of the specific task [32]. A prompt is a query or statement designed to instruct the model on what is being asked. The effectiveness of an LLM can be significantly improved with carefully crafted prompts, underscoring the importance of prompting techniques in leveraging the context of these models. Guo, et al. [21] claimed that the effectiveness of LLMs in identifying hate speech is highly contingent upon the design of the prompt."}, {"title": "3.3 Convolutional Neural Network Audio Feature Model", "content": "The CNN model is a class of deep learning algorithms primarily used for processing structured grid data, which is suitable for our audio parameters. CNN model is capable of using backpropagation through different layers to build an accurate classifier. Convolutional layers use small learnable filters to identify features across the input data. Activation layers like ReLU introduce non-linearity by converting negative values to zero, assisting the network in learning patterns. Pooling layers reduce the spatial dimensions of the data and reduce the computational load. Following several convolutional and pooling layers, the data is flattened into a 1D vector to classify at last."}, {"title": "4 SYSTEM DESIGN", "content": ""}, {"title": "4.1 Safe Guard Agent Design", "content": "The embodied LLM agent Safe Guard is designed to facilitate real-time interactions with players while proactively monitoring and moderating conversations in VRChat. Building on the previous work by Park et al. [40] and Wan et al. [49], Safe Guard incorporates several features for context management and moderation. The system maintains memories of interactions, capturing not only the dialogue but also the player's attitude, inferred mood based on input messages, and the time when the conversation took place [49]. This comprehensive memory is crucial for understanding and interpreting ongoing interactions in social VR. To manage and evaluate context effectively, Safe Guard uses advanced techniques such as exponential decay to prioritize recent interactions and cosine similarity metrics to assess the relevance of past conversations [49]. These methods ensure that Safe Guard accurately evaluates each interaction's importance and relevance to current discussions.\nDual Mode Operation: Safe Guard can operate in both conversational mode (engaging with a single user) and observational mode (monitoring group interactions), switching as needed based on the scenario.\nKey aspects of the agent system are defined below:\nGPT Module. ChatGPT is deployed throughout the process of Safe Guard system design to process user input, understand the context of social situations and generate appropriate observations or responses. It not only facilitates human-like interactions but also actively monitors for potential instances of hate speech.\nLLM-Agent Formation Module. LLM-agent is created by assigning it a base description that includes name, character details, preferences, etc [49]. Safe Guard is tailored specifically for hate speech detection. Its character details are \"A vigilant, neutral, and approachable guardian, focused on maintaining respectful communication within VR spaces\". Unity engine was used to modify the chosen avatar to include a set of animations for expressions and actions.\nObservation Database Module. The system maintains a collection of context observations that store all observations generated by LLM for each conversation. This includes initial observations from the LLM-agent's \"Base Description\". Further conversational contexts are captured by generating up to three observations for each message [49]. Memory observations are defined as combinations of Base descriptions and Context observations (see [49] for more details on the LLM-agent itself)."}, {"title": "4.2 Hate Speech Detection System", "content": "The real-time hate speech detection system is designed to integrate into VRChat, ensuring a safe and respectful environment. As shown in Fig. 2, the flowchart illustrates the workflow of the proposed Safe Guard hate speech detection system in conversational mode. The system operates similarly in observational mode when multiple players are presented, except that the LLM-agent does not generate dialogue with the players."}, {"title": "4.3 Prompt Engineering for LLM Model", "content": "To set up our hate speech detection system, we used specific prompts for ChatGPT 3.5 based on established hate speech definitions and reasoning to produce a binary response. We compared and decided on the most suitable and effective prompting method in the experiment stage. Since the project is applied to real-time detection, we focused on prompt methods that do not need lengthy context or multiple sequential prompts. We mainly evaluated three different approaches as discussed below."}, {"title": "4.3.1 Approach #1: Direct Prompting", "content": "For this method, we used a straightforward prompt to ask GPT to classify text as either \"Hate\" or \"Non Hate\" without providing any extra instruction. This prompting method has been proven to be effective in previous study [31]. Using the general prompt method can rapidly enable the LLM model to complete the voice hate speech detection task. The prompting message we used for this method was: \"You are a classifier that identifies whether a given text contains hate speech or not. Is the following text hate speech? Respond with Yes or No. Here is the text: \""}, {"title": "4.3.2 Approach #2: Prompting with Hate Speech Definition", "content": "This method involved providing a clear definition of hate speech within our use case and asking if a specific text fits that definition. The prompting message we used for this method was: \"Hate speech in social VR includes any speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. Consider this context carefully. Is the following text hate speech? Respond with Yes or No. Text: \""}, {"title": "4.3.3 Approach #3: Few-Shot Learning Prompting", "content": "Besides the definition of hate speech, we provided several examples with ground truth labels to help GPT learn what constitutes hate speech and what does not. The prompting message we used for this method was: \" Hate speech includes any speech that attacks a person or group based on attributes such as race, religion, ethnic origin, national origin, sex, disability, sexual orientation, or gender identity. Consider this context carefully.\nHere are some examples of hate speech and non-hate speech:\nExample 1: Text: \"I think people of all races should be treated equally.\" Hate Speech: No\nExample 2: Text: \"People from [name] country are all criminals and should be deported.\" Hate Speech: Yes\nExample 3: Text: \"Everyone deserves to be loved, regardless of their gender identity.\" Hate Speech: No\nExample 4: Text: \"I think all immigrants should get out of Canada.\" Hate Speech: Yes\nDetermine if the following text is hate speech. Respond with Yes or No. Text: \""}, {"title": "4.4 Audio feature Extraction", "content": "Voice consists of voice signals and their content. Apart from content, voice signals can also provide information for hate speech detection. Feature extraction is the process of extracting and tackling hidden information in the raw data signal [1]. We used root mean square(RMS) and Mel-Frequency Cepstrum Coefficients(MFCC) for audio feature analysis.\nRoot mean square (RMS) is the square root value of the mean of the sum of squares of the signal. In the context of audio signals, RMS is often used to measure the power or loudness of the signal [10]. It provides a single value that represents the average energy of the waveform, which is particularly useful for analyzing the amplitude of audio signals over time.\n$$RMS = \\sqrt{\\frac{1}{N} \\Sigma_{i=1}^N x[i]^2}$$\nwhere:\n\u2022 x[i] is the amplitude of the signal at the i-th sample,\n\u2022 N is the total number of samples.\nMel Frequency Cepstrum Coefficients (MFCCs) are widely used in speech and audio processing as a representation of the short-term power spectrum of sound [33, 1]. They are useful in various tasks such as speech recognition, speaker identification, emotion recognition, and audio classification. Given their utility, incorporating MFCCs into our hate speech detection method is beneficial. Ali et al. [1] stated that MFCC can be calculated by conducting five consecutive processes, namely signal framing, computing of the power spectrum, applying a Mel filter bank to the obtained power spectra, calculating the logarithm values of all filter banks, and finally applying the Discrete Cosine transform (DCT).\nAfter an initial exploration of audio features, we decided to use the 40-dimensional vector MFCC features in our CNN model, in order to capture key features of the user input audio data. To extract these features from WAV audio files, we used the Python library librosa and stored the results in a CSV file for further analysis."}, {"title": "4.5 Audio Feature Model", "content": "After extracting features of audio files, we used machine learning techniques to build a CNN classifier based on those features.\nInitially, we set up a sequential model consisting of a linear stack of layers. The input layer included a 2D convolutional layer with 32 filters, each of size 3x1, designed to extract spatial features from the input data, which has an input shape of 40 dimensions with 1 feature. Next, we added a second convolutional layer and a second max pooling layer. Finally, a flattening layer was used to convert the 2D data into 1D, followed by two fully connected dense layers to perform the final classification."}, {"title": "4.6 Using a Combination of LLM and Audio Feature Model to Detect Hate Speech", "content": "In our proposed system, we combined GPT 3.5 as the primary method with an audio-based CNN classifier to enhance the overall detection performance. The integration of these two methods aims to combine the strengths of both NLP and audio signal analysis to improve the accuracy and reliability of the detection results.\nAfter converting audio to text, the transcript is delivered to the prompted LLM model. The prompted LLM model goes through the transcript and detects whether hate speech is in it. Simultaneously, audio key features are extracted and analyzed by the CNN classifier to generate a probability score. If this probability score is greater than 0.5, the input is classified as 'Hate.' A threshold of 0.5 is commonly used for making binary classification decisions.\nTo determine the final classification result of the combined model, we applied the following decision rule: The audio input is classified as 'hate speech' only if both the GPT model and the audio-based CNN classifier predict it as 'hate.' Otherwise, the final classification is 'non-hate speech. Once the system successfully detects hate speech, the system will immediately display a voice notification in the agent's dialogue to indicate that hate speech has been detected (see Figure 2)."}, {"title": "4.7 Integration to Safe Guard Agent System", "content": "We integrated the hate speech detection system into the Safe Guard LLM-agent and processed the audio within the module to determine if hate speech was present. Once hate speech is detected, the program will immediately display a warning message in the Safe Guard dialogue. We fine-tuned the silent detection parameters, including increasing the max silence length to 2 seconds and lowering the threshold to -40db for activating the recording procedure. In this way, we were able to retrieve segmented sentences instead of a whole speech for audio processing. We also altered the sample rate to 44100Hz, which is the maximum rate that is compatible with the Safe Guard agent system to get better recording quality and support audio feature extraction."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 Procedure", "content": "VRChat System Setup for Experiment: To simulate the real scenario environment in VRChat, we connected two devices that log in with different accounts in VRChat.One account operated as the host server, running the LLM agent and detection system to capture audio from the test subject. Meanwhile, the second account logged in as the player, transmitting the test speech dataset via microphone.\nDuring the audio feature process, the combination of RMS and MFCCs played a key role in classifying the hate and non-hate datasets, yielding the best results for feature selection. Extracted features went through a CNN training model, which produced a similarity score to the recognized hate speech pattern.\nPrompting Experiment: We evaluated the effectiveness of our LLM hate speech detection system using different prompting techniques. We tested with the 50 videos containing hate speech from our primary MM Hate dataset. True labels for each data will be derived from the manual categorization of the video dataset."}, {"title": "5.2 Evaluation Metrics", "content": ""}, {"title": "5.2.1 Latency: Real-Time Detection Time", "content": "To evaluate the performance of the real-time hate speech detection system, we measured the latency at each stage of the detection process. The overall latency includes four main components as follows: (1) Audio Feature Extract Time: The duration of the audio feature extraction from raw audio data, (2) Speech-to-Text Conversion Time: The time taken to transcribe the audio input into text using a speech recognition engine, (3) Audio Feature Prediction Time: The time taken by the CNN classifier to classify the audio based on audio features as hate speech or non-hate speech, and (4) LLM Analysis Time: The time taken by the LLM to classify the text as hate speech or non-hate speech.\nThe total detection time is the sum of these individual times, representing the overall latency from receiving the audio input to providing the moderation output."}, {"title": "5.2.2 Accuracy Metrics", "content": "To evaluate the accuracy of our real-time voice-based hate speech detection system in social VR", "21": "as follows:\nAccuracy: Accuracy measures the proportion of correct detections (both true positives and true negatives) out of all detections. It is given by:\n$$Accuracy = \\frac{True Positives (TP) + True Neg"}]}