{"title": "Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?", "authors": ["D. Cotroneo", "F. C. Grasso", "R. Natella", "V. Orbinato"], "abstract": "Vulnerability prediction is valuable in identifying security issues more efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.", "sections": [{"title": "1 Introduction", "content": "Vulnerability prediction is an approach to analyze and predict which files or functions of a software system are most likely vulnerable. This approach enables focusing effort and resources (e.g., man-hours) on the vulnerable parts. However, vulnerability prediction has the drawback of being a source code-based approach, which is a restrictive hypothesis: source code is rarely available for several classes of software systems, such as proprietary software, legacy systems, or firmware monitoring specific hardware. Limited access to source code hinders vulnerability prediction, hence the need for new solutions to identify vulnerabilities when source code is unavailable [1-3].\nNeural decompilation is an emerging method that leverages modern deep learning techniques, such as transformers and self-attention, to translate binary code back into source code. One of the key benefits of this approach is that the control flow structure of the decompiled program resembles the original source code, in a closer way than disassemblers and traditional decompiler tools, such as Ghidra [4]. It is a data-driven approach, where the neural network learns the typical patterns of source code structures. Gaining access to the source code offers several advantages, such as enabling vulnerability detection, malware recognition, re-engineering of legacy systems, and facilitating static analysis. Moreover, binary decompilation enables vulnerability prediction in \u201cblack-box\u201d vulnerability assessments (e.g., proprietary software), where the assessor attacks the system from the same perspective of the attacker, without source code.\nIn recent years, several solutions have emerged to predict vulnerabilities through binary code analysis, which can"}, {"title": "2 Background", "content": "The main complexities of this work lie in the decompilation process, i.e., the transformation of executable code into source code, which is not completely reversible. Several decompilers are available in this field (e.g., Ghidra [4], RetDec [15]), achieving good levels of accuracy, however, they come with high costs in terms of knowledge of the specific language, and generation of control flow graphs. Traditional decompilation methods typically leverage an intermediate language (IL), independent from the architecture used to trace the source code, which increases the cost of developing such a system even more. In addition, the resulting decompiled files are not highly readable, as they closely reflect the structure of the assembly code."}, {"title": "2.1 Semantic Gap", "content": "To understand the issues involved in decompilation, we recap the steps of the compilation process, from a program written in a high-level language to the corresponding executable file. First, the program is compiled into an assembly file. Next, the assembly is transformed into a binary object file, i.e., machine code not yet executable because of unresolved function calls reported in the relocation information. Therefore, the linker must transform the object file into an executable file by linking standard library functions and other programs.\nIn contrast, transforming executable code into source code requires different methodologies and tools. The difficulties lie in the operations of disassembly and decompilation. Disassembly consists of lifting binary code to assembly code. There are several open-source tools available for this task. On the other hand, there is not a predefined reverse procedure for decompilation: once a program is compiled into low-level code most of the information is lost.\nThis issue stems from the semantic gap between high- and low-level code: machine code does not provide function-level abstraction since function calls are implemented through call"}, {"title": "2.2 Dataset", "content": "One of the complexities in dealing with the translation and vulnerability prediction tasks lies in finding an appropriate dataset. The dataset should meet the following requirements:\n\u2022 Compilable: the programs under analysis in the dataset must be compilable to generate executables for the binary analysis.\n\u2022 Standardized classification: the programs must be labeled according to the vulnerability they represent following an appropriate categorization system, i.e., a standardized vulnerability taxonomy.\nIn addition, we will need to pre-process the collection of programs before feeding the decompilation model. The output of such a model will be the input to the vulnerability prediction stage."}, {"title": "3 Methodology", "content": "Before presenting the experimental approach, we introduce the following assumptions:\n\u2022 To avoid translating binary code to high-level code directly, we disassembled the binary into assembly code. Consequently, the input for the decompilation model will be assembly code, further pre-processed in the next steps.\n\u2022 Assembly code is defined by the architecture used to generate the executables. In our experimental evaluation, the assembly language is for the x86 architecture.\n\u2022 The high-level target language of the translation is C/C++. These languages are still widely used to develop network applications, Internet of Things (IoT) systems, and systems with safety requirements.\n\u2022 The neural decompilation model must learn to translate code at the function level. Through pre-processing, we strip the programs of unnecessary information associated with libraries, namespaces, external modules, or main functions that only invoke other functions.\nOur experimental approach revolves around five phases: Binary Generation, Disassembly, Pre-processing, Neural Decompilation, and Vulnerability Prediction, as in Figure 2."}, {"title": "3.1 Binary Generation", "content": "This stage consists of compiling the programs under analysis to generate two executables for each of them, a bad and a good version [27]. Respectively, bad executables are vulnerable to the target CWE, while good executables implement countermeasures to prevent it.\nAs introduced in Section 2.2, Juliet contains 64, 099 samples that cover 118 CWEs, however, a subset of such programs is not compilable due to issues with external libraries."}, {"title": "3.2 Disassembly", "content": "This stage lifts the binary code to assembly code. This transformation depends on the target architecture for executables. Several tools are available for this task: we chose objdump [28], a popular and easy-to-use tool. The format of the executables is elf64-x86-64."}, {"title": "3.3 Pre-processing", "content": "Assembly code. The machine code files are not yet ready for training since they are extremely long and contain library functions irrelevant to our purpose. An assembly file consists of several sections, with the code section being .text. We removed all sections except .text since we are only interested in the algorithmic logic.\nAt this stage, it is still necessary to remove library functions defined within the same section, operating system directives, stack initialization, and the main function, which is constant in every program, making its contribution to learning negligible. What is left are exclusively the (non-)vulnerable functions of interest. We aim to let the model learn algorithm recognition and structural patterns related to functional programming logic. We also replaced function names with standard names for each test case. This operation allows the model to focus on the function content rather than the function name.\nThe main benefit of this stage concerns the length of the assembly code for each test case, which is drastically reduced by more than 90% (on average). Then, the assembly code is ready for the translation process.\nHigh-level code. High-level language programs require both similar and additional pre-processing operations to"}, {"title": "3.4 Neural Decompilation", "content": "Neural decompilation is an emerging approach to translating binary code into source code. The main advantages of this approach are the following:\n\u2022 The decompiled program control-flow structure traces the structure of the original high-level program, making the results human-readable. This happens because the transformer learns the typical structure of source programs.\n\u2022 Neural decompilation is a data-driven approach, hence it can be trained on new target languages or architectures with new samples set without depending on specific and hard-to-use toolchains.\nNeural decompilation leverages Neural Machine Translation (NMT), a machine learning approach to translating a given sequence of elements into another sequence through the use of Large Language Models (LLMs), deep learning algorithms able to perform a set of language processing tasks such as translation, classification or generation. In light of the complexity of the tasks, LLMs generally require large accurate datasets to be trained. The core of the LLMs is the Transformer neural network, a deep learning architecture consisting of an encoder-decoder structure based on the multi-headed attention mechanism [30], often used for translation from one language to another. In our case, we leverage NMT to translate machine code to high-level code. The main difference between the two uses is the number of tokens between the source and the translated text. There is a wide gap in the number of tokens between the machine and high-level code. Another difference is that, although natural language follows grammatical rules, a programming language is more structured by nature. The fine-tuned models for the translation task are the following:\n\u2022 CodeBERT [31]: is a large multi-layer bidirectional Transformer with an encoder-only architecture pre-trained on millions of lines of code across six different programming languages created by Microsoft. For the translation task, we combined it with a decoder to have an encoder-decoder architecture suitable for code generation [32].\n\u2022 CodeT5+ [33]: is a large multi-layer bidirectional Transformer with an encoder-decoder architecture, developed by Salesforce and pre-trained on diverse tasks including causal language modeling, contrastive learning, and text-code matching. We utilize the variant with model size 220M, trained from scratch following T5's architecture [32] and initialize it with a checkpoint further pre-trained on Python.\n\u2022 fairseq [34]: is a sequence modeling toolkit written in PyTorch that allows researchers and developers to train custom models for text translation and generation, created by Meta. This model is not pre-trained and supports encoder-decoder architectures for a wide variety of tasks. We selected this model on the basis of previous work [29]."}, {"title": "3.5 Vulnerability Prediction", "content": "This stage represents the core of the vulnerability prediction task. The goal is to identify whether a target program is vulnerable, along with the type of vulnerability affecting it. We addressed these problems as classification problems, following the direction of several studies about vulnerability prediction [8-10]. Specifically, we faced the following classification tasks:\n\u2022 Binary classification: deals with finding whether a target C/C++ function is vulnerable (bad) or not (good), regardless of the specific vulnerability.\n\u2022 Multi-class classification: aims to identify which vulnerability affects the target program. Vulnerability types are the classes to be predicted by the classification models. Such vulnerabilities are mapped to the CWE [13], a standard taxonomy defined by MITRE [14].\nTherefore, we train several models on a training set consisting of original C/C++ programs, i.e., non-decompiled, and then test them on a test set consisting of neurally decompiled C/C++ programs. We make sure that overfitting does not occur since none of the samples in the original C/C++ training set are contained in the decompiled test set. Looking at previous studies on this topic [9, 10, 20], we selected models that can handle the complexity of vulnerability prediction, including:\n\u2022 CodeBERT [31]: unlike its use for the neural decompilation task, we used it as an encoder-only architecture for vulnerability prediction.\n\u2022 CodeT5+ [33]: described in Section 3.4.\n\u2022 CodeGPT [35]: a Transformer based on the GPT-2 model family. CodeGPT provides a decoder-only architectural configuration. It was primarily designed for code generation and optimized for Python, but can also be adapted to generic code classification tasks.\n\u2022 Long Short-Term Memory (LSTM): a recurrent neural network with retroactive connections that provides short-term memory. This makes it fit for processing long input data sequences, as in the case of code classification.\n\u2022 Simple Recurrent Neural Networks (SRNN): it consists of a neural network with interconnections between units through cycles. This enables determining a temporal dynamicity, dependent on the information received at the previous times.\n\u2022 Gated Recurrent Unit (GRU): consists of a recurrent neural network that introduces a gating mechanism, i.e., a mechanism to decide which information is forwarded to the output."}, {"title": "4 Experimental Analysis", "content": "The analysis was performed on a virtual instance of Google Cloud running on Debian GNU/Linux OS version 10, equipped with an Intel Haswell CPU, 16 GB RAM, a 250 GB disk, and a Tesla V100-SXM2-16GB GPU.\nNeural Decompilation. The translation task was performed using fairseq, CodeBERT, and CodeT5+ (base: 220M) models. CodeBERT is trained by implementing an encoder-decoder framework where the encoder is initialized with the pre-trained CodeBERT weights, while the decoder is a transformer decoder comprising 6 stacked layers. The encoder leverages the RoBERTa architecture [37], with 12 attention heads, 768 hidden layers, 12 encoder layers, and 514 for the size of position embeddings.\nCodeT5+ has an encoder-decoder architecture with 12 decoder layers, each with 12 attention heads and 768 hidden layers, and 512 for the size of position embeddings. We set learning rates of 5e-5, batch size of 8, beam size of 10, evaluation steps of 314, and training steps of 2005 for both CodeBERT and CodeT5+ models. Source and target length were set to 512, the maximum length of the input sequences in both CodeBERT and CodeT5+ models. We truncated all the assembly programs longer than the maximum supported length.\nWe derived basic fairseq hyperparameters configuration for model training from the work of Hosseini et al. [29]. We chose a different optimizer, namely the nag optimizer. We performed 10-fold cross-validation to evaluate the decompilation models using the whole dataset and have a more robust and reliable evaluation. Specifically, we trained and evaluated the models ten times, each using one of the ten folds as the test set and the remaining nine as the training set. At the end of the ten iterations, performance is averaged to obtain a more reliable estimate of the model's predictive capabilities.\nVulnerability Prediction. For the neural network-based"}, {"title": "4.2 Evaluation - Neural Decompilation", "content": "We evaluated the translation quality using output similarity metrics including BLEU, METEOR, ROUGE, and Edit Distance (ED), illustrated in Table 4. Each metric expresses different characteristics of translation quality: BLEU allows capturing the precision of n-grams, giving importance to sequence length; ROUGE is useful in capturing code structure and overlapping semantic units, such as word sequences and"}, {"title": "4.3 Evaluation - Vulnerability Prediction", "content": "As introduced in Section 3.5, we addressed the vulnerability prediction process as a binary classification problem to state whether a target program is vulnerable and a multi-class problem to identify the specific CWE.\nBinary Classification. The first analysis concerns the classification of the target code as vulnerable/non-vulnerable. We submitted the C/C++ code translated by the neural decompilation model to the classification process, adopting the same data pre-processing for each model. We selected accuracy and F1-Score as the metrics to evaluate the performance of the models.\nTable 6 shows the models' performance for the binary classification on the decompiled C/C++ code. CodeGPT is the best-performing model with an accuracy and F1-Score of 95%. The performance is high for all models, with both"}, {"title": "4.4 Impact of decompilation quality on vulnerability prediction", "content": "The proposed solution leverages the output of the neural decompilation as the input for the vulnerability prediction. Consequently, it is both valuable and insightful to investigate how the quality of the decompiled code influences the performance of the subsequent classification model."}, {"title": "5 Lessons Learned", "content": "The results of the experimental study provided us with several lessons learned about using neural decompilation in the cybersecurity landscape.\nApplicability of neural decompilation to cybersecurity. The idea of leveraging neural decompilation for cybersecurity-related tasks is promising. Specifically, given the quality of the translated code, neural decompilation showed great potential for vulnerability prediction, with the possibility to unlock several novel applications, e.g., binary malware recognition. We analyzed three state-of-the-art models, i.e., fairseq,"}, {"title": "6 Related Work", "content": "Neural decompilation is a recent and innovative research trend in binary analysis. Hosseini et al. [29] introduced Beyond The C, a neural decompiler based on the fairseq model. This solution is retargetable for several high-level programming languages such as C, OCaml, Go, and Fortran. The results of their study are promising for all languages, scoring the best result in terms of ED (54%) on the decompilation of C/C++ code (training on 2, 000, 000 samples). Katz et al. [36] proposed an approach for neural decompilation using RNNs. The solution was trained and evaluated on snippets of binary machine code compiled from C source code. This solution achieved an ED equal to 30%, leveraging about 700,000 samples.\nThe neural decompilation phase of our experimental study builds on top of the solution proposed by Hosseini et al. but differently, we selected a different set of programs and performed a comparative analysis between state-of-the-art NMT models. In addition, rather than focusing on neural"}, {"title": "7 Threats to Validity", "content": "Dataset. The chosen dataset, Juliet C/C++ 1.3, is a synthetic dataset, i.e., novel data created from real data with the same statistical properties. Using a synthetic dataset provides a large amount of samples, accurately annotated with ground truth. However, the synthetic data do not perfectly align with actual data regarding the size and complexity of real code. Therefore, the vulnerability detection accuracy on synthetic data may be higher than in real data. This may hinder the external validity of the study. Nevertheless, the synthetic"}, {"title": "8 Conclusion", "content": "In this work, we presented an experimental study on vulnerability prediction in binary files. In particular, we leveraged neural decompilation to translate binary code into source code, treating both programs as simple text. Our goal was to demonstrate that a more effective solution for detecting vulnerabilities in binary files is to use neurally decompiled source code, rather than relying on complex toolchains to convert binary code into an intermediate representation or to extract control flows for decompilation. Our experimental analysis revealed that neural decompilation is fit for this task, achieving great results comparable with state-of-the-art works, and that transformer-based and DL models can accurately predict whether a binary file is vulnerable and identify the type of vulnerabilities, mapped to the MITRE CWE taxonomy, using the neurally decompiled code."}, {"title": "4.1 Experimental Setup", "content": "The analysis was performed on a virtual instance of Google Cloud running on Debian GNU/Linux OS version 10, equipped with an Intel Haswell CPU, 16 GB RAM, a 250 GB disk, and a Tesla V100-SXM2-16GB GPU.\nNeural Decompilation. The translation task was performed using fairseq, CodeBERT, and CodeT5+ (base: 220M) models. CodeBERT is trained by implementing an encoder-decoder framework where the encoder is initialized with the pre-trained CodeBERT weights, while the decoder is a transformer decoder comprising 6 stacked layers. The encoder leverages the RoBERTa architecture [37], with 12 attention heads, 768 hidden layers, 12 encoder layers, and 514 for the size of position embeddings.\nCodeT5+ has an encoder-decoder architecture with 12 decoder layers, each with 12 attention heads and 768 hidden layers, and 512 for the size of position embeddings. We set learning rates of 5e-5, batch size of 8, beam size of 10, evaluation steps of 314, and training steps of 2005 for both CodeBERT and CodeT5+ models. Source and target length were set to 512, the maximum length of the input sequences in both CodeBERT and CodeT5+ models. We truncated all the assembly programs longer than the maximum supported length.\nWe derived basic fairseq hyperparameters configuration for model training from the work of Hosseini et al. [29]. We chose a different optimizer, namely the nag optimizer. We performed 10-fold cross-validation to evaluate the decompilation models using the whole dataset and have a more robust and reliable evaluation. Specifically, we trained and evaluated the models ten times, each using one of the ten folds as the test set and the remaining nine as the training set. At the end of the ten iterations, performance is averaged to obtain a more reliable estimate of the model's predictive capabilities.\nVulnerability Prediction. For the neural network-based"}, {"title": "4.2 Evaluation - Neural Decompilation", "content": "We evaluated the translation quality using output similarity metrics including BLEU, METEOR, ROUGE, and Edit Distance (ED), illustrated in Table 4. Each metric expresses different characteristics of translation quality: BLEU allows capturing the precision of n-grams, giving importance to sequence length; ROUGE is useful in capturing code structure and overlapping semantic units, such as word sequences and"}, {"title": "4.3 Evaluation - Vulnerability Prediction", "content": "As introduced in Section 3.5, we addressed the vulnerability prediction process as a binary classification problem to state whether a target program is vulnerable and a multi-class problem to identify the specific CWE.\nBinary Classification. The first analysis concerns the classification of the target code as vulnerable/non-vulnerable. We submitted the C/C++ code translated by the neural decompilation model to the classification process, adopting the same data pre-processing for each model. We selected accuracy and F1-Score as the metrics to evaluate the performance of the models.\nTable 6 shows the models' performance for the binary classification on the decompiled C/C++ code. CodeGPT is the best-performing model with an accuracy and F1-Score of 95%. The performance is high for all models, with both"}, {"title": "4.4 Impact of decompilation quality on vulnerability prediction", "content": "The proposed solution leverages the output of the neural decompilation as the input for the vulnerability prediction. Consequently, it is both valuable and insightful to investigate how the quality of the decompiled code influences the performance of the subsequent classification model."}, {"title": "5 Lessons Learned", "content": "The results of the experimental study provided us with several lessons learned about using neural decompilation in the cybersecurity landscape.\nApplicability of neural decompilation to cybersecurity. The idea of leveraging neural decompilation for cybersecurity-related tasks is promising. Specifically, given the quality of the translated code, neural decompilation showed great potential for vulnerability prediction, with the possibility to unlock several novel applications, e.g., binary malware recognition. We analyzed three state-of-the-art models, i.e., fairseq,"}, {"title": "6 Related Work", "content": "Neural decompilation is a recent and innovative research trend in binary analysis. Hosseini et al. [29] introduced Beyond The C, a neural decompiler based on the fairseq model. This solution is retargetable for several high-level programming languages such as C, OCaml, Go, and Fortran. The results of their study are promising for all languages, scoring the best result in terms of ED (54%) on the decompilation of C/C++ code (training on 2, 000, 000 samples). Katz et al. [36] proposed an approach for neural decompilation using RNNs. The solution was trained and evaluated on snippets of binary machine code compiled from C source code. This solution achieved an ED equal to 30%, leveraging about 700,000 samples.\nThe neural decompilation phase of our experimental study builds on top of the solution proposed by Hosseini et al. but differently, we selected a different set of programs and performed a comparative analysis between state-of-the-art NMT models. In addition, rather than focusing on neural"}, {"title": "7 Threats to Validity", "content": "Dataset. The chosen dataset, Juliet C/C++ 1.3, is a synthetic dataset, i.e., novel data created from real data with the same statistical properties. Using a synthetic dataset provides a large amount of samples, accurately annotated with ground truth. However, the synthetic data do not perfectly align with actual data regarding the size and complexity of real code. Therefore, the vulnerability detection accuracy on synthetic data may be higher than in real data. This may hinder the external validity of the study. Nevertheless, the synthetic"}, {"title": "8 Conclusion", "content": "In this work, we presented an experimental study on vulnerability prediction in binary files. In particular, we leveraged neural decompilation to translate binary code into source code, treating both programs as simple text. Our goal was to demonstrate that a more effective solution for detecting vulnerabilities in binary files is to use neurally decompiled source code, rather than relying on complex toolchains to convert binary code into an intermediate representation or to extract control flows for decompilation. Our experimental analysis revealed that neural decompilation is fit for this task, achieving great results comparable with state-of-the-art works, and that transformer-based and DL models can accurately predict whether a binary file is vulnerable and identify the type of vulnerabilities, mapped to the MITRE CWE taxonomy, using the neurally decompiled code."}]}