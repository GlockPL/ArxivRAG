{"title": "Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation", "authors": ["Yuan Tian", "Daniel Lee", "Fei Wu", "Tung Mai", "Kun Qian", "Siddhartha Sahai", "Tianyi Zhang", "Yunyao Li"], "abstract": "Text-to-SQL models, which parse natural language (NL) questions to executable SQL queries, are increasingly adopted in real-world applications. However, deploying such models in the real world often requires adapting them to the highly specialized database schemas used in specific applications. We find that existing text-to-SQL models experience significant performance drops when applied to new schemas, primarily due to the lack of domain-specific data for fine-tuning. This data scarcity also limits the ability to effectively evaluate model performance in new domains. Continuously obtaining high-quality text-to-SQL data for evolving schemas is prohibitively expensive in real-world scenarios. To bridge this gap, we propose SQLSYNTH, a human-in-the-loop text-to-SQL data annotation system. SQLSYNTH streamlines the creation of high-quality text-to-SQL datasets through human-LLM collaboration in a structured workflow. A within-subjects user study comparing SQLSYNTH with manual annotation and ChatGPT shows that SQLSYNTH significantly accelerates text-to-SQL data annotation, reduces cognitive load, and produces datasets that are more accurate, natural, and diverse. Our code is available at https://github.com/adobe/nl_sql_analyzer.", "sections": [{"title": "1 INTRODUCTION", "content": "Natural language interfaces to databases can significantly democratize complex data analysis and decision-making processes for end-users by allowing them to express their intent through NL. These interfaces are mainly powered by text-to-SQL models that convert NL questions into SQL queries [61, 64, 66]. The resulting SQL queries are then executed on the database to retrieve the results for users. Recent advancements, notably in large language models (LLMs), have substantially enhanced NL interfaces and opened a promising market that is expected to grow around three times in five years [8, 55, 69].\nDespite these advancements, the accuracy and reliability of text-to-SQL models in real-world applications remain suboptimal, especially in high-stakes domains such as finance and healthcare where an error can cause severe consequences. One root cause of this inaccuracy is the domain shift, which is a common challenge in machine learning [9, 12, 52]. Specifically, deploying an NL interface to a real-world database requires the underlying text-to-SQL model to understand the database schema, which is often highly specialized with unique architectures and data contexts that public datasets such as Spider [96] and BIRD [46] do not cover. Our formative"}, {"title": "2 RELATED WORKD", "content": "2.1 SQL Generation from Natural Language\nDeveloping a natural language interface for databases has been a long-standing problem since the 1970s. In 1972, LUNAR [87] was proposed to enable geologists to query the chemical analysis data of lunar rocks. Early effort in this domain focuses on logic-based [19, 84] and rule-based [1, 42, 60, 65, 92] approaches. However, these approaches require significant human efforts to create the translation rules and are limited to a definite set of queries [32, 60].\nThe emergence of many public text-to-SQL datasets [46, 92, 96, 102] propel researchers to develop learning-based text-to-SQL models [29, 64, 66, 77, 79, 99]. Early text-to-SQL models mainly leverage the encoder-decoder architecture [50, 101, 103], where the encoder maps NL question and database schema to latent state and decoder maps the latent state to a SQL query. Recently, more work [15, 16, 54] has focused on leveraging large language models (LLMs). These LLMs are pre-trained on large corpora and exhibit general ability in semantic parsing. They can effectively solve text-to-SQL tasks in a zero-shot setting with prompts or a few demonstrations [22, 73]. In the meanwhile, some works fine-tune existing LLMs to achieve better performance LLMs [45, 59, 62, 86]. However, these approaches are optimized for general text-to-SQL capabilities and may not perform well on specialized schemas. SQLSYNTH"}, {"title": "2.2 Text-to-SQL Dataset Creation", "content": "2.2.1 Human-annotated Text-to-SQL Datasets. There has been a growing number of text-to-SQL datasets [10, 13, 14, 25, 30, 39, 43, 46, 72, 82, 93, 96, 98, 102]. For example, WikiSQL [102] contains 80,654 hand-annotated examples of questions and SQL queries across 24,241 Wikipedia tables. Spider [96] is a cross-domain text-to-SQL dataset annotated by 11 college students. It contains 10,181 questions and 5,693 SQL queries across 200 databases, spanning 138 different domains. Another notable dataset, BIRD [46], includes 12,751 queries over 95 databases spanning 37 professional domains created by crowd-sourcing.\nWhile these datasets attempt to incorporate real-world query scenarios, they are limited by the number of schemas. In practice, different applications may use significantly different schema architectures and query traffic, leading to significant disparities compared to existing datasets. Moreover, while each dataset contains thousands of queries in total, the number of queries per database remains insufficient to cover all query types, due to the high cost of human annotation.\n2.2.2 Text-to-SQL Data Augmentation & Synthesis. Compared with manual annotation, there have been a lot of efforts in automating the dataset construction process. The first line of research focuses on data augmentation from existing datasets, which transforms or expands an existing dataset to a new dataset [2, 51, 53, 67, 81, 94]. For example, SyntaxSQLNet [94] generates cross-domain data using query templates from the Spider dataset. CONDA [81] augments SParC [97] and CoSQL [95] by modifying SQL dialogue states to generate different queries and questions. However, such data augmentation is limited to the existing dataset, making it challenging to significantly increase the diversity of the data.\nTo address the diversity limitation of data augmentation, the second line of research focuses on sampling various queries based on the SQL grammar and then translating them into NL questions [20, 81]. While grammar-based sampling is feasible, translating SQL queries back to NL questions remains a challenge. Existing methods are either template-based [35, 36, 44, 56, 75, 77, 91] or model-based [21, 48, 80, 88], both of which have limitations. Template-based methods translate formal queries into NL questions based on pre-defined templates, which lack diversity and naturalness in generated questions. Model-based methods train or employ a pre-trained model to generate the NL question from the formal query. For example, [80] trained a BART-based [40] translation model that maps a SQL to an NL question. However, such translation models can introduce errors and make the dataset unreliable.\nRecently, research has demonstrated that LLMs can work as effective data annotators [7, 18, 24, 71, 83, 85, 90]. While LLMs can be prompted to generate text-to-SQL data, they are not well-suited for producing large amounts of diverse data from scratch.\nTo address these issues, SQLSYNTH first samples a SQL query based on grammar rules and translates it into an NL question using an LLM. To handle potential translation errors, SQLSYNTH adopts a human-in-the-loop inspection and repair method. It detects and"}, {"title": "2.3 Interactive Data Annotation", "content": "While pure synthetic datasets are not reliable, human annotation is time-consuming and labor-intensive. To strike a balance, interactive data annotation methods [4, 26, 33, 34, 38, 47, 89] were introduced to reduce annotation effort while maintaining annotation quality. These approaches typically employ an annotation model that provides suggestions for human reviewers to approve or correct.\nCompared to manual annotation, interactive annotation offers a significant advantage in that human annotators need only verify model-generated annotations rather than creating data from scratch. This can significantly accelerate the annotation process while reducing human effort. A common strategy of interactive annotation is utilizing active learning [6, 27, 33, 37, 47], which strategically selects the most informative data points for annotation. This approach aims to optimize the model's performance incrementally with the least amount of human-annotated data. However, a major limitation of active learning is its tendency to reinforce data biases. Although this approach selectively samples data points believed to be most valuable for model improvement, it may inadvertently focus on atypical examples that do not represent the full spectrum of the dataset. Consequently, the model trained on such datasets may develop a skewed understanding, resulting in poor performance.\nFacilitating efficient collaboration between intelligent systems and humans has long been a central theme in HCI research, initially introduced in the seminal work on man-computer symbiosis [49]. Nowadays, the imperfections of AI models in high-stakes domains underscore the need for enhanced human-AI collaboration. Interactive data annotation exemplifies this type of collaboration, aiming for more accurate and trustworthy outcomes. However, to the best of our knowledge, no effective interactive text-to-SQL annotation tools existed prior to this work."}, {"title": "3 FORMATIVE STUDY", "content": "To understand the specific requirements for text-to-SQL dataset annotation, we conducted a formative study by interviewing 5 engineers from Adobe. These interviewees have experienced annotating text-to-SQL datasets in their work. We describe our interview process in Section 3.1. Based on these interviews, we identified five major user needs in Section 3.2. Finally, we discuss our design rationale in Section 3.3, aiming to address the user needs."}, {"title": "3.1 Interview", "content": "We conducted 20-minute semi-structured interviews with each interviewee through a conversational and think-aloud process. During these interviews, we first asked about the motivation for text-to-SQL annotation in their use cases, specifically about the schemas they worked on and why obtaining more data was important. Interviewees reported that when deploying a new service, companies often needed to introduce new entities and restructure the original schema. However, after updating the schema, they typically found that model performance dropped dramatically. Their regression"}, {"title": "3.2 User Needs", "content": "N1: Effective Schema Comprehension. Text-to-SQL annotation assumes that users can easily understand the database schema specified in a certain format (e.g., Data Definition Language). However, our interviews indicate that it is cumbersome and error-prone for users to navigate and comprehend complex schemas from such a specification format.\nN2: Creating New Queries. Creating SQL queries requires a deep understanding of both database schema and SQL grammar. When creating a text-to-SQL dataset, users need to continually come up with new, diverse SQL queries. However, it is challenging for them to break free from preconceptions shaped by existing queries they have seen before.\nN3: Detecting Errors in the Annotated Data. An annotated dataset may include errors, which can deteriorate model performance and evaluation results. Our interviews suggest that annotators need an effective mechanism for detecting potential errors or ambiguity in the constructed queries.\nN4: Efficiently Correcting the Detected Errors. After identifying errors, users need an efficient way to correct these errors to ensure the accuracy and reliability of the dataset. They need to ensure the SQL query is syntactically correct, and the NL is semantically equivalent to the SQL query.\nN5: Improve Dataset Diversity. Dataset diversity is crucial for improving model performance and ensuring rigorous evaluation. Human annotation can easily introduce biases due to individual knowledge gaps and a lack of holistic understanding of the dataset composition. Thus, interviewees reported the need for an effective way to improve diversity and eliminate biases in the dataset."}, {"title": "3.3 Design Rationale", "content": "To support N1, our approach visualizes the database schema as a dynamic, editable graph. This enables users to quickly grasp the"}, {"title": "4 IMPLEMENTATION", "content": "SQLSYNTH comprises multiple UI components, each corresponding to a step in text-to-SQL dataset annotation and addressing specific user needs mentioned in Section 3.2. We describe each component in this section. Additional implementation details, such as algorithms and prompt design, are discussed in Appendix B and Appendix C."}, {"title": "4.1 Schema Visualization", "content": "Annotating text-to-SQL datasets requires annotators to understand the corresponding database schema. However, practical database schemas can be complex and challenging to comprehend. To facilitate user comprehension of schema, SQLSYNTH enables users to visualize the schema in an interactive graph, as shown in Figure 3. Users can upload the schema by either dragging a schema file onto the canvas or using the upload button. Each table is visualized as a box, with columns listed as rows within it. The primary keys are colored blue and marked with \"PK\". Reference relationships between columns in different tables are rendered as dashed lines, with a flow animation indicating the reference direction. To inspect details, users can hover over columns and tables to view data types or entity descriptions. The interface allows for dragging tables, zooming in and out, and panning across the view.\nThe graph is editable, allowing users to update the schema as needed. To add new tables, users can click the \"ADD TABLE\" button in the top-left corner (Fig. 3). Hovering over a table allows users"}, {"title": "4.2 Database Population", "content": "SQL queries often reference specific values in the database. However, there are often no existing records in the database to reference during annotation. To address this limitation, SQLSYNTH enables users to instantly create a sandbox database populated with diverse values (Fig. 4). This sandbox database serves two purposes: (1) it provides a source for retrieving values, and (2) it allows for executing the annotated SQL queries to validate their correctness.\nTo populate the database, users can specify the desired number of records and create them with a single click (Fig. 4). SQL-SYNTH employs a rule-based method to randomly synthesize records"}, {"title": "4.3 SQL Query Generation", "content": "Creating unbiased SQL queries is challenging, particularly when dealing with a new and complex database schema. To address this challenge, SQLSYNTH provides a suggested SQL query (Fig. 5 ) that is randomly sampled using SQL grammar and values in the populated database. Specifically, SQLSYNTH utilizes a pre-defined probabilistic context-free grammar (PCFG) tailored for SQL queries. This PCFG can be easily modified in a configuration file, as exemplified in Appendix B. While users can configure the grammar manually, SQLSYNTH also supports automatically learning keyword probability distributions from an imported dataset. Users can directly edit the suggested SQL query to meet specific needs and check the execution result via the \"EXECUTE\" button (Fig. 5).\nCompared to using LLMs to generate SQL queries directly, our PCFG-based approach offers more fine-grained control over query diversity and correctness. It mitigates issues such as bias or hallucination introduced by LLMs. This is the rationale behind our decision to first generate the SQL query and then translate it into natural language (NL). An alternative approach could be generating the NL question first and then generating the SQL query. However, this method has limitations compared to ours. First, generating a"}, {"title": "4.4 Natural Language Question Generation", "content": "Based on the SQL query, SQLSYNTH provides a suggested NL question by translating the SQL query using GPT-4 Turbo\u00b9 (Fig. 5 ). For more accurate translation, SQLSYNTH employs retrieval-augmented generation (RAG) [17, 41]. It retrieves similar examples from a text-to-SQL data pool, which collects previously annotated data and 1,500 real-world text-to-SQL pairs. Unlike commonly used retrievers in RAG, such as dense retriever [31] and BM25 [63], we develop an AST-based retriever tailored for SQL queries. Specifically, SQLSYNTH calculates similarity scores between SQL queries by measuring the tree edit distance between their abstract syntax trees, retrieving the top five examples with scores above 0.5. Using these examples, SQLSYNTH performs few-shot learning to translate the annotation SQL query into an NL question. Figure 13 shows the prompt. Furthermore, users can view the top similar examples"}, {"title": "4.5 Error Detection & Repair", "content": "4.5.1 SQL Step-by-step Explanation in NL. To enhance user comprehension of SQL queries and detect potential errors, SQLSYNTH explains the SQL query step by step in NL (Fig. 5). We reuse the rule-based explanation generation approach from STEPS [77], which parses the SQL query and translates each part of the query to an NL description based on templates. SQLSYNTH enhances this"}, {"title": "4.5.2 Visual Correspondence among SQL query, NL question, and Step-by-step Explanation.", "content": "The step-by-step explanation serves as a bridge between the SQL query and the NL question. When users click the \"CHECK ALIGNMENT\u201d button (Fig. 5g), SQLSYNTH creates a triple-linkage among these elements. First, since the step-by-step explanation is grammar-based, there is a one-to-one mapping between SQL components and explanation steps. Second, SQLSYNTH employs an LLM to align the step-by-step explanation with the NL question. For each explanation step, the LLM pinpoints related substrings in the NL question, maintaining a one-to-many mapping. When users hover over an explanation step (Fig. 5), SQLSYNTH highlights the corresponding SQL component and related question substrings in yellow. This triple-linkage helps users mentally connect the SQL query and the suggested NL question, enhancing user understanding of the data and aiding in the detection of potential errors."}, {"title": "4.5.3 Misalignment Detection & Correction.", "content": "While SQLSYNTH guarantees the syntactic correctness of SQL queries sampled by PCFG, the NL question generated by the LLM can include errors or ambiguity. SQLSYNTH proposes a novel interactive error detection and correction strategy by aligning the NL question with the SQL query through the step-by-step explanation. Motivated by research [78] showing that multi-agent collaboration enhances generation accuracy, SQLSYNTH accomplishes this task through a two-step prompting. We include our prompt design in Figures 15 and 16, with further details discussed in Appendix C. If any substring in the NL question fails to map to any step in the explanation, the substring will be highlighted in red (Fig. 5 \u2170), suggesting that this text may be irrelevant to this SQL query. Users can focus on the red text and consider removing them. Similarly, if a certain explanation step does not map to any partial text in the NL question, this step will be highlighted in red (Fig. 5), indicating the content mentioned in this step may be missing in the NL question. In this case, users can update the NL question by clicking the \"INJECT\" button on the corresponding step (Fig. 5). Then, the LLM is prompted to update the current NL question by amplifying the content mentioned in this step. Figure 17 shows the prompt."}, {"title": "4.5.4 Confidence Scoring.", "content": "To help users better assess the quality of annotated data, SQLSYNTH offers a post-annotation analysis via the \"POST-ANNOTATION ANALYSIS\" button (Fig. 6). Recent research has demonstrated that LLMs can determine semantic equivalence between SQL queries [100] and generate accurate confidence scores through self-reflection [70, 74]. Based on these findings, SQLSYNTH prompts the LLM to provide a final report and score indicating the quality and correctness of the data. The prompt used in this step is shown in Figure 18. The score is averaged after multiple rounds of analysis to ensure stability. This score serves as a confidence level, directing users to focus more on checking data with lower scores, as these data may contain potential issues.\nBased on the analysis provided by SQLSYNTH, users can choose to accept or reject the data (Fig. 66). Accepted data is collected in the right panel (Fig. 5k), where users can review and download the dataset at any time."}, {"title": "4.6 Automated Dataset Annotation", "content": "While SQLSYNTH enables users to annotate text-to-SQL datasets in an interactive manner, SQLSYNTH also supports fully automated data annotation without humans in the loop (Fig. 6). This is useful when users need a large amount of data without a perfect dataset quality (e.g., for fine-tuning LLMs). Users can specify how many queries to synthesize and start by one click. All generated data will be automatically collected on the right (Fig. 5k)."}, {"title": "4.7 Dataset Diversity Analysis", "content": "To ensure diversity and eliminate potential biases in the annotated dataset, SQLSYNTH allows users to monitor dataset composition and property distributions. Users can upload their dataset via drag-and-drop (Figure 7 ). SQLSYNTH then renders various property distributions in pie charts, bar charts, or line charts, in terms of SQL structure, keyword, clause number, column usage, etc. (Figure 7).\nFor example, users can monitor the number of referenced values in a bar chart. If users find that SQL queries with a sufficient number of referenced values are underrepresented in the current dataset, they can adjust the PCFG probabilities to generate SQL queries with more values. And they can selectively accept only those queries that contain an adequate number of values. In addition to ensuring diversity, this UI component generally improves human control during collaboration with the LLM, enabling users to better manage annotation pace and focus."}, {"title": "5 USAGE SCENARIO", "content": "Bob is a data scientist at a rapidly growing technology company. Now, his task is to create a high-quality text-to-SQL dataset for training and evaluating the natural language (NL) interface of the company's recently updated database system. Bob faces several challenges that make this task particularly daunting. First, the company has just completed a major update to its database schema, introducing new tables and relationships to accommodate its expanding business needs. This update makes previous datasets obsolete and incompatible with the current schema. Consequently, it is impossible to accurately evaluate the performance of the NL interface based on the updated database. Adding to the complexity, the schema now becomes highly complex, with numerous tables and reference relationships. Manually updating previous datasets to reflect these changes would be impractical. Bob realizes that he needs a solution that can handle this complexity efficiently and accurately. Furthermore, Bob needs to create diverse, unbiased SQL queries and their corresponding NL questions at scale. Doing this manually would be prohibitively time-consuming and challenging, especially given the complexity of the new schema. Recognizing these challenges, Bob decides to use the newly developed text-to-SQL data annotation tool, SQLSYNTH, to streamline his workflow and ensure the creation of a controllable, high-quality dataset.\nSchema Comprehension. Bob begins by uploading a JSON file containing the company's updated database schema to SQLSYNTH. As the schema loads onto the drag-and-drop canvas, Bob is immediately impressed by how SQLSYNTH transforms the complex JSON structure into an intuitive visual representation. Tables appear as clearly defined boxes with columns listed inside, while relationships between tables are displayed as animated dashed lines. The"}, {"title": "6 USER STUDY", "content": "To investigate the usability and effectiveness of SQLSYNTH, we conducted a within-subjects user study with 12 participants. The study compared SQLSYNTH with manual annotation and the use of a conversational AI assistant."}, {"title": "6.1 Participants", "content": "We recruited 12 participants (4 female, 8 male) from Adobe. They worked in different roles including Machine Learning Engineers, Research Scientists, Data Scientists, and Product Managers. Their works were directly or indirectly related to querying data in the database. All of them had either Master's or PhD degrees. Participants self-rated their proficiency in SQL (3 Beginner, 3 Basic, 4 Intermediate, 2 Advanced) and usage frequency of LLMs (6 Yearly, 2 Monthly, 2 Weekly, 3 Daily)."}, {"title": "6.2 Tasks", "content": "We randomly sampled 9 schemas on the widely used text-to-SQL benchmark, Spider [96]. We provided these schemas in JSON format, whose syntax was comprehensible to all participants. Based on the schema, participants were asked to annotate text-to-SQL data while optimizing both the data quantity and quality."}, {"title": "6.3 Comparison Baselines", "content": "To the best of our knowledge, no text-to-SQL data annotation tools were readily available for comparison at the time of the user study."}, {"title": "6.4 Protocol", "content": "Each study began with a demographic survey and study introduction. Participants then watched a 4-minute tutorial video of SQLSYNTH and spent 3 minutes practicing to get familiar with it. Meanwhile, we collected quality feedback from users.\nAfter participants were familiar with SQLSYNTH, they proceeded to annotate in the assigned condition (i.e., Manual, AI assistant, SQLSYNTH). Each task consisted of three 5-minute sessions, one for each condition. We randomized the order of assigned conditions to mitigate learning effects. For each session, participants were provided with a database schema and asked to annotate as many text-to-SQL datasets as possible. We asked participants to focus on not only the quantity but also the quality of annotated data.\nAfter each session, participants completed a post-task survey to rate their experience with the assigned condition. The survey included the System Usability Score (SUS) [5] and NASA Task Load Index (TLX) [23] questionnaires, using 7-point Likert scales to assess their perceptions. At the end of the study, participants filled out a final survey sharing their experiences, opinions, and thoughts. The entire study took approximately 70 minutes."}, {"title": "7 RESULTS", "content": "This section describes the results of our user study. We first present the analysis of user annotation performance in different conditions. We measure annotation speed and annotation quality. Then, we report user perception of different conditions, e.g., their confidence level of annotated data and the perceived cognitive load."}, {"title": "7.1 Annotation Speed", "content": "Since each session has a fixed annotation time period, we use the average number of annotated tasks to represent the annotation speed. We compare the number of tasks completed across three conditions: Manual, AI assistant, and SQLSYNTH. Table 1 presents the average annotation count completed within 5 minutes of the task session for each condition. When using SQLSYNTH, participants annotated the most tasks (Mean = 8.75, SD = 2.74), followed by"}, {"title": "7.2 Annotation Quality", "content": "We define the quality of a text-to-SQL dataset into (1) Correctness (whether there is a syntax error, and whether the NL question matches the SQL query), (2) Naturalness (whether the NL question is natural enough as a human daily question), and (2) Diversity (whether the dataset has comprehensive coverage of different entities and query types, without any biases).\n7.2.1 Correctness. To evaluate the correctness of participants' annotations, we manually review all data collected during the user study. We evaluate two types of errors in the annotated data. First, we look for SQL syntax errors or the misuse of entities and references in the database schema. We identify this type of error by executing the SQL query in a sandbox database that is adequately populated from the schema. The error leads to execution failures or the return of an empty result. Second, we evaluate the equivalence between the SQL query and the NL question. While the SQL might be syntactically correct on the schema, it may not accurately represent the intent of the NL question. In this case, we manually evaluate their equivalence. Table 2 shows the two types of error rates and the overall accuracy for each condition.\nWe observe different reasons for errors in manual annotation compared to using the AI assistant. During manual annotation, participants who were less proficient in SQL often made grammatical mistakes (29.24%). However, since they tended to write simple SQL queries, the equivalence error was less (5.15%). The AI assistant, ChatGPT, rarely introduces syntax errors. However, it tends to generate more complex SQL queries (e.g., multiple JOINs) than manual annotation. Despite participant refinement, these queries often fail to match the complex schema due to hallucination, leading to SQL execution errors (18.73%). Moreover, these complex queries pose greater challenges in maintaining equivalence with the NL question, resulting in the highest error rate of equivalence ((8.34%).\nCompared to manual annotation and using the AI assistant, using SQLSYNTH achieves the highest accuracy (95.56%). For SQL"}, {"title": "7.2.2 Naturalness.", "content": "In addition to correctness, the naturalness of the NL question is crucial for the quality of text-to-SQL data. While an NL question may accurately match its SQL query, it might be verbose. In the real world, people tend to ask concise questions that follow certain natural language patterns. To evaluate naturalness, we first calculate the Flesch-Kincaid Readability Score [11], an automatic metric measuring text readability on a scale from 0 to 100. To better assess naturalness, we further manually rate all annotated questions from 1 to 7 after masking the conditions.\u00b2"}, {"title": "7.2.3 Diversity.", "content": "To evaluate the diversity and potential biases in the annotated dataset, we analyze the composition of participant-annotated data across four dimensions, including the number of clauses, columns, tables, and values involved. We measure the diversity of each dimension using Simpson's Diversity Index [68], which is used to quantify the level of heterogeneity of a certain property.\nTable 5 shows the diversity and mean values for each method and dataset property. SQLSYNTH demonstrates better diversity in the"}, {"title": "7.3 User Cognitive Load & Usability Rating", "content": "Reducing cognitive load during data annotation is crucial, since it directly makes this process more cost-effective. Figure 8 illustrates participants' ratings on the five cognitive load factors from the NASA TLX questionnaire. The ANOVA test reveals statistically significant (\u03b1 = 0.001) differences in means for all factors: Mental Demand (p = 6.7 \u00d7 10\u22125), Temporal Demand (p = 6.0 \u00d7 10\u22126), Performance (p = 1.9\u00d710\u22125), Effort (p = 1.5\u00d710\u22125), and Frustration (p = 1.3 \u00d7 10-4).\nThe result demonstrates that SQLSYNTH can significantly reduce users' cognitive load compared to manual annotation and using"}, {"title": "8 DISCUSSION", "content": "8.1 Design Implications\nSQLSYNTH demonstrates significant performance improvements compared to manual annotation and using ChatGPT. Based on participants' feedback, we attribute the improvement to the four key design ideas of SQLSYNTH: (1) Structured workflow, (2) Data suggestion, (3) Error detection and repair, and (4) Data visualization.\nFirst, manual annotation and AI-assisted approaches lack structure and standards. In contrast, SQLSYNTH structures the workflow into concrete steps and provides clear guidance (e.g., the next component blinks in the UI). P11 wrote, \u201cOverall, the tool is extremely useful and facilitates the annotation, which is an admittedly open task. It adds structure to an unstructured task.\" Second, the grammar-based suggested SQL queries and LLM-based NL questions serve as unbiased, reliable starting points for annotation. It enables annotators to avoid the most challenging and biased step. Instead of creating data from scratch, annotators only need to refine the suggested ones. P3 wrote, \u201cEven though ChatGPT can give me plenty of data, I need to manually check its output. I feel connecting its output SQL to the schema is challenging given the limited time.", "It is useful for generating synthetic question and SQL pairs.\" P1 wrote, \u201cThis is almost like someone gives you a draft that you can just revise vs. gives you an empty doc for you to start from scratch in writing.\u201d Third, SQLSYNTH highlights potential errors according to the misalignment between SQL and NL, saving effort on error checking and correction. P6 wrote, \u201cI could see how the alignment feature could be useful for complex queries.\" SQLSYNTH then provides suggestions to fix the misalignments, which further reduces error correction efforts. P8 wrote, \u201cIt is helpful to edit by simply clicking buttons.": "hese two features effectively incorporate human knowledge into the system, providing a straightforward way for humans to collaborate with the LLM. Fourth, SQLSYNTH reduces mental effort in understanding complex database schemas and dataset composition through visualization. The visualization helps improve human control throughout the annotation. P2 wrote, \u201cI really like the schema page, I think it helps visualize the structure of the table"}, {"title": "8.2 Limitation and Future Work", "content": "While SQLSYNTH shows significant effectiveness in text-to-SQL data annotation, participants suggested several improvements for future work. P1 and P7 pointed out the suggested NL question may be inaccurate when the SQL query is complex, which requires a more advanced SQL-to-text approach in the backend. P7 mentioned that generating the suggested NL question can take seconds and hopes for optimization of this generation time. We believe this can be optimized by generating subsequent SQL queries and NL questions in the backend while users process previous data. Users will not have to wait for the system to dynamically generate suggested data. P3 and P11 suggested that adding a search function for entity names within complex schema visualizations would be helpful. To further increase controllability, P4 suggested adding options to control the properties in generated SQL rather than making it completely random. We think all of these suggestions are valuable for future improvements in SQLSYNTH."}, {"title": "9 CONCLUSION", "content": "This paper presents SQLSYNTH, a novel interactive text-to-SQL annotation system that enables users to create high-quality, schema-specific text-to-SQL datasets. SQLSYNTH integrates multiple functionalities, including schema customization, database synthesis, query alignment, dataset analysis, and additional features such as confidence scoring. A user study with 12 participants demonstrates that by combining these features, SQLsYNTH significantly reduces annotation time while improving the quality of the annotated data. SQLSYNTH effectively bridges the gap resulting from insufficient training and evaluation datasets for new or unexplored schemas."}, {"title": "A ADDITIONAL USER STUDY: DATABASE CUSTOMIZATION", "content": "The text-to-SQL data annotation process can be divided into two stages: (1) database schema customization and (2) text-to-SQL data annotation based on a provided schema. As the core contribution of SQLSYNTH, we primarily focused on evaluating the text-to-SQL data annotation stage in the main study. In this section, we reported the evaluation of database schema customization as an additional study. The participants and conditions were the same as the main study.\nFor the results, we began by analyzing participants' schema customization performance across different conditions, focusing on both accuracy and speed. Then, we reported user perceptions of the various conditions, including their confidence and cognitive load."}, {"title": "A.1 Tasks", "content": "To assess schema customization performance, we created a pool of schema editing tasks. For each sampled schema, we manually created 30 tasks requiring edits over the"}]}