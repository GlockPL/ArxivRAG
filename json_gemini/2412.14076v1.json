{"title": "Compositional Generalization Across Distributional Shifts with Sparse Tree Operations", "authors": ["Paul Soulos", "Henry Conklin", "Mattia Opper", "Paul Smolensky", "Jianfeng Gao", "Roland Fernandez"], "abstract": "Neural networks continue to struggle with compositional generalization, and this issue is exacerbated by a lack of massive pre-training. One successful approach for developing neural systems which exhibit human-like compositional generalization is hybrid neurosymbolic techniques. However, these techniques run into the core issues that plague symbolic approaches to AI: scalability and flexibility. The reason for this failure is that at their core, hybrid neurosymbolic models perform symbolic computation and relegate the scalable and flexible neural computation to parameterizing a symbolic system. We investigate a unified neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation. We extend a unified neurosymbolic architecture called the Differentiable Tree Machine in two central ways. First, we significantly increase the model's efficiency through the use of sparse vector representations of symbolic structures. Second, we enable its application beyond the restricted set of tree2tree problems to the more general class of seq2seq problems. The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.", "sections": [{"title": "Introduction", "content": "Deep learning models achieve remarkable performance across a broad range of natural language tasks [71], despite having difficulty generalizing outside of their training data, struggling with new words [36], known words in new contexts [29], and novel syntactic structures, like longer sequences with greater recursive depth [30, 39]. Increasingly this problem is addressed through data augmentation, which tries to make it less likely a model will encounter something unlike what it sees during training - reducing the degree by which it has to generalize [1, 15, 26]. However, even models trained on vast quantities of data struggle when evaluated on examples unlike those seen during training [32]. This stands in contrast to how humans process language, which enables robust generalization [51]. By breaking novel sentences into known parts, we can readily interpret phrases and constructions that we have never encountered before (e.g. 'At the airport I smiled myself an upgrade', [22]). Why do models trained on orders of magnitude more language data than a human hears in 200 lifetimes [25] still fail to acquire some of language's most essential properties?\nCentral to language's generalizability is compositional structure [49] where contentful units, like words, fit together in a structure, like a syntactic tree. Many classical approaches in NLP and Machine Learning attempt to induce a grammar from data in the hope of leveraging the same kinds of generalization seen in natural language [e.g. 34, 33, 68]. However, structured representations are not first-order primitives in most neural networks [44, 63]. Despite theoretical appeal, the strictures of purely discrete symbolic approaches have made them difficult to apply to the breadth of tasks and domains where deep learning models have proven successful [18]. In contrast, purely connectionist models like Transformers [71] - struggle with the kinds of sample efficiency and robust generalization ubiquitous to human learning.\nNeurosymbolic methods attempt to integrate neural and symbolic techniques to arrive at a system that is both compositional and flexible [4, 19, 20, 63]. While some neurosymbolic architectures achieve impressive compositional generalization, they are often brittle due to the symbolic core of their computation [58]. These methods are hybrid neurosymbolic systems, where the primary computation is symbolic, and the neural network serves to parameterize the symbolic space. We take a different approach, one where symbolic operations happen in vector space. In our system, neural and symbolic computations are unified into a single space; we multiply and add vector-embedded symbolic structures instead of multiplying and adding individual neurons.\nWe introduce a new technique for representing trees in vector space called Sparse Coordinate Trees (SCT). SCT allows us to perform structural operations: transformations which change the structure of an object without changing the content. This is a crucial aspect of compositionality, where the structure and content can be transformed independently. We extend the Differentiable Tree Machine (DTM), a system which operates over binary trees in vector space, into the Sparse Differentiable Tree Machine (sDTM) to improve performance and applicability to a larger variety of tasks\u00b2. While DTM processes vector-embedded binary trees as the primitive unit of computation, the order of operations and argument selection is governed by a Transformer. We present results showing that this unified approach retains many of the desirable properties of more brittle symbolic models with regards to generalization, while remaining flexible enough to work across a far wider set of tasks. While fully neural architectures or hybrid neurosymbolic techniques excel at certain types of generalization, we find that DTM, with its unified approach, excels across the widest array of shifts.\nThe main contributions from this paper are:\n\u2022 Sparse Coordinate Trees (SCT), a method for representing binary trees in vector space as sparse tensors. (\u00a73.1)\n\u2022 Bit-Shift Operating - systematic and parallelized tree operations for SCT. (\u00a73.2)\n\u2022 The introduction of Sparse Differentiable Tree Machine (sDTM), architectural improvements to the DTM to leverage SCT and drastically reduce parameter and memory usage. (\u00a74)\n\u2022 Techniques to apply DTM to seq2seq tasks by converting sequences into trees. (\u00a74.5)\n\u2022 Empirical comparisons between sDTM and various baselines showing sDTM's strong generalization across a wide variety of tasks. (\u00a75)"}, {"title": "Related Work", "content": "Work leveraging the generalizability of tree structures has a long history across Computer Science, Linguistics, and Cognitive Science [9, 45, 57, 64, 68]. Much of classical NLP aims to extract structured representations from text like constituency or dependency parses [for overview: 13, 42]. More recent work has shown the representations learned by sequence-to-sequence models without structural supervision can recover constituency, dependency, and part of speech information from latent representations in machine translation and language models [3, 6]. While those analyses show"}, {"title": "Differentiable Tree Operations Over Sparse Coordinate Trees", "content": "Representing trees in vector space enables us to perform differentiable structural operations on them. Soulos et al. [67] used Tensor Product Representations (TPRs) [64] for this purpose. TPRs use the tensor (or outer) product to represent trees in vector space (\u00a7A.1). Use of an outer product leads to a representation dimensionality that is multiplicative with both the embedding dimensionality and the number of possible tree nodes. Additionally, the number of nodes is itself an exponential function of the supported depth. This makes TPRs difficult to use in practice, given available memory is quickly exceeded as tree depth increases.\nIn this section, we introduce Sparse Coordinate Trees (SCT), a new schema for representing trees in vector space. We then define a library of parallelized tree operations and how to perform these operations on SCT."}, {"title": "Sparse Coordinate Trees (SCT)", "content": "Like TPRs, we want an encoding for trees that factorizes the representation into subspaces for structure and content respectively. This approach to representational spaces differs from models like an RNN and Transformer, which represent structure and content jointly in an unfactorized manner. By separating the structure and content subspaces a priori, we can operate over these two spaces independently. This decision is motivated by the fact that distinct treatment of these spaces is an essential aspect of compositionality.\nWe derive our tree representation scheme from the sparse coordinate list (COO) format. COO stores tensor data in tuples of (indices [integers], values [any format], size [integers]). The indices are N-dimensional to simulate a tensor of arbitrary shape (e.g. including dimensions such as batch or length). When an index is not indicated in indices, it is assumed that the corresponding value is 0.\nWe give structural meaning to COO representations by defining one dimension of indices as the tree position occupied by a value vector. Our tree addressing scheme is based on Gorn addresses [23]: to get the tree position from an index, convert the index to binary and read from right to left. A left-branch is indicated by a 0 and a right branch by a 1. To distinguish between leading 0s and left-branches (e.g. 010 vs 10), we start our addressing scheme at 1 instead of 0. This indicates that all 0s to the left of the most-significant 1 are unfilled and not left-branches. Section 5.2 discusses the performance, memory, and parameter comparison between DTM models which use standard TPRs and SCT."}, {"title": "Differentiable Tree Operations", "content": "To operate on the trees defined in the previous section, we need a set of functions. We use a small library of only three: left-child (left), right-child (right), and construct (cons) a new tree from a left and right subtree.3 Although these three functions are simple, along with the control operations of conditional branching and equality-checking, these five functions are Turing complete [45].\nIn addition to saving memory, SCT also provides a more efficient method for performing differentiable tree operations. The operations defined in Soulos et al. [67] require precomputing, storing, and applying linear transformations for left, right, and cons. Since our values and tree positional indices are kept separate, we can compute the results of left, right, and cons dramatically more efficiently using indexing, bit-shifts, and addition."}, {"title": "The Sparse Differentiable Tree Machine (sDTM)", "content": "Our work extends the Differentiable Tree Machine (DTM) introduced in Soulos et al. [67] with the Sparse Differentiable Tree Machine (sDTM). While similar to the original at a computational level, sDTM represents a different implementation of these concepts that make it dramatically more parameter and memory efficient. We also introduce techniques to apply sDTM to tasks with sequence input and output (seq2seq)."}, {"title": "Overview", "content": "SDTM uses our Sparse Coordinate Trees schema across its components. Like the original DTM, our model is comprised of an agent, interpreter, and memory. The Interpreter performs Equation 1 by applying the bit-shifting tree operations from Section 3.2 and weighting the result. The output from the interpreter is written to the next available memory slot, and the last memory slot is taken as the output.\nThe Agent is a Transformer encoder that takes an encoding of the memory as input and produces the inputs for Equation 1: \u0e1e\u0e35, T, and s. Two special tokens, <OP> and <ROOT>, are fed into the Agent to represent w and s. Each time a tree is written to memory, a fixed-dimensional encoding of that tree is produced and fed as a new token to the agent (\u00a74.2). The agent soft-selects tree arguments for the interpreter, T, by performing a weighted sum over the trees in memory.\nThe agent which implicitly parameterizes the conditional branching and control flow of the program is modeled by a Transformer, and it is possible for sDTM to face some of the generalization pitfalls that plague Transformers. The design of sDTM encourages compositional generalization through differentiable programs, but it does not strictly enforce the constraints of classical symbolic programs. As the results in Section 5 show, sDTM can learn generalizable solutions to some tasks despite the presence of a Transformer, but on some other tasks the issues with generalization are still present."}, {"title": "Pooling by attention", "content": "Each tree in memory needs to have a fixed-dimensional encoding to feed into the agent regardless of how many nodes are filled. Commonly this is done via pooling, like taking the means of the elements in the tree, or a linear transformation in the case of the original DTM. Instead, we use Pooling by Multi-headed Attention (PMA) [38], which performs a weighted sum over the elements, where the weight is derived based on query-key attention.\nAttention is permutation invariant to the ordering of key and value vectors, but it is important that our pooling considers tree position information. To enforce this, we convert the position indices to their binary vector representation b. This leads to an asymmetrical vector with only positive values, so instead we represent left branches as -1 and keep right branches as +1. The input to our pooling function is the concatenation of this positional encoding b with the token embedding 7 at that position: [2; 6].\nUnlike standard self attention, we use a separate learnable parameter for our query vector a\u2208 Rnum_heads\u00d7key_dim. We pass [2; 6] through linear transformations to generate keys ke [Rnum_heads\u00d7key_dim and values \u0e1b\u0e35 \u2208 Rnum_heads\u00d7value_dim. The result of this computation is always z\u2208 Rnum_heads \u00d7 value_dim given that \u1fb7 is fixed and does not depend on the input. The rest of the computation is identical to a Transformer with pre-layer normalization [74]."}, {"title": "Tree Pruning", "content": "While Sparse Coordinate Trees mean that trees with fewer filled nodes take up less memory, the way our model blends operations results in trees becoming dense. The interpreter returns a blend of all three operations at each step, including the cons operation which increases the size of the representation by combining two trees. In practice even as the entropy of the blending distribution drops, the probability of any operation never becomes fully 0. This means that over many steps, trees start to become dense due to repeated use of cons. In order to keep our trees sparse, we use pruning: only keeping the top-k nodes as measured by magnitude. k is a hyper-parameter that can be set along with the batch size depending on available memory."}, {"title": "Lexical Regularization", "content": "To aid lexical generalization, we add noise to our token embeddings. Before feeding an embedded batch into the model, we sample from a multi-variate standard normal for each position in each tree, adding the noise to the embeddings as a form of regularization [5]."}, {"title": "Handling Sequential Inputs and Outputs", "content": "seq2tree The original DTM can only be applied to tasks where a tree structure is known for both inputs and outputs. Here we provide an extension to allow DTM to process sequence inputs. To do this we treat each input token as a tree with only the root node occupied by the token embedding. We then initialize the tree memory with N trees, one for each token in the input sequence. The agent's attention mechanism is permutation-invariant, so in order to distinguish between two sequences which contain the same tokens but in different orders, we apply random sinusoidal positional encodings to the first N tokens passed to the agent [40, 55]. Random positional encodings sample a set of increasing integers from left-to-right instead of assigning a fixed position to each token. The purpose of left and right is to extract subtrees. Since in our seq2tree setting the input sequence is processed in a completely bottom-up manner, we restrict the agent and interpreter to only have a single operation: cons. Use of a single operation to construct new trees from subtrees aligns the DTM theoretically with the Minimalist Program [10], which addresses natural language's compositionality in terms of a single operation: merge.\nseq2seq To handle sequence inputs and outputs we convert the output sequence to a tree. One method to convert the output sequence into a tree is to use a parser. Alternatively, when a parser is not available, we can embed a sequence as the left-aligned leaves at uniform depth (LAUD). We insert a special token <EOB> to signify the end of a branch, similar to an <EOS> token."}, {"title": "Results", "content": "We consider models that are trained from scratch on the datasets they're evaluated on; while the compositional capabilities of large pre-trained models are under active debate [32], we are interested in the compositional abilities of the underlying architecture rather than those that may result from a pre-training objective. We compare sDTM to two fully neural models, a standard Transformer [71] as well as a relative universal Transformer (RU-Transformer) which was previously shown to improve systematic generalization on a variety of tasks [12]. We also compare our model to a hybrid neurosymbolic system, NQG [58, 69], a model which uses a neural network to learn a quasi-synchronous context-free grammar [62]. NQG was introduced alongside NQG-T5, which is a modular system that uses NQG when the grammar produces an answer and falls back to a fine-tuned large language model T5 [53]. As mentioned at the beginning of this section, we only compare to NQG in this paper since we want to evaluate models that have not undergone significant pre-training.\nFor each task, we test whether models generalize to samples drawn from various data distributions. Independent and identically distributed (IID) samples are drawn from a distribution shared with training data. We evaluate several out-of-distribution (OOD) shifts. One-shot lexical samples, while drawn from the same distribution as the training data, contain a word that was only seen in a single training sample. Similarly, Zero-shot lexical samples are those where the model is not exposed to a word at all during training. Structural/length generalization tests whether models can generalize to longer sequences (length) or nodes not encountered during training (structural). Template generalization withholds an abstract n-gram sequence during training, and then each test sample fits the template. Finally, maximum compound divergence (MCD) generates train and test sets with identical uni-gram distributions but maximally divergent n-grams frequencies [29]. Although models are often tested on a single type of generalization, we believe evaluating a model across a broad array of distributional shifts is essential for characterizing the robustness of its generalization performance."}, {"title": "Performance Regression (Active Logical)", "content": "Active Logical is a tree2tree task containing input and output trees in active voice and logical forms [67]. Transforming a tree in active voice to its logical form simulates semantic parsing, and transforming a logical form tree to active voice simulates natural language generation. For this dataset, there are three test sets: IID, 0-shot lexical, and structural. In addition to the baselines listed in the previous section, we also compare our modified SDTM to the original DTM. This enables us to confirm that our proposed changes to decrease parameter count and memory usage while increasing inference speed does not lead to a performance regression."}, {"title": "Scalability (FOR2LAM)", "content": "FOR2LAM is a tree2tree program translation task to translate an abstract syntax tree (AST) in an imperative language (FOR) to an AST in a functional language (LAM) [7]. Due to the depth of the trees in this dataset, DTM is unable to fit a batch size of 1 into memory. This makes FOR2LAM a good dataset to test the scalability of sDTM to more complex samples. We augment the FOR2LAM dataset with a 0-shot lexical test set. During training, only two variable names appear: 'x' and 'y'. For the 0-shot test, we replace all occurrences of x in the test set with a new token 'z'. We are unable to test DTM on FOR2LAM because a batch size of 1 does not fit into memory due to the depth of the trees in the dataset.\nResults on FOR2LAM are shown . All other models do well on the in-distribution test set, but only DTM is able to achieve substantive accuracy on the 0-shot lexical test. DTM's performance is impressive given work on data augmentation has shown the difficulty of few-shot generalization is inversely proportional to vocabulary size [50], with smaller vocabulary tasks being more challenging. This O-shot challenge is from 2 variables (x, y) to 3 (x, y, z), making it difficult enough that both transformer variants score 3%."}, {"title": "Seq2Tree (GeoQuery)", "content": "GeoQuery is a natural language to SQL dataset [77] where a model needs to map a question stated in natural language to a correctly formatted SQL query, including parentheses to mark functions and arguments. We use the parentheses and function argument relationship as the tree structure for our output. In this format, GeoQuery is a seq2tree task, and we follow the description from Section 4.5. We use the same preprocessing and data as Shaw et al. [58]. Like FOR2LAM, we are unable to test DTM on GeoQuery because a batch size of 1 does not fit into memory due to the depth of the trees in the dataset.\nResults for GeoQuery are shown . This is the most difficult task that we test because of the small training set, and the natural language input is not draw from a synthetic grammar. Given this, a potential symbolic solution to this task might be quite complex. We find that both NQG and DTM perform worse than the two Transformer variants on the IID test set. This also holds true for the Template split, where Transformers outperform the neurosymbolic models. On the Length and TMCD splits, all of the baselines achieve roughly the same performance while DTM performs slightly worse - the degree of variation in the input space and small training set appear to make it difficult for sDTM to find a compositional solution.\nIt is worth noting that there is substantial room for improvement across every model on GeoQuery. The small dataset with high variation poses a problem for both compositional methods of sDTM and NQG."}, {"title": "Seq2Seq (SCAN)", "content": "SCAN is a synthetic seq2seq task with training and test variations to examine out-of-distribution generalization [36]. To process seq2seq samples, we follow the description in Section 4.5. We compare two methods for embedding the output sequence into a tree by writing a parser for SCAN's output and comparing this to the left-aligned uniform-depth trees (LAUD). In addition to the standard test splits from SCAN, we introduce a 0-shot lexical test set as well.\nSince the trees in SCAN are not very deep, we are able to compare sDTM to DTM to isolate the effect of pooling by attention (\u00a74.2). We modify the original DTM to handle sequential inputs and outputs as described in Section 4.5. Replacing the linear transformation in DTM with pooling by attention in sDTM leads to drastically better results; DTM is unable to perform well even on the simple IID split, whereas sDTM performs well across many of the splits.\nAll baselines perform well on the IID test set, showing that they have learned the training distribution well. Transformer variants perform poorly on lexical, length, and MCD splits. The Transformers and sDTM perform well on the Template split while NQG completely fails."}, {"title": "Conclusions", "content": "We introduced the Sparse Differentiable Tree Machine (sDTM) and a novel schema for efficiently representing trees in vector space: Sparse Coordinate Trees (SCT). Unlike the fully neural and hybrid neurosymbolic baselines presented here, sDTM takes a unified approach whereby symbolic operations occur in vector space. While not perfect \u2014 sDTM struggles with MCD and Template shifts, as well as the extremely small GeoQuery dataset the model generalizes robustly across the widest variety of distributional shifts. sDTM is also uniquely capable of zero-shot lexical generalization, likely enabled by its factorization of content and structure.\nWhile these capacities for generalization are shared with the original DTM, our instantiation is computationally efficient (representing a 75x reduction in parameters) and can be applied to seq2seq, seq2tree, and tree2tree tasks. Our work reaffirms the ability of neurosymbolic approaches to bridge the flexibility of connectionist models with the generalization of symbolic systems. We believe continued focus on efficient neurosymbolic implementations can lead to architectures with the kinds of robust generalization, scalability, and flexibility characteristic of human intelligence."}, {"title": "Appendix", "content": "A.1 Sparse Coordinate Trees as Tensor Product Representations\nThis section shows that Sparse Coordinate Trees is the same as a TPR with the constraint that the role basis is the standard basis. TPRs define structural positions as role vectors $r_i \\in \\mathbb{R}^{d_r}$, and the content that fills these positions is defined by filler vectors $f_i \\in \\mathbb{R}^{d_f}$. For a particular role and filler pair, the filler $f_i$ is bound to the role $r_i$ using the tensor/outer product: $f_i \\otimes r_i \\in \\mathbb{R}^{d_f \\times d_r}$. The representation of an entire structure is the sum over all N individual filler-role pairs: $T = \\sum_{i=1}^{N} f_i \\otimes r_i \\in \\mathbb{R}^{d_f \\times d_r}$.\nAs shown in the previous two equations, the dimensionality of a single filler-role pair is equal to the dimensionality of an entire structure: both have dimensionality $\\mathbb{R}^{d_f \\times d_r}$. This means that a tree with only a filled root node takes up the same memory as a dense tree with every node filled. An important requirement for TPRs is that the role vectors must be linearly independent; this ensures that a filler can be unbound from a role without introducing noise using the inner product: $f_j = T r_j^*$, where ${r_i^*}$ is the basis dual to ${r_i}_{i}$. Previous work typically used randomly initialized and frozen orthonormal vectors to define the role basis. By defining our role vectors in a sparse manner as opposed to random initialization, we can greatly reduce the memory used by TPRs.\nClassic symbolic data structures grow in memory linearly with the number of filled positions. It is possible to replicate this behavior with TPRs by defining the role vectors to be the standard one-hot basis, which is orthonormal by definition. The i-th element of role vector $r_i$ is 1, and the other elements are 0. When a filler and role vector are both dense, the resulting bound vector is also dense. When the role vector is one-hot, the resulting bound vector is 0 everywhere except for column i which corresponds to the value 1 in $r_i$. By using a sparse tensor representation that only keeps track of dimensions that are not equal to 0, we can reduce the memory usage of TPRs to linear growth that scales with the number of filled positions, like a classical symbolic data structure. This however forgoes a motivating desideratum for the design of TPRs, that roles (and not just fillers) have similarity relations that support generalization across structural positions.\nWe can additionally improve the efficiency by refraining from performing the outer product. Since we are not performing a tensor product, this technique is only implicitly a Tensor Product Representation. Instead, we can keep the filler and role vectors in two aligned lists. A filler is bound to a role by sharing an index in our aligned lists. This is equivalent to the binding and unbinding from classical dense TPRs without having to perform multiplication.\nSince we are not performing an outer product, instead of storing sparse role vectors, we can simply store a role integer, where the integer corresponds to the one-hot dimension. We derive a tree addressing scheme based on Gorn addresses . In our scheme, addresses are read from right to left, giving the path from the root where a left-branch is indicated by a 0 and a right-branch is indicated by a 1. We need a way to distinguish between leading 0s and left-branches (e.g., 010 vs. 10), so we start our addressing scheme at 1 instead of 0. This indicates that all 0s to the left of the left-most 1 are unfilled and not left-branches; the left-most 1 and all preceding 0s are ignored when decoding the path-from-root.\nA.2 Agent Figure\nSee \nA.3 Lexical Regularization Ablation\nTo see the importance of adding noise to our input embeddings as defined in Section 4.4, we show the performance of sDTM with and without this regularization"}]}