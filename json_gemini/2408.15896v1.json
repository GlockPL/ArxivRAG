{"title": "A New Method for Cross-Lingual-based Semantic Role Labeling", "authors": ["Mohammad Ebrahimi", "Behrouz Minaei Bidgoli", "Nasim Khozouei"], "abstract": "Semantic role labeling is a crucial task in natural language processing, enabling better comprehension of natural language. However, the lack of annotated data in multiple languages has posed a challenge for researchers. To address this, a deep learning algorithm based on model transfer has been proposed. The algorithm utilizes a dataset consisting of the English portion of CoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency of training, only ten percent of the educational data from each language is used. The results of the proposed model demonstrate significant improvements compared to Niksirt et al.'s model. In monolingual mode, the proposed model achieved a 2.05% improvement on F1-score, while in cross-lingual mode, the improvement was even more substantial, reaching 6.23%. Worth noting is that the compared model only trained two of the four stages of semantic role labeling and employed golden data for the remaining two stages. This suggests that the actual superiority of the proposed model surpasses the reported numbers by a significant margin. The development of cross-lingual methods for semantic role labeling holds promise, particularly in addressing the scarcity of annotated data for various languages. These advancements pave the way for further research in understanding and processing natural language across different linguistic contexts.", "sections": [{"title": "Introduction", "content": "Semantic role labeling (SRL) is an essential part of natural language processing that aims to assign specific roles to different parts of a sentence, enabling a deeper understanding of language. Semantic role labeling is often described as the process of automatically answering questions such as \"who did what, to whom, when, where, why, and how.\". SRL has multiple applications in natural language processing, including multi-document summarization [1], question answering [2], reading comprehension [3], sentiment analysis [4], and caption generation [5]. Initially, researchers tackled this problem by manually creating various rules and applying them to sentences in a specific language. However, the introduction of traditional machine learning techniques, such as support vector machines and simple Bayes, significantly improved the results in this domain. This approach to solving SRL involved extracting information like syntactic information manually. Researchers then utilized various machine learning algorithms, such as decision trees, support vector machines, and Bayes, to extract predicates, and arguments and categorize semantic roles.\nHowever, while traditional machine learning methods have shown promise, recent advancements in deep learning and neural networks have provided even more accurate and efficient solutions to SRL. These approaches take advantage of large annotated datasets and leverage the power of neural architectures,"}, {"title": "Related Work", "content": "Semantic role labeling models can be categorized as either syntax-aware or syntax-agnostic, based on their utilization of syntactic information. Traditional approaches relied on manually extracted features, such as syntactic information, as sentence features. However, with the development of deep learning models, researchers started relying on automatic feature extraction and initially abandoned the use of syntactic information [15] [16] [17]. Nevertheless, some researchers believe that incorporating syntactic information into the input of deep learning models can enhance their performance. Consequently, using or"}, {"title": "Semantic Role Labeling Styles", "content": "Semantic role labeling is done in two main styles:\n\u2022\tFramenet style: presented in 1998 by Baker et al [21].\n\u2022\tPropbank style presented in 2005 by Palmer et al [22].\nIn the Propbank style, predicates and their arguments are represented using two approaches: spanbased and dependency-based.\n\u2022\tSpan-based semantic role labeling: first presented at the Computer Natural Language Learning conference in 2004 [23] and 2005 [24], utilizes a BIO representation. This means that the labeling is done by specifying the beginning (B) and inside (I) boundaries of the argument spans within the sentence.\n\u2022\tDependency-based semantic role labeling: introduced at the 2008 [25] and 2009 [26] Computational Natural Language Learning conference, captures the relationships between words in a sentence using dependency structures. This style focuses on identifying the roles that words play in relation to the main predicate."}, {"title": "End-to-End Semantic Role Labeling", "content": "The pipeline of semantic role labeling consists of four main stages: predicate identification, predicate sense disambiguation, argument identification, and argument classification. Initial research efforts focused on addressing each stage separately [27] [28] [29]. Recent studies have successfully demonstrated the benefits of tackling some of these sub-tasks jointly through multitask learning [30]. This means that when all four stages are performed by the model together, the algorithm is referred to as end-to-end.\nThe advantage of an end-to-end SRL approach is that it enables the model to capture the dependencies and interactions between different subtasks, leading to better overall performance. By jointly learning these tasks, the model can leverage the shared knowledge and patterns present in the data, enhancing its ability to understand the underlying semantic relationships between predicates and their arguments.\nHowever, it is important to note that implementing an end-to-end SRL system can pose challenges, such as dealing with the increased complexity and potential error propagation across the various subtasks. Nonetheless, current research efforts in multitasking learning have demonstrated promising results, highlighting the potential benefits of integrating these subtasks into a unified framework."}, {"title": "Monolingual Semantic Role Labeling", "content": "Monolingual semantic role labeling refers to models that are trained and applied in a single language, such as English. These models are primarily designed to understand the semantic roles of words and phrases within a sentence in that particular language. While monolingual SRL models are effective for the language they are trained on, they may not generalize well to other languages due to differences in grammar, syntax, and semantics."}, {"title": "Multilingual Semantic Role Labeling", "content": "Multilingual semantic role labeling (SRL) has gained significant attention since the introduction of the CoNLL-2009 multilingual annotated dataset in 2009. The dataset included six languages: Catalan, Spanish, English, Czech, German, and Chinese, and served as a foundation for research in this field. The primary objective of multilingual SRL is to develop language-independent models capable of generating reliable results across different languages. In 2019, He, Li, and Zhao [31] proposed a multi-language model that relied on synthetic information. This approach aimed to bridge the language gap by generating artificial data to improve model performance across multiple languages. However, a year later, Conia and Navigili [11] highlighted a potential limitation of this approach. They argued that syntactic information varies significantly across different languages in terms of morphology and structure, creating obstacles to the effectiveness of multilingual models. To overcome this limitation, Conia and Navigili introduced their multilingual model, which did not rely on syntactic information. By eliminating the use of syntactic features, this model aimed to achieve language independence more effectively. Their approach likely focused on capturing semantic dependencies and role assignments without explicitly considering the specific syntactic characteristics of each language. Overall, the field of multilingual SRL continues to evolve as researchers explore different approaches and techniques to create language-independent models capable of performing well across diverse languages."}, {"title": "Cross-lingual Semantic Role Labeling", "content": "Cross-lingual semantic role labeling (SRL) is a field that aims to overcome the challenge of insufficient training data for building SRL systems. Since semantic role annotations are currently only accessible for a small number of languages, researchers have been focusing on leveraging resources from a source language to reduce the effort and expense associated with creating models or annotations for a new target language. Recent advancements in representation learning have aided this goal. By employing techniques that enable models to understand the underlying semantics of text, researchers can use these models trained on source language data to perform SRL in a target language. This approach helps in overcoming the scarcity of annotated data for the target language. Since most of the research conducted in recent years has been dedicated to Cross-lingual semantic role labeling, we will introduce some of the suggested methods:\nAnnotation Projection: implementing crosslingual semantic role labeling faces the challenge of dissimilar predicate senses and semantic role sets across languages. This issue is evident in datasets like CoNLL-2009, which consists of multiple datasets for various languages such as English, Chinese, Catalan, Spanish, German, and Czech. These datasets often lack alignment as they are developed based on different linguistic theories. Consequently, this misalignment introduces additional challenges for cross-lingual semantic role labeling systems. Annotation projection is a common approach used in natural language processing. It relies on a large parallel text corpus that includes both the source and target languages. In this approach, the source language sentences are first automatically annotated with semantic role labels using a source semantic role labeler. These annotations are then projected onto the target side sentences based on word alignment between the source and target languages. This technique helps in leveraging the existing annotations to generate semantic role labels for the target language without requiring manual annotation [6] [7]. The work by Pado and Lapata [32], and Akbik et al. [33] focused on addressing these issues. They proposed the English PropBank, which consists of a set of predicates and a universal semantic role. This provided a framework for annotating semantic roles in English sentences.To extend this annotation to non- English sentences, they employed word alignment techniques commonly"}, {"title": "Model transfer:", "content": "Model transfer is a powerful approach in natural language processing that enables the application of a source language model to a new language [11] [12] [13] [14]. This involves modifying a source language model so that it can be directly applied to the target language. One way to achieve model transfer is by building multilingual models that rely on language-independent features, such as cross-lingual word representations and universal part-of-speech labels. These features can be transferred directly to target languages, allowing the model to work effectively across different languages. Multi-task learning can also be utilized within this approach, where the model is simultaneously trained on several languages. In the case of semantic role labeling, multi-task learning can be applied to parallel, translated, and aligned corpora. For example, the universal Propbank corpus can be utilized for this purpose. However, Conia and Navigili [14] have proposed a model that doesn't rely on parallelism or the uniformity of semantic role labels in different languages. Instead, their model employs universal encoders to learn a unified representation that can be shared across multiple languages. Overall, model transfer is a promising approach for building multilingual models that can be directly applied to new languages, leveraging language-independent features and multi-task learning techniques.\nThe proposed model belongs to the last approach type, known as model transfer. It utilizes two universal sentence encoders and universal predicateargument encoders that are language-independent. Multi-task"}, {"title": "Model", "content": "This model is capable of simultaneously handling predicate identification, predicate sense disambiguation, argument identification, and argument classification, so it falls under the extensive group of end-to-end systems; The model's architecture can be roughly partitioned into the following elements:\n\u2022\tA universal sentence encoder that shares its parameters among languages. It generates word encodings that capture information related to predicates.\n\u2022\tA universal predicate-argument encoder that also shares its parameters among languages. It models the relationships between predicates and arguments.\n\u2022\tA set of language-specific decoders that perform the following tasks: determining whether words are predicates or not, selecting the most appropriate sense for each predicate, and assigning a semantic role to each predicate-argument pair. These decoders consider various semantic role labeling inventories.\nThis model stands out from previous studies by eliminating the need for cross-source mapping, word alignment techniques, translation tools, other annotation transfer techniques, or parallel data in order to achieve accurate semantic role labeling. It achieves this solely through cross-linguistic implicit knowledge transfer. Figure 1 shows the architecture of the proposed model."}, {"title": "Input Representation", "content": "The input representation in pre-trained linguistic models like ELMo, BERT, and XLMROBERTa has become the standard due to their capability to encode extensive knowledge. Recent research, such as Hewitt and Manning [36] and Conia and Navigli [13], has shown that distinct levels of a language model encompass diverse syntactic and semantic characteristics. In this model, we join the hidden states from the top four inner layers of the language model to construct a textual representation for each input word."}, {"title": "In Figure 2, we illustrate the architecture of a 12- layer BERT model. For us, the word embedding is obtained by concatenating the hidden states from layers 9, 10, 11, and 12, as shown in Figure 3.", "content": "W = (w0,w1, . . . ,wi, . . . ,wn\u22121), a specific sentence\nh = \\mathbb{l}^k(wi|w), hidden state, where \\mathbb{l}^k represents the internal computation in the language model with K layers\nWe compute the encoding model ei as follows:\nh\u2081 = h\u00ec\u2295h-1h-2h-3\nei = Swish(W\u2122hi + b\u2122)"}, {"title": "xy = Concatenation of two vectors x and y.", "content": "The selection of an activation function is indeed crucial for the dynamics of training and performance in deep neural networks. While ReLU (f(x) = max(0,x)) has been widely successful, alternative activation functions have struggled to consistently outperform it. However, the Google Brain team has proposed a new activation function called Swish (f(x) = x \u00b7 sigmoid(x)).\nSwish offers a smoother slope compared to the older ReLU function [37]. This smoothness helps in addressing some of the issues associated with ReLU, such as the \"dying ReLU\" problem. Swish function's non-uniformity allows it to handle a wider range of data and adapt better to different types of inputs. The diagram in Figure 4 illustrates the Swish activation function visually. The Google Brain team believes that Swish could be a promising alternative to ReLU and may provide additional benefits in certain scenarios."}, {"title": "Multilingual BERT:", "content": "Multilingual BERT is a pretrained language model that encompasses 104 different languages. It has been trained extensively on a wide range of multilingual data derived from Wikipedia The training process follows a selfsupervised approach, wherein the model is fed with raw texts and generates inputs and labels automatically, without the need for human annotations. This allows the model to utilize publicly available data effectively.\nBERT is trained on two primary tasks: The first is Masked Linguistic Modeling (MLM), where a sentence is randomly masked by hiding 15% of its words. The model then attempts to predict the hidden words by processing the entire masked sentence. This differs from traditional models like Recurrent Neural Networks (RNNs) or GPT, which usually see words in sequence or hide tokens in subsequent steps. MLM enables BERT to learn a bidirectional representation of the sentence. The second task is Next Sentence Prediction (NSP), where pairs of masked sentences are provided as input during pre-training. These sentence pairs may or may not be adjacent in the original text. The objective is for the model to predict whether the two sentences were originally sequential or not. By undergoing these two training tasks, BERT gains a comprehensive understanding of the linguistic structure and context in various"}, {"title": "XLM-ROBERTa:", "content": "XLM-ROBERTa [38] is a variant of RoBERTa that has been pre-trained on a massive amount of multilingual data. It has received specialized training using 2.5 terabytes of meticulously processed CommonCrawl data, which includes text from 100 different languages. RoBERTa itself is a transformer-based model that has been pre-trained in a self-supervised manner. When we say \"self-supervised,\" it means that the model has been trained only on raw texts without any human labels. Instead, an automatic process is used to generate inputs and labels from these texts. This approach allows the model to utilize publicly available data and doesn't require human annotation. RoBERTa underwent training through the masked language modeling task, wherein specific words within the input text were masked, and the model made predictions for these masked words. This process helps the model learn an internal representation of 100 languages, enabling it to extract useful features for downstream tasks. For example, if you have a dataset of labeled sentences, you can use the features generated by XLM-ROBERTa as input to a standard classifier during fine-tuning. While XLMROBERTa can be used for masked language modeling, its primary purpose is to be fine-tuned for specific downstream tasks.\nAmong the two pre-trained models mentioned above, we opted to use XLM-RoBERTa. This choice was made based on its superior accuracy in various tasks compared to Multilingual BERT. The key differentiating factor is that XLM-ROBERTa utilizes RoBERTa instead of BERT. To validate our claim, we conducted practical evaluations and found that while these two models exhibit minimal differences with large training datasets, the disparities become more apparent, particularly in cross-lingual scenarios involving the Persian language. As a result, we proceeded with conducting our experiments exclusively using XLM-ROBERTa."}, {"title": "Universal Sentence Encoder", "content": "Building upon Fillmore's original intuition [39] regarding the presence of profound semantic relationships between predicates and other constituents within a sentence, Conia proposes the notion that these semantic relations can be preserved across different languages. Guided by this notion, he introduce a universal sentence encoder that utilizes shared parameters between languages. The main goal of this universal sentence encoder is to capture language-independent sentence-level information, such as predicate position and sense, that remains consistent across various languages. This implementation of the universal sentence encoder follows a similar approach to Marchegiani et al. [16], Cai et al. [40], and He et al. [31], employing a stack of Bidirectional LSTM layers. However, he address the issue of gradient vanishing by connecting the output of each layer to its input. In other words, given a sequence of word encodings e = (eo, e1, . . ., ei, . . ., en\u22121), the model calculates a sequence of encodings of time step t as follows:\n\\xi =\\begin{cases} e_i& \\text{if } j = 0 \\\\ t^{(j-1)} \\oplus \\text{BiLSTM}^{(j)}(t^{(j-1)})& \\text{Otherwise} \\end{cases}\nt = (t'_0, t'_1, ..., t'_{n-1})"}, {"title": "The i-th time step of the j-th BiLSTM layer is denoted as BiLSTM(.), while K' represents the total number of stacked layers. At each encoding time step ti, the model initiates by generating a predicate representation p\u2081 to indicate whether the corresponding word wi is a predicate. Furthermore, it produces a sense representation si that captures information about the sense of a predicate at position i.", "content": "pi = Swish(WPti + b\u00b2)\nsi = Swish(W\u00b3ti + b\u00b3)\nThe vector representations obtained for each time step, predicate, and predicate sense are situated within three shared spaces across different languages and formalisms employed for semantic role labeling implementation."}, {"title": "Universal Predicate-Argument Encoder", "content": "In accordance with the design of the universal sentence encoder, this model integrates a universal predicate-argument encoder that shares parameters across different languages. The purpose of this supplementary encoder is to capture the connections between predicate-argument pairs in a language-neutral manner. Like the universal sentence encoder, the universal predicate-argument encoder is constructed using a stack of BiLSTM layers. In more exact terms, when wp symbolizes a predicate within the input sentence w = (W0,W1, . . . ,Wp, . . . ,Wn-1), the model produces a series of encodings for the particular argument of predicate a through the following process:\n\\xi =\\begin{cases} t_p \\oplus t_i& \\text{If } j = 0 \\\\ a^{(j-1)}_i \\oplus \\text{BiLSTM}^{(j)}(a^{(j-1)}_i)& \\text{Otherwise} \\end{cases}\na=(a^{k''}, a^{k''},......a^{k''}_{n-1})\nWithin this context, the model produces a semantic role representation r\u2081 for the word w\u2081 by leveraging the i-th encoded time step t\u2081 from the universal sentence encoder and the overall number of stack layers K\".\nThe initiation of this process involves the encoding of each sentence-specific argument ai.\nr\u2081 = Swish(Wa\u00bf + b\")\nJust like the predicate and sense representations p and s, the semantic role r should make use of cross-linguistic information to abstract language-specific features, given that the predicate-argument encoder is consistent across all languages.."}, {"title": "Language-Specific Decoders", "content": "The encodings for predicates, senses, and semantic roles mentioned earlier are consistent across languages, requiring the model to focus on semantic features rather than surface-level ones like word order, part-of-speech tags, and syntactic rules that can vary between languages. However, the ultimate goal is for our model to offer semantic role annotations based on established predicate-argument structure databases like PropBank, Ancora, or PDTVallex. As a result, the proposed model comprises a series of linear decoders that determine whether a word wi is a predicate, the most suitable sense of the predicate wp, and the semantic role of the word w\u2081 in relation to a specific predicate wp. For each language 1:\n\\text{op}(w_i|l) = W^{p|l}p_i + b^{p|l}\n\\sigma(w_p|l) = W^{s|l}s_i + b^{s|l}\n\\sigma'(w_r|w_p, l) = W^{r|l}r_i + b^{r|l}\nWhile more intricate decoding strategies were an option, linear decoders offer two benefits in our scenario: firstly, they maintain the simplicity of the language model component and promote learning from the model's universal encoders. Secondly, they act as linear explorers, offering valuable insights into the model's ability to acquire cross-lingual knowledge."}, {"title": "Training objective", "content": "The objective of this training is to minimize the overall batch classification errors by training the model in a multitasking manner. This includes reducing cross-entropy in predicate identification, predicate sense disambiguation, and argument identification/classification across all languages. In a more formal manner, taking into account language 1 and the associated predicate identification error (p|l, predicate sense disambiguation error (s|l, and argument identification/classification error (r|l, the total error { can be expressed as:\nL = \\sum_{l \\in L}(L^{p|l} + L^{s\\l} + L^{r|l})\nwhere L is the set of languages\u2014and corresponding formalisms\u2014in the training set."}, {"title": "Fine Tuning", "content": "Fine-tuning involves the precise application of transfer learning techniques. Essentially, it entails adjusting a pre-trained model to suit a related task, thereby optimizing its performance.Assuming that the primary task is similar to the new task, using a predesigned and pre-trained artificial neural network allows us to leverage what the model has previously learned without the need to develop it from scratch.\nBERT, developed by Google, was a paradigm shift in natural language modeling, particularly due to the introduction of pre-training/fine-tuning. After pre-training in an unsupervised manner on a massive amount of textual data, the model can quickly be finetuned on a specific downstream task with relatively few labels, as it has already learned general language patterns during the training process. The intuition behind BERT is that the initial layers learn general language patterns that have some relevance to the downstream task, while the subsequent layers learn task-specific patterns. This intuition aligns with deep"}, {"title": "Datasets", "content": "Our proposed model is simultaneously trained on two parts of the CoNLL-2009 English dataset and the Persian PropBank corpus. In the original research, the model was trained on the complete CoNLL-2009 dataset using data from multiple languages, resulting in significant performance. However, due to the unavailability of the complete dataset, we only use the English portion of the CoNLL-2009 dataset. In the Persian dataset, the samples are allocated as follows: 10% for training data, 10% for validation data, and the remaining samples for testing data. The designed algorithm is trained and tested on these two datasets, and the results are reported using the F1 metric. The statistics of the CoNLL-2009 dataset and the Persian PropBank corpus are shown in Table 1 and Table 2, respectively."}, {"title": "Results", "content": "The proposed method is the first cross-lingual learning algorithm applied to the Persian language. Since the algorithm is trained on multiple languages and in a multitask manner, we can only utilize pretrained multilingual models such as Multilingual BERT and XLM-ROBERTa. However, XLM-ROBERTa model"}, {"title": "Experiments", "content": "To evaluate the proposed semantic role labeling model in resource-constrained settings, similar to our original paper, we use 10% of the training data for each language, namely Persian and English. To better compare and demonstrate the performance of our proposed model, we evaluate it in various settings, including monolingual and cross-lingual settings.\nIn Table 3, the F1 score results of the proposed model are reported on ten percent of the Persian language dataset, with varying amounts of English data from zero to one hundred percent, in both monolingual and cross-lingual settings. Please note that the monolingual setting refers to the model being trained and evaluated solely on one language, which means that in our experiments, the English data volume is zero percent. The cross-lingual setting occurs when two or more languages undergo multi-task learning during the training phase, and eventually, the model is evaluated on those languages. This refers to the next ten experiments in which we gradually increase the English data volume from ten percent to one hundred percent."}, {"title": "Discussion", "content": "We evaluated our proposed model on the Persian part of the CoNNL-2009 dataset, focusing on cross-lingual semantic role labeling. Since no previous research has been conducted on cross-lingual semantic role labeling in Persian, we compared the proposed model with the model by Nikseirat et al., which achieved the best results in Persian so far. In the proposed model, instead of using the entire training dataset, we only utilized 10% of it for cross-lingual semantic role labeling in both experiments. Following the conventions in English research, we followed a random split for training, validation, and testing, while in Persian, we used a configuration of 10% for testing, 10% for validation, and 80% for training data. The F1 measure in the monolingual mode was 71.76, and in the cross-lingual mode with 10% English data, it was 74.11. Furthermore, if we use all available English data, the F1 measure reaches 75.94. Therefore, this model managed to improve the Persian results by 4.18% compared to the monolingual mode by utilizing English data. By conducting similar experiments using the proposed model by Nikseirat et al., which is the best model for semantic role labeling in Persian, we obtained an F1 measure of 69.71. Thus, our proposed model achieved a 2.05% improvement in the monolingual mode and a 6.23% improvement in the cross-lingual mode compared to this model. However, since our model performs semantic role labeling in all four stages, unlike the assumption made in Nikseirat et al.'s research where only the last two stages are performed, the actual difference between the two models is significantly higher, reaching 6.23 We tried to improve this model by:\n\u2022\tInstead of using language-specific part-of-speech tags, we used universal part-of-speech tags, but we did not see much improvement. Our assumption was that pre-trained models like BERT are capable of extracting features related to part-ofspeech tags.\n\u2022\tInstead of xlm-roberta, we used bertmultilingual- base, which resulted in approximately 10% lower performance."}, {"title": "Future Work", "content": "The main priority of this model is to simultaneously train on data from multiple languages and prioritize improving the results for a language with limited resources. Therefore, in future work, it can be explored whether the model can be modified to solely focus on improving the results for a language with rich resources, while transferring the performance of the model on data from that language to a secondary priority. One solution we propose for this purpose is to weight the error function for different languages. Additionally, in recent years, new activation functions such as Mish [42] and SERF [43] have been introduced to improve the performance or speed of various tasks. In future work, the effect of replacing Swish with these activation functions or using a combination of them can be investigated. Furthermore, considering that five out of six languages in the CoNNL-2009 dataset are similar to each other, we suggest utilizing data resources for languages like Arabic alongside Persian and English in future work."}]}