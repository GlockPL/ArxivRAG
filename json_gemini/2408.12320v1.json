{"title": "PolyRouter: A Multi-LLM Querying System", "authors": ["Dimitris Stripelis", "Zijian Hu", "Jipeng Zhang", "Zhaozhuo Xu", "Alay Shah", "Han Jin", "Yuhang Yao", "Salman Avestimehr", "Chaoyang He"], "abstract": "With the rapid growth of Large Language Models (LLMs) across various domains, numerous new LLMs have emerged, each possessing domain-specific expertise. This proliferation has highlighted the need for quick, high-quality, and cost-effective LLM query response methods. Yet, no single LLM exists to efficiently balance this trilemma. Some models are powerful but extremely costly, while others are fast and inexpensive but qualitatively inferior. To address this challenge, we present PolyRouter, a non-monolithic LLM querying system that seamlessly integrates various LLM experts into a single query interface and dynamically routes incoming queries to the most high-performant expert based on query's requirements. Through extensive experiments, we demonstrate that when compared to standalone expert models, PolyRouter improves query efficiency by up to 40%, and leads to significant cost reductions of up to 30%, while maintaining or enhancing model performance by up to 10%.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance across a diverse set of challenging domain-specific tasks [2]. However, no single LLM can outperform all others across every task and use case [21]. Recent works [11, 17, 7] highlight the urgent need for efficient tools that can unify the expertise of multiple LLMs, combining them into a single cohesive unit. Given the increased costs and latency associated with querying models hosted at different providers [4], it is critical for multi-LLM querying systems to efficiently route queries to the most appropriate LLM expert. This must be done while balancing query execution throughput, monetary costs, and model performance\u2014a challenge we term the multi-LLM routing trilemma.\nOur aim is to provide an empirical solution to this trilemma by showcasing the potential of a multi-LLM routing system that improves this balance. We propose an LLM routing system, called PolyRouter, to explore the feasibility of building a multi-LLM routing model that leverages the collective power of multiple LLM experts. PolyRouter aims to efficiently, inexpensively, and accurately answer query prompts by selecting the most cost-effective and suitable LLM from a diverse set of expert models. Our contributions are as follows:\n\u2022 We empirically demonstrate the promise of different routing methods developed through the PolyRouter system in balancing query execution time, query cost, and model performance, leading to significant gains.\n\u2022 We show that, on average, our routing system outperforms standalone model experts."}, {"title": "Background & Related Work", "content": "LLM Routing. Depending on the mechanism used by routing methods to decide the most suitable LLM(s) to answer a given prompt, two distinct routing categories have been recently introduced: predictive/classification routers, which do not generate LLM outputs in advance, but instead, they predict the best LLM to handle a given prompt based on specific performance metrics [11, 17, 22] and cascading routers, which refer to routing methods that process a query request by executing it over a series or combinations of LLMs [4] until specific quality criteria are met. To train the predictive routers, different training methods have been recently introduced that leverage data augmentation techniques and human preference data [17] or existing benchmark datasets [21] to improve routing predictions. In this work, we too develop and evaluate predictive routing methods trained on standardized benchmark datasets to efficiently classify and direct query prompts to the best LLM expert.\nMixture-of-Experts. A typical MoE architecture [15] consists of a set of expert models trained to specialize in different data regions and a gating network model that determines the contribution of each expert to the final prediction. Recently, MoEs have witnessed wide adoption in the LLM domain as well, where multiple MLP experts are integrated into encoder and decoder blocks, to boost the training of extremely large networks [20, 13, 8]. Similar to these MoE approaches, the LLM routing methods can be seen a special case of an MoE architecture, where the predictive"}, {"title": "PolyRouter System Overview", "content": "To effectively learn and deploy a multi-LLM routing model, a sequence of different critical development phases need to be executed, from data preparation to router model training and evaluation and model deployment/serving. The proposed PolyRouter system's end-to-end pipeline shown in Figure 1 facilitates the development of these phases and in practice has drastically helped to swiftly develop, prototype and deploy different model routing methods into real-world settings.\nPhase 1: Router Data Preparation. The generation of the training and testing dataset for the routing model is a multi-step process. First, we need to find the appropriate domain specific (e.g., bio, coding, physical sciences) instruction datasets and model experts to which we want the routing model to learn propagating relevant query prompts. Thereafter, we perform a forward pass over each expert model (step 1) to collect the associated metrics required to train and test the performance of the routing model and create the experts prediction dataset (step 2). In this work, we collect the following metrics per instruction prompt: {negative log likelihood, BERT similarity score (BERTSim), inference time in seconds, total input tokens, total output tokens}; for more details on these metrics, please see section 4.1. Once the expert prediction dataset is created, we select one of the collected metrics to generate soft labels (step 3) and prepare the final training and testing dataset for the routing model (step 4). In the current work, we use the BERTSim scores to create soft labels and train the routing expert model classifier. We use soft labels, since we want the routing model to learn the ranking of the experts in terms of their prediction performance. To generate the soft labels of each expert model and for each instruction record, we pass the selected metric (e.g., similarity score, log loss), through a softmax function with temperature. For instance, for the r-instruction record, the expert (class) softmax probability $\\varphi_r(x;T)$ is given by: $\\varphi_r(x;T) = \\frac{exp(\\frac{x_r}{T})}{\\sum_{i=1}^E exp(\\frac{x_i}{T})}$ where $E$ is the total number of experts, $T$ is the temperature value, and $x = (x_1,x_2,...,x_E)$ is the vector of metric scores. In our evaluation, we generate expert's soft labels based on the BERT similarity scores and with a temperature value of $T = 10$.\nPhase 2: Router Training. Once the router's training and testing dataset is created, we pass the instruction records through the router's embedding model, e.g., Bag-of-Words, TF-IDF, BERT or other small or large language models, to create their vectorized representation (step 5). Then, we use the generated embeddings to train the prompt-to-expert classifier (step 6), using non-parametric, supervised learning approaches (e.g., kNN), classical deep learning models (e.g., MLP) or more advanced language sequencing pre-trained models (e.g., BERT). We provide more information on these routing models in section 4.4.\nPhase 3: Router Deployment. When the final routing model is trained, the model is deployed as a standalone endpoint on the platform (step 7), ready to receive user queries (either through CLI or web interface). Whenever a new user query is submitted, the router first tokenizes"}, {"title": "Experiments", "content": "In this section we discuss the metrics, expert models, benchmark datasets and routing methods we considered for evaluating the PolyRouter system."}, {"title": "Evaluation Criteria", "content": "All expert models and routing methods are evaluated on four dimensions: (1) total inference cost, (2) throughput, (3) BERT similarity score, and (4) negative log loss (NLL).\nTotal Inference Cost. For any expert model the total cost to execute a given test query is measured based on the input and output token costs. For a model $m$ that was prompted with a sequence of test queries that were used a total number of $T_i$ input tokens, and the model generated a total number of $T_o$ output tokens, with a $c_i$ and $c_o$ cost per 1 million input and output tokens, respectively, the total cost for the entire test query sequence is measured by: $C_m = \\frac{T_i}{1e6} * c_i + \\frac{T_o}{1e6} * c_o$. In the case of the routing methods that did not use one single model to answer the sequence of testing queries but routed different testing queries to different expert models $M$, the total cost is measured as: $C_r = \\sum_{m \\in M}C_m$. To measure the querying of standalone deployed expert models, we handpicked the price per million input and output tokens from different model providers. We provide the input and output token costs per model architecture in Table 2 in the Appendix section.\nThroughput. To measure the querying execution performance of a expert model and of different routing methods for the entire test query set, we compute the throughput for each query as the fraction of total output tokens $T_o^m$, generated by each model $m$, over the inference time in seconds, i.e., time from query submission to query completion, $t_{on}$. Specifically, the throughout for a single test query $i$ is measured as $T_i = \\frac{T_o}{t_{on}}$. For the entire set of test queries $N$, the mean throughput $\\bar{T}$ is computed as: $\\bar{T} = \\frac{1}{N}\\sum_i^N T_i$\nBERTSim. Given that each expert model uses its own vocabulary and tokenizer and to ensure that there is an equitable comparison between the responses generated by each expert, we evaluate the vectorized text similarity between the ground truth and the predicted answer of an expert through the cosine distance on the BERT embeddings. Such a vector representation allows for a soft measure of similarity [26]. We refer to this similarity score as BERTSim [26]. The cosine similarity of a reference (ground truth) vector $x_i$ and a candidate (predicted) vector $\\hat{x_i}$ is computed as: $\\frac{x_i \\cdot \\hat{x_i}}{||x_i||||\\hat{x_i}||}$. For every expert model and routing method we measure the BERTSim score across all test queries and we compute the final BERTSim score as the mean of all scores.\nNegative Log-Likelihood. We use the Negative Log-Likelihood (NLL) to measure the quality of the probabilistic predictions made by each expert model. Lower NLL values are indication that the model is assigning higher probabilities to the true classes and therefore reflecting better performance."}, {"title": "Methods", "content": "In principle, a single sequence's NLL is defined as:\n$L_{NLL} = \\sum_{t=1}^T -\\log P(y_t | X, Y_{1:t-1})$\nwhere $P(y_t | X, Y_{1:t-1})$ is the predicted probability of the t-th token in the sequence given the input sequence X and the previous tokens $Y_{1:t-1}$. In our evaluation, we measure the mean NLL over the generated sequence of every expert model and routing method across all test queries."}, {"title": "Expert Models", "content": "We choose several representative models across different domains as the expert models to verify the effectiveness of our routing method in the PolyRouter system. For the Biomedical domain, we selected two variants from Llama-3-8B (BioLlama-7B) [19] and Mistral-7B (BioMistral-7B) [16] models. Both models achieve excellent performance across many biomedical evaluation benchmarks. In the code domain, we select Meta's officially released Llama2-7B (CodeLlama-7B) [18] variant trained on code datasets. In the general instruction-following domain, we incorporate three instruction-tuned versions of LLMs across different sizes, i.e., Fox-1.6B a recently introduced powerful small language model, Mistral-7B-Instruct (MistralAI-7B) [12], and Qwen-7B-Instruct (Qwen-7B) [25]. Finally, for the math domain, we choose a strong reasoning model trained on large amounts of math documents, MathDeepSeek-7B-Instruct (MathDeepSeek-7B) [10]. For more details regarding models architecture and domain fine-tuning please refer to section E in the Appendix."}, {"title": "Datasets", "content": "All the datasets listed here are widely used by LLM developers [23, 24, 12] to evaluate model performance in commonsense reasoning, coding, and medical domains. To generate the final training and testing data for the investigating routing methods, we gather all records together from all datasets and perform a stratified 80% train, 20% test split per dataset.\nAi2-ARC [5]. The Ai2-ARC dataset consists of 7,787 natural science questions designed for standardized tests. We use its challenge partition with 2,590 samples, which includes only those questions that were answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\nGSM8k [6]. GSM8k is a high-quality dataset of grade school-level math word problems, covering relatively simple math concepts with 7,473 training and 1,319 testing samples.\nMBPP [1]. The MBPP dataset contains 974 basic programming problems suitable for entry-level programmers. It also includes text descriptions of the problems and test cases for functional correctness verification.\nPubMedQA [14]. The PubMedQA dataset is a biomedical question-answering dataset designed for answering research questions with yes/no/maybe responses. It contains 1,000 manually labeled question-answer pairs for cross-validation and testing."}, {"title": "Methods", "content": "Below, we describe the predictive routing methods we used during our evaluation.\nRandom-Router. To evaluate the performance of a random router, for every test query we randomly pick an expert to execute the query. After performing this step for all test queries, we"}, {"title": "Evaluation", "content": "repeat the entire process for 10 times. Let $E = (e_1, e_2, ..., e_n)$ be the collection of all experts, we randomly select an expert from $E$ in each trial. Let $e^i_j$ denote the $i$ expert randomly selected in the $j$-th trial, then the entire random expect selection process can be represented as: {$e^i_1,...,e^i_n$}. Once the collection of random experts is assembled, we submit the test query to each expert and collect all measurements to compute the evaluation metrics.\nkNN-Router. The kNN-Router first encodes all training queries $q_i \\in D^t$ using a sentence transformer. Then, for every test query, $q_t$, it finds its closest training query $q^*$ in terms of cosine similarity in the embedding space and subsequently executes the test query using the expert that exhibited the best performance for the most relevant training query. The best performing expert $e^*$ is the expert whose BERTSim score is the highest out of all the training query's experts, $q^*(E)$:\n$e^* = \\underset{i \\in D^t}{argmin}(\\frac{q_i \\cdot q_t}{||q_i|| \\cdot ||q_t||})$\n$e^* = \\underset{j \\in q^*(E)}{max} (BERTSim_j)$\nA schematic flow of the 1NN-Router's embedding similarity and expert selection is also shown in Figure 7. Given that we only need to find the most similar training query to a given test query, we subsequently refer to this method as 1NN-Router.\nMLP-Router. For learning our predictive MLP-Router, we use a simple 2-layer perceptron:\n$y_k = \\sqrt{\\sum_{j=1}^m \\omega_{jk}^{(2)} \\sigma \\bigg(\\sum_{i=1}^n \\omega_{ij}^{(1)} x_i + b_j^{(1)} \\bigg) + b_k^{(2)}}$\nTo train the MLP model, we convert the training queries into their vector representation by fitting a Bag-of-Words model. To learn the ranking of experts in terms of prediction performance, we use cross entropy loss on the scaled BERTSim scores. We used ReLU ($\\sigma$) and softmax ($\\sigma$) as the hidden and output layers' activation function, respectively.\nBERT-Router. To learn the BERT-Router, we performed a full parameter finetuning on a BERT model for sequence classification. We appended a classification head with a softmax activation funciton on top of BERT's final hidden layer outputs to map the BERT embeddings $H$ to the desired number of experts (classes):\n$y = softmax(WH + b), H = BERT(X)$\nTo fine-tune BERT, we first tokenize and encode all input training queries' text sequences $X$ using the BERT tokenizer and then update the pre-trained BERT model weights for a small number of epochs using cross entropy loss. Similar to the MLP-Router model, we train BERT-Router using the soft labels created by the scaled BERTSim scores.\nZero-Router. Following the work of [11], we also evaluate the performance of the routing methods against the average performance of the available LLMs without any routing logic (lower bound), i.e., no-routing approach.\nOptimal. We compare against two optimal cases (upper bounds), one refers to the optimal BERTSim performance per dataset (shown in Figure 2), and the other to the optimal performance recorded across all three evaluating dimensions (i.e., cost, throughput, model performance, shown in Figure 4). In the former case, the optimal value is measured by averaging the best BERTSim score recorded for every test query by any expert. In the latter case, the optimal set of values is the minimum cost, maximum throughput and maximum performance recorded by any expert model or router method."}, {"title": "Evaluation", "content": "To systematically evaluate all investigating expert models in terms of query response times, we deployed each model on a machine employed with 8 NVIDIA DGX H100 GPUs. Figures 2 and 3 show the BERTSim score and NLL value comparison between all routing and optimal methods."}, {"title": "Unlock the Potential of Collaborative Routing: Edge-to-Cloud", "content": "So far, we have discussed routing methods that can route query prompts to the most suitable expert hosted on a cloud service. However, the bigger promise of a model router is how to be effectively deployed on edge devices to help decide on whether a query prompt can be answered locally by a small model running on the edge or routed to an expert hosted on the cloud. Figure 5 shows such an architecture, where a model router is deployed locally on the edge device. The router is responsible for deciding whether a user's new request submitted to the edge device should be answered directly by a small model, such as a small language model (SLM), which is already running on the edge, or by a larger model deployed on a cloud provider, or through a combination of the two. By applying this approach, we can significantly reduce querying and communication costs on the edge while maintaining overall model and query performance.\nTo provide further insights towards the materialization of this edge-to-cloud collaborative approach, in the heatmap shown in Figure 6, we record the number of test queries answered by each expert model for every routing method. From the reported values, it is apparent that both the MLP-Router and the BERT-Router, route most of the test queries investigated during our experimental analysis (section 4) to the Fox-1.6B small language model, which is similar to the behavior observed by the Optimal (oracle) approach. However, other approaches like the Random-Router and 1NN-Router, distribute almost equally the number of queries across all model experts.\nIf we assume that an SLM, like the Fox-1.6B model, is deployed on the edge, our analysis shows that by learning the embedding space of existing query prompts using an MLP or BERT-based routing approach, the majority of queries will be forwarded to the most suitable expert, which in this case is the SLM. As a result, deploying such routing methods on the edge enables edge devices to decide whether a query prompt should be routed to the local SLM or a cloud model expert, paving the way for the effective development of collaborative edge-to-cloud model routing techniques."}, {"title": "Conclusion", "content": "We present our multi-LLM routing system, called PolyRouter, for the first time. With the Poly-Router system, users can easily interact with multiple LLM expert models hosted on the same or different platform providers, without being limited to a single monolithic LLM system. By utilizing a routing method capable of learning the embedding space of query prompts, such as MLP- and BERT-based methods, users can benefit from significant cost savings (up to 30%), improved query response times (up to 40%), and enhanced model performance (up to 10%). As part of our immediate future plans, we aim to evaluate the feasibility of dynamically adding and removing model experts during the router's endpoint deployment and test the routing efficacy of both small and large pre-trained language models."}]}