{"title": "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions\nwith Path Planning and Feedback", "authors": ["Qinzhuo Wu", "Wei Liu", "Jian Luan", "Bin Wang"], "abstract": "Recently, tool-augmented LLMs have gained\nincreasing attention. Given an instruction, tool-\naugmented LLMs can interact with various\nexternal tools in multiple rounds and provide\na final answer. However, previous LLMs were\ntrained on overly detailed instructions, which\nincluded API names or parameters, while real\nusers would not explicitly mention these API\ndetails. This leads to a gap between trained\nLLMs and real-world scenarios. In addition,\nmost works ignore whether the interaction pro-\ncess follows the instruction. To address these\nissues, we constructed a training dataset called\nMGToolBench, which contains statement and\ncategory-level instructions to better reflect\nreal-world scenarios. In addition, we pro-\npose ToolPlanner, a two-stage reinforcement\nlearning framework that utilizes path planning\nand two feedback mechanisms to enhance\nthe LLM's task completion and instruction-\nfollowing capabilities. Experimental results\nshow that ToolPlanner significantly improves\nthe Match Rate, Pass Rate and Win Rate by\n26.8%, 20.2%, and 5.6% compared to the\nSOTA model. Human evaluation verifies that\nthe multi-granularity instructions can better\nalign with users' usage habits. Our data and\ncode will be released upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Recently, tool-augmented large language models\n(LLMs) have shown their remarkable ability in\nutilizing various external tools, such as Hugging-\nFace models (Shen et al., 2023; Wu et al., 2023),\nreal-world applications (Liu et al., 2023b; Wang\net al., 2023b), and massive APIs (Tang et al., 2023;\nLiang et al., 2023). To simulate complex real-\nworld tasks, previous studies have continuously\nincreased the size of the external tool pool and the\ncomplexity of task instructions (Yang et al., 2023;\nRuan et al., 2023; Kong et al., 2023). LLMs need\nto break down complex instructions into subtasks,\ninteract with the tools in multiple rounds based on\neach subtask's requirement, and finally provide a\nreasonable answer. However, these instructions\noften tend to be overly detailed and specific, which\ndiffer from real-world scenarios.\nAfter observing online user cases, we noticed\nthat their proposed tasks are similar to the real user\nexamples in Figure 1. Users tend to describe their\ncurrent situation or the category of information they\nneed, rarely mentioning the tools they require, let\nalone the API names. Intuitively, users do not care\nwhich specific APIs LLM uses to complete their\ntasks and are unlikely to remember the functions of\nmassive APIs. For example, a ToolBench case like\n\"coordinates 39.5501\u00b0 N, 105.7821\u00b0 W\" is unlikely\nto occur in a real-world scenario.\nMoreover, previous works focused on whether\nLLMs could ultimately generate a reasonable\nanswer, while ignoring their ability to follow in-\nstructions (Wang et al., 2023a). For the ToolBench\nexample in Figure 1, the instruction explicitly\nrequires the LLM to complete the task with the\n\"True Way Places\" and \"Geocoder\" tools. In\nround 2, if the LLM decides to interact with the\n\"Weather\" tool instead of the \"Geocoder\" tool,\nit may still generate a valid answer. However,\nthis interaction process does not follow the given\ninstruction, which may result in a decrease in the\nquality of the final answer.\nTo address these issues, we constructed a train-\ning dataset called MGToolBench using ToolBench\nas the seed data. MGToolBench adopts a multi-\ngranularity user instruction mechanism to match\nuser behavior in real-world scenarios. In addition,\nwe propose ToolPlanner, a two-stage reinforcement\nlearning (RL) framework. In Stage 1, a supervised\nfine-tuning (SFT) model is used to generate a\nsolution tree for each instruction. In Stage 2, two\nmetrics, task completion, and instruction following,\nare used to score the generated solutions and\npair positive and negative responses. We further\nreinforce the SFT model with pairwise responses as\nfeedback to enhance the model's tool usage ability.\nFurthermore, a solution path planning mechanism\nis used to guide ToolPlanner during its multi-round\nreasoning process. The main contributions of this\npaper can be summarized as follows:\n\u2022 We constructed a multi-granularity instruction\ndataset called MGToolBench to reflect real-world\nscenarios. As far as we know, this is the first study\nexploring the ability of tool-augmented LLMs to\nfollow instructions of different granularities.\n\u2022 We proposed ToolPlanner, a two-stage RL\nframework that utilizes task completion and\ninstruction-following feedback to enhance the\nmodel's tool usage abilities. ToolPlanner includes\na solution path planning mechanism that provides\nhigh-level guidance for the reasoning process.\n\u2022 Experimental results show that ToolPlan-\nner outperforms existing state-of-the-art models.\nHuman evaluation confirms the multi-granularity\ninstruction mechanism's ability to generate instruc-\ntions that align with real-world scenarios."}, {"title": "2 Related Work", "content": "Tool-augmented LLMs Datasets: The research\ncommunity collects diverse datasets to facilitate\nresearch on tool-enhanced LLMs. API-Bank (Li\net al., 2023) provides a benchmark that includes\n264 annotated dialogues and 568 APIs. APIBench\n(Patil et al., 2023) collects 1,716 machine learning\nAPIs from 3 public model hubs. ToolBench\n(Qin et al., 2023) provides a high-quality dataset\ncontaining 16,464 real-world APIs collected from\nRapidAPI Hub. ToolAlpaca (Tang et al., 2023)\nuses real-world APIs similar to ToolBench, with\n3,938 instances. When generating instructions,\nthese works rely heavily on pre-selected tools or\nAPIs. This makes the instructions too detailed and\ninconsistent with the usage habits of real users.\nTool-augmented LLMs Framework: Many\nstudies have combined LLMs with massive external\ntools and APIs to access a wider range of resources\nand services (Parisi et al., 2022; Xu et al., 2023;\nLiang et al., 2023). Toolformer (Schick et al.,\n2024) trains LLM to directly generate responses\ncontaining API calls in the text. React (Yao\net al., 2022) interacts with external tools multiple\nrounds that follow the \"Thought-Action-Action\nInput-Observation\" format. ToolLLM (Qin et al.,\n2023) uses a tree-based method that can restart the\nreasoning process from a previous round. Ye et al.\n(2023) designed an Elo-based Self-Judgment Mech-\nanism (Elo and Sloan, 1978) that use ChatGPT as\na decision maker. ToolPlanner uses a two-stage\nRL framework with solution path planning and two\nfeedback mechanisms to guide the model in its\nreasoning process.\nReinforcement Learning on LLM: Recently,\nRL-based fine-tuning mechanisms have been em-\nployed to enhance the LLM's generation quality\n(Liu et al., 2023a; ?). Yuan et al. (2023) proposed\nan RRHF paradigm that encourages the model to\ngenerate results with better human preferences.\nQiao et al. (2023) enhances the model through\nfeedback derived from tool execution. We leverage\ntask completion and instruction-following feedback\nto score and sample pairwise responses at each\nsolution round."}, {"title": "3 Dataset Construction", "content": "3.1 Multi-Granularity Instruction Mechanism\nTo match user behavior in real-world scenarios,\nwe propose a multi-granularity user instruction\nmechanism. We have chosen three intermediate"}, {"title": "4 Models", "content": "4.1 Problem Definition\nThe tool-using task can be expressed as a multi-\nround reasoning process that generates a solution\nS and a final answer Y based on the given user\ninstruction X. As shown in Figure 4, ToolPlanner\nis composed of the Stage 1 SFT model and Stage\n2 RL model. In Stage 1, the SFT model is fine-\ntuned in a sequence-to-sequence manner, which\nincludes three modules: tag extraction, solution\npath planning, solution tree generation. In Stage\n2, following RRHF (Yuan et al., 2023), we sample\npairwise responses with the reward function and\nuse them to continue optimizing the SFT model.\n4.2 Stage 1: Supervised Finetuning\n4.2.1 Tag Extraction\nGiven a user instruction, ToolPlanner needs to\nextract the user's intent and generate a candidate\ntag list of three granularities. In Figure 4, from a\ntool-level instruction X, we can extract its tool-level\nlist as \"Priceline, ADSBx\" and its category-level"}, {"title": "4.2.2 Solution Path Planning", "content": "In this module, given an instruction and a candidate\ntag list, ToolPlanner generates a complete solution\npath as a high-level guide for the following process.\nAs shown in Figure 4, ToolPlanner believes that it\nneeds to call A1 first, followed by B1, and then B2\nto finally generate the answer."}, {"title": "4.2.3 Solution Tree Generation", "content": "With the user instruction, the candidate tag list,\nand the solution path as input, ToolPlanner needs\nto go through multiple rounds of interaction with\nexternal tools to obtain a solution tree. Each tool\ninteration round is an intermediate node in the\nsolution tree, including thought, generating an API\nrequest, and obtaining an observation. The leaf\nnode in the solution tree is a Finish node of the\ncurrent branch. Once LLM generates a Finish node\nwith an answer, the rightmost path of the solution"}, {"title": "4.3 Stage 2: Reinforcement Learning", "content": "4.3.1 Task Completion and Instruction\nFollowing Metrics\nTo better evaluate LLM's ability to complete tasks\nand follow instructions, we propose two metrics:\nTask completion measures whether the solution\ncan successfully complete the task. If the solution\nfinally provides a meaningful answer, mark it as\n\"Pass\". If the solution exceeds the max number of\nrounds or decides to restart, mark it as \"Not Pass\".\nInstruction-following measures whether the\nsolution follows the user instruction. If the solution\naccesses and only accesses all categories, tools or\nAPIs mentioned in the instruction, it should be\nmarked as \"Match\" at the corresponding level."}, {"title": "4.3.2 Pairing pairwise responses", "content": "Intuitively, we expect ToolPlanner to generate\nsolutions with positive rewards. Therefore, we\ncollected pairwise responses to further reinforce\nthe SFT model. Specifically, we pair a negative\nexample for each round of positive solutions. Two\nsolution rounds can form a pairwise response if\nthey share the same history rounds and their reward\nis positive and negative, respectively. Here, for the\ni-th round si of solution S, its reward $R(s_i|S_{<i})$\nequals the highest reward score among all the\nsolutions to which it belongs.\nIn Figure 5, S8=\"A1,B2,B3,\" is a positive\nsolution with 4 rounds. In round 3, B3 and C2 share\na common history \"A1,B2\" and $R(B3|A1,B2)=1$\nis greater than $R(C2|A1,B2)=-3$. We consider\n(B3,C2|A1,B2) as a pairwise response. In round 1,\nthere is no negative round for node A1. We will\nsample a round and ensure it belongs to a negative\nsolution. E.g., the pairwise responses could be\n(A1,C1) and (A1,O). Finally, for the t-th round\n$s_t$, we have at least a response pair $(s_t^1, s_t^2|s_{<t})$,\nwhere $R(s_t^1|s_{<t}) > R(s_t^2|s_{<t})$."}, {"title": "4.4 Training", "content": "In Stage 1, we use cross-entropy loss to train the\nSFT model to generate the candidate tag list C,\nsolution path P, solution tree S, and answer Y.\n$L = \\sum_t log P(S_t, Y|S_{<t}, P, C, X)$.\n(2)\nIn Stage 2, we use pairwise responses to further\nfinetune the solution tree generation module in the\nSFT model. In t-th round, for a pairwise response\n$(s_t^1, s_t^2|s_{<t})$, the ranking loss is defined as:\n$L_{rank} = \\sum_{R(s_t^1)>R(s_t^2)} min(0, P(s_t^2) - P(s_t^1)).$\n(3)\nThe final loss function L is a combination of the\ncross-entropy loss and ranking loss:\n$L_i = -log P(S_t| S_{<t}, X))$,\n(4)\n$L = L_i + \\beta L_{rank}.$\nHere, $\\beta$ is a hyperparameter."}, {"title": "5 Experiment", "content": "5.1 Dataset\nWe use the G3 split of ToolBench (Qin et al.,\n2023) as a seed to construct the MGToolBench\ndataset, which contains 75,888 solution steps for\nStage 1 SFT model training. To obtain more\nnegative solutions, we regenerate the solution\ntree for the multi-level instructions using the SFT\nmodel. Finally, we have 98,950 paired responses\nfor stage 2 RL model training. We use the official\nG3 split test set with 100 hybrid-level tasks for\nbetter comparison. Therefore, there was no overlap\nbetween the training set and the test set. We\nuse the multi-granularity instruction mechanism\nto generate test instructions at the other four levels."}, {"title": "5.2 Settings", "content": "Baselines. We compare our proposed ToolPlanner\nwith the following baselines: ChatGPT (gpt-3.5-\nturbo-16k) (OpenAI, 2022) is one of the most\nadvanced LLMs currently available. GPT4 (gpt-\n4-0314) (OpenAI, 2023) is a more powerful and\nintelligent LLM with stronger tool usage capabili-\nties. ToolLLaMA is a tool-use framework based on\nLLaMA-7B (Touvron et al., 2023), which includes\na separate API retriever and has been fine-tuned\nwith the ToolBench dataset."}, {"title": "Decoding Methods", "content": "Decoding Methods. 1.Chain-based Method: Fol-\nlowing ReAct (Yao et al., 2022), CoT@N_inde-\npendently runs chain-based reasoning N times\nuntil it finds a solution path that passes the task.\n2.Tree-based Method: Following DFSDT (Qin\net al., 2023), LLM treats ReAct's multi-step rea-\nsoning (Thought-Action-Observation) as a round\nand performs depth-first reasoning process in a tree\nstructure."}, {"title": "Main Metric", "content": "Main Metric. 1.Match Rate calculates the pro-\nportion of solutions that successfully match user\ninstructions at a certain tag level. 2.Pass Rate(Qin\net al., 2023) calculates the proportion of solutions\nthat successfully complete the task with a reason-\nable answer. 3. Win Rate uses ToolEval (Qin et al.,\n2023) to calculate the ratio at which ChatGPT\nprefers the generated answers over the golden\nanswers."}, {"title": "Human Evaluation Metric", "content": "Human Evaluation Metric. 1.Plausibility mea-\nsures whether an instruction is fluent, complete,\nand makes sense in describing a user's intent.\n2.Conciseness measures whether an instruction\nis concise. 3. Relevance measures whether an\ninstruction's instruct clause is clear and relevant\nto its statement. 4.Realness measures whether an\ninstruction aligns with the real-world scenarios."}, {"title": "Implementation Details", "content": "Implementation Details. To ensure fair com-\nparisons, we maintain consistent hyperparameters\nacross all the baselines and our models. Tool-\nPlanner chose LLaMA-7B as the backbone model\njust like ToolLLaMA. In Stage 1, models are\ntrained for 3 epochs on 75,888 instruction-solution\nrounds. In Stage 2, the model was trained for 2\nepochs on 98,950 pairwise responses."}, {"title": "5.3 Results and Discussions", "content": "The main experimental results for Match Rate, Pass\nRate, and Win Rate are presented in Table 1. From\nthe table we can observe that:\n\u2022 Models with a tree-based decoding method\nperform better on Pass and Win Rate, but worse on\nMatch Rate because tree-based method sacrifices\ninstruction-following ability to complete the task.\n\u2022 ToolPlanner achieves a significantly higher\nMatch Rate than other baselines, which we attribute\nto our multi-granularity instruction mechanism\nand instruction-following feedback. ToolPlanner\nexplicitly considers whether the tool meets the\ninstruction requirements in each interaction round,\nleading to a strong instruction-following ability."}, {"title": "5.4 Human Evaluation", "content": "To evaluate our multi-granularity instruction mech-\nanism, we conducted a human evaluation. We"}, {"title": "5.5 Ablation Study", "content": "Table 2 shows the results of several ablation\nexperiments. We can observe that:\nEffect of tag extraction and solution path plan-\nning mechanisms: Without solution path planning,\nthe Match Rate of \"ToolPlanner w/o Path\" would\ndecrease by 25.5%. Without tag extraction, \"Tool-\nPlanner w/o Tag\" decreases by 28.2%, 13.6%, and\n13.4% in its three metrics. Removing both of these"}, {"title": "5.6 Case Study", "content": "Figure 6 shows a case generated by GPT-4, Tool-\nLlama and our ToolPlanner. After two incorrect\nAPI requests, GPT-4 successfully called the \"5 day\nforecast\" API.However, it ignored the task of air\nquality prediction. ToolLlama mistakenly called\nthe \"Weather\" API from the \"Ambee Air Quality\"\ntool, as the term \"Weather\" and the task instruction\nare semantically related. This ultimately caused the\nmodel to forget to provide weather forecasts in the\nanswer. With the tag extraction and solution path\nprediction mechanism, our ToolPlanner can now\npredict the tools required to complete the entire\ntask on a global scale, rather than just selecting\nthe tools most relevant to the instruction. By using\nthe reinforcement learning, ToolPlanner can learn\nto avoid using tools that are not mentioned in the\ninstructions, and it can encourage the reasoning\nprocess to ultimately provide an answer."}, {"title": "6 Conclusion", "content": "In this work, we propose ToolPlanner, a two-\nstage RL framework that utilizes task completion\nfeedback and instruction-following feedback to\nenhance LLMs' reasoning and tool usage abilities.\nAdditionally, we constructed a training dataset\ncalled MGToolBench, which uses multi-granularity\ninstructions to simulate the usage habits of real\nusers. Experimental results show that ToolPlanner\nsignificantly improves the Match Rate, Pass Rate\nand Win Rate by 26.8%, 20.2%, and 5.6%. Hu-\nman evaluation verifies that the multi-granularity\ninstruction mechanism can generate instructions\nthat better align with user habits.\nBy addressing the challenges of tool-augmented"}, {"title": "Limitations", "content": "ToolPlanner's reasoning process takes too many\nrounds. Due to the use of a tree-like inference\nstructure, each instruction may require 4-30 rounds\nto generate a solution tree (as shown in Ap-\npendix A), and each round requires 3 interactions\n(Thought, Action, Action Input) with the LLM.\nThis limitations will be the focus of our future\nwork."}, {"title": "Ethics Statement", "content": "This paper was conducted in accordance with the\nACM Code of Ethics. The ToolBench dataset\nused in this work is publicly available (Qin et al.,\n2023), and our MGToolBench dataset is con-\nstructed using publicly available platforms and\ndata sources, ensuring that there are no privacy\nissues or violations. All data used in our research\nwas obtained following legal and ethical standards,\nand we do not collect any personally identifiable\ninformation.\nIn the human evaluation, we hired 3 crowd\nworkers from the crowdsourcing platform without\nany discrimination. For the instruction human\nevaluation, we provided them with 5 instructions\nof different granularity in MGToolBench. For the\nanswer generation human evaluation, we provided\nthem with the corresponding instructions and the\nfinal answers generated by different baselines. We\npaid these workers no less than RMB 100 per hour."}, {"title": "A HyperParameter Settings", "content": "We present the hyperparameters for Stage 1 SFT\nmodel and Stage 2 RL model in Table 5. We\nchoose LLaMA-7B as the backbone model, just\nlike ToolLLaMA, to ensure a fair comparison. The\nlearning rate is first warmed up to the set value, and\nthen linearly decayed to 0. We use 8 80GB Nvidia\nA100 GPUs for fine-tuning, typically costing 8\nhours for Stage 1 and 30 hours for Stage 2.\nFollowing (Chen et al., 2023), we set the max-\nimum sequence length to 8192. This is because\nthe prompt used to generate the solution tree\nwill exceed 4,500 characters after adding user\ninstructions and descriptions of Tools and APIs,\nas described in Appendix C.1.3. $\\beta$ in our loss\nfunction is 1 (Yuan et al., 2023; Liu et al., 2023a).\nThe maximum number of steps for each solution\nis 12. Since each round contains 3 steps (Thought,\nAction, Action Input), the maximum number of\nsteps for each solution is 4. For the chain-based\nmethod, N of CoT@N is set to 5. For the tree-\nbased method, each node in the solution tree has\nat most 2 children. At most two solution trees are\ngenerated in each reasoning process. Therefore, in\na reasoning process, if the model generates two full\nsolution trees, the maximum number of reasoning\nrounds is 30, as shown in Figure 7."}, {"title": "B Details for MGToolBench Dataset", "content": "B.1 Data Statistics\nWe report the statistics of seed data in Table 6,\nwhich is the intra-category multi-tool instruction\nsubset from ToolBench (Qin et al., 2023). It is the\nmost challenging subset in ToolBench. It requires\nthe combined use of multiple tools from different\ncategories, which helps to reflect complex real-\nworld scenarios. We removed seed tasks that did\nnot provide a proper candidate tag list or had an\ninvalid solution tree, leaving 4,435 remaining tasks.\nWe use the official test set for the intra-category\nmulti-tool instruction subset from ToolBench. The\ntest set consists of 100 tasks. We build test instruc-\ntions using the same approach as building multi-\ngranularity instructions in the training dataset.\nSpecifically, we set the original test instructions at\nthe hybrid level, and used these instructions, their\ncorresponding tag lists, and instruction generation\nprompts to feed into the GPT-4 model to generate\ntest instructions at the other three levels."}, {"title": "B.3 MGToolBench Dataset", "content": "B.3.1 Conflict between Instructions and Real\nUsers\nWhen constructing data, ToolBench will provide\nseveral tools, APIs, and their documentation to al-\nlow ChatGPT to generate user instructions. There-\nfore, ChatGPT tends to directly copy the API name\nor introduction from the documentation, rather than\nusing a more natural description.\nThe conflict between the generated instructions\nand user habits comes from two aspects:\n\u2022 1. many API names are designed for de-\nvelopers and do not conform to the usage\nhabits of real users."}, {"title": "B.3.2 multi-granularity instruction\ngeneration", "content": "In ToolBench, each instruction has a corresponding\nAPI list. Because when building data, ToolBench\nfirst samples several APIs, and then lets ChatGPT"}, {"title": "C Details for ToolPlanner", "content": "C.1 Stage1 SFT Model\nC.1.1 Prompt Design\nIn this section, we show the details of the prompt\ndesign in ToolPlanner.\nThe prompts of tag extraction, solution path\nplanning and solution tree generation are shown\nin Table 16, 17, 18, respectively.\nIn the Stage 1 training phase, we use the tag list,\nsolution path, and multi-round solution of 17,740\ncases to finetune ToolPlanner.\nIn the Stage 2 training phase, we use the 98,950\npairwise responses to further finetune ToolPlanner.\nIn the test phase, ToolPlanner uses prompts\nfor tag extraction and solution path planning to\nobtain the tag list and solution path. It then uses\nprompts for solution tree generation multiple times\nto generate the solution tree and final answer."}, {"title": "C.1.2 Inference Process of Tool-Augmented\nLLMS", "content": "In this section, we briefly introduced the framework\nfor existing tool-augmented LLMs. Existing tool-"}, {"title": "C.1.3", "content": "C.1.3 Inference Process of ToolPlanner\nIn this section, we provide a step-by-step inference\ncase to describe how ToolPlanner generates the tag\nlist, solution path, solution tree, and final answer\nstarting from a user instruction.\nAs shown in Table 19, a hybrid-level user\ninstruction asks the model to provide some funny\njokes, and specifies that these jokes should come\nfrom either API-Ninjas or Chuck Norris.\nFirst, ToolPlanner uses this instruction and the\nprompt in Table 16 for tag extraction. After tag\nextraction, ToolPlanner obtained tag lists at three\ndifferent levels.\nThen, ToolPlanner adds the tag lists to the\nmodel input and uses the prompt from Table\n17 to generate a solution path. After solution\npath planning, ToolPlanner obtained a three-step"}, {"title": "C.2 Stage 2 RL model", "content": "In this section, we provide a detailed description of\nthe process of extracting pairwise responses from\nthe solution tree. As discussed in section 4.3, the\nentire process consists of two steps: 1. Extracting\nsolution paths and scoring them. 2. Extracting\nsolution steps and pairing them up based on their\nreward scores."}, {"title": "C.2.1 Reward", "content": "Figure 9 shows an instruction and two correspond-\ning solution trees. Each path from the root node\nto a leaf node is considered a solution. The figure\ncontains a total of eight solution paths, namely S1,\nS2,..., and S8.\nWe use task completion and instruction-\nfollowing as metrics to score each solution."}, {"title": "C.2.2 Sampling and Ranking", "content": "After annotating each node in the solution tree with\na reward score, we can extract pairwise responses\nfrom it. When training ToolPlanner, each positive\nstep is used to calculate the cross-entropy loss\nto fine-tune the model. Therefore, we only use\nnodes with a reward score of 1 as positive examples\nand extract nodes with the same history steps and\nnegative reward scores as negative examples.\nIn Figure 9, only nodes belonging to the solution\npath S8 = \"A1, B2, B3, \u221a\" are considered positive\nexamples. For the node \u2713, $R(\\sqrt{}|A1,B2,B3) >$\nR(A3|A1,B2,B3), therefore, $(\\sqrt{}, A3)$ is a pairwise\nresponse with (A1, B2, B3) as the history steps.\nFor the node B3, $R(B3|A1,B2) > R(C2|A1,B2)$,\ntherefore, (B3, C2) is a pairwise response with (A1,\nB2) as the history steps. For the node B2 with A1\nas the history step, (B2, A2), (B2, C1), and (B2,\nA3) are three pairwise responses.\nFor nodes has no sibling nodes with a negative\nscore, like A1, we sample and pair them with a\nnegative example. There are three methods for\nsampling negative examples:\n\u2022 Select a Finish node to ensure that its path is\nmarked as \"Not Pass\", such as and \u00d7.\n\u2022 Select a node from another tool to ensure that\nits path is marked as \"Not Match\", such as C1\nand C2.\n\u2022 If the history steps do not match the instruc-\ntion, the node can be selected to end its path\nat \"Pass & Not Match\"."}, {"title": "D Experiment", "content": "D.1 Main Metric\nWe used three metrics in main experiments:\nMatch Rate measures the instruction following\nability of LLM. If the solution accesses and only\naccesses all tags described in the user instructions,\nit is considered to match the instruction at the\ncorresponding tag level. Match Rate calculates\nthe proportion of solutions that successfully match\nuser instructions at a certain tag level. When\ncalculating the Match Rate of a fine-grained tag\nlevel, such as API, we also calculate the Match Rate\nof its parent tag levels, such as Tool and Category.\nTaking an API-level instrucion as an example, if\nthe solution generated by LLM only uses the tools"}, {"title": "D.2 Human Evaluation on Multi Granularity\nInstructions", "content": "To evaluate whether our multi granularity instruc-\ntion mechanism can better reflect user behavior, we\nconducted a human evaluation using four metrics\nto compare user instructions at different levels.\n\u2022 Plausibility: This metric measures whether\nan instruction is fluent, complete, and makes\nsense in describing a user's intent. In other\nwords, it measures whether the instruction\nconforms to the grammar and semantic rules\nof the language, and is like an executable task\ninstruction.\n\u2022 Conciseness: This metric measures whether\nan instruction is consistent and includes all\nnecessary information. Are the instructions\neasy to understand and follow, or are they\noverly complicated and confusing?\n\u2022 Relevance: This metric measures whether the\n\"instruct\" part of an instruction is clear and\nrelevant to its statement. In other words, it\ndetermines whether the multiple tasks com-\npleted by different tools in the instructions\nare coherent and related to the statement\nsentences. For example, if the task statement\nis that the user needs to search for recipes,\nthe command should not suddenly switch\nto calling \"Playlist\" from the music tool\n\"Deezer\".\n\u2022 Realness: This metric measures whether an\ninstruction aligns with the usage habits of real\nusers, that is, whether the user is willing and\nable to use such instructions to instruct the\nmodel.\nWe randomly selected 100 instructions with\ndifferent granularities and asked three crowdwork-\ners to evaluate them. For each metric, we asked\nreviewers to rate the issues on a scale of 1-3 (with\n3 being the best). Table 23 provides examples of\ninstructions with different ratings for each metric.\nResults of each human evaluation metric are\npresented in Table 3. We can see that:\n\u2022 For plausibility, relevance, and realness, API-\nlevel instructions do not perform as well as\nothers. Human evaluation has found that many\nAPI names are designed for developers and"}, {"title": "D.3 Human Evaluation on Generated\nAnswers", "content": "As described in Section 5.2, Win Rate uses Chat-\nGPT to compare the generated answers of different\nbaselines with the golden answers from ToolBench.\nTo verify whether humans would make the same\njudgments as ChatGPT, we conducted human\nevaluations on the answers generated by different\nbaselines. We found two crowdsourcing workers\nwho were provided with the final answers of two\nbaselines on 100 Hybrid-level test cases, and asked\nthem to compare and annotate whether the answers\ncompleted the instructions. The results of the\nhuman evaluation are presented in Table 24. We\ncan see that,\n\u2022 Whether a solution passes or not has a signifi-\ncant impact on both the win rate and human\nevaluation. If the model does not generate a"}]}