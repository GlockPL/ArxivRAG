{"title": "LESS YET ROBUST: CRUCIAL REGION SELECTION FOR SCENE RECOGNITION", "authors": ["Jianqi Zhang", "Jingyao Wang", "Changwen Zheng", "Mengxuan Wang", "Lingyu Si", "Fanjiang Xu"], "abstract": "Scene recognition, particularly for aerial and underwater images, often suffers from various types of degradation, such as blurring or overexposure. Previous works that focus on convolutional neural networks have been shown to be able to extract panoramic semantic features and perform well on scene recognition tasks. However, low-quality images still impede model performance due to the inappropriate use of high-level semantic features. To address these To address these challenges, we propose an adaptive selection mechanism to identify the most important and robust regions with high-level features. Thus, the model can perform learning via these regions to avoid interference. implement a learnable mask in the neural network, which can filter high-level features by assigning weights to different regions of the feature matrix. We also introduce a regularization term to further enhance the significance of key high-level feature regions. Different from previous methods, our learnable matrix pays extra attention to regions that are important to multiple categories but may cause misclassification and sets constraints to reduce the influence of such regions. This is a plug-and-play architecture that can be easily extended to other methods. Additionally, we construct an Underwater Geological Scene Classification dataset to assess the effectiveness of our model. Extensive experimental results demonstrate the superiority and robustness of our proposed method over state-of-the-art techniques on two datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Scene recognition has attracted extensive attention from the scientific community and the public, which aims to classify a brief interpretation of the scene in the image. It has extensive application in environmental monitoring [1], rescue [2], marine archaeology [3] and serves significant value in military and aviation activities [4]. Unfortunately, recent scene recognition tasks mainly focus on land scene classification using high-resolution color remote images, such as [5]-[7], which may still face challenges in real-world applications, e.g., underwater [8]-[10] and remote sensing [11], [12] scenarios.\nRecently, deep neural network models based on CNNs have achieved great success in visual classification tasks and have begun to be introduced to scene recognition tasks [14]\u2013[17]. With the advancement of technology, models with deeper CNN architectures are generally implemented in the scene recognition field to extract invariant semantic features for classification. For instance, ResNet [18] and its variants, pre-trained on ImageNet [19], have shown great potential in downstream visual tasks. They primarily serve as feature extractors to obtain semantic features, which are then used as input for the classification head [20]\u2013[23].\nHowever, in real-world applications, scene recognition tasks can be affected by factors such as object occlusion, lighting blur, and overexposure. Previous CNNs-based methods, which learn from all regions of the image, may mistakenly incorporate these distracting elements into the decision-making process. Taking the CNNs-based model, ResNet-18, as an example, we visualize the attention regions that the model focuses on when predicting the labels of four images in Fig. 1. The red box area in Fig. 1 is sufficient to determine the image category, but ResNet18 focuses on regions significantly beyond this area. For example, in the first column, the model unnecessarily focuses on the sea anemone obscuring the scene, and in the fourth column, it concentrates excessively on areas unrelated to the label. This focus on irrelevant areas intro-duces additional noise, leading to performance degradation. Therefore, we propose focusing on more discriminative and robust regions of the image, allowing the model to learn from these areas to eliminate the impact of noise or occlusion on decision-making in practical applications. It is important to note that focusing solely on regions crucial for decision-making is not enough, as some regions that are important for multiple categories may lead to errors when the model tries to distinguish between them (as shown in the third column of Fig. 1). Fig. 4 provides further analysis of Fig. 1.\nBased on this insight, we develop a novel method that propels the model to focus on less but robust high-level feature regions. The model learned via these crucial semantic features will achieve higher performance. Specifically, we adopt a CNN-based extractor to obtain semantic features and then use a learnable mask matrix to locate important features. Consequently, this learnable matrix first re-assigns weights to different regions to strengthen the model's focus on decision-related semantics, and then introduces mask constraints to eliminate the semantic influence of category sharing, allowing the model to make decisions based on fewer but more robust regions. Our main contributions are in four aspects:\n\u2022\tWe provide a straightforward and valid feature selection strategy to propel the model to catch less yet crucial high-level semantic feature regions.\n\u2022\tWe propose an importance regularization term that en-courages sparsity in the mask by pushing more elements"}, {"title": "II. METHODOLOGY", "content": "In this section, we introduce the proposed method which performs learning via less but robust regions. The overall procedure of the proposed method is shown in Fig. 2.\nProblem Formulation\nGiven a ResNet backbone as a feature extractor, and a scene recognition image \\(X \\in R^{w\\times h \\times 3}\\) from sample set \\({X_i, Y_i}\\), where the \\(Y_i\\) is the label, the high-level feature map usually is obtained by:\n\\(E = ImageEncoder(X)\\)\nwhere \\(E\\in R^{c\\times d \\times k}\\) is a multi-channels feature map. Then, the predictive function is derived as follows:\n\\(y = f(GAP(E))\\)\nwhere GAP(\\(.\\)) denotes the global average pooling operation. Our goal in scene recognition is simply to transfer to a downstream task of fine-tuning the ResNet, i.e., utilizing high-level semantic features for predicting the label.\nHigh-level Semantic Feature Selection\nWe propose a feature selection method based on mask mechanism, aiming to automatically screen high-level seman-tic features to improve the performance of the model. In the traditional feature extraction process, all feature maps are usually directly used for subsequent task processing. However, not all features contribute equally to the target task. In order to improve the efficiency and accuracy of the model, we introduced a mask \\(M\\in [0,1]^{k\\times d}\\) based feature selection mechanism that enables the model to automatically learn how to filter out the most important high-level semantic features (1 represents preservation, 0 represents suppression). Equation 2 is derived as follows:\n\\(y = f(GAP(E_s)), E_s = E \\odot M\\),\nwhere \\(\\odot\\) represents the Hadamard product.\nThrough training, the model will automatically update the mask matrix M, The parameters in M preserve important features while suppressing redundant or irrelevant features.\nImportance Regularization\nWe expect to maximize the response of crucial high-level information with fewer regions. Then an L1-norm-based im-portance regularization term is introduced as follows:\n\\(l_{reg} = \\sum_{i=1}^{kxd} |m_i|, m_i \\in M\\).\nThis regularization term encourages sparsity in the mask by pushing the values of many elements \\(m_i\\) to zero, thus, reducing the contribution of unrelated high-level semantic feature regions. The total optimization target consists of the prediction loss and the importance regularization loss, i.e., \\(l_{total} = l_{pre} + \\lambda l_{reg}\\), where \\(\\lambda\\) is a hyperparameter to adjust the importance regularization loss. The \\(l_{pre}\\) is the CrossEntropy loss."}, {"title": "III. EXPERIMENTS", "content": "This section validates the effectiveness of the proposed method through extensive experiments. First, we introduce the experimental settings. Then, we present the comparative results of multiple models and our method on two datasets. Next, we further demonstrate the effectiveness of our method through visualization experiments and robustness experiments. Finally, we analyze the hyperparameter sensitivity.\nExperiment Setting\na) Datasets: The first dataset is our self-built UGS (Underwater Geological Scene) dataset, which contains seabed photos. These photos are taken by our Fendouzhe Striver deep-sea manned submersible using an underwater high-definition camera, with a resolution of 3648 x 2736. The photos are captured on the seabed of the East China Sea. The images in this dataset are divided into two categories: sediment and rock, with approximately 500 images per category, as shown in Fig. 3. The second dataset is the UCM dataset [7], which contains 21 categories of remote sensing images, with 500 images per category. Both datasets are divided into training, validation, and test sets in a ratio of 60%, 20%, and 20%, respectively.\nb) Baselines: We select three types of classic image classification methods for our experiments, including i) ResNet variants (ResNet18, ResNet50, ResNet101) [24], [25]; ii) MobileNet variants (MobileNet v2, MobileNet v3 small, MobileNet v3 large) [26], [27]; iii) ViT variants (ViT Base 16, ViT Base 32, ViT Large 16, ViT Large 32) [28], where 16 and 32 represent the patch size (length and width) of ViT.\nc) Implementation Details: All models are trained on a single V100 GPU, with a batch size of 16. All images are resized to 224x224 before training and testing. \\(\\lambda\\) is set to 0.1. For all experiments, we use the Adam optimizer [29], with a learning rate set to 1e-4 or 1e-3. The training is conducted for up to 200 epochs, with early stopping if the validation loss does not decrease for ten consecutive epochs.\nComparative Experimental Results\nIn the comparison experiments, we use ResNet vari-ants, MobileNet variants, VIT variants, and our proposed ResNet+mask matrix M method. Additionally, to validate the effectiveness of the mask matrix M, we also incorporate it into the MobileNet variants and conduct experiments. To eliminate the influence of randomness, each model is tested five times on each dataset, and the mean accuracy, standard deviation, and minimum accuracy are recorded. The comprehensive experimental results are shown in Table I. The best results are highlighted in red, and the second-best results are marked in blue. As shown in Table I, after incorporating the mask matrix M, the average accuracy of all models significantly improves, and the accuracy variance decreases for almost all"}, {"title": "IV. CONCLUSION", "content": "In this work, we propose a novel method that utilizes a mask-based feature selection mechanism to facilitate the model's focus on fewer yet robust high-level feature regions for the scene recognition task. An importance-based reg-ularization is developed to further emphasize those robust areas, preventing misclassification between similar scenes. This method is plug-and-play and can be applied to any CNN-based method. To simulate real-life scenarios, we conduct an underwater scene recognition dataset, called UGS, for evaluation. It contains a variety of real underwater collection scenes, covering practical situations such as biological occlu-sion and light blur. Extensive experimental results showcase the effectiveness and robustness of our method."}]}