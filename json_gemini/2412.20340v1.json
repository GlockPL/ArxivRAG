{"title": "Distilling Desired Comments for Enhanced Code Review with Large Language Models", "authors": ["Yongda Yu", "Lei Zhang", "Guoping Rong", "Haifeng Shen", "Jiahao Zhang", "Haoxiang Yan", "Guohao Shi", "Dong Shao", "Ruiqi Pan", "Yuan Li", "Qiushi Wang", "Zhao Tian"], "abstract": "There has been a growing interest in using Large Language Models (LLMs) for code review thanks to their proven proficiency in code comprehension. The primary objective of most review scenarios is to generate desired review comments (DRCs) that explicitly identify issues to trigger code fixes. However, existing LLM-based solutions are not so effective in generating DRCs for various reasons such as hallucination. To enhance their code review ability, they need to be fine-tuned with a customized dataset that is ideally full of DRCs. Nevertheless, such a dataset is not yet available, while manual annotation of DRCs is too laborious to be practical. In this paper, we propose a dataset distillation method, Desiview, which can automatically construct a distilled dataset by identifying DRCs from a code review dataset. Experiments on the CodeReviewer dataset comprising more than 150K review entries show that Desiview achieves an impressive performance of 88.93%, 80.37%, 86.67%, and 84.44% in terms of Precision, Recall, Accuracy, and F1, respectively, surpassing state-of-the-art methods. To validate the effect of such a distilled dataset on enhancing LLMs' code review ability, we first fine-tune the latest LLaMA series (i.e., LLaMA 3 and LLAMA 3.1) to build model Desiview4FT. We then enhance the model training effect through KTO alignment by feeding those review comments identified as non-DRCs to the LLMs, resulting in model Desiview4FA. Verification results indicate that Desiview4FA slightly outperforms Desiview4FT, while both models have significantly improved against the base models in terms of generating DRCs. Human evaluation confirms that both models identify issues more accurately and tend to generate review comments that better describe the issues contained in the code than the base LLMs do.", "sections": [{"title": "I. INTRODUCTION", "content": "Code review is a crucial component of modern software development and has been widely applied in the development of software systems [1]. The primary objective of most re- view scenarios is to generate review comments that explicitly identify issues in a code to trigger code fixes before it is executed for quality assurance [2], [3]. We refer to these comments as DRCs (Desired Review Comments). Typically, a DRC should accurately pinpoint the locations of the issues in the code, correctly describe the nature of the issues, and/or lead to meaningful subsequent repairs to the code. However, as code review is generally a lengthy and costly process [3], [4], considerable efforts have been made to automate the process by adopting machine learning or deep learning techniques [5]. In recent years, the emergence of Large Language Models (LLMs) has introduced new possibilities to automated code review [2]. Owing to their stronger semantic understanding capabilities than traditional machine learning methods and general language models, they have the potential to enable more accurate identification of subtle issues in the code. Ad- ditionally, their inherent content generation capabilities allow them to generate better review comments [2], [5]. However, existing LLM-based solutions may not be able to effectively generate DRCs for various reasons such as"}, {"title": "II. RELATED WORK", "content": "In this section, we describe related work to our study, including automated code review and applications of LLMS in software engineering.\nAutomated code review\nCode review, as an essential process in software devel- opment, has garnered widespread attention from researchers [4], [18]. Given that code review may consume a significant amount of reviewers' effort and time [3], [18], researchers have increasingly focused on building automated review systems to assist reviewers. An automated review system typically com- prises two components: defect detection and review comment recommendation/generation.\nDefect detection is used to find potential issues contained in the code snippets under review. For example, DACE [19] uses CNN and LSTM techniques to extract Diff features from the code, thereby predicting the quality of code Diff patches. Some pre-trained models also have been used to assess code quality, such as CodeBert [20] and CodeT5 [21]. CodeBert [20] is a bimodal pre-training model designed for programming languages and natural language. It performs well in tasks such as natural language-based code search and code documentation generation. CodeT5 [21] leverages a unified framework to support both code understanding and generation tasks, thereby facilitating multi-task learning. This method exhibits superior performance compared to previous techniques in several relevant tasks such as code understanding [20] and generation [22].\nReview comment recommendation/generation produces re- view comments through retrieval or generation methods. For example, CommentFinder [23] uses deep learning techniques to retrieve relevant code review comments, thereby reducing the time reviewers spend writing review comments. DCR [24] learns the similarity between code commit Diffs and review comments to retrieve review comments related to a specific code commit. CodeReviewer [5] achieves notable results in code defect detection, code review comment generation, and code repair tasks by constructing pre-training tasks targeted at code review in an end-to-end manner. LLaMA-Reviewer [2] introduces LLMs into code review tasks, using low-parameter fine-tuning techniques to fine-tune LLaMA, achieving impres- sive results in review comment generation. It is worth noting that the above two studies use the same dataset [5] for model training and verification. They assume the existence of review comments indicates ground truth without considering whether the review comments actually pertain to the code fixes.\nLarge language models for software engineering\nRecent years have witnessed widespread applications of LLMs in various software engineering tasks, especially in those related to code. For example, CodeLLaMA [25], an LLM by fine-tuning LLaMA2 [26] with a large amount of source code, achieves good performance on various code tasks. DeepSeek Coder [27] is pre-trained on 2 trillion tokens across"}, {"title": "III. METHODOLOGY", "content": "The primary objective of this research is to develop an LLM-based solution that is effective in generating DRCs for code review tasks. The pivotal component of our research methodology is to construct a customized dataset that contains a high proportion of DRCs with a novel dataset distillation method. Subsequently, with such a dataset, we first fine-tune the base model of LLaMA-3 and LLaMA-3.1 to develop the code review model of Desiview4FT and then align De- siview4FT to develop an enhanced model of Desiview4FA.\nDesiview: Constructing a distilled dataset\nThe proposed Desiview dataset distillation method com- prises two main steps: (1) identification of DRCs, and (2) dataset preparation and pre-processing.\nIdentification of DRCs: In theory, during a generation process, an LLM gradually generates content by continuously sampling data from the probability distribution of the next token, in which tokens with higher probabilities are more likely to be selected. When the average probability of the given answer is higher, the model is considered to be more certain about that answer. Based on this principle, researchers [44], [45] have proposed the concept of 'perplexity' and used it to evaluate models and guide the selection of hyperparameters [45]. Generally, the definition of 'perplexity' is as follows:\n$PPL(X) = exp\\{-\\frac{1}{N}\\sum_{i=1}^{N}logP(X_i|X<i)\\}$\nwhere $X = (x_0, X_1, ..., X_N)$ is the answer to be evaluated, $x_i$ is the i-th token, $logP(x_i|x_{<i})$ is the log-likelihood of the i-th token given the preceding tokens, and $N$ is the total number of tokens to be calculated. Perplexity is used to evaluate the model's ability to uniformly predict a specified set of tokens for a given content. The higher the perplexity, the lower the probability that the model successfully generates the given content, and vice versa.\nFor a code review task, the reviewer first writes review comments $R$ based on the original code commit $C_o$, denoted by $P(R|C_o)$. Subsequently, the developer writes code fixes $C_r$ based on the review comments $R$, denoted by $P(C_r|C_o, R)$, as shown in the upper left of Fig. 2. Since DRCs should lead to code fixes, as pointed out in several studies [3], we can calculate the desiredness score of review comments $DS$ according to the following formula:\n$DS = -(PPL(P(C_r|C_o, R)) \u2013 PPL(P(C_r|Co)))$"}, {"title": "IV. EVALUATION", "content": "In this section, we validate the performance of the Desiview dataset distillation method for identifying DRCs and examine the effect of using the distilled dataset to fine-tune and align LLMs on their ability to perform code review tasks. Specifi- cally, we aim to answer the following research questions:\nRQ1: How accurately can the dataset distillation method identify DRCs?\nRQ2: How much performance enhancement can LLMs gain by being fine-tuned and aligned with the distilled dataset?\nRQ1 aims to gauge the effectiveness of the proposed De- siview dataset distillation method in identifying desired review comments and subsequently constructing a high-quality dis- tilled dataset compared to that of existing alternative methods. RQ2 aims to test the hypothesis that LLMs fine-tuned and aligned with the distilled dataset (specifically Desiview4FT and Desiview4FA) can generate more desired review comments than those fine-tuned and aligned with the original dataset (specifically LLaMA-Reviewer).\nExperimental settings\nDataset. The base dataset is CodeReviewer dataset [5], which is the only public multi-programming language dataset for code review research in the open-source community and has been widely used in several studies [2], [5]. Besides, as pointed out in Section III-A, it is so far the only publicly available dataset that contains code snippets before and after the review as well as the review comments that meet the needs of this study.\nBenchmark approaches. For RQ1, we aim to compare the effectiveness of different methods for identifying DRCs. We choose the 10-line rule [17], GPT-3.5, and GPT-4o as the benchmark methods. The first method is one of the few rule- based approaches and has been adopted by several studies [17], [47]. Meanwhile, the GPT family has been widely used in numerous studies as a benchmark method for text comprehension and analysis. The latter, in particular, has been confirmed by many studies as one of the strongest LLMS"}, {"title": "V. DISCUSSION", "content": "The primary contribution of this work is the dataset dis- tillation method, Desiview, which can be used to construct a distilled dataset for fine-tuning LLMs to enhance their performance in code review tasks. This contribution has a pro- found impact on LLM-based code review research and can be generally extended to other LLM-based software engineering tasks. In this section, we discuss the implications of dataset distillation in code review research.\nDistilled dataset for training code review models\nTraining data is fundamental to generating effective auto- mated code reviews. The results of our study partially demon- strate the value a distilled dataset brings to LLM-based code review, confirming that distilled data often leads to better LLM performance [8], [32]. However, acquiring distilled datasets is generally challenging. Specifically, in the field of code review, previous studies have not effectively addressed this issue, aside from the extremely costly manual annotation methods [11], [12]. By leveraging the relationship between the content of review comments and the code fixes as a criterion, we intro- duce the perplexity metric to the quality assessment of code review data in terms of the desiredness of review comments. The proposed dataset distillation method enables automated and reliable acquisition of large-scale, high-quality review data at a low cost, thereby effectively addressing the scarcity of high-quality code review datasets. More importantly, we believe this method has the potential to be customized and generalized to support other code review objectives, such as performance bottlenecks [51] and security risks [52] and even other software engineering objectives, such as vulnerability detection [53]. Fig. 2 (upper left corner) illustrates a typical pull-request development mode widely used in the open-source community. Drawing on the principles behind Desiview, it"}, {"title": "VI. THREATS TO VALIDITY", "content": "In this section, we discuss several validity risks.\nDefinition of quality in terms of desiredness: In this pa- per, we define a high-quality code review dataset as one com- posed of only DRCs. This approach inevitably carries some validity risks. The concept of DRCs in this paper specifically refers to review comments that can trigger subsequent code fixes and improvements. This concept is based on multiple code review studies, all of which regard the detection of issues and triggering of subsequent code fixes as their primary purpose [3]. Our study does not intend to diminish other purposes or the unique meaning of DRCs in these scenarios. For instance, as shown in Fig. 1 (example C), acknowledging a developer's fix can also be meaningful to the developer. In fact, during our exploration process, we found that the high- quality dataset distilled according to our definition of DRC is"}, {"title": "VII. CONCLUSIONS", "content": "In this work, we propose a method for analyzing, assessing and automatically identifying DRCs. This solves one critical problem of automated construction of high-quality datasets for code review research. Empirical experiments reveal that this method surpasses all other methods in terms of identifying DRCs, including GPT-4o, in terms of accuracy. Using this method, we constructed a distilled dataset containing a high proportion of DRCs, which not only can be used to train a model to predict whether a new review comment is DRC but also support fine-tuning and aligning LLMs to perform better code review tasks in terms of generating DRCs. Both automated evaluation and human evaluation reveal that LLMS trained with the distilled dataset outperform those trained with the original dataset. Future work includes applying the pro- posed dataset distillation method to construct datasets suitable for different code review objectives, during which better LLMs can be leveraged to improve the accuracy of high-quality review comment identification. Additionally, using newer and stronger LLMs as the base models, along with new techniques for fine-tuning and alignment can also be explored to further enhance the application efficacy of distilled datasets."}]}