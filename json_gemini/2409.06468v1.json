{"title": "AN EFFECTIVE CONTEXT-BALANCED ADAPTATION APPROACH FOR\nLONG-TAILED SPEECH RECOGNITION", "authors": ["Yi-Cheng Wang", "Li-Ting Pai", "Bi-Cheng Yan", "Hsin-Wei Wang", "Chi-Han Lin", "Berlin Chen"], "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have become standard practice for various commercial applications. However, in real-world scenarios, the long-tailed nature of word distribution often leads E2E ASR models to perform well on common words but fall short in recognizing uncommon ones. Recently, the notion of a contextual adapter (CA) was proposed to infuse external knowledge represented by a context word list into E2E ASR models. Although CA can improve recognition performance on rare words, two crucial data imbalance problems remain. First, when using low-frequency words as context words during training, since these words rarely occur in the utterance, CA becomes prone to overfit on attending to the <no-context> token due to higher-frequency words not being present in the context list. Second, the long-tailed distribution within the context list itself still causes the model to perform poorly on low-frequency context words. In light of this, we explore in-depth the impact of altering the context list to have words with different frequency distributions on model performance, and meanwhile extend CA with a simple yet effective context-balanced learning objective 1. A series of experiments conducted on the AISHELL-1 benchmark dataset suggests that using all vocabulary words from the training corpus as the context list and pairing them with our balanced objective yields the best performance, demonstrating a significant reduction in character error rate (CER) by up to 1.21% and a more pronounced 9.44% reduction in the error rate of zero-shot words.", "sections": [{"title": "1. INTRODUCTION", "content": "Thanks to the scalability and streamline nature of end-to-end (E2E) neural models, general-purpose E2E automatic speech recognition (ASR) systems have achieved unprecedented success, significantly outperforming conventional hybrid ASR models on various benchmark tasks [1]. In real-world scenarios, the long-tailed nature of word distribution causes E2E ASR systems to excel with common words but struggle with uncommon ones [2][3][4]. This, however, is detrimental, particularly because uncommon words, such as domain-specific terms (e.g., contact and geo-location names), often play essential roles, and their misrecognition can severely impact downstream tasks like natural language understanding.\nIn recent years, significant efforts have been made to address this long-tailed issue by incorporating contextual information into E2E ASR systems. This family of methods is known as contextual ASR (CASR) [5]. CASR can be broadly categorized into two approaches: 1) post-training integration of external language models (LM); and 2) integration of contextual information during training. Representative methods of the first category include on-the-fly rescoring with domain-specific n-gram or neural language models [6], shallow fusion with a domain-biased weighted finite-state transducer (WFST), and others [7][8][9]. In the second category, the most prevalent methods involve neural contextual adaptation [10][11][12][13][14]. These methods typically encode a list of context words carrying specific contextual information into corresponding embeddings. These embeddings are then infused into the ASR model using a cross-attention mechanism [15] and jointly optimized with the ASR model. Furthermore, the integration of contextual embeddings for context words into the ASR model can be operationalized either in a latent space or in the output distribution [16][17] of the ASR model. Among the various contextual adaptation methods, the contextual adapter (CA) [12] has shown considerable effectiveness in enhancing the performance of recognition on uncommon words. We, however, in this paper, we identify two significant data imbalance issues that need to be addressed for CA. The first issue pertains to the context/no-context imbalance problem. When low-frequency words are used as context words during training, the CA model becomes prone to overfit on attending to <no-context> tokens due to higher-frequency words not being present in the context list. The second issue is the long-tailed distribution within the context list itself, causing the model to perform poorly on low-frequency context words. As shown in Figure 1, CA recognition performance decreases as the word frequency in the training set decreases.\nIn light of this, we explore in-depth the impact of altering the context list to have words with different frequency distributions on model performance, and meanwhile extend CA with a simple yet effective context-balanced learning objective for context words. Specifically, we guide the attention mechanism in the adapter by penalizing the attention score of high-frequency context words and promoting attention to rare ones. A series of experiments conducted on the AISHELL-1 benchmark dataset [18] suggests that using all vocabulary words as the context list and pairing them with our balanced objective yields the best performance, validating the effectiveness of our proposed methods.\nIn summary, our contributions are at least three-fold:\nIdentification of data imbalance problem in contextual adapter: We identify a previously under-explored issue of data imbalance in the rarity of training context lists and its impact on model performance for CASR models.\nBalanced learning objective: We introduce a novel balanced learning objective that penalizes high-frequency context words and promotes rare ones, effectively guiding the attention mechanism within the contextual adapter.\nEmpirical validation: Through extensive experiments on the AISHELL-1 dataset, we demonstrate that using all vocabulary words as the training context list and pairing them with our balanced learning objective achieve the best performance, validating the effectiveness of our proposed methods."}, {"title": "2. BACKGROUND", "content": "2.1. Long-tailed learning\nIn the scenario of long-tailed learning [19], a small fraction of words have many instances, while most words have only a few instances, as depicted in Figure 2 (blue line). This task is challenging due to two following predicaments: Imbalanced data across words causes ASR models to be biased towards the head words, resulting in poor performance on the tail words. The scarcity of tail words makes it even more challenging to train a robust model that performs well on rare word recognition. To improve the recognition performance of rare words, a contextual adapter (CA) infuses a predefined context list constructed from a set of rarely occurring words into the ASR model. Before delving into the long-tailed problem of CA, we will first introduce some mathematical notions related to long-tailed word recognition, followed by an introduction to our main ASR model. Lastly, we will describe the contextual adapter architecture.\nLet nw be the total number of occurrences of a specific word w in the training set and N = \u2211wnw denote the total word occurrences in the training set. In long-tailed learning, words are typically sorted by their occurrence n in descending order, as shown in Figure 2 (blue line).\n2.2. Attention-based encoder decoder (AED) model\nAn AED model generally consists of an encoder network and a decoder network. Given an audio signal O and its corresponding word sequence W = (W1,W2,\u2026, Wu). Let X = (X1,X2,\u2026, X7) represent the acoustic feature vectors extracted from O, and y = (y1, y2,\u2026, yM) be the corresponding character sequence of W. The encoder network, Encaco (\u00b7), processes a sequence of acoustic feature vectors, X, and produces a high-level representation, Haco = Encaco(X), where Haco \u2208 RT\u00d7D. The decoder network, Dec(\u00b7), then fuses the previously decoded text tokens, y1:m = (y1, y2,\u2026, ym), with the processed acoustic embeddings"}, {"title": "2.3. Contextual adapter (CA)", "content": "To improve the recognition accuracy of uncommon or even unseen words during inference, ASR contextual adaptation incorporates a predefined context word list to assist the ASR model in recognizing uncommon words. For instance, in a customer service call center, an ASR system can leverage the caller's account details and previous interactions to provide more accurate recognition.\nThe contextual adapter (CA) is one of the most effective contextual adaptation methods among recent studies. CA introduces two additional components to the original ASR model: a context encoder, denoted by Encctx (\u00b7), and an attention-based adapter. Let CY = {w | n \u2264 y} be a context list for training CA, where y \u2208 Z+ is a threshold that determines the word rarity in the context list and S be the size of Cr. The context encoder projects a list of context words into their corresponding context embeddings, represented by Hctx = Encctx (CY), where Hctx \u2208 RS\u00d7D. The attention-based adapter then integrates these context embeddings into the model through a cross-attention mechanism. Specifically, the CA module is inserted between the encoder network and the decoder network of an AED model. The process of cross-attention and the attention score of the context word cs at time t can be expressed by\nqt = Whacoks, ks = WKhctx, vs = WV hctx, (2)\nP(cs | xt) = \\frac{\\exp (qt \\cdot ks / \\sqrt{D})}{\\sum_{s'=1}^{S} \\exp (qt \\cdot ks' / \\sqrt{D})}, (3)\nwhere WQ, WK, and WV represent three distinct learnable linear transformation matrices. Here, hact \u2208 Haco is the t-th acoustic embedding, while, hctx \u2208 Hctx is the s-th context embedding. The attention score P(cs | xt) is used to compute a weighted sum of the value embeddings to form the biasing vector: baco = \u2211s\u2208S P(cs | xt) vs. The acoustic biasing matrix Baco = (baco1, baco2, \u2026, bacoT) is subsequently utilized to refine the intermediate representations of the acoustic embedding Haco by element-wise addition \u0124aco = Haco + Baco.\nFinally, the output probability of a possible next token ym+1 can be derived by:\nP(ym+1 | X, CY, y1:m) = Dec(\u0124aco, y1:m). (4)\n2.3.1. Training CA\nIt is worth noting that, the size of the context list, Cr, is typically very large (e.g., 10,000). To address this issue during the training phase, random sampling is often employed to create a more tractable subset as a surrogate for Cr, denoted by \u0108Y. This subset \u0108 consists of S context words, where S \u00ab S. It includes the reference context words Cref, context words chosen randomly from the full list CY, and a special token <no-context>, which serves as a fallback when there are no relevant context words to consider. Specifically, the reference context list Cref only contain the words that exist both in the reference sentence W and the full context word list CY, which can be describe as Cref = {wu | wu \u2208 CY for u = 1,2, \u2026, U}.\nThe context word subset \u0108 is then utilized to speed up the training of CA. This random context word sampling procedure is closely related to the concept of sampled SoftMax [20]."}, {"title": "3. DATA IMBALANCE PROBLEMS IN CA", "content": "Although the contextual adapter (CA) can enhance the ASR performance on uncommon words, our study has identified two significant data imbalance issues that need to be addressed. The first issue pertains to the imbalance problem of context/no-context words, and the second involves the long-tailed distribution of the context list itself.\n3.1. Context/no-context imbalance rate\nWhen low-frequency context words are utilized during CA training, most CA studies use named entities or rare words as the context list [12][16][17][21], which typically occur around 16 times or less in the training corpus. This would lead to the CA module overfitting on only attending to the <no-context> token. This overfitting happens because higher-frequency or common words occurring in a training utterance are usually absent from the context list, causes the model to inevitably fallback to the <no-context> token. This problem"}, {"title": "3.2. Long-tailed problem inside the context list", "content": "The long-tailed distribution within the context list, Cr, would often make the CA module perform poorly on low-frequency context words, as shown in Figure 3(b). To work around this problem, we extend the CA module with an effective context word balanced training objective, which will be described in the next section."}, {"title": "4. CONTEXT-BALANCED ADAPTER", "content": "In this section, we introduce our novel context-balanced adaptation approach, aimed at alleviate the imbalance problem of context/no-context words for CA training and the issue of long-tailed distribution within the context word list. Unlike previous studies that primarily used low-frequency words as context words for CA training, our approach utilizes all words in the training corpus as the context word list, effectively reducing the imbalance rate between context/no-context words. To address the long-tailed distribution problem within the context list, we designed an effective context-balance objective. This objective guides the attention mechanism in the adapter by penalizing the attention score of high-frequency context words and promoting attention to rare ones. The context-balance objective can be formulated as follows:\nLbalance = \u2211cs\u2208Cref w(cs)logP(cs), (5)\nP(Cs) = \\frac{1}{T}\u2211t=1T P(Cs | xt), (6)\nwhere Cref is a set of reference context words, w(c) is a re-balancing function, and P(cs) is the context prior probability estimated from the posterior probability, i.e., the attention score from the contextual adapter (c.f. Section 2.3.1). Originally, the re-balance function can simply return the inverse context word frequency. However, recent studies have shown poor performance with this strategy. Instead, the class-balanced loss (CB) [22] introduced a novel concept of effective number to approximate the expected sample number of different classes (in this case, words), which is an exponential function of their training label number. Following this, our re-balance function w(cs) = (1 \u2212 \u03b1)/(1 \u2212 \u03b1ncs) is defined to be inversely proportional to the effective number of context words n, where \u03b1 is the hyperparameter controlling the degree of re-weighting. Specifically, \u03b1 = 0 corresponds to no re-weighting, and \u03b1 = 1 corresponds to re-weighting by inverse word frequency.\nApart from our context-balance objective, we also leverage the connectionist temporal classification (CTC) loss to speed up the convergence of the adapter and guide the attention to be aligned with the correct context word at the correct frame, as proposed in [23]. The CTC guidance objective can be derived as follows:\nLctc = \u2212log  \u2211(c1,\u2026,cT)\u2208\u03b2\u22121(C)  \u220ft=1T P(ct = cs), (7)\nwhere \u03b2\u22121() returns all possible alignments compatible with the reference context word sequence C, and ct \u2208 CY. The overall objective function of our context-balanced adapter becomes:\nL = \u03bb1Laed + (1 \u2212 \u03bb1)(\u03bb2Lctc + (1 \u2212 \u03bb2)Lbalance), (8)\nwhere Laed is the loss function of the original AED model, and \u03bb1 and \u03bb2 are the multi task learning hyperparameters."}, {"title": "5. EXPERIMENTAL SETUP", "content": "5.1. Dataset and evaluation metrics\nOur experiments were conducted on AISHELL-1 dataset, a widely-used open-source speech corpus for assessing Chinese ASR systems. AISHELL-1 comprises over 170 hours of Mandarin speech data from various domains, including \"Finance,\" \"Science and Technology,\" \"Sports,\" \"Entertainment,\" and \"News.\"\nWe assessed our method using the character error rate (CER) and context CER (C-CER). Additionally, we calculated word-level error rates and context word-level error rates for many-shot (nw > 100), medium-shot (100 \u2265 nw > 20), few-shot (20 \u2265 nw > 0), and zero-shot (nw = 0) scenarios.\n5.2. Baseline and model configuration\nWe used a Conformer-Transformer model as the backbone for long-tailed speech recognition experiments. The network consists of a Conformer encoder [24] and a Transformer decoder [15] (denoted by Conformer for short). The Conformer encoder consists of 12 blocks, each with 2,048 hidden units"}, {"title": "5.3. Context word list configurations", "content": "Since Chinese lacks explicit word boundaries, we first perform word segmentation on the AISHELL-1 corpus, and filtering out single-character words (e.g.,\u201c\u6211,\u201d\u201c\u4ed6\u201d).\nTraining phase: For constructing the context list during model training, we set y = 24 for the CA method, which is a common setting, and to y = 216 for our context-balanced adapter, which includes all words from the training corpus. The size of the training context list subset S is set to 200 for all experiments mentioned above.\nInference phase: For constructing the context list for model evaluation, we collected words that occur less than 10 times in the test set to form our testing context list. For all evaluation, we set S to 200."}, {"title": "6. EXPERIMENTAL RESULTS", "content": "6.1. Main results\nTable 1 presents experimental results on the test set of AISHELL-1 for the baseline ASR model equipped with various contextual adapters (viz., iconic contextual adapter and the proposed context-balanced Adapter). For each method, the CER and C-CER is reported to evaluate the holistic model performance, and the word-level error rate is presented to assess the accuracy of different frequency of words and context words. From Table 1 we can make the following observations. First, for the baseline system, the word-level error rate of the rare words is significantly higher than that of common words. For example, there is a notable performance gap in error rate between many-shot and few-shot testing conditions. This"}, {"title": "6.2. Detailed analysis", "content": "Impact of different parameter settings on the proposed context-balanced adapter. The upper part of Table 2 presents an empirical study on the AISHELL-1 test set, examining the impact of various parameter settings of y and \u03bb2 values on overall performance of the proposed method, where y controls the rareness of the training context word list, while \u03bb2 determines the combination weight of the context-balanced objective.\nAt outset of experiments, we fix \u03bb2 = 1, and probe the impact of the rarity of the training context word list. From this table, we can observe that using high-frequency words as the context list (y = 216) in training phase can improve the performance of the contextual adapter. However, this performance gain is primarily driven by improvements in many-shot and medium-shot words, with limited enhancement observed for unseen words. This underscores that long-tailed distribution problems also exist in the context list. Next, we examine the usage of the context list in the proposed context-balanced adapter by setting \u03bb2 = 0.5 with various rareness y. We can observe that the combination of using a high-frequency context list y = 216 and incorporating the context-balanced loss \u03bb2 = 0.5 yields the best performance in terms of CER, C-CER, and across all shot categories. It is noteworthy that after adding the balanced objective for model training, the recognition performance for many-shot and medium-shot words diminishes slightly, but the performance for few-shot and unseen words improves significantly, as visually displayed in Figure 1.\nImpact of different settings of \u03b1. The button part of Table 2 presents an empirical study on the AISHELL-1 test set, examining the impact of different context-balanced hyperparameter \u03b1 values on overall ASR performance. Following Cui et al. [22], we report various \u03b1 values {0.9, 0.99, 0.999, 0.9999} in the training phase. It is evident that setting \u03b1 to 0.9 yields the best performance.\nQualitative analysis of the attention weights in the contextual adapter. In Figure 4, we demonstrate the attention weights in the contextual adapter to visualize the impact of training the contextual adapter with the context-balanced objective. Given a test utterance, \u201c\u56e0\u70ba\u805a\u96c6\u4e86\u904e\u591a\u516c\u5171\u8cc7\u6e90,\u201d we can observe that the attention weights of contextual adapter tends to densely distribute on the <no-context> token. To cope with this issue, integrating the proposed context-balanced loss into the optimization process of contextual adapter"}, {"title": "7. CONCLUSION", "content": "In this paper, we have addressed the challenges posed by the long-tailed distribution of words in E2E ASR systems. We identified the issue of data imbalance in training context lists and its impact on ASR performance, particularly for rare words. To mitigate this, we proposed a novel balanced learning objective that penalizes high-frequency context words while promoting attention to rare ones, for enhancing the performance of the contextual adapter (CA) model. Our extensive experiments on the AISHELL-1 benchmark dataset demonstrate that incorporating all vocabulary words as context words to form the context list, combined with our balanced learning objective, significantly improves recognition performance, especially for rare words. These results validate the effectiveness of our proposed methods and highlight the importance of addressing data imbalance in contextual ASR systems. Future work will explore more sophisticated forms of the balanced learning objective and investigate its applicability to other languages and datasets. We believe that our approach provides a promising avenue to improve the recognition of rare words in E2E ASR models, facilitating them to be more reliable and amenable to real-world applications."}]}