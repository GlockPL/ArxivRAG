{"title": "Accelerated AI Inference via Dynamic Execution Methods", "authors": ["Haim Barad", "Jascha Achterberg", "Jean Yu", "Tien Pei Chou"], "abstract": "In this paper, we focus on Dynamic Execution techniques\nthat optimize the computation flow based on input. This\naims to identify simpler problems that can be solved us-\ning fewer resources, similar to human cognition. The tech-\nniques discussed include early exit from deep networks,\nspeculative sampling for language models, and adaptive\nsteps for diffusion models. Experimental results demon-\nstrate that these dynamic approaches can significantly im-\nprove latency and throughput without compromising qual-\nity. When combined with model-based optimizations, such\nas quantization, dynamic execution provides a powerful\nmulti-pronged strategy to optimize Al inference.\nGenerative Al requires a large amount of compute re-\nsources. This is expected to grow, and demand for resources\nin data centers through to the edge is expected to continue\nto increase at high rates. We take advantage of existing\nresearch and provide additional innovations for some gen-\nerative optimizations. In the case of LLMs, we provide more\nefficient sampling methods that depend on the complexity of\nthe data. In the case of diffusion model generation, we pro-\nvide a new method that also leverages the difficulty of the\ninput prompt to predict an optimal early stopping point.\nTherefore, dynamic execution methods are relevant be-\ncause they add another dimension of performance optimiza-\ntions. Performance is critical from a competitive point of\nview, but increasing capacity can result in significant power\nsavings and cost savings. We have provided several integra-\ntions of these techniques into several Intel performance li-\nbraries and Huggingface Optimum. These integrations will\nmake them easier to use and increase the adoption of these\ntechniques.", "sections": [{"title": "1. Introduction", "content": "The computational requirements for AI inference are\ngrowing at a rapid rate. Larger models and increased de-\nmand (i.e. queries directly or via APIs) in these models\ndrive higher inference costs [1][2]. To control the energy\ndemands and hardware requirements of the newest gener-\nation AI models, we must find ways to run large-scale AI\narchitectures more efficiently while achieving the same ac-\ncuracy and performance levels. Past years of research have\nresulted in new methods to both make models more effi-\ncient by accelerating the inference of models and making\nthe models themselves more lightweight.\nWhen thinking about the efficiency of models, we want\nto optimize the computational resources needed to achieve\na given output of a computation. Given a specific output of\na computational / inference process, we can aim to tweak\nthe power consumption, throughput, and latency of a target\narchitecture. In this paper, we are starting with an overview\nof very established model compression techniques (quan-\ntization, sparsity), which can address power consumption\nthrough lowering the memory demand. However, our pri-\nmary focus is on methods to improve model throughput by\nallowing models to take shortcuts during the inference pro-\ncess. Note that model throughput is a very interesting tar-\nget, as it is directly correlated with the total costs of serving\nthe model as a service. Alongside a review of established\ntechniques, we will also provide an overview of novel re-\nsults on early exit, speculative sampling in token and fea-\nture space techniques in large language models (LLMs) and\ntechniques to accelerate diffusion models. We show that\nsignificant throughput improvements can be achieved by ex-\npanding established model architectures with the methods\npresented here. Out of the scope of this paper are that many\narchitectural improvements can be made given the represen-\ntation (e.g. ONNX) and runtime engines.\nWe believe that significant improvements that can be\nachieved through the inference optimization techniques pre-\nsented here should not be perceived as the result of arbitrary\ntricks of model optimization. Instead, they all have deep\nroots in the study of intelligent systems. John von Neumann\nhas already discussed that we should be able to build intelli-\ngent systems with low numerical precision given how noisy\nsignal communication is in the brain [3]. Similarly, large\nand powerful brains of mammals also achieve energy effi-"}, {"title": "2. Model Compression: Approximate Math", "content": "Let's dive into the concepts of quantization and spar-\nsity in the context of model compression. By no means are\nthese the only methods to compress a model's size, but we\nwill discuss these in the context of comparing model-based\noptimizations and dynamic execution."}, {"title": "2.1. Quantization", "content": "An excellent overview of both quantization and pruning\ntechniques has been published by Liang et al. [9]. These\npowerful techniques provide for improved performance by\nessentially compressing the model: by quantizing parame-\nters to a lower number of bits and/or eliminating redundant\ncomputations. Both of these methods improve the overall\ncompute requirements at inference time, since the model is\ncompressed. This compression can be achieved by a va-\nriety of methods, but they all come down to a brute-force\nmethod to approximate the uncompressed computation so\nthat the accuracy loss is minimized. In other words, I clas-\nsify these categories as 'approximate math'\nwe compress\nand increase the aggressiveness of compression techniques\nas long as accuracy degradation is still tolerable.\nIf you think about it in a generic way, many neural net-\nwork topologies compute a function:\n$y = F(x)$\nwhich already is some approximation that might help ac-\ncomplish a task. For example, if this is a classification prob-\nlem, the function F(x) might represent a decision boundary\nbetween classes, which is computed as a composite of lay-\ners. So, while F(x) might already be, in some sense, an\napproximation, we consider it to be the most accurate rep-\nresentation of the model. With some approaches that fall\ninto the approximate-math category, we instead compute:\n$y' = F'(x)$\nwhere F' is some approximation of the full computa-\ntional version of the model F(x). Therefore, the goal of"}, {"title": null, "content": "these approximate math techniques is to minimize the dif-\nference between the full math model and the model that has\nbeen compressed.\n$L_c = \\min|y' - y|$\nThis would represent a measure of loss in the compres-\nsion method. Often, we find these compression methods to\nbe valuable, as the compression loss $L_c$ is often tolerable\nand well worth the benefits of a smaller and more efficient\nmodel.\nNote that there are some additional related optimizations\n(e.g. factorization), but these still fit within a brute-force ap-\nproximate math approach. While the rest of this article will\nfocus on dynamic execution optimizations (notably Early\nExit), we should recognize that these two categories of op-\ntimizations can be used together to provide a completely\noptimized solution in many respects (latency, throughput,\npower consumption, cost, etc.)."}, {"title": "2.2. Sparsity", "content": "Sparsity, on the other hand, is a technique that aims\nto make most weights of the neural network zero. Based\non observation, in many models, only a small part of the\nweights contribute significantly to the final predictions.\nMathematically, sparsity can be induced by adding a reg-\nularization term to the loss function during training. This\nterm penalizes the model for having large weights, effec-\ntively pushing many of them towards zero.\nOnce the model is sparse, it can be compressed by\nsimply storing the locations and values of the nonzero\nweights. This significantly reduces the memory footprint\nof the model, as the zero weights do not need to be stored.\nLike quantization, sparsity introduces some approxima-\ntion error. However, if the original model has redundant\nweights (which is often the case), this error can be minimal.\nHowever, it should be noted that this approximation error\ncan be limited to the error in the training data, while the\nvalidation error might decrease due to a better generaliza-\ntion performance of the model [10].\nIn conclusion, both quantization and sparsity are pow-\nerful techniques for compressing neural networks. They\nwork by introducing mathematical approximations that re-\nduce the memory footprint and computational requirements\nof the model, often with minimal impact on accuracy. These\ntechniques are particularly useful for deploying models on\nresource-constrained devices such as mobile phones and\nembedded systems."}, {"title": "2.3. Results on Model Compression", "content": "It is beyond the scope of this paper to dive into model\ncompression techniques. Substantial research and tools are\nwidely available to exploit techniques such as quantization\nand pruning [9]. Often compressed models are publicly"}, {"title": null, "content": "available on hubs, such as HuggingFace, so that further\ncompression is often unnecessary.\nSince these benefits of the techniques often come from\nthe compression itself, there is an inherent speedup just\nfrom the compressed nature of the optimized model. These\nmethods are covered elsewhere and beyond the scope of this\npaper."}, {"title": "3. Dynamic Execution: Taking Short Cuts", "content": "In the dynamic execution scenario, we employ a set of\nspecialized techniques designed to scrutinize the specific\nquery at hand. These techniques involve a thorough exam-\nination of the query's structure, content, and context with\nthe aim of discerning whether the problem it represents can\nbe addressed in a more straightforward manner.\nThis approach mirrors the way humans tackle problem-\nsolving. Just as we, as humans, are often able to identify\nproblems that are 'easy' or 'simple' and solve them with\nless effort compared to 'hard' or 'complex' problems, these\ntechniques strive to do the same. They are designed to rec-\nognize simpler problems and solve them more efficiently,\nthereby saving computational resources and time.\nWe refer to this approach as Dynamic Execution. The\nterm 'dynamic' signifies the adaptability and flexibility of\nthis approach. Unlike static methods that rigidly adhere to a\npredetermined path regardless of the problem's nature, Dy-\nnamic Execution adjusts its strategy based on the specific\nproblem it encounters, that is, the opportunity is data de-\npendent.\nThe goal of Dynamic Execution is not to optimize the\nmodel itself, but to optimize the compute flow. In other\nwords, it seeks to streamline the process through which the\nmodel interacts with the data. By tailoring the compute flow\nto the data presented to the model, Dynamic Execution en-\nsures that the model's computational resources are utilized\nin the most efficient manner possible.\nIn essence, Dynamic Execution is about making the\nproblem-solving process as efficient and effective as pos-\nsible by adapting the strategy to the problem at hand, much\nlike how humans approach problem-solving. It is about\nworking smarter, not harder. This approach not only saves\ncomputational resources but also improves the speed and\naccuracy of the problem-solving process."}, {"title": "3.1. Dynamic Execution Explained", "content": "Early Exit is a strategy with a simple and easy-to-\nunderstand concept, and we will use this to explain the\nmethodology of dynamic execution. We then explore Early\nExit and other techniques aimed at Generative AI and dis-\ncuss the results."}, {"title": null, "content": "Although a deep network can represent a more complex\nand expressive boundary between classes (as shown in the\ncurved area), it is also clear that much of the data can be\nproperly classified with even the simplest of classification\nboundaries (e.g., even a linear classifier as shown by the\nstraight black line). In other words, data points outside\nthe two parallel green lines that bound the complex deci-\nsion boundary of the deep network can be accurately clas-\nsified even with a simple linear decision boundary. Points\nwithin the two parallel green lines are more difficult to dis-\ntinguish and require extra processing to accurately classify.\nWe use a confidence measure to determine whether we have\nenough information to confidently classify the data point at\nthat stage.\nEarly Exit is a category of techniques based on this sim-\nple principle of doing 'enough work' given the difficulty of\nthe input. There are other related methods that we will look\nat in this paper, including methods for generative sampling\nand early stopping of diffusion models.\nIn many respects, the 'Short-Cut Thinking' approach\nclosely matches human intuition, which often seeks short-\ncuts to problem solving, separately treating 'easy' problems\nin a different manner from 'hard' problems. This coarse\nclassification of the input data between easy and hard is\ndone to quickly determine when and how shortcuts can be\nmade to the computation, knowing that easy problems can\noften be treated with significantly less work.\nKeep in mind that with Dynamic Execution, we do not\nchange the model. We change how that model is computed\nby often doing fewer calculations (perhaps skipping unnec-\nessary computations).\nNote that the two optimization paradigms of model com-\npression and dynamic execution are not mutually exclusive.\nThere is no reason why we cannot apply multiple method-\nologies simultaneously to provide a sound optimization ap-\nproach to the inference task.\nIn this paper, we will focus on dynamic execution meth-\nods for inference optimizations, with a focus on Generative\nAI. Some example methods include:"}, {"title": null, "content": "\u2022 Early Exit - This technique involves adding exits at\nvarious stages in a deep neural network (DNN). The\nidea is to allow the network to terminate the inference\nprocess earlier for simpler tasks, thus saving computa-\ntional resources. It takes advantage of the observation\nthat some test examples can be easier to predict than\nothers [11], [12].\n\u2022 Speculative Sampling - This method aims to speed up\nthe inference process by computing several candidate\ntokens from a smaller draft model. These candidate\ntokens are then evaluated in parallel in the full target\nmodel [13], [14]."}, {"title": null, "content": "\u2022 Lookahead Decoding - This is a parallel decoding al-\ngorithm designed to accelerate Large Language Model"}, {"title": null, "content": "(LLM) inference. The method breaks the sequential\ndependency in autoregressive decoding by simultane-\nously extracting and verifying n-grams (cached) di-\nrectly with the LLM [15], [16]."}, {"title": null, "content": "\u2022 Medusa - Medusa is a method that augments LLM in-\nference by adding extra decoding heads to predict mul-\ntiple subsequent tokens in parallel. These heads each\nproduce multiple likely words for the corresponding\nposition. These options are then combined and pro-\ncessed using a tree-based attention mechanism [17]."}, {"title": null, "content": "\u2022 EAGLE (Extrapolation Algorithm for Greater\nLanguage-model Efficiency): EAGLE is a new base-\nline for fast decoding of LLMs with provable perfor-\nmance maintenance. This approach involves autore-\ngression at the feature level (instead of the token level)\nof LLM, allowing a significant increase in generation\nefficiency [18]. An improved version of Eagle dynam-\nically adjusts the tree structure of the candidate tokens\nusing confidence levels, for even better performance"}, {"title": null, "content": "[19]."}, {"title": null, "content": "\u2022 Contextual Sparsity - This technique exploits the ex-\nistence of contextual sparsity, which are small, input-\ndependent sets of attention heads and MLP parame-\nters that yield approximately the same output as the\ndense model for a given input. It can be accurately\npredicted and exploited to speed up LLM inference in\nwall-clock time without compromising LLM's quality\nor in-context learning ability [20]."}, {"title": null, "content": "\u2022 StepSaver: Early Stopping for Diffusion Generation,\nusing an innovative NLP model specifically fine-tuned\nto determine the minimal number of denoising steps\nrequired for any given text prompt. This advanced\nmodel serves as a real-time tool that recommends the\nideal denoise steps for generating high-quality images\nefficiently. It is designed to work seamlessly with the\nDiffusion model, ensuring that images are produced\nwith superior quality in the shortest possible time. [21]"}, {"title": null, "content": "\u2022 LLM Routing: Routing is a form of classification in\nwhich the prompt is used to determine the best model.\nThe prompt is then routed to this model. By best, we\ncan use different criteria to determine the most effec-\ntive model in terms of cost and accuracy. In many\nways, routing is a form of dynamic execution done at\nthe pipeline level where many of the other optimiza-\ntions we are focusing on in this paper is done to make\neach LLM more efficient. For example, RouteLLM\nis an open-source framework for serving LLM routers\nand provides several mechanisms for reference, such\nas matrix factorization. [22]"}, {"title": null, "content": "Please note that these are high-level descriptions, and the\nactual implementation of these methods can be complex and\nmay vary depending on the specific use case and model ar-\nchitecture."}, {"title": "3.2. Experiments", "content": "This section will briefly show the results of experiments\nfrom several different types of dynamic execution. These\nresults were reproduced on Intel platforms. It is beyond\nthe scope of this paper to go into full details of each of the\ntechniques, and references for each of these will be given\nshould the reader want further details."}, {"title": "3.3. Early Exit Results", "content": "We made use of the Early Exit strategy in several en-\ncoder models, including BERT, ROBERTA, and ALBERT.\nWe measured the speed-ups on glue scores for various en-\ntropy thresholds."}, {"title": "3.4. Speculative Sampling Results", "content": "Speculative sampling is a technique designed to accel-\nerate the decoding process of large language models [23],\n[24]. The concept behind speculative sampling is based\non the observation that the latency of parallel scoring of\nshort continuations, generated by a faster but less power-\nful draft model, is comparable to that of sampling a single\ntoken from the larger target model. This approach allows\nmultiple tokens to be generated from each transformer call,\nincreasing the speed of the decoding process.\nThe process of speculative sampling involves two mod-\nels: a smaller, faster draft model and a larger, slower tar-\nget model. The draft model speculates what the output is\nseveral steps into the future, while the target model deter-\nmines how many of those tokens we should accept. The\ndraft model decodes several tokens in a regular autoregres-\nsive fashion, and the probability outputs of the target and"}, {"title": null, "content": "the draft models on the new predicted sequence are com-\npared. Based on some rejection criteria, it is determined\nhow many of the speculated tokens we want to keep. If\na token is rejected, it is resampled using a combination of\nthe two distributions, and no more tokens are accepted. If\nall speculated tokens are accepted, an additional final token\ncan be sampled from the target model probability output.\nIn terms of performance boost, speculative sampling\nhas shown significant improvements. For instance, it was\nbenchmarked with Chinchilla, a 70 billion parameter lan-\nguage model, achieving a 2-2.5x decoding speedup in a dis-\ntributed setup, without compromising the sample quality or\nmaking modifications to the model itself. Another exam-\nple is the application of speculative decoding to Whisper, a\ngeneral purpose speech transcription model, which resulted\nin a 2x speed-up in inference throughput [25], [26]. Note\nthat speculative sampling can be used to boost CPU infer-\nence performance, but the boost will likely be less (typically\naround 1.5x).\nIn conclusion, speculative sampling is a promising tech-\nnique that leverages the strengths of both a draft and a target\nmodel to accelerate the decoding process of large language\nmodels. It offers a significant performance boost, making it\na valuable tool in the field of natural language processing.\nHowever, it is important to note that the actual performance\nboost can vary depending on the specific models and setup\nused."}, {"title": "3.5. EAGLE", "content": "EAGLE, which stands for Extrapolation Algorithm for\nGreater Language-model Efficiency, is a novel form of\nspeculative sampling designed to accelerate the decoding\nprocess of Large Language Models (LLMs) [18]. The key\nprinciple behind EAGLE is that the sequence of LLM fea-\nture vectors is compressible over time, making the predic-\ntion of subsequent feature vectors from previous ones eas-\nier\u00b9.\nUnlike traditional speculative sampling methods, EA-\nGLE operates the drafting process auto-regressively at the\nsecond-to-top-layer feature level. This approach is more\nstraightforward than working at the token level. EAGLE\naddresses the inherent uncertainty in feature-level autore-\ngression by incorporating a token sequence advanced by\none time step. This effectively resolves the uncertainty, en-\nabling precise second-to-top-layer feature prediction with\nminimal overhead [18]."}, {"title": null, "content": "In terms of a performance boost (increase in through-\nput and lower latency), EAGLE has shown significant im-\nprovements. It achieves a 2x speedup on gpt-fast, one of the\nfastest-known open-source inference libraries. It is 3x faster\nthan autoregressive sampling (13B), 2x faster than Looka-\nhead (13B), and 1.6x faster than Medusa (13B)."}, {"title": null, "content": "Moreover, EAGLE provably maintains consistency with\nvanilla decoding in the distribution of generated texts. For\nLLaMA2-Chat 70B, EAGLE achieved a latency speedup ra-\ntio of 2.7x-3.5x, doubled throughput, while maintaining the\ndistribution of generated text [18]."}, {"title": null, "content": "[@Joey - please add a short paragraph or two regard-\ning your results on PVC]\nIn conclusion, EAGLE is a promising speculative sam-\npling technique that provides a significant boost in infer-\nence performance. It maintains the original text distribution\nwhile accelerating the generation process. However, the ac-\ntual performance boost can vary depending on the specific\nmodels and setup used."}, {"title": "3.6. StepSaver for Diffusion Models", "content": "Diffusion models iteratively enhance a random noise sig-\nnal until it closely resembles the target data distribution\n[27]. When generating visual content such as images or\nvideos, diffusion models have demonstrated significant re-\nalism [28]. For example, video diffusion models and Sin-\nFusion represent instances of diffusion models utilized in\nvideo synthesis [29][30]. More recently, there has been\ngrowing attention towards models like OpenAI's Sora; how-"}, {"title": null, "content": "ever, this model is currently not publicly available due to its\nproprietary nature.\nPerformance in diffusion models involves a large num-\nber of iterations to recover images or videos from Gaussian\nnoise [31]. This process is called denoising and is trained\non a specific number of iterations of denoising. The number\nof iterations in this sampling procedure is a key factor in the\nquality of the generated data, as measured by metrics, such\nas FID.\nLatent space diffusion inference uses iterations in feature\nspace, and performance suffers from the expense of many\niterations required for quality output. Various techniques,\nsuch as patching transformation and transformer-based dif-\nfusion models [32], improve the efficiency of each iteration.\nStepSaver dynamically recommends significantly lower\ndenoising steps, which is critical to address the slow sam-\npling issue of stable diffusion models during image genera-\ntion [21]. The recommended steps also ensure better image\nquality."}, {"title": "4. Discussion", "content": "In this paper, we discuss the methodology of dynamic\nexecution methods in the context of inference optimiza-\ntions. Together with model-based optimizations, dynamic\nexecution offers opportunities that are data-dependent and\ndo not conflict with model-based methods.\nWe discussed several methods to optimize inference per-\nformance for different tasks, including Generative AI that\nare successful, cross-platform, and used on Intel products\nto increase inference performance (lower latency and high\nthroughput). Not only do the results show impressive per-\nformance benefits achieved through inference optimization\ntechniques, the techniques themselves also have deep roots\nin theories of efficient information processing by intelligent\nsystems. We therefore think that in the future they should\nnot solely be seen as post-hoc strategies to improve models\nwhich have already been built but should instead consider\nthem in the early steps of building a new model. Given the\neffectiveness of the techniques, considering inference opti-\nmization from the early stages of model building holds the\npromise of leading to a new wave of more efficient AI algo-\nrithms."}]}