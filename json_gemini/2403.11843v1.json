{"title": "Fuzzy Rough Choquet Distances for Classification", "authors": ["Adnan Theerens", "Chris Cornelis"], "abstract": "This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.", "sections": [{"title": "1 Introduction", "content": "In the field of machine learning, there is an ongoing search for algorithms that are more effective, robust, and explainable. Many important machine learning techniques rely on the idea of measuring similarity or dissimilarity between instances. This is commonly done using distance measures, forming the basis for fundamental instance-based classification algorithms like k-nearest neighbours (KNN) and fuzzy-rough nearest neighbours (FRNN, [10,12]), as well as algorithms such as k-means clustering and density-based spatial clustering (DBSCAN) [7] for unsupervised learning, and local outlier factor (LOF) [4] for anomaly detection.\nClassical distance measures, like the Euclidean or Manhattan distance, have been fundamental in traditional machine learning approaches. However, with the growing complexity and diversity of datasets, the shortcomings of these conventional measures are becoming more evident. The need for a more versatile, adaptive, and information-rich distance metric has become more apparent in order to better capture the nuanced relationships within the data. In classification tasks, it is important to ensure that the nearest neighbours belong to the same class. This is because the classification goal is to effectively separate"}, {"title": "2 Preliminaries", "content": "We will make use of the following fuzzy logic connectives."}, {"title": "2.1 Fuzzy rough set theory", "content": "Definition 1. A function $C: [0,1]^2 \\rightarrow [0,1]$ is called a conjunctor if it is increasing in both arguments and satisfies C(0,0) = C(1,0) = C(0,1) = 0, C(1,1) = 1 and C(1,x) = x for all x \u2208 [0,1]. A commutative and associative conjunctor T is called a t-norm.\nA function $I : [0,1]^2 \\rightarrow [0,1]$ is called an implicator if I(0,0) = I(0,1) = I(1,1) = 1, I(1,0) = 0 and for all $x_1,x_2, y_1, y_2$ in [0,1] the following holds:\n1. $x_1 \\leq x_2 \\Rightarrow I(x_1, y_1) \\geq I(x_2,y_1)$ (decreasing in the first argument),\n2. $y_1 \\leq y_2 \\Rightarrow I(x_1, y_1) \\leq I(x_1,y_2)$ (increasing in the second argument),\nA function $N : [0,1] \\rightarrow[0,1]$ is called a negator if it is non-increasing and satisfies N(0) = 1 and N(1) = 0. An implicator I induces a negator $N_I(x) := I(x, 0)$.\nSince t-norms are required to be associative, they can be extended naturally to a function $[0, 1]^n \\rightarrow [0, 1]$ for any natural number $n \\geq 2$.\nRough sets, introduced by Pawlak [14], aim to handle uncertainty related to indiscernibility. Fuzzy rough sets extend this approach to incorporate similarity and fuzzy concepts. We will use the notation $\\mathcal{P}(X)$ to represent the powerset of X and assume X to be finite throughout this paper. Likewise, we will use the notation $\\tilde{\\mathcal{P}}(X)$ to represent the set consisting of all fuzzy sets on X (i.e. functions of the form X \u2192 [0, 1]).\nDefinition 2 (Fuzzy rough sets). [15] Given $R\\in \\tilde{\\mathcal{P}}(X \\times X)$ and $A \\in \\tilde{\\mathcal{P}}(X)$, the lower and upper approximation of A w.r.t. R are defined as:\n$\\underline{apr}_R(A)(x) = \\min_{y \\in X} I(R(x, y), A(y))$, (1)\n$\\overline{apr}_R(A)(x) = \\max_{y \\in X} C(R(x, y), A(y))$, (2)\nwhere I is an implicator and C a conjunctor."}, {"title": "Proposition 1.", "content": "The lower and upper approximation satisfy relation monotonicity, i.e., for $A \\in \\tilde{\\mathcal{P}}(X)$ we have\n$(\\forall R_1, R_2 \\in \\tilde{\\mathcal{P}}(X \\times X)) \\ (R_1 \\subseteq R_2 \\Rightarrow (\\underline{apr}_{R_2}A) \\subseteq (\\underline{apr}_{R_1} A)).$"}, {"title": "2.2 Fuzzy-rough attribute measures", "content": "Consider a family of similarity relations $\\{R_B : B \\subseteq A\\}$ and a similarity relation $R_d$ for the decision attribute. The authors in [5] define the B-positive region $\\text{POS}_B$ as the fuzzy set in X defined as, for y \u2208 X,\n$\\text{POS}_{R_B}(y) = (\\bigcup_{x \\in X} \\underline{apr}_{R_d} (R_B(x)))(y).$"}, {"title": "Proposition 2 ([5]).", "content": "For y \u2208 X, if $R_d$ is a crisp relation,\n$\\text{POS}_{R_B}(y) = \\underline{apr}_{R_d} (R_d(y))(y).$\nThe value $\\text{POS}_{R_B}(y)$ can be interpreted as the degree to which similarity with respect to the conditional attributes B relates to similarity with respect to the decision attribute. Consequently, the predictive ability of a subset B to predict the decision attribute d, also called the degree of dependency of d on B, is defined as:\n$\\gamma_R(B) = \\frac{\\sum_{y \\in X} \\text{POS}_{R_B}(y)}{\\frac{\\sum_{y \\in X} \\text{POS}_{R_d}(y)}{|POS_{R_d}|}}.$"}, {"title": "2.3 Choquet integral", "content": "The Choquet integral induces a large class of aggregation functions, namely the class of all comonotone linear aggregation functions [2]. Since we will view the Choquet integral as an aggregation operator, we will restrict ourselves to measures (and Choquet integrals) on finite sets. For the general setting, we refer the reader to e.g. [22].\nDefinition 4. Let X be a finite set. A function $\u03bc : \\mathcal{P}(X) \u2192 [0, +\u221e[$ is called a monotone measure if:\n\u03bc(0) = 0 and (\u2200A, B\u2208 (P(X))(A \u2286 B \u21d2 \u03bc(A) \u2264 \u03bc(B))).\nA monotone measure is called additive if \u03bc(AUB) = \u03bc(A) + \u03bc(B) when A and B are disjoint. If \u03bc(X) = 1, we call \u03bc normalized.\nDefinition 5 ([22]). The Choquet integral of $f : X \u2192 R$ with respect to a monotone measure u on X is defined as:\n$\\int f d\u03bc = \\sum_{i=1}^n f(x_i) [\u03bc(A_i) \u2013 \u03bc(A_{i+1})]$,\nwhere (x1,x2,...,xn) is a permutation of X = (x1,x2,...,xn) such that\nf(x1) \u2264 f(x2) \u2264 \u2026 \u2264 f(xn),\nAi := {x1,...,xi} and \u03bc(An+1) := 0.\nProposition 3. [2] The Choquet integral with respect to an additive measure \u03bc is equal to\n$\\int f d\u03bc = \\sum_{x \\in X}f(x)\u03bc({x}).$\nProposition 4. [22] Suppose \u03bc1,\u03bc2 are two monotone measures with \u03bc1 \u2264 \u03bc2, then for any $f : X \u2192 R$ we have:\n$\\int f d\u03bc_1 \\leq \\int f d\u03bc_2.$"}, {"title": "3 Fuzzy Rough Choquet Distances", "content": "In this section, we introduce fuzzy rough Choquet distances using the \u03b3 and \u03b4 measures described in Section 2.1. Throughout this section, we will assume (X, A\u222a {d}) is a decision system."}, {"title": "3.1 Fuzzy rough Choquet p-distances", "content": "Inspired by Minkowski p-distances we define the Choquet p-distance as:\nDefinition 6 (Choquet p-distance). Suppose \u03bc is a monotone measure on the set of conditional attributes A and p is an integer. We define the Choquet p-distance $d_\u03bc^p$ with respect to the monotone measure \u03bc as follows:\n$d_\u03bc^p(x, y) := (\\int |a(x) \u2013 a(y)|^p d\u03bc(a))^{\\frac{1}{p}}$. (3)\nNote that when we use the counting measure #(A) = |A|, Eq. (3) turns into the Minkowski p-distance (cf. Proposition 3):\n$d_\\#(x, y) = (\\sum_{a \\in A}|a(x) \u2013 a(y)|^p)^{\\frac{1}{p}}.$\nThis distance is in turn equal to the Manhattan (or Boscovich) distance if p = 1 and equal to the Euclidean distance if p = 2. In the limiting case of p reaching infinity we get the Chebyshev distance:\n$\\lim_{p \\rightarrow +\\infty} d_\\#^p(x, y) = \\lim_{p \\rightarrow +\\infty} (\\sum_{a \\in A}|a(x) \u2013 a(y)|^p)^{\\frac{1}{p}} = \\max_{a \\in A}|a(x) \u2013 a(y)|.$\nSimilarly, if p goes to negative infinity, we get:\n$\\lim_{p \\rightarrow -\\infty} d_\\#^p(x, y) = \\lim_{p \\rightarrow -\\infty} (\\sum_{a \\in A}|a(x) \u2013 a(y)|^p)^{\\frac{1}{p}} = \\min_{a \\in A}|a(x) \u2013 a(y)|.$\nRemark 1. This definition varies slightly from previous definitions of Choquet distances ([1,3,21]), as it allows a more general set of distances beyond the ones that are based on the Euclidean distance (p = 2). We made this choice to allow for greater flexibility, recognizing that in supervised learning scenarios, different distance metrics, especially the Manhattan distance (p = 1), can be beneficial (cf. [11]).\nThe following example shows how allowing more general measures can be beneficial in a classification context."}, {"title": "3.2 Choquet distances for classification", "content": "In the case of classification, we have that $R_d$ is a crisp equivalence relation. In this case (using Proposition 2) we can simplify the positive region as follows:\n$\\text{POS}_{R_B}(y) = \\underline{apr}_{R_B} (R_d(y))(y) = \\min_z I(R_B(z, y), R_d(z,y))\n= \\min \\{\\min_{z \\in R_d y} I(R_B(z, y), R_d(z,y)), \\min_{z \\notin R_d y} I(R_B(z, y), R_d(z,y)) \\}\n= \\min \\{\\min_{z \\in R_d y} I(R_B(z, y), 1), \\min_{z \\notin R_d y} I(R_B(z,y),0) \\}\n= \\min \\{\\min_{z \\in R_d y} I(R_B(z, y), 1), \\min_{z \\notin R_d y} N_I(R_B(z,y)) \\}\n= \\min \\{\\min_{z \\notin R_d y} N_I(R_B(z,y)) \\}\n= \\min \\{d(z,y), z \\notin R_d y \\},$\nwhere $d(x, y) := N_I(R_B(x, y))$ for all x, y \u2208 X. Note that since RB represents a similarity measure, do can be interpreted as a distance measure. This leads us to generalize our \u03b3 and \u03b4 measure as follows:\n$\\gamma_d(B) = \\frac{\\sum_{y \\in X} \\min_{z \\notin R_d y} d_B(z,y)}{\\sum_{y \\in X} \\min_{z \\notin R_d y} d_d(z,y)}$ and $\\delta_d(B) = \\frac{\\min_{y \\in X} \\min_{z \\notin R_d y} d_B(z,y)}{\\min_{y \\in X} \\min_{z \\notin R_d y} d_d(z,y)}$ (4)\nDefinition 9. A family of distances $\\{d_B : X \\times X \\rightarrow [0, +\u221e[: B \\subseteq A\\}$ is called monotone if\n$(\\forall A, B \\in \\mathcal{P}(A))(A \\subseteq B \\Rightarrow d_A \\leq d_B)$.\nProposition 7. The degree of dependency $\u03b3_d$ and $\u03b4_d$ are monotone measures if the family of distances $\\{d_B : B \\subseteq A\\}$ is monotone.\nThe interpretation of Eq. (4) is that the dependency of a subset B towards the decision attribute can be interpreted as the normalized average (or minimum in the case of \u03b4) of the distances to the closest out-of-class instance.\nThis more general definition makes it easier to construct measures for classification:\nExample 3. Using the Manhattan distance $d_B(x,y) = \\sum_{a \\in B} |a(x) \u2013 a(y)|$, we get\n$\\gamma_d(B) = \\frac{\\sum_{y \\in X} \\min_{z \\notin R_d y} (\\sum_{a \\in B} |a(z) \u2013 a(y)|)}{\\sum_{y \\in X} \\min_{z \\notin R_d y} (\\sum_{a \\in A} |a(z) \u2013 a(y)|)}.$ (5)\nOf course the Manhattan distance could be replaced with any other distance.\nApplying Equation (5) to the decision system of Example 1, we get\n$\\gamma_d({\\{a_1\\}}) = 0.0, \\gamma_d({\\{a_2\\}}) = 0.19$ and $\\gamma_d({\\{a_3\\}}) = 0.63,$\n$\\gamma_d({\\{a_1,a_2\\}}) = 0.36, \\gamma_d({\\{a_1, a_3\\}}) = 0.64, \\gamma_d({\\{a_2, a_3\\}}) = 0.83,$\n$\\gamma_d(A)) = 1, \\gamma_d(\\emptyset) = 0.$ (6)"}, {"title": "4 Conclusion and future research", "content": "In summary, this paper introduces concrete monotone measures designed for application with the Choquet integral, aiming to define distances suitable for classification tasks. We have studied two different measures, two ways to monotonize them, and showed some inequalities between them. As these measures have the interpretation of attribute importance towards the decision attribute, we may claim that these distances are intuitive and promising.\nHowever, there is room for future exploration. Experimental evaluations, such as applying these distances in classification tasks on benchmark datasets using KNN, are needed. Additionally, further investigation into alternative measures that evaluate attribute importance in subsets of conditional attributes, such as ones based on fuzzy quantifiers or information theory, could lead to improved Choquet distances. For greater expressiveness, one may explore the use of the more general Choquet-Mahalanobis operator introduced in [21] to define a distance for supervised learning. Finally, an approach inspired by distance metric learning [16], where the measure is learned through optimizing a loss function could be interesting."}]}