{"title": "Rapid Word Learning Through Meta In-Context Learning", "authors": ["Wentao Wang", "Guangyuan Jiang", "Tal Linzen", "Brenden M. Lake"], "abstract": "Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.", "sections": [{"title": "Introduction", "content": "Children can quickly learn a new word, or at least make meaningful inferences about its meaning, given only a few examples of its usage (Carey and Bartlett, 1978; Bloom, 2000). For example, suppose a child who did not know the word ski hears the following mentions of the word (without visual examples): \"Susie learned to ski last winter\", \"People ski on tall mountains where there's lots of snow\", and \"I saw Susie ski fast down the snowy mountain.\u201d From these usage examples, the child might infer that ski is a verb for a winter activity involving sliding down snowy mountains, and could begin understanding and using the word appropriately in new contexts. This ability to generalize and use a new word in novel contexts from just a few examples reflects children's remarkable data efficiency in language learning, allowing them to quickly acquire vocabulary without requiring tens or hundreds of examples per word.\nCompared to humans, current pre-trained language models are inefficient word learners, both in the total amount of pre-training data and the number of examples needed for each word. Even though large language models (LLMs) are typically pre-trained on four or five orders of magnitude more language input than any single human could receive (Linzen, 2020; Frank, 2023), they struggle with systematic generalizations of words that are rare or unseen in their training data (Wei et al., 2021; Razeghi et al., 2022; Kim et al., 2022; Batsuren et al., 2024; Land and Bartolo, 2024).\nThis contrast between human learning and language model training raises two long-term research questions: 1) Could language models develop a human-like ability for few-shot word learning without astronomical amounts of training data? 2) Could existing LLMs be adapted to improve their few-shot word learning abilities, allowing them to systematically and flexibly use new words in new contexts?\nHere, we introduce a simple method, Meta-training for IN-context learNing Of Words (Minnow), to train or finetune a language model to develop an in-context few-shot word learning capability (see Figure 1 for an illustration of our method). We adopt meta-training (i.e., meta-learning) since it has had successes in endowing neural networks with stronger systematic generalization, closely related to our objective of word learning (see Russin et al., 2024 for a review of the successes). Specifically, we use Meta-training for In-Context Learning (MetaICL; Min et al., 2022; Chen et al., 2022) to train from scratch or finetune an auto-regressive language model to generate new usages of a new word given a set of illustrations of the new word in its previous context. In-context learning (ICL) builds and uses contextual representations of the new word on the fly without parameter updates. MetaICL repeats ICL on many different new words and optimizes the model parameters for a general word-learning ability.\nTo demonstrate the data efficiency of our method, we train language models from scratch with Minnow using small datasets: a corpus of child-directed speech (CHILDES; MacWhinney, 1992) and a corpus approximating the word count a child encounters during language acquisition (BabyLM-10M; Warstadt et al., 2023)."}, {"title": "Related Work", "content": "Word frequencies in natural corpora follow a highly skewed (Zipfian) distribution (Zipf, 1949), resulting in a heavy tail of rare words. Additionally, new words are constantly entering the language (Heaps, 1978). To represent all possible words, various word-form-based methods have been proposed, including subword- and character-based tokenizations and using morphological information (see Mielke et al., 2021 for a comprehensive survey). However, representing a word alone does not help in learning it from a few contexts in which it occurs. Models optimized for conventional language modeling still struggle with the usage of unfamiliar or completely novel words, tokens, or token sequences, where word-forms or token identities alone do not provide enough information (Ott et al., 2018; Schick and Sch\u00fctze, 2020; Wei et al., 2021; Razeghi et al., 2022; Kim et al., 2022; Batsuren et al., 2024; Land and Bartolo, 2024). Instead of representing new words based on word-forms, we discard word-form information and use a dedicated special placeholder token that is the same for every new word. In this way, we aim to develop a general and efficient ability to learn a word from a few contexts of its usage."}, {"title": "Few-Shot Word Learning", "content": "Another line of previous work targets the problem of learning a new word from a few examples. Most previous work aims to produce a representation for the new word, i.e., an embedding, that fits into the global word embedding space so it can be used in the same way as other learned words (Mikolov et al., 2013; Pennington et al., 2014). The embedding can be produced by aggregating the embeddings of the contexts that the new word appears in (Lazaridou et al., 2017; Khodak et al., 2018), finetuning the embedding within the context (Herbelot and Baroni, 2017; Lampinen and McClelland, 2017; Hewitt, 2021; Kim and Smolensky, 2021), or utilizing the word-form information (Luong et al., 2013; Schick and Sch\u00fctze, 2019). More recent work uses Transformer layers to produce the embedding based on Word2Vec embeddings (Hu et al., 2019, HiCE), or by aggregating similar embeddings of word contexts from a memory system (Sun et al., 2018, Mem2Vec). Also related to our approach, Teehan et al.'s (2024) work uses a meta-learning framework named CoLLEGe to train a Transformer encoder to produce an embedding for a new word from its examples of usage. Our method also targets few-shot word learning, but is simpler than Teehan et al. (2024) in architecture and training and does not"}, {"title": "Meta-training for In-Context Learning", "content": "Building on LLMs' in-context learning abilities (Brown et al., 2020), Meta-training for In-Context Learning (MetaICL) optimizes language models on multiple different tasks, each learned from a few in-context examples (Min et al., 2022; Chen et al., 2022).\n A class of tasks that MetaICL (or similar curriculums) aim to learn and generalize requires inferring the context-dependent mapping from the symbols to meanings (Lake and Baroni, 2023; Huang et al., 2024; Anand et al., 2025; Park et al., 2025). We follow this work to use MetaICL for our word learning task, in which the mapping from a new word to its meaning should be inferred purely from its usage in the context."}, {"title": "Method", "content": "The goal of our method, Minnow, is to enable a model to infer the meaning of a new word from a few examples of its usage so it can understand and generate novel usage examples of the word, coherently and systematically combining it with other words in new contexts. To achieve this, Minnow trains the model to generate another usage example of the new word\u2014a task that, when sufficiently challenging, requires mastery of this ability. Minnow is a general framework that can be applied to both training a model from scratch and finetuning a pre-trained model. After describing the method, we introduce the training data we use, a held-out word classification task for model evaluation and hyperparameter tuning, and how we use the off-the-shelf Llama-3 8B as a baseline for our experiments."}, {"title": "Method: Minnow", "content": "Following the typical meta-learning approach, we construct episodes ${T}_{i=1}^{N}$, each $T_i$ consists of $K$ examples ${x_k^{(i)}}_{k=1}^{K}$ sampled in accordance with the desired task (Figure 1: top). In each episode, the model's task is to learn a new word $w_i$; each example $x_k^{(i)}$ is a sentence illustrating how $w_i$ is used. We concatenate the examples ${x_k^{(i)}}_{k=1}^{K-1}$ into a single sequence, separated by a special separator token (<sep> when training from scratch or a reserved special token in the Llama-3 8B vocabulary when finetuning Llama-3 8B). The objective is next-token prediction on this concatenated sequence: we expect the model to predict a new usage example given the previous examples, i.e., $p(x_K^{(i)}|x_1^{(i)},...,x_{K-1}^{(i)})$. We replace (mask) all occurrences of $w_i$ in the sequence with a special placeholder token ([new-token] when training from scratch or a different reserved special token when finetuning Llama-3 8B). The same placeholder token for the new word is shared across all episodes, such that the model does not learn a new embedding"}, {"title": "Data", "content": "To demonstrate the data efficiency of our method compared to humans, we use data sources that are close to children's language input in quantity or quality (Warstadt et al., 2023). We construct one dataset from each of two corpora: CHILDES (MacWhinney, 1992) and BabyLM-10M (Warstadt et al., 2023). CHILDES is a corpus of transcriptions of child-caregiver speech interactions. We use input to children (excluding utterances produced by children) in the North American English portion of CHILDES. BabyLM is an English dataset including child-directed speech as well as additional data sources, such as children's books, transcriptions of dialogs between adults, and Wikipedia articles. We use the 10M word corpus constructed as part of the first BabyLM Challenge.\nEach dataset consists of two disjoint components, one for meta-learning (the leftmost set in Figure 1: top) and the other for language modeling (the leftmost set in Figure 1: bottom). We select a set of lower-frequency words in the corpus to be meta-learned in the meta-learning component. Each meta-learned word $w$ has a set of $n_w$ sentence examples illustrating its usage. We assign each sentence in the corpus to at most one meta-learned word, so the identity of the word masked by the placeholder"}, {"title": "Held-out Word Classification", "content": "We introduce a word classification task, in which we measure the model's ability to discriminate the identities of new words that were never seen during training (i.e., held-out), based on in-context study examples. Validation accuracy on this task is used to tune training hyperparameters (e.g., learning rate; described later).\nGiven a query example sentence $q$ that uses a new word and a set of $C$ candidate words ${w^{(c)}}_{c=1}^C$, the task for the model is to match the query example to the most suitable one among the $C$ candidate words. Each $w^{(c)}$ is represented by a context containing a set of $K-1$ study examples ${x_k^{(c)}}_{k=1}^{K-1}$ illustrating its usage. The context of $w^{(c)}$ is a sequence in the same format as the first $K-1$ examples in a training episode, ending with a separator token (e.g., <sep>): $x_1^{(c)}<sep>x_2^{(c)}<sep>...<sep>x_{K-1}^{(c)}<sep>$. The query example is formatted as a continuation sequence of the context: $q <sep>$. This formatting ensures that concatenating a context sequence and a query sequence results in a sequence with $K$ examples, just like a sequence for a meta-learning training episode. To determine the best match, we compute the conditional likelihood of the query sequence given the context: $P_{LM}(q|x_1^{(c)}x_2^{(c)},...,x_{K-1}^{(c)})$. The model predicts the word corresponding to the context with the highest likelihood: $arg \\max_c P_{LM}(q | x_1^{(c)}x_2^{(c)},...,x_{K-1}^{(c)})$. The prediction is correct if it is the ground-truth word in the query $q$.\nWe evaluate each model (trained from scratch or finetuned) by measuring the classification accuracy on held-out meta-learned words from the validation or test portions of the model's training or finetuning corpus. For each evaluation, we group $C$ distinct meta-learned words into a $C$-way classification task. For each word, we sample $K-1$ study examples and one query example to construct the task. See Appendix C for additional details on task construction."}, {"title": "Baseline: Off-the-shelf Llama-3 8B", "content": "For training models from scratch, we need an LLM that is pre-trained on massive data with conventional language modeling for data-efficiency comparison. To determine the effectiveness of finetuning an LLM, we need to evaluate its baseline word-learning ability. To address both needs, we use the off-the-shelf Llama-3 8B model as a baseline for word-learning tasks. We experiment with both the pre-trained and the instruction-tuned variants of the model. We primarily report baseline results from the pre-trained variant, and present results from the instruction-tuned variant only in the generative settings, where its performance may differ significantly from that of the pre-trained one. For evaluation, we present a meta-learning episode to Llama-3 8B in a text format similar to the training or finetuning sequences (Section 3.1), but designed to be more natural and closer to its pre-training data. In particular, we use a pseudo-word (e.g., \"dax\") as the placeholder for the new word, with a newline character and a star \"\\n *\" serving as the separator between examples, effectively formatting the examples as a list. Using the ski example in Section 1 again, the formatted text appears as follows:\n* Susie learned to dax last winter\n* People dax on tall mountains where there's lots of snow\n* I saw Susie dax fast down the snowy mountain\n*\nThe \"\\n *\" at the end serves as the last separator, like the last <sep> in the example sequence in Section 3.1."}, {"title": "Training Models From Scratch", "content": "In this section, we investigate whether models can develop the ability of few-shot word learning from human-scale input. We use the GPT-NeoX transformer architecture (Andonian et al., 2023) with configurations modified from Pythia-160M (Biderman et al., 2023)."}, {"title": "Finetuning Pre-trained LLMs", "content": "In this section, we test if our method can improve pre-trained LLMs' in-context few-shot word learning abilities. We finetune Llama-3 8B with Minnow three times on the meta-learning component of BabyLM-10M, each run with $K = 5$ examples per episode and a different random seed. We refer to the models finetuned with Minnow as Minnow models. We do not include the language modeling components since the LLM already learned a large vocabulary and is capable of language"}, {"title": "Held-out Word Classification", "content": "We first evaluate models on the held-out word classification task (Section 3.3). Finetuning Llama-3 8B with Minnow boosts the test 4-way ($C = 4$) classification accuracy from the baseline level of 78% to 87% on BabyLM-10M (and from 71% to 79% on CHILDES). We provide results for additional values of $K$ and $C$ in Appendix C, Table 6; broadly, across all settings, the Minnow model improves test accuracy by 8-10% over the Llama-3 8B baseline. These findings show that Minnow finetuning effectively improves the pre-trained LLM's in-context few-shot word learning ability.\nDespite these strong results, this task does not assess more fine-grained aspects of meaning that may not be apparent from discriminating an arbitrary set of words, and the semantic coherence of the usage contexts could be a shortcut utilized by the model (see Appendix C for further discussion). To address this, we provide the next analysis focusing on the syntactic categories of words."}, {"title": "Syntactic Category Classification", "content": "In this evaluation, we test if models can differentiate words in different syntactic categories, a crucial feature for systematic generalization. We follow the classification paradigm introduced in Section 3.3. We use the methodology of Kim and Smolensky (2021) as well as the dataset they constructed from MNLI, a Natural Language Inference dataset (Williams et al., 2017). The dataset focuses on four syntactic categories (noun, verb, adjective, and adverb) and tests the ability to differentiate each pair of categories. See Appendix D for details of the dataset.\nIn each instance of the classification task, we learn two new words $w^{(1)}$ and $w^{(2)}$ in different syntactic categories; the syntactic category of each new word $w^{(i)}$ is unambiguously signaled by a study example $x^{(i)}$ (replacing the word with the placeholder, e.g., [new-token]). For example, say $w^{(1)}$ is a noun and $w^{(2)}$ is a verb:\n(1) A [new-token] needs two people. (for $w^{(1)}$)\n(2) She [new-token] at the group. (for $w^{(2)}$)"}, {"title": "New Usage Example Generation", "content": "The two tests we have described so far evaluate models in a discriminative setting. Here, we quantitatively and qualitatively evaluate if models use the new word appropriately in a generative setting. For a Minnow model finetuned with $K$ examples per episode, we evaluate it by showing it $K-1$ in-context study examples, formatted as a sequence in the classification setting (Section 3.3). We ask the model to do what it was trained for: We prompt the model with this sequence of study examples, and because the sequence ends with a separator token, the model will continue the sequence by generating a new usage example, ending with another separator token as End-Of-Sequence.\nWe sample study examples from two datasets: the BabyLM-10M test portion in Section 3.2 and the Chimera dataset (Lazaridou et al., 2017). The Chimera dataset was specifically constructed for few-shot word learning. It has 33 different new words for learning, each referring to a \"chimera\" concept, i.e., a mixture of two existing and related concepts (e.g., a cello and a bagpipe). The usage examples of a new word are sentences using one of the components of the chimera, randomly extracted from a large corpus. See Appendix F for additional details of the dataset and our preprocessing.\nFor the quantitative evaluation, we compare a pair of new usage examples generated from Llama-3 8B baseline and a Minnow model finetuned from it. The"}, {"title": "Definition Generation", "content": "To further probe how well Minnow finetuning helps the model understand a new word, we prompt each model to generate a definition for the word given one or a few usage examples. We again use the methodology of Teehan et al. (2024) for definition generation and evaluation, as well as the two evaluation datasets they used: CoLLEGe-DefGen, which they created, and the Oxford dataset (Gadetsky et al., 2018). COLLEGE-DefGen was constructed by selecting 954 words from WordNet (Miller, 1995) and prompting GPT-4 (OpenAI, 2023) to generate one definition and five usage examples for each word. The model generates a definition from one, two, or three usage examples sampled for each word in this dataset (i.e., in 1-, 2-, or 3-shot settings). The Oxford test set consists of 12,232 words, each with a definition and a usage example collected from the Oxford Dictionary. The model generates a definition from the only usage example for each word in this dataset (i.e., in a 1-shot setting). To generate a definition, we prompt the model with the sequence of the usage example(s) (as in Section 5.3) followed by \"The word [new-token] in the above sentence(s) is defined as \""}, {"title": "Conclusion", "content": "In this work, we present Minnow, a new method to improve language models' capability to learn a new word from a few in-context usage examples. Minnow successfully induced this ability in models trained from scratch with human-scale linguistic data, as indicated by their performances in differentiating new words (Section 4). Minnow finetuning further improved the word learning performance of a pre-trained LLM (Llama-3 8B), as demonstrated in their improvements in differentiating new words (Section 5.1 and 5.2) as well as in generating new usage examples (Section 5.3) and definitions (Section 5.4) for the learned new words. In summary, this word-learning capability enables models to systematically and flexibly understand and use a new word in novel contexts, and can be immediately transferred to other words and tasks without additional training.\nThe efficacy of Minnow, or meta-learning in general, suggests that human-level efficiency in linguistic generalizations may be acquired through practicing over many instances of learning tasks, without presuming strict, explicit inductive biases (Russin et al., 2024; Irie and Lake, 2024). Whether models achieve the generalizations in this work through human-like mechanisms, such as systematicity and categorical abstraction, remains for future analysis."}, {"title": "Limitations", "content": "Learning Settings In this work, we consider word learning only in the text modality, in which the language model learns the meaning from the distribution of words. However, many words have real-world references, which usually accompany human word learning. We also use aggregated data from multiple sources, not from single-human/child input. Thus, a multimodal, grounded setting of word learning using a single agent's input would be more realistic.\nIn addition, we only consider learning a single new word on the fly. However, in real-world learning, both humans and models need to continually learn multiple words, usages, and even abstract rules (Mueller et al., 2024). Implementing this continual learning setting would be another future direction.\nNovelty of New Words When Testing LLMs When testing LLMs (Section 5), the words and example sentences we use may already exist in the pre-training data, potentially allowing LLMs to recall known word meanings rather than learn genuinely new ones (note, however, the Chimera dataset introduces new concepts which are unusual and not lexicalized). The performance of the baseline LLMs shows that, even with this potential worry, there is room for improvement, which the Minnow-finetuned LLMs are able to achieve.\nModels trained from scratch with Minnow do not have this limitation. Their training data explicitly ex-"}, {"title": "Word Usage Dataset Creation", "content": "As we mentioned in Section 3.2, we construct one dataset from each of two corpora: CHILDES (MacWhinney, 1992) and BabyLM-10M (Warstadt et al., 2023). The CHILDES dataset is licensed for use under a CC BY-NC-SA 3.0 license. Our scientific use is under the terms of the license. We did not find the license of the BabyLM dataset, which aggregated multiple public datasets. Since there is plenty of published work using this public dataset, we believe our scientific use does not violate any terms or conditions. In the following, we describe how we preprocess these two corpora and create a word usage dataset from each corpus.\nPreprocessing Since the basic units of our focus are words (as opposed to word pieces in other tokenization schemes), we need to identify words in the text. To achieve this, we apply the same word-level tokenization to all datasets (for consistency) and mark word boundaries by whitespace during preprocessing. Models trained from scratch use this word-level tokenization. When the text is used in finetuning Llama-3, which comes with its pre-trained subword tokenizer, we remove the unnatural spaces introduced by the word-level tokenization and tokenize the text again with Llama-3 tokenizer, so the text format becomes closer to its pre-training data (See the Finetuning paragraph in Appendix B for further details of this process). For CHILDES data, we preprocess the data in the same way as Yedetore et al. (2023) did, which uses children's input in the North American English portion, but we do not split and unk the data at the preprocessing stage. For BabyLM data, we use the data in the 10M track of the BabyLM Challenge 2023, which mixes 10 portions, each from a different data source (child- or adult-oriented, speech transcription or written text like Wikipedia). We exclude the QED portion for its poor quality (also mentioned in the 2nd BabyLM Challenge).\nWe apply word-level tokenization on untokenized portions, and then split the text into sentences using heuristics. We use spaCy for all word-level tokenization along with Part-Of-Speech tagging. We lowercase all text before preprocessing to unify the capitalization of words in different places. We deduplicate sentences and remove sentences having less than 1 word (not counting punctuation).\nAssigning sentences and splitting To create a dataset from a corpus, we first get the token frequencies of all words. (Here, a word means a word-form. We discuss its implications in Appendix H.) Then we select the set of words to be meta-learned. We will only consider nouns, verbs, adjectives, and adverbs to be meta-learned (a word's syntactic category is based on the word's most frequent Part-Of-Speech tag). We choose two thresholds for meta-learned words: the maximum frequency of a meta-learned word and the minimum number of exam-"}, {"title": "Model and Training Configurations", "content": "Training from scratch We slightly modify the configuration of Pythia-160M (Biderman et al., 2023), which uses the Transformer architecture GPT-NeoX (Andonian et al., 2023). The configuration has 12 layers and a hidden dimension size of 768. We change the vocabulary size according to the corresponding dataset, as shown in Table 5. We also include three special tokens in the vocabulary: the placeholder token [new-token], the separator token <sep>, and <unk>, as mentioned in Section 4. We change the Pythia configuration to tie the input and output embeddings. This makes the model parameter counts smaller, 86.7M and 102.5M for the model trained on CHILDES and BabyLM-10M, respectively. For both models, we use batch size (i.e., number of episodes/sequences per batch) 8 and AdamW optimizer (Loshchilov and Hutter, 2019) with initial learning rate $3 \\times 10^{-4}$, and reduce the learning rate by multiplying 0.1 when the validation loss has stopped improving for 2 epochs. We apply weight decay 0.07 and 0.15 when training on the CHILDES and BabyLM-10M datasets, respectively. Other configurations, such as no dropout, are kept the same as Pythia-160M. For each setting, we run 3 times with random seed {0, 1, 2}. Each run is performed on a single V100 GPU for 30 epochs (9-18 hours).\nFinetuning We finetune Llama-3 8B (Llama Team, Meta AI, 2024) with Minnow on each of the CHILDES and BabyLM-10M datasets, but we refer to the models finetuned on BabyLM-10M by default, as we mentioned in Section 5. We finetune from both the pre-trained and instruction-tuned variants of Llama-3 8B, but we refer to the models finetuned from the pre-trained variant by default, presenting results of finetuning from the instruction-tuned variant only in the generative settings, where their performance may differ significantly due to their different capabilities to follow the prompt. We use two reserved special tokens in Llama-3 tokenizer vocabulary as the placeholder token and the separator token. To make the tokenization more natural to the model's pre-training data, we clean up tokenization spaces in the text (e.g., the space before \",\", \"'\", or \"'s\") introduced by the word-level tokenization during preprocessing and make the placeholder token absorbs any preceding spaces of the word. Finetuning is minimally parameter-efficient: We finetune only the input and output embeddings of the two special tokens, while freezing all other parameters. Before finetuning, the input/output embedding of either token is initialized to the mean of all input/output embeddings (Hewitt, 2021). When finetuning the model on CHILDES with 5 examples per episode, we use batch size (i.e., number of episodes/sequences per batch) 32 and initial learning rate $3 \\times 10^{-3}$ and truncate the sequence to the max length of 80 tokens to control the memory usage. When finetuning the model on CHILDES with 10 examples per episode, we use batch size 8 and initial learning rate $3 \\times 10^{-4}$ and truncate the sequence to the max length of 180 tokens. When finetuning the model on BabyLM-10M with 5 examples per episode, we use batch size 16 and initial learning rate $1 \\times 10^{-3}$ and truncate the sequence to the max length of 160 tokens. Other settings are the same as when training from scratch except that we do not apply weight decay. Each run is performed on a single A100 GPU for 15 epochs on CHILDES (33 hours) or 12 epochs on BabyLM-10M (48 hours)."}, {"title": "Held-out Word Classification", "content": "As we mentioned in Section 3.3, we need different meta-learned words in the same group. Therefore, different from training, we sample only one episode of $K$ examples per word from the validation/test portions so we do not repeat the same word in a classification group. We also fix the shuffle order so all models are evaluated on the same classification task instances. We experimented with training models with $K \\in \\{5,10\\}$ examples per episode on CHILDES and BabyLM-10M and evaluated each of them on the corresponding dataset with the same $K$ and $C \\in \\{4,8\\}$. Training models with $K=10$ examples per episode on BabyLM-10M was unsuccessful because the concatenated sequence was too long, exceeding the GPU memory, so we do not have results in this setting.\nWe are aware of the weaknesses of this task. Discriminating a new word from an arbitrary set of other new words is a relatively weak test of word meaning learning. The task could be easy simply because different words are used in very different contexts, so the conditional likelihood may reflect just the coherence of the usage contexts between study and query examples, not the meaning of the new word (we demonstrate this point by an additional baseline below where we present the model only the usage contexts without new words). In addition, results from the task do not tell us what features of word meanings the model is learning. Our syntactic category classification task addresses these concerns by focusing on the syntactic aspect and breaking the semantic coherence between study and query examples (Section 5.2).\nBelow, we describe two baselines we run on this task.\nBaseline: Llama-3 8B learning a pseudo-word in context (Llama-3 8B with 'dax') This is the baseline model introduced in Section 3.4. We follow the format described there and additionally prepend a prompt to make the performance better: \"The following lines are lowercased example sentences using a new word 'dax' in random order, one per line:\". (We discuss the consequence of using a same pseudo-word in Appendix H.)\nAdditional Baseline: Llama-3 8B modeling the coherence of usage contexts (Llama-3 8B with \"\") This is the additional baseline to evaluate the effectiveness of utilizing just the coherence of the contexts, as we discussed above. We remove the new word from each example (equivalent to replacing the new word with an empty string), so only the usage context of each example is retained.\nFor these baselines, we also experimented with the instruction-tuned variant of Llama-3 8B but it performs worse on this task."}, {"title": "Syntactic Category Classification", "content": "As we mentioned in Section 5.2", "categories": "noun, verb, adjective, and adverb. Therefore, they have 6 pairs of categories for discrimination. For each category pair, the dataset contains two signal contexts (one for each category; we use them as the study examples) and 200 test sentences using a word unambiguously in either category (100 for each category; we use them as the query examples). The main difference between our approach and that of Kim and Smolensky (2021) is that, instead of finetuning a new word embedding on each signal context, we apply in-context learning, using each signal context as an in-context study example of the new word. Read Kim and Smolensky (2021) for further details.\nResults from models trained from scratch, Llama-3 8B baseline and models finetuned from Llama-3 8B on the 6 category pairs and their mean are visualized in Figure 2. Table 7 shows detailed results from Llama"}]}