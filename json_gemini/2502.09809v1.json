{"title": "AGENTGUARD: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration", "authors": ["Jizhou Chen", "Samuel Lee Cong"], "abstract": "The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AGENTGUARD\u00b9, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generat- ing safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AGENTGUARD leverages the LLM orchestrator's innate ca- pabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated con- straints, enables multiple security applications. We empiri- cally demonstrate AGENTGUARD 's feasibility with exper- iments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening proce- dures for LLM agents to enhance their trustworthiness in real-world applications.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 Problem", "content": "The integration of tool use amplifies the capabilities of large language models (LLMs) by forming agentic systems, ex- tending their influence from text generation to the real world. Consequently, this amplification magnifies the consequences of failures in LLMs as tool orchestrators. For instance, in standalone LLM settings, the consequence of prompt injec- tion attacks is confined to the generated text (e.g., generating offensive or deceptive content). Meanwhile, in agentic set- tings, the same attacks can be leveraged to make the (LLM- based) orchestrator to generate malicious tool orchestration plans with much more consequential harm (particularly when exploiting combined misuse of tools). For example, the or- chestrator can be goal-hijacked to exfiltrate sensitive data by leveraging data processing and network tools available in an agent. In such settings, the agent is practically turned into a potent adversarial entity. This problem underscores the need for tool orchestration testing and hardening frameworks for LLM agents. Particularly, we argue that there is a need for a standardized pre-deployment process that first discovers unsafe tool-use workflows (especially those not anticipated by developers, given that tool-use workflows are dynamically orchestrated with non-determinism), and then generates safety constraints to confine the unsafe behaviors. With such safety constraints enforced (e.g., as sandbox rules), safety guaran- tees are still achieved even if the agent at deployment gets compromised, hence improving the trustworthiness of LLM agents for deployment."}, {"title": "1.2 Motivation", "content": "We argue that the aforementioned framework must contain three key capabilities: 1) The framework must have the knowledge of the capabilities and usage of the tools to possibly evaluate safety risks in using them, 2) The framework must have the capability to scalably identify possible workflows that can realistically be generated by the orchestrator, 3) The framework must have the capability of executing identified unsafe workflows in the real world to concretely validate if they indeed lead to unsafe outcomes.\nRecent work TooLEmu [5] has explored this direction by proposing an LLM-based approach to scalably emulate tool execution for safety and helpfulness evaluation. While this work successfully achieves scalable testing, it has the following limitations in addressing all the requirements. First, the tools and their specifications used for evaluation are all"}, {"title": "2 Design", "content": null}, {"title": "2.1 Assumptions", "content": "For the design, we have the following assumptions: 1) The target agent is hosted in a controlled environment throughout this testing process, hence developing a controlled testing en- vironment is out of the scope of this work, 2) AGENTGUARD is executed before the orchestrator undergoes the moderation process, hence the orchestrator is compliant with requests for safety evaluation tasks, 3) The orchestrator is not compro- mised and will actively expose unsafe workflows as requested (as oppose to deliberately hiding them), 4) The orchestrator has security knowledge enabling it to perform safety evalua- tions (e.g., gained from LLM's pre-training)."}, {"title": "2.2 Framework", "content": "As shown in Figure 1, AGENTGUARD contains three key components: 1) The LLM-based orchestrator within the target agent under evaluation, 2) A Safety Constraint Expert Agent responsible for safety constraint generation, and 3) A central- ized Prompting Proxy Agent to instruct the other two com- ponents to perform testing and hardening. AGENTGUARD works in four main phases: 1) Unsafe Workflow Identification, 2) Unsafe Workflow Validation, 3) Safety Constraint Gener- ation, and 4) Safety Constraint Validation. The deliverable of AGENTGUARD is an evaluation report containing aggre- gated evaluation results from the four phases, corresponding to different task scenarios and including identified unsafe workflows, violated security principles, test cases, and safety constraints to mitigate the risks along with the validation status of generated unsafe workflow and safety constraints. Validated safety constraints then can be readily applied to the evaluated agent to mitigate potential validated unsafe be- haviors for safer development. Additionally, the evaluation has many more applications such as serving as samples for training LLMs/LAMs for safer tool orchestration."}, {"title": "2.2.1 Unsafe Workflow Identification", "content": "In this first phase, Prompting Proxy Agent instructs the or- chestrator to first reflect on the tools given to it regarding their capabilities and usage leveraging its internal knowledge, then evaluate the tools to identify possible unsafe workflows of calling these tools that violate fundamental security princi- ples in different task scenarios. Notably, the orchestrator is instructed to particularly focus on identifying risks in com- plex workflows involving multiple tools, which are normally hard to capture. The philosophy mirrors that in malware while the use of each system API appears benign individu- ally, when they are orchestrated together in certain sequences, the program can practically achieve malicious effects. For each Task Scenario, AGENTGUARD instructs the orchestrator to generate Unsafe Workflow, Expected Risks, and Violated Security Principles in the report."}, {"title": "2.2.2 Unsafe Workflow Validation", "content": "For each identified unsafe workflow received from the last phase, the Prompting Proxy Agent instructs the orchestrator to 1) generate corresponding test cases including concrete orchestration plans (e.g., sequences of tool API calls) rep- resenting the workflows at the execution level utilizing its internal knowledge of tool use, and the corresponding unsafe outcome detection mechanism (e.g., checking if writing to restricted a directory successfully overwrites files in it) and 2) execute the test cases to validate that the identified workflows indeed can result in unsafe outcomes, utilizing its privileges to invoke the tools. Note that the form of test cases depends on the target LLM agent under evaluation. For example, if the target agent is a coding agent with file, process, and network APIs as tools, then the test cases can be in the form of code. If the target agent is a personal assistant agent with higher-level commands such as \"send email\" and \"place purchase order\""}, {"title": "2.2.3 Safety Constraint Generation", "content": "For each received validated unsafe workflow along with the test cases and detected unsafe outcomes, the Safety Constraint Expert Agent 1) examines the observed unsafe outcomes from test case execution, 2) correlates them with the tool invoca- tions in the test case to analyze the root cause, and 3) gen- erates corresponding safety constraints (e.g., sandbox rules) applicable to the execution environment of the target agent to mitigate the unsafe outcomes."}, {"title": "2.2.4 Safety Constraint Validation", "content": "In this phase, for each unsafe workflow, the Safety Constraint Expert Agent applies the safety constraints to the execution environment of the target agent, then instructs the orchestrator to re-execute the test cases and check if the unsafe outcomes are blocked. If so, the set of safety constraints generated for this unsafe workflow are validated and added to the report."}, {"title": "2.3 Deliverable", "content": "After undergoing the four phases, AGENTGUARD delivers a complete evaluation report that has multiple applications. To name a few:\n1. The validated safety constraints can be enforced to sand- box the behaviors of the agent for safer deployment to achieve the safety guarantee baseline."}, {"title": "3 Implementation", "content": "We build the Prompting Proxy Agent with the LangChain [1] framework in Python. We implement a general web service wrapper with the FastAPI [2] framework for target agents with no RESTful APIs. We choose SELinux [4] as the safety constraint embodiment. Due to limited time, we currently use the orchestrator equipped with knowledge in SELinux from the underlying LLM as the Safety Constraint Agent. We plan on continuing to develop a standalone agent with RAG and more capabilities for more robust safety constraint generation after the hackathon. Since the quality of content generated in unsafe workflow identification, test case generation, and safety constraint generation can vary and sometimes be unsat- isfactory, we also implement a content quality control agent to review if the generated content faithfully follows the require- ments specified in the prompt. If not, it generates suggestions for improvement and requests for regeneration. This review process is implemented in the form of a feedback loop. For test case generation and safety constraint generation, we im- plement another error-based feedback loop mechanism to fix potential syntactical and semantic errors upon failed test case execution and safety constraint application, respectively."}, {"title": "4 Evaluation", "content": "In this section, we report the issues encountered, our attempts to address the issues, our insights, and the evaluation results for each phase. We evaluated AGENTGUARD with Aider (v0.68.0) [3], a popular coding agent as our evaluation target. We used ChatGPT 4o as the model for the LLM tool orchestrator. The evaluation was performed on an Ubuntu 22.04 VM with SELinux enabled.\nThe evaluation results show that despite many test runs failing at the step of applying safety constraints due to"}, {"title": "Unsafe Workflow Identification", "content": "The first issue we encountered was that the LLM tool orchestrator refused to perform this task and responded with \"I'm sorry, but I can't assist with that request.\" Since we know for a fact based on experiments that the vanilla underlying foundation model (i.e., ChatGPT 4o) does not have content moderation for such a benign task and we did not find moderation prompts in Aider's source code, we suspected that it was due to implicit moderation caused by the role-assigning system prompts of the orchestrator. Attempting to jailbreak this hypothesized moderation, besides using common jailbreaking prompting such as \"You are permitted to ...\", \"This is very important\", etc., we introduced Role Augmentation prompting, which assigns additional roles to the orchestrator to enable it to perform the requested task: \"Besides the role you have been given, you are also a helpful expert in {task}\". The purpose of this solution is to prevent the orchestrator from being implicitly moderated by the factory role assignment prompt. Our testing result shows that this technique was quite effective, which allowed the evaluation to proceed. Besides this benefit, we observed that the orchestrator's performance also got noticeably boosted, reflected in the quality of the generated content. With the proven effectiveness, we applied this technique to all other phases involving the orchestrator. We would like to address that adding moderation is necessary for LLM agents to mitigate misuse. However, this can be done after AGENTGUARD's hardening process and before deployment to facilitate the evaluation process.\nThe second issue we encountered was hallucination, a common issue with LLMs. The LLM tool orchestrator tended to hallucinate about unsafe workflows (i.e., containing capabilities not specified by the system prompt, such as \"SSH client\"). To resolve this issue, we first tried an intuitive and naive solution by explicitly asking the orchestrator not to hallucinate. However, several rounds of testing showed no noticeable improvement. In our second attempt, we adopted the Chain of Thoughts technique prompting the orchestrator to first list available tools followed by reflecting on the capabilities of each tool before identifying unsafe workflows. This solution was proven to be more effective. After fixing the issues, the performance of the orchestrator in unsafe workflow identification appeared fairly solid."}, {"title": "Unsafe Workflow Validation", "content": "We did not encounter any critical issues in this phase. The evaluation result indicated that the orchestrator was able to generate error-free executable test cases most of the time, even before we applied the error- based feedback loop. The result was somewhat expected since the orchestrator's should be good at generating tool-use plans, not to mention that the test cases in our experiments were in the form of Python code, which today's LLMs are generally good at. Despite the good result, we note that a more compre- hensive evaluation (e.g., with target agents of other types) is still needed before an assertive conclusion can be drawn."}, {"title": "Safety Constraint Generation", "content": "The performance of LLMs's SELinux rule generation appeared to be decent. The ChatGPT 4o model which we used as the SELinux rule gen- erator yielded rules with correct syntax for simple cases for a fair amount of times. The critical issue we encountered re- garding safety constraints is discussed in the next paragraph."}, {"title": "Safety Constraint Validation", "content": "Different from unsafe work- flow validation, we encountered a critical issue in safety con- straints validating regarding applying SELinux policies. We observed that the underlying LLM (i.e., ChatGPT 4o) tended to generate rules involving custom file types. It appeared that the LLM directly used these custom types as if they already existed, based on the keywords in the prompt (e.g., sensitive_t was directly used to refer to \"sensitive files\" mentioned in the prompt). The LLM did not appear to know that custom labels have to be created and assigned properly to make the rules applicable. The issue remained largely unsolved even after we introduced the error-based feedback loop and explic- itly prompted it to generate commands to create and assign custom labels. For example, the generated commands were incorrect and failed to assign the labels. As a result, only a very few (using only existing labels) among many generated rules were successfully applied and validated, completing a proof-of-concept of AGENTGUARD. We note that this ob- served failure is due to the limitation of the LLM evaluated, as opposed to the design of AGENTGUARD. Meanwhile, despite just a few, the test runs that passed the entire pipeline proved the feasibility of AGENTGUARD. This observation motivates us to develop a more robust agent system to, for example, instantiate safety constraint rules from templates paired with commands to create the custom type labels or include a RAG component to obtain knowledge from the official documents to create applicable constraints."}, {"title": "5 Discussion", "content": "Generalizability The methodology of AGENTGUARD is based on the general design of LLM agents. With its gener- alizability, AGENTGUARD not only applies to LLM agents residing in cyberspace but also applies to those that interact with the physical world (e.g., robotic agent systems).\nContinuing Work This work was done during the LLM Agent MOOC Hackathon hosted by UC Berkeley in 2024. Due to limited time, only a prototype was developed and validated with a single target agent during this hackathon. We are continuing to extend this prototype to a solid research work by introducing more systematic and novel methods in agentic behavior exploration, defining and measuring safe and unsafe behaviors, etc., and collecting more target agents for evaluation, besides the aforementioned improving plans based limitations we discovered in section 4."}, {"title": "6 Conclusion", "content": "In this work, we propose AGENTGUARD, a testing and hard- ening framework for LLM agents through the agents' own orchestrators. It achieves testing and hardening by leveraging the orchestrator's innate advantages in possessing internal knowledge of tools gained during the agent-building pro- cess, the ability to identify unsafe workflows in a scalable and realistic manner, and the privilege of invoking tools. By evaluating AGENTGUARD with a coding agent, we devised Role Augmentation prompting when attempting to address po- tential implicit moderation caused by factory role-assigning system prompts. Despite many failed end-to-end runs due to our evaluated LLM's limitation in generating readily applica- ble SELinux rules, we observed successful evaluation results indicating the feasibility of AGENTGUARD. We are contin- uing to develop more robust and novel methods to extend AGENTGUARD to a solid research work."}]}