{"title": "R-COT: REVERSE CHAIN-OF-THOUGHT PROBLEM\nGENERATION FOR GEOMETRIC REASONING IN LARGE\nMULTIMODAL MODELS", "authors": ["Linger Deng", "Yuliang Liu", "Bohan Li", "Dongliang Luo", "Liang Wu", "Chengquan Zhang", "Pengyuan Lyu", "Ziyang Zhang", "Gang Zhang", "Errui Ding", "Yingying Zhu", "Xiang Bai"], "abstract": "Existing Large Multimodal Models (LMMs) struggle with mathematical geomet-\nric reasoning due to a lack of high-quality image-text paired data. Current geomet-\nric data generation approaches, which apply preset templates to generate geomet-\nric data or use Large Language Models (LLMs) to rephrase questions and answers\n(Q&A), unavoidably limit data accuracy and diversity. To synthesize higher-\nquality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT) geom-\netry problem generation pipeline. First, we introduce GeoChain to produce high-\nfidelity geometric images and corresponding descriptions highlighting relations\namong geometric elements. We then design a Reverse A&Q method that reasons\nstep-by-step based on the descriptions and generates questions in reverse from\nthe reasoning results. Experiments demonstrate that the proposed method brings\nsignificant and consistent improvements on multiple LMM baselines, achieving\nnew performance records in the 2B, 7B, and 8B settings. Notably, R-CoT-8B sig-\nnificantly outperforms previous state-of-the-art open-source mathematical models\nby 16.6% on MathVista and 9.2% on GeoQA, while also surpassing the closed-\nsource model GPT-40 by an average of 13% across both datasets. The code is\navailable at https://github.com/dle666/R-CoT.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) exhibit excellent reasoning capabilities and draw extensive atten-\ntion from the artificial intelligence research community (Lu et al., 2023b) to mathematical problem-\nsolving in textual form (Chen et al., 2024b; Liao et al., 2024; Zhou et al., 2024; Zhao et al., 2024b;\nZhou & Zhao, 2024; Kim et al., 2024). However, LLMs still struggle to solve mathematical prob-\nlems involving images that require visual comprehension. Geometry problems, as typical mathe-\nmatical problems with images, play an important role in evaluating mathematical reasoning skills\n(Zhang et al., 2023c), requiring a high level of visual comprehension. Besides, even though some\nproblems are not related to geometry on the surface, they require the same skills for models (e.g.,\nfine-grained image comprehension skills and multi-step reasoning skills). With the appearance of\no1 (OpenAI, 2024), GPT-40 (Islam & Moushi, 2024), Gemini (Team et al., 2023), and numerous\nLarge Multimodal Models (LMMs) (Li et al., 2024a; Liu et al., 2024; Chen et al., 2024d; Bai et al.,\n2023), recent researches progressively investigate using LMMs to solve mathematical geometry\nproblems.\nAlthough LMMs show impressive results in general visual question-answering (VQA) tasks (Fan\net al., 2024; Liu et al., 2024), they still face challenges in solving mathematical geometry problems.\nThe main reason is that the training data for LMMs are mainly from natural scenes, which have"}, {"title": "2 RELATED WORK", "content": "Recent research aimed at improving geometric reasoning in LMMs can be broadly divided into two\ncategories. The first category focuses on inspiring geometric reasoning ability during the inference\nstage, while the other attempts to improve reasoning ability through targeted training.\nInspiring Model Potential During Geometric Inference. For the inference process, Zhao et al.\n(2024a) employs the chain of thought in visual and symbolic language modes to cross-validate and\ncorrect each other for the final result. Hu et al. (2024) utilizes code to generate images and solve\nproblems through a visual chain of thought. Meanwhile, Mouselinos et al. (2024) uses a LLM as\nan agent to call external tools.\nImproving Model Reasoning Ability During Geometric Training. For model training, sym-\nbolic geometry solvers like GeoS (Seo et al., 2015), Inter-GPS (Lu et al., 2021), and S2G (Tsai\net al., 2021) aim to build formal language systems that use formal language for deductive reasoning\non geometry problems. These systems are manually designed for formal languages with relatively\nsmall datasets, e.g. the GeoS dataset containing 186 problems and the Geometry3k (Lu et al., 2021)\ndataset containing about 3000 problems. The size of the datasets has increased slightly with the ad-\nvent of neural geometric solvers, such as UniGeo (Chen et al., 2022), GeoQA (Chen et al., 2021),\nGeoQA+ (Cao & Xiao, 2022), and PGPS9K (Zhang et al., 2023a), with a total size of around 25k.\nThe above datasets are collected manually, with high labeling costs and limited scale. With the rise\nof LMMs, these data scales are far from satisfactory for training, so many methods are devoted to\nbuilding larger datasets. G-LLaVA (Gao et al., 2023) uses an LLM to reword original Q&A pairs\nin the GeoQA and Geometry3k dataset, resulting in 115k geometric Q&A data and 60k alignment\ndata but does not increase the diversity of images and knowledge points. GeomVerse (Kazemi et al.,\n2023) uses a code-written engine to generate accurate geometric images and Q&A pairs, but there\nis still a certain gap between the generated images and real-world geometric images. Additionally,\nthe questions generated by the template lack diversity.\nTo synthesize geometry data with accuracy and diversity, we introduce R-CoT, a novel geometry\ndata generation pipeline that addresses visual hallucinations and reasoning limitations in LMMs.\nThis pipeline effectively generates the GeoMM dataset, featuring high-fidelity geometric images\nwith accurate and diverse Q&A pairs."}, {"title": "3 REVERSE CHAIN-OF-THOUGHT", "content": "The limited amount of high-quality mathematical geometry data restricts the geometric reasoning\nperformance of existing LMMs. Current data generation methods possess two main limitations: (1)\nAt the image level, synthetic images have an appearance gap with real-world geometric images.\n(2) At the text level, generated Q&A pairs lack accuracy and diversity, especially the relationships\nbetween geometry elements.\nTo address these issues, we propose R-CoT, a two-stage mathematical geometry data generation\npipeline. As shown in Fig. 2, in the first stage, to ensure the fidelity of the generated images,\nwe develop GeoChain by referring to real-world mathematical geometry images. GeoChain can\ngenerate high-fidelity geometric images with multiple geometric elements in different relations. In\nthe process of generating images, detailed image descriptions are also generated synchronously,\nwhich accurately describe the geometric elements and their relations and serve as priors for the\nsecond stage."}, {"title": "3.1 GEOCHAIN", "content": "To synthesize geometric images that are close to real-world geometric images, we design GeoChain,\na chain of geometric images and descriptions generation engine that can generate both high-fidelity\ngeometric images and their accurate descriptions. Only image descriptions will be used in the sub-\nsequent generation of geometric Q&A pairs.\nAs illustrated in Fig. 3, the GeoChain consists of three parts. Specifically, we first construct a\ngeometry substrate pool that contains 20 different geometry substrates. Next, we randomly sample\none or more substrates from this pool and input them into the Geometry Generation Chain. In the\nGeometry Generation Chain, the sampled substrates are combined into one geometric image step\nby step. Different from previous methods, our methods include many line operations (e.g., adding\na line that connects midpoints of neighbor edges), which are common in real-world mathematical\ngeometry images. Besides, at each step, we label the vertices with random letters (e.g., A, B, C) and\nannotate the geometric properties such as edge lengths and angles to create high-fidelity geometric\nimages. Corresponding image descriptions are also generated step by step according to predefined\ntemplates. It is worth mentioning that these descriptions not only describe geometric shapes but\nalso contain the relations between different geometric elements, such as points on which lines and\nwhether two lines intersect. These relation descriptions are essential for the generation of relational\ngeometry questions."}, {"title": "3.2 REVERSE A&Q", "content": "Current LLMs still have limitations in solving complex geometric problems, using LLMs to directly\ngenerate Q&A pairs in one step may bring incorrect information. Inspired by CoT, we propose the\nReverse A&Q (as shown in Fig. 4) to generate accurate and diverse Q&A pairs step by step using\nthe generated image descriptions. This process consists of three steps: Description Patch Reasoning,\nChain-of-Thought Fusion, and Question Generation."}, {"title": "3.3 GEOMM", "content": "Through the R-CoT pipeline, we construct a high-quality geometric dataset, GeoMM. Detailed sta-\ntistical information regarding the images and text within GeoMM is presented in Fig. 5.\nAt the image level, GeoMM contains 20 geometric shapes, with the most common being trian-\ngles, quadrilaterals, and circles. To ensure the model can interpret geometric images of varying\ncomplexity, GeoMM includes images categorized into four complexity levels, determined by the\nnumber of geometric shapes present. Unlike previous-generation engines, which primarily focus on\nconstructing geometric images through the combination of polygons or circles, we emphasize the\ncritical role of lines in geometric figures. Lines with special properties, such as midlines or radii,\nare foundational to many geometric theorems (e.g., the midline theorems). To enhance the richness\nof geometric knowledge embedded in Q&A pairs generated at later stages, we integrate line ele-\nments with specific properties (e.g., radii) into the images. This approach significantly improves the\nfidelity of the generated images."}, {"title": "4 EXPERIMENTS", "content": "Our R-CoT pipeline utilizes ERNIE Bot 4.0 as the core LLM. We train several LMMs (Bai et al.,\n2023; Liu et al., 2024; Huang et al., 2024; Zhang et al., 2023b; Chen et al., 2024d) using geometric\ninstruction data from the Geo170K (Gao et al., 2023) and our GeoMM dataset. Both the projected\nlinear layer and the language model are trainable during training. The models are trained for two\nepochs with a batch size of one per NPU (Ascend910-65G). For evaluation, we compare these\nmodels with other LMMs on the geometry problem solving on the testmini set of MathVista (Lu\net al., 2023a) and the test set of GeoQA (Chen et al., 2021) following Gao et al. (2023). We adopt\nTop-1 accuracy as the evaluation metric and employ the regular expression (Gao et al., 2023) to\nextract the predicted choices from the generated answers. The answer is considered incorrect if the\nregular expression fails to extract a valid answer."}, {"title": "4.2 EFFECTIVENESS OF GEOMM", "content": "Compared with existing datasets. We train our model using GeoMM and two recent synthetic\ndatasets for geometric problems, i.e. MAVIS (synthesis part) (Zhang et al., 2024) and Geom-\nVerse (Kazemi et al., 2023) at the same data scale for a fair comparison. Specifically, we sample\nthe data from each dataset to different scales. As we observe clear performance fluctuations caused\nby the quality of train data, we train the models three times at each data scale and report the aver-\nage Top-1 accuracy in Fig. 7 (a) and (b). In general, all three datasets can improve the geometry\nreasoning ability of the baseline model. The model trained using our GeoMM exhibits significantly\nsuperior performance in most settings, demonstrating the better quality of GeoMM. Moreover, as\nshown in Fig. 7 (c) and (d), the performance variance of our method is significantly lower. The"}, {"title": "4.3 ABLATION STUDY", "content": null}, {"title": "4.4 COMPARISON WITH PREVIOUS STATE-OF-THE-ART", "content": "With the proposed method, we train three specialized models for geometry problem solving\nnamed R-CoT-2B, R-CoT-7B, and R-CoT-8B based on Mini-Monkey-2B, InternLM-XC2-7B, and\nInternVL-2.0-8B, respectively. We compare our models with both general and mathematical LMMs\non the testmini set of MathVista and the test set of GeoQA. We use the same prompt prefix as\nG-LLaVA (Gao et al., 2023). As shown in Tab. 4, R-CoT-8B achieves the best performance on\nboth datasets. Specifically, it significantly surpasses advanced closed-source GPT-40 by 12.5% on\nMathVista and 14.5 % on GeoQA. Compared to mathematical LMMs, it still outperforms SOTA\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA."}, {"title": "5 DISCUSSION", "content": "To better understand why R-CoT leads to improvements, we conduct qualitative analysis by com-\nparing the best-performing closed-source LMM GPT-40 with our model. Examples from different\ntypes of geometric images are shown in Fig. 8. Our model generates a more concise chain of thought\nand consistently arrives at the correct answer. In contrast, GPT-40's problem-solving ability is pri-\nmarily limited by its perceptual understanding of geometry; for instance, it often misinterprets angle\nrelationships in these cases. We argue that our approach addresses this by introducing relational\nproblems that were overlooked in previous datasets, thereby enhancing the model's fine-grained\nperceptual abilities, and allowing the model to produce a more streamlined reasoning process. The\nresults also suggest that accurate comprehension of geometric components could be crucial for ef-\nfective reasoning. More examples can be found in Appendix D. Due to the reasoning capabilities of\ncurrent LMMs, we rely on LLMs to generate Q&A pairs. This can occasionally result in non-unique\nimages corresponding to the same descriptions. Although most of the generation results are correct,\nsome errors still persist."}, {"title": "6 CONCLUSION", "content": "We propose R-CoT, a novel reverse generation pipeline that significantly enhances the quality and\nfidelity of geometry Q&A pair generation. The data produced by R-CoT offers obvious advantages\nover previous synthesis geometry datasets, such as MAVIS and GeomVerse. Our approach achieves\nconsistent improvements over existing LMMs, setting new state-of-the-art results compared to both\nopen-source and closed-source models. Our results highlight the critical role of high-quality data\nin improving the geometric reasoning capabilities of LMMs. We will extend this method to other\ntypes of mathematical questions while exploring strategies to mitigate LMM visual hallucinations\nand improve data accuracy, providing further insights for future research."}, {"title": "A DETAILS OF PROMPT IN REVERSE A&Q", "content": "We used ERNIE Bot 4.0 to implement Reverse A&Q. We describe the prompts used in Reverse\nA&Q, including the prompts for the Description Patch Reasoning (Fig. 9), the Chain-of-Thought\nFusion (Fig. 10), and the Question Generation (Fig. 11). In these figures, the texts in blue are in-\nstructions, and in orange are the input information. Each prompt contains three contextual examples,\nand we show only one of them with the remaining examples replaced by ellipses."}, {"title": "B EXAMPLES OF GEOMM DATASET", "content": "Through the R-CoT, we construct a high-quality geometric dataset, GeoMM. In Fig. 12, we provide\na detailed overview of specific cases from GeoMM. These cases demonstrate the variety of mathe-\nmatical geometry question types covered by GeoMM, including solving for lengths, angles, areas,"}, {"title": "C THE CASE OF DIRECT GENERATION AND REVERSE A&Q GENERATION", "content": "The core idea of the Reverse A&Q is to improve the accuracy of Q&A pairs by first simplifying\nthe reasoning based on descriptions and then generating corresponding questions from the answers\nin a reversed manner. A straightforward approach is directly prompting ERNIE Bot 4.0 to generate\nQ&A pairs from the input image description. However, as shown in the left of Fig. 13, this approach\noften fails to determine the correct answer. In contrast, the Q&A pairs produced by Reverse A&Q\nare correct for all three instances with our design."}, {"title": "D THE CASE OF GEOMETRIC REASONING FOR GPT-40 AND OUR MODEL", "content": "We conduct qualitative analysis by comparing the best-performing closed-source LMM GPT-40 with\nour model. Fig. 14 shows several examples from different types of geometric images. We highlight\nthe incorrect key steps in red and the correct key steps in green. Obviously, our model generates a\nmore concise chain of thought and arrives at the correct answer."}, {"title": "4.1 SETUP", "content": "Our R-CoT pipeline utilizes ERNIE Bot 4.0 as the core LLM. We train several LMMs (Bai et al.,\n2023; Liu et al., 2024; Huang et al., 2024; Zhang et al., 2023b; Chen et al., 2024d) using geometric\ninstruction data from the Geo170K (Gao et al., 2023) and our GeoMM dataset. Both the projected\nlinear layer and the language model are trainable during training. The models are trained for two\nepochs with a batch size of one per NPU (Ascend910-65G). For evaluation, we compare these\nmodels with other LMMs on the geometry problem solving on the testmini set of MathVista (Lu\net al., 2023a) and the test set of GeoQA (Chen et al., 2021) following Gao et al. (2023). We adopt\nTop-1 accuracy as the evaluation metric and employ the regular expression (Gao et al., 2023) to\nextract the predicted choices from the generated answers. The answer is considered incorrect if the\nregular expression fails to extract a valid answer."}]}