{"title": "EmoSpeech: A Corpus of Emotionally Rich and Contextually Detailed Speech Annotations", "authors": ["Weizhen Bian", "Yubo Zhou", "Kaitai Zhang", "Xiaohan Gu"], "abstract": "Advances in text-to-speech (TTS) technology have significantly improved the quality of generated speech, closely matching the timbre and intonation of the target speaker. However, due to the inherent complexity of human emotional expression, the development of TTS systems capable of controlling subtle emotional differences remains a formidable challenge. Existing emotional speech databases often suffer from overly simplistic labelling schemes that fail to capture a wide range of emotional states, thus limiting the effectiveness of emotion synthesis in TTS applications. To this end, recent efforts have focussed on building databases that use natural language annotations to describe speech emotions. However, these approaches are costly and require more emotional depth to train robust systems. In this paper, we propose a novel process aimed at building databases by systematically extracting emotion-rich speech segments and annotating them with detailed natural language descriptions through a generative model. This approach enhances the emotional granularity of the database and significantly reduces the reliance on costly manual annotations by automatically augmenting the data with high-level language models. The resulting rich database provides a scalable and economically viable solution for developing a more nuanced and dynamic basis for developing emotionally controlled TTS systems.", "sections": [{"title": "1. Introduction", "content": "With the development of artificial intelligence technology, text-to-speech (TTS) models have been able to generate high-quality speech [23] [22] [14]. However, creating a TTS system that can control emotions is still a huge challenge[20]. These systems work to improve the accuracy of speech cloning and the ability to express complex nuanced emotions, resulting in more natural speech output [13] [9]. Considering the wide application of TTS and the importance of user experience, research on emotionally controllable TTS is necessary.\nIn text-to-speech (TTS) systems, the literature identifies two primary methods for manipulating speaking styles: utilizing emotion labels and mimicking reference audio [20]. Although the latter technique can be effective in modelling specific emotional expressions, it relies on audio samples that are of high quality and in the mood of the target emotion. Therefore, finding compliant reference audio samples can be challenging in practical applications[21]. As for emotion labeling, various strategies have been proposed to enhance the representation of complex emotions [24] developed a dual-layer model employing fully connected (FC) layers; one layer classifies emotions using one-hot encoding, while another estimates the intensity of these emotions as scalar values. However, this method cannot express complex emotions due to the limited number of emotion labels [20]. To address these limitations, [7] introduced soft labels that incorporate probabilities for undetected and mixed emotions, thereby increasing the labeling process's accuracy and complexity. Despite these enhancements, accurately capturing and labeling previously unseen complex emotions remains a significant challenge. Additionally, [18] suggested a novel method based on Global Style Tokens (GST), which utilizes an algorithm to balance internal and external distance ratios for weighing emotion information accurately by analyzing the distribution of emotions. However, this method's reliance on the distances between emotion embeddings may reduce the model's performance and degrade audio quality [7].\nThe accuracy of text-to-speech (TTS) models for emotion control is usually related to the training data [2]. Most databases contain only a few major emotional states, such as joy, anger, sadness, and surprise, as confirmed by multiple studies [25] [17] [4]. Furthermore, most databases contain only a limited number of speakers, which also limits the diversity of speech in the database [7]. Therefore, the lack of voice diversity in the database severely limits the ability of the TTS system to provide a wide range of voice features and accurate emotional expression. Therefore, there is an urgent need for a voice database containing multiple speakers and detailed emotional annotations to enhance the emotional control ability and generalization of TTS technology.\nSome researchers have devoted themselves to developing more complex emotional speech databases to address the above problems. For example, [8] has produced a database that annotates each speech sample with a sentence description. However, these annotations mainly capture objective speech features such as pitch, speaking rate, and loudness. Therefore, these features cannot fully represent the emotions in the speech, resulting in many emotional richnesses not being expressed. To further refine this approach, [11] introduced a method that separates features into style and speaker cues, while also dividing sentiment terms into perception and impression categories. Despite these innovations, the core problem remains: the main content of the output speech is still closely tied to the established labels of perception and impression words. Therefore, when the system encounters sentiment expressions outside of the predefined label library, it may not be able to express it accurately. To improve the accuracy of sentiment annotation, [20] developed a three-step annotation approach that relies entirely on human annotators to describe the emotions present in speech. While this approach can improve the accuracy of sentiment description, it requires a lot of human resources and time, posing a significant challenge to practical implementation in large data sets. In view of these challenges, it is necessary to adopt a new method that"}, {"title": "2. Method", "content": "Figure 1 shows an overview of the system for building the Emo-Speech speech database. The system consists of three main modules: a) target speech extraction, b) emotion recognition, and c) data augmentation. First, the data crawler automatically acquires speech data. Then, the target speech extraction module utilizes the Fast Whisper model [12] to transcribe the audio into text and uses predefined rules to filter out segments with weaker emotional content, ensuring that only segments with ap-parent emotional features are retained. Subsequently, the emotion recognition module adopts the SECAP pre-trained model [19] to obtain a detailed description of the audio emotional fea-tures to replace traditional single-word emotion labels. Finally, the data augmentation module is applied to enhance the diver-sity of emotional descriptions. This is achieved by using LLM, creating multiple versions of each sentence that convey the same emotional content in different ways of expression."}, {"title": "2.2. Target Speech Extraction", "content": "Our data collection focuses on audio from films and TV series in order to extract the most distinctive speech from the collected audio data. On the one hand this is to increase the richness of the database while ensuring generalisation of the model. This is be-cause film and TV dramas tend to contain more complex speech in terms of characters, dialects, and emotions than databases created through audio recordings. Given the diversity of these voices and the randomness of the sampling, our model will be more generalisable. On the other hand, this is to increase the realism of speech emotion. We believe that if emotions are ex-pressed directly without context, the audio recorded in this way may make the speech lose its authenticity due to the deliber-ate expression of emotions. However, the film and television dramas we crawled have narrative contexts that allow for more natural and richer emotions to be expressed. It is worth noting that we crawled more than 40 films and TV dramas and retained only the emotionally rich speech segments from them, so the EMOSpeech database contains most of the human emotions.\nTo ensure the intensity of emotion in the speech, we further filtered the crawled data(as shown in Figure 1 a). We specifi-cally targeted character dialogues, which are often more emo-tional than other audio such as narration. Target speech extrac-tion starts with slicing the audio to improve the efficiency of subsequent filtering steps. Using the Fast Whisper model [12], we converted the speech to text, using the presence of quota-tion marks to identify segments containing dialogue. As shown in Figure 1, the filtering stage involves applying the FunASR model [6] to accurately timestamp all dialogues. We then extract these dialogues, ensuring that only speech with rich emo-tional content is retained. This approach allows us to extract conversational audio with significant expressiveness, effectively populating the EmoSpeech database with higher quality emo-tional speech."}, {"title": "2.3. Emotion Recognition", "content": "In developing the EmoSpeech database, the Emotion Recogni-tion module plays a pivotal role in identifying and annotating the emotional content within speech. We have integrated"}, {"title": "2.4. Data Augmentation", "content": "In the Data Augmentation module of the EmoSpeech database project, we finetuned the LLM to generate more emotion de-screption based on the result of SECap to enhance the vari-ability and robustness of the emotional descriptions within our dataset[1]. Initially, the SECap framework provided us with a corpus of single-sentence sentiment descriptions. To provide the data needed for fine-tuning the LLM, we performed a man-ual rewriting process. Each original sentence was converted into five semantically similar but different sentences. These rewritten sentences were connected using the \"-\" separator to maintain a standardized format to facilitate consistent training and evaluation. These rewritten sentiment descriptions are then paired with the original sentences generated by SECap to form the input for fine-tuning the LLM. During the fine-tuning phase, the original and prompt sentences are used as input, while the five rewritten sentences are used as output. The dataset is di-vided into a training set and a validation set with a ratio of 80:20 to detect the fine-tuning results of the model at any time. Given experimental results of [3], we use ChatGPT-3.5[5] to implement this fine-tuning process. With this feature, we have greatly increased the diversity of sentiment descriptions in the database."}, {"title": "3. Dataset Analysis and Experimental Design", "content": "The EmoSpeech database is an innovative contribution to emotional speech synthesis, containing about 16 hours of audio. The corpus extracts audio samples from a large number of film and television dramas to capture a variety of emotional expressions and scenes. Each sample is described with a detailed natural language sentence instead of tradi-tional fixed emotion labels and categories, providing more accurate data for emotion-controlled TTS models. Regard-ing diversity, the dataset includes voices of different ages, genders, emotions, and accents, ensuring the generalization of the pipeline. In addition, the annotations in EmoSpeech are generated by the SECAP model and rewritten through LLM to increase data richness. This approach allows for a complex understanding of emotions, capturing both ob-vious emotional expressions and their potential intensity and nuances. Some of the data in the EmoSpeech dataset is as follows:\nThe core model of our pipeline is SECap, whose powerful speech annotation has been verified in [19]. However, in order to further validate the effectiveness of our entire pipeline, we conducted a detailed experimental study. Specifically, we first selected 100 audio samples randomly from website and generated natural language descriptions describing perceived emotions for each audio by our emotion recognition model. Subsequently, these initial annotations were augmented with data using a large-scale language model (LLM). Finally, for each audio segment, we generated multiple descriptive texts for this one emotion.\nFor the evaluation of both the original and paraphrased descriptions, we recruited five independent evaluators. Eval-uation criteria focused on the relevance and accuracy of the described emotions and the effectiveness of the paraphrasing in preserving the original emotional sentiment while introducing variation in its expression. Evaluators utilized a structured rating system, scoring each description on a scale from 1 to 5, where 1 represented minimal alignment with the emotional content of the audio, and 5 represented perfect alignment. The outcomes of this evaluation are compiled"}, {"title": "4. Conclusion & Future Work", "content": "In this paper, we present a new method for constructing an emotional speech database. The method not only extracts speech segments rich in emotional content, but also describes the emotions in the audio using detailed natural language. Our main contribution consists in enhancing the richness of the speech database by extracting the dialogue in the speech. We can tag the emotional content of speech with complete sentences and extend the data with LLM-based paraphrases.\nThis approach significantly reduces the costs associated with manual labour and provides a basis for the development of emotionally controlled TTS systems.\nIn the future, we should evaluate pipeline further. Although our process has shown good results, it has only been evaluated using the MOS subjective assessment. In order to further con-firm the validity of our method, it is important to incorporate ob-jective assessment methods. In addition, expanding the testing phase to include a larger and more diverse set of audio samples would provide a more comprehensive validation of the perfor-mance of the process."}]}