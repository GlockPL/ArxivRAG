{"title": "PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach", "authors": ["ZHIHAO LIN", "WEI MA", "MINGYI ZHOU", "YANJIE ZHAO", "HAOYU WANG", "YANG LIU", "JUN WANG", "LI LI"], "abstract": "In recent years, Large Language Models (LLMs) have gained widespread use, accompanied by increasing concerns over their security. Traditional jailbreak attacks rely on internal model details or have limitations when exploring the unsafe behavior of the victim model, limiting their generalizability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method inspired by the concept of escaping a security maze. This work is inspired by the game of rats escaping a maze. We think that each LLM has its unique \u201csecurity maze\u201d, and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-40-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate the potential for general artificial intelligence (Bubeck et al., 2023). However, LLMs also face security threats (Yao et al., 2024b). Currently, the primary approach to addressing LLM security threats is through safety alignment techniques (Bai et al.,"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Prelimary", "content": "Reinforcement learning is a machine learning method where an agent learns strategies by performing actions in an environment and learning from feedback to maximize cumulative rewards. It is similar to human trial-and-error learning and is suitable for complex, dynamic environments. The interaction between the agent and the environment is central: at each step, the agent receives the state of the environment, decides and executes an action, and the environment provides feedback in the form of a new state and immediate reward. This process repeats continuously, allowing the agent to gradually learn how to influence the environment through its actions to accumulate rewards. These algorithms explore optimal strategies through trial and error, capable of handling uncertainty and even finding solutions in environments that are difficult to understand."}, {"title": "2.2 Overview", "content": "Based on the aforementioned view of escaping the security zones of LLM, we proposed our approach, PathSeeker, as shown by Figure 2. Overall, we view jailbreak attacks on LLMs as a game of escaping a secure maze. By strategically executing predefined actions on the input, the attacker aims to achieve its goal of compromising the LLM. The rat attacker tries different actions, learns the lessons from the feedback of LLM and improve the next action. First, we select a harmful question from the question pool and a jailbreak template from the template pool as inputs for the mutators (question mutator and template mutator). We have defined multiple mutation actions for both mutators. Question mutator and template mutator will mutate the harmful question and the jailbreak template. To select mutation actions for modifying the harmful question and template, we adopted a multi-agent reinforcement learning method. The multi agents determine the actions based on the system state, which considers the inputs and responses of the LLM. After mutation, we combine the mutated question and the mutated jailbreak template as the input prompt for LLM. Judgement model will judge if the response contains the answer to the question or not, and also give one confidence score. If one attack is successful, the used jailbreak template for this attack will be added into the template pool. To compute the reward feedback for the reinforcement learning, we define a metrics (Information Quantization, IQ) to measure the information carried by response. IQ and the confidence score from Judgement model will be used as the reward for training the multiple agents. For the next iteration, we select the question and template again from the question and template pool. The multiple-agent reinforcement learning will make the new action choice based on the received feedback.\nIn the following subsections, we firstly introduce the challenges that we encountered during designing our approach. Then, we start introducing the important components of our approach, by the following order, selection of questions and template, mutators, judgement model, Information Quantization and multiple-agent reinforcement learning."}, {"title": "2.3 Challenges", "content": "When designing PathSeeker based on the the multiple-agent reinforcement learning, we encoun-tered four challenges and they definitely affect the design of our approach.\nChallenge 1. What is the action space? The action space defines the possible actions a rat can take at each step while attempting to escape a security maze. This involves not only altering the input to the LLM but also shaping the overall strategy for the escape process. In this process, we need to determine how to modify the input to the LLM to achieve the desired goal. We examine this action space in detail from two perspectives: first, by altering the harmful question itself, and second, by modifying the jailbreak template to explore different output results and test how various inputs affect the LLM's response. When designing specific actions, we adhere to a core principle: ensuring that harmful attributes remain unchanged while maximizing the diversity and ambiguity of input styles. This helps to test the system's stability under varied input conditions.\nChallenge 2. How can we automate the determination of whether an attack is successful? In the realm of reinforcement learning, the iterative training is typically required to enhance the model performance. The reinforcement learning process requires multiple training sessions, and relying on human judgment to evaluate the success of each attack is both time-consuming and impractical. Automating the assessment of attack success is a critical issue. To address this problem, we draw inspiration from existing technologies, particularly by adopting methods from GPTFuzz (Yu et al., 2023) and CodeChameleon (Lv et al., 2024). We introduce a judgement model to analyze and evaluate the output of LLM. This judgement model can automatically determine the success of an attack, making our training process more automated and efficient. Through this method, our system can adapt more quickly to changes and continuously optimize its performance throughout the iterative process.\nChallenge 3. How to define the system state of an LLM? When defining the system state of an LLM, we need to consider several factors. The state of the LLM typically comprises several key components: the input, internal weights, hidden state representations for a given input, output logits, and the final generated text. For closed-source LLMs, internal weights, hidden state representations, and output logits are inaccessible, preventing direct observation or debugging of the models' internal workings. In contrast, for open-source LLMs, we can extract and analyze these components, though doing so requires additional hardware resources and incurs extra time costs. Given these considerations, we choose to use the input and the final generated text output of the LLM as representations of its system state. This is a practical and efficient method because this information is easily accessible for all types of models. The input is a crucial factor in determining the behavior of LLMs, as it directly influences the response generation process and output content. However,"}, {"title": "2.4 Selection of Harmful Question and Jailbreak Template For Attack", "content": "In selecting from the seed pools (question and jailbreak template pools), we use a method that combines randomness with strategic selection. Harmful questions are chosen randomly, while jailbreak templates are selected using the UCB strategy (Li et al., 2010). The UCB strategy assigns a score based on the past performance of jailbreak templates in successful attacks. This strategy chooses the highest-scoring seed with a probability \\( \\delta \\) and makes a random selection with a probability value, \\( 1 \u2013 \\delta \\). This approach not only ensures that we prioritize jailbreak templates that are more likely to successfully attack but also effectively reduces the use of ineffective ones. Meanwhile, the probability of random selection allows us to explore the global optimal solution, enhancing the flexibility and comprehensiveness of the strategy. In our experiment, we set \\( \\delta \\) to 0.95."}, {"title": "2.5 Template Mutator and Question Mutator", "content": "The role of mutators is to modify the input of an LLM, a process achieved through selected actions. We utilize the text processing capabilities of LLMs to perform multifaceted and in-depth transformations on the input, aiming to confuse the target model. To mutate the jailbreak template and the question, we have designed two categories of action mutators: the template mutator and the question mutator. Among them, the template mutator employs five transformation operations from GPTFuzzer (Yu et al., 2023), which include: 1) Generate: create a brand-new story or scene description based on understanding the context and meaning, ensuring the style is similar to the original text. 2) Crossover: form a new mutated template by merging elements from two different jailbreak template seeds. 3) Expand: add more detailed and in-depth explanatory content to the existing template. 4) Shorten: compress the template to make it more concise and clear without altering its semantics. 5) Rephrase: restructure the wording of a given template, aiming to change the expression while maintaining the original semantics.\nWhen mutating harmful questions, our goal is to preserve their harmful semantic attributes as much as possible. We achieve this by altering the tone, confusing the original question, splitting the question, reconstructing grammar, and using synonym replacements, all with the aim of inducing the victim model to respond to these harmful prompts. Additionally, during strategy formulation,"}, {"title": "2.6 Judgement Model", "content": "We introduce the judgment model primarily to provide rapid feedback during the reinforcement learning training, thereby improving both learning efficiency and effectiveness. By incorporating the judgment model, we not only accelerate feedback acquisition for reinforcement learning but also automate the entire attack process, eliminating the need for human intervention. We implemented an LLM-based evaluation method, designing a judge prompt to guide the LLM in assessing the attack outcomes. This approach is widely used by evaluating LLM attackings (Chu et al., 2024,\nJin et al., 2024b, Mehrotra et al., 2023, Qi et al., 2024, Yao et al., 2024a) due to to high evaluation consistency (Jin et al., 2024b, Qi et al., 2024) between the LLM evaluator and the human. The judgment prompt we employed is shown by Figure 3. The judgment model outputs four labels with confidence scores: full refusal, full compliance, partial refusal, and partial compliance. We experimented with several SOTA large models, including GPT-4, GPT-40-mini, Deepseek-chat, Llama-3.1-70b, and Qwen2-72b, spanning both commercial and open-source options. During this process, we evaluated these models' performance, with particular emphasis on their ability to assess harmful content in inputs. After extensive testing and comparisons, we found that the commercial models excelled at identifying and filtering out potentially harmful content. Given the considerations of accuracy and API usage costs, we ultimately selected GPT-40-mini as our judgment model."}, {"title": "2.7 Information Quantification", "content": "Information Quantification is aimed at addressing how to extract valuable lessons from un-successful attack outcomes and relaying this feedback to the reinforcement learning. By closely observing the entire process of attacking LLM, we discovered that cleverly wrapping questions can effectively guide the targeted LLM to gradually provide an increasing amount of information. As shown on the right side of Figure 1, the LLM initially refuses to answer the harmful question"}, {"title": "2.8 Multiple-Agent Reinforcement Learning", "content": "Figure 5 illustrates how we employ reinforcement learning to execute the entire attack process. Our approach involves two agents: the question agent and the jailbreak template agent. The question agent is responsible for selecting the action of the question mutator, denoted as \\( a_q \\), while the template agent selects the action of the template mutator, denoted as \\( a_t \\). The question mutator and the template mutator execute the selected actions and combine their outputs into content, which is then input to the target model. The target LLM generates responses based on these input prompts. Our judgement model assesses the harmfulness of responses and generates a score, denoted as \\( J_{score} \\). At the same time, we calculate the amount of information in the response to obtain an IQ score. We then use the IQ and \\( J_{score} \\) to compute the reward denoted as r. Additionally, we vectorize the input prompt and output response (denoted as < P, R >) of the current system state using a small open-source embedding model to obtain the state vector, denoted as \\( s_{<P,R>} \\). Next, we select the harmful question and the jailbreak template (denoted as < Q, T >) from the respective question and template pools for the attack, embedding their mutated versions to generate vectors of harmful inputs, denoted as \\( v_{<Q,T>} \\). The system state \\( s_{<P,R>} \\), the vector of harmful inputs \\( v_{<Q,T>} \\), and the rewards r serve as inputs for the question agent and the the template agent."}, {"title": "2.8.1 Action Agents", "content": "We have defined two action agents: the question agent and the template agent. The question agent offers five action choices, each corresponding to one of the five operations of the question mutator: Euphemize, Confusion, Split, Restructure, and Substitution. These operations aim to mutate questions in various ways, allowing them to better adapt to different input contexts. Similarly, the template agent has five action choices, each linked to one of the template mutator's operations: Generate, Crossover, Expand, Shorten, and Rephrase. These operations enable the template agent to adjust templates with greater flexibility. Both of these agents are designed based on the Actor-Critic (Konda and Tsitsiklis, 1999). The primary responsibility of the actors is to make decisions regarding the action choices, while the critics are tasked with evaluating the effectiveness and rationality of these decisions. To accomplish this, both agents employ neural networks, allowing them to learn and adapt within complex environments. When making decisions, these two agents comprehensively consider the current system state (\\(S_{<P,R>}\\)), the reward (r), and the malicious input they are facing (\\(v_{<Q,T>}\\)). We have adopted a joint optimization framework, MADDPG (Lowe et al., 2017), to simultaneously optimize both the question agent and the template agent. A key advantage of MADDPG is that it enables each agent to fully account for the other's choices when making decisions, ensuring efficient collaboration between agents to successfully complete tasks."}, {"title": "2.8.2 Embedding LLM State and Malicious Inputs", "content": "Since we are constructing a black-box attack, we cannot access the model's output logits, weights, or internal hidden representations. Consequently, we can only consider the input and output text obtained from both open-source and closed-source models. The input text determines the output distribution of an LLM. Given the input, the output text reflects the LLM's internal computational mechanisms and can be used as an estimate of its internal state. Thus, we represent the system state of the LLM using its input and output text. However, these texts cannot be directly utilized as the agents' output. In PathSeeker, the question agent and the template agent rely on a multilayer perceptron (MLP), which can be trained quickly but has limitations in processing text. Furthermore, harmful questions and jailbreak template texts used to attack the model also cannot be directly used as input for the agents. To address this, we employ the open-source embedding model \u2018all-MiniLM-L6-v2' to convert text into vectors, which serve as input for the agents. This model was chosen for its relatively small number of parameters and strong performance. The model size of all-MiniLM-L6-v2 is 23 million parameters, ranked in the top 10 with the smallest size but fastest speed at this moment\u00b2. Therefore, in the multiple-agent reinforcement learning, we get the harmful input vector \\( v_{<Q,T>} = Embedding(Q, T) \\) and the LLM state vector \\( S_{<P,R>} = Embedding(P, R) \\) as the input of the question agent and the template agent."}, {"title": "2.8.3 Reward", "content": "We use the following equation to compute the total reward in the reinforcement learning,\n\\(r = \\alpha * r_{iQ} + (1 - \\alpha) * r_{j}, \\qquad 0 \\leq \\alpha \\leq 1\\)\nwhere,\n\\(r_{IQ} = \\begin{cases} [-\\log(1 + |\\Delta IQ|)] & \\text{if } \\Delta IQ < 0 \\\\ [\\log(1 + |\\Delta IQ|)] & \\text{if } \\Delta IQ \\geq 0 \\end{cases}\\)\n\\(r_{j} = \\begin{cases} [-\\log(1+|\\Delta J_{score}|) & \\text{if } J_{score} < 0 \\\\ [\\log(1 + |\\Delta J_{score}|)] & \\text{if } J_{score} \\geq 0 \\end{cases}\\)\nThe rewards \\( r_{iQ} \\) and \\( r_{j} \\) take into account two different factors. \\( r_{iQ} \\) is used to reward the agents based on the richness of vocabulary in the responses, while \\( r_{j} \\) rewards the agents based on the maliciousness of the answers. The purpose of \\( r_{iQ} \\) is to encourage the LLM to provide more elaborate responses rather than simply saying no. Meanwhile, \\( r_{j} \\) aims to guide the LLM to produce more harmful content. The parameter \\( \\alpha \\) controls the trade-off between \\( r_{iQ} \\) and \\( r_{j} \\). In our work, we set it to 0.5 to balance these two types of rewards. \\( r_{iQ} \\) is derived from the reward associated with \\( \\Delta IQ \\), while \\( r_{j} \\) comes from the confidence score \\( \\Delta J_{score} \\) of the judgment model. Here, \\( \\Delta \\) represents the difference between the current value and the previous value. If either IQ or \\( J_{score} \\) decreases compared to the last time, we assign a negative reward; otherwise, a positive reward is given. Both \\( r_{iQ} \\) and \\( r_{j} \\) are calculated using the same method through the logarithmic function. We utilize the log function to scale the rewards and ensure that the log output is positive by adding an offset of 1."}, {"title": "3 Evaluation", "content": ""}, {"title": "3.1 Research Questions", "content": "To evaluate the rationality and effectiveness of our design, we have the 5 following research questions (RQs):\nRQ1: How does the choice of mutation model for mutators affect PathSeeker?\nRQ2: How does the reinforcement learning affect PathSeeker?\nRQ3: How do the reward and the agent action of PathSeeker affect the performance?\nRQ4: How does PathSeeker compare to other black-box attack frameworks?\nRQ5: Can we transfer the template or the action agents from one to another for attack?\nWe first explored how to select the mutation model for the question mutator and template mutator (RQ1). In the selection process, we considered the attack performance and the time cost. Since our method requires iterative optimisation, we aimed to find a mutation model with good attack performance and fast response time. Secondly, we examined how reinforcement learning affects our method (RQ2). This was to demonstrate the rationale behind incorporating reinforcement"}, {"title": "3.2 Experimental Setup", "content": ""}, {"title": "3.2.1 Target models", "content": "To thoroughly evaluate our attack method, we selected a diverse set of models to represent both closed-source and open-source approaches. The closed-source models, such as the GPT series (GPT-3.5-turbo, GPT-40-mini), Claude (Claude-3.5-sonnet), and GLM-4-air, were chosen for their widespread use and advanced capabilities, which provide a robust benchmark for testing our method's effectiveness. For open-source models, we included the Llama series (Llama-2-7b-chat, Llama-2-13b-chat, Llama-3-70b, Llama-3.1-8b, Llama-3.1-70b, Llama-3.1-405b),\nDeepseek series (Deepseek-coder, Deepseek-chat), Gemma2-8b-instruct, Vicuna-7b, Gemini-1.5- flash, Qwen2-7b-instruct, and Mistral-NeMo. These models were selected due to their varying architectures, sizes, and community-driven development, allowing us to assess the generalizability of our attack across a broad spectrum of language models. This diverse selection ensures that our evaluation is comprehensive, covering a wide range of potential vulnerabilities in both proprietary and open-source environments.\nIn RQ1, we use Llama-2-7b-chat as the target model to evaluate how different mutation models impact PathSeeker and to identify the one that best balances performance and response time. We consider 5 candidates for the mutation models, GPT-40-mini, Deepseek-chat, Llama-3.1-70b, Qwen2-72b-instruct and Vicuna-7b. In RQ2, we choose GPT-3.5-turbo and Llama-2-7b-chat as target models to verify if the reinforcement learning is helpful for us to complete the attack task, escaping from the security zone successfully. In RQ3, we study the reward and action design of our approach using Llama-2-7b-chat and GLM-4-air as the ablation study. In RQ4, we select 13 target models to compare PathSeeker against other black-box attack frameworks, including GPT-3.5/40-mini, Deepseek-chat/coder, Calude-3.5, Gemini-1.5, GLM-4-air, Qwen2-7b-instruct, Gemma2-8b-instruct, Mistral-nemo and Llama-2-7b-chat/2-13b-chat/3.1-8b. In RQ5, we focus on the Llama-3.1 series to explore the transferability of templates and action agents. Especially, to better test the effectiveness of our approach, we include a very large Llama-3.1 model with 405b parameters."}, {"title": "3.2.2 Mutation Model", "content": "In our iterative process, one mutation model for question mutator and template mutator is employed to transform the original prompts and templates into varied content which is more likely to confuse the target model while preserving the original meaning. For this purpose, we utilize a range of mutation models, including GPT-40-mini, Deepseek-chat, Llama-3.1- 70b, Qwen2-72b-instruct, and Vicuna-7b. We evaluate the performance of these mutation models by comparing their effectiveness, while also considering factors such as cost and processing time, to select the most suitable model for our experiments."}, {"title": "3.2.3 Datasets", "content": "We used the same datasets as those utilized in GPTFuzzer. The 100 harmful ques-tions were either manually crafted by the authors or generated through crowdsourcing, ensuring they accurately reflect real-world scenarios. For the initial templates, we used the 77 templates selected by GPTFuzzer, which have been proven effective for conducting jailbreak attacks."}, {"title": "3.2.4 Baselines", "content": "We consider 5 SOTA jailbreak attack methods which are open source. The con- current works, RL-JACK (Chen et al., 2024) and RLTA (Wang et al., 2024), are not open source at the moment and we do not inlcude them.\n(1) ReneLLM (Ding et al., 2024), a method that utilizes prompt rewriting and scenario nesting techniques to bypass LLM security measures by disguising harmful prompts within seemingly benign tasks, making them difficult to detect and block."}, {"title": "3.2.5 Evaluation Metrics", "content": "We use the attack success rate (ASR) as our primary evaluation metric. The success of an attack is determined using an LLM evaluator based on two criteria: 1) the victim model does not refuse to respond to a harmful question; 2) the victim model provides information that generates inappropriate, sensitive, or dangerous content, or alters its behavior in a way that deviates from ethical guidelines. Even if the content is only partially inappropriate or indirectly incites misconduct, the attack is still considered successful in compromising the AI's safeguards.\nTo provide a more comprehensive evaluation, we employ both Top1-ASR and Top5-ASR metrics. Top1-ASR measures the ASR using the single most effective template identified during the iteration process, applied to a set of 100 questions. Top5-ASR, on the other hand, measures the ASR using the five most effective templates found during the iteration process, where success is defined as any one of these templates successfully compromising the LLM when applied to 100 questions. These templates are chosen based on their effectiveness in previous iterations, ensuring that the most robust and impactful adversarial inputs are used in the final evaluation against the models.\nConsidering that LLM can act as a reliable evaluator (Chu et al., 2024, Jin et al., 2024b, Mehrotra et al., 2023, Qi et al., 2024, Yao et al., 2024a), we prompt the GPT-40-mini to evaluate whether the attack is success based on the criteria above. We also employ the additional evaluation approach, Llama3 Guard (Llama Team, 2024), to cross-validate our approach's performance. Its results demon-strate the consistency of our outcomes. Besides, we took additional steps to ensure the precision of this evaluation method by manually verifying the results. Specifically, we randomly selected 50 evaluation results from all experiments for manual inspection, and the results showed a 100% accuracy rate. We think that this high level of accuracy is attributed to the significant effort and resources that commercial models have invested in identifying harmful content to ensure their safety and effectiveness. This commitment to safety alignment enables these models to provide more precise judgments when handling complex and sensitive content, thereby establishing a solid evaluation approach."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Choice of Mutation Model for Mutators (RQ1)", "content": "We use GPT-40-mini, Deepseek-chat, Llama-3.1-70b, Qwen2-72b-instruct, and Vicuna-7b as our mutation-model candidates. To evaluate the impact of each mutation model, we randomly select 60 harmful questions and 60 templates from our dataset. We use Llama-2-7b-chat as the target model because it is not large and has a relatively strong initial safety alignment, making it an ideal candidate to assess the effectiveness of generated adversarial examples in compromising model"}, {"title": "4.2 Impact of Reinforcement Learning (RQ2)", "content": "To validate the impact of reinforcement learning, we conducted experiments using single-agent reinforcement learning, multi-agent reinforcement learning, and a control setting without reinforcement learning. Each configuration was tested over 3000 rounds using both GPT-3.5-turbo and Llama-2-7b-chat. In the setting without reinforcement learning, the mutator applied actions randomly to the question and template. For single-agent reinforcement learning, the question agent and template agent are centralized into a single agent. Each output of the single agent contains one question action and one template action. This approach assumes that the combination of the original question and template offers 25 mutator options, resulting in 25 possible actions per step. We train the single agent using the DQN policy (Mnih, 2013). In contrast, multi-agent reinforcement learning independently trains the question agent and template agent using the MADDPG policy (Lowe et al., 2017), a classical method in multi-agent reinforcement learning. Each agent has 5 actions to select for mutating the question and template. As shown in Table 2, the single-agent DQN strategy outperforms the non-RL setting in both Top1-ASR and Top5-ASR across both LLMs. Additionally, the multi-agent MADDPG strategy achieves even greater improvements, demonstrating the effectiveness of reinforcement learning in enhancing attack success rates. We think that MADDPG is better than DQN in our context because: 1) DQN is more suitable for smaller action spaces; and 2) each agent in MADDPG focuses on its specific task while sharing decisions with other agents, making it more effective for solving complex tasks."}, {"title": "4.3 Design of Reinforcement Learning (RQ3)", "content": "We explore how the key components within PathSeeker impact its performance. Specifically, we compare the effects of the two types of the reward and examine how mutating both the question and template compares to mutating only the template in influencing the results. Our reward function comprises both \\( r_{IQ} \\) and \\( r_{j} \\). To determine whether both the IQ and the \\( J_{score} \\) of the judgement model are effective in PathSeeker, we conducted experiments using Llama-2-7b-chat and GLM-4-air. We create two additional groups: \\( r_{IQ} \\) only and \\( r_{j} \\) only. Besides, we create a group which just only mutate the jailbreak template. From Table 3, we can observe that using only \\( r_{IQ} \\) or \\( r_{j} \\) for the attack results in a lower ASR compared to PathSeeker. Addtionally, we compared our method with the approach that only mutates the template and found that, for both GLM-4-air and Llama-2-7b-chat, the ASR performance showed significant improvement."}, {"title": "4.4 Comparison with Other Attack Methods (RQ4)", "content": "Our comparison includes 13 models: GPT-3.5-turbo, GPT-40-mini, Claude-3.5-sonnet, Deepseek- chat/coder, Gemini-1.5-flash, GLM-4-air, Qwen2-7b-instruct, Gemma2-8b, Mistral-nemo, Llama- 2-7b-chat, Llama-2-13b-chat and Llama-3.1-8b. We compare our methods against five baselines: CodeChampeleon, GPTFuzzer, Jailbroken, ReNeLLM and Cipher. For GPTFuzzer and our approach, we evaluate performance using Top1-ASR and Top5-ASR as the evaluation metrics. Similarily, for ReNeLLM, Cipher and Jailbroken which use different strategies for mutating the origin questions, we use Top1-ASR and TopN-ASR, where N corresponds to the number of mutation strategies used by each method. Top1-ASR represents the attack success rate of the most effective mutation, while TopN-ASR indicates the success rate when considering all N effective mutations. This approach allows us to comprehensively assess the effectiveness and robustness of each method under varying mutation strategies. For CodeChameleon, it offers various formats; for our experiment, we selected the most effective policy-binary tree. We use the Code-ASR and Text-ASR as its evaluation metrics. In text mode, CodeChameleon manipulates natural language text prompts to generate adversarial inputs. The focus is on modifying the textual content to bypass the LLM's defenses, tricking it"}, {"title": "4.5 Transfer Attack (RQ5)", "content": "PathSeeker needs iteratively to complete the attack. The attack time cost increases as the model parameters grow in size. Given the substantial time commitment, we explored whether the top templates identified from one model or the weights of reinforcement learning agents could be transferred effectively to attack another model. We selected the five best templates from attacks on Gemma2-8b-instruct, GPT-40-mini, Deepseek-chat, GLM-4-air, and GPT-3.5-turbo and applied these templates to attack the widely used Llama series models (Llama-2-7b-chat, Llama-3-70b, Llama-3.1-8b, Llama-3.1-70b, Llama-3.1-405b)."}, {"title": "5 Related Works", "content": "Jailbreak attacks are strategies used to bypass the security and ethical guidelines of LLMs, leading to the generation of harmful content. These attacks threaten model security and can negatively impact users and society. They are classified into white-box attacks, which require access to the model's internal structure, and black-box attacks, which rely on observing input and output behaviour.\nWhite-box attacks refer to scenarios where attackers have access to the internal structure and parameters of the target model. This type of attack allows attackers to leverage the model's internal information to devise their attack strategies. Zou et al. (2023) propose GCG and it generates an adversarial suffix by combining greedy search and gradient-based search techniques. The generated adversarial prompts can not only target open language models (white-box attacks) but can also transfer to other undisclosed, fully black-box production models. Liao and Sun (2024) propose AmpleGCG and it is an enhancement of GCG that utilises internal access to the target LLM to perform jailbreak attacks by training a model to generate adversarial suffixes. AmpleGCG achieves a high success rate across various models; however, its effectiveness is limited when facing models with robust defence mechanisms, such as GPT-4. Zhu et al. (2024) propose AutoDAN and it is a gradient optimization-based attack method specifically designed to produce adversarial prompts capable of bypassing LLM security protections.\nBlack box attacks do not require any understanding of the internal structure or parameters of the target model; attackers deduce and execute their attacks solely based on the model's input and output. Ding et al. (2024) propose ReNeLLM that employs strategies of prompt rewriting and contextual nesting, modifying input prompts and embedding them in specific usage scenarios to confuse the LLM and bypass its security mechanisms. Lv et al. (2024) propose CodeChameleon and it circumvents the intent recognition phase of LLMs through personalised encrypted queries, relying on the model's ability to comprehend encrypted code, achieving better attack results than white box attacks like GCG and AutoDAN. Wei et al. (2023) propose Jailbroken and it identifies two patterns of failure in model security training, and based on these failure modes, the authors design various attack methods to bypass model security mechanisms, evaluating existing secure training models such as GPT-4 and Claude v1.3. Chao et al"}]}