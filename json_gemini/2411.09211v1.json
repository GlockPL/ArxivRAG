{"title": "Dynamic Neural Communication: Convergence of\nComputer Vision and Brain-Computer Interface", "authors": ["Ji-Ha Park", "Soowon Kim", "Seo-Hyun Lee", "Seong-Whan Lee"], "abstract": "Interpreting human neural signals to decode static\nspeech intentions such as text or images and dynamic speech\nintentions such as audio or video is showing great potential\nas an innovative communication tool. Human communication\naccompanies various features, such as articulatory movements,\nfacial expressions, and internal speech, all of which are reflected\nin neural signals. However, most studies only generate short or\nfragmented outputs, while providing informative communication\nby leveraging various features from neural signals remains\nchallenging. In this study, we introduce a dynamic neural com-\nmunication method that leverages current computer vision and\nbrain-computer interface technologies. Our approach captures\nthe user's intentions from neural signals and decodes visemes in\nshort time steps to produce dynamic visual outputs. The results\ndemonstrate the potential to rapidly capture and reconstruct lip\nmovements during natural speech attempts from human neural\nsignals, enabling dynamic neural communication through the\nconvergence of computer vision and brain-computer interface.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain-computer interface (BCI) is a technology that inter-\nprets brain signals, capturing various aspects of the user's\nintentions and mental states [1]\u2013[5]. There has been growing\ninterest in using this technology to assist communication by\ndecoding and delivering users' thoughts in diverse forms [6].\nBrain-to-speech (BTS) technology, which directly conveys a\nuser's intentions, has emerged as a significant new form of\nneural communication [7]. The BTS system aims to decode\nboth external and internal speech-related neural signals and\ngenerate speech outputs in formats such as text or audio.\nSpeech BCIs such as BTS not only serve as assistive and\nrehabilitative neuroprosthetics for individuals with speech im-\npairments but also demonstrate significant potential for broader\ndevelopment as a novel neural communication system for\ngeneral users [8]-[11].\nIn parallel, advancements in computer vision (CV) are\nenhancing the generation of talking faces through lip-sync\nmethods and decoding speech intentions through lip reading\nusing text, images, and audio [12]\u2013[16]. When combined with\nBCIs, these CV technologies can offer users a more immersive\nand realistic visual communication experience [17], [18].\nConsequently, there is increasing interest in decoding facial\nexpressions and articulatory movements from neural signals\nto generate dynamic visual outputs [19]. These are beginning\nto explore the representation of visual speech intentions from\nneural signals, an area that has not been extensively studied\nyet. Further efforts are required to generate multimodal-\nbased dynamic and realistic outputs, such as talking faces or\navatars [20].\nHowever, despite these promising advancements, most mul-\ntimodal BCI research based on neural signals still faces\nchallenges in visually reconstructing and dynamically pre-\nsenting users' speech intentions. Unlike more conventional\nand intuitive data such as text, speech, and images, capturing\nkey features of complex facial expressions and subtle lip\nmovements from noisy and limited neural signals remains a\nsignificant challenge. Decoding rapid and subtle movements\nin tasks that involve reconstructing continuous facial artic-\nulations remains challenging. While invasive methods offer\nthe potential for improved reconstruction quality due to their\nhigh spatial resolution, they involve surgical risks and limited\npracticality [10], [20]. Non-invasive methods allow for faster\nreconstruction due to their high temporal resolution, but high-\nquality reconstructions remain elusive due to the low signal-\nto-noise ratio [21]. Therefore, in-depth research is required to\ndevelop approaches that can reconstruct high-quality, realistic\nvisual outputs using non-invasive methods."}, {"title": "II. METHODS", "content": "An EEG cap was used to measure EEG signals from a\ntotal of 128 channels, including the reference electrode, and\nan additional 10 channels of EMG signals were measured\nusing separate electrodes. The EMG channels were evenly\ndistributed to capture facial movements and muscle signals\naround the mouth. For in-depth analysis, overt speech data\nfrom a single subject, a healthy male native English speaker,\nwere used, as this subject provided distinct signal recordings.\nA total of 474 sentences were selected from the MOCHA-\nTIMIT corpus for their distinct characteristics and recorded.\nThese were initially divided into 424 sentences for the train-\ning set and 50 sentences for the test set. The study was\nconducted by the Declaration of Helsinki and approved by\nthe Korea University Institutional Review Board [KUIRB-\n2022-0079-03]. The EEG signals were recorded using Brain\nVision/Recorder (Brain-Product GmbH, Germany), and the\ncorresponding audio signals were also recorded.\nFor all sentences, phonemes were segmented based on the\nreal spoken audio to align the viseme classes assigned to each\ntime interval [23]. Each phoneme unit was condensed into one\nof 15 viseme classes, allowing for the reconstruction of lip\nmovements with simplified classes [24], [25]. The segmented\nunits with varying time lengths were adjusted to a uniform\nlength for the training process and divided into short segments\nof 64 ms, 128 ms, and 256 ms. The training and test sets\nconsisted of a total of 13,826 and 1,646 trials, respectively."}, {"title": "B. Signal Preprocessing", "content": "We applied a 5th-order Butterworth bandpass filter within\nthe range of 30\u2013499 Hz, which was chosen to encompass the\nwide frequency range associated with speech and articulatory\ninformation [7]. Additionally, notch filtering was performed\nto remove line noise at harmonics of 60 Hz. Signal pre-\nprocessing was primarily conducted in Matlab, with Python\nused as needed. The main preprocessing tool utilized was the\nEEGLAB Toolbox [26]."}, {"title": "C. Viseme Decoding Model", "content": "The main backbone architecture and training details fol-\nlowed Kim et al. [27]. Unlike the conventional diffusion-\nbased model approach, which predicts noise at each step, this\nmodel had been specifically adapted for EEG data decoding\nby progressively adding noise to reconstruct the original EEG\nsignal, as shown in Fig. 1. To achieve this, denoising diffusion\nprobabilistic models (DDPM) based on a time-conditional U-\nNet architecture were used [28]. Additionally, a conditional\nautoencoder composed of an encoder $E_d$ and a decoder $D_y$\nwas employed to compensate for the information loss of the\nDDPM [29]. To indirectly incorporate the effect of the DDPM,\n$D_y$, instead of the output of $E_d$, was skip-connected to the\nDDPM layers, and both the original signal and the DDPM\noutput were also skip-connected to the stage immediately\npreceding the final layer of $D_y$. To effectively decode the\ncharacteristics of lip movements, channel attention was applied\nafter the final layer block of $E_d$. The channel attention layer\nlearned the importance of each channel and multiplied the\noriginal input by the differently assigned weights to produce an\nimproved output. $E_d$ extracted meaningful features related to"}, {"title": "D. Viseme-to-sentence Reconstruction", "content": "The phoneme-segmented and condensed viseme sequences\nunderwent an additional reconstruction process to align with\nthe original sentence. We conducted tests on a dataset of\n50 sentences, where the pre-trained model was designed and\napplied to guide the discrete results into a coherent sentence.\nTo effectively capture long-term dependencies in the sequence\ndata, we employed a long short-term memory (LSTM) model,\nwhich has demonstrated strong performance in time-series\nanalysis and natural language processing tasks. This was\ntrained with ground truth viseme sequences to ensure that\nthe predicted viseme sequences could accurately guide the\nreconstruction of the target sentences."}, {"title": "III. RESULTS AND DISCUSSION", "content": null}, {"title": "A. Viseme Decoding Performance", "content": "Table I displays the quantitative performance of viseme\ndecoding from overt speech. It compares the results when\nusing only EEG signals and when using both EEG and EMG\nsignals. The neural signals, epoched from actual speech audio,\nwere analyzed for three other time lengths: 64 ms, 128 ms, and\n256 ms. The classification results for 15 viseme classes were\nquantified, and the evaluation metrics used were accuracy, F1\u2013\nscore, and area under the curve (AUC). To gain a broader\nunderstanding of the classification performance, both top-1\nand top-3 accuracy were recorded. When both EEG and EMG\nsignals were combined, for the 64 ms time length, the top-\n1 accuracy, top-3 accuracy, F1-score, and AUC score were\n27.33, 54.09, 22.96, and 75.03, respectively. For the 128 ms\ntime length, the top-1 accuracy, top-3 accuracy, F1-score, and\nAUC score were 27.73, 52.35, 24.34, and 77.14, respectively.\nFor the 256 ms time length, the top-1 accuracy, top-3 accu-\nracy, F1-score, and AUC score were 29.39, 52.82, 25.30, and\n77.70, respectively. By incorporating EMG signals alongside\nEEG, we enhanced the decoding performance, compensating\nfor the limitations of noisy EEG signals. Muscle signals, such\nas facial and lip movements, strongly contributed to improving\ndecoding performance, proving that articulatory movement\ninformation has a significant impact on viseme decoding.\nAdditionally, when using only EEG signals, for the 64 ms\ntime length, the top-1 accuracy, top-3 accuracy, F1-score, and\nAUC score were 27.33, 54.09, 22.96, and 75.03, respectively.\nFor the 128 ms time length, the top-1 accuracy, top-3 accu-\nracy, F1-score, and AUC score were 27.73, 52.35, 24.34, and\n77.14, respectively. For the 256 ms time length, the top-1 ac-\ncuracy, top-3 accuracy, F1-score, and AUC score were 29.39,\n52.82, 25.30, and 77.70, respectively. Although performance\nslightly declined when using EEG signals alone without EMG\nsignals, the difference was not significant, demonstrating that\nit is still possible to decode lip shape information using only\nbrain signals. These results suggest the potential to decode\nvisemes not only from overt EEG but also from mimed or\nimagined EEG, showing promise for applications such as\nneuroprosthesis for speech impairments and the development\nof new forms of neural communication [10]."}, {"title": "B. Visual Interaction from Neural Signals", "content": "We analyzed the potential to capture and reconstruct\nlip movements within short time intervals during natural\nsentence-level speech. As shown in Table I, we compared\nperformance by mapping phonemes to visemes and adjusting\nthe time length of the corresponding segments in sentence\ndata. While using only EEG signals, the top-1 accuracy\nwas highest for segments with the longest time length of\n256 ms, but overall, shorter time lengths showed similar\nperformance levels without significant degradation. Although\ndecoding performance slightly decreased for concise data\nunder 64 ms due to the limited information available, we\nobserved fairly high viseme decoding results for moderately\nshort time lengths. This demonstrated the ability to capture\nrapidly changing lip movements and proved the potential for\nefficiently reconstructing visual speech intentions, serving as\na substitute for more granular decoding approaches that use a\nlarger number of classes, such as phonemes.\nAs shown in Fig. 2, the discretely derived viseme results\nwere represented as a one-dimensional array. Inference for\n50 sentences in the test set was conducted using a pre-\ntrained LSTM model to match the target sentence. As a\nresult, the variety of visemes decoded in short time intervals\ncould be reconstructed into the original sentences, and all\nwere accurately inferred within the pre-defined sentence set.\nThis demonstrated the potential to overcome the limitations\nof decoding precise and detailed speech intentions from neural\nsignals. Ultimately, expanding through the convergence of CV\ntechnologies could pave the way toward more universal and\ndynamic neural communication."}, {"title": "IV. CONCLUSION", "content": "We proposed viseme decoding framework that enables vi-\nsual neural communication. In speech attempts involving long\nsentences, similar to those in real conversations, phonemes\nwere segmented and labelled with condensed visemes to\nenhance decoding efficiency. Each segment was trained us-\ning short time intervals within the proposed diffusion-based\nmodel, and the feasibility of the decoding process was demon-\nstrated. Our viseme decoding-based method enabled the re-\nconstruction of pre-defined sentences. Moreover, by converg-\ning CV technologies with BCI, this approach demonstrated the\npotential to reconstruct dynamic visual outputs based on lip\nshape decoding, enabling lively and engaging communication.\nThe results of this study offer promising potential to advance\nneural communication beyond fragmented and simple forms,\nmoving towards more continuous, realistic, and intuitive in-\nteractions. To further build on these advancements, future\nworks could focus on reconstructing decoded visemes into\ncontinuous images, synchronized with real voice, and lip-\nsynced to generate realistic talking faces or avatars."}]}