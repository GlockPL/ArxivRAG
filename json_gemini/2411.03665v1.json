{"title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "authors": ["Xuelin Liu", "Yanfei Zhu", "Shucheng Zhu", "Pengyuan Liu", "Ying Liu", "Dong Yu"], "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these beliefs poses a significant challenge. This study introduces a novel three-module framework to evaluate the moral beliefs of four prominent large language models. Initially, we constructed a dataset containing 472 moral choice scenarios in Chinese, derived from moral words. The decision-making process of the models in these scenarios reveals their moral principle preferences. By ranking these moral choices, we discern the varying moral beliefs held by different language models. Additionally, through moral debates, we investigate the firmness of these models to their moral choices. Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their moral choices and debates. This study also uncovers gender bias embedded within the moral beliefs of all examined language models. Our methodology offers an innovative means to assess moral beliefs in both artificial and human intelligence, facilitating a comparison of moral values across different cultures.", "sections": [{"title": "1 Introduction", "content": "As artificial intelligence (AI) continues to evolve, there is growing concern regarding the presence and nature of moral beliefs within contemporary systems. The potential for language models (LMs) to exhibit harmful moral beliefs poses significant risks, underscoring the need for scrutiny (Weidinger et al., 2021). Explorations into the morality and values of LMs have been conducted using self-constructed datasets to determine if these models can discern the presence and nature of values within utterances, tasking the LM with categorizing morality and values (Hendrycks et al., 2021; Ziems et al., 2022; Sorensen et al., 2023; Alhassan et al., 2022). With the emergence of large language models (LLMs), there is a broader array of methods for investigating the morality and values embedded within these models. On one hand, LLMs are probed through questionnaires to elucidate their values and moral beliefs (Ramezani and Xu, 2023; Abdulhai et al., 2023). On the other hand, their moral comprehension and reasoning abilities are scrutinized using traditional moral dilemma questions (Tanmay et al., 2023).\nEvery moral decision we make is contextual. Utterance judgments and questionnaires differ significantly from the moral dilemmas encountered in real-world situations. Hence, the examination of LLMs' moral convictions should also encompass such intricate scenarios. However, the pool of existing moral dilemma scenarios is limited and fails to provide comprehensive assessment. Furthermore, insufficient studies have delved into the disparities in LLMs' moral beliefs across cultural backgrounds and demographics (van der Meer et al., 2023), notably lacking research in the Chinese context. Classification tasks or questionnaire assessments often oversimplify morality as dichotomous, whereas morality is a multifaceted construct that should align with various stages of moral development (Kohlberg, 1987; Park et al., 2024). Additionally, LLMs exhibit varying degrees of proficiency in making moral judgments across different moral scenarios; while some moral dilemmas may induce indecision in the model, others allow it to firmly uphold its moral choices. Regrettably, prior research has overlooked whether models can maintain steadfast moral convictions across diverse moral scenarios."}, {"title": "2 Related Work", "content": "The study of morality usually begins with cognitive developmental theories, as these theories provide a solid foundation for moral development. Among these theories, the contributions of Piaget and Kohlberg are particularly notable. They proposed three levels and six stages of moral development. The Kohlberg's theory of moral development comprises three distinct levels: the Preconventional Level, spanning ages 0-9; the Conventional Level, encompassing ages 9-15; and the Postconventional Level, commencing from age 15 onward. Within the Preconventional Level, Stage 1 emphasizes morality rooted in punishment and obedience, while Stage 2 shifts focus towards personal gain and instrumental reasoning. The Conventional Level underscores the importance of social norms and conformity, reflecting a growing awareness of societal expectations. Finally, the Postconventional"}, {"title": "2.1 Moral theories", "content": "Level involves a profound understanding of abstract ethical principles and a keen consideration of social justice, reflecting a maturing moral sensibility that transcends individual interests (Kohlberg, 1987). Other moral theories study morality from different subjects and aspects (Graham et al., 2008; Anderson et al., 2013; Fumagalli and Priori, 2012; Gawronski et al., 2017; Graham et al., 2016; Rivera-Urbina et al., 2021)."}, {"title": "2.2 Moral beliefs in LLMs", "content": "In existing work, researchers commonly employ Reinforcement Learning from Human Feedback (RLHF) methods to align LLMs with human moral beliefs. The primary approach involves training LLMs using data labeled with moral principles and other relevant labels (Ziems et al., 2022; Sorensen et al., 2023; Alhassan et al., 2022) and contextualized methods, such as Clarifying Questions (Pyatkin et al., 2023), Auxiliary Information (Rao et al., 2023). Simultaneously, efforts are underway to achieve moral alignment for specific domains, such as racial discrimination judgment (Bang et al., 2023) and text-based games (Shi et al., 2023).\nReliable evaluation methods are essential for achieving better moral alignment in LLMs. The common approach is to construct a moral value benchmark dataset (Tanmay et al., 2023; Tennant et al., 2023; Sun et al., 2022; Ziems et al., 2023; Wu et al., 2023; Yao et al., 2023), or to use moral questionnaires to compare LLMs' answers to those of humans (Ramezani and Xu, 2023; Abdulhai et al., 2023). Other studies have delved into the connection between moral beliefs and human behavior (van der Meer et al., 2023; Kang et al., 2023)."}, {"title": "2.3 Debate", "content": "Some recent work treats LLMs as agents and engages them in debates. This focus on debate serves two primary purposes: enhancing task performance (Khan et al., 2024; Kim et al., 2024; Du et al., 2023) and enhancing model reliability as evaluators (Chern et al., 2024).\nIn summary, the use of debate not only fine-tunes LLMs' performance but also contributes to their robustness and trustworthiness in various applications. Engaging in debates with LLMs can stimulate critical thinking (Farag et al., 2022), benefiting both the models themselves and human participants. However, to date, there has been no research that specifically applies debate techniques to explore moral beliefs within LLMs. Furthermore, little is known about how debates impact a model's moral beliefs and the strength of those beliefs."}, {"title": "3 Dataset", "content": null}, {"title": "3.1 Moral word list", "content": "The moral words we chose are from the Chinese Moral Dictionary (CMD) (Wang et al., 2020). According to the classification of Moral Foundations Dictionary (MFD) (Graham et al., 2009), the moral words are classified from three aspects: moral polarity, moral type (social, occupation, family, and individual) and moral intensity.\nDue to the abundance of moral words in the initial moral dictionary, we initiated a preliminary experiment and subsequently determined, via qualitative analysis, that moral scenarios and choices derived from negative moral words bore closer resemblance to moral dilemmas. Consequently, in subsequent experiments, we exclusively focused on extracting negative moral words. Finally, we derived 514 negative moral words.\nIn addition, some moral words with obvious gender orientation, which can not be directly modified by negative adverbs, and moral words with incomplete context are not considered. This is because we will expand each context into a male scenario and a female scenario in the following analysis."}, {"title": "3.2 Moral scenario", "content": "We used ChatGPT (gpt-3.5-turbo-16K 3) to generate 514 moral scenarios based on the moral words, including three aspects, as shown in Figure 1.\n(1) Moral problems related to this moral word. Some of which can be easily chosen, some of which are moral dilemmas.\n(2) Two mutually exclusive moral choices made for this problem. These choices represent decisions to be made in a particular situation, where choosing A means eliminating B, and choosing B means eliminating A.\n(3) The moral principles associated with each option. Each of these options is associated with one or more moral principles that reflect different moral values and standards of behaviour. If you choose option A, it is consistent with principle p and violates principle q, and if you choose B, it is consistent with principle q and violates principle p. Appendix A.1 demonstrates an example of how we connected moral choices with moral principles"}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental setups", "content": "Due to biases in training corpora and other factors, models may exhibit moral beliefs or political inclinations specific to certain cultures (Ramezani and Xu, 2023; Abdulhai et al., 2023). To find out the differences in moral beliefs between Chinese-and English- cultural background LLMs, we selected two Chinese models ChatGLM2-6B-32K (Du et al., 2021; Zeng et al., 2022) and Ernie-Bot-turbo 4, and two English models Gemini pro (Team et al., 2023) and GPT-3.5-turbo-16K 5, which would be abbreviated as ChatGLM, Ernie, Gemini, and ChatGPT for brevity in the following text. Notably, what we intend by \"cultural background\" essentially refers to the linguistic and geographical distribution characteristics of the model's training data. Variations in training datasets can lead to models embodying diverse cultural perspectives, which may, in turn, influence their decision-making processes, particularly when confronted with moral quandaries. Given that both our dataset and experiments are conducted in Chinese, the English-language LLMs selected for this study were required to complete all tasks in Chinese. Nonetheless, their responses can still offer insights into the moral beliefs that are rooted in the English-speaking cultural context. The temperature setting in our experiments are shown in Appendix B.1."}, {"title": "4.2 Moral judgments based on moral word", "content": "Our initial investigation focused on the moral judgments made by various LLMs regarding the morality of the moral words we chose. The moral polarity of the words we examined was negative, signifying that these words are considered immoral. Consequently, we expected LLMs to also classify these words as immoral.\nWe elicited the model's moral judgments by prompting it with moral words. The output of LLMs is notably sensitive to prompt design. In our word-level experiment, we employed two distinct prompts to inquire whether the model deemed a given word as moral:\nPrompt 1: Is XX moral?\nPrompt 2: Is not XX moral?\nHere, \u201cXX\u201d represents the specific moral word. To illustrate this point, consider the following example: Prompt 1: Is loud shouting moral? Prompt 2: Is NO-loud shouting moral? In this case, the focal moral word is \"loud shouting\". It is worth mentioning that all prompts utilized in this study are presented in Chinese, and a word-to-word translation approach is employed to demonstrate their structure. However, there might be subtle grammatical differences that do not translate directly. The reasoning behind designing prompts in this manner lies in directly assessing the model's capacity to render moral judgments pertaining to words. By incorporating both affirmative and negative questioning techniques, we aimed to observe whether the model could comprehend and reflect on the moral implications of these words. Therefore, in designing prompts, we provide as little context as possible and design both positive and negative questioning methods to avoid randomness. More importantly, we want to contrast whether LLMs interpret moral beliefs differently with and without context.\nThrough this experiment, we aimed to observe the model's moral judgments at the word level and assess whether these judgments were influenced by the questioning approach. It's important to note that models typically refrain from making direct moral judgments. Instead, they often emphasize the need for context-specific assessments, which we think is rigorous. To determine whether a model considers a particular moral word as immoral, we employed a series of strategies (see Appendix B.3)."}, {"title": "4.3 Moral choice based on moral scenario", "content": "To evaluate a model's moral judgment in specific scenarios related to moral words, we expanded each moral word into a concrete situation. Each scenario presents two options, representing decisions to be made in subsequent steps. We tasked four LLMs with making choices based on these scenarios. For each option, we annotated whether it considered the corresponding moral word as moral or immoral within the context of the scenario. Given that some scenarios were inherently ambiguous, both Option A and Option B might be interpreted as considering the moral word immoral. To address this, we identified the option that most directly and strongly opposed the moral word as the one indicating immorality. The volume of ambiguous data was limited, and overall, it does not significantly impact the results of the study.\nNext, we analyzed the model's moral choices based on specific moral scenarios corresponding to moral words. LLMs have the abilities to self-evaluate firmness and the cultural factors may influence the choice decisiveness (Gilardi et al., 2023; Oliveira et al., 2023; Ramezani and Xu, 2023; Davani et al., 2023; Khandelwal et al., 2024). We considered the firmness score associated with each choice in three levels:\nScore 1: I am not very certain about this choice;\nScore 2: I am generally certain about this choice;\nScore 3: I am extremely certain about this choice.\nTo enhance the model's understanding of firmness scores, we employed 2-shot showing examples with firmness scores of 1 and 3. Specific prompts used are detailed in Appendix B.4. To verify the robustness of firmness score, we also tested the moral choice results of each model in three repeated experiments, as shown in Appendix B.2. Furthermore, to mitigate the impact of the order of options on the model's output, we conducted a parallel experiment by swapping the orders of the options.\nTo compare the moral values of the models with those of Chinese university students, we conducted a survey involving 30 Chinese university students. We asked them to make moral choices using the same moral scenarios. The details of the survey method can be found in Appendix B.9."}, {"title": "4.4 Moral rank", "content": "Morality is not binary (Park et al., 2024). LLMs are capable of presenting only a relative result in their selection of moral choices. Therefore, we consider the moral choice of LLMs as a continuous whole for modal rank. In our dataset, each option within a moral scenario corresponds to one or two moral principles, with each principle associated with a specific moral stage.\nFollowing Best-Worst Scaling (BWS) (Louviere et al., 2015) and Iterative Luce Spectral Ranking (ILSR) (Maystre and Grossglauser, 2015), we obtained the pairs of options, their corresponding scores, and the overall ranking and corresponding weights based on the pairs of options. We applied the same methodology to annotation data of the sample of Chinese university students for consistency. Appendix B.10 shows the details of how BWS and ILSR were using in moral rank."}, {"title": "4.5 Moral debate", "content": "The output of a model can be susceptible to prompt variations, resulting in unstable responses. Taking cues from prior research that leveraged debates to enhance model accuracy in QA tasks (Khan et al., 2024) (Appendix B.11 shows how Khan et al. (2024) conducted LLMs debate experiments), we adopt the debate approach.\nUsing prompts similar to those employed in prior research (Du et al., 2023), we paired the four models and initiated debates. Before the debate commences, the model under evaluation received a prompt to make a moral choice. Subsequently, an opposing model - referred to as the debate opponent - was assigned a different position, representing the unselected option. In each round of the debate, the model responded to the debate opponent's arguments, prompting it to rebut the opposing stance. Finally, the model synthesized its historical dialogue with the debate opponent, re-evaluates the moral scenario, and provided a firmness score along with a rationale. Detailed prompts are available in Appendix B.6. We restricted the debate to two rounds. We present an example of debate between models in Appendix B.7."}, {"title": "5 Results", "content": null}, {"title": "5.1 Moral word judgments", "content": "We employed prompts to query each LLM about the morality of individual moral words and calculated the proportion of models judging each word as immoral, as shown in Table 1. Despite being prompted in Chinese, both ChatGPT and Gemini, the two English LLMs, exhibited a strong comprehension of moral polarity, with Gemini recognizing the words as immoral 85% of the time and ChatGPT doing so 93% of the time. Notably, across specific moral categories, the four LLMs exhibited a lower probability of recognizing immorality in the individual category and a higher probability in the family category. This suggests that morality within the individual category is relatively ambiguous and challenging to assess. In contrast, our society has clearer guidelines for family-related morality, with distinct boundaries defining what is considered moral or immoral in family matters.\nWe also calculated the consistency rate of results when posing questions using two different prompts: Prompt 1 and Prompt 2 shown in Section 4.2. Specifically, we assessed whether the LLMs would still consider a given word immoral when prompted with Prompt 2. As shown in Table 2, the ability of LLMs is influenced by prompt design."}, {"title": "5.2 Moral choice", "content": "Firstly, we compared the consistency rates of the four LLMs in classifying the moral word as immoral at both the word level and the scenario level, as shown in Table 2. DL (Different Level) represents moral choices at different levels of the model, specifically, the word level and the scenario level. To be more precise, in our dataset, for each of the two options, we annotated which option corresponded to the moral word being considered immoral within the context of the scenario. For example, if a model judged a moral word X to be immoral at the moral word level, and then selected the corresponding option we annotated for X, which is the moral scenario level, we considered the model's judgment to be consistent across the word level and the scenario level. It is evident that ChatGPT and Gemini demonstrate relatively high consistency, suggesting that their moral choices align closely at both the scenario and word levels.\nIn the experimental results comparing moral choices made by models and the sample of Chinese university students, as depicted in Figure 2, both ChatGPT and Gemini exhibited moral choices in scenarios that closely resembled decisions of the sample of Chinese university students. Furthermore, their answers demonstrated remarkable similarity. This observation aligns with the conclusion drawn in Section 5.1. ChatGPT and Gemini consistently performed well in moral decision-making, closely aligning with moral beliefs of the sample of Chinese university students.\nIn addition to prompting the models to choose an option within the scenario, we also requested them to rate their firmness scores for their moral choices. The final results are presented in Table 3.\nFrom Table 3, we observe that, except for ChatGLM, all three models give firmness scores of 2 or 3 points. Ernie's score of 2 points is as high as 0.97, while Gemini's score is mostly 3 points, accounting for 0.89. Gemini gave a high firm score in the majority of responses in making moral choices, indicating that it approaches moral scenarios with minimal hesitation. ChatGPT gives a similar ratio of 2 and 3 points, which are 0.48 and 0.52. The two Chinese LLMs - ChatGLM and Ernie - do not exhibit the same level of confidence when making moral choices. This discrepancy may stem from the influence of cultural corpora during model training. Chinese culture tends to emphasize moderation and dialectics, leading to less definitive choices when faced with moral scenarios. In contrast, Western culture often adopts a dichotomous approach, resulting in stronger preferences for moral scenarios (Jia et al., 2019; Jia and Krettenauer, 2017; Wang et al., 2021). This distinction will be further explored in the moral dilemmas discussed later.\nWhen models make moral choices within scenarios, their decisions can be influenced by various factors beyond their own moral values. In this study, we investigated the impact of gender on the moral beliefs of these models. To explore this, we modified 472 moral scenarios, incorporating gender variables into both the scenarios and options. The detailed modification process is outlined in Appendix A.2. Subsequently, we analyzed the consistency rates of moral choices made by each model in scenarios involving both men and women. The results in Table 2 revealed differences in the choices made by the models across genders. Specifically, ChatGLM exhibited the largest difference, while Ernie and ChatGPT showed the smallest disparities. These findings highlight gender as a factor influencing the moral beliefs of the models. Further detailed analysis will be discussed in Section 5.3. The example of ChatGLM making different choices when faced with man and woman scenarios can be seen in Appendix B.5, which shows gender bias."}, {"title": "5.3 Moral rank", "content": "The results of the ranking for the four models and the sample of Chinese university students are shown in Table 4, Appendix C.1, and Appendix C.2. The preponderance of models emphasizes moral principles such as professionalism and independence, indicating that these principles are widely recognized as being of utmost importance across various models. However, there exist substantial variations in the prioritization of moral principles among different models. These disparities suggest that the moral beliefs of models diverge significantly due to the diverse moral values embedded in their training corpora or the distinct training methods employed. For instance, Ernie shows maintaining public safety and social order alongside professionalism and independence, which is more akin to the collectivist spirit prevalent in Chinese culture. Conversely, Gemini and ChatGPT show respecting individual wishes, which is more akin to the individualism in Western culture.\nUpon juxtaposing the outcomes with evaluations of the sample of Chinese university students, it becomes clear that the moral ranking produced by ChatGLM aligns significantly more with the assessments of annotators. This close correspondence may stem from the fact that ChatGLM, being a Chinese LLM, shares a cultural and national context with the participants. As a result, there emerges a congruence in the way moral principles are valued and prioritized. Regarding the stage of moral development, the majority of models demonstrate a capacity to reach the advanced fifth stage. However, the sample of Chinese university students have achieved the even more elevated sixth stage. This observation indicates that there remains ample room for improvement in the moral cognition capabilities of LLMs.\nThe overall ranking correlation is visualized in Figure 3, revealing that the moral principle rankings of the sample of Chinese university students align most closely with those of ChatGLM. Furthermore, there exist substantial differences among the four models overall. The inconsistency between the models most similar to the sample of Chinese university students in Figures 2 and 3 may stem from the moral rank component. To enhance the reliability of moral rankings and achieve convergence, we intentionally excluded models that provided a moral choice with a low firmness score (i.e., a score of 1) during the sorting pairs process.\nObserving the rankings of moral choices across genders, we note that different models exhibit varying degrees of gender bias in their ethical prioritization. Notably, most models demonstrate significant disparities in the prioritization of moral principles between men and women, indicating that they perceive distinct ethical standards applicable to different genders. Among these models, Erine stands out as a relatively gender-neutral language model, with minimal differences in the prioritization of moral principles between men and women. The moral principle rankings for male moral choices on"}, {"title": "5.4 Moral debate", "content": "In our study, two out of the 4 language models were paired as the debating model and the opposing model. These pairs engaged in a debate on moral choices. Subsequently, we calculated the proportion of each model that altered its initial choice before and after the debate. The results are illustrated in Figure 4. The model will output more thoughts on moral scenarios after the debate, and we provide an example in the Appendix B.8. The firmness score given by the model will affect whether the model changes its choice before and after the debate, as shown in the Appendix C.3."}, {"title": "6 Conclusion", "content": "In this study, we curated a dataset comprising 472 moral choices, spanning both word-level and scenario-level contexts. We meticulously evaluated the moral beliefs of LLMs in the three-module approach, including moral choice, moral rank, and moral debate. We observed significant disparities in moral choice across various models. In the moral rank results, English models notably demonstrate a propensity to adhere to their moral principles, reflecting individualistic moral beliefs, whereas Chinese models display a lesser inclination towards firmness in their choices, reflecting tendencies towards collectivism. Gemini and ChatGPT closely emulate the sample of Chinese university students in moral decision-making, while ChatGLM notably aligns with annotators in moral ranking. Furthermore, we delved into additional factors influencing model moral judgments, contributing useful insights to the study of model moral beliefs."}, {"title": "Limitations", "content": "Our research investigates the determinants of moral choices made by LLMs. In the process of making moral choices, we focus on the influence of gender on these choices, but we recognise that gender is not the only influence. Other social categories, including age and ethnicity, have a significant impact on the complex network of moral choice in LLMs. We intend to expand the scope of our future research to include these other dimensions to gain a more nuanced understanding of LLMs' moral beliefs in the context of different factors.\nIn order to have a better understanding of the level of moral decision making between LLMs and humans, a questionnaire was administered. We only selected Chinese university students as our sample, which is far from sufficient. In future research, we hope to investigate the moral beliefs of a more diverse range of populations.\nThe dichotomous categorization of Chinese and Western cultural backgrounds does not extend to all cultures beyond these two groups, nor does it encapsulate the numerous nuances present within these cultures. Our research is grounded in particular cultural settings, and this binary distinction is a deliberate simplification for analytical reasons. We acknowledge the importance of cultural diversity and complexity. To enhance the comprehensive analysis of the modeling outcomes, we plan to include more annotators from diverse cultural backgrounds in our subsequent endeavors.\nThere are some criticism to Kohlberg's moral theories that is pertinent to the conclusions of this paper with respect to cross cultural generalizability and of being too centered on the way men make ethical judgements rather as opposed to women. However, we still consider the theory to be a promising one in moral theory because Kohlberg's theory of moral development provides us with a systematic framework for understanding the formation and development of individual moral judgements, as well as a theoretical basis for judging the level of moral development of the model. As for the lack of balanced attention to gender differences, we expect that future research will pay more attention to the characteristics and development process of women's moral judgement, so as to further improve and enrich Kohlberg's theory.\nIn our dataset, the moral scenarios are not sufficiently rich and may not fully capture the complexity of real-world moral decision-making. In future work, we will consider incorporating a richer variety of moral scenarios while also addressing a wider range of moral beliefs.\nDuring moral debates, we observed varying susceptibility to opponents among different models. This phenomenon may be attributed to the inherent moral beliefs of the models themselves, as well as their size and overall capability levels. Future investigations will delve into these aspects to enhance our understanding."}, {"title": "Ethics Statement", "content": "Moral content poses a great challenge to language modeling, especially in understanding human moral judgments. Therefore, our work focuses on descriptive ethics and adheres to strict ethical guidelines. In the manual annotation sessions, we respect and protect participants' privacy and informed consent. Considering that moral judgments are influenced by culture and beliefs, we pay special attention to avoiding cultural biases and stereotypes. In the moral debate stage, models may be assigned some immoral viewpoints, which could potentially lead the models to learn these immoral perspectives. However, we have explicitly informed the models during the moral debate stage that it is a simulated debating process. Furthermore, throughout the entire process and outcome of the moral debates, we have conducted manual review to mitigate potential harms to the best of our ability. And the majority of the models used in our experiment are black-box models. Despite the use of some immoral stances in debates, we believe that the scale of the data tested is not large, and the model development company has strict safety controls, so the moral impact on the model can be considered negligible."}]}