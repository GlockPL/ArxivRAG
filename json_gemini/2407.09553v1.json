{"title": "RESVMUNetX: A Low-Light Enhancement Network Based on VMamba", "authors": ["Shuang Wang", "Qingchuan Tao", "Zhenming Tang"], "abstract": "Image enhancement under low-light conditions is a continuously focused area in deep learning research. Current methods, mostly based on deep learning frameworks, utilize convolutional neural networks to extract deep features of images, but these methods fall short in capturing long-range information of images. Deep learning algorithms like Retinexformer attempt to improve the acquisition of global information by incorporating Transformer structures, yet there is room for improvement in terms of enhancement effects, real-time processing capabilities, and algorithm interpretability. In response to the issues present in existing algorithms, this study introduces a novel image enhancement network-Res VMUNetX, which adopts the method of error regression, directly enhancing the brightness and recovering structural details of input images by adding estimated illumination supplements. ResVMUNetX optimizes the capture of long-range information through an efficient VMamba architecture. Specifically, the network utilizes an improved VMUNet as the backbone for feature extraction, performs error estimation on low-light inputs, and achieves preliminary enhancement by directly adding pixels to the input. Subsequently, the low-light input is processed for local detail extraction and noise removal using a CNN-based and specially designed Denoise CNN module. This method effectively enhances the clarity and visual quality of low-light images, significantly reducing the parameter and computational load while also markedly improving the algorithm's computational speed through optimization with state-space models. Evaluation results on the LOL dataset indicate that ResVMUNetX surpasses existing image enhancement methods in both image quality and performance. More importantly, ResVMUNetX achieves a processing speed of up to 70 frames per second, not only confirming its exceptional efficacy in image enhancement but also demonstrating its potential for real-time processing in practical application scenarios.", "sections": [{"title": "1 Introduction", "content": "Low-light image enhancement has always been a highly challenging task in the field of computer vision and image processing. Images captured under low illumination conditions often exhibit dim and blurry visual effects with significant detail loss, which greatly impacts the execution of subsequent image processing tasks such as object detection and image segmentation. In practical detection scenarios, low-light conditions are often inevitable, making the effective enhancement of these low-light images a key issue in image processing research.\nTraditional low-light image enhancement techniques, such as histogram equalization and gamma correction, typically employ a strategy that overlooks the illumination component, directly manipulating the pixel values of the input image. This approach often fails to maintain the quality and naturalness of the image after enhancement when dealing with scenes of complex and variable lighting. With the deepening study of the human visual system, the introduction of Retinex theory, based on real-world physical phenomena, decomposes the image into illumination and reflection components, providing a solid theoretical foundation and prior knowledge for low-light image enhancement. Based on Retinex theory, many deep learning algorithms, such as RetinexNet, have shown certain enhancement effects. However, images captured under low-light conditions often come with a significant amount of noise and color distortion, which, when enhanced using Retinex theory, may lead to the amplification of noise, thereby degrading image quality. Given the aforementioned issues, the performance of algorithms based on Retinex theory has yet to achieve the desired enhancement effects.\nIn the realm of image processing powered by deep learning, the error regression method has proven its significant efficacy in enhancing image quality. This method, by accurately predicting and correcting distortions in images, has demonstrated outstanding performance in traditional image processing tasks. With the advancement of deep learning technologies, the concept of error regression has also been applied within complex neural network architectures, enabling these models to learn more intricate and detailed characteristics of image distortions. The integration of error regression methods with deep learning, through direct prediction and processing of errors, has also opened up new avenues for low-light image processing.\nConvolutional Neural Networks (CNN) models are widely used for image enhancement in low-light environments due to their outstanding ability to capture local features. CNNs can effectively recognize and learn spatial information in low-light images, demonstrating their capability in enhancing low-light images. However, CNNs have limitations in capturing and learning long-range information of images, leading to processed images often lacking global and contextual information, manifesting as difficulties in effectively addressing noise issues and detail loss in low-light images. To mitigate the impact of the lack of long-range information, researchers have turned their attention to Transformer models, which, through self-attention mechanisms, achieve perception of global information and modeling of long-range information, thus aiding in the recovery of detailed structures in low-light images. This not only compensates for the inadequacies of CNNs in processing global information but also achieves significant success in the field of image enhancement. However, algorithms based on Transformer architecture often come with high computational complexity and large model parameters, leading to substantial consumption of computational resources and difficulty in meeting the needs for real-time processing. This significantly limits the practical application scenarios of the algorithms.\nIn response to the issues and deficiencies present in the aforementioned algorithms, this study introduces an image enhancement algorithm based on error regression-ResVMUNetX. The training of ResVMUNetX is divided into two main phases. In the first phase, the core feature extraction network VMUNetX, based on the VMamba architecture, is utilized to deeply learn the global characteristics of images and accurately estimate the error components between low-light inputs and reference targets. By superimposing the estimated error components onto the original low-light inputs at the pixel level, the distortion between the low-light inputs and target references is compensated, achieving an initial image enhancement effect. Considering the significant amount of noise present in the original low-light inputs, the VMUNetX portion is frozen in the second phase, and the designed DenoiseCNN is used to denoise the low-light inputs. The inputs processed by DenoiseCNN and the error components output by the VMUNetX part are then added at the pixel level to achieve the final enhancement. This straightforward and efficient processing strategy significantly enhances the overall brightness and contrast of the images while effectively removing the noise in the original low-light images. Experimental results confirm the significant improvement in visual effects offered by this method, providing a practical and efficient image enhancement solution for real-time or computationally constrained application environments.\nTo evaluate the performance of ResVMUNetX in enhancing low-light images, we conducted extensive quantitative and qualitative tests on the widely used low-light dataset LOL. According to the data presented in Table 1, ResVMUNetX outperforms most of the current deep learning methods' state-of-the-art (SOTA) on this dataset. Furthermore, the test results in Table 2 indicate that ResVMUNetX has also achieved significant improvements in inference speed, greatly surpassing previous SOTA models and offering real-time processing capabilities. The main contributions of our research can be summarized as follows:\n1.In the field of low-light image enhancement, we introduce the ResVMUNetX network for low-light enhancement, based on the Mamba architecture and inspired by the concept of error regression.\n2.To reduce the noise in the input low-light images, we designed a specialized DenoiseCNN denoising module to process them, and its effectiveness has been verified.\n3.Through extensive experiments and data analysis, our method not only surpasses many high-performance deep learning algorithms in terms of quality but also achieves a significant lead in inference speed."}, {"title": "2 RELATED WORKS", "content": "2.1 Low-light Image Enhancement\nDistribution Mapping Model: In the early stages of exploring the domain of low-light image enhancement, researchers proposed the most intuitive method of distribution mapping. This approach amplifies the dark regions with lower values by mapping the distribution characteristics of low-light images. Classic techniques of this method include histogram equalization and gamma correction based on the S-curve. Although these techniques significantly enhance image visibility, they expand the pixel distribution without involving the semantic information of the image, leading to common issues such as color distortion and other artifacts that degrade the visual effects.\nTraditional Model Approach: Retinex theory provides an intuitive physical framework for image enhancement, positing that eliminating the illumination component of an image can restore the original reflection map, i.e., the image under normal brightness. As this theory has been further applied, a key point that has emerged is how to accurately estimate the illumination layer of an image. Methods based on this model often involve the introduction of manually set prior knowledge and require fine parameter tuning, which may lead to the enhanced image exhibiting unnatural artifacts and color biases, thereby exposing its insufficient generalization performance and the cumbersome optimization process. Moreover, these traditional methods often overlook the noise in images, potentially leading to the amplification of noise during the enhancement process, which in turn affects the final enhancement effect.\nDeep Learning Methods:Since 2017, deep learning methods have led the technological trend in low-light enhancement. These approaches are often based on the classical Retinex theory as a structural framework, yielding a series of innovative works. For instance, Wei and other researchers have successfully combined Retinex's decomposition theory with deep learning techniques, overcoming the limitations of CNN-based methods in capturing long-distance dependencies between different regions of an image. Retinexformer further introduced the transformer architecture, which, through its self-attention mechanism, addressed the issue of long-distance dependencies and combined it with a transformer design based on Retinex theory, allowing the method to be refined and optimized. However, due to its self-attention mechanism, the transformer model still faces significant computational challenges and complexities when processing long sequence data.\n2.2 State Space models\nRecently, State Space Models (SSMs) have gradually become a promising new direction within the research domain. As an innovative alternative architecture to existing CNNs and Transformers models, one representative model-S4 (Structured State Space Sequence model)-has been proposed in numerous literatures, mainly for modeling long-range information in images. Further exploration in this field has fostered the development of diverse structured state space models, featuring complex diagonal mechanisms, multi-input multi-output capabilities, diagonal decomposition, and low-rank operations, thereby significantly enhancing their feature extraction capabilities. Notably, modern SSMs like Mamba not only successfully establish dependencies of long-range information but also exhibit linear processing complexity relative to input size, making their computational efficiency superior to that of transformers, thus attracting widespread attention across various research fields. The selective scanning mechanism proposed by the Mamba model is comparable to popular base models in the vision domain, and Vision Mamba further advocates that pure SSM models can serve as a universal visual foundation architecture. This view has been validated through empirical research, showing significant progress in medical image segmentation tasks. Additionally, research in low-level vision tasks has also demonstrated positive outcomes. Inspired and encouraged by these research advancements, our work leverages the Mamba model's advantages in linear analysis of long-distance sequences and combines it with error regression theory for feature processing, achieving excellent low-light image enhancement effects. This further highlights the potential efficacy and practical application value of the Mamba model in the domain of low-light image enhancement."}, {"title": "3 Method", "content": "Based on the description in Figure 1, this section provides a concise analysis of the VMUNetX architecture. The network comprises two main components: VMUNetX, responsible for modeling long-range information, and DenoiseCNN, utilizing a classic CNN architecture for denoising. The training process of ResVMUNetX is divided into two main phases. In the first phase, the core feature extraction network VMUNetX, based on the VMamba architecture, is employed for deep learning of the global features of images. This phase primarily involves an accurate estimation of the error components between low-light inputs and reference targets, and superimposing the estimated error components onto the original low-light inputs at the pixel level. This effectively compensates for the distortion between the low-light inputs and target references, thereby achieving an initial image enhancement effect. In the second phase, considering the significant amount of noise present in the original low-light inputs, the VMUNetX part is frozen, and a specially designed DenoiseCNN is introduced to perform denoising on the low-light inputs. By adding the inputs processed by DenoiseCNN to the error components output by VMUNetX at the pixel level, we successfully achieve the final enhancement of the image. In short, the training process of the algorithm can be expressed in the following form:\n\\(Step1: pred = estimaiton error + initial input\\)\n\\(Step2: preddenoise = estimaiton error + initial inputdenoise\\)\n3.1 VMUNetX\nThe recently proposed VMUNet model leverages the efficient handling of remote information by the State Space Model (SSM) within the Mamba architecture, as well as the advantage of the Variable State Space (VSS) block in finely capturing contextual information, achieving significant results in medical image segmentation tasks. The Mamba architecture employs Selective Scanning 2D (SS2D) technology for serial processing of images, which can efficiently process images while maintaining a global receptive field. However, this processing method, if focused solely on image sequences, could lead to the loss of edge details and local structural information. To overcome this limitation, we have integrated traditional Convolutional Neural Network (CNN) technology on the basis of the Mamba architecture and proposed the VMUNetX model, aimed at applications in low-light image enhancement. VMUNetX is capable of efficiently capturing long-distance information with the Mamba model and delicately learning local edge information with CNNs simultaneously.\nStructurally, VMUNetX is divided into two main parts: VMUnet and the \"X\" structure. As shown in Figure 2a, the VMUnet part mainly consists of a Patch Embedding layer, encoder-decoder, projection layer, and the classic Skip Connection. The \"X\" structure is designed with a convolutional downsampling module and a mixed upsampling module during the encoding phase, achieving effective integration of the aforementioned information during the decoding phase. In the encoder, unlike the original VMUNet which uses four layers of encoding, VMUNetX, to avoid restrictions on the input image size while considering the actual enhancement effect and computational cost, only employs two layers of encoding. Each layer performs feature extraction through VSS blocks. By stacking VSS blocks of different depths, multi-scale feature acquisition is achieved, and each VSS encoding layer concludes with a merge operation to reduce the dimensionality of features and increase the number of channels, achieving downsampling. For these two layers of VSS encoding layers, the number of VSS blocks is set to [2, 3], with channel configurations of [C, 2C]. Additionally, we designed special convolutional downsampling and mixed upsampling modules to process feature maps of different layers and integrate them with the output of the decoder. In the decoder part, corresponding to the previous VSS encoding layers, the decoded features are upsampled and the number of channels is reduced through VSS blocks, restoring the size of the feature maps. The number of VSS blocks and the channel configurations correspond to those of the encoder, set to [3, 2] and [2C, C], respectively. Finally, the feature map size obtained through the mapping layer is restored to the original input size, while Skip Connections simply achieve direct integration of feature maps of different sizes through point-wise addition.\nVSS Block: The core component of the VM-UNet architecture is the Vision State Space (VSS) block, derived from the VMamba framework. The input processed by this block first goes through Layer Normalization, and then it is split into two paths: the first path applies a linear layer and an activation function for feature transformation; while the second path sequentially passes through a linear layer, depthwise separable convolution, and an activation function, followed by entering the SS2D module for in-depth feature extraction. After the SS2D module, Layer Normalization is used to standardize the output features, which are then element-wise multiplied by the features output from the first path. The subsequent linear layer integrates these features, and finally, they are element-wise added with the residual connection to form the final output of the VSS block. In the implementation of the VSS block, the SILU function is chosen by default as the activation function.\nSS2D:The SS2D structure mainly consists of three parts: the Scanning Expansion module, the S6 Feature Extraction module, and the Scanning Merge module, as shown in Figure 2. The Scanning Expansion module sequences the input image along four different directions (from top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right), ensuring a meticulous scan of all directions of the image and aiding in capturing multi-directional features. Subsequently, the S6 module performs feature extraction on sequences obtained from these different directions. Finally, the Scanning Merge module sums and merges the feature sequences from these different directions, allowing the processed feature map to return to the original spatial dimensions of the image. The S6 block, building upon the S4 block, incorporates a selective mechanism, enhancing the model's ability to discern and filter feature information, effectively retaining valuable information while eliminating redundancy.\nResidual Mixed Upsampling: In the mixed upsampling module, as shown in Figure 3,a strategy that combines direct upsampling and error regression is adopted, where error regression utilizes a method of residual connections to effectively enhance the feature maps of low-light images. This module is capable of increasing the size of the feature map while simultaneously ensuring dense transfer of feature information and effective control of the number of channels. It features two parallel branches, one responsible for enhancing features through a convolutional network and performing upsampling, and the other directly achieving upsampling through bilinear interpolation. The outputs of the two branches are merged by addition at the end, aiming to combine information of two different characteristics, thereby enriching the expressive capability of the final upsampled feature map. This design concept not only draws on the residual learning approach from traditional deep learning to enhance the model's learning capacity and accelerate training convergence but also utilizes the direct upsampling branch to preserve more original feature information. Through the mixed strategy, the upsampling module can more effectively retain important information in the image during the feature enhancement and upsampling process, making it suitable for low-light image enhancement domains that require high detail in images.\n3.2 DenoiseCNN\nIn practical applications of enhancing low-light inputs through brightness adjustment, prior research often simply multiplies or adds the input low-light images with the obtained illumination estimation results, frequently overlooking the intrinsic noise components in low-light images. To address this issue, we have specifically designed a deep network module named DenoiseCNN, focusing on noise elimination in low-light images.\nAs illustrated in Figure 2b, DenoiseCNN initially brightens the input low-light images, a step that allows for the capture of the images' general structural information, before merging them with the original images and passing them into the preprocessing layer. During the preprocessing phase, the input images undergo a series of convolutional operations that gradually enhance the richness of feature representation, followed by multiple layers of convolutional stacking to delicately capture image details. Moreover, by integrating the Squeeze-and-Excitation (SE) module into the network architecture, DenoiseCNN achieves adaptive recalibration of channel feature responses, effectively enhancing the model's focus on image information and suppressing noise. Subsequently, in the output stage of the network, DenoiseCNN reconstructs and gradually restores the image channels. Finally, drawing inspiration from the success of the dark channel prior in image dehazing applications, the estimated images undergo dark channel processing to further improve the naturalness and structural clarity of the image estimation.\n3.3 Loss Function\nDesigning a suitable loss function can effectively guide model training and enhance its performance. In the task of low-light image enhancement, relying on a single loss function is challenging to achieve satisfactory results. Therefore, referencing the work of Lin, we designed and configured multiple loss functions during the model training phase to drive the model's learning and obtain high-quality enhanced outputs.\nSSIM Loss: For low-light image enhancement, issues such as detail blurring and structural distortion often arise, severely affecting image quality. To ensure structural consistency and obtain high-quality images, it is essential to use SSIM (Structural Similarity Index) as part of the loss function. The SSIM loss function focuses on measuring the structural similarity between images, which helps in obtaining high-quality enhancement results. The definition of the SSIM loss function is as follows:\n\\(L_{ssim} = \\frac{(2\\mu_x\\mu_y + C1)(2\\sigma_{xy} + C2)}{(\\mu_x^2 + \\mu_y^2 + C1)(\\sigma_x^2 + \\sigma_y^2 + C2)}\\)\nWhere \u03bcx and \u00b5y represent the mean values of the pixels in the estimated and reference images respectively, ox and \u03c3y represent the variance,,6xyrepresents the covariance, and C1 and C2 are constants to avoid division by zero.\nPerceptual Loss: To further enhance the visual quality of the images, we also employed perceptual loss. By computing high-level feature information of the images using the VGG network, perceptual loss can measure the discrepancy between the estimated results and the reference images in terms of contextual semantics and structural information. The definition of perceptual loss is as follows:\n\\(L_{perceptual} = \\sum_{i,j} ||\\Phi_{i,j}(I_{pred}) - \\Phi_{i,j}(I_{ref})||_1\\)\nWhere\u03a6i, j represents the feature map of the j th convolutional layer in the i th block of the VGG-16 network, and ||1 represents the L1 loss.\nInner Loss: In constructing our model, in addition to traditional loss functions, we also designed a new loss function to ensure a more accurate reflection of the error between the predicted image and the target image. Specifically, we propose an inner loss, whose main purpose is to measure the difference between the predicted image and the low-illumination target image at the pixel level. The calculation formula for the inner loss is as follows:\n\\(L_{inner} = \\frac{torch.dot (pred\\_t\\_flatten, LL\\_t\\_flatten)}{N_1 \\cdot N_2 \\cdot N_3 \\cdot N_4}\\)\nWhere torch.dot represents the dot product operation between two flattened tensors, pred_t_flatten is the flattened tensor of the model's predicted output, and LL_t_flatten is the flattened tensor of the low-illumination target image. N1, N2, N3, and N4 respectively represent the size of the low-illumination target image tensor LL_t in each dimension. This loss function considers the overall coupling degree by calculating the dot product of the two images and averaging it, thereby guiding parameter updates during the model training process to reduce the overall difference between the prediction and the target.\nHistogram Loss: Given the significant impact of the overall distribution characteristics of images on model performance, this study introduces a histogram loss function. The purpose of this function is to quantify the discrepancy in pixel value distribution between the model's predicted output and the target image. By calculating the histogram distributions of both the estimated result and the reference target, and evaluating the differences between these distributions, the generated images can be made to closely match the target image in terms of pixel value distribution. Furthermore, drawing on Retinex theory, we designed two types of histogram losses: Histogram Loss and Histogram Loss'. Histogram Loss focuses on analyzing the distribution differences between the estimated result and the reference target; while Histogram Loss' concentrates on comparing the distribution differences between the reflection maps. Specifically, the reference reflection map output is obtained by dividing the high illumination output of the reference image by its low illumination output. Similarly, the estimated reflection map output is obtained by dividing the estimated result by the low-light input, and then Histogram Loss' is calculated based on these two. This method helps to further optimize model performance, ensuring that the model output is closer to reality not only in terms of brightness but also in color and texture details. The definition of histogram loss can be referred to the following formula:\n\\(L_{histo} = \\frac{1}{2} \\sum_{i} [\\frac{H_{pred}(i)}{\\sum_{i}H_{pred}(i)} - \\frac{H_{target}(i)}{\\sum_{i}H_{target}(i)}]\\)\nWhere Hpred and Htarget represent the histogram distributions of the predicted result and the reference target, respectively.\nTotal Loss: The total loss function is set as follows:\n\\(L_{total} =W_sL_{ssim}+W_pL_{perceptual} + W_iL_{inner}+W_hL_{histo}\\)\nBased on experience, we set ws, wp, Wi, and wh to 2.0, 1.2, 1.0, 1.0, respectively.Relevant fusion experiments can be seen in section 4.3."}, {"title": "4 Experiment", "content": "4.1 Datasets and Implementation details\nLOL:To conduct a systematic and comprehensive performance evaluation of ResVMUNetX, we have chosen the widely adopted low-light image enhancement dataset-LOL dataset as the testing benchmark. This dataset is divided into two versions: V1 and V2. In the LOL dataset, each low-light image is equipped with a corresponding standard reference image. We compare the differences between the network output enhanced images and these target reference images using quantitative metrics to comprehensively evaluate the model's performance. In the LOL v1 version, the dataset uses 485 images as the training set and 15 images as the test set. The LOL v2 dataset is further divided into two subsets: LOL-v2-real and LOL-v2-synthetic. The LOL-v2-real subset is captured by adjusting ISO and exposure time in real scenes, containing 689 pairs of training images and 100 pairs of test images. The LOL-v2-synthetic subset focuses on starting from RAW images, synthesizing low-light images and normal illumination image pairs by analyzing the light distribution in low-illumination environments. This subset contains a total of 1,000 pairs of low-light/regular illumination images, selecting 900 pairs for training and 100 pairs for testing. With the above experimental setup, we aim to comprehensively verify and demonstrate the actual role and potential capabilities of RetinexVMUNet in the task of low-light environment image enhancement.\nImplementation Details:Under the PyTorch framework, we successfully implemented the ResVMUNetX model and conducted model training and testing on a Linux system (equipped with CUDA 11.7, Python 3.9, and PyTorch 1.13 environment) using an NVIDIA RTX 4090 GPU. The entire training process of the model can generally be divided into two stages. In the first stage, the VMUNetX part is trained separately, where the illumination estimation output by VMUNetX and the low-light input are directly added to form the final output result. During the training process of the first stage, the initial learning rate is set to 2e-4, decreasing to 1.2 times the original every 100 epochs, with the first stage lasting for 400 epochs. In the second stage of training, the parameters of the already trained VMUNetX part are frozen, and then DenoiseCNN is introduced to denoise the low-light input. DenoiseCNN replaces the low-light input from the first stage and the illumination estimation output by VMUNetX for addition, obtaining the final enhancement result. During the training process of the second stage, the initial learning rate is set to 2e-3, decreasing to 1.2 times the original every 100 epochs, with the second stage lasting for 200 epochs. Throughout the training process, the Adam optimizer is used to iteratively optimize the model parameters. To reduce the model's loss value, the momentum term bl of the Adam optimizer is set to 0.9, and the RMSprop control term b2 is adjusted to 0.999.\n4.2 Low-light Image Enhancement\nIn Table 1, we conducted a quantitative comparison of RetsVMUNet and other supervised learning methods on the LOL V1 and LOL V2 datasets. To ensure fairness, most of the data results in the table were obtained using open-source code under the same training and testing environment, while some test results come from the original papers of the algorithms. As shown in the table, our method comprehensively surpasses other latest algorithms (SOTA) across multiple mainstream evaluation metrics. In Table 2, we measured the real-time inference speeds of various SOTA algorithms on a unified testing platform. The test results indicate that ResVMUNetX not only possesses excellent enhancement capabilities but also significantly leads other algorithms in terms of inference speed."}, {"title": "4.3 Ablation Study", "content": "On the LOL v1 dataset, we conducted a series of ablation studies aimed at evaluating the performance of five different network architectures. These architectures were modified by removing or adding specific components to compare their respective image enhancement effects.\n1. First, we tested the performance of the unmodified original VMUNet model to assess its basic enhancement effects.\n2. We integrated the \"X\" structure on the 2-layer VMUNet, which aims to optimize the processing of encoding layer information through a multi-scale extraction approach.\n3. We also incorporated our designed DenoiseCNN into VMUNetX to enhance the noise elimination capability for low-light input images.\n4. Furthermore, we evaluated the original VMUNet with the addition of DenoiseCNN separately to explore the impact of the denoising module on enhancement effects. Synthesizing all ablation experiment results, our RetsVMUNetX model achieved the highest performance on both PSNR and SSIM metrics.\nCompared to the original VMUNET, VMUNetX with the added \"X\" structure can capture edge and detail information more finely. By introducing the denoising module, it successfully addressed the noise issues not yet removed in the original input, significantly improving image quality."}, {"title": "5 Conclusion", "content": "In this study, we introduced a low-light image enhancement framework based on the Mamba architecture-ResVMUNetX. This framework, building upon VMUNet, incorporates additional convolutional neural network (CNN) upsampling and downsampling modules, aiming to address the shortcomings of the Mamba architecture in acquiring two-dimensional image edge information. Considering the inherent noise issues in low-light images, ResVMUNetX also integrates a customized DenoiseCNN denoising module, specifically responsible for noise elimination. Drawing on the idea of error regression, the framework adds the denoised low-light input to the illumination compensation estimation, ultimately achieving effective image enhancement. Through a series of extensive quantitative and qualitative evaluations, the results confirm that ResVMUNetX surpasses existing state-of-the-art technologies on the LOL dataset. Compared to similar image enhancement algorithms, our framework not only achieves significant improvements in inference speed but also demonstrates potential suitability for real-time application scenarios. Although ResVMUNetX has achieved excellent enhancement effects, issues such as color distortion still exist. Moreover, the current algorithm can only perform real-time inference on GPU devices. Therefore, future research will focus on further improving the model's inference speed and deploying the algorithm on various edge device platforms, such as FPGAs, to expand its practicality and applicability."}]}