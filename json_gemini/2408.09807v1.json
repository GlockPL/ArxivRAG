{"title": "World Models Increase Autonomy in Reinforcement Learning", "authors": ["Zhao Yang", "Thomas M. Moerland", "Mike Preuss", "Aske Plaat", "Edward S. Hu"], "abstract": "Reinforcement learning (RL) is an appealing paradigm for training intelligent agents, enabling\npolicy acquisition from the agent's own autonomously acquired experience. However, the\ntraining process of RL is far from automatic, requiring extensive human effort to reset the\nagent and environments. To tackle the challenging reset-free setting, we first demonstrate the\nsuperiority of model-based (MB) RL methods in such setting, showing that a straightforward\nadaptation of MBRL can outperform all the prior state-of-the-art methods while requiring\nless supervision. We then identify limitations inherent to this direct extension and propose\na solution called model-based reset-free (MoReFree) agent, which further enhances the\nperformance. MoReFree adapts two key mechanisms, exploration and policy learning, to\nhandle reset-free tasks by prioritizing task-relevant states. It exhibits superior data-efficiency\nacross various reset-free tasks without access to environmental reward or demonstrations\nwhile significantly outperforming privileged baselines that require supervision. Our findings\nsuggest model-based methods hold significant promise for reducing human effort in RL.", "sections": [{"title": "Introduction", "content": "Reinforcement learning presents an attractive framework for training capable agents. At first glance,\nRL training appears intuitive and autonomous once a reward is defined, the agent learns from its own\nautomatically gathered experience. However, in practice, RL training often assumes the access to environmental\nresets that can require significant human effort to setup, which poses a significant barrier for real world\napplications of RL like robotics.\nMost RL systems on real robots to date have employed various strategies to implement resets, all requiring\na considerable amount of effort (Levine et al., 2016; Yahya et al., 2017; Zhu et al., 2019; Nagabandi et al.,\n2020). In Nagabandi et al. (2020), which trains a dexterous hand to rotate balls, the practitioners had to\n(1) position a funnel underneath the hand to catch dropped balls, and (2) deploy a separate robot arm to"}, {"title": "Related Work", "content": "Reset-free RL: There is a growing interest in researching reinforcement learning methods that can effectively\naddress the complexities of reset-free training. Sharma et al. (2021b) proposes a reset-free RL benchmark\n(EARL) and finds that standard RL methods like SAC (Haarnoja et al., 2018) fail catastrophically in\nEARL. Multiple approaches have been proposed to address reset-free training, which we now summarize.\nOne approach is to add an additional reset policy, to bring the agent back to suitable states for learning\n(Eysenbach et al., 2017; Kim et al., 2022; Sharma et al., 2021a; 2022; Kim et al., 2023). LNT (Eysenbach\net al., 2017) and Kim et al. (2022) train a reset policy to bring the agent back to initial state distribution,\nsupervised by dense rewards and demonstrations respectively. MEDAL (Sharma et al., 2022; 2023), train a\ngoal-conditioned reset policy and direct it to reset goal states from demonstrations. IBC (Kim et al., 2023)\ndefines a curriculum for both task and reset policies without requiring demonstrations. VaPRL (Sharma\net al., 2021a) trains a single goal-conditioned policy to reach high value states close to the initial states.\nInstead of guiding the agent back to familiar states, R3L (Zhu et al., 2020) and Xu et al. (2020) learn to\nreset the policy to diverse initial states, resulting in a policy that is more robust to variations in starting\nstates. However, such methods are limited to tasks where exploration is unchallenging. The vast majority of\nreset-free approaches are model-free, with a few exceptions (Lu et al., 2020b;a). Other works (Gupta et al.,\n2021; Smith et al., 2019) model the reset-free RL training process as a multi-task RL problem and require\ncareful definition of the task distribution such that the tasks reset each other.\nGoal-conditioned Exploration: A common theme running through the aforementioned work is the\ninstantiation of a curriculum, often through commanding goal-conditioned policies, to keep the agent in\ntask-relevant portions of the environment while exploring. Closely related is the subfield of goal-conditioned\nexploration in RL, where a goal-conditioned agent selects its own goals during training time to generate\ndata. There is a large variety of approaches for goal selection, such as task progress (Baranes & Oudeyer,\n2013; Veeriah et al., 2018), intermediate difficulty (Florensa et al., 2018), value disagreement (Zhang et al.,\n2020), state novelty (Pong et al., 2019; Pitis et al., 2020), world model error (Hu et al., 2023; Sekar et al.,\n2020), and more. Many goal-conditioned exploration methods use the \"Go-Explore\" (Ecoffet et al., 2021)\nstrategy, which first selects a goal and runs the goal-conditioned policy (\"Go\"-phase), and then switches\nto an exploration policy for the latter half of the episode (\"Explore\"-phase). PEG (Hu et al., 2023), which\nMoReFree uses, extends Go-Explore to the model-based setting, and utilizes the world model to plan states\nwith higher exploration value as goals. However, such methods are not designed for the reset-free RL setting,\nand may suffer from over-exploration of task-irrelevant states."}, {"title": "Preliminaries", "content": "3.1 Reset-free RL\nWe follow the definition of reset-free RL from EARL (Sharma et al., 2021b), and extend it to the\ngoal-conditioned RL setting. Consider the goal-conditioned Markov decision process (MDP) $M$\n= $(S, G, A, p, r, p_0, P_{g*}, \\gamma)$. At each time step $t$ in the state $s_t \\in S$, a goal-conditioned policy $\\pi(\\cdot|s_t, g)$ under the\ngoal command $g\\in G$ selects an action $a_t \\in A$ and transitions to the next state $s_{t+1}$ with the probability\n$p(s_{t+1}|s_t, a_t)$, and gets a reward $r(s_t,a_t,g)$. $p_0$ is the initial state distribution, $p_{g*}$ is the evaluation goal\ndistribution, and $\\gamma$ is the discount factor.\nThe learning algorithm $A$ is defined: ${s_i, a_i, s_{i+1}}_{i=0}^{t-1} \\rightarrow (a_t, \\pi_t)$, which maps the transitions collected until\nthe time step $t$ to the action $a_t$ the agent should take in the non-episodic training and the best guess $\\pi_t$ of\nthe optimal policy $\\pi^*$ on the evaluation goal distribution $(p_{g*})$. In reset-free training the agent will only be\nreset to the initial state $s_0 \\sim p_0$ a few times. The evaluation of agents is still episodic. The agent always\nstarts from $s_0 \\sim p_0$, and is asked to achieve $g \\sim p_{g*}$. The evaluation objective for a policy $\\pi$ is:\n$J(\\pi) = E_{s_0\\sim p_0,g\\sim p_{g*},a_j\\sim\\pi(\\cdot|s_j,g),s_{j+1}\\sim p(\\cdot|s_j,a_j)}[\\sum_{j=0}^{T} \\gamma^j r(s_j, a_j, g)],\\qquad(1)$\nwhere $T$ is the total time steps during the evaluation. The goal of algorithm $A$ during the reset-free training\nis to minimize the performance difference $D(A)$ of the current policy $\\pi_\\tau$ and the optimal policy $\\pi^*$: \n$D(A) = \\sum_{t=0}^{\\infty}(J(\\pi^*) - J(\\pi_t)).\\qquad(2)$\nIn summary, the algorithm $A$ should output an action $a_t$ that the agent should take in the non-episodic data\ncollection and a policy $\\pi_t$ that can maximize $J(\\pi_t)$ at every time step $t$ based on all previously collected data.\n3.2 Model-based RL setup\nRecent goal-conditioned MBRL approaches like LEXA (Mendonca et al., 2021) and PEG (Hu et al., 2023)\ntrain goal-conditioned policies purely using synthetic data generated by learned world models. Their robust\nexploration demonstrate significant success in solving long-horizon goal-conditioned tasks. In the reset-free\nsetting, strong exploration is crucial, as the agent can no longer depend on episodic resets to bring it back to\ntask-relevant areas if it gets stuck. Therefore, we select PEG as the backbone MBRL agent for its strong\nexploration abilities and sample efficiency.\nPEG (Hu et al., 2023) is a model-based Go-Explore framework that extends LEXA (Mendonca et al., 2021),\nan unsupervised goal-conditioned variant of DreamerV2 (Hafner et al., 2020). The following components are"}, {"title": "Method", "content": "parameterized by $\\theta$ and learned:\n$\\begin{aligned}\n\\text { world model: } &T_\\theta(s_{t+1}|s_t, a_t) \\\\\n\\text { goal conditioned policy: } &\\pi_\\theta(a_t|s_t, g)\\\\\n\\text { exploration policy: } &\\pi_{\\theta^e}(a_t|s_t)\n\\end{aligned}\\qquad\\begin{aligned}\n\\text { goal conditioned value: } &V_\\theta^g(s_t,g) \\\\\n\\text { exploration value: } &V_{\\theta^e}^e(s_t)\n\\end{aligned}\\qquad(3)$\nThe world model is a recurrent state-space model (RSSM) which is trained to predict future states and is used\nas a learned simulator to train the policies and value functions. The goal-conditioned policy $\\pi_\\theta$ is trained to\nreach random states sampled from the replay buffer. The exploration policy $\\pi_{\\theta^e}$ is trained on an intrinsic\nmotivation reward that rewards world model error, expressed through the variance of an ensemble (Sekar\net al., 2020). Both policies are trained on simulated trajectory rollouts in the world model.\n\u25ba Self-supervised Goal-reaching Reward Function: Rather than assuming access to the environmental\nreward, PEG learns its own reward function. PEG uses a dynamical distance function (Hartikainen et al.,\n2019) as the reward function within world models, which predicts the number of actions between a start and\ngoal state. The distance function is trained on random state pairs from imaginary rollouts of $\\pi_\\theta$. $\\pi_g$ is then\ntrained to minimize the dynamical distance between its states and commanded goal state in imagination. See\nMendonca et al. (2021) for more details.\n\u25ba Phased Exploration via Go-Explore: For data-collection, PEG employs\nthe Go-Explore strategy. In the \"Go\"-phase, a goal is sampled from some goal\ndistribution $p$. The goal-conditioned policy, conditioned on the goal is run for some\ntime horizon $H_G$, resulting in trajectory $T_g$.\nThen, in the \"Explore\"-phase, starting from the last state in the \"Go\"-phase, the\nexploration policy is run for $H_e$ steps, resulting in $T_e$. The interleaving of goal-\nconditioned behavior with exploratory behavior results in more directed exploration\nand informative data. This in turn improves accuracy of the world model, and the\npolicies that train inside the world model.\nAs motivated in Section 1 and Figure 1, the direct application of PEG to the reset-free setting shows promising\nperformance but suffers from over-exploration of task-irrelevant states. To adapt model-based RL to the\nreset-free setting, we introduce MoReFree, a model-based approach that improves PEG to handle the lack of\nresets and overcome the over-exploration problem. MoReFree improves two key mechanisms of MBRL for\nreset-free training: exploration and policy training."}, {"title": "Back-and-Forth Go-Explore", "content": "First, we introduce MoReFree's procedure for collecting new datapoints in the real environment. PEG (Hu\net al., 2023) already has strong goal-conditioned exploration abilities, but was developed for solving episodic\ntasks. Without resets, PEG's Go-Explore procedure can undesirably linger in unfamiliar but task-irrelevant\nportions of the state space. This generates large amounts of uninformative trajectories, which in turn degrades\nworld model learning and policy optimization.\nMoReFree overcomes this by periodically directing the agent to return to the states relevant to the task (i.e.\ninitial and evaluation goals). We call this exploration procedure \"Back-and-Forth Go-Explore\", where we\nsample pairs of initial and evaluation goals and ask the agent to cycle back and forth between the goal pairs,\nperiodically interspersed with exploration phases (see Figure 2 top row)."}, {"title": "Learning to Achieve Relevant Goals in Imagination", "content": "Next, we describe how MoReFree trains the goal-conditioned policy in the world model. To train $\\pi_\\theta$,\nMoReFree samples various types of goals and executes $\\pi_f(\\cdot | \\cdot,g)$ inside the world model to generate\n\"imaginary\u201d trajectories. The trajectory data is scored using the learned dynamical distance reward mentioned\nin Section 3.2, and the policy is updated to maximize the expected return. This procedure is called\nimagination (Hafner et al., 2019), and allows the policy to be trained on vast amounts of synthetic trajectories\nto improve sample efficiency.\nFirst, we choose to sample evaluation goals from $p_{g*}$ since the policy\nwill be evaluated on its evaluation goal-reaching performance. Next,\nrecall that Back-and-Forth Go-Explore procedure also samples initial\nstates from $p_0$ as goals for the Go-phase to emulate resetting behavior.\nSince we would like $\\alpha$f to succeed in such cases so that the task is\nreset, we will also sample from $p_0$. Finally, we sample random states\nfrom the replay buffer to increase $\\pi_\\theta$'s ability to reach arbitrary states.\nThe sampling probability for each goal type is set to $\\alpha/2, \\alpha/2,1 \u2013 \\alpha$\nrespectively. In other words, MoReFree biases the goal-conditioned\npolicy optimization procedure to focus on achieving task-relevant\ngoals (i.e. evaluation and initial states), as they are used during\nevaluation and goal-conditioned exploration to condition the goal-\nreaching policy (see Figure 2 bottom row). This leads to additional"}, {"title": "Implementation Details", "content": "Our work builds on the top of PEG (Hu et al., 2023), and we use its default hyperparameters for world model,\npolicies, value functions and temporal reward function. We set the length of each phase for Go-Explore\n(HG, HE) to half the evaluation episode length for each task. We set the default value of $\\alpha = 0.2$ for all tasks\n(never tuned). See Appendix C.3 for more details and the supplemental for MoReFree code."}, {"title": "Experiments", "content": "We evaluate three MBRL methods (PEG (Hu et al., 2023), the extension reset-free PEG and our proposed\nmethod MoReFree) and four competitive reset-free baselines on eight reset-free tasks. We aim to address the\nfollowing questions: 1) Do MBRL approaches work well in reset-free tasks in terms of sample efficiency and"}, {"title": "Results", "content": "As shown in Fig 4, two reset-free model-based methods (MoReFree and reset-free PEG), without demonstra-\ntions or access to environmental reward, outperform other baselines with privileged access to supervision in\nboth final performance and sample efficiency in 7/8 tasks. We observe that the two reset-free MBRL methods\nlearn good behaviors: the pointmass agent hugs the wall of the UMaze to minimize travel time and the Fetch\nrobot deftly pushes and picks up the block into multiple target locations. MoReFree is always competitive\nwith or outperforms reset-free PEG, with large gains in the 3 hardest tasks: Push (hard) by 45%, Pick&Place\n(hard) by 13% and Ant (hard) by 36%. We observe that MoReFree learns non-trivial reset behaviors such as\npicking and pushing blocks back into the center of the table for the hard variants of the Fetch manipulation\ntasks. However, the original PEG performs poorly, suggesting that directly applying episodic MBRL methods\nin a reset-free setting without adaptations yields suboptimal results. See the website for videos of MoReFree\nand baselines.\nIn many tasks, the baselines fail to learn at all. We believe this is due the low sample budget, which may be\ntoo low for the baselines to fully explore the environment and learn the proper resetting behaviors necessary to\ntrain the actual task policy. In Appendix G, we increased the training budget by 3x for the IBC baseline and\nit still fails, underscoring the difficulty of the tasks and the sample-efficiency gains of MoReFree and MBRL.\nOn the other hand, we noticed that one environment, Sawyer Door, seemed particularly hard for MBRL\nagents to solve. We hypothesize that the dynamics of the task are hard to model, resulting in performance\ndegradation for model-based approaches (see Appendix F for more analysis)."}, {"title": "Analysis", "content": "To explain the performance differences between MoReFree and baselines, we closely analyze the exploration\nbehaviors.\nMoReFree focuses on task-relevant states. In Figure 5 we visualize the state visitation heatmaps of\nmethods in various environments, and also compute the percentage of \"task-relevant\" states (initial and goal\nregions, highlighted with white borders). We highlight two trends. First, the heatmaps show that MoReFree\nand reset-free PEG explore thoroughly while baselines have more myopic exploration patterns, as seen in the\nAnt heatmaps at the top."}, {"title": "Ablations", "content": "To justify our design choices, we ablate the two mechanisms of MoReFree, the back-and-forth exploration and\ntask-relevant goal-conditioned policy training, and plot the results in Figure 7 First, removing all mechanisms\n(MF w/o Explore & Imag.) reduces to reset-free PEG, and we can see a large gap in performance. Next,\nMF with Only Task Goals sets $\\alpha = 1$, which causes an extreme bias towards task-relevant states in the\nexploration and policy training. This also degrades performance, due to the need for strong exploration in\nthe reset-free setting. Examinations of more values for $\\alpha$ can be found in Appendix C.3.\nFinally, we isolate individual components of MoRe-\nFree. First, we disable Back-and-Forth Go-Explore\nby disallowing the sampling of initial or evaluation\ngoals during Go-Explore. Only exploratory goals are\nused in Go-Explore for this ablation (named MF\nw/o BF-GE). Next, in MF w/o Imag. we turn off\nthe initial / evaluation goal sampling in imagination,\nso only random replay buffer goals are used to train\n$\\pi_\\theta$. We see that both variants perform poorly. This\nis somewhat intuitive, as the two components rely\non each other. Having both forms a synergistic cycle\nwhere 1) the goal-conditioned policy's optimization\nis more focused towards reaching initial / goal states,\nand 2) the exploration is biased towards reaching\ninitial / goal states by using the goal-conditioned\npolicy we just optimized in step 1. If we remove one\nwithout the other, then the cycle breaks down. In"}, {"title": "Conclusion and Future Work", "content": "As a step towards reset-free training, we adapt model-based methods to the reset-free setting and demonstrate\ntheir superior performance. Specifically, we show that with minor modifications, unsupervised MBRL method\nsubstantially outperforms the state-of-the-art model-free baselines tailored for the reset-free setting while being\nmore autonomous (requires less supervision like environmental reward or demonstrations). We then identify\na limitation of unsupervised MBRL in the reset-free setting (over-exploration on task-irrelevant states), and\npropose MoReFree to address such limitations by focusing model-based exploration and goal-conditioned\npolicy training on task-relevant states. We conduct a through experimental study of MoReFree and baselines\nover 8 tasks, and show considerable performance gains over the MBRL baseline and prior state-of-the-art\nreset-free methods. Despite its overall success, MoReFree is not without limitations. Being a model-based\napproach, it inherits all associated disadvantages. For example, we believe Sawyer Door is a task where\nlearning the dynamics is harder than learning the policy (see Appendix F), disadvantaging MBRL approaches.\nNext, MoReFree uses a fixed percentage of task-relevant goals for exploration and imagination, whereas future\nwork could consider an adaptive curriculum. Finally, scaling MoReFree to high-dimensional observations\nwould be a natural extension. We hope MoReFree inspires future efforts in increasing autonomy in RL."}, {"title": "Broader Impacts", "content": "As we increase the autonomy of RL agents, the possibility of them acting in unexpected ways to maximize\nreward increases. The unsupervised exploration coupled alongside the learned reward functions further add\nto the unpredictability; neither mechanisms are very interpretable. As such, we expect research into value\nalignment, interpretability, and safety to be paramount as autonomy in RL improves."}, {"title": "Extended Related Work", "content": "Learned Reward Functions: Instead of requiring the environment to provide a reward function, the agent\ncan learn its own reward function from onboard sensors and data. Given human specified example states, e.g.\na goal image, VICE and C-Learning train reward classifiers over examples (Fu et al., 2018; Eysenbach et al.,\n2021) and agent data. The learned dynamical distance function (Hartikainen et al., 2019) learns to predict\nthe number of actions between pairs of states. The dynamical distance function is used by unsupervised\nMBRL approaches like LEXA and PEG (Mendonca et al., 2021; Hu et al., 2023) to train the goal-conditioned\npolicy."}, {"title": "Experimental Details", "content": "Environments\nPointUMaze: The state space is 7D and the action space is 2D. The initial state is (0,0), which located\nin the bottom-left corner, and noise sampled from U(-0.1, 0.1) is added when reset. The goal during the\nevaluation is always located in at the top-left corner of the U-shape maze. The maximum steps during the\nevaluation is 100. Hard reset will happen after every 2e5 steps. In the whole training process we performed,\nit only reset once at the beginning of the training. Taken from the IBC (Kim et al., 2023) paper.\nTabletop: The state space is 6D, and the action space is 3D. During the evaluation, four goal locations\nare sampled in turn, the initial state of the agent is always fixed and located in the center of the table. The\nmaximum steps during the evaluation is 200. Hard reset will happens after every 2e5 steps. In the whole\ntraining process we performed, it only reset once at the beginning of the training. Taken from the EARL\n(Sharma et al., 2021b) benchmark and also used in the IBC paper.\nSawyer Door: The state space is 7D and the action space is 4D. The position of door is initialized to open\nstate (60 degree with noise sampled from (0,18) degree) and the goal is always to close the door (0 degree).\nThe arm is initialized to a fixed location. Maximum number of steps is 300 for the evaluation. Hard reset\nwill happen after every 2e5 steps. In the whole training process we performed, it resets twice. Taken from\nthe EARL (Sharma et al., 2021b) benchmark and also used in the IBC paper.\nFetch Push and Pick&Place: The state space is 25D and action space is 4D. These are taken from the IBC\npaper. Authors converted the original Fetch environments to a reversible setting by defining a constraint on\nthe block position. The initial and goal distributions are identical to the original Fetch Push and Pick&Place.\nMore details can be found in the IBC paper.\nPush (hard): Different from the original Fetch Push task, in our case walls are added to prevent the block\nfrom dropping out of the table. The workspace of the robot arm is also limited. The block is always initialized\nto a fixed location, and goal distribution during the evaluation is U(-0.15, 15). Fetch Push used in the IBC\npaper, the block is limited by joint constraint, which shows unrealistic jittering behaviors near the limits\n(we observe such phenomenon by running model-based go-explore, the exploration policy prefers to always\ninteract with the block and keep pushing it towards the limit boundary, see videos on our project website 1).\nMeanwhile, the gripper is blocked, which makes the task easier. In our case, we release the gripper and it\ncan now open and close again which add two more dimension of the state space. We found it is important\nto release the gripper in our version of Push task, when the block is in corners, it will need to operate the\ngripper to drag the block escape from corners. The maximum steps the agent can take in 50 during the"}, {"title": "Baseline Implementations", "content": "PEG: We use the official implementation of PEG\u00b3 and only optimize the exploratory goal distribution once\nat the beginning of each reset-free training episode, i.e. HG and He are set to half of the reset-free episode\nlength.\nreset-free PEG: We extend the official implementation of PEG4 to reset-free setting by 1) set HG and HE\nto half of the evaluation episode length; 2) optimizing the goal distribution every HG + HE steps; 3) keeping\nall other hyperparameters the same as MoReFree.\nIBC: We use the official implementation from authors and keep hyperparameters unchanged.\nMEDAL: We follow the official implementation of MEDAL and use the deafult setting for experiments.\nSince MEDAL requires demonstrations, for tasks from EARL benchmark, demonstrations are provided. For\nother environments, we generate demonstrations by executing the final trained MoReFree to collect data. 30\nepisodes are generated for each task.\nR3L: We implement R3L agent by modifying the FBRL agent from MEDAL codebase. The backward policy\nis replaced by an exploration policy trained using the random network distillation (RND) objective (Burda\net al., 2018). The RND implementation we follow is from DI-engine7.\nOracle: This is a episodic SAC agent, we use the implementation from MEDAL codebase and keep all the\nhyper-parameters unchanged.\nMoReFree: Our agent is built on the model-based go-explore method PEG (Hu et al., 2023), we extend\ntheir codebase by adding back-and-forth goal sampling procedure and training on evaluation initial and goal\nstates in imagination goal-conditioned policy training. See our codebase in the supplemental."}, {"title": "Hyperparameters", "content": "Train ratio (i.e. Update to Data ratio) is an important hyper-parameter in MBRL. It controls how frequently\nthe agent is trained. Every n steps, a batch of data is sampled from the replay buffer, the world model is"}, {"title": "Results Clarification", "content": "In Push and Pick&Place results, we retrieved the final perfor-\nmance of MEDAL directly from the IBC paper (dashed purple\nlines) and did not have time to run R3L in these two envi-\nronments. R3L is shown to be a lot worse than MEDAL in\nthe MEDAL paper and performs obviously bad in other tasks\nshown in Figure 4. In Push (hard) and Pick&Place (hard), we\nran R3L and MEDAL with less budget since other methods\nclearly outperform and their learning curves do not show any\nevidence for going up."}, {"title": "Resource Usage", "content": "We submit jobs on a cluster with Nvidia 2080, 3090 and A100\nGPUs. Our model-based experiments take 1-2 days to finish,\nand the model-free baselines take half day to one day to run."}, {"title": "More Visualizations on Replay Buffer", "content": "We visualize the replay buffer of different agents on more tasks.\nSee Figure 9 for XY location of the mug in Tabletop, Figure 11 for XY location data of the agent in\nPointUMaze, Figure 10 for XZ location of the block in Pick&Place (hard) and Figure 12 for XY location\ndata of the block in Push (hard) and Pick&Place (hard). Overall, we see MoReFree explores the whole state\nspace better. Meanwhile, due to back-and-forth procedure, MoReFree collects more data near initial / goal\nstates, which are important for the evaluation. However, IBC, MEDAL, R3L and Oracle all fail to explore\nwell; their heatmaps are mostly populated with low visitation cells."}, {"title": "Detailed Ablations", "content": "We report learning curves for each variant agent we ablate in Section 5.3 on every task in Figure 13. Since\nMoReFree does not learn at all in Saywer Door task, we exclude the ablation for it. In each task, MoReFree\nis better or on par with all other ablations. Through learning curves, we see different components contribute\ndifferently on different tasks.\nWe further analyze the ablation on PointUMaze as an example by visualizing the replay buffer of different\nvariants, see Figure 14. In the performance on PointUMaze from Figure 13, sampling exploratory goals"}, {"title": "MBRL on Sawyer Door", "content": "We investigate why two MBRL methods fail on Sawyer Door tasks. Note that MoReFree is able to solve\nintermediate goals such as closing the door in some angles, but is unable to solve the original IBC evaluation\ngoal (see website for more videos).\nWe simplify Sawyer Door task by limiting the movement range of the robot to a box and also having a\nblock holds the door to prevent it from opening it too much, see Figure 15. Although MBRL methods are\ntrained on the simplified environment, we see learning curves on Sawyer Door are completely flat in Figure 4,\ncompared with other baselines trained on the original task. We wonder why MBRL methods can show the\nsame performance and gain benefits as it does in other environments.\nMoReFree and reset-free PEG use DreamerV2 as backbone agents and extend it to reset-free settings. We\nhypothesize that Dreamer itself, even under the episodic setting with task reward function, would not work\nwell. If that's the case, then MBRL methods in the reset-free setting with self-supervised reward function\nwould almost certainly not work either. For example, if the backbone agent cannot model the dynamics\nprecisely, then policy learning, dynamical distance reward learning, will be degraded."}, {"title": "More Analysis on Fetch Environments", "content": "Although IBC gains good final performance in Push and Pick&Place, it starts learning late compared with\nMBRL methods and fails entirely in our harder versions. We suspect IBC might need more computational"}]}