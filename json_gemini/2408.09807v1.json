{"title": "World Models Increase Autonomy in Reinforcement Learning", "authors": ["Zhao Yang", "Thomas M. Moerland", "Mike Preuss", "Aske Plaat", "Edward S. Hu"], "abstract": "Reinforcement learning (RL) is an appealing paradigm for training intelligent agents, enabling policy acquisition from the agent's own autonomously acquired experience. However, the training process of RL is far from automatic, requiring extensive human effort to reset the agent and environments. To tackle the challenging reset-free setting, we first demonstrate the superiority of model-based (MB) RL methods in such setting, showing that a straightforward adaptation of MBRL can outperform all the prior state-of-the-art methods while requiring less supervision. We then identify limitations inherent to this direct extension and propose a solution called model-based reset-free (MoReFree) agent, which further enhances the performance. MoReFree adapts two key mechanisms, exploration and policy learning, to handle reset-free tasks by prioritizing task-relevant states. It exhibits superior data-efficiency across various reset-free tasks without access to environmental reward or demonstrations while significantly outperforming privileged baselines that require supervision. Our findings suggest model-based methods hold significant promise for reducing human effort in RL.", "sections": [{"title": "Introduction", "content": "Reinforcement learning presents an attractive framework for training capable agents. At first glance, RL training appears intuitive and autonomous once a reward is defined, the agent learns from its own automatically gathered experience. However, in practice, RL training often assumes the access to environmental resets that can require significant human effort to setup, which poses a significant barrier for real world applications of RL like robotics.\nMost RL systems on real robots to date have employed various strategies to implement resets, all requiring a considerable amount of effort (Levine et al., 2016; Yahya et al., 2017; Zhu et al., 2019; Nagabandi et al., 2020). In Nagabandi et al. (2020), which trains a dexterous hand to rotate balls, the practitioners had to (1) position a funnel underneath the hand to catch dropped balls, and (2) deploy a separate robot arm to"}, {"title": "Related Work", "content": "Reset-free RL: There is a growing interest in researching reinforcement learning methods that can effectively address the complexities of reset-free training. Sharma et al. (2021b) proposes a reset-free RL benchmark (EARL) and finds that standard RL methods like SAC (Haarnoja et al., 2018) fail catastrophically in EARL. Multiple approaches have been proposed to address reset-free training, which we now summarize.\nOne approach is to add an additional reset policy, to bring the agent back to suitable states for learning (Eysenbach et al., 2017; Kim et al., 2022; Sharma et al., 2021a; 2022; Kim et al., 2023). LNT (Eysenbach et al., 2017) and Kim et al. (2022) train a reset policy to bring the agent back to initial state distribution, supervised by dense rewards and demonstrations respectively. MEDAL (Sharma et al., 2022; 2023), train a goal-conditioned reset policy and direct it to reset goal states from demonstrations. IBC (Kim et al., 2023) defines a curriculum for both task and reset policies without requiring demonstrations. VaPRL (Sharma et al., 2021a) trains a single goal-conditioned policy to reach high value states close to the initial states. Instead of guiding the agent back to familiar states, R3L (Zhu et al., 2020) and Xu et al. (2020) learn to reset the policy to diverse initial states, resulting in a policy that is more robust to variations in starting states. However, such methods are limited to tasks where exploration is unchallenging. The vast majority of reset-free approaches are model-free, with a few exceptions (Lu et al., 2020b;a). Other works (Gupta et al., 2021; Smith et al., 2019) model the reset-free RL training process as a multi-task RL problem and require careful definition of the task distribution such that the tasks reset each other.\nGoal-conditioned Exploration: A common theme running through the aforementioned work is the instantiation of a curriculum, often through commanding goal-conditioned policies, to keep the agent in task-relevant portions of the environment while exploring. Closely related is the subfield of goal-conditioned exploration in RL, where a goal-conditioned agent selects its own goals during training time to generate data. There is a large variety of approaches for goal selection, such as task progress (Baranes & Oudeyer, 2013; Veeriah et al., 2018), intermediate difficulty (Florensa et al., 2018), value disagreement (Zhang et al., 2020), state novelty (Pong et al., 2019; Pitis et al., 2020), world model error (Hu et al., 2023; Sekar et al., 2020), and more. Many goal-conditioned exploration methods use the \"Go-Explore\" (Ecoffet et al., 2021) strategy, which first selects a goal and runs the goal-conditioned policy (\"Go\"-phase), and then switches to an exploration policy for the latter half of the episode (\"Explore\"-phase). PEG (Hu et al., 2023), which MoReFree uses, extends Go-Explore to the model-based setting, and utilizes the world model to plan states with higher exploration value as goals. However, such methods are not designed for the reset-free RL setting, and may suffer from over-exploration of task-irrelevant states."}, {"title": "Preliminaries", "content": "3.1 Reset-free RL\nWe follow the definition of reset-free RL from EARL (Sharma et al., 2021b), and extend it to the goal-conditioned RL setting. Consider the goal-conditioned Markov decision process (MDP) $\\mathcal{M} = (\\mathcal{S}, \\mathcal{G}, \\mathcal{A}, p, r, p_o, p_g^*, \\gamma)$. At each time step $t$ in the state $s_t \\in \\mathcal{S}$, a goal-conditioned policy $\\pi(\\cdot|s_t, g)$ under the goal command $g \\in \\mathcal{G}$ selects an action $a_t \\in \\mathcal{A}$ and transitions to the next state $s_{t+1}$ with the probability $p(s_{t+1}|s_t, a_t)$, and gets a reward $r(s_t,a_t,g)$. $p_o$ is the initial state distribution, $p_g^*$ is the evaluation goal distribution, and $\\gamma$ is the discount factor.\nThe learning algorithm $\\mathcal{A}$ is defined: $\\{s_i, a_i, s_{i+1}\\}_{i=0}^{t-1} \\rightarrow (a_t, \\pi_t)$, which maps the transitions collected until the time step $t$ to the action $a_t$ the agent should take in the non-episodic training and the best guess $\\pi_t$ of the optimal policy $\\pi^*$ on the evaluation goal distribution ($p_g^*$). In reset-free training the agent will only be reset to the initial state $s_o \\sim p_o$ a few times. The evaluation of agents is still episodic. The agent always starts from $s_o \\sim p_o$, and is asked to achieve $g \\sim p_g^*$. The evaluation objective for a policy $\\pi$ is:\n$J(\\pi) = \\mathbb{E}_{s_0 \\sim p_o, g \\sim p_g^*, a_j \\sim \\pi( \\cdot |s_j, g), s_{j+1} \\sim p(\\cdot|s_j, a_j)} [\\sum_{j=0}^{T} \\gamma^j r(s_j, a_j, g)]$,\nwhere $T$ is the total time steps during the evaluation. The goal of algorithm $\\mathcal{A}$ during the reset-free training is to minimize the performance difference $D(\\mathcal{A})$ of the current policy $\\pi_\\tau$ and the optimal policy $\\pi^*$: \n$D(\\mathcal{A}) = \\sum_{t=0}^{\\infty}(J(\\pi^*) - J(\\pi_t)).$\nIn summary, the algorithm $\\mathcal{A}$ should output an action $a_t$ that the agent should take in the non-episodic data collection and a policy $\\pi_t$ that can maximize $J(\\pi_t)$ at every time step $t$ based on all previously collected data.\n3.2 Model-based RL setup\nRecent goal-conditioned MBRL approaches like LEXA (Mendonca et al., 2021) and PEG (Hu et al., 2023) train goal-conditioned policies purely using synthetic data generated by learned world models. Their robust exploration demonstrate significant success in solving long-horizon goal-conditioned tasks. In the reset-free setting, strong exploration is crucial, as the agent can no longer depend on episodic resets to bring it back to task-relevant areas if it gets stuck. Therefore, we select PEG as the backbone MBRL agent for its strong exploration abilities and sample efficiency.\nPEG (Hu et al., 2023) is a model-based Go-Explore framework that extends LEXA (Mendonca et al., 2021), an unsupervised goal-conditioned variant of DreamerV2 (Hafner et al., 2020). The following components are"}, {"title": "Method", "content": "As motivated in Section 1 and Figure 1, the direct application of PEG to the reset-free setting shows promising performance but suffers from over-exploration of task-irrelevant states. To adapt model-based RL to the reset-free setting, we introduce MoReFree, a model-based approach that improves PEG to handle the lack of resets and overcome the over-exploration problem. MoReFree improves two key mechanisms of MBRL for reset-free training: exploration and policy training.\n4.1 Back-and-Forth Go-Explore\nFirst, we introduce MoReFree's procedure for collecting new datapoints in the real environment. PEG (Hu et al., 2023) already has strong goal-conditioned exploration abilities, but was developed for solving episodic tasks. Without resets, PEG's Go-Explore procedure can undesirably linger in unfamiliar but task-irrelevant portions of the state space. This generates large amounts of uninformative trajectories, which in turn degrades world model learning and policy optimization.\nMoReFree overcomes this by periodically directing the agent to return to the states relevant to the task (i.e. initial and evaluation goals). We call this exploration procedure \"Back-and-Forth Go-Explore\", where we sample pairs of initial and evaluation goals and ask the agent to cycle back and forth between the goal pairs, periodically interspersed with exploration phases (see Figure 2 top row)."}, {"title": "Learning to Achieve Relevant Goals in Imagination", "content": "Next, we describe how MoReFree trains the goal-conditioned policy in the world model. To train $\\pi_g$, MoReFree samples various types of goals and executes $\\pi_F(\\cdot | \\cdot,g)$ inside the world model to generate \"imaginary\u201d trajectories. The trajectory data is scored using the learned dynamical distance reward mentioned in Section 3.2, and the policy is updated to maximize the expected return. This procedure is called imagination (Hafner et al., 2019), and allows the policy to be trained on vast amounts of synthetic trajectories to improve sample efficiency.\nFirst, we choose to sample evaluation goals from $p_{g^*}$ since the policy will be evaluated on its evaluation goal-reaching performance. Next, recall that Back-and-Forth Go-Explore procedure also samples initial states from $p_o$ as goals for the Go-phase to emulate resetting behavior. Since we would like $\\alpha$ to succeed in such cases so that the task is reset, we will also sample from $p_o$. Finally, we sample random states from the replay buffer to increase $\\pi_g$'s ability to reach arbitrary states. The sampling probability for each goal type is set to $\\alpha/2, \\alpha/2, 1 - \\alpha$ respectively. In other words, MoReFree biases the goal-conditioned policy optimization procedure to focus on achieving task-relevant goals (i.e. evaluation and initial states), as they are used during evaluation and goal-conditioned exploration to condition the goal-reaching policy (see Figure 2 bottom row). This leads to additional changes of line 13 in Algorithm 2."}, {"title": "Implementation Details", "content": "Our work builds on the top of PEG (Hu et al., 2023), and we use its default hyperparameters for world model, policies, value functions and temporal reward function. We set the length of each phase for Go-Explore ($H_G, H_E$) to half the evaluation episode length for each task. We set the default value of $\\alpha = 0.2$ for all tasks (never tuned). See Appendix C.3 for more details and the supplemental for MoReFree code."}, {"title": "Experiments", "content": "We evaluate three MBRL methods (PEG (Hu et al., 2023), the extension reset-free PEG and our proposed method MoReFree) and four competitive reset-free baselines on eight reset-free tasks. We aim to address the following questions: 1) Do MBRL approaches work well in reset-free tasks in terms of sample efficiency and"}, {"title": "Results", "content": "As shown in Fig 4, two reset-free model-based methods (MoReFree and reset-free PEG), without demonstrations or access to environmental reward, outperform other baselines with privileged access to supervision in both final performance and sample efficiency in 7/8 tasks. We observe that the two reset-free MBRL methods learn good behaviors: the pointmass agent hugs the wall of the UMaze to minimize travel time and the Fetch robot deftly pushes and picks up the block into multiple target locations. MoReFree is always competitive with or outperforms reset-free PEG, with large gains in the 3 hardest tasks: Push (hard) by 45%, Pick&Place (hard) by 13% and Ant (hard) by 36%. We observe that MoReFree learns non-trivial reset behaviors such as picking and pushing blocks back into the center of the table for the hard variants of the Fetch manipulation tasks. However, the original PEG performs poorly, suggesting that directly applying episodic MBRL methods in a reset-free setting without adaptations yields suboptimal results. See the website for videos of MoReFree and baselines.\nIn many tasks, the baselines fail to learn at all. We believe this is due the low sample budget, which may be too low for the baselines to fully explore the environment and learn the proper resetting behaviors necessary to train the actual task policy. In Appendix G, we increased the training budget by 3x for the IBC baseline and it still fails, underscoring the difficulty of the tasks and the sample-efficiency gains of MoReFree and MBRL. On the other hand, we noticed that one environment, Sawyer Door, seemed particularly hard for MBRL agents to solve. We hypothesize that the dynamics of the task are hard to model, resulting in performance degradation for model-based approaches (see Appendix F for more analysis)."}, {"title": "Analysis", "content": "To explain the performance differences between MoReFree and baselines, we closely analyze the exploration behaviors.\nMoReFree focuses on task-relevant states. In Figure 5 we visualize the state visitation heatmaps of methods in various environments, and also compute the percentage of \"task-relevant\" states (initial and goal regions, highlighted with white borders). We highlight two trends. First, the heatmaps show that MoReFree and reset-free PEG explore thoroughly while baselines have more myopic exploration patterns, as seen in the Ant heatmaps at the top."}, {"title": "Ablations", "content": "To justify our design choices, we ablate the two mechanisms of MoReFree, the back-and-forth exploration and task-relevant goal-conditioned policy training, and plot the results in Figure 7 First, removing all mechanisms (MF w/o Explore & Imag.) reduces to reset-free PEG, and we can see a large gap in performance. Next, MF with Only Task Goals sets $\\alpha = 1$, which causes an extreme bias towards task-relevant states in the exploration and policy training. This also degrades performance, due to the need for strong exploration in the reset-free setting. Examinations of more values for $\\alpha$ can be found in Appendix C.3.\nFinally, we isolate individual components of MoRe-Free. First, we disable Back-and-Forth Go-Explore by disallowing the sampling of initial or evaluation goals during Go-Explore. Only exploratory goals are used in Go-Explore for this ablation (named MF w/o BF-GE). Next, in MF w/o Imag. we turn off the initial / evaluation goal sampling in imagination, so only random replay buffer goals are used to train $\\pi_g$. We see that both variants perform poorly. This is somewhat intuitive, as the two components rely on each other. Having both forms a synergistic cycle where 1) the goal-conditioned policy's optimization is more focused towards reaching initial / goal states, and 2) the exploration is biased towards reaching initial / goal states by using the goal-conditioned policy we just optimized in step 1. If we remove one without the other, then the cycle breaks down. In MF w/o Imag., Back-and-Forth Go-Explore will suffer since $\\pi_g$ trained on random goals cannot reliably reach initial / evaluation goals. In MF w/o BF-GE, the exploration strategy will not seek initial / evaluation states, resulting in an inaccurate world model and degraded policy optimization. In summary, the ablations show that MoReFree's design is sound and is the major factor behind its success in the reset-free setting. See Appendix E for details."}, {"title": "Conclusion and Future Work", "content": "As a step towards reset-free training, we adapt model-based methods to the reset-free setting and demonstrate their superior performance. Specifically, we show that with minor modifications, unsupervised MBRL method substantially outperforms the state-of-the-art model-free baselines tailored for the reset-free setting while being more autonomous (requires less supervision like environmental reward or demonstrations). We then identify a limitation of unsupervised MBRL in the reset-free setting (over-exploration on task-irrelevant states), and propose MoReFree to address such limitations by focusing model-based exploration and goal-conditioned policy training on task-relevant states. We conduct a through experimental study of MoReFree and baselines over 8 tasks, and show considerable performance gains over the MBRL baseline and prior state-of-the-art reset-free methods. Despite its overall success, MoReFree is not without limitations. Being a model-based approach, it inherits all associated disadvantages. For example, we believe Sawyer Door is a task where learning the dynamics is harder than learning the policy (see Appendix F), disadvantaging MBRL approaches. Next, MoReFree uses a fixed percentage of task-relevant goals for exploration and imagination, whereas future work could consider an adaptive curriculum. Finally, scaling MoReFree to high-dimensional observations would be a natural extension. We hope MoReFree inspires future efforts in increasing autonomy in RL."}, {"title": "Broader Impacts", "content": "As we increase the autonomy of RL agents, the possibility of them acting in unexpected ways to maximize\nreward increases. The unsupervised exploration coupled alongside the learned reward functions further add\nto the unpredictability; neither mechanisms are very interpretable. As such, we expect research into value\nalignment, interpretability, and safety to be paramount as autonomy in RL improves."}, {"title": "Extended Related Work", "content": "Learned Reward Functions: Instead of requiring the environment to provide a reward function, the agent\ncan learn its own reward function from onboard sensors and data. Given human specified example states, e.g.\na goal image, VICE and C-Learning train reward classifiers over examples (Fu et al.,\n2018; Eysenbach et al.,\n2021) and agent data. The learned dynamical distance function (Hartikainen et al., 2019) learns to predict\nthe number of actions between pairs of states. The dynamical distance function is used by unsupervised\nMBRL approaches like LEXA and PEG (Mendonca et al., 2021; Hu et al., 2023) to train the goal-conditioned\npolicy."}, {"title": "Experimental Details", "content": "C.1 Environments\nPointUMaze: The state space is 7D and the action space is 2D. The initial state is (0,0), which located\nin the bottom-left corner, and noise sampled from U(-0.1, 0.1) is added when reset. The goal during the\nevaluation is always located in at the top-left corner of the U-shape maze. The maximum steps during the\nevaluation is 100. Hard reset will happen after every 2e5 steps. In the whole training process we performed,\nit only reset once at the beginning of the training. Taken from the IBC (Kim et al., 2023) paper.\nTabletop: The state space is 6D, and the action space is 3D. During the evaluation, four goal locations\nare sampled in turn, the initial state of the agent is always fixed and located in the center of the table. The\nmaximum steps during the evaluation is 200. Hard reset will happens after every 2e5 steps. In the whole\ntraining process we performed, it only reset once at the beginning of the training. Taken from the EARL\n(Sharma et al., 2021b) benchmark and also used in the IBC paper.\nSawyer Door: The state space is 7D and the action space is 4D. The position of door is initialized to open\nstate (60 degree with noise sampled from (0,18) degree) and the goal is always to close the door (0 degree).\nThe arm is initialized to a fixed location. Maximum number of steps is 300 for the evaluation. Hard reset\nwill happen after every 2e5 steps. In the whole training process we performed, it resets twice. Taken from\nthe EARL (Sharma et al., 2021b) benchmark and also used in the IBC paper.\nFetch Push and Pick&Place: The state space is 25D and action space is 4D. These are taken from the IBC\npaper. Authors converted the original Fetch environments to a reversible setting by defining a constraint on\nthe block position. The initial and goal distributions are identical to the original Fetch Push and Pick&Place.\nMore details can be found in the IBC paper.\nPush (hard): Different from the original Fetch Push task, in our case walls are added to prevent the block\nfrom dropping out of the table. The workspace of the robot arm is also limited. The block is always initialized\nto a fixed location, and goal distribution during the evaluation is U(-0.15, 15). Fetch Push used in the IBC\npaper, the block is limited by joint constraint, which shows unrealistic jittering behaviors near the limits\n(we observe such phenomenon by running model-based go-explore, the exploration policy prefers to always\ninteract with the block and keep pushing it towards the limit boundary, see videos on our project website 1).\nMeanwhile, the gripper is blocked, which makes the task easier. In our case, we release the gripper and it\ncan now open and close again which add two more dimension of the state space. We found it is important\nto release the gripper in our version of Push task, when the block is in corners, it will need to operate the\ngripper to drag the block escape from corners. The maximum steps the agent can take in 50 during the\n1https://sites.google.com/view/morefree"}, {"title": "Baseline Implementations", "content": "PEG: We use the official implementation of PEG\u00b3 and only optimize the exploratory goal distribution once\nat the beginning of each reset-free training episode, i.e. HG and He are set to half of the reset-free episode\nlength.\nreset-free PEG: We extend the official implementation of PEG4 to reset-free setting by 1) set HG and HE\nto half of the evaluation episode length; 2) optimizing the goal distribution every HG + HE steps; 3) keeping\nall other hyperparameters the same as MoReFree.\nIBC: We use the official implementation from authors and keep hyperparameters unchanged.\nMEDAL: We follow the official implementation of MEDAL and use the deafult setting for experiments.\nSince MEDAL requires demonstrations, for tasks from EARL benchmark, demonstrations are provided. For\nother environments, we generate demonstrations by executing the final trained MoReFree to collect data. 30\nepisodes are generated for each task.\nR3L: We implement R3L agent by modifying the FBRL agent from MEDAL codebase. The backward policy\nis replaced by an exploration policy trained using the random network distillation (RND) objective (Burda\net al., 2018). The RND implementation we follow is from DI-engine7.\nOracle: This is a episodic SAC agent, we use the implementation from MEDAL codebase and keep all the\nhyper-parameters unchanged.\nMoReFree: Our agent is built on the model-based go-explore method PEG (Hu et al., 2023), we extend\ntheir codebase by adding back-and-forth goal sampling procedure and training on evaluation initial and goal\nstates in imagination goal-conditioned policy training. See our codebase in the supplemental."}, {"title": "Hyperparameters", "content": "Train ratio (i.e. Update to Data ratio) is an important hyper-parameter in MBRL. It controls how frequently\nthe agent is trained. Every n steps, a batch of data is sampled from the replay buffer, the world model is"}, {"title": "Results Clarification", "content": "In Push and Pick&Place results, we retrieved the final perfor-mance of MEDAL directly from the IBC paper (dashed purple\nlines) and did not have time to run R3L in these two envi-ronments. R3L is shown to be a lot worse than MEDAL inthe MEDAL paper and performs obviously bad in other tasksshown in Figure 4. In Push (hard) and Pick&Place (hard), we\nran R3L and MEDAL with less budget since other methods\nclearly outperform and their learning curves do not show any\nevidence for going up."}, {"title": "Resource Usage", "content": "We submit jobs on a cluster with Nvidia 2080, 3090 and A100\nGPUs. Our model-based experiments take 1-2 days to finish,\nand the model-free baselines take half day to one day to run."}, {"title": "More Visualizations on Replay Buffer", "content": "We visualize the replay buffer of different agents on more tasks.\nSee Figure 9 for XY location of the mug in Tabletop, Figure 11 for XY location data of the agent in\nPointUMaze, Figure 10 for XZ location of the block in Pick&Place (hard) and Figure 12 for XY location\ndata of the block in Push (hard) and Pick&Place (hard). Overall, we see MoReFree explores the whole state\nspace better. Meanwhile, due to back-and-forth procedure, MoReFree collects more data near initial / goal\nstates, which are important for the evaluation. However, IBC, MEDAL, R3L and Oracle all fail to explore\nwell; their heatmaps are mostly populated with low visitation cells."}, {"title": "Detailed Ablations", "content": "We report learning curves for each variant agent we ablate in Section 5.3 on every task in Figure 13. Since\nMoReFree does not learn at all in Saywer Door task, we exclude the ablation for it. In each task, MoReFree\nis better or on par with all other ablations. Through learning curves, we see different components contribute\ndifferently on different tasks.\nWe further analyze the ablation on PointUMaze as an example by visualizing the replay buffer of different\nvariants, see Figure 14. In the performance on PointUMaze from Figure 13, sampling exploratory goals"}, {"title": "MBRL on Sawyer Door", "content": "We investigate why two MBRL methods fail on Sawyer Door tasks. Note that MoReFree is able to solve\nintermediate goals such as closing the door in some angles, but is unable to solve the original IBC evaluation\ngoal (see website for more videos).\nWe simplify Sawyer Door task by limiting the movement range of the robot to a box and also having a\nblock holds the door to prevent it from opening it too much, see Figure 15. Although MBRL methods are\ntrained on the simplified environment, we see learning curves on Sawyer Door are completely flat in Figure 4,\ncompared with other baselines trained on the original task. We wonder why MBRL methods can show the\nsame performance and gain benefits as it does in other environments.\nMoReFree and reset-free PEG use DreamerV2 as backbone agents and extend it to reset-free settings. We\nhypothesize that Dreamer itself, even under the episodic setting with task reward function, would not work\nwell. If that's the case, then MBRL methods in the reset-free setting with self-supervised reward function\nwould almost certainly not work either. For example, if the backbone agent cannot model the dynamics\nprecisely, then policy learning, dynamical distance reward learning, will be degraded."}, {"title": "Analysis on R3L", "content": "R3L trains two policies, one for reaching the goal and another that brings the agent to novel states. The\ngoal-reaching policy is trained using a learned classifier to classify the goal state and other states. Original\nR3L takes images as inputs, thus the trained classifier can successfully classify goal images from random\nstate images. In our work, we use low-dimensional state input. Outputs of the trained classifier on the whole\nstate space of PointUMaze is shown in Figure 19. We see that the classifier learns to output higher values\nfor states close to the goal state (red dot) and lower values for states further away. Nonetheless, due to the\nsmoothness of the output scope, states near the initial state (blue circle) that are numerically closer but"}]}