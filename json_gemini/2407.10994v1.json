{"title": "Panza: A Personalized Text Writing Assistant via Data Playback and Local Fine-Tuning", "authors": ["Armand Nicolicioiu", "Eugenia Iofinova", "Eldar Kurtic", "Mahdi Nikdan", "Andrei Panferov", "Ilia Markov", "Nir Shavit", "Dan Alistarh"], "abstract": "The availability of powerful open-source large language models (LLMs) opens exciting use-cases, such as automated personal assistants that adapt to the user's unique data and demands. Two key desiderata for such assistants are personalization-in the sense that the assistant should reflect the user's own style-and privacy-in the sense that users may prefer to always store their personal data locally, on their own computing device. In this application paper, we present a new design for such an automated assistant, for the specific use case of personal assistant for email generation, which we call Panza. Specifically, Panza can be both trained and inferenced locally on commodity hardware, and is personalized to the user's writing style. Panza's personalization features are based on a new technique called data playback, which allows us to fine-tune an LLM to better reflect a user's writing style using limited data. We show that, by combining efficient fine-tuning and inference methods, Panza can be executed entirely locally using limited resources-specifically, it can be executed within the same resources as a free Google Colab instance. Finally, our key methodological contribution is a careful study of evaluation metrics, and of how different choices of system components (e.g. the use of Retrieval-Augmented Generation or different fine-tuning approaches) impact the system's performance. Panza is available at https://github.com/IST-DASLab/PanzaMail.", "sections": [{"title": "1 Introduction", "content": "An automated personal assistant is a software application that can help the user with various repetitive tasks such as email, writing, or summarization.\nLarge Language Models (LLMs) are natural candidates for implementing personal assistants, as they can provide remarkably good results on such generative tasks. At the same time, many highly-capable LLMs reside in the cloud, and can only be accessed via an API. Specifically, this makes it expensive or impossible to support certain natural features for automated personal assistants, namely:\nModel personalization that is, customizing the model to the specific individual's tone, preferences, and personal history;\nPrivacy protection, that is, allow the model to have access to highly personal information of a caliber that-unlike corporate data\u2014 most people would not agree to share even if promised that the data is protected by the provider's cloud.\nA natural approach to addressing these constraints would be to execute these models locally, on the user's own data and hardware. However, this poses obvious challenges both in terms of data ingestion-that is, getting user data into a format that can be used to successfully train or fine-tune an LLM-and in terms of hardware efficiency-specifically, because fine-tuning or even inferencing over a capable LLM on user data may be technically impossible if done naively.\nIn this application paper, we take up this challenge for the limited, but interesting, case of designing and implementing a fully-local automated email writing assistant we call Panza, whose purpose is to generate relevant messages in the user's own style, given a user prompt, as well as access to a set of previously-sent user emails. Our main finding is that it is possible to obtain a capable assistant starting from existing pre-trained models such as Llama-3-8B (Meta, 2024), and that this application can be executed entirely locally, on a single machine with a commodity GPU, or even in a CPU-only environment. The overall structure of Panza is illustrated and described in Figure 1. While the focus of our work is applied, we present novel findings from the following perspectives:\nWe introduce a new technique called data playback whose goal is to personalize a generative LLMs output to match a user's writing style, given a relatively small number of text samples produced by a user, such as sent emails. Data playback works in two stages: first, given the text samples and a pre-trained (non-personalized) LLM, we use the LLM to summarize each text sample, but in imperative form, i.e. as an instruction from a user to an assistant. For email generation, each text sample becomes associated with an instruction which, if followed, should lead an ideal assistant to produce the original email exactly. We show that pre-trained instruction-tuned open LLMs such as Mistral and Llama have good performance on this instruction-generation task. In the second step, we use the (instruction, sample) pairs as training data for a personalized instruction tuning stage of the pre-trained LLM, with the goal of transferring the user's style onto the LLM. As such, data playback \u201creduces\u201d personalization to instruction tuning. The fine-tuned model can then be deployed for inference over unseen prompts, possibly in conjunction with Retrieval-Augmented Generation (RAG).\nOn the methodological side, we begin with an investigation of evaluation metrics for this task, in conjunction with studying the impact of different design options or even hyper-parameters on the final model accuracy. First, we observe that the BLEU / ROUGE / MAUVE metrics usually employed in these scenarios all show similar trends. Then, we show that data playback consistently outperforms both a prompted pre-trained model and a pre-trained model with RAG. We then perform one of the first in-depth studies of the impact of RAG on personalized model performance when applied either at training time or at inference time (or both) for LLMs.\nWe show that this entire pipeline can be executed in a resource-constrained setup. Specifically, we show that both the fine-tuning necessary for data playback as well as the inference and RAG components can be run efficiently and accurately on a system with a single commodity GPU. We show that the best parameter-efficient fine-tuning (PEFT) results are obtained using the Robust Adaptation (ROSA) method (Nikdan et al., 2024), which we find particularly suitable for style transfer, and can provide good results even with limited training data. Our main innovation on this point is a new accurate merging mechanism, which is required to accurately combine sparse and low-rank PEFT adapters into quantized weights.\nA general finding is that existing metrics are not a good fit to the highly-practical scenario where an LLM has to generate personalized text via RAG or fine-tuning. To address this challenge, we propose a blend of metrics to ensure that the desiderata of personalization and content are met. In addition, we show that, using our techniques, good performance for personalized text generation can be achieved using existing open LLMs."}, {"title": "2 Method", "content": "2.1 Overview\nThe Panza design, described in Figure 1, requires a pre-trained, possibly instruction-tuned, LLM and of a set of emails sent by the user. Both the LLM and the emails have dual use. First, the pre-trained LLM is used to rephrase the \"raw\" user emails in the form of instructions, which will be used for data playback (described below). Then, the LLM itself is going to be fine-tuned on these instructions, resulting on the Panza personalized model. Independently, the emails are used to generate a RAG database, employed at deployment time.\n2.2 The Data Playback Mechanism\nData playback is the core personalization technique behind Panza. The intuition behind data playback is that we would like to \"reduce\" the problem of creating emails in the user's style to a specific instance of instruction-tuning. In a practical usage scenario, the user query would come in the form of a standard instruction, e.g. \"Write an email to Joanne to set up a meeting on Wednesday.\", and the \"correct\" answer would be such an email written in the user's style. To induce this behavior from the LLM, data playback works in two steps:\nFirst, we use a pre-trained LLM to rephrase each email into a targeted instruction, containing just enough information for an assistant to write the original email.\nSecond, we fine-tune the LLM (possibly the same one) on the (instruction, email) pairs obtained in the first step, with a standard training objective which induces the LLM to reconstitute the original email as a response to the instruction. Optionally, we implement a training-time RAG component, which retrieves query-related emails and provides them as context to the LLM at training time. This is similar to retrieval-augmented fine-tuning (RAFT) (Zhang et al., 2024).\nAt the end of these two steps, we have obtained a personalized LLM which can respond to short user queries by writing targeted emails that should follow the user's style. Next, we describe how this model is deployed.\n2.3 Deployment\nIn the absence of computational or memory constraints, we perform full fine-tuning (FFT) of the base model, and inference over it, possibly adding a retrieval-augmented generation (RAG) component, which retrieves similar emails sent in the past. However, fine-tuning and deploying a powerful billion-parameter model locally requires a powerful GPU with significant memory. Therefore, we also investigate techniques for reducing these costs, as well as their impact in terms of accuracy metrics.\n2.4 Local Fine-Tuning and Inference\nMemory efficiency is critical in our setting, due to privacy constraints. For instance, full fine-tuning (FFT) of a Mistral-7B model (Jiang et al., 2023) in half-precision using a standard Adam optimizer (Kingma and Ba, 2015) requires more than 60GB of GPU memory. Such resources are rarely available in a consumer-grade local machine.\nWe tailor Panza to two resource-constrained settings; running on a GPU with under 24GB RAM and under 15GB RAM, each of which is detailed next. Throughout, we use Mistral-7B (Jiang et al., 2023) as a running example for costs, but the techniques apply to other base LLMs with similar size. We always consider a local training micro-batch size of 1 to minimize memory footprint, accumulating gradients whenever necessary. See Section 4 for more details. Training takes under an hour.\nPanza on a single GPU. We first assume local access to a single 24GB GPU (such as NVIDIA GeForce RTX 3090), which is relevant for users with a small GPU server or a strong gaming laptop. For this, we use Parameter-Efficient Fine-Tuning (PEFT) methods, which tune only a small (possibly extra) set of parameters to enable efficient adaptation of models to downstream tasks. We compare the standard LoRA method (Hu et al., 2021), as well as the more recent Robust Adaptation (ROSA) method (Nikdan et al., 2024), which we find to be particularly effective for style transfer. Specifically, by training a combination of low-rank and sparse adapters on top of the base weights, RoSA allows effective fine-tuning of a half-precision 7B model on less than 24GB of memory, with competitive accuracy relative to full fine-tuning. For deployment, we merge the ROSA adapters into the base model weights, with the inference requiring around 15GB.\nPanza under 15GB GPU memory This setting is particularly interesting since it allows training and deploying Panza on a free Colab instance (NVIDIA T4 GPU), or a gaming laptop. In addition to the GPU memory constraint, we also assume access to only 12GB of CPU memory, since that is the case for free Colab instances. We take a similar approach as above, while quantizing multiple components of the pipeline to 4 bits per parameter, detailed below.\nWe store the weights of the base model in doubly-quantized 4 bit precision (Dettmers et al., 2023), reducing the model size in memory by roughly 4x.\nAs part of the sparse adapter's mask generation, RoSA accumulates gradients of the base weights on CPU, violating the 12GB CPU memory restriction. To remedy this issue, we change the precision of the accumulators to 4 bits using a uniform quantization with group size 128.\nTo merge the half-precision ROSA adapters and the 4-bit base weights, we present a new accurate merging mechanism; for each layer, we dequantize the base weights, add adapters, and quantize the result back to 4 bits using the GPTQ method (Frantar et al., 2022). Our key contribution is an adapter-aware implemetation of the GPTQ algorithm, where the quantization is interleaved with merging per layer, without materializing the full half-precision model.\nThe above optimizations allow us to run Panza using the Mistral-7B-Instruct model variants on a single GPU under 15GB RAM."}, {"title": "3 Evaluation Protocol", "content": "3.1 Datasets\nA key challenge of this project is the lack of availability of e-mail datasets, due to the sensitive content of the data. To our knowledge, the only such repository contains the business e-mails of 144 Enron employees. This data was originally released by the Federal Energy Regulatory Committee; the version we use!(Cohen, 2015) is licensed for research. We use the emails of four employees(s.shackleton, k.mann, j.dasovich, and t.jones) with over 400 (English) e-mails each, these are identified by their first names - Sara, Kay, Jeff, and Tana. To avoid influencing the model by explicitly invoking Enron, we changed the name of the corporation and its executives.\nWe use three additional datasets. David, was anonymized manually by its author and donated for research use by an ML researcher with a clear understanding of its release and proposed use; this dataset will be released as part of this project. Two additional datasets, identified as Anon1 and Anon2, were also donated by ML researchers for this project, but will not be released due to the sensitive nature of their contents. Of the seven datasets used, six contain primarily business emails, and one, Anon2, contains primarily personal emails.\nThe test-train split for fine-tuning / PEFT is 80-20%, and only training emails are used to form the RAG database employed at inference time. To assess the quality of the LLM-generated email summarization prompts, used in the Data Playback process, 18 randomly selected emails from David and 20 randomly selected emails from Anon2 were annotated manually with prompts by the authors.\n3.2 Metrics\nPanza uses the text generation capabilities of LLMs for two tasks: to summarize a user's emails to create synthetic prompts for the data playback process, and to generate new emails in the test-time prompts. These two use cases both rely on the ability of LLMs to summarize or rephrase content; the email generation task additionally requires the recall of both general and user-specific knowledge1, and an imitation of a user's personal style.\nThus, we divide the evaluation benchmarks into four broad categories - paraphrasing quality, user-specific knowledge, general knowledge, and style. Of these, the data playback email summaries only need to perform best on the first benchmark.\nFor paraphrasing quality we rely on the BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics, which are standard to measure translation and summarization quality. Both metrics function by counting matching N-grams between the LLM output and one or several 'golden' responses (for the email generation task, the golden response is the email actually written by the user). The BLEU score is a weighted measure of uni-, bi- tri, and quad-grams that match exactly between the output and golden text strings, normalized by the string length; we use an equal weight of 0.25 for each N-gram length. ROUGE reports 1-gram and 2-gram precision, recall, and F1-score, as well as the longest substring precision, recall, and F1-score; in our paper, we use the longest-substring F1 score for maximum contrast with the BLEU metric. For both metrics, we use the Torchmetrics package. Both metrics are computed for each prompt/output combination, and we report the average across all prompts as the overall value. We do not compute either metric on a per-sentence basis, but rather compare n-grams in the full email text after dropping punctuation.\nAs there is no database of user-specific knowledge, we restrict the evaluation of such knowledge to what is contained in the user's emails. This working assumption enables us to use RAG-assisted email generation; here, it further enables to evaluate user-specific knowledge on the same test dataset as for the paraphrasing quality. We do not attempt to decouple knowledge content from paraphrasing quality, rather relying on the overall BLEU and ROUGE scores to reflect the correct imputed information. Note that, unlike the general knowledge desideratum, the user-specific information is better specified\u2014a prompt requesting the user's current address is easier to evaluate than one asking for suggestions for a travel destination-and so the N-gram match is an appropriate measure of quality.\nFor the world knowledge measurement, we rely on the standard six tasks that make up the popular Open-LLM leaderboard evaluation suite (Beeching et al., 2023), which we describe in more detail in Section 4.5. Together, these tasks test the model's ability to perform basic reasoning and factual recall in English, and are used as a broad evaluation of a model's quality.\nTo measure the quality of the style transfer, we use the MAUVE score (Pillutla et al., 2021), which was introduced to measure the degree of similarity between machine-generated text and human-written text. Specifically, the MAUVE score, relies on an estimate of the symmetric K-L divergence between the distribution of the golden labels and the distribution of model outputs. In order to estimate the necessary K-L divergences, the golden and output strings are tokenized and passed through an external LLM (we use GPT-2, as is common), producing text embeddings in a multidimensional space. Higher MAUVE score is correlated with higher difficulty in distinguishing the generated text from human text.\nDiscussion. A major challenge of this project is that the email generation task differs significantly from summarization or translation tasks, in that we generally expect the output email to be longer and more detailed than the prompt used to generate it, thus requiring some improvisation on the part of the model, and making it unlikely for the generated email to be close to the actual one. This is reflected in the BLEU/ROUGE scores, which are substantially lower than what would be considered acceptable for a translation or summarization task. Extensive manual review during the project development phase has shown that, nevertheless, these scores appear highly correlated with output email quality. We provide a few examples in Appendix C. As a rough guideline, human subjects generally agreed that models achieving above 0.2 average BLEU score and above 0.75 MAUVE score produced plausible emails."}, {"title": "4 Experimental Results", "content": "4.1 Baselines\nAs no other e-mail generators exist to our knowledge, we investigate the effect of Data Playback/finetuning by baselining against simply using prompt engineering to elicit personalization. We start with publicly available instruction-finetuned LLMs:\nAs the first baseline, we prompt the models with the following format:\nSystem preamble: Sets the role of the LLM as an email writing assistant.\nUser preamble: General information about the user, such as name, occupation, address.\nInstruction: The actual email writing instruction, as created by an LLM in the first phase of data playback.\nThis baseline (denoted Pretrained) provides a reasonable starting with respect to writing well-structured emails, but without any personalization since the model does not have information about the user's style.\nNext, we test if presenting a few samples of the user's previous emails during inference through a RAG component can provide enough information for the model to imitate the user's style. We select the closest $N_{RAG}$ previous emails, filtered by a relevancy threshold $T_{RAG}$, and add them to the input as an additional preamble. We denote this baseline as Pretrained + RAG.\n4.2 Fine-Tuning via Data Playback\nNext, we employ the complete data playback technique Panza is based on. After generating pairs of (instruction, email), we fine-tune the pre-trained LLM to reconstruct the email given the instruction. We analyze the following regimes: full fine-tuning (FFT) and two PEFT methods: Robust Adaptation (ROSA) and Low-Rank Adaptation (LoRA).\nFor both training and testing, the input is formatted with the same system and user preambles described for the baselines. We also test whether RAG can bring additional improvements.\nFurthermore, we explore if the model can learn how to better leverage previous e-mails in RAG by presenting the same type of augmented prompt during fine-tuning, parameterized by the number of closest emails $N_{RAG}$ and the relevancy threshold $T_{RAG}$. Additionally, to make the model robust to the absence of similar emails, we have a $P_{RAG}$ chance of not using any similar emails for a particular instruction during fine-tuning, even if there are matches in the database. This approach adapts Zhang et al. (2024) to our setting and is denoted as RAFT (Retrieval-Augmented Fine-Tuning).\nHyperparameter tuning. We found fine-tuning, and especially PEFT, to be highly sensitive to learning rate and the number of training epochs. To find suitable hyperparameters, we first used a greedy grid search approach with learning rates ranging from 10\u22126 to 10\u22124 and epoch ranges from 1 to 9, batch sizes of 8 and 16, and using the BLEU metric as the proxy for overall model quality. We used the realistic, non-anonymized Anon1 and Anon2 datasets for hyperparameter tuning and chose values that worked well for both. Overall, we found that learning rate of 10\u20135, batch size of 8, and 5 epochs (3 for FFT) to work well across all base models and finetuning styles. We then tuned these parameters further for the other users. The final values for all users are presented in Appendix E.\n4.3 Results Across Methods and Models\nWe find that all fine-tuning regimes outperform the Pretrained + RAG baselines by a large margin, adapting to the user's writing style. The results are illustrated in Figure 2. (We present a qualitative comparison of the generated emails in Appendix C; using leading closed LLMs yielded similarly poor results.) Interestingly, RoSA performs on par with FFT, consistently surpassing LoRA, especially in terms of MAUVE score. The same trend is observed for all the backbones we trained, and for all users (please see Appendix D for full results).\nIn Figure 3 we study the effect of RAG on models fine-tuned with RoSA. We observe that, although RAG clearly helps for the pretrained baseline, it reduces the average BLEU score for the ROSA fine-tuned model (second group), but may slightly increase the average MAUVE score. This is not desirable, as BLEU score (relative to the ground-truth email) is a closer measure of content accuracy than MAUVE. Based on analyzing individual samples, we hypothesize that this may be caused by the model re-using the RAG context too aggressively at deployment time, leading to emails that are very similar to past emails (so, preserving style), but less accurate in terms of content (leading to a lower average BLEU score). We observe that we can overcome this issue by introducing RAG during fine-tuning itself, i.e. using RAFT: when previous emails are presented during fine-tuning, the model \u201clearns\u201d to ignore them if irrelevant.\nIn Figure 4 we show that similar performance levels can be obtained by fine-tuning various LLM backbones, when performing FFT or ROSA-RAFT across Mistral-7B-Instruct-v0.2, Llama3-8B-Instruct, and Phi-3-mini-4k-instruct. While the achieved BLEU scores are very similar across models, the only significant difference is the higher MAUVE score achieved by the Llama3 model.\n4.4 Style Evaluation\nRecall that MAUVE measures the gap between machine-generated text and human-written text. Above, we reported the MAUVE scores on the test emails coming from the same user the model was trained for. Next, we do a pairwise comparison, evaluating models trained for different users on the test data of all the other users. This focuses precisely on how well style is reflected in generated emails. In Figure 5, we see that each model produces a high MAUVE score (0.6 to 1.0) only for the test emails of the user it was trained for, while it has close to 0 MAUVE score on any other user. In terms of BLEU/ROUGE score, all models have essentially the same performance on the test set of any given user, suggesting all models have similar paraphrasing capacity to express the given instruction, but each does it in the style of the user it was trained for. We therefore conclude that Data Playback is highly effective in terms of this metric.\n4.5 Maintaining General Knowledge\nDespite the primary objective of Panza being personalization, it is desirable for the assistant to retain the general knowledge acquired during pre-training and supervised fine-tuning. To assess the extent to which the model preserves its general knowledge post-personalization, we evaluated it using the few-shot setup of the popular Open-LLM Leaderboard evaluation suite (Beeching et al., 2023)."}, {"title": "4.6 The Impact of Compression", "content": "We now evaluate the memory-efficient version of Panza (described in Section 2.4), which requires less than 15GB of GPU memory, and 12GB CPU RAM. Particularly, we first examine how quantizing each component of the pipeline affects the results. Then, we show that quantizing all the components at the same time can achieve reasonable results while being more memory efficient than the non-quantized version.\nWe fine-tune the Mistral-Instruct-7b-v0.2 model using RoSA on the David dataset, and use the same three BLEU, ROUGE, and MAUVE metrics for evaluation. For each experiment, we select the best of 5 and 7 epochs and learning rates 10-4 and 10-5 in terms of BLEU score. We perform both summarization and fine-tuning using the same Mistral model (either half-precision or 4-bit quantized), since we find that the quantization methods perform better in this case compared to Llama-3 summaries.\nAs described in Section 2.4, quantization can be alternatively applied to 1) RoSA's base weights, 2) RoSA's gradient accumulators, and 3) the final model used for inference. In addition, the email summarization model should also be compressed. "}, {"title": "5 Limitations", "content": "The techniques provided by Panza provide a significant improvement, across a mix of metrics, with respect to on-device personalization, and, we believe, a compelling case of practically useful LLM personalization. Yet, more work should be done to be able to accurately measure LLM's performance on open-ended tasks such as e-mail generation, in particular with regard to measuring, and improving, its representation of the personal data of the user. Additionally, as a proof-of-concept, Panza has currently only been tested in Euro/US-centered English, leaving open the creation of such tools for other languages and cultures. Our techniques should be easily extensible to this case."}, {"title": "6 Ethical Considerations", "content": "We foresee two categories of risks from presenting a project like Panza. First, a malicious user who has access to a sufficient number of third party's emails (for instance, the ones received from that party) can use a tool like Panza to create a credible imitation of that party. Second, a style-matching tool like Panza can be used to create derivative work that can credibly be misrepresented as original (for instance, for completing school assignments). Panza's low price and accessibility may aid in such misuse; however, overall, the existence of public LLM models and finetuning methods already allows such misuse to occur."}, {"title": "A Prompt Engineering", "content": "In the first phase of the data playback, we generate summaries with the help of the following summarization prompt:\n''''''Summarize the following email that I wrote, in an imperative form, in one or two or maximum three sentences, and make sure to include relevant information, without copying the email content itself. The summary should look like an instruction directing someone to write the same email, and start with Instruction:\nHere is the email text:\n{email}''''''\nThen, to generate emails, we give the instructions back to the model using the following format:\n\u0648\u0648\u0648\u0648\u0648\u0648\n{system preamble}\n{user preamble}\n{rag prompt} # [optional]\nInstruction: {instruction}\n\u0648\u0648\u0648\u0648\u0648\u0648\nThe system preamble sets the role of the LLM as follows:\n''''''Your role is that of a helpful automated email assistant. I will provide you with a short instruction, and you have to write a well-formed email in my style following this instruction. Be sure to follow my email writing style! In case you see a nonsensical instruction, you should not reply with an email, but with the expression \"Sorry, but I don't get it.\u201d ''''''\nThe user preamble provides optional information about the user. For the five users in our experiments, we set it to \"My name is \u00a1First Name\u00bf \u00a1Last Name\u00bf\". Generally, it can be filled with any relevant information about the user, for instance:\n'"}, {"title": "B Summarization", "content": "In Table 5 we measure the summarization quality, by comparing against golden summaries from the users David and Anon1. In Table 6 we inspect several generated summaries for each model."}, {"title": "C Panza Generated Emails", "content": "Table 7 compares emails generated by models fine-tuned for different users, highlighting their style differences."}, {"title": "D Fine-Tuning Performance", "content": "We show complete results for all models, across every user in Tables 8 and 12 (Meta-Llama-3-8B-Instruct), Tables 9 and 13 (Mistral-7B-Instruct-v0.2), and Tables 10 and 14 (Phi-3-mini-4k-instruct). To compare between different models, we report the average results over all users in Table 11. We find that for all the models, data playback successfully incorporates the user's style. In Figure 6 we perform a pairwise style comparison for model trained on different users and the test emails of all the other users. This shows MAUVE score successfully captures style differences, while BLEU/ROUGE scores are limited to measuring the paraphrasing capacity and can't distinguish style."}, {"title": "E Hyperparameter Tuning", "content": "E.1 Inference\nWe perform generation using beam search with a temperature T = 0.7, number of top probability tokens to keep top_k = 50 and nucleus sampling parameter top-p = 0.7.\nE.2 Fine-Tuning\nWe perform a thorough hyperparameter tuning for every backbone, and every user over learning rate and number of epochs. For FFT we experiment with {1, 3, 5} epochs and learning rates between [10-3, 10-7]. For PEFT methods (RoSA, LoRA), we experiment with {1, 3, 5, 7, 9} epochs and learning rates between [10-\u00b3, 10-7]. Next, we present the best configuration found for each model, method and user.\nMeta-Llama-3-8B-Instruct For users David and Jeff: FFT for 3 epochs with a learning rate of 10\u20135; PEFT for 7 epochs with a learning rate of 10\u20135. For users Kay, Sara and Tana: FFT for 3 epochs with a learning rate of 10\u20134 and PEFT for 7 epochs with a learning rate of 10\u20134. For users Anon1, Anon2: FFT for 3 epochs with a learning rate of 10-5 and PEFT for 7 epochs with a learning rate of 10-4.\nMistral-7B-Instruct-v0.2 For users David and Jeff, Anon2: FFT for 3 epochs with a learning rate of 10-5; PEFT for 7 epochs with a learning rate of 10-4. For users Kay, Sara and Tana: FFT for 3 epochs with a learning rate of 10-5 and PEFT for 7 epochs with a learning rate of 10-5. For Anon1: FFT for 3 epochs with a learning rate of 10\u20135; ROSA for 7 epochs with a learning rate of 10\u20135, LORA for 7 epochs with a learning rate of 10\u20134.\nPhi-3-mini-4k-instruct Same hyperparameters for every users: FFT for 3 epochs with a learning rate of 10-4; PEFT for 7 epochs wth a learning rate of 10-4.\nE.3 RAG and RAFT\nWe set fixed values of the number of retrieval emails NRAG = 2 during RAFT, and NRAG = 3 at inference with RAG. We pick relatively low values, as every email retrieved increases the input length, thus the memory consumption. We experimented with larger NRAG for the Pretrained baseline, without significantly different results. For RAFT, we use PRAG = 0.55 chance to include relevant emails in the prompt. We use the same relevancy threshold TRAG = 0.2, tuned on the private users Anon1 and Anon2 to encourage recall rather than precision. This can retrieve irrelevant emails, but prevents missing important items from the user's history. Furthermore, RAFT learns how to better filter irrelevant information in case it is added to the prompt."}]}