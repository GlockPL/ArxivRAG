{"title": "Panza: A Personalized Text Writing Assistant\nvia Data Playback and Local Fine-Tuning", "authors": ["Armand Nicolicioiu", "Eugenia Iofinova", "Eldar Kurtic", "Mahdi Nikdan", "Andrei Panferov", "Ilia Markov", "Nir Shavit", "Dan Alistarh"], "abstract": "The availability of powerful open-source large\nlanguage models (LLMs) opens exciting use-\ncases, such as automated personal assistants\nthat adapt to the user's unique data and de-\nmands. Two key desiderata for such assistants\nare personalization-in the sense that the assis-\ntant should reflect the user's own style-and\nprivacy-in the sense that users may prefer to\nalways store their personal data locally, on their\nown computing device. In this application pa-\nper, we present a new design for such an au-\ntomated assistant, for the specific use case of\npersonal assistant for email generation, which\nwe call Panza. Specifically, Panza can be both\ntrained and inferenced locally on commodity\nhardware, and is personalized to the user's writ-\ning style. Panza's personalization features are\nbased on a new technique called data play-\nback, which allows us to fine-tune an LLM\nto better reflect a user's writing style using lim-\nited data. We show that, by combining effi-\ncient fine-tuning and inference methods, Panza\ncan be executed entirely locally using lim-\nited resources-specifically, it can be executed\nwithin the same resources as a free Google\nColab instance. Finally, our key methodolog-\nical contribution is a careful study of evalua-\ntion metrics, and of how different choices of\nsystem components (e.g. the use of Retrieval-\nAugmented Generation or different fine-tuning\napproaches) impact the system's performance.", "sections": [{"title": "Introduction", "content": "An automated personal assistant is a software ap-\nplication that can help the user with various repeti-\ntive tasks such as email, writing, or summarization.\nLarge Language Models (LLMs) are natural candi-\ndates for implementing personal assistants, as they\ncan provide remarkably good results on such gener-\native tasks. At the same time, many highly-capable\nLLMs reside in the cloud, and can only be accessed\nvia an API. Specifically, this makes it expensive or\nimpossible to support certain natural features for\nautomated personal assistants, namely:\n1. Model personalization that is, customizing\nthe model to the specific individual's tone,\npreferences, and personal history;\n2. Privacy protection, that is, allow the model\nto have access to highly personal informa-\ntion of a caliber that-unlike corporate data\u2014\nmost people would not agree to share even\nif promised that the data is protected by the\nprovider's cloud.\nA natural approach to addressing these con-\nstraints would be to execute these models locally,\non the user's own data and hardware. However,\nthis poses obvious challenges both in terms of\ndata ingestion-that is, getting user data into a for-\nmat that can be used to successfully train or fine-\ntune an LLM-and in terms of hardware efficiency-\nspecifically, because fine-tuning or even inferenc-\ning over a capable LLM on user data may be tech-\nnically impossible if done naively.\nIn this application paper, we take up this chal-\nlenge for the limited, but interesting, case of de-\nsigning and implementing a fully-local automated\nemail writing assistant we call Panza, whose pur-\npose is to generate relevant messages in the user's\nown style, given a user prompt, as well as access\nto a set of previously-sent user emails. Our main\nfinding is that it is possible to obtain a capable as-\nsistant starting from existing pre-trained models\nsuch as Llama-3-8B (Meta, 2024), and that this\napplication can be executed entirely locally, on a\nsingle machine with a commodity GPU, or even\nin a CPU-only environment. The overall structure"}, {"title": "Method", "content": "The Panza design, described in Figure 1, requires a\npre-trained, possibly instruction-tuned, LLM and\nof a set of emails sent by the user. Both the LLM\nand the emails have dual use. First, the pre-trained\nLLM is used to rephrase the \"raw\" user emails in\nthe form of instructions, which will be used for\ndata playback (described below). Then, the LLM\nitself is going to be fine-tuned on these instructions,\nresulting on the Panza personalized model. Inde-\npendently, the emails are used to generate a RAG\ndatabase, employed at deployment time.\nData playback is the core personalization technique\nbehind Panza. The intuition behind data playback\nis that we would like to \"reduce\u201d the problem of\ncreating emails in the user's style to a specific in-\nstance of instruction-tuning. In a practical usage\nscenario, the user query would come in the form\nof a standard instruction, e.g. \u201cWrite an email to\nJoanne to set up a meeting on Wednesday.\u201d, and the\n\u201ccorrect\u201d answer would be such an email written in\nthe user's style. To induce this behavior from the\nLLM, data playback works in two steps:\n1. First, we use a pre-trained LLM to rephrase\neach email into a targeted instruction, contain-\ning just enough information for an assistant to\nwrite the original email.\n2. Second, we fine-tune the LLM (possibly the\nsame one) on the (instruction, email) pairs\nobtained in the first step, with a standard train-\ning objective which induces the LLM to re-\nconstitute the original email as a response\nto the instruction. Optionally, we imple-\nment a training-time RAG component, which\nretrieves query-related emails and provides\nthem as context to the LLM at training time.\nThis is similar to retrieval-augmented fine-\ntuning (RAFT) (Zhang et al., 2024).\nAt the end of these two steps, we have obtained\na personalized LLM which can respond to short\nuser queries by writing targeted emails that should\nfollow the user's style. Next, we describe how this\nmodel is deployed.\nIn the absence of computational or memory con-\nstraints, we perform full fine-tuning (FFT) of the"}, {"title": "Deployment", "content": "to the GPU memory constraint, we also assume ac-\ncess to only 12GB of CPU memory, since that is\nthe case for free Colab instances. We take a simi-\nlar approach as above, while quantizing multiple\ncomponents of the pipeline to 4 bits per parameter,\ndetailed below.\n\u2022 Quantized base weights. We store the\nweights of the base model in doubly-quantized\n4 bit precision (Dettmers et al., 2023), reduc-\ning the model size in memory by roughly 4x.\n\u2022 Quantized accumulators. As part of the\nsparse adapter's mask generation, RoSA accu-\nmulates gradients of the base weights on CPU,\nviolating the 12GB CPU memory restriction.\nTo remedy this issue, we change the precision\nof the accumulators to 4 bits using a uniform\nquantization with group size 128.\n\u2022 Adapter merging. To merge the half-\nprecision ROSA adapters and the 4-bit base\nweights, we present a new accurate merg-\ning mechanism; for each layer, we dequan-\ntize the base weights, add adapters, and quan-\ntize the result back to 4 bits using the GPTQ\nmethod (Frantar et al., 2022). Our key contri-\nbution is an adapter-aware implemetation of\nthe GPTQ algorithm, where the quantization\nis interleaved with merging per layer, without\nmaterializing the full half-precision model.\nThe above optimizations allow us to run Panza\nusing the Mistral-7B-Instruct model vari-\nants on a single GPU under 15GB RAM."}, {"title": "Evaluation Protocol", "content": "A key challenge of this project is the lack of avail-\nability of e-mail datasets, due to the sensitive con-\ntent of the data. To our knowledge, the only such\nrepository contains the business e-mails of 144\nEnron employees. This data was originally re-\nl released by the Federal Energy Regulatory Commit-\ntee; the version we use!(Cohen, 2015) is licensed\nfor research. We use the emails of four employ-\nees(s.shackleton, k.mann, j.dasovich, and t.jones)\nwith over 400 (English) e-mails each, these are\nidentified by their first names - Sara, Kay, Jeff, and\nTana. To avoid influencing the model by explic-\nitly invoking Enron, we changed the name of the\ncorporation and its executives.\nWe use three additional datasets. David, was\nanonymized manually by its author and donated\nfor research use by an ML researcher with a clear\nunderstanding of its release and proposed use; this\ndataset will be released as part of this project.\nTwo additional datasets, identified as Anon1 and\nAnon2, were also donated by ML researchers for\nthis project, but will not be released due to the sensi-\ntive nature of their contents. Of the seven datasets\nused, six contain primarily business emails, and\none, Anon2, contains primarily personal emails.\nThe test-train split for fine-tuning / PEFT is 80-\n20%, and only training emails are used to form\nthe RAG database employed at inference time. To\nassess the quality of the LLM-generated email sum-\nmarization prompts, used in the Data Playback pro-\ncess, 18 randomly selected emails from David and\n20 randomly selected emails from Anon2 were an-"}, {"title": "Metrics", "content": "Panza uses the text generation capabilities of\nLLMs for two tasks: to summarize a user's emails\nto create synthetic prompts for the data playback\nprocess, and to generate new emails in the test-time\nprompts. These two use cases both rely on the\nability of LLMs to summarize or rephrase content;\nthe email generation task additionally requires the\nrecall of both general and user-specific knowledge\u00b9,\nand an imitation of a user's personal style.\nThus, we divide the evaluation benchmarks into\nfour broad categories - paraphrasing quality, user-\nspecific knowledge, general knowledge, and style.\nOf these, the data playback email summaries only\nneed to perform best on the first benchmark.\nFor paraphrasing quality we rely on the BLEU (Papineni et al., 2002) and\nROUGE (Lin, 2004) metrics, which are standard\nto measure translation and summarization quality.\nBoth metrics function by counting matching N-\ngrams between the LLM output and one or several\n'golden' responses (for the email generation task,\nthe golden response is the email actually written\nby the user). The BLEU score is a weighted mea-\nsure of uni-, bi- tri, and quad-grams that match\nexactly between the output and golden text strings,\nnormalized by the string length; we use an equal\nweight of 0.25 for each N-gram length. ROUGE\nreports 1-gram and 2-gram precision, recall, and\nF1-score, as well as the longest substring preci-\nsion, recall, and F1-score; in our paper, we use the\nlongest-substring F1 score for maximum contrast\nwith the BLEU metric. For both metrics, we use the\nTorchmetrics package. Both metrics are computed\nfor each prompt/output combination, and we report\nthe average across all prompts as the overall value.\nWe do not compute either metric on a per-sentence\nbasis, but rather compare n-grams in the full email\ntext after dropping punctuation.\nAs there is no database\nof user-specific knowledge, we restrict the evalua-\ntion of such knowledge to what is contained in the\nuser's emails. This working assumption enables\nus to use RAG-assisted email generation; here, it\nfurther enables to evaluate user-specific knowledge\non the same test dataset as for the paraphrasing\nquality. We do not attempt to decouple knowledge\ncontent from paraphrasing quality, rather relying on\nthe overall BLEU and ROUGE scores to reflect the\ncorrect imputed information. Note that, unlike the\ngeneral knowledge desideratum, the user-specific\ninformation is better specified\u2014a prompt request-\ning the user's current address is easier to evalu-\nate than one asking for suggestions for a travel\ndestination-and so the N-gram match is an appro-\npriate measure of quality.\nFor the world knowledge\nmeasurement, we rely on the standard six tasks\nthat make up the popular Open-LLM leaderboard\nevaluation suite (Beeching et al., 2023), which we\ndescribe in more detail in Section 4.5. Together,\nthese tasks test the model's ability to perform basic\nreasoning and factual recall in English, and are\nused as a broad evaluation of a model's quality.\nTo measure the quality of the style\ntransfer, we use the MAUVE score (Pillutla et al.,\n2021), which was introduced to measure the de-\ngree of similarity between machine-generated text\nand human-written text. Specifically, the MAUVE\nscore, relies on an estimate of the symmetric K-L\ndivergence between the distribution of the golden\nlabels and the distribution of model outputs. In\norder to estimate the necessary K-L divergences,\nthe golden and output strings are tokenized and\npassed through an external LLM (we use GPT-2,\nas is common), producing text embeddings in a\nmultidimensional space. Higher MAUVE score is"}, {"title": "Experimental Results", "content": "As no other e-mail generators exist to our\nknowledge, we investigate the effect of Data Play-\nback/finetuning by baselining against simply using\nprompt engineering to elicit personalization. We\nstart with publicly available instruction-finetuned\nLLMs:\nand\nAs the first\nbaseline, we prompt the models with the following\nformat:\n1. System preamble: Sets the role of the LLM as\nan email writing assistant.\n2. User preamble: General information about the\nuser, such as name, occupation, address.\n3. Instruction: The actual email writing instruc-\ntion, as created by an LLM in the first phase\nof data playback.\nThis baseline (denoted Pretrained) provides a\nreasonable starting with respect to writing well-\nstructured emails, but without any personalization\nsince the model does not have information about\nthe user's style.\nNext, we test if presenting a few samples of the\nuser's previous emails during inference through a\nRAG component can provide enough information\nfor the model to imitate the user's style. We select"}, {"title": "Fine-Tuning via Data Playback", "content": "Next, we employ the complete data playback tech-\nnique Panza is based on. After generating pairs of\n(instruction, email), we fine-tune the pre-trained\nLLM to reconstruct the email given the instruction.\nWe analyze the following regimes: full fine-tuning\n(FFT) and two PEFT methods: Robust Adaptation\n(ROSA) and Low-Rank Adaptation (LoRA).\nFor both training and testing, the input is for-\nformatted with the same system and user preambles\ndescribed for the baselines. We also test whether\nRAG can bring additional improvements.\nFurthermore, we explore if the model can learn\nhow to better leverage previous e-mails in RAG\nby presenting the same type of augmented prompt\nduring fine-tuning, parameterized by the number\nof closest emails $N_{RAG}$ and the relevancy thresh-\nold $T_{RAG}$. Additionally, to make the model ro-\nbust to the absence of similar emails, we have a\n$p_{RAG}$ chance of not using any similar emails for\na particular instruction during fine-tuning, even\nif there are matches in the database. This ap-\nproach adapts Zhang et al. (2024) to our setting\nand is denoted as RAFT (Retrieval-Augmented\nFine-Tuning).\nWe found fine-tuning,\nand especially PEFT, to be highly sensitive to learn-\ning rate and the number of training epochs. To find\nsuitable hyperparameters, we first used a greedy\ngrid search approach with learning rates ranging\nfrom $10^{-6}$ to $10^{-4}$ and epoch ranges from 1 to 9,\nbatch sizes of 8 and 16, and using the BLEU metric\nas the proxy for overall model quality. We used\nthe realistic, non-anonymized Anon1 and Anon2\ndatasets for hyperparameter tuning and chose val-\nues that worked well for both. Overall, we found\nthat learning rate of $10^{\u20135}$, batch size of 8, and 5\nepochs (3 for FFT) to work well across all base\nmodels and finetuning styles. We then tuned these\nparameters further for the other users. The final\nvalues for all users are presented in Appendix E."}, {"title": "Results Across Methods and Models", "content": "We find that all fine-tuning regimes outperform\nthe Pretrained + RAG baselines by a large margin,\nadapting to the user's writing style. The results are\nillustrated in Figure 2. (We present a qualitative"}, {"title": "Style Evaluation", "content": "Recall that MAUVE measures the gap between\nmachine-generated text and human-written text.\nAbove, we reported the MAUVE scores on the\ntest emails coming from the same user the model\nwas trained for. Next, we do a pairwise compari-\nson, evaluating models trained for different users\non the test data of all the other users. This focuses\nprecisely on how well style is reflected in gener-\nated emails. In Figure 5, we see that each model\nproduces a high MAUVE score (0.6 to 1.0) only\nfor the test emails of the user it was trained for,\nwhile it has close to 0 MAUVE score on any other\nuser. In terms of BLEU/ROUGE score, all models\nhave essentially the same performance on the test\nset of any given user, suggesting all models have\nsimilar paraphrasing capacity to express the given\ninstruction, but each does it in the style of the user"}, {"title": "Maintaining General Knowledge", "content": "Despite the primary objective of Panza being per-\nsonalization, it is desirable for the assistant to retain\nthe general knowledge acquired during pre-training\nand supervised fine-tuning. To assess the extent to\nwhich the model preserves its general knowledge\npost-personalization, we evaluated it using the few-\nshot setup of the popular Open-LLM Leaderboard\nevaluation suite (Beeching et al., 2023)."}, {"title": "The Impact of Compression", "content": "We now evaluate the memory-efficient version of\nPanza (described in Section 2.4), which requires\nless than 15GB of GPU memory, and 12GB CPU\nRAM. Particularly, we first examine how quan-\ntizing each component of the pipeline affects the\nresults. Then, we show that quantizing all the com-"}, {"title": "Limitations", "content": "The techniques provided by Panza provide a signif-\nicant improvement, across a mix of metrics, with\nrespect to on-device personalization, and, we be-\nlieve, a compelling case of practically useful LLM\npersonalization. Yet, more work should be done to\nbe able to accurately measure LLM's performance\non open-ended tasks such as e-mail generation, in\nparticular with regard to measuring, and improv-\ning, its representation of the personal data of the\nuser. Additionally, as a proof-of-concept, Panza\nhas currently only been tested in Euro/US-centered"}, {"title": "Ethical Considerations", "content": "We foresee two categories of risks from presenting\na project like Panza. First, a malicious user who has\naccess to a sufficient number of third party's emails\n(for instance, the ones received from that party) can\nuse a tool like Panza to create a credible imitation of\nthat party. Second, a style-matching tool like Panza\ncan be used to create derivative work that can cred-\nibly be misrepresented as original (for instance, for\ncompleting school assignments). Panza's low price\nand accessibility may aid in such misuse; however,\noverall, the existence of public LLM models and\nfinetuning methods already allows such misuse to\noccur."}, {"title": "Prompt Engineering", "content": "In the first phase of the data playback, we generate\nsummaries with the help of the following summa-\nrization prompt:\n''''''Summarize the following email that\nI wrote, in an imperative form, in one\nor two or maximum three sentences, and\nmake sure to include relevant informa-\ntion, without copying the email content\nitself. The summary should look like an\ninstruction directing someone to write\nthe same email, and start with Instruc-\ntion:\nHere is the email text:\n{email}''''''\nThen, to generate emails, we give the instruc-\ntions back to the model using the following format:\n\u0648\u0648\u0648\u0648\u0648\u0648\n{system preamble}\n{user preamble}\n{rag prompt} # [optional]\nInstruction: {instruction}\n\u0648\u0648\u0648\u0648\u0648\u0648\nThe system preamble sets the role of the LLM\nas follows:\n''''''Your role is that of a helpful auto-\nmated email assistant. I will provide you\nwith a short instruction, and you have to\nwrite a well-formed email in my style fol-\nlowing this instruction. Be sure to follow\nmy email writing style! In case you see\na nonsensical instruction, you should not\nreply with an email, but with the expres-\nsion \u201cSorry, but I don't get it.\u201d ''''''\nThe user preamble provides optional informa-\ntion about the user. For the five users in our ex-\nperiments, we set it to \u201cMy name is \u00a1First Name\u00bf\n\u00a1Last Name\u00bf\u201d. Generally, it can be filled with any\nrelevant information about the user, for instance:\n'"}, {"title": "Summarization", "content": "In Table 5 we measure the summarization quality,\nby comparing against golden summaries from the\nusers David and Anon1. In Table 6 we inspect\nseveral generated summaries for each model."}, {"title": "Panza Generated Emails", "content": "Table 7 compares emails generated by models fine-\ntuned for different users, highlighting their style\ndifferences."}, {"title": "Fine-Tuning Performance", "content": "We show complete results for all models, across\nevery user in Tables 8 and 12 (Meta-Llama-3-8B-\nInstruct), Tables 9 and 13 (Mistral-7B-Instruct-\nv0.2), and Tables 10 and 14 (Phi-3-mini-4k-\ninstruct). To compare between different models,\nwe report the average results over all users in Ta-\nble 11. We find that for all the models, data play-\nback successfully incorporates the user's style. In\nFigure 6 we perform a pairwise style comparison\nfor model trained on different users and the test\nemails of all the other users. This shows MAUVE\nscore successfully captures style differences, while\nBLEU/ROUGE scores are limited to measuring the\nparaphrasing capacity and can't distinguish style."}]}