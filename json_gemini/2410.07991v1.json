{"title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets", "authors": ["Tommaso Giorgi", "Lorenzo Cima", "Tiziano Fagni", "Marco Avvenuti", "Stefano Cresci"], "abstract": "The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.", "sections": [{"title": "Introduction", "content": "The proliferation of hateful and toxic speech on online social platforms is among the most severe threats to inclusive online spaces (Mathew et al. 2019). The exponential increase in user-generated content has made manual hate speech detection impractical, driving the development of automated systems (Garg et al. 2023). However, the effectiveness of automated detectors is heavily contingent upon the quality of their training data, which is typically human-labeled. Challenges arise as hate speech detection is inherently subjective: what one individual may perceive as offensive, another might consider acceptable or even normal. This subjectivity is shaped by the annotators' backgrounds, experiences, and cultural contexts. As such, socio-demographic attributes such as age, gender, race, and others, can influence how annotators interpret and label content, possibly leading to biases (Davani et al. 2023). In turn, biases may seep into the automated detection systems trained on such data, causing severe consequences such as the unwarranted removal of non-hateful content or the disproportionate targeting of marginalized groups (Nogara et al. 2025). Large language models (LLMs) have recently extended the set of AI tools in support of platform administrators. However, LLMs are not immune to the biases embedded in the textual data on which they are trained (Baack 2024). Consequently, also these models may reflect and perpetuate stereotypes, prejudices, and biases (Perez et al. 2023). Despite the development of various techniques aimed at reducing LLM biases, this remains an outstanding challenge (Ouyang et al. 2022). Interestingly, the capacity of LLMs to produce subjective outputs can also serve beneficial purposes. LLMs can be personalized and deployed as actors in agent-based simulations of online human interactions (T\u00f6rnberg et al. 2023), which represents a promising way to compensate for the limited availability of platform data in the post-API age. By accurately reproducing certain human biases, persona-based LLMs could lay the groundwork for the next generation of AI-driven social simulations. For these reasons, investigating biases in both human hate speech annotations and persona-based LLMs is a task of great theoretical and practical relevance (Tseng et al. 2024). Despite significant efforts, knowledge of human biases in hate speech annotations is still limited. First, many existing studies rely on small datasets. This results in works that only consider a narrow set of socio-demographic attributes, stud-"}, {"title": "Research focus.", "content": "Motivated by this knowledge gap, we investigate the biases that human annotators and persona-based LLMs exhibit in a hate speech annotation task. We analyze an extensive and publicly available dataset containing 136K hate speech labels assigned by 8K human annotators. For each hateful message, we also analyze the target of the attack. Targets and annotators are described by ten socio-demographic attributes, as summarized in Table 1. Our analysis allows us to answer the following research questions:\nRQ1a Are human annotators more sensitive to hate speech directed at individuals or groups sharing their own socio-demographic attributes?\nRQ1b How do annotators with a specific sociodemographic attribute differ in their labeling of hate speech compared to annotators without that attribute?\nIn RQ1a, we hypothesize that identification with the target group (i.e., the annotator's in-group) may heighten the annotator's sensitivity to hate, leading them to flag such content more often (Sachdeva et al. 2022b). Conversely, when the target belongs to an out-group, the annotator's perception of the hate may be less acute. RQ1b broadens the analysis by examining whether certain groups systematically label content differently than others, in relation to the targets of the hate. This analysis reveals possible disparities in how hate speech is perceived across diverse socio-demographic backgrounds.\nRQ2a How do persona-based LLMs with a specific sociodemographic attribute differ in their labeling of hate speech compared to LLMs without that attribute?\nRQ2b Are persona-based LLM biases the same as those reported by the human annotators they impersonate?\nWe move our focus from human biases to LLMs'. RQ2a mirrors the analysis in RQ1b by investigating the biases exhibited by persona-based LLMs. Finally, RQ2b determines whether persona-based LLMs replicate the biases observed in human annotators with matching attributes, thereby assessing the extent to which LLMs reflect or diverge from human patterns of bias in hate speech annotation."}, {"title": "Related Work", "content": "Annotator diversity and data quality. Numerous studies have examined how the characteristics of human annotators impact the quality of training datasets. Geva, Goldberg, and Berant (2019) demonstrated that datasets generated by a small number of high-quality crowd workers lack diversity, which affects model generalization. Similarly, Wich, Al Kuwatly, and Groh (2020) identified annotator bias in hate speech detection systems and proposed using community detection algorithms to group annotators and mitigate this bias. They also showed that socio-demographic attributes significantly affect bias in hate speech datasets (Al Kuwatly, Wich, and Groh 2020). Parmar et al. (2023) found biases even in the instructions used for crowdsourcing data annotations tasks. Finally, Sap et al. (2022) highlighted that annotator identity and beliefs affect toxicity ratings, stressing the need for contextualizing labels with social variables, as done in our work.\nBias in human hate speech annotations. The impact that biased datasets have on the reliability and fairness of automated hate speech detection systems sparked much research on the issue (Garg et al. 2023). Waseem and Hovy (2016) showed that cultural and socio-economic backgrounds lead to variability hate speech annotations. Inherent prejudices further skew labels, as human annotators may be more sensitive to hate speech against familiar groups while underestimating it against less familiar ones. Others found that socio-demographic factors such as age, gender, race and ethnicity, education, and English proficiency, influence annotation outcomes, with younger and minority annotators more likely to label content as hate speech (Al Kuwatly, Wich, and Groh 2020; Davani et al. 2023). Additionally, annotator beliefs and demographics can introduce further inconsistencies in crowdsourced annotations (Hettiachchi et al. 2023). Although much research has explored socio-demographic biases in hate speech annotations (Garg et al. 2023), most studies focused on a limited set of attributes, primarily race and ethnicity, gender, and age. Moreover, none investigated the interplay between annotator and target attributes.\nDemographic alignment of LLMs. LLM prompting has been used to experiment with persona-based and role-playing models, to test the alignment of LLM predictions with human opinions (Tseng et al. 2024). Studies involved replicating famous cognitive and social experiments (Aher, Arriaga, and Kalai 2023; Srivastava et al. 2023), as well as assessing agreement and personalization capabilities. Among the existing works, Beck et al. (2024) and Hu and Collier (2024) measured the impact of socio-demographic prompting on model performance, showing that it can enhance zero-shot learning in subjective tasks. Additional studies explored the personalities (e.g., Big5, MBTI) exhibited by LLMs when prompted to emulate certain human socio-psychological traits (Rao, Leung, and Miao 2023; La Cava, Costa, and Tagarelli 2024). Results showed marked differences between models and a moderate predisposition to personalization and psychological prompting. However, Santurkar et al. (2023) measured the alignment between human and LLM opinions across a wide array of demographic groups and topics, finding substantial misalignment. The misalignment persisted even after explicitly steering the LLMs towards particular groups. Similarly, Lee, An, and Thorne (2023) investigated the alignment of instruction-"}, {"title": "Dataset", "content": "The MEASURing Hate SPEECH corpus (Sachdeva et al. 2022a) is an extensive and publicly available dataset containing 135,556 hate speech labels assigned by 8,472 human annotators to 39,565 distinct social media posts. The posts were collected between March and August 2019 from three major online platforms: Twitter, Reddit, and YouTube. In addition to hate labels, the dataset includes rich socio-demographic information about both the annotators and the targets of the hateful messages. This socio-demographic information spans ten attributes, each with multiple values, resulting in a fine-grained and nuanced resource for in-depth studies of how socio-demographic characteristics influence perceptions of hate. The labels were obtained via a data annotation task on Amazon Mechanical Turk, where annotators were asked to classify each post as containing hate (hate label), not containing hate (non-hate), or whether they were uncertain (maybe). On average, each annotator labeled ~17 posts (\\(\\sigma = 3.8\\)) and each post has been labeled by ~3.5 different annotators (\\(\\sigma = 27.0\\)).\nAs shown in Table 1, annotators are described by eight attributes while hate targets by seven attributes. Five attributes overlap between annotators and targets, while the remaining are only available either for annotators (education, ideology, income) or targets (disability, origin). The distribution of annotators across the available attributes is almost uniform and close to the total number of annotators, meaning that each annotator provided information for the majority of attributes. Appendix Table T1 however shows that certain subgroups of annotators are more represented than others. Instead, the distribution of hate targets is very skewed, reflecting the tendency for online hate to be directed towards specific socio-demographic characteristics, such as race and gender. Nonetheless, the availability of a considerable number of annotations also for the least frequent hate targets enables statistically significant analyses across all subgroups.\nThe dataset follows the FAIR principles. It is available on a prominent cloud storage service, making it findable and accessible. It is interoperable in that it is released in a machine-readable format, and reusable due to the information contained in the original paper (Sachdeva et al. 2022a)."}, {"title": "Methodology", "content": "Annotation bias\nDefinition. Many definitions of bias have been proposed to date (Garg et al. 2023). Informed by these, we define \"annotation bias\" as the systematic deviation in the labeling of hate speech that arises due to the socio-demographic attributes of the annotator or the persona-based LLM. This bias manifests when the annotations of content are influenced not solely by the content itself, but also by the attributes of the annotator, persona-based LLM, or the target, resulting in over- or underestimations of hate speech.\nOperationalization. Our definition of bias entails the systematic comparison between the labels assigned by human"}, {"title": "Statistical significance.", "content": "Our comprehensive approach to investigating bias across a large set of socio-demographic attributes results in a substantial number of comparisons. However, as reported in Appendix Table T1, certain minority groups of annotators and targets are underrepresented in the data. Consequently, some comparisons are based on limited data, and the observed labeling differences may lack statistical significance. We address this issue by conducting Mann-Whitney tests to assess the statistical significance of the differences between the distributions of labels assigned by any two groups of annotators being compared. In the remainder, we only report results about those comparisons where the labeling differences are statistically significant at the 0.05 level. Due to data imbalance, we were unable to obtain any significant result for a small set of particularly underrepresented groups, highlighted in gray in Table T1.\nMeasures. The confusion matrices derived from statisti-"}, {"title": "Measures.", "content": "The confusion matrices derived from statistically significant comparisons serve as a foundational tool to identify biases in data (Wich et al. 2021). Then, to highlight the most relevant and severe instances of bias, we propose a set of quantitative indicators to measure the strength, direction, and prevalence of the identified biases:\nBias intensity (I). I is obtained as the difference between the sum of values in the lower triangle and the upper triangle of D, divided by the sum of both:\n\\(I = \\frac{\\sum_{i<j} D_{ij} \u2013 \\sum_{i>j} D_{ij}}{\\sum_{i<j} D_{ij} + \\sum_{i>j} D_{ij}}\\) \n\\(I \\in [-1, +1]\\), where \\(I \\approx -1\\) indicates a strong tendency to underestimate hate by \\(A(t = v)\\), while \\(I \\approx +1\\) suggests a strong tendency to overestimate. \\(I \\approx 0\\) indicates minimal bias. Thus, I conveys both the polarity of the bias (i.e., over- or underestimation) and its strength, offering a clear and concise measure of bias intensity.\nBias prevalence (P). P quantifies the overall occurrence of bias within the annotation process, irrespective of its intensity. It is calculated as the sum of the values in both the lower and upper triangles of D, divided by the sum of all values:\n\\(P = \\frac{\\sum_{i<j} D_{ij} + \\sum_{i>j} D_{ij}}{\\sum_{i,j} D_{ij}}\\) \n\\(P \\in [0,1]\\), where \\(P \\approx 0\\) suggests that the bias measured by I occurs in only a minority of cases. Conversely, larger values of P indicate that the bias occurs in a more substantial portion of the annotations, reflecting a non-negligible prevalence. Unlike I, which measures the direction and strength of bias, P focuses on the extent to which that bias is prevalent across the entire set of labels.\nAgreement (K). We complement the previous indicators with a standard measure of annotator agreement. Cohen's \\(\\kappa\\) gauges the extent of agreement between two groups of annotators, correcting for the agreement that could occur by chance. Here, it contextualizes findings from I and P, highlighting whether the observed biases occur in the context of high or low agreement between annotators."}, {"title": "Large language models", "content": "Model selection. We prioritize open-source models over commercial options (e.g., GPT-4) when selecting the LLMs for our experiments. First, open-source models are freely available, which enhances transparency and reproducibility of our research. Additionally, many open-source models have comparable accuracy to that of leading commercial LLMs. Open-source LLMs also require less computational resources, broadening accessibility and easing implementation. These choices make our results generalizable and broadly applicable. Driven by these considerations, we based our model selection on recent results on the performance of open-source LLMs, favoring high-performance and widely-used models (La Cava, Costa, and Tagarelli 2024). Our selection includes Llama-3-8B-Instruct (8B),"}, {"title": "Analyses and Results", "content": "RQ1a: Sensitivity to in-group hate\nAnalysis. We assess whether annotators are more sensible to hate directed at their own in-group. We iteratively fix an attribute and value. Then, we perform the analysis by selecting all labels assigned by annotators with the fixed attribute and value to posts targeted at the same group, and by comparing these annotations to those given to the same posts by annotators without the fixed attribute and value.\nResults. We found statistically significant labeling differences for 17 of 29 (59%) socio-demographic attributes. For each of these cases, we measured bias intensity (I), prevalence (P), and annotator agreement (\\(\\kappa\\)). Results are shown in Figure 3. Regarding the 17 significant differences, 11 (65%) were overestimations (I > 0) and 6 were underestimations. Bias intensity was generally mild or moderate, with \\(|I| \\leq \\pm 0.25\\) in most cases. However, we iden-"}, {"title": "Insights.", "content": "These results indicate that while in-group hypersensitivity is not universal, there is a mild tendency to overestimate in-group hate. Moreover, significant and sometimes substantial biases are present in specific socio-demographic contexts.\nRQ1b: Human biases in annotator-target dynamics\nAnalysis. We broaden our analysis of annotation bias in hate speech by examining all combinations of annotator and target attributes. For each iteration, we fix attribute t = v for annotators and attribute t' = v' for hate targets. We then compare the labels assigned by annotators with t = v to posts directed at targets with t' = v', against the labels provided by annotators with t \u2260 v. This approach allows us to systematically assess how the interplay between annotator and target attributes influence hate speech labeling.\nFigure 4 provides nuanced results on hate speech annotation biases. When observed row-wise, it allows identifying groups of annotators with the tendency to systematically over- or underestimate hate. As a notable example, our analysis reveals a marked age bias: younger annotators (teenagers and young adults) generally tend to underestimate hate, while older ones (middle aged and seniors)"}, {"title": "Insights.", "content": "Our results reveal multiple biases of the annotators and the targets independently, as well as others that arise from specific combinations of annotator and target attributes. These findings show that different annotator groups react distinctly depending on the socio-demographic characteristics of the hate target. While some of the identified"}, {"title": "Insights.", "content": "Our findings suggest that persona-based LLMs exhibit fewer labeling differences and, consequently, fewer biases (according to our definition) than human annotators. The reduced variability in the labels assigned by these LLMs, regardless of the socio-demographic attributes they were prompted to emulate, indicates that the model's personalization may not fully capture the nuanced biases present in human behavior. Despite this, we identified several marked"}, {"title": "RQ2b: Comparison of human and LLM biases", "content": "Analysis. We assess the extent to which the biases exhibited by persona-based LLMs are similar to those of human annotators. Concretely, we focus on the statistically significant cases in which both humans and persona-based LLMs exhibit a bias for the same combination of annotator and target attributes. For each of these cases, we compare bias intensity, bias prevalence, and agreement. This approach allows us to measure the correlation between the considered bias indicators of humans and LLMs, providing insights into how closely persona-based LLMs replicate human biases.\nResults. We found 86 statistically significant (p < 0.01) cases in which human annotators and persona-based LLMS exhibited a bias for the same combination of annotator and target attributes. As shown in Figure 7A, the Pearson correlation r = -0.023 between the intensity of human and LLM biases suggests the lack of a linear relationship. This near-zero correlation indicates that, overall, the biases exhibited by persona-based LLMs do not correspond to those observed in human annotators. A notable exception is the marked tendency that men annotators\u2014both human and LLM-impersonated\u2014exhibit to overestimate hate towards gay individuals, with bias intensity I = 0.74 and I = 0.89 for humans and LLMs respectively. Instead, Figure 7B shows that the correlation between the prevalence of human and LLM biases is moderately positive, with r = 0.388. This suggests that the biases that are (in)frequent among human annotators tend to be relatively (in)frequent among persona-based LLM annotations as well. Finally, the correlation r = 0.182 between the agreement measured for human and LLM biases, shown in Figure 7C, indicates a weak positive relationship. This reflects a slight tendency for the level of agreement among human annotators (with and without a certain attribute) to correspond with the level of agreement among LLMs (with and without the same attribute). However, the weak correlation suggests that while there is some degree of similarity in how humans and LLMs agree within and across certain socio-demographic attributes, the factors"}, {"title": "Robustness.", "content": "Results obtained with Llama3, Phi3, and Starling qualitatively confirm those of Solar, with correlations between human and LLM bias intensity ranging from r = 0.108 (Llama3) to r = -0.137 (Phi3). These additional figures show invariance of our results to the specific LLM used and reinforce the conclusion that persona-based LLMs have limited capacity to reproduce human biases.\nInsights. The persona-based LLMs that we used were, in general, unable to replicate human biases. This was particularly evident with respect to the strength and polarity of the biases. However, biases exhibited by persona-based LLMs occurred with a moderately similar frequency than the corresponding human biases, highlighting some similarities between the two. These results suggest that, although persona-based LLMs can mimic some aspects of human bias, they do not closely replicate the nuanced biases present in human hate speech annotations."}, {"title": "Discussion and Conclusions", "content": "Our results reveal that human annotators exhibit a mild tendency to overestimate in-group hate (RQ1a), and that different socio-demographic attributes among annotators lead to diverse labeling patterns (RQ1b). Furthermore, LLMs show little sensitivity to personalization via prompting, which likely accounts for the few biases observed, though they still display significant biases against certain minority groups (RQ2a). However, persona-based LLMs do not fully replicate human biases, demonstrating minimal alignment with human annotators (RQ2b).\nBias intensity and prevalence. Our methodology introduces two bias indicators\u2014intensity (I) and prevalence (P)\u2014that measure the severity and frequency of annotation biases. While P indicates the frequency of disagreement, I measures the extent of imbalance when such disagreement occurs. Clearly, the most concerning biases are those"}, {"title": "Implications.", "content": "In addition to being a valuable resource for scholars studying prejudices against vulnerable groups (Saha, Chandrasekharan, and De Choudhury 2019), our work also contributes to the development of fair hate speech detection systems. Our work is relevant in this area since human biases hinder the manual annotation of train-"}, {"title": "Limitations and Future work.", "content": "Some limitations must be acknowledged. Although extensive and rich, the social media data was collected from platforms with a marked US user-base and the annotations were obtained from US crowdsourcing contributors. Hence the data may primarily reflect US language, cultural norms, and perspectives. Moreover, the data is skewed in that some socio-demographic groups are over- or underrepresented and the data collection period was relatively short. These factors may limit the generalizability of the biases that we detected to other cultural contexts. Methodologically, our analysis of annotation bias, while informative, does not capture all dimensions of bias, such as contextual factors or annotators' subjective experiences. Moreover, although we experimented with various prompts and LLM models, the specific prompting strategy and LLM versions used may have influenced the results. These limitations highlight the need for more representative datasets, improved methods for LLM personalization such as automatic prompt optimization (Schulhoff et al. 2024), and deeper investigations into specific biases in future research. Other than these, future work should also focus on intersectional studies, as combinations of attributes, rather than individual ones, can cause new biases or reinforce existing ones (Kim et al. 2020). To this end, the analyzed dataset appears particularly well-suited for comprehensive intersectional analyses given the availability of rich socio-demographic information for both annotators and targets."}]}