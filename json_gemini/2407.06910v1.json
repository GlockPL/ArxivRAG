{"title": "Fine-grained large-scale content recommendations for MSX sellers", "authors": ["Manpreet Singh", "Ravdeep Pasricha", "Ravi Prasad Kondapalli", "Kiran R", "Nitish Singh", "Akshita Agarwalla", "Manoj R", "Manish Prabhakar", "Laurent Bou\u00e9"], "abstract": "One of the most critical tasks of Microsoft sellers is to meticulously track and nurture potential business opportunities through proactive engagement and tailored solutions. Recommender systems play a central role to help sellers achieve their goals. In this paper, we present a content recommendation model which surfaces various types of content (technical documentation, comparison with competitor products, customer success stories etc.) that sellers can share with their customers or use for their own self-learning. The model operates at the opportunity level which is the lowest possible granularity and the most relevant one for sellers. It is based on semantic matching between metadata from the contents and carefully selected attributes of the opportunities. Considering the volume of seller-managed opportunities in organizations such as Microsoft, we show how to perform efficient semantic matching over a very large number of opportunity-content combinations. The main challenge is to ensure that the top-5 relevant contents for each opportunity are recommended out of a total of \u2248 40,000 published contents. We achieve this target through an extensive comparison of different model architectures and feature selection. Finally, we further examine the quality of the recommendations in a quantitative manner using a combination of human domain experts as well as by using the recently proposed \u201cLLM as a judge\u201d framework.", "sections": [{"title": "1. Introduction", "content": "In large software organizations, sellers have to nurture, cultivate and maintain relationships with a large ecosystem of partners, customers and dependent stakeholders. At Microsoft, sellers use the Microsoft Sellers Experience - MSX tool to navigate this intricate landscape. Recently, more and more Copilot-inspired systems have been integrated into MSX to help guide sellers and improve their productivity. However, there had not yet been solutions that operate at the lowest level of granularity that sellers work at on a daily basis: the opportunity level.\nIn a CRM system, an \"opportunity\" refers to a potential revenue-generating event or transaction that arises during the course of managing customer relationships. It represents a chance for a business to convert a lead into a customer, close a deal, or expand its services to an existing customer. Opportunities are pivotal moments in the sales process that require careful nurturing and management to maximize the likelihood of success. Sharing the right content with the customer at each sales stage is one of the key factors that helps in moving the opportunity to the next stage. It is important to share only a few but relevant documents (pitch decks, customer success stories, battle cards to show the comparison with competitors...) with the customer to help them quickly understand the value of Microsoft products.\nIn this paper, we show how we have built an opportunity-level recommender system whose purpose is to present the sellers with the top-5 technical documents drawn from the Seismic content repository [1] to increase the sale velocity. Comprising of a very large catalog of technical documentation, product descriptions, customer success stories and more, Seismic is a leading content management system widely used across businesses to manage their digital content. In our case, we consider a catalog of approximately \u2248 40,000 unique documents. Since those documents are decided upon within the context of a specific opportunity, the goal is that they can be shared by the sellers to their customers in order to move the opportunity towards closure in a more targeted manner. Other than a simple rule-based engine which surfaces too many contents, sellers do not currently have any automated guidance as to which Seismic documents would be good candidates to share with their clients.\nWe start in Section 2 by discussing how we have formulated the recommender system as a form of semantic matching between opportunities and Seismic documents. Due to the large volume of opportunities, we discuss how we have designed our solution for large-scale semantic matching. Next, we discuss in Section 3 how we evaluate the quality of the recommendations using a mixture of different techniques to alleviate the fact that there is no ground-truth data. Finally, we illustrate in Section 4 how our solution has been integrated into MSX and is currently being used by real-world Microsoft sellers."}, {"title": "2. Large scale semantic matching for content recommendations", "content": "As mentioned in the introduction, the objective of this work is to recommend relevant Seismic documents given the context of a specific MSX opportunity.\nWith the democratization of large language models, semantic search has become a well-established technique and we describe in this section how the recommender model can be formulated in these terms. Generally, this formulation differs from traditional recommender systems [2] that rely on user-item interactions to make predictions. Collaborative filtering and nearest-neighbor methods analyze users' behavior and preferences to find similarities and identify recommendations. In contrast, our formulation as a semantic matching method relies on natural language understanding of the description of the Seismic documents and of the sellers.\nTo achieve this we follow the approach initiated in [3] where it was shown that it is possible to produce high-quality recommendations between Seismic documents and the end user of the recommendations. In this previous study, the end user was a real time chatbot conversation agent whereas here it is the context associated with specific opportunity. We use the same technique for metadata prompt engineering based on metadata of the Seismic documents and metadata of the opportunities. Essentially, the idea boils down to summarizing the Seismic documents and the opportunities into a short text-based description that contains the most important attributes.\nIn case of Seismic documents we use features like \"name\", \"description\", \"solution area\", \"product\" etc. Similarly for opportunities we look those features which may match those of the Seismic documents as much as possible. By using features which are common on both sides of the semantic matching, we maximize the chance of successful recommendations. The idea is to capture the most common features on both sides so that the language model based embeddings provide relevant documents for opportunities. More details about the architecture can be found in Section 2.2."}, {"title": "2.2. Model architecture", "content": "The architecture of the model itself is inspired by the one designed for the real-time Copilot recommender system previously developed and already deployed in MSX production [3].\nThe idea consists of leveraging a 2-stage system for fast (but slightly inaccurate) retrieval of the top-50 relevant documents (for each opportunity) followed by a much slower step of re-ranking using a cross-encoder model. Generally, both language models are pre-trained on the MS MARCO dataset [6] which is known to produce good embeddings for these types of question-answering systems. For more technical details regarding the data flow, we refer the reader to Fig. 1 and to [3] for the choice of parameters. One important difference from the Copilot model of [3] is that the current model is delivered in a daily-refreshed batch mode instead of real-time.\nBecause of the volume of opportunities for which the recommender system is making predictions, we have decomposed the operations of the model into three distinct parts:\n\u2022 A one-time pre-population of the recommendations for the entire dataset of \u2248 700,000 opportunities representing the last 6 quarters of open opportunities.\n\u2022 A daily batch-mode refresh of the 8-opportunities. Opportunities are classified as d-opportunities if they are net-new opportunities or if some critical properties have changed in the last 24 hours since the last refresh 1. In practice, we deal with \u2248 10,000 such opportunities on a daily basis.\n\u2022 Furthermore, in order to keep up with changes in the Seismic catalog, we refresh the content embeddings on a weekly cadence. The embeddings are stored in ADLS"}, {"title": "2.3. Orders of magnitude", "content": "Let us now turn our attention to the another aspect related to the scale of the semantic search at play here.\nOn one hand, the total number of opportunities with \"open status\" in FY23 and FY24 (i.e. last rolling 6 quarters) is \u2248 700,000. On the other hand, the total number of published documents is \u2248 40,000. Na\u00efvely, this means that one should consider all \u2248 700,000 \u00d7 40,000 opportunity-content pairs. This would result in \u2248 28 \u00d7 10\u00ba combinations. Clearly, one must introduce some filters in order to reduce this large number of combinations.\nIn the following, we use three features (\u201csales stage\", \"area\" and \"solution area\") as filters. This allows us to reduce the content search space to roughly \u2248 7,000 Seismic documents. Now we have \u2248 5 \u00d7 10\u00ba combinations reducing the overall computational effort required by \u2248 80%. Nonetheless, this still leaves us with multiple billions of combinations to perform so this still remains a very large scale semantic matching problem."}, {"title": "2.4. Run-time performance optimization", "content": "In order to reduce the computational complexity, an assumption is to consider opportunities as all independent from each other. (We discuss the limitations associated with this assumption in the conclusion.) Under this assumption, generating recommendations for all the opportunities becomes a fundamentally \u201cembarassingly-parallel\" task.\nSimple profiling reveals that it is the cross-encoder re-ranking stage which is overwhelmingly the most time-consuming part of the architecture described in Fig. 1. The total number of records that needs to go through this re-ranking is 50\u00d7 the number of opportunites since we retrieve 50 candidates for each opportunity. Each record is a pair of prompts: a content prompt shortlisted by the bi-encoder retriever and an original opportunity prompt.\nPandas User Defined Functions - UDFs [7] in PySpark are a popular technique to combine complex data transformation pipelines leveraging Python libraries that may not be natively available in Spark with the convenient parallelism offered by workloads on Spark cluster infrastructures. Accordingly, we utilize Pandas UDFs to distribute the computation across on an Azure Databricks cluster. Figure 2 shows the time taken to process number of opportunities with and without Pandas UDFs. This confirms that Pandas UDFs on Spark clusters lead to a consistent gain in performance.\nNote that incremental further gains may be obtained by proportionately increasing the size of the allocated Spark clusters. Using a Spark cluster with 96 vcores as required for production deployment of the model, we have reduced the processing time from \u2248 2s per opportunity to \u2248 90ms."}, {"title": "3. Relevance/performance evaluation of the recommendations", "content": "Evaluating the quality of recommender systems without ground-truth data poses a challenge due to the absence of a specific objective criteria for assessment. Consequently, it is common to rely on human experts to provide subjective evaluations. Even though this process does offer valuable insights, it can be prohibitively labor-intensive and costly.\nWe start in Section 3.1 by preparing a set of evaluation queries completed by 3 human domain experts and show how the cross-encoder scores produced by the model are in good agreement with the scores given by those experts. Next, we carry out in Section 3.2 an ablation study to investigate the relative importance of the features used in the prompts. Finally, we explore the LLM as a judge framework where GPT-4 is used as an independent evaluator in Section 3.3."}, {"title": "3.1. Human expert evaluation and cross-encoder scores as a proxy", "content": "We curated a list of 22 queries to be sent for evaluation by 3 human experts. For each of these queries, the recommended Seismic documents are predicted by the model and the top-5 results are presented to the evaluators who are asked to give them a rating from 0 to 5 (higher is better). Once those scores are provided by the experts, we ask the question of how much does the cross-encoder score align with human judgments.\nAs presented in Fig. 3, we see that there is indeed a strong positive Pearson correlation coefficient of 0.78 between the cross-encoder score and the scores produced by human experts (averaged over the 5 recommended items). This agreement is very good to separate bad recommendations from the better ones. This means that the cross-encoder score can realistically be used, at least in a binary classifier manner to quickly identify good vs. bad recommendations. Those can then be processed along towards human experts for further analysis with an improved \"triage\" time.\nIn the absence of ground-truth data, those correlation results based on Pearson and Spearman's statistics are encouraging. Furthermore, we note that the human reviewers confirmed that the documents they expected to see as a response to their queries consistently showed up in the top-5 recommendations. This gives an indication that the recall rate (although not directly measurable without ground-truth data) is at least acceptable from the stakeholder's perspective."}, {"title": "3.2. Ablation study", "content": "Now that we have established cross-encoder scores as a good proxy for human judgment of the relevance of the recommendations, we can use these model-produced scores, to assess the relative importance of the features used in the prompts (see Section 2.1). For the purpose of the analysis, we have divided the evaluation queries into 4 groups depending on the features that were used to design the prompts. Crucially, note that the human evaluators were never aware of the underlying division of the queries into 4 groups so this division did not influence their scores (nor were the evaluators aware of the cross-encoder scores).\nWe denote by Fo the set of 3 features that were used both on the documents as well as on the opportunity prompts:\nFn = {\"sales play\u201d, \u201csolution area\", \"product\"}\nThe 22 evaluation queries are divided into 4 groups consisting of:\nA All members of F are used in the prompts.\nB Remove 1 feature so that features from\nFo\\ \"sales play\"\nare used in the prompts.\nC Remove 1 feature so that features from\nFo\\ \"product\"\nare used in the prompts.\nD Remove 2 features so that features from\nFo\\\\\"solution area\" \\ \"product\"\nare used in the prompts.\nAs can be seen in Fig.4, it is clear that \"sales play\" is a critical feature without which the performance of the recommendations is severely affected. Indeed although groups A, C and D have similar performance, group B (i.e. the only one for which the \"sales play\" is missing) has a significantly lower cross-encoder for almost all of its queries. This confirms the importance of \"sales play\" which, as a short descriptive text, (such as \u201caccelerate innovation with low code\" or \"optimize finance and supply chain\" for example) provides a good opportunity to match Seismic documents and opportunities more precisely. This also indicates that improving upon this feature and advising the sellers to make \"sales play\u201d even more descriptive will have a beneficial impact on the relevance of the recommendations."}, {"title": "3.3. LLM as a judge", "content": "A growing trend in the literature has been the rise of leveraging LLMs as tools to evaluate the performance of tasks solved by other LLMs. Early results have indicated that strong LLM-based evaluators give scores that are in general alignment with those provided by human domain experts [8, 9, 10].\nFollowing this line of research, we have devised a simple prompt template so that GPT-4 is asked to give its own score for the same set of 22 curated evaluation queries. The actual prompt template is repeated below:\n\"role\" : \"You are an Al assistant that\nhelps people find information\".\n\"role\" : \"user\"; Given the following\nquery about an opportunity:\n\u2022 Opportunity Prompt\nAnd the following documents:\n\u2022 Doc [1], Doc [2], Doc [3], Doc [4],\nDoc [5]\nPlease perform the following tasks:\n\u2022 Calculate the similarity score\nbetween the query and each document.\nThe similarity score should reflect\nhow relevant each document is to\nthe information contained in the\nquery. Use a scale from 0 to 5,\nwhere 5 indicates a perfect match\nand 0 indicates no relevance.\n\u2022 Provide a brief justification for\nthe ranking based on the similarity\nscores.\nwhere Opportunity Prompt is replaced by the actual opportunity prompt and the Doc[i] are replaced with the recommended content for this opportunity as returned by the model.\nAs can be seen in Fig 5, we do indeed observe a weak correlation between the human expert scores and the scores returned by GPT-4 with a Pearson correlation coefficient of 0.42.\nOne challenge we have noticed is that GPT-4 seems \"hesitant\" to give very low scores even if its textual justification for the scores indicates a poor match between query and documents (this can be detected by seeing words such as \"however\", \"despite\"...). This behavior has been observed anecdotally by others though we have not been able to find a dedicated study to this form of \u201cpoliteness\u201d bias.\nWe have also tried different versions of the prompt by putting more emphasis on different aspects (such as rewarding more correct answers or enforcing strict constraints...). Eventually, we converged a prompt design that generated fair scores and refined it even more using GPT-4 to converge to the prompt shown above."}, {"title": "4. Integration in MSX", "content": "This new recommender model is fully integrated with a pre-existing Copilot interface previously developed in [3]. The end result, as it is seen by Microsoft sellers, is illustrated in Fig. 6 in the appendix.\nWhenever a seller opens an opportunity, he/she gets the option to see the recommended contents which can be shared (customer ready) or used by sellers for their private knowledge (i.e. confidential documents that cannot be shared with customers but may still be of value to the sellers). Customer ready contents can be sent to the customer from this window directly. The seller's email will be composed and \u201clivesendLink\u201d links for the selected contents will be generated automatically. Sellers also get the option to see their own past history with the Seismic documents for an opportunity. They are also offered the possibility to provide feedback about the quality of the recommendations. The \"search from all contents\" feature allows the sellers to search from all the content space in case they want to search for different/more contents than the recommended ones. This search functionality is powered by the real-time Copilot model [3]."}, {"title": "5. Conclusion", "content": "The recommender model described in this paper is now undergoing its pilot phase where the purpose is to gather feedback from real-world sellers before it is released to the entire MSX community. This phase is expected to last for the next 2 quarters during which we plan to gather telemetry about the documents shared by the sellers.\nMeanwhile, there are a number of points we are planning to investigate for further model improvements. For example, it would be interesting to fine-tune the sentence transformer models using different loss functions such as multiple negative ranking loss. In the future, we also plan to incorporate feedback from the sellers to improve the quality of the recommendations. Additionally, programmatic access to the actual content of the Seismic documents is still not available. It is our intention to take this data source into consideration as soon as it is available. This will require to modify the prompt engineering strategy discussed in Section 2.1 into a more elaborate multi-modal system. Finally, we discussed in Section 2.4 how we have treated each opportunity as independent of each other. Although this simplification was critical for computational efficiency reasons, it potentially limits the quality of the recommendations. For example, when different opportunities are managed by the same seller, it would be interesting to incorporate seller-specific features and dependencies between the related opportunities. We plan to study this personalization aspect in a future iteration of the model."}]}