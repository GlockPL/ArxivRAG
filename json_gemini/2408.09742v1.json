{"title": "Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs", "authors": ["Simon D. Angus", "Lachlan O'Neill"], "abstract": "Detecting and quantifying issue framing in textual discourse the perspective one takes to a given topic (e.g. climate science vs. denialism, misogyny vs. gender equality) - is highly valuable to a range of end-users from social and political scientists to program evaluators and policy analysts. However, conceptual framing is notoriously challenging for automated natural language processing (NLP) methods since the words and phrases used by either 'side' of an issue are often held in common, with only subtle stylistic flourishes separating their use. Here we develop and rigorously evaluate new detection methods for issue framing and narrative analysis within large text datasets. By introducing a novel application of next-token log probabilities derived from generative large language models (LLMs) we show that issue framing can be reliably and efficiently detected in large corpora with only a few examples of either perspective on a given issue, a method we call 'paired completion'. Through 192 independent experiments over three novel, synthetic datasets, we evaluate paired completion against prompt-based LLM methods and labelled methods using traditional NLP and recent LLM contextual embeddings. We additionally conduct a cost-based analysis to mark out the feasible set of performant methods at production-level scales, and a model bias analysis. Together, our work demonstrates a feasible path to scalable, accurate and low-bias issue-framing in large corpora.", "sections": [{"title": "Introduction", "content": "It is widely held that public narratives have the power \u2013 for better and worse to shape society (Shiller 2019; Patterson and Monroe 1998; Graber 2002; Barabas and Jerit 2009). For quantitative social scientists, a typical analytical strategy to quantify the occurrence, characteristics and dynamics of these important narratives is to use a 'framing' lens. According to the much-cited definition found in Entman (1993), framing is the process by which individuals \"select some aspects of a perceived reality and make them more salient in a communicating text\" with the purpose of promoting a particular interpretation or evaluation of reality. In essence, to frame, is to impose a world-view or 'way-of-thinking' in communication, with the hope that others will be persuaded to be convinced of the same. In the standard approach to framing analysis (Chong and Druckman 2007), one begins first, by identifying an issue (e.g. 'climate change'); second, by defining the dimensions of that issue (e.g. 'causes', 'economic impact', etc.); third, by developing framings of those dimensions (e.g. climate change/ causes/ framing: 'anthropogenic emissions are responsible for climate change'); and then finally, by the manual labelling of texts (sentences, paragraphs) as to their framing alignment. Computational approaches to framing quantification have addressed various aspects of this decomposition, often (unhelpfully) under the generic heading of 'framing' (we return to this point below) (Ali and Hassan 2022).\nAlmost all prior approaches to computational framing analysis consider the task as a supervised machine-learning problem, typically as a multi-class classification task (Field et al. 2018), and most often focusing on automatic labelling of dimensions (e.g. '2nd Amendment', 'Politics', 'Public Opinion') of a single issue (e.g. gun violence) (Liu et al. 2019a; Zhang et al. 2023), rather the more elaborate 'world-view' like conceptualisation that Entman (1993) and Chong and Druckman (2007) hold. Where studies consider conceptual framing identification, large amounts of labelled data are required, and reported accuracy is very low (below 0.6) (Morstatter et al. 2018; Mendelsohn, Budak, and Jurgens 2021), demonstrating the severe challenges inherent in automating an already difficult human-level task.\nIn this study, we introduce paired completion a low- resource, 'few-label', computationally efficient method that can accurately identify whether a target text aligns with one or other conceptual framing on a given issue (see Fig. 1). Importantly, and distinguishing it from previous methods, our approach: requires only a few (e.g. 5-10) example texts of a given framing (in fact, these can be generatively created); is low-bias compared to generative (prompt-based) LLM approaches; is significantly cheaper than generative approaches; and is highly flexible, switching issues or dimensions or framings is trivial.\nPaired completion takes advantage of the log-probability (logprob) outputs of an LLM\u00b9 to find conditional probabil-"}, {"title": "Textual Alignment & Paired Completion", "content": "To hone in on Entman's classic definition of framing, we reconceptualise the problem as one of \u201ctextual alignment\". Namely, two texts on some topic or issue arise from the same conceptual framing, if they share a high level of textual alignment \u2013 a measure of the likelihood (in some sense) that the two texts might be spoken by the same entity (with a constant conceptual framing). This implies the statements come from the same theoretical outlook, model of the world, and/or causal structure. It is important that the expressive entity is generally defined. For we will be, at times, leveraging generative AI LLMs to play the role of E, alongside human expression, to quantify the degree of alignment.\nDefinition 1 (Textual alignment). Given two conditioning texts a and b, and an expressive entity, E (e.g. a person, a generative AI LLM), text x is said to be more textually aligned with a versus b if it is more likely that x would be expressed by some E' who previously expressed a, than the alternate case where E' had previously expressed b.\nImportantly, Def 1 is not the same as similarity. Consider the texts, 'Getting a dog will improve your life' and, 'Getting a dog will ruin your life'. Whilst these are very similar (in fact, an LLM-powered contextual similarity score would be close to 1 for these texts), they are not textually aligned. If someone holds the view that dogs improve your life (framing A), it is highly unlikely that they would say that dogs ruin your life (framing B). Yet these texts are highly similar on sentiment (both are neutrally posed) and share an almost identical vocabulary. However, consider the third text, 'Pets help to keep you fit and healthy'. It is clear that this text is strongly textually aligned with framing A, but strongly dis-aligned with framing B. Yet, this text is perfectly dissimilar in vocabulary, and is of middling similarity in an LLM-powered contextual embedding space. These examples demonstrate that issue-framing, formalised as textual alignment, is both 'simple' for a human to perceive, yet difficult for existing computational methods (based on similarity, sentiment, vocab, embeddings) to detect.\nAs such, we desire a new set of tools to quantify textual alignment. We consider these tools in the context of the \"Issue-Framing\" task, where a user wishes to detect and quantify texts from a large corpus which share the same framing, via textual alignment. Suppose the user has a small set of texts which together lay out a given framing position A, as compared to an opposing framing position B with a similar number of texts. We then formalise this task as follows:\nDefinition 2 (The Issue-Framing Task). Given a corpus of texts X (target texts) and a set of priming (or framing) texts S = {A,B} comprising texts which represent framing A and B, for each $x \\in X$, quantify the textual alignment towards A and B."}, {"title": "Paired Completion", "content": "We propose the \"paired completion\" method as a solution for the textual-alignment definition given above. Figure 1 gives an overview of its components. Given some set of target texts on a given topic we wish to analyse, and a small set of texts which provide frames for perspective A and B on a given topic (e.g. 'get a dog' vs. 'don't get a dog'), we construct a pair of prompt sequences, $s_1 + x$ and $s_2 + x$ to pass to a generative LLM. Each prompt sequence is composed of a random selection from one of the priming sets (e.g. $s_1$ 'get a dog'), followed by the target text (x).\nFor example, a prompt sequence could be '[priming text from A, $s_1$] Owning a dog will improve your life. [target text, x] Dog owners have lower blood pressure and less stress in general.' A similar sequence would be created for the same text x with priming text(s) from set B. Each prompt sequence is then passed, one at a time, to a generative LLM, and instead of seeking a completion (i.e. generating new tokens) from the LLM, we instead exploit many LLM's ability to provide log-probabilities (the log of the likelihood that the model would have chosen that token/word next) for each token passed to the language model as if it had generated this exact sequence of text. By so doing, we generate two conditional log-probabilities, $lp(x|s_1)$ and $lp(x|s_2)$ (see details in sec 2.1), the conditional log-probs of x being the completion to the priming sequence $s_1$ and $s_2$ respectively.\nIn this way, we are leveraging the twin features of LLMs: first, that LLM attentional mechanisms are highly adept at representing the latent semantic state of a given text; and second, that LLMs have been trained to provide coherent sequences of text (i.e. to avoid non sequiturs). Together, the priming sequence will set the LLM on a particular statistical trajectory to keep the framing state consistent, which implies that if x is within this trajectory (i.e. x is textually aligned with the priming state), the summed log-probabilities the LLM assigns to the words in x will be high. Whereas, if x appears to contradict or speak for a different framing than the priming sequence, the log-probabilities for the words in x will be very low. It is this difference that we exploit by testing both priming sequences from A and B to then calculate the Diff metric.\nTo summarise, paired-completion leverages the 'deep' language modelling properties of LLM \u2013 their deep contextual representation of human meaning \u2013 to quantify the likelihood that a target text will follow from a given condi-"}, {"title": "The Diff Metric", "content": "Suppose we have a set of n priming sequences, $S = {s_1, s_2, ..., s_n}$, and a set of m target sequences $X = {x_1,..., x_m}$. We wish to find the relative alignment, in some sense, of the elements within X towards the different priming sequences in S.\nWe define the diff metric as follows:\n$\\Delta(s_1, s_2, x) = lp(x|s_1) - lp(x|s_2)$\nNote that $\\Delta(s_1, s_2, x) = -\\Delta(s_2, s_1, x)$.\nThe diff metric $\\Delta$ describes the difference between the conditional probability of sentence x after priming sequence $s_1$ and the conditional probability of sentence x after priming sequence $s_2$. In practice, we calculate the prior probabilities of all priming sequences $s \\in S$ as $p_s$, and all texts in $x \\in X$ as $p_x$, and the probability of a concatenated string $s+x$ as $p_{sx}$. Note that concatenation is not necessarily simple string concatenation, but rather ensures grammatical correctness - there is no perfect way to do this, but we found that just ensuring grammatical correctness seems to work sufficiently well in practice.\nWe then compute $lp(x|s) = \\frac{p_{sx}}{p_s}$ to find the conditional probability of x. We can compare this to the prior probability px to determine whether the presence of s has made x more or less likely, and we can compute $lp(x|s_1) - lp(x|s_2)$ (i.e. the $\\Delta$ metric). Since a larger logprob indicates a higher probability, $\\Delta$ will be positive if x is more likely after $s_1$ than after $s_2$, and negative if x is less likely after $s_1$ than after $s_2$. Because LLMs (and language models in general) might assign different prior probabilities to both the conditioning sentences s and the alignment text x, any such method must be robust to priors. This is why we use the difference in conditional probabilities of the same text with different prompts, which is robust to the prior probabilities of both s and x.\nOne interpretation of this approach, with reference to Def. 1, is that the LLM performs the role of the expressive entity E, and so provides a quantification of the likelihood that the text x follows text $s_1$, versus following text $s_2$, i.e. we obtain a measure of textual alignment.\nSince the core idea of paired completion is to use the priming/conditioning sequence to statistically deflect the LLM towards the given framing (and so, measure the model's degree of 'surprise' with the completion text) we conjecture that a longer conditioning sequence may lead to improved accuracy in classifying and retrieving texts aligned with a given framing. To explore this possibility we test two"}, {"title": "Synthetic Dataset Formation", "content": "As described earlier, existing \u2018framing' datasets, such as the MFC (Card et al. 2015) and GVFC (Liu et al. 2019a) are not well suited for application to the task we study here since they label dimensions of a topic as 'frames'. We are not aware of another comparable dataset that makes labels of conceptual framing available across a number of issues. For this reason, conducting evaluation with synthetic data was considered for the present study, although acknowledging inherent limitations (see last section). That said, there are some positive attributes of using synthetic data that we briefly outline.\nFirst, our initial experience with practitioners in fields that are attempting to change public narratives, demonstrated that non-synthetic (human authored) examples of framings can carry correlated linguistic features that may pollute analysis. For example, found narratives that carry a misogynistic perspective can be relative short and abrupt, whilst opposing narratives which speak for gender equality often are expressed with longer, more complex reasoning. Early testing showed that LLMs could pick up on linguistic features such as length and complexity, confusing the signal. Whereas, our paired synthetic pipeline (see full prompts etc. in Supplementary Information) is designed to provide a very balanced (tone, length, complexity etc.) dataset, with only the conceptual framing as the distinguishing feature of the texts.\nSecond, we were concerned that found text could be part of the training data of the LLMs we employed (either with prompting or paired completion). By using synthetic data, although we are in-effect 're-generating' realistic data, and we cannot exclude the possibility that sequences of real text were created, by using a higher temperature in generation (0.5) we are able to somewhat mitigate this. The idea being that again, our LLM methods focus on framing anlaysis, rather than 'familiarity'. We return to this point in limitations.\nThe synthetic dataset generation pipeline takes a topic (e.g. \"dog ownership\u201d, \u201cclimate change\", etc.) and produces a corpus of sentences that reflect different perspectives on the topic. The generation process is a two-step hierarchical process where we generate seed perspective and then generate sentences that align with each perspective. We also generate distillations (into a smaller number of sentences, e.g. 5), summaries, and simple names for each side, with each of these generated from the seed dataset (and having no knowledge of the sentences generated thereafter)."}, {"title": "Evaluation Approach", "content": "We compare the novel paired completion method with a total of four comparison approaches, representing a mix of traditional NLP and transformer-based LLM methods - three use a trained logistic regression (Hosmer Jr, Lemeshow, and Sturdivant 2013) classifier over varying training sample sizes, either employing TF-IDF vectors (Sparck Jones"}, {"title": "LLM Prompting", "content": "Starting with a corpus of texts to test, we construct a prompt with three components: 1) a static instructional component which provides the LLM with the task information; 2) a set of context texts that represent framing A and B to be tested (A, B); and 3) a single target text (x). Unlike in LLM paired completion, we do not require the LLM to provide log-probs for the input sequence, but instead, we obtain the log-probs of the first two tokens produced by the LLM in response to this prompt, i.e. the first two generated tokens. Note that, by virtue of the constraints in the prompt, these probabilities include the log-probs for both response A and B. We extract the probability of the first token of the label assigned to A (e.g. '[equality]' [1 token]), and B (e.g. \u2018[mis][og][yny]' [3 tokens]), respectively. With this information we can both identify which set the LLM has assigned the text to (based on the higher probability of its tokens) and calculate the equivalent Diff metric, $\\Delta(A, B, x)$.\nWe use a fixed prompt, which was initially fine-tuned for GPT-4 and GPT-3.5, and then further tuned for Mixtral-8x7b-Instruct-v0.1 and LLaMA-2-70B-Chat. In hindsight, it was a mistake to tune our prompts for GPT-4 first, as while GPT-4 was very likely going to give the best performance on the tasks at hand (compared to the other models in consideration), it was also a lot more forgiving of errors, confusing wording, and conflicting instructions within the prompt. It is also possible this somewhat biased the prompts towards the OpenAI models, but this was unavoidable given our preliminary results (not included in this paper) were gathered using only the OpenAI APIs; in any case, our results demonstrate the superiority of the open-source models on these tasks. We used a single prompt across all models in our final experiments."}, {"title": "Performance Analysis", "content": "In terms of true-positives (TP), false positives (PF) and false negatives (FN), the F1 score is calculated as,\n$f1 = \\frac{TP}{TP + (FP + FN)}$\nThe f1 score takes a value from 0 to 1, and will be equal to 1 when the method perfectly identifies all the \u2018As' in the data, and does not mis-identify any 'Bs' as 'As'.\nConfidence intervals (95%) for f1 scores were either calculated directly from replicates, in the case of the logistic regression methods (TF-IDF, word- and LLM- embeddings), and using bootstrapping in the LLM API cases (100 replicates, 1000 samples)."}, {"title": "Summary of experiments", "content": "Together, across the five methods, four topics, and related variants, 192 experiments were conducted, as summarised in Table 1."}, {"title": "Results & Discussion", "content": "Our experiments demonstrate strong performance across the board for both prompt-based and paired completion methods, as shown in Figure 2. Paired completion methods tend to statistically perform the same or better than prompt-based methods. This section includes a broad summary of results. More detailed results, tables, and discussion can be found in the appendices."}, {"title": "Comparative Analysis of Classification Methods", "content": "With sufficient data (200+ samples), the embedding approach was competitive with GPT-4 prompting. However, embeddings performed significantly worse in few- shot learning contexts. Among LLM instruct models, GPT- 4-Turbo outperformed all other models. GPT-3.5-Turbo, Mixtral-8x7b-Instruct-v0.1, and LLaMA-2-70B-Chat had similar performance, with LLaMA-2-70B-Chat having the highest propensity for failure modes. For the paired completion approach, performance trended with model parameter count, with LLaMA-2-70B performing best, followed by Mixtral-8x7b, davinci-002, and babbage-002. This consistency may occur because paired completion is less sensitive to model-specific factors like architecture, alignment, and fine-tuning.\nThe three methods for interaction with LLMs that were analysed are all effective, and uniquely suited to different scenarios. The paired completion approach proved highly effective, efficient, and robust to model-specific influences. Embedding-based methods are extremely cheap due to a combination of cheap models and fewer calls to the APIs, and proved very effective with sufficient data. However, this data threshold was far beyond the five exemplars used for the other two approaches, and performance suffered greatly when using 50 exemplars (which is still an order of magnitude more than the five exemplars provided to the other two methods). Prompt-based completion proved effective, particularly with GPT-4 (which does not support the logit outputs required for paired completion), but when possible we generally found paired completion to be more cost-"}, {"title": "Cost vs. Performance", "content": "An analysis of the cost-performance trade-off for the LLM methods (Figure 3) reveals that the paired completion approach with LLaMA-2-70b and Mixtral-8x7b is very cost- effective for their level of performance. While GPT-4 had the best overall performance, it was also by far the most expensive. Other configurations can be chosen based on requirements and funding availability. All LLM-based approaches were significantly more expensive than the embedding approaches, which require more data but proved competitive given sufficient training examples."}, {"title": "Model Bias", "content": "We observed differences in the bias displayed by models and techniques that were dataset-dependent (Figure 4). Embedding-based approaches appear most robust to bias, with no statistically significant bias found for any configuration. LLM-based approaches demonstrated bias in some scenarios, with the k = 2 paired completion configuration potentially reducing bias compared to k = 1. The top performing LLM paired completion methods (mxtrl-k=2; llama-k=2) show significantly less bias than the top LLM prompting approaches, including GPT-4. Further studies are needed to examine the sources of these biases, such as bias in training data, language modeling, or alignment. However, the results suggest the stronger LLM paired completion methods (e.g. llama-k=2) achieve a balance of high accuracy and low bias."}, {"title": "Limitations & Further Work", "content": "Use of synthetic data for evaluation Whilst using synthetic data for evaluation has some benefits (described earlier) there are also some significant limitations. Principally, we cannot be sure that our evaluation results would carry over to a human-labelled ground-truth data set. In the ideal case, a large expert annotation activity would be undertaken to generate a conceptual framing dataset covering a range of issues, and dimensions, and conceptual framings. Such a dataset would be of huge benefit to the field and would no doubt spur further refinement of framing analysis methods. As an intermediate step, a representative sample of our synthetic dataset could be validated by expert annotation to provide some comfort to our main findings.\nOpposing framings, extension beyond \u2018support,oppose\u2019 Our study leverages a traditional two-class classification paradigm by seeking each method's determination as to whether a given target text is drawn from synthetic framing set 'a' or 'b'. Whilst parsimonious and a reasonable starting position, a natural question arises as to how paired completion could be extended to any number of framings on a given dimension of an issue. For example, climate change (issue)"}, {"title": "Model bias in aligned models", "content": "The data seems to offer some support for the conjecture that aligned models are more prone to bias when performing framing alignment, but we cannot make any definitive claims without significantly more evidence and data. We only used three \"serious\" topics (climate change, domestic violence, and misogyny); for further study, we would significantly expand this (perhaps to 10, 20, or even 100 topics, ranging across and beyond, say, the Overton window (Russell 2006))."}]}