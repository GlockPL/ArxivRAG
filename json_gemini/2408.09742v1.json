{"title": "Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs", "authors": ["Simon D. Angus", "Lachlan O'Neill"], "abstract": "Detecting and quantifying issue framing in textual discourse\nthe perspective one takes to a given topic (e.g. climate sci-\nence vs. denialism, misogyny vs. gender equality) - is highly\nvaluable to a range of end-users from social and political\nscientists to program evaluators and policy analysts. How-\never, conceptual framing is notoriously challenging for au-\ntomated natural language processing (NLP) methods since\nthe words and phrases used by either 'side' of an issue are\noften held in common, with only subtle stylistic flourishes\nseparating their use. Here we develop and rigorously eval-\nuate new detection methods for issue framing and narrative\nanalysis within large text datasets. By introducing a novel ap-\nplication of next-token log probabilities derived from genera-\ntive large language models (LLMs) we show that issue fram-\ning can be reliably and efficiently detected in large corpora\nwith only a few examples of either perspective on a given\nissue, a method we call 'paired completion'. Through 192 in-\ndependent experiments over three novel, synthetic datasets,\nwe evaluate paired completion against prompt-based LLM\nmethods and labelled methods using traditional NLP and re-\ncent LLM contextual embeddings. We additionally conduct a\ncost-based analysis to mark out the feasible set of performant\nmethods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable,\naccurate and low-bias issue-framing in large corpora.", "sections": [{"title": "Introduction", "content": "It is widely held that public narratives have the power \u2013 for\nbetter and worse to shape society (Shiller 2019; Patterson\nand Monroe 1998; Graber 2002; Barabas and Jerit 2009).\nFor quantitative social scientists, a typical analytical strategy\nto quantify the occurrence, characteristics and dynamics of\nthese important narratives is to use a 'framing' lens. Accord-\ning to the much-cited definition found in Entman (1993),\nframing is the process by which individuals \"select some\naspects of a perceived reality and make them more salient\nin a communicating text\" with the purpose of promoting a\nparticular interpretation or evaluation of reality. In essence,\nto frame, is to impose a world-view or 'way-of-thinking' in\ncommunication, with the hope that others will be persuaded\nto be convinced of the same. In the standard approach to\nframing analysis (Chong and Druckman 2007), one begins\nfirst, by identifying an issue (e.g. 'climate change'); sec-\nond, by defining the dimensions of that issue (e.g. 'causes',\n'economic impact', etc.); third, by developing framings of\nthose dimensions (e.g. climate change/ causes/ framing: 'an-\nthropogenic emissions are responsible for climate change');\nand then finally, by the manual labelling of texts (sentences,\nparagraphs) as to their framing alignment. Computational\napproaches to framing quantification have addressed vari-\nous aspects of this decomposition, often (unhelpfully) under\nthe generic heading of 'framing' (we return to this point be-\nlow) (Ali and Hassan 2022).\nAlmost all prior approaches to computational framing\nanalysis consider the task as a supervised machine-learning\nproblem, typically as a multi-class classification task (Field\net al. 2018), and most often focusing on automatic labelling\nof dimensions (e.g. '2nd Amendment', 'Politics', 'Public\nOpinion') of a single issue (e.g. gun violence) (Liu et al.\n2019a; Zhang et al. 2023), rather the more elaborate 'world-\nview' like conceptualisation that Entman (1993) and Chong\nand Druckman (2007) hold. Where studies consider con-\nceptual framing identification, large amounts of labelled\ndata are required, and reported accuracy is very low (below\n0.6) (Morstatter et al. 2018; Mendelsohn, Budak, and Jur-\ngens 2021), demonstrating the severe challenges inherent in\nautomating an already difficult human-level task.\nIn this study, we introduce paired completion a low-\nresource, 'few-label', computationally efficient method that\ncan accurately identify whether a target text aligns with one\nor other conceptual framing on a given issue (see Fig. 1).\nImportantly, and distinguishing it from previous methods,\nour approach: requires only a few (e.g. 5-10) example texts\nof a given framing (in fact, these can be generatively cre-\nated); is low-bias compared to generative (prompt-based)\nLLM approaches; is significantly cheaper than generative\napproaches; and is highly flexible, switching issues or di-\nmensions or framings is trivial.\nPaired completion takes advantage of the log-probability\n(logprob) outputs of an LLM\u00b9 to find conditional probabil-"}, {"title": "1.1 Contributions", "content": "We introduce paired completion as a solution to the problem\nof textual alignment. We construct a series of high-quality\nsynthetic datasets using a novel method which captures nu-\nances of discourse on complex topics, and use these datasets\nto evaluate the performance of several approaches to tex-\ntual alignment. We demonstrate that paired completion is a\nnovel, efficient, and more effective method for performing\ntextual alignment (compared to a chat-based LLM baseline)."}, {"title": "1.2 Related Literature", "content": "'Framing' analysis Unfortunately, 'framing' analysis\ndoes not have a clear definition in the computational liter-\nature, as evidenced by the variety of tasks that arise in a re-\ncent survey of 37 'framing' studies (Ali and Hassan 2022).\nA starting point for many framing approaches is to lever-\nage existing corpora of labelled datasets. Here, the media\nframes corpus (MFC) (Card et al. 2015) and the gun vio-\nlence frame corpus (GVFC) (Liu et al. 2019a) have been the\nbasis of many methodological contributions. However, these\ndatasets conceptualise 'framing' as dimensions (ala Chong\nand Druckman (2007)) of a topic or issue, not conceptual\nframes as we have distinguished earlier. The MFC is com-\nposed of 15 generic 'frames' (Boydstun et al. 2013) such\nas 'economic', 'public opinion' and 'cultural identity' ap-\nplied to three issues (\u2018immigration', \u2018smoking', and 'same-\nsex marriage'). Thousands of annotations were recorded as\nto whether one of these dimensions were associated with the\ntext on a given issue. Likewise, the GVFC follows a similar\napproach, albeit tied more tightly to gun violence, 'frames'\nare equivalent to dimensions, and include '2nd Amend-\nment', 'Politics', and 'Public Opinion'. Typically, computa-\ntional methods approach framing in this way as a multi-class\nclassification problem, using supervised machine learning\nmethodologies such as featuring engineering, classifier se-\nlection and k-fold evaluation (Field et al. 2018; Liu et al.\n2019a; Aky\u00fcrek et al. 2020; Zhang et al. 2023). Common\nto all of these approaches is the need for large amounts of\nlabelled ground-truth data, and consequently, the outcome\nthat methods are not generalisable beyond the topics under\nstudy.\nWhere 'framing' is implemented in a closer way to the\nconceptual framing we address in this work, challenges re-\nmain around the need for large labelled datasets and the ac-\ncuracy of the methods. Morstatter et al. (2018) consider sup-\nport for, or against, 10 framings related to Balistic Missile\nDefence (BMD) in Europe over 823 online news articles\n(31k sentences). By writing out support- and opposition-\n(polarity) perspectives for each of the 10 framings, they are\nable to generate 20 framing-polarity classes, and apply tradi-\ntional NLP methods to multi-class prediction. Alternatively,\nMendelsohn et al. (2021) label 3.6k social media posts\n(tweets) related to immigration for the 15 generic 'fram-\nings' of Boydstun et al. (Boydstun et al. 2013) together with"}, {"title": "2 Textual Alignment & Paired Completion", "content": "To hone in on Entman's classic definition of framing, we\nreconceptualise the problem as one of \u201ctextual alignment\".\nNamely, two texts on some topic or issue arise from the\nsame conceptual framing, if they share a high level of tex-\ntual alignment \u2013 a measure of the likelihood (in some sense)\nthat the two texts might be spoken by the same entity (with\na constant conceptual framing). This implies the statements\ncome from the same theoretical outlook, model of the world,\nand/or causal structure. It is important that the expressive en-\ntity is generally defined. For we will be, at times, leveraging\ngenerative AI LLMs to play the role of E, alongside human\nexpression, to quantify the degree of alignment.\nDefinition 1 (Textual alignment). Given two conditioning\ntexts a and b, and an expressive entity, E (e.g. a person,\na generative AI LLM), text x is said to be more textually\naligned with a versus b if it is more likely that x would be\nexpressed by some E' who previously expressed a, than the\nalternate case where E' had previously expressed b.\nImportantly, Def 1 is not the same as similarity. Consider\nthe texts, 'Getting a dog will improve your life' and, 'Get-\nting a dog will ruin your life'. Whilst these are very sim-\nilar (in fact, an LLM-powered contextual similarity score\nwould be close to 1 for these texts), they are not textually\naligned. If someone holds the view that dogs improve your\nlife (framing A), it is highly unlikely that they would say that\ndogs ruin your life (framing B). Yet these texts are highly\nsimilar on sentiment (both are neutrally posed) and share\nan almost identical vocabulary. However, consider the third\ntext, 'Pets help to keep you fit and healthy'. It is clear that\nthis text is strongly textually aligned with framing A, but\nstrongly dis-aligned with framing B. Yet, this text is per-\nfectly dissimilar in vocabulary, and is of middling similar-\nity in an LLM-powered contextual embedding space. These\nexamples demonstrate that issue-framing, formalised as tex-\ntual alignment, is both 'simple' for a human to perceive, yet\ndifficult for existing computational methods (based on simi-\nlarity, sentiment, vocab, embeddings) to detect.\nAs such, we desire a new set of tools to quantify tex-\ntual alignment. We consider these tools in the context of\nthe \"Issue-Framing\" task, where a user wishes to detect and\nquantify texts from a large corpus which share the same\nframing, via textual alignment. Suppose the user has a small\nset of texts which together lay out a given framing position\nA, as compared to an opposing framing position B with a\nsimilar number of texts. We then formalise this task as fol-\nlows:\nDefinition 2 (The Issue-Framing Task). Given a corpus of\ntexts X (target texts) and a set of priming (or framing) texts\nS = {A,B} comprising texts which represent framing A\nand B, for each x \u2208 X, quantify the textual alignment to-\nwards A and B."}, {"title": "2.1 Paired Completion", "content": "We propose the \"paired completion\" method as a solution\nfor the textual-alignment definition given above. Figure 1\ngives an overview of its components. Given some set of tar-\nget texts on a given topic we wish to analyse, and a small set\nof texts which provide frames for perspective A and B on a\ngiven topic (e.g. 'get a dog' vs. 'don't get a dog'), we con-\nstruct a pair of prompt sequences, s\u2081 + x and s2 + x to pass\nto a generative LLM. Each prompt sequence is composed of\na random selection from one of the priming sets (e.g. s\u2081 'get\na dog'), followed by the target text (x).\nFor example, a prompt sequence could be '[priming text\nfrom A, 81] Owning a dog will improve your life. [target\ntext, x] Dog owners have lower blood pressure and less\nstress in general.' A similar sequence would be created for\nthe same text x with priming text(s) from set B. Each prompt\nsequence is then passed, one at a time, to a generative LLM,\nand instead of seeking a completion (i.e. generating new to-\nkens) from the LLM, we instead exploit many LLM's ability\nto provide log-probabilities (the log of the likelihood that the\nmodel would have chosen that token/word next) for each to-\nken passed to the language model as if it had generated this\nexact sequence of text. By so doing, we generate two condi-\ntional log-probabilities, $lp(x|s_1)$ and $lp(x|s_2)$ (see details in\nsec 2.1), the conditional log-probs of x being the completion\nto the priming sequence 81 and 82 respectively.\nIn this way, we are leveraging the twin features of LLMs:\nfirst, that LLM attentional mechanisms are highly adept at\nrepresenting the latent semantic state of a given text; and\nsecond, that LLMs have been trained to provide coherent\nsequences of text (i.e. to avoid non sequiturs). Together, the\npriming sequence will set the LLM on a particular statistical\ntrajectory to keep the framing state consistent, which implies\nthat if x is within this trajectory (i.e. x is textually aligned\nwith the priming state), the summed log-probabilities the\nLLM assigns to the words in x will be high. Whereas, if\nx appears to contradict or speak for a different framing than\nthe priming sequence, the log-probabilities for the words in\nx will be very low. It is this difference that we exploit by test-\ning both priming sequences from A and B to then calculate\nthe Diff metric.\nTo summarise, paired-completion leverages the 'deep'\nlanguage modelling properties of LLM \u2013 their deep con-\ntextual representation of human meaning \u2013 to quantify the\nlikelihood that a target text will follow from a given condi-"}, {"title": "The Diff Metric", "content": "Suppose we have a set of n priming se-\nquences, $S = {s_1, s_2, ..., s_n}$, and a set of m target se-\nquences $X = {x_1,..., x_m}$. We wish to find the relative\nalignment, in some sense, of the elements within X towards\nthe different priming sequences in S.\nWe define the diff metric as follows:\n$\u0394(s_1, s_2, x) = lp(x|s_1) - lp(x|s_2)$\nNote that $\u0394(s_1, s_2, x) = -\u2206(s_2, s_1, X)$.\nThe diff metric A describes the difference between the\nconditional probability of sentence x after priming sequence\n81 and the conditional probability of sentence x after prim-\ning sequence 82. In practice, we calculate the prior probabil-\nities of all priming sequences $s \u2208 S$ as $p_s$, and all texts in\n$x \u2208 X$ as $p_x$, and the probability of a concatenated string\n$s+ x$ as $p_{sx}$. Note that concatenation is not necessarily sim-\nple string concatenation, but rather ensures grammatical cor-\nrectness - there is no perfect way to do this, but we found that\njust ensuring grammatical correctness seems to work suffi-\nciently well in practice.\nWe then compute $lp(x|s) = \\frac{p_{sx}}{p_s}$ to find the con-\nditional probability of x. We can compare this to the prior\nprobability pr to determine whether the presence of s has\nmade x more or less likely, and we can compute $lp(x|s_1)$\n$lp(x|s_2)$ (i.e. the A metric). Since a larger logprob indicates\na higher probability, A will be positive if x is more likely\nafter s\u2081 than after 82, and negative if x is less likely after 81\nthan after 82. Because LLMs (and language models in gen-\neral) might assign different prior probabilities to both the\nconditioning sentences s and the alignment text x, any such\nmethod must be robust to priors. This is why we use the\ndifference in conditional probabilities of the same text with\ndifferent prompts, which is robust to the prior probabilities\nof both s and x.\nOne interpretation of this approach, with reference to\nDef. 1, is that the LLM performs the role of the expressive\nentity E, and so provides a quantification of the likelihood\nthat the text x follows text 81, versus following text s2, i.e.\nwe obtain a measure of textual alignment.\nSince the core idea of paired completion is to use the\npriming/conditioning sequence to statistically deflect the\nLLM towards the given framing (and so, measure the\nmodel's degree of 'surprise' with the completion text) we\nconjecture that a longer conditioning sequence may lead to\nimproved accuracy in classifying and retrieving texts aligned\nwith a given framing. To explore this possibility we test two"}, {"title": "4 Evaluation Approach", "content": "We compare the novel paired completion method with a total\nof four comparison approaches, representing a mix of tra-\nditional NLP and transformer-based LLM methods - three\nuse a trained logistic regression (Hosmer Jr, Lemeshow,\nand Sturdivant 2013) classifier over varying training sam-\nple sizes, either employing TF-IDF vectors (Sparck Jones"}, {"title": "4.1 LLM Prompting", "content": "Starting with a corpus of texts to test, we construct a prompt\nwith three components: 1) a static instructional component\nwhich provides the LLM with the task information; 2) a set\nof context texts that represent framing A and B to be tested\n(A, B); and 3) a single target text (x). Unlike in LLM paired\ncompletion, we do not require the LLM to provide log-probs\nfor the input sequence, but instead, we obtain the log-probs\nof the first two tokens produced by the LLM in response to\nthis prompt, i.e. the first two generated tokens. Note that, by\nvirtue of the constraints in the prompt, these probabilities\ninclude the log-probs for both response A and B. We ex-\ntract the probability of the first token of the label assigned to\nA (e.g. '[equality]' [1 token]), and B (e.g. \u2018[mis][og][yny]'\n[3 tokens]), respectively. With this information we can both\nidentify which set the LLM has assigned the text to (based\non the higher probability of its tokens) and calculate the\nequivalent Diff metric, \u2206(A, B, x).\nWe use a fixed prompt, which was initially fine-tuned\nfor GPT-4 and GPT-3.5, and then further tuned for Mixtral-\n8x7b-Instruct-v0.1 and LLaMA-2-70B-Chat. In hindsight, it\nwas a mistake to tune our prompts for GPT-4 first, as while\nGPT-4 was very likely going to give the best performance on\nthe tasks at hand (compared to the other models in consider-\nation), it was also a lot more forgiving of errors, confusing\nwording, and conflicting instructions within the prompt. It is\nalso possible this somewhat biased the prompts towards the\nOpenAI models, but this was unavoidable given our prelimi-\nnary results (not included in this paper) were gathered using\nonly the OpenAI APIs; in any case, our results demonstrate\nthe superiority of the open-source models on these tasks. We\nused a single prompt across all models in our final experi-\nments."}, {"title": "4.2 Performance Analysis", "content": "In terms of true-positives (TP), false positives (PF) and false\nnegatives (FN), the F1 score is calculated as,\n$f1 = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}$\nThe f1 score takes a value from 0 to 1, and will be equal to 1\nwhen the method perfectly identifies all the \u2018As' in the data,\nand does not mis-identify any 'Bs' as 'As'.\nConfidence intervals (95%) for f1 scores were either cal-\nculated directly from replicates, in the case of the logistic re-\ngression methods (TF-IDF, word- and LLM- embeddings),\nand using bootstrapping in the LLM API cases (100 repli-\ncates, 1000 samples)."}, {"title": "4.3 Summary of experiments", "content": "Together, across the five methods, four topics, and related\nvariants, 192 experiments were conducted, as summarised\nin Table 1."}, {"title": "5 Results & Discussion", "content": "Our experiments demonstrate strong performance across the\nboard for both prompt-based and paired completion meth-\nods, as shown in Figure 2. Paired completion methods tend\nto statistically perform the same or better than prompt-based\nmethods. This section includes a broad summary of results.\nMore detailed results, tables, and discussion can be found in\nthe appendices."}, {"title": "5.1 Comparative Analysis of Classification\nMethods", "content": "With sufficient data (200+ samples), the embedding ap-\nproach was competitive with GPT-4 prompting. How-\never, embeddings performed significantly worse in few-\nshot learning contexts. Among LLM instruct models, GPT-\n4-Turbo outperformed all other models. GPT-3.5-Turbo,\nMixtral-8x7b-Instruct-v0.1, and LLaMA-2-70B-Chat had\nsimilar performance, with LLaMA-2-70B-Chat having the\nhighest propensity for failure modes. For the paired com-\npletion approach, performance trended with model param-\neter count, with LLaMA-2-70B performing best, followed\nby Mixtral-8x7b, davinci-002, and babbage-002. This con-\nsistency may occur because paired completion is less sen-\nsitive to model-specific factors like architecture, alignment,\nand fine-tuning.\nThe three methods for interaction with LLMs that were\nanalysed are all effective, and uniquely suited to different\nscenarios. The paired completion approach proved highly\neffective, efficient, and robust to model-specific influences.\nEmbedding-based methods are extremely cheap due to a\ncombination of cheap models and fewer calls to the APIs,\nand proved very effective with sufficient data. However, this\ndata threshold was far beyond the five exemplars used for\nthe other two approaches, and performance suffered greatly\nwhen using 50 exemplars (which is still an order of mag-\nnitude more than the five exemplars provided to the other\ntwo methods). Prompt-based completion proved effective,\nparticularly with GPT-4 (which does not support the logit\noutputs required for paired completion), but when possi-\nble we generally found paired completion to be more cost-"}, {"title": "5.2 Cost vs. Performance", "content": "An analysis of the cost-performance trade-off for the LLM\nmethods (Figure 3) reveals that the paired completion ap-\nproach with LLaMA-2-70b and Mixtral-8x7b is very cost-\neffective for their level of performance. While GPT-4 had the\nbest overall performance, it was also by far the most expen-\nsive. Other configurations can be chosen based on require-\nments and funding availability. All LLM-based approaches\nwere significantly more expensive than the embedding ap-\nproaches, which require more data but proved competitive\ngiven sufficient training examples."}, {"title": "5.3 Model Bias", "content": "We observed differences in the bias displayed by mod-\nels and techniques that were dataset-dependent (Figure 4).\nEmbedding-based approaches appear most robust to bias,\nwith no statistically significant bias found for any configu-\nration. LLM-based approaches demonstrated bias in some\nscenarios, with the k = 2 paired completion configura-\ntion potentially reducing bias compared to k = 1. The top\nperforming LLM paired completion methods (mxtrl-k=2;\nllama-k=2) show significantly less bias than the top LLM\nprompting approaches, including GPT-4. Further studies are\nneeded to examine the sources of these biases, such as bias\nin training data, language modeling, or alignment. How-\never, the results suggest the stronger LLM paired completion\nmethods (e.g. llama-k=2) achieve a balance of high accuracy\nand low bias."}, {"title": "6 Limitations & Further Work", "content": "Use of synthetic data for evaluation Whilst using syn-\nthetic data for evaluation has some benefits (described ear-\nlier) there are also some significant limitations. Principally,\nwe cannot be sure that our evaluation results would carry\nover to a human-labelled ground-truth data set. In the ideal\ncase, a large expert annotation activity would be undertaken\nto generate a conceptual framing dataset covering a range\nof issues, and dimensions, and conceptual framings. Such\na dataset would be of huge benefit to the field and would\nno doubt spur further refinement of framing analysis meth-\nods. As an intermediate step, a representative sample of our\nsynthetic dataset could be validated by expert annotation to\nprovide some comfort to our main findings.\nOpposing framings, extension beyond \u2018support,oppose'\nOur study leverages a traditional two-class classification\nparadigm by seeking each method's determination as to\nwhether a given target text is drawn from synthetic framing\nset 'a' or 'b'. Whilst parsimonious and a reasonable starting\nposition, a natural question arises as to how paired comple-\ntion could be extended to any number of framings on a given\ndimension of an issue. For example, climate change (issue)"}]}