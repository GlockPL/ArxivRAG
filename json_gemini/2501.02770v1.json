{"title": "Multi-Agent Path Finding under Limited Communication Range Constraint via Dynamic Leading", "authors": ["Hoang-Dung Bui", "Erion Plaku", "Gregory J. Stein"], "abstract": "This paper proposes a novel framework to handle a multi-agent path finding problem under a limited communication range constraint, where all agents must have a connected communication channel to the rest of the team. Many existing approaches to multi-agent path finding (e.g., leader-follower platooning) overcome computational challenges of planning in this domain by planning one agent at a time in a fixed order. However, fixed leader-follower approaches can become stuck during planning, limiting their practical utility in dense-clutter environments. To overcome this limitation, we develop dynamic leading multi-agent path finding, which allows for dynamic reselection of the leading agent during path planning whenever progress cannot be made. The experiments show the efficiency of our framework, which can handle up to 25 agents with more than 90% success-rate across five environment types where baselines routinely fail.", "sections": [{"title": "INTRODUCTION", "content": "We want a team of agents navigate through an obstacle-rich environment to goals while maintaining constant team communication: a spanning tree created from range-limited communication between pairs of agents. This problem is relevant to scenarios like supply delivery during disasters or monitoring hostile environments, where the risk of losing agents is significant. To mitigate this risk, agents must ensure that the team is in constant communication throughout their movement. Maintaining the spanning tree while the agents head in different directions with varied lengths of actions makes pathfinding challenging in continuous time and space, even for holonomic agents. The challenge is compounded by agents starting at random positions, needing to pass through narrow passages and non-convex spaces without collisions, and then reach random goals.\nThe problem can theoretically be solved using a centralized approach [1]-[5], in which planning selects between team actions, each simultaneously specifying an action for all agents at once. However, this approach quickly suffers from the curse of dimensionality, as the difficulty of planning increases exponentially with the number of agents, making planning intractable for even relatively small problems. The platooning leader-follower approach [6]\u2013[9] mitigates this challenge by planning each agent in sequence, where one agent is selected as the leader and others act as followers, who plan so as to maintain communication to the agent that planned before them. However, establishing a fixed planning order for leader agent can result in issues during planning. These issues include the follower agents finding no action to follow the leader within communication range or becoming stuck once the leader reaches its goal (Fig. 1.a), or the team needing to spread out for the followers to reach their goals while maintaining communication range (Fig. 1.b).\nTo address these challenges, this work develops a frame-work for solving multi-agent pathfinding problem with lim-ited communication range (MALCR), ensuring a connected communication channel in the team at any time. We propose a novel technique: dynamic leading which allows any agent to become the new leader during multi-agent tree expansion, allowing planning to further expand the multi-agent tree if the current leader cannot make progress. Once the leader agent has planned, follower agents plan, ensuring that the resulting path is always in communication to at least one agent.\nWe introduce an algorithm for planning in multi-agent systems with communication distance constraints: MA-DL. The experiments show that our framework results in fast and effective planning, handling up to 25 agents with more than 90% success-rate across five environment types where baselines, centralized planning with composite states and platooning leader-follower approach, routinely fail."}, {"title": "RELATED WORK", "content": "There exist multiple approaches that seek to solve multi-agent path finding problems, in the absence of a communication constraint, that could in theory be adapted to solve"}, {"title": "PROBLEM DEFINITION", "content": "We formally define the Multi-Agent Path Finding with Limited Communication Range (MALCR) problem as follows. There are n agents {a1,...,an} and a known world W with obstacles O = {01,..., Om}. The agents start at the initial positions $s^{init} = (s^{init}_1,..., s^{init}_n)$ and head to the goals g = {91,...,gn}, where $s_i, g_i \u2208 W$ and $s_i, g_i \\notin O$. The initial positions and goals are chosen so as to satisfy the team communication constraint, formally defined below.\nThe world W is divided into sub-divisions A, which are obstacle-free regions. A graph G is created where the vertices are the centroids of A and the edges represent connections of neighboring sub-divisions. An agent moves to a neighboring sub-division with constant velocity $V_e$ by taking the action: $Move(v, u)$, where v,u are neighbor vertices in G. As reaching its goal, the agent takes the action of $Stop = Move(v, v)$. Agents moving over G are guaranteed to be collision-free with respect to the static obstacles. Time and agents' positions are continuous due to varied lengths of actions.\nWe define two levels of communication constraint for the agents. The first level is between two agents, referred to as agent communication constraint (ACOMM), which requires the distance between their positions to be less than or equal to the communication range $r_c$. The second level applies to the entire agent team, referred to as team communication constraint (TCOMM), which requires the team to form a spanning tree where the edges represent the connections between pairs of agents satisfying the ACOMM constraint. An action $Move(v, u)$ satisfies the ACOMM constraint if during the movement $Move(v, u)$ the agent has at least one neighboring agent within a distance of $r_c$.\nA collision occurs between two agents, $a_i$ and $a_j$, when their distance is less than a threshold $d_c$ at timestep t. A path is a sequence of waypoints to transition an agent from a position $p_s$ to position $p_g$ with constant velocity $v_e$. We say a path is reach-goal if it can lead the agent to the goal, is collision-free, and the movement between two sequential waypoints satisfies ACOMM constraint; When an agent stops at its goal, we still consider collision and ACOMM constraints. A path is valid if it is reach-goal and at the goal $p_g$ from $t_g$, the agent's action Stop still satisfies ACOMM and is collision-free.\nThe objective is to compute valid paths {$\\zeta_1,..., \\zeta_n$}, one for each agent, so that $\\zeta_i$ starts at $s^{init}_i$, reaches $g_i$, and the agents satisfy TCOMM constraints from the start time to the time the last agent reaches its goal. We develop a MAPF framework that seeks to reduce the overall planning time and the travel distances."}, {"title": "METHODS", "content": "Our framework, named as MA-DL, consists of two mod-ules: Multi-agent Path Finding with Dynamic Leading (MA-DL) and Single-Agent Path Finding (SAPF).\nHigh Level Overview: The main loop of the MA-DL module (Alg. 1) expands a multi-agent (MA) planning tree $T_{ma}$ whose growth is illustrated in Fig. 2. The MA tree is"}, {"title": "MA-DL MODULE", "content": "This module manages the multi-agent tree (MATree) $T_{ma}$, selects nodes to expand, dynamically selects planning orders, and triggers single-agent plannings.\nThe module gets the number of agents n, the goals g, initial start $s^{init}$, and the world W as inputs and returns the valid paths or, if valid paths cannot be found, those closest to the goal. The algorithm (Alg. 1) starts by initializing a multi-agent tree $T_{ma}$ with the root consisting of all initial positions $s^{init}$ (Alg. 1:1). The world W is divided into sub-divisions A of a predefined area, with obstacle-overlapping sub-divisions further subdivided along their largest dimension until they clear or reach a size threshold. A graph G is created from the centroids of the obstacle-free sub-divisions, while the edges represent the connections between neighboring sub-divisions (sharing boundaries or corners). We compute the shortest paths H from all sub-divisions to the agent goals g as heuristics by the function CALHEURISTIC() (Alg. 1:2). In the main loop (Alg. 1:3\u201315), a node v with smallest cost (defined at Alg. 4.14) is selected from $T_{ma}$. The cost of v is then increased to encourage selecting other nodes. A heuristic function INITORDER() returns an initial planning order p. A list of n paths $\\zeta$ is initialized from v (Alg. 1:4). If the node v is visited, the planning order p is shuffled to get a different planning order (Alg. 1:5). The planning loop (Alg. 1:6-13) attempts to find valid paths for the agents. The loop starts by setting the variable allRG to true (Alg. 1:7). The planning order p is shuffled after $m_t$ iterations of failing to reach the goal (Alg. 1:7). The for loop (Alg. 1:8-12) plans for each agent in order by calling the module SAPF and then the returned paths is inserted into $\\zeta^s$ (Alg. 1:10). If $\\zeta^s$ is reach-goal, it is then checked for validity (valid) by the function MODIFYIFOVERLAP(). The path is invalid if the Stop action at the goal at time t blocks the future movement of an already-planned agent (according to planning order p), a situation called collision-at-goal by Bui et al. [13]. If the situation occurs, the function modifies the earlier planned paths and set allRG to false (Alg. 1:11). If the path is not reach-goal, allRG is set to false (Alg.1:12).\nAfter all agent planning has completed, the function EXPANDMATREE() is triggered to expand $T_{ma}$ from v using the agent's paths $\\zeta^s$ (Alg. 1:14); then the node v is also marked as visited. If all agents reach the goals (allRG is still true), we break the while loop (Alg. 1:15), then return paths for all agents from $T_{ma}$ (Alg. 1:16-17)."}, {"title": "SAPF MODULE", "content": "Single-Agent Path Finding (SAPF) finds a shortest reach-goal path for an agent. This planner searches actions on G that satisfy the ACOMM constraint and are collision-free. With enough runtime, the planner can explore all possible"}, {"title": "EXPAND MULTI-AGENT TREE", "content": "Function EXPANDMATREE() (Alg. 4) integrates the paths from the single agents $\\zeta^s$ and adds the combined paths to expand the multi-agent tree $T_{ma}$. Each node on $T_{ma}$ has an asso-ciated timestep and the agent positions are interpolated to make them match for each single agent path.\nThe for loop (Alg. 4:2-3) collects all timesteps of the waypoints on the single paths and inserts into a priority queue timeList. The second for loop (Alg. 4:4-14) goes through all elements in timeList to create new nodes and add valid nodes into the tree $T_{ma}$. Each new node $u_n$ inherits the travel cost from its parent with timestep t (Alg. 4:5). We then go to each agent's path, interpolate to recover its position at time t (Alg. 4:7-12), and insert it into the state of node $u_n$. If t is larger than the max-timestep of the path (Alg. 4:7) and the path reaches the goal, the final position of $\\zeta$ is returned (Alg. 4:8). If the position is not the goal, the tree's expansion stops (Alg. 4:9).\nIf t is smaller or equal to the last timestep, the agent's position at t is interpolated by function GETPOSATTIME() (Alg. 4:10). The lines in Alg. 4:11-12 add the agent's positions into the node state; we then update the travel cost and the heuristic. Finally, the new node $u_n$ is added into the MATree $T_{ma}$, and $v_p$ is reassigned to continue the expansion (Alg. 4:14)."}, {"title": "EXPERIMENTS AND RESULTS", "content": "Experiments are conducted on five obstacle-rich environ-ments (Fig. 4). Our planner's performance is measured by three metrics: (1) success-rate, (2) runtime, and (3) per-agent travel distance. Our MA-DL approach is compared with two baselines: a centralized approach with composite states and a platooning leader-follower approach. All agents have 8 actions: to move one unit in the four cardinal directions and $\\sqrt{2}$ unit in their diagonal directions. We also evaluate the planner's robustness versus of runtime, goal configuration, and difficulty of environments."}, {"title": "Experimental Setup", "content": "Baselines: We have implemented two baselines: Central-ized approach with composite state (COMP) and Platooning Leader-Follower approach (PLF). COMP is selected because existing MAPF planners\u2014which typically advance the mo-tion of each agent in isolation of one another and then attempt to resolve conflicts as necessary\u2014are not well suited to handle the communication constraint, as the challenge of ensuring that all agents are in constant communication would require an undue number of repair operations, causing such planners to struggle (Section 2). Meanwhile, PLF is chosen because it is a state-of-the-art approach to solve MALCR problem.\nCOMP grows a multi-agent tree via A* using informed by a heuristic of sum of shortest-to-goal paths from each agent. In joint-actions of single and diagonal moves, the diagonal ones are trimmed down to equal length with the single moves.\nPLF also builds a multi-agent tree and using priority planning to grows the tree. At the root of tree expansion, the planning order is shuffled and so a random leader is chosen; planning order is then fixed for downstream nodes. The followers plan to follow another agent if its preceding agent reached its goal. If the current expan-sion fails after some iterations, the planning starts again at the root with a new random leader and corresponding planning order.\nEnvironments and Instances: We evaluate MA-DL in five obstacle-rich environment types: Random Forest, Office, Waves, Rings, and Maze (Fig. 4). All environments are square shapes of size 114 \u00d7 114 m. On each environment type, there are one hundred maps generated with random locations of obstacles.\nFor each number of agents, one instance is generated on each environment map. So, there are total of 12000 instances for the experiments. The start and goal configurations are chosen randomly on alternate sides of the maps (except"}, {"title": "Env. Type 1: Random Forest:", "content": "(Fig. 4) Obstacles with random shapes and sizes are distributed randomly occupying 10% of the environment's area."}, {"title": "Env. Type 2: Office:", "content": "(Fig. 4) The environments consists multiple rooms and hallways. The room has a fixed width and varying length from 9-13 m. There are three long hallways along the building, 2\u20133 short hallways connecting them. The hallway width varies from 7\u20139 m."}, {"title": "Env. Type 3: Waves:", "content": "(Fig. 4) The environments have wave-like obstacles which are separated at regular distances. Gaps are placed in each wave with random widths. The number of waves is set to 10."}, {"title": "Env. Type 4: Rings:", "content": "(Fig. 4) The environments are featured by concentric rings with six breaks of random widths within 6-8 m. The separation between the rings is set to 8 m. For each instance, the starts are randomly placed at the center, and the goals are on one of four corners of the maps."}, {"title": "Env. Type 5: Maze:", "content": "(Fig. 4) The environments consist of mazes generated by Kruskal's algorithm with size of 14 \u00d7 14. To ease in generation of the starting and goal configurations, boundary walls connecting to the top-most and bottom-most rows are removed."}, {"title": "Measuring Performance:", "content": "We evaluate our framework against COMP and PLF baselines on 100 instances for each environment type and number of agent, measuring success-rate, average runtime and average distance traveled per agent. Planning is considered successful if all agents reach their goals within 5 s of runtime. Runs that exceed that planning ceiling are assigned a travel distance of 300 m. The framework is evaluated with variations over number of agents, goal configurations, runtime, and environment difficulty levels."}, {"title": "Computing Resources:", "content": "The experiments ran on HOPPER, a computing cluster provided by GMU's Office of Research Computing. Each planning instance is run single threaded, yet experiment were run in parallel across 48 cores on a 2.40GHz processor. Our code was developed in C++ and compiled with g++-9.3.0."}, {"title": "Results", "content": "Results when varying the agent number: We tested the MA-DL planner with 2-25 agents, allocating 5 seconds of runtime, and the results are shown in Fig. 4. MA-DL performs well with up to 25 agents, achieving over 90% success-rate in all environments except Maze Env. The long narrow passages of the maze mean that if some agents stop at the beginning of these passages, it becomes very difficult for others to pass through, leading to extended runtime for expanding the MATree.\nBoth baseline planners struggle as the number of agents is increased. PLF manages up to 5 agents in Rings Env. and only 3-4 agents in other environments. The heuristic of shortest-to-goal path sum guides COMP well to handle up 4 agents in Random Forest, Rings, and Waves Env. However, with greater than 6 agents, the high dimensionality of the search space means that planning cannot reach the goal in the allotted time and success rate quickly declines.\nIn addition to average success rate, we also report the mean and standard deviation for runtime and travel distance"}, {"title": "COMPLETENESS ANALYSIS", "content": "Though our MA-DL planner represents an advance over leader-follower planners by overcoming a limitation of fixed-priority-order that results in their incompleteness, MA-DL is also incomplete in this domain. We present an analysis of specifically what causes this incompleteness and present a direction to overcome this limitation for future work.\nMa et al. [11] proved the following theorem for the incomplete prioritized planning:\nTheorem 1: Prioritized (fixed) planning with an arbitrary priority ordering is incomplete for MAPF in general.\nBased on this theorem, fixed and platooning leader-follower approaches are incomplete planners. A fix leader-follower planner never changes the planning order, while a platooning planner only does so when members leave the team. How-ever, while our MA-DL planner overcomes this limitation, it is also an incomplete algorithm. This is a consequence of the greedy nature of the single agent planner (SAPF), which always immediately returns upon finding a valid single agent path. We illustrate a scenario in Fig. 7 that MA-DL fails to solve: though the solution requires that agent $a_3$ select a longer path to allow other agents to pass by, the SAPF planner returns only the shortest path and so the team becomes stuck.\nFuture work could explore extending our MA-DL ap-proach to achieve completeness. For this to be possible, the SAPF single-agent planner would need to be extended to eventually generate all possible paths when expanding each agent, a modification that would require careful consideration and detailed study so that it would not dramatically slow planning performance. In this research, our focus is on devel-oping a fast and practical planner that can efficiently handle the MALCR problem, outperforming existing approaches. We plan to develop a complete version of the MA-DL planner in the future research."}, {"title": "CONCLUSION", "content": "This paper introduces the MA-DL framework to handle the MALCR problem. The core advance of our approach is dy-namic leading, which enables the dynamic reselection of the leading agent during path planning whenever progress stalls, overcoming a key limitation of state-of-the-art platooning"}]}