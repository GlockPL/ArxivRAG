{"title": "MGM: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media", "authors": ["Muhammad Arslan Manzoor", "Ruihong Zeng", "Dilshod Azizov", "Preslav Nakov", "Shangsong Liang"], "abstract": "In the current era of rapidly growing digital data, evaluating the political bias and factuality of news outlets has become more important for seeking reliable information online. In this work, we study the classification problem of profiling news media from the lens of political bias and factuality. Traditional profiling methods, such as Pre-trained Language Models (PLMs) and Graph Neural Networks (GNNs) have shown promising results, but they face notable challenges. PLMs focus solely on textual features, causing them to overlook the complex relationships between entities, while GNNs often struggle with media graphs containing disconnected components and insufficient labels. To address these limitations, we propose MediaGraphMind (MGM), an effective solution within a variational Expectation-Maximization (EM) framework. Instead of relying on limited neighboring nodes, MGM leverages features, structural patterns, and label information from globally similar nodes. Such a framework not only enables GNNs to capture long-range dependencies for learning expressive node representations but also enhances PLMs by integrating structural information and therefore improving the performance of both models. The extensive experiments demonstrate the effectiveness of the proposed framework and achieve new state-of-the-art results. Further, we share our repository\u00b9 which contains the dataset, code, and documentation.", "sections": [{"title": "Introduction", "content": "The rise of the Internet has offered many opportunities to publish information and to express opinions (Mehta and Goldwasser, 2023b). Concurrently, this easy means of distribution has accelerated the spread of misinformation and disinformation online which resembles news in form but lacks the journalistic standards that ensure its quality (Fairbanks et al., 2018). Vosoughi et al. (2018) has found that \u201cfake news\u201d spreads six times faster and reaches much farther than real news. Any delay in profiling in rapidly evolving digital landscapes can lead to unchecked distribution of misleading content (Liu et al., 2022). Profiling news outlets through NLP pipelines offers a proactive approach by enabling the early detection of potentially unreliable sources as soon as they publish content. Since outlets with a history of biased or false information are more likely to do so again, profiling the media in advance allows us to quickly identify probable \"fake news\" by evaluating the reliability of the source itself. (Nakov et al., 2024).\nEarly studies on automatic media profiling relied solely on text characteristics (Battaglia et al., 2018; P\u00e9rez-Rosas et al., 2017), which has proven particularly challenging. The complexity increases when the text features contain indeterminate noise, leading to classification errors (Baly et al., 2018, 2020a). Moreover, traditional methods struggle to capture the intricate relationships between entities, such as media outlets, content they publish, and audiences. Panayotov et al. (2022) constructed media graphs: nodes represent media, and edges represent audience overlap between media. They proposed a framework that captures both inherent and implicit information about media through interactive learning within the media ecosystem, addressing the limitations of relying solely on textual features. We analyze these media graphs and identify two key challenges: disconnected components and label sparsity. Disconnected components prevent GNNs from capturing long-range dependencies, limiting their ability to learn expressive node representations for classification tasks (Longa et al., 2024; Zhang et al., 2024). Prior studies (Yin et al., 2024; Tang et al., 2024) address similar issues by using memory-based approaches that store global information throughout the graph using external memory modules. However, these methods require significant memory to store all node embeddings.\nTo tackle these challenges, we present MGM, a novel method based on a variational Expectation-Maximization (EM) framework that augments existing Graph Neural Networks (GNNs) to capture and exploit global information in media graphs. MGM seamlessly integrates local and global patterns, node features, and labels from globally similar nodes to enhance performance. Unlike Graph Attention Networks (GATs) (Veli\u010dkovi\u0107 et al., 2018), which focus solely on local neighborhoods, MGM employs an external memory module to store precomputed node representations of all nodes. This approach not only reduces computational costs (Fey et al., 2021) but also facilitates efficient node embedding retrieval. Furthermore, MGM optimizes memory usage by focusing on a small set of candidate nodes, guided by a Dirichlet prior distribution (He et al., 2020).\nThe experimental results show that MGM substantially enhances the performance of baseline GNNs, delivering a 10% increase across all evaluation measures on the Media Bias/Fact Check (MBFC)\u00b2 data feature in the ACL-2020 (Baly et al., 2020b) and the EMNLP-2018 (Baly et al., 2018) datasets. Despite the lack of rich node features in the media graph, we enhance the dataset by scraping Articles and Wikipedia descriptions for ACL-2020. Pre-trained language models (PLMs) such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019b), DistilBERT (Sanh et al., 2019), and DeBERTaV3 (He et al., 2021) are fine-tuned to predict political bias and factuality. Where textual data of media are inaccessible, MGM's representation-based probabilities compliment the gap. Moreover, integrating MGM's probabilities with PLMs enhance the performance for both tasks. Our contributions are as follows:\n\u2022 We introduce MGM, an efficient and expressive approach that enhances GNNs for reliable news media profiling by leveraging global information and minimizing memory requirements via a sparse distribution.\n\u2022 We illustrate that MGM consistently outperforms vanilla GNNs for the detection of factuality and political bias across all baselines.\n\u2022 We validate that integrating the MGM features with the PLMs enhances performance"}, {"title": "Related Work", "content": ""}, {"title": "Political Bias and Factuality of Media", "content": "Early research on political bias detection focused on the analysis of textual content (Afroz et al., 2012; Battaglia et al., 2018; P\u00e9rez-Rosas et al., 2017; Conroy et al., 2015). To improve the performance, subsequent research added contextual information (Baly et al., 2020b; Hounsel et al., 2020; Castelo et al., 2019; Fairbanks et al., 2018), including the nuances of multimedia production (Huh et al., 2018), the associated infrastructure (Hounsel et al., 2020), and the social context (Baly et al., 2020b). Guo et al. (2022) used BERT (Devlin et al., 2019) to model the linguistic political bias in news articles. Fan et al. (2019) used annotated media from Budak et al. (2016), analyzing articles for political bias using distant supervision. Various methods measured political bias, including analyzing Twitter interactions (An et al., 2012; Stefanov et al., 2020), often using small datasets only in English (Da San Martino et al., 2023; Nakov et al., 2023a,b; Barr\u00f3n-Cede\u00f1o et al., 2023a,b; Azizov et al., 2023; Spinde et al., 2022).\nLei et al. (2022) improved political bias detection through discourse structures, Liu et al. (2019a) detected frames in gun violence reporting, and Lee et al. (2022) proposed framework for neutral summaries. Bang et al. (2023) proposed a polarity minimization loss to reduce framing bias in multi-document summarization. Liu et al. (2023) addressed framing bias in event understanding with a neutral event graph induction framework using graph-based approaches. Maab et al. (2024) and Lin et al. (2024) leveraged LLMs and vector databases for adaptability and explainability. Contributions include frameworks for detecting political bias (Trhlik and Stenetorp, 2024), media credibility via retrieval-augmented generation (RAG) (Schlichtkrull, 2024), and scalable LLM bias assessment (Bang et al., 2024). Demszky et al. (2019) examined polarization on social media, Das et al. (2024) and Zhao et al. (2024) analyzed event relationships in media narratives, and Kameswari and Mamidi (2021) introduced a corpus quantifying media bias. Kim and Guerzhoy (2024) showcased LLMs role in analyzing U.S. cultural patterns and media-driven behaviors.\nThe veracity of the news media has been explored using PLMs to estimate the reliability of the source, correlated with the ratings of human experts (Yang and Menczer, 2023). Mehta and Goldwasser (2023a) introduced a framework that combines graph-based models, PLMs, and human experience to profile news media, effectively identifying \"fake news\u201d with minimal human input. Recent approaches, such as Baly et al. (2020b), used gold labels and various English sources as features to profile media with PLMs. Azizov et al. (2024) conducted a cross-lingual evaluation of political bias and factuality. Although the features of the aforementioned studies are obtained from various sources, they neglect the inherent relationships between the media.\nTo bridge this gap, graphs emerged as a comprehensive and effective framework for representation learning (Mehta et al., 2022). However, this study focused solely on the factuality task, despite having available political bias labels and generalizing only R-GCN. Mehta and Goldwasser (2023a) introduced a model that combines graphs, LLMs, and human input for profiling. Panayotov et al. (2022) constructed a graph based on the principle of homophily, suggesting that similar media sources attract similar audiences. The framework leveraged the audience overlap of media outlets to build a huge graph that models the interactions between media and to learn expressive representation for the nodes using GNNs. However, media graphs are characterized by disconnected components and scarce labels. To overcome these limitations, we propose MGM to effectively capture the information across the entire graph."}, {"title": "Graph Neural Networks", "content": "The current design of GNNs follows the message-passing framework (Yang et al., 2022; Chen et al., 2024; Zeng et al., 2024), where they learn node representations by aggregating information from local neighbors. However, media graphs suffer from challenges such as multiple disconnected components and limited labels, making it difficult for GNNs to capture long-range dependencies and to learn effective node representations (Longa et al., 2024; Zhang et al., 2024). Recent efforts to integrate external memory modules to store the embeddings of all nodes allow GNNs to capture long-range dependencies across graphs (Yin et al., 2024; Tang et al., 2024). In addition, relational GNNs (Zhang et al., 2024) and event relation graphs (Lei et al., 2022) improve the detection and analysis of political bias. However, these methods typically require storing embeddings for all nodes in the graph, resulting in high memory costs and low efficiency during testing. Unlike previous approaches, MGM focuses on a small set of candidate nodes, which are more likely to be selected as global similar nodes based on a Dirichlet prior distribution applied to the training nodes (Sethuraman, 1994)."}, {"title": "Methodology", "content": "In this section, we present the problem formulation and provide a detailed description of the proposed framework, which leverages features, structural patterns, and label information from globally similar nodes to enhance GNNs performance. Furthermore, MGM integrates with PLMs to overcome the limitations of existing textual features to detect factuality and political bias."}, {"title": "Problem Formulation", "content": "We formulate the news media profiling task as a node classification problem in the semi-supervised graph learning setting (Kipf and Welling, 2016; Veli\u010dkovi\u0107 et al., 2018), where each node represents a news media outlet, the edges capture relationships such as audience overlap, and the node label indicates political bias or factuality, which are available only for a small subset of nodes. Specifically, let $G = {V,E,X,Y^\u00b9}$ represents a partially-labeled graph, where $V = {v_i}_{i=1}^N$ is a set of nodes, $E$ is a set of edges, and $N$ is the total number of nodes. The node features are denoted as $X \u2208 R^{N\u00d7F}$, where $F$ is the feature dimension. Since most nodes are unlabeled, $V$ can be divided into labeled nodes $V^l$ with labels $Y^l$, and unlabeled nodes $V^u$. The labels $Y^l \u2208 R^{N_l\u00d7C}$ are in a one-hot form, where $N_l$ and $C$ represent the number of labeled nodes and the number of classes, respectively. The goal of semi-supervised learning is to learn the model parameters $\u03b8$ by maximizing the marginal distribution of the overall labeled nodes, i.e., $p_\u03b8(Y^l | X,E) = \u03a0_{\u03b7\u2208V^l} p_\u03b8(y_\u03b7 | X, E)$ on the training graph."}, {"title": "The MGM Framework", "content": "Following (Qu et al., 2019, 2021), we adopt a probabilistic framework for node classification, treating node representations $Z$ as latent variables determined by a GNN. To improve the performance of the model, we propose to augment the GNNs with information about global similar nodes, i.e., nodes in the entire graph that have similar node features and local geometric structures. Specifically, we denote the set of global similar nodes of node $n$ as $t_n \u2208 {0,1}$, where $t_{nm} = 1$ indicates that node $m$ is a global node similar to $n$. Similarly to node representations, we also regard the similar node indicator $t_n$ as a latent variable. Therefore, the joint probability distribution of global information-enhanced method can be factorized as follows:\n$p_\u03b8(Y^l, T, Z | X,E) =$\n$= p_\u03b8(Z | X, E) p_\u03b8(T | Z) p_\u03b8(Y^l | Z, T),$\nwhere $T = [t_\u03b7]_{\u03b7\u2208V^l}$ are the global similar nodes of all nodes.\nHowever, finding global similar nodes with node representations requires computing representations for all nodes, which is expensive in terms of space and time (Fey et al., 2021). To alleviate this, we propose to store the embeddings of the labeled nodes in the memory and to use them to find global similar nodes. Consequently, the distribution of $T$ can be replaced by $p_\u03b8(T | \\hat{Z})$, where $\\hat{Z}$ is the embeddings of the labeled nodes in the memory, i.e., $V^l$. In this case, we can directly retrieve the representation from memory without computing representations for all nodes, thus making it more efficient to obtain the distribution of global similar nodes for both training and prediction.\nTo reduce the memory size, we select global similar nodes from a small set of candidate nodes, which are a subset of the training nodes. As a result, only the embeddings of these candidate nodes are stored in memory for prediction. To achieve this, we assume that $p_\u03b8(T | \\hat{Z})$ is a sparse distribution, concentrated on a few candidate nodes. Since the candidate set is not known, we introduce a latent variable $w_i$ for each node $i$, where $\u03c9_i \u2208 [0, 1]$, s.t. $\u03a3_{i=1}^{N_l} w_i = 1$. Here, $w_i$ represents the probability that the $i$-th node in the labeled node is a candidate node. Inspired by (He et al., 2020), we introduce a prior over $\u03c9$, i.e. $p_\u03b1(\u03c9)$ with parameter $\u03b1$. This prior is designed to encourage a sparse distribution over $\u03c9$. Therefore, the joint distribution of the method is now defined as:\n$p_\u03b8(Y^l, T, Z, \u03c9 | X, E, \\hat{Z}) = p_\u03b1(\u03c9)$ \n$p_\u03b8(Z | X, E) p_\u03b8(T | \u03c9, \\hat{Z}) p_\u03b8(Y^l | T, Z) .$\nNext, we introduce the parameterization of our probabilistic framework.\nPrior distribution over $w$. We use the Dirichlet distribution as the prior distribution over $w$, i.e., $p_\u03b1(\u03c9) \u03b1 \u03a0_{i=1}^{N_l} w_i^{\u03b1_i-1}$, where $\u03b1_i$ is the concentration parameter of the distribution. The concentration parameter \u03b1 is a positive value and a smaller value of \u03b1 prefers a sparser distribution over $w$ (He et al., 2020). In our experiments, we set $\u03b1 < 1$ to encourage the sparse nodes distribution.\nPrior distribution over node representations Z. We model the prior distribution over node representations as Gaussian distributions (Bojchevski and G\u00fcnnemann, 2018), which are obtained with GNNs due to their effectiveness in graph-learning tasks. Therefore, the prior distribution over $Z$ is defined as follows:\n$p_\u03b8(Z | X,E) = N(Z | GNN_\u0398(X, E), \u03c3I),$\nwhere $\u03c3I$ is the learned variance of the prior and $GNN_\u0398$ is an $L$-layer GNN with parameter $\u0398$.\nPrior distribution over T. To obtain global similar nodes of node $n$, we define a prior distribution over $T$ as follows:\n$p_\u03b8(T | w, \\hat{Z}) = Mul(T | K, f_\u03b8(w, \\hat{Z})),$\nwhere $Mul(\u00b7)$ represents the multinomial distribution, $K$ denotes the predefined number of global similar nodes, and $f_\u03b8$ is designed as a parameterized function that outputs the parameters of the multinomial distribution.\nPrediction of label Y. Finally, we use node representation $Z$ and information from global similar nodes to predict the label. Specifically, we leverage the labels of global similar nodes and first predict the label based on its representation:\n$p_\u03b8(Y | Z) = Cat(Y | Z),$\nwhere $p_\u03b8 (Y | Z)$ is formulated as a categorical distribution. Then, we predict the label using the labels of global similar nodes:\n$p_\u03b8 (Y | T) \u221d \u03a3_{N, M\u2208V^l} T_{NM} Y_M,$\nwhere $T_{NM}$ represents the indices of global similar nodes for the predicted nodes set $N$. Furthermore, $Y_M$ denotes the one-hot labels of the nodes in $M$, where $M$ is the set of global similar nodes. Finally, the predicted label distribution is defined as follows:\n$p_\u03b8(Y | Z, T) = \u03b7p_\u03b8(Y | Z)$\n$+ (1 \u2212 \u03b7) p_\u03b8(Y | T),$\nwhere $\u03b7 \u2208 [0,1]$ is a trade-off hyper-parameter. When $\u03b7 = 1$, our model only uses local representations of nodes for prediction, which degrades to vanilla GNNs. In contrast, when $0 < \u03b7 < 1$, our model predicts the labels of the nodes using information from both local neighbors and global similar nodes."}, {"title": "Training Process of MGM", "content": "Next, we explain how to learn the model parameters $\u0398$ based on the graph. Ideally, the marginal likelihood should be optimized during training:\n$p_\u03b8 (Y^l | X, E, \\hat{Z}) =$\n$= \u222b\u222b p_\u03b8(Y^l, T, Z, \u03c9 | X, E, \\hat{Z}) dZdw .$\nHowever, the computation of maximizing the marginal likelihood is intractable due to the marginalization of latent variables. As a result, we develop a variational Expectation-Maximization (EM) algorithm (Qu et al., 2019) to optimize its evidence lower bound (ELBO) instead:\n$L_{ELBO}(Y^l; \u03b8, \u03c6, \u03b1, \u03bb) = \u2212D_{KL}[q_\u03bb(\u03c9) || p_\u03b1(\u03c9)]$\n$\u2212 D_{KL} [q_\u03c6(Z | T, Y^l) || p_\u03b8(Z | X, E)]$\n$\u2212 D_{KL} [q_\u03c6(T | Y^l) || p_\u03b8 (T | w, \\hat{Z})]$ \n$+ E_{q_\u03c6(T|Y^l) q_\u03c6(Z|T,Y^l)} [logp_\u03b8 (Y^l | T,Z)],$\nwhere $D_{KL} [\u00b7||\u00b7]$ is the Kullback-Leibler (KL) divergence, $q$ represents the variational distribution to approximate the model posterior distribution and adheres to the following factorization form:\u00b3\n$q_\u03bb(\u03c9)q(T, Z, \u03c9 | Y^l)q(T | Y^l)q(Z | T, Y^l),$\nwhere $\u03c6$ and $\u03bb$ are variational parameters.\nNote that we use the mean-field assumption to approximate the posterior of $w$ to simplify the variational distributions. For computational convenience, we assume that the variational distributions of these latent variables have the same distribution form as their prior distributions. Hence, we define the variational distributions of $\u03c9$, $T$ and $Z$ to be Dirichlet, multinomial, and Gaussian distributions, respectively.\nNote that the KL divergence in Equation (9) has a closed-form solution, and we approximate the expectation using a Monte Carlo method by sampling from the variational distributions. In variational EM, the variational parameters $\u03c6$ and the model parameters $\u0398$ are learned alternately. In the E-step, we fix $\u0398$ and update $\u03c6$ by minimizing the KL divergence to approximate the true posteriors. In the M-step, we fix $\u03c6$ and update $\u0398$ by maximizing the expected log-likelihood."}, {"title": "Prediction Process of MGM", "content": "After training, we expect to obtain a sparse distribution $q_\u03bb(w)$, allowing us to select a subset of the candidate nodes. In this case, we can select candidate nodes over a certain probability threshold, thus reducing the memory size and improving the efficiency for prediction. Specifically, we calculate the expected value of $q_\u03bb(w)$ for each node $i$, which is given by $E_{q_\u03bb(w)} [W] = \u03bb_i/\u03a3_i \u03bb_i$, and then we select the top-M nodes that occupy 90% of the probability mass as candidate nodes.\nWe then leverage the embeddings of the memorized candidate nodes $\\hat{Z}\u03c9$ and $p_\u03b8$ to predict the labels of the test nodes $\u00f1$ based on Equation (7). We also provide an overview of the optimization process of the MGM model for the news media profiling in Algorithm 1."}, {"title": "Enhancing PLMs Predictions with MGM", "content": "Next, we demonstrate how MGM improves the performance of PLMs by incorporating information from global similar nodes. Given textual features $S$, such as those from Articles and Wikipedia pages for the media outlet, we first fine-tune the PLMs using the cross-entropy loss. Then, we concatenate the predicted label distribution from the PLMs with MGM to obtain the final label distribution:\n$p_{\u03c8,\u0398}(Y | S, Z, T) =$\n$= Softmax((\u03c8(p_Y(Y | S), p_\u03b8(Y | Z, T))W + b),$\nwhere $\u03c8$ are the parameters of the fine-tuned PLMs, $\u2299$ is the concatenation operation, $p_Y(Y | S)$ is the label distribution predicted by the fine-tuned PLMs, $p_\u03b8(Y | Z, T)$ is the label distribution predicted by MGM, which is based on Equation (7), $W$ and $b$ are the parameters of the linear classifier. More details are given in Figure 3 and Appendix D."}, {"title": "Experiments", "content": ""}, {"title": "Research Questions", "content": "We explore the following research questions (RQs):\n\u2022 (RQ1) Can MGM tackle disconnected components and label sparsity in media graphs for factuality and political bias detection tasks?\n\u2022 (RQ2) How do the number of global similar nodes K and the trade-off hyper-parameter \u03b7 affect the performance of MGM?\n\u2022 (RQ3) How does the memory module affect the performance of MGM?\n\u2022 (RQ4) How does MGM elevate the performance of PLMs when faced with the challenge of missing text in Wikipedia or Articles?"}, {"title": "Dataset", "content": "The dataset for factuality and political bias of news media introduced by Baly et al. (2020b) comprises 859 media sources, their domain names and corresponding gold labels. These labels are sourced from MBFC\u00b3, a platform supported by independent journalists. Factuality is given on a three-point scale: high, mixed, and low. Political bias is also on a three-point scale: left, center, right. Panayotov et al. (2022) used Alexa Rank to create a graph based on audience overlap, using the 859 media as seed nodes. Media sources that shared the same audience, as determined by Alexa Rank, were connected with an edge. Alexa Rank returned a maximum of five similar media sources for each medium, which could be part of the initial seed nodes or newly identified media. As depicted in Figure 1, BBC, MSNBC, and Reuters are listed as media sources in the MBFC dataset. The Alexa tool identified five related media for BBC and MSNBC, with an edge connecting them due to their shared audience overlap. In the resulting graph, the nodes represent the media sources, and the edges represent the percentage of audience overlap between two media. We use these publicly available graph data (the only one of its kind) to train GNNs for the factuality and political bias of the news media."}, {"title": "Discussion", "content": ""}, {"title": "Overall Performance", "content": "To answer RQ1, we conduct factuality and political bias classification experiments in a semi-supervised setting. The experimental results reported in Table 1 demonstrate that MGM can improve the performance of existing GNNs in almost all cases. For example, when applied to the Fact-2020 dataset, MGM improves the Macro-F1 performance of GCN, GAT, SGC, and DNA by 17.5%, 9.8%, 18.5%, and 11.4%, respectively. Similarly, for Bias-2020, we can observe that GNNs equipped with MGM consistently outperform the corresponding base models in all evaluation measures.\nWe conducted a series of experiments using different proportions of training labels to assess the performance of MGM as shown in Table 3. The results indicate a clear trend: as we increase the percentage of training labels, the model performance improves significantly compared to the baseline. Due to limited data, using a smaller percentage of training labels results in modest improvements over the baseline, constraining the model's ability to generalize well to unseen data. MGM effectively addresses RQ1 by leveraging global similar nodes in media graphs with disconnected components and label sparsity for the detection of factuality and political bias. Our evaluation extends to Fact-2018 and depicts MGM's stable performance across different datasets presented in Table 5. The results show that MGM is able to consistently improve all the baselines. Given the reasons described in Appendix A, experiments are not conducted on the political bias task of EMNLP-2018."}, {"title": "Impact of the Number of Global Similar Nodes", "content": "Next, we turn to RQ2 to understand the impact of the number of global similar nodes K. Specifically, we investigate the performance of MGM with different values of K. As shown in Figures 2(a) and 2(b), leveraging a few global similar nodes can improve the performance of the base GNNs. For example, both GCN and SGC exhibit similar patterns, peaking in performance at K=3 on the factuality task. The performance of GNNs enhanced with MGM decreases when K exceeds a certain threshold. This is attributed to the introduction of noise by incorporating excessive information from numerous global similar nodes."}, {"title": "Impact of the Trade-off Hyper-Parameter", "content": "Recall that in Section 3.2, we introduced a hyper-parameter \u03b7 that influences the predicted label distribution. When \u03b7 = 1, MGM only relies on local node representations for prediction, degrading to a vanilla GNN. In contrast, when \u03b7 < 1, our model incorporates information from both local neighbors and global similar nodes to predict the node labels. To further investigate the impact of the trade-off hyper-parameter \u03b7, we analyze the sensitivity of MGM to its value. The experimental results are shown in Figures 2(c) and 2(d). We find that compared to \u03b7 = 1, MGM yields improved performance when \u03b7 < 1 in most cases. For example, GCN achieves its best performance when integrated with MGM using an \u03b7 value of 0.8. As a result, the effectiveness of incorporating information from global similar nodes highlighted in the results validates the RQ2."}, {"title": "Effectiveness of the Memory Module", "content": "Recall that in Section 3.4, MGM leveraged a Dirichlet prior to select a small set of candidate nodes and stored their node embeddings in the sampled memory (Ms) for prediction. To compare the effectiveness of the sampled memory module to the full memory module (MG), which stores all the training node embeddings, we conducted a performance comparison between the two memory modules. The experimental results are given in Table 2, and they answer RQ3 that MGM using sampled memory achieves a performance comparable to MGM when using full memory. For example, for the GAT model, the performance is higher when using sampled memory compared to when using full memory. This suggests that the sampled memory effectively captures sufficient information, allowing MGM to maintain its performance even with limited memory. The experimental results on the Fact-2018 dataset reported in Table 4 also show consistent trends which validate the versatility of the memory module for media graphs."}, {"title": "Impact of Integrating MGM with PLMs", "content": "To answer RQ4, we integrate the MGM probabilities with those from deep learning models based on textual features, and we observe that this substantially enhances the performance. Initially, with zero probabilities for missing textual features, we achieved accuracies of 68.02% for political bias in Articles, 63.37% for Wikipedia, 63.37% for factuality in Articles, and 61.05% for Wikipedia, respectively (see Table 6). Replacing the zero probabilities with the best MGMGATv2 improve the performance by up to 30%. Further concatenating the best model probabilities in stage three led to additional gains, and in stage four, our models outperformed previous state-of-the-art results in political bias and factuality (Panayotov et al., 2022; Mehta et al., 2022) (can be seen in Table 7)."}, {"title": "Conclusion & Future Work", "content": "Our study focused on the underexplored problem of profiling news media in terms of factuality and political bias. To address the shortcomings of existing media graphs, we introduced MediaGraphMind (MGM), an innovative EM framework that significantly enhances the performance of GNNs by leveraging globally similar nodes. The external memory module of MGM efficiently stores and retrieves node representations, addressing the challenge of test-time inefficiency by selecting global similar nodes from a smaller candidate set based on a sparse node selection distribution. Our experiments demonstrate that the integration of MGM features with PLMs consistently improves over existing baselines and establishes a new state-of-the-art results.\nIn future work, we plan to explore multi-graph fusion, multi-task learning, and ordinal classification for diverse graph structures in media profiling."}, {"title": "Limitations", "content": "The graph dataset, originating from the ACL-2020 media nodes, was constructed using the Alexa Rank siteinfo tool, which is currently unavailable. Although the graph aids in the task by capturing the inherent and hidden relationships between media, building such graphs is complex and resource-intensive. The research largely relies on U.S.-centric definitions of political bias (left/center/right), which may not accurately capture the nuanced ideological biases present in news outlets from other cultural or political contexts. Moreover, the available graph is limited to the 2020 dataset. We are actively working on constructing graphs for the latest benchmarks, which include a larger number of media sources and updated MBFC rankings. Moreover, we faced limitations in collecting Articles and Wikipedia texts from media sources from the ACL-2020 dataset due to the inaccessibility of their websites."}, {"title": "Ethical Statement", "content": "Optimizing model architectures to enhance energy efficiency in training and inference operations is crucial to reducing environmental impact. Instead of relying on extensive computational resources to train complex models, which significantly increase carbon emissions, we propose improving model performance with less computational power. The Articles from the news media pages were compiled in strict compliance with legal and ethical standards. We carefully reviewed the terms of use for all websites to ensure that our data collection processes adhered to them. Our compilation focused solely on publicly available data, avoiding paywalls and subscription models. Transparent data collection methods were designed to minimize the impact on source websites, including limiting the access frequency to prevent resource strain."}]}