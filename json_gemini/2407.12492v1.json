{"title": "Test-Time Adaptation with State-Space Models", "authors": ["Mona Schirmer", "Dan Zhang", "Eric Nalisnick"], "abstract": "Distribution shifts between training and test data are all but inevitable over the lifecycle of a deployed model and lead to performance decay. Adapting the model can hopefully mitigate this drop in performance. Yet, adaptation is challenging since it must be unsupervised: we usually do not have access to any labeled data at test time. In this paper, we propose a probabilistic state-space model that can adapt a deployed model subjected to distribution drift. Our model learns the dynamics induced by distribution shifts on the last set of hidden features. Without requiring labels, we infer time-evolving class prototypes that serve as a dynamic classification head. Moreover, our approach is lightweight, modifying only the model's last linear layer. In experiments on real-world distribution shifts and synthetic corruptions, we demonstrate that our approach performs competitively with methods that require back-propagation and access to the model backbone. Our model especially excels in the case of small test batches-the most difficult setting.", "sections": [{"title": "Introduction", "content": "Predictive models often have an 'expiration date.' Real-world applications tend to exhibit distribution drift, meaning that the data points seen at test time are drawn from a distribution that is different than the training data's. Moreover, the test distribution usually becomes more unlike the training distribution as time goes on. An example of this is with recommendation systems: trends change, new products are released, old products are discontinued, etc. Unless a model is updated, its ability to make accurate predictions will expire, requiring the model to be taken offline and re-trained. Every iteration of this model life-cycle can be expensive and time consuming. Allowing models to remain 'fresh' for as long as possible is thus an open and consequential problem.\nIn this work, we propose State-space Test-time ADaptation (STAD), a method that delays the failure of a deployed model by performing unsupervised adaptation at test time. We perform this updating by modeling the dynamics of the parameters at a neural network's final layer, making our approach widely applicable and computationally lightweight. Specifically, we use a state-space model (SSM) to track how the weight vectors in the final layer-where each vector corresponds to a class-evolve under distribution drift. To generate predictions for the newly-acquired batch of test points, we use the SSM's fitted cluster means as the model's updated parameters. We focus on natural data shifts caused by a gradually changing environment rather than noise-based shift, which has been the focus of previous work [37, 38]. Our contributions are as follows,\n\u2022 In Section 3.2, we present STAD, a state-space model to learn the dynamics of how a classifier's last-layer weights evolve under distribution shift, without access to any labels. No previous work has explicitly modeled these dynamics, which we demonstrate is crucial via an ablation study.\n\u2022 In Sections 3.2 and 3.3, we provide two implementations of STAD\u2014one using Gaussian distributions and one using von Mises-Fisher distributions. Each represents a different assumption about the geometry of the model's last-layer representations."}, {"title": "Problem Setting", "content": "Data & Model We focus on the traditional setting of multi-label classification, where $X \\subset \\mathbb{R}^D$ denotes the input (feature) space and $Y \\subseteq \\{1, . . ., K\\}$ denotes the label space. Let $x$ and $y$ be random variables and $P(x, y) = P(x) P(y|x)$ the unknown source data distribution. We assume $x \\in X$ and $y \\in Y$ are realisations of x and y. The goal of classification is to find a mapping $f_\\theta$, with parameters $\\theta$, from the input space to the label space $f_\\theta : X \\rightarrow Y$. Fitting the classifier $f_\\theta$ is usually accomplished by minimizing an appropriate loss function (e.g. log loss). Yet, our method is agnostic to how $f_\\theta$ is trained and therefore easy to use with, for instance, a pre-trained model that has been downloaded from the web.\nDistribution Drift and Unsupervised Adaptation In predictive modeling, a classifier will almost always become 'stale,' as the test conditions inevitably change from those observed during training. In fact, the model's presence in the world can cause this change: a model that predicts when someone is likely to be infected with a contagious disease will, if effective and widely adopted, reduce the prevalence of that disease. Thus, we want our models to be 'fresh' for as long as possible and continue to make accurate predictions, in spite of data drift. More formally, let the data at test-time t be sampled from a distribution $Q_t(x, y) = Q_t(x) Q_t(y|x)$ such that $Q_t(x, y) \\neq P(x, y) \\forall t > 0$. Of course, we do not observe labels at test time, and hence we observe only a batch of features $X_t = \\{x_{1,t}, ..., x_{N_t,t}\\}$, where $x_{n,t} \\sim Q_t(x)$ (i.i.d.). Given the t-th batch of features $X_t$, the goal is to adapt $f_\\theta$, forming a new set of parameters $\\theta_t$ such that $f_{\\theta_t}$ has better predictive performance on $X_t$ than $f_\\theta$ would have. Since we can only observe features, we assume that the distribution shift must at least take the form of covariate shift: $Q_t(x) \\neq P(x) \\forall t > 0$. $Q_t(y|x)$ could shift as well, but this will be impossible to detect in our assumed setting. If $Q_t(y|x)$ does shift, it must do so modestly, as the classifier will become completely invalid if the relationship between features and labels changes to a severe degree. This setting is known as continual test-time adaptation (CTTA) [38, 15, 31, 29, 35, 12]."}, {"title": "Modelling the Dynamics of Distribution Shifts in Representation Space", "content": "We now present our method: the core idea is that CTTA can be done by tracking how distribution shift affect the model's representations. We employ linear state-space models (SSMs) to capture how test points evolve and drift. The SSM's cluster representations then serve as an adaptive classification head that evolves with the non-stationarity of the distribution drift. In Section 3.2, we first introduce the general model and then in Section 3.3, we propose an implementation that leverages the von-Mises-Fisher distribution to model hyperspherical features."}, {"title": "Modeling Shift in Representation Space", "content": "Whereas previous work has mostly attempted to adapt all levels of a neural network, we take the alternative approach of adapting only the last layer. Let the classifier $f_\\theta$ be a neural network having $L$ total layers. We will treat the first $L - 1$ as a black box, assuming they transform the original feature vector x into a new (lower dimensional) representation, which we denote as h. The original classifier then maps these representations to the classes as: $E[y|h] = \\text{softmax}_y (W_0h)$, where $\\text{softmax}_y (\\cdot)$ denotes the dimension of the softmax's output corresponding to the y-th label index and $W_0$ are the last-layer weights. As $W_0$ will only be valid for representations that are similar to the training data, we will discard these parameters when performing CTTA, learning new parameters $W_t$ for the t-th time step. These new parameters will be used to generate the adapted predictions through the same link function: $E[y|h] = \\text{softmax}_y (W_th)$.\nIn the setting of CTTA, we observe a batch of features $X_t$. Passing them through the model yields corresponding representations $H_t$, and this will be the 'data' used for the probabilistic model we will describe below. Specifically, we will model how the representations change from $H_t$ to $H_{t+1}$. Operating on this last set of hidden representations has several benefits. Firstly, test-time adaptation"}, {"title": "A Probabilistic Model of Shift Dynamics", "content": "We now describe our general method for adaptive last-layer parameters. We assume that, while the representations $H_t$ are changing over time, they are still maintaining some class structure in the form of clusters. Our model will seek to track this structure as it evolves over time. Using latent variables $w_{t,k}$, we will assume each representation is drawn conditioned on K latent vectors: $h_{t,n} \\sim p(h_t | W_{t,1},..., W_{t,K})$, where K is equal to the number of classes in the prediction task. After fitting the unsupervised model, the K latent vectors will be stacked to create $W_t$, the last-layer weights of the adapted predictive model (as introduced in 3.1). For the intuition of the approach, see Figure 1a. The blue red, and green clusters represent classes of a classification problem. As the distribution shifts from time step t 1 to t = 3, the class clusters shift in representation space. Our SSM aims to track this movement so that classification performance can still be preserved. We now move on to a technical description.\nNotation and Variables Let $H_t = (h_{t,1},..., h_{t,N_t}) \\in \\mathbb{R}^{D \\times N_t}$ denote the neural representations for $N_t$ data points at test time t. Let $W_t = (w_{t,1},..., w_{t,K}) \\in \\mathbb{R}^{D \\times K}$ denote the K weight vectors at test time t. As discussed above, the weight vector $w_{t,k}$ can be thought of as a latent prototype for class k at time t. We denote with $C_t = (c_{t,1}, . . ., c_{t,N_t}) \\in \\{0,1\\}^{K \\times N_t}$ the $N_t$ one-hot encoded latent class assignment vectors $c_{t,n} \\in \\{0,1\\}^K$ at time t. The k-th position of $c_{t,n}$ is denoted with $c_{t,n,k}$ and is 1 if $h_{t,n}$ belongs to class k and 0 otherwise. Like in standard (static) mixture models [5], the prior of the latent class assignments $p(c_{t,n})$ is a categorical distribution, $p(c_{t,n}) = Cat(\\pi_t)$ with $\\pi_t = (\\pi_{t,1},..., \\pi_{t,K}) \\in [0,1]^K$ and $\\sum_{k=1}^K \\pi_{t,k} = 1$. The mixing coefficient $\\pi_{t,k}$ gives the a priori probability of class k at time t and can be interpreted as the class proportions.\nDynamics Model We model the evolution of the K prototypes $W_t = (w_{t,1},..., w_{t,K})$ with K independent Markov processes. The resulting transition model is then:\nTransition model: $p(W_t | W_{t-1}, \\gamma^{\\text{trans}}) = \\prod_{k=1}^K P(w_{t,k} | w_{t-1,k}, \\gamma^{\\text{trans}}),$"}, {"title": "A Probabilistic Model of Shift Dynamics", "content": "where $\\gamma^{\\text{trans}}$ denote the parameters of the transition density, notably the transition noise. At each time step, the feature vectors $H_t$ are generated by a mixture distribution over the K classes,\nEmission model: $p(H_t | W_t, \\gamma^{\\text{ems}}) = \\prod_{n=1}^{N_t} \\sum_{k=1}^K \\pi_{t,k}p(h_{t,n} | w_{t,k}, \\gamma^{\\text{ems}}).$\nwhere $\\gamma^{\\text{ems}}$ are the emission parameters. We thus assume at each time step a standard mixture model over the K classes where the class prototype $w_{t,k}$ defines the latent class center and $\\pi_{t,k}$ the mixture weight for class k. The joint distribution of representations, prototypes and class assignments can be factorised as follows,\n$p(H_{1:T}, W_{1:T}, C_{1:T}) = p(W_1) \\prod_{t=1}^T p(C_t)p(H_t | W_t, C_t, \\gamma^{\\text{ems}}) \\prod_{t=2}^T p(W_t | W_{t-1}, \\gamma^{\\text{trans}})$\n$= \\prod_{k} P(w_{1,k}) \\prod_{t=1}^T \\prod_{n=1}^{N_t} p(c_{t,n}) [p(h_{t,n} | w_{t,k}, \\gamma^{\\text{ems}})^{c_{t,n,k}}] \\prod_{t=2}^T \\prod_{k} p(w_{t,k} | w_{t-1,k}, \\gamma^{\\text{trans}}).$\nWe use the notation $H_{1:T} = \\{H_t\\}_{t=1}^T$ to denote the representation vectors $H_t$ for all time steps and analogously for $W_{1:T}$ and $C_{1:T}$. The model's plate diagram is depicted in Figure 1b. The representation $H_{1:T}$ are the observed variables and are determined by the latent class prototypes $W_{1:T}$ and the latent class assignments $C_{1:T}$.\nPosterior Inference & Adapted Predictions The primary goal is to update the class prototypes $W_t$ with the information obtained by the $N_t$ representations of test time t. At each test time t, we are thus interested in the posterior distribution of the prototypes $p(W_t | H_{1:t})$. Once $p(W_t | H_{1:t})$ is known, we can update the classification weights with the new posterior mean. The class weights $W_t$ and class assignments $C_t$ can be inferred using the Expectation-Maximization (EM) algorithm. In the E-step, we compute $p(W_{1:T}C_{1:T} | H_{1:T})$. In the M-Step, we maximize the expected complete-data log likelihood with respect to the model parameters:\n$\\Phi^* = \\text{arg} \\max_\\Phi E_{p(W,C | H)} [\\log p(H_{1:T}, W_{1:T}, C_{1:T})]$,\nwhere $\\Phi$ denotes the parameters of the transition and emission density as well as the mixing coefficients, $\\Phi = {\\gamma^{\\text{trans}}, \\gamma^{\\text{ems}}, \\pi_{1:T}}$. After one optimization step, we collect the K class prototypes into a matrix $W_t$. Using the same hidden representations used to fit $W_t$, we generate the predictions using the original predictive model's softmax parameterization:\n$Y_{t,n} \\sim Cat \\left(y_{t,n} ; \\text{softmax}(W_t^T h_{t,n})\\right)$\nwhere $y_{t,n}$ denotes a prediction sampled for the representation vector $h_{t,n}$. Note that adaptation can be performed in an online fashion by optimizing Equation (5) incrementally considering points up to point t. To omit computing the complete-data log likelihood for an increasing sequence as time goes on, we employ a sliding window approach.\nGaussian Model The simplest parametric form for the transition and emissions models is Gaussian. In this case, the transition noise follows a multivariate Gaussian distribution with zero mean and global covariance $\\Sigma^{\\text{trans}} \\in \\mathbb{R}^{D \\times D}$. The resulting model can be seen as a mixture of K Kalman filters (KFs). For posterior inference, thanks to the linearity and Gaussian assumptions, the posterior expectation $E_{p(W,C | H)}[\\cdot]$ in Equation (5) can be computed analytically using the well known KF predict, update and smoothing equations [6, 5]. However, the closed-form computations come at a cost as they involve matrix inversions of dimensionality D \u00d7 D. Moreover, the parameter size scales $K \\times D^2$, risking overfitting and consuming substantial memory. These are limitations of the Gaussian formulation making it costly for high-dimensional feature spaces and impractical in low resource environments requiring instant predictions. In the next section, we discuss a model for spherical features that elegantly circumvents the limitations of a fully parameterized Gaussian formulation."}, {"title": "Von Mises-Fisher Model for Hyperspherical Features", "content": "Choosing Gaussian densities for the transition and emission models, as discussed above, assumes the representation space follows an Euclidean geometry. However, prior work has shown that assuming the hidden representations lie on the unit hypersphere results in a better inductive bias for OOD generalization [26, 2]. This is due to the norms of the representations being biased by in-domain information such as class balance, making angular distances a more reliable signal of class membership in the presence of distribution shift [26, 2]. We too employ the hyperspherical assumption by normalizing the hidden representations such that $\\lVert h \\rVert_2 = 1$ and model them with the von Mises-Fisher (vMF) distribution [25],\n$vMF(h; \\mu_k, \\kappa) = C_D(\\kappa) \\exp {\\kappa \\cdot \\mu_k^T h}$"}, {"content": "where $\\mu_k \\in \\mathbb{R}^D$ with $\\lVert \\mu_k \\rVert_2 = 1$ denotes the mean direction of class k, $\\kappa \\in \\mathbb{R}^+$ the concentration parameter, and $C_D(\\kappa)$ the normalization constant. High values of $\\kappa$ imply larger concentration around $\\mu_k$. The vMF distribution is proportional to a Gaussian distribution with isotropic variance and unit norm. While previous work [26, 27, 2] has mainly explored training objectives to encourage latent representations to be vMF-distributed, we apply Equation (7) to model the evolving representations.\nHyperspherical State-Space Model Returning to the SSM given above, we specify both transition and emission models as vMF distributions,\nHyperspherical transition model: $p(W_t|W_{t-1}) = \\prod_k vMF(w_{t,k}|w_{t-1,k}, \\kappa^{\\text{trans}})$"}, {"content": "Hyperspherical emission model: $p(H_t|W_t) = \\prod_{n=1}^{N_t} \\sum_{k=1}^K \\pi_{t,k} vMF(h_{t,n}|w_{t,k}, \\kappa^{\\text{ems}})$,\nThe parameter size of the vMF formulation only scales linearly with the feature dimension, i.e. $O(DK)$ instead of $O(D^2K)$ as for the Gaussian case. The noise parameters, $\\kappa^{\\text{trans}}, \\kappa^{\\text{ems}}$ are scalar values, and we shall use the reduced parameter size to experiment with class conditioned noise parameters $\\kappa_k^{\\text{trans}}, \\kappa_k^{\\text{ems}}, k = 1,..., K$ in Section 5. Figure 2 illustrates this STAD-vMF variant.\nPosterior Inference Unlike in the linear Gaussian case, the vMF distribution is not closed under marginalization. Consequentially, the posterior distribution required for the expectation in Equa- tion (5), $p(W_{1:T}C_{1:T}|H_{1:T})$, cannot be obtained in closed form. We employ a variational EM objective, approximating the posterior with mean-field variational inference, following Gopal and Yang [16]:\n$q(w_{t,k}) = vMF(\\cdot; \\rho_{t,k}, \\tau_{t,k}) \\quad q(c_{n,t}) = Cat(; \\lambda_{n,t}) \\quad \\forall t, n, k.$\nThe variational distribution $q(W, C)$ factorizes over n,t,k and the objective from Equation (5) becomes $\\text{arg} \\max_\\Phi, E_{q(w,c)} [\\log p(H_{1:T}, W_{1:T}, C_{1:T})]$. More details as well as the full maximisa- tion steps for $\\Phi = {\\kappa^{\\text{trans}}, \\kappa^{\\text{ems}}, {\\pi_{t,k}\\}_{t=1}^T \\\\}_{k=1}^K}$ can be found in Appendix B. Notably, posterior inference for this model is much more scalable than the Gaussian case, having operations that are linear in the dimensionality rather than cubic.\nRecovering the Softmax Predictive Distribution In addition to the inductive bias that is beneficial under distribution shift, using the vMF distribution has an additional desirable property: classification via the cluster assignments is equivalent to the original softmax-parameterized classifier. The equivalence is exact under the assumption of equal class proportions and sharing $\\kappa$ across classes:\n$p(c_{t,n,k} = 1|h_{t,n}, W_{t,1},..., W_{t,K}, \\kappa^{\\text{ems}}) = \\frac{vMF(h_{t,n}; w_{t,k}, \\kappa^{\\text{ems}})}{\\sum_{j=1}^K vMF(h_{t,n}; w_{t,j}, \\kappa^{\\text{ems}})}$\n$= \\frac{C_D(\\kappa^{\\text{ems}}) \\exp {\\kappa^{\\text{ems}} \\cdot w_{t,k}^T h_{t,n}}}{\\sum_{j=1}^K C_D(\\kappa^{\\text{ems}}) \\exp {\\kappa^{\\text{ems}} \\cdot w_{t,j}^T h_{t,n}}}$\n$= \\text{softmax} (\\kappa^{\\text{ems}} \\cdot W_t^T h_{t,n}),$"}, {"title": "Von Mises-Fisher Model for Hyperspherical Features", "content": "which is equivalent to a softmax with temperature-scaled logits, with the temperature set to $1/\\kappa^{\\text{ems}}$. Temperature scaling only affects the probabilities, not the modal class predic- tion. If using class-specific $\\kappa^{\\text{ems}}$ values and assuming imbalanced classes, then these terms show up as class-specific bias terms: $p(c_{t,n,k} = 1|h_{t,n}, W_{t,1},..., W_{t,K}, \\kappa_1^{\\text{ems}}, ..., \\kappa_k^{\\text{ems}}) \\propto \\exp{\\kappa_k^{\\text{ems}} \\cdot w_{t,k}^T h_{t,n} + \\log C_D(\\kappa_k^{\\text{ems}}) + \\log \\pi_{t,k}}$ where $C_D(\\kappa_k^{\\text{ems}})$ is the vMF's normalization constant and $\\pi_{t,k}$ is the mixing weight."}, {"title": "Related Work", "content": "Filtering for Deep Learning Traditional filtering models, and the Kalman filter [19] in particular, have recently found use in deep learning as a principled way of updating a latent state with new information. In sequence modelling, filter-based architectures are used to learn the latent state of an observation trajectory in both discrete [23, 20, 13, 4] and continuous time [33, 1, 43]. However, here, the filtering model mimics the dynamics of individual observation sequences while we are interested in modeling the dynamics of the data stream as a whole. Chang et al. [7] and Titsias et al. [36] employ Kalman filters in a supervised online learning setting to update neural network weights to a non-stationary data stream. Like our method, Titsias et al. [36] infers the evolution of the linear classification head. However, [7, 36] rely on labels to update the latent state of the weights after prediction. In contrast, our weight adaptation is entirely label-free.\nTest-Time Adaptation Maintaining reliable predictions under distribution shift at test time has driven several research directions such as continual learning [10] and domain adaptation [30, 39]. Our setting falls into test-time adaptation, where the goal is to adapt the source-trained model given only access to the unlabeled target data [24, 41]. A simple yet effective approach is to keep updating the batch normalization (BN) statistics at test time [34, 28]. Based on this insight, a common strategy is to learn the parameters of the BN layer during test time [34, 28, 37, 15, 29, 31]. For instance, TENT [37] learns the BN parameters by minimizing entropy on the predicted target labels. A variation of the setting arises when the test distribution itself changes over time, a more realistic scenario studied by continual test-time adaptation. The main challenge in this paradigm is to ensure adaptability while preventing catastrophic forgetting of the source distribution. To mitigate this trade off, strategies involve episodic resetting to the source parameters [38, 31] or test sample selection [29]. Nonetheless, it has been observed that many strategies still experience performance degradation after extended periods of adaptation [29, 15, 38, 31]."}, {"title": "Experiments", "content": "Our experimental goal is to demonstrate that STAD effectively models natural temporal drifts in the test-time covariates. This distinguishes our work from previous CTTA methods, which focus on domain shifts or shifts induced by corruption noise but do not consider natural evolution of time. Precisely, our setting of interest comprises (1) real-world shifts that (2) occur gradually over time and (3) lead to performance decay. Though ubiquitous in practice, systematic evaluation procedures for such drifts have only been considered fairly recently [40]. We use the evaluation protocol of the Wild-Time benchmark suite [40] to assess the adaptation performance of our model in this setting. In Section 5.1, we demonstrate that our method excels on natural temporal drifts yielding significant performance improvements in settings in which existing CTTA methods collapse. We show the importance of explicitly modeling shift dynamics via an ablation study in Section 5.2. Finally, in Section 5.3, we investigate limitations of our model and show how to diagnose failures ahead of time.\nDatasets We consider two image classification datasets exposed to natural temporal drifts. In addition to our main setting of interest, we also present results on a classic corruption dataset.\n\u2022 Yearbook [14]: a dataset of portraits of American high school students taken across eight decades. Data shift in the students' visual appearance is introduced by changing beauty standards, group norms, and demographic changes. We use the Wild-Time [40] pre-processing and evaluation procedure resulting into 33,431 images from 1930 to 2013. Each 32 \u00d7 32 pixel, grey-scaled image is associated with the student's gender as a binary target label. Images from 1930 to 1969 are used for training; the remaining years from 1970 to 2013 for testing.\n\u2022 FMoW-Time: the functional map of the world (FMoW) dataset [22] maps 224 \u00d7 224 RGB satellite images to one of 62 land use categories. Distribution shift is introduced by technical advancement and economic growth changing how humans make use of land over time. FMOW- Time [40] is an adaptation from FMoW-WILDS [22, 8] that splits a total of 141,696 images into a training time period (2002 - 2012) and a testing time period (2013 - 2017).\n\u2022 CIFAR-10-C: a dataset derived from CIFAR-10, to which 15 corruption / noise types are applied with 5 severity levels to introduce gradual distribution shift [17]. We increase the corruption severity starting from the lowest level (severity 1) to the most sever corruption (severity 5). This results in a test stream of 5 \u00d7 10,000 images for each corruption type. Since our goal is mimicking gradual distribution shifts, we are not interested in switching between images of different corruption types, as previous work has done [38, 37].\nSource Architectures We use a variety of source architectures to demonstrate the model-agnostic nature of our method. They vary in parameter counts, backbone architecture and dimensionality of the representation space.\n\u2022 CNN: We employ the four-block convolutional neural network trained by [40] to perform the binary gender prediction on the yearbook dataset. Presented results are averages over three different random training seeds. The dimension of the latent representation space is 32.\n\u2022 DenseNet: For FMoW-Time, we follow the backbone choice of [22, 40] and use DenseNet121 [18] for the land use classification task. Weights for three random trainings seeds are provided by [40]. The latent representation dimension is 1024.\n\u2022 WideResNet: For the CIFAR-10 experiment, we follow [35, 37] and use the pre-trained WideResNet-28 [42] model from the RobustBench benchmark [9]. The latent representation have a dimension of 512.\nBaselines Despite the source model, we compare against three baselines suitable for an unsuper- vised, continuously-changing test stream. These baselines cover the most dominant paradigms in CTTA: adapting normalization statistics, entropy minimization and anti-collapse mechanics.\n\u2022 Source Model: the un-adapted original model.\n\u2022 BatchNorm (BN) Adaptation [34, 28]: aims to adapt the source model to distributions shift by collecting normalization statistics (mean and variance) of the test data.\n\u2022 Test Entropy Minimization (TENT) [37]: goes one step further and optimizes the BN transfor- mation parameters (scale and shift) by minimizing entropy on test predictions.\n\u2022 Continual Test-Time Adaptation (CoTTA) [38]: takes a different approach by optimizing all model parameters with an entropy objective on augmentation averaged predictions and combines it with stochastic weight restore to prevent catastrophic forgetting."}, {"title": "Natural Temporal Distribution Drifts", "content": "We start by evaluating the adaptation abilities of STAD to natural temporal drifts on two image clas- sification datasets of the Wild-Time benchmark [40]. While only a binary problem, the Yearbook task is difficult as the images are low-resolution (32- dimensional feature space). FMoW-Time presents an even more challenging setting: 62 classes, high- resolution images, 1024-dimensional feature space. For Yearbook, we report both the vMF and Gaus- sian STAD variants. For FMoW-Time, the high- dimensional representation space makes the Gaussian model too computationally costly so we evaluate just the vMF version. For Yearbook we report results for a batch size of 2048 comprising all images of a year in one batch; for FMoW, we re-use the training batch size of 64.\nSTAD reliably adapts to natural shifts over time Table 1 shows overall accuracy, averaged over all time steps and three random seeds. Our methods best adapt to the natural temporal shift present in both datasets, improving upon the source model in both cases. Strikingly, traditional CTTA methods"}, {"title": "Ablation Study: How important is modeling the dynamics?", "content": "We next investigate the importance of STAD's temporal component. STAD is proposed with the assumption that adapting the class proto- types based on those of the previous time step fa- cilitates both rapid and reliable adaptation. How- ever, one could also consider a static version of STAD that does not have a transition model (Equation (1)). Rather, the class prototypes are computed as a standard mixture model (Equation (2)) and without considering previously inferred pro- totypes. Table 2 presents the accuracy differences between the static and dynamic versions of STAD in percentage points. Removing STAD's transition model results in a substantial performance drop of up to 28 percentage points. This supports our assumption that state-space models are well-suited for the task of continual adaptation."}, {"title": "Limitation: Representations are a bottleneck", "content": "We next turn to the CIFAR-10-C dataset, which is the most commonly used benchmark in the CTTA literature. Table 3 displays adaptation accuracy. STAD improves upon the source model for all"}, {"title": "Conclusion", "content": "We have presented a novel test-time adaptation strategy, State-space Test-time ADaptation (STAD), based on a probabilistic state-space model. Both our Gaussian and vMF variants of STAD track how the last-layer evolves under distribution shift, allowing a deployed model to perform unsupervised adaptation to the shift. Our framework outperforms state-of-the-art competitors such as TENT and COTTA on Wild-Time benchmarks. Yet we also identify points of improvement, as we found that random corruptions applied to CIFAR-10 degraded the representations to a degree that our method could be competitive with but not outperform these baselines adapting the model backbone. For future work, we will study diagnostics that reliably identify when the last-layer representations are suitable, more intensive adaptation is needed or if adaptation is possible at all. We will also investigate non-linear models of shift dynamics."}, {"title": "Appendix", "content": "The appendix is structured as follows:\n\u2022 Appendix A provides details on the Gaussian formulation of STAD (STAD-Gaussian) stating the employed transition and emission model.\n\u2022 Appendix B details the inference for the von Mises-Fisher formulation (STAD-vMF) listing the update equations for the variational EM step.\n\u2022 Appendix C contains various implementation details regarding our experiments on Yearbook and FMoW (Appendix C.1) and CIFAR-10-C (Appendix C.2).\n\u2022 Appendix D shows additional experimental results on adaptation under class imbalance (Appendix D.1) and presents runtime comparisons (Appendix D.2)."}, {"title": "STAD-Gaussian", "content": "We use a linear Gaussian transition model to describe the weight evolution over time: For each class k, the weight vector evolves according to a linear drift parameterized by a class-specific transition matrix $A_k \\in \\mathbb{R}^{D \\times D}$. This allows each class to have independent dynamics. The transition noise follows a multivariate Gaussian distribution with zero mean and global covariance $\\Sigma^{\\text{trans}} \\in \\mathbb{R}^{D \\times D}$. The transition noise covariance matrix is a shared parameter across classes and time points to prevent overfitting and keep parameter size at bay. Equation (12) states the Gaussian transition density.\nTransition model: $p(W_t|W_{t-1}) = \\prod_{k=1}^K N(w_{t,k}|A_kw_{t-1,k}, \\Sigma^{\\text{trans}})$"}, {"content": "Emission model: $p(H_t|W_t) = \\prod_{n=1"}, {"sections": "json", "content": "kth component is a multivariate normal with the weight vector of class k at time t as mean and $\\Sigma^{\\text{ems}} \\in \\mathbb{R}^{D \\times D}$ as class-independent covariance matrix. The resulting model can be seen as a mixture of K Kalman filters. Variants of it has found application in applied statistics [6].\nPosterior inference We use the EM objective of Equation (5) to maximize for the model parameters $\\Phi = {\\{A_k, {\\pi_{t,k}\\}_{t=1}^T \\}_{k=1}^K, \\Sigma^{\\text{trans}}, \\Sigma^{\\text{ems}}}$. Thanks to the linearity and Gaussian assumptions, the posterior expectation $E_{p(W,C | H)}[\\cdot]$ in Equation (5) can be computed analytically using the well known Kalman filter predict, update and smoothing equations [6, 5].\nComplexity The closed form computations of the posterior $p(W_t | H_{1:t})$ and smoothing $p(W_t | H_{1:T})$ densities come at a cost as they involve amongst others matrix inversions of dimension- ality D \u00d7 D. This results in considerable computational costs and can lead to numerical instabilities when feature dimension D is large. In addition, the parameter size scales K \u00d7 D2 risking overfitting and consuming substantial memory. These are limitations of the Gaussian formulation making it costly for high-dimensional feature spaces and impractical in low resource environments requiring instant predictions."}, {"title": "Inference for STAD-vMF", "content": "Complete-data log likelihood Using the von Mises-Fisher distribution as hyperspherical transi- tion (Equation (8)) and emission model (Equation (9)), the log of the complete-data likelihood in Equation (3) becomes\n$\\log p(H_{1:T}, W_{1:T}, C_{1:T}) = \\sum_k \\log p(W_{1,k})$\n$+ \\sum_{t=1}^T \\sum_{n=1}^{N_t} \\log p(c_{t,n}) + \\sum_{k=1}^K C_{t,n,k} \\log p(h_{t,n} | W_{t,k}, \\kappa^{\\text{ems}})$"}, {"content": "+ $\\sum_{t=2}^T \\sum_{k=1}^K \\log p(W_{t,k} | W_{t-1,k}, \\kappa^{\\text{trans}})$"}, {"content": "$= \\sum_k \\log C_D(\\kappa_{o,k}) + \\kappa_{o,k} \\mu_{o,k}^T \\mu_{1,k}$"}, {"content": "+ $\\sum_{t=1}^T \\sum_{n=1}^{N_t} \\sum_{k=1}^K C_{n,t,k} (\\log \\pi_{t,k} + \\log C_D(\\kappa^{\\text{ems}}) + \\kappa^{\\text{ems}} w_{t,k}^T h_{t,n})$"}, {"content": "+ $\\sum_{t=2}^T \\sum_{k=1}^K \\log C_D(\\kappa^{\\text{trans}}) + \\kappa^{\\text{trans}} w_{t-1,k}^T w_{t,k}$"}, {"content": "where $\\kappa_{o,k}$ and $\\mu_{0,k}$ denote the parameters of the first time step. In practise, we set $\\mu_{0,k}$ to the source weights and $\\kappa_{o,k} = 100$ (see Appendix C).\nVariational EM objective As described in Section 3.3, we approximate the posterior $P(W_{1:T}, C_{1:T} | H_{1:T})$ with a variational distribution $q(W_{1:T}, C_{1:T})$ assuming the factorised form\n$q(W_{1:T}, C_{1:T}) = \\prod_{t=1}^T \\prod_{k=1}^K q(W_{t,k}) \\prod_{n=1}^{N_t} q(c_{n,t})$,"}, {"content": "where we parameterise q(w_{t,k}) and q(c_{n,t}) with\n$q(w_{t,k}) = vMF(\\cdot; \\rho_{t,k}, \\tau_{t,k}) \\quad q(c_{n,t}) = Cat(; \\lambda_{n,t}) \\quad \\forall t, n, k.$\nWe obtain the variational EM objective\n$\\text{arg} \\max_{\\Phi} E_q [\\log p(H_{1:T}, W_{1:T}, C_{1:T})]$,"}, {"content": "where $E_{q(W_{1:T}, C_{1:T})}$ is denoted $E_q$ to reduce clutter.\nE-step Taking the expectation of the complete-data log likelihood (Equation (14)) with respect to the variational distribution (Equation (20)) gives\n$E_q [\\log p(H_{1:T}, W_{1:T}, C_{1:T})] = \\sum_k \\log C_D(\\kappa_{o,k}) + \\kappa_{o,k} \\mu_{o,k}^T E_q[W_{1,k}]$"}, {"content": "+ $\\sum_{t=1}^T \\sum_{n=1}^{N_t} \\sum_{k=1}^K E_q[C_{n,t,k}] (\\log \\pi_{t,k} + \\log C_D(\\kappa^{\\text{ems}}) + \\kappa^{\\text{ems}}] E_q[W_{t,k}]^T h_{t,n})$"}, {"content": "+ $\\sum_{t=2}^T \\sum_{k=1}^K \\log C_D(\\kappa^{\\text{trans}}) + \\kappa^{\\text{trans}} E_q[W_{t-1,k}]^T E_q[W_{t,k}]$"}, {"title": "Inference for STAD-vMF", "content": "Solving for the variational parameters, we obtain\n$\\lambda_{n,t,k} = \\frac{\\beta_{n,t,k}}{\\sum_{j=1}^K \\beta_{n,t,j}}$ with $\\beta_{n,t,k} = \\pi_{t,k} C_D(\\kappa^{\\text{ems}}) \\exp(\\kappa^{\\text{ems}} E_q[w_{t,k}]^T h_{n,t})$"}, {"content": "$\\rho_{t,k} = \\frac{\\kappa^{\\text{trans}} E_q[W_{t-1,k}] + \\kappa^{\\text{ems}}\\sum_{n=1}^{N_t} E_q [C_{n,t,k}] h_{n,t} + \\kappa^{\\text{trans}} E_q[W_{t+1,k}]}{\\Upsilon_{t,k}}$"}, {"content": "$\\gamma_{t,k} = ||\\rho_{t,k}||$"}, {"content": "The expectations are given by\n$E[C_{n,t,k}] = \\lambda_{n,t,k}$"}, {"content": "$E[W_{t,k}] = A_D(\\tau_{t,k}) \\rho_{t,k},$"}, {"content": "where $A_D(\\kappa) = \\frac{I_{D/2}(\\kappa)}{I_{D/2 - 1}(\\kappa)}$ and $I_v(a)$ denotes the modified Bessel function of the first kind with order v and argument a.\nM-step Maximizing objective (Equation (22)) with respect to the model parameters $\\Phi = {\\kappa^{\\text{trans}}, \\kappa^{\\text{ems}}, {\\pi_{t,k}\\}_{t=1}^T \\}$ gives\n$\\kappa^{\\text{trans}} = \\frac{\\kappa^{\\text{trans}} D - (\\kappa^{\\text{trans}})^3}{(\\kappa^{\\text{trans}})^2}$ with $\\tau^{\\text{trans}} = \\frac{\\sum_{t=2}^T \\sum_{k=1}^K E_q[W_{t-1,k}]^T E_q[W_{t,k}]}{(T - 1) \\times K}$"}, {"content": "$\\kappa^{\\text{ems}} = \\frac{\\kappa^{\\text{ems}} D - (\\kappa^{\\text{ems}})^3}{(\\kappa^{\\text{ems}})^2}$ with $\\tau^{\\text{ems}} = \\frac{\\sum_{t=1}^T \\sum_{k=1}^K \\sum_{n=1}^{N_t} E_q[C_{n,t,k}] E_q[w_{t,k}]^T h_{n,t}}{\\sum_{t=1}^T N_t}$"}, {"content": "$\\pi_{t,k} = \\frac{\\sum_{n=1}^{N_t} \\lambda_{n,t,k}}{N_t}$"}, {"content": "Here we made use of the approximation from Banerjee et al. [3] to compute an estimate for $\\kappa$,\n$\\kappa = \\frac{r D - r^3}{1 - r^2}$ with $r = A_D(\\kappa).$"}, {"title": "Experimental Details", "content": "We next list details on the experimental setup and hyperparameter configurations. All experiments are performed on NVIDIA RTX 6000 Ada with 48GB memory."}, {"title": "Yearbook and FMOW", "content": "We follow the Eval-Fix protocol of Wild-Time to assess adaptation performance on Yearbook and FMOW. In Eval-Fix, source models are trained on a fixed time period and evaluated on unseen, future time points. We use the models trained by Yao et al. [40] on the training time period and evaluate them on the test period. Yao et al. [40] uses a range of different training procedures. We use the checkpoints for plain empirical risk minimization. Three different random training seeds are provided by Yao et al. [40]. All adaptation methods are continuously running without resets. Each method uses one optimization step (via entropy minimization for TENT and CoTTA and Expectation-Maximization for STAD).\nBy the nature of test-time adaptation, choosing hyperparameters is difficult since one cannot assume access to a validation set of the test distribution in practise. To ensure we report the optimal perfor- mance of baseline models, we conduct a grid search on the test set for relevant hyperparameters and report the performance of the best setting. The hyperparameters tested as well as other configurations are listed next."}, {"title": "Yearbook and FMOW", "content": "BN [34, 28] Normalization statistics during test-time adaptation are a running estimates of both the training data and the incoming test statistics. No hyperparameter optimization is necessary here.\nTENT [37] Like in BN, the normalization statistics are based on both training and test set. As in Wang et al. [37], we use the same optimizer settings for test-time adaptation as used for training, except for the learning rate that we find via grid search on {1e-3, 1e-4,1e-5,1e-6,1e-7}. For both yearbook and FMoW, Adam optimizer [21] is used.\nCOTTA [38] We use the same optimizer as used during training (Adam optimizer [21]). For hyper- parameter optimization we follow the parameter suggestions by Wang et al. [38] and conduct a grid search for the learning rate ({1e-3, 1e-4, 1-5, 1e-6, 1e-7}), EMA factor ({0.99, 0.999, 0.9999}) and restoration factor ({0, 0.001, 0.01, 0.1}). We follow [38] by determining the augmentation con- fidence threshold as a function of the 5 % percentile for the softmax prediction confidence on the source images from the source model. Note that this requires access to the source data.\nSTAD-Gaussian We initialize the mixing coefficients with $\\pi_{t,k} = \\frac{1}{\\sqrt{t}K}$, the transition covariance matrix with $\\Sigma^{\\text{trans}} = 0.01 \\times I$ and the emission covariance matrix with $\\Sigma^{\\text{ems}} = 0.5 \\times I$. The prototypes at time t = 1 are initialized with the source weights. We found a normalization of the representations to be also beneficial for STAD-Gaussian. Note that despite normalization, the two models are not equivalent. STAD-Gaussian models the correlation between different dimensions of the representations and is therefore more expressive, while STAD-vMF assumes an isotropic variance.\nSTAD-vMF We initialize the mixing coefficients with $\\pi_{t,k} = \\frac{1}{\\sqrt{t}K}$ and the prototypes at time t = 1 with the source weights. For yearbook, we employ class specific noise parameters initialized with $\\kappa^{\\text{trans}} = 100$ and $\\kappa^{\\text{ems}} = 100$. For FMoW, we found a more restricted transition noise model beneficial. We follow suggestions by Gopal and Yang [16] to keep noise concentration parameters fixed instead of learning them via maximum likelihood in order to maintain a regularization term. The noise parameters remain $\\kappa^{\\text{trans}} = 1000$ and $\\kappa^{\\text{ems}} = 100$ throughout adaptation."}, {"title": "CIFAR-10-C", "content": "For the experiments on CIFAR-10-C, we construct a gradual distribution shift setting by increasing the corruption severity sequentially from level 1 to level 5. We adapt each model separately for each of the 15 corruption types. The source model is a WideResNet-28 [42] from RobustBench [9]. CIFAR-10-C is a well studied benchmark in CTTA and thus we take the hyperparameter settings of baseline methods reported in previous work [38]."}, {"title": "CIFAR-10-C", "content": "BN As in Appendix C.1, normalization statistics during test-time adaptation are a running estimates of both the training statistics and the incoming test statistics. No hyperparameter optimization is required.\nTENT As in [38], we use Adam optimizer with learning rate le-3.\nCOTTA We follow [38] and use Adam optimizer with learning rate 1e-3. The EMA factor is set to 0.999, the restoration factor is 0.01 and the augmentation confidence threshold is 0.92.\nSTAD-vMF We initialize the mixing coefficients with $\\pi_{t,k} = \\frac{1}{\\sqrt{t}K}$, the transition concentration parameter with $\\kappa^{\\text{trans}} = 100$, and the emission concentration parameter with $\\kappa^{\\text{ems}} = 100$. The prototypes at time t = 1 are initialized with the source weights."}, {"title": "Additional Results", "content": "D.1 STAD is more robust to class imbalance\nWe also investigate robustness to class imbalance through an experiment on the binary Yearbook classification task. In this experiment, each batch is sampled to maintain a fixed class share. Figure 5 presents adaptation accuracy across different class proportions. All baseline models struggle to outperform the source model when dealing with highly imbalanced classes (i.e. class proportion of 10%). However, STAD demonstrates adaptation gains significantly earlier than the baseline methods."}, {"title": "Runtime", "content": "In Table 4, we present the wall clock time per batch for the Wild-Time datasets, with figures shown relative to the source model. BN and TENT perform adaptation the fastest. STAD requires a fraction of time compared to COTTA, which updates all model parameters. Note that COTTA and TENT benefit from highly efficient back-propagation code, whereas our code base has not been optimized for maximum speed. Optimizing efficiency of our code could potentially make STAD even faster."}]}