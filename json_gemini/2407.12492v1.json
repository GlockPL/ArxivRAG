[{"title": "Test-Time Adaptation with State-Space Models", "authors": ["Mona Schirmer", "Dan Zhang", "Eric Nalisnick"], "abstract": "Distribution shifts between training and test data are all but inevitable over the lifecycle of a deployed model and lead to performance decay. Adapting the model can hopefully mitigate this drop in performance. Yet, adaptation is challenging since it must be unsupervised: we usually do not have access to any labeled data at test time. In this paper, we propose a probabilistic state-space model that can adapt a deployed model subjected to distribution drift. Our model learns the dynamics induced by distribution shifts on the last set of hidden features. Without requiring labels, we infer time-evolving class prototypes that serve as a dynamic classification head. Moreover, our approach is lightweight, modifying only the model's last linear layer. In experiments on real-world distribution shifts and synthetic corruptions, we demonstrate that our approach performs competitively with methods that require back-propagation and access to the model backbone. Our model especially excels in the case of small test batches-the most difficult setting.", "sections": [{"title": "Introduction", "content": "Predictive models often have an 'expiration date.' Real-world applications tend to exhibit distribution drift, meaning that the data points seen at test time are drawn from a distribution that is different than the training data's. Moreover, the test distribution usually becomes more unlike the training distribution as time goes on. An example of this is with recommendation systems: trends change, new products are released, old products are discontinued, etc. Unless a model is updated, its ability to make accurate predictions will expire, requiring the model to be taken offline and re-trained. Every iteration of this model life-cycle can be expensive and time consuming. Allowing models to remain 'fresh' for as long as possible is thus an open and consequential problem.\nIn this work, we propose State-space Test-time ADaptation (STAD), a method that delays the failure of a deployed model by performing unsupervised adaptation at test time. We perform this updating by modeling the dynamics of the parameters at a neural network's final layer, making our approach widely applicable and computationally lightweight. Specifically, we use a state-space model (SSM) to track how the weight vectors in the final layer-where each vector corresponds to a class-evolve under distribution drift. To generate predictions for the newly-acquired batch of test points, we use the SSM's fitted cluster means as the model's updated parameters. We focus on natural data shifts caused by a gradually changing environment rather than noise-based shift, which has been the focus of previous work [37, 38]. Our contributions are as follows,\n\u2022 In Section 3.2, we present STAD, a state-space model to learn the dynamics of how a classifier's last-layer weights evolve under distribution shift, without access to any labels. No previous work has explicitly modeled these dynamics, which we demonstrate is crucial via an ablation study.\n\u2022 In Sections 3.2 and 3.3, we provide two implementations of STAD\u2014one using Gaussian distributions and one using von Mises-Fisher distributions. Each represents a different assumption about the geometry of the model's last-layer representations."}, {"title": "Problem Setting", "content": "Data & Model We focus on the traditional setting of multi-label classification, where $\\mathcal{X} \\subset \\mathbb{R}^D$ denotes the input (feature) space and $\\mathcal{Y} \\subseteq \\{1, ..., K\\}$ denotes the label space. Let $x$ and $y$ be random variables and $P(x, y) = P(x) P(y|x)$ the unknown source data distribution. We assume $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ are realisations of $x$ and $y$. The goal of classification is to find a mapping $f_{\\theta}$, with parameters $\\theta$, from the input space to the label space $f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Y}$. Fitting the classifier $f_{\\theta}$ is usually accomplished by minimizing an appropriate loss function (e.g. log loss). Yet, our method is agnostic to how $f_{\\theta}$ is trained and therefore easy to use with, for instance, a pre-trained model that has been downloaded from the web.\nDistribution Drift and Unsupervised Adaptation In predictive modeling, a classifier will almost always become 'stale,' as the test conditions inevitably change from those observed during training. In fact, the model's presence in the world can cause this change: a model that predicts when someone is likely to be infected with a contagious disease will, if effective and widely adopted, reduce the prevalence of that disease. Thus, we want our models to be 'fresh' for as long as possible and continue to make accurate predictions, in spite of data drift. More formally, let the data at test-time $t$ be sampled from a distribution $Q_t(x, y) = Q_t(x) Q_t(y|x)$ such that $Q_t(x, y) \\neq P(x, y) \\forall t > 0$. Of course, we do not observe labels at test time, and hence we observe only a batch of features $X_t = \\{x_{1,t}, ..., x_{N_t,t}\\}$, where $x_{n,t} \\sim Q_t(x)$ (i.i.d.). Given the $t$-th batch of features $X_t$, the goal is to adapt $f_{\\theta}$, forming a new set of parameters $\\theta_t$ such that $f_{\\theta_t}$ has better predictive performance on $X_t$ than $f_{\\theta}$ would have. Since we can only observe features, we assume that the distribution shift must at least take the form of covariate shift: $Q_t(x) \\neq P(x) \\forall t > 0$. $Q_t(y|x)$ could shift as well, but this will be impossible to detect in our assumed setting. If $Q_t(y|x)$ does shift, it must do so modestly, as the classifier will become completely invalid if the relationship between features and labels changes to a severe degree. This setting is known as continual test-time adaptation (CTTA) [38, 15, 31, 29, 35, 12]."}, {"title": "Modelling the Dynamics of Distribution Shifts in Representation Space", "content": "We now present our method: the core idea is that CTTA can be done by tracking how distribution shift affect the model's representations. We employ linear state-space models (SSMs) to capture how test points evolve and drift. The SSM's cluster representations then serve as an adaptive classification head that evolves with the non-stationarity of the distribution drift. In Section 3.2, we first introduce the general model and then in Section 3.3, we propose an implementation that leverages the von-Mises-Fisher distribution to model hyperspherical features."}, {"title": "Modeling Shift in Representation Space", "content": "Whereas previous work has mostly attempted to adapt all levels of a neural network, we take the alternative approach of adapting only the last layer. Let the classifier $f_{\\theta}$ be a neural network having $L$ total layers. We will treat the first $L - 1$ as a black box, assuming they transform the original feature vector $x$ into a new (lower dimensional) representation, which we denote as $h$. The original classifier then maps these representations to the classes as: $\\mathbb{E}[y|h] = \\text{softmax}_y(W_0h)$, where $\\text{softmax}_y(\\cdot)$ denotes the dimension of the softmax's output corresponding to the $y$-th label index and $W_0$ are the last-layer weights. As $W_0$ will only be valid for representations that are similar to the training data, we will discard these parameters when performing CTTA, learning new parameters $W_t$ for the $t$-th time step. These new parameters will be used to generate the adapted predictions through the same link function: $\\mathbb{E}[y|h] = \\text{softmax}_y(W_th)$.\nIn the setting of CTTA, we observe a batch of features $X_t$. Passing them through the model yields corresponding representations $H_t$, and this will be the 'data' used for the probabilistic model we will describe below. Specifically, we will model how the representations change from $H_t$ to $H_{t+1}$. Operating on this last set of hidden representations has several benefits. Firstly, test-time adaptation"}, {"title": "A Probabilistic Model of Shift Dynamics", "content": "We now describe our general method for adaptive last-layer parameters. We assume that, while the representations $H_t$ are changing over time, they are still maintaining some class structure in the form of clusters. Our model will seek to track this structure as it evolves over time. Using latent variables $w_{t,k}$, we will assume each representation is drawn conditioned on $K$ latent vectors: $h_{t,n} \\sim p(h_t | W_{t,1}, ..., W_{t,K})$, where $K$ is equal to the number of classes in the prediction task. After fitting the unsupervised model, the $K$ latent vectors will be stacked to create $W_t$, the last-layer weights of the adapted predictive model (as introduced in 3.1). For the intuition of the approach, see Figure 1a. The blue red, and green clusters represent classes of a classification problem. As the distribution shifts from time step $t-1$ to $t = 3$, the class clusters shift in representation space. Our SSM aims to track this movement so that classification performance can still be preserved. We now move on to a technical description.\nNotation and Variables Let $H_t = (h_{t,1},..., h_{t,N_t}) \\in \\mathbb{R}^{D \\times N_t}$ denote the neural representations for $N_t$ data points at test time $t$. Let $W_t = (W_{t,1},..., w_{t,K}) \\in \\mathbb{R}^{D \\times K}$ denote the $K$ weight vectors at test time $t$. As discussed above, the weight vector $w_{t,k}$ can be thought of as a latent prototype for class $k$ at time $t$. We denote with $C_t = (c_{t,1}, ..., c_{t,N_t}) \\in \\{0,1\\}^{K \\times N_t}$ the $N_t$ one-hot encoded latent class assignment vectors $c_{t,n} \\in \\{0,1\\}^{K}$ at time $t$. The $k$-th position of $c_{t,n}$ is denoted with $c_{t,n,k}$ and is 1 if $h_{t,n}$ belongs to class $k$ and 0 otherwise. Like in standard (static) mixture models [5], the prior of the latent class assignments $p(c_{t,n})$ is a categorical distribution, $p(c_{t,n}) = \\text{Cat}(\\pi_t)$ with $\\pi_t = (\\pi_{t,1},..., \\pi_{t,K}) \\in [0,1]^K$ and $\\sum_{k=1}^K \\pi_{t,k} = 1$. The mixing coefficient $\\pi_{t,k}$ gives the a priori probability of class $k$ at time $t$ and can be interpreted as the class proportions.\nDynamics Model We model the evolution of the $K$ prototypes $W_t = (W_{t,1},..., w_{t,K})$ with $K$ independent Markov processes. The resulting transition model is then:\nTransition model: $p(W_t | W_{t-1}, \\varepsilon_{\\text{trans}}) = \\prod_{k=1}^K p(W_{t,k} | W_{t-1,k}, \\varepsilon_{\\text{trans}}),$"}, {"title": "Von Mises-Fisher Model for Hyperspherical Features", "content": "Choosing Gaussian densities for the transition and emission models, as discussed above, assumes the representation space follows an Euclidean geometry. However, prior work has shown that assuming the hidden representations lie on the unit hypersphere results in a better inductive bias for OOD generalization [26, 2]. This is due to the norms of the representations being biased by in-domain information such as class balance, making angular distances a more reliable signal of class membership in the presence of distribution shift [26, 2]. We too employ the hyperspherical assumption by normalizing the hidden representations such that $||h||_2 = 1$ and model them with the von Mises-Fisher (vMF) distribution [25],\n$\\text{vMF}(h; \\mu_k, \\kappa) = C_D(\\kappa) \\exp{\\{\\kappa \\cdot \\mu_k^{\\top} h\\}}$\nwhere $\\mu_k \\in \\mathbb{R}^D$ with $||\\mu_k||_2 = 1$ denotes the mean direction of class $k$, $\\kappa \\in \\mathbb{R}^+$ the concentration parameter, and $C_D(\\kappa)$ the normalization constant. High values of $\\kappa$ imply larger concentration around $\\mu_k$. The vMF distribution is proportional to a Gaussian distribution with isotropic variance and unit norm. While previous work [26, 27, 2] has mainly explored training objectives to encourage latent representations to be vMF-distributed, we apply Equation (7) to model the evolving representations.\nHyperspherical State-Space Model Returning to the SSM given above, we specify both transition and emission models as vMF distributions,\nHyperspherical transition model: $p(W_t | W_{t-1}) = \\prod_{k=1}^K \\text{vMF}(W_{t,k} | W_{t-1,k}, \\kappa_{\\text{trans}})$,\nHyperspherical emission model: $p(H_t | W_t) = \\prod_{n=1}^{N_t} \\sum_{k=1}^K \\pi_{t,k} \\text{vMF}(h_{t,n} | W_{t,k}, \\kappa_{\\text{ems}})$\nThe parameter size of the vMF formulation only scales linearly with the feature dimension, i.e. $O(DK)$ instead of $O(D^2K)$ as for the Gaussian case. The noise parameters, $\\kappa_{\\text{trans}}, \\kappa_{\\text{ems}}$ are scalar values, and we shall use the reduced parameter size to experiment with class conditioned noise parameters $\\kappa_{\\text{trans}}^k, \\kappa_{\\text{ems}}^k$, $k = 1, ..., K$ in Section 5. Figure 2 illustrates this STAD-vMF variant.\nPosterior Inference Unlike in the linear Gaussian case, the vMF distribution is not closed under marginalization. Consequentially, the posterior distribution required for the expectation in Equation (5), $p(W_{1:T} C_{1:T} | H_{1:T})$, cannot be obtained in closed form. We employ a variational EM objective, approximating the posterior with mean-field variational inference, following Gopal and Yang [16]:\n$\\begin{aligned} q(w_{t,k}) & = \\text{vMF}(\\cdot; \\rho_{t,k}, \\tau_{t,k}) \\\\ q(c_{n,t}) & = \\text{Cat}(; \\Lambda_{n,t}) \\end{aligned} \\quad \\forall t, n, k.$\nThe variational distribution $q(W, C)$ factorizes over $n, t, k$ and the objective from Equation (5) becomes $\\text{arg max}_{\\Phi} \\mathbb{E}_{q(w,c)} [\\text{log } p(H_{1:T}, W_{1:T}, C_{1:T})]$. More details as well as the full maximisation steps for $\\Phi = \\{\\kappa_{\\text{trans}}, \\kappa_{\\text{ems}}, \\{\\pi_{t,k}\\}_{t=1}^T\\}_{k=1}^K$ can be found in Appendix B. Notably, posterior inference for this model is much more scalable than the Gaussian case, having operations that are linear in the dimensionality rather than cubic.\nRecovering the Softmax Predictive Distribution In addition to the inductive bias that is beneficial under distribution shift, using the vMF distribution has an additional desirable property: classification via the cluster assignments is equivalent to the original softmax-parameterized classifier. The equivalence is exact under the assumption of equal class proportions and sharing $\\kappa$ across classes:\n$p(c_{t,n,k} = 1 | h_{t,n}, W_{t,1},\u00b7\u00b7\u00b7, W_{t,K}, \\kappa_{\\text{ems}}) = \\frac{\\text{vMF}(h_{t,n}; W_{t,k}, \\kappa_{\\text{ems}})}{\\sum_{j=1}^K \\text{vMF}(h_{t,n}; w_{t,j}, \\kappa_{\\text{ems}})} = \\frac{C_D(\\kappa_{\\text{ems}}) \\exp\\{\\kappa_{\\text{ems}} w_{t,k}^{\\top} h_{t,n}\\}}{\\sum_{j=1}^K C_D(\\kappa_{\\text{ems}}) \\exp\\{\\kappa_{\\text{ems}} w_{t,j}^{\\top} h_{t,n}\\}} = \\text{softmax } (\\kappa_{\\text{ems}} \\cdot W_t^{\\top} h_{t,n}),$"}, {"title": "Related Work", "content": "Filtering for Deep Learning Traditional filtering models, and the Kalman filter [19] in particular, have recently found use in deep learning as a principled way of updating a latent state with new information. In sequence modelling, filter-based architectures are used to learn the latent state of an observation trajectory in both discrete [23, 20, 13, 4] and continuous time [33, 1, 43]. However, here, the filtering model mimics the dynamics of individual observation sequences while we are interested in modeling the dynamics of the data stream as a whole. Chang et al. [7] and Titsias et al. [36] employ Kalman filters in a supervised online learning setting to update neural network weights to a non-stationary data stream. Like our method, Titsias et al. [36] infers the evolution of the linear classification head. However, [7, 36] rely on labels to update the latent state of the weights after prediction. In contrast, our weight adaptation is entirely label-free.\nTest-Time Adaptation Maintaining reliable predictions under distribution shift at test time has driven several research directions such as continual learning [10] and domain adaptation [30, 39]. Our setting falls into test-time adaptation, where the goal is to adapt the source-trained model given only access to the unlabeled target data [24, 41]. A simple yet effective approach is to keep updating the batch normalization (BN) statistics at test time [34, 28]. Based on this insight, a common strategy is to learn the parameters of the BN layer during test time [34, 28, 37, 15, 29, 31]. For instance, TENT [37] learns the BN parameters by minimizing entropy on the predicted target labels. A variation of the setting arises when the test distribution itself changes over time, a more realistic scenario studied by continual test-time adaptation. The main challenge in this paradigm is to ensure adaptability while preventing catastrophic forgetting of the source distribution. To mitigate this trade off, strategies involve episodic resetting to the source parameters [38, 31] or test sample selection [29]. Nonetheless, it has been observed that many strategies still experience performance degradation after extended periods of adaptation [29, 15, 38, 31]."}, {"title": "Experiments", "content": "Our experimental goal is to demonstrate that STAD effectively models natural temporal drifts in the test-time covariates. This distinguishes our work from previous CTTA methods, which focus on domain shifts or shifts induced by corruption noise but do not consider natural evolution of time. Precisely, our setting of interest comprises (1) real-world shifts that (2) occur gradually over time and (3) lead to performance decay. Though ubiquitous in practice, systematic evaluation procedures for such drifts have only been considered fairly recently [40]. We use the evaluation protocol of the Wild-Time benchmark suite [40] to assess the adaptation performance of our model in this setting. In Section 5.1, we demonstrate that our method excels on natural temporal drifts yielding significant performance improvements in settings in which existing CTTA methods collapse. We show the importance of explicitly modeling shift dynamics via an ablation study in Section 5.2. Finally, in Section 5.3, we investigate limitations of our model and show how to diagnose failures ahead of time.\nDatasets We consider two image classification datasets exposed to natural temporal drifts. In addition to our main setting of interest, we also present results on a classic corruption dataset.\n\u2022 Yearbook [14]: a dataset of portraits of American high school students taken across eight decades. Data shift in the students' visual appearance is introduced by changing beauty standards, group norms, and demographic changes. We use the Wild-Time [40] pre-processing and evaluation procedure resulting into 33,431 images from 1930 to 2013. Each 32 \u00d7 32 pixel, grey-scaled image is associated with the student's gender as a binary target label. Images from 1930 to 1969 are used for training; the remaining years from 1970 to 2013 for testing.\n\u2022 FMoW-Time: the functional map of the world (FMoW) dataset [22] maps 224 \u00d7 224 RGB satellite images to one of 62 land use categories. Distribution shift is introduced by technical advancement and economic growth changing how humans make use of land over time. FMOW-Time [40] is an adaptation from FMoW-WILDS [22, 8] that splits a total of 141,696 images into a training time period (2002 - 2012) and a testing time period (2013 - 2017).\n\u2022 CIFAR-10-C: a dataset derived from CIFAR-10, to which 15 corruption / noise types are applied with 5 severity levels to introduce gradual distribution shift [17]. We increase the corruption severity starting from the lowest level (severity 1) to the most sever corruption (severity 5). This results in a test stream of 5 \u00d7 10,000 images for each corruption type. Since our goal is mimicking gradual distribution shifts, we are not interested in switching between images of different corruption types, as previous work has done [38, 37].\nSource Architectures We use a variety of source architectures to demonstrate the model-agnostic nature of our method. They vary in parameter counts, backbone architecture and dimensionality of the representation space.\n\u2022 CNN: We employ the four-block convolutional neural network trained by [40] to perform the binary gender prediction on the yearbook dataset. Presented results are averages over three different random training seeds. The dimension of the latent representation space is 32.\n\u2022 DenseNet: For FMoW-Time, we follow the backbone choice of [22, 40] and use DenseNet121 [18] for the land use classification task. Weights for three random trainings seeds are provided by [40]. The latent representation dimension is 1024.\n\u2022 WideResNet: For the CIFAR-10 experiment, we follow [35, 37] and use the pre-trained WideResNet-28 [42] model from the RobustBench benchmark [9]. The latent representation have a dimension of 512.\nBaselines Despite the source model, we compare against three baselines suitable for an unsupervised, continuously-changing test stream. These baselines cover the most dominant paradigms in CTTA: adapting normalization statistics, entropy minimization and anti-collapse mechanics.\n\u2022 Source Model: the un-adapted original model.\n\u2022 BatchNorm (BN) Adaptation [34, 28]: aims to adapt the source model to distributions shift by collecting normalization statistics (mean and variance) of the test data.\n\u2022 Test Entropy Minimization (TENT) [37]: goes one step further and optimizes the BN transformation parameters (scale and shift) by minimizing entropy on test predictions.\n\u2022 Continual Test-Time Adaptation (CoTTA) [38]: takes a different approach by optimizing all model parameters with an entropy objective on augmentation averaged predictions and combines it with stochastic weight restore to prevent catastrophic forgetting."}, {"title": "Natural Temporal Distribution Drifts", "content": "We start by evaluating the adaptation abilities of STAD to natural temporal drifts on two image classification datasets of the Wild-Time benchmark [40].\nWhile only a binary problem, the Yearbook task is difficult as the images are low-resolution (32-dimensional feature space). FMoW-Time presents an even more challenging setting: 62 classes, high-resolution images, 1024-dimensional feature space. For Yearbook, we report both the vMF and Gaussian STAD variants. For FMoW-Time, the high-dimensional representation space makes the Gaussian model too computationally costly so we evaluate just the vMF version. For Yearbook we report results for a batch size of 2048 comprising all images of a year in one batch; for FMoW, we re-use the training batch size of 64.\nSTAD reliably adapts to natural shifts over time Table 1 shows overall accuracy, averaged over all time steps and three random seeds. Our methods best adapt to the natural temporal shift present in both datasets, improving upon the source model in both cases. Strikingly, traditional CTTA methods"}, {"title": "Ablation Study: How important is modeling the dynamics?", "content": "We next investigate the importance of STAD's temporal component. STAD is proposed with the assumption that adapting the class prototypes based on those of the previous time step facilitates both rapid and reliable adaptation. However, one could also consider a static version of STAD that does not have a transition model (Equation (1)). Rather, the class prototypes are computed as a standard mixture model (Equation (2)) and without considering previously inferred prototypes. Table 2 presents the accuracy differences between the static and dynamic versions of STAD in percentage points. Removing STAD's transition model results in a substantial performance drop of up to 28 percentage points. This supports our assumption that state-space models are well-suited for the task of continual adaptation."}, {"title": "Limitation: Representations are a bottleneck", "content": "We next turn to the CIFAR-10-C dataset, which is the most commonly used benchmark in the CTTA literature. Table 3 displays adaptation accuracy. STAD improves upon the source model for all"}, {"title": "Conclusion", "content": "We have presented a novel test-time adaptation strategy, State-space Test-time ADaptation (STAD), based on a probabilistic state-space model. Both our Gaussian and vMF variants of STAD track how the last-layer evolves under distribution shift, allowing a deployed model to perform unsupervised adaptation to the shift. Our framework outperforms state-of-the-art competitors such as TENT and COTTA on Wild-Time benchmarks. Yet we also identify points of improvement, as we found that random corruptions applied to CIFAR-10 degraded the representations to a degree that our method could be competitive with but not outperform these baselines adapting the model backbone. For future work, we will study diagnostics that reliably identify when the last-layer representations are suitable, more intensive adaptation is needed or if adaptation is possible at all. We will also investigate non-linear models of shift dynamics."}, {"title": "Appendix", "content": "The appendix is structured as follows:\n\u2022 Appendix A provides details on the Gaussian formulation of STAD (STAD-Gaussian) stating the employed transition and emission model.\n\u2022 Appendix B details the inference for the von Mises-Fisher formulation (STAD-vMF) listing the update equations for the variational EM step.\n\u2022 Appendix C contains various implementation details regarding our experiments on Yearbook and FMoW (Appendix C.1) and CIFAR-10-C (Appendix C.2).\n\u2022 Appendix D shows additional experimental results on adaptation under class imbalance (Appendix D.1) and presents runtime comparisons (Appendix D.2)."}, {"title": "STAD-Gaussian", "content": "We use a linear Gaussian transition model to describe the weight evolution over time: For each class $k$, the weight vector evolves according to a linear drift parameterized by a class-specific transition matrix $A_k \\in \\mathbb{R}^{D \\times D}$. This allows each class to have independent dynamics. The transition noise follows a multivariate Gaussian distribution with zero mean and global covariance $\\Sigma_{\\text{trans}} \\in \\mathbb{R}^{D \\times D}$. The transition noise covariance matrix is a shared parameter across classes and time points to prevent overfitting and keep parameter size at bay. Equation (12) states the Gaussian transition density.\nTransition model: $p(W_t | W_{t-1}) = \\prod_{k=1}^K \\mathcal{N}(W_{t,k} | A_kW_{t-1,k}, \\Sigma_{\\text{trans}})$,\nEmission model: $p(H_t | W_t) = \\prod_{n=1}^{N_t} \\sum_{k=1}^K \\pi_{t,k} \\mathcal{N}(h_{t,n} | W_{t,k}, \\Sigma_{\\text{ems}})$\nEquation (13) gives the emission model of the observed features $H_t$ at time $t$. As in Equation (2), the features at a given time $t$ are generated by a mixture distribution with mixing coefficient $\\pi_{t,k}$. The emission density of each of the $K$ component is a multivariate normal with the weight vector of class $k$ at time $t$ as mean and $\\Sigma_{\\text{ems}} \\in \\mathbb{R}^{D \\times D}$ as class-independent covariance matrix. The resulting model can be seen as a mixture of $K$ Kalman filters. Variants of it has found application in applied statistics [6].\nPosterior inference We use the EM objective of Equation (5) to maximize for the model parameters $\\Phi = \\{\\{A_k, \\{\\pi_{t,k}\\}_{t=1}^T\\}_{k=1}, \\Sigma_{\\text{trans}}, \\Sigma_{\\text{ems}}\\}$. Thanks to the linearity and Gaussian assumptions, the posterior expectation $\\mathbb{E}_{p(W,C|H)}[\\cdot]$ in Equation (5) can be computed analytically using the well known Kalman filter predict, update and smoothing equations [6, 5].\nComplexity The closed form computations of the posterior $p(W_t | H_{1:t})$ and smoothing $p(W_t | H_{1:T})$ densities come at a cost as they involve amongst others matrix inversions of dimensionality $D \\times D$. This results in considerable computational costs and can lead to numerical instabilities when feature dimension $D$ is large. In addition, the parameter size scales $K \\times D^2$ risking overfitting and consuming substantial memory. These are limitations of the Gaussian formulation making it costly for high-dimensional feature spaces and impractical in low resource environments requiring instant predictions."}, {"title": "Inference for STAD-vMF", "content": "Complete-data log likelihood Using the von Mises-Fisher distribution as hyperspherical transition (Equation (8)) and emission model (Equation (9))", "p(H_{1": "T"}, "W_{1:T}, C_{1:T}) = \\sum_{k=1}^K \\log p(W_{1,k}) \\\\\\ & + \\sum_{t=1}^T \\sum_{n=1}^{N_t} \\log p(c_{t,n}) + \\sum_{t=1}^T \\sum_{n=1}^{N_t} \\sum_{k=1}^K C_{t,n,k} \\log p(h_{t,n} | W_{t,k}, \\kappa_{\\text{ems}}) \\\\\\ &+ \\sum_{t=2}^T \\sum_{k=1}^K \\log p(W_{t,k} | W_{t-1,k}, \\kappa_{\\text{trans}}) \\\\\\ & = \\sum_{k} \\log C_D(\\kappa_{0,k}) + \\kappa_{0,k} \\mu_{0,k}^{\\top} \\mathbb{E}[W_{1,k}"], "P(W_{1": "T"}, {"1": "T"}, {"1": "T"}, {"1": "T"}, {"1": "T"}, {"1": "T"}, {"1": "T"}, {}, "p(H_{1", "W_{1", "C_{1", "mathbb{E}_q(W_{1", "C_{1", 14, 20, ["log p(H_{1:T}, W_{1:T}, C_{1:T})"], "big(\\"]