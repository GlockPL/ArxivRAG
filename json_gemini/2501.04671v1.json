{"title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "authors": ["Charles Corbi\u00e8re", "Simon Roburin", "Syrielle Montariol", "Antoine Bosselut", "Alexandre Alahi"], "abstract": "Large vision-language models (LVLMs) augment language models with visual understanding, enabling multi-modal reasoning. However, due to the modality gap between textual and visual data, they often face significant challenges, such as over-reliance on text priors, hallucinations, and limited capacity for complex visual reasoning. Existing benchmarks to evaluate visual reasoning in LVLMs often rely on schematic or synthetic images and on imprecise machine-generated explanations. To bridge the modality gap, we present DRIVINGVQA, a new benchmark derived from driving theory tests to evaluate visual chain-of-thought reasoning in complex real-world scenarios. It offers 3,931 expert-crafted multiple-choice problems and interleaved explanations grounded with entities relevant to the reasoning process. We leverage this dataset to perform an extensive study of LVLMs' ability to reason about complex visual scenarios. Our experiments reveal that open-source and proprietary LVLMs struggle with visual chain-of-thought reasoning under zero-shot settings. We investigate training strategies that leverage relevant entities to improve visual reasoning. Notably, we observe a performance boost up to 7% when reasoning over image tokens of cropped regions tied to these entities.", "sections": [{"title": "1. Introduction", "content": "Building on the strong textual processing capabilities of large language models (LLMs) [4, 15, 37], large vision-language models (LVLMs) [2, 7, 19, 21, 28, 38] extend LLMs to handle visual inputs. LVLMs brought significant improvements to multi-modal tasks such as visual question answering (VQA) [3, 12] and image captioning [30]. In particular, they paved the way for tasks involving complex visual reasoning [24]. Despite improvements in textual rea-soning achieved through model scaling and the introduction of dedicated prompting strategies, such as chain-of-thought prompting [42], their transferability to visual reasoning in LVLMs remains limited [33, 47]. This limitation is particularly evident in tasks requiring multi-object reasoning and relational understanding [6, 34], akin to the continuous analysis of their environment humans perform in com-"}, {"title": "2. Related Work", "content": "Prior studies demonstrate that LLMs performances greatly benefit from in-context learning by leveraging human writ-ten explanations [4, 43]. However, LVLMs fail to in-herit this reasoning capability with images. This setback is largely attributed to the domain gap between vision and text data. Significant research efforts have been made to im-prove multi-modal reasoning capabilities by focusing on the training procedure and new prompting techniques. Among them, VILA [19] investigates several pre-training regimes that favor the emergence of chain-of-thought reasoning in VLMs. Similarly, Flamingo [2] shows the benefits of large pre-training on interleaved visual and textual data. Shikra [7] utilized ChatGPT-generated dialogues to guide mod-els toward relevant regions of the input image, improv-ing localization-based reasoning. CogCOM [31] uses a set of predefined image manipulations to solve problems with grounded evidence. More recently, Visual-CoT [33] intro-duces a two-step procedure that first samples parts of the input image and then selects the appropriate answer."}, {"title": "2.1. Visual Reasoning Capabilities", "content": "Significant research efforts have been made to im-prove multi-modal reasoning capabilities by focusing on the training procedure and new prompting techniques. Among them, VILA [19] investigates several pre-training regimes that favor the emergence of chain-of-thought reasoning in VLMs. Similarly, Flamingo [2] shows the benefits of large pre-training on interleaved visual and textual data. Shikra [7] utilized ChatGPT-generated dialogues to guide mod-els toward relevant regions of the input image, improv-ing localization-based reasoning. CogCOM [31] uses a set of predefined image manipulations to solve problems with grounded evidence. More recently, Visual-CoT [33] intro-duces a two-step procedure that first samples parts of the input image and then selects the appropriate answer. Despite these advancements, existing methods rely on indirect guidance or synthetic explanations, limiting their applicability to real-world, domain-specific scenarios. In our experiments, we propose a novel framework that trains VLMs to first detect relevant entities within images, then generate interleaved explanations that incorporate these en-tities directly, enabling more grounded and interpretable visual-chain-of-thought reasoning (see Section 4.4)."}, {"title": "3. DRIVINGVQA Dataset", "content": "To evaluate the visual reasoning capabilities of LVLMS in real-world situations, we construct DRIVINGVQA, a dataset derived from the driving theory test. The dataset construction involves three phases: data collection focused on challenging real-world driving scenarios (Sec. 3.1), human-expert annotation of relevant entities (Sec. 3.2), and generation of interleaved explanations that explicitly tie rea-soning steps to visual elements (Sec. 3.3). This structured pipeline is illustrated in Fig. 3."}, {"title": "3.1. Data Collection", "content": "In France, obtaining a driver's license requires passing two examinations: the theoretical test, known as the \"Code de la Route,\" and a practical driving test. The theoretical exam comprises 40 multiple-choice questions (MCQs) covering traffic laws, road signs, and safe driving practices. Candi-"}, {"title": "3.2. Relevant Entities", "content": "We augment our dataset with a list of entities relevant to an-swering the question, along with their location in the image. Automated extraction. To ease this large-scale annota-tion step, we develop a pipeline that first identifies possi-ble entities and their locations within images. This process begins by leveraging human explanations to extract an ini-tial list of domain-specific key entities found in our dataset. Using GPT40-mini, we process each sample to extract en-tities referenced in the question, options, and explanation that are visible in the image. Subsequently, we use Ground-ingDINO [22] to localize each of these entities within the image, generating (entity label, bounding box coordinates) pairs. Finally, we apply heuristics to refine these outputs, such as grouping similar labels under unified entity names. Details of this pipeline are provided in Appendix A.2. Manual annotation. Human experts refine the pseudo-annotated data by removing irrelevant entities, correcting inaccurate labels, and adding missing entities. In total, 5,657 entities (spanning 256 unique labels) were annotated with precise bounding boxes, averaging 1.4 entities per im-"}, {"title": "3.3. Interleaved Explanations", "content": "To ground explanations with visual cues, we integrate rele-vant entities into human-crafted explanations using GPT40-mini, creating interleaved explanations. Our approach com-bines few-shot prompting, self-verification, and heuristics to embed each entity label, along with its bounding box co-ordinates or visual patch extracted from the image. If an annotated entity is missing from the explanation, a brief sentence is prepended to the interleaved explanation men-tioning the presence of this entity in the image. This in-terleaving mechanism is central to DRIVINGVQA, linking relevant entities with explanations to model the four stages of visual-chain-of-thought reasoning introduced in Sec. 1 and enabling detailed analysis of LVLM capabilities."}, {"title": "3.4. Reasoning type", "content": "In the context of driving, decision-making often depends on combining environmental cues, object detection, and spatial relationships. Inspired by previous works [23], we define a simple taxonomy of reasoning types: \u2022 Environment: Reasoning based on global elements in the image, such as road type, weather, lighting, or urban ver-sus rural settings. It may also involve static entities oc-cupying a significant portion of the image, like a tunnel, bridge, or roundabout. \u2022 Multiple objects: Requires identifying and reasoning about more than one object or region of interest to answer the question. For example, recognizing the presence of vehicles in conjunction with traffic signs and pedestrians. \u2022 Physical attribute: Focuses on recognizing physical at-tributes or actions of entities, such as a car's turn signal, the color of a sign, or a cyclist's motion. \u2022 Spatial relation: Involves understanding the spatial posi-tioning of objects relative to each other or the ego vehicle. Questions can be associated with multiple reasoning types. For instance, some may combine environmental recogni-tion with multi-object interactions, while others may require linking physical attributes to spatial reasoning."}, {"title": "4. Experiments", "content": "We investigate LVLMs' ability to perform visual reasoning to answer correctly DRIVINGVQA's questions, tackling the following research questions: 1. How well do popular LVLMs perform in real-world sce-narios involving complex visual reasoning? (4.2) 2. What type of information about relevant entities (name, localization, visual content) is the most useful for en-hancing LVLMs' visual reasoning? (4.3) 3. Can LVLMs learn how to predict the relevant entities to generate visual-chain-of-thought reasoning? (4.4) 4. What is the impact of learning to predict entities ex-tracted automatically from an external detector? (4.4)"}, {"title": "4.1. Experimental Setup", "content": "Evaluation Metrics. We implement a metric that mimics the French driving theory exam score: all correct answers have to be selected for each question. We also report the F1-Score. All results reported in Section 4 can be found in Appendix C with precision and recall."}, {"title": "4.2. Zero-Shot Evaluation", "content": "We evaluate the visual reasoning ability of popular VLMS in zero-shot settings to answer our first research question. We define a random baseline for comparison, running 100 times a random generator that selects responses from all answer combinations for each sample. We aggregate the mean exam score as well as the F1-score with their asso-ciated standard deviations. Furthermore, we estimate the human performance by recruiting participants with vary-ing levels of driving experience to capture a diverse range of human performance. We ask them to answer batches of 40 samples randomly selected from the test set. Each batch must be solved in less than 20 minutes to mimic the op-erational conditions of the driving theory test. In total, 6 annotators participated in the study and answered 240 ques-tions. Their average exam score is 71.7%. The official pass-ing score is 35/40, which corresponds to an exam score of 87.5%. However, as described in Section 3.1, we filtered the driving tests to keep only the questions that require visual reasoning, which tend to be the hardest. We report the performance on the DRIVINGVQA test set in Tab. 2 for three variants of the open-sourced model, LLaVA-OV (0.5B, 7B, 72B) and the closed-sourced model GPT-40. For each model, we use two prompting strategies: direct answer (noted QP-A) and reasoning before answer-ing (QP-EA). We use this notation format in the remainder of the paper, with the model input (QP = question and op-tions) before the dash, and the model output (A = answer, E = explanation) after the dash. The bigger LLaVA-OV, the better the performance. GPT-40 achieves the best re-sults in our benchmark, reaching a 60.7% exam score when prompted in a chain-of-thought fashion (QP-EA). Nevertheless, it remains far from the human baseline, with 71.7%. It demonstrates that DRIVINGVQA is a challenging bench-mark for general vision-language models, in particular due to the complexity of the involved visual reasoning and its domain-specific images. We also verify the impact of the image in DRIV-INGVQA. We observe a significant drop in performance in every model, with a decrease of 26 pts for GPT-40 (QP-"}, {"title": "4.3. Relevant Entities Prompting", "content": "In this section, we tackle our second research question: what type of information about relevant entities (name, lo-calization, visual content) is the most effective in enhancing VLMs' visual reasoning? To do so, we fine-tune LLaVA-OV-7B with different prompting strategies on the DRIV-INGVQA train set and evaluate the downstream perfor-mance on the test set, following the setup described in Fig-ure 5): \u2022 QPR-EA: The model is fed the question, list of options, and list of relevant entity names. It reasons before an-swering the questions. \u2022 QPRB-EA: On top of the previous strategy, the model is being fed the bounding box coordinates of the entities in the images. \u2022 QPRV-EA: Instead of the coordinates, the model is di-rectly fed with the image crops around the relevant enti-ties. We refer the reader to Appendix C for the implementation details of each prompting strategy."}, {"title": "4.4. Learning to Generate Relevant Entities", "content": "We investigate whether a vision-language model can be fine-tuned to learn how to generate relevant entities to per-form visual-chain-of-thought reasoning. We adapt the previously introduced relevant entities prompting format to the generative case: QP-REA (the model generates the list of relevant entities before reason-ing) and QP-RBEA (the model also generates the relevant entities coordinates before reasoning). To experiment with reasoning using entities' visual patches, we adopt a two-step conversational structure, as illustrated Fig. 6. The model is first asked to predict the list of relevant entities along-side their normalized bounding box coordinates in the im-age (QP-RB). Then, visual patches are automatically ex-tracted from the full image based on these predicted bound-ing boxes. The model finally provides its reasoning before answering the question. Following our conversation format typography, we refer to this approach as QP-RB-RV-EA. In Tab. 4, although each fine-tuning method achieves lower performances compared to their oracle counterpart (Tab. 3), all variants improve over simply fine-tuning with"}, {"title": "4.5. Analysis per number of entity", "content": "As a proxy of sample complexity, we consider the number of entities per sample to further analyze the performance of our models. We split the test set of DRIVINGVQA into chunks depending on the number of annotated relevant enti-ties. We consider the QP-EA setting as baseline: prompting the model to reason before answering. Figure 7 shows the average exam score across all runs for three systems tak-ing entities as input in the prompt (oracle, top part), and 4 systems learning to generate entities before answering the question (bottom part). In both cases, the performance gap between visual-crops-based methods (QPRV-EA, QP-RB-RV-EA) and the baseline widens as the number of entities"}, {"title": "4.6. Analysis per reasoning type", "content": "We explore the models' performance depending on the reasoning type annotations described in Sec. 3.4: environ-ment, multi-object, attribute, and spatial reasoning. Fig. 8 shows the average results over all seeds for QP-EA, splitting the test set into two subsets for each reason-ing type, depending on whether that reasoning is needed for each sample (1, in orange) or not (0, in blue). While en-vironment and spatial reasoning do not show a significant gap in performance between the two subsets, samples with multi-object or attribute reasoning show much lower perfor-mance than their vanilla counterpart, signaling much more difficult questions. Hence, we focus on these two reasoning types. We in-vestigate the impact of our various fine-tuning strategies on the gap in performance between the two subsets of the test set (Fig. 9, see full results in Appendix C.5). For attribute reasoning (top part), only adding the visual patches (QPRV-EA) lowers the performance gap between hard and easy questions, demonstrating the need for explicit visual infor-mation for the model to identify detailed physical elements in the image. For multi-object reasoning (bottom part), all fine-tuning strategies lower the gap with the baseline, with a significant performance increase even for the simplest set-ting (QPR-EA, with only entity names being provided); val-idating the importance of identifying key entities for this type of complex reasoning."}, {"title": "5. Conclusion", "content": "We introduce DRIVINGVQA, a visual reasoning dataset based on driving theory exam questions. This dataset con-tains multiple-choice questions on real-world images along with human-written explanations interleaved with multiple relevant entities, identified by their label and bounding box"}]}