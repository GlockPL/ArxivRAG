{"title": "A Comprehensive Study on Quantization Techniques for Large Language Models", "authors": ["Jiedong Lang", "Zhehao Guo", "Shuyu Huang"], "abstract": "Large Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning has been growing successfully with no-ticeable rate since 2000, and especially the recent 10 years with a compound growth rate of 30%. One of the most com-mon branches from machine learning, Deep learning, requires more data than other machine learning branches, which in turn requires more computational power. In general, deep learning models need powerful graphical processing units (GPUs) to handle the large volumes of data and complex calculations involved in training deep neural networks. However, access to the unlimited computational resource (GPUs) for deep learning models is far from ideal due to the high costs. A research field, Quantization in deep learning, aim to reduce the high cost of computations and memory by representing the weights and activation in deep learning models with low precision data types. In the Computer Science, a floating point(float32) consists of 32 bits requires much larger resources than a integer(int8) consists only 8 bits. This type of quantization is straight forward as reducing the number of bits of the data type, from float32 to int8, to consume less computational costs. In addition, mathematics operations including matrix multiplications can be performed with faster speed with lower precision data types.\nThe dictionary definition of quantization is the division of a quantity into a discrete number of small parts, often assumed to be integral multiples of a common quantity. The first use of quantization is rounding off and was analyzed by Shappard [1]. In addition, Shannon Entropy quantifies the uncertainty within a dataset, and the process of quantization can influence the calculated entropy by altering the precision of data values [2]. With the advancement of computer science and machine learning, quantization research fields expands significantly. One of the most common quantization techniques is 8-bit quantization which convert floating point data type to integer data type. While 32-bit single-precision floating-point has been the predominant numerical format for deep learning (DL) ap-plications, alternative formats has emerged recently to enhance the computational performance of these applications [3]. It is very common to train neural networks using 16-bit floating-point formats, such as fp16 or bfloat16, which are supported by most DL accelerators. After training, neural networks can be deployed for inference using even lower-precision formats, in-cluding floating-point, fixed-point, and integer representations. Low-precision formats confer several performance advantages. Firstly, many processors are equipped with higher-throughput mathematical pipelines for low-bit formats, thereby accelerat-ing computation-intensive tasks like convolutions and matrix multiplications. Secondly, reduced word sizes reduce memory bandwidth constraints, resulting in improved performance for bandwidth-limited computations. Third, smaller word sizes decrease memory size requirements, which enhances cache utilization and positively impacts various aspects of memory system performance. Utilizing the int8 quantization, this work achieves the ability to sustain model accuracy within 1% of the baseline floating-point networks. This is particularly notewor-thy for networks that are typically difficult to quantize, such as MobileNets and BERT-large. Moreover, Vector quantization used to compress the deep convolutional networks [4] is a good work supplements this research filed. In general, a CNN that works well object classification contains eight layers and a huge number of parameters, and it is widely known that the parameters are heavily over-parameterized. The goal of this work is to compress these parameters while maintain the high accuracy. This research mainly focus vector quantization methods for the compression of densely connected layers. It involves parameter binarization, scalar quantization through k-means clustering, and structured quantization employing product quantization or residual quantization, all of which lead to significant improvements in performance.\nLanguage model is a branch of machine learning that is designed to understand and generate natural language. Tech-nically, it understands the context of the prompts and generate the missing part with coherent and contextually appropriate language. Language models can be classified into four major types: Statistical Language Models (SLM), Neural Language Models (NLM), Pre-trained Language Models (PLM), and Large Language Models (LLM). Each of these models rep-resents a distinct approach to natural language processing, with varying techniques and capabilities for handling linguistic data [5]-[8]. While Statistical Language Models and Neural Language Models have been researched for decades, the Pre-trained Language Models and Large Language Models draw a lot of popularity in both academia and industry recently and the applications are widely used in various fields. For example, recommendation system [9], [10]. Pre-trained Language mod-els propose train largely amount of text data before fine-tune for specific downstream tasks. Based on the Transformer archi-tecture and the self-attention mechanism [11]\u2013[15], Pre-trained Language Models advances the performance for semantic-purpose language processing. BERT(Bidirectional Encoder Representations from Transformers) serves as an exemplary Pre-trained Language Model, pre-trained on a large corpus of text in an unsupervised manner using masked language modeling and next sentence prediction and fine-tuned for tasks like question-answering, sentiment analysis, and text classification. It has become the dominant paradigm in Pre-trained Language models due to its efficiency, scalability, and superior performance across multiple tasks [?], [16], [17]. Large Language Models represent an extension of research based on Pre-trained Language Models, building upon their foundational Transformer architectures to enhance capabilities in natural language processing. It's found that scaling up PLM model size and data size enhances the capacity to perform more effectively on downstream tasks. For example, GPT-3, a significantly larger PLM with 175 billion parameters, shares a similar architecture and pre-training tasks with standard PLMs. However, despite the primary focus of scaling being on model size, GPT-3 shows remarkable abilities and out performs standard PLM in solving complex tasks. In certain creative writing fields [18], GPT-3 model highly capable of producing creative content like poetry, song lyrics, or fic-tion that are coherent to specific styles or themes whereas BERT(340 million parameters) is not designed for creative writing tasks which only capable of completing sentences or predicting missing words. Due to Large Language Model's attribute that requires practical large-scale data processing and distributed parallel training, conducting repetitive studies to explore various training strategies is highly resource-intensive and costly.\nQuantization techniques can help mitigate the high costs associated with training Large Language Models(LLM) by re-ducing the computational and resource demands. By reducing the number of bits required for each of a model's weights, significantly decreases the overall model size. This reduction leads to LLM that consume less memory, require less storage space, are more energy-efficient, and enable faster inference. These advantages enable LLM to operate on a broader range of devices, including embedded devices and single GPU devices. For instance, supporting AI models on SLAM robotics devices [?], [19], [20] or decentralized web3 applications is very chal-lenging, as these systems cannot easily run full-sized models due to their limited capacity for handling high-cost computa-tions [21]-[24]. In such cases, quantization becomes necessary for enabling these integrations, as it reduces model size and computational requirements, making deployment on resource-constrained platforms achievable. While there are various quantization techniques, the two most notable types used in LLM are Post-Traning Quantization(PTQ) and Quantization-Aware Training(QAT). PTQ refers to a technique used to reduce the size and computational demands of a machine learning model after it has been trained and it only affects the inference state. The research work SmoothQuant [25] introduces a PTQ solution aimed at reducing hardware costs and democratizes LLMs. SmoothQuant enables 8-bit weight, 8-bit activation(W8A8) quantization for LLMs and it smooths the activation outliers by offline migrating the quantization dif-ficulty with a mathematically equivalent transformation given that weighs are easy to quantize but activations are not. QuIP [26] is another research work that employs PTQ in LLMs. This method is based on the insight that quantization benefits from incoherent weight and Hessian matrices. As a result, QuIP improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. On the other hand, Quantization-Aware Training (QAT) technique refers to opti-mize models for efficient inference by simulating the effects of quantization during the training process. In contrast to PTQ techniques, QAT integrates the weight conversion process dur-ing the training stage. The Research work Degree-Quant [27] efficiently improves inference time of Graph Neural Networks by utilizating the QAT technique. Degree-Quant explores the viability of training quantized GNNs, allowing the use of low precision integer arithmetic during inference. Models trained with Degree-Quant for INT8 quantization perform comparably to FP32 models in most cases, while INT4 models achieve up to a 26% improvement over baseline models. Moreover, EfficientQAT [28] is another research work proposes a more feasible QAT algorithm that satisfies reducing memory con-sumption during LLM training. EfficientQAT employs a two-step approach: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP) in which reducing accuracy loss in low-bit scenarios and then trains only the quantization parameters end-to-end. As a result, EfficientQAT outperforms previous quantization methods across a range of models with scales from 7B to 70B"}, {"title": "II. RANGE MAPPING", "content": ""}, {"title": "A. AFFINE QUATIZATION", "content": ""}, {"title": "1) THE EQUATION BEHIND THE AFFINE QUATIZATION:", "content": "The parameter matrices in machine learning models can be exceptionally large. For instance, Chat GPT-3 incorpo-rates approximately 175 billion parameters, represented using 16-bit floating-point precision (float16). Each float16 (FP16) occupies 16 bits, equivalent to 2 bytes. Consequently, the storage requirement for Chat GPT-3 amounts to approximately 350 gigabytes. Quantization is a technique employed to reduce the precision of parameters, thereby optimizing model perfor-mance and storage requirements. A widely adopted method for this purpose is the affine quantization scheme, which can be mathematically expressed by the following equation\n$Xq = (x * S + Z)$"}, {"title": "2) THE PARAMETERS BEHIND THE EQUATION:", "content": "Assume the full-precision data range is [\u03b2, \u03b1].\nx is the weight without quantization.\nXq is the quantized weight.\nS is the scaling factor, E.g, if converting from FP32 to FP16, would be 2.\nZ is the zero point. It represents in same precision as the quantized value of 0 in full-precision value. z = -round(\u03b2*s)-\n$2^{(\\beta - 1)}$\nQuantizing precision within the same data type, such as from FP32 to FP16, is relatively straightforward. However, the process becomes more complex when converting between different data types, such as from FP32 to INT8. The challenge lies in the fact that FP32 represents floating-point numbers, whereas INT8 is limited to integer values. Additionally, FP32 offers a much wider numerical range compared to INT8. \u03a4\u03bf overcome these challenges, a technique known as calibration is employed to determine an appropriate scaling factor, ensuring effective mapping between the two data types. In this case, scaling factor\n$S = \\frac{2^b - 1}{\\alpha - \\beta}$\nOnce the scaling factor is determined, it can be applied to map unquantized weights to their quantized counterparts effectively.\nThrough the application of quantization, the size of parameter matrices can be significantly reduced. For example, in the case of Chat GPT-3, if the precision is reduced from FP16 to FP4, each parameter's size would decrease by a factor of four, according to the previously discussed equation. As a result, the total storage requirement for the 175 billion parameters in the Chat GPT-3 model would be reduced to approximately 90 gigabytes, representing a substantial reduction compared to the full-precision model."}, {"title": "B. SCALE QUATIZATION", "content": ""}, {"title": "1) THE EQUATION BEHIND THE SCALE QUATIZATION:", "content": "Let us assume the range of full-precision value is [-\u03b1, a] and it will be quantized to a b-bit integer value x.\n$S = \\frac{2^b - 1}{\\alpha}$"}, {"title": "To quantize a floating data:", "content": "$Xq = (x * S)$"}, {"title": "2) DETAILS OF SCALE QUANTIZATION:", "content": "Different from affine quantization, scale quantization only performs quantization with a scale transformation. This is usually used for two data types have equal proportion and same zero points. [?]. For simplicity and symmetry, the range of int8 would be [-127, 127], instead of [-127,128], so the range would symmetric. But for lower bit representation, ignoring one number for symmetry might shorten the mapping value range."}, {"title": "III. QUANTIZATION GRANULARITY", "content": "The granularity of quantization can be decided based on the requirement of training the model.\nPER-LAYER QUANTIZATION n a neural network, all filters within the same layer share the same quantization pa-rameters, with the quantization range determined collectively based on the values across all filters in that layer. This level of quantization granularity simplifies the quantization process but often results in reduced model performance. The primary reason is that applying a uniform quantization strategy to all filters may introduce larger quantization errors for individual filters, compromising the model's overall accuracy.\nPER-CHANNEL QUANTIZATION To enhance perfor-mance following quantization, a higher granularity of quan-tization is employed. Instead of being applied at the layer level, quantization is performed at the filter level within each layer of the neural network. Each filter is assigned customized quantization parameters, tailored to the specific range of values within that filter. Although this approach increases the complexity of the quantization process, it generally leads to improved model performance compared to methods with lower granularity, as it reduces quantization errors for individual filters."}, {"title": "IV. QUANTIZATION CALIBRATION", "content": "The maximum and minimum values from the full-precision data range are utilized to scale the data to a lower precision. For example, when scaling from INT16 to INT8, the INT16 value range of [-32767,32767] is evenly mapped onto the INT8 value range of [-125, 125]. However, various methods exist to determine the optimal value range prior to quan-tization, aiming to enhance the overall performance of the quantized model.\nGLOBAL CALIBRATION This approach represents the simplest method of calibration. It selects the maximum and minimum values from the unquantized data without distinction and converts them to a lower precision. While this method simplifies the quantization process, it can compromise accu-racy, as not all values within the unquantized data range are necessarily relevant or required for optimal performance.\nMAX CALIBRATION This calibration method selects the maximum value from the unquantized data, rather than relying on the global value range of the unquantized data type. This approach enables a more precise alignment between the quantization process and the parameters, thereby mitigating the loss typically associated with quantization.\nKL DIVERGENCE CALIBRATION Kullback-Leibler (KL) divergence measures the difference between two proba-bility distributions. In the context of quantization, it is em-ployed to compare the distribution of quantized data with that of the original full-precision data. Various scaling factors are evaluated to generate different distributions for the full-precision data, and KL divergence is used to identify the scaling factor that minimizes information loss. This approach enables more effective quantization compared to max-value calibration, as it preserves data integrity more accurately.\nPERCENTILE CALIBRATION Percentile calibration is based on the distribution of the data rather than the full range of the original precision data type. For instance, in the case of normally distributed data, the minimum and maximum values are sparsely located at the distribution's tails. Percentile calibration focuses on a specified percentile, excluding values that fall below or above certain thresholds. This approach effectively narrows the range of full-precision values, thereby improving the performance of quantization. Optimal percentile values for calibration are typically 99.99% or 99.999%, as lower percentiles are considered too aggressive, excluding more high-magnitude values and negatively impacting quantization performance [29]."}, {"title": "V. QUANTIZATION TECHNIQUES", "content": "There are several methods available for implementing model quantization. This section presents a range of quantization techniques aimed at achieving faster computation, minimizing accuracy loss, and reducing the model's parameter size.\npost-training Quantization The primary concept of post-training quantization is to apply quantization after the model has been fully trained. Two common approaches are used to achieve this: Dynamic Quantization: In this method, quanti-zation occurs at runtime after each activation. However, this approach introduces additional computational overhead, poten-tially slowing down performance due to the extra processing required for each activation. Static Quantization: Here, the quantization parameters are pre-computed during the quan-tization process, prior to runtime. This method ensures the quantization of weights while minimizing runtime overhead, as the need for on-the-fly calculations is eliminated.\nQuantization-Aware Training The model accounts for the errors introduced by quantization by incorporating quantiza-tion operators at each activation during the training phase. These operators enable the model to recognize and adapt to quantization errors throughout the backpropagation process. As a result, this approach typically leads to reduced perfor-mance degradation while also facilitating faster computation [30].\nWeight Quantization Instead of quantizing all parameters in the model, this technique selectively quantizes only a subset. Specifically, it targets the weight matrices for quantization while preserving the activation values in their original pre-cision. By employing this approach, the data storage require-ments can be significantly reduced.\nActivation-Aware Weight Quantization While traditional weight quantization applies to all parameters within the weight matrices, alternative approaches can further reduce the number of parameters to be quantized, thereby enhancing quantization speed. Activation-Aware Weight Quantization (AWQ) [31] in-troduces a selective approach, recognizing that not all weights are equally important and only a subset requires quantization. This technique identifies critical weights based on activation magnitudes, retaining these essential weights in full precision while quantizing the non-critical ones. By focusing on the most influential parameters, this method minimizes accuracy loss while maintaining lower computational costs.\nAttention-Aware Weight Quantization This quantization technique utilizes the Hessian trace as a measure to determine the importance of weight matrices [32]. It leverages the attention mechanism, commonly employed in Large Language Models (LLMs). Weights identified as more significant based on attention scores are assigned higher-bit precision, while less important weights are quantized using lower-bit repre-sentations. This approach adopts mixed-precision quantization, resulting in improved performance compared to previous quan-tization techniques. Notably, it achieves superior efficiency in LLMs without compromising model accuracy."}, {"title": "VI. ANALYSIS OF QUANTIZATION APPROACHES IN LLMS", "content": "After exploring various quantization techniques and the theory behinds, we are going to focus more on the two most common used quantization methods in LLMs, Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). While both techniques enable models to operate in lower-precision formats, they differ in their trade-offs between ease of implementation, computational cost, and model accuracy. PTQ is widely favored for its simplicity and fast deployment, as it can be applied to a pre-trained model without requiring additional training. However, for large and complex models like LLMs, PTQ can result in considerable accuracy loss, since the model is not optimized for quantization. On the other hand, QAT involves simulating quantization during the training process, allowing the model to adjust to lower precision. Although this method results in better accuracy retention, it introduces additional computational overhead and longer training times. In this section, we dive deep into two noticeable works that one utilizing PTQ while another one employing QAT. We will examine their respective improvements and limitations in detail.\nGPTQ. is a quantization technique utilizing PTQ designed to reduce the size of models without great loss of accuracy on LLMs [33]. Previously, a research work, an Extreme Compression for Pre-trained Transformers Made Simple and Efficient, applies compression at the scale of GPT-175B [34]. The compression works well for low compression targets, e.g., 8-bit weights, but the work fails to preserve accuracy at higher rate and leaves question that if one-shot PTQ with higher accuracy a feasible approach. GPTQ addresses the challenge of efficient execution for models with hundreds of billions of parameters by introducing a novel PTQ method. This approach enables models to be executed within a few hours while compressing them to 3 or 4 bits per parameter without significant accuracy loss. Additionally, GPTQ sup-ports extreme quantization, successfully reducing models to as few as 2 bits. In practice, this allows the compressed OPT-175B model to run on a single NVIDIA A100 GPU or two NVIDIA A6000 GPUs. By implementing custom GPU kernels that leverage compression for faster memory loading, GPTQ achieves notable speed improvements around 3.25x on A100 GPUs and 4.5x on A6000 GPUs. However, its speed gains are limited during multiplication operations due to the restricted hardware capabilities for mixed-precision operands (e.g., FP16 x INT4) on mainstream architectures. Before delving into the algorithm, we first review the background of the research. This study implements Layer-wise Quantization, which performs quantization on a layer-by-layer basis and addresses a corresponding reconstruction problem within the PTQ technique. In simple terms, the objective is to find a matrix of quantized weights that minimizes the error in relation to the full-precision output of each layer. It can be formulated as a mathematical function as\n$\\mathop{\\arg \\min}\\_{W} ||WX - \\tilde{W}X||^2$ (1)\nwhere W denotes weights and X denotes the layer input. In addition, this research build on a work called Optimal Brain Quantization (OBQ) which aims to find the minimum error on each layer as we described earlier. As in Equation (1), OBQ solves by quantizing each row w of W independently and always updating the rest not quantized weights, in which the Hessian matrix is $H_F = 2X_F X_F^T$, F denotes the remaining full precision weights.\n$\\mathop{\\arg \\min}\\_{W_q} \\frac{(\\text{quant}(w_q) - w_q)^2}{[H_F]_{qq}}$ (2)\n$\\delta_F = \\frac{w_q - \\text{quant}(w_q)}{[H_F]_{qq}} (H^{-1}) :,q$\nAs defined in Equation (2), $w_q$ denotes the optimal weight to be quantized next, and the corresponding weights to be updated in F is denoted as F. OBQ repeatedly follows these equations during the quantization of weights, effec-tively solving them for medium-sized models. However, as model sizes grow larger (reaching billions of parameters), the computational cost increases significantly. To address this, GPTQ introduces modifications to the quantization procedure to improve computational efficiency. Instead of independently quantizing the weights of each row based on their corre-sponding errors, GPTQ quantizes the weights of all rows in the same order. This approach yields a final squared error comparable to the greedy optimal solution (OBQ). As a result, this method reduces computations on the set of full-precision weights F and $H^{-1}$, since they remain the same for all rows.\nIn the practice, the research validates GPTQ's perfor-mance against existing quantization methods, focusing on both smaller models, ResNet and BERT, and large-scale models, BLOOM and OPT. For quantization in small models, GPTQ performs comparably to other methods at 4-bit precision but is slightly less accurate than the most precise techniques at 3-bit precision. Despite this, GPTQ significantly outper-forms AdaQuant-the fastest PTQ method among those com-pared-in terms of speed. Overall, GPTQ is highly compet-itive with state-of-the-art PTQ methods for smaller models, reducing processing time from hours to less than a minute. (2) GPTQ successfully compresses the entire OPT and BLOOM models to 3-bit and 4-bit precision. The performance of these quantized models is evaluated across several language tasks, including WikiText2. As shown in Table III and Table I, GPTQ significantly outperforms RTN, with RTN showing a notable drop in performance at 3-bit precision for the OPT model. Overall, GPTQ successfully introduces an effective approach for quantizing Large Language Models (LLMs), enabling the accurate quantization of the largest public models to 3-bit and 4-bit precision with minimal accuracy loss. While the research demonstrates superior performance in Post-Training Quantization (PTQ), its focus is limited to generative tasks, with no exploration of activation quantization.\nLLM-QAT, Key-Value Data-Free Quantization Aware Training, is a specialized quantization technique designed to reduce quantization errors in the Key-Value (KV) caches of Large Language Models (LLMs). KV caches store critical intermediate outputs from attention layers, ensuring the model does not need to recompute information for each token. Unlike other quantization methods that primarily focus on activations or weights, KV-QAT targets these caches, which play a crucial role in every token's processing. While post-training quanti-zation is commonly employed in LLMs, Quantization-Aware Training (QAT) is underutilized due to the resource-intensive nature of LLM training and the limited availability of suitable training data. A notable limitation of post-training quantization is the significant degradation in performance when precision drops below 8 bits [35]. To address these challenges, KV-QAT emphasizes the quantization of KV caches and em-ploys knowledge distillation to overcome the lack of training data, particularly when the data is sensitive, restricted, or unavailable. To mitigate the data scarcity issue for QAT, KV-QAT uses self-generated data from the model itself. Data-free methods do not rely on the original training datasets but instead generate synthetic data to fine-tune the model, helping it adapt to quantization-induced errors. The data used for this purpose is derived from the next token predicted by the model. However, if the data generation strategy selects only the token with the highest probability, it can result in limited training data with low diversity, as generated sentences tend to be repetitive. To overcome this issue, a more effective strategy involves randomly selecting the next token based on the prob-ability distribution predicted by the model. This distribution is calculated using the softmax function\n$\\tilde{X} = \\alpha \\frac{X_R}{\\alpha}, \\alpha = \\frac{\\text{max}(X_R)}{2^{N-1}-1}$ (3)\nHere, $X$ denotes the quantized weights and activations, and $X_R$ denotes the full-precision weights and activa-tion.Additionally, per-token activation quantization and per-channel weight quantization are employed as illustrated in Figure 2. Per-token quantization refers to the process of quantizing the key-value pairs in the cache on a token-by-token basis, applying quantization individually to each token's data as it is generated. Furthermore, the quantization process is integrated with gradient computation, ensuring that the model learns to effectively handle quantized key-value pairs during training. During the QAT process, quantization is applied to the activation tensors associated with both the keys and values, enabling the model to adapt to the quantized representations throughout training. Furthermore, LLM-QAT addresses the challenge of limited access to training data. It leverages data generated by pre-trained models, eliminating the need for direct access to the original datasets, thereby ensuring effective fine-tuning without compromising performance."}, {"title": "VII. CONCLUSION", "content": "After thoroughly examining various popular quantization techniques, it is evident that their performance varies de-pending on the precision of the quantization. For example, GPTQ achieves optimal performance at 4-bit precision but experiences a decline at 3-bit precision. In addition, LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]. To maximize the performance of a given quantization technique, developers must carefully select appropriate precision settings. Future research on quantization techniques could further explore the impact of precision configuration, potentially leading to more refined and efficient quantization strategies."}]}