{"title": "WaveletGPT: Wavelets Meet Large Language Models", "authors": ["Prateek Verma"], "abstract": "Large Language Models (LLMs) have ushered in a new wave of artificial intelligence ad- \nvancements impacting every scientific field and discipline. They are trained on a simple \nobjective: to predict the next token given the previous context. We live in a world where \nmost of the data around us, e.g., text, audio, and music, has a multi-scale structure asso- \nciated with it. This paper infuses LLMs with traditional signal processing ideas, namely \nwavelets, during pre-training to take advantage of the structure. Without adding any extra \nparameters to a GPT-style LLM architecture in academic setup, we achieve the same \npre-training performance almost twice as fast in text, raw audio, and symbolic music. This \nis achieved by imposing a structure on intermediate embeddings. When trained for the same \nnumber of training steps, we achieve significant gains in performance, which is comparable to \npre-training a larger neural architecture. Our architecture allows every next token prediction \naccess to intermediate embeddings at different temporal resolutions in every Transformer \ndecoder block. This work will hopefully pave the way for incorporating multi-rate signal \nprocessing ideas into traditional LLM pre-training. Further, we showcase pushing model \nperformance by improving internal structure instead of just going after scale.", "sections": [{"title": "Introduction and Related Work", "content": "Large Language Models (LLMs) have ushered in a super-renaissance of AI models and are touching every \nscientific and engineering discipline. At the heart of this revolution has been the Transformer architecture \n(Vaswani et al., 2017), initially proposed for machine translation in natural language processing. Transformer \narchitecture became the backbone of GPT (Generative Pretrained Transformer) language models (Brown et, \n2020) first proposed by Open-AI, which has revolutionized the field. Modern LLMs are still trained on a \nstraightforward objective: To predict the next token given the previous context, preserving the causality \nassumption. The exact recipe has been shown to work not only for language but also for robotics (Brohan \net al., 2023b;a), protein sequences (Madani et al., 2020), raw audio waveforms(Verma & Chafe, 2021), acoustic \nand music tokens (Huang et al., 2019; Verma & Smith, 2020; Borsos et al., 2023), videos (Yan et al., 2021) to \nname a few. This simple recipe of tokenization/creating an embedding and feeding it to transformers also \nhas given rise to architectures in non-causal setups such as BERT(Devlin et al., 2019), Vision Transformers \n(Dosovitskiy et al., 2021), Audio Transformers (Verma & Berger, 2021) and Video Transformers (Selva et al., \n2023). The recent surge in multi-modal large language models similar to that proposed by Google with \nits Gemini family (Team et al., 2023) or multi-modal models like Chameleon (2024) would pave the way \nfor another wave of applications in the future. With increased performance by scale, some of the models \nlike GPT-3 are reaching hundreds of billions of parameters (Brown et, 2020) to that of Google's Switch \nTransformer has even reached trillion parameters (Fedus et al., 2022). This has led to recent concerns that AI \nresearch is slowly moving out of academics and is getting confined to industry researchers, as per the recent \nWashington Post article written by Nix (2024).\nThe theme for this work is to push the capabilities of the models to get capabilities of a much bigger architecture \nor achieve the same performance in smaller training steps. Briefly, we take intermediate embeddings after"}, {"title": "Dataset", "content": "We utilize three open-source datasets to showcase the strength of our proposed method. In addition, we \nchoose them from three different domains: natural language, symbolic music, and raw audio waveform. For \ntext, we choose text-8 (Mikolov et al., 2012). We choose this over other datasets as i)it is a popular and \nwidely cited character-level language modeling dataset for text and ii) in order to use a simple vocabulary"}, {"title": "Methodology", "content": "This section will describe the approach to incorporating wavelets into transformer-based Large Language \nmodels while retaining the causality assumption. The ideas described here are generic and can be easily \nextrapolated to setups without a Transformer architecture."}, {"title": "Incorporating Wavelets into Intermediate Embeddings", "content": "For any signal, we would compute one of the versions of discrete wavelet transform as we will describe and \nincorporate that back into the signal. Let us assume that $x^{(i)}$ is the output of the $l^{th}$ decoder layer and \nrepresents the activation along the $i^{th}$ coordinate. This activation signal will have a dimension equal to the \ncontext length of the transformer-based GPT model. In our case, we denote the context length as $L$. So now, \nif in the original GPT architecture, there were N + 1 layers, with the embedding dimension as E, we would \nget N.E signals of length L from all of the intermediate embeddings between two decoder blocks. E in our \ncase goes from [0 \u2013 128) dimensions."}, {"title": "Introduction to Wavelets", "content": "A wavelet is a signal that typically has zero mean and a non-zero norm. A wavelet transform was first \ndesigned to overcome the shortcomings of a traditional Fourier-based representation. Given any signal x[n], a \ndiscrete wavelet transform is akin to passing the signal through filters with different resolutions, as shown in \nFigure 2. In its simplest form, we will use Haar wavelet, a family of square-shaped functions throughout \nthis paper. The family is obtained from a mother wavelet via scaling and shift operations. Given a mother \nwavelet function $\\psi$, we come up with the child wavelets as\n$\\psi_{j,k}[n] = \\frac{1}{2^{j/2}} \\psi \\left(\\frac{n-k2^j}{2^j}\\right)$\nHere j is the scaling factor and k the shift factor. These are nothing but signals that are shifted, and scaled \nto capture the information of the signal of interest at various time-scales, with n being time or in our case \nthe context length. This should immediate strike similarity to that of the diagram in Figure 1 to capture \nvarious signals present in Transformer decoders intermediate layers at various resolutions. We now define \ndiscrete wavelet transform. Simply, it can pass any signal through filters and downsampling operations. \nThis operation, as seen in Figure 2, should immediately strike a resemblance to a convolutional neural net \nlike Resnet (He et. al, 2016), which consists of learned convolutional filters analogous to h[n] and g[n], \nand downsampling operation like max-pooling. In traditional state-of-the-art convolutional architecture, we \ntypically follow one branch of Figure 2, i.e., we take the output of filters, downsample, and do it recursively. \nThis was also one of the reasons wavelets were incredibly popular in the early 90s and 2000s for image \nunderstanding, as one can see parallels to that of convolutional architectures (Huang & Aviyente, 2008;\nKingsbury & Magarey, 1998). Let us assume that we choose a family of wavelets (Haar wavelet in our case);"}, {"title": "Connecting wavelets and LLM embeddings", "content": "Often, in many signal processing applications, the first-order detail coefficients and all of the approximate \ncoefficients are used to understand the contents of the signals at various levels. We also intend to carry out \nthe same operation, but we are now getting signals from intermediate transformer embeddings. However, we \ndo not take detailed coefficients and look into the approximate ones. This was our premise: that real-world \ndata around us is structured. For text, the structure at different levels ranges from letters, words, sentences, \nparagraphs, topic models, etc. In the case of symbolic music, it can be thought of as musical notes to motifs \nto pieces and so on. Since we chose Haar wavelet for the rest of this work, this can be approximated as \na simple averaging operation, as described in the previous section. If we keep going down the path of the \napproximate coefficients, we will eventually have only a single scalar, which is the average of the whole signal \nfor the case of the Haar wavelet. In order to get the same sequence length from the approximation coefficients \nas the original signal, there can be several ways, with up-sampling the signal back to the original length \nbeing one of them. As part of nomenclature, we call the signal approximated at a particular level with the \nsame length as the \"approximate signal\" at that level to discern it from the approximate coefficients, which \nare smaller in length. In Figure 2 (R), in order to get the signal approximation at various levels, which is \nequal to the original input signal x[n], the wavelet kernel being averaging operation, we take the approximate \ncoefficients and multiply it with the kernel at that level. ([1,1], [1,1,1,1], and so on). This can be reflected \nin the piece-wise constant function, as seen in Figure 2. We can see that for different embedding coordinates \nof LLM embeddings, we define different resolution kernels, each of them corresponding to a particular scale \nat which we should capture the data. The reconstructed signal $x_{recon}[n]$, which is one way of getting the"}, {"title": "Imposing Structure: Toy Example", "content": "As we can see from Figure 3 we have shown a toy example to depict how we impose a structure onto decoder \nTransformer embeddings. In Figure 3 (left), on top, eight variations along the token dimension are present, \nwith onset (largest values or sudden bursts) at token index numbers 32, 64, and so on and decreasing to zero \nin the next token and then again increasing to the largest value linearly till the next interval. As motivated in \nthe introduction before, datasets around us have an inherent structure present in them. In order to capture \nthis structure, we impose a structure onto intermediate Transformer embeddings in every layer. For the toy \nexample, we can see from Figure 3 (left), no bottom, that we retain the embeddings at the exact resolution \nfor half of the embedding dimensions (split by white line). For the other half of the embedding dimension, \nwe slowly increase the kernel length across the context length and causally compute the average. We reach"}, {"title": "Experiments", "content": "In this section, we explain how we incorporated the idea of infusing wavelets into a large language model \npre-training. We trained all of the models from scratch, which required substantial computing. However, \nthe main aim of these experiments is to show how the performance of the models across three modalities \nimproves with/without doing intermediate modifications on embeddings. Since we do not add any parameters \nwhen we modify intermediate embeddings with wavelet transform, we can compare the two models in terms \nof the performance boost the new architecture achieves and speedups."}, {"title": "Baseline And Training Setup", "content": "All models, similar to the GPT-2 architecture, consist of a stack of Transformer decoder layers. Since each \nrequires pre-training the models from scratch, we choose the following setup. Every modality, namely text, \nsymbolic music, and raw waveform, has the same architecture topology with a context length 512. We choose \nthe number of decoder blocks to be 10, with 128 as the embedding dimension, the feed-forward dimension to \nbe 512, and the number of heads to be 8. We opt for a two-layer feed-forward MLP inside the Transformer \nblock after the attention block instead of a single layer typically used in Vaswani et al. (2017), with both the \nlayers sharing the same number of neurons, i.e., 512, that of the feed-forward dimension. The final output \nlayer of the Transformer decoder is then followed by a dense layer of 2048 neurons, followed by a dense \nlayer of the same size as the vocabulary. This vocabulary size varies in the three modalities. For text8, it \nis 27, which is the number of characters plus an added extra token for space. For the raw waveform, we \nuse an 8-bit resolution waveform at 16kHz, which is similar to the reported in (Goel et al., 2022; Verma, \n2022), thus yielding 256 as a vocab size. For symbolic music, we utilize Google's tokenizer (Huang et al., \n2019) to convert MIDI data to discrete tokens yielding 388-sized vocabulary. The baseline models in all \nthree were simply a stack of Transformer decoder blocks without tinkering with any embeddings. For the \nproposed architecture we explained in the previous section, retain half of the embedding coordinates without \nany tweaks. For the other half, we impose a multi-scale structure parameterized by the coordinate in the \nembedding dimension for all intermediate layers. We do not add any single parameter in this setup and \ncompare the performance with this tweak for all three modalities. We do this because we want to showcase \nthe powerfulness of our algorithm for a rich variation of modalities for LLM pre-training. We do not compare \nagainst powerful, larger architectures going after scale, as this paper required pre-training from scratch.\nInstead, we take a shrunk-down version of GPT-2 architecture, viable in academia with limited resources and \ncompare it with/without adding wavelets to the architecture regarding pre-training performance. All models \nwere trained from scratch in the Tensorflow framework Abadi et al. (2016) for 25 epochs. We used a mirrored \nstrategy for multi-GPU training. The learning rate schedule was chosen to be 3e-4 to start with reduced \ntill 1e-5, whenever loss started plateauing. The number of training points available in all three models was \n1M, yielding the total number of tokens to be 1/2 billion. These were randomly cropped from the dataset of \nchoice. Apart from setting a default dropout rate of 0.1 in MLP and attention layers, no other regularization \nwas done. The performance metric chosen to compare is only the negative log-likelihood loss, as this method \nimproves the core architecture of the transformer-based GPT and helps achieve the objective we want to \nachieve: predict the next token. Since we are operating on intermediate embeddings, our work can hopefully \ngeneralize to setups with structured data similar to text, raw audio, and symbolic music, where one can go \nfrom a fine-grained structure to a coarse structure. We can see from Figure 3 how, for a toy example, we can \nimpose a multi-scale structure that allows the attention mechanism to not only learn dependencies across"}, {"title": "Performance on modalities", "content": "In this section, with the added modifications, we compare the performance of our baseline architecture \nacross three modalities, namely text, symbolic music, and audio waveform with/without the addition of \nwavelet-based intermediate operation. We see that we substantially increased performance in all three \nmodalities when we trained for the same number of training steps. To give an analogy for natural language, \na decrease of 0.04 in validation loss is akin to going from a 16-layer architecture to a 64-layer model on \na text-8 dataset (papers-with code, 2024). As shown in Figure 4, we achieve the same loss almost twice \nas fast as the original architecture regarding training steps for a shrunk down GPT architecture. This is \nparticularly important as the GPT-like architecture can indeed take advantage of the structure that we \nimposed on half of the embedding dimensions. This speedup, i.e., the number of epochs/steps taken to \nachieve the same performance when the loss starts to plateau, is even smaller for raw audio. One of the \nreasons this can be attributed to is that audio signals remain quasi-stationary for smaller time scales, i.e., \n20ms-30ms for harmonic sounds. For a sampling rate of 16KHz, a context length of 512 would correspond to \n32ms, which may be one of the reasons that some of the coordinates nail down the contents of the context in \nfewer coordinates onto which we impose structure. The convergence happens much faster for raw waveform \nLLM setup than it is almost twice as fast in text-8 and symbolic music. We also compare our modifications' \nabsolute clock run times in both learnable and non-learnable setups. We report the time it takes to complete \none epoch relative to our baseline architecture. We see from Table 1 that our method is computational \ninexpensive, as the only operation carried out was simple averaging for the case of Haar wavelet or learning a \nsingle filter convolutional kernel with variable context lengths over various embedding dimensions."}, {"title": "Effect of Depth And Model Dimension", "content": "Here, we explore two variants of our architecture - what would happen if we reduce the model dimension \nfrom 128 to 32 and reduce the number of layers. We carry out all the experiments for text-8. We can see that \nfor the variant where we reduce the model dimension to 32 for a 10-layer Transformer decoder architecture \nwith eight heads, the model still retains faster performance as a baseline, almost twice as fast as seen in \nFigure 4, and achieves the performance without doing the modification (as seen as baseline) in around ten \nepochs. For the second experiment, we retain the exact architecture as proposed in our experiments reported \nin Table 1. However, we only have 6 Transformer Decoder layers, keeping the rest of the parameters the \nsame (feed-forward dimension four times that of the model dimension, eight attention heads) to see the effect \nof depth. We see that the model continues to hold and again achieves the performance of the model trained \nfor about 25 epochs almost twice as fast. Both of these experiments are shown in Figure 4."}, {"title": "Making multi-scale kernels learnable", "content": "As described in the previous section, we can see that by adding no parameters onto a Transformer decoder \nlayers by imposing a multi-scale structure, we can make pre-training significantly faster on shrunk down \nGPT like architecture. In this experiment, we allow each of the kernels to be learnable. In the previous \nsection, we defined the shape of the kernel as a Haar wavelet. We looked at the approximate coefficients of \nintermediate layer activations across all layers, with different resolutions occurring at different embedding \ndimensions. Now, in this experiment, we allow each kernel to be learnable. So now, instead of a Haar wavelet \noperation, we allow each kernel to be learnable for getting the approximate signal for various resolutions. \nBefore, we were taking average to compute the approximate signal at a particular embedding dimension, \nwhich is convolutional with a kernel of length L equal to (1/L,1/L,1/L,1/L...). In this experiment, we \nmake the L length kernel learnable from scratch, another way to compute the approximate signal. This \nsimple operation for our base models only allows 0.02M (20k) extra parameters to the Transformer decoder \narchitecture. Unlike the previous setup, which did not add any extra parameter, this further improves our \nperformance from 40% to 46% faster speedup to get a similar baseline performance, as seen in Figure 4. This \nwas carried out on the text-8 dataset. All results are reported on cross-entropy loss computed to the base e. \nThis further validates our method and showcases further improvements and strength of our work."}, {"title": "Long Range Arena Benchmarks", "content": "We adapt our architecture to benchmark long-range arena (LRA) tasks Tay et al. (2021). It consists of \nvarious datasets that allow models to handle long-range prediction over sequence tasks over diverse domains, \npushing the ability of Transformer architecture and other variants. We use three modalities: text, images, \nand mathematical expressions to test the model's ability to understand similarity, structure, and reasoning \nover extended contexts. We only use transformer-based architecture as reported recently by Liu et al. (2024). \nThe other variants are state space architectures and hybrid models. For text, we carry out text classification \non IMDb review dataset (Maas et al., 2011) on byte-level data with a context length of 2048 as input. The \ngoal here is binary classification, which determines whether a movie has a positive or a negative review.\nFor images, we use classification on CIFAR-10 as part of the image modality of LRA benchmarks. It is \na pixel-level classification of the image that takes in as an input a sequence of pixels with values ranging \nfrom 0-255 with a length of 3072 and the output being one of the ten categories as the output. Finally, we \nbenchmark on Long ListOps. It tests the capability of the architecture to understand hierarchically structured \ndata in an extended context setup. As described in the LRA paper by Tay et al. (2021), \"The dataset is \ncomprised of sequences with a hierarchical structure and operators MAX, MEAN, MEDIAN and SUM_MOD that \nare enclosed by delimiters (brackets). An example (much shorter) sequence is as follows:\nINPUT: [MAX 4 3 [MIN 2 3] 10 (MEDIAN 1 5 8 9, 2]] OUTPUT: 5\nIn our task, we use a version of ListOps of sequence lengths of up to 2K to test the ability to reason \nhierarchically while handling long contexts. In the above example, the model needs to access all tokens and \nmodel the logical structure of the inputs to make a prediction. The task is a ten-way classification task and \nis considerably challenging.\" We use the setup provided by Khalitov et al. (2022) to extract the data and be"}, {"title": "Conclusion and Future Work", "content": "We showcase the powerful incorporation of a core signal processing idea, namely wavelets, into large language \nmodel pre-training. By imposing a multi-scale structure onto every intermediate embedding, we see that \nwe can achieve the same performance 40-60% faster, compared to the same baseline architecture, with"}]}