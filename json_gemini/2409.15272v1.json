{"title": "OMNIBENCH: TOWARDS THE FUTURE OF UNIVERSAL OMNI-LANGUAGE MODELS", "authors": ["Yizhi Li", "Ge Zhang", "Yinghao Ma", "Ruibin Yuan", "Kang Zhu", "Hangyu Guo", "Yiming Liang", "Jiaheng Liu", "Jian Yang", "Siwei Wu", "Xingwei Qu", "Jinjie Shi", "Xinyue Zhang", "Zhenzhu Yang", "Xiangzhou Wang", "Zhaoxiang Zhang", "Zachary Liu", "Emmanouil Benetos", "Wenhao Huang", "Chenghua Lin"], "abstract": "Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs). OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) most baselines models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images or/and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities. Codes and live leaderboard could be found at https://m-a-p.ai/OmniBench.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of artificial intelligence has ushered in a new era of multimodal large language models (MLLMs), capable of processing and interpreting diverse data types mainly including images, audio, and text (Li et al., 2024b). These models aim to emulate human-like understanding of the world by integrating information across multiple sensory modalities and learning a comprehensive context from the environment. While significant strides have been made in developing MLLMs that can handle two of the modalities, the ability to concurrently process and reason about the three aforementioned modalities remains a frontier yet to be fully explored.\n\nThe challenge in advancing MLLMs lies not only in their development but also in our capacity to evaluate their performance comprehensively. Current benchmarks often solely focus on image or audios, or limited image-text (Yue et al., 2024; Zhang et al., 2024) or audio-text combinations (Yang et al., 2024) for the dual-modality vision-language models (VLMs) (Lauren\u00e7on et al., 2024) or audio-language models (ALMs) (Chu et al., 2023a; Deng et al., 2023). This gap in evaluation tools has hindered the community to assess and improve the holistic capabilities of models right before the dawn of general-purpose MLLMs.\n\nTo address this critical need, we introduce OmniBench, a pioneering universal multimodal benchmark designed to rigorously evaluate MLLMs' capability to recognize, interpret, and reason across"}, {"title": "RELATED WORK", "content": "Multimodal Large Language Models. Many studies have been introduced to enable model to perceive complex audio signals and generate text responses based on human instructions. Notable examples in audio perception include SALMONN Tang et al. (2023), BLSPN Wang et al. (2023a), Speech-LLaMAN Wu et al. (2023), and Qwen-Audio Chu et al. (2023b), all of which demonstrate promising capabilities in audio-focused dialogues. Regarding the visual modality, the large visual language model also makes excellent achievements. BLIP2 Li et al. (2023) uses a Q-Former to align the visual knowledge with the textual information during the pre-training phase. LLaVA Liu et al. (2024b) is pre-trained on the GPT-4 generated multimodal language-image instruction-following"}, {"title": "OMNIBENCH", "content": "The OmniBench aims to create the first comprehensive benchmark for evaluating multimodal large language models that support simultaneous image, audio, and text inputs. While OmniBench is designed to evaluate the understanding capability of MLLMs on cross-modality complementary information, the models are required to interpret the multimodal input and provide accurate text answer. The problem could be formulated as following: given a tuple of (image, audio, text), the model is required to recognize the objects, re-build the contexts, and conduct reasoning based on the given information. The design logic and statistics of the dataset and the annotation protocols are introduced in this section."}, {"title": "BENCHMARK DESIGN", "content": "Building upon existing multimodal benchmarks, we propose a novel task type categorization in OmniBench that assesses a broad spectrum of reasoning and cognitive abilities. Our taxonomy progresses from fundamental perception (Object Identification & Description) to complex inference (Contextual & Environmental, Identity & Relationship). It incorporates temporal and logical order understanding of events (Action & Activity, Story Description, Plot Inference), spatial awareness (Contextual & Environmental), entity recognition (Object Identification & Description), symbolic processing (Text & Symbols), and quantitative reasoning (Count & Quantity). This comprehensive design evaluates both low-level perceptual skills and high-level cognitive functions, enabling a holistic assessment of multimodal language models' (MLLMs) capabilities to recognize, describe, integrate information, understand context, and make nuanced inferences. OmniBench comprises 1142 question-answer pairs, with task type distribution, text length, and image and audio characteristics detailed in Table 1. The dataset's audio content falls into three categories: speech (human vocal communication), sound events (non-speech natural, environmental and mechanical sounds), and music (various compositions and performances). Figure 2 illustrates the distribution of these audio types across task categories. This diverse task set aims to reveal the strengths and weaknesses of existing models in a tri-modal setting, potentially guiding the development of future omni-language models."}, {"title": "\u0391\u039d\u039dOTATION PROTOCOL", "content": "Annotation Scheme. Our annotation scheme is built upon a fundamental principle: the correct answer to each question must require information from both the image and audio components. This ensures that the benchmark effectively evaluates the model's ability to analyze information across modalities. As shown in Figure 3, we implemented a rigorous annotation pipeline consisting of"}, {"title": "EXPERIMENT SETTINGS", "content": "Baseline Models. We select three groups of MLLM baselines according to the modalities available: i. omni-language models: AnyGPT (Zhan et al., 2024), Video-SALMONN (Sun et al., 2024), UnifiedIO2 series (Lu et al., 2024b); ii. vision-language models: InternVL-2 series (Chen et al., 2024b), Qwen2-VL series team (2024), Deepseek-VL (Lu et al., 2024a), LLaVA-One-Vision series (Li et al., 2024a), Cambrian series (Tong et al., 2024), Xcomposer2-4KHD (Dong et al.,"}, {"title": "FINDINGS", "content": "Table 2 demonstrates that open-source omni-language model (OLM) baselines barely surpass random guessing accuracy across various settings. The lack of significant performance improvement in models with larger text backbones suggests that current open OLMs struggle to effectively interpret multimodal contexts and comprehend relationships between elements.\n\nDespite overall poor performance, open-source baselines generally exhibit higher accuracy on speech audio, indicating a potential bias towards speech data. In contrast, Gemini-1.5-Pro and Reka-core-20240501, the two available proprietary models evaluated in this tri-modal setting, shows more promising results. Regarding the scores across audio types, the Gemini-1.5-Pro shows a more balanced performances while Reka-core-20240501 showing a lag on modeling the sound events. Moreover, the comparison of Gemini-1.5-Pro's performance across full input context and ablated settings"}, {"title": "TEXTUAL APPROXIMATION ON IMAGES AND AUDIOS", "content": "As the absence of effective OLMs on the OmniBench, we further introduce the text alternatives of the images (I') and audios (A') to embrace more dual-modal image-text or audio-text language models to analyze the research progress of the topic. Using the textual approximations of the multimodal contexts, we evaluate the baselines and put the results using audio transcripts, image captions, and both of them in Table 3, Table 4 and Table 5 correspondingly."}, {"title": "Performance Changes of Open OLMs", "content": "We select the UnifiedIO-2 series OLMs to conduct the textual approximation experiments due to their relatively robust performances in the vanilla OmniBench evaluation setting suggested in Table 2. Compared with the vanilla setting, UnifiedIO-2 models shows different levels of performance gains in this settings, i.e. averagely 8.38%, 5.58% and 9.4% when replacing the audio, image, and both of them with corresponding texts. This indicates the shortcoming of existing OLMs on modeling the image and audio (especially the latter), and a defect of interpreting these multimodal information correctly in the instruction context."}, {"title": "Performances of Dual-modal MLLMs", "content": "In the setting of using text as the alternatives of audios and images, the VLMs show ALMs a generally better results (Table 3 vs Table 4). This could be caused by: 1) more available research resources including datasets and methods are allocated to image-text models; 2) the general audio are naturally harder to annotate. If the textual alternatives of images and audios could regarded as in equal quality, it seems to be easier for the future researcher to train an omni-language model from exisiting VLMs rather than ALMs."}, {"title": "Pure Textual Evaluation", "content": "The results in Table 5 reveal interesting patterns in model performance when both image and audio inputs are replaced with textual descriptions. Notably, the majority of models demonstrate improved accuracy when processing textual representations of multimodal data compared to their performance on either image captions or audio transcripts alone. This suggests that these models are better equipped to integrate information from multiple textual sources rather than handling raw multimodal inputs. For instance, Qwen2-Audio-7B-Instruct shows a significant jump in accuracy from 39.05% (audio transcript only) and 39.67% (image caption only) to 47.02% when given both textual representations. Similarly, proprietary models like GPT4-0 and Claude-3.5-Sonnet exhibit substantial gains, with GPT4-0 (0513) achieving an impressive 60.60% accuracy in the pure textual setting."}, {"title": "CONCLUSION AND FUTURE STUDY", "content": "The proposed novel multimodal benchmark, OmniBench, reveals that current open-source multimodal large language models struggle with simultaneous processing of visual, acoustic, and textual inputs. We observed a general bias towards speech audio and superior performance of vision-language models over audio-language models when using textual approximations. These findings underscore the need for more appropriate architecture designs for multimodal integration, diverse datasets for training, and techniques to reduce modality bias. OmniBench serves as a crucial tool for guiding advancements in multimodal language models, driving progress towards more advanced and versatile models towards human-like multimodal understanding and reasoning."}]}