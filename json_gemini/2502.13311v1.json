{"title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors", "authors": ["Jian Wang", "Yinpei Dai", "Yichi Zhang", "Ziqiao Ma", "Wenjie Li", "Joyce Chai"], "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.", "sections": [{"title": "1 Introduction", "content": "Tutoring has long been recognized as one of the most effective methods for enhancing human learning outcomes and addressing educational disparities (Hill et al., 2005). By providing personalized guidance to students, intelligent tutoring systems (ITS) have proven to be nearly as effective as human tutors in fostering deep understanding and skill acquisition, with research showing comparable learning gains (VanLehn, 2011; Rus et al., 2013). More recently, the advancement of large language models (LLMs) has offered unprecedented opportunities to replicate these benefits in tutoring agents (Dan et al., 2023; Jin et al., 2024; Chen et al., 2024), unlocking the enormous potential to solve knowledge-intensive tasks such as answering complex questions or clarifying concepts.\nPrevious research has extensively explored tutoring in educational fields, including language learning (Swartz and Yazdani, 2012; Stasaski et al., 2020), math reasoning (Demszky and Hill, 2023; Macina et al., 2023), and scientific concept education (Yuan et al., 2024; Yang et al., 2024b). Most aim to enhance students' understanding of target knowledge by employing pedagogical strategies such as recommending exercises (Deng et al.,"}, {"title": "2 Problem Definition", "content": "We formulate coding tutoring as an interactive dialogue process between a tutor and a student, where the goal is to help the student implement a working solution that passes predefined unit tests for a target coding task.\nFormally, the tutor is assigned a coding task T that consists of a function signature and a requirement description outlining the desired functionality. The tasks are repository-level, which require an understanding of multiple interdependent files within the codebase to implement a correct solution. The tutor has access to task-specific knowledge K, which includes (i) Code Contexts: Contextual code snippets surrounding the desired code, which help the tutor show examples when necessary; (ii) Reference Dependencies: Cross-referenced elements such as intra-class, intra-file, and cross-file dependencies, along with their corresponding descriptions (e.g., docstrings), which involve key knowledge for completing the desired code; and (iii) Reference Solution Steps: Key steps required to complete the target task, describing using natural languages.\nThe student is given the task T and possesses some subset of K as their prior knowledge, but the tutor remains unaware of which specific concepts"}, {"title": "3 Trace-and-Verify Agent Workflow", "content": "We propose Trace-and-Verify (TRAVER), an effective workflow for developing tutor agents (see the middle part in Figure 3). TRAVER integrates two key components: (i) explicit tracing of a student's knowledge state and (ii) utterance decoding guided by a verifier model for turn-by-turn verification."}, {"title": "3.1 Adapting to Student's Knowledge via Knowledge Tracing", "content": "Effective tutoring requires bridging the gap between a student's prior knowledge and the skills needed to solve the target coding task. To address this, we employ knowledge tracing (KT) (Corbett and Anderson, 1994; Abdelrahman et al., 2023; Scarlatos and Lan, 2024) to estimate the student's knowledge state at each dialogue turn. Specifically, we represent task-specific knowledge K as a set of knowledge components (KCs) {KC1, KC2, . . ., KCK}, where each KC is either a reference dependency or a solution step. At the t-th turn, the tutor agent explicitly assesses the student's belief of each KC using texts, based on the dialogue context Ct and the estimated belief Bt-1 at the previous turn. The current belief Bt indicates how many KCs the student has understood. With this estimation, the tutor is further prompted to focus more on missing KCs and generate utterances that address the student's knowledge gaps. The detailed prompt template is provided in Appendix D."}, {"title": "3.2 Utterance Decoding via Turn-by-Turn Verification", "content": "Based on the KT outcomes, the tutor agent aims to generate high-quality utterances that advance the tutoring process. However, LLMs often struggle to determine which utterances effectively guide students toward task completion. Drawing inspiration from value-guided search approaches (Lightman et al., 2024; Wang et al., 2024a; Zhang et al., 2024), we address this with a turn-by-turn verifier. The verifier Ve evaluates the quality of potential tutor responses by producing a reward score vt \u2208 [0, 1] based on three inputs: the target task T, current dialogue context Ct, and a candidate tutor utterance rt at turn t. To select the optimal response, we generate N candidate utterances through parallel sampling and choose the one that receives the highest reward score from the verifier.\nThe core of the verifier Ve is the turn-based reward vt at t-th turn, which should reflect (i) the cumulative progress made in the previous turns and (ii) the current turn's contribution to achieving the overall tutoring goal. Hence, vt can be iteratively defined as:\n$v_t = max(v_{t-1}+w_{r_t}, 0)  (v_0 = 0, t \\in [1,T])$ (1)\nwhere $w_{r_t}$ is the weighted reward quantifying the contribution of the current turn to the overall goal, T denotes the total number of turns. To compute $w_{r_t}$, we introduce the guiding distance $d_t = T - t$, which measures the remaining turns until the goal. The weighted reward is then calculated as:\n$w_{r_t} = \\frac{1}{d_t + 1} (\\frac{v_{t-1}}{v_{T-1}} o_{s_t} - 1)$ (2)\nwhere $o_{s_t} \\in$ 0,1 is a binary outcome indicating whether the tutor's t-th utterance contributes to the student's eventual successful completion of the target task. As the dialogue progresses, the guiding distance $d_t$ decreases, leading to larger weighted rewards for later turns. This design ensures the turn-based reward $v_t$ remains bounded while appropriately weighting the importance of each turn based on its proximity to the goal.\nWe train the verifier Ve using mean squared error (MSE) loss over n samples:\n$L = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{t=1}^{T_i} [(V_\\theta(T^{(i)}, C^{(i)}_t, r^{(i)}_t)) - U^{(i)}_t]^2 $ (3)\nwhere Ti denotes the number of turns for the i-th sample, $\u0398$ denotes the trainable parameters.\nDuring inference, the verifier serves as a plug-and-play module, which chooses the utterance with the highest reward from candidate utterances generated by parallel sampling at each turn, promoting progression toward tutoring task completion."}, {"title": "4 The DICT Evaluation Protocol", "content": "One key challenge in developing tutoring agents is the lack of robust evaluation methods. While human evaluation is essential, its high cost, time requirements, and complexity make it impractical for scalable benchmarking. To address this limitation, we present Dialogue for Coding Tutoring (DICT), an automatic protocol for evaluating tutor agents. The overview of DICT is shown in Figure 3. Our protocol employs LLMs to simulate students with varying levels of programming knowledge. First, tutors engage in multi-turn dialogues with students to tutor the task. Then, students demonstrate their learning outcome by implementing the solution code. We evaluate tutoring effectiveness through automated unit tests of the student-generated code. This automated approach enables controlled, reproducible, and scalable evaluations of tutoring agents."}, {"title": "4.1 Controlled Student Simulation", "content": "To evaluate how well tutors can adapt their strategies to students with varied prior knowledge, we simulate students of three knowledge levels: (1) Low-level: Students access no prior knowledge from K. They represent beginners with no familiarity with the target task. (2) Medium-level: These students are assigned a proportion (e.g., 50%) of the reference dependencies by random sampling, denoting that they have partial knowledge required for completing the task. (3) High-level: In addition to partial reference dependencies, these students are also provided with the code contexts, indicating that they have more comprehensive knowledge with contextual guidance.\nPre-Test. We create students of all three levels using the same LLM simulator, varying only in their knowledge level. However, this raises a critical question: do students with different knowledge levels actually demonstrate distinct performance in completing the coding tasks? To validate this, we conduct a preliminary coding test where each simulated student attempts to generate code for the target task T before any tutoring intervention (see Appendix D for prompting details).\nMetrics for Coding Test. Following previous studies (Austin et al., 2021; Li et al., 2024), we employ Recall@k and Pass@k as evaluation metrics to assess coding test performance. Recall@k measures the recall of reference dependencies in the generated programs. Specifically, students are asked to generate k programs per target task. For the i-th program, we extract its dependencies Pi using the Pyan parser (Pyan, 2023). We compare them with the reference dependencies R and compute the Recall@k as:\n$Recall@k = \\frac{\\mathbb{E}_{Targets Tasks} \\underset{i\\in[1, k]}{max} |R\\bigcap P_i|}{|R|}$ (4)\nwhere || denotes the number of elements of a set. Pass@k evaluates the functional correctness of the generated programs. After generating n \u2265 k programs per task, we execute them in Python interpreters to count the number of correct programs c that pass all test cases. Pass@k is computed as:\n$Pass@k = \\mathbb{E}_{Target Tasks} (1 - \\frac{{\\binom{n-c}{k}}}{\\binom{n}{k}})$ (5)\nwhere ${\\binom{n}{k}}$ denotes the number of ways to choose a subset of elements (also known as the binomial coefficient). Our pre-test results reported in \u00a75.3 show that the simulated students are effective."}, {"title": "4.2 Tutor-Student Interaction", "content": "As shown in Figure 3, we let an LLM-based tutor agent engage in a multi-turn dialogue with a chosen student, simulating a tutoring session. The tutor is initialized with the target coding task T and task-specific knowledge K. We ask the tutor to initiate the tutoring and communicate with the student turn by turn. A key challenge is determining when to terminate the tutoring. While the tutor could self-determine, our preliminary experiments revealed a tendency for overconfidence, leading to premature termination. This issue arises because most tutors overlook gaps in the student's prior knowledge. For a robust comparison, we follow Wang et al. (2023) and introduce an LLM-powered dialogue manager (see Figure 3). Operating from a \"God's perspective,\" the manager considers the dialogue context and all information from both the tutor and student, to decide whether the tutoring goal has been met. The tutoring terminates under one of two conditions: (i) The manager confirms that the tutoring goal is achieved; (ii) The dialogue reaches a predefined maximum of T turns."}, {"title": "4.3 Automatic Evaluation", "content": "Post-Test. To evaluate the effectiveness of tutor agents, we conducted a coding test after tutoring (referred to as the post-test). Given a target task T and a dialogue session $C = \\{s_t, r_t\\}_{t=1}^{T}$ with a simulated student, we ask the student to generate code for fulfilling the task. However, assuming that all dialogue content is retained during the coding test may be unrealistic. According to cognitive load theory (CLT) (Miller, 1956; Sweller, 2011), human working memory has a limited learning capacity at one time, and exceeding this capacity can hinder learning. We consider the student's cognitive load fCL at each turn by restricting the information retained from the tutor's utterance to a maximum threshold. Specifically, if a tutor utterance rt exceeds M words, only the latest M words are retained; otherwise, the full utterance is kept. Our post-test is formulated as:\n$Y_{code} = LM_{Student}([I; T; \\{s_t, f_{CL}(r_t)\\}_{t=1}^T])$ (6)\nwhere I represents the instruction for code generation. St and rt denote student and tutor utterances at t-th turn, respectively. A detailed template for the instruction can be found in Appendix D.\nEvaluation Metrics. Based on the coding test, students' post-test performance after tutoring is defined as the tutoring outcome (TO), measured by Recall and Pass. They represent the averages of Recall@k and Pass@k for k \u2208 {1,3,5,10}. A higher Recall score indicates the tutor is more capable of leading the student to acquire the dependency knowledge for coding; a higher Pass score denotes a higher success rate of guiding the student in completing the target coding task.\nDue to the difference in prior knowledge levels, we use the tutoring outcome rate (\u0394%) to normalize and fairly evaluate the tutor's performance. This is calculated by relative improvement before and after tutoring:\n$\\Delta\\%M = (\\frac{M_{post-test}-M_{pre-test}}{M_{pre-test}}) \\times 100\\%$ (7)\nwhere M denotes the metric for the coding test, which can be either Recall or Pass.\nTo further analyze the tutoring process, we propose the tutoring outcome curve (TOC). At each t-th turn, we ask simulated students to perform a post-test using the dialogue context up to that turn, i.e., {s<t, r<t}. The TOC is then plotted by tracking the Recall and Pass scores varying by turns. These curves exhibit how tutor agents guide students throughout the tutoring session."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nBenchmark. We adopt EvoCodeBench (Li et al., 2024) as the testbed due to its realistic repository-level Python coding tasks along with dependency annotations and repository contexts, providing a rich knowledge foundation for coding tutoring. We have 100 target coding tasks and split them equally into 5 folds, using 5-fold cross-validation for experiments. The detailed examples, statistics, and preprocessing are provided in Appendix A.1.\nStudent Simulator. Prior to tutoring, it is essential to ensure simulated students have not been exposed to any target coding task. To avoid data contamination, we use Mixtral-8x7B-Instruct (Jiang et al., 2024) as the student simulator. This model's training data only includes content up to 2023-9, whereas all coding tasks in EvoCodeBench are collected from the repositories created between 2023-10 and 2024-2. Furthermore, this model has strong conversational and coding abilities, making it well-suited for student simulation.\nBackbone Models. The backbone models for developing tutor agents are: Qwen2-Instruct (Yang et al., 2024a) with 7B and 72B variants, Llama-3.1-Instruct (Dubey et al., 2024) with 8B and 70B"}, {"title": "5.2 Experimental Results", "content": "How do various LLMs perform as tutor agents when provided with vanilla instructions? As shown in Table 1, various LLM-based tutors with vanilla instructions perform significantly inferior to the Oracle tutor, indicating clear limitations. Scaling the parameter size of open-source models like Qwen2 and Llama-3.1 generally improves performance. However, the large gaps in Pass and TOR-Pass scores suggest that simply using larger models is inadequate to guide students in successfully completing target coding tasks. These findings indicate that developing effective tutor agents requires not only detailed instructions but also mechanisms to facilitate tutoring outcomes in a structured way.\nAnother limitation that emerges from these results is the adaptability. As shown in Table 1, LLMs generally perform better with low-level than"}, {"title": "5.3 Analysis of Simulated Students", "content": "Since the DICT evaluation protocol relies on the LLM-simulated students, it is crucial to ensure that these students, with varying levels of prior knowledge, align with discrepancies in completing coding tasks. To validate this, we conducted a pre-test for the simulated students before tutoring, as described in \u00a74.1. Figure 6 shows that simulated low-, medium-, and high-level students exhibit significant performance differences in terms of Recall@k and Pass@k. Under a controlled setup, students at different levels demonstrate distinct abilities in completing target coding tasks. Therefore, our student simulation serves as a feasible proxy for real students, offering its advantages of scalability and cost-effectiveness for evaluating tutor agents."}, {"title": "5.4 Inference-Time Scaling with Verifiers", "content": "Using Llama-3.1-70B-Instruct as the backbone model, we vary the number of candidate tutor utterances per turn, i.e., N, within {1, 5, 10, 15, 20} and ask the verifier to select the best one based on predicted rewards. We also evaluate two baselines: (i) Chain-of-Thought (CoT) (Wei et al., 2022) prompting for selection (B1) and (ii) random selection (B2). As shown in Figure 2, TRAVER with"}, {"title": "5.5 Human Evaluation and Case Study", "content": "To evaluate the quality of tutor agents developed by different methods, we conducted a human evaluation for Vanilla Instruct, Self-Refine, TreeInstruct, and our TRAVER. We presented human evaluators with a pair of tutoring dialogues produced by two agents interacting with the same student. Evaluators were asked to determine which one is better from proactivity, adaptability, and coherence. Further details are provided in Appendix B.\nThe results demonstrate that our TRAVER significantly outperforms the compared methods in proactivity and adaptability, while also matching or surpassing Vanilla Instruct in coherence. We provide several examples in Appendix C. To further illustrate the performance of our tutor agents, we provide several examples in Appendix C."}, {"title": "6 Related Work", "content": "Interactive Tutoring. As an advanced form of intelligent tutoring systems (ITSs), interactive ITSs (Graesser et al., 2001; Rus et al., 2013) can provide personalized feedback and adaptive learning experiences. They have been extensively explored across various educational domains, such as language learning (Swartz and Yazdani, 2012; Stasaski et al., 2020; Caines et al., 2020; Kwon et al., 2024), math reasoning (Demszky and Hill, 2023; Macina et al., 2023; Wang et al., 2024b), and scientific concept education (Yuan et al., 2024; Yang et al., 2024b). These studies mainly focus on enhancing students' understanding of specific pieces of knowledge, using pedagogical strategies such as designing exercises (Deng et al., 2023; Wang et al., 2022; Lu et al., 2023) or selecting teaching examples (Ross and Andreas, 2024). Furthermore, the effectiveness of these approaches is often measured using closed-form assessments, such as question-answering (Yuan et al., 2024) or multiple-choice tests (Macina et al., 2023). Rather than specific pieces of knowledge, we investigate the domain of coding tutoring, which requires students to perform open-ended code generation to assess tutoring further.\nLLM-based Tutoring Agents. The rapid growth of large language models (LLMs) has expanded ITSs into tutoring agents (Yu et al., 2024). For instance, early efforts such as EduChat (Dan et al., 2023) introduced an educational chatbot for online tutoring, while CHATTUTOR (Chen et al., 2024) equipped tutor agents with course planning and adaptive quizzes to facilitate long-term interactions. More recently, AlgoBo (Jin et al., 2024), an LLM-based teachable agent, was developed to enhance students' coding skills. Existing LLM agents primarily play a reactive role, focusing on answering questions or clarifying concepts. In comparison, our coding tutoring is both goal-driven and personalized, requiring agents to proactively guide students toward completing targeted coding tasks while adapting to diverse levels of knowledge prior. Our work presents a novel method that empowers tutor agents to address these challenges.\nInference-Time Adaptation of LLMs To enhance the controllability of language generation in complex tasks, prior work has investigated guided decoding (Dathathri et al., 2020; Chaffin et al., 2022) during inference. More recently, a notable line of research (Lightman et al., 2024; Li et al., 2023; Wang et al., 2024a; Pan et al., 2024) has employed verifier models complemented with search algorithms to guide LLMs for agentic reasoning. These methods typically focus on static tasks, of-"}, {"title": "7 Conclusion", "content": "This work explores the potential of LLMs as coding tutors. We propose TRAVER, an effective agent workflow that incorporates knowledge tracing and turn-by-turn verification, to tackle key challenges in coding tutoring. While this work focuses on coding tutoring as an example, the proposed method extends beyond coding to other task-tutoring scenarios. We further introduce DICT, a novel evaluation protocol combining student simulation and coding tests to assess tutor performance. Such automated evaluation is critical for developing task-tutoring agents as it supports a systematic development and evaluation cycle. Although it's outside the scope of this paper, the best-performing agent from the automated evaluation can be further assessed through studies with real human students in the future."}, {"title": "Limitations", "content": "In this work, we employed LLMs to simulate students at different knowledge levels, serving as a proxy for real-world learners. While these simulated students offer convenience and scalability, their representation of the human learning process is inherently limited. The role-playing behavior may differ from that of actual students in tutoring scenarios. Future research should focus on improving the reliability of student simulation to better align with real-world human learning.\nIn addition, the tutor agents were primarily evaluated by interacting with simulated students in our experimental setup. It remains unclear how these agents would perform when guiding humans toward completing target coding tasks. An important direction for future work is to extend our evaluation protocol by incorporating human-in-the-loop assessments, where tutor agents interact directly with actual students with necessary programming backgrounds. This would offer deeper insights into the practical effectiveness of the developed agents in real-world settings."}, {"title": "Ethics Statement", "content": "We strictly follow the protocols governing the academic use of all LLMs. Our experimental datasets are publicly available and contain no sensitive or private information. We acknowledge that utterances generated by these LLMs may exhibit hallucinations or biases. By highlighting these issues, we aim to raise awareness among practitioners if the tutor agents are deployed to interact with real-world students in the future. Additionally, we used AI assistants, such as Github Copilot and ChatGPT, to assist with our experimentation."}]}