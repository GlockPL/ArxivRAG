{"title": "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty", "authors": ["Jiahui Cen", "Jianghao Lin", "Dong Zhou", "Weixuan Zhong", "Jin Chen", "Aimin Yang", "Yongmei Zhou"], "abstract": "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism", "sections": [{"title": "1 Introduction", "content": "Knowledge Tracing (KT) has become an indispensable component in intelligent tutoring systems, utilizing students' historical response data to assess their knowledge mastery and automatically predict their future performance and knowledge state[1]. In the educational domain, knowledge tracing can optimize students' learning paths and provide guidance for teachers, enabling targeted improvements in teaching strategies and personalized instruction. Through dynamic tracking of students' knowledge state, this technology effectively reduces teachers' workload while enhancing teaching quality. Fig 1 demonstrates a simple overview of Knowledge Tracing. By analyzing two students' performance across five mathematical problems, we observe distinct learning patterns. Student 1 shows an alternating pattern of correct and incorrect responses (correct-incorrect-correct-incorrect) in the first four questions, achieving a 50% accuracy rate. In comparison, student 2 demonstrates a more stable learning trajectory with a 75% accuracy rate in the first four questions, showing consistent improvement after an initial incorrect response in multiplication. Both LLM-based and Statistical-based difficulty assessments indicate that the final subtraction problem (difficulty: 59/58) is slightly less challenging than the previous subtraction problem (65/60). Considering the learning patterns, performance trajectories, and difficulty assessments, we predict that student 1 is likely to respond incorrectly to the final question, while student 2 is likely to provide a correct response.\nWith the rapid development of online education, vast amounts of learning behavior data have been recorded, encompassing not only the correctness of students' answers but also multi-dimensional information such as response time and learning resource utilization patterns. This abundance of data has established a solid foundation for advancing knowledge tracing technology. However, effectively utilizing this data to accurately assess students' knowledge state while balancing objective difficulty with students' subjective learning conditions remains a challenging issue. In recent years, the field of knowledge tracing has witnessed the emergence of numerous advanced deep learning models, broadly categorized into Deep Knowledge Tracing models (e.g., DKT[2], which revolutionized the field in 2015), Attention-based models (e.g., SAKT[3], AKT[4], which capture long-term dependencies), Memory-based"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Deep Learning-Based Knowledge Tracing", "content": "Knowledge tracing is essentially a time series classification task that predicts student performance based on their historical learning records. Early knowledge tracing research primarily focused on combining Bayesian Knowledge Tracing (BKT) [7] or using Item Response Theory (IRT)[8]. With the flourishing development of deep learning, numerous innovative models have emerged in this field. Piech et al. proposed Deep Knowledge Tracing (DKT), as the first model to introduce deep learning into knowledge tracing, employing a Recurrent Neural Network (RNN) architecture to capture temporal relationships in student interaction data[2]. This pioneering work not only significantly improved prediction performance but also laid a crucial foundation for subsequent research. Compared to traditional methods, DKT can automatically extract features from large-scale learning behavior data and effectively handle complex relationships among multiple knowledge concepts. Chen et al. proposed QIKT based on the DKT model, introducing question-centric knowledge acquisition, knowledge state, and problem-solving modules, breaking the traditional homogeneous question assumption[9]. The model innovatively combines IRT-based interpretable prediction layers, providing better interpretability while maintaining high prediction performance.\nFollowing DKT, memory-based models gradually emerged, with Dynamic Key-Value Memory Networks (DKVMN) proposed by Zhang et al. being the most representative. This model enhances the representation of student historical records through a key-value pair mechanism, more accurately capturing students' latent knowledge state[10]. DKVMN utilizes explicit memory representations to precisely track students' mastery of different knowledge concepts, and its unique memory mechanism optimizes the modeling of long-term dependencies.\nHowever, when facing sparse learning data scenarios (such as students interacting with only a few knowledge concepts), traditional models struggle to effectively capture implicit relationships within the data. To address this issue, attention-based models were developed. Liu et al. proposed AT-DKT based on the DKT model, introducing Question Tag (QT) prediction and Individual prior Knowledge (IK) prediction as auxiliary learning tasks[11]. The model uses Transformer encoders and masked attention mechanisms for QT tasks and student ability networks for IK tasks. Pandey et al. proposed Self-Attentive Knowledge Tracing (SAKT), which through its multi-head attention mechanism and feed-forward network, can identify knowledge concepts (KCs) related to the target KC from historical activities and predict student mastery based on limited historical data[3]. Compared to RNN-based approaches, SAKT demonstrates significant advantages in handling data sparsity issues. Huang et al. proposed sparseKT, a simple yet effective framework aimed at improving the robustness of attention-based knowledge tracing models[12]. The model's core innovation lies in introducing k-sparse attention mechanisms, making predictions by explicitly selecting the most relevant historical interactions.\nTaken together, the development of knowledge tracing models has evolved from traditional probabilistic models to deep learning approaches. However, how to better balance model performance, interpretability, and computational efficiency, as well as how to effectively integrate students' personalized characteristics, remain important directions for further exploration in this field. In traditional knowledge tracing models, predictions are mostly based on students' learning history records, such as problem IDs, knowledge concept IDs, and whether students are correct, without incorporating difficulty features into the training process. Therefore, it is impossible to combine"}, {"title": "2.2 Difficulty in Knowledge Tracing", "content": "Question difficulty, as a crucial evaluation metric in the educational process, can serve as a special predictive feature in knowledge tracing[13]. In recent years, research incorporating additional information such as difficulty has gradually increased, though most approaches employ single difficulty information[14].\nDIMKT is a difficulty-aware knowledge tracing model derived from response correctness rates, which improves knowledge tracing performance by establishing relationships between students' knowledge state and question difficulty levels to measure the difficulty effect [15]. The model's innovation lies in enhancing question representation by simultaneously considering both question-specific difficulty and knowledge concept difficulty, and designing three stages to capture the difficulty effect: first, calculating students' subjective difficulty perception before practice, then estimating students' personalized knowledge acquisition when answering questions of different difficulty levels, and finally updating students' knowledge state according to question difficulty.\nSimilarly focusing on difficulty, Liu et al. proposed the QDCKT model, which combines first-attempt correctness rate-based difficulty with graph attention mechanism, innovatively replacing traditional question IDs with question difficulty levels[16]. The model employs an LSTM sublayer to generate representations of historical learning sequences and uses a feed-forward neural network as the prediction layer. QDCKT introduces two key techniques: first using the Hann function to combine embeddings of nearby difficulty levels, and second introducing difficulty consistency constraints to ensure prediction results align with question difficulty levels.\nZhang et al. proposed GDPKT based on the Graph-based Knowledge Tracing (GKT) model [17]. It enhances knowledge tracing capabilities through heterogeneous graph neural networks and personalized difficulty modeling. The model introduces difficulty nodes into the heterogeneous graph, uses Meta-path to construct node representations, and combines difficulty perception and learning gain modules to model students' knowledge state personalized. The main innovation lies in modeling exercise difficulty as independent nodes in the graph while considering students' personalized perception and learning gains from exercises of different difficulties. Qiu et al. proposed MGEKT is a knowledge tracing model based on multi-graph embedding[18]. It adopts a dual-channel architecture, with one channel using node2vec Walk and Meta-path to enhance question representation, and the other channel using AGCN to process"}, {"title": "2.3 Large Language Model Application in Knowledge Tracing", "content": "In recent years, the field of Natural Language Processing has witnessed significant technological advancement with the emergence of increasingly sophisticated tools. Large Language Models (LLMs), as representatives of new-generation pre-trained models, have demonstrated exceptional performance across various domains. Models like ChatGPT and Claude, utilizing techniques such as Chain-of-Thought (COT), have substantially enhanced their capabilities in understanding and generating deep semantic information in natural language. However, as general-purpose language understanding and generation tools, LLMs show limitations in handling strong sequential problems in knowledge tracing, thus primarily serving as auxiliary tools in knowledge tracing tasks[19]. Yu et al. proposed ECKT, a knowledge tracing model based on large language models and CodeBERT[20]. It generates question descriptions and knowledge concepts through chain-of-thought reasoning and few-shot learning, uses BERT for embedding, and combines AST and attention mechanisms for code representation. ECKT innovates by using LLM to generate question descriptions and knowledge concepts, introducing difficulty embeddings to enhance representation, and employing stacked GRU to improve sequence learning capabilities. Guo et al. proposed EAKT (Enhanced Attribute-aware Knowledge Tracing), a LLM-based cold-start solution for knowledge tracing that leverages LLMs to extract additional information such as required problem-solving capabilities[21]. EAKT introduces three key innovations: first, it designs an attribute estimation module that utilizes models like GPT-4 with grouping strategies and chain-of-thought prompting to analyze questions and estimate"}, {"title": "3 Proposed Method", "content": ""}, {"title": "3.1 Problem Definition", "content": "Knowledge Tracing (KT) is a task that predicts students' future performance based on their historical interaction records in Learning Management Systems (LMS) or Intelligent Tutoring Systems (ITS). By analyzing students' learning trajectories, knowledge tracing models can assess students' knowledge state in real-time, thereby supporting personalized learning. The formal definition is as follows: Let S = {$s_1,s_2, ..., s_n$} be the set of students, Q = {$q_1, q_2,..., q_m$} be the set of questions, C = {$C_1, C_2, ..., C_k$} be the set of concepts, and R = {$0,1$} be the set of responses, where 0 indicates an incorrect answer and 1 indicates a correct answer. For student $s_i$, their interaction sequence can be represented as $x_i = {$X_{i1},X_{i2}, ..., X_{it} }$ , where each interaction $x_{it}$ is a triple ($q_{it}$, $C_{it}$, $r_{it}$) representing the question attempted, the concept involved, and the response result at timestamp t, respectively. The objective of knowledge tracing is to predict the probability $P(r_{i(t+1)} = 1 | X_1, X_{i2}, ..., X_{it})$ that a student will correctly answer a new question at timestamp t + 1, based on their historical interaction sequence {$X_{i1}, X_{i2}, ..., X_{it} }$. This is a typical sequential prediction problem that requires the model to effectively capture the dynamic changes in students' knowledge state."}, {"title": "3.2 Utilizing LLMs to determine the question's difficulty", "content": "To extract the implicit difficulty information from questions, we designed a specialized prompt that enables LLMs to perform similar question retrieval and difficulty"}, {"title": "3.3 Statistical and LLM-based Difficulty Calibration", "content": "In addition to LLM-assessed difficulty, statistical difficulty serves as another crucial indicator in our framework. Statistical difficulty is denoted as $d^{stat}_i$, represents an empirical measurement derived from historical student performance data, specifically using the correct response rate of all students for a particular question as the statistical difficulty indicator. The statistical difficulty for question i can be expressed by the following formula:\n$d^{stat}_i = \\frac{\\sum_{j=1}^{N} r_{ij}}{N}$   (1)\nwhere $d^{stat}_i$ represents the statistical difficulty of question i, $r_{ij}$ represents student j's response to question i (1 for correct, 0 for incorrect), N represents the total number of students who answer question i and $\\frac{\\sum_{j=1}^{N} r_{ij}}{N}$ represents the correct response rate.\nThis formula directly uses the correct response rate as the statistical difficulty indicator. A higher correct rate indicates lower difficulty, while a lower correct rate indicates higher difficulty. The difficulty value ranges from 0 to 1, where 1 indicates the easiest (all students answered correctly) and 0 indicates the most difficult (all students answered incorrectly).\nTo leverage the complementary strengths of both approaches, we integrate statistical difficulty with LLM-assessed difficulty using a multi-head attention mechanism. This integration produces a calibrated difficulty score ($d^{deal}_i$) that synthesizes the data-driven statistical insights with the semantic understanding capabilities of large language models. The multi-head attention mechanism enables our model to dynamically weight different aspects of both difficulty measurements, resulting in a more comprehensive and robust difficulty assessment that captures both empirical patterns and theoretical complexity. Thus, the formula of $d^{deal}_i$ is defined as\n$d^{deal}_i = MultiHeadAttention(d^{stat}_i, d^{llm}_i)$  (2)\nAfter calculating $d^{deal}_i$, it is mapped to question i. During the training process, each time series has $d^{deal}_i$, which represents the calibrated difficulty sequence corresponding to questions within that time series."}, {"title": "3.4 Difficulty Perception Bias Sequence", "content": "During the learning process, students' knowledge state often reflect their subjective perception of difficulty. When a student has a low mastery level of a certain concept, their knowledge state for that concept tends to be low. Based on this observation, we propose the concept of Difficulty Perception Bias Sequence (DPBS). The DPBS in timestamp t is calculated as\n$dpbs_t = d^{deal}_t - ks_{t-1}$   (3)\nwhere $d^{deal}_t$ represents the objective difficulty of the question assessed by the large language model at timestamp t, and $ks_{t-1}$ represents the student's knowledge state at timestamp t - 1. This difference reflects the degree of bias between a student's subjective difficulty perception and the objective difficulty of the question.\nWhen DPBS is close to zero ($dpbs_t \u2248 0$), it suggests an optimal match between question difficulty and student knowledge level, representing the ideal learning state. When DPBS is significantly positive ($dpbs_t > 0$), it indicates that the objective difficulty of the question far exceeds the student's current knowledge level, typically occurring when students with weaker foundations encounter challenging questions. When DPBS is significantly negative ($dpbs_t < 0$), it shows that the objective difficulty is much lower than the student's knowledge level, usually occurring when high-performing students solve basic questions."}, {"title": "3.5 Difficulty Mastery Ratio", "content": "In the process of personalized student modeling, the mastery ratio of students across different difficulty levels reveals distinctive patterns among individuals, which is crucial for personalized modeling. To effectively capture these individual differences, we propose the Difficulty Mastery Ratio (DMR), a novel metric that quantifies students' mastery patterns across varying difficulty levels, thereby enabling more precise personalized modeling of student learning characteristics.\nWe first partition the difficulty range [0,100] into five equal intervals, each representing a distinct difficulty level. Based on the statistical difficulty $d^{stat}_i$, we assign each question i to its corresponding difficulty interval and calculate the correct response rate within each interval. For student interaction data $X_{i(t-1)}$, the correct response rate $cr^t_k$ of each difficulty interval is appended to $x_{i(t-1)}$. Considering the natural occurrence of knowledge decay in the learning process, we introduce a forgetting factor $\\sigma$ to simulate the gradual forgetting process in student learning. The Difficulty Mastery Ratio (DMR) at timestamp t can be expressed by the following formula\n$cr^{t-1}_k = \\frac{\\sum_{i\u2208B^{t-1}_k} r^t_i}{|B^{t-1}_k|}$   (4)\n$dmr = \\sigma \u00b7 cr^{t-1}_k$  (5)\nwhere $B^t_k$ represents the kth difficulty interval (k = 1,2,3,4,5), $r^t_i$ represents the student's response to question i at timestamp t (1 for correct, 0 for incorrect), $|B^{t-1}_k|$ represents the number of questions in interval k at timestamp t and $\\sum_{i\u2208B^{t-1}_k}$ represents the current correct rate in the difficulty interval."}, {"title": "3.6 Dynamic Difficulty Adaptability Index", "content": "To comprehensively evaluate students' learning characteristics, we propose integrating DPBS and DMR to construct the Dynamic Difficulty Adaptability Index (DDAI). DPBS combines calibrated difficulty with students' subjective difficulty assessments, providing a comprehensive difficulty measure that encompasses both objective evaluation and subjective perception. Meanwhile, DMR captures students' mastery levels"}, {"title": "3.7 Dual-channel Difficulty Knowledge Tracing Model", "content": "Traditional knowledge tracing models typically rely solely on statistical difficulty (such as correct rates) when evaluating question difficulty, which presents several significant limitations: First, statistical difficulty is susceptible to factors such as sample size and data distribution imbalance, leading to insufficient objectivity in difficulty assessment; Second, this approach ignores individual students' subjective perception differences regarding question difficulty, failing to accurately reflect different students' learning characteristics; Third, static difficulty assessment cannot capture students' ability improvement and adaptability changes during the learning process. To overcome these limitations, we propose a more comprehensive solution: introducing Large Language Models (LLM) for question difficulty analysis, combining statistical difficulty with student subjective assessments to construct a multi-dimensional difficulty evaluation system. This approach not only enhances the objectivity and reliability of difficulty assessment but also enables personalized learning models for each student, more accurately predicting learning trajectories and performance.\nStudent Knowledge Gain. To simulate student learning behavior and integrate both objective question difficulty and students' subjective difficulty perception, we designed a novel knowledge tracing model DDKT. As mentioned above, after extracting $ddai_t$, we obtain its embedding vector $ddai^{emb}_t$. Subsequently, we perform element-wise multiplication between $ddai^{emb}_t$ at timestamp t and the student's"}, {"title": "3.8 Prediction Layer And Objective Function", "content": "After obtaining the student's knowledge state $ks_t$ at timestamp t, we can predict the student's performance at timestamp t + 1 based on $ks_t$. In our model, the input data consists of the student's knowledge state $ks_t$ at timestamp t multiplied by the knowledge representation embedding targeted at timestamp t + 1, with a sigmoid function outputting the probability of correct response. The knowledge representation embedding $target^{emb}$ includes the question $q_t$, concept $c_t$, statistical assessment difficulty $d^{stat}_t$, and large language model assessment difficulty $d^{llm}_t$ for timestamp t + 1. Specifically, $Y_{t+1}$ can be expressed as\n$d^{dem}_{t+1} = Embedding(d^{stat}_{t+1}) \u2295 Embedding(d^{llm}_{t+1})$\n$target^{emb}_{t+1} = q^{emb}_{t+1} \u2295 c^{emb}_{t+1} \u2295 d^{emb}_{t+1}$\n$Y_{t+1} = sigmoid(ks_t \u2217 target^{emb}_{t+1})$  (12)\nwhere Embedding(x) represents the embedding value of element x, $target^{emb}_{t+1}$ is the question representation, and $yt+1$ is the output prediction probability value. We use Binary Cross Entropy as the loss function, the function can be formula as\n$lloss = - \\frac{1}{N} \\sum^{N}_{n=1}[y_n \u00b7 log(x_n) + (1 \u2212 y_n) \u00b7 log(1 \u2212 x_n)]$  (13)\nwhere $I_n$ represents the groundtruth value, $y_n$ represents the predicted value, and \u039d represents the total number of samples."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "We selected two real-world public datasets to train and evaluate our model: XES3G5M and Eedi. The detailed information of the two datasets is shown below:\n\u2022 XES3G5M is a large-scale dataset that contains numerous questions and auxiliary information about related knowledge components (KCs) [25]. The XES3G5M dataset was collected from a real online mathematics learning platform and includes 7,652 questions, 865 KCs, and 5,549,635 learning interactions from 18,066 students."}, {"title": "4.2 Baselines", "content": "To validate the effectiveness of DDKT, we employed 9 different baselines. All models were trained and evaluated using default parameters on an A40 cluster. The details of all baselines are as follows:\n\u2022 DKT is the first model to introduce deep learning into knowledge tracing[2], implemented based on Recurrent Neural Networks (RNN/LSTM). This model evaluates students' knowledge state by modeling their learning process and predicts their mastery of questions and corresponding knowledge concepts.\n\u2022 SAKT is the first model to directly apply Transformer to knowledge tracing tasks, implemented based on self-attention mechanism [3]. This model proposes a self-attention model to capture long-term dependencies between student learning records, effectively improving the accuracy of knowledge state tracking.\n\u2022 sparseKT is a knowledge tracing model based on the SAKT model[12]. Through k-sparse attention mechanism, it employs soft-thresholding and top-K sparsification strategies to select the most relevant historical interaction information, and introduces question discrimination factors to capture individual differences among different questions under the same knowledge point.\n\u2022 DTransformer is a two-layer framework model based on Transformer, ensuring model stability through contrastive learning[26]. This model proposes a novel architecture to track students' learning activity patterns and improves model generalization through contrastive learning.\n\u2022 DKVMN is a memory network-based knsowledge tracing model[10], implemented using a static key matrix and a dynamic value matrix. This model defines a key matrix to store latent knowledge concepts and a value matrix to store student knowledge state, updating student knowledge state through read and write operations.\n\u2022 DIMKT is a knowledge tracing model that considers question difficulty effects, implemented based on deep learning architecture[15]. This model innovatively integrates question difficulty into the learning process, establishing relationships between student knowledge state and question difficulty levels, significantly improving prediction performance.\n\u2022 ReKT is a simple yet powerful knowledge tracing model, implemented based on the FRU (Forget-Retrieve-Update) architecture[27]. This model models student knowledge state from multiple perspectives, including question, concept, and domain knowledge state, achieving excellent performance while maintaining simplicity."}, {"title": "4.3 Experiment Setup", "content": "In our experiments, we first deployed GLM-4 as the local LLM to extract question difficulty $d^{llm}_i$. The difficulty coefficient ranges from 0 to 100 as a percentage scale, where higher values indicate greater question difficulty. During the data loading phase, the statistical difficulty $d^{stat}_i$ for question i is calculated for use in the Difficulty Perception Bias Sequence module.\nFollowing the Pykt benchmark format[31], we divided each dataset into 5 folds, where each record represents a student's learning sequence. We used fold 0 as the test set and folds 1-4 as the training set. For model optimization, we employed the Adam optimizer with MSE Loss as the loss function. The learning rate was set to 0.0001, dropout rate to 0.2, and MLP embedding dimension to 256. The forgetting factor \u03c3 was set to 0.8 for calculating the Difficulty Mastery Ratio. In the multi-head attention mechanism, we set the layers with 4 attention heads. All experiments were conducted on the same A40 cluster to minimize hardware-related variations. In terms of model evaluation, we used AUC as the evaluation metric. AUC measures the model's ability to distinguish between correct and incorrect responses, and is a commonly used evaluation metric in knowledge tracing scenarios. Finally, we set early stop to 10 epochs to reduce training time."}, {"title": "4.4 Experiment Result", "content": "Table 2 demonstrates the performance comparison between our proposed DDKT model and baseline models across two datasets. The results show that our DDKT model consistently outperforms other baselines, particularly in capturing complex learning patterns and handling diverse educational scenarios. On large-scale datasets like XES3G5M, our DDKT model exhibits superior performance compared to other baseline models, achieving AUC improvements ranging from 2% to 7%. On smaller datasets such as Eedi, our DDKT model also achieves the best performance, with AUC improvements ranging from 1% to 8%. The comparison also reveals that our DDKT"}, {"title": "4.5 Ablation Study", "content": "In our ablation study, we investigate the impact of three major components of DDKT on model performance: the Difficulty Mastery Ratio module, the Difficulty Perception Bias Sequence, and the Transformer Layer used in feature combination.\n\u2022 DDKT w/o DMR: This variant removes the personalized Difficulty Mastery Ratio component for tracking student learning status;\n\u2022 DDKT w/o DPBS: This variant removes the Difficulty Perception Bias Sequence component that combines objective and subjective difficulty measures for tracking student learning status;\n\u2022 DDKT w/o Transformer: This variant replaces the Transformer architecture with ordinary Multihead Attention for combining input embeddings to track student learning status;"}, {"title": "4.6 DDKT in cold-start scenarios", "content": "The cold-start problem is inherent in the field of knowledge tracing. When new questions are introduced into the time series, models struggle to fit these new items effectively due to the lack of historical student interaction records, leading to the cold-start problem in knowledge tracing. We evaluated DDKT's performance in cold-start scenarios using AUC as our evaluation metric. To simulate cold-start scenarios, we utilized the XES3G5M dataset, using 1%, 5%, 10%, 20%, 30%, 40%, and 50% of student records as training sets, with each student's learning sequence being proportionally reduced. We selected two comparison models: DIMKT, which showed the best performance in Experiment Results as our baseline, and sparseKT, which specializes in sparse datasets, as our comparative models. The experimental results are shown in Figure 3.\nThe experimental results demonstrate that our DDKT model performed remarkably well even with extremely limited data, maintaining outstanding performance across all datasets. Notably, when the dataset was reduced to 10% of its original size, our DDKT model showed improvements of 1.83% compared to DIMKT and 2.23% compared to sparseKT; when reduced to 20%, DDKT achieved approximately 2% improvement over both DIMKT and sparseKT models. DIMKT, which solely relies on statistical difficulty assessment as a question difficulty feature, suffered from performance degradation due to partial information masking in the source"}, {"title": "4.7 Student Personalization with DDKT", "content": "In the process of learning gradually, there are inherent differences among students. Therefore, it is crucial for the model to accurately express the students' mastery of knowledge and learning progress in the process of knowledge tracking for personalized modeling. The Difficulty Mastery Ratio (DMR) we proposed reflects students' personalized learning characteristics by recording their performance on questions of varying difficulties, helping to construct a more precise representation of knowledge state. To validate the effectiveness of DMR in personalized modeling, we designed three comparative experiments, setting the students' Difficulty Mastery Ratios to all 0s, all 1s, and all random values, respectively. The experimental results are shown in Figure 4.\nFrom the results, we found that when students' personalized difficulty mastery ratios were hidden, the model's performance was not as good, indicating that DMR indeed captured students' personalized characteristics during the learning process and significantly improved the model's predictive performance. When using all zeros,"}, {"title": "4.8 Visualizing Student's Study Process", "content": "To visualize the student's learning process more intuitively, we randomly selected one student's learning sequence in Eedi and recorded their dynamic difficulty adaptation indicators, knowledge state, difficulty perception bias sequence, and prediction probability values throughout their learning process, presenting them in the form of a heatmap. The sequence length is 199, representing the learning trajectory of 199 questions completed by the student. The heatmap of the student's learning process is shown in Figure 5.\nDuring the initial 40 exercises, the objective difficulty of questions generally maintained a high level between 0.7-0.8, while the student's knowledge state fluctuated only between 0.4-0.5, with DPBS showing a positive bias of approximately 0.3-0.4 in"}, {"title": "5 Conclusion", "content": "Difficulty, as a crucial indicator in the learning process, deserves thorough investigation in its semantic data mining and application in knowledge tracing. Previous difficulty models typically employed single difficulty indicators, either using LLM-assessed difficulty or statistical difficulty, without combining subjective question difficulty with students' objective difficulty. To deeply explore the impact of difficulty features on knowledge tracing and mitigate the cold start problem common in knowledge tracing, our proposed DDKT model incorporates LLM-assessed difficulty as part of the calibrated difficulty fusion, combines students' subjective and objective difficulty"}, {"title": "Declarations", "content": ""}, {"title": "Competing interests", "content": "The authors declared no potential conflicts of interest with respect to the re-search, authorship, and/or publication of this article."}, {"title": "Data, Materials and/or Code availability", "content": "The data are available from the corresponding author on reasonable request."}, {"title": "Funding", "content": "This work was supported by funding from Guangdong Philosophy and Social Science Foundation (Grant No. GD22WZX02-03), Guangdong Basic and Applied Basic Research Foundation (Grant No. 2023A1515110134) and Guangzhou Municipal Science and Technology Program (Grant No. 2024A04J3752)."}]}