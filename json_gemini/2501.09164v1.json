{"title": "The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching", "authors": ["Yevhen Kostiuk", "Oxana Vitman", "\u0141ukasz Gaga\u0142a", "Artur Kiulian"], "abstract": "In this work, we address the challenge of evaluating large language models (LLMs) on the short answer matching task for Latvian and Lithuanian languages. We introduce novel datasets consisting of 502 Latvian and 690 Lithuanian question-answer pairs. For each question-answer pair, we generated matched and non-matched answers using a set of alteration rules specifically designed to introduce small but meaningful changes in the text. These generated answers serve as test cases to assess the ability of LLMs to detect subtle differences in matching of the original answers. A subset of the datasets was manually verified for quality and accuracy. Our results show that while larger LLMs, such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in distinguishing matched and non-matched answers, smaller models show more variance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot examples, while Mistral Nemo 12b underperformed on detection of subtle text alteration, particularly in Lithuanian, even with additional examples. QWEN2.5 7b and Mistral 7b were able to obtain a strong and comparable performance to the larger 70b models in zero and few shot experiments. Moreover, the performance of Mistral 7b was weaker in few shot experiments. The code and the dataset are available on our GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "In educational domain, open-ended questions are commonly used and can be defined as questions that require a more elaborate response than simple yes-no or selection of a correct choice. These questions help to encourage a discussion, share ideas and provide more freedom for a student.\nEvaluation of responses to the open-ended question is a time-consuming and difficult task that requires an evaluator to carefully read each answer and compare it with the correct answers, ensuring they match. Automating this process makes it easier for evaluators to provide a feedback and analyze errors faster (Pillai et al., 2018; Sreevidhya and Narayanan, 2021).\nThe automatic short answer matching task addresses this challenge. The goal of the task is to predict whether an answer to the question is matching a correct answer. With the introduction of LLMs, reasonable performance was achieved on English and other high-resource languages for this problem (Ivanova and Handschuh, 2024). On the other hand, when it comes to low-resource settings, LLMs demonstrated weaker results, as well as displayed biases (Hackl et al., 2023; Lai et al., 2023).\nIn this work, we focus on Latvian and Lithuanian answer matching task, specifically on a detection of correct and incorrect responses that are similar to a set of reference \"gold\" answers, but differ in the key detail(s) to the question.\nWe automatically generated open-ended question-answer datasets for these languages based on Wikipedia. For this task, we do not focus on the factual correctness of the answers, as it does not affect the task. Each element of the dataset contains a question and its reference answer. Then we generated a set of answers that are matched with the reference answer and a set of non-matched answers. The non-matched answers are created as similarly as possible to the reference answers with respect to the words inclusion, but with the key words changed to make it incorrect. To generate the answers, we formulated different"}, {"title": "2 Related Work", "content": "Answer matching task can be viewed as a subtask of the automatic short answer grading (ASAG). The definition of what is a short answer and if it is acceptable can vary depending on the domain (Burrows et al., 2015; Bonthu et al., 2021). Nevertheless, all the definitions involve high semantic similarity between the correct answer(s) and predicted answers. The grading scale is also can be domain dependent (Zhang et al.; Divya et al., 2023; Krithika and Narayanan, 2015).\nWith the development of deep learning methods, they were widely used for the task, as they provide better robustness towards syntactic changes of the text rather than other methods (Bonthu et al., 2021), utilizing RNNs (Cai, 2019), CNNs (Chen and Zhou, 2019), transformers (Sung et al., 2019; Willms and Pad\u00f3, 2022) and so on. Some of the suggested methods are aimed to not only grade an answer, but to explain its flows and inaccuracies (Tornqvist et al., 2023).\nWith the rise of generative large language models (LLMs), they were applied for ASAG as well (Metzler et al., 2024; Ivanova and Handschuh, 2024; Chu et al., 2024; Schneider et al., 2023; Gr\u00e9visse, 2024; Yancey et al., 2023; Yoon, 2023). Analysis of LLMs for this task showed that they are capable of predicting consistent ratings for English (Hackl et al., 2023; Mizumoto and Eguchi, 2023). However, studies showed that the LLMs' performance on the non-English datasets is weaker (Lai et al., 2023; Dar\u0123is et al., 2024)."}, {"title": "3 Datasets", "content": "To generate answer matching datasets, the three-stage pipeline was implemented.\nFirstly, we used the approach for generating question-answer Knowledge and Instruction Dataset (KID) based on Wikipedia, introduced in (Kiulian et al., 2024) and adapted it for Latvian (Lat-KID) and Lithuanian (Lit-KID). More details are provided in the Section 3.1. The generated datasets consist of pairs of a question and a reference answer (assumed to be correct and relevant to the question), as well as a factual information that supports the answer."}, {"title": "3.1 Lat-KID and Lit-KID\nQuestion-Answering Datasets", "content": "For each language, we extracted top 1,000 articles for each month of the last 12 month from Wikipedia, resulting in 12,000 articles. From this pool, 1,000 articles with the top cumulative counts were extracted. The articles were filtered by their relevance to the corresponding country with Gemini 1.5 Pro (Team et al., 2024). Each article was separated into the paragraphs and at least 3 questions were generated for it with Gemini 1.5 Pro. The prompt contains additional fields to run a self-check on the quality of the question (standalone, in the correct language, natural sounding). The prompts are available in the project's GitHub."}, {"title": "3.2 Matched and Non-Matched Answers\nGeneration", "content": "Non-Matched Answers Generation. We defined two alteration rules for non-matched answers generation: incorporating minor changes (IMC) and changing domain related information (CDRI). IMC includes changes to the text that change a couple of key words like date, name, location etc, while keeping everything else unchanged. CDRI is similar to IMC, however its objective is to change a key term to the similar from the same domain. For example, changing the name of the first president to the second one, etc.\nTo generate non-matched answers, we utilized LLaMa3 7b2 and GPT-403.\nWhen generating IMC and CDRI answers, the model was presented with the few-shot example prompts (see Figure 2).\nMatched Answers Generation. We defined the following alteration rules for matched answers generation: adding more question-related entities (Ents), changing words to synonyms (Synonyms), adding more background information (MoreInfo), and style swap to exclamatory (Exclamatory).\nAs previously, we used GPT-40 and LLaMa3 7b. The models were presented with different prompts per rule. The code and prompts are available in the GitHub repository for the project4.\nPostprocessing. After the generating answers, the duplicates were removed. The resulting amount of (question, reference answer, generated answer) triplets is 3,012 (1,506 are matched and"}, {"title": "3.3 Manual Evaluation", "content": "We recruited two native speakers for Latvian and Lithuanian to evaluate the quality of the final generated dataset. They were presented with a random triplet of (question, reference answer, generated answer) and a description if the generated answer was generated by matched or non-matched method. Based on that, the annotators had to accept a triplet if the description fits the reference and generated answers. Otherwise, they had to reject sample. The results are presented in Appendix A."}, {"title": "4 Methodology", "content": "To evaluate the LLMs capabilities and an influence of the prompting strategy, we used two prompting methods per language for this task: zero shot (ZS) and few shot (FS). We set all the parameters to defaults with a random seed of 2.\nIn all of the methods, the model were instructed to start their output with True if the provided reference answer and a generated answer are matched otherwise with False. ZS and FS shared the same system prompt, but FS gave a model additional examples in corresponding language.\nWe evaluated LLaMa3.1 (8b and 70b) (Dubey et al., 2024), Mistral Nemo 12b and Mistral 7b (Jiang et al., 2023), and QWEN2.5 (7b and 72b) (Team, 2024; Yang et al., 2024) models. To parse the output, we checked if the model followed instructions about the output. If it did not, we retrieved the key words: \u201cTrue\u201d or \u201cFalse\u201d. If none of the words were presented, we counted it as an incorrect prediction."}, {"title": "5 Results and Discussion", "content": "The results are presented in Table 1, and on Figures 3 and 4. Additionally, we measured a percentage of times, when model followed the provided format and started with \"True\" or \"False\". The majority of models were able to output the correct format for 99% on Latvian samples. For Lithuanian, LLaMa3.1 8b generated text in correct format in 89% of times in ZS settings. In case of the FS, this value is 99%. Other models consistently followed the format with a rate of 99%. EuroLLM 9b was not able to follow a format at all in ZS settings for both languages, even though its results were legible, but impossible to parse. However, when presented with a few shot examples, it generated expected format.\nOur results demonstrated that larger LLMs (with 70b parameters) are capable of reliably detect matched and non-matched answers in Lithuanian and Latvian. We hypothesized that LLMs would output near perfect scores, however, smaller models performed differently. In case of Mistral Nemo, there was a slight decrease of results when switched from zero shot to a few shot approach in both languages. On the contrary, LLaMa3.1 8b performed better in a few shot scenario, improving its ZS score on 9%. QWEN2.5 7b performed nearly perfectly, achieving 99 accuracy score in both settings.\nDeeper analysis of results indicated that in case of Latvian, most of the models (except for LLaMa3.1 8b, MIstral 7b, and EuroLLM 9b) showed almost perfect performance on all the generated types of matched answers."}, {"title": "6 Conclusion", "content": "In conclusion, our findings demonstrate that large language models (LLMs) with greater parameter counts, such as QWEN2.5 72b and LLaMa3.1 70b, consistently achieve high accuracy in distinguishing matched and non-matched answers across both Latvian and Lithuanian, regardless of zero-shot or few-shot settings. Smaller models showed less robustness, with LLaMa3.1 8b and EuroLLM 9b benefiting from additional examples in few-shot scenarios. Mistral Nemo 12b struggled with detecting certain nuances, particularly in Lithuanian. QWEN2.5 7b and Mistral 7b were able to obtain a similar the performance to the larger 70b models, but in case of Mistral 7b the performance decreased in with a few shot approach. These results highlight the robustness of larger models and the potential for targeted improvements in smaller ones to address answer matching task with the defined set of alteration rules."}]}