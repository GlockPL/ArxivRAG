{"title": "An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism", "authors": ["Qing Zhang", "Haocheng Lv", "Jie Liu", "Zhiyun Chen", "Jianyong Duan", "Hao Wang", "Li He", "Mingying Xv"], "abstract": "With the rise of large-scale language models (LLMs), it is currently popular and effective to convert multimodal information into text descriptions for multimodal multi-hop question answering. However, we argue that the current methods of multi-modal multi-hop question answering still mainly face two challenges: 1) The retrieved evidence containing a large amount of redundant information, inevitably leads to a significant drop in performance due to irrelevant information misleading the prediction. 2) The reasoning process without interpretable reasoning steps makes the model difficult to discover the logical errors for handling complex questions. To solve these problems, we propose a unified LLMs-based approach but without heavily relying on them due to the LLM's potential errors, and innovatively treat multimodal multi-hop question answering as a joint entailment tree generation and question answering problem. Specifically, we design a multi-task learning framework with a focus on facilitating common knowledge sharing across interpretability and prediction tasks while preventing task-specific errors from interfering with each other via mixture of experts. Afterward, we design an iterative feedback mechanism to further enhance both tasks by feeding back the results of the joint training to the LLM for regenerating entailment trees, aiming to iteratively refine the potential answer. Notably, our method has won the first place in the official leaderboard of WebQA (since April 10, 2024), and achieves competitive results on MultimodalQA.", "sections": [{"title": "1 Introduction", "content": "Multimodal multi-hop question answering (MMQA) [3] is a complex task that involves multiple input sources such as text, tables, and images. It requires reasoning through different modalities to generate accurate and complete answers. Currently, most multimodal multi-hop question answering (QA) methods adopt the approach of converting multimodal information into textual descriptions (by transforming images through image caption models, and tables through natural language descriptions), and then using large-scale language models (LLMs) to generate answers [8, 11, 27, 28]. The salient advantage of this method is that it can leverage the powerful language understanding and generation capabilities of LLMs, as well as the interpretability of textual descriptions. However, this method indiscriminately converts all multimodal information into textual descriptions, inevitably producing a large amount of redundant information. In contrast, the previous method generates too much redundant textual information, which may mislead the model into generating incorrect answers. More importantly, there are inherent reasoning relationships among these key pieces of information that have been ignored by current methods for interpreting and utilizing this logical connection, making it difficult to avoid logical and factual errors when addressing complex questions [18]. Different from previous methods with redundancy and having no interpretability, we innovatively treat multimodal multi-hop question answering as a joint entailment tree generation and question answering problem. To the best of our knowledge, this is the first work introducing entailment tree generation from a mutual-beneficial perspective, bridging small model and large model generation in multimodal multi-hop question answering. In the field of Natural Language Processing (NLP), entailment tree generation is an important sub-task in the question-answering task [6, 10, 21] while it has not yet been applied in the multimodal domain. The reason is that it is nontrivial to directly apply the existing method in NLP to our cases for benefiting the answering beyond the interpretability. During our initial attempt to directly transfer entailment tree generation methods to the multimodal multi-hop question-answering domain, we discovered that the current entailment tree generation methods have very low accuracy in generating entailment trees. Even predicting the structure of entailment trees yields similarly low accuracy. Therefore, we propose a new LLMs-based method that introduces the task of entailment tree generation into the multimodal multi-hop question answering task, to address both prediction and interpretability problems, and allows the introduced entailment tree to be iteratively refined. Although LLMs show promising performance in many tasks, we still have observed that the entailment trees generated by LLM mainly contain two types of errors: 1) incorrect selection of leaf nodes. 2) incorrect structure of the entailment tree. Therefore, we propose the LLM and smaller model interactively based framework to carry out the fact retrieval generation task and the question-answering task. We use multi-task learning with smaller models, and use an iterative feedback mechanism to re-predict the structure of the entailment tree based on the leaf nodes and the answers predicted by the small model. Inspired by the idea of [9, 23, 24], to facilitate mutual enhancement among the small models' multi-task learning, we employ a shared multi-task mixture-of-experts model, allowing interactions between the fact selection and supervised QA tasks as guidance for LLM. Specifically, in entailment tree initialization stage, we iteratively use large-scale language models [2] to decompose an existing multi-hop question into sub-questions that need to be solved, and completes them based on existing evidence (question, answer) as facts to construct a fact base. For entailment tree generation [6], since even predicting the structure of the entailment tree can only achieve a very low accuracy, which is much harder for our case, we propose to first generate entailment tree structure without the details, using the existing method of entailment tree generation [6] in NLP, then use large models to continuously fill in the values of the missing intermediate nodes in the entailment tree. Different from those in previous entailment tree generation tasks, where the leaf nodes and answers were provided simultaneously [6, 10, 21], for multi-modal multi-hop question answering tasks, the answers are not visible during testing, so our definition of entailment tree generation tasks is different from the past [6]. When we build the entailment tree, the input is a set of leaf nodes (facts from the fact base) and a question as hypothesis, while previous methods require inputting a set of leaf nodes and answer. After initializing the entailment tree in our method, both the leaf nodes and intermediate nodes are filled (the intermediate nodes are generated by the LLM through the collection of their child nodes), and we do not predict the final answer during the initialization phase of the entailment tree, but predict the answer in the second stage. For a detailed definition of our entailment tree generation, please refer to section 3.1.2. Our method introduces entailment tree generation into the field of multi-modal multi-hop question answering. It filters facts by generating entailment tree, models logical relationships between different modalities, eliminates irrelevant information in multi-modal contexts, and maintains logical consistency. We conduct experiments on two public MMQA datasets, namely WebQA [3] and MultiModalQA [22]. We use accuracy, F1 score and"}, {"title": "2 Related Work", "content": "Multimodal Multi-Hop Question Answering Multimodal multi-hop question answering is a task requiring multimodal multi-hop reasoning to generate the final answer. VQA [1] is first proposed to answer questions from visual-only inputs. Later, WebQA [3] and MultimodalQA [22] require integrating information across free text, images, or semi-structured tables, to answer multi-hop reasoning question. To address the challenge of finding answers from multiple sources of information, MuRAG [5] design a multi-modal transformer architecture to accept both text and image feature inputs, and builds a million-scale dataset for pretraining the model. [18, 26, 28] unified multimodal information into text using image caption model and table linearization method, they proposed a new multimodal question answering paradigm, but there is no restriction during the process, resulting a lot of information redundancy and affecting the performance of the model. Also, there are many recent works on multimodal question answering using large models. [11] trained an image caption model to generate image caption for GPT-3 to understand images then generate responses; [16] use multimodal large model LLaVA to generate more accurate image caption, then construct different in-context learning templates according to each modalities, enabling GPT-3 to leverage its powerful performance in this task. Both approaches need to generate image caption for the large language model to understand question, but there are no conditional restrictions during the image caption generation stage; or when generating image captions directly based on multi-hop questions, the questions contain information cannot be asked by a single image, which causes errors during the image caption generation stage. For the above problems we found, we proposed an approach to filter redundant information through the logical structure of the entailment tree, ensuring the simplicity and relevance of information with the rationality of reasoning. Entailment Tree Generation The task of entailment tree generation currently serves NLP question answering systems primarily. [6] introduce EntailmentBank, a dataset specifically designed for the task of entailment tree generation. Each multi-step entailment tree in EntailmentBank serves as an explanation, clearly demonstrating the reasoning process behind a hypothesis based. Recent methods [10, 15, 21] have presented multi-step generation approaches, which iteratively select premise facts and generate intermediate conclusions. At present, the all-correct score (only if all of the leaves, steps, and intermediates are all correct) of entailment tree structure generation in the field of NLP is very low (2.9% in full corpus) and cannot be directly applied to other fields. In addition, in the current multimodal QA datasets, the questions are relatively simple. According to the statistics of two datasets [3, 22], the proportion of complex questions (with reasoning hops greater than or equal to 3) is only 11.3%. After removing the simple comparison questions, the proportion of complex questions only accounts for 1% of the total dataset. For the WebQA dataset, the proportion of complex questions (with reasoning hops greater than or equal to 3) is only 1%. These two datasets are currently the most complex in multimodal multi-hop QA, making them suitable for evaluating our methods. Multi-Task Mixture-of-Experts Recent news suggests that GPT-4's internal structure employs a mixture-of-experts (MoE) approach, which has been influential in the development of large-scale models [12, 24, 29]. The use of MoE as an architectural foundation has become prevalent in recent large models, propelling the advancement of both the models themselves and the MoE concept. Moreover, multi-task MoE models have seen significant development prior to their integration into large-scale models. [19] propose a multi-layer gated network based on different tasks, allowing each task to have its independent experts, thereby enabling the model to better capture the inter-task correlations. Additionally, based on the previous method, [23] propose a method which retains the shared experts, allowing for interaction between different experts. [9] devise a task-aware gating mechanism within sparse MoEs to route the input (tokens from different tasks) to specialized experts conditioned on the task. We combine the methods of multi-task MoE from previous research with the current MoE training approaches based on large models, enabling the multi-task MoE model to be suitable for multi-task learning with data generated by large-scale models."}, {"title": "3 Method", "content": "As shown in Figure 2, our method is divided into two stages: (a) entailment tree initialization stage and (b) iterative mixture-of-experts optimization stage. The goal of the first stage is to initialize the entailment tree for the use of small model in assisting with question answering and providing interpretability. We decompose the original question to build a fact base, and use LLM to initialize the structure of the entailment tree based on the fact base. The goal of the second stage is to correct the leaf node and structural errors in the initialized entailment tree through joint learning of fact retrieval generation task and question answering task, and to iteratively optimize the entailment tree through the feedback of the results of joint learning to the LLM."}, {"title": "3.1 Entailment Tree Initialization Stage", "content": "3.1.1 Fact Base Construction. In the fact base construction module, we need to decompose the multi-hop question into several sub-questions based on different evidence and process them differently according to the modality of the corresponding evidence. Decompose Multi-Hop Question First, we need to retrieve the multimodal evidence required to answer the multi-hop question. However, since our method generates image captions by decomposing the question, we use the global image caption and image attribute features to retrieve evidences according to the method in [28], and finally retrieve the required multimodal evidence set E:\n$$E = \\text{BERTretri}(\\text{Textevidence}) = [E_1, E_2, ..., E_n],$$\nwhere \"n\" represents the number of evidence in the evidence set. \"Textevidence\" refers to all the evidence obtained after converting \"images, text, tables\" into text, which is referred to as \"Textevidence\". After obtaining all the retrieved multimodal evidence E = [E1, E2, ..., En], we prompt GPT-3.5 to decompose the original question based on all the evidence E and generate n sub question qs with their corresponding evidence. Suppose that the k-th question of q\u00ba has L tokens, denoted as q = (yy),...,y), the decoding process can be formulated as:\n$$\\hat{E}_{k, y} = \\text{argmax}_{y}P_{\\text{LLM}} (E_k, y | y^{L_k-1}_{l=1} ; P_q, q, E)$$\nwhere pq is the instruction prompt. The outline of the prompt pq for LLM is as shown in Figure 3: Once all sub-questions are decomposed from the evidence, we process them based on the evidence type and store the resulting facts in the fact base.\nImage Fact For the image modality, we further decompose each sub-question into atomic questions qimg using GPT-3.5. Suppose that the r-th atomic question of qimg decomposed from qf has Li tokens, denoted as q\u2081 = (y), ...,y), the decoding process can be formulated as:\n$$\\hat{y_r^{L_i}} = \\text{argmax}_{y}P_{\\text{LLM}}(y|y^{l-1}_1; P_q, q_k, E_k)$$\nThen, we input the decomposed atomic question qimg and corresponding image Eim into the VQA model to obtain answers. We use"}, {"title": "3.1.2 Entailment Tree Generation", "content": "Definition 3.1. The input of entailment tree generation task consists of a corpus of premises C (facts from fact base) and a hypothesis h (original question). The objective is to generate an entailment tree T that explains the hypothesis h by using a subset of the premises in C as building blocks. Entailment trees are represented as a tuple T = (h, L, E, S), where leaf nodes li \u2208 L are retrieved from the corpus (i.e.L\u2286 C), internal tree nodes ei \u2208 E are intermediate conclusions (new sentences not present in corpus C, note that intermediate conclusions are generated by LLM), and si \u2208 S is a list of entailment steps that can explain the hypothesis h, which is always the tree root and the final conclusion. Entailment Tree Structure Generation The main function of the entailment tree structure generation module is to select leaf nodes and create the entailment tree. We use predefined symbols as introduced by [6]. The facts in the fact base act as potential leaf nodes, and the original question serves as the conclusion. Using GPT-3.5, the facts form leaf nodes, and multiple leaf nodes combine to create intermediate nodes. At this stage, only the facts and the original question are given, with intermediate nodes containing no specific information. During the tree initialization, we don't directly predict the answer but continuously fill the tree by predicting its structure and refining intermediate nodes through leaf node combinations. After predicting the structure of the entailment tree based on the question and perfecting the entailment tree, since the root node (question) does not contain the answer, we use the \"answer\" placeholder to replace the root node and input it into the second phase to perfect the entailment tree. We use the symbol \"&\" to denote \"and\", and \"\u2192\" to denote \"entails\", Suppose that the j-th original question sj and fact base FB = [fact1, fact2, ..., factm] are input into GPT-3.5 and generate entailment tree structure ts which has Lt tokens, denoted as t = (y), y, ..., y), the decoding process can be formulated as:\n$$\\hat{y} = \\text{argmax}_{y}P_{\\text{LLM}}(y|y^{L_t-1}_{l=1}; P_t, q_j, FB)$$\nwhere pt is the instruction prompt. The template of the prompt pt for LLM is as follows:\nEntailment Tree Refinement The entailment tree refinement module mainly completes the intermediate nodes span in the already generated entailment tree structure. The algorithm we use is shown below: The meaning of \"Split the"}, {"title": "3.2 Iterative Mixture-of-Experts Optimization Stage", "content": "Due to the possible leaf node selection errors and entailment tree structural errors in the previously mentioned initialized entailment tree, we use a hybrid expert model to jointly learn the fact retrieval generation task (retrieving leaf nodes) and question answering tasks, and correct the entailment tree structure through an iterative feedback mechanism. 3.2.1 Jointly Learning of Fact Retrieval Generation And Question Answering. First, we use the T5 encoder to extract the features of each fact in the fact base as Ffact [f[mean]' [mean]' f2 f[mean]], where fmean refers to the average of all features of a given fact. Afterwards, we convert the entailment tree into a natural language description denoted as Treetext etential and input Treenitial into the shared encoder of T5, and then concat it with original question q to obtain the final feature Fet:\n$$F_{\\text{et}} = \\text{SharedEncoder}(\\text{Concat}([T_{\\text{ree}}^{initial}, q]))$$\nOnce we have obtained the features, we input Fet into the Mixture-of-Experts model. Multi-Task Mixture-of-Experts Our mixture-of-experts model primarily consists of three parts: two gating networks, two task-specific expert networks, and one shared expert network. The task-specific expert networks are dedicated to the tasks of fact retrieval generation and question answering, respectively, while the shared expert network can be utilized for both tasks. The gating network is responsible for selecting the appropriate experts. Specifically, gating network A selects from the fact retrieval generation task and shared experts, while gating network B selects from the question answering task and shared experts. Top-2 Selection. According to the formulation above, when g() is a sparse vector, only part of the experts would be activated and updated by backpropagation during training. We set the gating layer as a top-K selection as:\n$$I(F_{\\text{et}}), V(F_{\\text{et}}) = \\text{TopK}(\\text{softmax}(f(F_{\\text{et}})))$$\nwhere f() is routing linear transformation Rd \u2192 Rne, ne denotes the number of all experts, and Fet \u2208 Rl\u00d7d. I(\u00b7) represent the index of the expert selected by the gating network for each token. V(.) represents the values corresponding to the top-K gating scores, which determines the contribution or weight of each selected expert for a given token. We generally follow [24] and set K = 2. Token-choice Routing We generally follow [24] for our routing design to ensure training stability. Given all trainable experts Expert() and input representation Fet, the output of MoE model can be formulated as:\n$$\\text{MoE}(F_{\\text{et}}) = \\sum_{k=1}^K V_k(F_{\\text{et}}) \\cdot \\text{Expert}_k(F_{\\text{et}})[I(F_{\\text{et}})]$$\nwhere Vk () represents the values corresponding to the k-th gating scores, Expert(\u00b7)k [I(\u00b7)] denotes the k-th expert selected by the indices I(.), D is the feature dimension of Fet. Please note that each expert is a FFN layer instead of a complete Transformer model in most MoE-based Transformer models, including ours. After we have selected the experts for the fact retrieval generation task and the question answering task through the gated network, we add MoE(Fet) with Fet to obtain the final MoE output:\n$$\\text{MoE}_f = \\text{add}(\\text{MoE}(F_{\\text{et}}), F_{\\text{et}})$$\nthen we input MoEf into the decoder of their respective tasks. Decoders for Multi-Tasks We handle the two decoders differently. The FRG Decoder performs cross-attention with all fact features and the entailment tree description passed through the Shared Encoder, while the QA Decoder only performs cross-attention with the entailment tree description passed through the Shared Encoder. The reason for doing this is that we hope the QA model can get the answer based solely on the entailment tree. If it cannot, then optimize the entailment tree through the FRG model and the Iterative Feedback Mechanism until the QA model can answer the question based solely on the entailment tree. The decoder for fact retrieval generation Decoder frg performs cross attention with the facts feature Ffact and shared encoder output Fet:\n$$w_t = \\text{CrossAttention}(\\text{Decoder}(q), \\text{MoE}_f), F_{\\text{fact}})$$\nwhere wt denotes the cross-attention weights at time step t. Then the decoder Decoderfrg stops to retrieval at time step |M|, i.e., the length of the evidences e, and then we utilize cross-entropy loss for it:\n$$L_{\\text{frg}} = - \\frac{1}{|M|} \\sum_{t=0}^{|M|} \\log \\frac{\\exp(w_{t,t^+})}{\\sum_{i=1}^{n} \\exp(w_{t,i})}$$\nwhere wt,i denotes the cross-attention scores of the i-th fact at time step t, wt,t+ denotes the score of the target source (The fact index sequence extracted from the entailment tree generated by LLM) at time step t, M is the number of retrieval steps. The decoder for the question answering task Decoderqa performs cross attention with the overall features MoEf, we utilize cross-entropy loss for it:\n$$9q = \\text{CrossAttention}(\\text{Decoder}(q), \\text{MoE}_f)$$\n9q is the result of the cross-attention between Decoderqa and MoEf. The decoder Decoderqa stops to generate at time step |A|, i.e., the length of the answer a, and then we utilize cross-entropy loss for it:\n$$L_{\\text{qa}} = - \\frac{1}{|A|} \\sum_{t=0}^{|A|} \\log \\frac{\\exp(g_{q,t^+})}{\\sum_{i=1}^{n} \\exp(g_{q,t})}$$\nwhere gq,t+ denotes the probability of the ground truth token at the time step t in the decoder's output sequence, gq,t denotes the probability distribution over all possible tokens in the vocabulary at the time step t. The overall loss function is as follows:\n$$L = L_{\\text{frg}} + L_{\\text{qa}}$$\n3.2.2 Iterative Feedback Mechanism. After we complete the tasks of fact retrieval generation and question answering, we first replace the fact indices obtained from the fact retrieval generation task in the fact base with the corresponding facts. Then we concatenate it with the final answer and input it into GPT-3.5 as additional information to correct the entailment tree and conduct a second round of training. We add a prompt pifm to the original prompt template which we use to generate the entailment tree structure: \"Given the following potentially relevant facts and the potentially"}, {"title": "4 Experiments", "content": "Datasets We conducted experiments on two of the most representative MMQA datasets: WebQA and MultimodalQA.\n1) WebQA Dataset [3] is a multimodal and multi-hop question answering dataset that contains QA pairs that require one or two images and text snippets to answer. Each question has a set of distractors that the model must consider along with the correct clues to provide an answer. WebQA uses BARTScore to measure both the fluency and keyword accuracy of the answer denoted as QA-FL and QA-Acc in Table 2. These two scores are multiplied together to obtain the QA score. The clue retrieval can be easily evaluated using F1 score.\n2) MultimodalQA Dataset [22] involves answering multi-hop complex questions by combining information from text, tables, and images. Each question also includes visual and text distractors. The performance is measured by F1 score at the word level and the Exact Match (EM) of the predicted answer. Implementation Details We conduct experiments on two datasets: WebQA, and MultimodalQA. The information source for WebQA includes both text and image modalities, while MultimodalQA focuses on text, images, and tables. For WebQA and MultimodalQA, a candidate clue list is given, and the model needs to find the most relevant clue to evaluate the accuracy of the clue retrieval. The backbone for retrieval is BERT [7]. We use LLaVA-1.5 as our VQA model, and use T5 as our FRG and QA model. We utilize the Transformers library and pretrained parameters from HuggingFace 4 and conduct experiments using 24G GPU cards. Further, AdamW [17] is used as the optimization algorithm with a learning rate of 1e - 4. The batch sizes for retrieval and qa are 32, 12. The iteration round k is set to 2, which is also similarly verified in [13]. The difference is that their method retrieves raw evidence, while ours uses refined reasoning logic after retrieval. We decide whether to continue iterating based on whether the QA accuracy (or exact match) on the validation set is no longer improving, further iterations no longer improve performance."}, {"title": "Ablation Study", "content": "In this experiment, we ablate the question decomposition, sub-question image caption modules, fact retrieval generation module, multi-task mixture-of-experts module, and iterative feedback module. When ablating the question decomposition module, we directly use the original question as input, and directly use the original question as the prompt for LLaVA to generate image captions. When ablating the sub-question image caption module and entailment tree generation modules, we directly concatenate the retrieved evidence and sub-questions and input them to GPT-3.5. The results of the ablation experiments are shown in Table 5. It can be seen that both question decomposition and the final sub-question image caption have a positive impact on the results, with the sub-question image description being particularly significant."}, {"title": "5 Conclusion", "content": "We follow the popular method [4, 8, 28] of unifying multimodal information into text. In this text-driven multimodal paradigm, we are the first to make an explanatory improvement through entailment tree generation. We construct entailment trees using LLMs and iteratively refine them with a multi-task MoE and feedback mechanism. This process generates entailment trees based on facts, aiding complex problem reasoning. Unlike previous methods, our approach enhances interpretability and improves question-answering accuracy. Our experiments demonstrate this approach's potential. While our method shows superior performance on two benchmarks, it has limitations. First, it may underutilize sub-question answers when prompting subsequent questions, despite potentially reducing error propagation. Second, most data in current multimodal multi-hop question answering datasets lacks complex reasoning. Our question decomposition module achieves improved performance with entailment trees, even assisting with a small fraction of complex issues by guiding the model's reasoning path, which shows the potential of the proposed method."}]}