{"title": "AUTRAINER: A MODULAR AND EXTENSIBLE DEEP LEARNING TOOLKIT FOR COMPUTER AUDITION TASKS*", "authors": ["Simon Rampp", "Andreas Triantafyllopoulos", "Manuel Milling", "Bj\u00f6rn W. Schuller"], "abstract": "This work introduces the key operating principles for autrainer, our new deep learning training\nframework for computer audition tasks. autrainer is a PyTorch-based toolkit that allows for rapid,\nreproducible, and easily extensible training on a variety of different computer audition tasks. Con-\ncretely, autrainer offers low-code training and supports a wide range of neural networks as well\nas preprocessing routines. In this work, we present an overview of its inner workings and key\ncapabilities.", "sections": [{"title": "1 Introduction", "content": "Reproducibility, code quality, and development speed constitute the \u2018impossible trinity' of contemporary experimental\nartificial intelligence (AI) research. Of the three, the first has attracted the most attention in recent literature [1],\nas reproducibility of findings is a cornerstone of science. However, the impact of the other two should not be\nunderestimated. Development speed allows the quick iteration of ideas a necessary prerequisite in experimental\nsciences and a prominent feature of AI research, as asserted by \"The Bitter Lesson\" of R. Sutton [2]. Similarly, code\nquality can be the key differentiating factor when it comes to \"standing on the shoulders of giants\", as shaky foundations\ncan lead to a spectacular collapse.\nThis is why toolkits that are easy-to-use and provide pre-baked reproducibility are critical for the proliferation and\nadaptation of new ideas. The not-so-recent renaissance of deep learning (DL) has been largely driven by the creation of\nsuch toolkits. TENSORFLOW 2, PYTORCH 3, and TRANSFORMERS 4 are many among numerous other toolkits that have\n'democratised' the use and development of DL algorithms. Yet, despite the fact that several of those toolkits feature\nsome support for the audio community, their initial development with other modalities in mind (primarily images or\ntext) has resulted in a lineage of design choices that makes them less suited for audio.\nIn the present work, we introduce autrainer as a remedy to this state of affairs. It is an 'audio-first' automated low-code\ntraining framework, offering an easily configurable interface for training, evaluating, and applying numerous audio DL\nmodels for classification and regression tasks. autrainer can be used via a command line interface (CLI) and Python"}, {"title": "2 Key operating principles", "content": "In this section, we describe the key operating principles of autrainer. We begin with its configuration management,\nfollowed by the data pipeline, training, and inference mode. As previously stated, the user can interact with autrainer\nusing its builtin CLI and Python CLI wrapper."}, {"title": "2.1 Hydra configurations", "content": "autrainer configures its various components using Hydra an open-source framework for scalable configuration\nmanagement based on YAML files. This allows for a low-code approach where the user can specify their key hyperpa-\nrameters in a YAML file. New functionality can be incorporated by specifying paths to local Python files and classes or\nfunctions implemented therein. For instance, this can be used to designate a new model architecture that has been locally\ntrained by the user or implement a custom, local dataset. As an example, Listing 1 illustrates an autrainer configuration,\ndefining a computation graph where a network of the PANN [3] family (CNN10) is trained on an Acoustic Scene\nClassification (ASC) (DCASE2016Task1-16k [4]) task using log-Mel spectrogram representations at a sample rate\nof 16 kHz that are extracted in a preprocessing step. Importantly, tagging and sharing configuration files allows for a\none-to-one reproduction of each experiment (assuming that added code is publicly available), as these files determine\nall the different aspects of the training process \u2013 including random seeds."}, {"title": "2.2 Workflow", "content": "The overall workflow for autrainer is shown in Fig. 1. Our goal is to make the use of the package as easy as possible;\nthus, we provide a main CLI entrypoint which allows the user to get started with model training as quickly as possible\n(even without writing a single line of code if they wish to use one of the prepackaged datasets). The choice to split up\nthe main workflow in three steps, namely fetch, preprocess, and train is also made to accommodate for parallel\nexecution of hyperparameter search, e. g., allows for parallel training by avoiding race conditions. An additional\npostprocess commands allows for an optional summarisation of results."}, {"title": "2.3 Data pipeline \u2013 autrainer fetch", "content": "The fetch command is responsible for preparing the raw audio data. This command is responsible for downloading\nthe data by calling the autrainer fetch CLI command. We aim to continually expand the datasets that can be used"}, {"title": "2.4 Feature extraction \u2013 autrainer preprocess", "content": "autrainer supports the following transformations of the signal:\nopenSMILE features : openSMILE is our widely-used feature extraction toolkit for speech analysis tasks [13].\nIt bundles numerous feature sets, such as the well-known eGeMAPS [14] or the official feature set of our\nINTERSPEECH ComParE Challenge series [15], and can be extended using configuration files. We utilise its\npython wrapper6.\nHugging Face transforms : As several of our supported models are released on Hugging Face, like wav2vec2.0\nor HuBERT, we allow the user to call a Hugging Face FeatureExtractor class which implements the\ntransforms needed for a given model to facilitate the interoperability of autrainer with the Hugging Face\necosystem."}, {"title": "2.4.1 Data augmentation", "content": "autrainer also comes with standard data augmentation methods that are widely used for computer audition tasks:\nSpecAugment: We offer the standard transforms proposed in SpecAugment [17], namely time masking, frequency\nmasking, and time warping.\nGaussian Noise: We support adding white, Gaussian noise\nMixup: We implement MixUp [18] and CutMix [19], two techniques that interpolate between different signals\ncontained within a batch (and accordingly adjust their labels).\nExternal: We provide interfaces to external libraries such as torchaudio, audiomentations, and torch-\naudiomentations for audio processing, as well as torchvision and albumentations for feature manipulation after\ntransforming audio signals into images.\nSimilar to transforms, augmentations have an order attribute to define the order of the augmentations. The augmentations\nare combined with the transform pipeline and sorted based on the order of the augmentations as well as the transforms.\nIn addition to the order of the augmentation, a seeded probability p of applying the augmentation can be specified.\nImportant: Augmentations from external libraries are not necessarily reproducible, we can only reproduce the probability\nof applying them but not the actual modification of the input. To create more complex augmentation pipelines, sequence\nand choice nodes can be used to create pipelines that resemble graph structures."}, {"title": "2.5 Model training \u2013 autrainer train", "content": "Model training is started by calling the autrainer train CLI command. This command utilises the general configu-\nration structure of autrainer, and allows the user to specify the models and data over which these should be trained, as\nwell as different criterions (i. e., loss functions), optimisers, (learning rate) schedulers, and other hyperparameters to\nsearch over. As configuration management is handled by Hydra, autrainer inherits all hyperparameter optimisation\nfunctionality, such as the one supported by Optuna [20]. Moreover, we support all PyTorch optimisers and schedulers."}, {"title": "2.5.1 Logging", "content": "On top of its internal logging and tracking (which takes care of storing model states and outputs), autrainer provides\ninterfaces to commonly-used machine learning operations (MLOps) libraries, such as MLflow [21] and TensorBoard [22]."}, {"title": "2.5.2 Supported tasks", "content": "Currently, autrainer only supports the tasks of single- and multi-label classification and regression (both single- and\nmulti-target). For each task, we provide a range of commonly-used losses and metrics, such as the (balanced) cross-\nentropy loss for classification and mean squared error for regression. Our long-term goal is to add support for additional\ntasks, such as Automated Audio Captioning (AAC) or Sound Event Detection (SED)."}, {"title": "2.5.3 Supported models", "content": "autrainer includes a constantly-growing list of common models and model architecture families that are used for audio\ntasks. These models are configurable by allowing for an adaptation of their standard hyperparameters (length, depth,\nkernel sizes, etc.).\nFFNN: Baseline feed-forward neural networks that can be configured according to the number of hidden layers,\nwidth, and other standard parameters. These allow the user to train a model using standard, fixed-length\nfeatures, such as openSMILE functionals.\nSeqFFNN: An extension of the above, sequence-based FFNNs, which first process dynamic features with a\nsequential model, such as Long short-term memory (LSTM) [23] or Gated Recurrent Unit (GRU) [24]\nnetworks.\nCRNN : End-to-end, convolution-recurrent neural networks (CRNNs) [25, 26] adapted from our End2You toolkit 10.\nPANN: The two best-performing models from PANNs, namely Cnn10 and Cnn14 [3]. These models can be both\ntrained from scratch or fine-tuned from the weights released by the original authors.\nTDNNFFNN : The Time-Delay Neural Network (TDNN) [27] pretrained on VoxCeleb1 [28] & VoxCeleb2 [29]\nincluded in SpeechBrain [30]\u00b9\u00b9 as a backbone to extract embeddings, which are then passed to a configurable\nFFNN for the final prediction.\nASTModel: The Audio Spectrogram Transformer (AST), optionally pretrained on AudioSet [31].\nLEAFNet : LEAFNet incorporates LEAF (Learnable Efficient Audio Frontend) and the additional components, as\nimplemented either in the original work [32] and included in SpeechBrain or the follow-up work of Meng et al.\n[33].\nW2V2FFNN : wav2vec2.0 [34] and HuBERT [35] models to extract audio embeddings, followed by a configurable\nFFNN as in Wagner et al. [36]. We support all different Hugging Face variants of wav2vec2.0 and HuBERT.\nWhisperFFNN : Similar to the above, but using Whisper instead of wav2vec2.0 or HuBERT [37].\nDeepSpectrum: Similar to DeepSpectrum [16], we allow the processing of spectrograms using image-based models,\nand add support for all the ones included in Torchvision [38] and Timm [39], both with randomly-initialised\nweights and their pretrained versions. Counter to our original work on DeepSpectrum, autrainer allows a joint\nfinetuning of the image-based models."}, {"title": "2.6 Postprocessing interface \u2013 autrainer postprocessing", "content": "Beyond the core training functionality, autrainer can process any finished training pipeline in an optional, customisable\nand extensible postprocessing routine acting on the saved training logs. This offers particular usability for grid searches\nover large hyperparameter spaces, summarising training curves and model performances across runs. autrainer further\nallows for the aggregation of trainings across certain (sets of) hyperparameters, such as random seeds or optimisers, in\nterms of average performance."}, {"title": "2.7 Inference interface - autrainer inference", "content": "autrainer includes an inference interface, which allows to use publicly-available model checkpoints and extract both\nmodel predictions and embeddings from the penultimate layer. This can be done with the autrainer inference\nCLI command. As part of the official release, we additionally provide pretrained models on Hugging Face 12 for\nspeech emotion recognition, ecoacoustics, and acoustic scene classification. We offer detailed model cards and usage\ninstructions for each published model."}, {"title": "3 autrainer design principles", "content": "In the previous sections, we have described the key features of autrainer. In the present section, we reiterate our key\ndesign considerations and highlight the strengths of our package.\nReproducibility: A major emphasis of our work was placed on the reproducibility of machine learning experiments\nfor computer audition. This has been ensured by the consistent setting of random seeds, and the strict definition\nof all experiment parameters in configuration files. While we do not take any steps to ensure that these\nconfiguration files cannot be tampered with, our workflow nevertheless enables researchers to reproduce the\nwork of original authors given the latter have released their configuration files and the corresponding autrainer\nversion.\nBaselines: autrainer allows a fair comparison with a number of readily-available 'standard' baselines for each\ndataset. Specifically, a user can rely on its grid-search functionality to compare their new model architecture to\nbaseline models using the same hyperparameters and computational budget. This reduces the considerable\nworkload of having to implement existing baselines from scratch (e. g., by porting code from non-maintained\nrepositories) and should help with the comparability of different methods.\nLow-code training: autrainer lowers the barrier of entry to the field of computer audition. For example, in the\ncase of computational bioacoustics, several of the expected users are biologists with little training in machine\nlearning applications. Relying on autrainer for the machine learning aspects allows them to benefit from\nadvances in that field, while only caring for implementing a dataset class that iterates through their data."}, {"title": "4 Future roadmap", "content": "By publicly releasing autrainer we wish to engage with the larger audio community to further expand the capabilities\nof our toolkit. In future releases, we aim to offer:\nMore datasets: Our goal is to expand our offering of off-the-shelf datasets to include the most commonly used\nbenchmarks and domain-specific datasets across different computer audition tasks.\nMore tasks: Currently, autrainer only supports standard classification, regression, and tagging. In the future, we\naim to expand it for AAC, SED, and Automatic Speech Recognition (ASR) by incorporating the appropriate\nlosses and data pipelines.\nMore models: We will additionally incorporate both specific model architectures and fundamentally different\nclasses of models \u2013 such as large audio models [40] \u2013 in juxtaposition with the tasks and datasets that will be\nadded."}, {"title": "5 Conclusion", "content": "This work described autrainer, an open-source toolkit aimed at computer audition projects that rely on deep learning.\nWe have outlined all major features and design principles for the current version of autrainer. Our main goals were to\noffer an easy-to-use, reproducible toolkit that can be easily configured and used as a low- or even no-code option. We\nlook forward to a more engaged conversation with the wider community as we continue to develop our toolkit in the\nyears to come."}]}