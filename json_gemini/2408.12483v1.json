{"title": "Not All Samples Should Be Utilized Equally: Towards\nUnderstanding and Improving Dataset Distillation", "authors": ["Shaobo Wang", "Yantai Yang", "Qilong Wang", "Kaixin Li", "Linfeng Zhang", "Junchi Yan"], "abstract": "Dataset Distillation (DD) aims to synthesize a small dataset capable of performing\ncomparably to the original dataset. Despite the success of numerous DD methods,\ntheoretical exploration of this area remains unaddressed. In this paper, we take\nan initial step towards understanding various matching-based DD methods from\nthe perspective of sample difficulty. We begin by empirically examining sample\ndifficulty, measured by gradient norm, and observe that different matching-based\nmethods roughly correspond to specific difficulty tendencies. We then extend the\nneural scaling laws of data pruning to DD to theoretically explain these matching-\nbased methods. Our findings suggest that prioritizing the synthesis of easier samples\nfrom the original dataset can enhance the quality of distilled datasets, especially\nin low IPC (image-per-class) settings. Based on our empirical observations and\ntheoretical analysis, we introduce the Sample Difficulty Correction (SDC) approach,\ndesigned to predominantly generate easier samples to achieve higher dataset quality.\nOur SDC can be seamlessly integrated into existing methods as a plugin with\nminimal code adjustments. Experimental results demonstrate that adding SDC\ngenerates higher-quality distilled datasets across 7 distillation methods and 6\ndatasets.", "sections": [{"title": "1 Introduction", "content": "In an era of data-centric AI, scaling laws [19] have shifted the focus to data quality. Under this\nscenario, dataset distillation (DD) [46, 39] has emerged as a solution for creating high-quality data\nsummaries. Unlike data pruning methods [13, 7, 45, 1] that directly select data points from original\ndatasets, DD methods are designed to generate novel data points through learning. The utility of DD\nmethods has been witnessed in fields such as privacy protection [6, 29, 4, 11], continual learning\n[38, 15, 50, 32], neural architecture search [2, 34, 43], and federated learning [14, 28, 18].\nAmong the various DD techniques, matching-based methods, particularly gradient matching (GM)\n[52, 51, 25, 20] and trajectory matching (TM) [3, 8, 12, 16], have demonstrated outstanding perfor-\nmance. However, a gap remains between their theoretical understanding and empirical success. To\noffer a unified explanation of these methods, we aim to explore the following question:\nQuestion 1: Is there a unified theory to explain existing matching-based DD methods?\nTo address Question 1, we first empirically examine the differences between matching-based distilla-\ntion methods. It is widely acknowledged that sample difficulty (Definition 1) is a crucial metric in\ndata-centric AI that significantly affects model performance, as seen in dataset pruning [44, 31, 30, 42],"}, {"title": "2 Preliminaries and Related Work", "content": "Dataset distillation involves synthesizing a small, condensed dataset $\\mathcal{D}_{syn}$ that efficiently encapsulates\nthe informational essence of a larger, authentic dataset $\\mathcal{D}_{real}$.\nGradient Matching (GM) based methods are pivotal in achieving distillation by ensuring the\nalignment of training gradients between surrogate models trained on both the original dataset $\\mathcal{D}_{real}$\nand the synthesized dataset $\\mathcal{D}_{syn}$. This method is first introduced by DC [52]. Let $\\theta_{t}$ represent the\nnetwork parameters sampled from distribution $P_{\\theta}$ at step $t$, and $\\mathcal{C}$ symbolizes the categories within\n$\\mathcal{D}_{real}$. The cross-entropy loss, denoted by $\\mathcal{L}$, is employed to assess the matching loss by comparing\nthe gradient alignment over a time horizon of $T$ steps. The formal optimization objective of DC is:\n$\\arg \\min _{\\mathcal{D}_{syn}} \\mathbb{E}_{\\theta_{0}\\sim P_{\\theta}, c \\sim \\mathcal{C}}\\left[\\sum_{t=0}^{T} \\mathcal{D}\\left[\\nabla_{\\theta} \\mathcal{L}_{\\mathcal{D}_{real}}\\left(\\theta_{t}\\right), \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{D}_{syn}}\\left(\\theta_{t}\\right)\\right]\\right]$\n(1)\nwhere $\\mathcal{D}$ measures the cumulative distances (e.g., cosine/L2 distance in DC) between the gradients of\nweights corresponding to each category output. The parameter updates for $\\theta$ are executed in an inner\nloop via gradient descent, with a specified learning rate $\\eta$:\n$\\theta_{t+1} \\leftarrow \\theta_{t}-\\eta \\cdot \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{D}_{syn}}\\left(\\theta_{t}\\right)$.\n(2)\nBuilding upon this, DSA [51] enhances DC by implementing consistent image augmentations on\nboth $\\mathcal{D}_{real}$ and $\\mathcal{D}_{syn}$ throughout the optimization process. Moreover, DCC [25] refines the gradient\nmatching objective by incorporating class contrastive signals at each gradient matching step, which\nresults in enhanced stability and performance. Combining DSA and DCC, DSAC [25] further\nintroduces improvements by synergizing these techniques. The revised optimization objective for\nDCC and DSAC is formulated as:\n$\\arg \\min _{\\mathcal{D}_{syn}} \\mathbb{E}_{\\theta_{0}\\sim P_{\\theta}}\\left[\\sum_{t=0}^{T} \\mathbb{E}_{\\mathcal{D}}\\left[\\mathcal{D}\\left(\\operatorname{ECE}\\mathcal{C}\\left[\\nabla_{\\theta} \\mathcal{L}_{\\mathcal{D}_{real}}\\left(\\theta_{t}\\right)\\right], \\operatorname{ECE}\\mathcal{C}\\left[\\nabla_{\\theta} \\mathcal{L}_{\\mathcal{D}_{syn}}\\left(\\theta_{t}\\right)\\right]\\right]\\right]\\right]$\n(3)\nTrajectory matching (TM) based approaches aim to match the training trajectories of surrogate\nmodels by optimizing over both the real dataset $\\mathcal{D}_{real}$ and the synthesized dataset $\\mathcal{D}_{syn}$. TM-based\nmethods were initially proposed in MTT [3]. Let $\\tau_{\\mathcal{D}_{real}}$ denote the expert training trajectories,\nrepresented as a sequential array of parameters $\\{\\theta_{t}^{\\mathcal{D}_{real}}\\}_{t=0}^{f}$, obtained from training a network on the\nreal dataset $\\mathcal{D}_{real}$. In parallel, $\\theta_{t}^{\\mathcal{D}_{syn}}$ refers to the parameter set of the network trained on $\\mathcal{D}_{syn}$ at step\n$t$. In each iteration, parameters $\\theta_{t}^{\\mathcal{D}_{real}}$ and $\\theta_{t}^{\\mathcal{D}_{syn}}$ are randomly selected from the expert trajectory\npool $\\{\\tau_{\\mathcal{D}_{real}}\\}$, serving as the initial and target parameters for trajectory alignment, where $M$ is a\npredetermined hyperparameter. TM-based methods enhance the synthetic dataset $\\mathcal{D}_{syn}$ by minimizing\nthe loss defined as:"}, {"title": "3 Method", "content": "3.1 A Closer Look at Sample Difficulty\nIn this subsection, we aim to intuitively understand dataset distillation through the concept of sample\ndifficulty (Definition 1), which is pivotal in data-centric AI [44, 31, 30, 5, 49, 9, 26]. We begin by\nempirically observing the evolution of sample difficulty during the distillation process. Firstly, we\nintroduce the commonly used definition of sample difficulty, namely the GraDN score (Definition 2),\nand validate the reliability of this metric. Furthermore, we track the GraDN score across current\ndataset distillation methods to delve deeper into their underlying mechanisms.\nDefinition 1 (Sample Difficulty [33]). Given a training pair $(x, y)$ and a series of pretrained models\nat training time $t$, the sample difficulty, denoted $x(x, y; \\Theta_{t})$, is defined as the expected probability of\n$(x, y)$ being misclassified by an ensemble of models $\\Theta_{t} \\in \\Theta_{t}$. Formally, it is presented as:\n$\\chi(x, y; \\Theta_{t})=\\mathbb{E}_{\\theta_{t} \\in \\Theta_{t}}\\left[\\mathbf{1}\\left(y \\neq \\theta_{t}(x)\\right)\\right]$,\n(6)\nwhere $\\mathbf{1}(z)$ is an indicator function that equals 1 if the boolean input $z$ is true, and 0 otherwise. In\nthis case, the indicator function equals to 1 if the sample $(x, y)$ is misclassified by the model with\nparameters $\\theta_{t}$, and 0 otherwise.\nDefinition 2 (GraDN Score [37]). Consider a training pair $(x, y)$, with $\\mathcal{L}$ representing the loss\nfunction. At time $t$, the GraDN score for $(x, y)$ is calculated as the average gradient norm of the loss\n$\\mathcal{L}$ across a diverse ensemble of models with parameters $\\Theta_{t} \\in \\Theta_{t}$:\n$\\operatorname{GraDN}(x, y; t)=\\mathbb{E}_{\\theta_{t} \\in \\Theta_{t}}\\left[\\left\\|\\nabla_{\\theta} \\mathcal{L}\\left(x, y; \\theta_{t}\\right)\\right\\|_{2}\\right]$,\n(7)\nwhere $\\nabla_{\\theta} \\mathcal{L}(x, y; \\theta_{t})$ denotes the gradient of loss $\\mathcal{L}$ on sample $(x, y)$ w.r.t. the model parameters $\\theta_{t}$,\nand $\\left\\|\\right\\|_{2}$ denotes $L_{2}$ norm."}, {"title": "3.2 An Analytical Theory for Explaining Matching-based Dataset Distillation", "content": "In Section 3.1, we empirically observed distinct trends in sample difficulty across various dataset\ndistillation methods. Here, we propose an analytical theory based on the neural scaling law to\nformally analyze sample difficulty in matching-based methods. We extend the theory of data pruning\npresented by [42] and validate its applicability within the context of DD using an expert-student\nperceptron model. Unlike data pruning, where the pruned dataset is directly selected from the original\ndataset, DD involves synthesizing a small, new, unseen dataset.\nWe start our analysis with tools from statistical mechanics [35]. Let us consider a classification\nproblem in dataset $\\mathcal{D}_{real}$ containing $d_{real}$ samples $\\{X_{i}, Y_{i}\\}_{i=1, \\ldots, d_{real}}$, where $x_{i} \\in \\mathbb{R}^{d} \\sim \\mathcal{N}\\left(0, I_{d}\\right)$\nare i.i.d. zero-mean, unit variance Gaussian inputs, and $y_{i}=\\operatorname{sign}\\left(\\Theta_{\\mathcal{D}_{real}} x_{i}\\right) \\in\\{-1,+1\\}$ are labels\ngenerated by an expert perceptron $\\Theta_{\\mathcal{D}_{real}} \\in \\mathbb{R}^{d}$. Our analysis is within the high-dimensional statistics\nlimit, where $d, d_{real} \\rightarrow \\infty$ while maintaining the ratio of total training samples to parameters\n$a_{t o t}=d_{real} / d_{a t} \\mathrm{O}(1)$. The general distillation algorithm proceeds as follows:\n1. Train a student perceptron on $\\mathcal{D}_{real}$ for a few epochs to obtain weights $\\Theta_{\\text {probe }}$. The gap between can\nbe measured by the angle $\\gamma$ between the probe student $\\Theta_{\\text {probe }}$ and the expert $\\Theta_{\\text {real }}$. If $\\Theta_{\\text {probe }} \\approx \\Theta_{\\text {real }}$,\nwe denote the $\\Theta_{\\text {probe }}$ as a perfect probe $(\\gamma=0)$. Otherwise, in imperfect probe cases, $\\gamma \\neq 0$.\n2. Compute the margin $m_{i}=\\left(\\Theta_{\\text {probe }} \\cdot y_{i} x_{i}\\right)$ for each training example, categorizing large (small)\nmargins as easy (hard) samples."}, {"title": "3.3 Matching with Sample Difficulty Correction", "content": "Based on our theoretical analysis of matching-based dataset distillation, we propose a novel method\nto enhance existing techniques for synthesizing higher-quality distilled datasets. Although TM-based\nmethods have achieved relative success on current benchmark datasets, they do not explicitly consider\nsample difficulty, which could ensure higher synthetic dataset quality.\nA direct approach to impose constraints on sample difficulty is to calculate the gradient norm for each\nsample as a metric to determine its utility. Let us consider the case of GM-based methods. At step $t$,\na batch of real samples $B_{real} \\sim \\mathcal{D}_{real}$ of class $c \\in \\mathcal{C}$ is to be matched with the gradients of a synthetic\nbatch $B_{syn} \\sim \\mathcal{D}_{syn}$. To decide whether to utilize each sample in $B_{real}$, it is natural to compute the\ngradient norm of each sample and utilize those with a score smaller than a predefined threshold $\\tau$."}, {"title": "4 Experiments", "content": "4.1 Basic Settings\nDatasets and baselines. For GM-based methods, we followed previous works to conduct experiments\non MNIST [10], FashionMNIST [48], SVHN [36] datasets. We utilized current GM-based methods,\nincluding DC [52], DSA [51], and DSAC [25] as baselines. For TM-based methods, we followed the\nrecent papers to use CIFAR-10, CIFAR-100 [21], and Tiny ImageNet [23] datasets. We performed\nexperiments on current baselines including MTT [3], FTD [12], TESLA [8], and DATM [16]. We"}, {"title": "4.2 Main Results", "content": "GM-based methods on MNIST, FashionMNIST, and SVHN. As presented in Table 1, we report\nthe results of three GM-based methods applied to MNIST, FashionMNIST, and SVHN datasets. Each\nmethod was evaluated with IPC (images-per-class) values of 1, 10, and 50. Notably, adding SDC\nimproves the test accuracy of baseline methods across all datasets and IPC values, demonstrating\nthe effectiveness of our approach. Notably, adding SDC to the original method improved the test\naccuracy of DSA by 1.2% on the SVHN dataset with IPC = 1, and by 1% with IPC = 50. For DC on\nthe FashionMNIST dataset with IPC = 50, the test accuracy was increased by 1.1% with SDC. All\nhyperparameters are detailed in Table 4.\nTM-based methods on CIFAR-10/100 and Tiny ImageNet. As shown in Table 2, we present\nthe results of four TM-based methods trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. By\nincorporating the average gradient norm as a regularization term during matching with SDC, the\nresulting test accuracy was generally improved. Notably, employing SDC improved the test accuracy\nof FTD on CIFAR-10 by 1.2% with IPC = 10 and 1.1% with IPC = 50, and enhanced the test\naccuracy of DATM on Tiny ImageNet by 0.6%. For FTD, we used EMA (exponential moving\naverage) just as in the original method[12]. All hyperparameters are detailed in Tables 5, 6, 7, and 8.\nGeneralization performance to other architectures. We evaluated the generalizability of synthetic\ndatasets generated through distillation. Specifically, we used DSAC and DATM, which are current\nSOTA methods in GM-based and TM-based distillation, respectively. After distillation, the synthetic\ndatasets were assessed using various neural networks, including ResNet-18 [17], VGG-11 [41],\nAlexNet [22], LeNet [24] and MLP. As shown in Table 3, even though our synthetic datasets were\ndistilled using ConvNet, it generalizes well across most networks. Notably, for the experiment of\nDATM on CIFAR-10 with IPC = 1, employing SDC resulted in an accuracy improvement of 4.61%\nwhen using AlexNet. Employing SDC to DSAC led to an accuracy improvement of 0.9% on SVHN\nwith IPC = 10 when using MLP. Additional results can be found in Appendix C.1."}, {"title": "4.3 Further Discussions", "content": "Discussion of SDC coefficient \u03bb. The selection of the regularization coefficient A is pivotal for the\nquality of the distilled dataset. Our theory suggests that a larger A typically produces better synthetic\ndatasets for smaller IPC values. Ideally, for low IPC settings, it is better to employ a large A to\nstrongly penalize sample difficulty, whereas, for high IPC settings, the required A can be small or even\nclose to zero in extreme cases. For simplicity and to maintain consistency across different datasets\nand baseline methods, we have set x = 0.002 as the default value in most of our experiments. As\ndemonstrated in Figure 5, this choice of A aligns with the IPC values. Results for FTD and TESLA\nare based on CIFAR-10, results for DSA are based on SVHN, and results for DSAC are based on\nMNIST. Additionally, we further show that the choice of A is not sensitive in Appendix C.3.\nAdaptive sample difficulty correction by adaptively increasing \u5165 during distillation. While our\nSDC seeks simplicity in regularization, DATM [16] claims that the matching difficulty is increased\nthrough optimization. Inspired by their observation, we implemented a strategy where A increases\nprogressively throughout the matching phases. This method is designed to incrementally adjust the\nfocus from easier to more complex patterns. Inspired by their observation, we applied an Adaptive\nSample Difficulty Correction (ASDC) strategy in our experiments with a TM-based method on the\nCIFAR-100 with IPC = 1 and with a GM-based method on the FashionMNIST with IPC = 1. The A of\nDATM was initialized to 0.02 and logarithmically increased to 0.08 over 10,000 iterations and DSAC\nwas initialized to 0.002 and logarithmically increased to 0.008 over 10,000 steps. For DATM, we use\nmax test accuracy, while for DSAC, we use test accuracy. Experimental results of ASDC validate its\npotential to significantly enhance learning by finetuning regularization according to the complexity\nof the learned patterns. Figure 6 illustrates that ASDC further improves our method within SOTA\nmatching methods. Additional results are provided in Appendix C.2."}, {"title": "5 Conclusion", "content": "In this study, we empirically examine the matching-based dataset distillation method in relation to\nsample difficulty, observing clear trends as measured by gradient norm. Additionally, we adapt a\nneural scaling law from data pruning to theoretically explain dataset distillation. Our theoretical\nanalysis suggests that for small synthetic datasets, the optimal approach is to generate data using\neasier samples from the original dataset rather than harder ones. To facilitate this, we propose\na simplicity-centric regularization method, termed Sample Difficulty Correction (SDC), aimed at\nimproving synthetic data quality by predominantly utilizing easier samples in the data generation\nprocess. This method can be easily incorporated to existing matching-based methods, and can be\nimplemented with a few lines of code. Experimental results underscore the importance of proper\nregularization within the optimization process for dataset distillation. We anticipate that this work\nwill deepen the theoretical understanding of dataset distillation."}]}