{"title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization", "authors": ["Yihan Du", "Anna Winnicki", "Gal Dalal", "Shie Mannor", "R. Srikant"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes\nwhile relying on a small amount of human feedback. However, there is limited theoretical justification for\nthis phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent\nempirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on\npolicy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient\n(PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the\nreward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer\nthe reward function. We provide performance bounds for PO-RLHF with low query complexity, which\nprovides insight into why a small amount of human feedback may be sufficient to get good performance\nwith RLHF. A key novelty is our trajectory-level elliptical potential analysis technique used to infer\nreward function parameters when comparison queries rather than reward observations are used. We\nprovide and analyze algorithms in two settings: linear and neural function approximation, PG-RLHF and\nNN-PG-RLHF, respectively.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) Sutton & Barto [2018], Agarwal et al. [2021] is a classic sequential decision-\nmaking problem where an agent interacts with an unknown environment in order to maximize the expected\ncumulative reward. In many applications, e.g., robotics and Large Language Models (LLMs) Ouyang et al.\n[2022], Achiam et al. [2023], the goal of the agent is complex and related to human evaluation. Additionally,\nthe reward function may be hard to manually design.\nTo handle these challenges, a framework called Reinforcement Learning from Human Feedback (RLHF) Christiano et al.\n[2017] has been proposed and has achieved huge empirical successes in ChatGPT Achiam et al. [2023]. In\nRLHF, the agent does not directly observe rewards, but has access to queries from humans on preferences\nbased on trajectories. The agent learns the quality of trajectories (policies) from the preference feedback\nover time in order to optimize performance. Existing empirical works have demonstrated the practical effi-\nciency of RLHF: human feedback can solve complex RL tasks by using fewer than 1% of the data from the\nagent's interactions with the environment Christiano et al. [2017].\nRecently, there have also been a number of theoretical RL papers which seek to provide analyze RLHF,\ne.g., Pacchiano et al. [2021], Chen et al. [2022], Zhu et al. [2023], Wang et al. [2023]. Most of these works"}, {"title": "2 Related Work", "content": "In this section, we discuss works that are most closely related to ours, and defer a detailed review to\nAppendix A.\nRLHF Christiano et al. [2017] has shown great empirical successes, especially in LLMs Ouyang et al.\n[2022], Achiam et al. [2023]. Recently, a number of works have started to theoretically analyze RLHF.\nXu et al. [2020], Novoseller et al. [2020], Pacchiano et al. [2021] study online RLHF for tabular MDPs.\nChen et al. [2022], Wang et al. [2023] consider online RLHF with general function approximation. Wang et al.\n[2023] design a reduction framework for RLHF, and prove that the sample complexity for RLHF is no higher\nthan that for standard RL. Zhu et al. [2023], Zhan et al. [2023a] study offline RLHF with function approxi-\nmation. Ji et al. [2023] seek to understand the empirical success of RLHF from the perspective of intrinsic\ndata bias.\nDifferent from the above works which mostly consider value-based algorithms, we analyze policy gradient\nRLHF algorithms with exploration, and show that the amount of data needed to implement RLHF is a small\nfraction of the amount of data needed to train an RL algorithm.\nOur work is also related to prior neural RL works, e.g., Cai et al. [2019], Wang et al. [2019], Xu et al.\n[2021], which theoretically analyze neural function approximation."}, {"title": "3 Formulation", "content": "In this section, we formally define the PO-RLHF problem.\nWe consider a discounted MDP M(S, A, r, P, \u03b3, Sinit). Specifically, S is the state space, and A is the\naction space. r : S \u00d7 A \u2194 [0,1] is an underlying reward function, so that r(s, a) specifies the reward of\ntaking action a in state s. In the RLHF setting, the agent cannot directly observer(s, a), and instead,\ncan only observe comparison feedback between trajectories generated according to r (detailed shortly).\nP : S \u00d7 A \u2194 \u2206s is an unknown transition distribution, and P(s'|s, a) gives the transition probability of\ntransitioning to s' if action a is taken in state s. Here for any set X, \u2206x denotes the space of all distributions\nover X, and \u03b3\u2208 [0,1) is a discount factor. We define a policy as a mapping \u3160 : S \u2194 \u2206a which specifies\nwhat action to take in a state.\nLet the state at steph be denoted by sh, and the action taken at step h be denoted by ah. The value\nfunction\n$V^{\\pi}(s) := E_{\\varsigma,\\pi} \\left[ \\sum_{h=0}^{\\infty} \\gamma^h r(s_h, a_h) | s_0 = s \\right],$ \nand the state-action value function\n$Q^{\\pi}(s, a) := E_{\\varsigma,\\pi} \\left[ \\sum_{h=0}^{\\infty} \\gamma^h r(s_h, a_h) | s_0 = s, a_0 = a \\right],$ \ndenote the expected sum of discounted rewards received under policy \u3160, starting from a given state s and\nstate-action pair (s, a), respectively. We define the optimal policy as \u03c0* := argmax\u03c0 V\u03c0 (Sinit).\nThe RLHF model is as follows. The agent starts from an initial state sinit. At each step h, the agent\nfirst observes the current state sh, and then takes an action ah according to her policy. After that, she\nobtains an underlying reward r(s, a) (not observed), and transitions to a next state sh+1 ~ P(\u00b7|sh, ah). The\nagent can choose to terminate the current trajectory with probability 1 y and restart from sinit at each\nstep. The agent can query humans to compare trajectories \u03c4(1) and \u03c4(2), and observe preference feedback"}, {"title": "3.1 Linear Function Approximation", "content": "In the linear setting, we consider the log-linear policy parameterization and linear reward function. Specifi-\ncally, there exists a known feature mapping \u03c6 : S \u00d7 A \u2192 Rd which specifies the feature vectors of state-action\npairs, and satisfies ||\u03c6(s, a)|| \u2264 1 for all (s, a) \u2208 S \u00d7 A. For parameter w \u2208 Rd, the log-linear policy is repre-\nsented as\n$\\pi_\\omega(a|s) := \\frac{\\exp(\\phi(s, a)^T \\omega)}{\\sum_{a' \\in A} \\exp(\\phi(s, a')^T \\omega)} .$\nWe make the following assumption on the reward function.\nAssumption 3.1 (Linear Reward Function). There exists some reward parameter \u03bc* \u2208 Rd such that\nr(s,a) := \u03c6(s,a)T\u03bc*."}, {"title": "3.2 Neural Function Approximation", "content": "In the neural function approximation setting, we parameterize the policy, value function and reward by\nneural networks.\nA two-layer ReLU neural network with input feature \u03c6(s, a), parameter w and width m is represented\nby Cai et al. [2019], Xu et al. [2021]\n$f(s, a; w) = \\frac{1}{m} \\sum_{l=1}^{m} b_l \\mathbb{I}\\{\\phi(s, a)^T [w]_l > 0\\} \\phi(s, a)^T [w]_l,$\nwhere b := [b1,..., bm]T \u2208 Rm, and w := [[w]1; . . . ; [w]m] \u2208 Rmd are the network parameters.\nWe initialize the parameters by bl ~ Unif([-1,1]) and [w\u00b0]l ~ Dinit for any l\u2208 [m]. Here Dinit is\nan initialization distribution, such that for any w' \u2208 Rd in the support of Dinit, c \u2264 ||w'||2 \u2264 c for some\nconstants c, c > 0. During training, we keep b fixed and only update w.\nWith a temperature parameter \u03b1 \u2208 R and a network parameter w \u2208 Rmd, a policy is represented by\n$\\pi_{\\alpha, \\omega} := \\frac{\\exp(\\alpha f(s, a; w))}{\\sum_{a' \\in A} \\exp(\\alpha f(s, a; w))}.$\nWe also use f (s, a; \u03b8) to approximate the state-action value function Q\u03c0 with another parameter \u03b8\u2208 Rmd\nand the same initialization as w, i.e., \u03b8\u00ba = w\u00b0.\nMoreover, we approximate the reward function r(s, a) by\n$h(s, a; \\mu) := \\frac{1}{m} \\sum_{l=1}^{m} b'_l \\mathbb{I}\\{\\phi(s, a)^T [\\mu]_l > 0\\} \\phi(s, a)^T [\\mu]_l,$\nwhere b' := [b'1,..., b'm]T \u2208 Rm and \u03bc := [[\u03bc]1; . . . ; [\u00b5]m] \u2208 Rmd are the reward network parameters. Simi-\nlarly, we initialize b'l ~ Unif([-1,1]) and [\u00b5\u00b0]l ~ Dinit for any l \u2208 [m], and only update \u03bc during training.\nFor any parameter \u03bc\u2208 Rmd and (s, a) \u2208 S \u00d7 A, let [\u03c8\u03bc(s, a)]l :=  1 {\u03c6(s,a)T [\u00b5]l > 0} \u03c6(s, a) \u2208 Rd\nfor any l \u2208 [m]. Let \u03c8\u03bc(s, a) := [[\u03c8\u03bc(s, a)]1; ... ; [\u03c8\u03bc(s,a)]m] \u2208 Rmd. We can similarly define \u03c8w(s, a).\nDefine a neural function class Rahimi & Recht [2007]:\n$\\mathcal{F}_{R,\\infty} := \\{h(s,a) = h(s,a; \\mu^\\circ) + \\int_{\\mathbb{R}^d}  \\mathbb{I}\\{\\phi(s,a)^T \\mu > 0\\}  \\phi(s, a)^T \\nu^\\mu (w) dp(w) : ||\\nu^\\mu (w)||_\\infty \\leq R\\},$\nwhere p: Rd \u2192 R is the density function of Dinit, and \u03bd\u03bc(w) : Rd \u2194 Rd together with h(s, a; \u03bc\u00ba)\nparameterize the element of FR,\u221e.\nIn the neural setting, we make the following assumptions.\nAssumption 3.2 (Neural Realizability of r). r \u2208 FR,\u221e.\nThis is a standard realizability assumption, and also made in prior neural RL works Wang et al. [2019],\nXu et al. [2021].\nAssumption 3.3 (Regularity of State-action Distribution). There exists an absolute constant Cscale \u2208 (0,1)\nsuch that for any v \u2208 Rd, x > 0, (s', a') \u2208 S \u00d7 A and policy \u03c0,\n$E_{(s,a)\\sim d^{\\pi}_{s',a'}} [\\mathbb{I}\\{\\phi(s,a)^Tv| \\leq x\\}] \\leq \\frac{C_{\\text{scale}} x}{\\|v\\|_2}.$\nThis is also a standard regularity assumption in the neural RL literature Cai et al. [2019], Wang et al.\n[2019], Xu et al. [2021]. For a random state-action pair (s, a) ~ d\u03c0, the probability of |\u03c6(s,a)Tv| \u2264 x\nscales with x and ||v||21."}, {"title": "3.3 Baseline Policy", "content": "We assume that we have access to a baseline policy, which will be used for comparison in our algorithms.\nFor any trajectory \u03c4 = (s0, a0,..., sH(\u03c4), aH(\u03c4)) and feature mapping x \u2208 {\u03c6, \u03c8\u03bc\u03bf }, let x(\u03c4) := \u03a3h=0x(sh, ah)."}, {"title": "4 PO-RLHF with Linear Function Approximation", "content": "We first study PO-RLHF with linear function approximation. We develop a policy gradient algorithm\nPG-RLHF which can explore the environment and adaptively collect human data."}, {"title": "4.1 Algorithm PG-RLHF", "content": "PG-RLHF builds upon the policy gradient algorithm PC-PG Agarwal et al. [2020] for standard RL. Our\nalgorithm is described in Algorithm 1. PG-RLHF runs N outer-loop phases for coverage update and reward\nestimation (Lines 2-13 in Algorithm 1), and T inner-loop iterations for policy optimization under given\ncoverage and reward model (Lines 6-15 in Algorithm 2). In each phase n, PG-RLHF first estimates the feature\ncovariance matrix Dov and updates the state-action coverage distribution pov, which is the average of the\nstate-action visitation distribution of all the policies \u03c0\u00ba,...,\u03c0\u03b7 used so far (Line 5 in Algorithm 1). Pov\nwill be the initial state-action distribution of the policy optimization in the inner-loop, and is gradually\nexpanded in each phase to improve the coverage.\nHuman Feedback Collection. Next, we collect human data for reward estimation. For any phase\nn\u2265 1, let OHF n be the distribution of the trajectory generated by starting from state-action distribution\n\u03c1\u03bf\u03bd, executing \u03c0\u00f1 and terminating with probability 1 \u2013 y at each step, where \u03c0\u00f1 ~ Unif([n]); For phase\nn = 0, OHF:= Onit (Line 6). In addition, let Orbase be the distribution of the trajectory generated"}, {"title": "4.2 Theoretical Guarantee of Algorithm PG-RLHF", "content": "Now we provide performance guarantees for algorithm PG-RLHF.\nFirst, following Agarwal et al. [2020], we define a bounded transfer function approximation error. Let\nbe the optimal solution to Eq. (3), and dimit (s, a) := dimit(s) 0 Unifa(a).\nAssumption 4.1 (Bounded Transfer Error). For any phase n > 0 and iteration t \u2265 0, there exists some\nEbias > 0 which satisfies"}, {"title": "4.3 Insight into the Practical Efficiency of RLHF", "content": "Below we compare our PG-RLHF and prior standard RL algorithm PC-PG Agarwal et al. [2020], and provide\nan insight behind the empirical success of RLHF.\nTable 1 shows that PG-RLHF needs additional \u00d5(N MH F ) samples due to the lack of direct reward signals.\nWe have O(MHF) \u2248 O(MSGD), since their convergence rates are the same (see Theorem 4.2). Then, the\nadditional samples needed by PG-RLHF is negligible compared to the total sample complexity. This implies\nthat RLHF does not introduce much hardness in terms of sample complexity, which matches the finding of\nrecent RLHF work Jin et al. [2020].\nRegarding the cost on reward observations, in standard RL, we require \u00d5((NK + NTMSGD) observations of true rewards. However, in RLHF, we do not need any observation of true rewards, but only\nuse O(NMHF) human queries. The ratio of the number of queries needed over the total sample complexity\nis about NTMSGD = y. This theoretically explains the empirical success of RLHF. RLHF only needs a\nsmall amount of comparison queries to achieve good performance as standard RL Christiano et al. [2017]."}, {"title": "5 PO-RLHF with Neural Function Approximation", "content": "In this section, we turn to the neural setting. We design an efficient algorithm NN-PG-RLHF, and derive a\nbiased MLE guarantee with neural approximation in analysis."}, {"title": "5.1 Algorithm NN-PG-RLHF", "content": "A detailed description and pseudo-code are provided in Appendix B. Here we provide a brief outline of the\nalgorithm. NN-PG-RLHF actively collects human data as exploration, learns a reward network with human"}, {"title": "6 Conclusion", "content": "In this work, we study exploration-driven policy optimization for RLHF. For the linear and neural function\napproximation settings, we propose efficient algorithms with active human data collection. Through the\ncomparison of results between RLHF and standard RL, we give a theoretical explanation for the query\nefficiency of RLHF. There is still a large space for future investigation. For example, it is interesting to\nexplore other potential reasons behind the success of RLHF, e.g., the structural advantage of preference\nfeedback over numerical feedback."}]}