{"title": "Towards Non-invasive and Personalized Management of Breast Cancer Patients from Multiparametric MRI via A Large Mixture-of-Modality-Experts Model", "authors": ["Luyang Luo", "Mingxiang Wu", "Mei Li", "Yi Xin", "Qiong Wang", "Varut Vardhanabhuti", "Winnie CW Chu", "Zhenhui Li", "Juan Zhou", "Pranav Rajpurkar", "Hao Chen"], "abstract": "Breast magnetic resonance imaging (MRI) is the imaging technique with the highest sensitivity for detecting breast cancer and is routinely used for women at high risk. Despite the comprehensive multiparametric protocol of breast MRI, existing artificial intelligence-based studies predominantly rely on single sequences and have limited validation. Here we report a large mixture-of-modality-experts model (MOME) that integrates multiparametric MRI information within a unified structure, offering a noninvasive method for personalized breast cancer management. We have curated the largest multiparametric breast MRI dataset, involving 5,205 patients from three hospitals in the north, southeast, and southwest of China, for the development and extensive evaluation of our model. MOME demonstrated accurate and robust identification of breast cancer. It achieved comparable performance for malignancy recognition to that of four senior radiologists and significantly outperformed a junior radiologist, with 0.913 AUROC, 0.948 AUPRC, 0.905 F1 score, and 0.723 MCC. Our findings suggest that MOME could reduce the need for biopsies in BI-RADS 4 patients with a ratio of 7.3%, classify triple-negative breast cancer with an AUROC of 0.709, and predict pathological complete response to neoadjuvant chemotherapy with an AUROC of 0.694. The model further supports scalable and interpretable inference, adapting to missing modalities and providing decision explanations by highlighting lesions and measuring modality contributions. MOME exemplifies a discriminative, robust, scalable, and interpretable multimodal model, paving the way for noninvasive, personalized management of breast cancer patients based on multiparametric breast imaging data.", "sections": [{"title": "Introduction", "content": "Breast cancer is the primary cause of cancer mortality in females worldwide [1]. Early detection and accurate, systematic treatment are crucial for reducing mortality, requiring precise malignancy screening and guidance by molecular subtyping and treatment response estimation [2], informed by breast cancer examinations and analytics. Breast magnetic resonance imaging (MRI) is the radiology technique with the highest sensitivity for breast cancer detection and plays an indispensable role in breast cancer screening and staging for high-risk women [3], holding promise as a non-invasive investigation approach. It is also recommended as a screening technique for women with dense breasts, a condition that is prevalent among women in Eastern populations, such as those in China [4]. Currently, reading breast MRI is majorly based on the American College of Radiology Breast Imaging Reporting and Data System (BI-RADS) [5], which requires comprehension of the information from multiparametric MRI data, routinely including the dynamic T1-weighted dynamic contrast-enhanced sequence (DCE-MRI), the T2-weighted imaging (T2WI), and the diffusion-weighted imaging (DWI) to improve differentiation of breast lesions.\nArtificial Intelligence (AI), typically deep learning [6], has shown remarkable progress in healthcare [7], including breast cancer imaging [8]. Despite the increasing diagnosis accuracy reported for AI-based breast cancer diagnosis, existing studies were mostly based on DCE-MRI [9-12], one single modality that often leads to high sensitivity with moderate specificity. However, the diagnosis and prognosis based on breast MRI is routinely a multiparametric process, and how other sequences may help in increasing the specificity and overall accuracy remains to be explore [13-16]. Apart from the challenges brought by the collection of the multi-sequence data, multimodal integration also faces technical difficulties raised by the heterogeneity and high dimensionality [17]. Specifically, different modalities often require tailored representation learners, and a meticulously designed fusion module is demanded for heterogeneous information interaction modeling. Such an architecture is absent in the literature on AI-based breast MRI analysis, and its clinical value remains to be further investigated.\nAs a recent generation of AI, foundation models (FMs) [18] were heralded as a promising solution to comprehend the heterogeneous multimodal information [19]. Particularly, FMs are developed with massive, diverse datasets and enable generality on multifaceted tasks by the large-scale pre-training paradigm [20, 21]. Moreover, the ability of FMs in unifying multimodal representations can be facilitated with a unified Transformer structure [22] by extensively modeling the cross-range dependencies among the input tokens. In medical image analysis, recent studies also demonstrated that these methods could match medical specialists' performance, such as chest X-rays [23, 24], pathology images [25-27], and transcriptomics [28]. Nevertheless, FMs often contain billions of parameters that need to be pre-trained from million-scale datasets, which is impractical for situations with less data. Under such circumstances, parameter-efficient fine-tuning provides the feasibility of leveraging the pre-training knowledge by adapting from existing FMs with a confined scale of trainable weights [29]. In spite of this, adapting the foundation model knowledge learned from 2D natural images for multiparametric MRI analysis encounters a significant domain gap, particularly with the increase in the number of modalities and dimensions.\nIn this study, we proposed a FM-based large mixture-of-modality-experts model (MOME) which inherits the long-range modeling capability of a transformer-based FM for multiparametric information fusion and could conduct flexibly inference with the design of mixture of experts (Fig. 1, supplementary Fig. 1). The model was developed and extensively evaluated based on the largest multiparametric breast MRI dataset collected from three hospitals in China, achieving comparable malignancy differentiating performance with National Health Commission (NHC)-qualified radiologists. With generality across data collected from the north, southeast, and southwest of China, the model exhibits its clinical value in decreasing unnecessary biopsies for BI-RADS 4 patients. Moreover, MOME could also conduct subtyping of triple-negative breast cancer and predicting of response to neoadjuvant chemotherapy. With these capabilities, we exemplified the clinical value of MOME in non-invasive, personalized management of breast cancer patient."}, {"title": "Results", "content": "Dataset characteristics\nThis study involved a total of 5,220 multiparametric breast MRI examinations from 5,205 patients, collected from three institutes across ten years (supplementary Fig. 2), that is, Dataset 1 (DS1, The Fifth Medical Center of Chinese PLA General Hospital), Dataset 2 (DS2, Shenzhen People's Hospital), and Dataset 3 (DS3, Yunnan Cancer Hospital). DS1 comprised 1,824 examinations took between November 2012 and July 2017. DS2 comprised 735 examinations collected between December 2018 and March 2022, and DS3 comprised 2,661 examinations obtained between November 2015 and October 2022. For malignancy classification, DS1 was split into training set (n=1,167) and validation set (n=150) for model development, internal testing set 1 (n=307) for evaluation, and internal testing set 2 (n=200) for comparison with radiologists. Different sets\nComparable to radiologists\nWe evaluated the malignancy diagnosis performance of MOME on the internal testing set 2 (n=200) and compared its performance with that of six radiologists (reader 1: less than five years of experience in breast MRI; readers 2 and 3: five to ten years of experience in breast MRI; readers 4, 5, and 6: more than ten years of experience in breast MRI; detailed performance can be found in supplementary Table 3)."}, {"title": "Generality across hospitals", "content": "MOME showed generalizable performance on differentiating malignancies from benign tumors across hospitals (Fig. 3). On the internal testing set 1 (n=307), the ROC (Fig. 3a) and PRC (Fig. 3c) analyses showed that MOME achieved 0.912 AUROC (95% CI 0.876, 0.942) and 0.942 AUPRC (95% CI 0.906, 0.970) on the internal testing set (n=307). Partial AUROC (pAUROC) at 90% sensitivity was 0.735 (95% CI 0.666. 0.811) and 0.800 (0.706 to 0.880) at 90% specificity (Fig. 3b). These results indicate that MOME identifies breast cancer patients with a high degree of accuracy.\nExternal validation was conducted on DS2 and DS3. Except for different breast MRI protocols (Methods section), DS2 and DS3 also possessed different demographics and different distributions of malignant cases compared to the internal dataset. Specifically, on DS2, MOME achieved 0.899 AUROC (95% CI 0.877, 0.922) and 0.887 AUPRC (95% CI 0.847, 0.923). pAUROC at 90% sensitivity was 0.740 (95% CI 0.695. 0.788) and 0.753 (95% CI: 0.698, 0.805) at 90% specificity. On DS3, MOME achieved 0.806 AUROC (95% CI: 0.790, 0.822) and 0.807 AUPRC (95% CI: 0.785, 0.827). pAUROC at 90% sensitivity was 0.617 (95% CI 0.594, 0.642) and 0.621 (95% CI: 0.600, 0.643) at 90% specificity. More detailed performance can be found in supplementary Table 5. These results reveal that MOME is discriminative and robustness."}, {"title": "Ablation investigation on modules and missing sequences", "content": "To investigate the influence of different modules, we developed different variants of MOME by removing each single component (ablation section in Methods) and compared their performance on the DS1 internal testing set 1. As can be observed from Table 1, removing all of the modality experts led to performance drop in all of the metrics (rows 1). On the metrics of AUROC and AUPRC, this variants had decreases of 1.0% and 1.1%, respectively, compared to those of MOME. We then replaced the soft mixture of modality experts with a MLP adapter (row 2), leading to performance drops of 2.3% for AUROC and 3.0% for AUPRC compared to those of"}, {"title": "Model decision interpretation", "content": "MOME is interpretable in highlighting the lesions and analyzing the contribution of each modality. Using integrated gradient [30], it can be observed that that MOME correctly attended to breast lesions when diagnosing"}, {"title": "Personalized management", "content": "MOME can be used in clinical process to improve personalized management for breast cancer patients. We first analyze the net benefit of using MOME to detect patient with malignancies. The decision curve analysis on DS1 internal testing set 1, DS1 internal testing set 2, DS2, and DS3 (Figs. 6a, b, c, d, respectively) indicate high net benefit across long ranges of preference thresholds, demonstrating its potential for decision support in screening malignancies.\nMOME can also reduce unnecessary biopsy for BI-RADS 4 patients. We investigated the trade-off between the number of correctly downgraded cases and the true positive rate by varying operating points of MOME. Based on the operating point found on DS1 and corresponding results on DS2, we found that at an operating point that identified 7.3% of BI-RADS 4 patients with benign tumors (8 out of 109 patients), no cancer would be missed with biopsy (n=86). By decision curve analysis (Fig. 6g), MOME also achieved higher net benefit across a long range of threshold probability, compared to the biopsy all strategy that commonly leveraged for BI-RADS 4 patients. These results demonstrated a high preference for using MOME for personalized biopsy recommendation for BI-RADS 4 patients.\nWe also investigated MOME for triple-negative breast cancer (TNBC) subtyping (Fig. 6f) and NACT pathologic complete response (pCR) prediction (Fig. 6g). Based on five-fold cross-validation, MOME achieved an AUROC of 0.709\u00b10.067 for TNBC subtyping (n=1,005) and an AUROC of 0.694\u00b10.029 for NACT response prediction (n=358). To note, TNBC patients are also more likely to achieve PCR after NACT.\nThese results showed that MOME has the potential to facilitate non-invasive personalized breast cancer patient management for malignancy screening, biopsy recommendation, and treatment decision support."}, {"title": "Discussion", "content": "The purpose of this work was to show MOME with high differential ability for multiparametric breast MRI analytics with a multiparametric, large-scale, and multi-center study. Typically, the management of breast cancer patients requires a series of examinations, such as mammography, ultrasound, MRI, histopathology, serology, genomics, and more. Personalized management would bring large benefit, yet it depends on accurate malignancy detection, molecular subtyping, and the patients' estimated response to different therapies. We have shown that MOME could accurately and robustly distinguish patients with malignancies from benign or normal subjects with evaluation across multiple hospitals. Compared to NHC-qualified radiologists, MOME showed no evidence of statistical differences with the performance of four out of six human experts and statistically significantly higher performance than the junior radiologists with less than five years of breast MRI experience. Typically, a biopsy is often suggested for a patient with BI-RADS 4 or above after a breast MRI examination, whereas MOME can be used to further characterize BI-RADS 4 patients into \"likely benign\" or \"likely malignant\" lesions, hence alleviate the need of biopsy for the former subgroup. Moreover, MOME was shown capable of finding TNBC patients who usually have better responses to NACT with an AUROC of 0.709. Using the pretreatment multiparametric MRI, we also found that the model could achieve 0.694 AUROC for NACT response prediction. These capabilities demonstrated MOME's clinical value in biopsy recommendation and treatment decision, thus facilitating efficient, non-invasive, and personalized breast cancer patient management.\nCompared to the previous studies, this work presents a more extensive evaluation of a multiparametric deep learning model with large-scale, diverse external assessment for malignancy classification. Specifically for breast MRI malignancy classification, an AUROC of 0.859 was reported in a single-center study on DCE-MRI from containing 1537 female cases [9]. Later, in a multi-center single-sequence study [11] involving 2,2984 DCE-MRI cases, AUROC of 0.92 was reported on its internal testing set. This model was found to achieve near-perfect performance (AUROCs were above 0.965) on two external datasets (n=922 and 131, respectively) with only invasive breast cancer while exhibiting a lower AUROC of 0.797 on a smaller yet more challenging external set (n=394). These results were limited to single-sequence MRI, that is, DCE-MRI, and AI-based integration of multiparametric breast MRI is less investigated. Meanwhile, extensive external evaluation was also recommended to comprehensively assess the model's generality [32, 33].\nThis study collected the largest multiparametric breast MRI dataset, representing typical populations from the north, southwest, and southeast of China, across a ten-year period. DS1, DS2 and DS3 were obtained with diverse imaging protocols, such as different scanning matrices, DWI b values, numbers of DCE-MRI sequences, and different demographics. Although the metrics on DS3 did not match those on DS1 and DS2, we note that DS3 possessed larger data shifts in terms of imaging protocol and cohort size. MOME achieved high performance to the two external datasets and performed consistently across different subgroups, demonstrating its generality. These findings demonstrate the ability of MOME to integrate high-dimensional multiparametric information in clinical settings.\nMethodologically, MOME provides a novel approach to adapting the powerful foundation models from the natural image domain to a more complex dimension, that is, the three-dimensional, multimodal breast MRI with temporal information in DCE. In specific, different breast MRI sequences provide different diagnostic information and result in varied breast cancer differentiation abilities, as observed in Table 4, which explains the difficulty of multiparametric fusion and inference, as other multimodal approaches may not necessarily outperform the pure DCE-MRI-based model. MOME had most of the parameters shared for each modality, which mimicked a siamese network structure [34] that can effectively model the similarity and differences among the inputs. Moreover, the first nine layers utilized sparse modality experts and inter-modality self-attention to adequately extract the modality-specific features. Then, the last few layers adopted the soft mixture of experts and applied self-attention for the holistic multimodal tokens to encourage cross-modal interaction. The combination of these characteristics finally led to the improved diagnostic accuracy of MOME. In addition, MOME offered flexible and explainable inference. The structural design enables MOME to infer with missing sequence(s), which is highly scalable for incomplete imaging protocols. MOME also exhibits inherent interpretability by providing pixel-level contributions on the input images and showing highlights of the lesions of interest for malignancy detection. Furthermore, our model provides a way for modality contribution investigation based on Shapley value with its support for missing-modal inference, which allows us to understand the decision-making in terms of modality contributions both locally and globally.\nWe acknowledge several limitations of the current study. First, we focused on multiparametric MRI, whereas other data, such as mammograms, breast ultrasounds, health records, and demographics, may also be generated during the clinical process and can provide extra insights into the patient's status. Our future work will scale the study with more available modalities using the proposed unified model structure. Second, our comparison with radiologists treated the AI system as a stand-alone reader. It would be of great value to see how the AI interpretation can affect the reader's decision and explore the model's role in real-world clinical settings. Also,"}, {"title": "Methods", "content": "Ethics approval\nAll datasets were collected under institutional review board approval (KYLX2023-163). All data were de-identified before the development of the model.\nMRI acquisition\nFor DS1, the magnetic resonance imaging (MRI) scans were performed on a 1.5T system (Magnetom Espree Pink; Siemens, Munich, Germany) with an 8-channel breast coil. Patients were positioned prone, with both breasts naturally aligned within the coil. Imaging included conventional scans: an axial T1-weighted 3D non-fat-suppressed sequence (TR/TE: 8.7/4.7 ms, matrix: 896\u00d7896, slice thickness: 1 mm), T2-weighted fat suppression (TR/TE: 2900/60 ms, matrix: 640\u00d7640, slice thickness: 4 mm), and diffusion-weighted imaging (DWI) with b-values of 400, 800, and 1000 s/mm\u00b2 (TR/TE: 6200/104 ms, matrix: 236x120, slice thickness: 4 mm). Dynamic contrast-enhanced MRI was conducted using a 3D fat-suppressed VIBE sequence before and 6 times after bolus injection (0.1 mmol/kg gadopentetate dimeglumine, Magnevist, Bayer, Berlin, Germany) at 2 mL/s, followed by a 20-mL saline flush. The examination spanned 7 minutes, with imaging parameters of TR/TE: 4.53/1.66 ms, matrix: 384\u00d7384, and slice thickness: 1.0 mm. Images of each phase were subtracted automatically.\nFor DS2, breast MRI examinations were performed using either a 1.5T Magnetom Avanto or 3.0T Magnetom Skyra magnetic resonance scanner (Siemens Healthineers, Erlangen, Germany) equipped with a dedicated breast coil. Patients were examined in the prone position. The scanning included the following sequences: axial T1-weighted non-fat suppressed images were acquired with TR/TE parameters of 559/12 ms (1.5T) and 6/2.5 ms (3.0T), a matrix of 448 \u00d7 448, and slice thickness of 4 mm (1.5T) and 1.6 mm (3.0T); axial T2-weighted images were obtained with TR/TE of 4500/102 ms (1.5T) and 4740/107 ms (3.0T), a matrix of 512 \u00d7 512 (1.5T) and 448 \u00d7 448 (3.0T), and slice thickness of 4 mm. A single-shot echo planar imaging pulse sequence was used to acquire diffusion-weighted images with the following parameters: TR/TE of 6400/97 ms (1.5T) and 5700/59 ms (3.0T), a matrix of 192 \u00d7 192 (1.5T) and 340 \u00d7 170 (3.0T), slice thickness of 4 mm, and b-values of 50/500/1000 s/mm2 (1.5T) and 50/400/800 s/mm\u00b2 (3.0T); DCE-MRI was performed during intravenous injection of 15 ml Gd-DTPA at 0.1 mmol/kg over 6 minutes and 41 seconds at a rate of 2.5 ml/s. The sequence included one pre-contrast axial image and five post-contrast axial scans spaced 30 seconds apart. DCE parameters were: TR/TE of 5.2/2.4 ms (1.5T) and 4.7/1.7 ms (3.0T), a matrix of 384 \u00d7 384 (1.5T) and 448 \u00d7 448 (3.0T), and slice thickness of 1.1 mm (1.5T) and 1.6 mm (3.0T). Images from each phase were automatically subtracted.\nFor DS3, breast MRI was performed on a 1.5T system (Magnetom Avanto; Siemens, Germany), equipped with an 4-channel breast phased array surface coil. Patients were examined in the prone position. The scanning steps are as follows: axial T1WI fast low angle shot 3D, flash 3D sequence (TR 8.6 ms, TE 4.7 ms, slice thickness 1 mm); Fat-suppressed transverse axial T2WI rapid inversion recovery (Turbo Inversion Recovery Magnitude, TIRM) sequence (TR 5600 ms, TE 56 ms, slice thickness 4 mm) scan; Axial diffusion-weighted imaging uses single shot echo plannar imaging (SS-EPI) sequence (TR 4900 ms, TE 84 ms, FOV 340 mm, slice thickness 4mm, the diffusion sensitivity factor b value is selected to be 0 s/mm\u00b2 and 800 s/mm\u00b2). Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) scan in transverse position: first scan the first-stage transverse position fat-suppressed T1WI (i.e., masked film), then inject contrast agent, and then continuous scanning of 5 continuous dynamic enhancement sequences, each period is 60 seconds. Twenty ml of Gd-DTPA-BMA (OmniScan, GE Healthcare, Ireland) was injected at a rate of 2.0 ml/s and then flushed with 20 ml of saline. Parameters of DCE-MRI were: TR 4.43 ms, TE 1.5 ms, matrix 352 \u00d7 324, slice thickness 1.7 mm. Images of each phase were subtracted automatically."}, {"title": "Data preprocessing", "content": "We generated three-dimensional breast region masks from T1-weighted fat-suppressed magnetic resonance images, following Zhou et al. [9]. Specifically, two-dimensional binary breast masks were obtained for each MR image slice by extracting boundaries and applying morphological processing methods. Then, all the two-dimensional masks were stacked to create a three-dimensional mask, which was smoothed using a 3D Gaussian filter (\u03b3 = 20). The obtained 3D masks were used to crop the MRI scans and mask out the air and chest regions. The main purpose of the breast mask was to reduce the input data dimensions.\nFor DS1, we utilized the 3D mask to crop all DCE-MRI subtraction sequences, resizing both DCE-MRI and the corresponding 3D mask to 384\u00d7384\u00d7128. The breast region in DCE-MRI was then extracted and normalized, followed by element-wise multiplication with the 3D mask. For T2 images, they were first resized to 384\u00d7384\u00d732 and then the breast area was cropped, with standardization (linear scaling to zero mean and unit variance) applied subsequently. For DWI, the sequence with the highest b value (1000 or 800) was used, and only standardization was performed for pre-processing. During training and testing, DCE-MRI, T2WI, and DWI was padded to 384 \u00d7 256 \u00d7 128, 384 \u00d7 256 \u00d7 48,and 256 \u00d7 128 \u00d7 32, respectively.\nFor DS2 and DS3, the procedures were similar to DS1 except that their DCE-MRI had 5 phases and were interpolated to 6 phases using first-order B-spline interpolation with a grid-constant mode."}, {"title": "Groundtruth", "content": "The malignant or benign labels for all patients from DS1, DS2, and DS3 were confirmed by histopathological examination. In DS1, 365 patients with breast cancer confirmed by histopathology underwent NACT. One cycle of NACT lasted for 21 days. After the second cycle, clinicians evaluated the response and tolerability of NACT for each patient. All patients underwent MRI scans before treatment and at least 2 follow-up studies. All patients underwent definitive surgery after the final cycle of treatment. For confirmed breast cancer, molecular subtypes were determined based on the Chinese Anti-Cancer Association and the immuno-histochemical results in the histopathological reports were analyzed by pathologists. The estrogen receptor (ER), progesterone receptor (PR), HER2 status, and Ki-67 index were used to define the molecular subtypes. Estrogen receptor and PR positivity were defined as more than 1% staining [35]. HER-2 positivity was defined as a score of 3+ by IHC or fluorescence in situ hybridization amplification with a score of 2+ or higher [35]. TNBC was determined by ER negative, PR negative, and HER2 negative."}, {"title": "MOME", "content": "MOME presents a unified, easily-extendable structure for multimodal data integration, such as multiparametric breast MRI. The input data were first embedded into features using different tokenizers and concatenated as input to a transformer architecture adapted from a foundation model.\nEach MRI sequence leveraged a tokenizer module with the same structure to embed the input into tokens, which was a sequence of embedded features, denoting as XDCE, XDWI, and XT2. The tokenizer contained three 3D convolutional layers (stride = 2) and ended with a maxpooling layer (stride = 2). Each convolution layer was appended with an instance normalization [36] layer and a ReLU layer [37]. The tokenizer downsampled the width, height, and number of slices of the MRI input with a scale of 1/16 and generated a feature map with 768 feature dimensions. The obtained feature maps were then flattened at the width, height, and slice dimension to form the input as a sequence. For example, the DCE-MRI with original shape of R384\u00d7256\u00d7128\u00d76 would be processed into R24\u00d716\u00d78\u00d7768 and flattened to XDCE \u2208 R3072\u00d7768. A CLS token XCLS \u2208 R1\u00d7768 was appended to XDCE and would be used for final classification. Finally, a learnable 1d positional embedding was added to the input tokens.\nThe transformer structure of MOME was adapted from BEiT3 [21]. BEiT3 was originally a vision-language foundation model with 276 million parameters and was pre-trained from 21 million image-text pairs, 14 million images, and 160 gigabytes of documents. The model contains twelve transformer blocks with the same structure for feature encoding, of which the process can be formulated as follows:\n$Z' = X^\\ell + \\text{MSA}((LN(X^\\ell)))$,\n$X^{\\ell+1} = Z' + \\text{FFN}(LN(Z'))$,\nwhere X is the input to the l-th block, MSA stands for the multi-head self-attention, FFN represents a feed-forward network with two linear projection layers, LF means layer norm [38], and $X^{\\ell+1}$ is the output and will be fed to the 1 + 1-th transformer block.\nBased on the foundation model, BEiT3, we further introduced the mixture of modality experts (MOME)to enable multimodal learning and fusion. Specifically, we fixed the pre-trained parameters of BEiT3 and injected"}, {"title": "simple trainable modules. We set the first k layers to learn from each different modality (that is, each different MRI sequence) by adding the sparse mixture of modality experts into the transformer blocks, which can be formulated as follows:", "content": "$Z'_i = X_i + \\text{MSA}(*(LN*(X_i)))$,\n$X_i^{\\ell+1} = Z'_i + \\text{FFN}(LN*(Z'_i)) + \\text{MOME}_{sparse}(LN*(Z'_i))$,\nwhere * means that the parameters of the module were loaded from pre-trained BEiT3 and fixed during training, and i is an index. In particular, X is the i-th feature from the set {XDCE,i, XDWI,i, XT2,i}, and MOMESparse is the i-th sparse MOME. Following the structure proposed by Yang et al. [39], MOMESparse takes the structure of a feed-forward structure with two layers of linear projections followed by a layer norm:\n$\\text{MOME}_{sparse}(X) = LN(LP(\\text{GELU}(LP(X))))$,\nwhere LP stands for a linear project layer, and GELU means the Gaussian Error Linear Units [40]. In this way, each sparse MOME would learn to encode one specific type of input sequence while most of the transformer block parameters were fixed and shared for each modality.\nThe original BEiT3 has no knowledge of fusing the multiparametric MRI information. Therefore, the multimodal fusion was conducted by the last 12-k transformer blocks using soft MOME, that is, MOMESoft. Here, the embedded features were concatenated to be the input to the adapted foundation model, denoted as X = [XDCE, XDWI, XT2], and a transformer block is formulated as follows:\n$Z' = X^\\ell + \\text{MSA}(*(LN*(X^\\ell)))$,\n$X^{\\ell+1} = Z' + \\text{FFN}(LN*(Z')) + \\text{MOME}_{Soft}(LN*(Z^\\ell))$,\nand the formulation of MOMESoft can be further elaborated as follows:\n$\\text{MOME}_{Soft}(X) = LN(LP(\\text{SMoE}(\\text{GELU}(LP(X)))))$,\nwhere SMoE is a soft mixture of expert (Soft MoE) [41]. Here, the linear projection layers were also anticipated to reduce and expand the feature dimensions, and SMOE was expected to learn to integrate the multimodal information.\nWithout loss of generation, let X \u2208 Rmxd be the input to SMOE, where m is the number of tokens of the input sequence data, and d is the feature dimension. In detail, soft MoE first obtains a set of slots \u03a6 that are linear combinations of all of the input tokens. The process can be further elaborated as follows:\n$D_{i,j} = \\frac{exp((X\\Phi)_{i,j})}{\\sum_{i'=1}^n exp((X\\Phi)_{i',j})}$,\n$\\tilde{X} = DX$,\nwhere \u03a6\u2208 Rd\u00d7(n.p) is a learnable linear projection. By the above, n\u00b7p slots were generated, and every p slots will be processed by an expert function, resulting in totally n experts. Let f represent the expert function, the rest process of soft MoE is to conduct expert function over the slots and map the slots back to tokens:\n$\\tilde{Y} = f[i/p] (\\tilde{X})$, \n$C_{i,j} = \\frac{exp((X\\Phi)_{i,j})}{\\sum_{i'=1}^n exp((X\\Phi)_{i',j})}$,\nY = CY,\nwhere Y\u2208 R(np)\u00d7d, and the expert function f is a linear projection function. The multiple expert functions were aimed to learn different fused features from the long sequence input generated from the multiparametric MRI. Then, the combination of experts acted as an ensemble learning strategy to improve the fusion results.\nAfter twelve transformer blocks, the CLS token was extracted and fed into a linear classification layer after layer norm. For inference with missing modalities, we simply removed the sparse expert corresponding to the missing sequence."}, {"title": "Compared Methods", "content": "Late Fusion took the average of the three unimodal models' outputs Feature Fusion concatenated the features from the unimodal models and generated output using a three-layer perception. The comparison with BEiT3 was to show that directly using the pre-trained parameters was not enough to utilize the foundation model's capability. We took the vision transformer part of BEiT3, fixed its parameters, and concatenated the multiparametric features to be the inputs, where the multimodal fusion was then conducted by the pre-trained self-attention modules inside BEiT3."}, {"title": "Implementation details", "content": "The first 9 transformer blocks of MOME were implemented with sparse MOME, and the last 3 transformer blocks of MOME were implemented with soft MOME. The number of experts used in soft MOME was set to 128, and each expert processed one input slot. Adam optimizer [42", "43": ".", "44": "with an NVIDIA GeForce RTX 3090 GPU.\nDuring training, data augmentation including random padding and random one-axis or two-axis flipping was adopted to enhance the data diversity. During inference, MOME generated patient-level prediction in the range of (0, 1)."}]}