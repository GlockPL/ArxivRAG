{"title": "Symbolic Parameter Learning in Probabilistic Answer Set Programming", "authors": ["Damiano Azzolini", "Elisabetta Gentili", "Fabrizio Riguzzi"], "abstract": "Parameter learning is a crucial task in the field of Statistical Relational Artificial Intelligence: given a probabilistic logic program and a set of observations in the form of interpretations, the goal is to learn the probabilities of the facts in the program such that the probabilities of the interpretations are maximized. In this paper, we propose two algorithms to solve such a task within the formalism of Probabilistic Answer Set Programming, both based on the extraction of symbolic equations representing the probabilities of the interpretations. The first solves the task using an off-the-shelf constrained optimization solver while the second is based on an implementation of the Expectation Maximization algorithm. Empirical results show that our proposals often outperform existing approaches based on projected answer set enumeration in terms of quality of the solution and in terms of execution time. The paper has been accepted at the ICLP2024 conference and is under consideration in Theory and Practice of Logic Programming (TPLP).", "sections": [{"title": "1 Introduction", "content": "Statistical Relational Artificial Intelligence (StarAI) (Raedt et al. 2016) is a subfield of Artificial Intelligence aiming at describing complex probabilistic domains with interpretable languages. Such languages are, for example, Markov Logic Networks (Richardson and Domingos 2006), Probabilistic Logic Programs (Riguzzi 2022; De Raedt et al. 2007), and Probabilistic Answer Set Programs (Cozman and Mau\u00e1 2020). Here, we focus on the last. Within StarAI, there are many problems that can be considered such as probabilistic inference, MAP inference, abduction, parameter learning, and structure learning. In particular, the task of parameter learning requires, given a probabilistic logic program and a set of observations (often called interpretations), tuning the probabilities of the probabilistic facts such that the likelihood of the observation is maximized. This is often solved by means of Expectation Maximization (EM) (Bellodi and Riguzzi 2013; Azzolini et al. 2024; Dries et al. 2015) or Gradient Descent (Gutmann et al. 2008).\nRecently, Azzolini and Riguzzi (2021) proposed to extract a symbolic equation for the probability of a query posed to a probabilistic logic program. From that equation, it is possible to model and solve complex constrained problems involving the probabilities of the facts (Azzolini 2023).\nIn this paper, we propose two algorithms to learn the parameters of probabilistic answer set programs. Both algorithms are based on the extraction of symbolic equations from a compact representation of the interpretations, but they differ in how they solve the problem. The first casts parameter learning as a nonlinear constrained optimization problem and leverages off-the-shelf solvers, thus bridging the area of probabilistic answer set programming with constrained optimization, while the second solves the problem using EM. Empirical results, also against an existing tool to solve the same task, on four different datasets with multiple configurations show the proposal based on constrained optimization is often significantly faster and more accurate w.r.t. the other approaches.\nThe paper is structured as follows: Section 2 discusses background knowledge, Section 3 proposes novel algorithms to solve the parameter learning task, that are tested in Section 5. Section 4 surveys related works and Section 6 concludes the paper."}, {"title": "2 Background", "content": "An answer set program is composed by a set of normal rules of the form\n$h:- b_0,..., b_m, not c_0,..., not c_n$\nwhere h, the $b_i$s, and the $c_i$s are atoms. h is called head while the conjunction of literals after the \u201c:-\u201d symbol is called body. A rule without a head is called constraint while a rule without a body is called fact. The semantics of an answer set program is based on the concept of stable model (Gelfond and Lifschitz 1988), often called answer set. The set of all possible ground atoms for a program P is called Herbrand base and denoted with $B_P$. The grounding of a program P is obtained by replacing variables with constants in $B_P$ in all possible ways. An interpretation is a subset of atoms of $B_P$ and it is called model if it satisfies all the groundings of P. An answer set I of P is a minimal model under set inclusion of the reduct of P w.r.t. I, where the reduct w.r.t. I is obtained by removing from P the rules whose body is false in I."}, {"title": "2.1 Probabilistic Answer Set Programming", "content": "We consider the Credal Semantics (CS) (Cozman and Mau\u00e1 2016; Mau\u00e1 and Cozman 2020; Lukasiewicz 2005) that associates a meaning to Answer Set Programs extended with probabilistic facts (De Raedt et al. 2007) of the form $p :: a$ where p is the probability associated with the atom a. Intuitively, such notation means that the fact a is present in the program with probability p and absent with probability 1 \u2212 p. These programs are called Probabilistic Answer Set Programs (PASP, and we use the same acronym to also indicate Probabilistic Answer Set Programming - the meaning will be clear from the context). A selection for the presence or absence of each probabilistic fact defines a world w whose probability is\n$P(w) = \\prod_{a \\in w} p \\cdot \\prod_{a \\notin w} (1 - p)$\nwhere with $a \\in w$ we indicate that a is present in w and with $a \\notin w$ that a is absent in w. A program with n probabilistic facts has $2^n$ worlds. Let us indicate with W the set of all possible worlds. Each world is an answer set program and it may have zero or more answer sets but the CS requires at least one. If this holds, the probability of a query q (a conjunction of ground literals) is defined by a lower and an upper bound. A world w contributes to both the lower and upper probability if each of its answer sets contains the query, i.e., it is a cautious consequence. If only some answer sets contain the query, i.e., it is a brave consequence, w only contributes to the upper probability. If the query is not present, we have no contribution to the probability bounds from w. In formulas,\n$\\underline{P}(q) = [\\underline{P}(q), \\overline{P}(q)] = [\\sum_{w_i \\in W | \\forall m \\in AS(w_i), m \\models q} P(w_i), \\sum_{w_i \\in W | \\exists m \\in AS(w_i), m \\models q} P(w_i)]$\nThe conditional probability of a query q given evidence e, also in the form of conjunction of ground literals, is (Cozman and Mau\u00e1 2020):\n$\\underline{P}(q | e) = \\frac{\\underline{P}(q, e)}{\\underline{P}(q, e) + \\overline{P}(not q, e)}$\n$\\overline{P}(q | e) = \\frac{\\overline{P}(q, e)}{\\underline{P}(q, e) + \\overline{P}(not q, e)}$\nFor the lower conditional probability $\\underline{P}(q | e)$, if $\\underline{P}(q, e) + \\overline{P}(not q, e) = 0$ and $\\underline{P}(q, e) > 0$, then $\\underline{P}(q | e) = 1$. Similarly, for the upper conditional probability $\\overline{P}(q | e)$, if $\\underline{P}(q, e) + \\overline{P}(not q, e) = 0$ and $\\overline{P}(not q, e) > 0$, then $\\overline{P}(q | e) = 0$. Both formulas are undefined if $\\underline{P}(q, e)$ and $\\overline{P}(not q, e)$ are 0.\nTo clarify, consider the following example."}, {"title": "Example 1", "content": "The following PASP encodes a simple graph reachability problem.\n0.  2::edge(1,2).\n1.  3::edge(2,4).\n2.  9::edge(1,3).\npath(X,Y):- connected(X,Z), path(Z,Y).\npath(X,Y):- connected(X,Y).\nconnected(X,Y):- edge(X,Y), not nconnected(X,Y).\nnconnected(X,Y):- edge(X,Y), not connected(X,Y).\nThe first three facts are probabilistic. The rules state that there is a path between X and Y if they are directly connected or if there is a path between Z and Y and X and Z are connected. Two nodes may or may not be connected if there is an edge between them. There are $2^3 = 8$ worlds to consider. If we want to compute the probability of the query $q = path(1,4)$, only $w_6$ and $w_7$ contribute to the upper bound (no contribution to the lower bound), obtaining $\\overline{P}(q) = [0, 0.06]$. If we observe $e = edge(2, 4)$, we get $\\overline{P}(q, e) = [0, 0.06]$, $\\overline{P}(not q, e) = [0.24, 0.3]$, thus $\\underline{P}(q | e) = 0$ and $\\overline{P}(q | e) = 0.2$.\nInference in PASP can be expressed as a Second Level Algebraic Model Counting Problem (2AMC) (Kiesel et al. 2022). Given a propositional theory T where its variables are grouped into two disjoint sets, $X_i$ and $X_o$, two commutative semirings $R^i = (D^i, \\oplus^i, \\otimes^i, \\text{noi}, \\text{noi})$ and $R^o = (D^o, \\oplus^o, \\otimes^o, \\text{no}, \\text{no})$, two weight functions,\n$w_i : lit(X_i) \\rightarrow D^i$ and $w_o : lit(X_o) \\rightarrow D^o$, and a transformation function $f : D^i \\rightarrow D^o$,\n2AMC is encoded as:\n$2AMC(T) = \\bigoplus_{I_o \\in \\mu(X_o)} \\bigotimes_{a \\in I_o} w_o(a) \\otimes^o f(\\bigoplus_{I_i \\in (T | I_o)} \\bigotimes_{b \\in I_i} w_i(b))$\nwhere $(T | I_o)$ is the set of assignments to the variables in $X_i$ such that each assignment, together with $I_o$, satisfies T and $\\mu(X_o)$ is the set of possible assignments to the variables in $X_o$. In other words, 2AMC requires to solve two Algebraic Model Counting (AMC) (Kimmig et al. 2017) tasks. The outer task focuses on the variables $X_o$, and for each possible assignment to these variables, an inner AMC task is performed by considering $X_i$. These two tasks are connected via a transformation function that turns values from the inner task into values for the outer task. Different instantiations of the components allow different tasks to be represented. To perform inference in PASP, Azzolini and Riguzzi (2023a) proposed to consider as innermost semiring $R^i = (\\mathbb{N}^2, +, \\cdot, (0, 0), (1, 1))$ with $X_i$ containing the atoms of the Herbrand base except the probabilistic facts and $w_i$ mapping not q to (0, 1) and all other literals to (1, 1), as outer semiring the two-dimensional probability semiring, i.e., $R^o = ([0, 1]^2, +, \\cdot, (0, 0), (1, 1))$, with $X_o$ containing the atoms of the probabilistic facts and $w_o$ associating (p, p) and (1 \u2212 p, 1 \u2212 p) to a and not a, respectively, for every probabilistic fact p :: a and (1, 1) to all the remaining literals, and as transformation function $f((n_1, n_2))$ returning the pair $(v_{lp}, v_{up})$ where $v_{lp} = 1$ if $n_1 = n_2$, 0 otherwise, and $v_{up} = 1$ if $n_1 > 0$, 0 otherwise.\n2AMC can be solved via knowledge compilation (Darwiche and Marquis 2002), often adopted in probabilistic logical settings, since it allows us to compactly represent the input theory with a tree or graph and then computing the probability of a query by traversing it. aspmc (Eiter et al. 2021; 2024) is one such tool, that has been proven more effective than other tools based on alternative techniques such as projected answer set enumeration (Azzolini et al. 2022; Azzolini and Riguzzi 2023a). aspmc converts the input theory into a negation normal form (NNF) formula, a tree where each internal node is labelled with either a conjunction (and-node) or a disjunction (or-node), and leaves are associated with the literals of the theory. More precisely, aspmc targets sd-DNNFs which are NNFs with three additional properties: i) the variables of children of and-nodes are disjoint (decomposability property); ii) the conjunction of any pair of children of or-nodes is logically inconsistent (determinism property); and iii) children of an or-node consider the same variables (smoothness property). Furthermore, aspmc also requires X-firstness (Kiesel et al. 2022), a property that constraints the order of appearance of variables: given two disjoint partitions X and Y of the variables in an NNF n, a node is termed pure if all variables departing from it are members of either X or Y. If this does not hold, the node is called mixed. An NNF has the X-firstness property if for each and-node, all of its children are pure nodes or if one child is mixed and all the other nodes are pure with variables belonging to X."}, {"title": "2.2 Parameter Learning In Probabilistic Answer Set Programs", "content": "We adopt the same Learning from Interpretations framework of (Azzolini et al. 2024), that we recall here for clarity. We denote a PASP with $P(\\Pi)$, where $\\Pi$ is the set of parameters that should be learnt. The parameters are the probabilities associated to (a subset of) probabilistic facts. We call such facts as learnable facts. Note that the probabilities of some probabilistic facts can be fixed, i.e., there can be some probabilistic facts that are not learnable facts. A partial interpretation $I = (I^+, I^-)$ is composed by two sets $I^+$ and $I^-$ that respectively represent the set of true and false atoms. It is called partial since it may specify the truth value of some atoms only. Given a partial interpretation I, we call the interpretation query\n$q_I = \\land_{i^+ \\in I^+} i^+ \\land_{i^- \\in I^-} not i^-$.\nThe probability of an interpretation I, P(I), is defined as the probability of its interpretation query, which is associated with a probability range since we interpret the program under the CS. Given a PASP $P(\\Pi)$, the lower and upper probability for an interpretation I are defined as\n$\\underline{P}(I | P(\\Pi)) = \\sum_{w \\in P(\\Pi) | \\forall m \\in AS(w), m \\models I} P(w)$,\n$\\overline{P}(I | P(\\Pi)) = \\sum_{w \\in P(\\Pi) | \\exists m \\in AS(w), m \\models I} P(w)$.\nGiven a PASP $P(\\Pi)$ and a set of (partial) interpretations I, the goal of the parameter learning task is to find a probability assignment to the probabilistic facts such that the product of the lower (or upper) probabilities of the partial interpretations is maximized, i.e., solve:\n$\\Pi^* = arg \\max_{\\Pi} \\underline{P}(I | P(\\Pi)) = arg \\max_{\\Pi} \\prod_{I \\in I} \\underline{P}(I | P(\\Pi))$\nwhich can be equivalently expressed as\n$\\Pi^* = arg \\max_{\\Pi} log(\\underline{P}(I | P(\\Pi))) = arg \\max \\sum_{I \\in I} log(\\underline{P}(I | P(\\Pi)))$\nalso known as log-likelihood (LL). The maximum value of the LL is 0, obtained when all the interpretations have probability 1. The use of log probabilities is often preferred since summations instead of products are considered, thus possibly preventing numerical issues, especially when many terms are close to 0.\nNote that, since the probability of a query (interpretation) is described by a range, we need to select whether we maximize the lower or upper probability. A solution that maximizes one of the two bounds may not be a solution that also maximizes the other bound. To see this, consider the program\n${{q :- a, b, not nq}, {nq :- a, b, not q}}$\nwhere a and b are both probabilistic with probability $p_a$ and $p_b$, respectively. Suppose we have the interpretation $I = ({q}, {})$. Here, $\\underline{P}(I) = 0$ while $\\overline{P}(I) = p_a p_b$. Thus, any probability assignment to $p_a$ and $p_b$ maximizes the lower probability (which is always 0) but only the assignment $p_a = 1$ and $p_b = 1$ maximizes $\\overline{P}(I)$."}, {"title": "Example 2", "content": "Consider the program shown in Example 1. Suppose we have two interpretations: $I_0 = ({path(1, 3)}, {path(1, 4)})$ and $I_1 = ({path(1, 4)}, {\\text{ }})$. Thus, we have two interpretation queries: $q_{I_0} = path(1, 3), not path(1, 4)$ and $q_{I_1} = path(1, 4)$. Suppose that the probabilities of all the four probabilistic facts can be set and call this set $\\Pi$. The parameter learning task involves solving:\n$\\Pi^* = arg \\max_\\Pi(log(\\underline{P}(q_{I_0} | \\Pi)) + log(\\underline{P}(q_{I_1} | \\Pi)))$.\nAzzolini et al. (2024) focused on ground probabilistic facts whose probabilities should be learnt and proposed an algorithm based on Expectation Maximization (EM) to solve the task. Suppose that the target is the upper probability. The treatment for the lower probability is analogous and only differs in the considered bound. The EM algorithm alternates an expectation phase and a maximization phase, until a certain criterion is met (usually, the difference between two consecutive iterations is less than a given threshold). This involves computing, in the expectation phase, for each probabilistic fact $a_i$ whose probability should be learnt:\n$E[a_{i_0}] = \\sum_{I \\in I} \\overline{P}(not a_i | I)$,   $E[a_{i_1}] = \\sum_{I \\in I} \\overline{P}(a_i | I)$.\nThese values are used in the maximization step to update each parameter $\\Pi_i$ as:\n$\\Pi_i = \\frac{E[a_{i_1}]}{E[a_{i_0}] + E[a_{i_1}]} = \\frac{\\sum_{I \\in I} \\overline{P}(a_i | I)}{\\sum_{I \\in I} \\overline{P}(not a_i | I) + \\overline{P}(a_i | I)}$"}, {"title": "3 Algorithms for Parameter Learning", "content": "We propose two algorithms for solving the parameter learning task, both based on the extraction of symbolic equations from the NNF (Darwiche and Marquis 2002) representing a query. So, we first describe this common part. In the following, when we consider the probability of a query we focus on the upper probability. The treatment for the lower probability is analogous and only differs in the considered bound."}, {"title": "3.1 Extracting Equations from a NNF", "content": "The upper probability of a query q is computed as a sum of products. If, instead of using the probabilities of the facts, we keep them symbolic (i.e., with their name), we can extract a nonlinear symbolic equation for the query, where the variables are the parameters associated with the learnable facts. Call this equation $f_{up}(\\Pi)$ where $\\Pi$ is the set of parameters. Its general form is\n$f_{up}(\\Pi) = \\sum_{w_i} \\prod_{a_j \\in w_i} p_j \\prod_{a_j \\notin w_i} (1 - p_j) \\cdot k_i$\nwhere $k_i$ is the contribution of the probabilistic facts with fixed probability for world $w_i$. We can cast the task of extracting an equation for a query as a 2AMC problem.\nTo do so, we can consider as inner semiring and as transformation function the ones proposed by Azzolini and Riguzzi (2023a) and described in Section 2.1. From this inner semiring we obtain two values, one for the lower and one for the upper probability. The sensitivity semiring by Kimmig et al. (2017) allows the extraction of an equation from an AMC task. We have two values to consider, so we extend that semiring to $R^o = (\\mathbb{R}[X], +, \\cdot, (0, 0), (1, 1))$ with\n$w_o(l) = \\begin{cases}\n(p, p) & \\text{for a p.f. p :: a with fixed probability and l = a}\n(1 - p, 1 - p) & \\text{for a p.f. p :: a with fixed probability and l = not a}\n(\\pi_a, \\pi_a) & \\text{for a learnable fact } \\pi_a \\text{ :: a and l = a}\n(1 - \\pi_a, 1 - \\pi_a) & \\text{for a learnable fact } \\pi_a \\text{ :: a and l = not a}\n(1, 1) & \\text{otherwise}\n\\end{cases}$\nwhere p.f. stands for probabilistic fact and $\\mathbb{R}[X]$ is the set of real valued functions parameterized by X. Variable $\\pi_a$ indicates the symbolic probability of the learnable fact a. In this way, we obtain a nonlinear equation that represents the probability of a query. When evaluated by replacing variables with actual numerical values, we obtain the probability of the query when the learnable facts have such values. Simplifying the obtained equation is also a crucial step since it might significantly reduce the number of operations needed to evaluate it."}, {"title": "Example 3", "content": "If we consider Example 1 with query path(1, 4) and associate a variable $\\pi_{xy}$ with each edge(x, y) probabilistic fact (and consider them as learnable), the symbolic equation for its upper probability, i.e.,\n$f_{up}, is \\pi_{12} \\cdot \\pi_{24} \\cdot \\pi_{13} + \\pi_{12} \\cdot \\pi_{24} \\cdot (1 - \\pi_{13})$,\nthat can be simplified to $\\pi_{12} \\cdot \\pi_{24}$.\nWe now show how to adopt symbolic equations to solve the parameter learning task."}, {"title": "3.2 Solving Parameter Learning with Constrained Optimization", "content": "The learning task requires maximizing the sum of the log-probabilities of the interpretations where the tunable parameters are the facts whose probability should be learnt. Algorithm 1 sketches the involved steps. We can extract the equation for the probability of each interpretation from the NNF and consider each parameter of a learnable fact as a variable. To reduce the size of the equation we simplify it. Then, we set up a constrained nonlinear optimization problem where the target is the maximization of the sum of the equations representing the log probabilities of the interpretations. We need to also impose that the parameters of the learnable facts are between 0 and 1. In this way, we can easily adopt off-the-shelf solvers and do not need to write a specialized one."}, {"title": "3.3 Solving Parameter Learning with Expectation Maximization", "content": "We propose another algorithm that instead performs Expectation Maximization as per Equation 4 and Equation 5. It is sketched in Algorithm 2. For each interpretation I, we add its interpretation query $q_I$ into the program. To compute $\\overline{P}(a_j | I_k)$ for each learnable probabilistic fact $a_j$ and each interpretation $I_k$ we proceed as follows: first, we compute $\\overline{P}(a_j, I_k)$ and $\\overline{P}(not a_j, I_k)$. Then, Equation 2 allows us to compute $\\overline{P}(a_j | I_k)$. Similarly for $\\underline{P}(a_j | I_k)$. We extract an equation for each of these queries and iteratively evaluate them until the convergence of the EM algorithm that alternates the expectation phase with function EXPECTATION, the maximization phase with function MAXIMIZATION, and the computation of the log-likelihood with function COMPUTELL. We consider as default convergence criterion a variation of the log-likelihood less than $5 \\cdot 10^{-4}$ between two subsequent iterations. However, this parameter can be set by the user. If we denote with $n_p$ the number of probabilistic facts whose probabilities should be learnt and $n_i$ the number of interpretations, we need to extract equations for $2 \\cdot n_p n_i$ queries. However, this is possible with only one pass of the NNF: across different iterations, the structure of the program is the same, only the probabilities, and thus the result of the queries, will change. Thus, we can store the performed operations in memory and use only those to reevaluate the probabilities, without having to rebuild the NNF at each iteration."}, {"title": "Example 4", "content": "Consider Example 2 and its two interpretation queries, $q_{I_0}$ and $q_{I_1}$. If we consider their symbolic equations we have $\\pi_{12} \\cdot \\pi_{24}$ and $\\pi_{13} \\cdot (\\pi_{12} + \\pi_{24} - \\pi_{12} \\cdot \\pi_{24})$, where with $\\pi_{xy}$ we indicate the probability of $edge(x, y)$. If we compactly denote with I the set of all the probabilities (i.e., all the $\\pi_{xy}$), the optimization problem requires solving\n$\\Pi^* = arg \\max (log(\\pi_{12} \\cdot \\pi_{24}) + log(\\pi_{13} \\cdot (\\pi_{12} + \\pi_{24} - \\pi_{12} \\cdot \\pi_{24})))$.\nThe optimal solution is to set the values of all the $\\pi_{xy}$ to 1, obtaining a log-likelihood of 0. Note that, in general, it is not always possible to obtain a LL of 0."}, {"title": "4 Related Work", "content": "There are many different techniques available to solve the parameter learning task, but most of them only work for programs with a unique model per world: PRISM was one of the first tools considering inference and parameter learning in PLP. Its first implementation, which dates back to 1995, offered an algorithm based on EM. The same approach is also adopted in EMBLEM, which learns the parameters of Logic Programs with Annotated Disjunctions, i.e., logic programs with disjunctive rules where each head atom is associated with a probability, and in ProbLog2 that learn the parameters of ProbLog programs from partial interpretations adopting the LFI-ProbLog algorithm. LeProbLog still considers ProbLog programs but uses gradient descent to solve the task.\nFew tools consider PASP and allow multiple models per world. dPASP is a framework to perform parameter learning in Probabilistic Answer Set Programs but targets the max-ent semantics, where the probability of a query is the sum of the probabilities of the models where the query is true. They propose different algorithms based on the computation of a fixed point and gradient ascent. However, they do not target the Credal Semantics. Parameter Learning under the Credal Semantics is also available in PASTA. Here, we adopt the same setting (learning from interpretations) but we address the task with an inference algorithm based on 2AMC and extraction of symbolic equations, rather than projected answer set enumeration, which has been empirically proven more effective. This is also proved in our experimental evaluation.\nLastly, there are alternative semantics to adopt in the context of Probabilistic Answer Set Programming, namely, P-log, LPMLN, and smProbLog. The relation among these has been partially explored but a complete treatment providing a general comparison is still missing. The parameter learning task under these semantics has been partially addressed and an in-depth comparison between all the existing approaches can be an interesting future work."}, {"title": "5 Experiments", "content": "We ran the experiments on a computer with 16 GB of RAM running at 2.40GHz with 8 hours of time limit. The goal of the experiments is many-fold: i) finding which one of the algorithms is faster; ii) discovering which one of the algorithms better solves the optimization problem represented by iii) evaluating whether different initial probability values impact on the execution time; and iv) comparing our approach against the algorithm based on projected answer set enumeration of called PASTA. We considered only PASTA since it is the only algorithm that currently solves the task of parameter learning in PASP under the credal semantics. We used as backend solver for the computation of the formula", "rules": "nred(X) :- node(X)", "blue(X).\ngreen(X)": "", "X)": "", "green(X).\ne(X,Y)": "", "edge(X,Y).\ne(Y,X)": "", "edge(Y,X).\nc0": "", "red(Y).\nc1": "", "green(Y).\nc2": "", "blue(Y).\nvalid": "", "is": "nbought(spaghetti", "john)": "", "bought(steak,john).\nbought(steak,john)": "", "bought(spaghetti,john).\nbought(spaghetti,mary)": "", "bought(beans,mary).\nbought(beans,mary)": "", "bought(spaghetti,mary).\nbought(spaghetti)": "", "bought(spaghetti,_).\nbought(steak)": "", "bought(steak,_).": ""}, {"is": "n0.  1::asthma_f(1).\n1.  4::asthma_fact(1).\n2.  1::asthma_f(2).\n3.  4::asthma_fact(2).\n4.  2 :: predisposition.\n5.  3::stress(1).\n6.  3::stress(2).\nsmokes (X) :- stress(X).\nsmokes (X) :- influences(Y", "smokes(Y).\nasthma_rule(X)": "", "asthma_fact(X).\nasthma(X)": "", "asthma_f(X).\nasthma(X)": "", "asthma_rule(X).\nill(X)": "", "n_ill(X).\nn_ill(X)": ""}]}