{"title": "A System and Benchmark for LLM-based Q&A on Heterogeneous Data", "authors": ["Achille Fokoue", "Srideepika Jayaraman", "Elham Khabiri", "Jeffrey O. Kephart", "Yingjie Li", "Dhruv Shah", "Youssef Drissi", "Fenno F. Heath III", "Anu Bhamidipaty", "Fateh A. Tipu", "Robert J.Baseman"], "abstract": "In many industrial settings, users wish to ask questions whose answers may be found in structured data sources such as a spreadsheets, databases, APIs, or combinations thereof. Often, the user doesn't know how to identify or access the right data source. This problem is compounded even further if multiple (and potentially siloed) data sources must be assembled to derive the answer. Recently, various Text-to-SQL applications that leverage Large Language Models (LLMs) have addressed some of these problems by enabling users to ask questions in natural language. However, these applications remain impractical in realistic industrial settings because they fail to cope with the data source heterogeneity that typifies such environments. In this paper, we address heterogeneity by introducing the siwarex platform, which enables seamless natural language access to both databases and APIs. To demonstrate the effectiveness of siwarex, we extend the popular Spider dataset and benchmark by replacing some of its tables by data retrieval APIs. We find that siwarex does a good job of coping with data source heterogeneity. Our modified Spider benchmark will soon be available to the research community.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant performance improvements in Large Language Models (LLMs) have driven the widespread adoption of natural language interfaces that access, retrieve, and reason over a variety of structured data sources to answer questions. In enterprise settings, this paradigm shift promises to democratize access to critical data and analytics by users with limited knowledge of formal structured query languages (e.g., SQL to access databases) or API invocation.\nIn practically any industrial environment, one finds a mixture of different types of structured data sources, including various flavors of SQL databases, NoSQL databases, and APIs. A master DB might contain records on sales, customers, or assets. Time series data might be available from an API such as the OSISoft PI web interface or an instance of the GE Digital Historian. Sales forecasting and other analytical capabilities may also be available via APIs.\nUnfortunately, existing Q&A systems do not support such heterogeneous environments. Some, like SQL tools used in the Langchain (Developers, Accessed: 2024-07-16), are designed with the assumption that all structured information is available from a single DB system, such as postgres. Others (e.g., (Patil et al., 2023)) assume that all data are obtained by accessing APIs. Unsurprisingly, existing benchmarks reinforce this unrealistic assumption of homogeneity. Multiple benchmarks (Yu et al., 2018; Zhong et al., 2017; Li et al., 2023a) have been developed to assess the capability of LLMs to successfully convert natural language questions into SQL queries against a database (the text-to-SQL problem). In particular, all data that comprise the Spider (Yu et al., 2018) benchmark are stored in SQLite tables. Many other benchmarks (Patil et al., 2023; Li et al., 2023b) evaluate the ability of LLMs to invoke the right APIs to answer a user's question. However, no existing benchmarks measure the efficacy of tools that cope with heterogeneous data sources.\nThe purpose of this paper is to introduce a system (siwarex) and a benchmark that explicitly address this critical gap. After a brief review of relevant literature in section 2, in section 3, we introduce siwarex, a framework that supports question-answering across heterogeneous data sources consisting of a mixture of different types of databases and API calls. siwarex does so by exploiting:\n1. an LLM for the natural language understanding of the question and the generation of SQL statements given a unified relational represen-"}, {"title": "2 Related work", "content": "In this section, we overview the most relevant prior benchmarks and LLM-based Q&A tools.\nTwo of the most popular pure Text-to-SQL benchmarks are Spider (Yu et al., 2018) and BIRD (Li et al., 2023a). The Spider benchmark is a text-to-sql dataset that contains a set of databases that each possess multiple tables and an associated set of natural language queries for which the correct SQL translation is known. The ground truth for a given natural language query is an SQL statement that appropriately retrieves and filters information from the relevant tables. An agent's benchmark score reflects how well its generated SQL statements match the ground truth averaged over the set of natural language queries.\nOne popular pure API benchmark is APIBench (Patil et al., 2023), which has introduced a dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. Another is API-Bank (Li et al., 2023b), a benchmark for tool-augmented LLMs that includes an evaluation system with API tools and tool-use dialogues to assess the capabilities of a given LLM in planning, retrieving, and calling APIs.\nIn contrast to these pure Text-to-SQL or API benchmarks, the benchmark we introduce in Section 4 is the first to assess a system's ability to handle a mixture of DB accesses and API calls to answer natural language questions.\nDAIL-SQL (Gao et al., 2023) is the Text-to-SQL method that gained the highest position on the Spider Leaderboard among opensource solutions (second place overall). The authors provided a comprehensive analysis of the performance of various open and closed source LLMs in various settings (pretrained vs fine-tuned vs aligned and zero-shot vs multiple-shot). Their experimental findings (their Table 3) motivated our use of the Mixtral 8x7b model (Jiang et al., 2024), on the grounds that it achieves nearly the best execution accuracy on zero-shot evaluation on the Spider benchmark (0.66 vs 0.68 for CodeLLAMA-34B, the best open-source model) and it has a more liberal license.\nGorilla (Patil et al., 2023) is a LLaMA-based model for writing API calls by using self-instruct fine-tuning and retrieval to select from a large, overlapping, and changing set of APIs. We have chosen to employ it in our baseline experiments of Section 5 because it has performed very well on the APIBench benchmark."}, {"title": "3 The siwarex Framework", "content": "This section describes the components of our siwarex framework, which is built on top of a ReAct (Yao et al., 2022) agent implemented using the LangChain ReAct Agent\u00b9.\nsiwarex leverages two data source schema that can be provided by users or derived semi-automatically from domain metadata:\n1. The Abstract Schema, in the form of an Entity-Relationship Diagram, provides a global view of the data source properties and interrelationships in a format that is agnostic to whether the data source is a database table or an API."}, {"title": "4 New benchmark datasets", "content": "In order to assess the ability of siwarex or any other agent to cope with heterogeneous data sources, we need an appropriate benchmark. Since no such benchmark exists currently, we must create one. While in theory it might be possible to pool a large set of questions and answers adapted from real industrial Q&A examples, such an approach would likely face severe practical and political obstacles. Instead, we have opted to modify the popular Spider benchmark (Yu et al., 2018).\nTo assess the ability of siwarex or any other agent to translate utterances into an appropriate mixture of database (DB) and API calls, we extend the Spider benchmark by randomly replacing an adjustable percentage ATTR (the API to Table Ratio) of the DB tables with equivalent API calls. An ATTR of 0% reduces to the traditional Spider benchmark, while an ATTR of 100% represents the opposite extreme in which all data required to answer a question must be accessed via APIs. Once APIs are generated for a given ATTR, the replaced tables are deleted from the original database."}, {"title": "5 Evaluation", "content": "To evaluate siwarex, we compare its performance with that of a simple baseline consisting of a ReAct agent that does not include most of the siwarex components introduced in Section 3.\nThe Baseline agent uses two state-of-the-art tools:\n1. Gorilla (Patil et al., 2023), currently the top open-source API-invoking system on the Berkeley Function-Calling Leaderboard\u00b2; and\n2. SQLDatabaseToolkit, a tool for interacting with SQL databases, from LangChain community\u00b3 configured to use Mixtral 8x7 (Jiang et al., 2024) as its LLM.\nIt decomposes an input question into more basic natural language questions, each of which can be answered by retrieving the required information from a single data modality. Basic questions requiring information from DB tables are routed to the SQLDatabaseToolkit, while those requiring information available from APIs are sent to the Gorilla tool. As shown in Figure 6, the ReAct prompt explicitly includes the list of available APIs and DB tables, thereby enabling the Baseline agent to properly decompose the input questions into single-modality basic questions and route those basic questions to the right tool.\nThe evaluation metric is the execution accuracy measured by comparing the results produced by our system (or the baseline system) against those produced by the evaluation of the gold standard spider sql query on the original spider db. We use the sophisticated comparison approach that was introduced by (Zhong et al., 2020). For siwarex and the Baseline, the execution accuracy aggregated over all levels of difficulty is plotted in Figure 3 as a function of ATTR (the API to Table Ratio).\nWithout any APIs (at 0%), both siwarex and the baseline system perform very close to the state-of-the-art open-source LLM (CodeLLAMA-34B) on Zero-shot evaluation (see Table 3 in (Zhong et al., 2020) which reports 0.68 accuracy for CodeLLAMA-34B). However, as the proportion of API calls increases, the performance of the baseline system deteriorates significantly whereas the performance of siwarex degrades only modestly. Figs. 4 and 5, which plot execution accuracy over \"easy\" and \"extra hard\" questions respectively, tell a similar story. As expected, the accuracy for easy and extra hard questions are above and below that of the question set as a whole, and those for \"medium\" and \"hard\" questions are bracketed between these two extremes (Figures 7 and 8.)\nA careful error analysis suggests that several key issues account for much of the baseline system's poor performance:\n1. Sequencing. State-of-the-system function calling LLM based systems are not trained, fine-tuned or optimized to perform complex API sequencing, merging, and aggregation tasks. They tend to perform relatively well on questions whose answers require performing a single API call. However, on our benchmark with an ATTR of 100%, even the easier Spider queries (e.g., \"What is the total number of singers?\") typically require invoking multiple APIs and sequencing them properly (e.g., invoke getAllSingers() and then invoke getSize() on the previous result).\n2. Routing. On datasets with a mixture of APIs and DB tables, the master LLM (Mixtral 8x7) often decomposes the problem correctly, but then it fails to properly route the decomposed questions to the tool (db or API tool) that has the right information to answer it.\n3. Hallucination. Even when the right API is selected for invocation, the arguments for its invocation are often hallucinated values.\nsiwarex avoids all the above issues by providing to an LLM a single relational view that removes all the complexity of dealing with multiple heterogeneous sources. To the LLM, everything appears to be relational, and thus it can leverage its Text2SQL capability to generate a SQL query for each NL question. The Query Rewriter is then responsible to inject API invocations through UDFs with the proper arguments (inferred from a deterministic analysis of the SQL query)."}, {"title": "6 Conclusion", "content": "In this paper, we introduce both siwarex, a novel system for LLM-based Q&A over heterogeneous data source, and a new benchmark (which we plan to make publicly available) that simulates the common heterogeneous data environments encountered in various industries. Our experimental evaluation shows that siwarex performs remarkably well at different levels of data heterogeneity compared to a baseline made of two independent open-source state-of-the-art systems that access databases and APIs, respectively."}, {"title": "7 Limitations", "content": "One limitation that we are eager to address in future work is that our benchmark's evaluation metric only considers the execution accuracy of the final results. Especially since we wish to produce a benchmark that meaningfully captures practical issues that arise in industrial settings, it is incumbent on us to augment this metric with an execution performance metric (i.e. efficiency or speed) like that introduced by BIRD (Li et al., 2023a).\nAdditionally, the insights we derived in Section 5 should now be factored into a new version of siwarex that can be tested with our new benchmark (with the extra performance metric included).\nFinally, the current work only considers a limited form of data heterogeneity in which DB accesses are replaced with API calls. While this is a natural and convenient way to extend the Spider benchmark, we can further improve the realism of our benchmark by introducing greater diversity in the type of databases being used (for example, mixing in other types of SQL databases along with some NoSQL databases) and introducing APIs that are not just replacements for database calls but perform calculations (e.g. statistical operations) or analytics (e.g. timeseries correlations)."}]}