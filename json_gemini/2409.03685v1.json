{"title": "View-Invariant Policy Learning via Zero-Shot Novel View Synthesis", "authors": ["Stephen Tian", "Blake Wulfe", "Kyle Sargent", "Katherine Liu", "Sergey Zakharov", "Vitor Guizilini", "Jiajun Wu"], "abstract": "Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.", "sections": [{"title": "Introduction", "content": "A foundation model for robotic manipulation must be able to perform a multitude of tasks, generalizing not only to different environments and goal specifications but also to varying robotic embodiments. A particular robotic embodiment often comes with its own sensor configuration and perception pipeline. This variety is a major challenge for current systems, which are often trained and deployed with carefully controlled or meticulously calibrated perception pipelines. One approach to training models that can scale to diverse tasks as well as perceptual inputs is to train on a common modality, such as third-person RGB images, for which diverse data are relatively plentiful [1].\nA challenge in using these data is that policies learned by current methods struggle to generalize across perceptual shifts for single RGB images. In this paper, we study one ubiquitous and practically challenging shift: when the camera viewpoint is altered. Prior studies have found that policies trained on RGB images collected from fixed viewpoints are consistently unable to generalize to visual inputs from other camera poses [2, 3, 4]."}, {"title": "Related Work", "content": "Learning viewpoint-robust robotic policies. Learning deep neural network policies that can generalize to different observational viewpoints has been discussed at length in the literature. One set of approaches effectively augment the input data to a learned policy or dynamics model with additional 2D images rendered from differing camera viewpoints. These renderings are often obtained from simulators [5, 6] or by reprojecting 2D images [9]. Augmenting training with simulator data can improve robustness on simulation environments, but these methods must then address the challenge of sim-to-real transfer for deployment on real systems. In this work, we study methods for learning invariant policies directly using robot data from the deployment setting, including real robot trajectories. Existing work [10] performs view augmentation of real-world wrist camera images; however, this is performed with the goal of mitigating covariate shift as opposed to improving camera pose robustness, and requires many views of a static scene to generate realistic novel views.\nAnother line of work forms explicit 3D representations of the scene such as point clouds or voxels to leverage equivariance properties [11, 12, 13, 14, 15], or projects from these representations to 2D images originating from canonical camera poses [16]. While these approaches have been shown to be robust to novel camera views [3], they require well-calibrated camera extrinsics, which can be practically challenging and time-consuming to collect, and are not present in all existing datasets (for example, the majority of datasets in Open X-Embodiment [1] do not provide camera extrinsics).\nRather than rely on explicit 3D representations, a related body of work learns latent representations that are robust to variations in camera pose. These methods often use view synthesis or contrastive learning as a pretraining or auxiliary objective [17, 18, 19, 6], and also often require accurate extrinsics, can be computationally expensive to run at inference time, or impose restrictive requirements on the latent space that can make multi-task learning challenging.\nA technique that has shown promise in reducing the observational domain gap in robotic learning settings is the use of wrist-mounted or eye-in-hand cameras as part of the observation space [20, 21]."}, {"title": "Preliminaries", "content": "The techniques we discuss can be flexibly applied to many visuomotor policy learning settings; however, for a systematic and computationally constrained evaluation, we choose to study them in the context of visual imitation learning.\nWe frame each robotic manipulation problem as a discrete-time partially observed Markov decision process (POMDP), with state space S, action space A, transition function P, reward function R, and observation function O. This observation function maps states and actions into the observation space conditioned on extrinsic parameters E. We assume access to a dataset D consisting of M expert demonstrations $\\mathcal{T}_{0:M}: \\mathcal{T}_i = \\{(o_0, a_0,..., o_t, a_t,..., o_\\mathcal{T}, a_\\mathcal{T})\\}$ where $\\mathcal{T}$ is the total number of timesteps in a particular demonstration. Concretely, the observation o consists of both low-dimensional observations in the form of robot proprioceptive information, as well as RGB image observations $o_I \\in \\mathbb{R}^{H \\times W \\times 3}$ captured by a fixed third-person camera with extrinsics $\\mathcal{E}_{orig}$.\nThe objective is to learn a policy $\\pi(a|o)$ that solves the task, where observed images $o$ are captured by a camera with extrinsics $\\mathcal{E}_{test}$ sampled from a distribution $\\mathbb{E}_{test}$. Critically, we do not assume access to the environment or the ability to place additional sensors at training time."}, {"title": "Zero-Shot Novel View Synthesis from a Single Image", "content": "We define the single-image novel view synthesis (NVS) problem as finding a function $M(I_{context}, f, \\mathcal{E}_{context}, \\mathcal{E}_{target})$ that, given an input image $I_{context} \\in \\mathbb{R}^{H \\times W \\times 3}$ of a scene captured with camera extrinsics (e.g., camera pose) $\\mathcal{E}_{context}$ and simplified intrinsics represented by a field of view f, renders an image of the same scene $I_{context}$ captured with camera extrinsics $\\mathcal{E}_{target}$.\nTo extend this setting to zero-shot novel view synthesis, we further assume that the image $I_{orig}$ depicts a robotic task that was not seen when training M. As we will describe in Section 5, we"}, {"title": "Learning View Invariance with Zero-Shot Novel View Synthesis Models", "content": "In this section, we describe VISTA, the data augmentation scheme for view-invariant policy learning that we study in the remainder of the paper. The method is summarized in Figure 3.\nTo learn viewpoint-invariance, some prior works augment experience with images rendered from virtual cameras in simulation [5, 6]. However, we wish to learn viewpoint-invariant policies directly from existing offline datasets, which could be from inaccessible simulated environments or data collected in the real world. Furthermore, many robotic datasets do not contain the multiview observations or depth images needed for 3D reconstruction. Thus, we explore using single image novel view synthesis methods to perform augmentation.\nConcretely, given a single-image novel view synthesis model $M(I_{context}, f, \\mathcal{E}_{context}, \\mathcal{E}_{target})$, VISTA uses M to replace each frame of a demonstration trajectory with a synthesized frame with independently randomly sampled target extrinsics $\\mathcal{E}_{target} \\sim \\mathbb{E}_{train}$. That is, we independently replace each observation-action pair (o, a) with $(M(o_I, f, \\mathcal{E}_{context}, \\mathcal{E}_{target}), a)$. For the sake of systematic evaluation, in our simulated experiments, we assume knowledge of both the initial camera pose $\\mathcal{E}_{context}$ and the target distribution $\\mathbb{E}_{target}$. However, the novel view synthesis models we study use only the relative poses between $\\mathcal{E}_{context}$ and $\\mathcal{E}_{target}$; absolute poses are not required and are not used in real-world experiments. We assume that the field of view is known.\nVISTA has several appealing properties. First, while methods that form explicit 3D representations must either use multi-view images or assume static scenes when performing structure-from-motion, it avoids the computational expense of 3D reconstruction and takes advantage of the fact that a scene is static at any slice in time. Second, VISTA does not add additional computational complexity at inference time, as the trained policy's forward pass remains the same. Lastly, VISTA inherits improvements in the modeling and generalization capability of novel view synthesis models.\nWe center our analysis around a particular novel-view synthesis model, ZeroNVS [8]. ZeroNVS is a latent diffusion model that generates novel views of an input image given a specified camera transformation. It is initialized from Stable Diffusion [35] and fine-tuned on a diverse collection of 3D scenes, therefore achieving strong zero-shot performance on a wide variety of scene types. Moreover, as a generative model, it tends to generate novel views which are crisp and realistic, mitigating the domain gap between generated and real images.\nAlthough ZeroNVS provides reasonable predictions even in zero-shot settings, we found that it also has failure modes that generate images that appear to contain extreme close-ups of objects in the scene, potentially due to poor extrinsics in the training dataset. To partially address these scenarios, we simply reject and resample images that have a perceptual similarity (LPIPS) [36] distance larger than a value $\\eta$ from the input image, which we found to slightly improve performance.\nWhile many techniques for imitation learning have been proposed, as a strong baseline that fits our computational budget, we use the implementation of behavior cloning with a Gaussian mixture"}, {"title": "Experimental Analysis", "content": "In this section, we perform empirical analyses to answer the following questions:\nQ1: Can policies trained with data augmented by novel view synthesis models trained on large-scale out-of-domain datasets improve robustness to novel viewpoints? How do these models compare to existing alternatives?\nQ2: Can finetuning novel view synthesis models on robotic data improve the performance of VISTA when applied to unseen tasks with larger viewpoint changes?\nQ3: How do methods providing augmented third-person viewpoints interact with strategies for reducing the observational domain gap, such as adding wrist-mounted cameras?\nQ4: Can VISTA be applied to learn policies on real robots directly using real-world data? How does finetuning view synthesis models on diverse real robot data affect downstream performance?\nSimulated experimental setup. We perform simulated experiments using the robomimic framework built on the MuJoCo simulator [39], with additional tasks introduced in MimicGen [40]. For the Lift and Nut Assembly tasks, we use the proficient-human expert demonstration datasets from robomimic for training. For the remainder of the tasks, we train using the first 200 demonstrations of D0 datasets and evaluate using the D0 environment variants as defined by MimicGen.\nTo contextualize the results, we introduce the following baseline methods:\n\u2022 Single view. To represent the performance of a model trained without view-invariance, this performs behavioral cloning using only the source demonstration dataset.\n\u2022 Simulator (oracle). As an upper bound of the performance of per-frame random augmentation for learning viewpoint invariance, this baseline directly uses the simulator to render novel viewpoints.\n\u2022 Depth estimation + reprojection. This baseline follows the augmentation scheme described in Section 4. It synthesizes novel views from RGB images using a three-stage pipeline. Because we do not assume access to depth, it first performs metric depth estimation using an off-the-shelf"}, {"title": "Conclusion and Limitations", "content": "Limitations. While VISTA is effective at improving the viewpoint robustness of policies, it does have certain limitations. First, the computational expense of generating novel views can be significant. Second, augmenting views during policy training can increase training time and therefore computational expense. Third, sampling views during data augmentation requires some distribution of poses from which to sample. This distribution must cover the reasonable space of views expected at deployment time. Fourth, single-image novel view synthesis models often perform poorly when the novel view is at a camera pose that differs dramatically from the original camera pose, and this limits the distribution from which views may be sampled during data augmentation.\nConclusion. In this paper, we presented VISTA, a simple yet effective method for making policies robust to changes in camera pose between training and deployment time. Using 3D priors from single image view synthesis methods trained on large-scale data, VISTA performs data augmentation to learn policies invariant to camera pose in an imitation learning context. Experiments in both simulated and real world environments demonstrated improved robustness to novel viewpoints of our approach over baselines, particularly when using view synthesis models finetuned on robotic data (though applied zero-shot with respect to tasks). There are a number of promising directions for future work, but of particular interest is studying the performance of this data augmentation scheme at scale across a large dataset of robotic demonstrations."}, {"title": "Real World Robot Setup", "content": "We use a Franka Research 3 (FR3) robot in our real world experiments. The hardware setup is otherwise a replica of that introduced by Khazatsky et al. [7]. Specifically, the robot is mounted to a mobile desk base (although we keep it fixed in our experiments) and two ZED 2 stereo cameras provide observations for the robot. An overview of the real-world robot setup is shown in Figure 9.\nWe use a Meta Quest 2 controller (also as per the DROID hardware setup) to collect teleoperated expert demonstrations. We collect 150 human expert demonstrations of the place cup on saucer task, randomizing the position of the cup and saucer after each task. Each demonstration trajectory lasts approximately 15 seconds of wall clock time.\nWhen performing evaluations, we score task completion based on two stages: 1) Reaching the cup in a grasp attempt based on determination by a human rater and 2) Completing the task, which means that the cup is above and touching the surface of the saucer at some point during the trajectory."}, {"title": "Finetuning ZeroNVS on the DROID Dataset", "content": "To finetune ZeroNVS on the DROID dataset, we first collect a random subset of 3000 trajectories from the DROID dataset. Then, for each trajectory, we uniformly randomly sample 10 timestamps from the duration of the video, and consider the trajectory frozen at each of those times as a \u201cscene\". Thus, we effectively have 30000 scenes. For each scene, we extract 4 views, which correspond to stereo images from the two external cameras. Although the DROID dataset does contain wrist camera data, we do not use it, as the wrist camera poses are much more challenging for synthesizing novel views."}]}