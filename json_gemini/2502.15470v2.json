{"title": "PAPI: Exploiting Dynamic Parallelism\nin Large Language Model Decoding with a\nProcessing-In-Memory-Enabled Computing System", "authors": ["Yintao He", "Haiyu Mao", "Christina Giannoula", "Mohammad Sadrosadati", "Juan G\u00f3mez-Luna", "Huawei Li", "Xiaowei Li", "Ying Wang", "Onur Mutlu"], "abstract": "Large language models (LLMs) are widely used for natural\nlanguage understanding and text generation. An LLM model\nrelies on a time-consuming step called LLM decoding to generate\noutput tokens. Several prior works focus on improving the per-\nformance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decod-\ning has both compute-bound and memory-bound kernels. Some\nprior works statically identify and map these different kernels\nto a heterogeneous architecture consisting of both processing-in-\nmemory (PIM) units and computation-centric accelerators (e.g.,\nGPUs). We observe that characteristics of LLM decoding kernels\n(e.g., whether or not a kernel is memory-bound) can change dy-\nnamically due to parameter changes to meet user and/or system\ndemands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-\nfits-all approach of designing PIM units inefficient due to a large\ndegree of heterogeneity even in memory-bound kernels.\nIn this paper, we aim to accelerate LLM decoding while con-\nsidering the dynamically changing characteristics of the kernels\ninvolved. We propose PAPI (PArallel Decoding with PIM), a\nPIM-enabled heterogeneous architecture that exploits dynamic\nscheduling of compute-bound or memory-bound kernels to suit-\nable hardware units. PAPI has two key mechanisms: (1) online\nkernel characterization to dynamically schedule kernels to the\nmost suitable hardware units at runtime and (2) a PIM-enabled\nheterogeneous computing system that harmoniously orchestrates\nboth computation-centric processing units (GPU) and hybrid\nPIM units with different computing capabilities. Our experimen-\ntal results on three broadly-used LLMs (i.e., LLaMA-65B, GPT-3\n66B, and GPT-3 175B) show that PAPI achieves 1.8\u00d7 and 11.1\u00d7\nspeedups over a state-of-the-art heterogeneous LLM accelera-\ntor (i.e., GPU and PIM) and a state-of-the-art PIM-only LLM\naccelerator, respectively.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved remarkable\nsuccess across a wide range of applications, excelling not only\nin natural language processing tasks such as code genera-\ntion [1, 2], question answering [3, 4], but also image [5\u20137]\nand video processing [8]. Efficient LLM inference is crucial to\nunlocking the full potential of these models. LLM inference\nconsists of two phases: prefill and decoding [9]. In the prefill\nphase, the LLM model processes all input tokens in a request\nto create hidden states for the decoding phase and generate\nthe first output token. Subsequently, during the conventional\ndecoding phase, the model generates an output token per de-\ncoding iteration.\nLow execution time of LLM inference is crucial for both user\nexperience in real-time applications and hardware utilization\nefficiency [10]. The LLM decoding phase dominates the exe-\ncution time in the LLM inference tasks [11, 12]. For instance,\nthe serial decoding of the GPT-3 175B model is responsible for\n96% of the overall execution time when the input and output\nlengths are 32 [13]. The impact of LLM decoding on overall\nexecution time increases as the output length grows, which\nis essential for generating more detailed and comprehensive\nLLM responses [14].\nTo improve the performance of the decoding phase, prior\nworks employ two main parallelism techniques: batching\n[15\u201317] and speculative decoding [18\u201321]. These techniques\nenable the generation of multiple tokens, known as parallel\ndecoding, in one decoding iteration, to accelerate decoding.\nWe define the number of decoding tokens that are simulta-\nneously generated as decoding parallelism. Decoding paral-\nlelism directly affects the utilization of memory and computa-\ntion resources. As a result, some kernels in decoding become\ncompute-bound, while others become memory-bound. Recent\nworks explore processing-in-memory (PIM) enabled hybrid\ndesigns (i.e., heterogeneous architectures using both PIM units\nand computation-centric accelerators, like GPUs) [22\u201326] to\naccelerate the LLM inference process by mapping compute-\nbound and memory-bound kernels to computation-centric and\nmemory-centric accelerators. These works statically charac-\nterize LLM decoding kernels and, based on statically-identified\ncharacteristics, schedule different types of kernels to different\ncomputation units (e.g., PIM units and computation-centric\naccelerators like GPUs).\nTo study the effectiveness of static scheduling, we profile the\nkernels used in the decoding phase that employs state-of-the-\nart parallelism techniques. We observe that some kernels shift\nfrom being compute-bound to memory-bound (or vice versa)\ndynamically in response to variations in decoding parallelism.\nThis is because decoding parallelism changes dynamically at\nruntime. There are three main reasons for these dynamic\nchanges in decoding parallelism. First, the maximum decoding\nparallelism in a computing system is limited by the memory\nrequirement of requests, which is dependent on request output\nlengths and cannot be predicted prior to execution. Second,\nthe maximum decoding parallelism is also affected by different\nuser requirements, like quality of service (QoS) [27]. Third,\nsome parallelism techniques [17, 28] employ dynamic opti-\nmization approaches that adjust the configuration of decoding\nparallelism (i.e., batch size and speculation length) at runtime\nto enhance system performance (see Section 3). We conclude"}, {"title": "2. Background", "content": "2.1. LLM Inference\nAn LLM structure contains several transformer-based de-\ncoders, as illustrated in Figure 1(a). Each decoder includes four\nkernels: QKV (Query, Key, and Value) generation, multi-head\nattention, projection, and feedforward networks [31]. These\nkernels can be divided into two types: fully-connected (FC)\nlayers in orange and a multi-head attention layer in green. All\nkernels consist of general matrix-vector multiplication (GEMV)\ncomputations.\nFigure 1(b) presents an overview of LLM inference, which\nincludes two phases: prefill and decoding. In the prefill phase,\nthe model processes multiple tokens within the input sequence\nsimultaneously to generate the first output token. The decod-\ning phase has multiple decoding iterations. In each iteration,\nthe model takes the last output token as input to generate a\nnew output token. The output sequence is generated one by\none sequentially until the <|eos|> (end of a sentence) token [31].\nThis serialized process is called serial decoding. Compared to\nthe prefill phase, the decoding phase typically takes most of\nthe end-to-end inference time [21]. The number of decoding\niterations depends on the output token length. With serial\ndecoding, generating K output tokens takes K decoding itera-\ntions [18].\nIn LLM inference, the output of QKV generation, i.e., K and\nV matrices, is stored as these output values are reused multiple\ntimes in the multi-head attention kernel during subsequent\ndecoding iterations. In serial decoding, LLM inference requires\nGPUs/TPUs to load the large weight matrices and KV matrices\nfrom off-chip memory to on-chip memory/caches during each\nserial decoding iteration, causing high data movement and\nperformance overheads.\n2.2. Optimization Techniques in LLM Inference\nTo overcome performance overheads of conventional serial\ndecoding, researchers have developed two optimization tech-\nniques: batching [15\u201317,31] and speculative decoding [18\u201321].\nThese methods facilitate the concurrent decoding of multiple\ntokens, thereby improving data reuse of weight matrices via\ngeneration of multiple tokens, which improves performance.\n2.2.1. Batching. Batching [15\u201317,31] is a parallelism tech-\nnique to process multiple input sequences concurrently. It\nallows a single decoding step to generate multiple tokens from\ndifferent user requests. This enables request-level parallelism\n(RLP) during inference, where RLP refers to the number of\nrequests executed in parallel. For example, as shown in Fig-\nure 1(c), when an LLM processes two input requests simultane-\nously, RLP is 2. A state-of-the-art batching mechanism is mixed\ncontinuous batching [16, 17]. In mixed continuous batching,"}, {"title": "3. Motivation", "content": "3.1. Analysis of LLM Inference\nWe analyze the computation and memory requirements of\nLLM inference by evaluating the arithmetic intensity of fully-\nconnected (FC) and multi-head attention kernels. We vary the\nbatch size, when batching is enabled, and speculation length,\nwhen speculative decoding is enabled. Figure 2(a) shows the\nroofline model for the OPT-30B model [32] using a high-end\nNVIDIA A100 GPU [29] with 312 TFLOPS peak computation\nperformance and 1935 GB/s peak memory bandwidth, for FC\nand attention kernels, as the batch size increases from 4 to 128,\nwith a speculation length of 8. We make two key observations.\nFirst, when the batch size is small, i.e., 4, 8, 16, the decoding\nphase is memory-bound, i.e., both the attention and FC kernels\nare bottlenecked by memory bandwidth. Second, when the\nbatch size is > 32, the FC kernel becomes compute-bound,\nwhile the attention kernel is memory-bound. The arithmetic\nintensity of the FC kernel increases with the batch size. How-\never, the arithmetic intensity of the attention kernel does not\nchange when the batch size increases, because there is no data\nreuse in batching for the attention kernel.\nFigure 2(b) shows the roofline analysis of FC and attention\nkernels when we vary the speculation length from 2 to 8, with\na batch size of 32. We observe that the arithmetic intensity\nof the attention and FC kernels increases with the specula-\ntion length. With a batch size of 32, the FC kernel becomes\ncompute-bound when the speculation length exceeds 6. In con-\ntrast, although the arithmetic intensity of the attention kernel\nincreases with speculation length, the attention kernel remains\nmemory-bound. This is because the arithmetic intensity of\nthe attention kernel increases only slightly with speculation\nlength, and batching has no effect on it, as batching primarily\nimproves weight data reuse, which does not affect the attention\nkernel.\n3.2. Varying Parallelization Levels in LLM Inference\nIn real-world LLM tasks, both batch size and speculation\nlength vary significantly at runtime due to changes in user\nrequests and potential adjustments to speculation length to\noptimize performance [28]. We elaborate on why the paral-\nlelism in LLM inference dynamically changes during runtime\nin real-world scenarios.\nInitial Request-Level Parallelism (Initial RLP): Initial RLP\nrefers to the RLP when batched execution begins. Initial RLP\ncan vary significantly in real-world LLM serving scenarios,\ncausing the batch size to vary greatly. This is due to three\nmajor reasons.\n(a) Service Level Objective (SLO) Limits: Increasing RLP can\nenhance throughput but increases inference latency per re-\nquest [33]. Under the online serving scenario, different user\nlatency SLOs dictate varying maximum batch sizes. For exam-\nple, while a DGX A100 computing system [34] with 1,280 GB\nmemory can support up to 854 requests per batch, a 30 ms SLO\nrequires setting the initial RLP to be as low as 22 [23].\n(b) Memory Capacity Limits: Initial RLP is also constrained\nby the system's memory capacity, particularly for KV cache\nstorage. A computing system with 640 GB HBM can house\n282 requests with input and output lengths of 128, but only\n18 requests with input and output lengths of 2048 [23]. In\nthe latter case, the batch size needs to be smaller, as longer\nsequences need more memory capacity for KV cache for multi-\nhead attention.\n(c) Dynamic Batching: Dynamic batching [35] starts processing\na batch once the batch is full or exceeds a time limit. Therefore,\nwhen requests are infrequent, an LLM serving system with\ndynamic batching may start processing with different batch\nsizes, and thus, different RLP values.\nRuntime Request-Level Parallelism (Runtime RLP): Run-\ntime RLP refers to the RLP during the execution of a batch\nof requests. Runtime RLP depends on the batching mecha-\nnism used, which may be static batching or mixed continuous\nbatching [17].\nTraditional LLM serving systems [35, 36] use static batching\nwith batch-level scheduling. In this approach, no new requests\nare processed until all requests from the current batch have fin-\nished. Since each request has a unique output length, runtime\nRLP dynamically varies. As shown in Figure 3, runtime RLP\ndynamically decreases as each request of the current batch\nfinishes (i.e., as more decoding iterations take place) [37].\nMixed continuous batching [16, 17] allows token-level\nscheduling, where new requests can be added to be processed\nby the LLM serving system, while the system is executing\nrequests in the current batch. In this case, runtime RLP dy-\nnamically changes to keep the hardware resource utilization as\nhigh as possible, and it is dependent on when and how many\nrequests are added to each batch."}, {"title": "4. PAPI: Overview", "content": "Given that LLM inference exhibits varying parallelization\nlevels during runtime, an intelligent dynamic scheduling policy\nis necessary to identify the most suitable computing hardware\nfor a given kernel at a given time. The key challenge is to\ndesign a kernel offloading and allocation scheme that monitors\ndynamic parallelism online at low cost (in terms of latency\nand energy consumption) and selects the best-fit computing\nhardware to fully and efficiently utilize the available hardware\nresources.\n4.1. PAPI: Key Components\nWe propose the PAPI architecture and framework. Figure 5\nshows the overview of the PAPI framework. PAPI has three\nkey components explained next.\nHeterogeneous Architecture. We propose a heterogeneous\narchitecture to effectively cater to both compute-bound and\nmemory-bound kernels of LLMs. This architecture includes (1)\na host CPU, (2) a high-performance processor with PIM mem-\nory units (FC-PIM), and (3) physically separated (i.e., disaggre-\ngated) PIM units (Attn-PIM). The high-performance processor\nincludes processing units (hereafter referred to as PUs), e.g.,\nGPU tensor cores [53], PIM memory units (i.e., HBM-based PIM\ndevices), and a hardware scheduler. In our evaluation, we use\nGPU tensor cores for the PUs, but any other high-performance\nprocessor designed for compute-bound kernels (e.g., TPU [54]\nor NPU [55]) could also be used for this design. The host CPU\nsends instructions to the high-performance processor and the\nphysically separate Attn-PIM devices, which are disaggregated\nfrom the high-performance processor.\nHybrid PIM Units. We propose two types of PIM units to\ncater to the different parallelization levels of the FC and at-\ntention kernels of LLMs. FC-PIM units offer relatively high\ncomputation capabilities to cater to the FC kernels, while Attn-\nPIM units provide a larger memory capacity tailored to the\nattention kernel. The hybrid PIM units are designed to over-\ncome the limitations of prior existing PIM designs for LLMs\n(e.g., [23, 30, 56]), which typically support a single PIM unit\ntype with fixed computation capabilities. PAPI separates FC\nand attention kernels across different PIM devices. Since atten-\ntion kernels are always memory-bound, they are assigned to\nthe Attn-PIM devices. FC kernels can be either compute- or\nmemory-bound, and thus they can be dynamically allocated\nby the scheduler to either PUs or FC-PIM units.\nDynamic Parallelism-Aware Scheduling. As analyzed in\nSection 3.2, we need to identify whether or not the FC layer\nis memory-bound and dynamically offload it to the FC-PIM\nunits or the PUs of the high-performance processor. Instead,\nthe attention kernel is always memory-bound, only running\non the Attn-PIM units. We introduce a hardware scheduler\n(green block in Figure 5(a)) that monitors runtime paralleliza-\ntion changes and implements dynamic scheduling. When the\nparallelization level changes, our scheduler executes a low-cost\nidentification step, and offloads the FC kernel to the best-fit\ncomputing hardware. When the scheduler identifies the FC\nkernel as memory-bound, it executes FC on the FC-PIM devices.\nWhen it identifies FC as compute-bound, it executes FC on the\nhigh-performance processor PUs. In the latter case, FC-PIM\nmemory units are used as main memory to keep the weight\nparameters, which are loaded and processed by the PUs. Fig-\nure 5(d) illustrates an example of PAPI's dynamic monitoring.\nEvery time the parallelization level of the FC kernel changes,\nour dynamic monitoring framework is involved, identifying\nmemory-bound or compute-bound kernels, and reallocating\nthem to different units as needed."}, {"title": "5. PAPI Dynamic Scheduling", "content": "We propose an effective scheduling mechanism to offload FC\nkernels to PUs or FC-PIM units at runtime with low latency and\nlow energy consumption. In this section, we first explain how\nthe scheduling mechanism determines whether an FC kernel is\nmemory-bound, and then provide the implementation details\nof the runtime scheduling."}, {"title": "5.1. Memory-Boundedness Identification of the FC\nKernel", "content": "We identify whether or not the FC kernel is memory-bound\nby estimating its arithmetic intensity. Assume that the weight\nmatrix dimensions of the FC kernel are (h, h) and the input\ngiven is (RLP \u00d7 TLP, h), where h is the hidden dimension\nin the LLM structure. The arithmetic intensity of an FC kernel\ncan be calculated as follows:\n$AI = \\frac{\\#Flops}{\\#Bytes} = \\frac{RLP \\times TLP \\times h^2 \\times 2}{(2 \\times RLP \\times TLP \\times h + h^2) \\times 2}$\nIn state-of-the-art LLMs, the hidden dimension h is typically\nlarge to support their advanced natural language process-\ning tasks [4]. For example, h = 12288 in the GPT-3 175B\nmodel [32], and the arithmetic intensity can be estimated as\nfollows:\n$AI \\approx RLP \\times TLP$\nTherefore, we can use RLP \u00d7 TLP to estimate the arithmetic\nintensity of an FC kernel, where RLP and TLP are known at\nruntime.\nTo evaluate the accuracy of our arithmetic intensity estima-\ntion, we assess the FC kernel in the GPT-3 66B model using\nvarious RLP and TLP configurations. Figure 6 shows the ac-\ntual obtained arithmetic intensity our estimated values. In\nmost cases, our estimations very closely match the actual arith-\nmetic intensity. When parallelization level is very large (e.g.,\nRLP=128), the estimated value is slightly larger than the ac-\ntual arithmetic intensity. In such cases, the actual arithmetic\nintensity of the FC kernel exceeds the maximum theoretical\ncomputation throughput of the PUs of the high-performance\nprocessor. Therefore, this small deviation does not impact\nthe offloading decision, correctly identifying the FC kernel as\ncompute-bound and ensuring accurate scheduling."}, {"title": "5.2. Runtime Scheduling Implementation", "content": "Based on estimated arithmetic intensity, we identify\nmemory-bound or compute-bound FC kernels and dynami-\ncally schedule them to the best-fit computing hardware units\nat runtime. The scheduling process is executed on the host CPU\nin two steps: (a) initial scheduling and (b) runtime scheduling.\n5.2.1. Initial Scheduling. In the initial scheduling step, we\ndecide to offload FC kernels to PUs or FC-PIM units before the\nLLM serving starts. RLP is set to the batch size, and TLP\nis set to the system-defined speculation length. We multiply\nRLP by TLP to estimate the arithmetic intensity and compare\nit to a memory-boundedness threshold a to make the offloading\ndecision. If the estimated value is larger than a, the FC kernel is\nestimated as compute-bound and offloaded to PUs; otherwise,\nit is estimated as memory-bound and executed on FC-PIM\nunits. The threshold a is determined through offline iterative\nevaluation, where we run the FC kernel on both PIM and PU\nunits under varying parallelization levels, using the observed\nexecution times to establish the best a to choose.\n5.2.2. Runtime Scheduling. In runtime scheduling, we moni-\ntor changes in parallelism, predict the current arithmetic inten-\nsity, and determine whether or not to reschedule FC kernels to\na different computing hardware (i.e., from PUs to FC-PIM units\nand vice versa). We use a token-level scheduling scheme to\ntrack parallelism changes and make real-time decisions based\non the estimated arithmetic intensity. The process involves\nfour steps, which we describe next.\nFirst, after each decoding, we gather the output tokens of\nall requests in the current batch into a single vector. Second,\nwe count the number of <|eos|> tokens in this vector to track\nchanges in RLP. If the count is greater than zero, it indicates\nthat some requests have been finished, releasing the corre-\nsponding PIM resources allocated to Attn-PIM. TLP is typi-\ncally set initially and does not change frequently at runtime, so\nwe monitor changes in TLP with a direct approach: the TLP\nvalue is stored in a dedicated register, and if the system soft-\nware running on the host CPU modifies TLP, the host CPU\nnotifies (sending instructions) the PAPI system to update the\nregister accordingly. Third, we calculate RLP \u00d7 TLP to pre-\ndict the arithmetic intensity of the next decoding. Fourth, we\ncompare the estimated value with the memory-bound thresh-\nold a to decide whether rescheduling FC kernels from PUs/FC-\nPIM units to FC-PIM/PUs is needed. Figure 5(d) shows an\nexample of our proposed dynamic scheduling technique that\nenables the execution of LLM decoding on the most suitable\nhardware units of our proposed architecture based on the real-\ntime demands of the workload, significantly optimizing the\nperformance of LLM inference."}, {"title": "6. PAPI Architecture", "content": "6.1. FC-PIM Design\nTo meet the computation demands of the FC kernel, we\nneed to design a PIM solution with relatively high computation\nparallelism, while satisfying the necessary power constraints.\nWe modify and use an open-sourced HBM-based PIM simulator\n[23] that is based on Ramulator 2.0 [57, 58] to evaluate energy\nconsumption and power across different PIM configurations.\nWe first examine the energy breakdown in a traditional\nPIM design (e.g., [23]) that integrates one processing core per\nmemory bank, referred to as 1P1B. The energy consumption\nof PIM execution comes from three parts: DRAM Access,\nTrans fer, and Computation. DRAM Access includes the\nenergy consumption required to activate and precharge an\nHBM DRAM row to read the weight data. Transfer includes\nthe energy consumption of transferring activation data from\nthe buffer die, via the TSV, global controller, and bank group\ncontroller, to the processing core. Computation includes\nthe computation energy in floating point multiplication units\n(FPUs) of the processing core. As shown in Figure 7(a), most of\nthe energy in PIM execution is consumed by DRAM Access,\nwhich accounts for 96.7% of the total energy consumption\u00b9\nshould satisfy the following condition:\n$m(n \\times A_{FPU} + A_{bank}) \\le \u0410_{max}$\nThis equation allows us to calculate m to obtain the maximum\ncapacity achievable in a PIM-enabled HBM die using an nP1B\nPIM configuration.\nWe use the analytical tool CACTI-3DD [60] to estimate area.\nThe area of one HBM bank $A_{bank}$ is 0.83mm\u00b2 3 using a 22nm\ntechnology node. The area of one HBM die is constrained to\n121 mm\u00b2 according to prior work [61]. The area of one FPU\n$A_{FPU}$ is 0.1025 mm\u00b2 [23]. Thus, the equation for a 4P1B PIM\nconfiguration becomes as follows:\n$m(0.1025 \\times 4 + 0.83) < 121$\nTherefore, the maximum number of memory banks must be\nsmaller than 97. In our design, we use 96 banks per HBM\nmemory unit, i.e., 3 bank groups (BGs) in the 8-High HBM\nstack, so as to meet the area constraint of one HBM die with a\n4P1B PIM configuration, as shown in Figure 5(b).\n6.2. Attn-PIM Design\nTo address the varying arithmetic intensity, computation\ndemands, and memory footprint of FC and attention kernels,\nwhile ensuring high hardware resource utilization, we propose\ndedicated Attn-PIM units, separate from the FC-PIM units (as\ndescribed in Section 6.1). The Attn-PIM units are disaggregated\nfrom the high-performance processor through an interconnect.\nThis disaggregated design of Attn-PIM allows us to tackle the\ngrowing memory footprint demands of KV caches of LLMs, as\nwe explain next.\nWe find that FC kernels of LLMs have larger computation\nintensity and result in significantly larger latency than the\nattention kernels, while attention kernels have larger mem-\nory footprint demands. Therefore, given a fixed area budget,\nFC-PIM requires a configuration with higher computation ca-\npability, while the attention kernel does not need as much\ncomputation capability. To meet these constraints, we allocate\nFC-PIM devices with high execution parallelism, i.e., 4 FPUs\nper DRAM bank (as described in Section 6.1). In contrast, we\nallocate a larger number of Attn-PIM devices, each of which\nhas lower execution parallelism, using 1 FPU for every two\nbanks, as shown in Figures 5(b) and (c), respectively. Using\na single FPU for two banks in Attn-PIM devices ensures that\npower consumption stays within the HBM power constraints.\nFor attention kernels with a speculation length of 1, a single\nFPU at 666 MHz with 20.8 MB/s per-bank bandwidth (1P1B)\nmatches the arithmetic intensity of the kernel. However, due to\nthe lack of data reuse in this kernel, the power consumption of\n1P1B exceeds the power budget, as shown in Figure 7(c). Con-\nsequently, we adopt the 1P2B configuration for each Attn-PIM\ndevice to stay within the power consumption limits.\nAfter configuring the FC-PIM and Attn-PIM hardware de-\nsigns, we determine how many of each of the two types of\nPIM devices are required for the entire system to efficiently\nrun LLM inference. We configure the total number of PIM\ndevices in the system considering the capacity requirement\nof LLM inference. The memory capacity requirement for the\nFC kernel is determined solely by the model size and does\nnot change during runtime. However, the memory capacity\nrequired for the attention kernel increases linearly with the"}, {"title": "6.3. System Integration", "content": "Figure 5(a) shows an overview of the interconnection net-\nwork between the Attn-PIM devices and the high-performance\nprocessor and host CPU, where the high-performance proces-\nsor consists of FC-PIM devices and processing units. FC-PIM\ndevices require high-speed communication with the process-\ning units due to the large volume of weight parameters trans-\nferred. Therefore, we select high-speed interconnects like\nNVLink [62] to connect the FC-PIM devices with the process-\ning units. NVLink provides the required data throughput to\nensure that the FC kernels can be executed efficiently without\nbeing bottlenecked by data transfer speeds. In contrast, the\nattention kernel primarily involves small data transfers, such\nas byte-level Q vector, so a standard interconnect like PCIe\n(Peripheral Component Interconnect Express) [63] or CXL\n(Compute Express Link) [64] suffices, depending on the num-\nber of devices. PCIe theoretically supports up to 32 devices per\nbus [65], while CXL can scale to 4,096 devices [64]. These con-\nventional links offer adequate bandwidth for attention kernels\nand are more cost-effective than high-speed ones.\n6.4. Data Partitioning Across PIM Devices\nFor the attention kernel, we distribute attention heads across\nAttn-PIM units, with each head assigned to a separate HBM\ndevice. We employ the attention mapping scheme from At-\ntAcc [23] on an HBM device, which ensures efficient data\nmovement and parallelism across the PIM architecture. Specifi-\ncally, the KT matrix is partitioned column-wise at the pseudo-\nchannel and bank-group levels, and row-wise at the bank and\nmultiplier level. Conversely, the V matrix is partitioned row-\nwise at the pseudo-channel and bank-group levels, and column-\nwise at the bank and multiplier level.\nFor the FC kernel, the large weight matrix is first divided\ninto smaller 2D blocks, each mapped to an HBM device. At the\npseudo-channel, bank-group, and bank levels, these weight\nblocks are partitioned similarly to the KT matrix in the at-\ntention kernel: column-wise at the pseudo-channel and bank-group levels, and row-wise at the bank level.\n6.5. Practicality and Architectural Scalability\nComplementary PIM Units for Diverse Workloads. We\ndesign different FC-PIM and Attn-PIM devices to address dis-\ntinct computation and memory access patterns in LLMs while\nmaintaining hardware practicality. Both FC-PIM and Attn-\nPIM devices share the same bank-level computation fabric\nand memory hierarchy. The key difference lies in the number\nof processing units (PUs) per bank, which is tailored to the\nspecific computation characteristics of LLM tasks. Attn-PIM,"}, {"title": "7. Evaluation", "content": "7.1. Evaluation Methodology\nComparison Points and Simulation Methodology.\nWe compare PAPI with three state-of-the-art systems: (a)\nA100+AttAcc: a heterogeneous computing platform with 6\nNVIDIA A100 GPUs [29] and AttAcc PIM-based units (one\nFPU unit per DRAM bank, i.e., 1P1B configuration), which is\nthe state-of-the-art design proposed by prior work [23]. All"}, {"title": "8. Related Work", "content": "To our knowledge, PAPI provides the first architecture and a\nruntime framework to tackle dynamically varying paralleliza-\ntion levels and hence dynamically varying computation and\nmemory demands of real-world LLM workloads. We compre-\nhensively compare PAPI to two state-of-the-art PIM designs,\nAttAcc [23", "30": "demonstrating PAPI's signifi-\ncant performance and energy benefits over them (Section 7.2).\nPIM-enabled LLM accelerators. The PIM computing\nparadigm [82", "12,30,56,75,83\u201389": "provides a promising opportunity to\naccelerate the memory-bound kernels in the decoding phase.\nDRAM-based PIM [83, 90", "56": "offloads both FC and\nattention kernels to GDDR6-PIM accelerators, outperforming\nA100 GPUs in single-batch scenarios. However, the architec-\nture performs poorly when FC kernels are compute-bound,\ne.g., with larger batch sizes.\nPrior works propose heterogenous PIM-enabled computing\nsystems for LLM inference. AttAcc [23", "25": "of-\nfloads all FC kernels to PIM to efficiently handle non-batched\nrequests. This would provide low performance in scenarios\ninvolving batched requests, which are common in real-world\nLLM inference. SpecPIM [24"}]}