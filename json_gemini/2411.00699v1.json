{"title": "Algorithmic Transparency in Forecasting Support Systems", "authors": ["Leif Feddersen"], "abstract": "Most organizations adjust their statistical forecasts (e.g. on sales) manually. Forecasting Support Systems (FSS) enable the related process of automated forecast generation and manual adjustments. As the FSS user interface connects user and statistical algorithm, it is an obvious lever for facilitating beneficial adjustments whilst discouraging harmful adjustments. This paper reviews and organizes the literature on judgemental forecasting, forecast adjustments, and FSS design. I argue that algorithmic transparency may be a key factor towards better, integrative forecasting and test this assertion with three FSS designs that vary in their degrees of transparency based on time series decomposition. I find transparency to reduce the variance and amount of harmful forecast adjustments. Letting users adjust the algorithm's transparent components themselves, however, leads to widely varied and overall most detrimental adjustments. Responses indicate a risk of overwhelming users with algorithmic transparency without adequate training. Accordingly, self-reported satisfaction is highest with a non-transparent FSS.", "sections": [{"title": "1. Introduction", "content": "Man or machine\u2014who is the better decision-maker? This question, some would argue, was answered almost 70 years ago in favor of machines by Meehl (1954) in the context of clinical versus statistical disease prediction. Despite recent advances in artificial intelligence\u2014making machines even more flexible yet precise decision-makers\u2014humans often insist on having the last say. Past misadventures with decision automation\u2014for instance, the automated demand forecasting issues at Nike (Worthen, 2003)\u2014and legitimate warnings about a potential 'intelligence explosion' in the future (Russell et al., 2015) suggest that this hesitation is justifiable. However, relying solely on human decision-making brings with it a long list of documented flaws. Using their respective strengths, the reconciliation and integration of human and algorithmic decision-making appears to be a sensible overarching goal for research and practice.\nForecasting is a crucial organizational function, which supports the Sales and Operations Planning Process (S&OP). The generation and adjustment of forecasts is often facilitated and supported by a Forecasting Support System (FSS) (Boylan and Syntetos, 2010, p.233), a subcategory of Decision Support Systems (DSSs). Its common understanding is that of software that connects to the organization's database, visualizes data, produces statistical forecasts, and allows for incorporating managerial judgment (Fildes et al., 2006, p.352).\nMost organizations first automatically create quantitative forecasts, e.g., on sales or demand, and subsequently adjust them manually (Fildes et al., 2009), often via an FSS user interface. Case studies on forecasting processes suggest that the effects of adjustments on forecasting accuracy are highly variable, often resulting in only a small net improvement at the expense of valuable working hours (Fildes and Goodwin, 2021). Controlled laboratory studies on judgmental forecasting and adjustments paint an even more pessimistic picture of human extrapolation capabilities. Human forecasters suffer from biases, noise (Hogarth and Makridakis, 1981), and frequently unwarranted aversions to algorithmic advice (Dietvorst et al., 2015).\nMitigating these problems and facilitating more accurate forecasts re-"}, {"title": "2. Literature Review", "content": "This literature review begins by examining isolated forecasting tasks with minimal context provided to human judges, then extends to experiments and case studies where additional information is incorporated. These sections focus on human biases in judgment, rooted in the 'heuristics and biases' program (Tversky and Kahneman, 1974). Subsequently, the task environment is explored from the 'fast and frugal' perspective (Gigerenzer, 1991), highlighting discrepancies between laboratory studies and practice. Finally, I discuss empirical findings on FSS design aimed at mitigating these challenges, including studies that structure information input and output in ways transferable to FSS design."}, {"title": "2.1. Isolated Forecasting Tasks", "content": "In isolated forecasting tasks, where subjects are provided only with the historical time series or a statistical baseline forecast, they exhibit decision biases leading to inferior predictions compared to algorithmic forecasts (Hogarth and Makridakis, 1981, p. 126). These biases have been established within the general 'heuristics and biases' program (Tversky and Kahneman, 1974) and adapted for judgmental forecasting, such as the anchor and adjust heuristic, while others are forecasting-specific\nAnchor and Adjust. The anchor and adjust heuristic suggests that human judges initially form or possess a fixed value before engaging deeply in a decision task. In time series forecasting, this anchor may be the most recent observation (Kremer et al., 2011), the series' mean (Bolton et al., 2012), an initial prediction (Lim and O'Connor, 1996a), or another salient cue. Decision-makers often adjust conservatively from this anchor, regardless of its rational predictive strength. Interestingly, Lawrence and O'Connor (1995) found that judges excessively adjusted the supposed anchor\u2014a normative statistical prediction\u2014which may be explained by algorithm aversion, discussed later."}, {"title": "Noise Modeling and Hindsight Bias.", "content": "Noise modeling refers to the tendency of human judges to perceive patterns in random fluctuations and extrapolate them into the future, leading to inaccurate forecasts. For instance, O'Connor et al. (1993) challenge the assumption that humans adapt better to changing environments than algorithms. Their study shows that MBA and Ph.D. students forecasting artificial time series with level changes are not only slower than exponential smoothing methods in detecting these changes but also attempt to extrapolate from random noise. Similarly, Kremer et al. (2011) describe the overweighing of recent observations relative to the complete history-a phenomenon they term system neglect\u2014which leads to overreaction in stable environments and underreaction in unstable ones. This behavior is closely related to the base-rate fallacy (Tversky and Kahneman, 1974), where judges ignore underlying probabilities when faced with salient cues.\nAn underlying cause of noise modeling is hindsight bias, where decision-makers perceive past events as more predictable than they were (Roese and Vohs, 2012). This bias leads them to overlook alternative outcomes, fostering overconfidence in their predictive abilities. The combination of perceiving illusory patterns in randomness and overestimating one's capabilities exacerbates errors in judgmental forecasting."}, {"title": "Extreme Preference.", "content": "Noise modeling can lead to extreme forecasts, and evidence suggests that extremeness is a desirable attribute in judgmental forecasts and adjustments. For instance, Kahneman and Tversky (1973) found that students given less predictive information did not produce more conservative forecasts as rationality would suggest; instead, they maintained extreme predictions. Similarly, De Bondt and Thaler (1990) analyzed real-world security analysts' earnings forecasts and observed that their adjustments resulted in forecasts that were, on average, 54% more extreme for a one-year horizon and 117% for a two-year horizon. Dietvorst and Bharti (2020) experimentally confirmed that decision-makers exhibit diminishing sensitivity to errors, preferring high-variance human judgments over low-variance algorithmic predictions, even when the latter are more accurate. This preference for high-variance models is also observed in a case study by Fildes and Goodwin (2021, p.12), where some managers are described as intolerant of noise (i.e., unexplained residuals and random error) in fitted models."}, {"title": "Trend Dampening.", "content": "Another observed behavior is trend dampening, where judges reduce the magnitude of linear trends (i.e., regress to the mean), especially for downward-sloping series (e.g., Lawrence and Makridakis, 1989; Webby and Edmundson, 1994). As demonstrated by Gardner and Mckenzie (1985), this behavior often reflects the rational expectation that most trends do not continue indefinitely. While humans are notoriously bad at grasping exponential growth, the concept of diminishing returns (i.e., logarithmic or damped linear growth) is familiar to most people."}, {"title": "Gambler's Fallacy.", "content": "The gambler's fallacy is a robust phenomenon where decision-makers expect a reversal after a streak of similar outcomes. In forecasting, its manifestation is less straightforward. Kremer et al. (2011) operationalize forecasts that go against the recent pseudo-trend of a stationary time series as a result of the gambler's fallacy but find that only 11% of subjects forecast accordingly. Additionally, Petropoulos et al. (2016) identify several detrimental forecast adjustments attributable to other decision biases but not the gambler's fallacy. These findings suggest that the gambler's fallacy is not prevalent in typical forecasting tasks unless specific cues lead judges to erroneously anticipate mean reversion."}, {"title": "2.2. Forecasting with Extra Information", "content": "A common objection against algorithmic predictions is, \"I know things the model cannot know.\u201d Indeed, omitted variables can lead to model deficiencies. Paul Meehl famously coined the \"broken-leg\u201d cue example, describing a statistical model predicting cinema attendance. Regardless of the model's average accuracy, knowing a person has broken their leg allows us to predict with certainty they will not attend the cinema (Meehl, 1954). In forecasting, a \"broken-leg\" cue might be knowledge of a supply shortage, a recent trend change, or a planned promotion. Importantly, by definition, the cue must not be included in the model; otherwise, correcting for it leads to the double counting bias (Jones et al., 2006). I now review findings on how well human judges incorporate extra-model information."}, {"title": "2.2.1. Laboratory Research", "content": "Lim and O'Connor (1996b) employ a simple experimental setup: Participants forecast soft-drink sales at Bondi Beach given historical sales. They are then provided either with a statistical forecast, temperature data (either highly predictive or mildly predictive), or both, and can adjust their initial forecast. Results show that participants are sensitive to the temperature data's predictive validity and utilize the statistical forecast. However, they do so suboptimally and do not increase their weighting of the extra information over time. Interestingly, they prefer the statistical model over the temperature data, possibly because temperature uses a different scale.\nA study by Harvey et al. (1994) uses a more complex forecasting problem. Participants are asked to forecast the sinusoidal pattern of train passengers and criminals on the train. While subjects are relatively precise in separate forecasts, their performance drops significantly when forecasting passengers given only the criminals' time series. Correct forecasts would have required participants to internalize the relationship between the two series, which involved a phase shift and amplitude difference.\nAdditional studies by Garner (1982) and Becker et al. (2007, 2008) yield similar results, indicating that humans incorporate quantitative extra information suboptimally (Leitner and Leopold-Wildburger, 2011, p.462), especially if the relationship is nonlinear. This finding is not surprising given human limitations in processing quantitative data accurately; an adequate statistical model would easily produce optimal forecasts in these conditions.\nSoft information refers to non-quantifiable cues, such as rumors, gossip, or advice. Operationalizing soft information in laboratory research is challenging because providing it explicitly makes it less 'soft' in the traditional sense (Mintzberg, 1975). With that in mind, several studies have supplied human forecasters with soft information.\nIn Remus et al. (1998), business students forecast time series given correct, incorrect, or no soft information framed as rumors about future movements. Participants successfully utilize correct information and quickly identify incorrect information. Receiving correct information after incorrect information leads to short-term discounting of new information, but this effect does not persist long-term.\nGoodwin et al. (2011) explore the effectiveness of restricting and guiding FSS designs. They find that participants are particularly persistent in acting on rumors, which only indicate the direction (not magnitude) of necessary adjustments and are correct only 62.5% of the time. They hypothesize that rumors were more salient in their study than other information sources and, in practice, are often associated with obligations within the organizational hierarchy."}, {"title": "2.2.2. Case and Field Studies", "content": "Case studies provide a better gauge for the effects of managerial intervention on forecast accuracy but, lacking experimental controls, make statements about cause and effect more challenging. With these qualifications in mind, we discuss relevant findings.\nBlattberg and Hoch (1990) examine expert and statistical forecasts in five companies. Their analysis reveals that both forecast types have comparable cross-validated R\u00b2 and that combining model and expert forecasts improves accuracy. They calculate that managers' forecasts explain, on average, 25% of the model's unexplained variance, supporting the notion that managerial intuition is valuable but flawed.\nSanders and Ritzman (1992) compare real-world forecasts of experienced warehouse planners with those of students and a statistical ensemble method. Their results show that the statistical forecast performs similarly to experts for time series with low variability, but experts have an advantage for highly variable series.\nSimilarly, Edmundson et al. (1988) distinguish practitioners with product and industry knowledge from those with only industry knowledge. Their results show that only product knowledge leads to significantly more accurate forecasts.\nA series of studies by Mathews and Diamantopoulos (1986, 1989, 1990, 1992) involving a UK healthcare company found that judgmental adjustments increase forecasting accuracy overall but introduce an optimism bias. Managers are effective at selecting inaccurate statistical forecasts for revision, and large adjustments more often lead to improvements than small adjustments.\nEroglu and Croxton (2010) analyze the influence of personality and motivational factors on forecasting biases. Notably, the propensity to adjust forecasts is reliably determined by work experience in the current position, an internal locus of control, and a challenge-seeking orientation.\nFildes and Goodwin (2021) provide a detailed case study of the forecasting process in a UK pharmaceutical company. They find that while large negative adjustments added value, positive adjustments, especially small ones, decreased accuracy on average. The managers displayed typical noise modeling behavior and adjusted forecasts to satisfy colleagues' desires for smooth-looking but overfitted models."}, {"title": "2.3. The Task Environment", "content": "The previous sections paint a pessimistic picture of human involvement in forecast creation. However, some case studies find that managers enhance statistical forecasts under certain circumstances. These observations are consistent with the perspective of ecological rationality proposed by Luan et al. (2019), which offers an alternative to the \"heuristics and biases\" program by Tversky and Kahneman (1974).\nBoth approaches agree that humans often use heuristics simple decision rules that ignore some information and do not involve mathematical optimization. They differ in interpretation: While heuristics can lead to biased decisions in inappropriate environments, they can produce \"fast and frugal\" decisions when matched to the environment (Luan et al., 2019, p.2). The appropriateness of a heuristic for a given environment determines its accuracy, and selecting an appropriate heuristic is a hallmark of an intelligent decision-maker (Gigerenzer, 2015, p.178).\nTherefore, successful practitioners may be examples of effectively applied ecologically rational heuristics. Heuristics are often better suited for real-world situations characterized by uncertainty, whereas statistical models are more appropriate for problems involving risk with known probabilities.\nEcological rationality also implies that forecast adjustments may reflect goals beyond improving accuracy. Personal and organizational circumstances can impose additional motives, leading to adjustments that seem irrational from an accuracy standpoint."}, {"title": "2.3.2. The Organizational Environment", "content": "The organizational environment can impose various motives for over-adjusting forecasts. These motives can arise from the adjuster's role within the organization or from the firm's functional and structural organization.\nJudgmental adjustments can serve as a signal that a manager has reviewed and approved the forecasts. Beyond this basic signal, adjustments can convey competence and credibility, especially in environments with low levels of post-adjustment accuracy tracking (Fildes and Petropoulos, 2015). Adjustments may also help managers maintain and improve their forecasting skills in case the algorithmic system fails (Bainbridge, 1983; De Baets and Harvey, 2018).\nInvolvement in forecast creation increases ownership and acceptance of the forecast (e.g., Lawrence et al., 2002). This increased acceptance extends beyond the adjuster; for example, analysts may adjust forecasts to satisfy colleagues' preferences (Fildes and Goodwin, 2021, p.12). \u00d6nkal et al. (2008) demonstrate experimentally that judges prefer judgmentally adjusted fore-"}, {"title": "Structure-Driven Motives.", "content": "In a functionally distributed organization, forecasts can have different implications for each functional area, leading to incentives to bias forecasts. Mello (2009) and Oliva and Watson (2009, p.143) describe scenarios such as enforcing, filtering, hedging, sandbagging, spinning, second-guessing, and withholding, where forecasts are deliberately adjusted upwards or downwards due to various organizational motives."}, {"title": "2.3.3. The Experimental Environment", "content": "Laboratory settings can also facilitate unintended ecological rationality among subjects. \"Winner takes all\" incentives, as implemented in some studies (Prahl and Van Swol, 2017; Lim and O'Connor, 1995, 1996a), nudge subjects to provide more extreme forecasts. This occurs because extreme deviations may increase the chance of outperforming competitors or the provided algorithmic forecast.\nIncentivizing participants based on absolute performance is a potential remedy. However, defining an appropriate incentive scheme must account for loss aversion (Kahneman and Tversky, 1979). An effective incentive scheme for judgmental forecasting studies should encourage accuracy without promoting extreme adjustments."}, {"title": "2.4. The Forecasting Support System", "content": "An optimal FSS, as formulated by Fildes et al. (2006, p.354), \" (i) improves the forecaster's ability to realize when judgmental intervention is appropriate and (ii) enables the user to apply accurate judgmental interventions when these are appropriate.\"\nDesign approaches toward these goals have been the subject of diverse empirical and theoretical contributions. Key areas include restrictiveness and guidance, model selection and parameterization, trust in algorithmic advice, and the structuring of informational inputs and outputs."}, {"title": "2.4.1. Restrictiveness and Guidance", "content": "In his seminal work on DSS design, Silver (1991) introduces the concepts of guidance and restrictiveness as \"meta-support\u201d for using a DSS. For example, an FSS could be highly restrictive, allowing no human input, or it could permit adjustments and guide users on when and how to adjust forecasts."}, {"title": "Effort Manipulations.", "content": "Subtle restrictiveness can be implemented by making desirable forecasting strategies less effortful than less desirable ones (Fildes et al., 2006, p.355). Based on the effort-accuracy framework by Payne et al. (1993), this approach suggests that increasing the effort required to make adjustments can reduce harmful adjustments. Goodwin (2000) find that requiring explicit requests for adjustments or asking for reasons reduces harmful adjustments while maintaining beneficial ones."}, {"title": "Explicit Support.", "content": "Goodwin et al. (2011) compare the effects of restrictive and guiding FSS designs. They find that guidance can encourage necessary adjustments and discourage unnecessary ones, but overly restrictive designs may backfire, leading to larger and more harmful adjustments."}, {"title": "2.4.2. Model Selection and Parameterization", "content": "Allowing users control over model selection and parameterization can increase satisfaction and reduce subsequent adjustments (Lawrence et al., 2002; Petropoulos et al., 2018). While human decision-makers may not always select the optimal model, they are often effective at avoiding the worst options. This aligns with case study reports where managers successfully identified and corrected poor statistical forecasts."}, {"title": "2.4.3. Algorithm Aversion and Trust", "content": "Despite the demonstrated superiority of algorithmic predictions in many contexts (Meehl, 1954; Dawes, 1979), algorithmic advice remains underutilized. Dietvorst et al. (2015) coin the term \"algorithm aversion\u201d to describe this tendency.\nA systematic review by Burton et al. (2020) identifies five major drivers of algorithm aversion: false expectations, lack of perceived control, cognitive compatibility, lack of incentives, and divergent rationalities. These factors contribute to underutilization of algorithmic advice, especially after observing algorithm errors.\nExperimental research shows that judges rely less on algorithmic advice after seeing it err, even when it outperforms human judgment (Prahl and Van Swol, 2017; Dietvorst et al., 2015). This may be due to inflated initial expectations of algorithmic accuracy and skepticism about the algorithm's ability to learn from mistakes (Berger et al., 2021)."}, {"title": "2.4.4. Algorithmic Transparency", "content": "Algorithmic transparency\u2014the extent to which the inner workings or logic of automated systems are known to human operators\u2014can mitigate algorithm aversion (Seong and Bisantz, 2008, p.611).\nTransparency enables users to develop a mental model of the algorithm, facilitating trust and appropriate reliance. However, making complex algorithms transparent requires higher levels of abstraction. Additionally, if an algorithm is perceived as too simple upon being made transparent, users may distrust it, leading to disuse (Lehmann et al., 2022).\nTransparent algorithms can also help users understand the decision object better. By tracking the decision process, users can learn about relationships in the data and intervene more precisely when algorithmic decisions contradict extra-model information.\nWhile algorithmic transparency holds promise, research in the context of time series forecasting is limited. Further exploration is needed to understand its potential benefits and pitfalls."}, {"title": "2.4.5. Structuring and Decomposition", "content": "Structuring judgmental decisions through decomposition is a well-established method to reduce bias and noise. Experiments have shown that decomposing time series into components like trend and seasonality allows human forecasters to make more accurate predictions.\nEdmundson (1990) developed an application called GRAFFECT, guiding users to identify trends and seasonal patterns before forecasting. This structured approach resulted in forecasts significantly better than unstructured judgmental forecasts and comparable to statistical methods.\nMarmier and Cheikhrouhou (2010) focus on integrating expert knowledge about event effects into forecasts. By categorizing factors (e.g., transient factors, trend changes) and incorporating them structurally, they demonstrate significant improvements over purely statistical forecasts.\nAsimakopoulos et al. (2009) compare user satisfaction among FSS prototypes with varying levels of information structuring. Participants preferred highly structured information and valued opportunities to document and reflect on their reasoning."}, {"title": "2.5. Visual Design and Elements", "content": "The visual display of information in an FSS can significantly impact user interaction. The format used to display data, the arrangement of elements, and the prominence of information all influence decision-making (Silver, 1991).\nFor example, Harvey and Bolger (1996) find that graphical displays help subjects identify trends but may also lead to perceiving trends in noise.\nTheocharis et al. (2018) show that line graphs prime viewers to see data as sequences, which can be beneficial for trended data but may encourage erroneous trend identification in random data.\nBar graphs, while effective at conveying metrics, can introduce biases. Newman and Scholl (2012) demonstrate a \"within-the-bar\" bias, where subjects perceive points within the bar as more likely. Harvey and Reimers (2012) show that bar graphs lead to systematically lower forecasts than points and lines for trended series."}, {"title": "2.6. Literature Review Summary", "content": "This literature review highlights the current state of judgmental forecasts and adjustments, the handling of extra information, the role of the task environment, and the central importance of FSS design. Human judges are generally less accurate at extrapolating univariate time series than statistical methods and often struggle to incorporate extra-model information effectively. Experienced forecasters in practice can enhance statistical forecasts, but results are mixed, and the cost of human intervention must be considered. Adjustment motives may extend beyond improving accuracy due to personal and organizational factors.\nThe critical role of the FSS has been widely recognized. Research shows that users appreciate transparent explanations, structure, and opportunities to incorporate their judgment. Attempts to restrict or guide FSS usage are promising but can backfire if not carefully designed. Algorithmic transparency is underexplored in the FSS context and presents potential for fruitful research, connecting well with successful concepts like forecast explanations and structured decision-making."}, {"title": "3. Derivation of Hypotheses", "content": "The literature review revealed that algorithmic transparency and related concepts like understandability and explanations positively affect advice utilization. However, to the best of my knowledge, no experimental study to date has examined algorithmic transparency in the context of an FSS-aided forecasting task.\nI posit that algorithmic transparency should improve all relevant metrics in a scenario where a sales manager reviews, adjusts, and signs off algorithmic forecasts. Overall, transparency should increase trust in the statistical forecast and lessen the desire to adjust it:\nHypothesis 1a. Increasing FSS transparency leads to a smaller adjustment volume.\nSince most adjustments, especially without extra predictive information, harm accuracy, it follows that:\nHypothesis 1b. Increasing FSS transparency leads to higher forecast quality post adjustments.\nThe first two hypotheses relate to the quality of final forecasts and adjustment behavior. For an FSS to be beneficial, it is also necessary that it is acceptable to its users; otherwise, it might go unused in the long run. Transparency is a promising feature to increase acceptance as it can foster understanding and learning about the model and the modeled system. Users might even enjoy exploring how an algorithm interprets a time series. Thus, Hypothesis 2 posits:\nHypothesis 2. Increasing FSS transparency leads to higher user satisfaction.\nThe next section describes the background of the experimental study aimed at exploring these hypotheses."}, {"title": "4. Experimental Background", "content": "An integral part of an FSS is the underlying model generating forecasts. For algorithmic transparency to be effective, the model must be decomposable into interpretable parts. Prophet accommodates these requirements and is therefore used to generate all forecasts for this work's experiment (Taylor and Letham, 2018). Below, I briefly outline its core components and practical implications.\nProphet was developed to allow robust, automated forecasting at scale while enabling flexible integration of domain knowledge by analysts without deep time series expertise. It is a Generalized Additive Model (GAM) with the following formulation:\n$y(t) = g(t) + s(t) + h(t) + e_t$\nwhere $y(t)$ represents the forecast, $g(t)$ is the non-periodic piecewise trend, $s(t)$ the periodic seasonalities, $h(t)$ external regressors like holidays or events, and $e_t$ is the error term.\nProphet's trend component is piece-wise between changepoints, which allows for flexibility in adapting to trend changes. Seasonal cycles can be modeled with an arbitrary number of Fourier series, and external regressors like holidays can be included. Prophet's backend, built in Stan, provides posterior estimates for all parameters, offering probabilistic forecasts.\nThe main advantage of Prophet is its interpretability and its suitability for the \"Analyst in the Loop\" process, where flagged forecasts can be reviewed and adjusted by analysts based on their domain knowledge. Prophet flags forecasts based on criteria like underperformance against benchmarks (e.g., naive forecasts) or specific performance thresholds (e.g., maximum Mean Absolute Percentage Error (MAPE)). Adjustments can be made in either a programming environment (Python/R) or an interactive application (e.g., Voila or Shiny)."}, {"title": "4.2. M5 Competition Data", "content": "The M5 Competition, launched in 2020, provided Walmart sales data for forecasting. It includes daily sales data for 3,049 products across 10 stores over a 1,941-day period, with a total of 42,840 hierarchical time series. The dataset includes extra features like holidays, promotions, and prices, allowing for complex, real-world forecasting scenarios (Makridakis et al., 2022)."}, {"title": "4.3. Data Selection and Model Specifications", "content": "For the experiment, I select a subset of the M5 dataset, specifically focusing on \"foods\u201d to ensure relevance and reduce cognitive load in the experiment. Ten time series were chosen for their comparability, with a forecast horizon of 14 days to reflect realistic decision-making processes while simplifying participant tasks. Prophet models were optimized based on cross-validation, primarily tuning the trend and seasonality regularization parameters."}, {"title": "4.4. Participant Selection and Recruitment", "content": "Participants are recruited from mTurk, a platform allowing fast and cost-effective collection of data. Using cloudresearch (Litman et al., 2017), participant selection is enhanced by filtering for only US-based Workers with management or sales roles, or students aiming for such positions, to increase external validity. I conduct pre-studies to screen for Workers showing engagement with forecasting tasks. Those were invited to participate in the main experiment."}, {"title": "4.5. Incentive Scheme", "content": "Performance-dependent incentives are critical in ensuring truthful and engaged responses. In this study, I incentivize participants based on improvements over algorithmic forecasts. Each adjustment is judged on its ability to improve forecast accuracy, encouraging careful consideration without rewarding extreme adjustments. This scheme is designed to reflect organizational environments where adjustments are often not tracked, allowing for selective reporting of beneficial adjustments."}, {"title": "4.6. Measured Indicators", "content": "To test the hypotheses, several indicators are measured:"}, {"title": "Adjustment Volume.", "content": "Adjustment Volume (AV) measures the sum of absolute adjustments made by participants relative to optimal adjustments for 100% accuracy:\n$AV = \\frac{\\sum_{t=1}^{T} |y_{t}^{model} - y_{t}^{final} |}{\\sum_{t=1}^{T} |y_{t}^{model} - y_{t}^{truth}|}$"}, {"title": "Adjustment Frequency.", "content": "Adjustment Frequency (DF) captures how often participants adjust forecasts, calculated as the fraction of forecasts that were modified:\n$Adjustment Frequencyp = \\frac{\\sum_{t=1}^{N} I(AV)}{N}$"}, {"title": "Relative MAE.", "content": "Relative Mean Absolute Error (rMAE) is the ratio of the Mean Absolute Error (MAE) of the participant's forecast to the algorithmic forecast:\n$rMAE_{a,b} = \\frac{MAE_{a}}{MAE_{b}}$"}, {"title": "Mean Absolute Percentage Error (MAPE).", "content": "measures the average percentage error between the participant's forecast and the actual values:\n$MAPE = \\frac{1}{T} \\sum_{t} \\frac{|y_{t} - \\theta_{t}|}{y_{t}}$"}, {"title": "5. Experimental Study", "content": "This work's experimental study examines the effects of algorithmic transparency on user interaction and satisfaction with an FSS. To that end, I implement three FSS designs with increasing degrees of transparency and let participants review, adjust, and sign-off Prophet forecasts within the respective interfaces. I hypothesized that algorithmic transparency would lead to overall less but better adjustments and increase user satisfaction."}, {"title": "5.1. Operationalizing Algorithmic Transparency", "content": "I operationalize FSS transparency by juxtaposing three FSS designs: The Opaque FSS Design Treatment (O) makes no attempts at explaining the underlying algorithmic model. The Transparent FSS Design Treatment (T) makes the model transparent to the user by dynamically displaying its components. The Transparently Adjustable FSS Design Treatment (TA) further allows the individual adjustment of the components.\nTransparently Adjustable. The TA FSS operationalizes the concept of algorithmic transparency to the maximum extent, that I view behaviorally feasible in the context of an online experiment. Bunn (1991, p.513) conclude their literature review on the integration of judgmental and statistical forecasting with the call for \u201can interactive decomposition structure\u201d, explicitly referencing the judgmental seasonal decomposition approach by Edmundson (1990) as a promising basis towards a synthesis of judgment and algorithm. The TA FSS aims to be exactly that. It allows for judgmental overrides, just like GRAFFECT (Edmundson, 1990), but a statistical algorithm sets all initial parameters (see Section 4.3)"}, {"title": "5.2. Experimental Procedure", "content": "I set up the recruitment of 240 Workers on mTurk with the requirements described in Section 4.4.\nAfter entering their Worker ID, participants are informed that the graph shows the sales data for one out of three foods product. As a product manager, they are responsible for the 14-day ahead forecasts for three such products and an algorithmic model aids them. The incentive is prominently displayed and reads that accuracy improvements over the model forecast through adjustments result in a bonus of 0.20 USD per percentage point, up to 1 USD per product, hence up to 3 USD per Human Intelligence Task (HIT).\nNext, for TA and T, participants learn that the model understands the sales data \"step by step\". First, the trend level (\u201cgeneral level of sales", "yearly pattern\", and the small graph for yearly effects appears below the main graph. In the next step, weekly effects and the weekly graph are added analogously. Finally, the model adds the event effects to the gray line to represent the \u201cmodel's final estimation": "nFor the Opaque FSS design, the steps mentioned above reduce to the direct display of the fitted model without further explanations.\nNext, participants are asked to familiarize themselves with the functions for navigating the graph and exploring the data:"}, {"title": "5.3. Overhaul in Response to Pre-Test", "content": "A pre-test informed the previously outlined FSS designs and experimental procedure. It implemented an early version of the experimental design, which revealed several design flaws. For the pre-test, I invited a small sample of ten Workers who displayed high levels of engagement in the pre-studies (as measured by engagement time and number of interactions), to partake in the planned experiment while commenting on things they liked, disliked, found confusing, or suspected to be confusing to others. The comments turned out to be longer and more detailed than I had hoped but indicated that roughly one-half of respondents was at some point confused about one or more crucial elements. Their points of confusion or criticism matched those of friends and peers who engaged with my early FSS prototype. I will present the lessons learned in terms of their implications for the final FSS and experimental design, contrasting the old and revised new versions:"}, {"title": "5.4. Results", "content": "Due to mTurk's mechanics, each experimental treatment initially contains more than 80 participants. I drop seven duplicate participants from T who managed to participate in it after having participated in O. I further drop 14 participants with overall completion times below three minutes."}, {"title": "5.4.1. Hypotheses Testing", "content": "Adjustment Volume. The average AV per treatment is shown in Table 2. Both transparent conditions (T and TA) have a significantly lower AV than the Opaque (O). This finding may be driven by the lower DF (see Equation"}]}