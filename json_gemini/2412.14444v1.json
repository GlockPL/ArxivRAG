{"title": "GenHMR: Generative Human Mesh Recovery", "authors": ["Muhammad Usama Saleem", "Ekkasit Pinyoanuntapong", "Pu Wang", "Hongfei Xue", "Srijan Das", "Chen Chen"], "abstract": "Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at https://m-usamasaleem.github.io/publication/ GenHMR/GenHMR.html", "sections": [{"title": "Introduction", "content": "Recovering 3D human mesh from monocular images is an essential task in computer vision, with applications spanning diverse fields, such as character animation for video games and movies, metaverse, human-computer interaction, and sports performance optimization. However, recovering 3D human mesh from monocular images remains challenging due to inherent ambiguities in lifting 2D observations to 3D space, flexible body kinematic structures, complex intersections with the environment, and insufficient annotated 3D data (Tian et al. 2023). To address these challenges, recent efforts have been focusing on two methods: (1) deterministic HMR and (2) probabilistic HMR. Both methods face critical limitations.\nDeterministic methods, as the dominant approach for HMR, are designed to produce a single prediction for a given 2D image. These methods estimate the shape and pose parameters of 3D body model either by regressing from 2D image features extracted from deep neural networks (Choi et al. 2022; Kocabas et al. 2021; Kanazawa et al. 2018) or by directly optimizing the parametric body model by fitting it to 2D image cues, such as 2D keypoints (Bogo et al. 2016; Pavlakos et al. 2019; Xu et al. 2020), silhouettes (Omran et al. 2018), and part segmentations (Lassner et al. 2017). Recent deterministic HMR models, utilizing vision transformers as their backbone, have achieved state-of-the-art (SOTA) mesh reconstruction accuracy. However, despite"}, {"title": "Related Work", "content": "The field of Human Mesh Recovery (HMR) from monocular images has been primarily dominated by deterministic approaches, which aim to generate a single output for a given 2D image. The early work mainly adopts optimization-based approaches to fit a parametric model human model such as SMPL (Loper et al. 2015) to 2D image cues (Pavlakos et al. 2019; Kolotouros et al. 2019; Lassner et al. 2017). Later on, learning-based methods become more prevalent, which leverage CNNs to directly regress SMPL parameters from images (Choi et al. 2022; Kocabas et al. 2021; Kanazawa et al. 2018) and videos (Cho et al. 2023; Kanazawa et al. 2019). Recently, vision transformers (Alexey 2020) have been adopted for HMR tasks. For example, HMR 2.0 (Goel et al. 2023) and TokenHMR (Dwivedi et al. 2024) achieve the state-of-the-art mesh reconstruction accuracy by leveraging transformer's ability of modeling the long-range correlations to learn the dependencies of different human body parts in HMR tasks. However, these deterministic methods are limited by their single-output nature, which fails to capture the inherent depth ambiguity in complex scenarios, leading to reconstruction errors when multiple plausible 3D interpretations exist.\nTo address the limitations of deterministic methods, various probabilistic models have been exploited to address the inherent uncertainty in the reconstruction process and enable the generation of multiple plausible 3D mesh predictions from a single 2D image. These methods include mixture density networks (MDNs) (Bishop 1994; Li and Lee 2019), conditional variational autoencoders (CVAEs) (Sohn, Lee, and Yan 2015; Pavlakos et al. 2018), and normalizing flows (Wehrbein et al. 2021; Kolotouros et al. 2021). Recent advancements in diffusion-based HMR models, such as DDHPose (Cai et al. 2024), Diff-HMR (Cho and Kim 2023), and D3DP (Shan et al. 2023), have shown significant promise in generating diverse and realistic human meshes. However, these approaches face a great challenge in synthesizing multiple predictions into a single, coherent 3D pose. Current methods often rely on simplistic aggregation techniques, such as averaging all hypotheses (Wehrbein et al."}, {"title": "Proposed Method: GenHMR", "content": "The goal of GenHMR is to achieve accurate 3D human mesh reconstructions from monocular images I by learning body pose 0, shape \u03b2, and camera parameters T. As shown in Figure 2, GenHMR comprises two main modules: the pose tokenizer and the image-conditioned masked transformer. The pose tokenizer converts 3D human pose parameters @ into a sequence of discrete pose tokens. The image-conditioned masked transformer predicts masked pose tokens based on multi-scale image features extracted by an image encoder. During inference, we use an iterative decoding process to predict high-confidence pose tokens, progressively refining predictions by masking low-confidence tokens and leveraging both image semantics and inter-token dependencies. To further enhance accuracy, a 2D pose-guided sampling strategy is proposed to optimize the pose queries by aligning the re-projected 3D pose with the estimated 2D pose. Additionally, the shape parameters B and weak perspective camera parameters T are directly regressed from the image features, completing the 3D human mesh reconstruction process."}, {"title": "Body Model", "content": "We utilize SMPL, a differentiable parametric body model (Loper et al. 2015), to represent the human body. The SMPL model encodes the body using pose parameters \u03b8\u2208 R^{24\u00d73} and shape parameters \u03b2\u2208 R^{10}. The pose parameters 0 = [0_1,..., 0_{24}] include the global orientation 0_1 \u2208 R^3 of the whole body and the local rotations [0_2,...,0_{24}] \u2208 R^{23\u00d73} of the body joints, where each \u03b8k represents the axis-angle rotation of joint k relative to its parent in the kinematic tree. Given these pose and shape parameters, the SMPL model generates a body mesh M(\u03b8, \u03b2) \u2208 R^{3\u00d7N}, where N = 6890 vertices. The body joints J\u2208 R^{3\u00d7k} are then defined as a linear combination of these vertices, calculated using J = MW, where W \u2208 R^{N\u00d7k} represents fixed weights that map vertices to joints."}, {"title": "Pose Tokenizer", "content": "The goal of the pose tokenizer is to learn a discrete latent space for 3D pose parameters by quantizing the encoder's output into learned codebook C, as shown in Figure 2(a). We leverage the VQ-VAE (Van Den Oord, Vinyals et al. 2017) to pretrain the pose tokenizer. Specifically, given the SMPL pose parameters 0, we use a convolution encoder E to map the pose parameters @ into a latent embedding z. Each embedding z is then quantized to codes c\u2208 C by finding the nearest codebook entry based on the Euclidean distance, described by z\u2081 = arg min_{ck \u2208C} ||zi - Ck ||2. Then, the total loss function is defined as follows\nL_{vq} = A_{re}L_{re} + \u03bb_{e}|| sg[z] - C||2 + \u03bb_{a}||z - sg[c]||2\nwhich consists of a SMPL reconstruction loss, a latent embedding loss and a commitment loss, where A_{re}, \u03bb\u03b5, and \u03bb\u03b1 are their respective weights. sg[.] represents the stop-gradient operator. To improve reconstruction quality, we employ an L1 loss L_{re} = A_{\u03b8}L_{e} + A_{V}L_{v} + A_{L}L_{J} to minimize the difference between the SMPL parameters and their ground-truth, including pose parameters 0, mesh vertices V, and kinematic joints J. This tokenizer is optimized using a straight-through gradient estimator, with the codebooks being updated via exponential moving average and codebook reset, following the methodology outlined in (Esser, Rombach, and Ommer 2021; Williams et al. 2020)."}, {"title": "Image Conditioned Masked Transformer", "content": "The image conditioned masked transformer comprises two main components: the image encoder and the masked transformer decoder with multi-scale deformable cross attention.\nOur encoder employs a vision transformer (ViT) to extract image features (Alexey 2020; Dwivedi et al. 2024; Goel et al. 2023). We utilize the ViT-H/16 variant, which processes 16x16 pixel patches through transformer layers to generate feature tokens. Inspired by ViT-Det (Alexey 2020), we adopt a multi-scale feature approach by upsampling initial feature map from encoder to create a set of feature maps with varying resolutions. High-resolution feature maps capture fine-grained visual details (e.g., the presence and rotation of individual joints), while low-resolution feature maps preserve high-level semantics, e.g., the structure of the human skeleton.\nOur decoder employs a\nwhose inputs are the pose token se-\nfrom the pose tokenizer. These pose tokens\nattends to the multi-\nThe multi-scale deformable atten-\nMSDA(Y, py, {x_l}_{l=1}^L) = \u03a3_{l=1}^L\u03a3_{k=1}^K A_{lyk} W_a (py + p_{lyk})"}, {"title": "Training Strategy", "content": "Given a pose token sequence Y =\nfrom the pose tokenizer where L denotes the se-\nour model is trained to reconstruct the pose\ntioned on the image prompt under ran-randomly mask\nr(r)\u00b7 L] tokens, where y(t) \u2208 [0,1] is a\na uniform distri-\nSimilar to the ones from generative text-replacing the masked to-the corrupted pose sequence YM. The categorical distribu-is p(yi|\u04ae\u043c, \u0425), which explicitlyto minimize the negative log\nL_{mask} = -E_{Y\u2208D} \u03a3_{V_{i\u2208 [1,L]}} log p(yi|YM, X)          (1)\nThe training loss,accurately\nditional 3D loss been the need"}, {"title": "Inference Strategy", "content": "As shown in Fig. 3, our inference strategy comprises two key stages: (1) uncertainty-guided sampling, which iteratively samples high-confidence pose tokens based on their probabilistic distributions and (2) 2D pose-guided refinement, which fine-tunes the sampled pose tokens to further minimize 3D reconstruction uncertainty by ensuring the consistency between the 3D body mesh and 2D pose estimates.\nThe sampling process be-where allby performing where\nare re-masked and is a decaying function of iteration t that produces\nto the posesuch as"}, {"title": "Experiments", "content": "We trained the pose tokenizer using the AMASS (Mahmood et al. 2019) standard training split and MOYO (Tripathi et al. 2023). For GenHMR, following prior work (Goel et al. 2023) and to ensure fair comparisons, we used standard datasets (SD): Human3.6M (H36M) (Ionescu et al. 2013), COCO (Lin et al. 2014), MPI-INF-3DHP (Mehta et al. 2017), and MPII (Andriluka et al. 2014).\nWe evaluate GenHMR using the Mean\nWe also report theassess the align-We also report theare"}, {"title": "Conclusion", "content": "In this paper, we introduced GenHMR, a novel generative approach to monocular human mesh recovery that effectively addresses the longstanding challenges of depth ambiguity and occlusion. By reformulating HMR as an image-conditioned generative task, GenHMR explicitly models and mitigates uncertainties in the complex 2D-to-3D mapping process. At its core, GenHMR consists of two key components: a pose tokenizer that encodes 3D human poses into discrete tokens within a latent space, and an image-conditional masked transformer that learns rich probabilistic distributions of these pose tokens. These learned distributions enable two powerful inference techniques: uncertainty-guided iterative sampling and 2D pose-guided refinement, which together produce robust and accurate 3D human mesh reconstructions. Extensive experiments on benchmark datasets demonstrate GenHMR's superiority over SOTA HMR methods."}, {"title": "Overview", "content": "The appendix is organized into the following sections:\nImplementation Details\nAblation for Pose TokenizerSampling\nInference"}, {"title": "Implementation Details", "content": "Our model, GenHMR, implemented using PyTorch, consists of two primary training stages: the pose tokenizer and the image-conditioned masked transformer.\ndiscrete pose representations using mocap data from theformat using SMPL-X instructions (Pavlakos et al. 2019).local rotations\nfor consistent end-to-end training, approximating the categor-starting at tstart"}, {"title": "Data Augmentation", "content": "of valid human poses,Pose Tokenizer benehts fromincorporating prior information aboutis crucial for the overall performance of GenHMR.Durings such as\naugmentedboth and poses.Data"}, {"title": "Camera Model", "content": "using\nformula J2D"}, {"title": "Ablation for Pose Tokenizer", "content": "In Tables 5 and 6, we present an extensive ablation study\nexperiments"}, {"title": "Training on Large-Scale Datasets", "content": "To enhance GenHMR's real-world performance and generaliza-"}, {"title": "Varying Temperature Schedules in Cosine Annealing", "content": "In our study, we explored the impact of various temperature\nthe cosine to gradually reduce t from an inithroughout the"}, {"title": "Influence", "content": "functions."}, {"title": "TokenSampling", "content": "The"}]}