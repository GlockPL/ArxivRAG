{"title": "OMNIBAL: TOWARDS FAST INSTRUCT-TUNING FOR VISION-LANGUAGE MODELS VIA OMNIVERSE COMPUTATION BALANCE", "authors": ["Yongqiang Yao", "Jingru Tan", "Jiahao Hu", "Feizhao Zhang", "Xin Jin", "Bo Li", "Ruihao Gong", "Pengfei Liu"], "abstract": "Recently, vision-language instruct-tuning models have made significant progress\ndue to their more comprehensive understanding of the world. In this work, we\ndiscovered that large-scale 3D parallel training on those models leads to an im-\nbalanced computation load across different devices. The vision and language\nparts are inherently heterogeneous: their data distribution and model architec-\nture differ significantly, which affects distributed training efficiency. We rebal-\nanced the computational loads from data, model, and memory perspectives to\naddress this issue, achieving more balanced computation across devices. These\nthree components are not independent but are closely connected, forming an om-\nniverse balanced training framework. Specifically, for the data, we grouped in-\nstances into new balanced mini-batches within and across devices. For the model,\nwe employed a search-based method to achieve a more balanced partitioning.\nFor memory optimization, we adaptively adjusted the re-computation strategy\nfor each partition to utilize the available memory fully. We conducted exten-\nsive experiments to validate the effectiveness of our method. Compared with the\nopen-source training code of InternVL-Chat, we significantly reduced GPU days,\nachieving about 1.8x speed-up. Our method's efficacy and generalizability were\nfurther demonstrated across various models and datasets. Codes will be released\nat https://github.com/ModelTC/OmniBal.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing large language models (LLM) has brought new possibilities to many fields. Multi-modal\nmodels, particularly Vision-Language Models (VLMs) Alayrac et al. (2022); Team et al. (2023a);\nReid et al. (2024); Liu et al. (2023a); Bai et al. (2023b); Chen et al. (2023), are advancing rapidly\ndue to their deeper understanding of the world. The training scale of VLM models is continually\ngrowing. More texts and images with higher resolution are used on the data level. Compared with\nthe LLaVA-1.5 model Liu et al. (2023a), the InternVL-Chat Chen et al. (2024) model has expanded\nthe dataset size from 665K to 5M and increased image resolution from 336x336 to 3840x2160.\nOn the model level, larger vision encoders are adopted. The InternVL-Chat model upgrades the\nvisual encoder from ~300M ViT-L-336px Radford et al. (2021) to ~6B InternViT-448px Chen\net al. (2023). The scale of large models and datasets results in a time-consuming training process.\nTherefore, efficient training strategies are necessary for the field's rapid development.\n3D parallelism Shoeybi et al. (2019); Rajbhandari et al. (2020) is a popular framework for large-\nscale distributed training, which allows data and models to be distributed across multiple devices.\nBalancing computational load across devices is crucial in 3D parallelism, as it minimizes idle times\nand significantly improves training efficiency.\nIn this work, we found that for instruct-tuning large vision-language models, the heterogeneous\nnature of data and model structures brings new challenges to 3D parallelism training: (1) Varying"}, {"title": "2 RELATED WORK", "content": "2.1 LARGE LANGUAGE MODEL (LLM)\nLarge language models, such as ChatGPT OpenAI (2023a), GPT-4 OpenAI (2023b), Llama series\nTouvron et al. (2023a;b); AI (2024), and Gemini series Team et al. (2023b); Reid et al. (2024), have\nseen significant advancements recently. These models leverage vast amounts of data to perform\nself-supervised learning by predicting the next token in a sequence. Large language models have\ndemonstrated powerful generalization capabilities across various tasks, particularly in few-shot and\nzero-shot scenarios. Following the scaling law, the performance of these models improves with in-\ncreases in model and data scale. Researchers have observed that LLMs exhibit emergent phenomena\nwhen the training scale reaches a certain threshold, showing sudden improvements in language un-\nderstanding capabilities. Therefore, existing large language models are often trained at super large\nscales.\n2.2 MULTI-MODAL LARGE LANGUAGE MODEL(MLLM)\nAlthough LLM can be applied to various NLP tasks, they are typically built on textual data and can\nonly accept text inputs. However, real-world scenarios often involve rich multi-modal information,\ntypically images. It has driven the development of large vision language models (VLMs). Visual\nencoders like Vision Transformer (ViT) Dosovitskiy et al. (2021) are usually used to incorporate\nvision information. A cross-modal connector is also required to align the vision encoder outputs\nto the language models. LLaVA Touvron et al. (2023a) use the simplest MLP, BLIP series Li\net al. (2022; 2023); Dai et al. (2024) use Q-former, Qwen-VL-Chat Bai et al. (2023b) use a cross-\nattention module. VLMs significantly expand large models' capabilities and application scenarios by\ninstruct-tuning with text and image data. However, introducing multi-modal data and heterogeneous\nencoders also brings new challenges to the model training process.\n2.3 LARGE-SCALE DISTRIBUTED TRAINING\nDistributed training is essential for efficiently utilizing multiple GPUs to train large language mod-\nels. It is achieved through 3D parallelism Shoeybi et al. (2019); Rajbhandari et al. (2020): data,\ntensor, and pipeline parallelism. Data Parallelism splits the entire dataset into mini-batches and as-\nsigns them to multiple devices, each with a model replica. This approach maximizes the use of GPU\npower for large datasets. DeepSpeed Zero Rajbhandari et al. (2020) enhances it by reducing weight\nredundancy. However, it can still be challenged by the memory limits of individual devices when\nhandling huge models. Tensor Parallelism distributes a model's weight matrices across multiple\ndevices, enabling parallel matrix operations Shoeybi et al. (2019) and reducing per-device mem-\nory requirements. This method accelerates computation but requires dense inter-device commu-\nnication, typically restricted to single-node deployments to minimize latency. Pipeline Parallelism\ndivides a deep learning model into segments and assigns them to different devices, creating a com-\nputation flow like a production line. This technique facilitates larger model scaling across nodes.\nGPipe Huang et al. (2019) proposes micro-batching. PipeDream Narayanan et al. (2019) further\nproposes a one-forward-one-backward (1F1B) scheme to optimize memory usage. In pipeline par-\nallelism, uneven layer partitioning can cause significant pipeline bubbles. DreamPipe Narayanan\net al. (2019) and AdaPipe Sun et al. (2024) optimize model partitioning and re-computation strate-\ngies based on profiling and dynamic programming, respectively. However, these advancements are\nprimarily tested in text-based models and may require adaptation for large multi-modal scenarios."}, {"title": "3 COMPUTATION IMBALANCE", "content": "In this section, we explore the unique challenges of large-scale distributed training for vision-\nlanguage models, focusing on three key aspects: data, model, and memory.\nData Imbalance: LLMs are trained on pure text data using next-token prediction as the objec-\ntive. This allows arbitrary extraction of text sub-strings for batch training, ensuring consistent input\nlengths. In contrast, VLMs are trained on texts and images, requiring data integrity to be maintained\nwithout arbitrary truncation. The varying number of images, their resolutions, and text lengths in"}, {"title": "4 METHOD", "content": "This section presents our computation-balanced framework OmniBal for training large vision-\nlanguage models. To address imbalanced computational loads across devices, we first manage the\nlarge variations in data input sizes, which is the most fundamental issue in the computation imbal-\nance problem. This enables the possibility of balanced model partitioning. Finally, we optimize the\nre-computation strategy for each partition.\n4.1 BALANCED DYNAMIC MINI-BATCH\nDue to the inherently paired nature of the data used for training VL models, each sample contains\nvarious images and text segments, resulting in non-fixed input sizes. We evaluated input imbalance\nfrom two perspectives: within-device samples and cross-device mini-batches.\nPad ratio (within-device): When combining samples of different sizes into a mini-batch, smaller\nsamples need to be padded to ensure uniform input sizes. The pad ratio is calculated as follows:\n\\(PadRatio = \\frac{\\sum_{i=1}^{N}(nt_{max} - nt_{i})}{nt_{max} \\times B}\\)\n(1)\nWhere \\(nt_{max}\\) represents the maximum number of tokens in a mini-batch of size B, and \\(nt_{i}\\) denotes\nthe number of tokens for sample i within that mini-batch.\nDist ratio (cross-device): Even after padding, the sizes of mini-batches on different devices may\nvary, leading to different input scales across devices. The distribution ratio is calculated as follows:\n\\(Dist Raito = \\frac{ \\sum_{i=1}^{ND} (NT_{max} - NT_{i})}{NT_{max} \\times ND}\\)\n(2)\nWhere ND represents the number of devices, NTmax denotes the maximum number of mini-batch\ntokens across all devices, and \\(NT_{i}\\) refers to the number of mini-batch tokens on the ith device.\nNon-fixed input sizes in VLMs have a larger pad ratio and dist ratio, as shown in Table 4 (row 1). A\nhigh pad ratio wastes computational resources, while a high dist ratio causes device idle time. They\nsignificantly impact training throughput efficiency.\nTo address this issue, we adaptively group multiple samples to ensure that the new samples have\nimage and text sizes within a relatively fixed range, which we refer to as a Balanced Dynamic\nMini-Batch, as illustrated in Figure 2. However, determining the optimal grouping strategy is a\nchallenging problem. We have designed an iterative method using sampling and filtering to select\nan optimal grouping strategy. As illustrated in Algorithm 1 and Algorithm 2, our method Iterative\nSampling and Filtering (ISF) involves the following steps:"}, {"title": "4.2 BALANCED MODEL PARTITIONING", "content": "With the input data sizes fixed, we carefully divide the model parameters across devices. Given the\nnumber of layers L in the model and the pipeline parallel size N, our goal is to find an optimal\npartition strategy P = (P(1), P(2), p(3), . . ., P(N-1)) such that the training speed of the model is\nmaximized. Here, P(1) < P(2) < P(3) < ... < P(N\u22121), and the ith partition Mi consists of layers\nLk, where P(i-1) < Lk < P(i), with P(0) = 1 and P(N) = L+1. For example, given a model with\nL = 20 layers and pipeline size N = 4, assume that we have an optimal partition P = (5, 10, 15).\nThe first partition Mi consists of layers L1, L2, ...L4 since P(0) = 1, P(1) = 5.\nHowever, achieving balanced pipeline partitioning for VLMs is a more challenging task compared to\nLLMs. We must consider: (1) Model Heterogeneity: The structural differences between visual and\nlanguage models make simple parameter-based or layer-based partition strategies unsuitable. (2)\nCommunication Overheads: Different partition strategies have different communication volumes\nsince the activation number of each layer may vary significantly in VLMs. (3) Hardware Variability:\nDifferent platforms have varying capabilities. Some platforms could have negligible communication\noverhead due to high network bandwidth. We designed a heuristic search algorithm based on the\nabove analysis to find the optimal partition. We first identified a set of partitions that possibly contain\nthe optimal solution {Pk = (P(1), P(2), P (3), ..., P(N-1)) | k = 1,2,3,...}. Then, we select the\noptimal partition P* based on actual running time:"}, {"title": "4.3 BALANCED ADAPTIVE RE-COMPUTATION", "content": "Thanks to our proposed balanced dynamic mini-batch and balanced model partition, we have bal-\nanced computational loads for each partition. The memory requirements are also fixed because the\ncomputational demand has been fixed. Therefore, we can adjust the optimal re-computation strategy\nbased on memory needs instead of using the most aggressive strategy to prevent crashes. Fewer\nre-computations accelerate the model's backward step, significantly improving the training speed.\nAdditionally, we found that heterogeneous architectures have different memory requirements. For\nexample, the vision model in InternVL-Chat-1.5 requires more GPU memory than the language\nmodel under the same computational load. Therefore, we need to individually analyze the mem-\nory requirements for each layer of vision and language models and adaptively set the optimal re-\ncomputation strategy for each layer. Specifically, we first calculate the memory required for each\nlayer of vision or language models through profiling. Next, according to the remaining memory\nof each device, we determine the number of layers in a sub-model Mi where the re-computation\nstrategy can be canceled."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first introduce the models and datasets. Then, we demonstrate the acceleration\nachieved compared to current state-of-the-art VL models. Following that, we provide a detailed\ncomparison of each component proposed in our method and its contribution to training acceleration.\nFinally, we conduct extensive experimental analysis."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Model & Dataset setting: We conduct experiments following the open-source InternVL-Chat-\n1.5 setting. Our vision and language models are InternViT-6B and InternLM2-20B, respectively.\nWe employ two configurations: InternVL-Chat-1.5 (6+20B) and InternVL-Chat-1.5-Plus (6+34B).\nAs the InternVL-Chat-1.5 dataset is not yet available, we utilize the InternVL-Chat-1.2 dataset,\nwhich comprises approximately 1.2 million samples, as an alternative. All other training settings\nremain unchanged. We use GPU Days as our evaluation metric to estimate the total computing time.\nSpecifically, we report GPU Days based on A100 GPU usage to evaluate the speed-up performance.\nImplementation Details: We determine Q or text Qt by using use statistics of datasets. First, we\ntraverse the entire dataset and collect the summation of the lengths of all text tokens and the number\nof images. Then, We calculate the average number of text tokens per image. We set Qt =4K as the\nlength of the longest text token in the dataset and use the calculated text-to-image ratio to determine\nQ = 9. For images, we set Q = Qv, and for text, we set Q = Qt 128."}, {"title": "5.2 MAIN RESULTS", "content": "We demonstrate the superiority of our method under various settings in Table 1. Our baseline model\nis InternVL-Chat-1.5 (6+20B) Chen et al. (2024), and we follow their setting, using DeepSpeed\nZeRO-3 as the training backend. Our proposed method reduced GPU days from 38.9 to 25.3, achiev-\ning a 1.54x speedup. Simultaneously, we consistently maintained comparable performance across\ncommonly used datasets, such as MMB-EN/CN Liu et al. (2023c), ChartQA Masry et al. (2022),\nAI2D Kembhavi et al. (2016), MMVet Yu et al. (2023), and MME Fu et al. (2023).\nWe conducted experiments using another advanced training framework, Megatron-DeepSpeed. Un-\nlike DeepSpeed, Megatron-DeepSpeed integrates tensor and pipeline parallelism alongside data par-\nallelism, enhancing its suitability for larger-scale models. However, directly applying 3D parallelism\ncan slow down training due to the heterogeneous nature of VLM models, which complicates bal-\nanced partitioning and leads to computational inefficiencies. The table shows that switching to\nMegatron-DeepSpeed increased GPU days from 38.9 to 61.8. Our method addresses this issue by\nachieving computational balance across data, model, and memory. Utilizing our approach, we sig-\nnificantly reduced GPU days from 61.8 to 21.3, demonstrating that computational balance is crucial\nfor effective 3D parallelism. Notably, our method also outperformed DeepSpeed, reducing GPU\ndays from 25.3 to 21.3, highlighting the superiority of 3D parallelism when balanced computation\nis achieved.\nWe also report results under a larger-scale setting (InternVL-Chat-1.5-Plus) to verify the general-\nizability of our method. The larger model consistently improved, accelerating the training process\nwhile maintaining model performance. Specifically, we achieved speedups of 1.53x with the Deep-\nSpeed backend and 1.8x with the Megatron-DeepSpeed backend."}, {"title": "5.3 ABLATION ANALYSIS", "content": "In this section, we conduct ablation experiments on each component of our method, using InternVL-\nChat-1.5 as the baseline model with a 3D parallel Megatron-DeepSpeed backend. Table 2 illustrates\nthe impact of each part of our method. Due to computational imbalance, the baseline model's train-\ning speed is very slow, requiring 61.8 GPU days. By achieving data balance, GPU days are reduced\nfrom 61.8 to 51.9. This improvement enables us to achieve a more balanced model partition, further\nreducing the training time to 29 GPU days. Finally, optimizing memory usage with an adaptive re-\ncomputation strategy reduces GPU days from 29.0 to 21.3. These results demonstrate that a holistic\nbalance encompassing data, model, and memory is crucial for ensuring efficient training speed in\nVLM training. Below, we provide a more detailed analysis of each component.\nThe Importance of Data Balance: In Table 4, we investigate the importance of maintaining data\nbalance in large-scale distributed training. We compared four methods: (1) Baseline Method: Ran-\ndomly combining data into a mini-batch and padding according to the longest input in the mini-\nbatch. (2) Sorted Method: Combining samples with similar text and image sizes into a mini-batch\nto minimize padding within devices. (3) Device-group Method: Grouping samples with similar\ntext and image sizes across devices to minimize idle times. (4) Our Balanced Dynamic Mini-batch\nMethod: Using Iterative Sampling and Filtering (ISF) to combine mini-batches dynamically. This\ncomparison highlights the effectiveness of our Balanced Dynamic Mini-batch method in achieving\nsuperior data balance and optimizing training efficiency.\nFrom the table, we can observe the following: (1) Baseline Approach: The baseline approach is\nthe slowest due to the completely random combination of different-sized samples, which causes\nsignificant size variation within a mini-batch. This leads to excessive padding, with a padding ratio\nof 0.31. Additionally, there is considerable variation in mini-batch sizes across different devices, as\nevidenced by the Dist Ratio of 0.34 for ViT and 0.30 for LLM. This results in significant computation\ndisparities between devices, causing many devices to idle and wait, thereby severely impacting the\nmodel's throughput efficiency. (2) Sorted Method: This method enhances throughput efficiency by\npre-grouping samples of similar sizes into mini-batches, thus reducing the internal padding ratio\nto 0.014. Minimizing the number of redundant tokens within mini-batches effectively lowers the\nGPU days required to 54.0. (3) Device-Group Method: This method aims to reduce device idle time\nby ensuring consistent input sizes across devices. It significantly improves the Dist Ratio for ViT\nand LLM, reducing them to 0.125 and 0.228, respectively. However, this approach only balances\ninput sizes between devices and neglects the balance within mini-batches on each device. As a\nresult, excessive padding occurs, with a padding ratio of 0.378, wasting substantial computational\nresources. (4) Our Approach: Our approach balances input sizes within mini-batches on each device\nand across devices simultaneously. As shown in the table, it reduces both padding and the Dist"}, {"title": "5.4 COMPONENT ANALYSIS", "content": "Convergence of ISF: We study the convergence performance of ISF, and the results are shown in\nFigure 5. On LLava-665K Liu et al. (2023a), we observed that the dist ratio in both the vision\nand language data was sufficiently low after just one iteration. After five iterations, the dist ratio\nstabilized significantly. In practice, we iterate ten times for stable results, which takes less than\n1 minute. The computation cost is negligible compared to the overall runtime. We also tested\nour method on two additional datasets, InternVl-1.2M Chen et al. (2024) and LCS558K Liu et al.\n(2023b), and observed consistent convergence speeds. These findings demonstrate the effectiveness\nand generalizability of our method.\nGeneralization Capability: We studied the generalization capability of our method from multiple\naspects: (1) Different Datasets: We conducted experiments on three different datasets. As shown\nin Table 3, we achieved consistently low dist ratios on LLava-665K, InternVL-1.2M, and LCS558K\nand significantly improved training speed. (2) Different Models: We tested various combinations\nof vision and language models. For vision models, in addition to InternVL-6B, we used the open-\nsource EVA-CLIP models, ranging from 1B Sun et al. (2023a) to 18B Sun et al. (2023b). For\nlanguage models, we employed Llama3-8B, 70B AI (2024), Yi-34B NousResearch (2023), and the\nlarge-scale Qwen1.5-110B Bai et al. (2023a). Table 7 shows our method greatly reduces GPU days\nfor model training. Notably, we achieved a 3.2x speedup for the largest model, Qwen1.5-110B,\ndemonstrating the advantages of our approach for large-scale VLM model training. (3) Different\nTasks: Besides SFT tasks, we also tested pretraining tasks, as shown in Table 9. For models ranging\nfrom 6B to 20B, we trained both ViT and MLP components, and for larger models like 6B-34B\nand 6B-70B, we trained only the MLP component. We observed consistent improvements across all\nsettings, especially for the largest model, where we reduced GPU days from 16.8 to 9.6. (4) Different\nImage Resolutions: We also test our method with different image resolutions input; from Table 10,\nit can be observed that our method also achieves a very satisfactory acceleration effect. (5) Different\nModel-series: We also validated our method using another popular open-source model, LLava-1.6,"}, {"title": "6 CONCLUSION", "content": "In this work, we effectively addressed the issue of imbalanced computation loads in large-scale\n3D parallel training of vision-language models by rebalancing across data, model, and memory\ndimensions. Experimental results demonstrate that our method can significantly reduce GPU days\non many open-source models. The effectiveness and generalizability of our approach were also\nvalidated across various models, datasets, and hardware platforms. Our method can accelerate the\ndevelopment of this field by enabling more efficient training."}]}