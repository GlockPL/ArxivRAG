{"title": "OMNIBAL: TOWARDS FAST INSTRUCT-TUNING FOR VISION-LANGUAGE MODELS VIA OMNIVERSE COMPUTATION BALANCE", "authors": ["Yongqiang Yao", "Jingru Tan", "Jiahao Hu", "Feizhao Zhang", "Xin Jin", "Bo Li", "Ruihao Gong", "Pengfei Liu"], "abstract": "Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an im- balanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architec- ture differ significantly, which affects distributed training efficiency. We rebal- anced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an om- niverse balanced training framework. Specifically, for the data, we grouped in- stances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted exten- sive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our method's efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at https://github.com/ModelTC/OmniBal.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing large language models (LLM) has brought new possibilities to many fields. Multi-modal models, particularly Vision-Language Models (VLMs) Alayrac et al. (2022); Team et al. (2023a); Reid et al. (2024); Liu et al. (2023a); Bai et al. (2023b); Chen et al. (2023), are advancing rapidly due to their deeper understanding of the world. The training scale of VLM models is continually growing. More texts and images with higher resolution are used on the data level. Compared with the LLaVA-1.5 model Liu et al. (2023a), the InternVL-Chat Chen et al. (2024) model has expanded the dataset size from 665K to 5M and increased image resolution from 336x336 to 3840x2160. On the model level, larger vision encoders are adopted. The InternVL-Chat model upgrades the visual encoder from ~300M ViT-L-336px Radford et al. (2021) to ~6B InternViT-448px Chen et al. (2023). The scale of large models and datasets results in a time-consuming training process. Therefore, efficient training strategies are necessary for the field's rapid development.\n3D parallelism Shoeybi et al. (2019); Rajbhandari et al. (2020) is a popular framework for large- scale distributed training, which allows data and models to be distributed across multiple devices. Balancing computational load across devices is crucial in 3D parallelism, as it minimizes idle times and significantly improves training efficiency.\nIn this work, we found that for instruct-tuning large vision-language models, the heterogeneous nature of data and model structures brings new challenges to 3D parallelism training: (1) Varying input sizes of texts and images cause fluctuating computational loads across training iterations and devices. (2) The heterogeneity of text and image encoders results in inherent differences in the computational load of their transformer blocks. Along with varying input sizes, this inevitably leads to uneven computational load and computational bubbles. (3) Input size variation and computational imbalance compel us to use the most aggressive re-computation (checkpointing) Li et al. (2014) strategy to prevent program crashes, which significantly wastes computational resources. We refer to those issues caused by the heterogeneity in data and model structures in large multi-modal models as the Computation Imbalance problem, which significantly reduces training efficiency."}, {"title": "2 RELATED WORK", "content": "2.1 LARGE LANGUAGE MODEL (LLM)\nLarge language models, such as ChatGPT OpenAI (2023a), GPT-4 OpenAI (2023b), Llama series Touvron et al. (2023a;b); AI (2024), and Gemini series Team et al. (2023b); Reid et al. (2024), have seen significant advancements recently. These models leverage vast amounts of data to perform self-supervised learning by predicting the next token in a sequence. Large language models have demonstrated powerful generalization capabilities across various tasks, particularly in few-shot and zero-shot scenarios. Following the scaling law, the performance of these models improves with in- creases in model and data scale. Researchers have observed that LLMs exhibit emergent phenomena when the training scale reaches a certain threshold, showing sudden improvements in language un- derstanding capabilities. Therefore, existing large language models are often trained at super large scales.\n2.2 MULTI-MODAL LARGE LANGUAGE MODEL(MLLM)\nAlthough LLM can be applied to various NLP tasks, they are typically built on textual data and can only accept text inputs. However, real-world scenarios often involve rich multi-modal information, typically images. It has driven the development of large vision language models (VLMs). Visual encoders like Vision Transformer (ViT) Dosovitskiy et al. (2021) are usually used to incorporate vision information. A cross-modal connector is also required to align the vision encoder outputs to the language models. LLaVA Touvron et al. (2023a) use the simplest MLP, BLIP series Li et al. (2022; 2023); Dai et al. (2024) use Q-former, Qwen-VL-Chat Bai et al. (2023b) use a cross- attention module. VLMs significantly expand large models' capabilities and application scenarios by instruct-tuning with text and image data. However, introducing multi-modal data and heterogeneous encoders also brings new challenges to the model training process.\n2.3 LARGE-SCALE DISTRIBUTED TRAINING\nDistributed training is essential for efficiently utilizing multiple GPUs to train large language mod- els. It is achieved through 3D parallelism Shoeybi et al. (2019); Rajbhandari et al. (2020): data, tensor, and pipeline parallelism. Data Parallelism splits the entire dataset into mini-batches and as- signs them to multiple devices, each with a model replica. This approach maximizes the use of GPU power for large datasets. DeepSpeed Zero Rajbhandari et al. (2020) enhances it by reducing weight redundancy. However, it can still be challenged by the memory limits of individual devices when handling huge models. Tensor Parallelism distributes a model's weight matrices across multiple devices, enabling parallel matrix operations Shoeybi et al. (2019) and reducing per-device mem- ory requirements. This method accelerates computation but requires dense inter-device commu- nication, typically restricted to single-node deployments to minimize latency. Pipeline Parallelism divides a deep learning model into segments and assigns them to different devices, creating a com- putation flow like a production line. This technique facilitates larger model scaling across nodes. GPipe Huang et al. (2019) proposes micro-batching. PipeDream Narayanan et al. (2019) further proposes a one-forward-one-backward (1F1B) scheme to optimize memory usage. In pipeline par- allelism, uneven layer partitioning can cause significant pipeline bubbles. DreamPipe Narayanan et al. (2019) and AdaPipe Sun et al. (2024) optimize model partitioning and re-computation strate- gies based on profiling and dynamic programming, respectively. However, these advancements are primarily tested in text-based models and may require adaptation for large multi-modal scenarios."}, {"title": "3 COMPUTATION IMBALANCE", "content": "In this section, we explore the unique challenges of large-scale distributed training for vision-language models, focusing on three key aspects: data, model, and memory.\nData Imbalance: LLMs are trained on pure text data using next-token prediction as the objec- tive. This allows arbitrary extraction of text sub-strings for batch training, ensuring consistent input lengths. In contrast, VLMs are trained on texts and images, requiring data integrity to be maintained without arbitrary truncation. The varying number of images, their resolutions, and text lengths in"}, {"title": "4 METHOD", "content": "This section presents our computation-balanced framework OmniBal for training large vision- language models. To address imbalanced computational loads across devices, we first manage the large variations in data input sizes, which is the most fundamental issue in the computation imbal- ance problem. This enables the possibility of balanced model partitioning. Finally, we optimize the re-computation strategy for each partition.\n4.1 BALANCED DYNAMIC MINI-BATCH\nDue to the inherently paired nature of the data used for training VL models, each sample contains various images and text segments, resulting in non-fixed input sizes. We evaluated input imbalance from two perspectives: within-device samples and cross-device mini-batches.\nPad ratio (within-device): When combining samples of different sizes into a mini-batch, smaller samples need to be padded to ensure uniform input sizes. The pad ratio is calculated as follows:\nPadRatio =$\\frac{\\sum_{i=1}^{N}(ntmax - nti)}{ntmax \\times B}$    (1)\nWhere ntmax represents the maximum number of tokens in a mini-batch of size B, and nti denotes the number of tokens for sample i within that mini-batch.\nDist ratio (cross-device): Even after padding, the sizes of mini-batches on different devices may vary, leading to different input scales across devices. The distribution ratio is calculated as follows:\nDist Raito =$\\frac{ND \\times (NTmax - NTi)}{NTmax \\times ND}$       (2)\nWhere ND represents the number of devices, NTmax denotes the maximum number of mini-batch tokens across all devices, and NT\u2081 refers to the number of mini-batch tokens on the ith device. Non-fixed input sizes in VLMs have a larger pad ratio and dist ratio, as shown in Table 4 (row 1). A high pad ratio wastes computational resources, while a high dist ratio causes device idle time. They significantly impact training throughput efficiency.\nTo address this issue, we adaptively group multiple samples to ensure that the new samples have image and text sizes within a relatively fixed range, which we refer to as a Balanced Dynamic Mini-Batch, as illustrated in Figure 2. However, determining the optimal grouping strategy is a challenging problem. We have designed an iterative method using sampling and filtering to select an optimal grouping strategy. As illustrated in Algorithm 1 and Algorithm 2, our method Iterative Sampling and Filtering (ISF) involves the following steps:"}, {"title": "4.2 BALANCED MODEL PARTITIONING", "content": "With the input data sizes fixed, we carefully divide the model parameters across devices. Given the number of layers L in the model and the pipeline parallel size N, our goal is to find an optimal partition strategy P = (P(1), P(2), p(3), . . ., P(N-1)) such that the training speed of the model is maximized. Here, P(1) < P(2) < P(3) < ... < P(N\u22121), and the ith partition M\u00bf consists of layers Lk, where P(i-1) < Lk < P(i), with P(0) = 1 and P(N) = L+1. For example, given a model with L = 20 layers and pipeline size N = 4, assume that we have an optimal partition P = (5, 10, 15). The first partition M\u00bf consists of layers L1, L2, ...L4 since P(0) = 1, P(1) = 5.\nHowever, achieving balanced pipeline partitioning for VLMs is a more challenging task compared to LLMs. We must consider: (1) Model Heterogeneity: The structural differences between visual and language models make simple parameter-based or layer-based partition strategies unsuitable. (2) Communication Overheads: Different partition strategies have different communication volumes since the activation number of each layer may vary significantly in VLMs. (3) Hardware Variability: Different platforms have varying capabilities. Some platforms could have negligible communication overhead due to high network bandwidth. We designed a heuristic search algorithm based on the above analysis to find the optimal partition. We first identified a set of partitions that possibly contain the optimal solution {Pk = (P(1)k, P(2)k, P(3)k, ..., P(N-1)k) | k = 1,2,3,...}. Then, we select the optimal partition P* based on actual running time:\nP* = arg min f(Pi)   (3)\nPi\nHere, f(Pi) is the average running time obtained from training the model for 20 iterations under partition f(Pi).\nPartition Candidates: We start by profiling each layer's computation time FWD(Li). A greedy algorithm is employed to compute the anchor partition P+, where the computation time of sub- model Mi is close. Around anchor partition P+, we create a set of partition candidates by jittering p(1), p(2), p(3), within a radius of r layers. There are total 3 \u00d7 3 \u00d7 3 = 27 partition candidates when r = 1, N = 4.\nPartition Metrics: When r and N are very large, there will be a significant number of partition candidates, making it inefficient to run and evaluate the time for each one. Therefore, we designed two metrics to rank these candidates.\nThe first metric is the difference in running time between different pipeline stages. Smaller differ- ences generally result in fewer bubbles and, therefore, faster execution. We use the variance of the running times of different pipeline stages to measure this difference.\nVAR(fwd_time) =$\\sum_{i=1}^{N}(FWD(Mi) - FWD(\\overline{M_{i}}))^{2}$        (4)\nThe second metric is the total point-to-point communication volume of the partition strategy Pi. It depends on P\u2081 consisting of (P(1), P(2), P(3), ...)\nSUM(comm) =$\\sum_{i=1}^{N-1} ACTIV(Lpi)$          (5)\nWhere Lpi is the last layer of partition strategy P(i) and ACTIV(Lpi) is the activation number of layer Lpi, indicating the point-to-point communication of this partition strategy P(i). We use the sum of VAR(fwd_time) and SUM(comm) as the metric for the partition and rank them to select the top K candidates for speed evaluation."}, {"title": "4.3 BALANCED ADAPTIVE RE-COMPUTATION", "content": "Thanks to our proposed balanced dynamic mini-batch and balanced model partition, we have bal- anced computational loads for each partition. The memory requirements are also fixed because the computational demand has been fixed. Therefore, we can adjust the optimal re-computation strategy based on memory needs instead of using the most aggressive strategy to prevent crashes. Fewer re-computations accelerate the model's backward step, significantly improving the training speed.\nAdditionally, we found that heterogeneous architectures have different memory requirements. For example, the vision model in InternVL-Chat-1.5 requires more GPU memory than the language model under the same computational load. Therefore, we need to individually analyze the mem- ory requirements for each layer of vision and language models and adaptively set the optimal re- computation strategy for each layer. Specifically, we first calculate the memory required for each layer of vision or language models through profiling. Next, according to the remaining memory of each device, we determine the number of layers in a sub-model Mi where the re-computation strategy can be canceled."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first introduce the models and datasets. Then, we demonstrate the acceleration achieved compared to current state-of-the-art VL models. Following that, we provide a detailed comparison of each component proposed in our method and its contribution to training acceleration. Finally, we conduct extensive experimental analysis."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Model & Dataset setting: We conduct experiments following the open-source InternVL-Chat- 1.5 setting. Our vision and language models are InternViT-6B and InternLM2-20B, respectively. We employ two configurations: InternVL-Chat-1.5 (6+20B) and InternVL-Chat-1.5-Plus (6+34B). As the InternVL-Chat-1.5 dataset is not yet available, we utilize the InternVL-Chat-1.2 dataset, which comprises approximately 1.2 million samples, as an alternative. All other training settings remain unchanged. We use GPU Days as our evaluation metric to estimate the total computing time. Specifically, we report GPU Days based on A100 GPU usage to evaluate the speed-up performance.\nImplementation Details: We determine Q or text Qt by using use statistics of datasets. First, we traverse the entire dataset and collect the summation of the lengths of all text tokens and the number of images. Then, We calculate the average number of text tokens per image. We set Qt =4K as the length of the longest text token in the dataset and use the calculated text-to-image ratio to determine Q = 9. For images, we set Q = Qv, and for text, we set Q = Qt 128."}, {"title": "5.2 MAIN RESULTS", "content": "We demonstrate the superiority of our method under various settings in Table 1. Our baseline model is InternVL-Chat-1.5 (6+20B) Chen et al. (2024), and we follow their setting, using DeepSpeed ZeRO-3 as the training backend. Our proposed method reduced GPU days from 38.9 to 25.3, achiev- ing a 1.54x speedup. Simultaneously, we consistently maintained comparable performance across commonly used datasets, such as MMB-EN/CN Liu et al. (2023c), ChartQA Masry et al. (2022), AI2D Kembhavi et al. (2016), MMVet Yu et al. (2023), and MME Fu et al. (2023).\nWe conducted experiments using another advanced training framework, Megatron-DeepSpeed. Un- like DeepSpeed, Megatron-DeepSpeed integrates tensor and pipeline parallelism alongside data par- allelism, enhancing its suitability for larger-scale models. However, directly applying 3D parallelism can slow down training due to the heterogeneous nature of VLM models, which complicates bal- anced partitioning and leads to computational inefficiencies. The table shows that switching to Megatron-DeepSpeed increased GPU days from 38.9 to 61.8. Our method addresses this issue by achieving computational balance across data, model, and memory. Utilizing our approach, we sig- nificantly reduced GPU days from 61.8 to 21.3, demonstrating that computational balance is crucial for effective 3D parallelism. Notably, our method also outperformed DeepSpeed, reducing GPU days from 25.3 to 21.3, highlighting the superiority of 3D parallelism when balanced computation is achieved.\nWe also report results under a larger-scale setting (InternVL-Chat-1.5-Plus) to verify the general- izability of our method. The larger model consistently improved, accelerating the training process while maintaining model performance. Specifically, we achieved speedups of 1.53x with the Deep- Speed backend and 1.8x with the Megatron-DeepSpeed backend."}, {"title": "5.3 ABLATION ANALYSIS", "content": "In this section, we conduct ablation experiments on each component of our method, using InternVL- Chat-1.5 as the baseline model with a 3D parallel Megatron-DeepSpeed backend. Table 2 illustrates the impact of each part of our method. Due to computational imbalance, the baseline model's train- ing speed is very slow, requiring 61.8 GPU days. By achieving data balance, GPU days are reduced from 61.8 to 51.9. This improvement enables us to achieve a more balanced model partition, further reducing the training time to 29 GPU days. Finally, optimizing memory usage with an adaptive re-computation strategy reduces GPU days from 29.0 to 21.3. These results demonstrate that a holistic balance encompassing data, model, and memory is crucial for ensuring efficient training speed in VLM training. Below, we provide a more detailed analysis of each component.\nThe Importance of Data Balance: In Table 4, we investigate the importance of maintaining data balance in large-scale distributed training. We compared four methods: (1) Baseline Method: Ran- domly combining data into a mini-batch and padding according to the longest input in the mini- batch. (2) Sorted Method: Combining samples with similar text and image sizes into a mini-batch to minimize padding within devices. (3) Device-group Method: Grouping samples with similar text and image sizes across devices to minimize idle times. (4) Our Balanced Dynamic Mini-batch Method: Using Iterative Sampling and Filtering (ISF) to combine mini-batches dynamically. This comparison highlights the effectiveness of our Balanced Dynamic Mini-batch method in achieving superior data balance and optimizing training efficiency.\nFrom the table, we can observe the following: (1) Baseline Approach: The baseline approach is the slowest due to the completely random combination of different-sized samples, which causes significant size variation within a mini-batch. This leads to excessive padding, with a padding ratio of 0.31. Additionally, there is considerable variation in mini-batch sizes across different devices, as evidenced by the Dist Ratio of 0.34 for ViT and 0.30 for LLM. This results in significant computation disparities between devices, causing many devices to idle and wait, thereby severely impacting the model's throughput efficiency. (2) Sorted Method: This method enhances throughput efficiency by pre-grouping samples of similar sizes into mini-batches, thus reducing the internal padding ratio to 0.014. Minimizing the number of redundant tokens within mini-batches effectively lowers the GPU days required to 54.0. (3) Device-Group Method: This method aims to reduce device idle time by ensuring consistent input sizes across devices. It significantly improves the Dist Ratio for ViT and LLM, reducing them to 0.125 and 0.228, respectively. However, this approach only balances input sizes between devices and neglects the balance within mini-batches on each device. As a result, excessive padding occurs, with a padding ratio of 0.378, wasting substantial computational resources. (4) Our Approach: Our approach balances input sizes within mini-batches on each device and across devices simultaneously. As shown in the table, it reduces both padding and the Dist"}, {"title": "The Importance of Model Balance", "content": "In Table 5, we examine the impact of balanced model par- titioning, particularly in the context of determining partition strategies for pipeline parallelism. For large language model (LLM) training, common partitioning methods include (1) parameter- based partitioning and (2) layer-based partitioning. Additionally, (3) profile-based methods such as DreamPipe Narayanan et al. (2019) estimate the computation time for each layer and use this in- formation to partition the model effectively. Finally, (4) our search-based Balanced Model Partition method identifies the optimal partition strategy from a small set of partition candidates.\n(1) Parameter-based and (2) layer-based methods split the model based on parameters or the number of layers, respectively. As shown in Table 5, these methods achieve the lowest variation in param- eters (VAR(param)) and the number of layers (VAR(num_layer)) across devices, indicating balance in these metrics. However, due to the heterogeneous nature of vision and language models (VLMs), balancing parameters or layers does not equate to balanced computational loads. The variation in forward time (VAR(fwd_time)) for each stage remains high, at 93.6 and 20.1, respectively, leading to significant computational inefficiencies in the pipeline. The (3) profile-based method ensures the optimal VAR(fwd_time) with the lowest value of 6.5. However, this partitioning occurs before the vision model's token sub-sampling operation, significantly increasing communication overhead by 16.6 compared to the baseline (1), thereby affecting training speed. Our proposed Balanced Model Partition (BMP) method searches within a high-quality partition space to find the optimal strategy. As shown in Table 5, our method achieves the best results, with 29.0 GPU days, outperforming the parameter-based method by 13.2 days. Notably, our method does not achieve low VAR(param) and VAR(num_layer), indicating that parameters and the number of layers are ineffective metrics for evaluating model balance in VLM scenarios."}, {"title": "The Importance of Memory Balance", "content": "We examine the significance of memory balance, with the results presented in Table 6. In the baseline model, the input sizes for both the vision and language components are not fixed, with vision token lengths ranging from 4K to 20K and language token lengths ranging from 1K to 16K. As a result, the memory demand on each GPU varies. Even with the most aggressive re-computation strategy, the remaining memory on an 80G A100 GPU can be as low as 7.3G in extreme cases. We achieved balance at both the data and model levels by implement- ing our proposed balanced dynamic mini-batch and model partition methods. This approach controls the computational load on each device and maintains relative balance, significantly improving train- ing speed from 61.8 to 29.0 GPU days. However, due to inherent differences between vision and"}, {"title": "5.4 COMPONENT ANALYSIS", "content": "Convergence of ISF: We study the convergence performance of ISF, and the results are shown in Figure 5. On LLava-665K Liu et al. (2023a), we observed that the dist ratio in both the vision and language data was sufficiently low after just one iteration. After five iterations, the dist ratio stabilized significantly. In practice, we iterate ten times for stable results, which takes less than 1 minute. The computation cost is negligible compared to the overall runtime. We also tested our method on two additional datasets, InternVl-1.2M Chen et al. (2024) and LCS558K Liu et al. (2023b), and observed consistent convergence speeds. These findings demonstrate the effectiveness and generalizability of our method.\nGeneralization Capability: We studied the generalization capability of our method from multiple aspects: (1) Different Datasets: We conducted experiments on three different datasets. As shown in Table 3, we achieved consistently low dist ratios on LLava-665K, InternVL-1.2M, and LCS558K and significantly improved training speed. (2) Different Models: We tested various combinations of vision and language models. For vision models, in addition to InternVL-6B, we used the open- source EVA-CLIP models, ranging from 1B Sun et al. (2023a) to 18B Sun et al. (2023b). For language models, we employed Llama3-8B, 70B AI (2024), Yi-34B NousResearch (2023), and the large-scale Qwen1.5-110B Bai et al. (2023a). Table 7 shows our method greatly reduces GPU days for model training. Notably, we achieved a 3.2x speedup for the largest model, Qwen1.5-110B, demonstrating the advantages of our approach for large-scale VLM model training. (3) Different Tasks: Besides SFT tasks, we also tested pretraining tasks, as shown in Table 9. For models ranging from 6B to 20B, we trained both ViT and MLP components, and for larger models like 6B-34B and 6B-70B, we trained only the MLP component. We observed consistent improvements across all settings, especially for the largest model, where we reduced GPU days from 16.8 to 9.6. (4) Different Image Resolutions: We also test our method with different image resolutions input; from Table 10, it can be observed that our method also achieves a very satisfactory acceleration effect. (5) Different Model-series: We also validated our method using another popular open-source model, LLava-1.6,"}, {"title": "6 CONCLUSION", "content": "In this work, we effectively addressed the issue of imbalanced computation loads in large-scale 3D parallel training of vision-language models by rebalancing across data, model, and memory dimensions. Experimental results demonstrate that our method can significantly reduce GPU days on many open-source models. The effectiveness and generalizability of our approach were also validated across various models, datasets, and hardware platforms. Our method can accelerate the development of this field by enabling more efficient training."}]}