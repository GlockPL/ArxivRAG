{"title": "Robust Semi-supervised Multimodal Medical Image Segmentation via Cross Modality Collaboration", "authors": ["Xiaogen Zhou", "Yiyou Sun", "Min Deng", "Winnie Chiu Wing Chu", "Qi Dou"], "abstract": "Multimodal learning leverages complementary information derived from different modalities, thereby enhancing performance in medical image segmentation. However, prevailing multimodal learning methods heavily rely on extensive well-annotated data from various modalities to achieve accurate segmentation performance. This dependence often poses a challenge in clinical settings due to limited availability of such data. Moreover, the inherent anatomical misalignment between different imaging modalities further complicates the endeavor to enhance segmentation performance. To address this problem, we propose a novel semi-supervised multimodal segmentation framework that is robust to scarce labeled data and misaligned modalities. Our framework employs a novel cross modality collaboration strategy to distill modality-independent knowledge, which is inherently associated with each modality, and integrates this information into a unified fusion layer for feature amalgamation. With a channel-wise semantic consistency loss, our framework ensures alignment of modality-independent information from a feature-wise perspective across modalities, thereby fortifying it against misalignments in multimodal scenarios. Furthermore, our framework effectively integrates contrastive consistent learning to regulate anatomical structures, facilitating anatomical-wise prediction alignment on unlabeled data in semi-supervised segmentation tasks. Our method achieves competitive performance compared to other multimodal methods across three tasks: cardiac, abdominal multi-organ, and thyroid-associated orbitopathy segmentations. It also demonstrates outstanding robustness in scenarios involving scarce labeled data and misaligned modalities. Code is available at: https://github.com/med-air/CMC.", "sections": [{"title": "1 Introduction", "content": "Multimodal learning is a long-standing topic with great importance for medical image analysis. It leverages synergistic information of multiple modalities to outperform single-modality solutions. Many multimodal methods have been developed for medical image segmentation. These methods either concatenate a large array of well-annotated multiple Magnetic Resonance Imaging (MRI) and computed tomography (CT) as input [14,15], or fuse higher-level information from pre-aligned modalities within a latent space [2,8]. However, in real-world clinical settings, the availability of extensive collections of meticulously annotated and anatomically aligned datasets from various modalities cannot be always guaranteed. This is primarily due to the expensive costs associated with detailed annotation and a shortage of experienced clinicians available for labeling tasks. Consequently, robustness to sparsely labeled data or misaligned modalities during inference is essential for a widely-applicable multimodal learning method.\nA typical solution to address the scarcity of labeled data is to employ semi-supervised learning techniques [1,16]. These methods can effectively leverage the limited labeled data in conjunction with abundant unlabeled data to train deep learning models, thereby mitigating the dependence on costly labeled data while maintaining satisfactory performance. However, most existing semi-supervised learning methods are inherently constrained to single-modality input, failing to capitalize on the wealth of multimodal data frequently encountered in daily clinical routines. Furthermore, Zhu et al. [18] and Zhang et al. [13] have attempted to address this by applying semi-supervised learning to minimize consistency loss on abundant unlabeled data across different modalities. However, different modalities vary in intensity distributions and appearances, and simply minimizing consistency loss may not be sufficient for effective feature fusion and alignment. To address this challenge, a recent work [2] introduces a modality-collaborative semi-supervised framework. By leveraging modality-specific knowledge, this method endeavors to enhance multimodal image segmentation, thereby facilitating more effective feature fusion and alignment.\nFor the effective extraction of modality-invariant representations that capture critical information and enable feature fusion across various modalities, it is beneficial to identify and leverage modality-specific knowledge. This can be accomplished through modality-independent feature extraction, which distills the characteristics and semantic representations of each modality [2,3]. Additionally, integrating a modality-collaborative strategy can significantly enhance feature fusion and alignment across different modalities, ensuring effective synergistic integration of multimodal data [2,14]. However, these works fail to enforce constraints to regularize the underlying anatomical structures during the feature fusion stage, which is crucial for maintaining alignment.\nIn this paper, we propose a novel semi-supervised multimodal framework that incorporates cross modality collaboration and contrastive consistent learning. This framework is robust to scenarios with limited labeled data and misaligned modalities. Our framework exploits a novel cross modality-collaboration strategy to regularize the underlying channel-wise features and distill modality-independent knowledge. The modality-independent knowledge extracted from each modality is fused to a common fusion layer, which contains collaborative information for segmentation. To effectively extract modality-independent in-"}, {"title": "2 Methodology", "content": "An overview of our framework is presented in Fig. 1. We first employ two 3D modality-specific encoders to get the initial modality-independent features from multimodal inputs. Then, we introduce a cross-modality collaboration strategy to facilitate feature fusion and channel-wise alignment. Subsequently, we introduce a contrastive consistent learning module aimed at reducing inconsistencies in anatomical structures across prediction maps derived from the unlabeled data. The details of our method are elaborated in the following sections."}, {"title": "2.1 Cross Modality Collaboration for Alignment and Feature Fusion", "content": "We denote the multimodal images by $\\mathcal{D}^{l} = \\{\\{(x_{a, i}, y_i)\\}_{i=1}^{N}, \\{(x_{b, i}, y_i)\\}_{i=1}^{N}\\}$ and $\\mathcal{D}^{u} = \\{\\{(x_{a, j}^{u})\\}_{j=1}^{M}, \\{(x_{b, j}^{u})\\}_{j=1}^{M}\\}$, where $x_{a, i}$ and $x_{b, i}$ denote the labeled and unlabeled data in modality a and b, respectively. The $y_i$ and $y_i^{'}$ are the corresponding segmentation masks for $x_{a, i}$ and $x_{b, i}$. The $N$ and $M$ denote the number of labeled and unlabled samples, and $N \\ll M$. Each modality $x_{a, i}$ and $x_{b, i}$ is input to modality-specific encoder $E^{a}$ and $E^{b}$, respectively, which are fine-tuned from the pre-trained encoder of SAM-Med3D [10]. Moreover, as shown in Fig. 1 (b), we integrate an adapter module within each Vision Transformer (ViT) block. This adapter serves as a bottleneck infrastructure and consists of a sequence of two 3D convolutional layers with different kernel sizes.\nThen, we obtain its distinctive semantic (DS) features by $F_{ds}^{a} = E^{a}(x_i^{l})$ and $F_{ds}^{b} = E^{b}(x_i^{l})$. For effective extraction of distinctive semantic features $F_{ds}^{a}$ and $F_{ds}^{b}$, which are expected to regularize underlying anatomical structures from a perspective of channel-wise feature alignment across modalities, we introduce a Channel-wise Semantic Consistency (CSC) loss which is used to align the channel-wise features. In specific, the $L_{csc}$ loss can be defined as follows:\n$L_{csc} = \\min_{E^{a}, E^{b}} - \\sum_{c=1}^{C} \\log\\frac{exp(Sim((F_{ds}^{a})_c, (F_{ds}^{b})_c))}{\\sum_{c=1}^{C} exp(Sim((F_{ds}^{a})_c,(F_{ds}^{b})_c))}$,\nwhere $c \\in \\{1, 2, ..., C\\}$ denotes the $c^{th}$ channel of two features, i.e., $F_{ds}^{a}$ and $F_{ds}^{b}$, respectively. $Sim(...)$ represents the cosine similarity [9] used to measure the channel-wise semantic feature similarity between the latent representations.\nAfter the feature-wise alignment procedure, which mitigates the influence of modality-specific appearance features from multimodal data, we introduce a novel Modality-Independent Awareness (MIA) module to further harness modality independent knowledge from each modality for effective feature fusion. As is known from literature of deep neural networks, modality-independent knowledge is useful in learning generalized and robust representations across various imaging modalities [2,17]. As depicted in Fig. 1, the MIA module integrates modality-aware attention mechanisms [15], 3D convolutional layers, and a fusion layer. Subsequently, the features $F_{mia}^{a}$ and $F_{mia}^{b}$ generated by each MIA module are subsequently merged in a fusion layer to produce the fused feature."}, {"title": "2.2 Contrastive Consistent Learning with Contrastive Consistency", "content": "Contrastive consistency has shown effectiveness in semi-supervised learning [2,13]. The formulation of the contrastive consistency term is based on the correlation and differential information across various modalities. The consistency of their predictions acts as the impetus for semi-supervised cross-modal knowledge reciprocal learning. This also holds true for our scenario. We first build two decoders $\\mathcal{D}_{a}$ and $\\mathcal{D}_{b}$, which are fine-tuned the pre-trained foundation model decoder [6] on our datasets. During training, the weights of our model are optimized using a supervised loss function $\\mathcal{L}_{Sup}$ on labeled data, which is defined as follows:\n$\\mathcal{L}_{Sup} = \\mathbb{E}_{E_a,E_b,\\mathcal{D}_{a},\\mathcal{D}_{b}} [\\mathcal{L}_{CE}(\\mathcal{D}_{a}(E_{a}(x_i^l)), y_i) + \\mathcal{L}_{Dice}(\\mathcal{D}_{b}(E_{b}(x_i^l)), y_i))]$,\nwhere $\\mathcal{L}_{CE}$ and $\\mathcal{L}_{Dice}$ represent cross-entropy loss function and Dice coefficient loss function. To further regularize underlying anatomical structures from a perspective of anatomical-wise predictions alignment on unlabeled data across modalities for multimodal segmentation, we introduce a Contrastive Anatomical-similar Consistency (CAC) loss. This loss is designed to measure the anatomical similarity between predictions of different modalities on unlabeled data, thereby improving the accuracy and robustness of multimodal segmentation. The CAC loss incorporates the Dice coefficient similarity $Sim_{Dice}$ [13] as a metric for assessing anatomical similarity. The $\\mathcal{L}_{CAC}$ loss is defined as follows:\n$\\mathcal{L}_{CAC} = \\min_{E_a, E_b, \\mathcal{D}_{a}, \\mathcal{D}_{b}} \\sum_{j} -\\log \\frac{exp(Sim_{Dice}(\\mathcal{D}_{a}(E_{a}(x_{u,j})), \\mathcal{D}_{b}(E_{b}(x_{u,j}))))}{\\sum_{i=1}^{M} exp(Sim_{Dice}(\\mathcal{D}_{a}(E_{a}(x_{u,j})), \\mathcal{D}_{b}(E_{b}(x_{u,j}))))}$,\nOverall, the total loss of our framework is $\\mathcal{L}_{total} = \\mathcal{L}_{Sup} + \\alpha \\mathcal{L}_{CSC} + \\beta \\mathcal{L}_{CAC}$. To balance between these losses, we adopt a ramp-up weighting coefficient $\\alpha = 0.1e^{-5(1-\\frac{t}{t_{max}})}$ and a ramp-down weighting coefficient $\\beta = 0.1e^{-5(\\frac{t}{t_{max}})}$. This strategy follows related works [12,18], where $t$ and $t_{max}$ denote the current and maximum number of epochs, respectively. All approaches were implemented using PyTorch on multiple NVIDIA GPUs. We trained the models using the Adam optimizer with an initial learning rate of $10^{-5}$. The input size is 96 \u00d7 96 \u00d7 96 voxels and batch size is 4."}, {"title": "3 Experiments", "content": "We extensively evaluated our framework on three datasets of multi-modal image segmentation, including two public challenge datasets and one private dataset."}, {"title": "3.1 Datasets", "content": "MS-CMRSeg Dataset. The MS-CMRSeg [11] dataset consists of cardiac magnetic resonance images from 45 patients with cardiomyopathy. The dataset includes three modalities: late gadolinium enhancement (LGE), T2-weighted (T2w) and balanced-steady state free precession (BSSFP). Notably, the dataset are paired as they originate from the same patients and are registered. The ground truth of the left ventricular cavity (LV), right ventricular cavity (RV), and left ventricular myocardium (Myo) is provided. For our experiment, we used 40 LGE/BSSFP image pairs for training and 5 pairs for testing and validation.\nAMOS Dataset. The AMOS dataset [5] comes from a multi-modality abdominal multi-organ segmentation challenge. It includes 300 CT and 60 MRI images from multi-center, multi-modality and multi-phase, each annotations includes 15 abdominal organs. Notably, the CT/MRI data of AMOS dataset is unpaired as they originate from different patients. The training set consists of 200 CT and 40 MRI images, and the testing set comprises 100 CT and 20 MRI images. In our experiments, we randomly selected 40 CT images from the 200 available in the training set and included 40 MRI images, creating a new training set. Additionally, we randomly selected 20 CT images from the 100 available in the testing set and included 20 MRI images, forming a new test and validation set.\nTAO Dataset. The thyroid-associated orbitopathy (TAO) dataset is an in-house multimodal dataset of thyroid-associated orbitopathy collected from the Gerald Choa Neuroscience Centre MRI Core Facility at the Prince of Wales Hos-pital in Hong Kong. This dataset comprises 100 cases, each of which underwent orbital MRI and received a definitive TAO diagnosis. Each case includes pre-contrast T1-weighted (T1) and fat-suppressed post-contrast T1-weighted (T1c), which are sequentially acquired from the same patient. Notably, the T1/Tlc data in the TAO dataset is unpaired due to differences in in-plane resolution. All MRI was performed on a 3.0 T Siemens scanner with a Head/Neck 64 Channel coil. T1 data was acquired using volumetric interpolated breath-hold examination (VIBE) pulse sequence with in-plane resolution of 0.555 \u00d7 0.555 mm\u00b2 and slice thickness of 1.5 mm. Tlc data was collected using a fat-suppressed spoiled-gradient echo (GRE) core sequence with in-plane resolution of 0.625 \u00d7 0.625 mm\u00b2 and slice thickness of 1.5 mm. The manual annotation was performed by a trained rater using ITK-SNAP, under the guidance of a senior radiologist who has over 20 years of experience. The segmentation mask includes 8 anatomical structures of extraocular muscles, i.e., herniation of the lacrimal gland (LG), and compression and edema of the optic nerve (ON), inferior oblique muscle (IOM), superior oblique muscle (SOM), superior rectus (SR), lateral rectus (LR), medial rectus (MR), and inferior rectus (IR). For our experiments, we randomly selected 80 T1/T1c image pairs from 100 cases as the training set, and the remaining 20 T1/T1c image pairs for the testing and validation set."}, {"title": "3.2 Comparison with State-of-the-art Methods", "content": "We used the CLIP-Driven Universal Model pre-trained weights [6] as our backbone when using a single RTX 3090 GPU. When using multiple GPUs, we used the SAM-Med3D [10] pre-trained weights [6] as our backbone. We conducted extensive experiments, using 10% and 20% labeled data ratios from three datasets for training, and employed the Dice score and Average Symmetric Surface Distance (ASSD) for quantitative evaluation. We compared our model with other multimodal learning methods, such as EFCD [4] and mmFor [14]. Additionally, we compared our model with semi-supervised multimodal learning approaches, such as UMML [18] and CML [13], and the fully supervised method V-Net [7]. Table 1 presents the quantitative performance of different methods on the AMOS [5] and TAO datasets. The results show that our framework considerably surpasses the comparison methods in both CT and MRI modalities, achieving high Dice scores in a label-scarce scenario on the AMOS [5]. As is well known, unpaired data from different modalities originate from different patients and cannot be directly aligned. This complicates the extraction of consistent features from different modalities compared to paired data. Despite the AMOS dataset [5] is unpaired, our model demonstrates superior performance. This can be attributed to its architecture, which is based on the Transformer model of SAM-Med3D [10]. The input images are divided into patches, which are then linearly embedded and combined with positional encoding, providing strong feature learning capability. We then utilize the CSC loss to align the channel-wise features from multimodal images. Furthermore, we introduce a novel MIA module to effectively harness modality-independent knowledge from each modality, facilitating efficient feature fusion. Consequently, our model achieves superior performance in semi-supervised multimodal segmentation tasks. For the TAO dataset, specifically for the T1 and T1c modalities, our framework has also shown promising results. Fig. 2 provides a comparative analysis of segmentation performance across various models on the MS-CMRSeg dataset. The performance results evaluated using the ASSD score on the MS-CMRSeg dataset, are presented in Table 3 of the supplementary material. It is evident from the results that our method surpasses the comparison methods in both BSSFP and LGE modalities, consistently achieving high Dice scores."}, {"title": "3.3 Ablation Study", "content": "We investigate the effectiveness of three key components in our method: modality-specific encoder, cross modality collaboration (CMC) strategy, and contrastive consistent learning (CCL) module. As shown in Table 2, we first set up a baseline network without using these components. Subsequently, we add the modality-specific encoder, CMC strategy, and CCL module one by one into the baseline network. Integrating these components consistently enhances performance, thereby highlighting their critical significance within our framework. Fig. 3 (c) presents an ablation analysis of our framework on the TAO dataset, comparing the performance with and without using the CMC strategy. Additionally, we conduct an ablation study on the MS-CMRSeg [11] dataset to compare the performance of using single and multiple modalities for training our model, as shown in Fig. 3 (d). The results demonstrate that using multiple modalities (i.e., BSSFP+LGE) outperforms using a single modality (i.e., BSSFP or LGE). The integration of the CMC strategy into our framework significantly enhances the highlighting of object regions and improves the capacity for class-discriminative localization compared to frameworks without the CMC strategy. Consequently, this demonstrates that the CMC strategy is able to foster a more efficient feature fusion."}, {"title": "4 Conclusion", "content": "We propose a novel semi-supervised multimodal segmentation framework that synergistically incorporates cross-modality collaboration and contrastive consistent learning strategies to achieve modality-independent knowledge and feature alignment across different modalities. We validate our method on three benchmark datasets and achieve comparable results to the state-of-the-art approaches. Notably, the framework exhibits exceptional robustness against significant inference variations, underscoring its potential for widespread clinical application in real-world clinical settings. More efficient fusion strategies on 3D medical image datasets and cross-modality settings will be conducted in our future studies."}]}