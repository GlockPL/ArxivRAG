{"title": "SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model", "authors": ["Chunlin Yu", "Hanqing Wang", "Ye Shi", "Haoyang Luo", "Sibei Yang", "Jingyi Yu", "Jingya Wang"], "abstract": "3D affordance segmentation aims to link human instructions to touchable regions of 3D objects for embodied manipulations. Existing efforts typically adhere to single-object, single-affordance paradigms, where each affordance type or explicit instruction strictly corresponds to a specific affordance region and are unable to handle long-horizon tasks. Such a paradigm cannot actively reason about complex user intentions that often imply sequential affordances. In this paper, we introduce the Sequential 3D Affordance Reasoning task, which extends the traditional paradigm by reasoning from cumbersome user intentions and then decomposing them into a series of segmentation maps. Toward this, we construct the first instruction-based affordance segmentation benchmark that includes reasoning over both single and sequential affordances, comprising 180K instruction-point cloud pairs. Based on the benchmark, we propose our model, SeqAfford, to unlock the 3D multi-modal large language model with additional affordance segmentation abilities, which ensures reasoning with world knowledge and fine-grained affordance grounding in a cohesive framework. We further introduce a multi-granular language-point integration module to endow 3D dense prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization with sequential reasoning abilities.", "sections": [{"title": "1. Introduction", "content": "Affordance is a crucial lens through which humans and embodied agents interact with various objects of the world. When provided with human instructions, affordance aims to highlight the actionable possibilities of these objects, linking visual perception with manipulation. While 2D affordance offers visual cues that suggest potential actions to embodied systems, 3D affordance provides a more direct and intuitive guidance for executing tasks in the realistic 3D world, and thus solidify the foundation for downstream robot manipulation tasks.\nPrevious work on 3D affordances has largely focused on the single-object, single-affordance paradigm, where affordance maps are grounded either in affordance categories [5, 24] or 2D demonstration images [6, 39]. Recently, language models have been employed to pair 3D objects with natural language questions [12], each designed to elicit a specific affordance. For example, the question \"How can you go through the door?\" can be interpreted by language models like BERT [9] or ROBERTa [16] as referring to the \"openable\" affordance component (e.g., the door handle). However, regardless of whether the grounding source is an affordance phrase, HOI image, or a natural question, each grounding source generally corresponds to a fixed affordance type.\nUnfortunately, current systems are incapable of actively reasoning based on complex user intentions, breaking them down into actionable primitives, and formulating a chain of affordances derived from each primitive. Real-world physical interactions, in particular, require modeling the inherent complexity of human instructions that often involve handling multiple objects and navigating through sequential affordances. For example, when agents are required to \"use the microwave to reheat the food inside the bowl\", they must first grasp the bowl, then open the microwave before using it to contain the bowl. From this perspective, the multi-object sequential reasoning ability is indispensable for shaping the next-generation affordance systems.\nRecently, Large-Language Models (LLMs) [2, 25, 43] have demonstrated exceptional sequential reasoning abilities, ingrained with internalized common-sense knowledge encoded from vast text data corpora. On top of that, the emergence of 3D Multimodal Large-Language Models (MLLMs) [28, 37] have further expanded their possibilities in understanding various object shapes in the 3D world. However, even the latest 3D MLLMs are not panaceas for reasoning about visual affordances from 3D objects, as they primarily focus on object-centered text generation tasks. This, therefore, highlights a pressing question: Can we devise a 3D multi-modal large language model to sequentially reason and segment multi-object affordances based on long-horizon human instructions?\nIn this paper, we introduce a new task called Sequential 3D Affordance Reasoning, designed to narrow the gap with real-world demands. Towards this, we construct a large-scale sequential affordance reasoning benchmark, containing 180K instruction-point cloud pairs. To ensure the diversity of the instruction data, four distinct methods are used for generating instructions. These methods include utilizing immersive 2D HOI images and detailed, colorful mesh renderings to prompt GPT-4o to generate diverse instructions, drawing on its inherent world knowledge, as illustrated in Fig. 2. Supported by this benchmark, we introduce our model SeqAfford, which unlocks the current 3D MLLMs with sequential affordance segmentation abilities. To further bolster the 3D dense prediction and reasoning task, we introduce a multi-granular language-point integration module, where dense point features conditioned on segmentation tokens of the large language model are integrated with sparse point features for subsequent dense prediction tasks. This module not only effectively injects the reasoning results of large language models into the dense point features, but enhances the affordance segmentation task with multi-granular levels of representation.\nTo summarize, our contributions are as follows:\n\u2022 We introduce the Sequential 3D Affordance Reasoning task, which involves the sequential reasoning and segmentation of affordances based on complex human instructions. This paradigm is crucial for the development of next-generation affordance systems.\n\u2022 We develop a large-scale sequential affordance reasoning benchmark with 180K instruction-point cloud pairs, serving as a comprehensive resource to advance research in affordance reasoning.\n\u2022 We are the first to leverage the internalized world knowledge of the pre-aligned 3D MLLMs to achieve multi-target sequential reasoning and explanations in a cohesive framework."}, {"title": "2. Related Work", "content": "3D Affordance Segmentation. With the rapid advancement of embodied AI, research on 3D affordance has increasingly garnered significant attention from both academia and industry. 3D AffordanceNet [5], built on point cloud data from PartNet [21], was the first to construct a fine-grained 3D affordance dataset, establishing a benchmark for 3D affordance research. Building on this, Yang et al. [39] proposed leveraging universal knowledge extracted from 2D human-object interaction (HOI) images to assist in 3D affordance segmentation. However, these methods rely solely on visual information to infer affordances, without considering that embodied agents in the real world communicate with humans through language. Consequently, such methods are limited in their applicability for direct deployment in embodied agents. Recently, Li et al. [12] introduced a language-based affordance segmentation task, promoting the integration of natural language and affordance understanding.\nFollowing this, Chu and Zhang [3] utilized LLMs to locate objects in 2D images and subsequently retrieve corresponding objects from a 3D dataset. Although these approaches leverage the reasoning capabilities of LLMs, they lack the ability to perform joint visual and language alignment, failing to bridge the gap between 2D and 3D modalities. Furthermore, their research assigns each text statement to a single specific affordance, overlooking the complexity of scenarios that often require the coordination of multiple affordances. In response to these limitations, we propose the integration of 3D multimodal large language models (MLLMs) into affordance segmentation. This novel approach enables the model to simultaneously comprehend contextual semantics and point cloud data, thereby facilitating affordance reasoning across a wide range of complex scenarios. By incorporating both natural language and 3D data, our model offers a more comprehensive and versatile understanding of affordances, rendering it better suited for real-world applications in embodied AI.\nMultimodal Large Language Models. Large Language Models (LLMs) have achieved remarkable success in processing natural language, and researchers have been working to extend these models' reasoning capabilities into the visual domain. Early efforts [2, 14, 23, 43] have made significant strides by enabling LLMs to process both visual and textual information concurrently, thereby introducing an initial level of human-like multimodal reasoning. However, for many practical applications, such as visual segmentation, these models lack the necessary fine-grained perception required for detailed visual tasks. To address this issue, research efforts [17, 33, 40, 42] enable the localization of specific regions within images by encoding spatial coordinates as tokens, improving the models' ability to reason about precise areas within the visual data.\nBuilding on these 2D MLLM advancements, research has increasingly expanded into the 3D domain. Following the paradigm of LLaVA, PointLLM [37] replaces the vision encoder with a 3D point encoder, enabling the processing of 3D data within the latent space of LLMs and facilitating 3D object understanding. Similarly, ShapeLLM [28] is built upon the enhanced 3D encoder RECON++, which strengthens geometric understanding through multi-view image distillation. Other models, such as 3D-LLM [7], leverage 2D foundational models, like CLIP ViT [29], to process multi-view rendered images of 3D point clouds, thereby integrating the 3D world into LLMs. However, despite these advancements, the majority of existing MLLMs are primarily focused on scene-level and object-level understanding, lacking the ability to recognize and segment fine-grained affordances of 3D objects in diverse semantic contexts. Addressing this limitation, our study aims to endow MLLMs with affordance-aware perception, enabling them to interpret and act upon 3D objects more effectively in context-sensitive scenarios."}, {"title": "3. Dataset", "content": "Affordance segmentation involves understanding the operability of objects in various contexts, and the varying complexity of intentions adds significant challenges to the process. Simple instructions typically pertain to the direct usage of an object, such as grasping a cup or opening a door. In contrast, complex instructions may involve multi-step actions or require contextual understanding, such as using a cup for a specific occasion or purpose. To address the challenge of affordance segmentation based on both simple and complex instructions, we constructed a dataset of instruction-point cloud pairs based on 3D AffordanceNet [5], encompassing both simple and complex types of intentions, which includes 162,386 instruction-point cloud pairs in the single affordance segmentation setting and 20,847 pairs in the sequential affordance segmentation setting, comprising a total of 18,371 point cloud instances across 23 object categories.\n3.1. Dataset Collection\nPoint Cloud. Our point cloud data and affordance annotations are entirely sourced from 3D AffordanceNet [5]. In the simple instruction setting, we generated five instructions for each affordance of every point cloud instance. For the sequential affordance segmentation setting, we carefully selected point cloud categories that support this configuration and generated corresponding instructions for each affordance sequence combination.\nInstruction. To create instructions, we developed four methods for generating instructions using GPT-4 [1]. Unlike LASO [12] which generates the same texts for all point cloud instances of a specific affordance type for each category, we generate text for each point cloud by utilizing the point cloud instance from 3D AffordanceNet [5] to trace back to the Mesh-rendered images in the PartNet [21] dataset. Additionally, we collect HOI images corresponding to the affordance types from IAGNet [39] to alleviate GPT's hallucination issues, enabling better understanding. Fig. 2 illustrates our ways of generating instructions in detail.\n3.2. Statistics and Annlysis\nOur dataset comprises 162,386 instruction-point cloud pairs in the single affordance segmentation setting and 20,847 pairs in the sequential affordance segmentation setting, making up a total of 18,371 point cloud instances across 23 object categories.\nBased on the complexity of the instructions, we have divided them into two settings. The first setting is based on instructions that can only infer a single specific affordance, which we define as the \"Single Affordance Segmentation Task\". The second setting is based on instructions that can infer a combination of multiple affordances in sequence, which we define as the \u201cSequential Affordance Segmentation Task\u201d.\nInspired by the evaluation settings presented in LASO [12] and IAGNet [39], we propose two types of distinct dataset configurations including Seen and Unseen. Seen: This default setting maintains similar distributions of object classes and affordance types across both training and testing phases. Unseen: This configuration is specifically designed to evaluate the model's ability to generalize affordance knowledge. In this setup, certain affordance-object pairings are deliberately omitted from the training set but introduced during testing. For instance, while the model may learn to grasp objects like bags and mugs during training, it is expected to generalize the \"grasp\" affordance to earphones, a pairing absent from the training data."}, {"title": "4. Method", "content": "4.1. Architecture Overview\nThe overall architecture of SeqAfford is presented in Fig. 3. Generally, SeqAfford mainly consists of three components: 1) a 3D vision encoder benefited from large-scale 3D representation learning, which provides solid foundations for dense prediction tasks; 2) a 3D Multi-modal Large Language Model (MLLM) $F$ that exhibits affordance reasoning ability with the aid of internalized world knowledge; 3) a Multi-Granular Language-Point Integration module that considers the effective integration the point features and the segmentation tokens of MLLM, synergizing both reasoning and segmentation tasks from a multi-granular feature perspective."}, {"title": "4.2. Network Architecture", "content": "3D MLLM Backbone. Recently, a series of 3D multi-modal language models have been proposed to deepen the understanding of open-world 3D objects, among which ShapeLLM is recently pretrained for understanding various embodied interactions. In light of this, we adopt ShapeLLM [28] as our backbone, denoted as $F$, where the point cloud encoder ReCon++ is pre-trained via multi-view distillation based on ReCon [27], and the LLM is drawn from LLaMa [32]. Previous work on 3D affordance tasks typically employed 3D backbones [24, 39] or utilized separate point-language encoders [12], which may fall short in reasoning and open-world generalization abilities. Here, we leap ahead by directly utilizing a unified 3D MLLM instead of relying solely on pure LLMs or other visual structures as our backbones, for two main reasons: 1) it opens new possibilities for open-world 3D objects understanding, bolstering the generalization of unseen objects or affordances; 2) it internalizes affordance perception ability, compressing it into natural language form, thus preparing for the subsequent affordance reasoning task.\nSquential Affordance Reasoning. Despite the efficacy of 3D MLLMs in aligning 3D representations with natural language, they are primarily designed for object-oriented text generation tasks and lack the capability for 3D dense prediction tasks, particularly in fine-grained affordance segmentation. To encapsulate the segmentation ability into 3D MLLMs, a specific segmentation token <SEG> can be appended to the vocabulary set of the MLLM, inspired by [10].\nFormally, when provided with point cloud $X_{point}$ and a text instruction $X_{txt}$ that demonstrates the user intentions on these potential objects, the 3D MLLM absorbs this multimodal information and generates a text response $y_{txt}$. It can be formulated as\n$y_{txt} = F(X_{point}, X_{txt}),$ (1)\nwhere the output $\\tilde{y}_{txt}$ would include several <SEG> tokens, where a single <SEG> indicates a segmentation result within the sequence. We then extract the last-layer embeddings $\\{h_{seg}^{(i)}\\}_{i=0}^{S-1}$ corresponding to the <SEG> tokens, where S is the number of predicted affordance sequences. Afterwards, an MLP projection layer to obtain $\\{H_{seg}^{(i)}\\}_{i=0}^{S-1}$ as follows\n$H_{seg}^{(i)} = Proj(h_{seg}^{(i)}).$ (2)\nMulti-Granular Language-Point Integration. After obtaining several segmentation tokens that indicate a sequence of regions for reasoning where the given objects can be afforded, the remaining work entails integrating the abstracted reasoning results into 3D point clouds for dense affordance predictions. Therefore, the multi-granular language-point integration module mainly consists of two stages: 1) the multi-granular feature propagation process, which iteratively up-samples the point cloud features into dense features with multiple granularities considered; 2) the point-language integration stage, which distills the informative language features (i.e. the segmentation tokens) into the dense visual features (i.e. the 3D point features), and fuses the integrated dense features with global sparse features for final affordance segmentation. This serves as a crucial step towards reasoning and segmenting affordances in a cohesive framework.\nTo enable multi-granular feature propagation, as shown in Fig. 4, we hierarchically up-sample (UpS. in Fig. 4) the"}, {"title": "4.3. Training objectives", "content": "Our objective is to train an end-to-end MLLM capable of generating diverse texts while simultaneously predicting point-wise affordance masks. To this end, we employ auto-regressive cross-entropy loss $L_{c}$ for text generation, Dice loss $L_{d}$ and Binary Cross-Entropy loss $L_{b}$ for guiding the segmentation mask prediction.\n$L = \\lambda_{c}L_{c}(y_{txt}, \\hat{y}_{txt}) + \\lambda_{b}L_{b}(y_{mask}, \\hat{y}_{mask}) + \\lambda_{d}L_{d}(y_{mask}, \\hat{y}_{mask}),$ (3)\nwhere the weights $\\lambda_{c}$, $\\lambda_{b}$, $\\lambda_{d}$ are utilized to balance the different loss items."}, {"title": "5. Experiment", "content": "We conduct extensive experiments to evaluate the effectiveness of our proposed dataset, task, and method, including both Single and Sequential Affordance segmentation tasks. In Sec. 5.1, we assess the capability of our model to ground the Single Affordance with simple instruction. Sec. 5.2 studies a more challenging task where the model is requested to predict the sequential affordances. Various ablation experiments on our model are performed in Sec. 5.3.\nImplementation Details. We employ ShapeLLM [28] as our 3D MLLM module in this paper, with the ShapeLLM-7B checkpoint as the default setting and we freeze the 3D encoder during training. We adopt Uni3D as the 3D vision encoder to enhance the 3D dense prediction tasks. Unless otherwise stated, the projection layer is implemented as a multi-layer perceptron. We employ LoRA [8] for efficient fine-tuning and set the rank of LoRA to 8 by default. Additionally, we utilize AdamW [19] optimizer with the learning rate and weight decay set to 0.0002 and 0, respectively. We adopt a cosine learning rate scheduler, with the warm-up iteration ratio set to 0.03. All attentions in ShapeLLM [28] are replaced by flash-attention [4] during training. The training is done on one A100 GPU for 10 epochs for the main experiments and during training, we use all mentioned datasets in Sec. 3 for joint training by leveraging task-specific prompts. For evaluation on a specific dataset, we finetune the trained model on the corresponding dataset.\nEvaluation Metrics and Baseline. To provide a comprehensive and effective evaluation, we follow previous works and finally chose four evaluation metrics: Area Under the Curve (AUC) [18], Mean Intersection Over Union (mIOU) [30], SIMilarity (SIM) [31] and Mean Absolute Error (MAE) [35]. To the best of our knowledge, LASO [12] is the most similar to our work. For a thorough comparison of our method, we conduct comparisons based on its setup in the Single Affordance segmentation task, comparisons with other baselines were also implemented following the approach mentioned in it. While in the sequential affordance segmentation task, we offer sequential information to these models enabling them to perform \u201csequential\" reasoning.\""}, {"title": "5.1. Results on Language-Guided Single Affordance Segmentation Task", "content": "As detailed in Table. 2, our model demonstrates superior performance across all evaluation metrics compared to the baseline methods. Unlike conventional segmentation tasks, language-guided Single Affordance segmentation demands not just identification but the integration of perception and cognition, necessitating the model's reasoning capabilities and access to world knowledge. Existing approaches struggle with implicit queries due to their lack of integration of perception and cognition, which further underscores the task's inherent challenges. In contrast, our model leverages MLLMs to bridge this gap, demonstrating superior performance by comprehending and interpreting the queries accurately."}, {"title": "5.2. Results on Language-Guided Sequential Affordance Reasoning Task", "content": "The sequential affordance reasoning task implies the ability to infer multiple affordances from a single text, which requires a more profound integration of cognitive and perceptual capabilities compared to the Single Affordance segmentation task. In our model, to make the Multimodal Large Language Model (MLLM) more comprehensible, we use the format <SEG> to represent the sequence of affordances and decode to obtain the affordance mask based on them. The previous models did not possess this capability. To compare them with our method, we used GPT to decompose the original instructions into new sequence instructions, then fed the decomposed instructions separately as inputs to these models, enabling them to perform \u201csequential\" reasoning. The main results are shown in Table. 2, and we speculate that the reason our model performs better is that, compared to LASO [12] which merely uses language models to encode the input text, we have introduced a multimodal 3D large language model. This model has a much stronger capability for integrating 3D data and text than a purely linguistic model, and it possesses a richer knowledge of the world, enabling it to handle multimodal sequential tasks more effectively. The sequential reasoning results are visualized in Fig. 5 (b), where our model can understand how a task is connected with sequential actionable affordances involving multi-objects. This ability is not only attributed to the challenging benchmark collected from diverse sources but also the powerful world knowledge internalized in 3D MLLMs.\""}, {"title": "5.3. Ablation Study", "content": "We conduct various ablation studies to assess the impact of different model implementations on our model SeqAfford's performance, including the multi-granular language-point integration module and the different choices of 3D vision encoder backbones.\nMulti-Granular Language-Point Integration Module. Introducing the Multi-Granular Language-Point Integration (MGLP) module results in a substantial improvement over the baseline, as shown in Table. 3. This underscores our method's capability to minimize information loss and this indicates that our method enables deep integration of high-"}, {"title": "6. Conclusion", "content": "In this paper, we introduce a novel task called the Sequential 3D Affordance Reasoning Task, designed to address complex user intentions that may involve sequential or long-horizon subtasks, transcending the conventional single-object, single-affordance paradigm. Our contributions include the construction of a large-scale 3D affordance benchmark comprising 180K instruction-point pairs collected from diverse sources, and the development of an advanced 3D multimodal LLM that leverages world knowledge to interpret complex user intentions, producing sequential, actionable affordance maps with reasonable explanations. SeqAfford advances 3D affordance reasoning by integrating LLM capabilities, enhancing the model's ability to handle complex, sequential tasks in real-world contexts. Our future work will focus on scene-level fine-grained reasoning to enable more sophisticated and context-aware affordance segmentation for embodied intelligent agents."}]}