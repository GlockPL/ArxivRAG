{"title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body", "authors": ["Zeqing Wang", "Qingyang Ma", "Wentao Wan", "Haojie Li", "Keze Wang", "Yonghong Tian"], "abstract": "Recent improvements in visual synthesis have significantly enhanced the depiction of generated human photos, which are pivotal due to their wide applicability and demand. Nonetheless, the existing text-to-image or text-to-video models often generate low-quality human photos that might differ considerably from real-world body structures, referred to as \u201cabnormal human bodies\". Such abnormalities, typically deemed unacceptable, pose considerable challenges in the detection and repair of them within human photos. These challenges require precise abnormality recognition capabilities, which entail pinpointing both the location and the abnormality type. Intuitively, Visual Language Models (VLMs) that have obtained remarkable performance on various visual tasks are quite suitable for this task. However, their performance on abnormality detection in human photos is quite poor. Hence, it is quite important to highlight this task for the research community. In this paper, we first introduce a simple yet challenging task, i.e., Fine-grained Human-body Abnormality Detection (FHAD), and construct two high-quality datasets for evaluation. Then, we propose a meticulous framework, named HumanCalibrator, which identifies and repairs abnormalities in human body structures while preserving the other content. Experiments indicate that our HumanCalibrator achieves high accuracy in abnormality detection and accomplishes an increase in visual comparisons while preserving the other visual content.", "sections": [{"title": "1. Introduction", "content": "Visual content generation models have demonstrated the capacity to create highly realistic representations within human photos, which hold considerable importance across various downstream tasks such as virtual reality, augmented reality, and the entertainment industry. Recent developments in text-to-image [3, 46, 47, 50] and text-to-video models [25, 42, 62] have enhanced both the quality and the realistic of generated human photos. However, these models frequently struggle to accurately replicate human body structures as they exist in the real world, leading to human photos with abnormalities such as absent or redundant body parts. Compared to low-quality, this abnormality is more unacceptable because it is more noticeable and has a larger gap with the real-world human body structure.\nSome methods try to solve this problem by adding additional constraints, such as Pose-ControlNet [66], HumanSD [22], and T2I-Adapter [38]. However, these methods are always hard to use due to their extra input or additional training. HumanRefiner [14], in another way, tackles the problem as a post-process method. It detects abnormalities in the generated human photos and then regenerates the whole content. However, this coarse-grained detection method can only detect which existing visual content is abnormal, ignoring the absent part. Furthermore, such a method can not preserve the background information of the original photo, which also limits its generalizability.\nTo detect abnormalities, it is intuitive to use a Vision-Language Model (VLM), which processes strong perception and reasoning capabilities and has been applied to various downstream perception tasks [10, 28, 43], as the back-"}, {"title": "2. Related Work", "content": "Vision-Language Model: Leveraging extensive pre-training datasets and benefitting from Large Language Models (LLMs) [1, 4, 5, 9, 21, 54, 56, 57, 61, 64] along with powerful Vision Encoders [13, 17, 29, 55], VLMs [8, 11, 17, 19, 29, 30, 33, 45, 55, 60] have achieved success in a range of visual tasks. These models excel at various perception and reasoning tasks [20, 23, 35, 52], prompting research that fine-tunes VLMs for specific applications [7, 48, 49, 67]. However, due to these VLMs being mainly based on text-image alignment training strategies, our experiments demonstrate that even the most powerful VLMs(such as OpenAI's GPT40) still fall short of detection abnormalities.\nDetection in AIGC: Detection within AIGC comprises various tasks; some initiatives aim to discern AIGC products [6, 34, 36, 40, 44]. Recently, [14] tried to fix the abnormalities in the existing body parts, although it is limited to providing only coarse-grained results, leading to inconsistencies with the original visuals. In contrast, we introduce a novel task in abnormality detection,i.e., the FHAD. To the best of our knowledge, this is the first attempt at such Fine-Grained abnormality detection in AIGC products.\nEvaluation in AIGC: Other methodologies interpret detection as a quality assessment exercise, evaluating AIGC products from diverse angles [27, 31, 41]. For instance, VideoPhy [2] assesses videos based on physical common sense, DEVIL [32] focuses on dynamic quality, and some studies assess the overall quality of AIGC videos with text-image alignment and video characteristics. Our proposed method, distinct from these, strives to ascertain whether the generated human body structure could occur in the real world. \nVisual Content Generation: The realm of visual content generation has undergone considerable evolution. Initial endeavors, such as GAN-based architectures [15, 37, 59,"}, {"title": "3. Task Definition", "content": "Fine-grained Human-body Abnormality Detection (FHAD): The goal of FHAD is to identify the differences in body structure from real-world humans in any given visual content that includes human photos. To achieve FHAD, the method needs to output the following two parts: (1) the semantic flag of abnormality $S_a \\in A$, that is, what type of body part abnormality $a$ exists. (2) the location of the abnormality $a$, output in the form of a bounding box $B_a$. For the input visual content within body part abnormalities $X$ and the pre-defined abnormalities set $A$, we consider any detection method as $M_d$, the task can be formatted as:\n$[B_a^o, S_a^o] = M_d(X)$.\nAs shown in Figure 1, for a given human photo, the method needs to detect the redundant hand in the red bounding box.\nHuman-body Abnormality Define: After reviewing a large number of generated human photos, we conduct an analysis of body part abnormalities and identified 12 distinct body part abnormalities which often create a significant gap between the real-world human body structure. It contains two types, the absent and redundant body parts. For the class of body parts, we identify the following body"}, {"title": "4. Methodology", "content": "4.1. Solution for Absent Body Part Detection\nWithin a detection pipeline, suppose the predefined human body classes of the whole body part as $P$ with the corresponding bounding box $B$ for the given visual content"}, {"title": "{$<p, b>}_0^n= M_a^d (P^e, B^e; X)$", "content": "where $n$ denotes the number of absent body parts.\nFor this task, the VLM is an intuitive choice since it is trained on a vast dataset and exhibits strong capabilities across a wide range of downstream tasks. However, the results demonstrate that (Sec. 5), though VLMs have been trained with a large quantity of normal data like ours, they still lack awareness of the abnormality on COCO Human-Aware Val which is based on real-world images and only contain absent abnormalities. We discuss this phenomenon in Appendix C. To solve this problem, the simplest and most intuitive method is to manually annotate a large training dataset on real AIGC data like [2, 14]. However, for the tasks we propose, extensive manual annotation is unrealistic and extremely costly (we perform the detailed annotation process in Appendix D). Furthermore, data annotated based on a specific generative model will inevitably contain certain biases. Therefore, our goal shifted towards automating the construction of these training data from a real-world dataset, which is also more aligned with our target.\nInspired by the process of humans detecting absent abnormalities, we propose the following body part correlation training strategy. For the given visual content $X$ with a normal human, we first ground all its body parts ${<p_i, b_i>}_0^n$, where $n$ is the number of grounded body parts, based on our predefined set $P$. For each body part representation $(p_i, b_i)$, we apply a mask operation to obtain:\n$X_i = mask(X, (p_i, b_i))$,\nwhere mask replaces the area $b_i$ with the background of $X$. For each masked image, we maintain:\n${(X_i, (p_i,b_i))}_{i=0}^n = {(X_i, (p_i, b_i))}_{i=0}^n$,\nto get the absent body part training sample $(X_i, (p_i, b_i))$. Similar to the current training objective of VLMs and LLMs, for each masked image $X_i$ and its corresponding absent body part representation $(p_i^a, b_i^a)$, we optimize the following auto-regressive objective:\n$p((p_i^a, b_i^a) | X_i, I_a) = \\prod_{j=1}^L p(x_j | X_i, I_a, {<j, (p_i^a, b_i^a), <j})$,"}, {"title": "4.2. Solution For Redundant Body Part Detection", "content": "where $\\theta$ represents the trainable parameters of the VLM, $L$ is the length of the concatenated instruction $I_a$ and the perception and position of the current absent body part $(p_i^a, b_i^a)$.\nCompared to absent body part abnormality, the situation of redundant body part abnormality is more diverse, which is reflected in the following two parts: (1) The position of the redundant body parts, which can appear in any area of the given photo. (2) The number of redundant body parts, which can be arbitrary. These two factors make the judgment no longer dependent on the existing body parts $P^e$. For each redundant body parts $p_r$ with its corresponding area $b_r$, for given visual content $X$ and the detection method $M_r$ can be formalized as:\n${<p_r^o, b_r^o>}_0^n = M_r^R(X)$,\nwhere $n$ is the number of redundant body parts. In contrast with Eq. 2, Eq. 6 indicates that addressing redundant bodies relies more on global visual information $X$ rather than the existing body parts $P^e$ within $X$. Based on this, we utilize a diffusion-based inpainting model $R$ with strong contextual understanding capabilities, combined with the grounding model $G$, to detect redundant body parts. In detail, for the given $X$, the model $G$ can ground all body parts $p \\in P$ with their locations $b$, as:\n${<p_r^o, b_r^o>}_0^n = G(X, P)$,"}, {"title": "4.3. Human Calibrator", "content": "where $n$ is the number of body parts in the given $X$. Then for each $(p, b)$, we use the model $R$ to re-generate the\nwhere $j$ is the number of the detected redundant body parts and $\\tau$ is the grounding threshold.\nBy leveraging the proposed Absent Human-body Detector $D$ with absent body part abnormality perception in Sec 4.1, and our proposed method for handling redundant body parts in Sec 4.2 we develop a comprehensive framework named HumanCalibrator, for the proposed Fine-Grained Human-body Abnormality Detection (FHAD) task. Furthermore, leveraging the ability of fine-grained detection, our HumanCalibrator can repair abnormalities of body parts while preserving other visual content unchanged. \nsteps:\n\u2022 Step I: Detect redundant body parts in the given visual content $X$. The first step is to obtain the set of redundant body parts in $X$ via Eq. 7, Eq. 8,i.e., ${<p, b>}_k=0$, where k is the number of redundant body parts. Identifying redundant body parts first serves two purposes. (1) Provide a better image base for addressing absent body part abnormalities. (2) Self-refine during the process of addressing absent human body abnormalities. If it is found that the detected absenting body part is the same as the previously identified redundant bodies, it can be considered that this is a wrongly resolved redundant body, thereby improving the accuracy of the entire framework.\n\u2022 Step II: Detect cyclically absent body parts in the given visual content $X$. We use $D$ to detect the absent body part in the current $X$ and obtain the detection result $(p_a, b_a)$. Subsequently, we utilize the inpainting prompt template $T$, which is predefined according to $P$, to obtain the corresponding repair prompt $T(p_a)$. By combining $p_a$ and $b_a$, we use the inpainting model $R$ to obtain a new image $X'$. The resulting $X'$ is then used as the image for the next detection cycle. This process continues until $D$ determines that there are no new $(p_a, b_a)$ in the current $X'$, which can be formalized as:\n$X_{t+1} = \\begin{cases}\nR(X,T(p^a), b^a), & \\text{if } (p^a, b^a) = D(X_t) \\neq 0)\\\\\nX_t, & \\text{otherwise}.\n\\end{cases}$"}, {"title": "5. Experiment", "content": "Through this loop, we can obtain all the absent bodies in the image, denoted as ${<p, b>}_q=0$, where $n$ is the number of detected absent body parts.\n\u2022 Step III: Repair the abnormality detected above. After detecting the redundant and absent body parts separately,\nmarkedly easier for human cognition. The intricacies of our comparative analysis are further discussed in Appendix C, and the specifics of the baseline models we test are detailed in Appendix B.\nDetails of Human Calibrator: For the absent body part detector, we finetune the LLaVAv1.5 7B [33] on COCO Train Split via Eq. 5. The format of the training data is similar to the COCO Human-Aware Val, some cases are shown in Appendix F. All training runs on 4 NVIDIA A800 GPUs. It takes around 30 hours to fine-tune 2 epochs with a learning rate of $2 \\times 10^{-5}$. We also test its ability on the COCO Human-Aware Val, the results are shown in Figure 6. For the other pretrained models used in our HumanCalibrator, we list more details in the Appendix A.\nBody Part Abnormality Detection. We evaluate the accuracy of our Human Calibrator's perception ability of abnormalities on the AIGC Human-Aware 1K dataset. Similar to the assessment on the COCO Human-Aware Val, we also benchmark several recent powerful VLMs as our baseline, the results are shown in Table 1, and we provide the acc and false detection rate (FDR) for each specific category. Similar to the results on the COCO Human-Aware Val, existing VLMs have difficulty in accurately perceiving abnormalities. In the aspect of absent detection, the trained Absent Human Detector effectively detects the existing abnormalities while maintaining a low FDR. In terms of absences, even under training-free conditions, our method achieves better results compared to the baseline. We analyze these results in detail in Appendix \u0412.\nExperiments on Exploring Current VLMs. To objectively verify the ability of the existing VLM to detect the abnormalities of body structure, we employ the automatically generated COCO Human-Aware Val to test the VLMs. It is important to note that the COCO Human-Aware Val is automatically produced based on the entire COCO Val Split. We compared different vision-language models (VLMs), including the state-of-the-art Open-source VLM InternVL2 and Closed-source VLM GPT4o and GPT4o-mini, along with contrastive learning-based models such as CLIP, as presented in Figure 6. The results demonstrate that despite their strong performance in many visual tasks, these models struggle with abnormal perceptions of body parts, displaying accuracies close to random guesses. Interestingly, even though humans and these VLMs are exposed to similar volumes of normal data, abnormality detection remains"}, {"title": "5.1. Body Part Abnormality Repair", "content": "Metrics. The goal of the task we proposed is to detect and locate the abnormality of body parts that make the human different from real-world humans, to comprehensively evaluate the effectiveness of our proposed HumanCalibrator in solving the task, we adopt six metrics for quantitative evaluation: (1) accuracy and FDR of abnormality detection, which calculate the detection results for each category of absent and redundant body parts. (2) Clip Score, the similarity between the image and the original prompt. (3) Human CLIP Score, the similarity between the image and the prompt which only contains only descriptions related to the human. (4) Human Concept Score, the similarity to the concept of 'human'. (5) FID, we use the origin image as the real image and the repaired image as the generated image to assess the extent to which our repair method preserves the original information. (6) Latent Consistency, consistency between the original and repaired image in the latent space.\nRepair Quality provides insight into the overall performance from two aspects. (1) The accuracy of the detected bounding boxes, due to the worse repair results caused by inaccurate re-generation to the detected areas compared to the original image. (2) Whether our repair makes the person in the visual content more similar to a real-world human, which is the motivation of our proposed task. CLIP Score and Human CLIP Score are used to evaluate the quality of the repairs, with the Human CLIP Score focusing on the prompt describing the person, which is more closely related to our task. The Human Concept Score measures the distance between the human in the image and a 'human' in the real world. As shown in Table 2. Compared to the original visual content, the repaired images show a certain degree of improvement in various metrics. The extent of our improvement is not significant, which is due to our good maintenance of consistency outside the abnormal parts of the image. To evaluate this consistency, we further measured the metrics on visual consistency.\nVisual Consistency. A key advantage of our proposed fine-grained anomaly detection is its ability to repair only the abnormalities while maintaining the consistency of other information. We compare our method with the pose-condition-based abnormality repair method [14] in FID and Latent space, as shown in Table 2. It can be observed that our HumanCalibrator maintains good visual consistency at both metrics. The details of the evaluation are shown in Appendix E.\nCase Study. We present three levels of case studies: image-level, video-level, and generalization-level. (1) Image Case Study: As shown in Figure 7, owing to the fine-grained abnormality detection of our HumanCalibrator, it repairs the abnormalities in the human body structure within images while preserving other normal and unrelated information. (2) Video Case Study: As illustrated in Figure 8, due to the detecting and repairing ability of our HumanCalibrator, we can repair the first and last frames of a video and regenerate the intermediate frames with a keyframe interpolation model. It shows that with the repaired first and last"}, {"title": "6. Conclusion", "content": "frames and the original prompt, our Human Calibrator can repair abnormalities while maintaining the other content of the video. (3) Generalization-level: Our HumanCalibrator also demonstrates strong performance on outputs generated\nIn this paper, we propose HumanCalibrator, a fine-grained level abnormal detection and repair framework in AIGC visual content with two datasets across different domains. It can detect abnormal body parts, and repair the abnormality while maintaining the other visual content. However, there are still limitations to our proposed framework, e.g. the predefined abnormal human body class limits the generalizability. In the future, we plan to extend our method to support more visual categories and more types of abnormalities."}]}