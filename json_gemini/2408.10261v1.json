{"title": "Relational Graph Convolutional Networks Do Not Learn Sound Rules", "authors": ["Matthew Morris", "David J. Tena Cucala", "Bernardo Cuenca Grau", "Ian Horrocks"], "abstract": "Graph neural networks (GNNs) are frequently used to predict missing facts in knowledge graphs (KGs). Motivated by the lack of explainability for the outputs of these models, recent work has aimed to explain their predictions using Datalog, a widely used logic-based formalism. However, such work has been restricted to certain subclasses of GNNs. In this paper, we consider one of the most popular GNN architectures for KGs, R-GCN, and we provide two methods to extract rules that explain its predictions and are sound, in the sense that each fact derived by the rules is also predicted by the GNN, for any input dataset. Furthermore, we provide a method that can verify that certain classes of Datalog rules are not sound for the R-GCN. In our experiments, we train R-GCNs on KG completion benchmarks, and we are able to verify that no Datalog rule is sound for these models, even though the models often obtain high to near-perfect accuracy. This raises some concerns about the ability of R-GCN models to generalise and about the explainability of their predictions. We further provide two variations to the training paradigm of R-GCN that encourage it to learn sound rules and find a trade-off between model accuracy and the number of learned sound rules.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are graph-structured knowledge bases where nodes and edges represent entities and their relationships, respectively (Hogan et al. 2022). KGs can be typically stored as sets of unary and binary facts, and are being exploited in an increasing range of applications (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014;\nSuchanek, Kasneci, and Weikum 2007;\nBouchard, Singh, and Trouillon 2015;\nHamilton, Ying, and Leskovec 2017).\nKnowledge graphs are, however, often incomplete, which has led to the rapid development of the field of KG completion\u2014the task of extending an incomplete KG with all missing facts holding in its (unknown) complete version. KG completion is typically conceptualised as a classification problem, where the aim is to learn a function that, given the incomplete KG and a candidate fact as input, decides whether the latter holds in the completion of the former. A wide range of KG approaches have been proposed in the literature, including embedding-based approaches with distance-based scoring functions (Bordes et al. 2013;\nSun et al. 2018), tensor products (Nickel et al. 2011;\nYang et al. 2015), box embeddings (Abboud et al. 2020),\nrecurrent neural networks (Sadeghian et al. 2019), differentiable reasoning (Rockt\u00e4schel and Riedel 2017;\nEvans and Grefenstette 2018), and LLMs\n(Yao, Mao, and Luo 2019; Xie et al. 2022). Amongst\nall neural approaches to KG completion, however, those based on graph neural networks (GNNs) have received special attention (Ioannidis, Marques, and Giannakis 2019;\nLiu et al. 2021; Pflueger, Tena Cucala, and Kostylev 2022).\nThese include R-GCN (Schlichtkrull et al. 2018)\nand its extensions (Tian et al. 2020; Cai et al. 2019;\nVashishth et al. 2019; Yu et al. 2021; Shang et al. 2019;\nLiu et al. 2021), where the basic R-GCN model remains a common baseline for evaluating against or as part of a larger system (Gutteridge et al. 2023; Liu et al. 2023;\nLi et al. 2023; Tang et al. 2024;\nZhang et al. 2023;\nLin et al. 2023).\nAlthough these embedding and neural-based approaches to KG completion have proved effective in practice, their predictions are difficult to explain and interpret (Garnelo and Shanahan 2019). This is in contrast to logic-based and neuro-symbolic approaches to KG completion, such as rule learning methods, where the extracted rules can be used to generate rigourous proofs explaining the prediction of any given fact. RuleN (Meilicke et al. 2018) and AnyBURL (Meilicke et al. 2018) heuristically identify Datalog rules from the given data and apply them directly for completing the input graph. RNNLogic (Qu et al. 2020) uses a probabilistic model to select the most promising rules. Other works attempt to extract Datalog rules from trained neural models, including Neural-LP\n(Yang, Yang, and Cohen 2017),\nDRUM (Sadeghian et al. 2019), and Neural Theorem Provers (Rockt\u00e4schel and Riedel 2017).\nAs shown in (Tena Cucala, Cuenca Grau, and Motik 2022;\nWang et al. 2023), however, the extracted Datalog rules are not faithful to the model in the sense that the rules may derive, for some dataset, facts that are not predicted by the model (unsoundness) as well as failing to derive predicted facts (incompleteness).\nAs a result, there is increasing interest in the development of neural KG completion methods whose predictions can be faithfully characterised by means of rule-based reasoning."}, {"title": "Our Contribution", "content": "In this paper, we consider sum-GNNs, which use sum as aggregation function and which do not impose restrictions on model weights. These GNNs can be seen both as an extension to the GNNs in (Tena Cucala et al. 2023) with sum aggregation but without the monotonicity requirement, as well as a variant of the R-GCN model. The functions learnt by sum-GNNs may be non-monotonic: predicted facts may be invalidated by adding new facts to the input KG. As a result, these GNNs cannot, in general, be faithfully captured by (negation-free) Datalog programs. Our aim in this paper is to identify a subset of the output channels (i.e. features) of the GNN which exhibit monotonic behaviour, and for which sound Datalog rules may be extracted. The ability to extract sound rules is important as it allows us to explain model predictions associated to the identified output channels. Furthermore, we provide means for identifying unbounded output channels for which no sound Datalog rule exists, hence suggesting that these channels inherently exhibit non-monotonic behaviour.\nWe conducted experiments on the benchmark datasets by (Teru, Denis, and Hamilton 2020) and also use the rule-based LogInfer evaluation framework described in (Liu et al. 2023), which we extended to include a mixture of monotonic and non-monotonic rules. Our experiments show that even under ideal scenarios, without restrictions on the training process, all the channels in the trained GNN are unbounded (even for monotonic LogInfer benchmarks), which implies that there are no sound Datalog rules for the model. We then consider two adjustments to the training process where weights sufficiently close to zero (as specified by a given threshold) are clamped to zero iteratively during training. As the weight clamping threshold increases, we observe that an increasing number of output channels exhibit monotonic behaviour and we obtain more sound rules, although the model accuracy diminishes. Hence, there is a trade-off between model performance and rule extraction."}, {"title": "2 Background", "content": "Datalog We fix a signature of countably infinite, disjoint sets of predicates and constants, where each predicate is associated with a non-negative integer arity. We also consider a countably infinite set of variables disjoint with the sets of predicates and constants. A term is a variable or a constant. An atom is an expression of the form \\( R(t_1, ..., t_n) \\), where each \\( t_i \\) is a term and R is a predicate with arity n. A literal is an atom or any inequality \\( t_1 \\neq t_2 \\). A literal is ground if it contains no variables. A fact is a ground atom and a dataset D is a finite set of facts. A (Datalog) rule is an expression of the form\n\\[ B_1 \\land ... \\land B_n \\rightarrow H, \\]\nwhere \\( B_1, ..., B_n \\) are its body literals and H is its head atom. We use the standard safety requirements for rules: every variable that appears in a rule must occur in a body atom. Furthermore, to avoid vacuous rules, we require that each inequality in the body of a rule mentions two different terms. A (Datalog) program is a finite set of rules. A substitution v maps finitely many variables to constants. For literal a and a substitution v defined on each variable in a, av is obtained by replacing each occurrence of a variable x in a with v(x). For a dataset D and a ground atom B, we write \\( D \\models B \\) if \\( B \\in D \\); furthermore, given constants \\( a_1 \\) and \\( a_2 \\), we write \\( D \\models a_1 \\neq a_2 \\) if \\( a_1 \\neq a_2 \\), for uniformity. The immedi-ate consequence operator \\( T_r \\) for a ruler of form (1) maps a dataset D to dataset \\( T_r(D) \\) containing Hv for each substitution v such that \\( D \\models B_iv \\) for each \\( i \\in \\{1, ..., n\\} \\). For a program P, \\( T_P(D) = \\cup_{r\\in P} T_r(D) \\).\nGraphs We consider real-valued vectors and matrices. For v a vector and \\( i > 0 \\), \\( v[i] \\) denotes the i-th element of v. For A a matrix and \\( i, j > 0 \\), \\( A[i, j] \\) denotes the element in row i and column j of A. A function \\( \\sigma: \\mathbb{R} \\rightarrow \\mathbb{R} \\cup \\{0\\} \\) is monoton-ically increasing if \\( x < y \\) implies \\( \\sigma(x) \\leq \\sigma(y) \\). We apply functions to vectors element-wise.\nFor a finite set Col of colours and \\( \\delta \\in \\mathbb{N} \\), a (Col, \\( \\delta \\))-graph G is a tuple \\( (V, \\{E_c\\}_{c \\in Col}, \\Lambda) \\) where V is a finite vertex set, each \\( E_c \\subseteq V \\times V \\) is a set of directed edges, and \\( \\Lambda \\) assigns to each \\( v \\in V \\) a vector of dimension \\( \\delta \\). When \\( \\Lambda \\) is clear from the context, we abbreviate the labelling \\( \\Lambda(v) \\) as v. Graph G is undirected if \\( E_c \\) is symmetric for each \\( c \\in Col \\) and is Boolean if \\( v[i] \\in \\{0, 1\\} \\) for each \\( v \\in V \\) and \\( i \\in \\{1, ..., \\delta\\} \\).\nGraph Neural Networks We consider GNNs with sum ag-gregation. A (Col,d)-sum graph neural network (sum-GNN) N with L\u2265 1 layers is a tuple\n\\[ <\\{A^l\\}_{1 \\leq l \\leq L}, \\{B_c^l\\}_{c \\in Col, 1 \\leq l \\leq L}, \\{b^l\\}_{l}, \\sigma_l, clst>, \\]\nwhere, for each \\( l \\in \\{1, ..., L\\} \\) and \\( c \\in Col \\), matrices \\( A^l \\) and \\( B_c^l \\) are of dimension \\( \\delta_l \\times \\delta_{l-1} \\) with \\( \\delta_0 = \\delta_L = d \\), \\( b^l \\) is a vector of dimension \\( \\delta_l \\), \\( \\sigma_l: \\mathbb{R} \\rightarrow \\mathbb{R}^+ \\cup \\{0\\} \\) is a monoton-ically increasing activation function with non-negative range, and \\( clst: \\mathbb{R} \\rightarrow \\{0, 1\\} \\) for threshold t \u2208 R is a step clas-sification function such that \\( clst(x) = 1 \\) if \\( x > t \\) and \\( clst(x) = 0 \\) otherwise.\nApplying N to a (Col, d)-graph induces a sequence of la-bels \\( v^0, v^1, ..., v^L \\) for each vertex v in the graph as follows. First, \\( v^0 \\) is the initial labelling of the input graph; then, for"}, {"title": "3 Partitioning the Channels of a GNN", "content": "In this section, we first provide two approaches for identi-fying channels in a sum-GNN that exhibit monotonic be-haviour, which will later allow us to extract sound Dat-alog rules with head predicates corresponding to these channels. Candidate Datalog rules can be effectively checked for soundness using the approach developed by (Tena Cucala et al. 2023) for monotonic GNNs. Furthermore, we provide an approach for identifying channels for which no sound Datalog rule exists. Our techniques are data-independent and rely on direct analysis of the dependencies between feature vector components via the parameters of the model.\n3.1 Safe Channels\nIn this section, we introduce the notion of a safe channel of a sum-GNN N. Intuitively, a channel is safe if, for any dataset D, the computation of its value for each vertex v \u2208enc(D) through the application of N to D is affected only by non-negative values in the weight matrices of the GNN.\nFor instance, consider a simple sum-GNN where matrix \\( A^l \\) for layer l is given below\n\\[ A^l = \\begin{pmatrix}\n  1 & 0 & 1 & 4\\\\\n  1 & 0 & 0 & 2\\\\\n -8 & -1 & 0 & -2\\\\\n\\end{pmatrix} \\]\nand each of the matrices \\( B_c^l \\) for c\u2208 Col contain only ze-roes. Furthermore, assume that layer l - 1 has four chan-nels, where the third one has been identified as unsafe and all other channels have been identified as safe. Then, for an arbitrary vertex v, the product of \\( A^l \\) and \\( v^{l-1} \\) in the com-putation of \\( v^l \\) reveals that the second channel in l is safe, because the unsafe component in \\( v^{l-1} \\) is multiplied by zero"}, {"title": "4 Experiments", "content": "We train sum-GNNs on several link prediction datasets, us-ing the dataset transformation described in Section 2. For each dataset, we train a sum-GNN with ReLU activation functions, biases, and two layers (this architecture corre-sponds to R-GCN with biases). The hidden layer of the GNN has twice as many channels as its input. Moreover, we also train four additional instances of sum-GNNs, us-ing a modified training paradigm to facilitate the learning of sound rules (see Section 4.2). For each trained model, we compute standard classification metrics, such as precision, recall, accuracy, and F1 score, and area under the precision-recall curve (AUPRC).\nWe train all our models using binary cross entropy loss for training and the Adam optimizer with a standard learn-ing rate of 0.001. For each model, we choose the classifica-tion threshold by computing the accuracy on the validation set across a range of 108 thresholds between 0 and 1 and selecting the one which maximises accuracy. We run each experiment across 10 different random seeds and present the aggregated metrics. Experiments are run using PyTorch Ge-ometric, with a CPU on a Linux server.\nChannel Classification and Rule Extraction For each trained model, we compute which output channels of the model were safe, stable, increasing, and unbounded (using 1000 random samples of pairs of Boolean feature vectors and colour sequences). On all datasets, for each channel p that is stable or increasing, we iterate over each Data-log rule in the signature with up to two body atoms and a head predicate Up, and count the number of sound rules, using Proposition 7 to check soundness. In benchmarks with a large number of predicates, we only check rules with one body atom, since searching the space of rules with two body atoms is intractable. For datasets created with LogIn-fer (Liu et al. 2023), which are obtained by enriching a pre-existing dataset with the consequences of a known set of Datalog rules, we also check if these rules were sound for the model.\n4.1 Datasets\nWe use three benchmarks provided by (Teru, Denis, and Hamilton 2020): WN18RRv1, FB237v1, and NELLv1; each of these benchmarks provides datasets for training, validation, and testing, as well as negative examples.\nWe also use LogInfer (Liu et al. 2023), a frame-work which augments a dataset by applying Data-log rules of a certain shape-called a \"pattern\"-and adding the consequences of the rules back to the dataset. We apply the LogInfer framework to datasets FB15K-237 (Toutanova and Chen 2015) and WN18RR (Dettmers et al. 2018). We consider the very simple rule pat-terns hierarchy and symmetry defined in (Liu et al. 2023); we also use a new monotonic pattern cup, which has a tree-like structure, and a non-monotonic rule pattern non-monotonic hierarchy (nmhier); all patterns are shown in Ta-ble 1. For each pattern P, we refer to the datasets obtained by enriching FB15K-237 and WN18RR by FB-P and WN-P, re-spectively. For each dataset and pattern, we randomly select 10% of the enriched training dataset to be used as targets and the rest as inputs to the model. Furthermore, we con-sider an additional instance of FB15K-237 and WN18RR where we again apply the hierarchy pattern, but we include a much larger number of consequences in the enriched train-ing dataset; we refer to these datasets as FB-superhier and WN-superhier, as a shorthand for super-hierarchy. The pur-pose of this is to create a dataset where it should be as easy as possible for a model to learn the rules applied in the dataset's creation. Finally, we use LogInfer to augment FB15K-237 and WN18RR using multiple rule patterns at the same time; in particular, we combine monotonic with non-monotonic rule patterns. This allows us to test the ability of our rule extraction methods to recover the sound monotonic rules that were used to generate the enriched dataset, in the pres-ence of facts derived by non-monotonic rules. For example, WN-hier_nmhier refers to a LogInfer dataset where WN is extended using the hierarchy and non-monotonic hierarchy"}, {"title": "5 Conclusion", "content": "In this paper, we provided two ways to extract sound rules from sum-GNNs and a procedure to prove that certain rules are not sound for a sum-GNN. Our methods rely on classify-ing the output channels of the sum-GNN. We found that, in our experiments, R-GCN, a specific instance of sum-GNN, provably has no sound rules when trained in the standard way, even when using ideal datasets. We provided two alter-natives to train R-GCN, the first of which clamps all negative weights to zero and results in R-GCN being entirely mono-tonic, yielding good performance and many sensible sound rules on datasets with monotonic patterns, but poor perfor-mance on datasets with a mixture of monotonic and non-monotonic patterns. Our second alternative yields a trade-off between accuracy and the number of sound rules. We found that, in practice, almost every channel of the sum-GNN is classified in a manner that allows us to either extract sound rules or prove that there are no associated sound rules.\nThe limitations of this work are as follows. First, our anal-ysis only considers GNNs with sum aggregation and mono-tonically increasing activation functions: it is unclear how sound rules can be extracted from models that use mean ag-gregation or GELU activation functions. Furthermore, our methods do not guarantee that every output channel will be characterised such that we can show that it either has no sound rules or it can be checked for sound rules.\nFor future work, we aim to consider other GNN archi-tectures, extend our rule extraction to non-monotonic log-ics, provide relaxed definitions of soundness, and consider R-GCN with a scoring function (such as DistMult) as a de-coder, instead of using a dataset transformation to perform link prediction."}]}