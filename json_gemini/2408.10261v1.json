{"title": "Relational Graph Convolutional Networks Do Not Learn Sound Rules", "authors": ["Matthew Morris", "David J. Tena Cucala", "Bernardo Cuenca Grau", "Ian Horrocks"], "abstract": "Graph neural networks (GNNs) are frequently used to predict missing facts in knowledge graphs (KGs). Motivated by the lack of explainability for the outputs of these models, recent work has aimed to explain their predictions using Datalog, a widely used logic-based formalism. However, such work has been restricted to certain subclasses of GNNs. In this paper, we consider one of the most popular GNN architectures for KGs, R-GCN, and we provide two methods to extract rules that explain its predictions and are sound, in the sense that each fact derived by the rules is also predicted by the GNN, for any input dataset. Furthermore, we provide a method that can verify that certain classes of Datalog rules are not sound for the R-GCN. In our experiments, we train R-GCNs on KG completion benchmarks, and we are able to verify that no Datalog rule is sound for these models, even though the models often obtain high to near-perfect accuracy. This raises some concerns about the ability of R-GCN models to generalise and about the explainability of their predictions. We further provide two variations to the training paradigm of R-GCN that encourage it to learn sound rules and find a trade-off between model accuracy and the number of learned sound rules.", "sections": [{"title": "Introduction", "content": "Knowledge graphs (KGs) are graph-structured knowledge bases where nodes and edges represent entities and their relationships, respectively (Hogan et al. 2022). KGs can be typically stored as sets of unary and binary facts, and are being exploited in an increasing range of applications (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014; Suchanek, Kasneci, and Weikum 2007; Bouchard, Singh, and Trouillon 2015; Hamilton, Ying, and Leskovec 2017). Knowledge graphs are, however, often incomplete, which has led to the rapid development of the field of KG completion\u2014the task of extending an incomplete KG with all missing facts holding in its (unknown) complete version. KG completion is typically conceptualised as a classification problem, where the aim is to learn a function that, given the incomplete KG and a candidate fact as input, decides whether the latter holds in the completion of the former. A wide range of KG approaches have been proposed in the literature, including embedding-based approaches with distance-based scoring functions (Bordes et al. 2013; Sun et al. 2018), tensor products (Nickel et al. 2011; Yang et al. 2015), box embeddings (Abboud et al. 2020), recurrent neural networks (Sadeghian et al. 2019), differentiable reasoning (Rockt\u00e4schel and Riedel 2017; Evans and Grefenstette 2018), and LLMs (Yao, Mao, and Luo 2019; Xie et al. 2022). Amongst all neural approaches to KG completion, however, those based on graph neural networks (GNNs) have received special attention (Ioannidis, Marques, and Giannakis 2019; Liu et al. 2021; Pflueger, Tena Cucala, and Kostylev 2022). These include R-GCN (Schlichtkrull et al. 2018) and its extensions (Tian et al. 2020; Cai et al. 2019; Vashishth et al. 2019; Yu et al. 2021; Shang et al. 2019; Liu et al. 2021), where the basic R-GCN model remains a common baseline for evaluating against or as part of a larger system (Gutteridge et al. 2023; Liu et al. 2023; Li et al. 2023; Tang et al. 2024; Zhang et al. 2023; Lin et al. 2023). Although these embedding and neural-based approaches to KG completion have proved effective in practice, their predictions are difficult to explain and interpret (Garnelo and Shanahan 2019). This is in contrast to logic-based and neuro-symbolic approaches to KG completion, such as rule learning methods, where the extracted rules can be used to generate rigourous proofs explaining the prediction of any given fact. RuleN (Meilicke et al. 2018) and AnyBURL (Meilicke et al. 2018) heuristically identify Datalog rules from the given data and apply them directly for completing the input graph. RNNLogic (Qu et al. 2020) uses a probabilistic model to select the most promising rules. Other works attempt to extract Datalog rules from trained neural models, including Neural-LP (Yang, Yang, and Cohen 2017), DRUM (Sadeghian et al. 2019), and Neural Theorem Provers (Rockt\u00e4schel and Riedel 2017). As shown in (Tena Cucala, Cuenca Grau, and Motik 2022; Wang et al. 2023), however, the extracted Datalog rules are not faithful to the model in the sense that the rules may derive, for some dataset, facts that are not predicted by the model (unsoundness) as well as failing to derive predicted facts (incompleteness). As a result, there is increasing interest in the development of neural KG completion methods whose predictions can be faithfully characterised by means of rule-based reasoning."}, {"title": "Our Contribution", "content": "In this paper, we consider sum-GNNs, which use sum as aggregation function and which do not impose restrictions on model weights. These GNNs can be seen both as an extension to the GNNs in (Tena Cucala et al. 2023) with sum aggregation but without the monotonicity requirement, as well as a variant of the R-GCN model. The functions learnt by sum-GNNs may be non-monotonic: predicted facts may be invalidated by adding new facts to the input KG. As a result, these GNNs cannot, in general, be faithfully captured by (negation-free) Datalog programs. Our aim in this paper is to identify a subset of the output channels (i.e. features) of the GNN which exhibit monotonic behaviour, and for which sound Datalog rules may be extracted. The ability to extract sound rules is important as it allows us to explain model predictions associated to the identified output channels. Furthermore, we provide means for identifying unbounded output channels for which no sound Datalog rule exists, hence suggesting that these channels inherently exhibit non-monotonic behaviour. We conducted experiments on the benchmark datasets by (Teru, Denis, and Hamilton 2020) and also use the rule-based LogInfer evaluation framework described in (Liu et al. 2023), which we extended to include a mixture of monotonic and non-monotonic rules. Our experiments show that even under ideal scenarios, without restrictions on the training process, all the channels in the trained GNN are unbounded (even for monotonic LogInfer benchmarks), which implies that there are no sound Datalog rules for the model. We then consider two adjustments to the training process where weights sufficiently close to zero (as specified by a given threshold) are clamped to zero iteratively during training. As the weight clamping threshold increases, we observe that an increasing number of output channels exhibit monotonic behaviour and we obtain more sound rules, although the model accuracy diminishes. Hence, there is a trade-off between model performance and rule extraction."}, {"title": "Background", "content": "Datalog We fix a signature of countably infinite, disjoint sets of predicates and constants, where each predicate is associated with a non-negative integer arity. We also consider a countably infinite set of variables disjoint with the sets of predicates and constants. A term is a variable or a constant. An atom is an expression of the form R(t1, ..., tn), where each ti is a term and R is a predicate with arity n. A literal is an atom or any inequality t\u2081 \u2260 t2. A literal is ground if it contains no variables. A fact is a ground atom and a dataset D is a finite set of facts. A (Datalog) rule is an expression of the form\nB\u2081 \u2227 ... \u2227 Bn \u2192 H, (1)\nwhere B1, ..., Bn are its body literals and H is its head atom. We use the standard safety requirements for rules: every variable that appears in a rule must occur in a body atom. Furthermore, to avoid vacuous rules, we require that each inequality in the body of a rule mentions two different terms. A (Datalog) program is a finite set of rules. A substitution v maps finitely many variables to constants. For literal a and a substitution v defined on each variable in a, av is obtained by replacing each occurrence of a variable x in a with v(x). For a dataset D and a ground atom B, we write D |= B if B \u2208 D; furthermore, given constants a\u2081 and a2, we write D |= a1 \u2260 a2 if a1 \u2260 a2, for uniformity. The immedi-ate consequence operator Tr for a ruler of form (1) maps a dataset D to dataset Tr(D) containing Hv for each substitution v such that D |= Biv for each i \u2208 {1, . . ., n}. For a program P, TP(D) = \u222ar\u2208P Tr(D).\nGraphs We consider real-valued vectors and matrices. For v a vector and i > 0, v[i] denotes the i-th element of v. For A a matrix and i, j > 0, A[i, j] denotes the element in row i and column j of A. A function \u03c3: \u211d \u2192 \u211d is monotonically increasing if x < y implies \u03c3(x) \u2264 \u03c3(y). We apply functions to vectors element-wise.\nFor a finite set Col of colours and \u03b4 \u2208 \u2115, a (Col, \u03b4)-graph G is a tuple (V, {Ec}c\u2208Col, A) where V is a finite vertex set, each Ec \u2286 V \u00d7 V is a set of directed edges, and A assigns to each v \u2208 V a vector of dimension \u03b4. When A is clear from the context, we abbreviate the labelling A(v) as v. Graph G is undirected if Ec is symmetric for each c \u2208 Col and is Boolean if v[i] \u2208 {0, 1} for each v \u2208 V and i \u2208 {1, ..., \u03b4}.\nGraph Neural Networks We consider GNNs with sum aggregation. A (Col,\u03b4)-sum graph neural network (sum-GNN) N with L\u2265 1 layers is a tuple\n\u27e8{Ae}1\u2264l\u2264L, {Bc}c\u2208Col,1\u2264l\u2264L, {be}1\u2264l\u2264L, \u03c3e, clst\u27e9, (2)\nwhere, for each l \u2208 {1, ..., L} and c \u2208 Col, matrices Ae and B are of dimension de \u00d7 \u03b4\u03b5\u22121 with \u03b40 = \u03b4\u2081 = d, be is a vector of dimension \u03b4\u03b5, \u03c3\u03b5 : \u211d \u2192 \u211d+ \u222a {0} is a monotonically increasing activation function with non-negative range, and clst: \u211d \u2192 {0, 1} for threshold t \u2208 \u211d is a step classification function such that clst(x) = 1 if x > t and clst(x) = 0 otherwise.\nApplying N to a (Col, d)-graph induces a sequence of labels v0, V1, ..., v\u2081 for each vertex v in the graph as follows. First, vo is the initial labelling of the input graph; then, for"}, {"title": null, "content": "each 1 < l < L, ve is defined by the following expression:\nve = \u03c3\u03b5(be + Aeve\u22121 + \u03a3Bc\u03a3ue\u22121). (3)\nc\u2208Col (v,u)\u2208Ec\nThe output of N is a (Col,d)-graph with the same vertices and edges as the input graph, but where each vertex is labelled by clst(VL). For layer l \u2208 {0, ..., L} of N, each i \u2208 {1, ..., de} is referred to as a channel.\nThe R-GCN model (Schlichtkrull et al. 2018) can be seen as a sum-GNN variant with ReLU activations and zero biases in all layers. The definition in (Schlichtkrull et al. 2018) includes a normalisation parameter Ci,r (cf. their Equation (2)), which can depend on a predicate r and/or vertex i under special consideration when applying the GNN. A dependency on the vertex implies that the values of these parameters are data-dependent\u2014that is, they are computed at test time based on the concrete graph over which the GNN is evaluated (e.g., Ci,r could be computed as the number of r-neighbours of vertex i). Sum-GNNs capture the R-GCN model under the assumption that the normalisation parameters are data-independent and hence can be fixed after training; we further assume for simplicity that they are set to 1.\nDataset Transformations Through sum-GNNs A sum-GNN N can be used to realise a transformation TN from datasets to datasets over a given finite signature (Tena Cucala et al. 2023). To this end, the input dataset must be first encoded into a graph that can be directly processed by the sum-GNN, and the graph resulting from the sum-GNN application must be subsequently decoded back into an output dataset. Several encoding/decoding schemes have been proposed in the literature. We adopt the so-called canonical scheme, which is a straightforward way of converting datasets to coloured graphs. In particular, colours in graphs correspond to binary predicates in the signature and channels of feature vectors in the input and output layers of the sum-GNN to unary predicates. For each p \u2208 {1, ..., \u03b4}, we denote the unary predicate corresponding to channel p as Up. More precisely, the canonical encoding enc(D) of a dataset D is the Boolean (Col, \u03b4)-graph with a vertex va for each constant a in D and a c-coloured edge (va, vb) for each fact Re(a, b) \u2208 D. Furthermore, given a vertex va corresponding to constant a, vector component va [p] is set to 1 if and only if Up(a) \u2208 D, for p \u2208 {1, ..., \u03b4}. The decoder dec is the inverse of the encoder. The canonical dataset transformation induced by a sum-GNN N is then defined as: TN(D) = dec(N(enc(D))). We abbreviate N(enc(D)) by N(D).\nSoundness and Completeness A Datalog program or rule a is sound for a sum-GNN N if Ta(D) \u2286 T\u338f(D) for each dataset D. Conversely, a is complete for N if Tw(D) \u2286 Ta(D) for each dataset D. Finally, we say that a is equivalent to N if it is both sound and complete for N. The following proposition establishes that the containment relation that defines soundness for a rule or program still holds when the operators are composed a finite number of times.\nProposition 1. If a is a rule or program sound for sum-GNN N, then for any dataset D and k \u2208 N, the containment holds"}, {"title": null, "content": "when Ta and T\u0145 are composed k times: Tk(D) \u2286 Tk (D).\nLink Prediction The link prediction task assumes a given incomplete dataset D and an (unknown) completion D* of D containing all missing binary facts that are considered true over the predicates and constants of D. Thus, given a fact a involving a binary predicate and constants from D, the task is to predict whether a \u2208 D*. To perform link prediction with a sum-GNN N, we use the (non-canonical) encoding and decoding from (Tena Cucala et al. 2021), which differs from the canonical encoding in that vertices can now also encode pairs of constants, so that both unary and binary facts can be represented in the channels of the input and output layers. This allows us to derive new binary facts (which the canonical transformation cannot do). As shown in Section 3.2 of (Tena Cucala et al. 2023), this non-canonical encoding can be expressed as the composition of a Datalog program Penc and the canonical encoding; similarly, the decoding can be seen as the composition of the canonical decoding followed by the application of a Datalog program Pdec. Hence, we can use these programs to lift any rule extraction results obtained for the canonical transformation TN to the end-to-end transformation Pdec (TN (Penc(D)))."}, {"title": "Partitioning the Channels of a GNN", "content": "In this section, we first provide two approaches for identifying channels in a sum-GNN that exhibit monotonic behaviour, which will later allow us to extract sound Datalog rules with head predicates corresponding to these channels. Candidate Datalog rules can be effectively checked for soundness using the approach developed by (Tena Cucala et al. 2023) for monotonic GNNs. Furthermore, we provide an approach for identifying channels for which no sound Datalog rule exists. Our techniques are data-independent and rely on direct analysis of the dependencies between feature vector components via the parameters of the model."}, {"title": "Safe Channels", "content": "In this section, we introduce the notion of a safe channel of a sum-GNN N. Intuitively, a channel is safe if, for any dataset D, the computation of its value for each vertex v \u2208enc(D) through the application of N to D is affected only by non-negative values in the weight matrices of the GNN.\nFor instance, consider a simple sum-GNN where matrix Ae for layer l is given below\nAe =\n1 0 1 4\n1 0 0 2\n\u22128 \u22121 0 \u22122\nand each of the matrices B for c\u2208 Col contain only zeroes. Furthermore, assume that layer l - 1 has four channels, where the third one has been identified as unsafe and all other channels have been identified as safe. Then, for an arbitrary vertex v, the product of Ae and ve-1 in the computation of ve reveals that the second channel in l is safe, because the unsafe component in ve-1 is multiplied by zero"}, {"title": null, "content": "(Ae[2, 3]), and the safe components are multiplied by non-negative matrix values. In contrast, the first channel of l is unsafe since its computation involves the product of an unsafe component of ve\u20131 with a non-zero matrix component (Ae[1, 3]), and the third channel is unsafe due to the product of a component of ve\u22121 with a negative matrix value (e.g. Ae[3, 1]). The following definition generalises this intuition.\nDefinition 2. Let N be a sum-GNN as in Equation (2). All channels i \u2208 {1, ..., \u03b4\u03bf} are safe at layer l = 0. Channel \u0456 \u2208 {1, ..., \u03b4\u03b5} is safe at layer l \u2208 {1, ..., L} if the i-th row of each matrix Ae and {(B)}c\u2208Col contains only non-negative values and, additionally, the j-th element in each such row is zero for each j \u2208 {1, ..., de\u22121} such that j is unsafe in layer l - 1. Otherwise, i is unsafe.\nWe can now show that safe channels in a GNN exhibit monotonic behaviour. In particular, the value of a safe channel may only increase (or stay the same) when new facts are added to the input dataset.\nLemma 3. Let N be a sum-GNN as in Equation (2), let D', D be datasets satisfying D' \u2286 D, let v \u2208 enc(D'), and let ve and v'e be the vectors associated to v in layer l upon applying N to D and D' respectively. If channel i in N is safe at layer l, then ve[i] \u2264 ve[i].\nProof. We proceed by induction on l. The base case l = 0 holds trivially by the definition of the canonical encoding and the fact that D'C D. For the inductive step, assume that the claim holds for l \u22121 > 0 and that channel i is safe at layer l. We show that ve[i] \u2264 ve[i]. Consider the computation of ve[i] and ve[i] as per Equation (3). The value (Aeve-1)[i] is the sum over all j \u2208 {1, ..., de-1} of Ae[i, j]ve-1[j]. By Definition 2, this sum involves only non-negative values and can be restricted to values of j corresponding to safe channels at layer l - 1. By induction, each such safe j satisfies v\u00e9_1[j] \u2264 ve\u22121[j] and hence Ae[i, j]ve_1[i] \u2264 Ae[i, j]ve-1[j]; thus, (Aeve_1)[i] \u2264 (Aeve-1)[i].\nConsider now c \u2208 Col and let E\u00ba and (EC)' be the c-coloured edges in enc(D) and enc(D') respectively. The sums involved in the products (Bc\u03a3(v,u)\u2208Ec ul\u22121)[i] and (Bc\u03a3(\u03c5,\u03ba)\u2208(Ec), ue_1)[i] contain only non-negative values and can similarly be restricted to safe channels at layer l-1. By induction, each such safe j satisfies that Vu \u2208 enc(D'), ue-1[j] \u2264 ue-1[j]. Furthermore, (EC)\u2032 \u2286 E given that D'C D. We conclude that\n(\u03a3\u0392c\u03a3ue\u22121) [i] \u2264 (\u03a3Bc\u03a3ue-1)[i].\nceCol (\u03c5,\u03ba)\u2208(\u0395)'\nceCol (v,u)\u2208Ec\nBy combining the previous inequalities and taking into account that \u03c3\u03b5 is monotonically increasing, we conclude that ve[i] \u2264 ve[i], as required.\nWe conclude this section by showing that the identification of safe channels allows for the extraction of sound rules from a trained sum-GNN model. In particular, given a candidate Datalog rule, it is possible to algorithmically verify its"}, {"title": null, "content": "soundness using the approach of (Tena Cucala et al. 2023). The soundness check for a ruler of the form (1) involves considering an arbitrary (but fixed) set containing as many constants as variables in r, and considering each substitution v mapping variables in r to these constants and satisfying the inequalities in r. For Dr, the dataset consisting of each fact Biv such that Bi is a body atom in r, we check whether the GNN predicts the fact Hv, corresponding to the grounding of the head atom. If this holds for each considered v, then the rule is sound; otherwise, the substitution v for which it does not hold provides a counter-example to soundness.\nProposition 4. Let N be a sum-GNN as in Equation (2), and let r be a rule of the form (1) where H mentions a unary predicate Up. Let S be an arbitrary set of as many constants as there are variables in r. Assume channel p in N is safe at layer L. Then r is sound for TN if and only if \u0397\u03bd \u2208 T(D) for each substitution v mapping the variables of r to constants in S and such that D |= Biv for each inequality Bi in r.\nProof. To prove the soundness of r, consider an arbitrary dataset D. We show that Tr(D) \u2286 TN(D). To this end, we consider an arbitrary fact in Tr(D) and show that it is also contained in TN(D). By the definition of the immediate consequence operator Tr, this fact is of the form \u0397\u03bc, where \u03bcis a substitution from the variables of r to constants in D satisfying D |= B\u2081\u03bc for each body literal B\u2081 of r. Let \u03c3 be an arbitrary one-to-one mapping from the co-domain of \u03bc to some subset of S; such a mapping exists because S has as least as many constants as variables in r. Let v be the composition of \u03bc and \u03c3.\nObserve that for each body inequality Bi of r, we have D |= Biv because D |= \u0392\u2081\u03bc and o is injective. Therefore, by the assumption of the proposition, \u0397\u03bd \u2208 Tw(D). Now, observe that the result of applying TN to a dataset does not depend on the identity of the constants, but only on the structure of the dataset; therefore, TN is invariant under one-to-one mappings of constants, and since o is one such map, \u0397\u03bd \u2208 T\u2116(D) implies \u0397\u03bc \u2208 \u03a4\u3126(D). Now, let a be the single constant in \u0397\u03bc. Since DC D by definition of \u03bc, and channel p is safe at layer L, we can apply Lemma 3 to conclude that v'\u2081[P] \u2264 VL [p], for v the vertex corresponding to a in enc(Dr), and v\u2081 and v\u2081 the feature vectors in layer L computed for v by N on Dre and D, respectively. But \u0397\u03bc\u2208 TN(D) implies that clst(v'\u2081[p]) = 1 and so v'\u2081[p] \u2265 t. Hence, V\u2081[p] \u2265 t, and so clst(VL[P]) = 1, which implies that \u0397\u03bc \u2208 T\u2081(D), as we wanted to show.\nIf on the other hand, if Hv \u2209 T\u2116(D) for some substitution v defined as in the proposition, then T\u2084(D) \u2288 TN(D), as \u0397\u03bd \u2208 Tr(D). Thus, r is unsound for TN.\nThe identification of safe channels provides a sufficient condition for monotonic behaviour that can be easily computed in practice and enables rule extraction. The fact that a channel is unsafe, however, does not imply that it behaves non-monotonically. In the following section, we provide a more involved analysis of the dependencies between channels and model parameters which yields a more fine-grained channel classification. In particular, we identify two"}, {"title": "Stable and Increasing Channels", "content": "In this section, we provide a classification of the channels of the GNN depending on their behaviour under updates involving the addition of new facts to an input dataset. Intuitively, stable channels are those whose value always remains unaffected by such updates; in turn, increasing channels cannot decrease in value, whereas decreasing channels cannot increase in value. All remaining channels are categorised as undetermined. Consider again a sum-GNN where matrix Ae for layer l is given below and each of the matrices B for c \u2208 Col contain only zeroes.\nAe =\n1\n0-1-2\n2\n1 -3 0\n1 0\n1\n2\n-30\n2 0\nFurthermore, assume again that layer l-1 has four channels, where the first has been identified as increasing, the second one as undetermined, the third one as decreasing, and the fourth one as stable. For an arbitrary vertex v, the product of Ae and ve-1 in the computation of ve reveals that the first channel of l is increasing since the undetermined component of ve-1 is multiplied by 0, the increasing component by a positive matrix value, and the decreasing component by a negative matrix value. The second channel is undetermined, since it involves the product of an undetermined component with a non-zero matrix value (Ae[1,1]). The third channel is also undetermined, since it is a mixture of increasing and decreasing values: positive-times-increasing is increasing, whereas positive-times-decreasing is decreasing, so it cannot be known a-priori whether the sum of these values will increase or decrease. Finally, the fourth channel of l is decreasing since the undetermined component of ve-1 is multiplied by 0, and increasing (resp. decreasing) components are multiplied by negative (resp. positive) matrix values.\nThe following definition formalises these intuitions and extends the analysis to the matrices B involved in neighbourhood aggregation.\nDefinition 5. Let N be a sum-GNN as in Equation (2). All channels are increasing at layer 0, A channel i \u2208 {1,..., de} is stable at layer l \u2208 {1, ..., L} if both of the following conditions hold for each j \u2208 {1, . . ., \u0431\u0435-1}:\n\u2022 B[i, j] = 0 for each c \u2208 Col, and\n\u2022 Ae[i, j] \u2260 0 implies that j is stable in layer l \u2013 1.\nIt is increasing (resp. decreasing) at layer l if it is not stable and, for each j \u2208 {1, . . ., de\u22121 }, these conditions hold:\n1. if j is increasing in l - 1, then Ae[i, j] \u2265 0 (resp. Ae[i, j] < 0);\n2. if j is decreasing in \u2113 \u2013 1, then Ae[i,j] < 0 (resp. Ae[i, j] > 0) and B\u00bf[i, j] = 0 for each c \u2208 Col;\n3. if j is undetermined in l \u2013 1, then Ae[i, j] = 0 and B[i, j] = 0 for each c \u2208 Col; and\n4. B[i, j] \u2265 0 (resp. B\u00bf[i, j] < 0) for each c \u2208 Col."}, {"title": null, "content": "It is undetermined at layer l if it is neither stable, increasing, nor decreasing.\nNote that a channel cannot be both increasing and decreasing, because satisfying the conditions for both classes would imply that it is stable, which is incompatible with being increasing or decreasing.\nThe following lemma shows that the behaviour of the channel types aligns with their intended interpretation.\nLemma 6. Let N be a sum-GNN of L layers, and let D', D be datasets satisfying D'\u2286 D. For each vertex v \u2208 enc(D'), layer l \u2208 {0,...,L}, and channel i \u2208 {1, ..., de}, the following hold:\n\u2022 If i is stable at layer l, then ve[i] = ve[i];\n\u2022 If i is increasing at layer l, then ve[i] \u2264 ve[i], and\n\u2022 If i is decreasing at layer l, then ve[i] \u2265 ve[i],\nwhere ve and v'e are the vectors induced for v in layer l by applying N to D and D' respectively.\nProof sketch. The full proof is given in Appendix A.2. We show the claim of the lemma by induction on l. The base case holds because v[i] < vo[i] for each i \u2208 {1,...,d} by definition of enc, and all channels are increasing in layer 0. For the inductive step, we prove that it holds at layer l for stable, increasing, and decreasing i. For stable i, notice that (Aeve\u22121)[i] is just a sum over channels j that are stable at l - 1, since the non-stable j's are zeroed out. The induction hypothesis then implies v'e_1[i] = ve\u22121[j] for each such j. Furthermore, B\u0119[i, j] = 0 for every j and c. Hence, ve[i] = ve[i].\nFor increasing i, consider the four possibilities for j at l - 1 and conditions (1) - (3) in the definitions. For example, if j is increasing, then from (1) we have Ae[i, j] > 0 and by our induction hypothesis, v\u00e9\u22121[j] \u2264 ve\u22121[j]. In any of these cases, we find that Ae[i, j]ve_1[i] \u2264 Ae[i, j]ve-1[j]. For the product involving B, if j is decreasing or undetermined at l - 1 then from (2) and (3) we have that B\u0119[i, j] = 0. Otherwise, by the inductive hypothesis we obtain ue_1[i] \u2264 ue\u22121[j] for every node u \u2208 enc(D'). Then since (E)' \u2286 E, the inequality is preserved when summing over neighbours of v. Also, since from (4) we have that B[i, j] \u2265 0, we find that for all j,\nBe[i, j](\u03a3ue-1) [j] \u2264 Be[i, j](\u03a3ue-1)[2].\n(\u03c5,\u03ba)\u2208(\u0395)'\nTherefore, by monotonicity of o, ve[i] \u2264 ve[i]. For decreasing i, the proof is very similar to that for increasing i. \u03a0\nBoth increasing and stable channels are amenable to rule extraction. In particular, the soundness check in Proposition 4 extends seamlessly to this new setting.\nProposition 7. Let N be a sum-GNN as in Equation (2), and let r be a rule of the form (1) where H mentions a unary predicate Up. Let S be an arbitrary set of as many constants as there are variables in r. Assume channel p in N is increasing or stable at layer L. Then r is sound for TN if and only if Hv \u2208 TN(Dr) for each substitution v mapping the variables of r to constants in S and such that D |= Biv for each inequality Bi in r."}, {"title": null, "content": "On the other hand, if a channel is decreasing or undetermined, it does not behave monotonically, and so it may or may not have sound rules. We conclude this section by relating the classes of channels described here to those of Section 3.1, with a full proof given in Appendix A.4.\nTheorem 8. Safe channels are increasing or stable. There exist increasing unsafe channels and stable unsafe channels."}, {"title": "Unbounded Channels", "content": "In Section 3.2, we noted that extracting sound Datalog rules for channels that are decreasing or undetermined might be possible. In this section, we identify a subset of such channels, which we refer to as unbounded, for which no sound Datalog rule may exist. Thus, being unbounded provides a sufficient condition for non-monotonic channel behaviour.\nThe techniques in this section require that the sum-GNN uses ReLU as the activation function in all but (possibly) the last layer L. Furthermore, we require the co-domain of the activation function in layer L to include a number strictly less than the threshold t of the classification function clst. This restriction is non-essential and simply excludes GNNs that derive all possible facts regardless of the input dataset, in which case all rules would be sound.\nDefinition 9. Channel p \u2208 {1,...,d} is unbounded at layer L if there exist a Boolean vector yo of dimension \u03b4\u03bf, and a sequence C1,..., CL of (not necessarily distinct) colours in Col, such that, with {y}-1 the sequence defined inductively as ye := ReLU(Bye-1) for each 1 < l < L \u2212 1, it holds that (BYL-1)[p] < 0.\nIntuitively, the value of an unbounded channel at layer L for a given vertex can always be made smaller than the classification threshold t by extending the input graph in a precise way. This helps us prove that each ruler with head predicate Up corresponding to an unbounded channel p is not sound for Ty. In particular, we first generate the dataset Dr, where v is a substitution that maps each variable in r to a different constant, and we let a := v(x). Then we extend De to a dataset D' in a way that ensures that the value of channel p at layer L for the vertex va in D' is smaller than the threshold t. Thus, Up(a) \u2209 TN(D'), even though Up(a) is clearly in Tr(D'), so dataset D' is a counterexample to the soundness of r.\nThe following theorem formally states this result. The full proof of the theorem is given in Appendix A.5.\nTheorem 10. Let N be a sum-GNN as in Equation (2), where \u03c3\u03b5 is ReLU for each 1 < l < L\u22121, and the co-domain of OL contains a number strictly less than the threshold t of the classification function clst. Then, each rule with head predicate Up corresponding to an unbounded channel p is unsound for N.\nProof sketch. Consider an unbounded channel p at layer L and a ruler with head Up(x). Let v be an arbitrary substitution mapping each variable in r to a different constant. Let G = enc(D) and let va be the vertex corresponding to constant a := v(x). Let yo and C1, . . ., CL be a Boolean vector and a sequence of colours, respectively, satisfying the"}, {"title": ""}]}