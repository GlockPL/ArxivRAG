{"title": "An Introduction to Cognidynamics*", "authors": ["Marco Gori"], "abstract": "This paper gives an introduction to Cognidynamics, that is to the dynamics of cognitive systems driven by optimal objectives imposed over time when they interact either with a defined virtual or with a real-world environment. The proposed theory is developed in the general framework of dynamic programming which leads to think of computational laws dictated by classic Hamiltonian equations. Those equations lead to the formulation of a neural propagation scheme in cognitive agents modeled by dynamic neural networks which exhibits locality in both space and time, thus contributing the longstanding debate on biological plausibility of learning algorithms like Backpropagation. We interpret the learning process in terms of energy exchange with the environment and show the crucial role of energy dissipation and its links with focus of attention mechanisms and conscious behavior.", "sections": [{"title": "Introduction", "content": "The introduction of focus of attention in the Transformer architecture [23] can likely be regarded as a paradigm-shift in Machine Learning. Interestingly, transformer-based architectures mostly reported superior results compared to recurrent neural networks, whose architecture may potentially be more adequate for sequential tasks. However, one should bear in mind that the limitation of capturing long-term dependencies by gradient-based learning algorithms were early pointed out about three decades ago [2, 17, 10]. A remarkable ingredient to face the problems of gradient vanishing is that of adopting the gating mechanisms proposed in the LSTM architecture (see e.g. [9] for an early evidence of the effectiveness of the proposal). LSTM architectures have been in fact the reference architecture for challenging experiments in the last decades and, especially, in conjunction with the explosion of Deep Learning.\nThis paper proposes a reformulation of learning which relies on full human-based protocols, where machines are expected to conquer cognitive skills from environmental interactions over time. We assume that, at each time instant, data acquired from the environment is processed with the purpose of updating the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, we assume to process on-line information just like animals and humans without neither store nor access to internal data collections. We formulate learning in the continuous setting of computation, which better reflects most natural processes\u00b9. This setting differs remarkably from the usual assumptions of learning on the basis of collections of separated sequences; basically, we do not rely on sequence segmentation, but process one single sequence which corresponds with the agent life.\nThis corresponds with promoting the role of time, which ordinarily indexes the environmental information. Learning is regarded as an optimization problem of a functional risk that arises from the environmental information over the agent life. We use the mathematical apparatus of Dynamic Programming and Optimal Control to approach the problem. We show that the distinctive spirit of learning springs out from the need to solve the Hamilton equations by Cauchy initial-ization, just like in most related problems of Theoretical Physics. Interestingly, the explicit dependence on time of the Hamiltonian, which reflects the interaction with the environment, leads to the problem of devising approximations of the optimal solution that can only be given by imposing boundary conditions\u00b2.\nThis paper shows that minimization of the functional risk with given boundary conditions can be approximated by using Cauchy conditions, which opens the doors to truly on-line computational schemes. We analyze the system dynamics behind learning in terms of energy exchange. The main result is that learning can only take place if we properly introduce appropriate focus of attention mechanisms, that turn out to play a crucial role in order to control the energy accumulation in the agent. The theory suggests that any dynamical model in the proposed optimization setting must involve additional learning parameters whose purpose is that of stabilizing Hamiltonian neural propagation and expose them gradually to the environment. This is a fundamental difference with respect to classic machine learning schemes, since this also corresponds with evidence from developmental psychology [12, 13]. The term Cognidynamics that is introduced in this paper refers to a theory for describing the neural propagation scheme emerging from the described optimization process that is required to take place on-line by fully matching the spirit of the classic citation by Danish theologian, philosopher, and poet S\u00f8ren Kierkegaard: \"Life can only be understood backwards; but it must be lived forwards.\" Interestingly, computers can also somewhat \"live backwards\" due to the accumulation and interaction with huge data collections, which is in fact one of the reasons of the recent spectacular results obtained by Large Language Models. However, a theory of learning for interpreting live or virtual live - going forward without collecting data is, in itself, a great scientific challenge. Moreover, it can open the doors to a truly"}, {"title": "Collectionless A\u0391\u0399", "content": "The big picture of Artificial Intelligence that emerges from by Russel and Norvig [19] is centered around a few classic topics, whose methodologies can, amongst others, be characterized by the noticeable difference that while \"symbolic AI\" is mostly collectionless, \"sub-symbolic AI\" is currently strongly relying on huge data collections. Interestingly, Machine Learning, Communicating, Perceiving, and Acting relies mostly on Statistical methodologies whose effectiveness has been dramatically improved in the last decade because of the access to huge data collections. This has been in fact likely the most important ingredient of the success of Machine Learning that has found a comfortable place under the umbrella of Statistics. As such, by and large, scientists have gradually become accustomed to taking for granted the fact that it is necessary to progressively accumulate increasingly large data collections. It is noteworthy that even symbolic approaches to AI are based on relevant collections of information, but in that case they are primarily knowledge bases and there is no data directly collected from the environment. When focusing on the difference of information that is stored, a question naturally arises concerning the possibility of exhibiting intelligent behavior only thanks to an appropriate internal representation of knowledge. Clearly, while the knowledge representation typically enjoys the elegance and compactness of logic formalism, the storage of patterns apparently leads to the inevitable direction of accumulating big data collections. However this is indeed very unlikely to happen in nature. Animals of all species organize environmental information for their own purposes without collecting the patterns that they acquire every day at every moment of their life. This leads to believe that there is room for collapsing to the common framework\u00b3 of \"Collectionless AI\" also for Machine Learning. The environmental interaction, including the information coming from humans plays a crucial role in the learning process, as well as the agent-by-agent communication. We think of agents that can be managed by edge computing devices, without necessarily having access to servers, cloud computing and, more generally, to the Internet. This requires thinking of new learning protocols where machines learn in lifelong manner and are expected to conquer cognitive skills in a truly human-like context that is characterized by environmental interactions, without storing the information acquired from the environment.\nThe former completely depends on data collections (clouds) that were possibly supervised beforehand, and where there is no direct/interactive connection orthogonal technological direction where the emphasis shifts back from cloud computing and huge data collections to thin personal computer and industrial devices that can create a society of intelligent agents which are fully using their computational resources instead of simply acting as lazy clients who are mostly involved simply in handling communication processes."}, {"title": "Why do we need a theory of Cognidynamics?", "content": "Beginning from the excellent models for the behavior of single neurons [11], the dynamical system hypothesis in neuroscience and cognition has been massively investigated over the past decades. As early as at the beginning of the nineties, Anderson, Pellionisz, and Rosenfeld [1] edited a seminal book where, amongst others, an important part was devoted to \"Computation and Neurobiology.\""}, {"title": "Formulation of Lifelong Learning", "content": "In this section we propose a formulation of Lifelong Learning where the classic notion of time plays the role of protagonist. We introduce a class of intelligent agents that we call NARNIAN: NAtuRe-iNspIred computational Intelligence agents. In particular we consider the continuous interpretation of time mostly used in Physics, though an associated discrete-time setting can replace the analysis carried out in this paper without significant differences. The choice of the continuous setting of computation comes from the mentioned objective of exploring laws of cognition following the spirit that drives other temporal laws in Science. As mentioned in Section 3, once the temporal environment is given, one can think of intelligent agents as dynamical systems that are expected to exhibit a high degree of adaptation capabilities. Moreover, just like any living organism, in addition to continuously react to the environment, intelligent agents in temporal environments can likely benefit from inheriting the capacity of reproduction, growth and development, and energy utilization.\nSURVIVAL AND REPRODUCTION\nSurvival is a slippery topic, but one could easily think of intelligent agents which are either alive or dead. Reproduction is the mechanism for conquering the state of alive; it is in fact somewhat connected to the massively experimented transfer learning schemes in artificial neural networks. Intelligent agents can follow classic scheme of evolutionary computation which nicely fits in this temporal environment framework4.\nGROWTH AND DEVELOPMENT\nA fundamental difference from current machine learning models based on access to large collections of data immediately emerges in any lifelong learning scheme that is based on a temporal environment, as normally occurs in nature. Every living organism experiences processes of gradual cognitive growth which in some evolved species, especially humans, also presents rather clearly defined development phases [12]. To some extent, it is as if nature intervenes in a sort of protection against excessive exposure to information. For example, the vision process in newborns undergoes a developmental phase in which visual scenes are strongly blurred, and it takes nearly one year to achieve adult's visual acuity. Are those developmental schemes connected to the specificity of human biology or do they come from more general information-based principles? This paper provides results to sustain the position that similar developmental schemes are characterized by the appropriate adaptation of specific neural connections that prevent information overloading. This is claimed come from informatio-based principles that are independent of specific biological species.\nENERGY UTILIZATION\nAs it will be shown in section 7, the process of learning can be interpreted as energy dissipation in the dynamical systems which is acting as an intelligent agent. Basically, it will be shown that the agent receives environmental energy which is used to change the internal energy by dissipation. Clearly, the agent must prevent from accumulating too much internal energy during learning, which is in fact an important driving heuristics for the system dynamics. The end of any learning process corresponds with ending the dissipation process which suggests in any case to carry out policies for bounding the internal and the dissipation energy. Interestingly, just like for sustaining developmental schemes, any intelligent agent implements policies aimed at controlling the energy exchange with the environment by the appropriate adaptation of specific neural connections.\nNEURAL NETWORK ARCHITECTURE AND FORMULATION OF LEARNING"}, {"title": "Hamiltonian spatiotemporal locality", "content": "We begin considering the optimal problem defined by Eq. (4), for which we can offer the classic solution from the theory of Optimal Control (see Section A in the Appendix). The classic framework of the theory of optimal control suggests to express compactly the pair Neural Network - Lagrangian by the corresponding\nobeys the ODE\n$\\begin{aligned} &\\dot{\\xi}_i(t) = u_i(t) \\\\ &\\dot{a}_i(t) = \\sigma\\left( \\sum_{j\\in V} W_{ij}(t) \\xi_j(t) \\right) \\\\ &\\dot{W}_{ij}(t) = V_{ij}(t) V_{ij}(t) \\\\ &\\dot{W}_{ij}(t) = \\Xi_{ij} (t) \\Xi_{ij} (t) \\\\ &\\dot{\\xi}_i(t) = a_i(t) \\left[ - \\xi_i(t) + \\sigma\\left(\\dot{a}_i(t)\\right) \\right] \\end{aligned}$\n$\\begin{aligned} &i \\in \\mathcal{I} \\\\ &i \\in \\mathcal{V} \\\\ &(i,j) \\in \\mathcal{A} \\\\ &(i,j) \\in \\mathcal{A} \\\\ &i \\in \\mathcal{V} \\end{aligned}$         (1)\nwhich holds on the horizon (0,T). Here T > 0 can also be infinites. We denote by $x := (\\xi, \\text{flatten}(w))$ the overall state which contains the neural activation $\\xi$ and the weights associated with $A$, while $\\sigma(\\cdot)$ is a sigmoidal function (e.g. $\\sigma(\\cdot) = \\tanh(\\cdot)$). The system dynamics of the weights $w_{ij}$ is driven by the corresponding velocity $v_{ij}$, which is properly filtered by $\\Xi_{ij}$ and $w_{ij}$, respectively. At any $t$, the value $a_i(t) = \\tau_i^{-1}(t) \\ge 0$ can be regarded as inverse of time constants $\\tau_i(t)$ and somewhat characterizes the velocity of performing the associated computation of the activation $a_i(t)$.\nRemark 1. Network Gating Functions\n$a, \\Xi, \\text{ and } w$ are gating functions. Notice that $a$ plays a crucial role for the char- acterization of the system dynamics. When $a_i(t) = 0$ the neuron is inhibited, whereas it is activated whenever $a_i(t) > 0$. It is worth mentioning that such a fundamental difference in the state (activated/inhibited) has been systematically observed in biological neurons. The role of $\\Xi$ and $w$ is that of providing an ap- propriate gating mechanism on the weights. As it will be shown in remainder of the paper, they carry out a different type of system dynamics that nicely matches the need of gradual learning and energy control, respectively. We can easily see that $\\Xi$ can perform time warping, whereas $w$ can carry out pruning strategies.\nThe dynamical system defined by Eq. 1, is characterized by the state $x \\sim (\\xi,w)$ and by $v$, which acts as the velocity of the weights. This is in fact the function that will be used for driving the learning process, since it defines the evolution of the weights $w$. Notice that, just like $w$, the network gating functions $\\zeta \\sim (a, \\psi, \\omega)$ can also be thought of learning functions. This is in fact a fundamental architectural difference with respect to traditional recurrent neural network models. We will see that an associated learning process takes place concerning the weights $\\zeta$.\nThe interaction with the environment of the recurrent neural network is expected to minimize the following functional risk\n$\\mathcal{R}(v, T) = \\int_0^T \\left( \\sum_{i\\in V} \\sum_{j\\in V} m_{ij}(s) v_{ij}(s) + \\sum_{i \\in O} \\Phi_i(s) \\mathcal{V}(f_i(s), s) \\right) ds$                (2)\nHere $\\mathcal{L}(x, v, s) = \\frac{1}{2} \\sum_{i\\in V} \\sum_{j\\in V} m_{ij}(s)v_{ij}(s) + \\sum_{i\\in O} \\Phi_i(s) \\mathcal{V}(f_i(s), s)$ is the Lagrangian of the paired system (1). The Lagrangian is characterized by:"}, {"title": "Co-state heuristics for learning (", "content": "Here we discuss some fundamental necessary conditions that we need to guarantee to allow an intelligent agent to learn. Basically, we need to upper bound both the state and co-state dynamics, which is a well-known problem in the optimal control theory.\nBoundedness of the state i\nWe begin stating a proposition on the BIBO stability of the recurrent neural network model described by ODE 1 which comes from a classic result of System Theory stated in the following Lemma.\nLemma 3. Let is consider the ODE\n$\\dot{x}(t) + a(t)x(t) = u(t).$\n(18)\nThen the solution can be expressed as\n$x(t) = x(0) \\exp \\left( -\\int_0^t a(\\tau) d\\tau \\right) + \\int_0^t \\exp \\left( - \\int_s^t a(\\tau) d\\tau \\right) u(s) ds $\n(19)\nProof. Let us define the integrating factor\n$I(s) := \\exp \\left( \\int_0^s a(\\tau) d\\tau \\right)$.\nIf we multiply both sides of ODE (18) we get\n$I(s)\\dot{x}(s) + a(s)I(s)x(s) = I(s)u(s)$.\nNow we have $D(I(s)x(s)) = \\dot{x}(s)\\dot{I}(s) + I(s)\\ddot{x}(s)$ and then we get\n$D(I(s)x(s)) - \\dot{x}(s)\\dot{I}(s) + a(s)I(s)x(s) = I(s)u(s)$.\nNow we have $\\dot{I}(s) = a(s)I(s)$ and, therefore, if we integrate over [0,t] we have\n$I(t)x(t) - I(0)x(0) = \\int_0^t I(s)u(s)ds$\nand, finally,\n$\\begin{aligned} x(t) = x(0)I^{-1}(t) + \\int_0^t I^{-1}(t) I(s) u(s) ds \\\\ = x(0) \\exp \\left( - \\int_0^t a(\\tau) d\\tau \\right) + \\int_0^t I^{-1}(t) I(s) u(s) ds \\\\ = x(0) \\exp \\left( - \\int_0^t a(\\tau) d\\tau \\right) + \\int_0^t \\exp \\left( - \\int_s^t a(\\tau) d\\tau + \\int_0^s a(\\tau) d\\tau \\right) u(s) ds \\\\ = x(0) \\exp \\left( - \\int_0^t a(\\tau) d\\tau \\right) + \\int_0^t \\exp \\left( - \\int_s^t a(\\tau) d\\tau \\right) u(s) ds \\end{aligned}$\nWe can now promptly use this lemma for the BIBO stability of the recurrent neural network.\nProposition 3. BIBO stability of the state i\nLet us assume that $\\forall t \\in [0, T], \\forall i \\in \\mathcal{V}: a_i(t) > a > 0$. Then the recurrent neural network described by ODE (1) is BIBO stable.\nProof. Let $\\mu_i(t) = \\sigma(\\sum_j w_{ij}(t) x_j(t))$ be. Then, from the boundedness of $w$ we can always find $B_{\\xi} \\in \\mathbb{R^+}$ such that $|\\mu_i(t)| < B_{\\xi}$. As a consequence we have\n$\\begin{aligned} |\\xi_i(t)| = |\\xi_i(0) \\exp \\left( - \\int_0^t a_i(\\tau) d\\tau \\right) + \\int_0^t \\exp \\left( - \\int_s^t a_i(\\tau) d\\tau \\right) \\mu_i(s) ds | \\\\ \\le |\\xi_i(0)| + \\int_0^t e^{-a(t-s)} |\\mu_i(s)| ds \\\\ = |\\xi_i(0)| + \\frac{B_{\\xi}}{a} (1 - e^{-at}) < |\\xi_i(0)| + \\frac{B_{\\xi}}{a}. \\end{aligned}$\n(20)\nNotice that the strict inequality condition $a_i(t) > a > 0$ is crucially used in the proof. For example, in the extreme case in which $a(t) = 0$ we have $|\\xi_i(t)| = |\\int_0^t \\mu_i(s) ds|$ that is not necessarily bounded in the case of bounded input. Interestingly, the specific dynamical structure of the system dynamics of $\\xi_i$ does allow to end up into the BIBO property also in case in which the related function $a_i(t) = 0$. In that case, from equation 1 we straightforwardly conclude that $\\dot{\\xi}_i = \\xi_i(0)$. This is in fact very important since the learning policies on the dissipation parameters only involves to force $a_i(t) = 0$. As already mentioned, this nicely matches neurobiological evidence on the presence of inhibited neurons.\nCo-state boundedness and \u03da heuristics\nIn addition to bounding the state, clearly the system dynamics imposes also to bound the co-state. This also corresponds with the need of bounding the energy exchanged with the environment that, as it will be shown in the remainder of the paper, also involves the expression of the Hamiltonian.\nANALYSIS ON Pi\nWe are interested in analysing dynamical modes for which $d/dt((1/2)p_i^2) < 0$. From eq. 10 we get\n$\\dot{p}_i(t)p_i(t) = - s_i(t) \\Phi_i(t) \\mathcal{V}_{\\xi_i} (\\xi(t), t)p_i(t) + s_i(t) a_i(t) p_i^2(t) - s_i(t) p_i(t) \\sum_{K\\in ch[i]} \\left[ a_K(t) \\sigma' (a_K(t)) W_{Ki}(t) p_K(t) \\right] \\cdot \\left( \\sum_{j\\in par[K]} W_{Kj} (t) W_{Kj} (t) \\xi_j (t) \\right) W_{Ki}(t);$\nWe can promptly end up into the following proposition:"}, {"title": "Energy balance", "content": "Now we carry out a classic analysis on the energy balance coming from the interaction of the agent with the environment. We begin considering the contribution $H_t$. In case $s_i(t) = 1$ we have $H_{\\xi} \\cdot \\dot{\\xi} + H_p \\cdot \\dot{p} = 0$, which leads to $\\dot{H} = (d/dt)H = H_t$.\nDefinition 2. The terms\n$\\mathcal{E} := \\int_0^t \\Phi_i(\\tau) \\mathcal{V}_{\\xi_i} (\\xi(\\tau), t)d\\tau$\n$\\mathcal{D} := D_a + D_B + D_\\alpha + D_\\omega$\n(23)\nare referred to as the environmental energy and the dissipated energy, respectively, where\n$\\begin{aligned} &D_e := -\\int_0^t \\Phi_i(\\tau) \\mathcal{V}(\\xi_i(\\tau), t)d\\tau \\\\ &D_\\beta := -\\frac{1}{2} \\int_0^t \\sum_{i,j} B_{ij}(\\tau) p_{ij}^2(\\tau) d\\tau \\\\ &D_\\alpha := -\\int_0^t \\sum_i a_i(\\tau) p_i(\\tau) \\left[ - \\xi_i(\\tau) + \\sigma(a_i(\\tau)) \\right] d\\tau \\\\ &D_\\omega := -\\int_0^t \\sum_i a_i(\\tau) p_i(\\tau) \\sigma' (a_i(\\tau)) \\sum_j w_{ij} (\\tau) w_{ij} (\\tau) \\xi_j (\\tau) dt \\end{aligned}$\n(24)\nThe dissipation energy arises because of the temporal changes of $a_i, \\Xi_{ij}, \\tau_i, \\Phi_i$, even though the role of $\\Xi_{ij}$ is replaced with $B_{ij}$.\nTheorem 4. - I Principle of Cognidynamics\nThe system dynamics evolves under the energy balance\n$\\mathcal{E} = \\Delta H + \\mathcal{D}$\n(25)\nProof.\n$\\begin{aligned} H_{a=T} &= \\partial_s \\left( -\\frac{1}{2} \\sum_{i,j} \\beta_{ij} (s) p_{ij} (s) + \\sum_{i\\in O} \\Phi_i(s) \\mathcal{V}(\\xi(s), s) \\right. \\\\ &+ \\sum_{i\\in V} a_i(s) p_i(s) \\left[ - \\xi_i(s) + \\sigma\\left(\\sum_j w_{ij} (s) w_{ij} (s) \\xi_j (s) \\right) \\right] \\Bigg|_{s=T} \\\\ &= -\\sum \\beta_{ij} (\\tau) \\dot{p}_{ij} (\\tau) + \\sum_{i\\in O} \\Phi_i(\\tau) \\mathcal{V}_{\\xi} (\\xi(\\tau), \\tau) + \\Phi_i(\\tau) \\mathcal{V}(\\xi(\\tau), \\tau) \\\\ &+ \\sum_{i\\in V} a_i(\\tau) p_i(\\tau) \\left[ - \\dot{\\xi}_i(\\tau) + \\sigma\\left(\\sum_j w_{ij} (\\tau) w_{ij} (\\tau) \\dot{\\xi}_j (\\tau) \\right) \\right] \\end{aligned}$"}, {"title": "Gravitational neural networks", "content": "A fundamental problem that plagues neural network-based approaches to life-long learning is that when attacking new tasks they typically offer no guarantees against catastrophically adapting learned weights that were already used for successfully solving previously learned tasks. While the proposed Hamiltonian learning scheme offers a truly new scheme of learning for recurrent neural networks, in principle, it shares this shortcoming with related gradient-based methods. The problem seems to have an architectural origin that certainly remains in dynamic neural networks. Furthermore, it is worth mentioning that the natural dynamic behavior suitable for the interpretation of cognitive processes is to continuously generate trajectories in the phase space. In other words, while some neurons can be deactivated, convergence to fixed points is not biologically plausible. It is very interesting to note that these needs are satisfied simply by an appropriate dynamic structure of the system which requires sustaining of trajectories through the presence of conservative processes.\n$\\begin{aligned} &\\ddot{y}_{i\\kappa}(t) = \\dot{y}_{i\\kappa}(t); \\\\ &\\ddot{y}_{i\\kappa}(t) = -(d-2)G \\sum_{h\\in ne[i]} \\frac{y_{h\\kappa} - y_{i\\kappa}}{\\left[ \\sum_{\\kappa=1}^d (y_{h\\kappa} - y_{i\\kappa})^2 \\right]^{d/2}} \\sigma(\\xi_h); \\end{aligned}$        (26)\n$Y_{ik}(w,t) = e^{-iwt} Y_{ik} (t)$\nHere the state becomes $(\\xi_i, w_{ij}, Y_{ik})$. The memory allocation is compactly expressed by $[Y_{ik}] \\leftarrow \\xi_i$. Finally, a frequency-based response is well-suited to report decision-based information."}, {"title": "Conclusions", "content": "This paper focuses on the interpretation of natural learning in the optimization framework of dynamic programming which gives rise to Hamiltonian-Jacobi-Bellman equations. The paper promotes a collectionless approach to Machine Learning that strongly parallels what happens in nature and uses basic ideas that are massively adopted in Theoretical Physics. The system dynamics which drives the learning process is in fact dictated by Hamiltonian ODE, which turns out to parallel classic gradient descent methods in Statistical Machine Learning.\nThe most important contribution of the paper is that of addressing the longstanding questions on the stability of learning in recurrent neural networks, and to show that the answer comes from the introduction of an appropriate law which drives the control of dissipative weights. The proposed mathematical framework offers the appropriate tools for understanding the exchange of energy between the agent and the environment and suggests that the process of dissipation decreases the entropy of the system, thus creating ordered structures. In particularly, it becomes clear that we need an appropriate developmental scheme which requires filtering the inputs coming from the environment for offering a well-posed formulation of learning.\nInterestingly, the adoption of the proposal Hamiltonian learning approach leads to also to a computational scheme that, unlike BPTT and RTRL, is local in both time and space.\nThe proposed theory makes use of the continuous setting of computation to interpret learning as the discovery of a stationary point of the cognitive action, which tightly parallels the interpretation of Newtonian laws in Mechanics. This facilitates the development of the main results of the paper and offers the substrate for investigating links with Developmental Psychology and Neuroscience. However, we are mostly planning to work towards the translation in the discrete setting of computation of the proposed learning approach, which is in fact quite straightforward. It can open the doors to any application of Machine Learning involving time, where the emphasis is moved to the collectionless approach joined with the central role of focus of attention."}, {"title": "HJB equations", "content": "Let us consider of the Value Function V : [0,T] \u00d7 X \u2192 R : (t,\u00a7) \u2192 V(t, x)\n$V(t, x) := \\mathcal{J}_T + \\min_{w} \\int_t^T ds \\mathcal{L}(\\xi(s), w(s), s).$                  (27)\nHere, \u00a7(s) is the trajectory in [t, T] driven by\n$\\dot{\\xi}(s) = f(\\xi(s), w(s), s)$                      (28)\nwhich begins with \u00a7(t) = x. Function V is sometimes also referred to as the cost-to-go. Here $\\mathcal{J}_T \\ge 0$ is the final value that might be regarded as\n$\\mathcal{J}_T = \\int_t^T ds \\mathcal{L}(\\xi(s), w(s), s).$\nBasically, the introduction of $\\mathcal{J}_T$ leads to consider the special case in which T = \u221e when there exists the integral $V(t, x) := \\min_w \\int_t^T ds \\mathcal{L}(\\xi(s), w(s), s)$. We begin with a couple of premises on $(f, \\mathcal{L})$ that are very important in the following.\nFunction f and $\\mathcal{L}$ are supposed to be continuous and differentiable with respect to x,w whereas we make no assumption on the continuity with respect to t.\nThe optimum is determined by using Bellman's principle. We consider the general case in which \u0192 and $\\mathcal{L}$ posses analytical regularities only with respect to x, that is we assume that $f(x, w,t)$ and $\\mathcal{L}(x, w,t)$ admit continuous partial derivates with respect to x only. In particular we assume that w and t only posses a finite number of discontinuities and that $f(x, w, t), \\mathcal{L}(x, w, t)$ are always bounded in [0,T]. Under this assumption we can always grid [0,T] in such a way that the mentioned discontinuities correspond with nodes in the grid.\nGiven At > 0, we want to see the relationship between the value function at t and at t + At, where the optimal value on the trajectory x* is correspondently moved to x* + \u2206x*. We have\n$V(t, x^*) = \\min_{w([t,T])} \\left( V(t + \\Delta t, x + \\Delta x) + \\int_{t + \\Delta t}^T ds \\mathcal{L}(x(s), w(s), s) \\right)$"}, {"title": "Method of characteristics", "content": "The HJB approach to optimization assumes that one knows the boundary con- ditions at the end-point of the interval. Unfortunately, in that form, they are neither useful for conception nor for the understanding of learning schemes Now we will shown that classic Hamiltonian dynamics that satisfies the HJB equations for time-independent Hamiltonians also works for the general case of time-variant Hamiltonians.\nHAMILTONIAN DYNAMICS IS SUFFICIENT\nLet us consider the following (HJ) initial-point problem\n$\\begin{array}{ll} (HJ) & V_t(t, x) + H(x, V_x(t, x, t)) = 0. \\\\ & V(0,x) = g(x). \\end{array}$                                  (36)\nWe want to convert this PDE problem into an ODE that can open a dramatically different computational perspective. We use the method of characteristic. Now, let us introduce the co-state p as $p := V_x$ and consider the total derivative\u2079 of its coordinate\n$\\dot{p}_k(t) = V_{xxt}(t, x(t)) + V_{xxx_i} \\dot{x}_i$.                                          (37)"}, {"title": "Hamilton equations and Lagrangian multipliers", "content": "A possible way to attack optimization under constraints is to use the Lagrangian approach and find the stationary points of\n$\\mathcal{J"}, "L = \\mathcal{J}_T + \\int_0^T dt \\left( \\mathcal{L}(x(t), w(t),t) + \\lambda(t) \\cdot (f(x(t), w(t), t)) - \\dot{x}(t)) \\right)$ (42)\nWe introduce the Hamiltonian on the optimal trajectory by setting\n$H(x(t), \\lambda(t), w(t),t) := \\mathcal{L}(x(t), w(t), t) + \\lambda(t) \\cdot f(x(t), w(t), t),$\nin such a way to re-write $\\mathcal{J}_L$ as\n$\\mathcal{J}_L(x, \\lambda) = \\mathcal{J}_T + \\int_0^T dt (H(x(t), \\lambda(t), w(t),t) - \\lambda(t) \\cdot \\dot{x}(t))$            (43)\nNow, in order to determine a stationary solution of $\\mathcal{J}_L$, if we use by part inte- gration, we can promptly see that we can replace $\\lambda(t) \\cdot \\dot{x}(t)$ with $-\\dot{\\lambda}(t) \\cdot x(t)$. We have\n$\\int_0^T dt \\lambda(t) \\cdot \\dot{x}(t) = [\\lambda(t) \\cdot x(t)"]}