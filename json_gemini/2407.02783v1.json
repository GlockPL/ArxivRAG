{"title": "52B to 1T: Lessons Learned via Tele-FLM Series", "authors": ["Xiang Li", "Yiqun Yao", "Xin Jiang", "Xuezhi Fang", "Chao Wang", "Xinzhang Liu", "Zihan Wang", "Yu Zhao", "Xin Wang", "Yuyao Huang", "Shuangyong Song", "Yongxiang Li", "Zheng Zhang", "Bo Zhao", "Aixin Sun", "Yequan Wang", "Zhongjiang He", "Zhongyuan Wang", "Xuelong Li", "Tiejun Huang"], "abstract": "Large Language Models (LLMs) represent a significant stride toward Artificial\nGeneral Intelligence. As scaling laws underscore the potential of increasing model\nsizes, the academic community has intensified its investigations into LLMs with\ncapacities exceeding 50 billion parameters. This technical report builds on our\nprior work with Tele-FLM (also known as FLM-2), a publicly available 52-billion-\nparameter model. We delve into two primary areas: we first discuss our observa-\ntion of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which supports the \u201cless\nis more\" approach for SFT data construction; second, we demonstrate our experi-\nments and analyses on the best practices for progressively growing a model from\n52 billion to 102 billion, and subsequently to 1 trillion parameters. We will open-\nsource a 1T model checkpoint, namely Tele-FLM-1T, to advance further training\nand research.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable general capabilities\n. Research on scaling laws indicates that metrics related to perplexity (e.g., loss and\nBPB) improve as training FLOPs increase. Given that high-quality data may be limited due to factors\nsuch as copyright constraints and the proliferation of LLM-generated content on the web, there is\na growing interest within the community in scaling up model sizes. Recent iterations of popular\nLLM series, such as Mistral at 141B [9], DeepSeek at 236B [3], Grok at 314B [1], and Llama-3\nexceeding 400B parameters [2], underscore a trend toward models with 1 trillion parameters. To\nbenefit the explorations on extremely large language models, we trained a series of models, namely\nTele-FLM (a.k.a. FLM-2), in which we first train a 52B model, and grow it to 1T parameters, with\nan intermediate stage of 102B. We outline techniques for efficiently and robustly training the 52B\nmodel in [13]. As a consequent work, we focus on two prominent areas of research with the Tele-\nFLM models: alignment with human and progressive learning.\nTo align with humans, we focus on supervised fine-tuning for instruct-following tasks, while defer-\nring exploration of reward-based methods to future work. We explore different data combination and\ntraining settings, finding that leveraging the existing knowledge and capabilities of the foundation\nmodel with a limited dataset of instruction-focused tasks yields better results than merely increasing\nthe volume of instruction data , even when the instructional responses are of high quality. This\nis consistent with prevailing views that highlights the importance of building a strong foundation"}, {"title": "Tele-FLM-Chat", "content": "model. One of our best-performing instruct models, namely Tele-FLM-Chat, is demonstrated at\nhttps://www.modelscope.cn/studios/FLM/ChatFLM/summary.\nFurther, our exploration into progressive learning facilitated the development of a 1T model from\nthe initial 52B checkpoint. The central strategy involves expanding the model's structure during\nthe pre-training phase and utilizing function-preserving growth techniques to transfer\nknowledge seamlessly from one stage to the next. Guided by empirical results from smaller models,\nwe expanded the 52B model to 102B and ultimately to 1T parameters, establishing an efficient\ntraining protocol for extremely large language models without encountering post-growth divergence.\nWe plan to release the weights of our final model, Tele-FLM-1T, to support ongoing research and\nfacilitate further model training."}, {"title": "Supervised Fine-tuning", "content": "Data. We focus on the task of Chinese instruct following. For instruct data, we collect 1 million\nopen-sourced instructs as our full corpus, and sample different subsets from it to study the influence\nof various domains on model performance. The quality of responses is crucial, so we do extensive\nwork to improve data quality.\nAlthough the SFT data we curated covers diverge topics and intentions, and is equipped with im-\nproved responses, we observe detrimental impacts by fine-tuning Tele-FLM-52B with the entire\ndataset. Our investigations spanned multiple domain combinations and ratios, including chatting,\nmathematics, coding, reasoning, data processing, language understanding, brainstorming, and gen-\neration. Among them, our best results come from using a subset of 30k samples, including: (1) 25k\nsamples that are categorized as \u201cmaths\u201d by external clustering methods, which includes textual ques-\ntions on mathematical problems (mainly in elementary and junior school level), along with general\nquestions about mathematical concepts, and (2) 5k samples containing coding problems and multi-\nturn dialogues. We first sample a larger set from these domains and calculate the perplexity of the\nresponses using our Tele-FLM-52B base model [13]. We leverage 50% of the samples exhibiting\nthe lowest perplexity for SFT.\nTraining Settings. We fine-tune Tele-FLM-52B for 4 epochs with a global batch size of 128. We\nset the learning rate to 2.7e-5, which equals to the end of the pre-training stage. The learning rate is\ndecayed to le-9 with a linear schedule. The best result is achieved with the checkpoint corresponding\nto the end of the second epoch."}, {"title": "Evaluation", "content": "We evaluation Tele-FLM-Chat with AlignBench [15], a public alignment evaluation benchmark, as\nwell as TeleEval, an internal evaluation benchmark with a similar organization and mechanism."}, {"title": "AlignBench", "content": "We evaluate the alignment performance of Tele-FLM-Chat in Chinese across various domains utiliz-\ning AlignBench [15]. AlignBench is a comprehensive and multidimensional evaluation benchmark\ndesigned to assess Chinese large language models' alignment performance. It encompasses 8 cat-\negories with a total of 683 question-answer pairs, covering areas such as fundamental language\nability (Fund.), Chinese advanced understanding (Chi.), open-ended questions (Open.), writing abil-\nity (Writ.), logical reasoning (Logi.), mathematics (Math.), task-oriented role playing (Role.), and\nprofessional knowledge (Pro.). This benchmark furnishes questions, model responses, scoring cri-\nteria, and reference answers for model assessment, with GPT-4 and CritiqueLLM serving as\njudge models. These judge models provide scores and scoring rationales based on the provided\ncriteria.\nResults on AlignBench are illustrated in Table 1. With a 52B base model and 30k SFT samples,\nTele-FLM-Chat reaches 91% of GPT-4's performance, and 82% of the more advanced GPT-4-1106-\npreview. Notably, Tele-FLM-Chat is comparable (97%) or even outperforms (107%) the GPT-4\nseries on Chinese language understanding and generation tasks. However, there remains a significant"}, {"title": "TeleEval", "content": "Drawing on experience from various established evaluation datasets, we develop our own evaluation\ndataset, namely TeleEval, which places a greater emphasis on aspects such as mathematics, security,\nand anti-hallucination. TeleEval includes a subset of 2500 single-turn instruct-following tests, which\nare categorized into 7 domains: daily chat and question answering (Chat), professional question\nanswering (Pro.), translation (Trans.), logical thinking (Logic), long article generation (Writing),\ntruthfulness and anti-hallucination (Truth.), and security test (Safety). These test sets are distinct\nfrom, or only marginally similar to, existing benchmarks.\nResults on TeleEval are presented in Table 2. Consistent with findings from AlignBench, our re-\nsults demonstrate that a small amount of maths, code, and multi-turn dialog instructions surprisingly\nyields a decent SFT model on almost all domains. However, maths and reasoning tasks remain as ex-\nception. Notably, on TeleEval, Tele-FLM-Chat reaches 93% of GPT-4-1106-preview's performance."}, {"title": "Tele-FLM-1T", "content": "Similar to our previous work of FLM-101B [12], the training of Tele-FLM-1T utilizes a staged\ngrowth strategy. This technique is essential for training large-scaled models with extremely limited\ncomputational budgets."}, {"title": "Model Architecture", "content": "Based on growth technology, the Tele-FLM-1T model training is divided into three stages by pa-\nrameter size: 52B, 102B, and 1TB. Tele-FLM-52B represents the intermediate result of the\n52B stage. Each stage of the model uses the same backbone structure. Tele-FLM models utilize the\nstandard GPT-style decoder-only transformer architecture with pre-normalization and an added\nLayerNorm to the output of the last layer. RMSNorm is used for normalization, and SwiGLU\nfor the activation function. Rotary Positional Embedding (RoPE) is employed. The em-\nbedding layer is untied from the language modeling head. Linear bias disabled in the attention and\nall MLP modules."}, {"title": "Growth Strategies", "content": "In the Tele-FLM-1T training protocol, we implement aggressive growth with an enhanced growth\nstrategy originating from our previous work MSG [26], a methodology that achieves strict function-\npreserving growth. Building on this, we further refine the depth expansion technique and explore\nhyperparameter tuning methods applicable to the MSG scheme.\nWidth Growth. Under the MSG framework [26], width growth refers to increasing the hyperpa-\nrameters regarding hidden_dim, head_num, and ffn_dim. In this work, kv_channels (dimension of\nthe projections in MHA) is maintained at 128, establishing a relationship where head_num =\nhidden_dim/kv_channels. Therefore, we only utilize the growth of hidden_dim and ffn_dim.\nTypically, ffn_dim and hidden_dim have a default ratio. In a traditional Transformer, ffn_dim equals\nto hidden_dim \u00d7 4, while with SwiGLU, ffn_dim equals to hidden_dim\u00d78/3. However, our previous\nexperiments indicate that this default ratio is not always optimal. Therefore, we search for the\noptimal ffn_dim values at each growth stage.\nThe main idea of MSG is to use external masks to neutralize the effects of new structures on the\nmodel's function. Initially, with the mask set to 0, the function remains strictly preserved. Over\ntime, we gradually increase the mask to integrate the influence of new structures, ultimately reach-\ning the target structure with the mask set to 1. This integration is governed by a linear mapping\nfunction between the training steps and the mask values, utilizing a hyperparameter named as the\ngrowth_transition_step, which equals to the total steps through which the mask vanishes.\nDepth Growth. Depth growth refers to increasing layer_num. For each new layer, we initiate the\nparameters by duplicating those of an adjacent layer. During training, we implement a mask mecha-\nnism similar to Width Growth. When the mask is set to 0, the computation flow remains unchanged\nfrom its state prior to expansion. A similar decay scheduler based on the growth_transition_step\nhyperparameter ensures a smooth and controlled growth transition.\nOur observations indicate that copying different layers significantly affects the convergence of the\npost-growth model. Therefore, we propose a layer selection method based on the input-output dis-\ntance of the layers. Throughout the training process, we track the changes in the Euclidean and\ncosine distances of hidden states between the inputs and outputs of each layer. As training pro-\ngresses, layers closer to the middle exhibit smaller input-output distances, while those at the head and\ntail show larger distances. This is a representation collapse issue specific to the pre-normalization\narchitecture . Based on this distance metric, we derive the following layer selection criteria: (1)\nPrioritize layers with the smallest distance metric (we found Euclidean distances work better than\ncommonly-used cosine distance ); (2) In cases of comparable distances, select layers nearer\nthe end of the sequence; (3) Avoid duplicating parameters from any single layer more than twice.\nThese guidelines ensure that the expanded model's computation flow aligns closely with that of the\npre-growth state, promoting enhanced model convergence."}, {"title": "Pre-training Details", "content": "Table 4 presents detailed parameter configurations for training the Tele-FLM-1T model. The entire\ntraining process utilizes 2318.7B training tokens. Consistent dataset proportions are applied across"}, {"title": "Lessons Learned", "content": "We summarize the lessons learned from our explorations in supervised fine-tuning and progressive\ngrowth, with our Tele-FLM (FLM-2) model series.\nSupervised Fine-tuning.\n\u2022 We observe a successful case following the philosophy of \"less is more\u201d for fine-tuning. We\nfind that learning the instruct-following format from a modest amount of data is sufficient\nfor a wide range of language understanding and generation tasks. This indicates that SFT\nhas the capability to elicit the the latent knowledge embedded within pre-trained models.\n\u2022 For reasoning tasks (maths, logical reasoning, coding), more sophisticated techniques and\nlarger volumes of high-quality data may be required, as we observe that fine-tuning with\nelementary-level maths problems does not translate well to more challenging tasks.\nProgressive Growth.\n\u2022 Function-preserving growth proved viable in training models exceeding 100B parameters,\nand capable to handle the rapid growth from 102B to 1T. In the whole training process, we\nobserve that the model manages to recover the knowledge learned in the previous stage. It\nis critical to preserve a proper aspect ratio to ensure convergence and loss reduction. We\nalso observe that each increase in model size resulted in a more pronounced rate of loss\nreduction. However, since training a 1T model from scratch requires immense resources,\nwe can not investigate the actual performance of our 1T model vs. one trained from scratch,"}, {"title": null, "content": "nor could we conduct a comprehensive benchmark evaluation for the 1T model due to\nlimited computational budgets.\n\u2022 As the model scale exceeds 1T, the optimization issue becomes intricate, especially when\nthe model is not trained from scratch, but grown from an existing one. Some experiences\nand inspirations from small model training are beneficial for the large model scenarios,\nwhile others are not. Our initial observations indicate that the training hyperparameters\nlisted in Table 4 are effective, yet further exploration is necessary to refine the operators,\ninitialization, and schedules."}]}