{"title": "LEVERAGING LLMS TO ENABLE NATURAL LANGUAGE SEARCH\nON GO-TO-MARKET PLATFORMS", "authors": ["Jesse Yao", "Saurav Acharya", "Priyaranjan Parida", "Srinivas Attipalli", "Ali Dasdan"], "abstract": "Enterprise searches require users to have complex knowledge of queries, configurations, and metadata,\nrendering it difficult for them to access information as needed. Most go-to-market (GTM) platforms\nutilize advanced search, an interface that enables users to filter queries by various fields using\ncategories or keywords, which, historically, however, has proven to be exceedingly cumbersome,\nas users are faced with seemingly hundreds of options, fields, and buttons. Consequently, querying\nwith natural language has long been ideal, a notion further empowered by Large Language Models\n(LLMs).\nIn this paper, we implement and evaluate a solution for the Zoominfo product for sellers, which\nprompts the LLM with natural language, producing search fields through entity extraction that are\nthen converted into a search query. The intermediary search fields offer numerous advantages for\neach query, including the elimination of syntax errors, simpler ground truths, and an intuitive format\nfor the LLM to interpret.\nWe paired this pipeline with many advanced prompt engineering strategies, featuring an intricate\nsystem message, few-shot prompting, chain-of-thought (CoT) reasoning, and execution refinement.\nFurthermore, we manually created the ground truth for 500+ natural language queries, enabling\nthe supervised fine-tuning of Llama-3-8B-Instruct and the introduction of sophisticated numerical\nmetrics.\nComprehensive experiments with closed, open source, and fine-tuned LLM models were conducted\nthrough exact, Jaccard, cosine, and semantic similarity on individual search entities to demonstrate\nthe efficacy of our approach. Overall, the most accurate closed model had an average accuracy of\n97% per query, with only one field performing under 90%, with comparable results observed from\nthe fine-tuned models.", "sections": [{"title": "1 Introduction", "content": "The ability to construct and execute enterprise searches is integral for all sales and marketing departments in today's\ndata-driven world, where quick access to relevant information is the key to decision-making. This fundamental capability\nfor many customers is traditionally gated behind specialized understanding of queries, configurations, and metadata,\nposing as a significant barrier to widespread accessibility for non-technical users on go-to-market (GTM) platforms,\nwhich help sellers and marketers with data and products to acquire new customers and retain their existing customers.\nCurrently, enterprise searches on GTM platforms utilize a familiar concept known as \u201cadvanced search\". By entering\nkeywords and selecting categories in numerous fields, the user can filter based on desired requirements and search\nterms, allowing for information retrieval without the technical aspects. However, this interface often presents users\nwith a complex array of options, fields, and buttons, leading to usability challenges and an overall arduous experience,\nespecially for larger databases [1, 2]."}, {"title": "2 Related Work", "content": "Translating from natural language to various query languages or query interfaces is not new. We give a brief overview\nof the related work."}, {"title": "2.1 NLP to SQL", "content": "Natural language (NL) processing (NLP) to Structured Query Language (SQL) is a well-known challenge within the\nmachine learning industry, with countless attempts made by companies even before the existence of LLMs [7, 8, 9,\n10, 11, 12, 13, 14, 15]. The rise of LLMs has significantly increased the potential to address this problem [16], as"}, {"title": "2.2 LLM Based IR", "content": "Information Retrieval (IR) systems, including search engines and enterprise searches, have similarly been revolutionized\nby LLMs [22, 23, 24, 25, 26, 27]. Specifically for enterprise searches, Bulfamante [23] proposed a pipeline that\nleveraged semantic embeddings to identify and return documents most relevant to the user's natural language query.\nOnce retrieved, these documents are processed by the LLM to craft detailed responses to the user's question. This\nretrieval augmented generation (RAG) system was tested against RAGAS metrics and achieved commendable results.\nRegarding entity extraction with LLMs, Dagdelen et al. [27] explores its application in chemistry, including ideas such\nas materials information extraction, identifying chemical formulae, and recognizing inorganic materials."}, {"title": "3 Proposed Approach", "content": "The proposed system in Fig. 1 can be divided into two phases: the conversion of a natural language query to search\nentities and the mapping of search entities to a search service query. Before presenting these phases, we give a brief\nintroduction to the ZI Sales product."}, {"title": "3.1 ZI Sales: Zoominfo's sales product", "content": "Zoominfo's sales product (called ZI Sales) is a search engine for sellers to perform contact (business person) and\ncompany searches using a free-form query as well as using various filters such as name, title, revenue, employee count,\nand the like. The number of filters is over 200."}, {"title": "3.2 Natural Language to Search Entities", "content": "First, a LLM is utilized to transform the natural language input into a JSON output of advanced search fields. The\nintermediary search entities provide several benefits, the most notable being the elimination of syntax errors. Directly\ngenerating the search service query from the LLM would result in issues such as missing semicolons, misplaced fields,\nand incorrect structure, making execution refinement impossible. Errors would only be detected upon running the query,\nwhich is simply too late in the process. In contrast, the LLM, with proper prompt engineering, is capable of generating\nJSON outputs without syntax errors due to the simplicity of the structure. In addition, it is trivial to detect if the output\nis not a JSON; in such cases, we can simply prompt the LLM to regenerate the response with a slightly altered prompt\nuntil a valid output is produced, a process known as execution refinement.\nOut of the 509 natural language queries submitted in the beta test, zero queries from all Anthropic and fine-tuned\nmodels after prompt engineering had syntax errors. In GPT-3.5-Turbo [29], two responses outputted \"I'm sorry, I didn't\nquite catch that\" or a similar sentiment that were fixed after the execution refinement.\nFurthermore, the JSON file provides a subtle yet significant advantage: the complication of creating the ground truth.\nFor each query, the ground truth is a simple JSON file, which is notably easier to create from a human perspective\nthan a search service query; for a search service query, it is quite arduous to even create 50 queries let alone 500. This\nintermediary JSON approach allowed us to develop an extensive ground truth, which naturally strengthens the accuracy,\nallows for numerical metrics, and enables supervised fine-tuning."}, {"title": "3.3 Search Entities to Search Service Query", "content": "The transformation of the JSON search entities into a search service query was mapped using a conversion function that\nalso detects and removes certain errors, ensuring smooth query execution."}, {"title": "4 Fields and Query Types", "content": "An advanced search field is a designated parameter in a search interface used to refine queries based on specific criteria,\nsuch as categories, keywords, or metadata. For instance, the \u201ccompany_name\u201d field will exclusively accept company\nnames, ensuring that only those names are outputted in the final table. Listed in Table 1 are all the supported fields.\nThese fields were explicitly selected to strike a delicate balance since too many fields may reduce the system's accuracy,\nwhile too few may not adequately cover the desired range of searches.\nWith respect to the desired range, fewer than 10 out of the 509 queries were supported by the Advanced search and not\nby the LLM. Among them, only two were affected by a limited word bank in a categorical field while the remainder\ninvolved searches based on intent. In addition, approximately 100 queries were not supported by the Advanced search\nitself, likely due to the unclear instructions in the beta test. For instance, a query asking for account information like\n\"find me a list of C-level IT contacts in New York that I have not exported to Salesforce\" or ambiguous queries such as\njust entering a period. Ultimately, around 400 queries remain for the experiments."}, {"title": "5 LLM Optimization", "content": "We discuss multiple optimization techniques we used in our research."}, {"title": "5.1 Prompt Engineering", "content": "Prompt Engineering is the practice of designing and refining input prompts to optimize the performance of the LLM.\nBeyond JSON outputs and execution refinement, other advanced prompt engineering techniques were deployed within\nthe system as well."}, {"title": "5.1.1 System Message", "content": "The system message defines the role of the LLM, the schema of the table, and the hidden logic within the task. For our\nuse case, there is only one schema for the Zoominfo sales product, allowing the complete integration of the schema\ninto the system message; however, varying schemas and fields would require the use of retrieval-augmented generation\n(RAG) rather than a static message. Consequently, the top-performing model, Claude 3.5 Sonnet, utilized a system\nmessage that contained 3095 words.\nIn the system message, the schema of the Zoominfo sales product is meticulously defined. For instance, company\nkeywords were described as follows:\n\"company_keywords\": List [str]\nDescription: Filter for companies with these specialties, products, services, or"}, {"title": "5.1.2 Few-Shot Learning", "content": "Few-shot learning involves providing the LLM with a few examples (shots) to help the model understand the desired\noutput format and reasoning. In the context of our model, it entails presenting the model with question-answer pairs,\nwhere the question is a natural language query and the answer takes the form of a JSON file. One shot would resemble\nthe following:\nPrompt: decision makers at Zoominfo and Chorus in the us\nAnswer in json format:\n{\n\"company_name\": [\"Zoominfo\", \"Chorus\"],\n\"management_levels\": [\"C-Level\", \"VP-Level\"],\n\"location\": {\n\"us_states\": [\"United States\"]\n},\n\"person_or_company\": \"person\"\n}\nClaude 3.5 Sonnet used a total of 11 shots. Each shot was selectively chosen to unveil hidden logic and clarify the\nambiguity of the human language. For example, what does the term \u201cdecision makers\" mean? We have defined it to the\nmodel as C-Level and VP-Level, but this is open to interpretation. These rationales were explicitly stated by another\ntechnique known as chain-of-thought (CoT) prompting."}, {"title": "5.1.3 Chain-of-Thought (CoT)", "content": "CoT prompting is defined as guiding the LLM step-by-step through the exact reasoning process to determine the desired\noutput. In our context, it serves to precisely describe and justify the expected JSON from the natural language query.\nSince natural language is potentially quite vague, or logic that seems intuitive to a human might not be intuitive to a\nLLM, CoT aids the LLM by providing explicit rationale. For instance, in the example above:\nReasoning:\nNotice that \"decision makers\" refers to C-Level and VP-Level executives.\nAlso notice that because titles and management_levels are mutually exclusive and since\na specific title was not specified, the management_levels field can exist. Note that\ntitles and departments are also mutually exclusive, but management_levels and departments\nare not.\nNotice that because two companies were mentioned, company_name has two entries in the list"}, {"title": "5.2 Fine-Tuning", "content": "Fine-tuning models involves additional training with datasets to improve the LLM's performance on specific tasks. This\nprocess of adjusting the model's parameters allows the model to adapt its general knowledge to a more nuanced target\ndomain, resulting in more accurate and relevant outputs. Fine-tuning can be further enhanced by Low-Rank Adaptation\n(LoRA) [30], which involves the decomposition of lower weight matrices into lower-dimensional representations. LORA\nsignificantly reduces the computation resources required and shortens the training time, leading to a more efficient\ntraining.\nWe conducted supervised fine-tuning on the open source model Llama3-8B-Instruct, using the ground truth as a labeled\ndataset (note that Llama3-70B was not tested due to limited GPU resources). After setting aside 10% of the data for the\ntest set, the model was trained using an 80% training and 20% validation split and computed on Nvidia L4 GPUs [6]\nthrough Google Cloud."}, {"title": "6 Experiments", "content": "We discuss the similarity metrics, experiments, and results."}, {"title": "6.1 Similarity Metrics", "content": "To measure the quality of the search results at a granular level, we implemented similarity metrics specific to each field\nlisted in Table 1, with the overall query accuracy as the average across every field. We utilized four different metrics,\neach tailored to integer, categorical, and free-text fields, and compared them against the ground truth. The four metrics\nare as follows:\n\u2022 Exact match - score of 1 if they exactly match and 0 otherwise. To remain consistent, all strings were made lowercase\nand all lists were sorted before validating an exact match.\n\u2022 Jaccard similarity - the total number of matches divided by the total number of unique elements between two sets.\n\u2022 Cosine similarity - the similarity between two texts based on the cosine of the angle between their frequency vector\nrepresentations.\n\u2022 Semantic similarity - the similarity between the meaning of two text segments by utilizing word embeddings [31]."}, {"title": "6.2 Evaluation", "content": "In the evaluation of prompt engineering techniques, fine-tuning, and LLM models, we divided the experiment into two\nparts: closed models and open source models."}, {"title": "6.2.1 Closed Models", "content": "Out of Sonnet 3.5, Haiku, and GPT-3.5-Turbo, the most accurate model, after prompt engineering and execution\nrefinement, was Sonnet 3.5, which demonstrated an average query accuracy of 97% across all metrics. Notably, in\nthis calculation, free text fields were weighed three times more than integer fields, as they were assessed using three\ndistinct metrics compared to a singular one. However, since free text fields are the worst-performing, the 97% actually\nrepresents a lower and more conservative estimate. Due to the imbalance of weights, it is imperative to examine\nperformance at the individual field level, where results are equally, if not more, impressive than the overall query score.\nSpecifically, across all individual fields, only three fields in Sonnet 3.5 exhibit performance statistics below 95%, with\nmerely one field falling below the 90% threshold.\nAs shown in Fig. 3, most fields score relatively well even for exact match metrics. The three lowest-performing exact\nmatch fields are, unsurprisingly, all free text fields, which, as illustrated in Table 2, perform better with regard to cosine\nand semantic similarity. Intriguingly, all categorical text fields performed exceptionally well, with no metric below\n95%, likely due to the rigorous pre-definition of word banks. Even subtle keywords, such as \"northeast,\" referring to a"}, {"title": "6.2.2 Open Source Models", "content": "The final model is a fine-tuned version of Llama3-8B-Instruct, an open source model. Despite a measly 8 billion\nparameters, the fine-tuned model (average score of 0.956) significantly surpasses both the base Llama3 model and\nGPT-3.5-Turbo, performing comparably with the Anthropic models. Similar to Haiku, it excels in several metrics\ncompared to Sonnet 3.5, although it falls slightly short in the overall average performance."}, {"title": "7 Conclusion", "content": "The integration of natural language to execute enterprise searches represents a significant advantage for users on GTM\nplatforms to be able to extract information. This study demonstrated the effectiveness of querying an enterprise database\nusing natural language inputs, converting them into structured JSON queries via an intermediary format. By leveraging\nadvanced prompt engineering techniques such as system messages, few-shot prompting, and chain-of-thought reasoning,\nalongside rigorous evaluation against the ground truth and execution refinement, this approach achieved exceptional\nresults in accuracy for every field supported. The evaluation of query performance used sophisticated metrics including\nexact match, Jaccard, cosine, and semantic similarity highlighted robust average accuracy rates of 97% across queries,\nwith only three fields scoring below 95% for Claude 3.5 Sonnet. Furthermore, fine-tuning Llama3-8B-Instruct yielded\nsimilar results despite the disparity in the number of parameters and the size of the model.\nOur methodology solidifies not only the feasibility of a natural language enterprise search but also the potential and\naccuracy of such an approach. It offers an effective and streamlined alternative to existing GTM platform's advanced\nsearch, and as technologies continue to evolve, this concept of natural language will further enhance the usability and\nefficiency of the information retrieval processes in the future."}, {"title": "8 Future Work", "content": "Due to limitations on GPU hardware, we were constrained to only fine-tuning the Llama3-8B-Instruct model. In the\nfuture, we plan to fine-tune the latest Llama models, Llama3.1-70B and 405B [32]. Furthermore, we aim to implement\nadvanced queries with negation logic and support intent queries, while maintaining the current accuracy."}]}