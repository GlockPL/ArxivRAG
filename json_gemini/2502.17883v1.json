{"title": "From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring", "authors": ["Matteo Contini", "Victor Illien", "Julien Barde", "Sylvain Poulain", "Serge Bernard", "Alexis Joly", "Sylvain Bonhommeau"], "abstract": "Drone-based remote sensing combined with AI-driven methodologies has shown great potential for accurate mapping and monitoring of coral reef ecosystems. This study presents a novel multi-scale approach to coral reef monitoring, integrating fine-scale underwater imagery with medium-scale aerial imagery. Underwater images are captured using an Autonomous Surface Vehicle (ASV), while aerial images are acquired with an aerial drone. A transformer-based deep-learning model is trained on underwater images to detect the presence of 31 classes covering various coral morphotypes, associated fauna, and habitats. These predictions serve as annotations for training a second model applied to aerial images. The transfer of information across scales is achieved through a weighted footprint method that accounts for partial overlaps between underwater image footprints and aerial image tiles. The results show that the multi-scale methodology successfully extends fine-scale classification to larger reef areas, achieving a high degree of accuracy in predicting coral morphotypes and associated habitats. The method showed a strong alignment between underwater-derived annotations and ground truth data, reflected by an AUC (Area Under the Curve) score of 0.9251. This shows that the integration of underwater and aerial imagery, supported by deep-learning models, can facilitate scalable and accurate reef assessments. This study demonstrates the potential of combining multi-scale imaging and AI to facilitate the monitoring and conservation of coral reefs. Our approach leverages the strengths of underwater and aerial imagery, ensuring the precision of fine-scale analysis while extending it to cover a broader reef area.", "sections": [{"title": "1 Introduction", "content": "Coral reefs are among the richest ecosystems on Earth in terms of species diversity. Moreover, they provide a number of key services: they function as natural barriers safeguarding coastlines from erosion and extreme weather events and serve as habitats and breeding grounds for innumerable marine species [1]. Additionally, they support local economies by offering resources for fishing, tourism and potential medicinal compounds [2], [3]. However, these ecosystems are under serious threat from human activities. Destructive and illegal fishing practices [4], anthropogenically derived chemical pollutants [5] and coastal development [6] are some of the main causes of coral reef degradation. Climate change poses an even greater risk through ocean acidification and warming, leading to widespread coral bleaching and habitat loss [7].\nIn December 2022 the Global Biodiversity Framework was adopted at the 15th Conference of the Parties (COP15) with the objective of protecting 30% of Earth's lands, oceans, coastal areas, and inland water by 2030 [8]. This ambitious goal requires the development of innovative monitoring techniques to assess the status of marine ecosystems and guide conservation efforts.\nNew techniques based on deep learning are emerging, offering the potential to revolutionize marine monitoring. In [9, 10] the authors developed a deep-learning based method for monitoring marine biodiversity using environmental DNA (eDNA). Besides, [11] proposed the use of convolutional neural networks (CNNs) to predict species distributions in the open ocean by leveraging environmental data and species occurrences. Artificial Intelligence (AI) models can also be used, coupled with Autonomous Surface Vehicles (ASVs), to classify spatialised underwater images generating high resolution species distribution maps [12] This approach enables fine-grained annotations and predictions, allowing the distinction of coral morphotypes and the identification of specific classes, such as algae, that are often difficult to discern. The main drawback of survey methods based solely on ASVs is that, since underwater images are taken at a very fine scale, it is difficult to cover large areas of coral reefs. This implies that, when areas on the order of tens of hectares are to be monitored, the method poses challenges in terms of cost, processing time and ease of deployment.\nRecent advances in imaging technologies have opened up new possibilities for large-scale coral reef monitoring. Drone-based imaging has emerged as a valuable tool for coastal habitat mapping and monitoring, providing a cost-effective method for high-resolution habitat classification when combined with machine learning techniques [13]. The main limitation of using only aerial images is that they do not directly provide detailed information on individual benthic organisms. Thus, the annotation of aerial images is usually made on broader classes (e.g., hard bottom, mixed substrate, soft bottom and seagrass). Here we argue that they contain sufficient information to infer the benthic community as a whole if they are combined with fine-grained annotations inferred from underwater images.\nTo give an illustration of this problem, Figure 1 shows an orthophoto obtained from aerial images on the left and an underwater image on the right, collected by an ASV, corresponding to the zone delimited by the red rectangle. The medium-scale image on the left shows a complex assemblage of corals. However,"}, {"title": "2 Materials and methods", "content": ""}, {"title": "2.1 Underwater image acquisition", "content": "Underwater images were collected using an Autonomous Surface Vehicle (ASV) equipped with a GoPro Hero 8 camera and a differential GPS Emlid Reach M2 mounted on a waterproof case. The version of the ASV builds on a previous version developed in [17]. In order to end up with georeferenced images embedded with attitude metadata (roll, pitch and yaw angles), the following steps were taken:\n1. Time synchronization between the camera and the GPS clocks.\n2. GPS position correction.\n3. Bathymetry data correction using local geoid parameters and attitude data of the ASV.\n4. Image georeferencing using the corrected GPS position and attitude data.\n50 missions were carried out in the lagoon of Reunion Island: 30 in the Saint-Leu lagoon and 20 in the Trou d'eau lagoon. For additional details regarding the processes of time synchronization, metadata correction, and other technical aspects, please refer to Appendix A where the corresponding subsections are discussed in depth. For further details on time synchronization, GPS position correction and image georeferencing, please refer to Appendix A, where these aspects are explained in detail."}, {"title": "2.2 Aerial image acquisition", "content": "Aerial drone images were taken with a DJI Mavic 2 Pro drone. Images were collected following good practices in use in aerial imagery [18].\nSince this drone is not equipped with a differential GPS, once images were taken and the SfM model was built, the orthophoto was georeferenced by collecting ground control points (GCPs) using a differential GPS. To obtain a high-resolution orthophoto with high positioning precision, the following steps were taken:\n1. Mission planning: check the equipment, request authorizations from French authorities, weather conditions and plan the flight mission.\n2. Mission execution: fly the drone at an altitude of 60m over the area of interest adapting camera settings to the specific conditions of the day.\n3. Image processing: build the Structure from Motion (SfM) model using images taken during the flight mission. This was done using OpenDroneMap.\n4. GCP collection: collect GCPs using a GPS with centimetric accuracy.\n5. Orthophoto georeferencing: georeference the orthophoto using the GCPs.\nTwo missions were carried out in the lagoon of Reunion Island: one in the Saint-Leu lagoon and the other in the Trou d'eau lagoon. For further details on mission planning, execution, image processing, and orthophoto georeferencing, please refer to Appendix B, where these aspects are explained in detail."}, {"title": "2.3 Multi-scale positioning", "content": "Since the objective is to pass information from a fine scale (underwater images) to a larger scale (drone images), the precision of the relative position between underwater and drone images is crucial.\nIn order to validate the georeferencing of underwater images with respect to aerial images, we used data from IGN (Institut national de l'information g\u00e9ographique et foresti\u00e8re\u00b9). Each 3 to 4 years, IGN produces BD ORTHO\u00ae: a collection of orthophotos with a default resolution of 20 cm. The last orthophoto produced by IGN on Reunion island was in 2022, so we decided to use this data as a reference to validate the georeferencing of our data.\nTwo visual criteria were then used to confirm the georeferencing of the underwater and aerial images with respect to the BD ORTHOR orthophoto:\n\u2022 Relative georeferencing: find the presence of easily recognizable objects in both underwater and aerial orthophoto and compare them on a GIS software. Often the presence of Porites corals can be used to compare the two scales since their contours are easily recognizable in both types of images. See Figure \u0417\u0430.\n\u2022 Aerial absolute georeferencing: find the presence of easily recognizable coral colonies in both aerial and BD ORTHOR orthophoto and compare them on a GIS software. See Figure 3b.\nThe combination of the relative georeferencing between underwater and aerial images and the aerial absolute georeferencing with respect to the BD ORTHO\u00ae orthophoto allowed us to crosscheck the georeferencing of the underwater images with respect to national baseline data."}, {"title": "2.4 Underwater image classification", "content": "The underwater deep learning model builds on the Dino V2 architecture, which is a vision transformer model that has been shown to outperform convolutional neural networks on image classification tasks [19]. The model has been trained on the open source dataset Seatizen Atlas image dataset composed of 51 distinct"}, {"title": "2.5 Upscaling predictions", "content": "Once the orthophoto is georeferenced and underwater inference is done, the key step is to correctly pass the information from the underwater model to the aerial model. The objective is to train an aerial model based on underwater predictions, without spending time on manual annotations of aerial images.\nThis is achieved by following the steps below:\n1. Split aerial orthophoto into tiles, ensuring consistency in ground surface representation of each tile across different sessions. Each tile represents an area of 1.5m x 1.5m. See Section 2.5.1.\n2. Filter useless aerial tiles (e.g., black tiles issued from SfM processing errors or tiles without corresponding underwater images). See Section 2.5.2.\n3. Associate each aerial tile with underwater images whose camera GPS position is within the tile boundaries. See Section 2.5.3.\n4. Compute the footprint of each underwater image and filter aerial tiles with not enough underwater coverage. See Section 2.5.3.\n5. Transform underwater predictions into aerial annotations. See Section 2.5.4."}, {"title": "2.5.1 Orthophoto tiling", "content": "The first step is to split the aerial orthophoto into tiles. This is done by taking into account the ground sample distance (GSD) of each orthophoto. This approach guarantees that while tile dimensions in pixels may vary due to different GSDs, each tile consistently represents a fixed area on the ground. This method allows for standardized comparison and analysis of images across different datasets and sessions, maintaining a consistent spatial resolution. Splitting the orthophoto into too small tiles results in images without enough context to be correctly classified and/or with an insufficient resolution. On the contrary, splitting the"}, {"title": "2.5.2 Useless tiles filtering", "content": "The second step is to filter out useless tiles. This is done by removing tiles with a high percentage of black pixels (due to errors in SfM processing) and tiles with no corresponding underwater images. Examples of such tiles are shown in Figure 4. In Figure 4a we can see an example of a tile extracted from the aerial orthophoto of the Saint Leu lagoon in Reunion Island with a high percentage of black pixels."}, {"title": "2.5.3 Footprint calculation and tile coverage assessment", "content": "The third step involves associating underwater predictions with aerial tiles. This assignment is achieved by identifying underwater images whose camera position centre falls within the boundaries of the aerial tiles. After associating underwater images to aerial tiles, the next step is to compute the footprint of each underwater image to filter out aerial tiles with not enough underwater coverage. In Figure 5, we outline the process to calculate the footprint of underwater images based on data from ASV sensors. Using bathymetric data from the echosounder, we measure the distance between the camera and the seabed, which determines the scale of the area captured in each image. The camera orientation in the XYZ axis plane is defined by the roll, pitch, and yaw angles, which determine how the field of view (FOV) is directed relative to the seafloor. Finally the FOV, divided into horizontal $(FOVh)$ and vertical $(FOVy)$ angles, defines the area visible to the camera. By projecting these angles down to the seafloor, we calculate the intersection points, forming a polygonal footprint that represents the region covered by the image. This footprint is necessary to associate each underwater image with a specific area of the seafloor. Merging the footprints of all underwater images associated with a tile, we obtain the union of the footprints, which represents the area covered by the underwater images associated with the tile.\nThis allows us to filter out tiles with not enough underwater coverage. An example of such a tile is shown in Figure 4b, where a group of tiles extracted on the same orthophoto is shown. Tiles whose center is represented by a red point are classified as useful, since they are completely covered by underwater images. On the contrary, tiles represented by an orange triangle are classified as useless, since they do not have enough coverage of underwater images."}, {"title": "2.5.4 Transforming underwater predictions into aerial annotations", "content": "The fifth step is to transform underwater predictions into aerial annotations. The trivial approach would be to associate the presence of a class c in a tile t if at least one underwater image associated with the tile is\npredicted as belonging to the class c by the teacher model. Or, in other words, if class c is not predicted as being absent on all underwater images associated with tile t, which can be formulated as:\n$\\forall c \\in C, t \\in T, I(y_c = 1 | t) = 1 - \\prod_{x \\in X(t)} [1 - h_{\\text{teacher}}(x)]$\nwhere:\n\u2022 C is the set of classes described in Section 2.5.5\n\u2022 T is the set of tiles\n\u2022 $y_c \\in \\{0; 1\\}$ is the binary label associated with the presence/absence of class c\n\u2022 $I(y_c = 1 | t) \\in \\{0;1\\}$ is a binary function indicating the presence or absence of class c in tile t.\n\u2022 X(t) is the set of underwater images associated with tile t\n\u2022 $h_{\\text{teacher}}(x) \\in \\{0;1\\}$ is the binary prediction associated with the presence/absence of class c in underwater image x\nThe drawback of this approach is that it does not consider the footprint of underwater images, tending to overestimate the presence probability of a class in a tile.\nA more realistic approach, since not all underwater images footprint fall entirely within the boundaries of a specific tile, needs to compute the intersection between the underwater image footprint and the corresponding tile. This allows us to give more weight to underwater images that are completely within a tile and less weight to underwater images that are only partially within a tile.\nThe orthophoto in Figure 6 gives an example with the corresponding predictions on underwater images. In the right part of Figure 6a, a colony of Acropora Tabular corals is visible. Proceeding with tile extraction from the orthophoto, we obtain the tile in Figure 6b. Since the Acropora Tabular corals do not fall within the tile, we would like that, after computing the tile annotation starting from underwater predictions, the probability for the class Acropora Tabular associated with this tile will be weak. Unfortunately, it may happen that underwater images that have the center within the tile (but not all the footprint) include classes that are outside the tile bounds: as shown in Figure 6c, where a part of the Acropora Tabular coral colony is visible in the right part of the underwater image. In these cases, weighting underwater predictions based\non the intersection between the underwater image footprint and the tile allows reducing the impact of these images on the final aerial annotations. This is shown in Figure 6d where predictions on underwater images are represented with circles on a red ramp and aerial annotations are represented with stars on a blue ramp. Even if in the underwater image in Figure 6c on the right of the tile the presence of the Acropora Tabular class is predicted, the overlap between the underwater image and the aerial tile is weak. Consequently, the probability of presence of the Acropora Tabular class on the tile is mitigated: ending up with an annotation of 0.4 (while the blue star on the tile just on the right indicates a probability of presence of 0.98).\nTo take account of the intersection between the underwater image footprint and the tile bounds, we can consider that the probability of presence is proportional to the intersection area relative to the underwater image area. Therefore, we can modify Equation 1 as follows:\n$\\forall c \\in C, \\forall t \\in T, P(y_c = 1 | t) = 1 - \\prod_{x \\in X(t)} \\left[1 - \\left( \\frac{s(x \\cap t)}{s(x)} \\right) h_{\\text{teacher}}(x) \\right]$\nwhere:\n\u2022 $P(y_c = 1 | t) \\in [0, 1]$ is the probability of presence of class c in tile t"}, {"title": "2.5.5 Aerial dataset", "content": "Following the upscaling process detailed in Section 2.5, starting from two aerial orthophotos of the Trou d'eau and Saint-Leu lagoons in Reunion Island measuring 189,682 m\u00b2 and 204,748 m\u00b2 respectively, we ended up with 4,911 and 6,832 annotated tiles respectively for a total of 11,743 annotated tiles.\nSince the upscaling process implies a loss in the image resolution, we made some changes about the classes to be predicted:\n1. The first change was to merge Algae classes into a single class called Algae, indeed distinguishing between the different types of algae (Algal Assemblage, Algae Halimeda, Algae Coralline and Algae Turf) is a task that requires a higher resolution than the one we have2.\n2. The second change was to remove underwater classes that do not have a corresponding aerial class: Blurred images (an underwater blurred image does not imply a blurred aerial image) and Homo Sapiens (since human body parts in underwater images do not imply human body parts in aerial images).\n3. The third change was to remove underwater classes that are not relevant for the aerial images, i.e. Fish, Sea cucumber and Sea urchin. The first two classes, even if visible in some aerial images, were removed because underwater and aerial images are not taken at the same time, so that the presence of a sea cucumber or a fish in an underwater image does not imply the presence of those organisms in the corresponding aerial image. The last one was removed since those organisms are not visible at all in aerial images.\nFinally, we retained only classes for which there was a sufficient number of annotations. Thus, removing classes that have less than 200 annotations in the aerial dataset, we ended up with 12 classes:\n\u2022 Coral"}, {"title": "2.5.6 Aerial deep learning model (student model)", "content": "To train the student model with soft labels, we use the Binary Cross-Entropy (BCE) with logits loss function. This loss measures the divergence between the predicted logits of the student model and the soft labels P(yc = 1 | t) \u2208 [0, 1] generated by the teacher model. Specifically, the loss for class c in tile t is given by:\n$L_{BCE}(t, c) = - \\left[P(Y_c = 1 | t) \\cdot log(p_{\\text{student}}(Y_c = 1 | t)) + (1 - P(y_c = 1 | t)) \\cdot log(1 - p_{\\text{student}}(Y_c = 1 | t))\\right]$\nwhere:\n\u2022 P(yc = 1 | t) \u2208 [0, 1] is the soft label provided by the teacher model for class cin tile t, as described in equations 2 and 3\n\u2022 $p_{\\text{student}}(Y_c = 1 | t) \\in [0, 1]$ is the probabilistic output of the student model for class c in tile t, obtained through a sigmoid function on top of the final layer of the model.\nTo maintain consistency with underwater predictions, we used the same architecture for the student model as the one used for the teacher model (i.e. the DinoV2 model [19]).\nThe only difference is that, since the underwater model was trained with binary values and the aerial model has to be trained on probabilities, when computing evaluation metrics during the training process we cannot use the accuracy, precision, recall and F1-score metrics. Instead, we will compute the Root Mean Squared Error (RMSE), the Mean Absolute Error (MAE) and the Kullback-Leibler (KL) divergence metrics."}, {"title": "2.6 Test zone and model evaluation", "content": "To evaluate the performance of the aerial deep learning model, we selected a test zone within the Trou d'eau lagoon, see Figure 7. This area was chosen due to its diverse composition of coral morphotypes, habitats, and other marine organisms, representing a challenging environment for model validation. The test zone comprises 194 underwater images, corresponding to 28 aerial tiles, for a total of 63m\u00b2.\nThe annotation process for the aerial tiles is carried out as follows:\n1. For each aerial tile, underwater images with centroids located within the tile boundaries are identified\n2. These underwater images are then projected in QGIS to assess the portions of each image that intersect the aerial tile boundaries\n3. Each cropped underwater image is manually annotated with fine-grained precision\n4. As a result, each aerial tile is annotated with a level of detail comparable to that of underwater imagery and is therefore considered as ground truth data"}, {"title": "3 Results", "content": ""}, {"title": "3.1 Upscaling process evaluation", "content": "In order to evaluate the upscaling process, we compared the generated annotations with ground truth annotations in the geospatial test zone in the Trou d'eau lagoon. We first evaluated the quality of the soft labels generated with our methods described in Equations 2 and 3. As they provide a presence probability for each class, we can actually measure their AUC on the ground truth annotations. With a value of 0.9211 for annotations generated through Equation 2 and 0.9251 for annotations generated through Equation 3, both methods show a high level of accuracy in transferring information across scales.\nTo further evaluate both methods, we then measured the performance of the student model trained with either method. In the following, we will call Model_spatial_only the model trained from annotations generated through Equation 2 and Model_distilled the model trained from the annotations generated through Equation 3. Only the second model integrates information about the teacher's model confidence (= knowledge distillation). The first model integrates the hard labels predicted by the teacher and the spatial coverage. We first looked at the evaluation metrics measured on the soft labels themselves (using the random test set)."}, {"title": "3.2 Prediction maps", "content": "Once the aerial (student) model is trained, we can use it in inference mode to generate high resolution maps of large areas. In particular, we ran it on 20,027 tiles in the Trou d'eau lagoon and 61,059 tiles in the Saint-Leu lagoons. For each tile, we used the output of the student model as the probability of presence of each class and then we generated prediction maps for each class."}, {"title": "4 Discussion", "content": "This study demonstrates the potential of combining underwater and aerial imagery to improve monitoring and management of coral reef ecosystems. Although previous research has highlighted the advantages of using both imaging techniques, this work, to our knowledge, is the first to synergize AI models across different scales. By using high-resolution underwater AI predictions to train a larger-scale aerial model, we ensure the precision of underwater analysis while extending it to cover a broader reef area. This multi-scale approach has the potential to advance marine monitoring, and it can also be applied to other fields such as agriculture, forestry, urban planning, and so forth."}, {"title": "4.1 Transferring information across scales: model independence and flexibility", "content": ""}, {"title": "4.1.1 Upscaling process evaluation", "content": "While having a well-performing fine-scale model is needed for reliable medium-scale annotations, working on the model architecture in order to gain a few percentage points of accuracy is not the core of our methodology. The primary objective of our workflow is to train a medium-scale student model to mimic the behaviour of a fine-scale teacher model, without the need to reannotate medium-scale images."}, {"title": "4.1.2 Aerial model evaluation", "content": "The model's performance on the neural network test set underscores its robustness. As shown in Table 1, the RMSE, MAE, and KL Divergence metrics are all low, indicating that the model's predictions closely match annotations on the test set. The relatively low MAE compared to the RMSE (0.1143 vs. 0.1546) suggests that predictions on average deviate from the true values by less than 12%, indicating a high level of accuracy. Occasional larger discrepancies make the RMSE slightly higher, but the overall results are still very promising. The high AUC value (0.7952) computed on the test zone further confirms the model's strong performance, indicating that the model can reliably generate probability estimates that closely align with the true distributions of the labels.\nWith regard to possible improvements concerning the deep learning model, as previously mentioned, in this study we used Dino V2 as a backbone: one of the SOTA (State Of The Art) computer vision models that currently performs the best on benchmark datasets. Applying transfer learning, we take advantage of the model's strong generalization capabilities while fine-tuning it to address our specific problem. However, we recognize that with the rapid advancements in artificial intelligence, our model may already be on the path to obsolescence [23]. Continuously updating the model predictions to train the medium-scale model would enable us to incorporate the latest breakthroughs in AI, ensuring increasingly refined annotation quality over time."}, {"title": "4.2 Georeferencing challenges in multi-scale monitoring", "content": "A framework that enables the transfer of information from fine-scale to a broader-scale imagery relies essentially on achieving precise alignment between the two layers of data. As shown in Figure 6 the benthic substrate can vary significantly across small distances, so that in order to upscale underwater predictions to aerial annotations, data need to be accurately georeferenced.\nUsing differential GPS technology is the first step to achieve precise positioning, but does not guarantee a centimetric accuracy. To improve the ASV positioning, we used PPK techniques thanks to the CentipedeRTK network 3, ending up with a centimetric accuracy in the ASV positioning. Unfortunately, the position of the ASV is not the same as the position of the image, since waves can change the attitude of the ASV by tilting the direction of the camera from the vertical axis (see Figure 5). To correct this, we used the camera angles on the three axes (Roll, Pitch and Yaw) and the echosounder data to compute the footprint of underwater images. Ending up with the latitude and longitude of the four corners defining the footprint"}, {"title": "4.3 Expanding spatial coverage and species identification", "content": ""}, {"title": "4.3.1 Satellite imagery upscaling", "content": "This study highlights several areas for future improvement. Although the classification accuracy was high across most classes, certain coral types remain challenging to differentiate at the aerial scale due to image resolution constraints. Addressing this limitation may involve refining aerial image resolution by reducing the flight altitude (respecting the regulations in force in the country where the data is collected), employing more advanced image processing techniques, using higher-quality drones / cameras with better sensors [24] or even using hyperspectral cameras to capture more detailed information about the reef [25].\nFinally, mimicking the idea presented in this study, we could extend the methodology to the satellite scale. In [26] the authors discuss the complementary nature of UAV and satellite data, pointing out that integrating these technologies can improve spatial and temporal resolution in remote sensing applications. Successful integration would allow rapid monitoring of large areas, significantly reducing the need for field data collection, as satellite imagery is often freely available and collected at regular intervals. Furthermore, access to a historical archive of satellite imagery provides a unique opportunity to study ecosystem evolution over past decades, while supporting long-term monitoring efforts in the future. This extended temporal and spatial coverage could greatly improve our understanding of ecological change on a global scale."}, {"title": "4.3.2 Expanding to slow-moving species identification through synchronized imaging", "content": "The collection of both fine-scale and aerial images simultaneously has the potential to enable the identification of certain benthic species, which would otherwise be challenging to recognise. To illustrate, slow-moving organisms such as sea cucumbers are frequently visible in aerial images (e.g., Figure 7). Given that these species move at a slow speed, synchronised collection of both image types would provide the temporal and spatial alignment necessary for passing the information from the fine-scale model to the medium-scale model. Although this data collection approach imposes additional constraints compared to the method used in this study, where fine-scale and medium-scale data can be collected days or even months apart, it offers the advantage of providing additional ecological insights that would otherwise be inaccessible [27]."}, {"title": "5 Conclusion", "content": "In this study, we presented a novel methodology for transferring information across scales in coral reef monitoring. By combining high-resolution underwater imagery with medium-scale aerial data, we were able to train a deep learning model to predict benthic substrate composition over a large reef area. Our approach leverages the strengths of both imaging techniques, ensuring the precision of fine-scale analysis while extending it to cover a broader reef area. This multi-scale framework has the potential to revolutionize marine monitoring, providing a more comprehensive and efficient way to assess coral reef health."}, {"title": "6 CRediT authorship contribution statement", "content": "Matteo Contini: Conceptualization, Data curation, Methodology, Formal analysis, Writing - original draft. Victor Illien: Conceptualization, Data curation, Methodology, Software, Writing review and editing. Julien Barde: Conceptualization, Data curation, Funding acquisition, Methodology, Supervision, Writing review and editing. Sylvain Poulain: Data curation, Methodology, Software. Serge Bernard: Conceptualization, Data curation, Funding acquisition, Methodology, Supervision, Writing review and editing. Alexis Joly: Conceptualization, Data curation, Funding acquisition, Methodology, Project administration, Writing review and editing. Sylvain Bonhommeau: Conceptualization, Data curation, Funding acquisition, Methodology, Project administration, Writing - review and editing."}, {"title": "7 Acknowledgement", "content": "The authors acknowledge the P\u00f4le de Calcul et de Donn\u00e9es Marines (PCDM) for providing DATARMOR storage, support services and computational resources. We extend our heartfelt thanks to Laurence Maurel, Cam Ly Rintz, Leanne Carpentier, Magali Duval, Laura Babet, Belen De Ana, Anne-Elise Nieblas, Arthur Lazennec, Victor Russias, Mervyn Ravitchandirane, Mohan Julien, Pierre Gogendeau, Thomas Chevrier and Justine Talpaert Daudon, involved in the annotation and collection of data for this study. Their contributions were indispensable to our research efforts. We also thank Emilien Alvarez for his valuable advice on drone imaging."}, {"title": "8 Funding", "content": "This work was supported by several projects: Seatizen (Ifremer internal grant), Plancha (supported by the Contrat de convergence et de transformation 2019-2022, mesure 3.3.1.1 de la Pr\u00e9fecture de la R\u00e9union, France), IOT project (funded by FEDER INTERREG V and Prefet de La R\u00e9union: grant #20181306-0018039 and the Contrat de Convergence et de Transformation de la Pr\u00e9fecture de La R\u00e9union), Ocean and Climate Priority Research Programme, FISH-PREDICT project (funded by the IA-Biodiv ANR project: ANR-21-AAFI-0001-01), B1-4SEA funded by Explorations de Monaco and G2OI FEDER INTERREG V (grant #20201454-0018095)."}, {"title": "9 Conflict of Interest statement", "content": "The authors declare no conflict of interest."}, {"title": "10 Data availability", "content": "The data that support the findings of this study are openly available in Zenodo at Seatizen Atlas. All code for data processing associated with the current submission is available on drone-upscaling Github.\nThe code for downloading data associated with the current submission is available on zenodo-tools Github.\nThe code used to train the neural network model used in the current submission is available on DinoVdeau Github."}, {"title": "11 Declaration of Generative AI and AI-assisted technologies in the writ- ing process", "content": "During the preparation of this work the author(s) used ChatGPT-40 and GitHub Copilot in order to improve language and readability. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the published article. This tools were not involved in the design, implementation, data analysis, or manuscript preparation of the study."}, {"title": "A ASV supplementary informations", "content": ""}, {"title": "A.1 Time synchronization", "content": "Videos were cut into frames with a rate of 2.997 fps, so that the cutting frame rate $f_c = 2.997$ fps is a divisor of the video frame rate $f_r = 23.976$, ensuring that the ratio $\\frac{f_r}{f_u}$ is an integer.\nSince we use time in order to synchronize metadata and images, we need a method to assign a precise timestamp to each frame. Before each data acquisition, as differences of several seconds/minutes can be observed between the clocks of the different devices (the GPS receiver clock is not the same as the camera), the user films the time given by a GPS application on his mobile phone with the camera in order to associate the exact satellite time (UTC+0) to a specific frame or image. In the case where the time filmed with the camera follows UTC standards, leap seconds caused by the difference between UTC time and GPS time must be taken into account when synchronizing the GPS position with the images. This specific frame can then be used as a starting point to correct the timestamp of all images by using the frame rate $f_c$ and the number of frames between the starting frame and the frame of interest.\nCutting frames with a rate that is a divisor of the video frame rate is particularly important when working with precise position accuracy."}, {"title": "A.2 Metadata correction", "content": "Since the ASV is equipped with a differential GPS, PPK (Post-Processed Kinematic) corrections can be applied to the GPS position of the rover in order to get a centimetric accuracy.\nIndeed, for each data collection event a mobile base station has been strategically deployed near the field mission. The mobile base station, connected to the CentipedeRTK network which provides real-time corrections to the GPS base station, ensures a high precision of the GPS base position [28"}]}