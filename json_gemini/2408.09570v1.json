{"title": "SAY MY NAME: A MODEL'S BIAS DISCOVERY FRAMEWORK", "authors": ["Massimiliano Ciranni", "Luca Molinaro", "Carlo Alberto Barbano", "Attilio Fiandrotti", "Vittorio Murino", "Vito Paolo Pastore", "Enzo Tartaglione"], "abstract": "In the last few years, due to the broad applicability of deep learning to downstream tasks and end-to-end training capabilities, increasingly more concerns about potential biases to specific, non-representative patterns have been raised. Many works focusing on unsupervised debiasing usually leverage the tendency of deep models to learn \u201ceasier\u201d samples, for example by clustering the latent space to obtain bias pseudo-labels. However, the interpretation of such pseudo-labels is not trivial, especially for a non-expert end user, as it does not provide semantic information about the bias features. To address this issue, we introduce \"Say My Name\u201d (SaMyNa), the first tool to identify biases within deep models semantically. Unlike existing methods, our approach focuses on biases learned by the model. Our text-based pipeline enhances explainability and supports debiasing efforts: applicable during either training or post-hoc validation, our method can disentangle task-related information and proposes itself as a tool to analyze biases. Evaluation on traditional benchmarks demonstrates its effectiveness in detecting biases and even disclaiming them, showcasing its broad applicability for model diagnosis.", "sections": [{"title": "1 Introduction", "content": "In the past decade, advances in technology have made it possible to widely use deep learning (DL) techniques, greatly impacting the computer vision field. By allowing systems to be trained end-to-end, DL offers potentially fast model deployability to solve complex problems, leading to fast progress and changing how we perceive and analyze visual information. Today, deep learning is applied in various real-world scenarios, such as self-driving cars [1], medical imaging [2], and augmented reality [3].\nThese huge possibilities hinder potential pitfalls. One challenge to face when deploying DL solutions lies in guaranteeing that the model does not over-rely on specific patterns that are non-representative of the real-world data distribution [4, 5]. This is key for safety, fairness, and ethics [6]. To provide a simple example, when deploying solutions in autonomous driving, simple tasks like pedestrian detection might be implicitly solved by associating specific background elements (like sidewalks or pedestrian crossings) with the presence of pedestrians. This reliance on environmental cues can act as shortcuts for the network [7], leading to poor performance when the context changes, such as when a pedestrian is crossing a"}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "road without a marked crossing lane. DL models, if not discouraged, tend to rely on spurious correlations captured at training time, especially when they are easier to learn than the actual semantic attributes [8]. We refer to these spurious correlations as biases, and we say that the model that learned such shortcuts is biased to them.\nIn 2021, the European Commission introduced the Artificial Intelligence Act (AI Act) to regulate AI based on the potential risks it poses [9]. Like the General Data Protection Regulation (GDPR) [10], the AI Act could set in the next few years a global standard. Ensuring that DL models avoid spurious biases that could affect their safety, trust, and accountability is not only essential for user safety but might soon also become a legal requirement.\nA massive recent effort has been conducted by the Computer Vision community to try to discourage the presence of biases. In a nutshell, we can roughly distinguish three main research lines: (i) supervised, where the labels of the bias are provided [11, 12]; (ii) bias-tailored, where a hint on what the potential bias might be is provided prior to training, and an ad-hoc model is deployed to capture it [13, 14]; (iii) unsupervised, where biases are guessed directed within the vanilla-trained model [8, 15, 16, 17]. When deploying a DL solution in the wild, the latter line of research appears to be the best fit, as detailed information about bias is almost surely missing. Although some solutions already exist in this context [18, 8, 15, 19], there is still a gap in the literature related to the problem of naming a specific bias affecting a DL model, providing natural language descriptors that can be directly interpretable by a human. Existing solutions either start from a predefined set of attributes [20, 21] or require computationally expensive operations, as a captioning of the entire training set [18].\nOur work here moves the first steps to mine specific model's biases. Unlike approaches that discover biases in the dataset, our focus is oriented on naming the biases captured by the model under exam. We mine the specific features in common with these samples and we associate semantic (textual) meaning to them. From this, an expert user (or prospectively a certifier software) can search and discriminate whether the learned feature is a bias or rather, a feature for the system (Fig. 1). Through \u201cSay My Name\u201d (SaMyNa), we aim at providing a tool that enhances explainability for the DL model's learned features, on top of which, if necessary, any state-of-the-art debiasing approach can be used to sanitize the model.\nOur contributions are here summarized at a glance."}, {"title": "2 Related Works", "content": "The problem of bias and model debiasing has been widely explored in recent years, within three main frameworks, differing from how or if bias knowledge is explored for mitigating model dependency on bias: supervised, bias-tailored, and unsupervised.\nSupervised Debiasing. Supervised methods require explicit knowledge of the bias, generally in the form of labels, indicating whether a sample presents a certain bias or not. One of the most typical approaches consists of training an explicit bias classifier, trained on the same representation space as the target classifier, in an adversarial way, forcing the encoder to extract unbiased representations [22, 23]. Alternatively, bias labels can be exploited to identify pre-defined groups, training a model to minimize the worst-case training loss, thus pushing the model towards learning biased samples [24]. Another possibility is represented by regularization terms, which aim at achieving invariance to bias features [11, 25].\nBias-Tailored Debiasing. Bias-tailored approaches usually rely on some kind of knowledge about the bias nature. For example, if the bias is textural, then custom architectures can be designed to be more sensitive to textural information. For example, [26] propose ReBias, where a custom texture bias-capturing model is designed using 1x1 convolutions. A similar approach is followed by [12], where a BagNet-18 [27] is used as a bias-capturing model."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Unsupervised Debiasing. Differently from the previously described approaches, unsupervised debiasing methods do not assume any prior knowledge of the bias, facing a more realistic situation where bias is unknown. Nam et al. [8] propose LfF, where a vanilla bias-capturing model is trained with a focus on easier samples (bias-aligned), using the Generalized Cross-Entropy (GCE) loss [28], while a debiased network is trained by giving more importance to the samples that the bias-capturing model struggles to discriminate. Ji et al. [19] propose an unsupervised clustering method that learns representations invariant to some unknown or \u201cdistractor\" classes in the data, by employing over-clustering. A set of unsupervised methods relies on the assumption that bias-conflicting samples are likely to be misclassified by a biased model [29, 30]. In [30] a model is trained for a few epochs and then used in inference on the training set, considering misclassified samples as bias-conflicting and vice-versa. The debiasing is then performed by up-sampling the predicted bias-conflicting samples. In [29] the training set is split into a fixed number of subsets, training a model on each of them. Then, the trained models are ensembled into a bias-commmittee and the entire training set is fed to the committee, proposing that debiasing can be performed using a weighted ERM, where the weights are proportional to the number of models in the ensemble misclassifying a certain sample. Similarly to [15], we are able to identify during the training of a vanilla model in which moment the bias is potentially best fitted by the model, with the advantage of working directly at the output of the same model instead of mining the information in its latent space. This comes both with computational advantages (given that the latent space is typically higher dimensional) and with better interpretability of the outcome, given that we work in the model's output space.\nBias Naming. Recently, methods exploiting natural language and vision-language models to identify and mitigate bias have been proposed [20, 18, 31]. Zhang et al. [31] introduce a method capable of determining subsets of images with similar attributes systematically misclassified by a model (i.e., error slices) and a rectification method based on language. However, it starts from a pre-defined set of attributes, thus hindering the possibility of discovering completely unknown and multiple biases. [20] exploits a cross-modal embedding space to identify error slices, providing natural language predictions of the identified slices. Wiles et al. [21] propose a method for automatically determining a model's failures, exploiting large-scale vision-language models and captioners to provide interpretable descriptors of such failures in natural language. In [18], the authors propose to extract class-wise keywords representative of bias, later used for model debiasing, exploiting group-DRO [24] on the identified groups. In this work, a CLIP score is defined using the similarity between extracted keywords and correctly and misclassified samples (class-wise) and used to find the keywords associated with a bias. In [32], the authors use large-language models and text prompts for bias discovery in text-to-image generative models. Differently from the previously cited works, we introduce an unsupervised method for diagnosing a model dependency on bias for image classification tasks, that can either be performed during or after training and only exploits task-related knowledge to provide a transparent analysis on the potential hidden biases captured by the model."}, {"title": "3 Method", "content": "In this section, we present our proposed method SaMyNa to identify potential spurious correlations learned by the model under analysis (Sec. 3.2). Our method can be plugged to perform model diagnosis either at training time or at test: for the first case, we also present an approach to identify at which point to mine a potential bias for the target model (Sec. 3.1)."}, {"title": "3.1 Mining Model's Biases", "content": "Consider a supervised classification setup (having C target classes), where we learn from a dataset $D_{train}$ containing N input samples $(x_1,..., x_N) \\in X$, each with a corresponding ground truth label $(\\hat{y}_1, ..., \\hat{y}_N) \\in Y$. A deep neural network M, trained for t iterations, produces an output distribution $y_{t,n} \\in R^C$ over the C classes, for each input $x_n$ (typically, the activation of the last layer is a softmax). The network is trained to match the ground truth label $\\hat{y}_n$ by minimizing a loss function such as cross-entropy.\nIf any bias is present in the training set, however, the learning process could drive the model towards the selection of spurious features [24, 8, 26], resulting in misclassification errors. Recent findings show that it is possible to identify the moment when the model best fits the bias in the training set [15]. We will formulate here the problem of identifying, at training time, when the trained model maximally fits a potential bias.\nWe say that M misclassifies the n-th sample at the t-th learning iteration if $\\hat{y}_n \\neq argmax(y_{t,n})$. Focusing on this example, we know that by minimizing the loss function we aim at increasing the value of the $\\hat{y}_n$-th component $Y_{t,n}(\\hat{y}_n)$ (while decreasing all the others). Inspired by the Hinge loss function [33], we can define a per-sample distance metric telling us how far the n-th sample is from being correctly classified:\n$d_{t,n} = \\begin{cases} max(y_{t,n}) - Y_{t,n}(\\hat{y}_n) & \\text{if } argmax(Y_{t,n}) \\neq \\hat{y}_n \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Intuitively, the higher (1) is, the most Mt is confident in misclassifying n. We are interested in finding the iteration t* such that the model most confidently misclassifies a pool of samples:\n$t^* = argmax_t \\frac{1}{n} \\sum_n \\delta [y_n \\neq argmax(y_{t,n})] d_{t,n},$\nwhere $\\delta$ is the Kronecker delta and $\\bar{\\delta} = 1 - \\delta$. When reaching t*, the most informative samples are the misclassified ones: given that the model is most confident in misclassifying them, then the model has clearly learned some spurious features. Differently from prior works [15] speculating that misclassified samples embody a bias (with the goal of applying debiasing methods), our goal is to understand why these samples are misclassified, ultimately providing an end user of the system the possibility to acknowledge the presence of a bias. For the model Mt we will split the training dataset in a pool of correctly classified samples $D_{correct}$ and misclassified $D_{misclass}$"}, {"title": "3.2 Bias Naming", "content": "We present here SaMyNa, our bias naming approach starting from a trained model M from which we aim to run our bias naming tool. Fig. 2 proposes an overview of the main pipeline we used, consisting of the following steps:\n1. Samples subset selection. Given that the objective of our proposed method is to identify biases learned by M, we are allowed to propose a subset of most representative samples for a given target class (Sec. 3.2.1).\n2. Samples captioning. Once the samples are selected, a multimodal LLM captioning tool is used to extract a textual description of each sample (Sec. 3.2.2).\n3. Keywords selection. Starting from the computed captions, we mine keywords in common from the textual description of the samples within the same learned class (Sec. 3.2.3).\n4. Learned class embedding. Starting from a textual description of the samples, we can extract the shared information between the correctly classified samples and the incorrectly classified ones: this will constitute the embedding for a potential bias (Sec. 3.2.4).\n5. Keywords ranking. The embedding of the learned class is compared with the embedding of recurrent keywords in the captions and we get a ranking for the keywords most aligned with the learned class (Sec. 3.2.5)."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Given M, for a given target class c, we extract the pool of correctly classified samples $D_{correct}(c)$ and samples misclassified as e $D_{misclass}(c)$.\u2020 Provided that M clusters both $D_{correct}(c)$ and $D_{misclass}(c)$ together, our hypothesis is that these two share a common set of features, behind which we might find a bias. In the typical deployment scenario, the correctly classified examples are abundant, and, for instance, M projects them in a very narrow neighborhood of its latent space. We build on top of this observation and run a k-medoid algorithm to reduce the cardinality of correctly classified samples. Our long-range objective will be indeed to capture the set of features that are correctly learned by the model, and k-medoids is a natural choice to have a good coverage of the latent space for M."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "At this point, we will generate captions from the selected samples. To do this, we use a pre-trained multimodal large language model that takes as input both a prompt and an image. The choice of the prompt is kept generic and asks to generate a textual description of the content of the image, providing some context of the target task. Through all our experiments we will use a large-scale captioner, given that biases might hide in very specific characteristics of the provided images. Images are also preprocessed to ensure they meet the requirements of the model, such as size."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "From the captions obtained in the previous step (Sec. 3.2.2), we select recurrent keywords. First, we perform some NLP standard processing, including for example stop-words removal. Each caption is word-level tokenized [34], and every token is considered a potential keyword. Then, we count the frequencies of the obtained keywords within the same class c. Lastly, we filter out the keywords appearing in the captions of less than the $f_{min}$ fraction of samples. The remaining keywords are aggregated and constitute a keywords proposal pool $\\Psi$."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "In parallel to keywords selection, we aim at having a representation for the learned class, disentangled from the specific domain M is trained to. To do this, we work in the embedding space of a pre-trained text encoder. From the generated captions we obtain, for each class c, the embedding matrices $E_{correct}(c) \\in R^{|D_{correct}(c)| \\times Z}$ and $E_{misclass}(c) \\in R^{|D_{misclass}(c)| \\times Z}$, where and Z is the dimensionality of the embedding vectors.\nOur goal here is to calculate the embeddings $E^*(c) \\in R^{Z}$ semantically representing the learned representation of the class c. Not only: we would like to disentangle this from the features in common to all the classes learned from the model, given that M could be trained (and tested) to fit a specific domain, which in such specific case it would not constitute a bias but rather a feature. For this, we first calculate the average embedding E(c) for a specific learned class:\n$E(c) = \\frac{1}{2 [|D_{correct}(c)|\\cdot |D_{misclass}(c)|]} [\\sum_{i=1}^{|D_{correct}(c)|}E_{correct}(c) + \\sum_{j=1}^{|D_{misclass}(c)|}E_{misclass}(c)]$\nE(c) will now contain all the common features of the class c. However, it will also contain some information shared in the entire dataset (for example common characteristics of the different classes). From this, we can extract the embedding without the shared information from the dataset through:\n$E^* (c) = E(c) - \\frac{1}{C} \\sum_{i=1}^{C}E(i).$\nThe intuition of this approach originates from the arithmetic and semantic properties of natural language latent spaces [35]. To provide a realistic example, consider the task of gender recognition from facial pictures, in which the hair color is a spurious correlation. E(c) might contain features related to concepts such as \u201cblonde\u201d and \u201cface\u201d. As we are only interested in the former, computing $E^* (c)$ is an effective solution to filter out the shared information \u201cface\u201d."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Now, we are ready to compare the embedding of each keyword with $E^* (c)$ using the cosine similarity:\n$s(\\psi, c) = sim[\\psi_{embed}, E^* (c)], \\psi \\epsilon \\Psi,$\nplease note that we have dropped the \"train\" subscript from the D partitioning as this pipeline will work equally well also when using a validation set."}, {"title": "4 Empirical Results", "content": "We provide here the main results obtained. We highlight that, for visualization purposes, all the figures contain up to the top nine keywords identified by SaMyNa: the full results, along with the ablation study, are presented in the Supplementary material. For our experiments, we have employed an NVIDIA A5000 with 24GB of VRAM, except for the captioning step for which we have employed an NVIDIA A100 equipped with 80GB of VRAM. The source code, in the Supplementary material, will be open-sourced upon acceptance of the article."}, {"title": "4.1 Setup", "content": "Models tested. We tested the most popular architectures benchmarked from the debiasing literature: ResNet-18 for CelebA and BAR, and ResNet-50 for Waterbirds and ImageNet-A. All the models are pre-trained on ImageNet-1K, with architecture and weights provided by torchvision. On ImageNet-A, models are run only in inference, as we are interested in mining biases already existing in the original pre-trained models, while for all the other experiments we apply the training procedure described in Sec. 3.1. For this step, we train with a batch size of 128 and a learning rate of 0.001 for Waterbirds, as done in [24]; for CelebA, we use a batch size of 256 and a learning rate of 0.0001, following [8]. For both, we employ SGD with Nestorov momentum set to 0.9. Finally, for BAR, we employ a batch size of 256 and a learning rate of 0.001, with Adam as optimizer [36].\nCaptioning. For the captioning model, we used LLaVA-NeXT [37]# in its 34B configuration, quantized in 8 bits. The prompts we used to generate the captions are personalized for each dataset and their length is limited to 300 tokens. All the employed captions can be consulted in the Supplementary material.\nClass and keywords embedding. For this part, we used the Sentence-BERT model from the sentence-transformers [38] library to generate 384-dimensional embeddings of the captions. The minimum frequency for the keywords $f_{min}$ is 15%. For the correctly classified samples, we set k = 10 and $t_{sim}$ is set to 0.2."}, {"title": "4.2 Datasets", "content": "Before we present the results obtained with our bias naming pipeline, we briefly describe the datasets employed in this study: Waterbirds [24], CelebA [39], BAR [8], and ImageNet-A [40]."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Waterbirds. The barplot in Fig. 3 shows the candidate bias keywords for Waterbirds, alongside their relative similarity value. We can observe how the obtained keywords are mainly related to the background information (tree and forest for landbirds; sea and ocean for waterbirds). Most importantly, the top keywords display high similarity values, indicating a high correlation with their class targets. We deduce that the model suffers from a bias about image backgrounds, which indeed is the case for Waterbirds. It is also worth noticing how the top similarities differ among the two classes. We hypothesize two possible factors causing it: (i) model bias towards sea is stronger than the one towards tree, as it is constituted by simpler visual patterns, easier to learn for the network; (ii) the landbirds class has a much larger population, thus allowing for the bias on this class to be averaged over more instances than the waterbirds case.\nCelebA. In analyzing the possible biases of our vanilla model on CelebA (see Fig. 4), we find that the top-1 keyword for both classes represents a gender: male/man for class not blonde (with similarity \u2248 0.4), and woman for class blonde (similarity of 0.49). Additionally, we find among the blonde class, keyword terms typically associated with gender stereotypes, such as makeup and lipstick, with moderately high similarity. This suggests that the vanilla model is not only biased regarding the classification task but also incorporates unfair features, arising from societal biases reflected in the training data.\nBAR. Bar presents a more challenging situation, due to the complete absence of bias group annotations. Regardless, the output of our approach still provides insights about the biases captured by the model. In the training class climbing, scenes where the subject is ascending on rocks, are overly represented, and thus the vanilla model has wrongfully learned to rely on the presence of rocky backgrounds. This is reflected by the top three keywords in Fig. 5 (cliff, rock and rocks), all having similarities above 0.4. Similar considerations can be made on the other classes: pitch,"}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "To evaluate the capabilities of our method in describing potential biases at inference time, we design a dedicated experiment involving ImageNet-A. In particular, we are interested in finding specific model failures and extracting an interpretable set of keywords that can guide an expert practitioner to tackle them. With this aim, we first build an evaluation set as the union of the whole ImageNet-A dataset with the samples in the validation set of ImageNet-1K sharing the same 200 categories of ImageNet-A. Then, we run the model in inference over this dataset, collecting $D_{correct}$ and $D_{misclass}$ directly without any additional training. In this experiment, we are not interested in finding dataset-wise biases, but rather in assessing the behavior of our approach in specific and challenging real-world scenarios. Hence, we derive a specific case study from the systematic model confusion obtained from its predictions, which could hide the presence of a possible model bias. Another analysis describing more general DL diagnosing features is provided in the Supplementary Material, while here we limit the discussion to the model bias perspective."}, {"title": "5 Conclusion", "content": "In this work, we presented \"Say My Name\" (SaMyNa), a tool designed to identify and address biases within deep learning models semantically. Unlike similar methods that generate bias pseudo-labels without clear semantic information, SaMyNa offers a text-based pipeline that enhances the explainability of the bias extraction process from the model. Our approach, validated on well-known benchmarks, proved its effectiveness by both providing the keywords encoding the bias and assigning an interpretable score telling how much the model under analysis is biased to the found attribute. SaMyNa proposes itself not only as a post-hoc analysis tool: through its bias mining approach, it can determine the specific moment the model might be fitting a bias, for which there is in principle no need for a validation set to mine and name the bias. SaMyNa's ambition is to self-establish as a foundational tool in making deep learning models more transparent and fair, offering practical solutions for the scientific community and end-users alike."}, {"title": "Appendix", "content": null}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "Besides the study provided in Sec. 4.3.2 of the main paper, we present here another case study on the ImageNet-A dataset, comparing two other critical classes: nails and mushrooms. Indeed, also for this case, the tested ResNet-50 model presents a big error between these two classes: is there a bias involved? Running our SaMyNa (Fig. 8), we observe that indeed there is a big correlation towards certain concepts for the nails class; however, these hardly resemble biases, but rather features of the target class. Indeed, we can easily imagine that concepts like metal, frame or rusted can be easily associated with the target nails class. At this point, where is this big confusion arising from? The answer comes from a visual inspection of the samples, wherein multiple cases the shape factor of the two classes is extremely similar (both show a bulge on top and a thinner body underneath), making the classification task harder. In this case, we deduce that the model simply was unable to properly fit the two classes because of a lack of samples in the training set."}, {"title": "B Ablation Study", "content": null}, {"title": "B.1 Ablations on the Bias Mining step", "content": "In this section we provide visualizations on the output distributions on the Waterbirds target when training a ResNet-50 on Waterbirds in the same setup as described in Sec. 4.1. Fig. 9 proposes visualizations of the output distributions for the bias-target aligned samples in blue (waterbirds and sea landscape), and bias-target conflicting samples in orange (waterbirds and ground landscape), in three different moments of the training: in the early stages (at t = 1, on the left, where t is the training epoch), at the chosen bias extraction time t* (t = 6, at the center) and in the final stages (t = 10,"}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "on the right). We recall that the entire bias extraction process happens directly on the training set $D_{train}$ and does not require the employment of a validation/test set. We observe here that, at extraction time, there is an evident separation between bias-aligned and bias-conflicting samples, and we are able to confidently isolate the most biased among the conflicting samples (the orange distribution having a larger population below the random guess threshold). This does not hold in case the extraction time is delayed, with the bias-conflicting distribution not exhibiting a peak anymore."}, {"title": "B.2 Ablation on Bias Naming", "content": null}, {"title": "B.2.1 Ablation on Text Embedders", "content": null}, {"title": "B.2.2 Ablation on $f_{min}$", "content": "We propose here, in Tab. 3, the results we obtain for varying values for the hyperparameter $f_{min}$, i.e. the minimum frequency with which a keyword has to appear so that it can be considered as a possible output. What is possible to observe is that the higher $f_{min}$ the fewer and fewer keywords will be selected, and at some point, one class will be wrongfully marked as not holding a bias (with $f_{min}$ = 0.60). On the other hand, when not employing filtering at all ($f_{min}$ = 0), a lot of more fine-grained classes like coniferous or brownish arise. We find the chosen threshold ($f_{min}$ = 0.15) a fair compromise. As a sanity check, we also observe no change in the score for the remaining keywords."}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "We present here the ablation on k, that selects the cardinality of aligned and conflicting samples per class. Fig. 10 reports the study for 5 most occurring keywords in the cases under exam, while the full results are later reported in Table 12. In the general case, we observe that for lower values of k the similarity score is in general lower, evidencing that the information extraction process is less accurate due to a general lack of information (and in general variety). This trend is particularly evident in more generic keywords like forest, trees, and beach. Finer-grained keywords like foliage and waves show a more irregular trend due to the specific sample selection. Overall we find that a fair compromise between performance and complexity is given by the intermediate k = 10 for which some keywords like"}, {"title": "LOOKING AT MODEL DEBIASING THROUGH THE LENS OF ANOMALY DETECTION", "content": "In this ablation study, we provide an example of the keywords resulting from SaMyNa when not employing any thresholding on the minimum similarity values for the found keywords. In Tab. 4, we provide the full output only for landbirds, due to the excessive length of the unfiltered output. We also provide the full list of the resulting keywords as a text file, available in the supplementary materials zip archive (keywords_ablation_tmin.txt). From an analysis of the emerging keywords, we observe three interesting intervals of values. High similarity values indicate those concepts that correlate well specifically with the learned class (filtered from the target class) and are those presented in the paper. Concepts whose similarity is close to zero are not correlated: indeed, we can find keywords like long, muted, and given that are neutral concepts. Interestingly, we can also identify concepts like background and environment that are super-classes of the two biases. This confirms that SaMyNa works properly since it puts itself in the best spot to best discriminate the two biases. Finally, the third region is for negatively correlated concepts (anti-correlated), where we easily find the concepts correlated with the other class (waterbirds) given that we are in a binary classification task."}, {"title": "C SaMyNa on an unbiased dataset", "content": "We propose here a study on a virtually balanced version of the Waterbirds dataset. Fig. 11 reports the results in a graphical form, while Tab. 11 in a later section reports the numerical values. While we should have a-priori removed the bias by balancing the dataset, resulting in a general, massive reduction of the similarity scores, we still observe some mild correlations arising, especially for the waterbirds class. Specifically, besides the seagull keyword evidencing a (potential) higher presence of seagulls in the data split, we still see some concepts like coastal and sea correlating with the learned class. This is expected: given that these features are easy to learn, the model still captures them, but the low similarity score indicates that it does not heavily rely on them. This shows that, despite balancing the dataset, some biased features can still permeate through the model, depending on how easy they are to capture. This further motivates our work, focusing on model debiasing rather than dataset debiasing. The presence of plumage of feathers keywords"}, {"title": "D Bias discovery on Vision Transformer models", "content": "We present here a study on two popular pre-trained Vision Transformers architectures: ViTb-16 and Swin-V2. Tab. 5 reports the outcome of SaMyNa for the classes crayfish, rhinoceros beetle, stick insect and cockroach of ImageNet-A. Despite the potential of generalization for these architectures, we are still able to observe, although with different magnitudes, some biases. Regarding ViTb-16, the class crayfish is still associated with meal and cockroach is associated with floor: interestingly the impact of hand for the stick insect is heavily reduced compared to the ResNet model, while with the introduction of sliding windows it goes back up for Swin-V2. In general, we notice that these architectures, although still suffering from bias, are less prone to it, probably due to finer training enhanced by larger parametrization combined with the self-attention mechanism they embody."}, {"title": "E Outputs of SaMyNa", "content": "In this section, we provide the detailed output of our bias naming process for the experiments described in the main paper. We report the obtained keywords alongside their associated cosine similarity, in decreasing order of value, for"}]}