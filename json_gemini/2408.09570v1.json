{"title": "SAY MY NAME: A MODEL'S BIAS DISCOVERY FRAMEWORK", "authors": ["Massimiliano Ciranni", "Luca Molinaro", "Carlo Alberto Barbano", "Attilio Fiandrotti", "Vittorio Murino", "Vito Paolo Pastore", "Enzo Tartaglione"], "abstract": "In the last few years, due to the broad applicability of deep learning to downstream tasks and end-to-end training capabilities, increasingly more concerns about potential biases to specific, non-representative patterns have been raised. Many works focusing on unsupervised debiasing usually leverage the tendency of deep models to learn \u201ceasier\u201d samples, for example by clustering the latent space to obtain bias pseudo-labels. However, the interpretation of such pseudo-labels is not trivial, especially for a non-expert end user, as it does not provide semantic information about the bias features. To address this issue, we introduce \"Say My Name\u201d (SaMyNa), the first tool to identify biases within deep models semantically. Unlike existing methods, our approach focuses on biases learned by the model. Our text-based pipeline enhances explainability and supports debiasing efforts: applicable during either training or post-hoc validation, our method can disentangle task-related information and proposes itself as a tool to analyze biases. Evaluation on traditional benchmarks demonstrates its effectiveness in detecting biases and even disclaiming them, showcasing its broad applicability for model diagnosis.", "sections": [{"title": "1 Introduction", "content": "In the past decade, advances in technology have made it possible to widely use deep learning (DL) techniques, greatly impacting the computer vision field. By allowing systems to be trained end-to-end, DL offers potentially fast model deployability to solve complex problems, leading to fast progress and changing how we perceive and analyze visual information. Today, deep learning is applied in various real-world scenarios, such as self-driving cars [1], medical imaging [2], and augmented reality [3].\nThese huge possibilities hinder potential pitfalls. One challenge to face when deploying DL solutions lies in guaranteeing that the model does not over-rely on specific patterns that are non-representative of the real-world data distribution [4, 5]. This is key for safety, fairness, and ethics [6]. To provide a simple example, when deploying solutions in autonomous driving, simple tasks like pedestrian detection might be implicitly solved by associating specific background elements (like sidewalks or pedestrian crossings) with the presence of pedestrians. This reliance on environmental cues can act as shortcuts for the network [7], leading to poor performance when the context changes, such as when a pedestrian is crossing a"}, {"title": "2 Related Works", "content": "The problem of bias and model debiasing has been widely explored in recent years, within three main frameworks, differing from how or if bias knowledge is explored for mitigating model dependency on bias: supervised, bias-tailored, and unsupervised.\nSupervised Debiasing. Supervised methods require explicit knowledge of the bias, generally in the form of labels, indicating whether a sample presents a certain bias or not. One of the most typical approaches consists of training an explicit bias classifier, trained on the same representation space as the target classifier, in an adversarial way, forcing the encoder to extract unbiased representations [22, 23]. Alternatively, bias labels can be exploited to identify pre-defined groups, training a model to minimize the worst-case training loss, thus pushing the model towards learning biased samples [24]. Another possibility is represented by regularization terms, which aim at achieving invariance to bias features [11, 25].\nBias-Tailored Debiasing. Bias-tailored approaches usually rely on some kind of knowledge about the bias nature. For example, if the bias is textural, then custom architectures can be designed to be more sensitive to textural information. For example, [26] propose ReBias, where a custom texture bias-capturing model is designed using 1x1 convolutions. A similar approach is followed by [12], where a BagNet-18 [27] is used as a bias-capturing model."}, {"title": "3 Method", "content": "In this section, we present our proposed method SaMyNa to identify potential spurious correlations learned by the model under analysis (Sec. 3.2). Our method can be plugged to perform model diagnosis either at training time or at test: for the first case, we also present an approach to identify at which point to mine a potential bias for the target model (Sec. 3.1)."}, {"title": "3.1 Mining Model's Biases", "content": "Consider a supervised classification setup (having $C$ target classes), where we learn from a dataset $\\mathcal{D}_{train}$ containing $N$ input samples $(\\mathbf{x}_1,\\dots,\\mathbf{x}_N) \\in \\mathcal{X}$, each with a corresponding ground truth label $(\\hat{y}_1, \\dots, \\hat{y}_N) \\in \\mathcal{Y}$. A deep neural network $M$, trained for $t$ iterations, produces an output distribution $\\mathbf{y}_{t,n} \\in \\mathbb{R}^C$ over the $C$ classes, for each input $\\mathbf{x}_n$ (typically, the activation of the last layer is a softmax). The network is trained to match the ground truth label $\\hat{y}_n$ by minimizing a loss function such as cross-entropy.\nIf any bias is present in the training set, however, the learning process could drive the model towards the selection of spurious features [24, 8, 26], resulting in misclassification errors. Recent findings show that it is possible to identify the moment when the model best fits the bias in the training set [15]. We will formulate here the problem of identifying, at training time, when the trained model maximally fits a potential bias.\nWe say that $M$ misclassifies the $n$-th sample at the $t$-th learning iteration if $\\hat{y}_n \\neq \\arg\\max(\\mathbf{y}_{t.n})$. Focusing on this example, we know that by minimizing the loss function we aim at increasing the value of the $\\hat{y}_n$-th component $\\mathbf{y}_{t.n}(\\hat{y}_n)$ (while decreasing all the others). Inspired by the Hinge loss function [33], we can define a per-sample distance metric telling us how far the $n$-th sample is from being correctly classified:\n$$ d_{t,n} = \\begin{cases} \\max(\\mathbf{y}_{t,n}) - \\mathbf{y}_{t.n} (\\hat{y}_n) & \\text{if } \\arg\\max(\\mathbf{y}_{t,n}) \\neq \\hat{y}_n \\\\ 0 & \\text{otherwise}. \\end{cases} $$"}, {"title": "3.2 Bias Naming", "content": "We present here SaMyNa, our bias naming approach starting from a trained model $M$ from which we aim to run our bias naming tool. Fig. 2 proposes an overview of the main pipeline we used, consisting of the following steps:\n1. Samples subset selection. Given that the objective of our proposed method is to identify biases learned by $M$, we are allowed to propose a subset of most representative samples for a given target class (Sec. 3.2.1).\n2. Samples captioning. Once the samples are selected, a multimodal LLM captioning tool is used to extract a textual description of each sample (Sec. 3.2.2).\n3. Keywords selection. Starting from the computed captions, we mine keywords in common from the textual description of the samples within the same learned class (Sec. 3.2.3).\n4. Learned class embedding. Starting from a textual description of the samples, we can extract the shared information between the correctly classified samples and the incorrectly classified ones: this will constitute the embedding for a potential bias (Sec. 3.2.4).\n5. Keywords ranking. The embedding of the learned class is compared with the embedding of recurrent keywords in the captions and we get a ranking for the keywords most aligned with the learned class (Sec. 3.2.5)."}, {"title": "3.2.1 Samples subset selection", "content": "Given $M$, for a given target class $c$, we extract the pool of correctly classified samples $\\mathcal{D}_{correct}(c)$ and samples misclassified as $c \\in \\mathcal{D}_{misclass}(c)$.\u2020 Provided that $M$ clusters both $\\mathcal{D}_{correct}(c)$ and $\\mathcal{D}_{misclass}(c)$ together, our hypothesis is that these two share a common set of features, behind which we might find a bias. In the typical deployment scenario, the correctly classified examples are abundant, and, for instance, $M$ projects them in a very narrow neighborhood of its latent space. We build on top of this observation and run a k-medoid algorithm to reduce the cardinality of correctly classified samples. Our long-range objective will be indeed to capture the set of features that are correctly learned by the model, and k-medoids is a natural choice to have a good coverage of the latent space for $M$."}, {"title": "3.2.2 Samples captioning", "content": "At this point, we will generate captions from the selected samples. To do this, we use a pre-trained multimodal large language model that takes as input both a prompt and an image. The choice of the prompt is kept generic and asks to generate a textual description of the content of the image, providing some context of the target task. Through all our experiments we will use a large-scale captioner, given that biases might hide in very specific characteristics of the provided images. Images are also preprocessed to ensure they meet the requirements of the model, such as size."}, {"title": "3.2.3 Keywords selection", "content": "From the captions obtained in the previous step (Sec. 3.2.2), we select recurrent keywords. First, we perform some NLP standard processing, including for example stop-words removal. Each caption is word-level tokenized [34], and every token is considered a potential keyword. Then, we count the frequencies of the obtained keywords within the same class $c$. Lastly, we filter out the keywords appearing in the captions of less than the $f_{min}$ fraction of samples. The remaining keywords are aggregated and constitute a keywords proposal pool $\\Psi$."}, {"title": "3.2.4 Learned class embedding", "content": "In parallel to keywords selection, we aim at having a representation for the learned class, disentangled from the specific domain $M$ is trained to. To do this, we work in the embedding space of a pre-trained text encoder. From the generated captions we obtain, for each class $c$, the embedding matrices $\\mathcal{E}_{correct}(c) \\in \\mathbb{R}^{|\\mathcal{D}_{correct}(c)| \\times Z}$ and $\\mathcal{E}_{misclass}(c) \\in \\mathbb{R}^{|\\mathcal{D}_{misclass}(c)| \\times Z}$, where and $Z$ is the dimensionality of the embedding vectors.\nOur goal here is to calculate the embeddings $\\mathcal{E}^*(c) \\in \\mathbb{R}^Z$ semantically representing the learned representation of the class $c$. Not only: we would like to disentangle this from the features in common to all the classes learned from the model, given that $M$ could be trained (and tested) to fit a specific domain, which in such specific case it would not constitute a bias but rather a feature. For this, we first calculate the average embedding $E(c)$ for a specific learned class:\n$$ E(c) = \\frac{\\sum_{i=1}^{|\\mathcal{D}_{correct}(c)|}\\mathcal{E}_{correct}(c) + \\sum_{j=1}^{|\\mathcal{D}_{misclass}(c)|}\\mathcal{E}_{misclass}(c)}{2 [|\\mathcal{D}_{correct}(c)|\\cdot |\\mathcal{D}_{misclass}(c)|]} $$\n$E(c)$ will now contain all the common features of the class $c$. However, it will also contain some information shared in the entire dataset (for example common characteristics of the different classes). From this, we can extract the embedding without the shared information from the dataset through:\n$$ E^* (c) = E(c) - \\frac{1}{C} \\sum_{i=1}^C E(i). $$\nThe intuition of this approach originates from the arithmetic and semantic properties of natural language latent spaces [35]. To provide a realistic example, consider the task of gender recognition from facial pictures, in which the hair color is a spurious correlation. $E(c)$ might contain features related to concepts such as \u201cblonde\u201d and \u201cface\u201d. As we are only interested in the former, computing $E^* (c)$ is an effective solution to filter out the shared information \u201cface\u201d."}, {"title": "3.2.5 Keywords ranking", "content": "Now, we are ready to compare the embedding of each keyword with $E^* (c)$ using the cosine similarity:\n$$ s(\\psi, c) = \\text{sim}[\\psi_{embed}, E^* (c)], \\quad \\psi \\in \\Psi, $$"}, {"title": "4 Empirical Results", "content": "We provide here the main results obtained. We highlight that, for visualization purposes, all the figures contain up to the top nine keywords identified by SaMyNa: the full results, along with the ablation study, are presented in the Supplementary material. For our experiments, we have employed an NVIDIA A5000 with 24GB of VRAM, except for the captioning step for which we have employed an NVIDIA A100 equipped with 80GB of VRAM. The source code, in the Supplementary material, will be open-sourced upon acceptance of the article."}, {"title": "4.1 Setup", "content": "Models tested. We tested the most popular architectures benchmarked from the debiasing literature: ResNet-18 for CelebA and BAR, and ResNet-50 for Waterbirds and ImageNet-A. All the models are pre-trained on ImageNet-1K, with architecture and weights provided by torchvision. On ImageNet-A, models are run only in inference, as we are interested in mining biases already existing in the original pre-trained models, while for all the other experiments we apply the training procedure described in Sec. 3.1. For this step, we train with a batch size of 128 and a learning rate of 0.001 for Waterbirds, as done in [24]; for CelebA, we use a batch size of 256 and a learning rate of 0.0001, following [8]. For both, we employ SGD with Nestorov momentum set to 0.9. Finally, for BAR, we employ a batch size of 256 and a learning rate of 0.001, with Adam as optimizer [36].\nCaptioning. For the captioning model, we used LLaVA-NeXT [37]# in its 34B configuration, quantized in 8 bits. The prompts we used to generate the captions are personalized for each dataset and their length is limited to 300 tokens. All the employed captions can be consulted in the Supplementary material.\nClass and keywords embedding. For this part, we used the Sentence-BERT model from the sentence-transformers [38] library to generate 384-dimensional embeddings of the captions. The minimum frequency for the keywords $f_{min}$ is 15%. For the correctly classified samples, we set $k = 10$ and $t_{sim}$ is set to 0.2."}, {"title": "4.2 Datasets", "content": "Before we present the results obtained with our bias naming pipeline, we briefly describe the datasets employed in this study: Waterbirds [24], CelebA [39], BAR [8], and ImageNet-A [40].\nWaterbirds. Waterbirds is an image dataset introduced in [24] to test optimization methods able to be robust against"}, {"title": "4.3 Main results", "content": "4.3.1 Naming the bias at training time\nWe begin by discussing the results of our bias naming pipeline when applied in a model's training process on Waterbirds, CelebA, and BAR.\nWaterbirds. The barplot in Fig. 3 shows the candidate bias keywords for Waterbirds, alongside their relative similarity value. We can observe how the obtained keywords are mainly related to the background information (tree and forest for landbirds; sea and ocean for waterbirds). Most importantly, the top keywords display high similarity values, indicating a high correlation with their class targets. We deduce that the model suffers from a bias about image backgrounds, which indeed is the case for Waterbirds. It is also worth noticing how the top similarities differ among the two classes. We hypothesize two possible factors causing it: (i) model bias towards sea is stronger than the one towards tree, as it is constituted by simpler visual patterns, easier to learn for the network; (ii) the landbirds class has a much larger population, thus allowing for the bias on this class to be averaged over more instances than the waterbirds case.\nCelebA. In analyzing the possible biases of our vanilla model on CelebA (see Fig. 4), we find that the top-1 keyword for both classes represents a gender: male/man for class not blonde (with similarity \u2248 0.4), and woman for class blonde (similarity of 0.49). Additionally, we find among the blonde class, keyword terms typically associated with gender stereotypes, such as makeup and lipstick, with moderately high similarity. This suggests that the vanilla model is not only biased regarding the classification task but also incorporates unfair features, arising from societal biases reflected in the training data.\nBAR. Bar presents a more challenging situation, due to the complete absence of bias group annotations. Regardless, the output of our approach still provides insights about the biases captured by the model. In the training class climbing, scenes where the subject is ascending on rocks, are overly represented, and thus the vanilla model has wrongfully learned to rely on the presence of rocky backgrounds. This is reflected by the top three keywords in Fig. 5 (cliff, rock and rocks), all having similarities above 0.4. Similar considerations can be made on the other classes: pitch,"}, {"title": "4.3.2 Naming the bias at inference time", "content": "To evaluate the capabilities of our method in describing potential biases at inference time, we design a dedicated experiment involving ImageNet-A. In particular, we are interested in finding specific model failures and extracting an interpretable set of keywords that can guide an expert practitioner to tackle them. With this aim, we first build an evaluation set as the union of the whole ImageNet-A dataset with the samples in the validation set of ImageNet-1K sharing the same 200 categories of ImageNet-A. Then, we run the model in inference over this dataset, collecting $\\mathcal{D}_{correct}$ and $\\mathcal{D}_{misclass}$ directly without any additional training. In this experiment, we are not interested in finding dataset-wise biases, but rather in assessing the behavior of our approach in specific and challenging real-world scenarios. Hence, we derive a specific case study from the systematic model confusion obtained from its predictions, which could hide the presence of a possible model bias. Another analysis describing more general DL diagnosing features is provided in the Supplementary Material, while here we limit the discussion to the model bias perspective."}, {"title": "4.4 Testing against the discovered bias", "content": "Starting from the obtained keywords, which are extracted to describe potential bias affecting the classification model, we desire to validate if the groups suggested by SaMyNa are indeed the ones impacting the generalization capabilities of our network. To this end, we evaluate the biased vanilla model on the test set of Waterbirds and CelebA, which provide extensive annotations, and on the same dataset constructed for the experiment on ImageNet-A. BAR is not included in this experiment due to the absence of bias group annotations. The conflicting set contains all the bias-misaligned samples: hence, we consider all other samples as aligned. As shown in Tab. 1, the biased network shows an important drop in accuracy for conflicting samples if compared to the aligned ones, confirming that the model was relying on spurious correlations in the training data that were correctly reflected by the keywords and similarities extracted by SaMyNa."}, {"title": "4.5 Visual feedback", "content": "For visualization purposes only, SaMyNa can leverage a visual encoder instead of a text encoder to identify the part of the image where the potential bias is located. To do this, we adopt the same strategy described in Sec. 3.2.4 to generate the learned class embeddings $E^* (c)$ directly using image embeddings generated with CLIP's visual encoder [42]. We can then compare $E^*(c)$ with the embeddings of patches from the image we want to analyze."}, {"title": "5 Conclusion", "content": "In this work, we presented \"Say My Name\" (SaMyNa), a tool designed to identify and address biases within deep learning models semantically. Unlike similar methods that generate bias pseudo-labels without clear semantic information, SaMyNa offers a text-based pipeline that enhances the explainability of the bias extraction process from the model. Our approach, validated on well-known benchmarks, proved its effectiveness by both providing the keywords encoding the bias and assigning an interpretable score telling how much the model under analysis is biased to the found attribute. SaMyNa proposes itself not only as a post-hoc analysis tool: through its bias mining approach, it can determine the specific moment the model might be fitting a bias, for which there is in principle no need for a validation set to mine and name the bias. SaMyNa's ambition is to self-establish as a foundational tool in making deep learning models more transparent and fair, offering practical solutions for the scientific community and end-users alike."}]}