{"title": "Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms", "authors": ["Sheila Schoepp", "Mehran Taghian", "Shotaro Miwa", "Yoshihiro Mitsuka", "Shadan Golestan", "Osmar R. Za\u00efane"], "abstract": "Industry is rapidly moving towards fully autonomous and interconnected systems that can detect and adapt to changing conditions, including machine hardware faults. Traditional methods for adding hardware fault tolerance to machines involve duplicating components and algorithmically reconfiguring a machine's processes when a fault occurs. However, the growing interest in reinforcement learning-based robotic control offers a new perspective on achieving hardware fault tolerance. However, limited research has explored the potential of these approaches for hardware fault tolerance in machines. This paper investigates the potential of two state-of-the-art reinforcement learning algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), to enhance hardware fault tolerance into machines. We assess the performance of these algorithms in two OpenAI Gym simulated environments, Ant-v2 and FetchReach-v1. Robot models in these environments are subjected to six simulated hardware faults. Additionally, we conduct an ablation study to determine the optimal method for transferring an agent's knowledge, acquired through learning in a normal (pre-fault) environment, to a (post-)fault environment in a continual learning setting. Our results demonstrate that reinforcement learning-based approaches can enhance hardware fault tolerance in simulated machines, with adaptation occurring within minutes. Specifically, PPO exhibits the fastest adaptation when retaining the knowledge within its models, while SAC performs best when discarding all acquired knowledge. Overall, this study highlights the potential of reinforcement learning-based approaches, such as PPO and SAC, for hardware fault tolerance in machines. These findings pave the way for the development of robust and adaptive machines capable of effectively operating in real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "Automation is revolutionizing industries, enhancing productivity and efficiency (Haleem et al., 2021; Javaid et al., 2021; Vaidya et al., 2018). Automation is achieved by integrating machines equipped with sensors, such as robots, with Artificial Intelligence (AI)-powered analytical systems (agents), enabling real-time data collection and analysis. These agents make the critical decisions necessary for automation, including facilitating the real-time identification and diagnosing of hardware faults in machines through fault detection and fault diagnosis, respectively (Riazi et al., 2019; Zhang et al., 2017; Samanta and Nataraj, 2009). While fault detection and fault diagnosis are essential processes that inform on faults, they do not address adaptation to hardware faults. To succeed in real-world deployments, it is crucial for machines to adapt to unexpected events, such as hardware faults (Chen et al., 2023; Schrum and Gombolay, 2019). As such, enhancing hardware fault tolerance in machines is imperative.\nAn established method of enhancing hardware fault tolerance in machines is through redundancy, where critical hardware components are duplicated to mitigate the risk of failure (Guiochet et al., 2017; Visinsky et al., 1994, 1991). However, this approach has significant drawbacks, including increased machine size, weight, power consumption, and financial costs (Dubrova, 2013). Moreover, retrofitting existing machines with redundant components is often impossible. Therefore, exploring alternative approaches that do not rely on redundancy provides a significant advantage in enhancing hardware fault tolerance in machines.\nTaking inspiration from nature, one can imagine an agent adapting a machine's behaviour in response to a hardware fault. Animals, for example, demonstrate an extraordinary ability to adapt. Animals can modify their gait in the presence of an injured limb, using non-injured limbs in a compensatory manner (Fuchs et al., 2014; Jarvis et al., 2013). An established approach to incorporate adaptable, compensatory behaviours into machines is through algorithmic reconfiguration. In algorithmic reconfiguration, an agent adjusts the underlying algorithms that govern hardware usage within a machine, enabling adaptation to changes in hardware conditions (Guiochet et al., 2017; Visinsky et al., 1994, 1991). Algorithmic reconfiguration may entail altering the algorithm's hyperparameter settings, adjusting the algorithm's model architecture, or even switching to an entirely different algorithm.\nIn essence, algorithmic reconfiguration is closely linked to continual learning. With continual learning, when a machine faces evolving hardware conditions throughout its lifetime, it must adapt to succeed in the changed conditions. Adaptation in continual learning involves leveraging the continuous flow of data to modify aspects of the algorithm, such as the models parameters and/or the storage contents. To accelerate the adaptation process, previously acquired knowledge can be strategically transferred to the new task (Chen and Liu, 2018).\nIn nature, adaptation to physical changes is often achieved through a trial-and-error approach, enhanced by knowledge acquired from past experiences. Continual Reinforcement Learning (CRL) (Abel et al., 2024; Sutton and Barto, 2018; Xu and Zhu, 2018) is an ideal strategy for an agent tasked with adapting a machine's behaviour to unexpected events, such as hardware faults. CRL typically entails an agent continuously interacting with a non-stationary environment, learning from feedback (trial-and-error), and strategically transferring acquired knowledge.\nDespite CRL being a valuable strategy for enhancing hardware fault tolerance in machines, it remains relatively unexplored. In this study, we examine how CRL can enhance hardware fault tolerance in simulated machines. Specifically, we study the impact of retaining or discarding components of the agent's acquired knowledge."}, {"title": "2. Background and Related Work", "content": "This section serves as a foundation for understanding the research context and examines the existing literature relevant to our study. In Section 2.1, we provide a comprehensive overview of key concepts and techniques in RL. In Section 2.2, we review notable works to provide a comprehensive understanding of the research landscape. By presenting this background information and discussing relevant works, we aim to establish a solid foundation for our research and highlight the unique contributions of our study in this field."}, {"title": "2.1. Reinforcement Learning", "content": "RL is a subfield of machine learning that enables agents to autonomously learn by making sequential decisions through interactions with their environment. Sequential decision-making problems are typically modelled as a Markov Decision Process (MDP). An MDP is defined by the tuple (S, A, R, P, \u03b3), where S denotes the state space; A denotes the action space; R : S \u00d7 A \u2192 R is the reward function; P(st+1|st, at) represents the MDP dynamics, which is the probability of transitioning to state st+1 after taking action at in state st, where st, st+1 \u2208 S, at \u2208 A; and \u03b3 \u2208 [0,1] denotes the discount rate (Sutton and Barto, 2018).\nInteraction between the agent and the environment occurs over a series of discrete time steps, represented by t = 0, 1, 2, ..., H, where H is the horizon, which may be infinite. During each interaction, the environment provides the agent with information on its current state st. Guided by its policy, represented as the function \u03c0 : S \u2192 A, the agent selects and executes an action at \u2208 A(st) for the next time step. Subsequently, the agent receives feedback in the form of a numerical reward rt+1 \u2208 R, and the environment updates the agent on its new state st+1. The sequence of states and actions forms a trajectory, denoted by \u03c4. The primary objective of the agent is to maximize the expected discounted cumulative reward received in a trajectory \u03c4, known as the expected discounted return, denoted by J(\u03c0) = \u0395\u03c4\u03c0 [\u2211t=0H \u03b3tR(st, at)]. This objective guides the agent's learning, as it continually seeks to select actions that result in higher rewards. The optimization problem that aims to maximize the objective function J(\u03c0) can be expressed as:\n\u03c0* = arg max\u03c0 J(\u03c0), (1)\nwhere \u03c0* is the optimal policy."}, {"title": "2.1.1. Policy Gradient Methods", "content": "Policy gradient methods optimize the policy directly, as described by Equation 1. The policy is represented as a parameterized function, i.e., a neural network, denoted as \u03c0\u03bf(\u00b7|s), where \u03b8\u2208 Rd represents the policy's d-dimensional parameters. In policy gradient methods, the policy parameters are evaluated using a performance measure and are iteratively updated using gradient ascent. This process improves the policy by identifying parameter values that yield higher expected returns."}, {"title": "2.1.2. Proximal Policy Optimization (PPO)", "content": "PPO is an on-policy algorithm designed to limit the magnitude of policy updates, thus ensuring stable and gradual changes during training (Schulman et al., 2017). PPO has proven to be an effective algorithm for robotic applications (Kuo and Chen, 2023).\nA PPO agent collects experiences \u2014 namely, a collection of trajectories through interactions with the environment, while following the current policy. The collected experiences are stored in the agent's memory M. Once M is full, these trajectories are used to compute the loss using a surrogate objective function, LCLIP (\u03b8), which approximates policy improvement and guides the policy update from the current policy \u03c0\u03b8\u03bf\u03b9\u03b1 to an updated candidate policy \u03c0\u03bf. \u03a4\u03bf prevent drastic policy changes that could destabilize learning, LCLIP (\u03b8) incorporates a clipping mechanism, constraining the updated candidate policy to be closely aligned with the current policy. The parameters \u03b8 are optimized over one or more epochs, using mini-batches of experiences sampled from M, according to the following equation:\n\u03b8* = arg max\u03b8 LCLIP (\u03b8) (2)\nAfter completing the update, the experiences stored in M are discarded. The algorithm then collects new experiences using the updated policy and stores them in M. This iterative process allows PPO to refine the policy gradually and converge towards an optimal or near-optimal solution. We denote the solution at convergence, in the normal environment, as OPPO. For more details on PPO, we refer readers to the original paper (Schulman et al., 2017)."}, {"title": "2.1.3. Soft Actor-Critic (SAC)", "content": "SAC is an off-policy algorithm that is well-suited for real-world problems, owing to its low sample complexity and its robustness to various hyperparameter settings (Haarnoja et al., 2018, 2019). A distinguishing feature of SAC is its use of an entropy regularization term in its objective function, denoted as J'(\u03c0\u03bf(\u00b7|s)). The entropy regularization term encourages exploration by favouring stochastic policies. A temperature coefficient for the entropy regularization term adjusts the balance between entropy and expected returns. SAC incorporates an automatic adjustment mechanism for this coefficient, achieved by imposing a lower-bound average policy entropy constraint during the optimization process. This mechanism enables SAC to strike a delicate balance between exploration and exploitation, allowing the agent to learn a policy that effectively adapts to diverse environmental conditions while maximizing expected returns.\nTo facilitate the learning process, SAC uses a replay buffer, denoted as B, to retain past experiences. Once B reaches a specified minimum capacity, batches of experiences specifically, past agent-environment interactions are uniformly sampled from B at each time step and used to update the parameters. The parameters are optimized using:\n\u03b8* = arg max\u03b8 J'(\u03c0\u03bf(\u00b7|\u03c2)) (3)\nFollowing each update, a new experience is generated under the updated policy and is added to the replay buffer B, replacing the oldest stored experience.\nIt is worth noting that each experience stored in the replay buffer is generated under a different policy. The ability to reuse past experiences, even when generated under different policies, is what enables SAC to be highly sample-efficient, requiring fewer interactions with the environment compared to on-policy algorithms, such as PPO.\nWe refer to the SAC solution at convergence, in the normal environment, as OSAC. For more details on SAC, we refer readers to the original paper (Haarnoja et al., 2018)."}, {"title": "2.2. Related Works", "content": ""}, {"title": "2.2.1. Collection of Pre-Trained Policies", "content": "The algorithm developed by Raileanu et al. (2020), known as Policy-Dynamics Value Functions (PD-VF), demonstrated rapid adaptability to previously unseen environments with changed dynamics. The methodology used in this work involved training a set of policies correspondingly with trajectories collected within a set of environments, each characterized by its unique dynamics. At test time, when faced with an unseen environment, each policy from the set was evaluated in the test environment for a small number of time steps. The policy that exhibited the highest performance in the test environment was considered to be the adapted policy. To evaluate the effectiveness of their approach, Raileanu et al. (2020) applied PD-VF to four Ant-Legs environments, where modifications were made to the length of the Ant-v2 robot model's linkages.\nA limitation of this work is that, unlike online learning algorithms such as SAC and PPO, PD-VF does not learn a policy in real-time. Rather, it selects the best-performing policy from a collection of pre-trained policies. This approach deprives improvement of a transferred policy through interaction with an environment. In contrast, SAC and PPO are online learning algorithms capable of adapting a transferred policy. Furthermore, a competing algorithm, PPOall, was trained in a multi-task setting, where negative transfer can hinder performance if the training set tasks are too dissimilar (Rosenstein et al., 2005), rendering this method of limited practical value in fault adaptation domain, where fault tasks may be drastically different from normal ones. In our study, we train PPO on a single task, demonstrating that the learned, high-performing policy is both transferable and quickly adaptable to unseen tasks with similar structure, and outperforming the reported performance of PD-VF on comparable tasks."}, {"title": "2.2.2. Meta-Reinforcement Learning", "content": "Nagabandi et al. (2018) introduced two variants of meta-RL algorithms, namely a Gradient-Based Adaptive Learner (GrBAL) and a Recurrence-Based Adaptive Learner (ReBAL), which demonstrated superior speed and sample efficiency in adapting to hardware faults in robots. These two algorithms learn a model of the environment dynamics, enabling rapid adaptation to hardware faults with as little as 1.5 - 3 hours of real-world experience. Nevertheless, it is worth noting that a third model-based RL algorithm, not belonging to the meta-RL class, performed comparably, underscoring the sample efficiency of model-based approaches. The evaluation of the algorithms was conducted on an Ant: crippled leg task, which involved damaging a single leg of the Ant-v2 robot model.\nA drawback of the algorithms proposed by Nagabandi et al. (2018) is that machine faults can arise unexpectedly, exhibiting diverse and unpredictable characteristics. Meta-RL algorithms, such as GrBAL and ReBAL, require prior knowledge of the fault type to define a task distribution customized for a particular fault, which limits their practicality. In contrast, PPO and SAC have the advantage of adapting without any prior knowledge of the fault type. This flexibility enables them to effectively handle a wide range of faults without the need for specialized pre-training. Furthermore, we show that PPO and SAC outperform GrBAL and ReBAL on comparable tasks, emphasizing the tendency of model-based approaches to achieve suboptimal asymptotic performance when compared to model-free approaches."}, {"title": "2.2.3. Pre-Processing In Simulation", "content": "Cully et al. (2014) used a robot simulator to construct a low-dimensional behaviour performance map. In the event of a fault, a real-world robot would take a trial-and-error approach to fault adaptation, iteratively selecting and executing the projected best-performing behaviour from the behaviour performance map, and subsequently updating the map with the real-world performance. This iterative procedure would continue until a high-performing behaviour, surpassing a predefined threshold, was identified. Notably, this approach demonstrated significant success, achieving recovery within minutes in a low-dimensional parameter search space.\nThis approach, however, had certain limitations. Firstly, it relied on the availability of a simulator to generate a behaviour-performance map, which limited its practicality for robots lacking simulators. Furthermore, this method faced challenges when confronted with a high-dimensional parameter search space. Our study addresses these limitations by offering broad applicability to all robots without the need for prior computation in simulation. Furthermore, we demonstrate the effectiveness of adaptation using the PPO and SAC algorithms, even within high-dimensional parameter spaces, by utilizing raw sensor output data to select continuous actions."}, {"title": "2.2.4. A Policy for Each Actuator", "content": "Huang et al. (2020) introduced a decentralized approach to learning called Shared Modular Policies (SMP), where each component of a robot, e.g., actuator, was treated as an individual module. Each module had its own policy, which was instantiated from a global policy (i.e., a single, reusable neural network). The input to each policy included raw data from the module's local sensors. A system of message passing allowed for communication between modules. SMP was trained in a multi-task learning setting, where each task involved a robot model with a unique arrangement of linkages and/or joints. Huang et al. (2020) evaluated SMP in the OpenAI Gym environments Walker2D-v2, Humanoid-v2, Hopper-v2, and HalfCheetah-v2. Although SMP was shown to perform well on simple tasks, it faced difficulties on more complex tasks (e.g., Humanoid-v2). The performance of SMP was compared to a baseline, where a centralized policy was trained using Twin Delayed DDPG (TD3) (Fujimoto et al., 2018) in a multi-task setting, highlighting the effectiveness of SMP. Notably, previous work has shown that training a centralized policy in a multi-task setting, as was done with TD3, can lead to performance issues due to negative transfer from vastly different tasks (Rosenstein et al., 2005).\nIn our study, we employ a centralized approach to learning, using a single policy to control all actuators in the robot model, within a single-task setting. This approach to learning demonstrates high performance within a high-dimensional state space effectively capturing interactions and interdependencies among actuators, and is shown to be transferable and quickly adaptable to new, unseen environments. Unlike SMP, our transferred policy does not require significant pre-training in a multi-task setting. Instead, it is trained using experiences collected under normal conditions, emphasizing its practicality and efficiency."}, {"title": "3. Methodology", "content": "In this section, we describe the experimental procedures used in our study exploring the potential of PPO and SAC in enhancing hardware fault tolerance in simulated machines. Our methodology consists of three phases: (1) learning a task in a normal (pre-fault) environment, (2) introducing a hardware fault, and (3) continuing learning the task from Phase 1 in a (post-)fault environment.\nIn our study, we use two OpenAI Gym environments, Ant-v2 and FetchReach-v1, simulated using the MuJoCo 2.0 physics engine (Todorov et al., 2012)."}, {"title": "3.1. Experimental Phases", "content": "In this section, we describe the three phases of our experiments."}, {"title": "3.1.1. Phase 1", "content": "Phase 1 involves having an agent learn a task in a standard OpenAI Gym environment for t* time steps, using either PPO or SAC. In the OpenAI Gym Ant-v2 environment, the task requires an agent to learn a gait that maximizes the forward propulsion speed of a four-legged ant. In the OpenAI Gym FetchReach-v1 environment, the task involves an agent learning to move the end effector (or closed gripper) of a robot arm to a randomly generated, three-dimensional goal position as quickly and accurately as possible. We refer to the standard OpenAI Gym environment as a normal (pre-fault) environment.\nAt the end of Phase 1, the knowledge attained by the agent in the normal environment can be represented as Kt=t*={0PPO, M} and Kt=t*={0sac, B}, respectively."}, {"title": "3.1.2. Phase 2", "content": "In Phase 2, a hardware fault is introduced into the environment. This modification aims to replicate a real-world hardware fault for experimental purposes. The hardware faults we replicate are described in detail in Section 3.2."}, {"title": "3.1.3. Phase 3", "content": "Phase 3 involves the continuation of learning the task with a robot model experiencing the hardware fault introduced in Phase 2. We refer to the modified OpenAI Gym environment as a fault environment.\nAt the start of Phase 3, the agent's knowledge from Phase 1, Kt=t*, is transferred\u00b9. To investigate knowledge transfer in the context of continual learning, we conduct an ablation study with four different approaches, as shown in Table 1."}, {"title": "3.2. Hardware Faults", "content": "In this section, we detail the specific hardware faults introduced in our experiments."}, {"title": "3.2.1. Ant-v2", "content": "In the Ant-v2 environment, we introduce a fault to the right rear leg of the ant. This leg is one of two legs primarily responsible for generating\nthe forward pushing motion of the ant, while the other two legs primarily contribute to stability, as evidenced by numerous runs conducted within the normal environment. By introducing a fault to the right rear leg, the agent's learned policy is no longer effective its ability to effectively propel the ant forward is impeded. Consequently, the agent experiences difficulty in achieving the desired task of maximizing the ant's forward speed.\nWe implement four distinct faults in separate instances of the Ant-v2 environment, as shown in Figure 2. Each fault independently alters the underlying dynamics of the environment, thereby impacting the ant's locomotion capabilities. The following paragraphs describe each fault type in detail.\nReduced Range of Motion. In real-world machines, the presence of external or internal debris can disrupt the gear mechanics within a joint, limiting its Range of Motion (ROM). To simulate this fault, we consider two cases that restrict the ROM of a joint in the ant's right rear leg: (1) restricting the hip joint motion to [-5, 5] degrees (Figure 2a), and (2) restricting the ankle joint motion to [65, 70] degrees (Figure 2b)2.\nBroken Limb. Real-world robot linkages are susceptible to damage from physical impacts (Steinbauer-Wagner, 2012). A linkage, the rigid section between the joints on a robot, can sustain damage in such a way that it can become broken and (1) dissociated from the robot frame, or (2) partially dissociated from the robot frame, with the linkage held together by a weakened, unbroken section or inner connector cables. In our experiments, we consider these two types of faults. To simulate the former (i.e., a broken, severed linkage, as shown in Figure 2c), we reduce the length of the lower link in the ant's right rear leg by half. To simulate the latter (i.e., a broken, unsevered linkage, as shown in Figure 2d), we also reduce the length of the lower link in the ant's right rear leg by half. However, at the end of the modified link, we add an unactuated hinge joint with a range of [-180, 180] degrees and a link with the same shape and length as the removed section. We then adjust our observation vector to exclude entries that report data for the added joint and link, ensuring it matches the dimensionality of the unmodified Ant-v2 environment observation vector."}, {"title": "3.2.2. FetchReach-v1", "content": "In the FetchReach-v1 environment, we introduce a fault to the shoulder lift joint in one instance and to the elbow flex joint in another. We choose these two joints because they are crucial for the robot arm to succeed in its task. Introducing a fault to either joint causes the the robot arm to experience inaccuracies, resulting in delays in the robot arm reaching the goal position with its end effector.\nFrozen Position Sensor. A frozen sensor is a common type of sensor fault. A frozen sensor consistently reports a constant value that may not accurately reflect the physical property being measured (Visinsky et al., 1991). The first fault in FetchReach-v1, depicted in Figure 3a, is a frozen shoulder lift position sensor. To simulate this fault, we modify the robot's shoulder lift position sensor so that it continuously reports a position of -1.5 radians. This inaccurately reported position subsequently affects the computed position of the robot's end effector, which is calculated using forward kinematics within MuJoCo. As a result, this fault impacts the observation obtained in FetchReach-v1. Notably, this fault does not alter the underlying dynamics of the environment itself. Rather, it creates a discrepancy because the observation no longer accurately represents the true state of the robot, with one entry being noisy.\nPosition Slippage. In mechanical gears, gear teeth can break, leading to operational issues as the gear slips to the next non-broken tooth, causing inaccurate gear movements. Our second fault in FetchReach-v1, depicted in Figure 3b, is a slippery elbow flex joint. This fault causes the joint to move further than commanded by a constant value when an action command is given. Specifically, if the joint position is expected to change by x radians, this fault causes the joint position to move by x + c radians, where cis a constant noise value of 0.05 radians."}, {"title": "3.3. Evaluation", "content": "For each experiment, we conduct 30 runs using seeds ranging from 0 to 29. All experiments assess learning progress through policy evaluation roll-outs. At predefined intervals, learning is temporarily paused and the current policy is evaluated for 10 episodes. The reported metric for our experiments is the average return in a policy evaluation rollout, calculated by averaging across 30 runs. Additionally, we compute 95% confidence intervals using a t-distribution and indicate them on our plots with shaded regions.\nIn our primary experiments, we ensure both the normal and fault environments are allocated the same learning durations, allowing the algorithms to reach convergence. In the Ant-v2 environment, the learning duration for the PPO algorithm is set at 600 million time steps, with an evaluation frequency of 3 million time steps. For the SAC algorithm, the learning duration is 20 million time steps, with evaluations every 100,000 time steps. In the FetchReach-v1 environment, PPO is trained for 6 million time steps with an evaluation frequency of 30,000 time steps, while SAC trains for 2 million time steps, with evaluations every 10,000 time steps.\nTo highlight the early adaptation capabilities of each algorithm, we conduct a secondary set of experiments in the fault environments, adjusting both the learning durations and evaluation frequencies. In the Ant-v2 fault environments, both the PPO and SAC algorithms have learning duration of 300,000 time steps, with evaluations conducted every 10,000 steps. Similarly, in the FetchReach-v2 fault environments, the learning duration for both algorithms is set at 30,000 time steps, with evaluations every 10,000 steps."}, {"title": "3.4. Algorithm Implementation", "content": "In this section, we present a concise overview of our implementation of two RL algorithms: Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). We describe the neural network architectures for each algorithm, including the number of layers and nodes. Additionally, we highlight specific implementation choices such as the use of code-level optimizations."}, {"title": "3.4.1. Proximal Policy Optimization", "content": "Our implementation of PPO uses a single neural network comprising a policy network and a value network. Both internal networks are designed as feedforward networks with two hidden layers, each consisting of 64 nodes and using hyperbolic tangent activation functions. The policy network incorporates a learnable parameter known as the log standard deviation, which is initially set to 0. The Adam optimizer (Kingma and Ba, 2017) is used for optimization. To improve the performance of PPO, we incorporate code-level optimizations recommended by Engstrom et al. (2020), including linear learning rate decay, use of a generalized advantage estimator (Mnih et al., 2016), orthogonal initializations (Saxe et al., 2013), and hyperbolic tangent activation functions. In the Ant-v2 environment, we deviate from the standard linear learning rate decay rate of 1/#updates. Instead, we decay at a rate of 0.25/#updates, which leads to improved convergence. Notably, prior to learning in the fault environment, we reset PPO's learning rate to its initial value. This reset allows the algorithm to adapt to the changed environment."}, {"title": "3.4.2. Soft Actor-Critic", "content": "Our implementation of SAC uses two twinned soft Q-networks and a policy network. All networks are feedforward networks with two hidden layers, each consisting of 256 nodes and using Rectified Linear Unit (ReLU) activations. Each layer is initialized using Xavier uniform initialization (Glorot and Bengio, 2010). Optimization is performed using the Adam optimizer (Kingma and Ba, 2017)."}, {"title": "3.4.3. Hyperparameter Optimization", "content": "Real-world machines are typically optimized to perform effectively under normal conditions. When a fault occurs, conducting a new hyperparameter search tailored to the changed conditions can be time-consuming and computationally demanding. Therefore, we focus our hyperparameter optimization exclusively on learning the OpenAI Gym task in a normal environment (i.e., with a normal robot model).\nTo optimize the hyperparameters of each algorithm, we adopt a random search strategy. We employ this strategy by randomly selecting values for each hyperparameter from either a specified continuous interval [a, b] or a specified set x1, x2,.... To ensure thorough exploration, we evaluate a minimum of 80 unique hyperparameter configurations for each algorithm. This is achieved by varying the Numpy random seed values from 0 to 100. \u03a4\u03bf mitigate the impact of random fluctuations and obtain a reliable estimate of the algorithm's performance, we conduct 10 runs for each hyperparameter configuration using seed values ranging from 0 to 9. Tables 2 and 3 provide an overview of the possible values used to randomly draw hyperparameters for PPO and SAC, respectively.\nTo identify the best hyperparameter configuration for each algorithm, we employ a two-step process. We first determine the hyperparameter configuration(s) that achieved the highest asymptotic performance. The highest asymptotic performance is defined as the highest average return in the final 10 policy evaluation rollouts, averaged across 30 runs. In cases where multiple hyperparameter configurations demonstrate comparable highest asymptotic performance, we further evaluate them based on the speed of initial learning. This involves assessing how quickly the average return improves over time. Consequently, the best hyperparameter configuration is determined by considering the highest asymptotic performance and, if necessary for exclusion, the best initial performance. By following this methodology, we aim to identify the hyperparameter configuration that maximizes the algorithm's overall performance in terms of achieving high average returns and efficient learning.\nTables 2 and 3 summarize the best performing hyperparameters for each standard OpenAI Gym environment (i.e., normal environment)."}, {"title": "4. Results", "content": "In this section, we address four distinct focal points of investigation. In Section 4.1, we examine whether PPO and SAC exhibit adaptation to hardware faults. In Section 4.2, we analyze the differences in PPO and SAC in performance, sample efficiency, and adaptation speed in a fault environment. In Section 4.3, we assess the performance disparities among the four knowledge transfer approaches in a CRL and hardware fault setting. Finally, in Section 4.4, we compare PPO and SAC with previous strategies for hardware fault adaptation."}, {"title": "4.1. Adaptation to Hardware Faults", "content": "To demonstrate the adaptation of an agent to a hardware fault, we assess its ability to learn compensatory behaviours through model (e.g., policy) adaptation. A change in the agent's policy should alter the state-visitation distribution within the environment. To gather data for this evaluation, we run a specified algorithm and policy in the fault environment for 100 episodes, recording the joint angles visited throughout.\nIn Figure 4, heatmaps depict the observed shift in the distribution of visited joint angles in the Ankle ROM Restriction fault environment. Each heat map shows the probability of visiting angles within the normalized range for each joint with the specified algorithm and policy. For each algorithm, we compare: (a) the knowledge acquired in the normal environment and transferred to the fault environment without adaptation and (b) the knowledge acquired in the normal environment, transferred to the fault environment, and subsequently adapted (fine-tuned) within the fault environment. For both PPO and SAC, the transferred and adapted policy shows a reduction in how often the hip-4 joint reaches the upper end of its range when compared to the transferred and unadapted policy. Additionally, we observe variations in how frequently the other joints reach the upper or lower limits of their respective ranges."}, {"title": "4.2. Performance, Sample Efficiency, and Real-Time Speed", "content": "In our second comparative evaluation, we examine the performance, sample efficiency, and real-time speed of the adaptation process with PPO and SAC in the six fault environments. Figures 5 and 6 illustrate the average return achieved by each of the four knowledge transfer options with respect to the number of real experiences (i.e., time steps) in the fault environments. Figure 5 depicts the real-time equivalent of 10 hours (PPO) and 4.8 hours (SAC) of learning in the four Ant-v2 fault environments, while Figure 6 depicts the real-time equivalent of 1.1 hours (PPO) and 0.6 hours (SAC) of learning in the two FetchReach-v1 fault environments.\nFor our evaluation, we focus on each algorithm's best knowledge transfer approach in each fault environment. We define each algorithm's best knowledge transfer approach as the approach exhibiting the highest performance throughout the entire learning process (depicted in Figures 5 and 6). For example, in the Hip ROM Restriction fault environment shown in Figure 5a, the best knowledge transfer approach for PPO is to retain the model parameters 0, noting that discarding or retaining the memory M has little impact on performance; whereas for SAC, the best knowledge transfer approach is to retain the model parameters \u03b8 and the replay buffer B.\nWhen comparing each algorithm's best knowledge transfer approach, our results show that PPO significantly outperforms SAC in the four Ant-v2 fault environments depicted in Figure 5. Notably, PPO converges significantly faster than SAC in real-time; within 1 million time steps (or 2.4 hours real-time), SAC reaches a performance that is 10-40% less than that reached with PPO after 3 million time steps (or 30 minutes real-time). In the FetchReach-v1 fault environments depicted in Figure 6, the differences in performance between PPO and SAC are comparable. PPO attains near-asymptotic performance in both fault environments within 100,000 time steps (or 11 minutes real-time), while SAC reaches similar, near-asymptotic performance in fewer than 50,000 time steps (or 9 minutes real-time).\nOur findings demonstrate that PPO exhibits a comparable or faster real-time adaptation to faults when evaluated against SAC. Furthermore, PPO achieves higher performance in the high-dimensional Ant-v2 fault environments and near-equivalent performance in the low-dimensional FetchReach-v1 fault environments. However, due to its on-policy nature, PPO requires 2 to 3 times more real experiences to reach near-asymptotic performance, indicating its lower sample efficiency compared to SAC."}, {"title": "4.3. Transfer of Task Knowledge", "content": "In our third comparative evaluation, we investigate the performance of the four knowledge transfer approaches in a CRL and hardware fault adaptation setting. Our baseline (Approach 4) involves no knowledge transfer; tuples {OPPO, M} and {0sac, B"}]}