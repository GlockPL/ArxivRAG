{"title": "Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms", "authors": ["Sheila Schoepp", "Mehran Taghian", "Shotaro Miwa", "Yoshihiro Mitsuka", "Shadan Golestan", "Osmar R. Za\u00efane"], "abstract": "Industry is rapidly moving towards fully autonomous and interconnected systems that can detect and adapt to changing conditions, including machine hardware faults. Traditional methods for adding hardware fault tolerance to machines involve duplicating components and algorithmically reconfiguring a machine's processes when a fault occurs. However, the growing interest in reinforcement learning-based robotic control offers a new perspective on achieving hardware fault tolerance. However, limited research has explored the potential of these approaches for hardware fault tolerance in machines. This paper investigates the potential of two state-of-the-art reinforcement learning algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), to enhance hardware fault tolerance into machines. We assess the performance of these algorithms in two OpenAI Gym simulated environments, Ant-v2 and FetchReach-v1. Robot models in these environments are subjected to six simulated hardware faults. Additionally, we conduct an ablation study to determine the optimal method for transferring an agent's knowledge, acquired through learning in a normal (pre-fault) environment, to a (post-)fault environment in a continual learning setting. Our results demonstrate that reinforcement learning-based approaches can enhance hardware fault tolerance in simulated machines, with adaptation occurring within minutes. Specifically, PPO exhibits the fastest adaptation when retaining the knowledge within its models, while SAC performs best when discarding all acquired knowledge. Overall, this study highlights the potential of reinforcement learning-based approaches, such as PPO and SAC, for hardware fault tolerance in machines. These findings pave the way for the development of robust and adaptive machines capable of effectively operating in real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "Automation is revolutionizing industries, enhancing productivity and ef- ficiency (Haleem et al., 2021; Javaid et al., 2021; Vaidya et al., 2018). Au- tomation is achieved by integrating machines equipped with sensors, such as robots, with Artificial Intelligence (AI)-powered analytical systems (agents), enabling real-time data collection and analysis. These agents make the crit- ical decisions necessary for automation, including facilitating the real-time identification and diagnosing of hardware faults in machines through fault detection and fault diagnosis, respectively (Riazi et al., 2019; Zhang et al., 2017; Samanta and Nataraj, 2009). While fault detection and fault diagnosis are essential processes that inform on faults, they do not address adapta- tion to hardware faults. To succeed in real-world deployments, it is crucial for machines to adapt to unexpected events, such as hardware faults (Chen et al., 2023; Schrum and Gombolay, 2019). As such, enhancing hardware fault tolerance in machines is imperative.\nAn established method of enhancing hardware fault tolerance in machines is through redundancy, where critical hardware components are duplicated to mitigate the risk of failure (Guiochet et al., 2017; Visinsky et al., 1994, 1991). However, this approach has significant drawbacks, including increased ma- chine size, weight, power consumption, and financial costs (Dubrova, 2013). Moreover, retrofitting existing machines with redundant components is of-"}, {"title": null, "content": "ten impossible. Therefore, exploring alternative approaches that do not rely on redundancy provides a significant advantage in enhancing hardware fault tolerance in machines.\nTaking inspiration from nature, one can imagine an agent adapting a machine's behaviour in response to a hardware fault. Animals, for example, demonstrate an extraordinary ability to adapt. Animals can modify their gait in the presence of an injured limb, using non-injured limbs in a compensatory manner (Fuchs et al., 2014; Jarvis et al., 2013). An established approach to incorporate adaptable, compensatory behaviours into machines is through algorithmic reconfiguration. In algorithmic reconfiguration, an agent adjusts the underlying algorithms that govern hardware usage within a machine, enabling adaptation to changes in hardware conditions (Guiochet et al., 2017; Visinsky et al., 1994, 1991). Algorithmic reconfiguration may entail altering the algorithm's hyperparameter settings, adjusting the algorithm's model architecture, or even switching to an entirely different algorithm.\nIn essence, algorithmic reconfiguration is closely linked to continual learn- ing. With continual learning, when a machine faces evolving hardware con- ditions throughout its lifetime, it must adapt to succeed in the changed con- ditions. Adaptation in continual learning involves leveraging the continuous flow of data to modify aspects of the algorithm, such as the models pa- rameters and/or the storage contents. To accelerate the adaptation process, previously acquired knowledge can be strategically transferred to the new task (Chen and Liu, 2018).\nIn nature, adaptation to physical changes is often achieved through a trial-and-error approach, enhanced by knowledge acquired from past experi- ences. Continual Reinforcement Learning (CRL) (Abel et al., 2024; Sutton and Barto, 2018; Xu and Zhu, 2018) is an ideal strategy for an agent tasked with adapting a machine's behaviour to unexpected events, such as hard- ware faults. CRL typically entails an agent continuously interacting with a non-stationary environment, learning from feedback (trial-and-error), and strategically transferring acquired knowledge.\nDespite CRL being a valuable strategy for enhancing hardware fault toler- ance in machines, it remains relatively unexplored. In this study, we examine how CRL can enhance hardware fault tolerance in simulated machines. Figure 1 outlines our study's approach, where knowledge acquired by an agent learn- ing in a normal environment (Kt=T) is selectively transferred and adapted by the same agent in a changed, fault environment. Specifically, we study the impact of retaining or discarding components of the agent's acquired knowl-"}, {"title": null, "content": "edge, such as the learned model parameters and the storage contents, at the onset of a fault. We examine how retaining and fine-tuning, or discarding, the model parameters learned in a normal environment affects adaptation in a fault environment. Additionally, we explore the effect of retaining or dis- carding the storage contents, which, if retained, may initially serve as a proxy for a model of the normal environment. Our hypothesis is that Kt=t* contains useful information that can enable faster adaptation to hardware faults. We empirically evaluate the effectiveness of two state-of-the-art Reinforcement Learning (RL) algorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), in improving hardware fault tolerance in machines. For our evaluation, we use widely accepted benchmarks from the OpenAI Gym toolkit, namely Ant-v2 and FetchReach-v1 (Brockman et al., 2016).\nTo the best of our knowledge, this study is the first to comprehensively examine the performance of PPO and SAC in the context of hardware fault enhancement in machines. Our findings provide valuable insights into op- timizing knowledge transfer in a CRL and hardware fault problem setting, thereby accelerating machine adaptation.\nIn the subsequent sections, we outline the framework and findings of our study. In Section 2, we review the foundational knowledge and literature related to our study. In Section 3, we describe the methods applied in our study. Within Section 4, we present our results. In Section 5, we discuss the interpretation of our findings and outline potential directions for future work. Lastly, in Section 6, we summarize the outcomes of our work and highlight their broader significance."}, {"title": "2. Background and Related Work", "content": "This section serves as a foundation for understanding the research con- text and examines the existing literature relevant to our study. In Section 2.1, we provide a comprehensive overview of key concepts and techniques in RL. In Section 2.2, we review notable works to provide a comprehen- sive understanding of the research landscape. By presenting this background information and discussing relevant works, we aim to establish a solid foun- dation for our research and highlight the unique contributions of our study in this field."}, {"title": "2.1. Reinforcement Learning", "content": "RL is a subfield of machine learning that enables agents to autonomously learn by making sequential decisions through interactions with their envi- ronment. Sequential decision-making problems are typically modelled as a Markov Decision Process (MDP). An MDP is defined by the tuple (S, A, R, P, \u03b3), where S denotes the state space; A denotes the action space; R : S \u00d7 A \u2192 R is the reward function; P(st+1|st, at) represents the MDP dynamics, which is the probability of transitioning to state st+1 after taking action at in state st, where st, st+1 \u2208 S, at \u2208 A; and \u03b3 \u2208 [0, 1] denotes the discount rate (Sutton and Barto, 2018).\nInteraction between the agent and the environment occurs over a series of discrete time steps, represented by t = 0, 1, 2, ..., H, where H is the horizon, which may be infinite. During each interaction, the environment provides the agent with information on its current state st. Guided by its policy, repre- sented as the function \u03c0 : S \u2192 A, the agent selects and executes an action at \u2208 A(st) for the next time step. Subsequently, the agent receives feedback in the form of a numerical reward rt+1 \u2208 R, and the environment updates the agent on its new state st+1. The sequence of states and actions forms a tra- jectory, denoted by \u03c4. The primary objective of the agent is to maximize the expected discounted cumulative reward received in a trajectory \u03c4, known as the expected discounted return, denoted by J(\u03c0) = \u0395\u03c4\u03c0 [\u2211t=0 H \u03b3tR(st, at)]. This objective guides the agent's learning, as it continually seeks to select actions that result in higher rewards. The optimization problem that aims to maximize the objective function J(\u03c0) can be expressed as:\n\u03c0* = arg max J(\u03c0), (1)\n\u03c0\nwhere \u03c0* is the optimal policy."}, {"title": "2.1.1. Policy Gradient Methods", "content": "Policy gradient methods optimize the policy directly, as described by Equation 1. The policy is represented as a parameterized function, i.e., a neural network, denoted as \u03c0\u03b8(\u00b7|s), where \u03b8 \u2208 Rd represents the policy's d- dimensional parameters. In policy gradient methods, the policy parameters are evaluated using a performance measure and are iteratively updated using gradient ascent. This process improves the policy by identifying parameter values that yield higher expected returns."}, {"title": "2.1.2. Proximal Policy Optimization (PPO)", "content": "PPO is an on-policy algorithm designed to limit the magnitude of policy updates, thus ensuring stable and gradual changes during training (Schul- man et al., 2017). PPO has proven to be an effective algorithm for robotic applications (Kuo and Chen, 2023).\nA PPO agent collects experiences namely, a collection of trajectories through interactions with the environment, while following the current policy. The collected experiences are stored in the agent's memory M. Once M is full, these trajectories are used to compute the loss using a surrogate objective function, LCLIP (\u03b8), which approximates policy improvement and guides the policy update from the current policy \u03c0\u03b8old to an updated can- didate policy \u03c0\u03b8. To prevent drastic policy changes that could destabilize learning, LCLIP (\u03b8) incorporates a clipping mechanism, constraining the up- dated candidate policy to be closely aligned with the current policy. The parameters \u03b8 are optimized over one or more epochs, using mini-batches of experiences sampled from M, according to the following equation:\n\u03b8* = arg max LCLIP (\u03b8) (2)\n\u03b8\nAfter completing the update, the experiences stored in Mare discarded. The algorithm then collects new experiences using the updated policy and stores them in M. This iterative process allows PPO to refine the policy gradually and converge towards an optimal or near-optimal solution. We denote the solution at convergence, in the normal environment, as OPPO. For more details on PPO, we refer readers to the original paper (Schulman et al., 2017)."}, {"title": "2.1.3. Soft Actor-Critic (SAC)", "content": "SAC is an off-policy algorithm that is well-suited for real-world problems, owing to its low sample complexity and its robustness to various hyperparam- eter settings (Haarnoja et al., 2018, 2019). A distinguishing feature of SAC is its use of an entropy regularization term in its objective function, denoted as J'(\u03c0\u03b8(\u00b7|s)). The entropy regularization term encourages exploration by favouring stochastic policies. A temperature coefficient for the entropy reg- ularization term adjusts the balance between entropy and expected returns. SAC incorporates an automatic adjustment mechanism for this coefficient, achieved by imposing a lower-bound average policy entropy constraint during the optimization process. This mechanism enables SAC to strike a delicate balance between exploration and exploitation, allowing the agent to learn a policy that effectively adapts to diverse environmental conditions while maximizing expected returns.\nTo facilitate the learning process, SAC uses a replay buffer, denoted as B, to retain past experiences. Once B reaches a specified minimum capacity, batches of experiences specifically, past agent-environment interactions are uniformly sampled from B at each time step and used to update the parameters. The parameters are optimized using:\n\u03b8* = arg max J' (\u03c0\u03b8(\u00b7|s)) (3)\n\u03b8\nFollowing each update, a new experience is generated under the updated policy and is added to the replay buffer B, replacing the oldest stored expe- rience.\nIt is worth noting that each experience stored in the replay buffer is generated under a different policy. The ability to reuse past experiences, even when generated under different policies, is what enables SAC to be highly sample-efficient, requiring fewer interactions with the environment compared to on-policy algorithms, such as PPO.\nWe refer to the SAC solution at convergence, in the normal environ- ment, as OSAC. For more details on SAC, we refer readers to the original paper (Haarnoja et al., 2018)."}, {"title": "2.2. Related Works", "content": ""}, {"title": "2.2.1. Collection of Pre-Trained Policies", "content": "The algorithm developed by Raileanu et al. (2020), known as Policy- Dynamics Value Functions (PD-VF), demonstrated rapid adaptability to"}, {"title": null, "content": "previously unseen environments with changed dynamics. The methodology used in this work involved training a set of policies correspondingly with trajectories collected within a set of environments, each characterized by its unique dynamics. At test time, when faced with an unseen environment, each policy from the set was evaluated in the test environment for a small num- ber of time steps. The policy that exhibited the highest performance in the test environment was considered to be the adapted policy. To evaluate the effectiveness of their approach, Raileanu et al. (2020) applied PD-VF to four Ant-Legs environments, where modifications were made to the length of the Ant-v2 robot model's linkages.\nA limitation of this work is that, unlike online learning algorithms such as SAC and PPO, PD-VF does not learn a policy in real-time. Rather, it selects the best-performing policy from a collection of pre-trained policies. This approach deprives improvement of a transferred policy through inter- action with an environment. In contrast, SAC and PPO are online learning algorithms capable of adapting a transferred policy. Furthermore, a compet- ing algorithm, PPOall, was trained in a multi-task setting, where negative transfer can hinder performance if the training set tasks are too dissimilar (Rosenstein et al., 2005), rendering this method of limited practical value in fault adaptation domain, where fault tasks may be drastically different from normal ones. In our study, we train PPO on a single task, demonstrating that the learned, high-performing policy is both transferable and quickly adapt- able to unseen tasks with similar structure, and outperforming the reported performance of PD-VF on comparable tasks."}, {"title": "2.2.2. Meta-Reinforcement Learning", "content": "Nagabandi et al. (2018) introduced two variants of meta-RL algorithms, namely a Gradient-Based Adaptive Learner (GrBAL) and a Recurrence- Based Adaptive Learner (ReBAL), which demonstrated superior speed and sample efficiency in adapting to hardware faults in robots. These two algo- rithms learn a model of the environment dynamics, enabling rapid adapta- tion to hardware faults with as little as 1.5 - 3 hours of real-world experience. Nevertheless, it is worth noting that a third model-based RL algorithm, not belonging to the meta-RL class, performed comparably, underscoring the sample efficiency of model-based approaches. The evaluation of the algo- rithms was conducted on an Ant: crippled leg task, which involved damaging a single leg of the Ant-v2 robot model.\nA drawback of the algorithms proposed by Nagabandi et al. (2018) is that"}, {"title": null, "content": "machine faults can arise unexpectedly, exhibiting diverse and unpredictable characteristics. Meta-RL algorithms, such as GrBAL and ReBAL, require prior knowledge of the fault type to define a task distribution customized for a particular fault, which limits their practicality. In contrast, PPO and SAC have the advantage of adapting without any prior knowledge of the fault type. This flexibility enables them to effectively handle a wide range of faults without the need for specialized pre-training. Furthermore, we show that PPO and SAC outperform GrBAL and ReBAL on comparable tasks, emphasizing the tendency of model-based approaches to achieve suboptimal asymptotic performance when compared to model-free approaches."}, {"title": "2.2.3. Pre-Processing In Simulation", "content": "Cully et al. (2014) used a robot simulator to construct a low-dimensional behaviour performance map. In the event of a fault, a real-world robot would take a trial-and-error approach to fault adaptation, iteratively selecting and executing the projected best-performing behaviour from the behaviour per- formance map, and subsequently updating the map with the real-world per- formance. This iterative procedure would continue until a high-performing behaviour, surpassing a predefined threshold, was identified. Notably, this ap- proach demonstrated significant success, achieving recovery within minutes in a low-dimensional parameter search space.\nThis approach, however, had certain limitations. Firstly, it relied on the availability of a simulator to generate a behaviour-performance map, which limited its practicality for robots lacking simulators. Furthermore, this method faced challenges when confronted with a high-dimensional parameter search space. Our study addresses these limitations by offering broad applicability to all robots without the need for prior computation in simulation. Further- more, we demonstrate the effectiveness of adaptation using the PPO and SAC algorithms, even within high-dimensional parameter spaces, by utilizing raw sensor output data to select continuous actions."}, {"title": "2.2.4. A Policy for Each Actuator", "content": "Huang et al. (2020) introduced a decentralized approach to learning called Shared Modular Policies (SMP), where each component of a robot, e.g., actu- ator, was treated as an individual module. Each module had its own policy, which was instantiated from a global policy (i.e., a single, reusable neural network). The input to each policy included raw data from the module's lo- cal sensors. A system of message passing allowed for communication between"}, {"title": null, "content": "modules. SMP was trained in a multi-task learning setting, where each task involved a robot model with a unique arrangement of linkages and/or joints. Huang et al. (2020) evaluated SMP in the OpenAI Gym environments Walker2D-v2, Humanoid-v2, Hopper-v2, and HalfCheetah-v2. Although SMP was shown to perform well on simple tasks, it faced difficulties on more com- plex tasks (e.g., Humanoid-v2). The performance of SMP was compared to a baseline, where a centralized policy was trained using Twin Delayed DDPG (TD3) (Fujimoto et al., 2018) in a multi-task setting, highlighting the effec- tiveness of SMP. Notably, previous work has shown that training a centralized policy in a multi-task setting, as was done with TD3, can lead to performance issues due to negative transfer from vastly different tasks (Rosenstein et al., 2005).\nIn our study, we employ a centralized approach to learning, using a sin- gle policy to control all actuators in the robot model, within a single-task setting. This approach to learning demonstrates high performance within a high-dimensional state space effectively capturing interactions and interde- pendencies among actuators, and is shown to be transferable and quickly adaptable to new, unseen environments. Unlike SMP, our transferred policy does not require significant pre-training in a multi-task setting. Instead, it is trained using experiences collected under normal conditions, emphasizing its practicality and efficiency."}, {"title": "3. Methodology", "content": "In this section, we describe the experimental procedures used in our study exploring the potential of PPO and SAC in enhancing hardware fault toler- ance in simulated machines. Our methodology consists of three phases: (1) learning a task in a normal (pre-fault) environment, (2) introducing a hard- ware fault, and (3) continuing learning the task from Phase 1 in a (post-)fault environment.\nIn our study, we use two OpenAI Gym environments, Ant-v2 and FetchReach- v1, simulated using the MuJoCo 2.0 physics engine (Todorov et al., 2012)."}, {"title": "3.1. Experimental Phases", "content": "In this section, we describe the three phases of our experiments."}, {"title": "3.1.1. Phase 1", "content": "Phase 1 involves having an agent learn a task in a standard OpenAI Gym environment for t* time steps, using either PPO or SAC. In the OpenAI Gym Ant-v2 environment, the task requires an agent to learn a gait that maximizes the forward propulsion speed of a four-legged ant. In the Ope- nAI Gym FetchReach-v1 environment, the task involves an agent learning to move the end effector (or closed gripper) of a robot arm to a randomly gen- erated, three-dimensional goal position as quickly and accurately as possible. We refer to the standard OpenAI Gym environment as a normal (pre-fault) environment.\nAt the end of Phase 1, the knowledge attained by the agent in the normal environment can be represented as Kt=t*={\u03b8PPO, M} and Kt=t*={\u03b8sac, B}, respectively."}, {"title": "3.1.2. Phase 2", "content": "In Phase 2, a hardware fault is introduced into the environment. This modification aims to replicate a real-world hardware fault for experimental purposes. The hardware faults we replicate are described in detail in Section 3.2."}, {"title": "3.1.3. Phase 3", "content": "Phase 3 involves the continuation of learning the task with a robot model experiencing the hardware fault introduced in Phase 2. We refer to the mod- ified OpenAI Gym environment as a fault environment.\nAt the start of Phase 3, the agent's knowledge from Phase 1, Kt=t*, is transferred\u00b9. To investigate knowledge transfer in the context of continual learning, we conduct an ablation study with four different approaches, as shown in Table 1."}, {"title": "3.2. Hardware Faults", "content": "In this section, we detail the specific hardware faults introduced in our experiments."}, {"title": "3.2.1. Ant-v2", "content": "In the Ant-v2 environment, we introduce a fault to the right rear leg of the ant. This leg is one of two legs primarily responsible for generating"}, {"title": "3.3. Evaluation", "content": "For each experiment, we conduct 30 runs using seeds ranging from 0 to 29. All experiments assess learning progress through policy evaluation roll- outs. At predefined intervals, learning is temporarily paused and the current policy is evaluated for 10 episodes. The reported metric for our experiments is the average return in a policy evaluation rollout, calculated by averaging across 30 runs. Additionally, we compute 95% confidence intervals using a t-distribution and indicate them on our plots with shaded regions.\nIn our primary experiments, we ensure both the normal and fault envi- ronments are allocated the same learning durations, allowing the algorithms to reach convergence. In the Ant-v2 environment, the learning duration for the PPO algorithm is set at 600 million time steps, with an evaluation fre- quency of 3 million time steps. For the SAC algorithm, the learning duration is 20 million time steps, with evaluations every 100,000 time steps. In the FetchReach-v1 environment, PPO is trained for 6 million time steps with an evaluation frequency of 30,000 time steps, while SAC trains for 2 million time steps, with evaluations every 10,000 time steps.\nTo highlight the early adaptation capabilities of each algorithm, we con- duct a secondary set of experiments in the fault environments, adjusting both the learning durations and evaluation frequencies. In the Ant-v2 fault environments, both the PPO and SAC algorithms have learning duration of 300,000 time steps, with evaluations conducted every 10,000 steps. Similarly, in the FetchReach-v2 fault environments, the learning duration for both al- gorithms is set at 30,000 time steps, with evaluations every 10,000 steps."}, {"title": "3.4. Algorithm Implementation", "content": "In this section, we present a concise overview of our implementation of two RL algorithms: Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). We describe the neural network architectures for each algorithm, including the number of layers and nodes. Additionally, we highlight specific implementation choices such as the use of code-level optimizations."}, {"title": "3.4.1. Proximal Policy Optimization", "content": "Our implementation of PPO uses a single neural network comprising a policy network and a value network. Both internal networks are designed as feedforward networks with two hidden layers, each consisting of 64 nodes and using hyperbolic tangent activation functions. The policy network incor- porates a learnable parameter known as the log standard deviation, which"}, {"title": "3.4.2. Soft Actor-Critic", "content": "Our implementation of SAC uses two twinned soft Q-networks and a pol- icy network. All networks are feedforward networks with two hidden layers, each consisting of 256 nodes and using Rectified Linear Unit (ReLU) acti- vations. Each layer is initialized using Xavier uniform initialization (Glorot and Bengio, 2010). Optimization is performed using the Adam optimizer (Kingma and Ba, 2017)."}, {"title": "3.4.3. Hyperparameter Optimization", "content": "Real-world machines are typically optimized to perform effectively under normal conditions. When a fault occurs, conducting a new hyperparameter search tailored to the changed conditions can be time-consuming and compu- tationally demanding. Therefore, we focus our hyperparameter optimization exclusively on learning the OpenAI Gym task in a normal environment (i.e., with a normal robot model).\nTo optimize the hyperparameters of each algorithm, we adopt a random search strategy. We employ this strategy by randomly selecting values for each hyperparameter from either a specified continuous interval [a, b] or a specified set x1, x2,.... To ensure thorough exploration, we evaluate a min- imum of 80 unique hyperparameter configurations for each algorithm. This is achieved by varying the Numpy random seed values from 0 to 100. \u03a4\u03bf mitigate the impact of random fluctuations and obtain a reliable estimate of the algorithm's performance, we conduct 10 runs for each hyperparameter configuration using seed values ranging from 0 to 9. Tables 2 and 3 provide an overview of the possible values used to randomly draw hyperparameters for PPO and SAC, respectively.\nTo identify the best hyperparameter configuration for each algorithm, we employ a two-step process. We first determine the hyperparameter config- uration(s) that achieved the highest asymptotic performance. The highest asymptotic performance is defined as the highest average return in the final 10 policy evaluation rollouts, averaged across 30 runs. In cases where multiple hyperparameter configurations demonstrate comparable highest asymptotic performance, we further evaluate them based on the speed of initial learning."}, {"title": null, "content": "This involves assessing how quickly the average return improves over time. Consequently, the best hyperparameter configuration is determined by considering the highest asymptotic performance and, if necessary for exclu- sion, the best initial performance. By following this methodology, we aim to identify the hyperparameter configuration that maximizes the algorithm's overall performance in terms of achieving high average returns and efficient learning.\nTables 2 and 3 summarize the best performing hyperparameters for each standard OpenAI Gym environment (i.e., normal environment)."}, {"title": "4. Results", "content": "In this section, we address four distinct focal points of investigation. In Section 4.1, we examine whether PPO and SAC exhibit adaptation to hard- ware faults. In Section 4.2, we analyze the differences in PPO and SAC in performance, sample efficiency, and adaptation speed in a fault environment. In Section 4.3, we assess the performance disparities among the four knowl- edge transfer approaches in a CRL and hardware fault setting. Finally, in Section 4.4, we compare PPO and SAC with previous strategies for hard- ware fault adaptation."}, {"title": "4.1. Adaptation to Hardware Faults", "content": "To demonstrate the adaptation of an agent to a hardware fault, we assess its ability to learn compensatory behaviours through model (e.g., policy) adaptation. A change in the agent's policy should alter the state-visitation distribution within the environment. To gather data for this evaluation, we run a specified algorithm and policy in the fault environment for 100 episodes, recording the joint angles visited throughout.\nIn Figure 4, heatmaps depict the observed shift in the distribution of vis- ited joint angles in the Ankle ROM Restriction fault environment. Each heat map shows the probability of visiting angles within the normalized range for each joint with the specified algorithm and policy. For each algorithm, we compare: (a) the knowledge acquired in the normal environment and trans- ferred to the fault environment without adaptation and (b) the knowledge acquired in the normal environment, transferred to the fault environment, and subsequently adapted (fine-tuned) within the fault environment. For both PPO and SAC, the transferred and adapted policy shows a reduction in how often the hip-4 joint reaches the upper end of its range when compared"}, {"title": null, "content": "to the transferred and unadapted policy. Additionally, we observe variations in how frequently the other joints reach the upper or lower limits of their respective ranges."}, {"title": "4.2. Performance, Sample Efficiency, and Real-Time Speed", "content": "In our second comparative evaluation, we examine the performance, sam- ple efficiency, and real-time speed of the adaptation process with PPO and SAC in the six fault environments. Figures 5 and 6 illustrate the average return achieved by each of the four knowledge transfer options with respect to the number of real experiences (i.e., time steps) in the fault environments. Figure 5 depicts the real-time equivalent of 10 hours (PPO) and 4.8 hours"}, {"title": null, "content": "(SAC) of learning in the four Ant-v2 fault environments, while Figure 6 de- picts the real-time equivalent of 1.1 hours (PPO) and 0.6 hours (SAC) of learning in the two FetchReach-v1 fault environments.\nFor our evaluation, we focus on each algorithm's best knowledge transfer approach in each fault environment. We define each algorithm's best knowl- edge transfer approach as the approach exhibiting the highest performance throughout the entire learning process (depicted in Figures 5 and 6). For example, in the Hip ROM Restriction fault environment shown in Figure 5a, the best knowledge transfer approach for PPO is to retain the model param- eters \u03b8, noting that discarding or retaining the memory M has little impact on performance; whereas for SAC, the best knowledge transfer approach is to retain the model parameters \u03b8 and the replay buffer B.\nWhen comparing each algorithm's best knowledge transfer approach, our results show that PPO significantly outperforms SAC in the four Ant-v2"}, {"title": null, "content": "fault environments depicted in Figure 5. Notably, PPO converges significantly faster than SAC in real-time; within 1 million time steps (or 2.4 hours real- time), SAC reaches a performance that is 10-40% less than that reached with PPO after 3 million time steps (or 30 minutes real-time). In the FetchReach- v1 fault environments depicted in Figure 6, the differences in performance between PPO and SAC are comparable. PPO attains near-asymptotic perfor- mance in both fault environments within 100,000 time steps (or 11 minutes real-time), while SAC reaches similar, near-asymptotic performance in fewer than 50,000 time steps (or 9 minutes real-time).\nOur findings demonstrate that PPO exhibits a comparable or faster real- time adaptation to faults when evaluated against SAC. Furthermore, PPO achieves higher performance in the high-dimensional Ant-v2 fault environ- ments and near-equivalent performance in the low-dimensional FetchReach- v1 fault environments. However, due to its on-policy nature, PPO requires 2 to 3 times more real experiences to reach near-asymptotic performance, indicating its lower sample efficiency compared to SAC."}, {"title": "4.3. Transfer of Task Knowledge", "content": "In our third comparative evaluation, we investigate the performance of the four knowledge transfer approaches in a CRL and hardware fault adaptation setting. Our baseline (Approach 4) involves no knowledge transfer; tuples {OPPO, M} and {\u03b8sac, B} are discarded for PPO and SAC, respectively. Consequently, the policy is trained with data solely collected in each fault environment and is unbiased to the normal environment.\nFigures 7 and 8 show the early performance of each knowledge transfer approach in the Ant-v2 and FetchReach-v1 fault environments at two criti- cal points in the learning process: (1) immediately after the onset of a fault, where knowledge from the normal environment has been transferred (and dis- carded, if applicable), but no learning in the fault environment has occurred; and (2) 300,000 time steps (Ant-v2) and 30,000 time steps (FetchReach- v1) after the onset of a fault, chosen to highlight the notable differences in early performance among the four knowledge transfer approaches in each fault environment. The asymptotic performance of the baseline in each fault environment is indicated by a black dashed line.\nOur results show that, with PPO, retaining and fine-tuning the model parameters OPPO generally leads to the best early performance in the fault environments, as the knowledge contained within the models facilitates rapid adaptation. However, we observe an exception with the Frozen Shoulder Lift"}, {"title": null, "content": "Position Sensor fault (depicted in Figure 8). In this particular case, retaining the model parameters OPPO leads to sub-optimal performance with PPO. The Frozen Shoulder Lift Position Sensor fault is unique as it is the only fault that does not alter the dynamics of the environment. Rather, an observation in this fault environment includes a noisy element that inaccurately reports the position of the Shoulder Lift joint.\\Additionally, we find that whether the memory Mis retained or dis- carded with PPO has minimal impact on performance in the fault environ- ments. In our experiments, the memory capacity is small and all experiences are discarded from the memory after a single update. Consequently, the old"}, {"title": null, "content": "experiences retained in the memory have minimal influence on adaptation.\nFurthermore, in Figures 5 and 6, we observe that for PPO, the variability in asymptotic performance across all four knowledge transfer options is slight in most fault environments.\nAccording to Figure 7, with SAC, we observe that our baseline (Ap- proach 4) generally leads to the best early performance, often outperforming the three other knowledge transfer approaches. The second-best knowledge transfer approach for SAC, in terms of early performance, is the retention of all knowledge (i.e., the model parameters Osac and the replay buffer contents B). Similar to PPO, the retained model parameters Osac contain knowledge that facilitates adaptation to the new environment. However, in contrast to"}, {"title": "PPO", "content": "we find that retaining or disposing of the contents of the replay buffer B significantly impacts the performance of SAC. For example, when the model parameters Osac are retained and fine-tuned, disposing of the replay buffer B substantially degrades the performance of SAC (as depicted in Figure 7b).\nIn Figure 5, our results show significant variability in the asymptotic performance of SAC across all four knowledge transfer approaches in the high-dimensional Ant-v2 fault environments. Conversely, Figure 6 illustrates that, in the low-dimensional FetchReach-v1 fault environments, the variabil- ity in SAC's performance is less pronounced. These results suggests that, when using SAC, the manner in which knowledge is transferred to the fault environment depends heavily on the task at hand."}, {"title": "4.4. Comparison With Prior Fault Adaptation Approaches", "content": "In our fourth comparative evaluation, we compare the performance of PPO and SAC in our four Ant-v2 fault environments (shown in Figure 7) to the performance of three meta-RL algorithms in an Ant: crippled leg en- vironment (Nagabandi et al., 2018) and the performance of PD-VF in four Ant-Legs environments (Raileanu et al., 2020).\nIn the meta-RL experiments conducted by Nagabandi et al. (2018), the early performance of two proposed meta-RL algorithms, namely GrBAL and ReBAL, reached an average return of 430 in their Ant: crippled leg environ- ment. The authors do not provide information on the asymptotic performance of these two algorithms. However, they reported the asymptotic performance of a third meta-RL algorithm, Model-Agnostic Meta-Learning (MAML), in the same environment as 710.\nIn the PD-VF experiments conducted by Raileanu et al. (2020), the per- formance of PD-VF in their four Ant-Legs environments ranged from 200 to 350, consistently outperforming PPOall. PPOall, trained in a multi-task learning setting, achieved a performance in the range of 120 to 320. Mean- while, PPOenv, trained exclusively in the test environment, achieved a per- formance in the range of 374 to 862, consistently outperforming PD-VF.\nIn our Ant-v2 fault environments, both PPO and SAC surpass the perfor- mance of these prior methods. When the model parameters are retained, PPO achieves an average return in the range of 934 to 2919 with no adaptation and 3006 to 4681 within 300,000 time steps. When discarding all knowledge, SAC achieves an average return in the range of 1506 to 2273 within 300,000 time steps. Notably, PPO, with no adaptation, attains a performance 2 to 7 times higher than that reported for GrBAL and ReBAL, and 1.3 to 4 times"}, {"title": null, "content": "higher than the asymptotic performance of MAML. After some adaptation, PPO achieves an early performance 7 to 11 times higher than the early per- formance reported for GrBAL and ReBAL, and 4 to 7 times higher than the asymptotic performance reported for MAML. Similarly, SAC, after adap- tation, achieves an early performance 3 to 5 times higher than GrBAL and ReBAL, and 2 to 3 times higher than the asymptotic performance of MAML. Furthermore, when the models are retained, PPO, with no adaptation, also outperforms PPOenv. It is worth noting that other factors, such as the im- plementation of PPO and the disparity between the fault environments, may additionally contribute to these differences in performance.\nWhile a more in-depth comparison of these methods is required, our pre- liminary comparative evaluation indicates that PPO and SAC achieve com- petitive performance compared to previous related work, especially when an appropriate knowledge transfer method is chosen. This comparison suggests the potential of more sophisticated knowledge transfer methods for off-the- shelf RL algorithms in the context of machine fault adaptation."}, {"title": "5. Discussion and Future Work", "content": "The goal of this paper is to explore the effectiveness of transferring knowl- edge acquired by an RL agent in a normal environment, where the agent learns to perform a task with a fully-functioning robot model, to a fault envi- ronment, where the agent must adapt to complete the same task with a robot model experiencing a hardware fault. Four transfer learning approaches are examined. Tables 4 and 5 summarize our findings regarding the early adap- tation speed of three knowledge transfer approaches relative to our baseline (Approach 4) by calculating the percentage of time steps saved. For this cal- culation, we first determine the number of time steps required for the baseline to reach the lower bound of its 95% confidence interval for performance (i.e., average return) at 300,000 time steps, which we represent as tbaseline. We then determine the number of time steps required for each of the three approaches to reach the lower bound of the baseline's 95% confidence interval for perfor- mance at 300,000 time steps, which we denote as tapproach. To compute the percentage of time steps saved, we use the formula:\n% = 100 - (tapproach \u00d7 100) (4)"}, {"title": null, "content": "behaviour, especially when encountering unforeseen faults. Thus, the chal- lenge of learning in safety-critical environments remains an open research avenue, as evidenced by several works focusing on safe adaptation Zhang et al. (2020); Peng et al. (2022). While our current work concentrates on en- hancing the performance of state-of-the-art RL algorithms using knowledge transfer methods, we recognize the importance of safe adaptation in robotic applications. Our future work will explore how fault adaptation techniques can be designed with a focus on safety in various environments, including those involving other agents (human or robotic), delicate obstacles, and sce- narios where a robot may risk self-harm."}, {"title": "6. Conclusion", "content": "In this study, we have showcased the effectiveness of two RL algorithms, PPO and SAC, in enhancing hardware fault tolerance in simulated machines. We conducted an ablation study to determine the most optimal approach for knowledge transfer in a continual learning and hardware fault adaptation setting.\nOur findings indicate that transferring the models learned in the normal environment to the fault environment with PPO is generally the most effec- tive knowledge transfer option. This approach yields the fastest performance improvement with the fewest real experiences. Remarkably, the performance achieved with PPO, after a brief period of adaptation, surpasses that of meta- RL algorithms, in a comparable broken limb environment (Nagabandi et al., 2018).\nFor SAC, our results demonstrate that discarding all knowledge acquired in the normal environment and learning in the fault environment from scratch is generally the preferred approach. Furthermore, the transfer of specific com- ponents of the algorithm's knowledge, such as the models and replay buffer contents, had a significant impact on performance.\nIn conclusion, our study highlights the potential of RL algorithms as a promising solution for enhancing hardware fault tolerance in real-world machines. This solution holds promise for various industries, where it can mitigate the consequences of minor fault-related events, thereby minimizing productivity losses."}]}