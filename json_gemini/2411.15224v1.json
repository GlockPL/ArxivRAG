{"title": "Parameter Efficient Mamba Tuning via Projector-targeted Diagonal-centric Linear Transformation", "authors": ["Seokil Ham", "Hee-Seon Kim", "Sangmin Woo", "Changick Kim"], "abstract": "Despite the growing interest in Mamba architecture as a potential replacement for Transformer architecture, parameter-efficient fine-tuning (PEFT) approaches for Mamba remain largely unexplored. In our study, we introduce two key insights-driven strategies for PEFT in Mamba architecture: (1) While state-space models (SSMs) have been regarded as the cornerstone of Mamba architecture, then expected to play a primary role in transfer learning, our findings reveal that Projectors\u2014not SSMs\u2014are the predominant contributors to transfer learning, and (2) Based on our observation that adapting pretrained Projectors to new tasks can be effectively approximated through a near-diagonal linear transformation, we propose a novel PEFT method specialized to Mamba architecture: Projector-targeted Diagonal-centric Linear Transformation (ProDiaL). ProDiaL focuses on optimizing only diagonal-centric linear transformation matrices, without directly fine-tuning the pretrained Projector weights. This targeted approach allows efficient task adaptation, utilizing less than 1% of the total parameters, and exhibits strong performance across both vision and language Mamba models, highlighting its versatility and effectiveness.", "sections": [{"title": "1. Introduction", "content": "Recently, Mamba architecture [5, 13, 33, 53] has been garnered attention as a promising alternative to the Transformer architecture [45]. While Transformer models have become the foundational architecture for deep learning across various fields, they suffer from a significant limitation: their inference time increases quadratically with the length of input tokens. To overcome this limitation, Mamba architecture introduces a selective mechanism within state-space models (SSMs) [10, 14] and hardware-aware operations, allowing dynamic and linear computation with respect to input size.\nAs an alternative to Transformers, Mamba architecture has become widely used in large models [5, 9, 12, 13, 27, 40, 43] due to its computational efficiency. However, despite this efficiency, large models based on Mamba still face significant computational and memory costs during full fine-tuning. While existing parameter-efficient fine-tuning"}, {"title": "2. Related work", "content": "2.1. State Space Models\nState Space Models (SSMs) [23] are dynamic systems that represent the relationships between inputs, hidden states, and outputs over time. A notable example, Structured State Space Model (S4) [14\u201316, 21, 35, 42] is designed to handle long-range sequences with Linear Time Invariance (LTI) system, which ensures consistent outputs for identical inputs regardless of their temporal positions in a sequence. A key advantage of S4 is that its computational cost scales linearly with sequence length. However, the fixed internal state transition matrix over time restricts the model's flexibility in adjusting to changing content, limiting its effectiveness for tasks that require context-based reasoning. As a result, despite of the quadratic computational cost with respect to the sequence length, Transformers are preferred for processing sequential data [2, 39, 44, 45].\nTo address these limitations of SSMs, Mamba [5, 13] introduces a selective mechanism and hardware-aware operation to overcome the quadratic computational costs of Transformers. This enables Mamba to support context-aware reasoning with linear computational cost, extending its applications to various sequential data tasks including language and speech tasks [12, 13, 40]. Similar to Transformers, Mamba is also being applied in the vision domain. Vision Mamba models [9, 29, 33, 37, 43, 53] adopt bidirectional scanning methods to effectively represent 2-D spatial information of images as 1-D sequences. This feature extracting strategy has proven effective, being adopted not only in simple vision tasks such as classification [29, 33, 37, 53] but also in more complex vision tasks like image generation tasks [9, 43].\nDespite the success of these Mamba models and the growing scale of Mamba models, PEFT of Mamba architectures for downstream tasks remains largely unexplored. Therefore, in this work, we uncover the core component in Mamba architecture relevant to PEFT and propose a novel PEFT method tailored to Mamba, called ProDiaL, based on new insights from our analysis."}, {"title": "2.2. Parameter Efficient Fine-Tuning", "content": "Fine-tuning large models often demands significant computational and memory resources, especially when working with limited data, which might increase the risk of overfitting. To address these challenges, parameter-efficient fine-tuning (PEFT) methods have emerged, particularly targeting Transformer-based architectures. Key PEFT techniques include: (1) Adapters integrate learnable modules within pre-trained models, allowing them to train separately from the model's primary frozen weights. Adapter variations include serial configurations (e.g., Serial Adapter [19]) and parallel structures (e.g., AdaptFormer [3], ControlNet [51]). (2) Prompt Tuning uses trainable embeddings that guide downstream learning in visual tasks, optimizing model adaptation for vision applications (e.g., Visual Prompt Tuning [22], LLaMA-Adapter [52], DMP [18]). (3) Subset Fine-tuning modifies specific model parameters, such as bias terms (e.g., BitFit [48], DiffFit [47]) or selected key-value weights (e.g., Custom Diffusion [26]) within Transformer attention layers, thereby reducing training overhead. (4) Low-Rank Adaptation (LoRA) [20] incorporates low-rank matrices for fine-tuning, preserving the original model weights while facilitating effective downstream learning. This method is particularly useful for conserving model integrity within Transformer architectures (e.g., VeRA [24], DORA [32], QLORA [7], MTLORA [1]).\nDespite the success of these PEFT methods with Transformer models [45], their application to Mamba architectures [5, 13, 33, 53] remains largely unexplored. Recently, Halloran et al. [17] examined the use of PEFT techniques, specifically LoRA, within Mamba SSMs. Unlike previous works, we reveal that PEFT in Mamba architectures is more effective when applied to Projectors rather than SSMs. Building on this insight, we introduce ProDiaL, a novel PEFT method focused on efficient adaptation of Projectors, specialized to Mamba architecture."}, {"title": "3. Preliminary", "content": "To address the limitations inherent in transformer architectures\u2014specifically, the quadratic increase in computational cost with respect to the number of input tokens\u2014the Mamba architecture is introduced as a novel solution centered around the SSM.\nInitially, the SSM is a LTI system that maps an input sequence $x(t)$ to a hidden state $h(t)$ and predicts the output $y(t)$ by leveraging both $x(t)$ and $h(t)$. The process is defined by the following system of Ordinary Differential Equations (ODEs):\n```latex\n    h'(t) = Ah(t) + Bx(t),\n    y(t) = Ch(t) + Dx(t),\n```\nwhere $x(t) \\in \\mathbb{R}$ is the continuous input sequence, $h(t) \\in \\mathbb{R}^N$ is the hidden state, and $y(t) \\in \\mathbb{R}$ is the output sequence. While this equation addresses continuous signals, discretization is required for processing discrete signals. In Mamba, the Zero-Order Hold technique is used to discretize parameters $A$ and $B$, utilizing a step size parameter $\\Delta$ to derive the discrete equivalents:\n```latex\n    \\overline{A} = \\exp(\\Delta A),\n    \\overline{B} = (\\exp(\\Delta A) - I)(\\Delta A)^{-1}B,\n    \\overline{C} = C.\n```\nWith these discretized parameters $\\overline{A}$, $\\overline{B}$, and $\\overline{C}$, the SSM is reformulated to address discrete signals as follows:\n```latex\n    h_n = \\overline{A}h_{n-1}+ \\overline{B}x_n,\n    y_n = Ch_n,\n```\nwhere $n$ indexes the input sequence for the discrete signal. In contrast to the continuous formulation in Eq. (1), this discretized ODE explicitly separates current and previous states, with the hidden state $h_n$ computed from the current input $x_n$ and the previous state $h_{n-1}$.\nThe architecture of a Mamba block is a combination of the SSM-based model [10] and a gated MLP [31]. Therefore, in addition to the SSM, Mamba blocks also include two Projectors and a 1D Convolution layer, as illustrated in Fig. 1, representing the original Mamba block."}, {"title": "4. Methodology", "content": "Current PEFT methods are primarily designed for Transformer-based architectures. However, despite Mamba architecture is widely used in large size models such as LLMs or Diffusion models, it remains underexplored in terms of PEFT techniques. In this section, we analyze the impact of each component in Mamba blocks on downstream task performance by fine-tuning various combinations of these components, and propose a novel PEFT method specialized to Mamba architecture.\n4.1. Targeting Projectors for PEFT\nAs a first investigation of PEFT in Mamba architecture, there is a need to identify the optimal parameters for efficient adaptation to downstream tasks and to determine which components are most effective for applying PEFT methods. In Transformer-based models, PEFT methods typically target the attention module as the primary component for adapting to downstream tasks [1, 7, 20, 24, 32]. Following this prior knowledge, a natural approach for PEFT in Mamba might be to fine-tune the SSM, which plays a core role similar to the attention mechanism in Transformers. However, the effectiveness of fine-tuning the SSM in the Mamba architecture has not actually been explored yet."}, {"title": "4.2. ProDiaL: Projector-targeted Diagonal-centric Linear Transformation", "content": "4.2.1. Primarily Training Diagonal Entries in $T$.\nBuilding on our observation that Projectors are essential for transfer learning in Mamba architecture, we analyze the relationship between the pretrained Projector weight $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ and fine-tuned Projector weight $W' \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ to gain deeper insights. Firstly, rather than respresenting a relationship between $W$ and $W'$ as $W' = W + \\Delta W$ following [20, 24, 32], we approach this from an uncommon perspective by interpreting the relationship as linear and formulate it as follows:\n```latex\nW' = WT,\n```\nwhere $T \\in \\mathbb{R}^{d_{in} \\times d_{in}}$ represents a linear transformation matrix representing the relationship between the fine-tuned and pretrained Projector weights. Given the fine-tuned and pretrained Projector weights, we can deterministically calculate $T_{det}$ using the pseudo-inverse of $W$ as follows:\n```latex\nT_{det} = W^{-1}W',\n```"}, {"title": "4.2.2. Decomposing Diagonal and Off-diagonal Entries.", "content": "Despite our observations highlight the importance of diagonal entries in linear transformation matrix, applying the existing PEFT methods like LoRA [20] and DORA [32] to Projectors cannot fully consider these insights. Therefore, we propose a novel Projector-targeted PEFT method, which is specialized for Mamba architecture, called Projector-targeted Diagonal-centric Linear Transformation (ProDiaL). The overall framework of ProDiaL is illustrated in Fig. 3.\nConsidering the different scales of the diagonal and non-diagonal components, as shown in Fig. 2, we decompose the linear transformation matrix $T$ into a diagonal matrix $D \\in \\mathbb{R}^{d_{in} \\times d_{in}}$ and a off-diagonal matrix $b \\in \\mathbb{R}^{d_{in} \\times d_{in}}$. Given that the values of $b$ are small, we simplify the term $Wb$ as $\\epsilon$. This decomposition can be expressed as follows:\n```latex\nW' = WT = W(D + b)\n    = WD + Wb\n    = WD + \\epsilon.\n```\nThis decomposition forms the main strategy of ProDiaL: we freeze the pretrained Projector weight $W$ and fine-tune only the diagonal matrix $D$ and off-diagonal matrix $\\epsilon$, indirectly updating $W$ towards the fine-tuned Projector weight $W'$. Specifically, to enhance flexibility, we replace the diagonal matrix $D$ with a block-diagonal matrix $D_b$ and introduce a learnable scaling parameter $s$, as inspired by Fig. 4. This allows subtle transformations, such as minor rotations,"}, {"title": "5. Experiment", "content": "In this section, we evaluate the effectiveness of applying PEFT methods to Projectors within the Mamba architecture using our ProDiaL across various downstream tasks.\n5.1. Experiment Setup\nTo evaluate the effectiveness of fine-tuning Projectors using our ProDiaL, we conducted experiments on Mamba-130M [13] for Mamba LLM and Vim-tiny [53] for Vision Mamba. Specifically, we optimize Mamba LLM pre-trained on the PILE dataset [11] to reasoning tasks [4, 41, 49] and Vision Mamba model pre-trained on the ImageNet dataset [6] to other classification tasks [25, 28, 36]. For Mamba LLM, we measure accuracy by sampling $N$ checkpoints of fine-tuned model weights every $M$ iterations, selecting the checkpoint with the highest test accuracy among"}, {"title": "5.2. Experiment Results", "content": "As the first study to report downstream task performance of Mamba LLM and Vision Mamba within Mamba architecture research, we provide extensive experimental results across various datasets, as shown in Tab. 3. Given our experiments yield consistent results across both Mamba LLM and Vision Mamba, we explain the findings comprehensively.\nOur results reveal that fine-tuning only the Projectors\u2014or even a single Projector (either input or output)\u2014achieves competitive performance to full fine-tuning, but with a significantly fewer learnable parameters. However, fully fine-tuning the entire Mamba model or Projectors often leads to overfitting due to the large number of parameters relative to the small size of downstream task data, highlighting the necessity of PEFT methods.\nAmong existing PEFT methods, LoRA [20] and DORA [32] applied to Mamba's Projectors shows impressive downstream performance, demonstrating that high accuracy can be achieved with fewer parameters by targeting Projectors in Mamba. For example, compared to Strong [17]\u2014which uses LoRA to train $W_x$, both Projectors, and Embeddings\u2014applying LoRA solely to Projectors achieves significant downstream task performance while utilizing only 63.6% of the parameter counts used by Strong for Mamba LLM and 69.8% for Vision Mamba.\nBeyond the existing PEFT methods, our ProDiaL, specifically designed based on the analysis of Projectors in Mamba architecture, consistently achieves superior downstream performance with few learnable parameters. This results stem not only from targeting Projectors in Mamba architecture but also from highlighting the importance of diagonal entries observed in the linear transformation relationship between pretrained and fine-tuned Projectors. In Table 3, we align the number of parameters of ProDiaL with those of LoRA and DORA for comparison. More detailed settings, including hyperparameters for $r_b$ and $r_e$, are provided in the Supplementary Material."}, {"title": "5.3. Results Across Various Model Sizes", "content": "Table 4 highlights performance across different model sizes, including Mamba-370M and Mamba-1.4B for Mamba LLM and Vim-small for Vision Mamba, evaluated on the Caltech [28] and HellaSwag [49] datasets, respectively. Consistent with findings from smaller mod-"}, {"title": "5.4. Analysis", "content": "Q: Training Only Diagonal Components is Sufficient?\nTo empirically verify that training primarily the diagonal elements of a linear transformation matrix $T$ is sufficient, we conduct a experiment, comparing four approaches: (1) directly fine-tuning $T$, (2) fine-tuning only the diagonal-centric matrix, (3) fine-tuning only the diagonal matrix, (4) fine-tuning only the off-diagonal matrix. The four"}, {"title": "5.5. Ablation Study", "content": "Our ProDiaL consists of a learnable block-diagonal matrix $D_b$, a non-diagonal matrix $\\epsilon$, and a scaling factor $s$, which together enable effective learning of downstream tasks in Mamba architecture. To analyze the contribution of each component, we perform an ablation study by adding components incrementally. As shown in Tab. 7, fine-tuning only the diagonal-centric matrices achieves significant downstream task performance. While fine-tuning only diagonal-centric matrices proves effective, fine-tuning both the non-diagonal matrices and the diagonal matrices yields even higher downstream task performance. In addition, inspired by [38], incorporating scaling factors at the output stage further enhances downstream task performance. These results demonstrate that each component of ProDiaL contributes meaningfully to maximizing downstream task performance within the Mamba architecture."}, {"title": "6. Limitations", "content": "Our ProDiaL primarily relies on LoRA [20] to efficiently train off-diagonal matrix $\\epsilon$. Developing a method specifically for optimizing $\\epsilon$ could further enhance performance, and is an interesting direction for future work."}, {"title": "7. Conclusion", "content": "In this work, we reveal that Projectors play a critical role in transfer learning within Mamba architecture. In addition, based on our observation that fine-tuned Projectors can be approximated through a near-diagonal linear transformation, we propose ProDiaL, a novel PEFT method specifically designed for Mamba architecture. Rather than fine-tuning SSM, ProDiaL targets Projectors, optimizing them indirectly through a diagonal-centric linear transformation. This enables ProDiaL to achieve superior performance while fine-tuning less than 1% of the model parameters. Our experiments across both vision and language Mamba models demonstrate ProDiaL's effectiveness, establishing a new benchmark for PEFT in Mamba-based architecture."}, {"title": "9. Trade-off between Performance and Number of Parameters by Controlling $r_b$ and $r_e$", "content": "Our ProDiaL method offers superior flexibility in determining learnable parameters by controlling the size of small block matrices ($x_1, ..., x_{r_b}$) in the block-diagonal matrix $D_b$ using $r_b$ and the low-rank value for LoRA using $r_e$. To examine how performance and the number of parameters vary depending on $r_b$ and $r_e$, we conducted experiments using the Vim-tiny model [53] on the Caltech [28] and Flowers datasets [36]. The hyperparameter $r_{b1}$ controls the small block size of the Input Projectors, while $r_{b2}$ controls the small block size of the Output Projectors.\nInterestingly, even replacing the block-diagonal matrix with a diagonal matrix\u2014represented by the case ($r_{b1}, r_{b2}$) = (192, 384)\u2014yields comparable performance with the smallest number of parameters among the same $r_e$ values. Secondly, we observe that the optimal number of parameters for the best performance depends on the dataset. For the Caltech dataset, the highest accuracy is achieved with a relatively small number of parameters, whereas for the Flowers dataset, the best accuracy requires the largest number of parameters. This suggests that simpler datasets (those with higher baseline accuracy) can achieve high performance with fewer parameters, while more complex datasets (those with lower baseline accuracy) need a larger parameters for high performance."}, {"title": "10. Experiment Details", "content": "10.1. Models & Datasets\nFirst, Mamba LLM [13] is the first model to implement the Mamba architecture, achieving faster inference than transformer-based LLMs as input token sizes increase. For Mamba LLM, we adapt the model pretrained on the PILE dataset [11] to other reasoning task datasets: HellaSwag [49], Winogrande [41], ARC-Easy [4], and ARC-Challenge [4]. The HellaSwag dataset is a challenging benchmark for commonsense reasoning that requires contextual understanding to predict the most plausible continuation of a given scenario from multiple choices. The Winogrande dataset is a large-scale benchmark for commonsense reasoning, consisting of sentence pairs with subtle differences, requiring the model to determine the best sentence completion by resolving nuanced context clues and pronoun references. The ARC-Easy dataset, a subset of the AI2 Reasoning Challenge (ARC), contains straightforward science"}, {"title": "10.2. Training and Evaluation", "content": "Our experiments are mainly conducted on Vim [53] base on Mamba 2 architecture [5] and Mamba LLM [13] based on Mamba 1 architecture. Below, we detail the experimental settings for each dataset.\n10.2.1. Mamba LLM Experiments\nIn the experiments presented in Table 3 of the main manuscript, we use the Mamba-130M, configured with 24 layers and a maximum sequence length of 512. The dimensions for the input projectors are set as follows: input projectors have $d_{in}$ = 768 and $d_{out}$ = 3072, while output projectors are configured with $d_{in}$ = 1536 and $d_{out}$ = 768.\nFor the results in Table 4 of the main manuscript, we employ the larger Mamba-370M and Mamba-1.4B models, both configured with 48 layers and a maximum sequence length of 512. Mamba-370M uses input projectors with $d_{in}$ = 1024 and $d_{out}$ = 4096 and output projectors with $d_{in}$ = 2048 and $d_{out}$ = 1024. For Mamba-1.4B, the input projectors have $d_{in}$ = 2048 and $d_{out}$ = 8192, while the output projectors have $d_{in}$ = 4096 and $d_{out}$ = 2048.\nFor training Mamba LLM models, we use the AdamW optimizer with a batch size of 4 and a constant learning rate scheduler. Additional hyperparameters including ProDiaL's settings, which control the number of learnable parameters, are provided in Tabs. 9 and 10.\n10.2.2. Vision Mamba Experiments\nIn the experiments in Table 3 of the main manuscript, we use the Vim-tiny, which has 24 layers, a patch size of 16, and an input image size of 224. The input projectors are set with $d_{in}$ = 192 and $d_{out}$ = 768, and output projectors have $d_{in}$ = 384 and $d_{out}$ = 192. We train the Vim-tiny model for 300 epochs with a batch size of 128, using the AdamW optimizer with a learning rate of 5e-4 and weight decay of 0.1. A cosine learning rate scheduler with 5 warm-up epochs starting from 1e-6 is applied. Hyperparameters for ProDiaL, determining the number of learnable parameters, are detailed in Tab. 11.\nIn Table 4 of the main manuscript, we use the Vim-small, which also has 24 layers, a patch size of 16, and an input image size of 224. Whereas, input projectors are configured"}, {"title": "8. Algorithms of ProDiaL", "content": "The ProDiaL (Projector-targeted Diagonal-centric Linear Transformation) is a parameter-efficient fine-tuning (PEFT) method specifically designed for Mamba architecture's Projectors. It efficiently adapts pretrained Projector weights W to downstream tasks via a diagonal-centric linear transformation, significantly minimizing the number of learnable parameters. Algorithm 1 presents the full detailed ProDiaL algorithm.\nThe algorithm begins with the initialization of key components and hyperparameters. Firstly, small block matrices $x_1,..., x_{r_b}$ are initialized as identity matrices and used to construct the auxiliary block-diagonal transformation matrix $D_a$. The hyperparameter $r_b$ controls the size and the number of these small block matrices. As $r_b$ increases, the sizes of each small block decreases, and the number of small blocks increases, as illustrated in Fig. 5. When $r_b$ equals the"}]}