{"title": "JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models", "authors": ["Shuyi Liu", "Simiao Cui", "Haoran Bu", "Yuming Shang", "Xi Zhang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context.", "sections": [{"title": "1 Introduction", "content": "Large language models have achieved remarkable progress across numerous domains, exhibiting unprecedented capabilities and vast knowledge repositories that have captivated extensive scholarly inquiry [1,17]. Alongside their impressive capabilities, there are also significant concerns regarding the safety and trustworthiness of these models, such as their ability to generate harmful or"}, {"title": "2 Related Work", "content": "This section reviews existing Chinese benchmarks designed for safety evaluation, as well as the jailbreak attacks that serve as key factors in improving the effectiveness of test data."}, {"title": "2.1 Benchmarks for Safety Evaluation", "content": "Previous benchmarks have primarily focused on the specific risk assessment of LLMs, ranging from text toxicity [6,11] and social bias [22] to hallucination [28]. As the capabilities and complexity of LLMs continue to increase, a growing number of benchmarks have emerged to evaluate the overall safety of these models [20,13]. However, these benchmarks primarily focus on English scenarios, whereas JailBench concentrates on the Chinese language context, aiming to provide a deeper assessment of LLM safety in Chinese.\nWith the rapid advancement of LLMs' Chinese language capabilities, several Chinese-specific benchmarks have also been constructed [29,15,7,23]. For instance, SafetyBench evaluates LLM safety through multiple-choice questions in both Chinese and English. While Flames is notable for its adversarial design, pushing the boundaries of evaluating value alignment in Chinese LLMs. Nevertheless, these benchmarks generally suffer limited effectiveness in thoroughly evaluating LLM safety, as increasingly robust defenses against malicious prompts pose challenges for detecting deeper security vulnerabilities [15,20]. This limitation underscores the necessity of enhancing the harmfulness of test data."}, {"title": "2.2 Jailbreak Attacks on LLMS", "content": "We introduce jailbreak attacks [2] into the construction of JailBench to improve the effectiveness of thorough safety evaluation. Early jailbreak attacks on LLMs primarily relied on manually crafted scenarios specifically designed to bypass the models' safeguards [10,4]. These approaches also included translating harmful prompts into low-resource languages [24] or using cryptography to conceal harmful intentions [26]. These carefully designed jailbreak templates can serve as high-quality prompt resources for JailBench construction.\nTo minimize the human effort and time required to craft jailbreak prompts, researchers have explored various automated red-teaming methods. These approaches range from utilizing search optimization algorithms to generate adversarial prompts [31,8] to leveraging LLMs as prompt optimizers [3,27]. Particularly relevant to our work are the dynamic prompt optimization [12] and iteration techniques [25]. These techniques maintain a \"template pool\" of effective jailbreak templates which can be easily combined with standard harmful queries to rapidly generate numerous high-risk prompts. Consequently, we propose the AJPE framework, which leverages the language capabilities of LLMs to perform few-shot learning for generating more targeted and context-aware jailbreak prompts, potentially increasing the effectiveness and efficiency of jailbreak attacks, thereby providing a more rigorous and comprehensive assessment of LLMs safety within the Chinese language context.\nDifferent from existing Chinese benchmarks, JailBench incorporates advanced jailbreak attacks with automatic prompt generation for thorough safety evaluations, offering comprehensive security vulnerability identification for LLMs."}, {"title": "3 JailBench Construction", "content": "In this section, we will introduce the taxonomy definition and the dataset construction procedure in detail."}, {"title": "3.1 Safety Categories", "content": "The Basic Security Requirements for Generative Artificial Intelligence Services\u00b9 standard highlights the key security risks associated with Chinese context. Building on these requirements, we collaborate with experts in the fields of security and linguistics to develop a two-level hierarchical safety categorisation standard, which encompasses 5 distinct domains and 40 categories of risk types. The classification system is designed for comprehensive coverage, clarity, and applicability across diverse generative AI security scenarios, while its hierarchical structure enables nuanced differentiation between various risk types, thereby enhancing the accuracy of security assessment precision and facilitating targeted mitigation strategies."}, {"title": "3.2 Data Collection", "content": "The construction of JailBench involved a comprehensive data collection process to create a robust and diverse dataset for assessing LLM security vulnerabilities. This section outlines our approach for question collection and augmentation.\nQuestion Collection. To construct a comprehensive security testing dataset, we initially aggregate a substantial volume of harmful queries from publicly available datasets [5,31,29,7], which will be translated into Chinese and undergo manual correction, forming the foundational raw data for dataset construction.\nWe use distinct methods for processing labeled and unlabeled data to ensure accurate categorization. For labeled data, we find that although these datasets employed diverse classification systems, most categories could be systematically mapped to JailBench classification criteria due to its comprehensive structure. For other unlabeled data, we employ prompt engineering techniques to guide ChatGPT to accurately categorize the questions into the appropriate JailBench categories. To ensure the reliability of classification, we perform random sampling and manual verification of the data labels, rigorously ensuring that all entries are accurately annotated according to our taxonomy.\nQuestion Augmentation. Acknowledging the unique characteristics of the Chinese linguistic and cultural context, certain safety categories suffer from a scarcity of data. Considering the prohibitive cost and complexity associated with manual data generation, we continue to instruct ChatGPT to generate new instances of unsafe data through few-shot learning techniques, as shown in"}, {"title": "3.3 Jailbreak Enhancement", "content": "As modern LLMs have developed robust security measures against harmful content, conventional safety evaluations often fail to expose the potential vulnerabilities within these models. To address this challenge and uncover deeper security breaches, we leverage jailbreak attack techniques to generate more potent and inducive queries. This section details our process of integrating existing jailbreak templates and use automated prompt generation to construct JailBench.\nJailbreak Templates Integration. Due to the scarcity of instructions bypassing LLM's safeguards, manual composition or random searches prove inefficient for generating prompts at scale. An effective solution is to harness the advanced language capabilities of LLMs to learn from effective jailbreak techniques and template patterns. Consequently, our objective is to construct an extensive pool of effective jailbreak templates, enabling the rapid generation of high-risk jailbreak prompts by simply concatenating templates with existing harmful queries.\nAutomatic Jailbreak Prompt Engineer. Considering the scarcity and the limited diversity of jailbreak templates, we propose the AJPE method to address these deficiencies. AJPE introduces the concept of prompt engineering, leveraging the powerful learning capabilities of LLMs to emulate existing jailbreak templates and achieve large-scale generation of jailbreak prompts.\nAs shown in Figure 5, the AJPE workflow begins with templates testing and harmful data acquisition. We randomly sample jailbreak templates and combine them with the original questions from JailBench Seed to form a jailbreak-enhanced dataset. Based on the dataset we conduct security assessments on the target LLM groups, and the outputs of LLMs are then classified for harmfulness using the security evaluators. Templates with attack success rates below a preset threshold will be discarded and the remaining jailbreak templates as well as the corresponding harmful input-output pairs obtained during the evaluation will serve as the components for automated jailbreak prompt generation.\nThe core part of our AJPE is the prompt generation module, which operates by instructing ChatGPT to generate corresponding instructions that achieve the desired input-output objectives. By learning from a set of sample harmful text pairs, the module then produces a number of jailbreak prompts with varying levels of inducibility, mimicking the characteristics of the provided examples.\nIn the final quality screening stage, we adopt the scoring function design [30] and use the log-probability of model outputs as indicators of prompt effectiveness, which is because log-probability is commonly used to represent the LLM's confidence in the generated text. Based on the prompt scoring mechanism, We select the highest-performing prompts to include in the jailbreak template pool.\nThrough the AJPE method, we successfully expand our jailbreak prompts to over 1,000. Considering the constraints on the dataset size and the computational cost of experiments, we carefully select 20 most effective jailbreak prompts from the pool and combine them with 540 questions in JailBench Seed. This strategic combination results in the construction of the JailBench dataset, which contains 10,800 jailbreak-enhanced test cases, ensuring a comprehensive and manageable dataset that rigorously challenges the security mechanisms of LLMs while maintaining practical feasibility for extensive evaluations.\nJailBench stands out among existing Chinese safety benchmarks, as shown in Table 1, featuring a substantial dataset of 10.8k queries, the broadest range of classification categories, and the highest ASR against ChatGPT. These characteristics make JailBench an effective tool for LLM security assessment."}, {"title": "4 Experiments", "content": "In this section, we first introduce our experimental setup. To validate the effectiveness of JailBench, evaluate potential security vulnerabilities in target LLMs, and assess the attack efficiency of AJPE, we conduct large-scale experiments on a diverse set of popular LLMs and derive several relevant research conclusions."}, {"title": "4.1 Experiment Setup", "content": "Evaluated Models. As shown in Table 2, we evaluate 13 widely recognized models that are proficient in generating Chinese content.\nEvaluation Metrics. We employ the Attack Success Rate (ASR) as our primary metric to assess the defensive capabilities of LLMs against jailbreak attacks, which is defined as the ratio of test queries that successfully breach LLMs' safety guardrails and induce harmful outputs n to the total queries m:\n$$ASR = \\frac{n}{m}$$\nTo evaluate the effectiveness of AJPE-generated jailbreak prompts, we further consider the impact of instruction length on the efficiency of attacks. We introduce the Attack Efficiency (AE) metric, which incorporates a ratio between the length of each successful jailbreak prompt $l_s$ and the length of the original prompt $l_o$, with k representing the scaling factor:\n$$AE = k \\times \\frac{1}{m} \\sum_{i=1}^{n} e^{\\frac{l_{s,i}}{l_{o,i}}}$$"}, {"title": "4.2 Results and Analysis", "content": "JailBench Evaluation. We conduct comprehensive experiments to evaluate the safety vulnerability of 13 LLMs using our JailBench Seed and JailBench datasets. Table 2 presents the overall performance of the evaluated LLMs.\nFirstly, JailBench substantially increases the ASR compared to the original dataset (3.19% to 58.95%), while also achieving the highest ASR against ChatGPT among existing Chinese benchmarks. These results clearly demonstrate the effectiveness of the AJPE-enhanced dataset in exposing LLM security vulnerabilities and underscore the importance of deep-seated safety evaluations.\nSecondly, LLMs exhibit varying levels of vulnerability across different categories and models. GPT-4 achieves the strongest overall safety performance with the lowest ASR, while Mistral-7B-Instruct shows the highest ASR across all domains, underscoring its inadequate safety alignment within the Chinese context. Notably, domestically developed Chinese LLMs show lower ASR (0.93%) compared to their international counterparts (4.61%), particularly excelling in the Violation of Core Socialist Values category, suggesting that they have undergone stricter safety management during training on Chinese-language corpora.\nLastly, for jailbreak-enhanced datasets, models with larger parameter sizes within the same LLM families generally exhibit higher jailbreak vulnerability. This suggests that more powerful models may be more susceptible to jailbreak attacks, highlighting a potential trade-off between LLM capability and safety. Alternatively, newer LLMs consistently demonstrate improved safety performance, indicating better alignment with human values and greater trustworthiness.\nAJPE Evaluation. To assess the effectiveness of prompts generated by AJPE, we compare AJPE against 5 powerful jailbreak methods. Table 3 presents the results in terms of Attack Success Rate (ASR) and Attack Efficiency (AE).\nFirstly, AJPE achieves the highest average ASR among all methods, demonstrating its superior ability to assimilate key features of existing jailbreak attacks while integrating novel jailbreak strategies with enhanced inducement capabilities, thereby elevating the potency and scope of the generated prompts. Although AutoDAN and Jailbroken also achieve impressive ASRs for certain LLMs, their AE is comparatively lower, which can be attributed to their reliance on manually crafted, lengthy jailbreak templates including redundant components.\nSecondly, AJPE also attains the highest average AE, indicating its capacity to remove ineffective components of manually crafted jailbreak prompts and integrate more potent content to bypass LLM safeguards within relatively concise jailbreak templates. In contrast, DeepInception exhibits the second-highest AE, but its efficiency comes at the cost of the lowest average ASR (5.0%), highlighting the trade-off between attack brevity and success rate.\nIn summary, AJPE strikes a balance between high success rate and efficiency, demonstrating its ability to generate targeted and inducing testing prompts. This positions AJPE as a valuable tool for identifying vulnerabilities in LLM safety mechanisms, contributing significantly to the construction of JailBench. The results also underscore the importance of considering both ASR and AE when evaluating the overall effectiveness of jailbreak techniques."}, {"title": "5 Conclusion", "content": "In this paper, we introduce JailBench, the first comprehensive Chinese benchmark for assessing safety vulnerabilities in LLMs, comprising 10,800 jailbreak-enhanced questions with a 73.86% ASR against ChatGPT. We also develop a safety taxonomy and AJPE framework for prompt construction. Evaluation on 13 LLMs reveals JailBench's effectiveness and persistent challenges in LLMs safety, particularly against jailbreak attacks. This study highlights the importance of LLM security assessments and its alignment with ethical standards."}]}