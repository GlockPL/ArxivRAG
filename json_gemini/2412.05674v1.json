{"title": "No-Free-Lunch Theories for Tensor-Network Machine Learning Models", "authors": ["Jing-Chuan Wu", "Qi Ye", "Dong-Ling Deng", "Li-Wei Yu"], "abstract": "Tensor network machine learning models have shown remarkable versatility in tackling complex data-driven tasks, ranging from quantum many-body problems to classical pattern recognitions. Despite their promising performance, a comprehensive understanding of the underlying assumptions and limitations of these models is still lacking. In this work, we focus on the rigorous formulation of their no-free-lunch theorem essential yet notoriously challenging to formalize for specific tensor network machine learning models. In particular, we rigorously analyze the generalization risks of learning target output functions from input data encoded in tensor network states. We first prove a no-free-lunch theorem for machine learning models based on matrix product states, i.e., the one-dimensional tensor network states. Furthermore, we circumvent the challenging issue of calculating the partition function for two-dimensional Ising model, and prove the no-free-lunch theorem for the case of two-dimensional projected entangled-pair state, by introducing the combinatorial method associated to the \"puzzle of polyominoes\". Our findings reveal the intrinsic limitations of tensor network-based learning models in a rigorous fashion, and open up an avenue for future analytical exploration of both the strengths and limitations of quantum-inspired machine learning frameworks.", "sections": [{"title": "Tensor network machine learning.", "content": "Tensor networks (TNs) have emerged as a powerful tool for studying quantum many-body systems, demonstrating remarkable versatility across various domains of quantum physics [1-6]. This success has catalyzed a growing interest in harnessing TNs for machine learning applications [7-59], where they have shown promise in diverse areas such as dimensionality reduction [9, 10], model compression [29, 54], natural language processing [59-61], generative models [17, 49]. In particular, TNs have demonstrated distinct advantages in the development of efficient, interpretable machine learning models [47]. Meanwhile, TN-based machine learning models offer intriguing theoretical and practical advantages, including insights into learning theory and the development of quantum machine learning frameworks. In addition, TNs provide a powerful framework for developing quantum machine learning theories, showing potential exponential advantages over those classical models [22, 62, 63]. Recent advancements also highlight the use of TNs to mitigate barren plateaus in quantum learning models [64]. On the practical side, TN techniques, including canonical form and renormalization [11, 16], are instrumental in optimizing and training machine learning models, further supported by the development of open-source libraries [65, 66]. In the vein of TN-based machine learning theory, a number of pioneering works have been conducted, including these on the barren plateau problem [67-71], model generalization [72], and mutual information scaling [44]. Despite this rapid progress, the field of TN-based machine learning is still in its infancy, with many fundamental aspects remaining largely unexplored. Here, we study the generalization ability of TN-based machine learning models, with a focus on establishing their no-free-lunch (NFL) theorem.\nThe NFL theorem is one of the most fundamental theorems in the classical machine learning theory [73-75]. It states that, averaged over all possible problems, every algorithm performs equally well when applied to problems they were not specifically designed for. Consequently, the average performance of a machine learning model across all target functions is highly dependent on the size of the input set, highlighting the importance of large datasets in building robust models, such as large language models. Inspired by the critical role of NFL theorem in classical machine learning, significant progress has been made in developing NFL theorem for quantum learning models [76-82]. The quantum NFL theorem establishes straightforward connections between quantum features and the capabilities of quantum learning models. For instance, in practical quantum learning setups with a finite number of measurements, entangled data exhibits a dual effect on prediction errors. With sufficient measurements, highly entangled data can reduce prediction errors. This is consistent with the ideal case of infinite measurements [78]. Conversely, with few measurements, highly entangled data can amplify predicting errors [81]. These results highlight how quantum features contribute to the advantages in quantum machine learning models.\nFor TN-based machine learning models, the classical data is encoded in TN states with specific structures, and the corresponding data distribution can be quite different from those in other machine learning models. It is hence highly desirable to investigate how to rigorously formulate the NFL theorem for the specific TN-based models, as well as to study how the bond and physical dimensions of TNs would influence the lower bounds of the average risks. We face two major challenges in establishing the NFL theorem for higher-dimensional TN models: The first one is to calculate the variance of random TN states. This can be mapped to calculating the partition function of a high-dimensional Ising model, which is notoriously a challenging task. The second one is to embed the information learned from the training set (corresponding to the diagonalized sectors of the matrix $M^{+}$ in Fig. 1) into the higher-order tensor properly, which notably differs from the case for quantum learning models [76, 78] and requires judicious design. In this paper, we tackle these challenges and rigorously prove the NFL theorem for TN models. Our setup is to learn a target unitary operation $M$ based on data samples encoded into TN states, see Fig. 1 for illustration. We first prove the NFL theorem for the case of 1D matrix product state (MPS). We show that the lower bound of the average predicting risk highly depends on the size of linearly independent training samples. We then focus on the 2D projected entangled-pair states (PEPS). We note that proving the NFL theorem for 2D PEPS poses significant challenges, as it equates to calculating the partition function of the 2D Ising model, which is a notoriously difficult problem. Here, we circumvent this obstacle by introducing a combinatorial method associated to the \"puzzle of polyominoes\" [68]. In addition, we carry out numerical simulations to support our analytical results.\nWe consider a task of learning the unknown unitary operation $M$ based on the input of TN states. Without loss of generality, we take the 1D MPS for demonstration. The MPS under periodic boundary condition has the form $|\\psi\\rangle = \\sum_{i} tr[A^{(1)}A^{(2)}...A^{(n)}]|i_1, i_2,..., i_n\\rangle$, where $A^{(k)}$ denotes the $D \\times D$ tensor with $D$ representing the bond dimension, $i^{(k)}$ denotes the state of k-th physical site with physical dimension $d$. We define the unitary embedded MPS by converting each $D \\times D \\times d$ tensor $A^{(k)}$ to a $Dd \\times Dd$ unitary $U^{(k)}$ [83-88], as depicted in Fig. 1. We define the labeled training set $S = \\{(|\\psi_j\\rangle, |\\phi_j\\rangle)|j = 1, 2, ...t\\}$, where the site size $|S| = t$, the MPS $|\\psi_j\\rangle$ belongs to the feature Hilbert space, and the state $|\\phi_j\\rangle = M|\\psi_j\\rangle$ belongs to the label Hilbert space. We learn the target unitary $M$ by minimizing the loss function $L = \\frac{1}{t}\\sum_{j=1}^{t} | M|\\psi_j\\rangle - P_s|\\psi_j\\rangle |^2 / |\\langle\\psi_j|\\psi_j\\rangle|^2$, where $P_s$ denotes the variational quantum circuit with sufficient expressivity. If the model is properly trained, then one has $P_s|\\psi_j\\rangle = M|\\psi_j\\rangle$, $\\forall |\\psi_j\\rangle \\in S$ up to an overall phase. To quantify the predicting accuracy of our TN model on learning the target $M$, we define the predicting risk function by the following trace-norm formula\n$R_M(P_s) = \\int dx|| (M|x\\rangle\\langle x|M^{\\dagger} - P_s|x\\rangle\\langle x|P_s^{\\dagger})/(|\\langle x|x\\rangle|)||_1$,\nwhere $||A||_1 = tr[\\sqrt{A^{\\dagger}A}]$ denotes the trace norm of $A$, $|x\\rangle$ represents the unitary embedded MPS, and the integral is over the Haar measure of all local unitary tensors $\\{U^{(i)}|i = 1,2,......n\\}$ in Fig. 1. $R_M(P_s)$ represents the prediction error of the trained model. For proper learning without training errors, $R_M(P_s)$ is equivalent to the generalization error [89]. We note that the norm $|\\langle x|x\\rangle|$ is exponentially concentrated around one [88]. The risk function describes the probability that $P_s$ fails to reproduce the target unitary $M$ with the random MPS as the testing input [76]. One can extend the above MPS-based learning model to higher dimensional tensor-network based models, by replacing $|\\psi\\rangle$ and $|x\\rangle$ into the higher dimensional TN states. With the above risk function, one can then study the NFL theorem for TN-based models in learning a target unitary operation $M$.\nThe 1D case. The MPSs have found broad applications in machine learning, particularly in the context of learning and representing complex patterns and data, including dimensionality reduction, generative models, and so on. Now we consider the NFL theorem for the case of MPS. To showcase the NFL theorem, we adopt the risk function $R_M(P_s)$ and calculate the average risk over arbitrary target unitary $M$ and arbitrary training set $S$. Here for simplicity, we suppose that the states in the training set are linearly independent [78].\nDefine the risk function $R_M(P_s)$ in Eq. (1) for learning a target n-qubit unitary $M$ based on the input of MPSs, where $P_s$ represents the hypothesis unitary learned from the training set $S$. Given a linear independent training set with size $t_k = d^n - d^{n-k}$, the integer $k \\in [1, n-1]$, $d$ is the physical dimension of MPS, and $n$ denotes the qubit number of the system. The average risk is lower bounded by\n$E_{M,s} [R_M(P_s)] \\geq 1 - (1 - \\frac{2}{d^n})(1 + (dAB)^n) - (\\frac{1}{d^k} + \\frac{1}{d^{n-k}})(A^2 + B^2)(1 + (dAB)^{n-k}),$\nwhere $A = \\frac{D+1}{Dd+1}$, $B = \\frac{D-1}{Dd-1}$, and $D$ is the bond dimension of MPS.\nWe sketch the main idea here and leave the detailed proof to the Supplemental Materials [90]. Since the norm $|\\langle x|x\\rangle|$ in Eq. (1) is exponentially concentrated around one [88], one simplifies the risk function $R_M(P_s)$ to be $R_M(P_s) \\rightarrow 1 - \\int dx |\\langle x|M^{\\dagger}P_s|x\\rangle|^2$. For the convenience of analytical calculations, we consider the training set $S$ with size $|S| = t_k = d^n - d^{n-k}$. For proper learning where $P_s|\\psi_j\\rangle = M|\\psi_j\\rangle$, $\\forall |\\psi_j\\rangle \\in S$ up to an overall phase, we consider the case that the learned unitary $P_s$ obeys $M^{\\dagger}P_s = W = e^{i\\theta}I_{t_k} \\oplus Y$, where the global phase term $e^{i\\theta}I_{t_k}$ locates in the space of the training set, $I_{t_k}$ denotes the $t_k \\times t_k$ identity matrix, and the $d^{n-k} \\times d^{n-k}$ unitary matrix $Y$ locates in the complementary subspace. For simplicity, we suppose that the unitary $Y$ locates in an $(n-k)$-qudit subsystem. Utilizing the property that each local random unitary $U^{(i)}$ of the unitary embedded MPS $|x\\rangle$ constitutes approximately unitary 2-design [67, 88], one can map the 2-moment integral of $|x\\rangle$ into calculating the partition function of 1D classical Ising model. Averaging over all possible $M$ and all training sets $S$, one obtains the analytical lower bound of $E_{M,S}[R_M(P_s)]$. This leads to Eq. (2) and completes the proof.\nTheorem 1 establishes an analytical lower bound for the average risk of the MPS-based machine learning model, and thus quantifies the capability of the model in learning an arbitrary target unitary with an arbitrary training set. Eq. (2) indicates that the average risk depends solely on the training set size $|S| = t_k$. Through the numerical calculations, we show that the average risk decreases monotonically with respect to the increasing $|S|$. Rigorous proofs show that the average risk is lower bounded by zero for the full training set, whereas the average risk is lower bounded by one for the empty training set. These results formalize the NFL theorem for MPS-based machine learning models. Apart from the case of learning arbitrary target unitaries from the input of MPSs, we also consider the case of learning matrix product operators from quantum states, and analytically formulate the corresponding NFL theorem [90].\nThe 2D case. - 2D TN states present notable advantages for studying quantum many-body systems and computational applications [91-99]. They enable efficient representation of large quantum systems with reduced computational costs, respecting the area law for entanglement entropy [100]. However, the contraction of a general PEPS without any canonical form is a #P-hard problem [101], where the complexity of computing different physical properties, e.g., the norm and expectation value, shall grow exponentially with respect to the system size. Here we formulate the NFL theorem for the PEPS-based machine learning models.\nDefine the risk function $R_M(P_s)$ in Eq. (1) for learning a target $L^2$-qubit unitary $M$ based on the input of PEPS, where $P_s$ represents the hypothesis unitary learned from the training set $S$. Given a linear independent training set with size $t_k = d^{L^2} - d^{L^2-k}$, the integer $k \\in [1, L^2-1]$, $d$ represents the physical dimension and virtual dimension of PEPS, and $L^2$ denotes the qubit number of the system. The average risk is lower bounded by\n$E_{M,S} [R_M(P_s)] \\geq 1 - (1 + c(0.7)^2) [1 - \\frac{2}{d^{L^2-k}} + (1 + \\frac{2D^4d-2}{D^4d^3-d})^k (1 + G(1/4, 1/D^2))^{2l}],$\nwhere $l = [\\sqrt{k}]$, $G(q,p) = (\\frac{\\sqrt{(1+q)(1+q-qp)}}{1 - q(2+p) + q^2(1-p)} -1)$, D is the bond dimension of the PEPS and c is a constant.\nWe outline the main idea here and leave the details to the Supplemental Materials [90]. For the 2D case, one critical step is to map the calculation of the second moment for 2D uniatry embedded PEPS into calculating the partition functions of 2D classical Ising models, which is typically a challenging problem. To address this, we introduce the combinatorial method based on the \"puzzle of polyominoes\" [68], and thus convert the problem of calculating partition functions into enumerating the directed polyominoes in a planar graph [102]. Then by utilizing the generating function $G(p,q)$ for enumerating the directed polyominoes, one can efficiently compute the value of the second moment integral of 2D unitary embedded PEPS. Consequently, one obtains the analytical lower bound of the average risk function $E_{M,S}[R_M(P_s)]$. This leads to Eq. (3) and completes the proof.\nTheorem 2 establishes a lower bound for the average risk function of PEPS-based machine learning models. It indicates that, in the task of learning an arbitrary unitary with the input of PEPS, when averaged over arbitrary training datasets and learning models, the generalization risk of a PEPS-based learning model depends solely on the size $t_k$ of the training set. As the size $t_k$ increases, the average risk decreases, eventually reaching zero when the training set is complete. These results thus rigorously formalize the NFL theorem for the PEPS-based machine learning models.\nOur NFL theorems for TN-based models are different from those for classical machine learning models. In classical supervised learning, to learn a target function $f$ based on the labeled training set $S = \\{(X_i, Y_i)|X_i \\in X, Y_i = f(x_i) \\in Y\\}_{i=1}^t$, one typical NFL formulation is $E_{f,s} [R_f(h_s)] \\geq (1 - 1/|Y|) (1 - t/|X|)$, where $|X|$ ($|Y|$) represents the dimension of $X$ ($Y$), and the risk function $R_f(h_s) = Pr[h_s(x) \\neq f(x)]$ means the probability that the hypothesis output $h_s(x)$ differs from $f(x)$. We notice that the lower bound of the average risk for such classical models is determined by the feature dimension $|X|$ and label dimension $|Y|$ of training samples, as well as the size $t$ of training set. Whereas for TN-based models, the lower bound of the average risk is not only determined by the size of training set, but also dependent on the bond dimension $D$ and physical dimension $d$ of the TN states. Our theorems highlight the influence of the TN model's internal structure on its performance limits."}, {"title": "Numerical results.", "content": "In our previous theorems, we have analytically obtained the lower bound of the average generalization risk. To show how these theorems perform in practice, we carry out numerical simulations based on the open-source package ITensors.jl [103, 104] in the Julia programming language. Based on the MPS machine learning model, we consider the supervised task of learning target unitaries $U$ with the labeled training samples $\\{(|\\psi_j\\rangle, U|\\psi_j\\rangle)\\}$, where $|\\psi_j\\rangle$ denotes the normalized MPS. We then plot the average generalization risks of the trained MPS-based learning models with respect to different qubit size $n$ of learning models, as depicted in Fig. 2. For example, in $n = 4$, we randomly generate a 16-dimensional target unitary and a training set of MPSs, and conduct the MPS-based supervised task. By repeatedly conducting learning tasks for different target unitaries, one obtains the average generalization risk for different size of training sets. We see from Fig. 2 that the average error risks decrease with respect to the training set size. This is consistent with the analytical lower bound of the average risks predicted in our theorems."}, {"title": "Discussion.", "content": "Our results provide a fundamental understanding on the generalization limits of TN-based models, extract how the performance of these specifically structured models would be limited by the NFL theorem, and analytically unveil that the lower bound of the average risk depends on both the bound and physical dimensions of TNs. Our findings would inspire further research on the learning capabilities of TN-based models with quantum computer, where TNs are employed as efficient representations of quantum circuit models. One potential direction is to incorporate the issues of practical quantum computing hardware, such as the noise and finite measurement times [81], into the analytical study of generalization ability for TN-based learning models. From the perspective of experiments, future research could focus on experimentally validating NFL bounds in practical quantum computing environments. As quantum hardware continues to advance, testing these theoretical predictions on real quantum systems will be crucial to understanding how NFL constraints manifest in noisy, resource-limited settings. Such experiments would also help refine our theoretical models, potentially revealing new strategies for optimizing TN-based machine learning models for practical applications. In addition, our methods can be applied to study the generalization of deep quantum neural network [105], which can be efficiently trained and has been experimentally realized on a superconducting quantum processor [106]. We observe that the deep model can be mapped to an MPS-based model. Thus our framework can be naturally adapted to formulate the NFL for deep quantum neural networks.\nIn summary, we have rigorously formulated the NFL theorems in the TN-based machine learning models. Particularly, we consider the supervised task of learning arbitrary target unitary based on the TN models, and then present the analytical lower bounds for the average risk of the models. Our results reveal the intrinsic limitations in learning arbitrary unitaries from input states encoded via TNs. The risk bounds, which depend on both the bond and physical dimensions, provide a quantitative understanding of the connections between model generalization and training set size. In addition, to validate our theoretical predictions, we numerically conduct the practical supervised learning task and show that the average generalization risks over sufficient number of learning tasks and input sets decrease with respect to the size of training dataset. Our results offer valuable guidelines for designing more efficient models, and open promising research directions aimed at improving the generalization capabilities of quantum-inspired TN machine learning systems."}]}