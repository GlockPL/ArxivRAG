{"title": "Inverse Delayed Reinforcement Learning", "authors": ["Simon Sinong Zhan", "Qingyuan Wu", "Zhian Ruan", "Frank Yang", "Philip Wang", "Yixuan Wang", "Ruochen Jiao", "Chao Huang", "Qi Zhu"], "abstract": "Inverse Reinforcement Learning (IRL) has demonstrated effectiveness in a variety of imitation tasks. In this paper, we introduce an IRL framework designed to extract rewarding features from expert trajectories affected by delayed disturbances. Instead of relying on direct observations, our approach employs an efficient off-policy adversarial training framework to derive expert features and recover optimal policies from augmented delayed observations. Empirical evaluations in the MuJoCo environment under diverse delay settings validate the effectiveness of our method. Furthermore, we provide a theoretical analysis showing that recovering expert policies from augmented delayed observations outperforms using direct delayed observations.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has achieved remarkable success across diverse domains, including video and board games (Berner et al., 2019; Silver et al., 2017), robotics (Kormushev et al., 2013), and safety-critical autonomous systems (Wang et al., 2023a,b; Zhan et al., 2024b). Despite these advancements, RL heavily relies on the quality of the reward function, which often demands significant domain expertise, labor, and time to design (Russell, 1998). To address this challenge, Kalman (1964) introduced the concept of the inverse problem of optimal control theory, providing a way to bypass explicit reward or cost function specification. With the integration of machine learning techniques, Imitation Learning (IL) has evolved into two main branches. Behavior Cloning (BC (Torabi et al., 2018)) directly learns from expert demonstrations by aligning with the distribution of expert behaviors. In contrast, Inverse Reinforcement Learning (IRL (Arora and Doshi, 2021)) focuses on extracting reward functions from expert behavior to guide policy learning. These methods have shown significant promise in real-world applications, including autonomous driving (Codevilla et al., 2018) and legged locomotion (Peng et al., 2020). However, expanding IL to broader applications still requires attention to address real-world environmental challenges, such as hybrid dynamic (Hempel"}, {"title": "2. Related Works", "content": "Delayed RL. Delayed signals within the RL setting can be categorized into three scenarios namely rewards delay, actions delay, and observations delay. Rewards delay has been studied extensively (Han et al., 2022; Arjona-Medina et al., 2019; Zhang et al., 2023), and in this study, we concentrate on observations and actions delay, which have also been proved to be the equivalent problem (Katsikopoulos and Engelbrecht, 2003b). Early approaches apply RL techniques to the original state space. While it maintains high computational efficiency, the performance significantly deteriorates in delayed observation due to the absence of Markovian property. Subsequent improvements leveraged various predictive models like deterministic generators (Walsh et al., 2009), Gaussian models (Chen et al., 2021), and transformer architectures (Liotet et al., 2021). Additionally, there are also attempts building upon the world model with delay adaption (Karamzade et al., 2024; Valensi et al., 2024). However, learning from the original state space cannot effectively compute approximation errors accumulated from observation delays, causing the performance to deteriorate with suboptimal solutions in delayed settings (Liotet et al., 2021). The augmentation-based approach augments the state/action space with relevant past actions/states. This approach is notably more promising as it restores the Markovian property (Altman and Nain, 1992; Katsikopoulos and Engelbrecht, 2003a; Kim et al., 2023; Wu et al., 2024a) but possesses inherent sample complexity/efficiency issues. Solving this issue, Wu et al. (2024b) proposes a novel framework to transfer the original delayed RL objective into a behavior cloning of the delay-free policy through variational inference techniques. In contrast to the number of delayed RL research, there are limited works in the delayed IRL setting.\nInverse RL. Early research in Inverse Optimal Control (IOC) (Kalman, 1964) focused on recovering optimal control rule that maximizes margins between expert demonstrations and alternative policies, which evolves into machine-learning version Inverse Reinforcement Learning (IRL)(Ng et al., 2000). Bayesian methodologies later explored varied reward priors - from Boltzmann distributions (Ramachandran and Amir, 2007; Choi and Kim, 2011; Chan and van der Schaar, 2021) to Gaussian Processes (Levine et al., 2011). Concurrent development on statistical approaches expanded the field through multi-class classification (Klein et al., 2012; Brown et al., 2019) and decision tree methods (Levine et al., 2010). Entropy-based optimization leverages the Maximum Entropy(ME) principle (Shore and Johnson, 1980) to determine trajectory distributions from reward parameters. Ziebart et al. (2008, 2010) reformulated reward inference as a maximum likelihood problem using a linear combination of hand-crafted features. Wulfmeier et al. (2015) extended it to deep neural network representation, while Finn et al. (2016) added importance sampling for model-free estimation. Inspired by Generative Adversarial Networks, the latest advances in IRL center on adversarial methods, where discriminator networks learn reward functions by distinguishing between expert and agent behaviors (Ho and Ermon, 2016; Fu et al., 2017). There are extensions to solve sample efficiency (Kostrikov et al., 2018; Blond\u00e9 and Kalousis, 2019) and to solve stochasticity MDP issue (Zhan et al., 2024a). As for IRL under delayed scenarios, there has been some attention on delayed rewards (Krishnan et al., 2016, 2019). However, to our knowledge, few have considered delayed action and observations under IRL settings."}, {"title": "3. Preliminaries", "content": "In delay-free IRL setting, we consider a standard MDP without reward function M' defined as a tuple (S, A, T, \u03b3, \u03c1), where S is the state space s \u2208 S, A is the action space a \u2208 A, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition probability function, \u03b3 \u2208 (0,1) is the discount factor, and po is the initial state distribution. The discounted visitation distribution of trajectory \u0442 with policy \u03c0 is given by:\n$$p(\u03c4) = \u03c1_0 \\prod_{t=0}^{T-1} T(s_{t+1}|s_t, a_t)\u03c0(a_t|s_t),$$\nwhere T is the horizon. Given an MDP M' without reward and expert trajectories collected in a data buffer Dexp = {T1, ..., Tn} where ti represents individual trajectories collected using expert demonstration policy \u03c0\u0395 : S \u2192 A, IRL infers reward function Re : S \u00d7 A \u2192 R, where \u03b8 is the reward parameter. Maximizing the entropy of distribution over paths subject to the feature constraints from observation (Ziebart et al., 2008, 2010), the optimal reward parameters are obtained by\n$$\u03b8^* = arg \\max_\u03b8  \\sum_{D_{exp}} log p(\u03c4_i|\u03b8).$$\nUnder delayed RL settings, we consider MDPs with an observation delay between the action taken and when its state transition and reward are observed, termed delayed MDPs. Assuming under a constant observation delay \u2206, a delayed MDP M\u25b3 inherits the Markov property based on the augmentation approaches (Altman and Nain, 1992; Katsikopoulos and Engelbrecht, 2003a). It can be formulated as a tuple (X, A, T\u2206, R\u2206, \u03b3, \u03c1\u2206). The augmented state space is defined as X := S \u00d7 A\u25b3, where an augmented state xt = {St-\u2206, at-\u2206,\u2026\u2026,at\u22121} \u2208 X. The delayed transition function is defined as:\n$$T_\\triangle(x_{t+1}/x_t, a_t) := T(s_{t-\\triangle+1}|s_{t-\\triangle}, a_{t-\\triangle})\\delta_{a_t}(a') \\prod_{i=1}^{\\triangle - 1} \\delta_{a_{t-i}}(a'_{t-i}),$$\nwhere 8 is the Dirac distribution. The delayed reward function is defined as R\u25b3(xt,at) :=\nEst~b(\u00b7\\xt) [R(st, at)] where b is the belief function defined as:\n$$b(s_t|x_t) := \\prod_{i=0}^{\\triangle-1} T(s_{t-\\triangle+i+1}/s_{t-\\triangle+i}, a_{t-\\triangle+i})ds_{t-\\triangle+i+1}.$$\nAnd the initial augmented state distribution is defined as p\u2206 = \u03c10 \u03a0=1 \u03b4\u03b1;. Correspondingly, we can define the trajectory visitation probability in the delayed MDP M\u25b3 with policy \u03c0\u25b3 as:\n$$p(\u03c4_\\triangle) = \u03c1_\\triangle \\prod_{t=0}^{T-1} T_\\triangle(x_{t+1}/x_t, a_t)\u03c0_\\triangle(a_t|x_t).$$\nUnder the delayed IRL setting, expert trajectories exhibit temporal misalignment, where the observed sequence follows the pattern (st-\u2206, at, St-\u2206+1,...). This misalignment raises a fundamental question: what kind of representation should be used in reward shaping to recover the expert policy? Specifically, the policy and reward can be conditioned on the delayed observation state st\u2212\u2206,"}, {"title": "4. Theoretical Analysis", "content": "In this section, we analyze the difference in optimal performance when the same IRL algorithm is applied using either the delayed observation state or the augmented state. It is important to note that the reward functions in these two cases are also defined differently depending on different inputs. To quantify this performance difference, we first examine the discrepancy between the recovered reward functions under each state representation. We then extend this analysis to evaluate the impact on the value function, assuming the same policy optimization method is used. We assume that learned reward functions and transition dynamic functions satisfy the Lipschitz Continuity (LC) property, which is a common assumption that appears in RL setting (Rachelson and Lagoudakis, 2010). And reward functions are bounded by a maximum value Rmax.\nDefinition 1 (Lipschitz Continuous Reward Function (Rachelson and Lagoudakis, 2010)) A reward function R is LR-Lipschitz Continuous, if, \u2200(s1, a1), (s2, a2) \u2208 S \u00d7 A, it satisfies\n$$d_R(R(s_1, a_1) - R(s_2, a_2)) \\leq L_R(d_S(s_1, s_2) + d_A(a_1, a_2)).$$\nDefinition 2 (Time Lipschitz Continuous Dynamic (Metelli et al., 2020)) A dynamic is LT-Time Lipschitz Continuous, if, \u2200(s1, a1), (s2, a2) \u2208 S \u00d7 A, it satisfies\n$$W_1(T(\\cdot|s, a)||\u03b4_s) \\leq L_T.$$\nwhere Euclidean distance d is adopted to describe distance in a deterministic space (e.g., ds for state space S, da for action space A and dr for reward space R), and L1-Wasserstein distance (Villani et al., 2009), denoted as W\u2081, is used in a probabilistic space. From the above assumptions, we can infer the Lipschitz continuity on belief function. Detailed proof can be found in Appendix A.\nLemma 3 (Time Lipschitz Continuous Belief) Given a Lr-Time Lipschitz Continuous Dynamic, the belief b is LR-Time Lipschitz Continuous, \u2200xt \u2208 X, satisfying\n$$W_1(b(x_t)||s_{t-\\triangle}) \\leq \\triangle L_T.$$\nNext, we extend the continuity bounds to the learned reward functions defining on delayed observation state and augmented state respectively. Note that we assume both reward functions are recovered using the same IRL algorithm and are parameterized by MLP with ReLU activation, which satisfies the Lipschitz continuous assumption (Virmaux and Scaman, 2018). Details of the IRL algorithm and implementation are presented in Section 5, and the detailed proof is in Appendix A.\nLemma 4 (Reward Delayed Difference Upper Bound) Given a LR-Time Lipschitz Continuous Dynamic and LR-Lipschitz Continuous Reward function, \u2200xt \u2208 X, the upper bound of reward delayed difference is as follows:\n$$d_R (R_\\triangle(x_t, a_t) \u2013 R(s_{t-\\triangle}, a_t)) \\leq \\triangle L_R L_T.$$"}, {"title": "5. Off-Policy Inverse Delayed RL", "content": "In this section, we present the framework of our off-policy Inverse Delayed Reinforcement Learning (IDRL) approach. We begin by introducing the overall structure of the framework and provide a detailed explanation of the adversarial formulation for the reward function, along with the underlying intuition and adaption to the off-policy framework. Following this, we delve into the specifics of the algorithm, including the augmentation of expert trajectories with delayed observations and the policy optimization techniques employed. The overall algorithmic framework can be found in Algorithm 1."}, {"title": "Adversarial Formulation.", "content": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) inspire our adversarial framework, where a binary discriminator De(xt, at) is trained to distinguish between augmented state-action samples from expert demonstrations and those generated by the imitator policy \u03c0\u25b3, where De has the following form:\n$$D_\u03b8(x, \u03b1) = \\frac{exp(R_\u03b8(x, \u03b1))}{exp(R_\u03b8(x, \u03b1)) + \u03c0_\\triangle(a|x)},$$\nwhere Re is a multilayer perceptron (MLP) parameterized by \u03b8 and can be interpreted as the reward used with little modification introduced in the next paragraph. In this context, the imitator policy \u03c0\u25b3 serves as a generator, improving itself to \"fool\u201d the discriminator by making its generated samples indistinguishable from expert samples. The discriminator is trained using the following cross-entropy loss, designed to classify samples as either coming from the expert or the imitation policy:\n$$L_{disc} = -E_{D_{exp}} [log D_\u03b8(x, \u03b1)] \u2013 E_{\u03c0_\\triangle} [log(1 \u2013 D_\u03b8(x,a))].$$\nThe proof of state-action occupancy match between the policy induced from the above adversarial formulation and expert policy have been shown by Ho and Ermon (2016) under the on-policy fashion. The proof sketch should be similar to our modified adversarial formulation with an extension to the augmented state. In the following, we elaborate on the extension to off-policy setting and additional loss terms introduced to stabilize GAN training. To enable an off-policy fashion, importance sampling\nmust be applied to the second term A, resulting in \u0395\u03c0\u0394[log(1 \u2013 De(x, \u03b1))]. However, estimating this density ratio is computationally challenging. Additionally, omitting this term has been observed to improve the algorithm's performance in practice, potentially due to reduced gradient variance during updates (Neal, 2001). Despite this improvement, training using cross-entropy loss alone often results in instability due to the complex interaction between the discriminator and the generator policy, which is also updated every iteration. To address these issues, we incorporate additional regularization terms: a gradient penalty Lgrad and an entropy regularization term Lentropy. These modifications help stabilize training by preventing excessively large gradient updates per training epoch (Nagarajan and Kolter, 2017; Arjovsky et al., 2017). The final loss function for the discriminator is defined as:\n$$L_{disc} = -E_{D_{exp}} [log D_\u03b8(x, a)] \u2013 E_{dgen} [log(1 \u2013 D_\u03b8(x, a))] + L_{grad} + L_{entropy}.$$"}, {"title": "Policy Optimization.", "content": "In this section, we introduce the reward function derived from the discriminator D and the policy optimization method used to improve policy in each iteration. To extract the reward signal from the discriminator for each policy update, we use the following formula:\n$$R_\u03b8(x, \u03b1) = log(D_\u03b8(x, a) + \u03b4) \u2013 log(1 \u2013 D_\u03b8(x, a) + \u03b4),$$\nwhere 8 is a marginal constant to prevent numerical error in computation. After derivation Re resembles policy entropy regularized reward for soft policy update, which can be used to satisfy\nthe delayed RL objectives max ET\u25b3~p(T\u25b3) [\u03a3= Ro(TA) \u2013 H(\u03c0)] (Haarnoja et al., 2018). To\nupdate policy \u03c0\u25b3 for each iteration, we also apply the augmented approach introduced in Section 2. However, using state augmentation for both reward learning and policy optimization is sample inefficient. Thus, we apply the auxiliary delayed RL approach, which learns a value function for short delays and uses bootstrapping and policy improvement techniques to adjust it for long delays (Wu et al., 2024a). The detailed version of the algorithm can be found in Appendix C."}, {"title": "6. Experiment", "content": "In this section, we evaluate the performance of our Off-Policy Inverse Delayed RL framework. We aim to demonstrate the capability of our method to recover expert behaviors under delayed settings. All experiments are conducted on the MuJoCo benchmarks (Todorov et al., 2012). All the expert trajectories are collected by an expert agent trained with VDPO (Wu et al., 2024b) under MuJoCo environments with 5, 10, and 25 delay steps. We compare our approach with the on-policy algorithms AIRL (Fu et al., 2017), behavior cloning (Torabi et al., 2018), and the off-policy method Discriminator Actor-Critic (DAC) (Kostrikov et al., 2018) based on delayed observation states. For policy optimization to IRL approaches, we use Proximal Policy Optimization (PPO) (Schulman et al., 2017) for AIRL, and Soft Actor-Critic (SAC) (Haarnoja et al., 2018) for DAC. All implementations of PPO and SAC are referenced from the Clean RL library (Huang et al., 2022). Each algorithm is trained with 100k environmental steps and evaluated each 1k steps across 5 different seeds for InvertedPendulum-v4. For Hopper-v4, HalfCheetah-v4, Walker2d-v4, and Ant-v4, AIRL is trained with 10M steps and evaluated every 100k steps across 5 different seeds, but DAC and our algorithm are trained with 1M environmental steps and evaluated every 10k steps across 5 different seeds. We conduct the aforementioned series of experiments under various numbers of expert trajectories ranging from 10 to 1000. All the experiments are run on the Desktop equipped with RTX 4090 and Core-i9 13900K. Training graphs are provided in Appendix B. Across different scenarios, we showcase our method's efficacy in two dimensions.\n\u2022 Performance Superiority: Our method consistently outperforms baseline approaches in recovering expert behaviors across various environments, even under diverse delay-length settings. When expert demonstrations are sufficient, our method achieves near-complete recovery of expert performance, whereas most baselines fail to learn meaningful policies.\n\u2022 Robust Utilization of Limited Expert Demonstrations: Even with limited expert demonstrations, our method demonstrates the ability to recover a reasonable policy, outperforming baselines that struggle to reproduce expert behaviors in most environments."}, {"title": "6.1. Impact of Varying Delays", "content": "We investigate the effect of varying delays (5, 10, and 25 delay steps) on performance, using 1000 expert demonstration trajectories. Detailed learning curves in Appendix B support our findings. As shown in Table 1, our IDRL framework consistently outperforms baseline methods (AIRL, DAC, and BC) across all tested MuJoCo environments. Notably, as delays increase, the performance of the expert policy deteriorates, leading to noisier expert trajectories and a corresponding decline in baseline performance. While baseline methods exhibit significant degradation or fail entirely in certain environments, IDRL maintains near-expert performance across all delay conditions, demonstrating remarkable robustness. This resilience is particularly evident in complex environments like Ant-v4 and Walker2d-v4, where IDRL consistently achieves superior performance, whereas DAC and AIRL fail to adapt. BC can recover part of the expert behaviors, but still a large margin below ours. These advantages are even more notable in the low dimensional task (InvertedPendulum-v4) and medium dimensional tasks (Hopper-v4 and HalfCheetah-v4). IDRL's robustness stems from its augmented state representation, which enriches the feature space to better capture delayed dependencies, combined with advanced policy optimization techniques that mitigate the adverse"}, {"title": "6.2. Quantity of Expert Demonstrations", "content": "We analyze the impact of the quantity of expert demonstrations on training performance under a moderate 10 delay steps, with results summarized in Table 2. Across all methods, an overall increasing trend in return value is observed as the number of expert trajectories increases (10, 100, and 1000). In InvertedPendulum-v4, IDRL consistently performs at the expert level with performance increases according to the increase in quantity of expert demonstrations, demonstrating its resilience and efficiency regardless of the number of expert trajectories, while all the other baselines fail to recover meaningful expert behaviors. In Hopper-v4, IDRL similarly scales to expert-level performance as the quantity of demonstrations increases. Though there are notable gaps with different available quantity of expert demonstrations, our method still outperforms all baselines even with fewer demonstrations. We can observe a similar trend in Walker2d-v4, where our method also possesses a significant performance margin compared to all the other baselines. While BC initially outperforms IDRL with limited demonstrations (10 and 100 trajectories) in HalfCheetah-v4, IDRL demonstrates superior scalability as more expert data becomes available. With 1000 trajectories,"}, {"title": "7. Conclusion", "content": "In this work, we introduced an Inverse Delayed Reinforcement Learning (IDRL) framework that effectively addresses the challenges of learning from expert demonstrations under delayed settings. By leveraging augmented state representations and advanced policy optimization techniques, our method demonstrated superior performance across a range of environments and delay conditions, consistently outperforming baseline methods. Our empirical results validated the theoretical advantages of using augmented states over direct delayed observations, highlighting the robustness and scalability of IDRL, particularly when faced with varying delays and quantities of expert data. While our approach significantly advances state-of-the-art imitation learning with delays, several avenues for future research remain open. First, extending IDRL to more complex, high-dimensional environments, such as those involving real-world robotics, would further test its robustness and generalizability. Additionally, exploring methods to reduce the dependency on large quantities of expert demonstrations, such as leveraging transfer learning or combining imitation learning with self-supervised approaches, could broaden the applicability of IDRL in data-scarce scenarios. Finally, integrating IDRL with online adaptation mechanisms to dynamically handle non-stationary delays during deployment represents an exciting direction for future work."}]}