{"title": "Distilling System 2 into System 1", "authors": ["Ping Yu", "Jing Xu", "Jason Weston", "Ilia Kulikov"], "abstract": "Large language models (LLMs) can spend ex- tra compute during inference to generate in- termediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 tech- niques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised meth- ods to \"compile\" (distill) higher quality outputs from System 2 techniques back into LLM gen- erations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such tech- niques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that System 2 dis- tillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.", "sections": [{"title": "Introduction", "content": "Generating intermediate thoughts allows a model (or human!) to reason and plan in order to success- fully complete a task or respond to an instruction. We refer to such deliberate thinking as System 2 reasoning, following its description for humans in Sloman (1996); Kahneman (2011) and later for AI models (Bengio, 2017; LeCun, 2022; Weston and Sukhbaatar, 2023). In System 2 reasoning effortful mental activity is exerted, especially in situations where System 1 \u2013 more automatic thinking \u2013 is likely to make errors. In standard Large Language Models (LLMs) we thus define System 1 as appli- cation of the Transformer (Vaswani et al., 2017) to directly produce a response given an input, with- out generation of intermediate tokens. We define System 2 as any approach which generates inter- mediate tokens, including methods that perform search, or prompt multiple times, before finally generating a response. A battery of such System 2 techniques have been proposed, among them Chain- of-Thought (Wei et al., 2022), Tree-of-Thoughts (Yao et al., 2024), Graph-of-Thoughts (Besta et al., 2024), Branch-Solve-Merge (Saha et al., 2023), System 2 Attention (Weston and Sukhbaatar, 2023), Rephrase and Respond (Deng et al., 2023a) and more. Many of these methods are shown to produce more accurate results due to this explicit reasoning, but typically do so at much higher inference cost and latency for a response. Due to the latter, many of these approaches are not used in production sys- tems, which mostly use System 1 generations.\nFor a human, the process of learning to trans- fer a skill from deliberate (System 2) to automatic (System 1) in psychology is referred to as auto- maticity, and the use of procedural memory (Cohen and Squire, 1980). For example, when driving to work for the first time one might typically expend conscious effort planning and making decisions to get there. After a driver repeats this route, the"}, {"title": "Related work", "content": "2.1 System 1 and System 2 in Humans\nIn humans, System 1 reasoning is described as be- ing capable of recognizing patterns, making quick judgments, and understanding simple or familiar symbols. For instance, it is used to identify com- mon traffic signs, recognize faces, or associate ba- sic symbols with specific emotions or ideas. How- ever, for complex problem-solving or for example manipulation of abstract symbols (like algebraic equations or logical statements), System 2 reason-"}, {"title": "System 1 and System 2 Models", "content": "We refer to a neural network that outputs a response directly without intermediate outputs as a System 1 model. Such a network can nevertheless com- pute intermediate latent representations in its lay- ers before it outputs a response. As these states are represented as vectors they typically encode dis- tributed knowledge, rather than discrete decisions, and have difficulty manipulating complex symbolic reasoning tasks directly (Nye et al., 2021; Cobbe et al., 2021; Yu et al., 2023; Li et al., 2024), which is analogous to issues with System 1 reasoning in humans. Nevertheless, a vast array of tasks can be solved with success directly in this manner without intermediate generations (Radford et al., 2019).\nNye et al. (2021) showed that the same language model that is unable to perform complex multi-step computations can perform those tasks when asked to generate intermediate steps into a \"scratchpad\" using either few-shot prompting or supervised train- ing. Chain-of-thought reasoning was shown to be elicited from LLMs even using zero-shot prompt- ing (Kojima et al., 2022) as well as by supervised (Cobbe et al., 2021) or few-shot (Wei et al., 2022) methods. LLM pretraining allows such reasoning to be built into the model because reasoning steps in discrete symbols (text) are present in the train- ing corpora written by humans. Such System 2 model approaches output discrete tokens which is good for making sequential correct logical reason- ing steps but obviously has a downside if the reasoning is generated incorrectly. An incorrect discrete decision is difficult to recover from, un- like latent vector-based reasoning that might more"}, {"title": "(Standard) Distillation", "content": "The concept of distillation is usually applied to tak- ing separate models, a powerful teacher model (or multiple teacher models) and a less powerful stu- dent model with separate parameters. The student model is then trained to mimic the behavior of the teacher(s). Methods of distillation include training the student to have similar output distributions (Hin- ton et al., 2015), layer activations (Adriana et al., 2015) or derivatives of the target teacher outputs (Czarnecki et al., 2017). Earlier works considered distillation from an ensemble of multiple teacher models (Bucilu\u0103 et al., 2006; Hinton et al., 2015). As neural networks have become larger, distilling from a larger to a smaller network has become a common paradigm (Ba and Caruana, 2014). In con- trast, in our work the teacher and student model are the same language model, but applied differently (either with intermediate reasoning, or not).\nFor chain-of-thought reasoning in particular, sev- eral distillation approaches have been considered (Wang et al., 2023; Li et al., 2023a; Chen et al., 2024). These again follow the paradigm of distill- ing a separate larger model's output into a smaller model. The student is asked to mimic the System 2 behavior by generating similar internal thoughts as the teacher model, in contrast to our work where the goal is to not generate internal thoughts (to im- prove System 1). Some exceptions are Deng et al. (2023b, 2024). The former still uses a separate stu- dent and teacher model, but attempts to distill the intermediate thought tokens into the layers of the network by representing reasoning steps as vectors and then setting them as targets. The latter recent work attempts to distill CoT by gradually removing the intermediate steps, which can improve perfor- mance greatly compared to not doing so, but still does not match explicit CoT."}, {"title": "Distilling System 2 into System 1", "content": "3.1 Setup: System 1 and System 2 models\nGiven an input x, in this work we consider the setting of a single model, in our case a large lan- guage model (LLM), that is capable of two modes of response:\n(i) System 1: Produces the output y directly. This is done by forwarding through the layers of the underlying autoregressive neural network (Transformer) to produce the output tokens.\n(ii) System 2: We define System 2 models as methods that use the underlying Transformer to generate intermediate output tokens z of any kind before generating the final response tokens. This may include multiple calls (prompts).\nMore formally, we consider a System 2 model SII as a function that takes an LLM \\(p_{\\theta}\\) and input x, and can call the LLM possibly repeatedly to gener- ate intermediate tokens z using a specific algorithm, before returning an output y:\n\\(S_{II}(x; p_{\\theta}) \\rightarrow z, y.\\)   (1)\nSystem 2 approaches can potentially involve mul- tiple prompts, branching, iteration and search, all the while using the LLM to generate intermediate results for further processing. In contrast, a System 1 model only considers the original input x and calls the LLM \\(p_{\\theta}\\) directly to produce an output y:\n\\(S_1(x) = p_{\\theta}(x) \\rightarrow y.\\)   (2)\nThere are many existing instantiations of Sys- tem 2 models. Chain-of-thought prompting only requires a single LLM prompt, but still outputs intermediate generations before a final response, typically used in math and other reasoning tasks (Wei et al., 2022).\nMethods like System 2 Attention (Weston and Sukhbaatar, 2023) and Rephrase and Respond (Deng et al., 2023a) require two calls to the LLM, where in the former the first call is used to attend to the context and remove bias, and in the latter to ex- pand on the question. The second call is then used to finally respond to the answer given the intermedi- ate generations. Some methods are much more so- phisticated for example Branch-Solve-Merge (Saha et al., 2023) which generates a plan via an LLM which branches into several more LLM calls until a final stage merges the results."}, {"title": "Method: System 2 Distillation", "content": "Many System 2 methods, by their nature, are sig- nificantly slower at inference time due to multiple prompt calls and generation of intermediate tokens. The aim of System 2 Distillation is to distill all the reasoning from S I I back into S 1 so that the di- rect outputs from the language model \\(p_{\\theta}(x)\\) are improved. We assume a setting where the model has access to unlabeled inputs X from which it can learn, in analogy to how humans learn their proce- dural memory without supervision. For language- based tasks, it is common to have access to in- struction following prompts (inputs) as they can be collected from humans, e.g. the 1M released Wild- Chat interactions (Zhao et al., 2024) where inputs are given but correct labels are unknown. Hence this is a realistic setup.\nThe first step of the proposed method is to gen- erate responses using the System 2 model over the unlabeled inputs X:\n\\(Y_{s_{II}}^i = S_{II}(x^i; p_{\\theta}), \\forall x^i \\in X.\\)  (3)\nNote we discard (do not store) the intermediate outputs z from Eq. 1. These responses \\(Y_{s_{II}}^i\\) can then be used directly as System 2 distillation targets for fine-tuning a System 1 model. However, they are subject to noise: some of these responses could be high quality, while others could be low quality or incorrect. For shortform QA and reasoning tasks involving a short response with a typically unique correct (but unknown) answer, we thus consider an unsupervised curation step to attempt to improve training data quality. We consider two variations which both rely on a consistency criterion:\n\u2022 self-consistency of outputs: we sample \\(S_{II}(x^i; p_{\\theta})\\) a total of N times, and accept the response that is the majority vote; if there is no majority winner, we discard the example.\n\u2022 self-consistency under input perturbation: we perturb the input \\(x^i\\) in such a way that the output should not change, e.g. changing the order of multiple-choice items in the prompt, and compute S I I for each perturbation; if the outputs do not agree, we discard the example.\nAfter that, we end up with the synthetic dataset (X S I I , Y S I I ), where XS I I is a filtered subset of X with targets YS I I . The final step is then supervised fine-tuning of the LLM with parameters \\(p_{\\theta}\\) using this distilled training set. We typically initialize this model from the current state \\(p_{\\theta}\\) and continue training with the new dataset.\nAfter fine-tuning we obtain an LLM \\(p_{\\theta}\\) which is a System 1 model that is expected to provide out- puts and performance gains similar to the evaluated System 2 model."}, {"title": "Experiments", "content": "4.1 Training and Evaluation Setup\nWe use Llama-2-70B-chat (Touvron et al., 2023) as the base model for all our experiments. We re- quire a base model of sufficient power that it can be performant as a System 2 model, but also have open weights that can be fine-tuned, hence this choice. We consider several System 2 methods, including Rephrase and Respond (RaR), System 2 Attention (S2A), Branch-Solve-Merge (BSM), and Chain-of-Thought (CoT), focusing on tasks where each method has demonstrated strong performance. For System 1, we conduct zero-shot inference us- ing the instruction-tuned base model as a standard baseline. We report task-specific metrics for each task, and the \"#Tokens\" metric which measures the average number of tokens generated per input across the evaluation set. For System 2 methods this includes both intermediate token generations as well as the final output token generations. De- tailed descriptions of the experimental setups are available in the Appendix A.2.\n4.2 Rephrase and Respond Distillation\nRephrase and Response (RaR) (Deng et al., 2023a) is a System 2 method that first prompts the lan- guage model to rephrase the original question with further elaboration, and then secondly to generate a response based on the rephrased question with the aim that this provides superior output. The au- thors introduce two approaches, 1-step RaR and 2-step RaR, where the latter involves two separate prompts rather than a combined one as in the for- mer, see Appendix A.1 for specific prompts. They find that 2-step RaR significantly improves perfor- mance on several reasoning tasks that are challeng- ing for the baseline LLM. We consider two tasks from the original paper where it performed well: the last letter concatenation task and coin flip rea-"}, {"title": "Chain-of-Thought Distillation", "content": "Chain-of-Thought (CoT) (Wei et al., 2022) has been shown to be an effective method to improve LLM's reasoning abilities, such as for solving grad- uate school math problems. The LLM generates intermediate tokens that are steps (chain) of reason- ing (thoughts) before it produces the final answer. We consider two variants of the approach: (i) few- shot CoT, whereby multiple [question, CoT, an- swer] examples from the training set are provided as part of the context followed by the question; and (ii) zero-shot, whereby an explicit instruction to think \"step by step\" is added to the prompt in addition to the question, see Appendix Figure 10.\nDistillation data We use CoT to produce an- swers for questions from the training split of GSM8k (Cobbe et al., 2021) (which we consider unlabeled), using majority voting with K = 10. The resulting distillation training set consists of 7461 [question, answer] pairs i.e., without any in- termediate reasoning steps. The accuracy of the self-supervised targets, computed for analysis pur- poses, is 56.81%.\nEvaluation We report evaluation accuracy com- puted over the GSM8k test set with majority voting"}, {"title": "Conclusion", "content": "Recent work has shown that complex reasoning procedures using LLMs in the inner loop, called System 2 approaches, can improve performance. In this work we have shown that in many cases it is possible to distill this System 2 reasoning into the outputs of the LLM without intermedi- ate generations while maintaining, or sometimes even improving, performance. While not all meth- ods can be distilled easily using our method, with Chain-of-Thought for complex reasoning being a challenging counterexample, this is possible for di- verse approaches. Our method works for System 2 Attention for dealing with bias and irrelevant con- text, Rephrase and Respond for clarifying task in- structions, and Branch-Solve-Merge for improved LLM-as-a-Judge evaluation. Pragmatically, distill- ing these approaches makes them more likely to be used by LLM practitioners, and they are more effi- cient at inference time. Looking forward, systems that can distill useful tasks in this way free up more time to spend on reasoning about the tasks that they cannot yet do well, just as humans do. Hence, we expect exploring this approach in a continuous training loop will be a fruitful research direction."}, {"title": "Limitations", "content": "In this paper, we explored three System 2 meth- ods-RaR, S2A, and BSM-which have been suc- cessfully distilled, yielding enhanced results com- pared to the original System 1 performance while incurring lower inference costs than System 2. However, the effectiveness of these methods can vary depending on the specific task or the dataset used for model training. For instance, we observed that the CoT method could not be effectively dis- tilled back to System 1 using our method. We note that recent methods have tried alternative ways to distill CoT (Deng et al., 2023b, 2024).\nMoreover, due to the self-supervised nature of these methods, model performance relies on the specific filters applied. In our study, we de- pended on a consistency criterion that includes self- consistency of outputs and self-consistency under input perturbation. Although there are multiple alternative strategies to enhance data quality in self- supervised learning, these were not explored in our research."}, {"title": "Appendix", "content": "A.1 Prompts\n{question}\nReword and elaborate on the inquiry, then provide an answer.\nFigure 3: 1-step RaR prompt. The 1-step RaR process involves the model rephrasing the question and subse- quently providing an answer, all in a single step.\n{question}\nBased on the details given in the initial inquiry, could you kindly rephrase the question and separate these 2 words in the revised question? Please ensure these 2 words remain unchanged from the original question.\n{rephrased question}\nFigure 4: 2-step RaR prompt for last letter concate- nation task, step 1 (top), step 2 (down) The 1-step RaR process involves the model rephrasing the question and subsequently providing an answer, all in a single step.\nA.2 Experiment Details\nModel training We use Llama2 70B Chat as the initialization for SFT training with CE loss. The loss is only applied on the answer part of the se- quence. Model is trained with dropout 0.1, learning rate 5.5e-6, with warmup 1. Table 5 shows details about total training steps and total training tokens per step.\nS2A For S2A, in both generation stages we use nucleus sampling with top-p value 0.9. During distillation, for USC, in some cases the generated answers are too long and 20 do not fit in the Llama2 context. In these rare cases we reduce the answer set to 10 or select an answer randomly if 10 gener- ated answers are still too long.\nBSM Figure 14 shows the overview of Branch- solve-merge. We copied figure from Saha et al. (2023)."}]}