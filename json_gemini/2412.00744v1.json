[{"title": "A Cross-Scene Benchmark for Open-World Drone Active Tracking", "authors": ["Haowei Sun", "Jinwu Hu", "Zhirui Zhang", "Haoyuan Tian", "Xinze Xie", "Yufeng Wang", "Zhuliang Yu", "Xiaohua Xie", "Mingkui Tan"], "abstract": "Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark, the complexity of open-world environments with frequent interference, and the diverse motion behavior of dynamic targets. To address these issues, we propose a unified cross-scene cross-domain benchmark for open-world drone active tracking called DAT. The DAT benchmark provides 24 visually complex environments to assess the algorithms' cross-scene and cross-domain generalization abilities, and high-fidelity modeling of realistic robot dynamics. Additionally, we propose a reinforcement learning-based drone tracking method called R-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the agent tracking performance in vast environments with complex interference. We design a goal-centered reward function to provide precise feedback to the drone agent, preventing targets farther from the center of view from receiving higher rewards than closer ones. This allows the drone to adapt to the diverse motion behavior of open-world targets. Experiments demonstrate that the R-VAT has about 400% improvement over the SOTA method in terms of the cumulative reward metric.", "sections": [{"title": "1. Introduction", "content": "Visual Active Tracking (VAT) aims to autonomously follow a target object by controlling the motion system of the tracker based on visual observations [68, 73]. It is widely used in real-world applications such as drone target tracking and security surveillance [20, 48, 66, 70]. Unlike visual passive tracking [4, 6, 9, 28, 51, 60, 67, 75], which involves proposing a 2D bounding box for the target on a frame-by-frame with a fixed camera pose, VAT actively adjusts the camera position to maintain the target within the field of view. Passive visual tracking often falls short in real-world scenarios due to the highly dynamic nature of most targets. Thus, VAT offers a more practical yet challenging solution for effective tracking in dynamic environments.\nRecently, VAT methods have evolved into two main categories: pipeline VAT methods [14, 34, 40] and reinforcement learning-based VAT methods [17, 18, 33, 73]. Pipeline VAT methods employ a sequential framework where the visual tracking [5, 28, 29, 56] and control models are connected in series. Here, the object tracking model first processes the input image to estimate the target position and then the control model to generate the necessary control signals. While this modular design allows for clear task separation, these methods often require significant manual effort to label the training data, and the combination of modules requires extra effort for tuning and implementation. To address these issues, reinforcement learning-based VAT methods integrate visual tracking and control within a unified framework. These methods eliminate the need for separate tuning of the visual tracking and control modules by using a unified framework to map raw visual inputs directly to control actions. Therefore, the reinforcement learning-based VAT methods simplify system design and increase the efficiency of learning adaptive tracking behaviors in dynamic environments.\nUnfortunately, achieving accurate drone visual active tracking with reinforcement learning remains challenging, partly for the following reasons. 1) Missing unified benchmark. Existing benchmark scenes are limited in scope, low in complexity, and few in number, and unable to adequately validate the performance of agents (see Tab. 1). Previous methods provide limited tracking targets and sensors, insufficient to establish benchmarks for different tasks. 2) Vast environments with complex interference. Open-world tracking involves large, dynamic environments with frequent interference. It brings significant challenges for agents to accurately follow targets. Training directly in these conditions often leads to slow convergence or difficulty in building robust tracking behaviors. 3) Complex targets with diverse motion behaviors. Open-world targets often exhibit complex, unpredictable behaviors, requiring agents to adapt to varied movements and orientations. Existing methods assume a fixed forward-facing view, leading to reward functions that inaccurately reflect tracking performance across different behaviors and perspectives.\nTo address the above limitations, we first propose a unified cross-scene benchmark for open-world drone active tracking (called DAT) that simulates the diversity and complexity of the real world as possible (see Fig. 1). Specifically, the DAT benchmark provides 24 visually complex scenes to validate the algorithms' cross-scene and cross-domain generalization abilities. It offers comprehensive support for diverse tracking scenarios with 2 tracker types, 24 target types, and 6 different sensor types, with plug-and-play interfaces that facilitate the integration of custom robot models and controllers. To better replicate real-world conditions, DAT employs the webots simulation software [31] for high-fidelity modeling of realistic robot dynamics. It incorporates the Simulation of Urban Mobility (SUMO) [30] for managing target behavior, enabling diverse and efficient path modeling that surpasses traditional rule-based approaches. Second, we propose a novel drone visual active tracking with reinforcement learning method (called R-VAT), aiming to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning [35, 47, 58, 76], we propose a Curriculum-Based Training strategy that progressively improves agent performance in vast environments with complex interference. In addition, we design a goal-centered reward function to provide accurate feedback to the agent, enabling it to adapt to the target's diverse motion behaviors. Unlike existing methods, this function is designed at the image level to prevent targets farther from the center of view from receiving higher rewards than closer ones.\nTo summarize, our contributions are as follows:\n\u2022 A comprehensive drone active tracking benchmark. We present the DAT benchmark, simulat-"}, {"title": "2. Task Definition of Drone Active Tracking", "content": "The DAT task seeks to train a model to control a drone for active target tracking in dynamic environments (see Fig. 2). Using visual and motion sensor data, the model learns actions to keep the target centered in view, ensuring robust performance across diverse scenarios.\nObservation spaces. The target is initially positioned at the center of the camera's field of view, and the observation space comprises data acquired from sensors (e.g., RGB images with a resolution of 84 \u00d7 84).\nAction spaces. There are two options for the action space: discrete action space and continuous action space. The discrete action space is a 7-dimensional vector, representing movements such as forward, backward, left translation, right translation, counterclockwise rotation, clockwise rotation, and stopping. The speed corresponding to each action must be set by the user. The continuous action space is a 4-dimensional vector that defines speed values in the forward, lateral, vertical, and yaw directions.\nSuccess criterion of DAT task. We define a success criterion when the model can keep the target object, which is initially located at the center of the camera's field of view, in the middle of the image for a long duration.\nChallenges. Open-world drone active tracking presents substantial challenges primarily due to the scarcity of real-world data and the high costs and risks associated with real-world trial-and-error. This highlights the critical need for building complex and diverse simulation environments. Additionally, open-world scenes are characterized by high diversity and dynamic elements, which introduce complex interferences, further challenging the model's robustness. Moreover, real-world tracking targets exhibit diverse and unpredictable behaviors, making it difficult for models to adapt. Therefore, improving the adaptability of models to track complex targets effectively is another key challenge."}, {"title": "3. DAT Benchmark with Diverse Settings", "content": "We develop the DAT benchmark (see Fig. 3) to evaluate the model's cross-scene and cross-domain adaptability in 6 large outdoor environments with 4 weather conditions.\n3.1. Diverse Scene Construction\nThe construction of the DAT scene aims to simulate the diversity and complexity of the real world. To enhance diversity, we select 6 categories of outdoor scenes and 4 weather conditions, covering almost all natural conditions that applications may face. To simulate the real-world complexity, we model 7 aspects: scene area, building density, color richness, road density, terrain density, tree density, and tunnel density. Specifically, the scene area refers to the extent of the scene, building density is the ratio of the number of buildings to the scene area, and color richness is the number of dominant colors in the scene. These three aspects primarily depict the complexity of the visual background. Road density is measured by evaluating the density of complex elements such as intersections, terrain density is determined by assessing the density of special terrain features like mountains in the scene. These two aspects primarily depict the complexity of tracking target behavior. The tree density and tunnel density are calculated as the ratio of the number of trees and tunnels to the scene area, and are used to measure the level of visual occlusion within the scene.\nAs shown in Fig. 3(a), the six different scenarios exhibit unique and realistic complexity across the seven aspects:\n\u2022 Citystreet scene covers an area of 0.7 square kilometers. It has a road density of 38.7 and a tree density of 97.5, mainly testing the agent's efficiency against tree occlusions.\n\u2022 Village scene spans 1.4 square kilometers. This scene features a mountainous terrain density of 20.1 and a tunnel density of 6.3, requiring the agent to predict the target's movement when it is fully obscured by tunnels.\n\u2022 Downtown scene covers 0.8 square kilometers. The scene includes complex road elements and a very high building density of 304.9, challenging the agent's tracking accuracy and obstacle avoidance capabilities.\n\u2022 Lake scene encompasses 1.6 square kilometers. The density of road elements is 68.1, and the richness of background colors is rated at 5, challenging the agent's robustness across varying features and colors.\n\u2022 Farmland scene covers an area of 0.7 square kilometers. The color richness is rated at 5, with multiple color patches spread throughout the scene, posing a significant challenge to the agent's adaptability to multi-color environments.\n\u2022 Desert scene covers 1.1 square kilometers. It includes a mountainous terrain density of 37.1 and a road element density of 31.0. Some roads are covered by sand, testing the agent's adaptability to such challenging conditions.\nThe four weather conditions mainly test the agent's cross-domain adaptability. Foggy reduces visibility, with a visibility range of 400m to 2000m. Night reduces brightness and light uniformity, and snow changes the color tone of the environment. The above 24 scenes can fully measure the performance of the agent active adaptation (see Fig. 1).\n3.2. Various Trackers and Targets Construction\nDrone Active Tracking in real-world applications involves various trackers and targets depending on the specific task. For example, drones are typically used as trackers, with automobiles as targets in security monitoring. The DAT benchmark provides diverse trackers and targets, making it adaptable for different use cases.\nTracker. The DAT benchmark supports two tracker types: drones and ground robots. The drone used is the DJI Matrice 100 [12], with a default flight controller [53]. The DAT benchmark provides visual and motion sensors (see Fig. 3(c)) that can obtain the state parameters of the drone relative to the world coordinate system. The drone position and velocity are obtained via GPS, angular velocity via gyroscope, Euler angles are obtained from the IMU and can be converted into quaternion, and acceleration via accelerometer. The LiDAR provides point cloud data. To better replicate real-world conditions, drones are equipped with a 3-axis gimbal, similar to those on commercial drones like the DJI Mini 3 Pro [13], enabling precise camera adjustments and adding complexity beyond static point trackers. For ground robots, users can select from various robot models. The benchmark employs webots simulation software to provide a high-fidelity dynamic engine, accurately modeling tracker movements, collisions, and interactions, crucial for evaluating tracking algorithms in challenging scenarios. Additionally, we provide common parameters for further task customization. See Appendix B for details.\nTargets. The DAT benchmark includes five categories of targets: automobile, motorbike, pedestrian, wheel robot, and legged robot, providing a total of 24 tracking targets, each with built-in controllers (see Fig. 3(d)). To support custom robot designs, a plug-and-play interface is available, allowing users to easily integrate robot models and controllers into the benchmark. See Appendix B for details.\nTarget Management. Realistic target behavior and path diversity are essential for simulating authentic environments. The DAT benchmark uses the SUMO traffic simulator to manage all scene targets. For example, SUMO generates random vehicle paths and dynamically controls actions like acceleration, lane changes, and stops. It can also refresh vehicles based on configurations and randomize their types and colors, creating a high-fidelity traffic system."}, {"title": "4. VAT with Reinforcement Learning", "content": "In this paper, we primarily focus on visual active tracking (VAT), a core task within the DAT benchmark. We propose a drone visual active tracking with reinforcement learning method (called R-VAT), aiming to improve the performance of tracking targets in complex scenes. As shown in Fig. 2, we model drone active tracking as a Markov Decision Process (MDP) and train a Drone Agent to track a target in the open scene. At time step t, the Drone Agent takes the current state st as input and selects an action. Then the reward is calculated via Eq. (2). The trajectories are collected to train the agent via the Curriculum-Based Training strategy.\n4.1. MDP for Drone Active Tracking\nWe explore a Drone Agent that achieves end-to-end camera control for better performance in highly dynamic, long-term visual tracking tasks. We model the end-to-end visual active tracking task as an MDP: (S, A, R, \u03b3, T). In this representation, S denotes the state space, A represents the action space, and \u03b3 is the discount factor. At each time step t, the agent takes the state $s_t \\in S$ as input and performs an action $a_t \\in A$. Next, the simulator transitions to the next state"}, {"title": "4.2. Curriculum Learning for Agent Training", "content": "The DAT environment contains numerous dynamic targets and diverse obstacles, making it challenging for the agent to discover sufficient successful trajectories. This difficulty results in low convergence rates and limited performance. Progressively training the agent from simpler to more complex environments increases the percentage of successful trajectories, enhancing performance and accelerating learning for the final task [57]. Therefore, we propose a Curriculum-Based Training (CBT) strategy for tracking to optimize agent training in complex environments.\nTraining Algorithm. To address the challenges of learning in complex environments, we employ the Proximal Policy Optimization (PPO) [49] algorithm, which is widely recognized for its robustness and efficiency in complex continuous control tasks. Given the complexity of the environment, we adopt a multi-stage training strategy, which divides the model training into two stages to build the agent's tracking capabilities progressively. The first stage E1, consists of a simplified environment where the target trajectory is a straight line, and there are no obstacles or visual interferences. The agent learns to keep the target centered in the image through the reward signal from Eq. (2). Once the reward obtained for agent in first stage E\u2081 converges as:\n$\\frac{1}{T} \\sum_{t=1}^{T} r_t \\geq \\eta,$\\nwhere rt represents the reward obtained by the agent at time step t, T is the total number of time steps over which the reward is averaged, and \u03b7 is a predefined threshold indicating satisfactory performance. Once this condition is satisfied, the agent transitions to the second stage E2, where the agent encounters more varied tracking target movements and more complex visual information, such as tree occlusions, crosswalks, and poles. The goal for the agent is to develop stronger task generalization abilities based on task understanding gained in the first stage.\nData Augmentation. While simpler settings facilitate the agent's learning of task objectives, they also heighten the risk of the agent rapidly converging to a locally optimal action distribution, which can undermine the exploration process. Consequently, implementing data augmentation is essential for enhancing the agent's exploratory capabilities. This is achieved through the randomization of the drone's initial position and orientation relative to the target, which necessitates a broader range of actions from the agent to maximize rewards. Moreover, to enhance the agent's spatial perception ability, randomization is also introduced in its gimbal pitch angle. See Appendix C for details."}, {"title": "4.3. Goal-Centered Reward Design", "content": "Open-world targets often exhibit complex, unpredictable behaviors, requiring agents to adapt to varied movements and orientations. However, existing methods assume the drone is parallel to the target, leading to reward functions that inaccurately reflect tracking performance across different behaviors and perspectives (see Appendix C for theoretical proof). To address this, our reward function is defined only at the image level, relying on the target position within the image I \u2208 R84\u00d784. The reward decreases as the target moves away from the center Io of the image, and no reward is given if the target is outside as:\n$r_t = \\begin{cases} \\tanh \\left( \\alpha (1 - x)^3 \\right) \\frac{P_{uc} - I_{oG}}{E_{cG} - I_{oG}}, & I_{target} \\in I_{clip} \\\\ 0 & otherwise \\end{cases},$\\nwhere $x = \\frac{|P_{vc} - I_{oG}|}{|E_{cG} - I_{oG}|}$ represents the position of the target relative to the center of the image, which is calculated by projecting the image onto the ground. Specifically, we calculate the ratio of the distance from the projection $I_{OG}$ of the image center on the ground to the center $p_{vc}$ of the target (line segment $| P_{vc} - I_{oG}|$) and the distance from the intersec-"}, {"title": "6. Conclusion and Potential Impacts", "content": "In this paper, we propose a unified cross-scene cross-domain benchmark for open-world drone active tracking, called DAT. The DAT benchmark provides 24 visually complex environments to assess the algorithms' cross-scene and cross-domain generalization abilities, and high-fidelity modeling of realistic robot dynamics. Additionally, we propose a reinforcement learning-based drone tracking method"}, {"title": "A. Related Work", "content": "A.1. Passive Object Tracking\nMost of the proposed visual tracking benchmarks belong to passive visual tracking. LaSOT [21] and OTB2015 [64] benchmarks contain a large number of ground-based videos. These benchmarks include target videos, and the tracking algorithms utilize both the video frames and the target labels for tracking. However, ground cameras tend to be affected by occlusion and suffer from the shortcoming of limited perceptual range, so the need for drone viewpoint tracking is gradually increasing in practical applications. UAV123 [37] and VisDrone2019 [19] benchmarks are proposed for drone viewpoint, expanding the spatial dimension of perception. Meanwhile, the single-object tracking benchmarks have difficulties for many targets. MOT20 [16] and TAO [15] benchmarks are proposed for multi-object tracking to solve the above problems. In addition, the above benchmarks include videos from the RGB camera. The RGB camera's recognition capabilities are limited in complex scenes, such as ocean environments, and challenging weather conditions, including nighttime and foggy. IPATCH [41] provides extra infrared images and other sensors like GPS to supplement the information of the sea scene. Huang et al. propose Anti-UAV410 [25], which provides infrared camera images for drone tracking.\nVisual object tracking methods can be categorized into three main types: Tracking by Detection, Detection and Tracking (D&T), and pure tracking. Tracking by Detection methods [6, 7, 62] treat tracking as a sequence of independent detection tasks. These methods use object detection algorithms [44, 46] to identify the target object in each frame, connecting the detections through data association methods [27, 63] for continuous tracking. While effective in multi-target tracking, these methods may suffer from high computational demands and issues with target occlusion. D&T approaches [42, 59, 72] integrate detection and tracking, creating end-to-end models that ensure seamless information flow and reduce redundant calculations through shared feature extraction networks. Pure tracking methods can be categorized into two main types: Correlation Filters (CF) [24, 38, 65] and Siamese Networks (SN) [5, 29, 56]. CF-based models train correlation filters on regions of interest, while SN-based models compare target templates with search areas to enable precise single-target tracking.\nA.2. Visual Active Tracking\nPassive visual tracking often falls short in real-world scenarios due to the highly dynamic nature of most targets. Visual Active Tracking (VAT) aims to autonomously follow a target object by controlling the motion system of the tracker based on visual observations [34, 68, 73]. Thus, VAT offers a more practical yet challenging solution for effective tracking in dynamic environments. Maalouf et al. [34] propose a two-stage tracking method (named FAn), which is based on a tracking model and a PID control model. This method accomplishes the fusion of perception and decision-making by transferring control information from the visual tracking model to the control model. However, the visual network necessitates extensive human labeling effort and the control model requires parameter adjustments for each scene, significantly constraining the model's generalizability. Recently, many approaches [17, 18, 33, 73] model the VAT task as a Markov Decision Process and employ end-to-end training with reinforcement learning, resulting in a significant enhancement of the agent's generalizability.\nThe complexity and diversity of VAT benchmarks are crucial for training agents with high generalizability. One common approach [17, 18, 73] to enhancing environmental diversity involves modifying texture features and lighting conditions within a single scene. However, these methods often result in low scene fidelity and unrealistic object placement. While UE4 [22] is used to create photorealistic environments in some benchmarks [33, 73], these benchmarks still face limitations in diversity and map size. Furthermore, the scenarios provided by these methods are often task-specific, offering limited configurability and lacking a unified benchmark for VAT tasks.\nExisting approaches to VAT frequently neglect the randomness of target trajectories and the scalability of platforms. Target trajectories are typically predefined by rule-based patterns [17, 18, 33], which significantly restrict the exploration space. Zhong et al. [73] introduce learnable agents as targets, increasing trajectory randomness but adding additional cost. Most benchmarks provide only a single category of target [17, 18, 33, 73], limiting scalability and necessitating repetitive work for environment development. Zhou et al. [74] utilize CoppeliaSim [3] to provide five categories of noncooperative space objects. However, the use of a solid black background makes it unsuitable for general VAT scenarios. In contrast, our environment supports diverse, real-world target types and offers unified, lightweight management of target behaviors, ensuring both rationality and randomness in their actions.\nA.3. Reinforcement Learning in Visual Tracking\nReinforcement learning (RL) is commonly used in visual object tracking [45, 69, 71]. Song et al. [52] propose a decision-making mechanism based on hierarchical reinforcement learning (HRL), which achieves state-of-the-art performance while maintaining a balance between accuracy and computational efficiency. However, the actions generated by reinforcement learning in the aforementioned work cannot directly influence the camera's viewpoint, thereby failing to fully leverage the decision-making capabilities. Real-world applications increasingly require robust track-12"}, {"title": "A.4. Curriculum Learning in Robot Control", "content": "Curriculum Learning (CL) is a training strategy that mimics a human curriculum by training models on simpler subsets of data at first and gradually expanding to larger and more difficult subsets of data until they are trained on the entire dataset. As for robot control, reinforcement learning training is difficult due to the complexity of the training scenarios and the large action spaces. Therefore, curriculum learning is often required to reduce the difficulty of agent training. For instance, many works improve the walking ability of legged robots by adjusting terrain parameters through curriculum learning [35, 47]. Other studies improve the pushing and grasping performance of robotic arms by progressively increasing task difficulty [32, 39, 55].\nIn this paper, Curriculum Learning is introduced in the VAT task, and the training environment is transitioned from simple features to complex scenarios to achieve successful tracking of agent in complex outdoor environments."}, {"title": "B. More Details of DAT Benchmark", "content": "Scenario Construction. In this study, three scenarios: citystreet, downtown, and lake are derived from real-world locations. The corresponding map segments for these scenarios are extracted from OpenStreetMap (OSM) [2], and road information, such as the number of lanes and intersection rules, is edited using the JOSM tool [1]. Then configurations are then converted into webots-compatible assets. Specifically, the citystreet scenario is based on a small town in Los Angeles, the downtown scenario is derived from Manhattan, and the lake scenario is modeled after Wolf Lake Memorial Park in Indiana. In contrast, the village, desert, and farmland maps possess complex and unique features that are not adequately captured by OpenStreetMap (OSM) data. For example, the village map features mountainous terrain with tunnels, while the farmland map is characterized by diverse multicolored patterns. To overcome these limitations, this study utilizes Creo software [26] to model detailed scene elements, which are then integrated into the webots for constructing realistic maps.\nTargets. All tracking target illustrations are presented in Fig. 6. Specifically, Fig. 6(a) presents automobile and motorbike tracking targets, including passenger vehicles (the first seven cars), buses, trucks, trailers, and motorcycles (such as scooters and motorbikes). These two categories of tracking targets leverage Simulation of Urban Mobility (SUMO) [30] for road behavior modeling and interaction management with other targets. In contrast, Fig. 6(b)-(d) display pedestrian, wheeled robot, and legged robot tracking targets, respectively. These three types of targets utilize SUMO paths for position initialization and rely on specific controllers for action and behavior management.\nSensors. In the real world VAT tasks, a single camera cannot ensure the agent's stability and robustness. Thus, integration with other sensors is often required. The DAT benchmark provides common sensors that can obtain the drone's state parameters relative to the world coordinate system. The drone's position and velocity are determined using GPS, while its acceleration is measured by an accelerometer, providing essential self-referential data for visual navigation tasks. Angular velocity is recorded via a gyroscope, and Euler angles obtained from the IMU are converted into quaternions to facilitate state estimation and ensure orientation stability. Additionally, the RPLIDAR A2, provided by DAT, generates point cloud data, which supports tasks such as obstacle avoidance and navigation by delivering detailed environmental information. The specific sensors, their parameters, and potential tasks are summarized in Tab. 8.\nAdditional Parameters. The training process of VAT agents often requires additional parameters for effective reward design. To facilitate this, DAT benchmark provides 4 categories comprising a total of 13 parameters, supporting diverse reward design strategies, as detailed in Tab. 9.\nFirst are the camera parameters, which mainly include image width cameraWidth, image height cameraHeight, field of view cameraFov, and focal length cameraF. Utilizing these, the camera plane can be projected onto the ground to aid in reward construction.\nNext is the homogeneous transformation matrix (HTM). In the reward design, coordinate transformations are often required to express physical quantities within a unified coordinate system, enabling consistent calculations. For example, prior studies [17, 18, 33] transform the position, velocity, and acceleration of targets into the tracker's coordinate system to construct rewards. To support such operations, DAT benchmark provides Tct, the HTM mapping the drone camera coordinate system to the world coordinate system, and Ttw, the HTM mapping the tracking target's coordinate system to the world coordinate system.\nAdditionally, for the state of the tracker itself, cameraMidPos represents the position of the drone camera's optical center in the world coordinate system. The parameter crash indicates whether the drone collides with any buildings in the scene, which can be used in reward design for obstacle avoidance tasks.\nLastly, for ease of model training in simulations, reward design often depends on some privileged information, i.e., variables that are almost impossible to obtain in real-world settings. Thus, DAT benchmark also provides such adaptations. For example, carMidGlobalPos gives the target's position in the world coordinate system, and carDronePosori represents the target's orientation and position relative to the drone coordinate system, frequently used in VAT reward design [17, 18, 33]. Furthermore, information on the target's movement direction and type is provided, enabling training the drone's predictive ability for target movement.\nTask Configuration. We encapsulate the scenes, tasks, and data augmentation into Python classes, and provide 3 different environment classes for different algorithm requirements. The base environment class directly interacts with webots and is designed to support asynchronous reinforcement learning algorithms, such as the asynchronous advantage actor-critic (A3C) algorithm [36]. The Gymnasium environment class wraps the base environment class into a Gymnasium [54] interface, enabling direct compatibility with popular reinforcement learning libraries, such as Stable-Baselines3 [43] and Tianshou [61] for efficient"}, {"title": "C. More Details of Proposed R-VAT", "content": "C.1. Theoretical Proof of Reward Design\nCurrent reinforcement learning-based VAT methods assume a fixed forward-view perspective [17, 18, 33, 73] and design rewards based on physical distance. Distance-based reward design approaches fail to accurately reflect the performance of the agent in tracking tasks, especially in real-world applications that require a variable top-down perspective. In this section, we first demonstrate the effectiveness of distance-based reward design under the assumption of a fixed forward-view perspective. We then demonstrate that, under a top-down perspective, these approaches can result in scenarios where targets farther from the image center are assigned higher rewards than those closer to it.\nThe distance-based reward design approaches leverage the physical distance of the tracking target relative to the tracker to define the reward. While the exact reward formulations in existing methods differ, the underlying principle and the associated potential issues remain consistent. Therefore, we take the reward formulation from [18] as an example for the subsequent theoretical proof. The reward in [18] is expressed along the x, y, and z directions as follows:\n$r_x(\u00b7) = \\max (0, 1 - |\\frac{Y_x(\u00b7)}{AFOV} - d_r |),$\n$r_y(\u00b7) = \\max (0, 1 - |\\arctan(\\frac{Y_y(\u00b7)}{f}) |),$\n$r_z(\u00b7) = \\max (0, 1 - |\\frac{Y_z(\u00b7)}{f}) |),$\nwhere the reward is defined in the tracker body-fixed frame, as illustrated in Fig. 7(a).\nEffectiveness under a fixed forward-view perspective. As illustrated in Fig. 7(a), when the tracker adopts a fixed-forward perspective, the image plane is parallel to the plane containing the target. Based on the principle of pinhole imaging (see Fig. 8), the quadrilateral LU\u2081LD1 RD1 RU1 on the image plane is geometrically similar to the quadrilateral LUG1 LDG1RDG1RUG1 on the target plane, which forms a rectangle. Furthermore, the principle of pinhole imaging can also be utilized to establish the mapping range between the image plane and the target plane.\nTo simplify the analysis of the reward function shown in Eq. (12), we define a function $f(x) = 1- |A \\arctan x |$ where $A = \\frac{2}{AFOV}$ and $x = \\frac{y_y(\u00b7)}{f}$. The function is monotonically decreasing when x>0 and monotonically increasing when x<0. Specifically, given a fixed yx, the relationship"}, {"title": "C.2. More Details", "content": "Network Structure. The structure of the R-VAT method is shown in Fig. 12. In this figure", "22": "m", "1.38": "rad. These parameters are consistent throughout each episode. Meanwhile", "\u03c0": "rad", "2.5": "U [2.5", "4.5": "m.\nDetails on coordinate transformations. Given two planes Po: $\\mathbf{n"}, {"P\u2081": "mathbf{n"}, {"follows": "n$\\mathbf{n"}, {"G_w": "z = h$ defined in the world coordinate system {w"}, "with its representation in the camera coordinate system {c} denoted as Gc : AGx + BGY + CGcz + DG = 0, the homogeneous vectors of these two planes are $\\mathbf{P}_{G} = (0,0,1,-h)$ and $\\mathbf{P}_{GC} = (A_{GC}, B_{G}, C_{G}, D_{Gc})$.\nFurthermore, from Tab. 9, we can obtain the HTM Tct from {c} to {w} defined as follows:\n$T_{ct} = \\begin{bmatrix} R_{et} & t_{et} \\\\ 0 & 1 \\end{bmatrix}$\nwhere Rct is the rotation matrix from {c} to {w}, which can be expressed in row vector form as: Rct = [r1, r2, r3"], "follows": "n$D_{te"}, {"mathbf{r}_{3}^{T}": "h + \\mathbf{r"}, {"follows": "n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)},$\nwhere \u03c0\u03b8 and \u03c0\u03b8\u03bf\u03b9\u03b1 are the new and old policies. Additionally, to"}]