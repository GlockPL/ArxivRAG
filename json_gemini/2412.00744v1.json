{"title": "A Cross-Scene Benchmark for Open-World Drone Active Tracking", "authors": ["Haowei Sun", "Jinwu Hu", "Zhirui Zhang", "Haoyuan Tian", "Xinze Xie", "Yufeng Wang", "Zhuliang Yu", "Xiaohua Xie", "Mingkui Tan"], "abstract": "Drone Visual Active Tracking aims to autonomously follow a target object by controlling the motion system based on visual observations, providing a more practical solution for effective tracking in dynamic environments. However, accurate Drone Visual Active Tracking using reinforcement learning remains challenging due to the absence of a unified benchmark, the complexity of open-world environments with frequent interference, and the diverse motion behavior of dynamic targets. To address these issues, we propose a unified cross-scene cross-domain benchmark for open-world drone active tracking called DAT. The DAT benchmark provides 24 visually complex environments to assess the algorithms' cross-scene and cross-domain generalization abilities, and high-fidelity modeling of realistic robot dynamics. Additionally, we propose a reinforcement learning-based drone tracking method called R-VAT, which aims to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning, we introduce a Curriculum-Based Training strategy that progressively enhances the agent tracking performance in vast environments with complex interference. We design a goal-centered reward function to provide precise feedback to the drone agent, preventing targets farther from the center of view from receiving higher rewards than closer ones. This allows the drone to adapt to the diverse motion behavior of open-world targets. Experiments demonstrate that the R-VAT has about 400% improvement over the SOTA method in terms of the cumulative reward metric.", "sections": [{"title": "1. Introduction", "content": "Visual Active Tracking (VAT) aims to autonomously follow a target object by controlling the motion system of the tracker based on visual observations [68, 73]. It is widely used in real-world applications such as drone target tracking and security surveillance [20, 48, 66, 70]. Unlike visual passive tracking [4, 6, 9, 28, 51, 60, 67, 75], which involves proposing a 2D bounding box for the target on a frame-by-frame with a fixed camera pose, VAT actively adjusts the camera position to maintain the target within the field of view. Passive visual tracking often falls short in real-world scenarios due to the highly dynamic nature of most targets. Thus, VAT offers a more practical yet challenging solution for effective tracking in dynamic environments.\nRecently, VAT methods have evolved into two main categories: pipeline VAT methods [14, 34, 40] and reinforcement learning-based VAT methods [17, 18, 33, 73]. Pipeline VAT methods employ a sequential framework where the visual tracking [5, 28, 29, 56] and control models are connected in series. Here, the object tracking model first processes the input image to estimate the target position and then the control model to generate the necessary control signals. While this modular design allows for clear task separation, these methods often require significant manual effort to label the training data, and the combination of modules requires extra effort for tuning and implementation. To address these issues, reinforcement learning-based VAT methods integrate visual tracking and control within a unified framework. These methods eliminate the need for separate tuning of the visual tracking and control modules by using a unified framework to map raw visual inputs directly to control actions. Therefore, the reinforcement learning-based VAT methods simplify system design and increase the efficiency of learning adaptive tracking behaviors in dynamic environments.\nUnfortunately, achieving accurate drone visual active tracking with reinforcement learning remains challenging, partly for the following reasons. 1) Missing unified benchmark. Existing benchmark scenes are limited in scope, low in complexity, and few in number, and unable to adequately validate the performance of agents (see Tab. 1). Previous methods provide limited tracking targets and sensors, insufficient to establish benchmarks for different tasks. 2) Vast environments with complex interference. Open-world tracking involves large, dynamic environments with frequent interference. It brings significant challenges for agents to accurately follow targets. Training directly in these conditions often leads to slow convergence or difficulty in building robust tracking behaviors. 3) Complex targets with diverse motion behaviors. Open-world targets often exhibit complex, unpredictable behaviors, requiring agents to adapt to varied movements and orientations. Existing methods assume a fixed forward-facing view, leading to reward functions that inaccurately reflect tracking performance across different behaviors and perspectives.\nTo address the above limitations, we first propose a unified cross-scene benchmark for open-world drone active tracking (called DAT) that simulates the diversity and complexity of the real world as possible (see Fig. 1). Specifically, the DAT benchmark provides 24 visually complex scenes to validate the algorithms' cross-scene and cross-domain generalization abilities. It offers comprehensive support for diverse tracking scenarios with 2 tracker types, 24 target types, and 6 different sensor types, with plug-and-play interfaces that facilitate the integration of custom robot models and controllers. To better replicate real-world conditions, DAT employs the webots simulation software [31] for high-fidelity modeling of realistic robot dynamics. It incorporates the Simulation of Urban Mobility (SUMO) [30] for managing target behavior, enabling diverse and efficient path modeling that surpasses traditional rule-based approaches. Second, we propose a novel drone visual active tracking with reinforcement learning method (called R-VAT), aiming to improve the performance of drone tracking targets in complex scenarios. Specifically, inspired by curriculum learning [35, 47, 58, 76], we propose a Curriculum-Based Training strategy that progressively improves agent performance in vast environments with complex interference. In addition, we design a goal-centered reward function to provide accurate feedback to the agent, enabling it to adapt to the target's diverse motion behaviors. Unlike existing methods, this function is designed at the image level to prevent targets farther from the center of view from receiving higher rewards than closer ones."}, {"title": "2. Task Definition of Drone Active Tracking", "content": "The DAT task seeks to train a model to control a drone for active target tracking in dynamic environments (see Fig. 2). Using visual and motion sensor data, the model learns actions to keep the target centered in view, ensuring robust performance across diverse scenarios.\nObservation spaces. The target is initially positioned at the center of the camera's field of view, and the observation space comprises data acquired from sensors (e.g., RGB images with a resolution of 84 \u00d7 84).\nAction spaces. There are two options for the action space: discrete action space and continuous action space. The discrete action space is a 7-dimensional vector, representing movements such as forward, backward, left translation, right translation, counterclockwise rotation, clockwise rotation, and stopping. The speed corresponding to each action must be set by the user. The continuous action space is a 4-dimensional vector that defines speed values in the forward, lateral, vertical, and yaw directions.\nSuccess criterion of DAT task. We define a success criterion when the model can keep the target object, which is initially located at the center of the camera's field of view, in the middle of the image for a long duration.\nChallenges. Open-world drone active tracking presents substantial challenges primarily due to the scarcity of real-world data and the high costs and risks associated with real-world trial-and-error. This highlights the critical need for building complex and diverse simulation environments. Additionally, open-world scenes are characterized by high diversity and dynamic elements, which introduce complex interferences, further challenging the model's robustness. Moreover, real-world tracking targets exhibit diverse and unpredictable behaviors, making it difficult for models to adapt. Therefore, improving the adaptability of models to track complex targets effectively is another key challenge."}, {"title": "3. DAT Benchmark with Diverse Settings", "content": "We develop the DAT benchmark (see Fig. 3) to evaluate the model's cross-scene and cross-domain adaptability in 6 large outdoor environments with 4 weather conditions."}, {"title": "3.1. Diverse Scene Construction", "content": "The construction of the DAT scene aims to simulate the diversity and complexity of the real world. To enhance diversity, we select 6 categories of outdoor scenes and 4 weather conditions, covering almost all natural conditions that applications may face. To simulate the real-world complexity, we model 7 aspects: scene area, building density, color richness, road density, terrain density, tree density, and tunnel density. Specifically, the scene area refers to the extent of the scene, building density is the ratio of the number of buildings to the scene area, and color richness is the number of dominant colors in the scene. These three aspects primarily depict the complexity of the visual background. Road density is measured by evaluating the density of complex elements such as intersections, terrain density is determined by assessing the density of special terrain features like mountains in the scene. These two aspects primarily depict the complexity of tracking target behavior. The tree density and tunnel density are calculated as the ratio of the number of trees and tunnels to the scene area, and are used to measure the level of visual occlusion within the scene.\nAs shown in Fig. 3(a), the six different scenarios exhibit unique and realistic complexity across the seven aspects:\n\u2022 Citystreet scene covers an area of 0.7 square kilometers. It has a road density of 38.7 and a tree density of 97.5, mainly testing the agent's efficiency against tree occlusions.\n\u2022 Village scene spans 1.4 square kilometers. This scene features a mountainous terrain density of 20.1 and a tunnel density of 6.3, requiring the agent to predict the target's movement when it is fully obscured by tunnels.\n\u2022 Downtown scene covers 0.8 square kilometers. The scene includes complex road elements and a very high building density of 304.9, challenging the agent's tracking accuracy and obstacle avoidance capabilities.\n\u2022 Lake scene encompasses 1.6 square kilometers. The density of road elements is 68.1, and the richness of background colors is rated at 5, challenging the agent's robustness across varying features and colors.\n\u2022 Farmland scene covers an area of 0.7 square kilometers. The color richness is rated at 5, with multiple color patches spread throughout the scene, posing a significant challenge to the agent's adaptability to multi-color environments.\n\u2022 Desert scene covers 1.1 square kilometers. It includes a mountainous terrain density of 37.1 and a road element density of 31.0. Some roads are covered by sand, testing the agent's adaptability to such challenging conditions.\nThe four weather conditions mainly test the agent's cross-domain adaptability. Foggy reduces visibility, with a visibility range of 400m to 2000m. Night reduces brightness and light uniformity, and snow changes the color tone of the environment. The above 24 scenes can fully measure the performance of the agent active adaptation (see Fig. 1)."}, {"title": "3.2. Various Trackers and Targets Construction", "content": "Drone Active Tracking in real-world applications involves various trackers and targets depending on the specific task. For example, drones are typically used as trackers, with automobiles as targets in security monitoring. The DAT benchmark provides diverse trackers and targets, making it adaptable for different use cases.\nTracker. The DAT benchmark supports two tracker types: drones and ground robots. The drone used is the DJI Matrice 100 [12], with a default flight controller [53]. The DAT benchmark provides visual and motion sensors (see Fig. 3(c)) that can obtain the state parameters of the drone relative to the world coordinate system. The drone position and velocity are obtained via GPS, angular velocity via gyroscope, Euler angles are obtained from the IMU and can be converted into quaternion, and acceleration via accelerometer. The LiDAR provides point cloud data. To better replicate real-world conditions, drones are equipped with a 3-axis gimbal, similar to those on commercial drones like the DJI Mini 3 Pro [13], enabling precise camera adjustments and adding complexity beyond static point trackers. For ground robots, users can select from various robot models. The benchmark employs webots simulation software to provide a high-fidelity dynamic engine, accurately modeling tracker movements, collisions, and interactions, crucial for evaluating tracking algorithms in challenging scenarios. Additionally, we provide common parameters for further task customization. See Appendix B for details.\nTargets. The DAT benchmark includes five categories of targets: automobile, motorbike, pedestrian, wheel robot, and legged robot, providing a total of 24 tracking targets, each with built-in controllers (see Fig. 3(d)). To support custom robot designs, a plug-and-play interface is available, allowing users to easily integrate robot models and controllers into the benchmark. See Appendix B for details.\nTarget Management. Realistic target behavior and path diversity are essential for simulating authentic environments. The DAT benchmark uses the SUMO traffic simulator to manage all scene targets. For example, SUMO generates random vehicle paths and dynamically controls actions like acceleration, lane changes, and stops. It can also refresh vehicles based on configurations and randomize their types and colors, creating a high-fidelity traffic system."}, {"title": "4. VAT with Reinforcement Learning", "content": "In this paper, we primarily focus on visual active tracking (VAT), a core task within the DAT benchmark. We propose a drone visual active tracking with reinforcement learning method (called R-VAT), aiming to improve the performance of tracking targets in complex scenes. As shown in Fig. 2, we model drone active tracking as a Markov Decision Process (MDP) and train a Drone Agent to track a target in the open scene. At time step t, the Drone Agent takes the current state $s_t$ as input and selects an action. Then the reward is calculated via Eq. (2). The trajectories are collected to train the agent via the Curriculum-Based Training strategy."}, {"title": "4.1. MDP for Drone Active Tracking", "content": "We explore a Drone Agent that achieves end-to-end camera control for better performance in highly dynamic, long-term visual tracking tasks. We model the end-to-end visual active tracking task as an MDP: $(S, A, R, \\gamma, T)$. In this representation, S denotes the state space, A represents the action space, and $\\gamma$ is the discount factor. At each time step t, the agent takes the state $s_t \\in S$ as input and performs an action $a_t \\in A$. Next, the simulator transitions to the next state $s_{t+1} = T(s_t, a_t)$ and calculates the reward $r_t = R(s_t, a_t)$ for the current step. The details of the MDP are as follows:\nState S is the visual information of the scene. At each time step t, the camera captures one image of size 84 \u00d7 84 as the current state.\nAction A is a set of discrete actions that the drone can take, including forward, backward, left translation, right translation, counterclockwise rotation, clockwise rotation, and stopping, respectively. At each time step, the Drone Agent selects an action $a_t \\in A$ based on the state $s_t$ and actively controls the camera movement.\nTransition T(st, at) is a function $T : S \\times A \\rightarrow S$ that maps st to st+1. In this paper, we use the webots dynamics engine to provide a realistic transition function.\nReward $R(s_t, a_t)$ is the reward function. The reward we designed is constructed only at the image level. The details are given in Sec. 4.3.\nNetwork structure of Drone Agent. Since the VAT task requires the agent to maintain long-term tracking, a dynamics model is essential for capturing temporal information. Therefore, similar to previous works [17, 33, 73], we select a CNN combined with a gated recurrent unit [11] network architecture as the backbone (see Appendix C)."}, {"title": "4.2. Curriculum Learning for Agent Training", "content": "The DAT environment contains numerous dynamic targets and diverse obstacles, making it challenging for the agent to discover sufficient successful trajectories. This difficulty results in low convergence rates and limited performance. Progressively training the agent from simpler to more complex environments increases the percentage of successful trajectories, enhancing performance and accelerating learning for the final task [57]. Therefore, we propose a Curriculum-Based Training (CBT) strategy for tracking to optimize agent training in complex environments.\nTraining Algorithm. To address the challenges of learning in complex environments, we employ the Proximal Policy Optimization (PPO) [49] algorithm, which is widely recognized for its robustness and efficiency in complex continuous control tasks. Given the complexity of the environment, we adopt a multi-stage training strategy, which divides the model training into two stages to build the agent's tracking capabilities progressively. The first stage $E_1$, consists of a simplified environment where the target trajectory is a straight line, and there are no obstacles or visual interferences. The agent learns to keep the target centered in the image through the reward signal from Eq. (2). Once the reward obtained for agent in first stage $E_1$ converges as:\n$\\frac{1}{T} \\sum_{t=1}^{T} r_t \\geq \\eta$, (1)\nwhere $r_t$ represents the reward obtained by the agent at time step t, T is the total number of time steps over which the reward is averaged, and $\\eta$ is a predefined threshold indicating satisfactory performance. Once this condition is satisfied, the agent transitions to the second stage $E_2$, where the agent encounters more varied tracking target movements and more complex visual information, such as tree occlusions, crosswalks, and poles. The goal for the agent is to develop stronger task generalization abilities based on task understanding gained in the first stage.\nData Augmentation. While simpler settings facilitate the agent's learning of task objectives, they also heighten the risk of the agent rapidly converging to a locally optimal action distribution, which can undermine the exploration process. Consequently, implementing data augmentation is essential for enhancing the agent's exploratory capabilities. This is achieved through the randomization of the drone's initial position and orientation relative to the target, which necessitates a broader range of actions from the agent to maximize rewards. Moreover, to enhance the agent's spatial perception ability, randomization is also introduced in its gimbal pitch angle. See Appendix C for details."}, {"title": "4.3. Goal-Centered Reward Design", "content": "Open-world targets often exhibit complex, unpredictable behaviors, requiring agents to adapt to varied movements and orientations. However, existing methods assume the drone is parallel to the target, leading to reward functions that inaccurately reflect tracking performance across different behaviors and perspectives (see Appendix C for theoretical proof). To address this, our reward function is defined only at the image level, relying on the target position within the image $I \\in R^{84 \\times 84}$. The reward decreases as the target moves away from the center $I_O$ of the image, and no reward is given if the target is outside as:\n$r_t = \\begin{cases} \\text{tanh} (x(1-x)^3) & I_{\\text{target}} \\in I_{\\text{clip}} \\\\ 0 & \\text{otherwise} \\end{cases}$, (2)\nwhere $x = \\frac{|\\text{pvc-IOG}|}{|\\text{Eco-Loc}|}$ represents the position of the target relative to the center of the image, which is calculated by projecting the image onto the ground. Specifically, we calculate the ratio of the distance from the projection $I_{OG}$ of the image center on the ground to the center $p_{vc}$ of the target (line segment $|p_{vc}-\\text{IOG}|$) and the distance from the intersection $E_G$ of the line connecting these two centers with the edge of the image projection to the center $I_{og}$ (line segment $|E_G-\\text{IOG}|$), as shown in Fig. 4(b). The attenuation degree of $r_t$ can be adjusted using the hyperparameter $\\alpha$, set to 4. The tanh() provides a strong indication of the task target due to its relatively quick decay at the center of the image. $I_{clip}$ is the truncated image range set to prevent the drone from keeping the target at the edge of the image for a long time. The truncation of the image can be controlled using the hyperparameter $\\lambda_{clip}$ as:\n$\\lambda_{clip} = \\frac{W_{\\text{Iclip}}}{W}$, (3)\nwhere $W$ and $W_{\\text{Iclip}}$ are the widths of the original image and the truncated image, respectively, we set $\\lambda_{clip} = 0.7$.\nThe reward function relies on the judgment of the corner points of the image: Left Up (LU), Left Down (LD), Right Up (RU), and Right Down (RD) and the target coordinates along with their projections. To map corner points onto the ground, the effective focal length of the camera (f) must first be estimated, as shown in Fig. 4(a). This can be calculated using the pinhole imaging principle [8], as follows:\n$f = \\frac{W}{2 \\tan (\\frac{FoV}{2})}$, (4)\nwhere $W$ is the camera image width (in pixels) and $FoV$ is the field of view of the tracker camera. After obtaining the focal length, we project the image corner points onto the target plane to check if the target falls within the image range, as shown in Fig. 4(a). This involves finding intersections between lines extending from the camera optical center through its corner points and the ground plane.\nAs shown in Fig. 4(a), in the camera coordinate system {c}, the optical center and four corner points have the coordinates C(0,0,0), LU(-f,-$\\frac{W}{2}$,$\\frac{H}{2}$), LD(-f,-$\\frac{W}{2}$,$\\frac{-H}{2}$), RU(-f,$\\frac{W}{2}$,$\\frac{H}{2}$), RD(-f,$\\frac{W}{2}$,$\\frac{-H}{2}$), where W and H are the image width and height. Therefore, the equation of the line connecting the four corner points and the optical center in {c} can be constructed as follows:\n$\\begin{aligned} l_{LUC}: & \\frac{x}{-f} = \\frac{y}{-\\frac{W}{2}} = \\frac{z}{\\frac{H}{2}} = t \\\\ l_{LDC}: & \\frac{x}{-f} = \\frac{y}{-\\frac{W}{2}} = \\frac{z}{-\\frac{H}{2}} = t \\\\ l_{RUC}: & \\frac{x}{-f} = \\frac{y}{\\frac{W}{2}} = \\frac{z}{\\frac{H}{2}} = t \\\\ l_{RDC}: & \\frac{x}{-f} = \\frac{y}{\\frac{W}{2}} = \\frac{z}{-\\frac{H}{2}} = t \\end{aligned}$, (5)\nwhere $l_{LUC}$ is the line connecting the upper left corner point LU to the optical center C, with same notation applying to $l_{LDC}$, $l_{RUC}$ and $l_{RDC}$. In the DAT scenes, the road surfaces that the cars travel on are smooth. Thus, in the world coordinate system {w} shown in Fig. 4(a), the ground plane $G_w$ as: $G_w: z = h$, where h represents the height of the ground. Next, the $G_w$ needs to be transformed to the camera coordinate system {c} to perform the intersection solution.\nIn coordinate transformations, a homogeneous transformation matrix (HTM) can be used to map a plane into another coordinate system. Suppose the analytical equation of the original plane $P_0$ is given by $A_0x+B_0y+C_0z+D_0=0$. The transformed plane $P_1$ has the equation $A_1x+B_1y+C_1z+D_1=0$. The homogeneous vectors for the two planes are $p_0= [n_0, D_0]$ and $p_1= [n_1, D_1]$, where $n_0$ and $n_1$ are the normal vectors. The HTM $T_{01}$ from plane $P_0$ to $P_1$ as:\n$T_{01} = \\begin{bmatrix} R_{01} & t_{01} \\\\ 0 & 1 \\end{bmatrix}$, (6)\nwhere $R_{01}$ and $t_{01}$ represent the rotation matrix and translation vector, respectively. The normal vector $n_1$ of $P_1$ can be expressed as: $n_1=R_{01}n_0$.\nAssume that the points $p_0 = [x_0, y_0, z_0]^T$ and $p_1 = [x_1, y_1, z_1]^T$ lie on the planes $P_0$ and $P_1$, respectively. These two points satisfy as follows:\n$\\begin{aligned} n_0p_0 + D_0 &= 0, \\\\ n_1p_1 + D_1 &= 0. \\end{aligned}$, (7)\nBased on coordinate transformation principles, the relationship between the two points can be derived as $p_1 = R_{ct}p_0 + t_{ct}$. Therefore, by solving the simultaneous equations, the expression for constant term $D_1$ can be obtained:\nD_1 = D_0 - n_1t_{01}. (8)\nAccording to the above equations, the HTM $T_{ct}$ from coordinate system {c} to {w} be used to map the homogeneous coordinate vector $P_G = (0,0,1, -h)$ of the ground plane to the camera coordinate system {c}, resulting in $P_G=(A_{G_c}, B_{G_c},C_{G_c}, D_{G_c})$. The equation of the ground plane in {c} is given as (see Appendix C for details):\nA_{G_c}x + B_{G_c}y + C_{G_c}z + D_{G_c} = 0. (9)\nFinally, we can combine Eq. (9) and Eq. (5) to solve the mapping of the four image corner points on the ground plane in the camera coordinate system {c} as follows:\n$\\begin{aligned} LUG: & (-f, -\\frac{W}{2}, \\frac{H}{2})t_{LU} \\\\ LDG: & (-f, -\\frac{W}{2}, -\\frac{H}{2})t_{LD} \\\\ RUG: & (-f, \\frac{W}{2}, \\frac{H}{2})t_{RU} \\\\ RDG: & (-f, \\frac{W}{2}, -\\frac{H}{2})t_{RD} \\end{aligned}$, (10)\nwhere LUG, LDG, RUG and RDG represent the mapping points of LU, LD, RU and RD on the ground plane. The expressions of $t_{LU}, t_{LD}, t_{RU}, and t_{RD}$ are as follows:\n$\\begin{aligned} t_{LU} &= D_{G_c}(A_{G_c}f + B_{G_c}\\frac{W}{2} - C_{G_c}\\frac{H}{2})^{-1} \\\\ t_{LD} &= D_{G_c}(A_{G_c}f + B_{G_c}\\frac{W}{2} + C_{G_c}\\frac{H}{2})^{-1} \\\\ t_{RU} &= D_{G_c}(A_{G_c}f - B_{G_c}\\frac{W}{2} - C_{G_c}\\frac{H}{2})^{-1} \\\\ t_{RD} &= D_{G_c}(A_{G_c}f - B_{G_c}\\frac{W}{2} + C_{G_c}\\frac{H}{2})^{-1} \\end{aligned}$, (11)"}, {"title": "4. VAT with Reinforcement Learning", "content": "Thus, for the car with homogeneous coordinates $P_{vG} = (x_v, y_u, z_v, 1)^T$ in {w}, it can be transformed to the camera coordinate system {c} using $P_{vc} = T_{ct} P_{vG}$.\nIn addition, the image center points $I_O$, whose coordinates in the {c} system are (-f, 0, 0), along with the line connecting it with the optical center C can be used to determine the intersection point $I_{OG}$(-$\\frac{fD_{G_c}}{A_{G_c}}$, 0, 0) on the ground plane in {c}.\nTherefore, the coordinates of the car $p_{vc}$, along with the four intersection points (LUG, LDG, RUG, and RDG) and the image center $I_{OG}$ can be utilized to calculate the reward."}, {"title": "5. Experiments", "content": "5.1. Experimental Settings\nImplementation Details. We conduct cross-scene and cross-domain tests. In cross-scene testing, the agent trained under daytime conditions in one environment is tested in different scenarios with the same weather. For cross-domain testing, it is evaluated in the same scene but under varying weather conditions. The training involves a range of 9.2M to 21.3M steps across 35 parallel environments (see Appendix E). The webots simulation runs at 500Hz, with the algorithm updating every four steps (125Hz). Episodes last up to 1500 simulation steps and were terminated early if the drone lost the target for over 100 consecutive steps, collided, or crashed. The drone translation speed is set to 40m/s, and rotational speed to 2rad/s. The map features 40 vehicles, each with a maximum speed of 20m/s and acceleration of \u00b125m/s\u00b2. During testing, the altitude is set to 22 meters, the pitch angle to 1.37 radians, and the target initializes at the camera's center.\nMetrics. We use cumulative reward (CR = $\\frac{1}{E_t} \\sum_{t=1}^{E_t} r_{ct}$) and tracking success rate (TSR = $\\frac{1}{E_{ml}} \\sum_{t=1}^{E_{ml}} r_{dt} \\times 100%$) to evaluate the agent performance. The CR primarily measures the tracker's ability to keep the target centered in the image, where $r_{ct}$ is the dense reward at time step t from Eq. (2), and Er is the length of the episode. The TSR measures the tracker's ability to maintain the target within the field of view, where $r_{dt}$ is the sparse reward (See Appendix C) at time step t and $r_{dt} = 1$ means the target within the field of view, $E_{ml}$ is the maximum episode length. Additionally, we position each agent relative to the target at $[0, \\frac{\\pi}{2}, \\pi, \\frac{3\\pi}{2}]$ rad for 10 episodes each, totally 40 episodes. The mean and variance of these results are then calculated as evaluation metrics for the specific map. The model's final cross-scene and cross-domain performance are both obtained by averaging results across different scenes.\nBaselines. We reproduce the two SOTA VAT methods: AOT [33] and D-VAT [18]. See Appendix D for details."}, {"title": "5.2. Comparison Experiments", "content": "We compare our R-VAT with the SOTA methods for within-scene performance and cross-scene cross-domain generalization performance on the DAT benchmark. As shown in Fig. 5, our method achieves consistently higher and steadily increasing rewards throughout training, demonstrating its effectiveness in improving tracking performance across scenarios compared to other methods.\nWithin-scene performance. We train the model on all scenes and evaluate it on the original scene. Our R-VAT performs significantly better than other methods as shown in Tab. 2. For the CR, the average performance improvement on six maps relative to the D-VAT method is 591%(35\u2192242). Regarding the TSR, the average enhancement is 279%(0.19\u21920.72).\nCross-scene performance. Our method demonstrates strong cross-scene generalization, as shown in Tab. 2. Specifically, R-VAT achieves a 376%(37\u2192176) improvement in average CR and a 200%(0.19 \u2192 0.57) improvement in average TSR compared to D-VAT.\nCross-domain performance. As shown in Tab. 3, our method outperforms existing methods significantly in cross-domain generalization. Specifically, the R-VAT demonstrates an average CR enhancement of 509%(35 \u2192 213) relative to D-VAT and TSR boost of 253%(0.19 \u2192 0.67)."}, {"title": "5.3. Ablation Experiments", "content": "We conduct ablation experiments on a Curriculum-Based Training strategy to demonstrate its effectiveness in challenging scenarios. Moreover, we conduct a validation on the farmland map to assess the effectiveness of the data augmentation techniques discussed in Sec. 4.2, as well as the reward design outlined in Sec. 4.3. Detailed descriptions of these methods are provided in Appendix E.\nEffectiveness of Curriculum-Based Training strategy. As shown in Tab. 4 and Tab. 5, the R-VAT method exhibits the most significant improvements over the reinforcement learning approach on maps citystreet, downtown, and farmland, which are characterized by dense elements and visual complexity. In the within-scene, cross-scene, and cross-domain tests, enhancements in the CR are 61%(150 \u2192 242), 54%(114\u2192176), and 68%(127\u2192213), respectively, while improvements in the TSR are 44% (0.50 \u2192 0.72), 44%(0.41 \u2192 0.59), and 49%(0.45 \u2192 0.67).\nEffectiveness of data augmentation. We apply randomization of angles (AR), horizontal displacement (HR), vertical displacement (VR), and gimbal pitch angle (PR) to enhance the tracking ability of the agent (see Sec. 4.2 for details). As shown in Tab. 6, adjusting AR or PR significantly improves the agent's performance, indicating that enhancing exploratory behavior through angle variations and improving spatial awareness by randomizing the gimbal pitch angle yield positive outcomes.\nEffectiveness of reward design. We contrast the performance of the R-VAT method when using the reward defined in Eq. (2) and that in [18]. As shown in Tab. 7, significant performance enhancements (about 800% improvement in TSR across-scene and cross-domain) are evident on the farmland map with the utilization of Eq. (2), underscoring the pronounced effectiveness of the proposed reward design in this work. More experiments can be seen in Appendix E."}, {"title": "6. Conclusion and Potential Impacts", "content": "In this paper", "including": 1}]}