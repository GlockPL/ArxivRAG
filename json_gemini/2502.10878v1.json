{"title": "Broadcast Channel Cooperative Gain: An Operational Interpretation of Partial Information Decomposition", "authors": ["Chao Tian", "Shlomo Shamai (Shitz)"], "abstract": "Partial information decomposition has recently found applications in biological signal processing and\nmachine learning. Despite its impacts, the decomposition was introduced through an informal and heuris-\ntic route, and its exact operational meaning is unclear. In this work, we fill this gap by connecting partial\ninformation decomposition to the capacity of the broadcast channel, which has been well-studied in the\ninformation theory literature. We show that the synergistic information in the decomposition can be\nrigorously interpreted as the cooperative gain, or a lower bound of this gain, on the corresponding broad-\ncast channel. This interpretation can help practitioners to better explain and expand the applications of\nthe partial information decomposition technique.", "sections": [{"title": "Introduction", "content": "Shannon's mutual information has been widely accepted as a measure to gauge the amount of information\nthat can be revealed by one random variable regarding another random variable. Partial information de-\ncomposition (PID) is an approach to refine and further decompose this fundamental quantity to explain the\neffect of interactions among several random variables. Recently, this approach has found applications in\nbiological information processing [1-7] and machine learning [8-10].\nThere exist different approaches to decompose the total information [11-25], but the general idea in the\ncase with two observables is as follows: the total information revealed by X and Y regarding a third quantity\nT is the mutual information between (X, Y) and T, i.e., $I(X, Y; T)$, and it needs to be decomposed into four\nnon-negative parts:\n\u2022 The common (or redundancy) information in X and Y, regarding T;\n\u2022 The unique information in X, but not in Y, regarding T;\n\u2022 The unique information in Y, but not in X, regarding T;\n\u2022 The synergistic information (or complementary) information in X and Y, regarding T, which only\nbecomes useful when combined, but useless otherwise.\nThis decomposition helps to explain the effect of combining X and Y, or separately using X or Y, to infer\nT. For example, in multi-modality machine learning, X can represent one modality such as the vision image\nof the event, Y another modality such as the soundtrack of the event, and T is the event's label. In this case,\nwe can use the amounts of synergistic and unique information to determine which multi-modality models\nwill be the most effective in capturing the interactions and therefore most likely to be accurate. In neural\nsignal processing, the quantities can be used in a similar manner to interpret different biological signals.\nOne of the most influential notions of partial information decomposition was proposed by Bertschinger\net al. [13], sometimes referred to as the BROJA PID (the initials of the authors)\u00b9. The desirable properties\nof this definition were thoroughly studied in [13], but in terms of the operation meaning, only a qualitative\njustification in a decision-making setting was given. The intuition and motivation were that these quantities"}, {"title": "Partial Information Decomposition", "content": "Before formally introducing the PID defined in [13], let us consider a few simple settings to understand\nintuitively how the decomposed information should behave.\n1. Common information dominant: X is a uniformly distributed Bernoulli random variable and $Y = T =$\nX. It is clear that $I(X, Y;T) = 1$. In this case, the common information should be 1, since X and Y\nare exactly identical. The other components should all be 0, since there is no unique information in\neither X or Y regarding T, and there is no synergistic information when combining X and Y.\n2. Synergistic information dominant: X and Y are uniformly distributed Bernoulli random variables\nindependent of each other, and $T = X \\oplus Y$, where $\\oplus$ is the XOR operation. Here, the synergistic\ninformation should be 1, and the other components should be 0. This is because X or Y alone do not\nreveal any information regarding T, and since they are completely independent, they do not share any\ncommon information either, but their combination reveals the full information on T.\n3. Component-wise decomposition: $X = (X_1, X_2)$ and $Y = (Y_1, Y_2)$, where $X_1, X_2, Y_1, Y_2$ are all uniformly-\ndistributed Bernoulli random variables, mutually independent of each other. Let $T = (X_1, Y_1, X_2\\oplus Y_2)$.\nClearly, in this case, the common information is still 0, but the two kinds of unique information are\nboth 1, and the synergistic information is also 1."}, {"title": "Partial Information Decomposition", "content": "We next introduce the formal definition for the partial information decomposition given in [13]. Let X, Y,T\nbe three random variables in their respective alphabet X, Y, and T, which follow the joint distribution\n$P_{X,Y,T}$. The total information between X, Y and T is written as $I_P(T; X, Y)$, where the subscript P is\nused to emphasize the distribution that the mutual information is computed for. As mentioned earlier, the\ndecomposition first needs to satisfy the total information rule:\nTotal information:\n$I_P(T; X, Y) = I^{(C)}(T; X,Y) + I^{(U_X)}(T; X,Y) + I^{(U_Y)}(T; X, Y) + I^{(S)}(T; X, Y),$ \n(1)\nwhere the superscript C indicates common (or redundancy) information, Ux indicates unique information\nfrom X, Uy indicates unique information from Y, and S indicates synergistic (or complementary) infor-\nmation. This condition simply states that the total information is the summation of four parts. The\ndecomposition also needs to satisfy the individual information rule:\nIndividual information:\n$I_P(T; X) = I^{(C)}(T; X, Y) + I^{(U_X)}(T; X, Y)$\n$I_P(T;Y) = I^{(C)}(T; X, Y) + I^{(U_Y)}(T; X, Y).$\n(2)\nThis rule dictates that the information that can be revealed from X regarding T needs to be the summation\nof the unique information in X, and the common information in both X and Y. Note that the synergistic\ninformation is not included here, since it can only manifest when combining X and Y."}, {"title": "Broadcast Channels", "content": "The broadcast channel is a well-studied communication system in classic information theory [26], where a\ntransmitter wishes to send two independent messages to two receivers through a channel with a single input\nsignal that induces two separate output signals; see Fig. 1 (a). To accomplish this goal, the transmitter must\nencode the messages in such a way that the individual messages can be decoded by the intended receivers.\nClearly, if the first receiver is completely ignored, the transmitter can send more information to the second\nreceiver, and vice versa. In other words, there is a tradeoff between the amounts of information that can\nbe sent to the two receivers. One particular important quantity here is the sum of the rates that can be\nsupported on this channel, often referred to as the sum-rate capacity.\nNext, we provide a rigorous definition of the broadcast channel capacity region, where the channel is\nallowed to be used many times at a time, i.e., a block code where the block length approaches infinity. A two-\nuser broadcast channel is specified by an input alphabet T, two output alphabets X and Y, and a conditional\nprobability distribution $P_{X,Y|T}$ that gives the channel law for each symbol $t \\in T$ and $(x, y) \\in X \\times y$. The\nalphabets are usually assumed to be finite, though the results usually hold under more general assumptions\n[29].\nDefinition 1. For a blocklength $n \\in N$, let $M_1$ and $M_2$ be two positive integers. An $(n, M_1, M_2)$-code for\nthe broadcast channel $P_{X,Y|T}$ consists of:\n\u2022 Two message sets:\n$M_1 = \\{1, 2, . . ., M_1\\}, M_2 = \\{1,2,..., M_2\\},$ \n\u2022 An encoding function\n$f: M_1\\times M_2 \\rightarrow T^n,$ \nwhich assigns to each pair of messages $(m_1, m_2)$ a length-n input sequence $T^n = (T_1, T_2, . . .,T_n)$,\n\u2022 Two decoding functions,\n$g_1: X^n \\rightarrow M_1,  g_2: Y^n \\rightarrow M_2,$ \nwhere $\\hat{M}_1 = g_1(X^n)$ is the estimate of message $M_1$ by receiver 1, and $\\hat{M}_2 = g_2(Y^n)$ is the estimate of\nmessage $M_2$ by receiver 2.\nDefinition 2. For an $(n, M_1, M_2)$-code, let $(M_1, M_2)$ be chosen uniformly from $M_1 \\times M_2$. The average\nprobability of error is defined as\n$P_e^{(n)} = Pr\\{(M_1, M_2) \\neq (\\hat{M}_1, \\hat{M}_2)\\}.$ \nA sequence of codes (indexed by n) is said to have vanishing error probability if $P_e^{(n)} \\rightarrow 0$ as $n \\rightarrow \\infty$."}, {"title": "Main Result: An Operational Interpretation of PID", "content": "In this section, we provide the main result of this work, which is an operational interpretation of partial\ninformation decomposition."}, {"title": "PID via Sato's Outer Bound", "content": "Researchers in the information theory community made numerous efforts to identify a computable character-\nization of the capacity region of general broadcast channels (see the textbook [29] for a historical summary),\nyet at this time, a complete solution is still elusive. Nevertheless, significant progress has indeed been made\ntoward this goal. Particularly, Sato [30] provided an outer bound for C, and it can be specialized to yield an\nupper bound for the sum-rate capacity of the general broadcast channel as follows:\n$C_{sum} \\leq C_{Sato} \\triangleq min_{Q \\in Q'} max_{P_T} I_{P_{TQ_{X,Y|T}}}(X,Y;T),$\n(5)\nwhere the set Q' is defined as follows\n$Q' = \\{Q \\in \\Delta_{XXY|T} : Q_{X|T} = P_{X|T}, Q_{Y|T} = P_{Y|T}\\},$\n(6)\ni.e., the set of conditional distributions that the marginal conditional distributions $P_{X|T}$ and $P_{Y|T}$ are\npreserved. The inner maximization is over the possible marginal distribution of the random variable $P_T$\nin the alphabet T. The form already bears a certain similarity to (4). Note that for channels on general\nalphabets (i.e., not necessarily optimized on a compact space), the maximization should be replaced by the\nsupremum and the minimization by the infimum. Due to the minimax form, the meaning is not yet clear,\nbut the max-min inequality implies that\n$min_{Q \\in Q'} max_{P_T} I_{P_{TQ_{X,Y|T}}}(X, Y; T) \\geq max_{P_T} min_{Q \\in Q'} I_{P_{TQ_{X,Y|T}}}(X, Y; T) = max_{P_T} min_{Q \\in Q} I_{Q_{X,Y,T}}(X, Y; T),$\n(7)\nwhere the set of Q is exactly the one defined in (4) with $Q_{X,T} = P_T Q_{X|T}$ and $Q_{Y,T} = P_T Q_{Y|T}$. The\ninner minimization of this form is exactly the same as the second term in (3). Though the max-min form\ndoes not yield a true upper bound of the sum-rate capacity, in the PID setting we consider, $P_T$ is always\nfixed, therefore, the max-min and min-max forms are in fact equivalent in this setting. The equivalence in"}, {"title": "Gaussian MIMO Broadcast Channel and Gaussian PID", "content": "One setting where a full capacity region characterization is indeed known is the Gaussian multiple-input\nmultiple-output (MIMO) channel [28,36]. In the two-user Gaussian MIMO broadcast channel, the channel\ntransition probability $P_{X|T}$ and $P_{Y|T}$ are given, with T being the transmitter input variable, and X, Y"}, {"title": "Revisiting the Examples", "content": "Let us now revisit the examples given earlier, and attempt to understand them in the broadcast channel\nsetting:\n1. Common information dominant: Here the two receivers both know the transmitted signal completely,\nand therefore, there is no difference even if they are allowed to cooperate, and the cooperative gain is\n0."}, {"title": "Conclusion", "content": "We provide an operational interpretation of the partial information decomposition given in [13] via a con-\nnection to the well-studied broadcast channel capacity in the information theory literature. The synergistic\ninformation is directly connected to the cooperative gain on the corresponding broadcast channel, either\nquantitatively exactly equal or being a lower bound. This interpretation can help us understand better why\nsuch decomposition can be used to guide model selection in machine learning and in the analysis of biological\nsignals, and potentially design new methods to utilize it.\nWe note that in the information theory literature, broadcast channels with cooperative receivers have\nbeen studied carefully [39], where the two receivers are allowed to conference on rate-limited links. It may be\nof interest to consider the corresponding notion of partial information decompositions, when the synergistic\ninformation can be parametrized by the degree of cooperation between the receivers."}]}