{"title": "INCREASING THE VALUE OF INFORMATION DURING PLANNING IN UNCERTAIN ENVIRONMENTS", "authors": ["GAURAB POKHAREL"], "abstract": "Prior studies have demonstrated that for many real-world problems, POMDPs can be solved through online algorithms both quickly and with near optimality [10, 8, 6]. However, on an important set of problems where there is a large time delay between when the agent can gather information and when it needs to use that information, these solutions fail to adequately consider the value of information. As a result, information gathering actions, even when they are critical in the optimal policy, will be ignored by existing solu- tions, leading to sub-optimal decisions by the agent. In this research, we develop a novel solution that rectifies this problem by introducing a new algorithm that improves upon state-of-the-art online planning by better reflecting on the value of actions that gather infor- mation. We do this by adding Entropy to the UCB1 heuristic in the POMCP algorithm. We test this solution on the hallway problem. Results indicate that our new algorithm performs significantly better than POMCP.", "sections": [{"title": "1. INTRODUCTION", "content": "We as humans instinctively gather information or ask clarifying questions when faced with task completion in uncertain situations. We know to do this because, even though we are delaying the task at hand, it is ultimately in our favour to work with complete information. Ideally, online planning algorithms like POMCP [10], whose sole job is to make plans for agents acting in uncertain situations, know to do the same. They would be able to strategically pick actions that will provide the information to best guide the agent's decision making. However, unlike humans, who can easily correlate information gain with the ease of task accomplishment, these algorithms cannot. It is a difficult task to get them to decide to take any action that explicitly gathers new information.\nMoreover, it would be highly unrealistic to assume that machines that employ these algorithms have complete, deterministic information about the environment that they are in. Such domains of problem-solving, where machines do not have deterministic information can be modelled using Partially Observable Markov Decision Processes (POMDPs) and this is what online planners solve. The solution to a POMDP is a sequence of actions called a policy which is the most optimal for an agent to take in order to reach its goal given its current (belief) state. Sometimes, this optimal sequence has one or more sub- sequences where carrying out an action means that the agent needs to collect information. Herein lies the crux of the problem. Existing approaches to solving POMDPs tell the agent that actions that gather information are not actually important because they do not bring"}, {"title": "2. BACKGROUND", "content": "the agent physically any closer to the goal. The planning algorithms do not see the value in taking this information gathering action. A situation where this happens is when there is a large time delay between when the information is collected and when this information is actually being used.\nFurthermore, real-world applications of these planning algorithms complicate the issue. They come with constraints where the planner has limited time before it needs to gather information, process it, and come up with an action that it thinks is the most viable given the previous two steps. Examples of situations like this can be seen all the time. Almost all computer-assisted human tasks face this problem. When a computer system is assisting a human with a task, to understand what the goal of the user is, the system needs to know well in advance so that it can help the user accomplish the end goal. The computer should be able to ask early in the process or infer what the user is trying to do rather than wait until later even though it will be a long time before ultimately that information might be used. So, how do we get the algorithm to see the value in these actions that explicitly gather information, especially when there is a large time delay between when they gather that information and when it's used?\nIn this paper, we explore a new way in which we can model the planning algorithm in a way that the information gathering actions' value is increased in the planning process. We do this in a computationally inexpensive manner and while preserving the anytime nature of existing solutions. The rest of the paper is organized as follows: Section 2 gives the background which is necessary to understand the problem definition detailed in Section 3. Section 4 details the specific solution that this project proposes. Section 5 describes the experimental setup designed to evaluate our solution, followed by a discussion of the results of our experiments. We conclude by summarizing our research and suggesting important avenues of future work in Section 6."}, {"title": "2.1. POMDP Definition.", "content": "A Partially Observable Markov Decision Process (POMDP) is a mathematical frame- work which allows us to model decision making in an environment where the agent has incomplete and non-deterministic information. Given a belief that an agent holds about the environment, a POMDP framework guides an agent in deciding what action to take, models how the environment might change and what rewards and observations the agent might re- ceive from the environment, and updates the agent's belief based on received observations. More formally, a POMDP is modeled as a 7-tuple (S, A, T, R, \u0396, \u039f, \u03b3) [4] where:\n\u2022 S is the set of all environment states (i.e., the situations an agent can find itself in)\n\u2022 A is the set of all possible actions the agent can take to accomplish tasks and goals\n\u2022 Z is the set of all possible observations the agent can receive that inform its uncer- tain beliefs about the current state of the environment"}, {"title": "2.2. Offline vs. Online Planning.", "content": "There are two contrasting approaches to estimating the Bellman Equation and construct- ing the policy an agent should follow: offline and online planning.\nOffline planning involves finding a global policy that prescribes the best possible action that an agent can take for every belief b \u2208 B before it operates in the environment. Thus, the agent goes into the environment with existing knowledge of what it needs to do for any situation. Popular and/or state-of-the-art algorithms for offline planning include PBVI [7], HSVI [11], SARSOP [2], Perseus [12]. However, offline planning is really only tractable for problems with small state spaces where only a small number (maybe hundreds or low thousands) of beliefs are actually reachable by the agent from its initial belief. For larger problems, with thousands, if not millions of states, the problem of finding a policy for every possible belief becomes increasingly complicated and time consuming due to the nature of Equations 4, 5 and 6. Furthermore, of all the possible belief states in B, not all of them are even reachable, so computing a policy for them would be a waste of precious time and resources.\nIn contrast, online planning algorithms generally compute good local policies for only the belief that the agent currently holds, then re-plans after it receives an observation and forms a new belief about the environment's state. Thus, the agent interleaves planning and acting as it operates in the environment (see Algorithm 1). This is generally done through look-ahead search to find the best action to execute at each time step in the environment. Popular and/or state-of-the-art algorithms for online planning include AEMS2, LSEM [3], POMCP [10], DESPOT [13]. These are anytime algorithms which means that they can return a valid solution to a problem even if it is interrupted before it ends. These algorithms are expected to find better solutions the longer they keep running. However, planning in real-time almost always means that there is a time constraint. As such, these anytime algorithms are able to return a policy that the algorithm has so far found to be the best as they calculate approximate solutions, instead of calculating exact values in Equation 4 for all beliefs. The more time these planners get, the better these approximations get, and the better policy they will generate.\nIn online approaches, we can generally bound the error of the approximate solution [9], finding useful policies with fewer computational resources. Furthermore, in extremely large problem domains, offline algorithms become intractable and online planning is the only way to go. As such, in this paper, we mainly concern ourselves with online planning algorithms, more specifically with POMCP."}, {"title": "2.3. Policy Trees.", "content": "One common approach to calculating the Bellman equation (Eq. 4) and generating a pol- icy, especially for online approaches, is to construct policy trees consisting of alternating layers of belief nodes and action nodes. The tree is created following a general structure"}, {"title": "2.4. POMCP.", "content": "One of the most computationally expensive parts of POMDP planning is calculating next beliefs (using Equation 1, which alone is O(|S|2) and must be performed every time a next belief is considered in the Bellman equation (Equation 4). Partially Observable Monte Carlo Planning (POMCP) [10] is a very popular and widely used online planning that goes around this issue using Monte Carlo Simulations.\nInstead of doing complete belief updates using Equation 1, POMCP approximates the probability distribution represented by a belief by using unweighted particle filters, with each particle being a representation of a state and a filter being a collection of such particles. The probability of the agent being in a particular state is represented by the number of particles of each type in a particle filter. These particle filters are stored in belief nodes and as the number of particles used goes to infinity, the particle filter comes to represent an exact belief.\nPOMCP constructs a planning tree using Monte Carlo Simulation, following the general structure of Algorithm 1. It makes use of the UCB1 heuristic [1] to pick the next node to expand. During the construction of a policy tree, bo in Fig. 1 starts off with an initial particle filter consisting of a pre-defined number of particles, sampled evenly from all the possible start states in the environment. The algorithm picks out a particle at random, simulates an action on it, and adds the subsequent belief nodes based on the received observation. This particle is propagated down the tree as more belief and action nodes get added. These nodes keep a copy of the particles that pass through them, gradually building up a filter."}, {"title": "3. THE PROBLEM", "content": "One of the benefits of using a POMDP as the representation of a decision problem is that it explicitly considers uncertainty in the environment. To reduce this uncertainty and achieve high utility, solvers such as POMCP will balance taking actions that gather infor- mation to reduce the uncertainty in the agent's beliefs and actions that achieve task-oriented rewards. The solver develops such policies because higher certainty about the true (hidden) state of the environment implies that the agent can, with less error, pick the sequence of ac- tions that gives it the maximum reward. In other words, the probability that an agent takes an action leading to an unexpected bad reward decreases the more certain its knowledge of the current state.\nThis phenomenon is accurately demonstrated in the Tiger Domain [4]. This domain consists of an agent that needs to choose from two identical doors. One of the doors has a tiger behind it and the other a pot of gold. Opening the door with the tiger gives the agent a reward of -100 while opening the other gives provides a reward of +10. The agent starts with a 50% belief that the tiger is behind the left door and a 50% belief that the tiger the right door (representing complete uncertainty about the tiger's location), implying that the expected reward of opening either door is -45. At this point, the agent should choose a"}, {"title": "3.1. Long Hallway Domain.", "content": "To demonstrate this problem with POMCP, we introduce a novel benchmark domain called the Long Hallway domain, which generalizes a classic, yet easier benchmark called the Hallway problem [4]. This new benchmark is illustrated in Figure 2.\nIn this new domain, there are ((18+2\u00d7k\u2081+2\u00d7k2) \u00d74) states: there are four orientations in each of the (18 + 2 \u00d7 k\u2081 + 2 \u00d7 k2) rooms, for instance in figure 2 the agent shown is in the state 'facing north in room a' state. There are 8 goal states and 8 trap states the two stars (denoted by stars) and two traps (denoted by pits) each with the four different orientations, respectively. Then, there are 48 observations that the agent could possibly make, the relative locations of the four different walls, whether the room is an ordinary room without special observation, or the special \u2018left' and \u2018right' observations in rooms f (2\u00d72\u00d72\u00d72\u00d73 = 48). The set of actions consist of wait, move forward, move backward, turn left, and turn right. The agent's task is to enter the room marked with the star (which"}, {"title": "4. THE SOLUTION", "content": "Every existing POMDP solution has a different way of biasing the best action. POMCP, for instance, uses the UCB1 heuristic which calculates the Q-Value and estimates the up- per bound of that Q-value to create an optimistic view of how that action might perform. However, using only this ensures that task oriented actions with high immediate rewards are heavily biased. Any action that collects information used in the future will seem un- necessary and as getting in the way of task completion. We want to convince the algorithm that, even though information gathering actions do not look beneficial in the near term, they are actually optimal in the long term. We need to be able to do so by preemptively signaling the planning algorithm that a specific trajectory down the tree is desired without having to send a maximum number of trajectories down the path (i.e. without depending on the accuracy of the Q-approximation).\nTo come up with a solution, let us go back to Equation 8 and analyse how voi changes. In the problem section, we talked about how the POMCP algorithm already implicitly takes into account the value of information when picking an action, as seen in the Tiger domain. However, when the approximation of the Q-values are not accurate enough, this advantage is nullified. The next logical question is, how can we approximate the Q-values for the trajectories without actually having to send down particles to approximate them. The short answer is that we cannot. Instead, we propose a different quantity as a proxy for the value of information in such situations: we introduce entropy in the heuristic that is used to pick the best node to expand.\nGiven a belief, the entropy of the belief is measured by the Equation:\n$H(b) = \\sum_{s \\in S} b(s) \\cdot log b(s)$"}, {"title": "5. RESULTS AND DISCUSSION", "content": "To determine whether our algorithm POMCPe improves upon POMCP for planning in environments where there is a long time delay between when information is gathered and when it is used, we conducted experiments comparing both algorithms in the Long Hall- way problem. In particular, we ran each algorithm 100 times on randomly chosen starting locations (room a of the left hallway or the right hallway) with K\u2081 = K2 = 1. We av- eraged the results from 100 runs so that we weren't gauging the performance based on"}, {"title": "5.1. Experimental Setup.", "content": "To determine whether our algorithm POMCPe improves upon POMCP for planning in environments where there is a long time delay between when information is gathered and when it is used, we conducted experiments comparing both algorithms in the Long Hall- way problem. In particular, we ran each algorithm 100 times on randomly chosen starting locations (room a of the left hallway or the right hallway) with K\u2081 = K2 = 1. We av- eraged the results from 100 runs so that we weren't gauging the performance based on"}, {"title": "5.2. Comparing POMCP and POMCPe.", "content": "As we can see in Table 1, POMCPe strongly outperforms POMCP in the domain for these values of K\u2081 = K2 = 1. The cumulative rewards for the most optimal sequence of actions in this domain would be 44.84 discounted and 88 cumulative. So our POMCPE algorithm did not quite find an optimal solution, although it was much closer than POMCP. The fact that our discounted reward was smaller than optimal but cumulative was so close implies that (1) the agent was reaching the goal state almost every run, but (2) sometimes it was taking more steps than necessary to reach the goal, i.e picking actions like wait, or doing then undoing actions. A look into the experiment logs re-enforces this idea. We would need better hyper-parameter tuning to prevent this from happening. Nonetheless, POMCPe does extremely well compared to POMCP, almost 100% of the time picking to go into the small side hallway, getting the special observation, and end up getting the +100 reward. Whereas POMCP never went down the side hallway, thereby never resolving entropy and always keeping the same uncertain belief with a 50% - 50% probability of being in either hallway, therefore not knowing what to do when it reached the top row. Hence, its cumulative reward is close to 0, which is expected given its purely uncertain belief."}, {"title": "5.3. Evaluating with Larger Hallways.", "content": "Now that we experimentally verified the issues that persist within POMCP, we wanted to see how far we can push POMCPe. So, we increased the values of K\u2081 and K2 to see POM- CPe's behavior in increasingly dense tree. The results from this experiment is summarized below."}, {"title": "6. CONCLUSION AND FUTURE WORK", "content": "This project set out to point out and rectify a flaw in POMCP: its inability to prioritize information gathering actions in the planning process when there is a large delay between when the information is gathered and when it is used. We achieved great success in doing so by highlighting POMCP's terrible results in the modified start state within the Long Hall- way domain and by coming up with a novel algorithm, POMCPe, that performs much better in similar situations. This algorithm works by considering the reduction in entropy (as a direct result of performing an action) in the planning process and biases search towards tra- jectories that present a maximum reduction. We tested this novel algorithm against POMCP in the Long Hallway with the modified start states, K\u2081 = K2 = 1 and K\u2081 = K2 = 2. In each one of these cases, POMCPe was able to direct the agent towards the special obser- vation in the small side hallway and almost always got the +100 reward whereas POMCP did terribly even in the modified start. However, POMCPe seems to get the agent to stay in the environment much longer than necessary resulting in a lower expected discounted and cumulative reward.\nFuture work for this project would involve better hyper-parameter tuning to prevent the agent from staying in the environment too long while using POMCPe. Another reason why this could be happening is that in the planning process, there are multiple trajectories with the same reduction in entropy, except one of them has multiple unnecessary actions in it before ultimately resolving entropy. If the planner encounters this longer trajectory before it encounters the most optimal one, it could end up biasing search towards this longer trajectory, ultimately leading to sub-optimal behaviour and the agent staying in the environment too long. To prevent this from happening, we will experiment with discounted entropy propagation within the tree i.e. the farther away the reduction in entropy is, the less the trajectory will be prioritized.\nWe would also make the transition and observation function non-deterministic to see how performance would change in the hallway domain. Furthermore, we will also test POMCPe in domains other than the hallway and see if the results extend to them as well. Once we have more results over multiple domains, we will work on establishing a rigorous theoretical analysis for why POMCPe works and that it performs just as well or better than POMCP.\nThis project has effectively demonstrated that POMCPe can increase the value of infor- mation gathering actions while planning in uncertain environments."}]}