{"title": "ArmorCLIP: A Hybrid Defense Strategy for Boosting Adversarial Robustness in Vision-Language Models", "authors": ["Yijun Li", "Yuhan Liang", "Yumeng Niu", "Qianhe Shen", "Hangyu Liu"], "abstract": "The robustness of Vision-Language Models (VLMs) such as CLIP is critical for their deployment in safety-critical applications like autonomous driving, healthcare diagnostics, and security systems, where accurate interpretation of visual and textual data is essential. However, these models are highly susceptible to adversarial attacks, which can severely compromise their performance and reliability in real-world scenarios. Previous methods have primarily focused on improving robustness through adversarial training and generating adversarial examples using models like FGSM, AutoAttack, and DeepFool. However, these approaches often rely on strong assumptions, such as fixed perturbation norms or predefined attack patterns, and involve high computational complexity, making them challenging to implement in practical settings. In this paper, we propose a novel adversarial training framework that integrates multiple attack strategies and advanced machine learning techniques to significantly enhance the robustness of VLMs against a broad range of adversarial attacks. Experiments conducted on real-world datasets, including CIFAR-10 and CIFAR-100, demonstrate that the proposed method significantly enhances model robustness. The fine-tuned CLIP model achieved an accuracy of 43.5% on adversarially perturbed images, compared to only 4% for the baseline model. The neural network model achieved a high accuracy of 98% in these challenging classification tasks, while the XGBoost model reached a success rate of 85.26% in prediction tasks.", "sections": [{"title": "1. INTRODUCTION", "content": ""}, {"title": "1.1. Importance of the Problem", "content": "The deployment of Vision-Language Models (VLMs), such as CLIP, in critical real-world applications like autonomous driving, healthcare diagnostics, and security systems underscores the importance of ensuring their robustness. These models must accurately interpret and associate visual and textual data to function effectively. However, their susceptibility to adversarial attacks presents a significant challenge. Adversarial attacks exploit vulnerabilities in these models, potentially causing them to misinterpret data, which could lead to severe consequences in safety-critical applications [1]. Thus, enhancing the robustness of VLMs against such attacks is crucial for maintaining the reliability and safety of systems that rely on them."}, {"title": "1.2. Categories of Existing Methods", "content": "To mitigate the adversarial vulnerability of deep learning models, two primary defense strategies have been developed: adversarial training and ensemble learning methods.\nAdversarial Training: This method trains models on both clean and adversarial examples to enhance robustness. While it has shown some effectiveness, it is computationally expensive and often fails to generalize to novel or adaptive attack strategies, limiting its practical applicability in diverse real-world scenarios [2].\nEnsemble Learning Methods: Techniques such as boosting (e.g., XGBoost, LightGBM) aim to improve robustness by combining multiple models. Although ensemble methods can increase resilience by aggregating diverse predictions, they also face limitations. High computational costs and vulnerability to adaptive adversarial strategies that target the ensemble mechanism itself are significant drawbacks. Additionally, the effectiveness of ensemble methods depends on the diversity and independence of the models used, which can be challenging to achieve consistently.\nDespite these advancements, existing methods often rely on specific assumptions, such as fixed perturbation norms or predefined attack patterns, which may not hold in real-world scenarios where adversaries use unexpected or sophisticated strategies."}, {"title": "1.3. Our Contributions", "content": "This research introduces a novel approach to improving the adversarial robustness of VLMs by integrating advanced adversarial training methods with innovative machine-learning techniques. We combine multiple attack strategies, including FGSM, AutoAttack, and DeepFool, to develop a hybrid model configuration that enhances model resilience. Additionally, we employ ensemble learning methods such as XGBoost and LightGBM to evaluate and improve the detection and classification of adversarial examples. These methods are specifically focused on identifying instances where models like CLIP initially fail.\nThe main contributions of our research are summarized as follows:\nMethodological Innovation: Development of a hybrid adversarial training framework that leverages multiple model configurations to bolster robustness against diverse adversarial attacks.\nAlgorithmic Advancement: Application of XGBoost and LightGBM for enhanced adversarial example detection and classification, dynamically adapting model defenses based on the nature of the attack.\nEmpirical Validation: Extensive testing on multiple datasets demonstrates the improved robustness and accuracy of the enhanced CLIP model against various adversarial attacks."}, {"title": "1.4. Thesis Layout", "content": "The thesis is structured as follows: Section 2 reviews related work in adversarial attacks and defenses in deep learning, as well as applications of VLMs and robustness challenges. Section 3 details our methodology, including the attack models, adversarial training strategies, and integration of machine learning techniques. Section 4 describes the experimental setup and evaluation protocols. Section 5 presents our findings, comparing the performance of baseline and enhanced models. Finally, Section 6 concludes with a discussion of the implications of our research, and Section 7 indicates directions for future work."}, {"title": "2. RELATED WORKS", "content": ""}, {"title": "2.1. Adversarial Attacks and Defenses in Deep Learning", "content": "Recent advancements in deep learning have highlighted the importance of developing robust models that can resist adversarial attacks. Traditional defenses, such as adversarial training and defensive distillation, have been explored extensively. However, these methods are often limited by high computational costs and the challenge of generalizing to novel attacks. Researchers have continued to explore new strategies, including input transformations and hybrid defense mechanisms, to address these limitations [3]."}, {"title": "2.2. Applications of VLMs and Robustness Challenges", "content": "Recent studies have explored using advanced machine learning techniques, such as ensemble learning and boosting methods, to enhance model robustness against adversarial attacks. For instance, XGBoost and LightGBM have been employed to detect and classify adversarial examples effectively, providing a robust mechanism to safeguard against such attacks [4]. These models, by aggregating the strengths of multiple learners, can potentially mitigate the impact of adversarial perturbations by improving the model's overall robustness."}, {"title": "2.3. Machine Learning Techniques for Robustness Enhancement", "content": "Ensemble learning techniques, including boosting algorithms like XGBoost and LightGBM, have shown promise in improving model robustness against adversarial attacks. By combining multiple weak learners into a single, stronger model, these techniques can reduce the vulnerability of individual models to adversarial examples [5]. However, the effectiveness of ensemble methods hinges on the diversity and independence of the models used, which can be challenging to maintain consistently. This research builds upon these insights by integrating ensemble learning with adversarial training to enhance VLM robustness."}, {"title": "2.4. Three Attack Models", "content": "To assess the robustness of VLMs, we utilized three well-known adversarial attack methods:\nFast Gradient Sign Method (FGSM): A gradient-based attack that generates adversarial examples by applying a perturbation in the direction of the gradient of the loss function aimed at maximizing the model's prediction error [1].\nDeepFool: An iterative method that calculates the minimal perturbation needed to move an input image across the model's decision boundary, resulting in misclassification [6]."}, {"title": "3. METHOD", "content": ""}, {"title": "3.1. Problem Setup", "content": "In this study, we focus on the problem of classifying adversarially perturbed images using various machine learning models. Let X represent the input images, Y the true labels, and \u0176 the predicted labels. The dataset consists of N samples, where each sample $x_i \\in X$ is an image, and $y_i \\in Y$ is its corresponding label. The objective is to minimize the misclassification rate under adversarial conditions, which introduces significant challenges in maintaining the robustness of the classification models.\nAdditionally, the models are also evaluated on their ability to predict the labels Y for new, particularly under adversarial conditions. This extends the study to assess the generalization capabilities of the models when predicting labels for new inputs that may also be adversarially perturbed. This problem is a standard classification task with added complexity due to adversarial attacks, following the framework outlined in previous studies."}, {"title": "3.2. Adversarial Training ---CLIP", "content": "In this study, we extended the capabilities of the Contrastive Language-Image Pretraining (CLIP) model by subjecting it to adversarial training aimed at improving its robustness against adversarial examples. The CLIP model, introduced by Radford et al., leverages a large-scale dataset of images and textual descriptions to learn visual concepts in a zero-shot manner, enabling it to perform various vision-language tasks without task-specific fine-tuning [8].\nAs shown in Figure 1, the samples used for adversarial training were generated by two different adversarial models, sequential attack integration and feature-level attack fusion. The generated samples were then combined into a new dataset for adversarial training of CLIP. The adversarial training process consists of two parts: the first step is to retrain CLIP, and the second step is evaluation."}, {"title": "3.1.1. Sequential Attack Integration", "content": "Sequential Attack Integration is a method where multiple attack strategies are applied in a sequence, progressively strengthening the adversarial perturbation. This multi-step approach ensures that the final adversarial example is more effective at circumventing model defenses.\nInitial Perturbation: Apply FGSM to generate a base adversarial example x'."}, {"title": "3.1.2. Feature-Level Attack Fusion", "content": "Feature-Level Attack Fusion is an advanced technique that synthesizes adversarial examples by combining features extracted from different attack-generated perturbations. This approach aims to leverage the strengths of each attack to produce a composite adversarial example that is more resilient to model defenses."}, {"title": "Independent Adversarial Example Generation:", "content": "Generate adversarial examples x'FGSM, x'Deepfool, and x'Autoattack using the respective methods."}, {"title": "Feature Fusion:", "content": "the features extracted from different adversarial examples are weighted and combined to generate a composite feature representation:\n$X_{combined} = \\frac{X'_{FGSM} + X'_{DeepFool} + X'_{AutoAttack}}{3}$"}, {"title": "3.1.3. Model Retraining", "content": "The model retraining process is as follows:\nData Preparation: The data preparation process involved two key stages: preprocessing and constructing."}, {"title": "Model Retraining:", "content": "The CLIP model is retrained using the expanded training set. A standard cross-entropy loss function is used, and learning rates and other training parameters are adjusted based on the model's performance on the validation set. During training, the model's performance on both clean and adversarial samples is closely monitored. After training, the fine-tuned model and its configurations are saved, ensuring easy future deployment and use."}, {"title": "Performance Evaluation:", "content": "The model's performance was evaluated on a validation set consisting of adversarial samples paired with textual labels. The evaluation metrics included accuracy, precision, recall, and F1 score, providing a detailed assessment of the model's robustness. The results were visualized through bar charts, comparing the baseline and fine-tuned models, and a confusion matrix was generated to offer a comprehensive view of the model's performance across different classes."}, {"title": "3.2. Classifier training", "content": "The training process began with loading and preprocessing the CIFAR-10 test set using the CLIP model's preprocessing pipeline, which included resizing the images to 224x224 pixels and normalizing them. Next, we implemented three adversarial attacks (DeepFool, FGSM, and AutoAttack) and extracted features from both the original and adversarially perturbed images. To ensure consistency in data input, these features were normalized and combined into a comprehensive feature set, encompassing adversarial samples generated by all attack methods."}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Datasets", "content": ""}, {"title": "4.1.1. Retrain Clip", "content": "The dataset was divided into two main subsets: a training set and a validation set. The training set comprised primarily of adversarial examples, which were used to fine-tune the CLIP model. This subset was essential for teaching the model to recognize and correctly classify images even when they had been deliberately altered to cause misclassification. The validation set contained a mix of clean and adversarial examples, allowing for a comprehensive assessment of the model's performance post-training. The validation set was critical for evaluating the model's generalization capabilities, ensuring that it could handle both perturbed and unperturbed inputs effectively."}, {"title": "4.1.2. Classifier Datasets", "content": "For this study, we utilized the CIFAR-10 test set, consisting of 10,000 images across 10 classes. The images were preprocessed using the CLIP model's standard pipeline, which involved resizing and normalization to ensure compatibility with the model's input requirements.\nIn addition to the original images, we generated adversarial examples using Attack models such as DeepFool, FGSM, and AutoAttack. The CLIP model was then used to extract features from both the original and adversarially perturbed images. These features were combined into a comprehensive dataset, along with the true labels, to evaluate the robustness and accuracy of various machine learning models under adversarial conditions."}, {"title": "4.2. Experimental Setup", "content": ""}, {"title": "4.2.1. Experimental Setup for Clip", "content": "The experiments were conducted on a computing environment natively equipped with a GPU to accelerate the training and evaluation processes. The fine-tuning and evaluation scripts were implemented in Python, using machine learning libraries such as PyTorch and the transformers library by Hugging Face."}, {"title": "4.3. Baselines", "content": ""}, {"title": "4.3.1. Clip Model", "content": "In this experiment, the original CLIP model was employed as the baseline to evaluate the effectiveness of adversarial training. The vit-base-patch32 model, developed by OpenAI, is a vision-language model that integrates the Transformer architecture with visual feature extraction. It has been pretrained on a large-scale image-text paired dataset, enabling it to demonstrate robust performance in tasks involving both vision and language. The baseline model, having only undergone pretraining on a large-scale dataset without any exposure to adversarial examples, is expected to perform well on standard image classification tasks. However, due to the lack of specialized training against adversarial samples, it exhibits significant vulnerability when faced with carefully crafted adversarial attacks. The selection of vit-base-patch32 as the baseline model was driven by its widespread usage and recognized performance, making it an ideal reference point for assessing the improvements brought by adversarial training."}, {"title": "4.3.2. Classifier Model", "content": "In our experiments, we employed four baseline machine learning models: AdaBoost, XGBoost, LightGBM, and Neural Networks. AdaBoost combines multiple weak classifiers into a strong one through iterative weight adjustments and weighted voting. XGBoost builds decision trees sequentially to minimize classification errors, leveraging cumulative predictions and learning rates. LightGBM grows trees leaf-wise, splitting the leaf with the highest information gain to optimize performance. Neural Networks, with their multi-layer architecture and non-linear transformations, use activation functions like ReLU to capture complex patterns in data, making them highly effective for tasks requiring deep learning."}, {"title": "5. EVALUATION", "content": ""}, {"title": "5.1. For Clip Retrain", "content": "The performance of both the baseline and fine-tuned models was rigorously evaluated using a set of standard metrics: accuracy, precision, recall, and F1 score. These metrics were selected for their ability to provide a comprehensive assessment of the model's classification capabilities across various dimensions.\nAccuracy was calculated as the proportion of correct predictions out of the total predictions, providing an overall measure of the model's performance. Precision, defined as the ratio of true positive predictions to the total positive predictions, reflects the model's ability to minimize false positives, while recall, the ratio of true positives to actual positives, indicates the model's sensitivity in detecting true positives. The F1 score, as the harmonic mean of precision and recall, offers a balanced metric, particularly useful in cases where there is an imbalance between these two measures.\nIn addition to these metrics, a confusion matrix was generated to visually represent the distribution of correct and incorrect predictions across different classes. The confusion matrix offered detailed insights into the specific categories where the model performed well or struggled, allowing for a more granular analysis of its strengths and weaknesses.\nThe performance comparison between the baseline CLIP model and the fine-tuned model revealed significant improvements as a result of the adversarial training. The baseline model, which had only undergone pretraining on a standard large-scale dataset, exhibited a relatively low accuracy of 4% on the adversarial validation set. This low performance highlighted the baseline model's vulnerability to adversarial attacks, as it was not specifically trained to handle such perturbations.\nIn contrast, the fine-tuned model achieved an accuracy of 43.5% on the same validation set, representing a substantial increase over the baseline. This improvement demonstrated the effectiveness of adversarial training in enhancing the model's robustness. The precision,"}, {"title": "5.2. For Classifier Performance", "content": ""}, {"title": "5.2.1. Comparison", "content": ""}, {"title": "5.2.2. Classifier Sensitivity Analysis", "content": "In the sensitivity analysis, it was observed that the four models responded differently to changes in hyperparameters such as learning rate and tree depth. AdaBoost demonstrated extreme sensitivity to these changes, with performance significantly degrading even with minor adjustments. This sensitivity makes AdaBoost more prone to performance deterioration in adversarial environments. In contrast, XGBoost, LightGBM, and the Neural Network exhibited higher robustness against hyperparameter variations, with LightGBM and the Neural Network particularly maintaining high classification accuracy and stability across different settings."}, {"title": "5.2.3. Classifier In-depth Analysis", "content": "The in-depth analysis further examined the misclassification patterns of each model in adversarial settings. AdaBoost, due to its focus on hard-to-classify samples which often includes adversarial examples was more susceptible to misclassification under these conditions. While XGBoost performed reasonably well, it was outperformed by LightGBM and the Neural Network in handling complex data structures and adversarial noise. LightGBM excelled due to its optimized leaf-wise growth strategy, effectively managing complex data and adversarial perturbations. The Neural Network, with its deep architecture and non-linear representation capabilities, demonstrated superior performance in capturing intricate patterns in images, especially in adversarial scenarios. Overall, LightGBM and the Neural Network were the most effective in dealing with adversarial samples."}, {"title": "5.2.4. Prediction Accuracy and Misclassification Analysis in XGBoost Model", "content": "In this section, we provide a detailed visualization and analysis of the model's predictions on new data (images 3000 to 3100), focusing on prediction probabilities, success rate, and instances of misclassification. The model demonstrated a strong overall performance with a success rate of 85.26%, accurately predicting the majority of the image labels. As Figure 11 shows, the prediction probabilities revealed that the model maintained high confidence in its correct classifications, exemplified by a 99.54% confidence for Image 3000, which was correctly classified as class 5.\nHowever, the analysis also identified several instances of misclassification, predominantly occurring between classes with overlapping visual features, such as classes 3 and 5 or classes 6 and 2. These errors suggest that the model struggles to differentiate between categories with similar characteristics. Additionally, lower confidence in some incorrect predictions, as seen in the misclassification of Image 3015 (true label 6, predicted as 2), reflects a degree of uncertainty in the model's decision-making process.\nFurthermore, the model encountered challenges with images containing complex features or potential adversarial perturbations. This highlights areas where further improvements are necessary to enhance the model's robustness and accuracy in more challenging scenarios, ensuring better generalization and performance across a broader range of inputs."}, {"title": "6. FUTURE WORKS", "content": "While our findings contribute significantly to the field of adversarial robustness, there are several avenues for future research:\nExploration of Other Adversarial Attack Strategies: Future studies could explore the application of additional adversarial attack strategies to further test and refine the robustness of VLMs. Incorporating more diverse adversarial examples could help in developing models that are even more resistant to various types of perturbations.\nDynamic Adversarial Training: Implementing dynamic adversarial training that adjusts the type and strength of adversarial examples during the training process could provide more robust defenses. By adapting in real-time to different adversarial strategies, models could learn to generalize better to unforeseen attacks.\nIntegration of Advanced Machine Learning Models: Further exploration into advanced machine learning models beyond XGBoost and LightGBM, such as CatBoost or more sophisticated neural network architectures, could enhance the robustness of VLMs. Research could focus on optimizing these models specifically for adversarial training to leverage their unique strengths fully."}, {"title": "7. CONCLUSION", "content": "In this study, we enhanced the robustness of the CLIP model against adversarial attacks. The experimental results demonstrate that the fine-tuned CLIP model significantly improved its ability to handle adversarial inputs, achieving an accuracy of 43.5% on adversarially perturbed images, compared to only 4% for the baseline model. This significant improvement, along with enhancements in precision, recall, and F1 score, highlights the effectiveness of our adversarial training framework.\nWe further analyzed the success rates of different attack methods and the performance of various models across different metrics. Specifically, the success rates of the three attack methods were as follows: DeepFool at 83%, FGSM at 36%, and AutoAttack at 95%. These results show that AutoAttack had the highest success rate, while FGSM was relatively weaker. These comparisons help us better understand the effectiveness of these attack methods and their threats to model robustness.\nIn terms of model performance comparison, LightGBM and Neural Networks performed exceptionally well across all metrics, achieving accuracies of 96% and 98%, respectively, significantly outperforming XGBoost and AdaBoost. It is worth noting that LightGBM, with its optimized leaf-wise growth strategy, not only achieved 96% accuracy but also excelled in handling high-dimensional data and countering adversarial perturbations. Neural Networks, on the other hand, were particularly outstanding in these challenging classification tasks, with F1 scores, precision, and recall all reaching 98%. In contrast, XGBoost achieved an accuracy of 94%, while AdaBoost, due to its sensitivity to noise and adversarial perturbations, performed relatively poorly across all metrics, with an accuracy of only 55%.\nIt is also noteworthy that the XGBoost model achieved a success rate of 85.26% when predicting new data (images 3000 to 3100), indicating strong overall performance on these data points. However, further analysis revealed that the model still faced challenges when dealing with images with complex features or potential adversarial perturbations, particularly in distinguishing between similar classes. This finding underscores the need for further improvements in model robustness and generalization capabilities in more challenging scenarios. In summary, the proposed method not only significantly improved the accuracy and robustness of the model under adversarial perturbations but also demonstrated superior performance in practical applications, providing a practical solution for enhancing model reliability."}]}