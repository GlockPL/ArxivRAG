{"title": "Assessing Data Augmentation-Induced Bias in Training and Testing of Machine Learning Models", "authors": ["Riddhi More", "Jeremy S. Bradbury"], "abstract": "Data augmentation has become a standard practice in software engineering to address limited or imbalanced data sets, particularly in specialized domains like test classification and bug detection where data can be scarce. Although techniques such as SMOTE and mutation-based augmentation are widely used in software testing and debugging applications, a rigorous understanding of how augmented training data impacts model bias is lacking. It is especially critical to consider bias in scenarios where augmented data sets are used not just in training but also in testing models. Through a comprehensive case study of flaky test classification, we demonstrate how to test for bias and understand the impact that the inclusion of augmented samples in testing sets can have on model evaluation.", "sections": [{"title": "I. INTRODUCTION", "content": "Data augmentation \u201c...involves enhancing the sufficiency and diversity of training examples without explicitly collecting new data... The essence of data augmentation lies in generat-ing new data by altering existing data points through various transformations\" [1]. Data augmentation has become a vital technique in software engineering (SE) to address training data limitations (e.g., small data set size, data set imbalance) in machine learning applications. This is particularly true with the advent of large language models in application areas such as code generation [2]. Augmentation methods for software engineering data sets include Synthetic Minority Oversampling Technique (SMOTE) [3], mutation analysis, neural network-based methods [4] and more recently the use of LLMs to augment or train data [2], [5].\nHowever, a critical methodological concern arises when augmented samples appear in both training and testing sets, potentially inflating performance metrics through pattern recognition rather than true generalization. This issue is especially pronounced in code-based tasks, where augmentation must preserve both syntactic and semantic validity [5]. We present a systematic investigation into augmentation-induced bias in SE models, using flaky test classification as a case study. Through carefully designed experiments, we:\n\u2022 Quantify the extent of performance inflation when augmented samples appear in both training and testing sets\n\u2022 Analyze how different types of code augmentation affect model bias across various categories of flaky tests\n\u2022 Propose guidelines for proper data set splitting when using augmented data to ensure more reliable model evaluation\n\u2022 Demonstrate how controlling for augmentation bias can provide more accurate assessments of model generalization capabilities\nOur findings reveal differences in model performance when evaluating truly independent test cases versus augmented variants of training samples. These results have important implications for how we assess and report the effectiveness of models trained on augmented data in SE tasks. Furthermore, our analysis provides insight on the development of more robust evaluation methodologies that can better predict real-world model performance.\nThe rest of this paper is organized as follows: Section II provides background on data augmentation in SE and current evaluation practices. Section III details our experimental methodology for assessing augmentation bias through a case study, findings, and discussion. Section IV concludes with a summary of our contributions and their significance to the SE community. The code is available in our GitHub repository."}, {"title": "II. BACKGROUND", "content": "A. Limited and Imbalanced data sets in SE\nThe use of machine learning and LLMs to solve special-ized software engineering tasks in niche domains is often challenged by limited or imbalanced data sets that result from a lack of available data. For example, this challenge is particularly evident in software testing, where the collection of comprehensive test suites covering all possible scenarios is often impractical [6]. Recent techniques, including the combination of fuzzing with LLM, have attempted to address the limited diversity in test data sets [6].\nWithin software testing, the problem of flaky test clas-sification exemplifies these data limitations. Flaky tests are tests that exhibit non-deterministic behavior, often passing and failing inconsistently when run on the same code version [7]. Research has successfully addressed the scarcity of labeled"}, {"title": "B. Current Practices and Potential Bias", "content": "While data augmentation has been utilized successfully across software engineering, current practices in evaluat-ing models trained on augmented data raise legitimate con-cerns.Comprehensive surveys of large model-based data aug-mentation have highlighted how these approaches, while out-performing traditional methods, introduce new challenges with respect to the quality and reliability of the augmented data [1].\nA significant concern lies in the common practice of in-cluding augmented samples as well as samples used in the generation of augmented samples, in both training and testing data sets. This practice can potentially lead to artificially inflated performance metrics that are not representative of data that is unconnected to augmentation. This practice becomes particularly problematic when variants of the same original sample appear in both the training and testing data sets. In this instance, models may learn to recognize patterns introduced by the augmentation process rather than truly generalizing to solve the underlying task.\nDespite the widespread use of data augmentation in SE tasks, the potential bias introduced by these practices has not been systematically studied. This gap in understanding motivates our current case study, which aims to quantify and characterize the impact of augmentation-induced bias in one particular instance of model evaluation."}, {"title": "III. CASE STUDY: ASSESSING DATA AUGMENTATION BIAS IN FLAKYCAT TRAINING", "content": "In this case study, we examine the introduction of bias through data augmentation in the context of flaky test clas-sification. We utilize the FlakyCat data set [9] and conduct two experiments to assess both the effectiveness and potential drawbacks of augmentation on the classification of flaky tests."}, {"title": "A. data set Composition and Preparation", "content": "To avoid self-confirmatory bias in our case study, we rely on an existing augmentation technique that was applied to the FlakyCat data set by a third-party [9]. The FlakyCat dataset consists of five primary categories of flaky tests: async wait (Async), test order dependency (TOD), time, concurrency (Conc), and unordered collections (UC). Each sample includes an original version (v0) and two augmented versions (v1, v2). An adapted Synthetic Minority Oversampling Technique (SMOTE) was utilized by Akli et al. [9] to create the aug-mented versions. SMOTE is a well-established data augmenta-tion method known for addressing class imbalances effectively. Akli et al. adapted SMOTE for test code by generating variants through controlled mutations of non-flakiness-related elements, such as variable names, constants (e.g., strings), and test method names, as well as adding unused variable declarations. These elements were replaced with randomly generated words to differentiate the augmented version source code from the original source code.\nThe adapted SMOTE method used by Akli et al. expanded the FlakyCat data set threefold across all flaky test categories while preserving the original flakiness distribution."}, {"title": "B. Training Configuration", "content": "For our experiments, we used a model from our previous work on FlakyXBert [10], which included a Siamese network classifier and CodeBERT [11] as its base encoder to generate the initial Flaky test code representations. The model was trained for 200 epochs using the contrastive loss function to distinguish between different root causes of flakiness by comparing pairs of test cases.\nThe training hyperparameters were:\n\u2022 Learning rate: 1 \u00d7 10-5\n\u2022 Batch size: 8\n\u2022 Optimizer: Adam with weight decay\nThese hyperparameters were selected to enhance the model's ability to learn from a small number of training examples without overfitting while optimizing the balance between training speed and memory usage.\nAll experiments were performed on a Dell RTX 4090 workstation with Nvidia drivers (ver. 555) and CUDA 12.5.\nThe experiments followed two distinct protocols (Experi-ment 1 and 2 as described in the following sections), with each maintaining its own training and testing splits according to their respective objectives."}, {"title": "C. Experiment 1 \u2013 Augmentation Impact Assessment", "content": "1) Experimental Design: Our experiment evaluates the im-pact of data augmentation using a two-phase approach with the same initial random sampling of test cases. In both phases, we split the dataset into training and testing sets using only original test cases. In Phase A, we establish the model's baseline performance by training and evaluating it on these original cases.\nIn Phase B, we follow a strict protocol: when an original test case is selected for training, both of its augmented versions are also included in the training set. Similarly, if a test case is selected for testing, both its original and augmented versions appear in the test set. This ensures a more accurate evaluation of the model's ability to generalize to unseen cases and assess whether it learns meaningful patterns rather than artifacts of augmentation.\nThis design enables a direct performance comparison with and without augmentation, quantifying its benefits."}, {"title": "2) Results and Analysis", "content": "This experiment evaluated whether data augmentation introduces bias by ensuring that related test cases (original and augmented versions) were kept together in training or testing sets. The results shown in Figure 1 reveal several key insights regarding augmentation's impact.\nThe model trained with augmented data (Phase B) con-sistently outperformed the baseline model trained only on original data (Phase A) across all categories, with an average F1 score improvement of 12% (from 48% to 60%). This improvement, achieved while maintaining the integrity of test case relationships, suggests that the augmentation process has an impact on the performance of the model.\nMost notably, categories with typically challenging pat-terns such as Async Wait and Time showed substantial im-provements (0.29 to 0.58 and 0.30 to 0.40 respectively), indicating that augmentation helps capture complex patterns. This suggests that augmentation potentially enhances the key characteristics that make these tests flaky."}, {"title": "D. Experiment 2 \u2013 Augmentation Bias Analysis", "content": "1) Experimental Design: The second experiment assesses the model's generalization by comparing its performance on augmented training data with entirely new test cases. The model is trained exclusively on original test case versions (v0) and evaluated on two test sets: Testing Set 1, containing original versions (v0) of unseen test cases, and Testing Set 2, comprising augmented versions (v1 and v2) of training cases.\nThis design examines whether the model performs differ-ently on augmented variants of familiar cases versus new cases, providing insight into its ability to generalize flaky test characteristics or its tendency to overfit augmentation patterns."}, {"title": "2) Results and Analysis", "content": "This experiment evaluated bias in the augmentation process by comparing model performance on augmented training data versus entirely new test cases, focus-ing on whether the model learns generalizable characteristics of flaky tests or becomes biased by patterns introduced through augmentation.\nThe results in Figure 2 reveal a systematic bias: the model performs consistently better on augmented versions of tests seen during training, with an average F1 score difference of 8% compared to new test cases. This performance gap is evident across most flaky test categories, except for Test Order Dependency. The findings suggest that augmentation intro-duces artifacts that make augmented tests easier to classify, though it also helps the model learn useful patterns for unseen data.\nThe exception in Test Order Dependency, where the model performs slightly better on new data, highlights potential differences in how augmentation interacts with this category or suggests that test order dependencies have more distinctive patterns that generalize well without relying on augmentation.\nThese results underscore a key challenge with data aug-mentation: while it improves overall performance (as shown in Experiment 1), it appears to do so partly by introducing systematic patterns that may not align with real-world flaky test variations. This highlights the need for augmentation techniques that can generate more naturally varied test cases while retaining the essential characteristics of flaky tests."}, {"title": "E. Threats to Validity", "content": "We have intentionally applied our analysis to augmented data created by third-party researchers (Akli et al. [9]) to reduce self-confirmatory bias.\nAkli et al. used an adapted SMOTE approach and the results we obtained from evaluating their augmented FlakyCat data set may not generalize to other augmentation strategies or other variations of SMOTE. Furthermore, because our results are based on a single dataset (FlakyCat) they may not generalize to other datasets either."}, {"title": "F. Implications for Practice", "content": "Based on our preliminary results, we see several practical implications:\n\u2022 The systematic performance gap between augmented and new test cases (8% average F1 score difference) suggests there is a need to maintain a completely separate validation set of original, non-augmented data dur-ing model evaluation and testing. This practice will help provide a more realistic assessment of the model performance rather than an evaluation that overestimates performance due to bias. Furthermore, this also means that data set creators need to clearly identify and label augmented data to support unbiased use by third-party researchers.\n\u2022 The varying effectiveness of augmentation across differ-ent flaky test categories (e.g., significant improvements for Async Wait versus mixed results for Test Order Dependency) indicates there is a potential benefit of category-specific augmentation strategies rather than a one-size-fits-all approach to data augmentation.\n\u2022 The implications of bias in data augmentation likely extend beyond flaky test detection. Researchers in areas such as defect prediction and fault localization can benefit from assess bias in augmented data as well as strategies to reduce any identified bias and thus equitably improve model performance across diverse contexts."}, {"title": "IV. CONCLUSIONS", "content": "This paper presents a case study on augmentation-induced bias in a flaky test classification model. Our experiments reveal both benefits and biases from using the augmented FlakyCat dataset. Specifically, we observed an average F1 score difference of 8% between evaluations on augmented and truly independent test cases, with bias varying across flaky test categories, ranging from minimal in Test Order Dependency (5%) to more substantial in Unordered Collections (13%). This indicates that augmentation effectiveness and bias depend on underlying code patterns.\nBased on our findings, we recommend using strictly sepa-rated, non-augmented validation sets for evaluation and adopt-ing category-specific augmentation strategies to maximize benefits. While augmentation is valuable for addressing data scarcity and diversity issues in software engineering, our findings highlight the need for careful use of augmented data and robust evaluations to detect biases and ensure results reflect real-world model performance."}, {"title": "V. FUTURE WORK", "content": "In the future, more studies of augmentation techniques, such as SMOTE and mutation-based methods, are needed to understand not just the benefits of augmentation, but also the risk of bias. Additionally, a broader investigation into the prevalence, usage practices, and impact of augmented data in real-world software testing practices would provide valuable insights. Lastly, developing best practices for mutation-based augmentation will help refine methodologies and promote their responsible and unbiased application in software engineering."}]}