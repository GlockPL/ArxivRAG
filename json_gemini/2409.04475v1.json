{"title": "Revolutionizing Database Q&A with Large Language Models: Comprehensive Benchmark and Evaluation", "authors": ["Yihang Zheng", "Bo Li", "Zhenghao Lin", "Yi Luo", "Xuanhe Zhou", "Chen Lin", "Jinsong Su", "Guoliang Li", "Shifu Li"], "abstract": "The development of Large Language Models (LLMs) has revolutionized Q&A across various industries, including the database domain. However, there is still a lack of a comprehensive benchmark to evaluate the capabilities of different LLMs and their modular components in database Q&A. To this end, we introduce DQA, the first comprehensive database Q&A benchmark. DQA features an innovative LLM-based method for automating the generation, cleaning, and rewriting of database Q&A, resulting in over 240,000 Q&A pairs in English and Chinese. These Q&A pairs cover nearly all aspects of database knowledge, including database manuals, database blogs, and database tools. This inclusion allows for additional assessment of LLMs' Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG) capabilities in the database Q&A task. Furthermore, we propose a comprehensive LLM-based database Q&A testbed on DQA. This testbed is highly modular and scalable, with both basic and advanced components like Question Classification Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Besides, DQA provides a complete evaluation pipeline, featuring diverse metrics and a standardized evaluation process to ensure comprehensiveness, accuracy, and fairness. We use DQA to evaluate the database Q&A capabilities under the proposed testbed comprehensively. The evaluation reveals findings like (i) the strengths and limitations of nine different LLM-based Q&A bots and (ii) the performance impact and potential improvements of various service components (e.g., QCR, RAG, TIG). We hope our benchmark and findings will better guide the future development of LLM-based database Q&A research.", "sections": [{"title": "1 Introduction", "content": "Database maintenance takes more than 20% in total database cost [47], and it heavily relies on human DBAs to maintain databases. However, it is tedious to configure, tune, optimize and upgrade databases, especially for millions of database instances on the cloud. Therefore, there is a need to design an intelligent database Q&A bot as a copilot of DBAs. As illustrated in Figure 1 (a), database Q&As can be generally categorized into (1) General DB queries, which involve fundamental concepts in the database domain, e.g., SQL grammar, the definition of the E-R data model. (2) Product-specific queries, which relate to the use of particular database products, e.g., set up a local PostgreSQL database, operational instruction for Snowflake. (3) Instance-specific queries, which are centered on a particular database instance, e.g., fault diagnosis and data analysis for a bank's database running on Postgres v16 with Intel i7 CPU.\nTraditionally, seeking solutions to these questions is challenging, often requiring users to consult a vast array of textbooks, product documents and search engines. Users also need to repeat manual operations to obtain contextual information and engage in multi-round dialogues. For example, as shown in Figure 1 (b), users have great trouble getting the final answer by repeatedly (i) searching diagnosis knowledge and then (ii) verifying in their database environment (e.g., examine error messages or resource usage).\nThe emergence of Large Language Models (LLMs) [23] has revolutionized Q&A in various domains, such as medicine [42, 44], finance [16], earth science [4], law [7], etc. Our vision for the LLM-based DB Q&A bot is that, as shown in Figure 1 (c), the user only needs to input a query, and the bot will autonomously utilize various external sources such as DBMS manual documents and database instance information to provide a customized and accurate answer. However, whether the current LLMs can fully realize our vision is unclear. For example, as shown in Figure 1 (d), even the most popular and powerful LLM (GPT-4) cannot always give correct and targeted answers. There lacks a comprehensive evaluation of LLMs on DB-specific Q&A tasks that can enable us to understand the challenges and opportunities of database QA in the era of LLMs. Although numerous research efforts have evaluated LLMs from different aspects, they have not been dedicated to database Q&A tasks, and face the following challenges.\nC1: Database Q&A Dataset. To increase the scalability of manually designed datasets, current evaluations of LLMs are mostly based on data collected from the Internet [11, 26, 27, 34], which has the following issues in the DB domain. (1) Low Question Quality. Online questions are too brief and lack essential contextual information. For example, a question on slow database performance without hardware (CPU, IO) or query information is impossible to diagnose. (2) Low Answer Quality. Many online answers are factually incorrect, overly concise, or too subjective. (3) Limited Diversity. Due to factors such as conformity bias, questions in online communities like StackOverflow tend to center on a narrow range of topics and DBMS products, e.g., users are too shy to pose \"silly\" questions, or most questions concern popular DBMS products.\nC2: Database Q&A bot testbed. Previous evaluations of LLM [20] primarily focus on a standalone LLM. Unfortunately, regardless of the LLM backbone architectures, a series of components are indispensable in a DB Q&A bot system to answer a wide range of DB questions. These components include: pre-training to equip the LLM with domain knowledge to answer DB general questions, fine-tuning to enhance the LLM to follow DB-specific instructions, routing to adopt different reasoning logics for various types of questions, retrieving to consult an external product knowledge source to answer product-specific questions, and tool invocating to interact with the DB environment to answer instance-specific questions. Thus, a testbed is required to support all LLMs and integrate with these components.\nC3: Database Q&A evaluation pipeline. Existing evaluations often rely on LLM's final output, which is inadequate for measuring DB Q&A performance. On the one hand, proper evaluation protocols and metrics for intermediate steps are lacking to measure the multi-dimensional abilities of LLMs. For instance, how do we measure the ability of DB-specific planning and reasoning? A part of these abilities depends on correctly organizing and utilizing different DB tools in the step of tool invocations. On the other hand, measuring the end-to-end performance (i.e., answer quality) is far from trivial. The LLM produces an unstable final answer because there are multiple intermediate steps with unpredictable outcomes. It is imperative to adopt a standardized evaluation pipeline that minimizes the drift in the final answer and fully reflects the LLM's abilities in the context of DB Q&A.\nTo address these challenges, we construct a comprehensive benchmark DQA (Database Question-Answer) and present a thorough evaluation based on DQA.\nTo address C1, we construct a large Q&A dataset with high quality and diversity. We collect a variety of online queries from online DB courses and forums and enrich the queries with contextual details to enhance question qualities. We collect, clean and rewrite online answers to improve answer qualities. We propose a novel method based on few-shot learning and chain of thought to extract Q&A pairs from DBMS documents and instances to increase Q&A diversity and cover more DBMS products, instances and tools.\nTo address C2, we propose a full-chain testbed that incorporates and modularizes all necessary components of a complete DB Q&A bot to handle real-production DB Q&A scenarios, including pre-training, fine-tuning, Question-Category Routing (QCR), Prompt-Template Engineering (PTE), Retriever-Augmented Generation (RAG) and Tool-Invocation Generation (TIG). We have improved the implementation of each module to better adapt a general-purpose LLM to the DB Q&A task."}, {"title": "2 Related Work", "content": "2.1 Q&A by Large Language Models\nQ&A with General-purpose LLMs. LLMs such as GPT-3.5 [23], LLaMa [31] and PaLM [1] have emerged when researchers have discovered that model performance can be enhanced by scaling up both the model and training data, in accordance with scaling laws. These models demonstrate Q&A capabilities far beyond traditional pre-trained language models. For instance, GPT-4 [24] achieves human-level performance on the majority of professional and academic examinations, ranking in the top 10% of candidates on the bar exam. Additionally, to meet the demands of edge deployment (i.e, deploy on consumer-grade GPUs), smaller yet powerful LLMs have also been developed. Examples include Llama3-7B [31], Mistral-7B [15], Baichuan2-13B [38] and Qwen-14B [2]. More smaller models, such as Yuan-2B [35], have also been developed. However, research indicates that the performance of these models is constrained by their size, making them suitable only for simple, singular tasks.\nQ&A with Domain-Specific LLMs. In the professional domain, due to (1) the diverse language styles of questions and (2) the complexity and depth of expertise, many customized LLMs have been developed. These include models in domains such as medicine [42, 44], finance [16], earth science [4] and law [7]. These LLMs typically possess deeper domain-specific knowledge and capabilities. Consequently, many smaller-scale models can achieve or even surpass the Q&A abilities of GPT-4 in their respective domains.\nQ&A with knowledge-augment to LLMs. To ensure the quality of Q&A in LLMs, researchers have introduced external knowledge to enhance answer generation. The external knowledge can be brought as follows: (1) By retrieving documents [10] or guidelines[21] for questions to enhance the answer generation. (2) By using structured knowledge, such as knowledge graphs, for reliable reasoning [29]. (3) By using external tools triggered by LLMs to acquire knowledge even being an agent [33].\n2.2 Large Language Models for Database\nLLMs for Database Optimization. Since LLMs have demonstrated outstanding capabilities in knowledge comprehension and contextual understanding, researchers in the database domain have started to explore LLMs for various database-related tasks. Raul et al. [9] argue that LLMs can ground database tuples, schemas and queries in novel ways. Zhou et al. [45] have proposed that LLMs can serve as Database Administrators (DBAs) by constructing LLM-based intelligent agents. The D-Bot [46] initiative applies LLMs to intelligent database diagnosis. Additionally, Liu et al. [19] use LLMs for query rewriting. These advancements illustrate the expanding usage of LLMs in the database domain. However, these studies primarily focus on specific database tasks, rather than answering general user questions in our paper.\nLLMs for NL2SQL. For data management tasks, databases serve as tools, requiring SQL for user-database interactions. LLMs have shown impressive capabilities in converting natural language into SQL, bringing about significant changes in simplifying user interactions with databases. Works such as Binder-SQL [6], DIN-SQL [25] and BIRD [17] enable LLMs to generate corresponding SQL statements directly from natural language requirements. DB-GPT [37] extends this concept by creating a more comprehensive process, allowing users to input their requirements in natural language and receive complete visualized data analysis and reports. This showcases the potential of LLMs to enhance accessibility and efficiency in data management tasks. Compared to these works, our paper considers more comprehensive questions that the database users may ask, rather than focusing solely on SQL programming and data analysis pipeline as addressed in the previous works."}, {"title": "3 DQA Dataset Generation", "content": "A dataset comprising question and answer pairs is necessary to evaluate the performance of a DB Q&A bot. Manually constructing such a dataset is too costly. However, online resources can not provide high-quality and broad-coverage database-related questions.\nAs shown in Figure 1 (a), the DB queries are divided into three subsets, corresponding to three key skill sets of an LLM-based DB Q&A bot. General DB queries can evaluate whether the bot grasps DB-related concepts and knowledge. Product-specific queries can evaluate whether the bot applies knowledge and adapts to real-life DB circumstances. Instance-specific queries can evaluate the proficiency of the bot in DB troubleshooting.\nThe three query categories are different in (1) data sources: general DB queries are publicly available, while product-specific queries and instance-specific queries are almost impossible to get complete examples online. (2) problem background: the latter two categories (i.e., product-specific queries and instance-specific queries) need supporting information, such as the product manual and instance context. (3) ground-truth answers: the latter two categories must provide retrieval results or tool invocation results to demonstrate the reasoning and produce trustworthy answers.\nData Statistics. Accordingly, we propose methods to construct the dataset for each category separately. As shown in Table 1, we construct a dataset with bi-lingual Q&A pairs on the three categories, translating English pairs into Chinese and vice-versa, leading to a total of over 120,000 Q&A pairs in each language.\n3.1 General DB Q&A\nData sources. We have two types of data sources. (1) As in the construction of other domain-specific evaluation datasets, exams are a good data source because exams offer objective and typically accurate ground truths. We collect 2000 unique multiple-choice questions from 4 DB university textbooks, 3 online courses and 28 online course exams. (2) To ensure that the DQA dataset covers a wide range of daily questions asked by DB users, we collect all resolved Q&A entries from the largest English and Chinese online DB communities, namely the database section of StackOverflow, the Database Administrators section of StackExchanges and MoDB6.\nStep 1: Filtering. One major problem of online Q&A pairs is that the answers are not guaranteed to be factually accurate. Thus, we filter the collected content based on online feedback. First, we compute the ROUGE-1 score, which measures the overlap of unigrams between queries. Queries with a large ROUGE-1 score (\u2265 0.8) are merged to de-duplicate the questions and reduce possible grammatical problems. For each question, we retain only the accepted answers and those with high upvotes to ensure the factual correctness of the responses.\nStep 2: Rewriting. The answers in the Q&A data collected are insufficient as evaluation ground truth. For example, the exam questions are only associated with letter options, and the LLMs' generation may be too random when only generating a letter option, making the answer unstable in assessing the professional levels of LLMs. Therefore, for each exam query, we extend the answer by instructing GPT-4 to provide detailed explanations for the answer choices. Meanwhile, online responses are also often overly concise, emotional and subjective. As shown in Table 2, while the replies explicitly advise the inquirer to learn basic Linux knowledge, they do not specify a learning path or key points, and the tone is not user-friendly. For each online query, we reform the accepted response by instructing GPT-4 to convert it to a \"detailed, professional and friendly\" writing style. Table 2 shows the prompts, and the rewritten results are more specific and friendly.\n3.2 Product-Specific Q&A\nConstructing product-specific Q&A pairs from online sources can be problematic. Because most DBMS products are constantly being updated, online queries can be outdated, and the accepted answers can be unreliable and inaccurate. Thus, it is important to ground the LLM on an external knowledge source, e.g., a certain product manual. We construct the product-specific Q&A pairs via the workflow illustrated in Figure 2."}, {"title": "3.3 Instance-Specific Q&A", "content": "It is infeasible to construct instance-specific Q&A pairs from online sources. Online queries are almost always incomplete due to privacy reasons, missing necessary instance-level contextual information, such as the database's table structures, workload information, etc. It is impractical to conduct DB interaction with the specified DB instance referred to in the online query to restore the contextual information. Therefore, we have to generate instance-specific Q&A pairs by LLMs automatically.\nThere are numerous database tools provided in DBMS that support database monitoring, optimizing and analyzing for DB developers, administrators and analysts. The LLM's proficiency in answering instance-specific questions relies on whether LLMs can accurately invoke different tools to obtain the instance's contextual information. Thus, as shown in Figure 3, our dataset construction workflow starts with building a DB tool pool.\nStep 1: Constructing DB tool pool. (1) We first survey the DB tools commonly used in real-production systems. As shown in Table 3, we identify six types of common DB tools that are frequently used for data modeling, database monitoring and performance optimization. The implementation of each tool type, including the exact tool name and the format of input and output, may vary for different DBMS products. (2) The common tools can not cover all DB tools, especially new ones developed to meet the demands of a real-production system. We expand the common tools to a set of generalization tools to evaluate the Q&A bot's generalization ability to utilize different DB tools properly. We include scenarios such as Operations Diagnostics, Business Intelligence, Performance Monitoring and Tuning, Data Analysis, System Utilization Analysis, Deployment Issues, Operations Management, Query Optimization, Backup and Recovery, Permission Management, Index Management, and Database Tuning. We require GPT-4 to generate imaginary DB tools that can benefit the above DB scenarios.\nStep 2: Generating questions. For each tool, including common tools and generalization tools, we require GPT-4 to generate questions that can be solved by the target tool using the prompt in Figure 3. Moreover, we want to effectively evaluate the Q&A bot's planning ability, which involves adequately combining several DB tools and organizing tools with the right action order. Thus, we manually construct 3-4 questions for each common tool and scenario that demand a chain of multiple tool invocations, and we use these questions as few-shot examples to encourage GPT-4 to generate complex questions. We post-process the resulting questions to ensure that more than 50% of the generated questions are solved by invoking at least two DB tools.\nStep 3: Generating answers. (1) First, we manually produce answer cases for manually constructed questions above. The DB experts compose the answer cases in the following procedure: construct a real DB instance according to the description in the question, call the DB tools when necessary, and answer the question based on real tool feedback. (2) Then we use the answer cases as few-shot learning examples to guide GPT-4 to generate answers efficiently. We adopt the Chain-Of-Thought(COT) prompting technique to generate an answer for each question. The prompt [40] encourages GPT-4 to break down the problem of answer generation into a series of intermediate reasoning steps. Each step corresponds to either tool invocation or answer generation, including \"Thought\", \"Action,\" \"Action_Input,\" \"Observation,\" and \"answer.\" Here, \u201cThought\u201d is the logical reasoning, \"Action\" and \"Action_Input\" are the tool name and the tool's input, \"Observation\" is the instance's information returned by calling the tool. This way, even if errors occur, e.g., GPT-4 produces incorrect actions to trigger the tool or the observation does not simulate the tool's output, GPT-4 can switch to alternative approaches, resulting in more accurate and reliable answers.\nStep 4: Polishing answers. Finally, we ask GPT-4 to rethink and polish the answer. This step is different for common tools and generalization tools. (1) For each answer to common tools, since the common tools are real DB tools with pre-defined formats of tool output, we ask GPT-4 to examine its output to ensure the format of the answer is correct to trigger the tool. (2) For each answer relating to generalization tools, since the generalization tools are imagined and reasonably inferred by GPT-4, they do not have a pre-defined format; we ask GPT-4 to summarize the tool's format."}, {"title": "4 DQA Testbed", "content": "When adopting a general-purpose LLM for DB Q&A, various auxiliary modules are indispensable to leverage and adapt the LLM's general knowledge of linguistic patterns and common senses into the DB environment. Currently, there is no complete database Q&A testbed that incorporates LLM and various auxiliary modules. Table 4 compares the completeness of the proposed testbed with recent LLM adaptation frameworks in the open domain and DB domain. Existing works overlook some important modules. Specifically, without domain continual pre-training and fine-tuning, the LLMs are not able to evolve as new domain-specific topics emerge. Without question routing, the LLMs can become trapped in incorrect logical reasoning paths, e.g., attempting to answer a product-specific problem on its own leads to hallucination while there are product manuals to provide factual groundings. Limited coverage of the DB tools can harm the LLM's ability as a DB agent. On the contrary, our proposed testbed supports a full chain of auxiliary modules for LLM's domain adaptation.\nThe workflow of the proposed testbed is shown in Figure 4. Offline. Before deployment, the core LLM module goes through the stage of continual pre-training and fine-tuning to acquire more specific DB concepts and skills while preserving the LLM's general linguistic knowledge. The user can also load a knowledge source of documents (usually up-to-date materials that do not appear in the training corpus, or in-house data for privacy reasons) stored in the form of a vectorized database.\nOnline. When the user submits a query, it first goes through the Question Classification Routing (QCR) module to determine the logical structure of reasoning an answer. Depending on the result of QCR, i.e., the type of question, it is directed to an appropriate prompt in the Prompt Template Engineering (PTE) module. Keywords in the prompt generated by the PTE module will trigger the Retrieval Augmented Generation (RAG) or Tool Invocation Generation (TIG) module to append to the content of the prompt. For example, if the query is related to a certain DB product, then to mitigate hallucination, the RAG module is triggered to retrieve trusted data and generate more accurate and relevant answers. The process can iterate for a few rounds if needed. For example, if answering the query needs to perform data analysis, the schema tool is first triggered to fetch the table structure of the database. Based on the results output by the schema tool, a selection tool is triggered to execute a SQL selection query, to compute the required data statistics in the database. Finally, the LLM is instructed by the prompt to generate the answer."}, {"title": "4.1 Pre-training and Fine-tuning", "content": "Pre-training. To enhance the model's expertise in the database domain, we first continue the pre-training of the backbone model. Specifically, we extensively collect pre-training corpora related to databases, comprising approximately 47,000 entries each in Chinese and English, totaling around 100 million tokens. This corpus includes major database textbooks, official documentation of various database products, and selected authoritative reports and articles on databases. For the preparation of our pre-training, we conduct a cleaning and deduplication process on the collected pre-training data. Subsequently, we process the data into text blocks containing 4096 tokens each, which are then fed into the backbone for continual pre-training. This training phase effectively enriches the model's knowledge in the database field and lays a solid foundation for subsequent fine-tuning.\nFine-tuning. To improve the LLM's ability to follow instructions and solve DB-specific tasks, we propose a sequential fine-tuning strategy, including three stages. We prioritize the fine-tuning sequence based on the crucial abilities in DB problem-solving. For instance, the first fine-tuning stage focuses on enhancing the LLM's NL2SQL and table understanding ability using NL2SQL data like Spider [41] because it is fundamental in DB tasks. In the second fine-tuning stage, a mixture of different fine-tuning data is adopted. The fine-tuning data includes (1) general conversational datasets like Stanford Alpaca [30] to mitigate the LLM's forgetting of general dialogue skills, and (2) reformulated questions from DQA using corresponding prompts in the PTE module to enhance the LLM's understanding of the prompt template. The last fine-tuning stage focuses on enhancing the alignment of LLM's final response with DB experts in terms of quality and format, by using answer cases written by DB experts in Section 3. The specific settings will be detailed in Section 6."}, {"title": "4.2 Question Classification Routing", "content": "The Question Classification Routing (QCR) module is designed to automatically categorize user queries and route them to different customized prompt templates. We categorize the questions into 5 categories: general database-related inquiries, product-specific inquiries, instance-specific inquiries, unsafe inquiries, and other inquiries not related to the aforementioned categories. In this paper, we implement three methods of QCR modules.\n(1) LLM-based Classification Method. We use a prompt, which is designed to elicit a classification response from GPT-4.\n(2) Classifier. We train an XLNet-based [39] classifier. We construct the training data where each question is labeled as \u201cunsafe\", \"safe but irrelevant\", \"DB general\", \"product-specific\", or \"instance-specific\". The positive samples for the \"unsafe\" category are collected from Safety-Prompts [28] and BeaverTails-Evaluation [14]. The \"safe but irrelevant\" samples are collected from Alpaca [30] and Longbench [3]. The rest three categories are from the training set of DQA (which does not overlap with the test set used in Section 6).\n(3) Hierarchical Classifier. Training a single function to predict all possible labels is more difficult. Furthermore, a \"flat\" classifier method requires a balanced amount of training queries for each class. Alternatively, we train a hierarchical classifier, which first classifies safe and unsafe questions and then classifies the safe questions into four sub-classes. We use an independent XLNet-based [39] classifier at each level."}, {"title": "4.3 Prompt Template Engineering", "content": "The Prompt Template Engineering (PTE) module contains customized prompt templates for various categories of queries. The template comprises with slot expressed as \u201c{{}}\u201d that can be appended. Keywords in the prompt templates can activate other modules to append the slot. For instance. Specifically, the used templates are as follows:\n(1) General Knowledge Q&A. Defined as follows:\nYou are an expert in the field of general database issues, which do not involve specific database instances. Do not allow any fabrications to be added to the answer. Please provide a specific and detailed response, for example, including the exact commands or code needed by the user.\nQuestion:{{Q}}\nHere, the slot {{Q}} is to be filled by the query content.\n(2) Product-specific Q&A. Defined as follows:\nYou are an expert in the field of database issues, which are related to specific databases. Answer questions in a concise and professional manner based on the information in \"Knowledge\", which is from database documents. Do not allow any fabrications to be added to the answer.\nQuestion: {{Q}}; Knowledge:{{K}}\nHere, the keyword \"Knowledge\" can trigger the RAG module, and the retrieved text block will be appended to the slot {{K}}.\n(3) Instance-specific Q&A. Defined as follows:\nYou are an expert in the specific database instance and capable of using tools to extract information from that database. Your responses should draw on your expertise in the database field and provide specific solutions.\nThe tools you can use: {{T}}\nUse the following format:\nQuestion: ...; Thought: ...; Action: ...; Action_Input: ...; Observation: ...; ...; Final_Answer: ...\nQuestion: {{Q}}\n{{Agent_Scratchpad}}\nHere, the keyword \"using tools\" can trigger the TIG module. The content after \"Action:\" and \"Action_Input:\" will be used to invocate the appropriate tools correctly, and the tool's output will be extracted, summarized and appended after \"Observation:\". More details are described in Section 4.5\n(4) DB-irrelevant Q&A. We will require the system to refuse to answer all questions unrelated to databases. Defined as follows:\nYou are a friendly, LLM-based database Q&A system, and you can only answer questions related to databases. When users ask questions unrelated to databases, please kindly refuse to answer and explain the reason.\nQuestion: {{Q}}"}, {"title": "4.4 Retrieval Augment Generation", "content": "The Retrieval Augment Generation (RAG) module is used to extract additional external knowledge from documents to enhance LLMs.\nAs shown in Figure 4, the module first segments texts from the knowledge base into independent text blocks. Each text block is then processed through an embedding model to be transformed into a dense vector and stored in a vector database, establishing mappings between texts and vectors. Similarly, when a user submits a query, it is also transformed into a vector using the same embedding model, which is then matched against the vectors of the text blocks in the database based on similarity computation. The system obtains the most relevant text block vector, appends it to the prompt and feeds the prompt to the core LLM module. The LLM then generates precise and relevant responses to the user's query based on the knowledge.\nWe adopt Langchain-Chatchat 10 to implement a vector storage and retrieval-based RAG module. We use the Faiss [?] vector database to efficiently manage and retrieve large-scale embedding vectors, using Bge-large [36] as the embedding model to map text into dense vectors."}, {"title": "4.5 Tool Invocation Generation", "content": "The Tool Invocation Generation (TIG) module is designed to extract context information regarding the database instance to tailor a customized answer.\nWe first implement a Chain of Thought (COT) prompt template following ReAct [40]. This prompt template encourages the LLM to think in a loop according to the following chain of thought: (1) \"Thought\": Think and reason based on the currently available information to determine the tools that need to be invoked. (2) \"Action\" and \"Action Input\": Provide the name of the tool to be invoked and its input in the right format. (3) \"Observation\": The tool will provide the results of the invocation.\nAs shown in Figure 4, the LLM initially outputs the COT containing the first tool it wants to invoke and the input for the tool. The tool trigger reads the output and interrupts the LLM from continuing its output. Simultaneously, the TIG module matches the tool's name (content after \"Action:\") in the tool pool, which comprises the common tools listed in Table 3. If it finds the appropriate tool, it invokes the corresponding tool interface based on the content after \"Action_Input:\".\nWhen interacting with the database instance, the tool will output some results. The output will be organized into text form and appended to the LLM's output after \"Observation:\". We optimize tool outputs to boost efficiency and accuracy. Enhancements involve (1) filtering highly relevant content from extensive outputs and (2) transforming structured data, like tables, into Markdown format for streamlined processing.\nAfter the tool execution, the TIG module will end the interruption, and let the LLM continue thinking based on the results of \"Observation\" to determine if additional tools need to be called. If more tools are needed, the process will iterate. Otherwise, if the LLM determines there is sufficient information to solve the problem, it will directly output the final answer."}, {"title": "5 DQA Evaluation Pipeline", "content": "We make two lines of effort toward building a thorough, precise and impartial evaluation of question-answering databases (DBs): (1) modularized evaluation protocols and metrics, which rigorously assess intermediate stages such as DB document retrieval, tool selection and tool utilization; and (2) a standardized end-to-end evaluation that ensures stability and control."}, {"title": "5.1 Modularized Evaluation", "content": "QCR Evaluation. Accuracy is the most critical metric when evaluating the QCR module. If the questions are not accurately categorized, the LLM can not provide customized responses. Given the ground truth class labels mentioned in Section 4.2, the accuracy is defined as $Acc = \\sum_i m_{i,i}/\\sum_i (\\sum_j m_{i,j})$, where $m_{i,j}$ is the number of queries that belong to ground truth class i that are classified as class j. We will also report the F1 result of each category i in the experiment as $F1(i) = 2(precision(i)\\cdot recall(i))/(precision(i)+recall(i))$, where $precision(i) = m_{i,i}/\\sum_j m_{i,j}$ and $recall(i) = m_{i,i}/\\sum_j m_{j,i}$.\nRAG Evaluation. Firstly, the performance of RAG can be measured by how precise the retrieval is, i.e., whether the retrieval text is relevant to the question. Thus, we can use the P@n metric. In particular, given that the relevance of external knowledge is crucial to the answer quality, we consider only the top three retrieval results. The precision of the RAG module is defined as $P@3 = \\sum_{i \\le 3} I\\{r_{i,j}\\}/(3 \\sum_j)$, where $r_{i, j}$ is the $i\u2212th$ ranked text block in the top-3 retrieval results of query j, and $I\\{\\}$ is an indicator function that returns whether the text block is relevant as labeled in the retrieval annotation in Section 3, $\\sum_j$ means the RAG module's performance is averaged over all queries.\nSecondly, the RAG module in the testbed is optional. The LLM can properly function without RAG or with incorrect RAG results. Therefore, we can also evaluate the performance of the RAG module by comparing the end-to-end performance with and without RAG.\nTIG Evaluation. The TIG module enhances the LLM by calling appropriate database tools and utilizing the interaction results between the tools and database instances. Two core dimensions need to be measured: (1) Can TIG select the correct set and order of tools for the query? (2) Can TIG correctly trigger the tool, i.e., does the tool invocation adhere to the prompt and tool format requirements? Therefore, we propose TSA (Tool Selection Accuracy) and TFA (Tool Format Accuracy) to measure these two dimensions respectively.\n(1) TSA: This metric measures whether the correct tools are chosen to solve problems. It is important to consider the order of actions to measure the DB-specific planning ability. For example, the input of the succeeding tool is usually based on and formulated from the preceding tool's output. Therefore, if there is an error in the current tool invocation, the subsequent tool invocations will no longer be included in the metric calculation. Specifically, the TSA (Tool Selection Accuracy) is defined as:\n$TSA = \\frac{\\sum_{i \\geq 1,j} I\\{t_{ij}\\} / k_j}{\\sum_{i \\leq \\min { k_j,I\\{t_{kj,j}\\} = 0}}\\}$,(1)\nwhere $t_{ij}$ is i-th tool in COT for the query j, $I\\{\\}$ is an indicator function that returns whether the tool (the name after \"Action\") is labeled in the tool annotation in Section 3, and $k_j$ means the number of LLM tool invocations.\n(2) TFA: This metric measures the accuracy of the tool invocation format. Due to the diversity and subjectivity of tool format requirements, particularly in the generalized tool section Q&A, it is very challenging to assess format compliance using predefined rules. Therefore, we employ GPT-4 as an expert adjudicator model to judge whether tool invocations meet the format requirements. Similarly, we consider the order of tools. Specifically, the TFA (Tool Format Accuracy) is defined as:\n$TFA = \\frac{\\sum_{i \\geq 1,j} G\\{t_{ij}\\} / k_j}{\\sum_{i \\leq \\min { k_j,G\\{t_{kj,j}\\} = 0}}\\}$,(2)\nwhere $t_{ij}$ is the i\u2212th tool in COT for the query j, G{} is the output of GPT-4 that decides whether the tool input (the content after \"Action_Input\") is correct, and $k_j$ means the number of LLM tool invocations."}, {"title": "5.2 End-to-End Evaluation", "content": "LLMs are sensitive to prompt engineering. The"}]}