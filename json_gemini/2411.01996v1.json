{"title": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task", "authors": ["Hoonick Lee", "Mogan Gim", "Donghyeon Park", "Donghee Choi", "Jaewoo Kang"], "abstract": "The advent of Large Language Models (LLMs) have shown promise in various creative domains, including culinary arts. However, many LLMs still struggle to deliver the desired level of culinary creativity, especially when tasked with adapting recipes to meet specific cultural requirements. This study focuses on cuisine transfer-applying elements of one cuisine to another-to assess LLMs' culinary creativity. We employ a diverse set of LLMs to generate and evaluate culturally adapted recipes, comparing their evaluations against LLM and human judgments. We introduce the ASH (authenticity, sensitivity harmony) benchmark to evaluate LLMs' recipe generation abilities in the cuisine transfer task, assessing their cultural accuracy and creativity in the culinary domain. Our findings reveal crucial insights into both generative and evaluative capabilities of LLMs in the culinary domain, highlighting strengths and limitations in understanding and applying cultural nuances in recipe creation. The code and dataset used in this project is openly available in https://github.com/dmis-lab/CulinaryASH/.", "sections": [{"title": "Introduction", "content": "Culinarians around the globe are striving to innovate new recipe ideas. Such creative process involves exploring novel ingredient pairings (Park et al., 2019), combinations (Gim et al., 2022) or adjusting ingredient proportions (Choi et al., 2023). Meanwhile, cuisine is a cultural embodiment of culinary arts that can be associated with a specific region, religion, or history (Kocevski and Risteski, 2020; Nguyen et al., 2022, 2023; Palta and Rudinger, 2023). It defines the style of handling ingredients to create dishes typically recorded as recipes. As the culinary world is becoming increasingly culturally interconnected, cuisine transfer, which involves applying the elements of the source cuisine to base recipe of target cuisine, has become a well-established practice for creating new recipe possibilities (Shin et al., 2024; Markham et al., 2023). \nAs cuisine transfer can be treated as a sub-task of textual recipe generation, we avert our focus towards the recent technical advancements in generative large language models (LLMs) (Team et al., 2024; Touvron et al., 2023; Dubey et al., 2024; Jiang et al., 2023; OpenAI, 2024). Conventional LLMs possess the ability to understand users' instruction-based prompts and generate desirable responses based on their massively parameterized knowledge (Ouyang et al., 2022; Chung et al., 2024; Rafailov et al., 2024). They are also capable of evaluating LLM-generated texts which helps relieve the burden of manual evaluation by humans (Min et al., 2023). These advancements have led to the advent of automatic evaluation frameworks which are now a well-established research topic. \nDespite these circumstances, there have been seldom previous works that attempted to develop a LLM-driven evaluation framework especially in the culinary domain. This is mainly due to cooking being more related to creativity rather than factuality where the latter has been widely explored by previous works (Jeong et al., 2024; Bishop et al., 2024; Tang et al., 2024; Chiang and Lee, 2024; Zheng et al., 2023; Manakul et al., 2023). Evaluating generated recipes based on cooking creativity is a non-trivial task (Choi et al., 2024; Yagcioglu et al., 2018; Liu et al., 2020; H. Lee et al., 2020; Chandu et al., 2019; Le et al., 2023; Antognini et al., 2023) which presents a novel challenge when trying to employ automatic evaluation approaches in the culinary domain. It is also important to note that almost none of the automatic frameworks have been deployed in creativity-oriented generation tasks. \nTo address this issue, we designed an automatic"}, {"title": "Recipe Generation with Cuisine Transfer", "content": "In the ASH benchmark, six open-source LLMs were each assigned 800 instructions for the cuisine transfer task, generating a total of 4,800 recipes. Details can be found in Supplementary Section C."}, {"title": "Evaluation of Generated Recipes", "content": "ASH benchmark introduces the following evaluation criteria that measures a LLM's recipe generation ability in cuisine transfer tasks.\n\u2022 authenticity: This criterion assesses how well the generated recipe maintains the essence of its original base dish.\n\u2022 sensitivity: This criterion assesses how well the generated recipe reflects the culinary elements transferred from the target cuisine.\n\u2022 harmony: This criterion not only assesses the overall quality of the generated recipe but also balance between authenticity and sensitivity\nThe details of evaluators are described in Supplementary section E."}, {"title": "Results", "content": null}, {"title": "ASH Benchmark", "content": null}, {"title": "Creation of Cuisine Transfer Instructions", "content": "To evaluate generative language models' ability to adapt to diverse cuisine styles, we created 800 standardized cuisine transfer instructions based on 20 base dishes and 40 cuisines, expanding on prior work by incorporating religious and historical cuisines (e.g., Buddhist, Aztec) and achieving balanced continental representation. Table 1 provides a summary of the cuisines and dishes used, with each prompt requesting a recipe that includes ingredients and step-by-step instructions. Details can be found in Supplementary Section B"}, {"title": "Overall analysis of ratings assigned to LLM-generated recipes", "content": "We generated heatmaps using the ratings of all evaluation criteria made by the LLM evaluators (Figure 1). While the distribution of ratings assigned to the six recipe generators were relatively even, we observed several differences across the recipe evaluators. In authenticity , both gemini variants assigned relatively low ratings while 11ama2:13b was inclined towards all generators with the lowest deviance of its ratings. The ratings were harsher in harmony where gemma2:2b, gemma2:9b and gemini-1.5-flash were generally more negative compared to others. In contrast, other evaluators displayed more leniency.\nThe overall distribution of ratings for sensitivity is way higher than the two evaluation criteria with 4.68 being the highest average rating assigned by llama3.1:8b to its identical model and 3.19 being the lowest assigned by gemini-1.5-flash to llama2:13b. We hypothesize that high sensitivity with relatively low authenticity and harmony means both the recipe generator and evaluator possess limited culinary knowledge. The generator likely focuses heavily on incorporating specific culinary elements without considering the broader cooking context. Similarly, the evaluator may be overly focused on the presence of these elements in the text, rather than evaluating the overall coherence of the recipe."}, {"title": "Cuisine-specific analysis of ratings assigned to LLM-generated recipes", "content": "We investigated which cuisines have the highest and lowest ratings across all generator-evaluator LLM pairs for each evaluation criterion in the ASH benchmark experiment. Table 2 shows the top and bottom ranked cuisines for each criterion. Interestingly, recipes generated through Ethiopian cuisine transfer received the lowest ratings only in authenticity and harmony while the sensitivity ratings are relatively ranked higher.\nFor deeper investigation, we generated heatmaps (Figure 2 using the sensitivity ratings provided by the recipe evaluators. We additionally examined the top frequently used words (excluding stop-words and cuisine names) for each generator model as well.\nIn Figure 2a, all recipe generators received highly positive sensitivity ratings from almost all evaluators for the Kosher cuisine transfer task. As shown in Table 2, Kosher cuisine remarkably achieved the highest ratings in all three criteria. In fact, one of the mostly used words in the generated recipes is certified which may implicate Kosher-certified food ingredients. After examining the generated recipes and evaluation rationales, we discovered that the generators' focus tended to be polarized towards certified. Conclusively, the strict emphasis on this aspect appeared to drive more favorable ratings from the evaluators.\nAccording to Figure 2b, one of the most frequently used words is berbere, a spice mixture popularly used in Ethopian dishes. While this ingredient played a pivotal role in attracting positive sensitivity ratings, we speculate that its combination with the base dishes did not seem to help the generator earn good authenticity and harmony ratings from the evaluators which may be due to the unexplored culinary knowledge space related to Ethiopian cuisine in conventional LLMs. In Figure 2c, recipe generators mistral:7b and llama2:13b obtained relatively negative rat-"}, {"title": "Inter-agreement analysis of recipe evaluators", "content": "We performed two inter-agreement analysis of recipe evaluators which are LLM-LLM and LLM-human annotators. Figure 4 shows the LLM-LLM inter-agreement scores averaged across the evaluation criteria. While the gemini and llama variants seem to have discrepancy to each other, LLMs from the same family tend to resemble each other in terms of their agreement scores. As shown in 3, we observed that 11ama2:13b tended to show the least rating differences in all evaluation criteria (0.503, 0.550, 0.460). Considering that the human annotators are not culinary experts, we speculate that evaluators with limited culinary knowledge may possess narrower recipe evaluation standards, often assigning more positive ratings, a trend also observed in the human evaluations."}, {"title": "Conclusion", "content": "We performed an thorough investigation on LLMs' generative behavior under a specific culinary domain task called cuisine transfer. We developed a novel benchmark ASH that addresses the important aspects of cuisine transfer when evaluating the generated recipes using authenticity sensitivity and harmony We analyzed the evaluation results yielded by our designed automatic evaluation framework and derived interesting findings that hints the current state of culinary knowledge in LLMs for future research in this creativity domain."}, {"title": "Ethical Statement", "content": "Our study examines the ability of large language models (LLMs) to generate culturally sensitive recipe adaptations through cuisine transfer, which involves incorporating elements from one cuisine into another. We are aware that LLMs may sometimes produce outputs that could inadvertently misrepresent or oversimplify cultural practices. To mitigate this, we have implemented evaluation criteria that prioritize authenticity, sensitivity, and harmony, aiming to ensure that generated recipes are respectful and reflective of the intended cultural elements. However, we acknowledge that despite our efforts, the model may still generate content that could be viewed as controversial or culturally insensitive, depending on individual perspectives.\nAdditionally, we recognize the importance of human input in evaluating cultural nuances, so we involved a diverse group of human annotators from various cultural backgrounds. While the range of human raters was limited, we strived for as much diversity as possible to provide balanced perspectives. All data used in this study, including recipes and prompts, were sourced from publicly available datasets. This research does not involve any personal or sensitive data, and all annotations were conducted by adults who consented to participate in the evaluation."}, {"title": "Limitations", "content": "Our study has several limitations, primarily related to the constraints of using LLMs for nuanced cultural tasks. First, while we calculated agreement scores to assess the alignment between LLM and human evaluations, traditional Kappa scores are not ideal for continuous score variations. Using regression-based approaches might yield more accurate measures of evaluator agreement, and further research could explore these alternatives.\nThe results may also be vulnerable to variations in prompt setup and evaluation format. Prompt design significantly affects model output, and subtle differences in phrasing can lead to diverse interpretations by the LLMs. Similarly, the rating scale we used may oversimplify complex culinary judgments, potentially limiting the depth of assessment for each criterion. Future work could involve refining prompts and evaluation metrics for more nuanced feedback.\nThe study is additionally limited by the dependency on specific models, particularly proprietary models like GPT-4, which may introduce biases based on their training data and inaccessible architectural details. These model-specific factors mean that results may vary with other LLMs or future iterations of the same models.\nFinally, our use of human annotators, while culturally diverse, is not exhaustive. The sample size and diversity of annotators could be expanded in future studies to encompass a wider range of cultural perspectives. This would further enrich our understanding of cultural sensitivities and improve the robustness of our evaluation framework."}, {"title": "Potential Risks", "content": "While our dataset is designed to evaluate culturally sensitive recipe generation, if it is used to further train or fine-tune language models, it may lead to unintended behaviors. This is due to the inherent variability in generated text, as well as potential limitations in the suitability of human or machine evaluations for certain cultural contexts.\nDespite our efforts to ensure dataset quality, the limited number and cultural diversity of evaluators may mean our dataset does not comprehensively represent all perspectives within the cultural and culinary domains. This could restrict its effectiveness in capturing the full breadth of cultural nuance necessary for such applications."}, {"title": "Experiment setup", "content": "For each generator-evaluator pair in each evaluation criterion (authenticity, sensitivity, harmony), we calculated the mean and standard deviation of the 4,000 ratings across all generated recipes which underwent cuisine transfer."}, {"title": "Creation of Cuisine Transfer Instructions Details", "content": "We first created cuisine transfer instructions to assess a generative language model's ability to understand and adapt to other cuisine styles. Inspired from Shin et al., we selected 20 base food dishes used in their work.\nAdditionally, we incorporated the 20 cuisines used in their study, primarily categorized by region. As our benchmark aims to provide a comprehensive measure of an LLM's culinary awareness from a broader cultural perspective, we expanded the set to include religious and historical cuisines such as Buddhist and Aztec cuisines. Also, as regional cuisine can be clustered into continental categories, we added regional cuisines to achieve a balance across different continents.\nTable 1 presents a truncated list of cuisines and base food dishes used in the benchmark. With the total number of cuisines and base food dishes being 40 and 20, we created 800 cuisine transfer instructions. Each prompt follows a standardized format specifying the base dish to be applied with a specific cuisine transfer and asking the model to generate a textual recipe comprising ingredients and instruction steps."}, {"title": "Recipe Generation Details", "content": "In our proposed benchmark, 6 open-source LLMs (gemma2:2b (Team et al., 2024), gemma2:9b (Team et al., 2024), 11ama2:13b (Touvron et al., 2023), llama3.1:8b (Dubey et al., 2024), mistral:7b (Jiang et al., 2023), gpt-4o-mini (OpenAI, 2024)) were selected to perform the cuisine transfer task, each given with 800 instructions. The selection of models were based on their model size, as well as their popularity displayed in the Ollama\u00b9 models catalog"}, {"title": "Recipe Evaluation Details", "content": "All LLM recipe evaluators were prompted with the same instruction format which is shown in Figure 5. Additionally, a group of five human annotators from diverse backgrounds (USA, South Korea, South Korea, Uganda, and Russia) were asked to evaluate a randomly selected set of 200 recipes, stratified by cuisine. This process yielded an additional 1,000 human-annotated ratings, bringing the total number of evaluation results to 130, 600."}, {"title": "Evaluator Details", "content": "The models used for evaluating the recipes (recipe evaluators) are not only the open source LLMs previously used in the cuisine transfer task,"}]}