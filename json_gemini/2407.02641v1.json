{"title": "Learning Graph Structures and Uncertainty for Accurate and Calibrated Time-series Forecasting", "authors": ["Harshavardhan Kamarthi", "Lingkai Kong", "Alexander Rodr\u00edguez", "Chao Zhang", "B. Aditya Prakash"], "abstract": "Multi-variate time series forecasting is an important problem with a wide range of applications. Recent works model the relations between time-series as graphs and have shown that propagating information over the relation graph can improve time series forecast-ing. However, in many cases, relational information is not available or is noisy and reliable. Moreover, most works ignore the under-lying uncertainty of time-series both for structure learning and deriving the forecasts resulting in the structure not capturing the uncertainty resulting in forecast distributions with poor uncertainty estimates. We tackle this challenge and introduce STOIC, that lever-ages stochastic correlations between time-series to learn underlying structure between time-series and to provide well-calibrated and accurate forecasts. Over a wide-range of benchmark datasets STOIC provides around 16% more accurate and 14% better-calibrated fore-casts. STOIC also shows better adaptation to noise in data during inference and captures important and useful relational information in various benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "While there has been a lot of work on modeling and forecasting univariate time-series [7], the problem of multivariate time-series forecasting is more challenging. This is because modeling indi-vidual signals independently may not be sufficient to capture the underlying relationships between the signals which are essential for strong predictive performance. Therefore, many multivariate models model sparse correlations between signals based on prior knowledge of underlying structure using Convolutional networks [20] or Graph Neural networks [15, 35, 36].\nHowever, in many real-world applications, the graph structure is not available or is unreliable. In such cases, the problem of learning underlying patterns [5, 24, 30] is an active area of research [37] in applications such as traffic prediction and energy forecasting. Most methods use a joint learning approach to train the parameters of both graph inference and forecasting modules.\nHowever, most previous works focus only on point forecasting and do not leverage uncertainty when modeling the structure. Sys-tematically modeling this uncertainty into the modeling pipeline can help the model adapt to unseen patterns such as when modeling a novel pandemic [26]. Therefore, the learned structure from exist-ing models may not be adapted to noise in data or to distributional shifts commonly encountered in real-world datasets.\nIn this paper, we tackle the challenge of leveraging structure learning to provide accurate and calibrated probabilistic forecasts for all signals of a multivariate time-series. We introduce a novel probabilistic neural multivariate time-series model, STOIC (Stochastic Graph Inference for Calibrated Forecasting), that leverages functional neural process framework [22] to model uncertainty in tem-poral patterns of individual time-series as well as a joint structure learning module that leverages both pair-wise similarities of time-series and their uncertainty to model the graph distribution of the underlying structure. STOIC then leverages the distribution of learned structure to provide accurate and calibrated forecast distributions for all the time-series.\nOur contributions can be summarized as follows: (1) Deep prob-abilistic multivariate forecasting model using Joint Structure learning: We propose a Neural Process based probabilistic deep learning model that captures complex temporal and structural cor-relations and uncertainty over multivariate time-series. (2) State-of-art accuracy and calibration in multivariate forecasting: We evaluate STOIC against previous state-of-art models in a wide range of benchmarks and observe 16.5% more accurate and 14.7% better calibration performance. We also show that STOIC is signifi-cantly better adapted to provide consistent performance with the injection of varying degrees of noise into datasets due to modeling uncertainty. (3) Mining useful structural patterns: We provide multiple case studies to show that STOIC identifies useful domain-specific patterns based on the graphs learned such as modeling relations between stocks of the same sectors, location proximity in traffic sensors, and epidemic forecasting."}, {"title": "2 METHODOLOGY", "content": "Problem Formulation. Consider a multi-variate time-series dataset\nN\ni=1\nof N time-series D = {yi}\u2081 over T time-steps. Let yi \u2208 RT\ndenote time-series i and y(t) be the value at time t. Further, let\ny(t) \u2208 RN be the vector of all time-series values at time t. Given\ntime-series values from till current time t as y(1:t), the goal of\nprobabilistic multivariate forecasting is to train a model M that\nprovides a forecast distribution:\nPM (y(t+1:t+t)\\y (1:1); \u03b8),\n(1)\nwhich should be accurate, i.e, has mean close to ground truth as well as calibrated, i.e., the confidence intervals of the forecasts precisely mimic actual empirical probabilities [4, 10].\nFormally, the goal of joint-structure learning for probabilistic forecasting is to learn a global graph G from y(1:t) and leverage it to provide accurate and well-calibrated forecast distributions:\nPM (y(t+1:t+t) \\G, y (1:1); \u03b8) \u221d PM (G|y(1:1); \u03b8).\n(2)\nOverview. STOIC models stochasticity and uncertainty of time-series when generating structural relations across time-series. It also adap-tively leverages relations and uncertainty from past data using the functional process framework [10, 22]. STOIC's generative process can be summarized as: (1) The input time-series values are encoded using a Probabilistic Time-series Encoder (PTE) that models a multi-variate Gaussian Distribution to model each time-series capturing both time-series patterns and inherent uncertainty. (2) The similar-ity between the sampled stochastic encoding of each time-series from PTE is used to sample a graph via the Graph Generation Mod-ule (GGM). (3) Recurrent Graph Neural Encoder (RGNE) contains a series of Recurrent neural layers and Graph Convolutional Net-works which derive the encoding of each time-series leveraging the learned graph. (4) We also model the similarity of encodings of input time-series with past data using a reference correlation net-work (RCN). (5) Finally, STOIC uses the graph-refined embeddings and historical information from RCN to learn the parameters of the output distribution.\nProbabilistic Time-series Encoder. We first model both the infor-mation and uncertainty for each of the N time-series, by using deep sequential models to capture complex temporal patterns of the input time-series sequence \u0443(t':t). We use a GRU [2] followed by Self-attention [32] over the hidden embeddings of GRU:\n{hi}Ni=1 = {Self-Atten(GRU(y()))}Ni=1.\n(3)\nWe then model the final latent embeddings of univariate time-series as a multivariate Gaussian distribution similar to VAEs [13]:\nhi, log \u03c3hi = NNh(hi), hi \u223c N(\u00b5hi, \u03c3hi).\n(4)\nwhere NN\u0127 is a single layer of feed-forward neural network. The output latent embeddings H = {hi}Ni=1 are stochastic embeddings sampled from Gaussian parameterized by {\u00b5(hi), \u03c3(hi)}Ni=1 which captures uncertainty of temporal patterns.\nProbabilistic Graph Generation Module. Since it is computationally expensive to model all possible relations between time-series, we aim to mine sparse stochastic relations between time-series along with the uncertainty of the underlying relations. We generate a stochastic relational graph (SRG) G of N nodes that model the similarity across all time-series. We use a stochastic process to generate G leveraging stochastic latent embeddings H from PTE. We parametrize the adjacency matrix A(H), \u2208 {0, 1}N\u00d7N of G by modelling existence of each edge Ai,j as a Bernoulli distribution parameterized by bij derived as:\n\u03c3i,j = sig(NNG2 (NNG1 (hi)) + NNG1 (hj))\n(5)\nwhere NNG1 and NNG2 are feed-forward networks. We sample\nthe adjacency matrix which captures temporal uncertainty in hi, hj and relational uncertainty in \u03c3i,j.\n(Ai,j = Aj,i) \u223c Bernoulli(\u03c3i,j); \u2200i \u2264 j.\n(6)\nRecurrent Graph Neural Encoder. We combine the relational infor-mation from SRG with temporal information via a combination of Graph Neural Networks and recurrent networks:\nv(t) = GRU-Cell(u(t-1),y(t)); {u(t) }t' = GNN({v},A) (7)\nWe input hi as the initial hidden embedding for GRU-Cell at ini-tial time-step t' to impart temporal information from PTE. We\nfinally combine the intermediate embeddings {u(t)}t'=t {ui(t)}t'=t using self-attention to get Graph-refined Embeddings U = {ui}Ni=1, where:\nui = Self-Attention({ui(t) }t).\n(8)\nReference Correlation Network. This historical similarity is useful since time-series shows similar patterns to past data. Therefore, we model relations with past historical data of all N time-series of datasets. We encode the past information of all time-series into reference embeddings G = {gj}j=1:\nHgj, gj = PTE(y(1:1)), gj \u223c N(\u00b5gj, gj).\n(9)\nThen, similar to [10] we sample edges of a bipartite Reference Correlation Network S between reference embeddings G and Graph-refined Embeddings U based on their similarity as:\nk(ui, gj) = exp(\u2212y||ui \u2212 gj||\u00b2), Si,j \u223c Bernoulli(\u03ba(ui, gj)). (10)\nwhere y is a learnable parameter. To leverage the similar refer-ence embeddings sampled for each time-series i, we aggregate the sampled reference embeddings to form the RCN embeddings Z = {zi} as:\nzi \u223c N(NNz1 \u03a3 (gj), exp(\u2211 NNz2(gj)) ),\n(11)\nNj:Sij=1 Nj:Sij=1\nwhere NNz1 and NNz2 are single fully-connected layers. Therefore, the data from reference embeddings that show similar patterns to input time-series are more likely to be sampled.\nAdaptive Distribution Decoder. The decoder parameterizes the fore-cast distribution using multiple perspectives of information and uncertainty from previous modules: Graph-refined embeddings U, RCN embeddings Z and a global embedding of all reference embeddings g derived as: g = Self-Attention({gj}j). However, information from each of the modules may have varied importance based. Therefore, we use a weighted aggregation of these embeddings: ki = l1ui + l2zi + l3g to get the input embedding ki for the decoder where {li}i=1 are learnable parameters. The final output forecast distribution is derived as:\nYi(t+1) \u223c N(NNy1(ki), exp(NNy2(ki)))\n(12)"}, {"title": "3 EXPERIMENT SETUP", "content": "Baselines. We compare with general state-of-art forecasting mod-els that include (1) statistical models like ARIMA [7], (2) general forecasting models: DEEPAR [28], EPIFNP [10] (3) Graph-learning based forecasting models: MTGNN [33], GDN [3], GTS [30], NRI [14]. Note that we are performing probabilistic forecasting while most of the baselines are modeled for point forecasts. Therefore, for methods like ARIMA, DEEPAR, MTGNN and GDN, we leverage an ensemble of ten independently trained models to derive the forecast distribution [18].\nDatasets. We evaluate our models against eight multivariate time-series datasets from a wide range of applications that have been used in past works. The main statistics of the datasets are summarized in Table 1. (1) Flu-Symptoms: We forecast a symptomatic measure of flu incidence rate based on wILI (weighted Influenza-like illness outpatients) that are provided by CDC for each of the 50 US states. We train on seasons from 2010 to 2015 and evaluate on seasons from 2015 to 2020. (2) Covid-19: We forecast the weekly incidence of Covid-19 mortality from June 2020 to March 2021 for each of the 50 US states [11, 26] using incidence data starting from April 2020. (3) Similar to [24], we use the daily closing prices for stocks of companies in S&P 100 using the yfinance package [1] from July 2014 to June 2019. The last 400 trading days are used for testing. (4) Electricity: We use a popular multivariate time-series dataset for power consumption forecasting used in past works [37]. We forecast power consumption for 15-60 minutes. We train for 1 year and test on data from the next year. (5) Traffic prediction: We use 2 datasets related to traffic speed prediction. METR-LA and PEMS-BAYS [21] are datasets of traffic speed at various spots in Los Angeles and San Francisco. We use the last 10% of the time-series for testing. (6) Transit demand: NYC-Bike and NYC-Taxi [34] measure bike sharing and taxi demand respectively in New York from April to June 2016."}, {"title": "4 CONCLUSION", "content": "We introduced STOIC, a probabilistic multivariate forecasting model that performs joint structure learning and forecasting leveraging un-certainty in time-series data to provide accurate and well-calibrated forecasts. We observed that STOIC performs 8.5% better in accu-racy and 14.7% better in calibration over multivariate time-series benchmarks from various domains. Due to structure learning and uncertainty modeling, we also observed that STOIC better adapts to the injection of varying degrees of noise to data during infer-ence with STOIC's performance drop being almost half of the other state-of-art baselines. Finally, we observed that STOIC identifies useful patterns across time-series based on inferred graphs such as the correlation of stock from similar sectors, location proximity of traffic sensors and geographical adjacency and mobility patterns across US states for epidemic forecasting. While our work focuses on time-series with real values modeled as Gaussian distribution, our method can be extended to other distributions modeling dif-ferent kinds of continuous signals. Further, STOIC only models a single global structure across time-series similar to most previous works. Therefore, extending our work to learn dynamic graphs that can adapt to changing underlying time-series relations or model multiple temporal scales could be an important direction for future research."}, {"title": "A RELATED WORK", "content": "Multivariate forecasting using domain-based structural data. Deep neural networks have achieved great success in probabilistic time series forecasting. DEEPAR [28] trains an auto-regressive recurrent network to predict the parameters of the forecast distributions. Other works including deep Markov models [16] and deep state space models [19, 25] explicitly model the transition and emission components with neural networks. Recently, EPIFNP [10] leverages functional neural processes and achieves state-of-art performance in epidemic forecasting. However, all these methods treat each time series individually and suffer from limited forecasting performance. Leveraging the relation structure among time-series to improve forecasting performance is an emerging area. GCRN [29], DCRNN [21], STGCN [35] and T-GCN [36] adopt graph neural networks to capture the relationships among time series and provide better representations for each individual sequence. However, these meth-ods all assume that the ground-truth graph structure is available in advance, which is often unknown in many real world applications.\nStructure learning for time-series forecasting. When the underlying structure is unknown, we need to jointly perform graph structure learning and time-series forecasting. MTGNN [33] and GDN [3] parameterize the graph as a degree-k graph to promote sparsity but their training can be difficult since the top-K operation is not differentiable. GTS [30] uses the Gumbel-softmax trick [8] for differ-entiable structure learning and uses prior knowledge to regularize the graph structure. The graph learned by GTS is a global graph shared by all the time series. Therefore, it is not flexible since it cannot adjust the graph structure for changing inputs at inference time. NRI [14] employs a variational auto-encoder architecture and can produce different structures for different encoding inputs. It is more flexible than GTS but needs more memory to store the indi-vidual graphs. However, as previously discussed, these works do not model the uncertainty of time-series during structure learning and forecasting and do not focus on the calibration of their forecast distribution."}, {"title": "B TRAINING DETAILS", "content": "The architecture of PTE and RCN is similar to [10] with GRU being bi-directional and having 60 hidden units. NN\u0127, NNG1, NNG2, NNz1 and NNz2 and GRU of RGNE also have 60 hidden units. Therefore, Graph-refined embeddings U, RCN embeddings Z and global em-bedding g are 60 dimensional vectors.\nWe used Adam optimizer [12] with a learning rate of 0.001. We found that using a batch size of 64 or 128 provided good perfor-mance with stable training. We used early stopping with patience of 200 epochs to prevent overfitting. For each of the 20 independent runs, we initialized the random seeds of all packages to 1-20. In general, variance in performance across different random seeds was not significant for all models."}, {"title": "C\nABLATION STUDIES", "content": "We evaluate the efficacy of various modeling choices of STOIC. Specifically, we access the influence of graph generation, RCN, and weighted aggregation (Equation 2) via the following ablation variants of STOIC:\n\u2022 STOIC-NOGRAPH: We remove the GGM and GCN modules of RGNE and therefore do not use Graph-refined embed-dings U in the decoder.\n\u2022 STOIC-NOREFCORR: We remove the RCN module and do not use RCN embeddings Z in the decoder.\n\u2022 STOIC-NOWTAGG: We replace weighted aggregation with concatenation in Equation 2 as\nki = ui \u2295 Zi(13)\nwhere \u2295 is the concatenation operator.\nForecasting performance. As shown in Table 3, STOIC on average outperforms all the ablation variants with 8.5% better accuracy and 3.2% better CS. On average, the worst performing variant is STOIC-NOGRAPH followed by STOIC-NOREFCORR and STOIC-NOWTAGG, showing the importance of graph generation and leveraging learned relations across time-series.\nRobustness to noise. Similar to Section 3, we also test the abla-tion variants' robustness in the NFI task with performance de-crease compared in Figure 3. Performance continuously decreases with an increase in p. STOIC-NOGRAPH is most susceptible to a decrease in performance with a 22-41% decrease in performance at p = 0.2 which again shows the importance of structure to ro-bustness. STOIC-NOREFCORR and STOIC-NOWTAGG's performance degradation range from 15-32%. Finally, we observe that STOIC is again significantly more resilient to noise compared to all other variants."}, {"title": "C.1 Relations Captured by Inferred Graphs", "content": "As mentioned before, for all benchmarks there is no 'ground-truth' structure to compare against. However, in line with previous works [24, 27, 30, 33], we consider various meaningful domain-specific re-lations for the time-series of these datasets and study how well STOIC's inferred graphs capture them through following case stud-ies. Note that, for STOIC and baselines that use a sampling strategy to construct the graph (GTS, NRI, GTS, LDS), we calculate edge probability of each pair of nodes based on sampled graphs. For other graph generating baselines, we directly use the graphs inferred.\nCase Study 1: Sector-level correlations of stocks in S&P-100. Two stocks representing companies from the same sectors typically influ-ence each other more than stocks from different sectors. Therefore, we measure the correlation of edge probabilities for stocks in the same sectors. We first construct a Sector-partion graph as follows. We partition the stock time-series into sectors and construct a set of fully connected components where each component contains nodes of a sector. There are no edges across different sectors. We then measure the correlation of the edge probability matrix with the adjacency matrix of Sector-partion graph."}]}