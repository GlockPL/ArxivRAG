{"title": "Learning Graph Structures and Uncertainty for Accurate and Calibrated Time-series Forecasting", "authors": ["Harshavardhan Kamarthi", "Lingkai Kong", "Alexander Rodr\u00edguez", "Chao Zhang", "B. Aditya Prakash"], "abstract": "Multi-variate time series forecasting is an important problem with a wide range of applications. Recent works model the relations between time-series as graphs and have shown that propagating information over the relation graph can improve time series forecasting. However, in many cases, relational information is not available or is noisy and reliable. Moreover, most works ignore the underlying uncertainty of time-series both for structure learning and deriving the forecasts resulting in the structure not capturing the uncertainty resulting in forecast distributions with poor uncertainty estimates. We tackle this challenge and introduce STOIC, that leverages stochastic correlations between time-series to learn underlying structure between time-series and to provide well-calibrated and accurate forecasts. Over a wide-range of benchmark datasets STOIC provides around 16% more accurate and 14% better-calibrated forecasts. STOIC also shows better adaptation to noise in data during inference and captures important and useful relational information in various benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "While there has been a lot of work on modeling and forecasting univariate time-series [7], the problem of multivariate time-series forecasting is more challenging. This is because modeling individual signals independently may not be sufficient to capture the underlying relationships between the signals which are essential for strong predictive performance. Therefore, many multivariate models model sparse correlations between signals based on prior knowledge of underlying structure using Convolutional networks [20] or Graph Neural networks [15, 35, 36].\nHowever, in many real-world applications, the graph structure is not available or is unreliable. In such cases, the problem of learning underlying patterns [5, 24, 30] is an active area of research [37] in applications such as traffic prediction and energy forecasting. Most methods use a joint learning approach to train the parameters of both graph inference and forecasting modules.\nHowever, most previous works focus only on point forecasting and do not leverage uncertainty when modeling the structure. Systematically modeling this uncertainty into the modeling pipeline can help the model adapt to unseen patterns such as when modeling a novel pandemic [26]. Therefore, the learned structure from existing models may not be adapted to noise in data or to distributional shifts commonly encountered in real-world datasets.\nIn this paper, we tackle the challenge of leveraging structure learning to provide accurate and calibrated probabilistic forecasts for all signals of a multivariate time-series. We introduce a novel probabilistic neural multivariate time-series model, STOIC (Stochastic Graph Inference for Calibrated Forecasting), that leverages functional neural process framework [22] to model uncertainty in temporal patterns of individual time-series as well as a joint structure learning module that leverages both pair-wise similarities of time-series and their uncertainty to model the graph distribution of the underlying structure. STOIC then leverages the distribution of learned structure to provide accurate and calibrated forecast distributions for all the time-series.\nOur contributions can be summarized as follows: (1) Deep probabilistic multivariate forecasting model using Joint Structure learning: We propose a Neural Process based probabilistic deep learning model that captures complex temporal and structural correlations and uncertainty over multivariate time-series. (2) State-of-art accuracy and calibration in multivariate forecasting: We evaluate STOIC against previous state-of-art models in a wide range of benchmarks and observe 16.5% more accurate and 14.7% better calibration performance. We also show that STOIC is significantly better adapted to provide consistent performance with the injection of varying degrees of noise into datasets due to modeling uncertainty. (3) Mining useful structural patterns: We provide multiple case studies to show that STOIC identifies useful domain-specific patterns based on the graphs learned such as modeling relations between stocks of the same sectors, location proximity in traffic sensors, and epidemic forecasting."}, {"title": "2 METHODOLOGY", "content": "Problem Formulation. Consider a multi-variate time-series dataset\nN\ni=1\nof N time-series D = {y}\u2081 over T time-steps. Let y\u1d62 \u2208 \u211d\u1d40\ndenote time-series i and y\u1d62\u207d\u1d57\u207e be the value at time t. Further, let\ny\u207d\u1d57\u207e \u2208 \u211d\u1d3a be the vector of all time-series values at time t. Given\ntime-series values from till current time t as y\u207d\u00b9\u207b\u1d57\u207e, the goal of\nprobabilistic multivariate forecasting is to train a model M that\nprovides a forecast distribution:\nPM(y\u207d\u1d57\u207a\u00b9\u207b\u1d57\u207a\u03c4\u207e|y\u207d\u00b9\u207b\u1d57\u207e; \u03b8), (1)\nwhich should be accurate, i.e, has mean close to ground truth as well\nas calibrated, i.e., the confidence intervals of the forecasts precisely\nmimic actual empirical probabilities [4, 10].\nFormally, the goal of joint-structure learning for probabilistic\nforecasting is to learn a global graph G from y\u207d\u00b9\u207b\u1d57\u207e and leverage it\nto provide accurate and well-calibrated forecast distributions:\nPM(y\u207d\u1d57\u207a\u00b9\u207b\u1d57\u207a\u03c4\u207e|G, y\u207d\u00b9\u207b\u1d57\u207e; \u03b8) PM(G|y\u207d\u00b9\u207b\u1d57\u207e; \u03b8). (2)\nOverview. STOIC models stochasticity and uncertainty of time-series\nwhen generating structural relations across time-series. It also adap-\ntively leverages relations and uncertainty from past data using the\nfunctional process framework [10, 22]. STOIC's generative process\ncan be summarized as: (1) The input time-series values are encoded\nusing a Probabilistic Time-series Encoder (PTE) that models a multi-\nvariate Gaussian Distribution to model each time-series capturing\nboth time-series patterns and inherent uncertainty. (2) The similar-\nity between the sampled stochastic encoding of each time-series\nfrom PTE is used to sample a graph via the Graph Generation Mod-\nule (GGM). (3) Recurrent Graph Neural Encoder (RGNE) contains\na series of Recurrent neural layers and Graph Convolutional Net-\nworks which derive the encoding of each time-series leveraging\nthe learned graph. (4) We also model the similarity of encodings of\ninput time-series with past data using a reference correlation net-\nwork (RCN). (5) Finally, STOIC uses the graph-refined embeddings\nand historical information from RCN to learn the parameters of the\noutput distribution.\nProbabilistic Time-series Encoder. We first model both the infor-\nmation and uncertainty for each of the N time-series, by using\na deep sequential models to capture complex temporal patterns of\nthe input time-series sequence y\u1d62\u207d\u1d57\u2032\u207b\u1d57\u207e. We use a GRU [2] followed\nby Self-attention [32] over the hidden embeddings of GRU:\n{h\u1d62}\u1d62\u1d3a\u208c\u2081 = {Self-Atten(GRU(y\u1d62\u207d\u1d57\u2032\u207b\u1d57\u207e))}\u1d62\u1d3a\u208c\u2081. (3)\nWe then model the final latent embeddings of univariate time-series\nas a multivariate Gaussian distribution similar to VAEs [13]:\nh\u1d62, log \u03c3\u2095\u1d62 = NN\u2095(h\u1d62), h\u1d62 \u223c \ud835\udca9(\u03bc\u2095\u1d62, \u03c3\u2095\u1d62). (4)\nwhere NN\u2095 is a single layer of feed-forward neural network. The\noutput latent embeddings H = {h\u1d62}\u1d62 are stochastic embeddings\nsampled from Gaussian parameterized by {\u03bc(h\u1d62), \u03c3(h\u1d62)}\u1d62. which\ncaptures uncertainty of temporal patterns.\nProbabilistic Graph Generation Module. Since it is computationally\nexpensive to model all possible relations between time-series, we\naim to mine sparse stochastic relations between time-series along\nwith the uncertainty of the underlying relations. We generate a\nstochastic relational graph (SRG) G of N nodes that model the\nsimilarity across all time-series. We use a stochastic process to\ngenerate G leveraging stochastic latent embeddings H from PTE.\nWe parametrize the adjacency matrix A(H), \u2208 {0, 1}\u1d3a\u00d7\u1d3a of G by\nmodelling existence of each edge A\u1d62,\u2c7c as a Bernoulli distribution\nparameterized by b\u1d62,\u2c7c derived as:\nb\u1d62,\u2c7c = sig(NN_G\u2082 (NN_G\u2081 (h\u1d62)) + NN_G\u2081 (h\u2c7c)) (5)\nwhere NN_G\u2081 and NN_G\u2082 are feed-forward networks. We sample\nthe adjacency matrix which captures temporal uncertainty in h\u1d62, h\u2c7c\nand relational uncertainty in b\u1d62,\u2c7c.\n(A\u1d62,\u2c7c = A\u2c7c,\u1d62) \u223c Bernoulli(b\u1d62,\u2c7c); \u2200i \u2264 j. (6)\nRecurrent Graph Neural Encoder. We combine the relational infor-\nmation from SRG with temporal information via a combination of\nGraph Neural Networks and recurrent networks:\nv\u1d62\u207d\u1d57\u207e = GRU-Cell(u\u1d62\u207d\u1d57\u207b\u00b9\u207e,y\u1d62\u207d\u1d57\u207e); {u\u1d62\u207d\u1d57\u207e}\u209c\u208c\u1d57\u2032 = GNN({v\u1d62\u207d\u1d57\u207e}\u209c\u208c\u1d57\u2032, A) (7)\nWe input h\u1d62 as the initial hidden embedding for GRU-Cell at ini-\ntial time-step t\u2032 to impart temporal information from PTE. We\nfinally combine the intermediate embeddings {u\u1d62\u207d\u1d57\u207e}\u209c\u208c\u1d57\u2032 using self-\nattention to get Graph-refined Embeddings U = {u\u1d62}, where:\nu\u1d62 = Self-Attention({u\u1d62\u207d\u1d57\u207e}\u209c). (8)\nReference Correlation Network. This historical similarity is useful\nsince time-series shows similar patterns to past data. Therefore,\nwe model relations with past historical data of all N time-series\nof datasets. We encode the past information of all time-series into\nreference embeddings G = {g\u2c7c}\u2c7c:\nH_gj, g\u2c7c = PTE(y\u2c7c\u207d\u00b9\u207b\u1d57\u207e), g\u2c7c \u223c \ud835\udca9(\u03bc_gj, \u03c3_gj). (9)\nThen, similar to [10] we sample edges of a bipartite Reference\nCorrelation Network S between reference embeddings G and Graph-\nrefined Embeddings U based on their similarity as:\nk(u\u1d62, g\u2c7c) = exp(-\u03b3||u\u1d62-g\u2c7c||\u00b2), S\u1d62,\u2c7c \u223c Bernoulli(\u03ba(u\u1d62, g\u2c7c)). (10)\nwhere \u03b3 is a learnable parameter. To leverage the similar refer-\nence embeddings sampled for each time-series i, we aggregate\nthe sampled reference embeddings to form the RCN embeddings\nZ = {z\u1d62} as:\nz\u1d62 \u223c \ud835\udca9(\\frac{NN_{Z1} (\u03a3_{j:S_{ij=1}} g_j)}{\\sum_{j:S_{ij=1}} exp(NN_{Z2}(g_j))}, (11)\nwhere NN_Z1 and NN_Z2 are single fully-connected layers. Therefore,\nthe data from reference embeddings that show similar patterns to\ninput time-series are more likely to be sampled.\nAdaptive Distribution Decoder. The decoder parameterizes the fore-\ncast distribution using multiple perspectives of information and\nuncertainty from previous modules: Graph-refined embeddings\nU, RCN embeddings Z and a global embedding of all reference\nembeddings g derived as: g = Self-Attention({g\u2c7c}\u2c7c). However, in-\nformation from each of the modules may have varied importance\nbased. Therefore, we use a weighted aggregation of these embed-\ndings: k\u1d62 = l\u2081u\u1d62 + l\u2082z\u1d62 + l\u2083g to get the input embedding k\u1d62 for the\ndecoder where {l\u1d62}\u2083\u00b9 are learnable parameters. The final output\nforecast distribution is derived as:\ny\u1d62\u207d\u1d57\u207a\u00b9\u207e \u223c \ud835\udca9(NN_{Y1}(k_i), exp(NN_{Y2}(k_i))) (12)"}, {"title": "3 EXPERIMENT SETUP", "content": "Baselines. We compare with general state-of-art forecasting mod-\nels that include (1) statistical models like ARIMA [7], (2) general\nforecasting models: DEEPAR [28], EPIFNP [10] (3) Graph-learning\nbased forecasting models: MTGNN [33], GDN [3], GTS [30], NRI\n[14]. Note that we are performing probabilistic forecasting while\nmost of the baselines are modeled for point forecasts. Therefore, for\nmethods like ARIMA, DEEPAR, MTGNN and GDN, we leverage an\nensemble of ten independently trained models to derive the forecast\ndistribution [18].\nDatasets. We evaluate our models against eight multivariate time-\nseries datasets from a wide range of applications that have been used\nin past works. The main statistics of the datasets are summarized\nin Table 1. (1) Flu-Symptoms: We forecast a symptomatic measure\nof flu incidence rate based on wILI (weighted Influenza-like illness\noutpatients) that are provided by CDC for each of the 50 US states.\nWe train on seasons from 2010 to 2015 and evaluate on seasons\nfrom 2015 to 2020. (2) Covid-19: We forecast the weekly incidence\nof Covid-19 mortality from June 2020 to March 2021 for each of\nthe 50 US states [11, 26] using incidence data starting from April\n2020. (3) Similar to [24], we use the daily closing prices for stocks of\ncompanies in S& P 100 using the yfinance package [1] from July\n2014 to June 2019. The last 400 trading days are used for testing. (4)\nElectricity: We use a popular multivariate time-series dataset\nfor power consumption forecasting used in past works [37]. We\nforecast power consumption for 15-60 minutes. We train for 1 year\nand test on data from the next year. (5) Traffic prediction: We use 2\ndatasets related to traffic speed prediction. METR-LA and PEMS-BAYS\n[21] are datasets of traffic speed at various spots in Los Angeles and\nSan Francisco. We use the last 10% of the time-series for testing. (6)\nTransit demand: NYC-Bike and NYC-Taxi [34] measure bike sharing\nand taxi demand respectively in New York from April to June 2016."}, {"title": "4 CONCLUSION", "content": "We introduced STOIC, a probabilistic multivariate forecasting model\nthat performs joint structure learning and forecasting leveraging un-\ncertainty in time-series data to provide accurate and well-calibrated\nforecasts. We observed that STOIC performs 8.5% better in accu-\nracy and 14.7% better in calibration over multivariate time-series\nbenchmarks from various domains. Due to structure learning and\nuncertainty modeling, we also observed that STOIC better adapts\nto the injection of varying degrees of noise to data during infer-\nence with STOIC's performance drop being almost half of the other\nstate-of-art baselines. Finally, we observed that STOIC identifies\nuseful patterns across time-series based on inferred graphs such as\nthe correlation of stock from similar sectors, location proximity of\ntraffic sensors and geographical adjacency and mobility patterns\nacross US states for epidemic forecasting. While our work focuses\non time-series with real values modeled as Gaussian distribution,\nour method can be extended to other distributions modeling dif-\nferent kinds of continuous signals. Further, STOIC only models a\nsingle global structure across time-series similar to most previous\nworks. Therefore, extending our work to learn dynamic graphs that\ncan adapt to changing underlying time-series relations or model\nmultiple temporal scales could be an important direction for future\nresearch."}]}