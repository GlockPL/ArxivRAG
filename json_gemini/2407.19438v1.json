{"title": "CONVERSATIONAL AI MULTI-AGENT INTEROPERABILITY", "authors": ["Diego Gosmar", "Deborah A. Dahl", "Emmett Coin"], "abstract": "This paper analyses Conversational AI multi-agent interoperability frameworks and describes the novel architecture proposed by the Open Voice Interoperability initiative (Linux Foundation AI & DATA), also known briefly as OVON (Open Voice Network). The new approach is illustrated, along with the main components, delineating the key benefits and use cases for deploying standard multi-modal AI agency (or agentic AI) communications.\nBeginning with Universal APIs based on Natural Language, the framework establishes and enables interoperable interactions among diverse Conversational AI agents, including chatbots, voicebots, videobots, and human agents. Furthermore, a new Discovery specification framework is introduced, designed to efficiently look up agents providing specific services and to obtain accurate information about these services through a standard Manifest publication, accessible via an extended set of Natural Language-based APIs. The main purpose of this contribution is to significantly enhance the capabilities and scalability of AI interactions across various platforms.\nThe novel architecture for interoperable Conversational AI assistants is designed to generalize, being replicable and accessible via open repositories.", "sections": [{"title": "1 INTRODUCTION", "content": "As of this writing, the number of Chatbots and Voicebots available worldwide has reached figures above 200,000. For instance, a particular Chatbot Directory website reports[1] a total of over 183,000 bots related to Facebook Messenger and Amazon Alexa Skills alone.\nAccording to an analysis by Tidio.com[2] roughly 1.5 billion people are using chatbots. The number of chatbot users worldwide is expected to continue growing, and by 2027, chatbots are projected to become the primary customer service channel for a quarter of businesses. This dramatic proliferation and commercial importance of Chatbots and Voicebots means that it is imperative to consider the overall worldwide ecosystem of Chatbots and Voicebots in order to improve how it can be made more efficient and effective.\nFor the remainder of this document, the term \"agent\" will be used to refer to an entity with the capacity to act, while \"agency\" or \"agentic\u201d will denote the exercise or manifestation of this capacity, in accordance with the definition provided by Markus Schlosser[3].\nCurrently, Conversational AI encompasses interactions between human agents and AI agents, as well as an increasing number of interactions occurring directly among AI agents themselves. This research[4] analyses how Conversational AI interactions are becoming pervasive and, in some use cases, nearly indistinguishable from human-to-AI interactions, particularly in terms of technical user experience and channels.\nAccording to Cyberprotection Magazine[5], only 4% of web content is accessible through search engines. This implies that 96% of web content is not available to public generative AI large language models (LLMs). Consequently, AI agents will need to communicate with each other outside of public LLMs, especially when they need to escalate issues or request specific information available only from vertical and specialized AI assistants.\nAs a consequence, the proliferation of fragmented and specialized AI agents needing to communicate with each other, but without an interoperable framework, could lead to implementation complexity, duplication of effort, and ultimately user frustration. Defining interoperable standards for Conversational AI agent interactions will help mitigate these potential issues and increase scalability. For those reasons, the Open Voice Interoperability Initiative has defined a set of Universal APIs based on the Interoperable Conversation Envelope Specifications[6].\nThe rest of the paper is structured as follows. Section 2 describes earlier related work and how it differs from the work described in this paper. Section 3 describes an open source approach to intelligent assistant interoperability. Section 4 provides some state diagrams that illustrate the life cycle of an exchange among interoperable agents. We review two detailed use cases in Section 5. Section 6 describes future directions and Section 7 covers our conclusions."}, {"title": "2 PREVIOUS WORK", "content": "In this section we review other investigations into ways to enable independent conversational assistants to exchange information. There are three previous threads of work addressing the problem of collaborating independent conversa- tional assistants.\nThe first approach does not strictly involve independent assistants but independent modality components that collaborate, similarly to independent assistants, using a standard API. These modality components perform dialog functions such as speech recognition, natural language understanding, text-to-speech and so on. Examples of these kinds of architectures are the W3C Multimodal Architecture[7] and the Galaxy Communicator Software Infrastructure[8]. We mention these in order to clarify how this previous work differs from the work we describe in this paper.\nThe second approach (Agentic AI) enables independent assistants to collaborate by hard-wiring the assistants together at development time. Examples of this are AutoGen[9] and OpenDevin[10]. This approach is more flexible than develop- ing a single system that can handle all of the anticipated functions, because new functionalities can be added by adding new assistants. This is especially useful for coding development and testing, i.e., having different LLMs performing specific tasks like software development and other LLMs \"reflecting\" on the result to perform test simulations, fix issues and improve the results. However, this approach has several drawbacks. It still requires all of the collaborating assistants to be known ahead of time, it requires the developer to be familiar with all of their APIs based on specific coding instead of natural language, and it requires all of the assistants to be based on a common technology; in this case LLMs. As a result, with this approach it would be quite complex to deploy interoperable Conversational AI assistants capable of handing over voice, video, and chat interactions from humans to different levels of independent AI agents specialised in Natural conversations.\nAnother system for supporting the interaction of independent assistants is VoiceXML[11]. Very simple collaboration among voice dialog systems was made possible with the VoiceXML  element, which instructed the platform to transfer a user's call to another location, either to a phone number or, more generally, to a URI. The transfer could be either a \"bridge\" transfer that would return the call to the originating VoiceXML platform at the end, or a \"blind\" transfer which did not allow the original platform to resume the call. VoiceXML differs from the work described here in that a VoiceXML transfer requires that Agent B, the receiving agent, was either another VoiceXML application or a human agent, while this paper proposes that conversational assistants with any internal architecture should be able to collaborate as long as they adhere to the communication standards.\nIn the third approach, fewer assumptions are made about the internal structures of the assistants. For example, in the Open Agent Architecture[12], it is only necessary that the assistants adhere to the Inter Agent Communication Language (ICL); their internal architecture is not important. However, the messages sent between agents are in the form of a request for the secondary agent to do something, which requires the requesting agent to have good information about the secondary agent's capabilities. Moreover, secondary agents need to have the ability to interpret ICL, which is"}, {"title": "3 CONVERSATIONAL AI INTEROPERABILITY", "content": "The Open Voice Interoperability initiative has released the first version of the Interoperable Conversation Envelope Specification, with further updates available here[6]."}, {"title": "4 STATE DIAGRAM REPRESENTATION", "content": "Let's define a state diagram representing AI agents moving from one status to another in the environment as a visual representation that illustrates the different states an AI agent can occupy and the transitions between these states based on specific events or conditions. This type of diagram is useful for modelling the behaviour of AI agents, showing how they react to various stimuli or inputs and change their state accordingly. In particular, for conversational AI agents, a state diagram represents the different states or modes the agent can be in during a conversation with a user or another AI agent, and how it transitions between these states based on the inputs and interactions with the counterpart. This representation is useful for mapping the OVON Interoperability Specifications to the dynamic statuses and standard messages exchanged. Note that the state diagrams shown here only describe states that are relevant to inter-agent collaboration. In the course of performing the functions they're designed for, agents can be in a wide variety of internal states that are not of concern for interoperability among agents.\nThe key components of such a state diagram are as follows:\nStates: the different conditions or modes the conversational AI agent can be in during a conversation. Each state might represent a specific context or task the agent is handling.\nTransitions: the changes from one state to another, triggered by specific user inputs, agent messages, or internal"}, {"title": "4.1 Interoperable Conversation Envelope Diagrams", "content": "The OVON (Open Voice Network) Conversation Envelope is a universal JSON structure whose purpose is to allow human or automatic agents (assistants) to interoperably participate in a conversation."}, {"title": "4.2 Interoperable Assistant Manifest Diagrams", "content": "By using the Assistant Manifest specifications each agent can describe its capabilities to other agents. This provides the basis for a DISCOVERY process by which an agent can find agents with specific capabilities. The state diagram in"}, {"title": "5 USE CASES", "content": "This section describes two scenarios in which multiple agents interact to coordinate and optimize a series of errands. This interaction is designed according to the Universal Open API specifications for Agentic Natural Language Multimodal Communications as described in this paper."}, {"title": "Smart Errands Use Case", "content": "In the first scenario, Emmett, a human, seeks assistance from Cassandra, his general AI assistant, to manage and streamline his possible errands efficiently. The AI assistants at various service points - Pat at Blooming Town Florist, Andrew at the Post Office, Charles at the hardware store, and Sukanya the Host at Thai Palace - facilitate the transactions. Emmett, a human, has the following goals:\n\u2022 Order some flowers for his wife's birthday.\n\u2022 Check on the repair of the chainsaw he left at the hardware store.\n\u2022 Order some carryout Thai food for lunch.\n\u2022 Find the cost of mailing a 2 pound package to California."}, {"title": "Smart Library Use Case", "content": "The second use case illustrates an actual experiment conducted to provide potential conversational AI chatbot support to Estonian citizens[26]. It outlines a scenario where various AI assistants collaborate to provide support for a human user, Lea, who needs assistance with literature research. The AI assistants are characterized by distinct roles, different technologies and AI capabilities."}, {"title": "6 IMPROVEMENTS AND FUTURE DIRECTIONS", "content": "While the interoperability specifications are currently able to support many practical use cases, there are several future enhancements that we believe will extend their applicability to even more use cases.\n\u2022 The ability to represent multimodal interchanges among agents\n\u2022 Support for multi-party conversations\n\u2022 Formats to enable agents to exchange background information, conversation history, and other context in addition to users' utterances\n\u2022 Redaction of sensitive data"}, {"title": "7 CONCLUSIONS", "content": "The proposed approach based on a Universal set of Open APIs for Agentic Natural Language Multimodal Communica- tions, utilising the Open Voice Conversation Envelope API, offers significant advancements in enabling independent conversational assistants to collaborate. The key benefits of this approach include its very loose coupling of assistants, which allows for a high degree of flexibility and ease of integration. It is agnostic to the underlying technologies on which agents are based, thereby supporting a diverse range of technologies, including Generative AI LLMs, LMM, LAM, Non-Generative AI, and even non-AI systems, across multiple communication channels such as voice, video, and text. The approach imposes minimal requirements on cooperating assistants, thus reducing the complexity of adding new ones. By relying on natural language interpretation, it simplifies the communication process between assistants. Additionally, it can seamlessly send information to human agents, enhancing its versatility in handling user requests. These features collectively represent a step forward in the development of interoperable and flexible conversational AI systems.\nThe state diagrams and use cases previously described illustrate how the specifications are meticulously designed to remain independent of any specific conversational AI technology, including but not limited to large language models (LLMs). This design ensures wide compatibility and interoperability across various AI platforms and technologies.\nThe proposed architecture for interoperable Conversational AI assistants is designed to be replicable and accessible via open repositories. Everyone can access the OVON specifications[14] and the Sandbox[25], an experimental browser/as- sistant system that implements InteropDialogEventSpecs and supports OVON Envelopes. This system features text and speech interfaces, includes a local server for hosting browser-based applications, and provides a list of assistant servers for experimentation. Additionally, it offers a basic Python as well as LLM based assistant server frameworks for developing new assistants.\nFurthermore, incorporating the proposed advancements into the interoperability specifications will profoundly elevate the capabilities of Conversational AI platforms in managing security, ethical interactions, and accountability. These advancements, previously detailed, include comprehensive authentication protocols, bias mitigation, transparency measures, and ethical guidelines, alongside enhanced data protection techniques. Implementing these measures ensures AI platforms not only meet current needs but are also prepared for broader and more complex applications, making them indispensable tools in a technology-driven future."}]}