{"title": "The influence of persona and conversational task on social interactions with a LLM controlled embodied conversational agent", "authors": ["Leon O. H. Kroczek", "Alexander May", "Selina Hettenkofer", "Andreas Ruider", "Bernd Ludwig", "Andreas M\u00fchlberger"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in conversational tasks. Embodying an LLM as a virtual human allows users to engage in face-to-face social interactions in Virtual Reality. However, the influence of person- and task-related factors in social interactions with LLM-controlled agents remains unclear. In this study, forty-six participants interacted with a virtual agent whose persona was manipulated as \u201cextravert\u201d or \u201cintrovert\u201d in three different conversational tasks (small talk, knowledge test, convincing). Social-evaluation, emotional experience, and realism were assessed using ratings. Interactive engagement was measured by quantifying participants\u2019 words and conversational turns. Finally, we measured participants\u2019 willingness to ask the agent for help during the knowledge test. Our findings show that the extraverted agent was more positively evaluated, elicited a more pleasant experience and greater engagement, and was assessed as more realistic compared to the introverted agent. Whereas persona did not affect the tendency to ask for help, participants were generally more confident in the answer when they had help of the LLM. Variation of personality traits of LLM-controlled embodied virtual agents, therefore, affects social-emotional processing and behavior in virtual interactions. Embodied virtual agents allow the presentation of naturalistic social encounters in a virtual environment.", "sections": [{"title": "1. Introduction", "content": "The rise of large language models (LLMs) has revolutionized human-computer interaction, enabling more sophisticated and intuitive exchanges between people and machines. LLMs can be instated as conversational agents (e.g. ChatGPT), allowing users to engage in meaningful language-based interactions. While earlier versions of conversational agents were based on simple pattern matching algorithms that generated pre-defined output (Ramesh et al., 2017), LLMs use probabilistic generative processes based on training data to generate responses (Generative Pre-Trained Transformer, GPT; Yenduri et al., 2024). Furthermore, because LLMs can consider the contextual information of an input, they allow the generation of individualized responses that match the user\u2019s intention. Because of these properties LLM controlled conversational agents are increasingly being used in chat based interactions for example in customer service and healthcare (Rivas & Zhao, 2023; Thirunavukarasu et al., 2023). However, while chat-based interactions are useful in many tasks, face-to-face conversations may be advantageous as they allow the simulation of ecological valid human-to-human interactions and the presentation of additional information via non-verbal cues (e.g., gesture, gaze, facial expressions). Importantly, face-to-face social interactions are better suited for establishing a deeper social connection between interactive partners than text based interactions (Sacco & Ismail, 2014). This has inspired the development of embodied conversational agents, in which computer-based dialogue systems are combined with animated virtual agents (Huang, 2018). Presenting embodied conversational agents in Virtual Reality (VR) as 3D, life-like virtual humans enable users to engage in multimodal, face-to-face conversation with an interactive partner in front of them. As with chatbots, recent attempts have been made to increase the naturalism of conversations by incorporating LLMs as embodied conversational agents (Hasan et al., 2023; Lim et al., 2024). The high degree of flexibility and individualization of LLM controlled conversations makes them superior to scripted speech stimuli implemented in virtual scenarios that can only follow a pre-defined conversational path. Another benefit of LLMs is that they are pretrained for standard conversation in natural language. Without the need to collect huge, but specialized data corpora for training dialogue models, LLMs can be integrated immediately in virtual agents. This allows use cases in research focused on social interactions as well as applications, for example conversations with virtual patients can be used in medical education (Graf et al., 2024) and extended conversations could be implemented in virtual exposure therapy .However, while such set-ups can have a profound impact on human experience and behavior, little is known about the underlying mechanisms and contextual influences that drive social interactions with LLM controlled embodied virtual agents.\nIn real-life social interactions people are fast to form social evaluations of their interactive partners (Bar et al., 2006; Satchell, 2019) and similar effects have been found for interactions with virtual agents (Guadagno et al., 2011). According to the computers as social actors framework these results can be explained by the automatic and unconscious response to social cues regardless of whether these cues are produced by a computer or a human (Nass & Moon, 2000). In addition, it has been argued that behavioral realism, i.e. the degree to which an agent\u2019s social cues resemble human ones, plays a crucial role in this regard (Von Der P\u00fctten et al., 2010) with more behavioral realism resulting in more social behavior towards the agent. Regarding human-AI interactions, previous findings support the computer as social actors framework for interactions with chatbots, e.g. by showing that an agent\u2019s empathic expressions lead to a more favorable social evaluation than neutral expressions (Liu & Sundar, 2018)."}, {"title": "2. Methods", "content": "2.1. Participants\nForty-six healthy volunteers were recruited at Regensburg University (mean age = 21.24 years , SD = 2.59, 36 females). Participants were randomly assigned into two groups of 23 participants with groups being matched for gender. A sensitivity analysis conducted using G*Power (Faul et al., 2009) revealed that this sample size allowed the detection of a medium to large effect size of \u03b7p2 = .10 with a power of 0.8 for a mixed effect ANOVA design. Participants did not report mental or neurological disorders and had normal or corrected-to-normal vision. All participants gave written informed consent. The study was conducted in line with the Declaration of Helsinki and was approved by the ethics committee of Regensburg University (24-3669-101). Participants received credit points for compensation.\n2.2. Experimental-Design\nThe study used a mixed design with the agent\u2019s persona (extraverted vs. introverted) as a between-subjects factor and conversational task (small talk vs. knowledge test vs. convincing) as a within-subject factor. During the knowledge test an additional manipulation was introduced by alternating between easy and difficult questions that had to be answered by the participants. Sympathy, Valence, Arousal, Closeness, and Naturalism were measured via self-report after each conversational task and number of words and turns during each task were assessed as behavioral measures. In the knowledge test, the percentage of questions for which the participants asked the agent for help was assessed as an additional metric. Finally, the"}, {"title": "2.3. Apparatus", "content": "2.3.1. Technical Set-Up\nAll system components are shown in the schematic overview in Figure 1. Virtual Reality was presented via a head-mounted display (Vive Pro, HTC, Taoyuan, Taiwan). The virtual environment was rendered and controlled using Unreal Engine 5.2 (Epic Games Inc., Raleigh, USA). Participants were placed in a virtual room at a table face-to- face with a male virtual agent (MetaHuman. Epic Games Inc.). The same agent was used in both between-subject personality conditions and the distance between participant and agent was set to 1.8 meter. Eye gaze of the virtual agent was controlled so that the agent was always looking towards the participant. Participant speech was sampled with an external USB microphone (t-bone SC 420, Thomann, Germany) and converted to text using the wav2vec model (Baevski et al., 2020). The text input was then forwarded as a prompt to a text generation model (\"SauerkrautLM-HerO\", https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-Hero) which was run on a separate computer using the text generation inference toolkit with streaming enabled. Importantly, deploying all components (including the LLM) on local servers allowed us to maintain full control over all data without the need to rely on third-party services and in accordance to data protection policies. Across all experimental sessions, the average time of the LLM to generate a response was 1.29 sec (SD = 0.26, range = 0.68 - 4.11 sec).\nTwo different personas were implemented as context prompts to the text generation model, i.e. prompts that preceded the user input, depending on the persona manipulation \u201cextravert\u201d or \u201cintrovert\". The personas included details about the communicative style and gave examples on how the agent would answer particular questions (see Table 1, full text in supplementary material). The descriptions varied in how persona were described as being sociable, talkative, and enjoying engaging with other people which all relates to the dimension of extraversion in the five-factor theory of personality (McCrae & John, 1992). Each persona also included the same information about how the LLM had to answer specific questions about preferences. This information was included to ensure that no differences in sympathy based on preferences (e.g. liking dogs better than cats) could arise between personas. Additional parameters, such as the maximum number of generated sentences and the temperature parameter, were manipulated between the personas (Table 1). The text output of the LLM was then converted into speech (male voice) using a speech2text model (Silero, https://github.com/snakers4/silero-models). The same voice was used for all personality conditions. Finally, the resulting audio was streamed to the Audio2Face application (Nvidia Omniverse, https://build.nvidia.com/nvidia/audio2face) where live lip movements were animated based on the \"Mark\" set-up. Animations and audio were then played by the virtual agent using the LiveLink plugin in the Unreal Engine.\n2.3.2. Ratings and Questionnaires\nAfter each conversational task, participants were asked to rate their experience using a visual analog scale (values 0 - 100). Ratings included dimensions to describe emotional experience (valence and arousal), as well as social evaluation of the virtual agent (sympathy and closeness). These ratings included sympathy (\u201cHow sympathetic was the virtual person?\u201d, 0 = very unsympathetic, 100 = very sympathetic), valence (\u201cHow unpleasant or pleasant did you feel during the interaction?\u201d, 0 = very unpleasant, 100 = very pleasant), arousal (\u201cHow high was your arousal during the interaction\u201d), closeness (\u201cHow close/connected did you feel to the virtual person?", "How unnatural or natural did you experience the interaction?": "very unnatural, 100 = very natural). Because the \"knowledge test\" conversational task required participants to answer questions about general world knowledge, two additional ratings were included that asked for participants' confidence in the correctness of their answer for questions answered with or without help of the"}, {"title": "2.6. Data Preprocessing and Statistical Analysis", "content": "During the experiment participants' speech was transcribed using the speech-to-text model (see above) and written to a logfile. These data were analyzed by counting the number of words and turns (e.g. an uninterrupted segment of speech of the participant with no interleaved response from the virtual agent) for each question during a conversational task. Note that to characterize interactive engagement, words and turns were analyzed per question rather than as a total number. This was necessary because otherwise simply asking the displayed question and then switching to the next question without any true interactive engagement would have increased the total number of words or turns (and this behavior would have been more likely to occur for the introverted persona where responses were short). One participant was excluded from the analysis because only one question was asked.\nStatistical analysis were conducted in the R environment (R Core Team, 2016). Data for each dependent variable were analyzed using a mixed effect ANOVA with persona as between-subject and conversation task as a within-subject factor. Sphericity violations were corrected using Greenhaus-Geisser correction. In case of significant interaction effects post-hoc t-test were conducted using Holm method to correct for multiple comparisons. Alpha level was set at .05."}, {"title": "2.7. Open Science Statement", "content": "Study procedures and hypotheses were preregistered (https://osf.io/tukqh/?view_only=977721578f4d42a5bf941 7d6d08bbe46). Ratings, questionnaires, and secondary data on conversations (word number, turns) as well as analysis scripts are publicly available in an online repository (https://osf.io/ws7jf/?view_only=7bb7b39b401a444d8ec5 9bcccb4d2e93). Note that transcriptions of conversations may contain personal information and are, therefore, not publicly available."}, {"title": "3. Results", "content": "3.1. Manipulation Check\nThe number of words generated by the LLM per turn was compared between personas as a manipulation check. The extraverted persona generated significantly more words (M = 24.19, SD = 75.62) per turn than the introverted persona (M = 15.35, SD = 29.80), t(41.90) = 11.08, p < .001, d = 3.30 and was thus significantly more talkative. These data demonstrate that the personality manipulation via persona prompts was successful and resulted in the intended effects (see also validation of LLM persona above).\n3.2. Social Evaluation of Virtual Agent\nWe investigated how interacting with different LLM personas during different conversational tasks affected participants' social evaluation of the virtual agents in terms of sympathy and closeness (Figure 2).\nAnalysis of sympathy of the virtual agent revealed a main effect of persona, F(1,44) = 21.57, p < .001, \\eta_{\\rho}^{2} = .33, a main effect of conversational task, F(2, 88) = 9.82, p < .001, \\eta_{\\rho}^{2} = .18, but no interaction effect between persona and conversational task, F(2, 88) = 2.97, p = .056, \\eta_{\\rho}^{2}= .06. Sympathy was higher for extraverted (M = 58.20, SD ="}, {"title": "3.3. Emotional Experience", "content": "To characterize the influence of LLM persona and conversational task on emotional experience valence and arousal ratings were analyzed (Figure 3).\nANOVA results for valence ratings revealed a main effect of persona, F(1,44) = 6.08, p = .018, \\eta_{\\rho}^{2} = .12, but no main effect of conversational task, F(2, 88) = 0.34, p =.714.001, \\eta_{\\rho}^{2} < .01, and no interaction effect between persona and conversational task, F(2, 88) = 0.33, p = .720, \\eta_{\\rho}^{2} < .01. Participants interacting with the extraverted persona rated their experience as more pleasant (M = 56.97, SD = 13.64) than participants interacting with the introverted persona (M=45.01, SD = 18.88), but experience was not influenced by conversational task.\nAnalysis of arousal ratings revealed only a main effect of conversational task, F(2, 88) = 5.49, p = .006, \\eta_{\\rho}^{2} = .11, but no main effect of persona, F(1, 44) = 1.03, p = .316, \\eta_{\\rho}^{2} = .02, and no interaction effect, F(2, 88) = 1.52, p = .224, \\eta_{\\rho}^{2} = .03. Participants rated both the knowledge test"}, {"title": "3.4. Experience of virtual interactions", "content": "Another goal was to investigate how LLM persona and conversational task influenced how realistically the interactions with the virtual agent were rated and how social presence was experienced (Figure 4).\nANOVA of the realism ratings revealed a main effect of conversational task, F(2, 88) = 6.89, p = .002, \\eta_{\\rho}^{2} = .14, but no main effect of persona, F(1, 44) = 3.43, p = .071, \\eta_{\\rho}^{2} = .07. However, there was a significant interaction effect of persona and conversational task, F(2, 88) = 5.07, p = .008, \\eta_{\\rho}^{2}= .10. Post-hoc Welch t-tests revealed that interaction with the extraverted persona (M = 38.26, SD = 20.14) was rated as more realistic than interaction with the introverted persona (M = 20.52, SD = 17.07) during the convincing task, t(42.85) = 3.22, p = .002, but there was no difference in realism between personas for the small talk (Extraverted: M = 26.91. SD = 16.38; Introverted: 21.78, SD = 16.13), t(43.99) = 1.07, p = .290, and the knowledge test task (Extraverted: M = 35.09, SD = 23.99; Introverted: 32.61, SD = 15.38), t(37.46) = 0.41, p = .679. A follow-up on the main effect of conversational task showed that the knowledge test was rated as more realistic than the small talk task, t(45) = 3.38, p = .004, but there were no differences between the small talk and convincing task, t(45) = -1.78, p = .129, and between the knowledge test and convincing task, t(45) = 1.90. p = .129. Overall, persona affected the experienced realism of an interaction only in"}, {"title": "3.5. Behavioral Engagement during Interaction", "content": "In addition to self-reports, behavioral parameters were measured to investigate how LLM persona and conversational task affect participants' engagement in the interaction with the virtual agent (Figure 5). Three parameters were assessed for this: (1) The number of pre- defined topics/questions that participants worked through during a conversational task. Note, that a higher number means that participants were faster to switch to the next topic and therefore engaged less with the virtual agent for a given topic. (2) The number of words per topic measured the average number of words that participants spoke during a topic, more words means that participants were more engaged in the conversation. (3) The number of initiated turns per topic measured how many back-and-forth alternations with the virtual agent were initialized by participants for a single topic in average. A higher number of turns indicates more interactive engagement.\nFirst, the number of the pre-defined topics/questions that participants worked through with the virtual agent was analyzed. ANOVA results revealed a significant main"}, {"title": "3.6. Using Al for help in the knowledge test", "content": "During the knowledge test, participants had to answer questions and were free to ask the virtual agent for help. Questions were grouped into easy and diffult questions. The percentage of questions for which participants asked for help was then analyzed with a mixed ANOVA using the between-subject factor persona and the within-subject factor question difficulty. The analysis revealed a significant main effect of difficulty, F(1, 44) = 666.49, p < .001, \\eta_{\\rho}^{2} = .94, but no effect of persona, F(1, 44) = 1.01, p = .321, \\eta_{\\rho}^{2} = .02, and no interaction effect between persona and difficulty, F(1, 44) = 0.28, p = .601, \\rho^{2} < .01. Therefore participants asked the LLM-controlled agent more frequently for help when difficult questions had to be answered (M = 90.03 %, SD = 9.91%) compared to when easy questions had to be answered (M = 23.12 %, SD = 17.47%), but asking for help was not influenced by persona of the LLM controlled virtual agent.\nIn addition, participants rated how confident they were about the correctness of their answers, either for questions answered with or without help of the virtual agent (factor help). Analysis of these confidence ratings revealed a main effect of help, F(1, 44) = 11.19, p = .002, \\eta_{\\rho}^{2}= .20, but no effect of persona, F(1, 44) = 0.71, p = .404, \\eta_{\\rho}^{2} = .02, and no interaction effect, F(1, 44) = 0.11, p = .740, \\eta_{\\rho}^{2} < .01."}, {"title": "4. Discussion", "content": "The goal of this study was to investigate how the persona of an LLM controlled embodied conversational agent modulates experience and behavior in social interactions across different conversational tasks. Participants interacted with either a extraverted or an introverted LLM persona during a small talk, a knowledge test or a convincing task. In order to characterize the influence of these manipulations we assessed social-evaluative processes, emotional experience, and evaluation of the VR scenario, as well as behavioral parameters such as interactive engagement and participants' tendency to ask the LLM for help. Overall, we found consistent evidence for an effect of persona in line with our hypotheses that conversing with the extraverted persona resulted in a more positive social evaluation, a more positive emotional experience and greater interactive engagement than conversing with the introverted persona. There was, however, no consistent interaction effect between LLM persona and conversational tasks. The effect of LLM persona was only modulated by conversational tasks with respect to the experienced realism and number of words, in a sense that the relative increase in realism and engagement from introverted to extraverted was most pronounced in the convincing task. This finding did not confirm the hypothesis that the effect of extraversion would be greater in the small talk compared to the knowledge test and convincing task. Furthermore, in contrast to our hypothesis the small talk was not more pleasant than the knowledge test and convincing task. Finally, while we found no support for the hypothesis that a extraverted LLM personality would increase participants tendency to ask the virtual agent for help during the knowledge test, we did find that raising question difficulty strongly increased the tendency to ask for help and that participants were more confident in the answers when they had asked the LLM controlled agent for help. Overall, the present study demonstrates that persona of a LLM embodied conversational agent strongly influences experience and behavior in social interactions, although more data are required to characterize the influence of conversational tasks.\nPrompting persona traits in a LLM strongly affects social evaluation of the virtual agent, emotional experience, and interactive engagement. Interaction with the extraverted persona increased the feeling of sympathy towards the virtual agent compared to the introverted persona, similarly as in real-word interactions (Wortman & Wood, 2011). In line with findings from chat-based interactions (Liu & Sundar, 2018) our results demonstrate that users are sensitive to the communicative style of a virtual interactive agent and adapt their behavior and socio- emotional processing accordingly. The more positive evaluation of the extraverted persona in comparison to the introverted persona is in line with previous studies which found more positive social evaluations (i.e. liking) in real- life for persons scoring high on sociable aspects of extraversion such as being talkative and joyful (Wortman & Wood, 2011). However, it should be noted that extraversion may have a strong influence on an initial positive evaluation but may have a weaker influence on the formation of social bonds across longer interactions (Harris & Vazire, 2016). Interestingly, this was also reflected in the present study, where we did not observe an influence of LLM persona on interpersonal closeness. However, such deeper social connections may develop over longer periods of time as has been demonstrated for multi-session interactions with a chatbot (Araujo & Bol, 2024). Overall, the present study demonstrates that manipulations of extraversion via LLM prompts impact socio-evaluative processes of face-to-face social interactions in immersive virtual environments and simulate findings from human- human interaction.\nA further target of our study was to shed light on the influence of task characteristics on social interaction. Interestingly, participants experienced the argumentation focused conversation as equally pleasant yet more arousing than the small talk conversation. This finding suggests that the exchange of arguments was experienced as a deeper conversation rather than an adverse disputation and can be explained by the fact that the LLM was trained to be a helpful assistant and therefore responses of the model were always respectful and constructive. Furthermore, we observed a modulation of the impact of persona by task for the evaluation of interactive realism. Interaction with the extraverted persona was experienced as more realistic than interaction with the introverted persona in the deeper argumentative conversations, but persona did not affect realism in the other conversational tasks. This suggests that"}]}