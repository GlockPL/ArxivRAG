{"title": "Hyperedge Anomaly Detection with Hypergraph Neural Network", "authors": ["Md. Tanvir Alam", "Chowdhury Farhan Ahmed", "Carson K. Leung"], "abstract": "Hypergraph is a data structure that enables us to model higher-order associations\namong data entities. Conventional graph-structured data can represent pairwise\nrelationships only, whereas hypergraph enables us to associate any number of\nentities, which is essential in many real-life applications. Hypergraph learning\nalgorithms have been well-studied for numerous problem settings, such as node\nclassification, link prediction, etc. However, much less research has been con-\nducted on anomaly detection from hypergraphs. Anomaly detection identifies\nevents that deviate from the usual pattern and can be applied to hypergraphs to\ndetect unusual higher-order associations. In this work, we propose an end-to-end\nhypergraph neural network-based model for identifying anomalous associations\nin a hypergraph. Our proposed algorithm operates in an unsupervised manner\nwithout requiring any labeled data. Extensive experimentation on several real-life\ndatasets demonstrates the effectiveness of our model in detecting anomalous\nhyperedges.", "sections": [{"title": "Introduction", "content": "Graph-structured data can naturally represent pair-wise relationships, which helps model a wide range\nof real-life problems. Graph neural network-based machine learning models have been explored\nextensively for node classification, link prediction, anomaly detection, etc. However, graphs fail to\npreserve relationships beyond pairs, such as co-authorship networks, social groups, etc. Hypergraphs,\non the other hand, can model higher-order complex relationships and associate any number of\nentities. Neural network-based machine learning models have also been developed for hypergraphs.\nThese models primarily focus on node classification, node clustering, link prediction, etc. Anomaly\nidentification in hypergraphs has received less attention comparatively.\n\nAnomaly detection is a core data mining task that identifies unusual events deviating from the\nnorm. Graph-based anomaly detection has attracted researchers' attention. Methods have been\ndevised to detect anomalies within a single graph [1-3]. Most earlier research uses statistical\nmodels or substructure mining, which limits the capability of the methods in terms of scalability\nand generalization. Recent research has utilized the expressive power of Graph Neural Networks\n(GNN) for node classification [4], clustering [5], link prediction [6], etc. Consequently, GNNs are\napplied in graph anomaly detection also [7]. Algorithms have been developed to detect anomalous\nnodes within a graph [8, 9], with applications such as identifying compromised nodes in a network,\ndetecting spam accounts in social networks, etc. Anomalous edge detection involves spotting unusual\nconnections between nodes [10], crucial for uncovering unusual communication patterns or fraudulent\ntransactions[11]. On the contrary, graph-level anomaly detection identifies anomalous graphs within\na set of graphs [12, 13]."}, {"title": "Related works", "content": "In this section, we discuss the research works related to our work. We focus on hypergraph learning\nmethods and hypergraph anomaly detection."}, {"title": "Hypergraph Learning Methods", "content": "Following the impactful utilization of deep learning to graph-structured data, hypergraph neural\nnetwork has been devised for learning hypergraph-related tasks. Initially, a spectral hypergraph\nembedding method based on the hypergraph Laplacian was introduced in [22]. HGNN [17] general-\nized graph convolutional network [23] to the hypergraph domain by propagating features through a\nsingle-stage message-passing framework. HyperGCN [18] also adapted graph convolutional network\nto hypergraph by approximating the hypergraph with a graph. To properly capture the higher-order\nrelationships in the representations, AllSet [19] proposed a two-stage message-passing framework.\nIn this approach, instead of learning the node representations from the neighborhood, the hyperedge\nrepresentations are learned from the node representations/features of the previous layer first. Then,\nthe hyperedge representations are aggregated to learn the node representations. HCoN[20] devised\na model for both node and hyperedge classification that considers both node and hyperedge from\nthe previous layer. Method for hyperedge prediction [21] based on hypergraph neural networks is\nexplored that adopts generative adversarial training to generate negative examples."}, {"title": "Hypergraph Anomaly Detection", "content": "Anomaly detection methods for graph-structured data can be broadly categorized into four categories:\n(1) Node Anomaly Detection, (2) Edge Anomaly Detection, (3) Subgraph Anomaly Detection, and (4)\nGraph-level Anomaly Detection. OCGNN [7] proposes an end-to-end framework for node anomaly\ndetection by extending a one-class support vector machine. BWGNN [8] analyzes spectral energy\ndistributions and devises a graph neural network to detect node anomalies. A scalable approach for\ndetecting anomalies in a dynamic graph setting is proposed in [10]. IGAD [12] introduces a Point\nMutual information-based loss function to graph neural network for graph-level anomaly detection.\nOCGTL [13] develops a one-class graph transformation learning model to detect anomalies from a\nset of graphs. For hypergraph data, research on anomaly detection has mostly focused on hyperedge\nlevel anomalies. An anomaly detection method for hyperedge streams using minhash and locally\nsensitive hashing was developed by LSH [14]. HashNWalk [15], an incremental algorithm, uses\nrandom walk similarities and hash functions to identify anomalies in hyperedge streams. To scale the\nalgorithm for large-scale streams, it maintains a constant-size summary of the stream to calculate\nthe anomaly scores. A variational expectation-maximization algorithm is tailored for hypergraphs to\ndetect anomalies through probability mass function estimation in [16]."}, {"title": "Preliminaries", "content": "In this section, we introduce notations, problem definitions, and preliminaries on hyperedge anomaly\ndetection."}, {"title": "Notations", "content": "A hypergraph can be represented as $H = (V, E, X)$, where $V = {v_1, v_2, ..., v_{|V|}}$ is the set of nodes\nor vertices, $E = {e_1, e_2, ..., e_{|E|}}$ is the set of hyperedges, and $X \\in R^{|V|\\times d}$ is the feature matrix.\nEach hyperedge is a subset of the vertices set that it connects, i.e., $\\forall e\\in E\\exists e \\subseteq V$. The incidence matrix\nof hypergraph H is denoted by $A_H \\in R^{|E|\\times|V|}$, where the (i, j)-th entry is 1 if the i-th hyperedge\ncontains the j-th vertex, and 0 otherwise. $X_v$ represents the d-dimensional feature vector of the node\n$v$."}, {"title": "Problem Definition", "content": "Given a hypergraph $H = (V, E, X)$, the task of hyperedge anomaly detection is to learn a function\n$f: 2^V\\rightarrow [0, 1]$ that assigns an anomaly score to a hyperedge. A higher score indicates a higher\nlikelihood of being an anomaly for a hyperedge. Note that the hyperedge is not necessarily a member\nof E."}, {"title": "Proposed Methods", "content": "In this section, we present our proposed model, HAD. HAD learns the node embeddings using\na hypergraph neural network. For each hyperedge, we derive its embedding by pooling from the\nembeddings of the nodes it connects. We define the centroid of the hypergraph as the mean of all the\nhyperedge embeddings in the hypergraph. Finally, we train a one-class classifier that optimizes the\nmean Euclidean distance between the hyperedge embeddings and the centroid."}, {"title": "Learning Node Embeddings", "content": "In our model, We begin by learning the node embeddings of the hypergraph using a hypergraph\nneural network architecture. The vector representation of a hyperedge $e \\in E$ at layer $l$, $Z_e^l$, is derived\nfrom the embeddings of the nodes it contains from the previous layer.\n\n$Z_e^l = ENN^l(\\sum_{v\\in e} Z_v^{l-1})$   (1)\n\nHere, $ENN^l$ is the multi-layer perceptron for hyperedges at layer $l$. $Z_v^{l-1}$ is the node embedding\nof the node/vertex v of layer l \u2013 1. We derive the vector representation of a node $v \\in V$ at layer l,\ndenoted as $Z_v^l$, from the embeddings of the hyperedges containing the node.\n\n$Z_v^l = VNN^l(\\sum_{e\\in E_v} Z_e^l)$   (2)\n\nHere, $VNN^l$ is the multi-layer perceptron for nodes at layer l. $E_v$ is the set of hyperedges that\ncontains v. That is, $E_v = {e : e \\in E \\text{ and } v \\in e}$. Note that $Z_v^0 = VNN^0(X_v)$, where $X_v$ is the\nd-dimensional feature vector of the node v from the feature matrix."}, {"title": "Learning Hyperedge Embeddings", "content": "The embedding of a hyperedge e at the final layer L, $Z_e^L$, is learned by pooling from embeddings of\nthe nodes it connects. We use maxmin pooling, subtracting the element-wise minimum values from\nthe element-wise maximum values, which captures the diversity of the nodes within the hyperedge.\nThis information of diversity within a hyperedge may be crucial for detecting anomalies.\n\n$Z_e^L = max{\\bigcup_{v\\in e} Z_v^{L-1}} \u2013 min{\\bigcup_{v\\in e} Z_v^{L-1}}$   (3)"}, {"title": "One Class Classifier", "content": "In order to make our model trainable end-to-end, we have developed a one-class classifier by\noptimizing an objective function. We compute a centroid of the given hypergraph as a reference\npoint for calculating anomaly scores. The centroid, $C_H$, is calculated by taking the mean of all the\nhyperedge embeddings in the hypergraph.\n\n$C_H = \\frac{1}{|E|}\\sum_{e\\in E} (Z_e^L)$   (4)\n\nThe anomaly score of a hyperedge e, $f(e)$, is calculated by its Euclidean distance from the embedding\n$Z_e^L$ to the hypergraph centroid $C_H$.\n\n$f(e) = ||Z_e^L - C_H ||_2$   (5)"}, {"title": "Experimental Results", "content": "In this section, we present the experimental settings and perform an extensive result analysis of our\nproposed algorithm. Section 5.1 describes the experimental setup. Section 5.2 presents the details of\nthe datasets. Section 5.3 discusses the baseline considered for comparison and Section 5.4 analyzes\nthe experimental results. Finally, in Section 5.5, we visualize the anomaly scores for inlier and\nanomalous hyperedges."}, {"title": "Experimental Setup", "content": "To evaluate the performance of our algorithm, we conducted experiments on six real-world datasets.\nWe implemented the algorithm using Python3 programming language and utilized an Intel Core\ni7-6700k CPU @ 4.00 GHz with 16 GB RAM to conduct the experiments. We split the inlier\nhyperedges for each dataset into a training set (80%) and an inlier test set (20%). The training\nset containing inlier hyperedges is only used to train the model. In contrast, the test set contains\nanomalous hyperedges and an oversampled inlier test set to address the class imbalance issue. We\nmeasured the AUROC value for performance metrics and used five-fold cross-validation, considering\nthe mean value over five runs. We set the number of layers L to two and the loss_threshold to\n0.0001 for all the datasets. Note that we only require labeled data to test the performance of our\nalgorithm, and we avoid using any validation set since the model we proposed is unsupervised."}, {"title": "Datasets", "content": "We have collected six real-life hypergraph datasets for our experiments from different domains. In\nTable 1, we present the statistical summary of the datasets, and the dataset descriptions are provided\nbelow.\nMushroom: It contains information about various mushroom species. For each species, 22 nominal\nvalue attributes are recorded. The species are categorized as either edible or poisonous. We created a\nhypergraph using the nominal values as nodes, with each hyperedge representing a species connecting\nthe nodes of its nominal values. The edible species are considered as inliers, whereas the poisonous\nspecies are considered as anomalies. Due to the absence of any node features, we assigned each node\nwith unique identity vectors as features.\nCo-citation datasets: Citeseer, Cora, and Pubmed are three co-citation datasets containing infor-\nmation about papers, their citations, and co-citations. In the hypergraph representation, Each node\nrepresents a paper, and each hyperedge connects papers cited in another paper. Bag-of-words features\nfrom the paper abstracts are used as node features. For hyperedge classification, on datasets where\nlabels are unavailable, the labels of the nodes in a hyperedge are used to label the hyperedge [20].\nFollowing this approach, we also utilized the node labels. We considered the hyperedges containing a\nnode labelled with the most frequent node label as inliers and all the other hyperedges as anomalies.\nAuthorship datasets: CoraA and DBLP are two authorship datasets we considered in our ex-\nperiments. Each node represents a paper in these hypergraphs, and the hyperedges connect papers\nauthored by a specific author. Similar to co-citation datasets, bag-of-words features of the abstracts are\nused as node features. The same strategy as co-citation datasets is used to distinguish the anomalous\nhyperedges."}, {"title": "Baselines", "content": "Our experiments considered three baseline methods for performing comparative performance analysis.\nWe applied LSH [14], an algorithm proposed for anomaly detection in a hyperedge stream, by\nrandomly shuffling the order of hyperedges. Similarly, we have considered HashNWalk [15], another\nanomaly detection algorithm for hyperedge streams. We implemented VEM [16], a variational\nexpectation-minimization algorithm, to detect hyperedge anomalies. To calculate the AUROC score,\nwe first normalized the anomaly score for all the algorithms. Furthermore, we have implemented two\nvariations of our algorithm. First, we utilized mean pooling instead of maxmin pooling (Equation\n3) in the final layer to examine the significance of capturing diversity in detecting anomalies and\nnamed the model HAD-Mean. Second, we fixed the centroid $C_H$ as proposed in existing one-class\nclassifiers [24] and named the model HAD-Fixed. However, for HAD-Fixed, the loss value (Algorithm\n1, line 13) converges below the loss_threshold value of 0.0001 as used in our experimental setup.\nWe have run the training for 1000 epochs to deal with the issue instead of using the loss_threshold\nvalue."}, {"title": "Results", "content": "In Table 2, we present the AUROC scores in percentages of our model along with the baselines for\nall six datasets. The best result of all models is highlighted in bold. We observe that our model has\noutperformed all the baselines significantly on all the datasets. On dataset Mushroom, HAD-Mean\nand HAD (Proposed) both achieve perfect scores of 100%, significantly outperforming other methods.\nVEM is the next best, with a score of 72.04%. The AUROC score for HashNWalk is 57.47%, while\nLSH performs poorly at 32.02%. The AUROC score of our model is 27.95% more than that of VEM,\nthe highest among the baselines. On Citeseer, the AUROC score is 8.49% more than LSH, the best-\nperforming baseline. The improvements are also significant for CoraA and Cora, which are 13.94%\nand 10.92%, respectively. On Pubmed, the difference is marginal compared to HashNWalk (0.03%).\nFor the hypergraph DBLP, the improvement is 2.92%. The better performance is also evident in the\ncomparison with the variants HAD-Mean and HAD-Fixed. HAD-Fixed performs the worst among\nthe HAD variants, particularly on datasets Mushroom, Citeseer, CoraA, and DBLP. HAD-Mean\nperforms well on certain datasets, particularly Mushroom and Cora, but fails to match the overall\nperformance of HAD-Proposed. The better AUROC score of our proposed model than HAD-Mean\ndemonstrates the importance of the knowledge of diversity within a hyperedge for anomaly detection.\nThe AUROC score of HAD-Fixed is also lower than our proposed model. Compared to HAD-Mean,\nthe AUROC score is lower on all the datasets except Pubmed for HAD-Fixed. When the centroid is\nfixed, it becomes challenging for the model to converge and learn due to the possibility of selecting\na poor centroid. Having a dynamic centroid allows the model to learn more easily by choosing a\nsuitable centroid, leading to improved performance."}, {"title": "Visualization", "content": "In Figure 3, we visualize the anomaly scores of the hyperedges for all six datasets. In most cases, the\nanomaly scores for inliers are clustered close to zero, while anomalies tend to have higher scores."}, {"title": "Conclusion", "content": "In this paper, we proposed HAD, an anomaly detection algorithm for hyperedges in a hypergraph.\nOur proposed model is unsupervised and needs no labeled data. We devised an end-to-end model\nthat employs a hypergraph neural network to learn hyperedge representations and then predicts the\nanomaly score with a one-class classifier. To the best of our knowledge, we are the first to propose a\ndeep neural network-based model for anomaly detection on hypergraphs. We collected six real-life\nhypergraph datasets from different domains to evaluate the performance. We performed a comparative\nresult analysis of our method against the state-of-the-art research. A significantly higher AUROC\nscore across the datasets demonstrates the effectiveness of our algorithm. Future research directions\nfor this work may include utilizing labeled data, entire anomalous hypergraph detection, etc. The\nsource code of our algorithm implementation is available online\u00b9"}]}