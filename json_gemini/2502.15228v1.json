{"title": "AutoMR: A Universal Time Series Motion Recognition Pipeline", "authors": ["Likun Zhang", "Sicheng Yang", "Zhuo Wang", "Haining Liang", "Junxiao Shen"], "abstract": "In this paper, we present an end-to-end automated motion recognition (AutoMR) pipeline designed for multimodal datasets. The proposed framework seamlessly integrates data preprocessing, model training, hyperparameter tuning, and evaluation, enabling robust performance across diverse scenarios. Our approach addresses two primary challenges: 1) variability in sensor data formats and parameters across datasets, which traditionally requires task-specific machine learning implementations, and 2) the complexity and time consumption of hyperparameter tuning for optimal model performance. Our library features an all-in-one solution incorporating QuartzNet as the core model, automated hyperparameter tuning, and comprehensive metrics tracking. Extensive experiments demonstrate its effectiveness on 10 diverse datasets, and most achieve state-of-the-art performance. This work lays a solid foundation for deploying motion-capture solutions across varied real-world applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Gesture and motion analysis is crucial in fields such as human-computer interaction [29], healthcare, and robotics. The growing availability of multimodal datasets from wearable sensors [27], cameras, and motion capture systems has driven the need for scalable and adaptable gesture recognition solutions. However, leveraging these datasets effectively presents several challenges, as illustrated in Figure 1.\nOne major challenge is the variability in dataset formats, sampling rates, and noise levels. Data collected from different sensors, such as IMUs, skeletal motion capture, or video-based tracking, often require dataset-specific preprocessing pipelines, increasing complexity and limiting scalability. Additionally, gesture recognition models typically need to be tailored to specific datasets or application scenarios. The absence of a unified framework results in repetitive model design and optimization efforts, leading to increased computational costs and resource demands.\nAnother obstacle is the complexity of hyperparameter tuning. Selecting appropriate learning rates, batch sizes, and network architectures is critical for achieving high performance but often requires domain expertise and substantial computational resources. This challenge is particularly pronounced for non-experts who lack experience in fine-tuning deep learning models. Furthermore, many existing gesture recognition frameworks require extensive customization and expert knowledge [15], making them less accessible to non-specialists. Simplifying the deployment process is essential to enable broader adoption across various fields [32].\nTo address these challenges, this paper introduces AutoMR, a unified framework that automates the motion recognition pipeline, encompassing data preprocessing, model training, and hyperparameter tuning. AutoMR standard-izes datasets, eliminates the need for manual preprocessing, and enhances performance through automated tuning. Its compatibility with multiple datasets and sensor modalities ensures scalability and facilitates deployment. Evaluated on ten benchmark datasets, AutoMR demonstrates competitive performance against state-of-the-art models. By reducing manual effort and computational costs, the framework streamlines model development and improves accessibility, making motion recognition more efficient for researchers and practitioners. To fully assess the effectiveness and reproducibility of our method, We encourage readers to review the provided code: https:// github.com/X-Intelligence-Labs/AutoMR. This code demonstrates our state-of-the-art (SOTA) results achieved on eight datasets and showcases the fully automatic and adaptive nature of our model without any parameter tuning. More details are available on the website: https://x-intelligence-labs."}, {"title": "II. RELATED WORK", "content": "Existing research on gesture and motion data analysis spans task-specific models [19], AutoMR frameworks [12], and hyperparameter tuning tools [20]. While each of these areas has made significant contributions, they also present certain limitations when applied to multimodal datasets.\nTask-specific models are widely used in gesture and motion recognition. For instance, DeepConvLSTM combines convolutional layers and LSTMs to capture spatiotemporal patterns, showing strong performance on UCI-HAR [17]. Similarly, Temporal Convolutional Networks (TCNs) use dilated convolutions to model long-range dependencies in time-series data [1], demonstrating success on OPPORTUNITY [2] and other multimodal datasets. However, these methods are typically tailored to specific modalities (e.g., IMU or skeletal motion) and often lack scalability to new data types such as SEMG signals or unsegmented motion capture data.\nAuto training frameworks, such as Auto-sklearn and H2O AutoGR [13], offer automation in model selection and hyperparameter tuning. These tools reduce the expertise required to build machine learning pipelines and have demonstrated potential in time-series classification. However, these frameworks are not tailored for multimodal datasets and often require extensive customization for preprocessing and handling heterogeneous sensor data. For example, TPOT's limited modality-specific pipelines restrict its applicability to datasets like LMDHG, where skeletal motion data demands complex preprocessing and feature extraction [14].\nFrameworks such as Optuna and Hyperopt are commonly used for optimizing hyperparameters [26]. These tools use algorithms like Bayesian optimization to efficiently explore the search space, and they have been applied to datasets like MHEALTH [31] and OPPORTUNITY [23] for tuning hyperparameters such as learning rates and network depths. However, they are not integrated into end-to-end workflows and often require significant manual setup for data preprocessing, model training, and evaluation."}, {"title": "III. METHOD", "content": "AutoMR provides a unified solution for motion recognition, addressing challenges such as dataset diversity, model scalability, and hyperparameter optimization, as illustrated in Figure 2. The core layer includes modules responsible for dataset preprocessing, augmentation, training management, model selection, and hyperparameter tuning (Figure 3). The dataset module standardizes multimodal input formats, ensuring consistency across different datasets, while the augmentation module introduces transformations to improve generalization. The training module orchestrates model learning, incorporating configurations from the model factory module and optimizing performance through the AutoML hyperparameter module.\nThe dataset-specific layer consists of training scripts such as main MHEALTH, main UCI-HAR, and so on, which interact with the core modules to preprocess data, configure models, and execute training. This structured approach minimizes manual adaptation while ensuring scalability, allowing AutoMR to generalize across diverse datasets and facilitate efficient motion recognition."}, {"title": "B. Data Processing and Model Factory", "content": "To ensure dataset consistency, we define structured formats that include modality specifications, gesture labels, and sampling parameters. This modular approach supports ten datasets Shrec2021, MHEALTH, UCI-HAR, DB4, Berkeley-MHAD, LMDHG, and four subsets of the OPPORTUNITY dataset (see TABLE I) and aligns diverse data modalities for compatibility with subsequent model training.\nIn addition, our model factory dynamically selects, configures, and instantiates models for gesture recognition. We choose QuartzNet as our primary model due to its efficient processing of sequential data [28]. QuartzNet employs depthwise separable convolutions to reduce parameters while preserving feature extraction, and its residual connections improve gradient flow for deeper architectures. Varying kernel sizes and dilation rates enable multi-scale temporal pattern recognition [18]. To accommodate datasets of varying complexity, we also employ QuartzNetLarge, which allows adjustments in block structure, input channels, and head channels [21]."}, {"title": "C. Training Module", "content": "The training module, built on TorchTNT, optimizes QuartzNet by automating metric tracking and incorporating techniques such as gradient clipping, anomaly detection, and dynamic learning rate scheduling. It supports multi-class classification using cross-entropy loss and evaluates performance using metrics like accuracy, F1 score, and confusion matrices. Our module includes checkpointing mechanisms to prevent data loss and ensure reproducibility by saving the best-performing model parameters, and we use TensorBoard for real-time logging to facilitate monitoring and debugging."}, {"title": "D. Hyperparameter Tuning", "content": "AutoMR automates hyperparameter tuning using SMAC [3] to dynamically optimize performance. The configuration space adapts to each dataset by tuning general parameters (learning rates, weight decay, dropout rates, batch sizes) and QuartzNet-specific settings (number of blocks, cells per block, input channels, kernel sizes), with the ConfigSpace library managing these ranges to balance cost and efficiency.\nThe process begins with the ModelHyperOptimizer defining parameter ranges and initializing models. We then train for a fixed number of epochs using standard techniques, evaluating performance via accuracy, F1 score, and loss. SMAC iteratively refines the search, storing the best configurations for reuse unless re-optimization is required. For ablation studies, we also perform manual tuning with batch sizes of 32, 64, 128, or 256 to assess their impact on performance."}, {"title": "IV. EXPERIMENTAL RESULTS ANALYSIS", "content": "Table II and Figure 4 compare the performance of AutoMR with existing state-of-the-art (SOTA) methods across ten benchmark datasets. AutoMR achieves the highest accuracy on eight of the ten datasets, demonstrating its generalizability and adaptability across different sensor modalities and gesture recognition tasks. Notably, AutoMR significantly outperforms previous models on datasets such as OPPORTUNITY, where it achieves over 5% higher accuracy compared to prior SOTA models. However, performance on DB4 and LMDHG is slightly lower than SOTA, indicating potential areas for future improvement, particularly in handling highly noisy or unstructured motion data.\nTo evaluate the impact of automated hyperparameter tuning, an ablation study was conducted comparing automatic and manual tuning across multiple datasets. Table III and Figure 5 present a detailed comparison of model performance metrics, including accuracy, precision, recall, and F1-score. The results indicate that automated tuning achieves comparable or even slightly superior performance to manually\ntuned models in most datasets, suggesting that AutoMR can effectively optimize hyperparameters without requiring expert intervention. Notably, datasets such as DB4 and LMDHG show slight discrepancies, where manual tuning provides marginal improvements. This suggests that highly unstructured or sensor-specific datasets may still benefit from fine-tuned adjustments."}, {"title": "V. CONCLUSION", "content": "The evaluation of AutoMR across ten datasets demonstrates its effectiveness in automating motion recognition by streamlining data preprocessing, model training, and hyperparameter tuning. AutoMR achieves state-of-the-art performance on eight datasets, highlighting its adaptability across different sensor modalities. However, performance on DB4 and LMDHG remains lower due to the limitations of QuartzNet's 1D convolutional architecture in capturing spatial dependencies and long-range temporal patterns. Addressing these limitations will require integrating 2D CNNs for enhanced spatial feature extraction and hybrid models that combine sequential and spatial analysis. Additionally, transformer-based architectures may further improve performance by capturing long-range dependencies in unsegmented motion sequences. Beyond model enhancements, expanding dataset coverage with diverse sensor modalities and developing adaptive hyperparameter tuning will improve generalizability. Open-source collaboration will further drive refinements, ensuring continuous advancements."}]}