{"title": "Improving vision-language alignment with graph spiking hybrid Networks", "authors": ["Siyu Zhang", "Heming Zheng", "Yiming Wu", "Yeming Chen"], "abstract": "To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "VISION-language (VL) aligned representation is one of the serious challenges within the current multimodal domain, which aims to encode image and text features and map them into a shared space, facilitating robust cross-modal semantic comprehension and interaction. VL-based multi-modal interaction technology has been successfully applied in dialogue systems, autonomous driving, and assisted medi-cal. In recent years, substantial advancements in Transformer and self-supervised learning have promoted the emergence of a large number of methods based on vision-language pre-training (VLP). VLP, as an important training paradigm, promotes cross-modal representation of large-scale image-text pairs. It has a wide range of downstream application potentials, such as Visual Question Answering (VQA), Visual Entailment (VE), Image-Text Retrieval (ITR), and Natural Language Visual Reasoning (NLVR). The core of the VLP paradigm is modal fusion, which includes intra-modal and inter-modal fusion. The fusion mode can be categorized into two types: i) Dual-stream model, which integrates two independent unimodal encoders to process VL information separately. ii) Single-stream model, which aims to learn a unified representation of VL and subsequently input them into the Transformer. This model omits the architectural design of the fusion process in the dual-stream model, which is the strategy adopted by most current VLP models. Early visual embeddings relied on box-level features from predefined detectors. However, this method has some issues that cannot be ignored. First, the coarse box-level representation contains redundant information such as noise or blurred pixels (i.e., overlapping boundaries). Second, the interaction between boxes is prone to interference from irrelevant backgrounds, making it difficult to provide the accurate localization of objects. Third, the local features at the box level are inadequate for predicting the holistic scene of the image, thereby hinder-ing the effective modeling of contextual semantic relations. Recently, some works have tried to exploit grid features as a means to transcend the aforementioned limitations. Subsequently, ViLT is inspired by ViT and introduced patch features as image embeddings. Unlike grid features, image patches are directly linearly projected to improve model efficiency. Although these embedding techniques have made good progress owing to diverse tokenization strategies, they still encounter limitations in semantic understanding. On the one hand, complex image tokenization is different from lan-guage tokens that are discrete and one-dimension arranged. This inherent distribution difference between multimodalities should not be ignored. On the other hand, embedding lengthy visual sequences from high-dimensional space significantly increases the computational burden. Given this, we believe that learning good visual representation is a key factor in aligning VL semantics. In this article, we propose a novel visual semantic encoding module. Ideally, the salient regions and relations of different instances should be presented com-prehensively and clearly to facilitate visual comprehension. To this end, we propose to adopt panoptic segmentation to capture fine-grained image features and use them as vector tokens for subsequent embedding. To model local and global contextual relations, we construct an interwoven Graph Spiking Hybrid Network (GSHN). On the one hand, GSHN possesses a significant sparsity of SNNs and naturally generates sparse attention coefficients for efficient feature aggregation. On the other hand, GSHN involves the potential composition and hierarchical relations of different networks, which is more conducive to capturing deep visual semantic information. Considering the workload of SNN, we employ a contrastive learning (CL) method to achieve the recall of nodes with similar semantics. It has the following two advantages: i) effectively avoids the membrane potential reset problem caused by the input of a single sample. ii) The integrated similar samples can be regarded as a \"video frame\" as the input of SNN, which facilitates enhancing the capacity of SNN to capture spatiotemporal information and speeds up the computational efficiency of the model. Furthermore, we develop a Spiked Text Learning (STL) pre-training task for aligning VL semantic labels, which is adept at refining discrete semantic inputs by learning textual features. We conduct extensive experiments and demonstrate effectiveness on public datasets for multiple VL tasks, including VQA, VE, image-text retrieval, and VR. Ablation experiments further verify the rationality and effectiveness of our model design.\nIn summary, the main contributions of this work are as follows:\n\u2022 We propose an abstract visual-semantic representation module that refines the visual input and encodes rich and compact high-level semantic features.\n\u2022 We design a novel Graph Spiking Hybrid Network (GSHN), which studies the joint learning of SNN-based discrete semantic modules and GAT-based continuous semantic modules, aiming to enforce the intrinsic pro-cessing ability of spatiotemporal information in a robust, hierarchical, and multi-domain manner.\n\u2022 We use contrastive learning to model the semantic com-monalities of similar node inputs, which helps avoid model overfitting on limited data and improves the computational efficiency of the model.\n\u2022 We design a spiked text learning (STL) pre-training task to supervise the output of SNN, thereby facilitating deep semantic mining of text features. Importantly, experi-ments verify the successful application of this method on multiple downstream tasks and achieve competitive results.\nThe rest of this paper is organized as follows. In Section II, we briefly introduce related work. In Section III, we elaborate on the proposed visual semantics module and GSHN. In Section IV, we further analyze and discuss the experimental results. Finally, Section V provides a conclusion and next plans."}, {"title": "II. RELATED WORK", "content": "Both vision and text involve tokenization and embedding processes. However, in contrast to the discrete, concise, and abstract language tokens, visual representations encompass more diverse and intricate information, attributable to their pixel continuity and high dimensionality. To follow text em-bedding, the image tokenization process is typically divided into three stages: i) Region-based, requiring pre-trained ob-ject detectors to extract features. Notably, most VLP models employ Faster R-CNN as the visual embedding. For example, Anderson et al. [6] adopted region features to train VL tasks. ii) Grid-based, wherein some works attempt to partition the image into uniform grids and directly perform convolution op-erations to encode features. For instance, Jiang et al. [7] used grid features to train a VQA model. Huang et al. [8] proposed the Pixel-BERT method, which only randomly trains some pixels to improve computational efficiency. Later on, Huang et al. [12] designed a visual dictionary to encode compact visual features. Although this method is straightforward to imple-ment and does not require a pre-defined object detector, the deepening of the network layers and the redundant information will increase the computational complexity. iii) Patch-based, it obtains features through the linear projection of image patches. Kim et al. [9] proposed a VLP model based on the Vision Transformer (ViT). In addition, some recent large models [13], [14] have adopted ViT as visual embedding. Although the primary advantage of this method lies in its efficiency, the use of fixed-size patches that span across multiple visual regions may ignore the detailed features of object boundaries."}, {"title": "B. VL-aligned representation", "content": "There exists an essential difference in spatial dimensions between vision and language. Achieving a good VL-aligned representation is the core goal of the VLP task. The dual-stream and single-stream modes are the primary structure designs of existing VLP methods. The dual-stream first en-codes independent vision and language features. Then, the Transformer is used to align the VL representation semantic space at a high level. It has various network layers and structure configurations, which help in addressing complex VL tasks. Tan et al. [15] proposed LXMERT to model the relations between two modes. It is a large-scale Transformer model that mainly contains three encoders: object relations, language, and cross-modal. Yu et al. [16] presented a knowledge-enhanced method, named ERNIE-ViL, which endeavors to establish intricate semantic connections across vision and language from scene graphs. Li et al. [17] developed a novel model based on 12 datasets to explore the relations between vision and language. Single-stream directly fuses vision and language at the feature level as input to the Transformer. Gan et al. [18] introduced the VILLA model, which uses adversarial training to achieve VL-aligned representation learning. Huang et al. [12] proposed SOHO, which constructs a visual dic-tionary to generate visual features and achieves cross-modal comprehension by aligning language tokens. Kim et al. [9] proposed a vision and language Transformer (ViLT), which speeds up the training of VLP models by simplifying visual input. In addition, Li et al. [19] proposed SemVLP by mixing two modes, which can jointly align low-level and high-level semantics between image and text representations."}, {"title": "C. Spiking Neural Networks", "content": "The Spiking neural network (SNN) [20] is regarded as the third-generation evolution within the artificial neural network (ANN). It has attracted widespread attention due to its low power consumption and uniqueness in processing sparse data. In recent years, SNNs have been successfully applied to challenging visual tasks such as object detection, recognition, and segmentation. As researchers pursue the development of higher-performance SNNs, more and more novel methods have been proposed. Kim et al. [21] introduced Spiking-YOLO, the first SNN model for object detection through spike-based mechanisms. Subsequently, several works have attempted to explore deep spiking convolutional networks. Lee et al. [22] used spike-based backpropagation to directly train deep con-volutional SNNs, which achieved comparable performance to ANNs in recognition tasks. Hu et al. [23] proposed a spiking version of the deep residual network (ResNet). Furthermore, they provide a compensation mechanism to alleviate the dis-cretization errors accumulated by deep SNNs. Recently, some attempts have been made to extend SNNs to the graph domain. Zhu et al. [24] proposed an end-to-end SpikingGCN, which integrates graph convolution with the fidelity properties of SNN. In this framework, the original graph data is encoded as a sequence of spikes. Some works combine SNN with Transformer. For example, Zhang et al. [25] developed a spike transformer network (STNet) for tracking single objects, which can dynamically extract and fuse spatial and temporal information. Wang et al. [26] proposed the masked spiking Transformer (MST) framework, which utilizes the random spike masking (RSM) method to reduce energy consumption. Zhou et al. [27] first adopted pure SNN to construct a spiking visual Transformer."}, {"title": "III. METHODS", "content": "In this section, we initially present an overview of the proposed GSHN, encompassing key components such as the visual embedding process, continuous semantic encoder, discrete semantic encoder, and hybrid semantic transmission mechanism. Then, we perform a unified VL alignment process and introduce the STL task. The overall training procedure of GSHN is depicted in Algorithm 1."}, {"title": "A. Problem Statement", "content": "Text tokenization can flexibly express more intricate se-mantic information due to the discretization and additivity in its semantic space. In light of this, we argue that it is necessary to construct a discrete image semantic space to bridge the VL semantic gap. Similar to text tokenization, we treat mask instance input as the \"sentence\" instead of a rough hard mapping of the \"word\", which will reflect its intrinsic structural relations by refining the semantic interactions of each mask instance. We introduce an efficient GSHN model, the design of which is motivated by a hybrid architecture of integrating both latent continuous and discrete semantic variables. Specifically, the mask instance after segmentation optimization can be viewed as a graph node, and we first use GAT to encode graph node features to process continuous weight vectors. Then, we use the spike sequence of SNN to process discrete weight computation. Leveraging the spe-cial sparsity, energy efficiency, and robustness of SNN, we combine SNN and GAT to build a visual semantic module for aligned text. This strategy draws on the complementary advantages of hybrid to constrain the information distribution of the original space.\nModeling high-quality hybrid networks is the core goal of this work. Traditional GNNs are mainly used to develop static graphs, while SNNs have received widespread attention due to their good spatiotemporal dynamics and ability to process time-series data. Given the differences in their processing methods, it is not suitable to directly employ GNN-based workloads and evaluation metrics to verify SNNs. To this end, we need to focus on the following two aspects: i) Developing adaptive workloads for SNN, including optimiz-ing input, output, and corresponding pre-training tasks. ii) Enhancing the inductive ability of the GSHN architecture, ensuring it can capture underlying patterns in VL. Fig. 1 presents a comprehensive overview of the proposed model. First, panoptic segmentation is performed on the original image to obtain tokenized input. Then, a dynamic scale-fused GSHN is constructed to encode continuous and discrete node feature representations. Here, we construct a selectable semantic memory unit based on the spike activation output of the SNN and extract the optimized high-level visual semantics by training the information weight ratio. Finally, we perform VL alignment to achieve an end-to-end pre-training task."}, {"title": "B. Visual Embedding", "content": "To represent the completeness and coherence of image scene comprehension, different from previous methods based on rectangular bounding box detection and grid features, we obtain fine-grained image representation based on panoptic segmentation. It is worth noting that panoptic segmentation extracts feature vectors by performing operations at the mask level (i.e., instance shape), which avoids the negative effects brought by irrelevant or overlapping regions around the ob-jects. Specifically, given an image $I$, each instance in the image will generate a candidate segment Mask (i.e., a set of binary masks) after the segmentation module.\n$\\text{Mask} = \\text{Segment}(I)$                                                                                                                     (1)\nwhere $\\text{Mask} = \\{\\text{mask}_1,...,\\text{mask}_L\\}$, $\\text{mask}_i \\in \\{0,1\\}^{H \\times W}$ represents a mask of which pixels belong to an instance. $L$ is the number of masks. $H$ and $W$ are the height and width of the image, respectively. It is worth noting that the Mask contains the thing class predicted by instance segmentation and the stuff class predicted by semantic segmentation. Furthermore, the output category prediction is omitted.\nMeanwhile, inspired by the Convolutional Feature Masking (CFM) method [28], we obtain the feature map of the image from the convolutional layer of a pre-trained CNN and resize the mask to match the size of the feature map. The mask instance feature $V$ obtained by segmentation is expressed as follows:\n$V = \\text{CNN}(I) \\otimes \\text{resize}(\\text{Mask})$                                                                                                        (2)\nwhere $V = \\{v_1, ..., v_k\\}$, $v_i \\in \\mathbb{R}^D$. $k$ represents the number of segmented regions. $\\otimes$ represents an element-wise product opera-tion on each channel of the feature map. Here, $V = [V^t, V^s]$, which contains thing class $V^t\\{v_1,...,v_{k_t}^t\\}$, $v^t \\in \\mathbb{R}^D$ and stuff class $V^s\\{v_1^s, ..., v_{k_s}^s\\}$, $v^s \\in \\mathbb{R}^D$."}, {"title": "C. Graph Spiking Hybird network", "content": "Continuous visual encoder: To preserve rich visual seman-tics, we propose to use a trainable GAT [29] to model contin-uous semantic variables. GAT, as a variant of GNN, aims to design adaptive edge weights of the self-attention mechanism and update its embedding representation by aggregating its neighbor nodes. More specifically, given a graph $G = (X, A)$, it contains the feature vector set $X = [x_1,...,x_n] \\in \\mathbb{R}^{n \\times d}$ of the graph nodes and the adjacency matrix $A = [a_{ij}] \\in \\mathbb{R}^{n \\times n}$. Here, $a_{ij}$ represents the edge weight between nodes $i$ and $j$. $n$ is the number of nodes, and each node has $d$ dimensions. First, for each node $i$ and its neighboring node $j$, the original feature vectors $x_i$ and $x_j$ are transformed. The obtained node feature vectors are expressed as follows:\n$f_i = x_iW, f_j = x_jW$                                                                                                                              (3)\nwhere $W \\in \\mathbb{R}^{d \\times d'}$ represents the learnable weight matrix. Then, the edge attention score calculation is performed and its expression is provided as follows:\n$e(f_i, f_j) = \\text{LeakyReLU}([f_i || f_j] \\cdot a)$                                                                                                 (4)\nwhere $\\text{LeakyReLU}$ represents a nonlinear activation function, $||$ represents the vector concatenation operation on the feature vectors $f_i$ and $f_j$, and $a \\in \\mathbb{R}^{1 \\times 2d'}$ represents a learnable attention parameter vector.\nNext, for each node $i$, a softmax function is applied to normalize all its neighboring nodes $j \\in N_i$, and the attention weight $\\alpha_{ij}$ is defined as:\n$\\alpha_{ij} = \\text{softmax}_j(e(f_i, f_j)) = \\frac{\\text{exp}(e(f_i, f_j))}{\\sum_{j' \\in N_i} \\text{exp}(e(f_i, f_{j'}))}$                                                             (5)\nwhere $N_i$ represents the set of neighboring nodes of node $i$. Finally, the feature vector $f'_i$ of node $i$ is updated using the normalized attention weights and the corresponding neighbor-hood node features.\n$f'_i = \\sigma(\\sum_{j \\in N_i} \\alpha_{ij}f_j)$                                                                                                                        (6)\nwhere $\\sigma(\\cdot)$ represents a nonlinear activation function.\nWithout box-level restrictions, we define an end-to-end learning and updating process from feature input to output. Given the input features $X$ of each batch, the formulated continuous visual encoder $V(\\cdot)$ can be expressed as:\n$F_{GAT} = V(X; \\Theta)$                                                                                                                               (7)\nwhere $F_{GAT} = [f'_1, \\dots, f'_n] \\in \\mathbb{R}^{n \\times d'}$ represents the output feature. $X = [x_1,...,x_n] \\in \\mathbb{R}^{n \\times d}$ represents the input graph node features. $n$ is the number of graph nodes, $d$ and $d'$ represent the dimensions of input and output node features, respectively. $\\Theta$ denotes the parameter set for training GAT.\nDiscrete semantic encoder: Different from the basic in-formation captured by continuous weight calculation in GAT, we model a discrete semantic encoder that facilitates learning inductive biases (unseen nodes or links in the graph) to generate predictions. SNNs are bio-inspired neural networks with great research potential due to their sparsity and low power consumption. SNNs utilize discrete pulses (termed spikes) rather than continuous values as carriers of information transmission and only emit a pulse when the membrane potential of the neuron reaches the threshold to transmit binary spike sequences (i.e., 0 or 1) and asynchronous information. In this work, SNN is incorporated into the model architecture design, aiming to better exploit spatiotemporal properties and sparsity to learn multi-network patterns with a high degree of freedom fusion. The update of membrane potential usually follows the popular Leaky Integrate-and-Fire (LIF) [30], which effectively simulates the spiking behavior of neurons. This method accounts for the incremental variation in input current over time, and its behavior is described as:\n$s = \\text{argmax}(U_{r2}, (U_t - U_{r1} + \\Delta U_w))$                                                                                                           (8)\nwhere $U_{r2}$ is the signed reset potential. $U_t$ represents the membrane potential $U$ at a given time step $t$. $U_{r1}$ is the resting potential and $\\Delta U_w$ represents the post-synaptic input. When the threshold $U_{r2}$ is exceeded by the membrane potential $U$, $s=1$, otherwise $U$ is immediately reset, i.e., $s = 0$. The corresponding soft activated is defined as follows:\n$\\sigma = (U_t - U_{r1} + \\Delta U_w) - U_{r2}$                                                                                                                           (9)\nHere, the tanh function is selected as the surrogate function, which can effectively alleviate the instability factors that occur when the pulse function performs gradient descent.\nSNN needs to respond to events to process asynchronous information, which is different from FGAT to fix the infor-mation at each location to achieve synchronous information processing. Since FGAT encodes continuous variable values, while SNN accepts discrete spike inputs. To solve the feature propagation problem of SNN to deploy discrete semantic encoders, we use a popular method of probabilistic sampling to encode the input FGAT into a discrete spike sequence. The discrete semantic encoder $H(\\cdot)$ can be formalized as follows:\n$S = \\text{Acc}_{i=1}^b H(P_{\\text{binary}, i}; \\check{S}_i)$                                                                                                            (10)\nwhere $S = [S_1,..., S_b] \\in \\mathbb{R}^{n \\times d''}$ represents the sparse spike sequence. Acc means executing the accumulation operation of $P_{\\text{binary}}$ from $i = 1$ to $b$ into the membrane potential, $P_{\\text{binary}} \\in \\mathbb{R}^{n \\times dx\\{0,1\\} \\times T}$. $T$ means the time window size. $\\check{S}_i$ denotes the parameter set for training SNN. To better obtain discrete semantic features, STBP-tdBN [31] is adopted to assist neuron training, which helps accelerate model conver-gence."}, {"title": "D. Model Input and Output", "content": "Encoder Input: In this work, we use contrastive learning to capture similar graph node representations as model input, which is different from random sampling. To obtain node features with similar semantics, we need to reorder the samples with the highest matching scores in a batch calculated by ITM [33], and then use the retrieve function to recall the samples. During the training process, we pre-freeze the SNN training parameters and adaptively configure epochs to ensure that only the input order is adjusted without changing the total amount of model calculation. For the continuous visual encoder, we assume that each graph is an independent sample, which is independent of other graphs. During the training of GAT, multiple graphs are set to be combined into a batch for synchronous calculation. This not only helps to improve computational efficiency but also allows the model to learn commonalities between different graphs. For the discrete semantic encoder, the input in a batch is a converted spike sequence. These spike sequences can form a visual stream with similar semantics, which as a time dimension can continuously learn events to respond to pulses, thereby realizing information transfer between graph nodes. In addition, the reorganization graph can be regarded as a hard sample sampling process, which is beneficial to the representation learning of semantics.\nEncoder Output: For SNN, the activation state of its neu-rons is accumulated, which enables SNN to remember previ-ous input information and process historical information in the time dimension. Since SNN does not reset to the initial state after each forward propagation, we need to focus on discussing reasonable output methods rather than precise predictions. In addition, SNNs based on brain-like models have difficulty competing with GATs based on continuous computation in classification tasks, which shows that it is not appropriate to directly sum or concatenate the continuous and discrete variables of the output. Given the binary nature of spike events, SNNs prefer \"yes or no\" rather than \u201cwhat\u201d questions. To this end, we view SNNs as a selector that screens basic semantic features by activating continuous visual streams. Inspired by [12], we define a flexible semantic memory unit $M \\in \\mathbb{R}^{d' \\times d''}$ in combination with SNN, which can freely combine these basic semantics to provide more diverse features. Note that M is similar to a container rather than a fixed dictionary that can only perform one query. Discrete semantic information representation can be formalized as:\n$E_{SNN} = \\sum_{j=1}^{d''} M_j * \\frac{\\sum_{i=1}^b S_i}{i}$                                                                                                                        (11)\nwhere $E_{SNN} \\in \\mathbb{R}^{n \\times d''}$, $M_j$ represents the $j$th basic semantic feature, and S is the pulse feature. Here, SNN only needs to perform basic semantic feature learning within a batch. This strategy is better than continuous calculation. Since SNN is binary, when the integration time window is set to 1, the complex multiplication operation between input and weight can be omitted to achieve lower energy consumption.\nHybrid transmission: The proposed GSHN decouples two important structures, continuous vision, and discrete seman-tics. Fig. 2 provides the detailed framework of GSHN. When the model performs the training within a batch, the continuous output of GAT encoding needs to calculate its total information at once. Unlike SNN with spatiotemporal properties, it needs to accumulate input activation properties in the time dimension to reduce the time step of each graph spike sampling, which is more advantageous than the previous reset process. However, it is crucial to effectively fuse the output differences between the two. To this end, we design an information transfer unit with intermediate representation, which not only integrates the heterogeneous properties of GAT and SNN but also realizes information optimization with dynamic proportional fusion to bridge the gap between the two. More specifically, we first need to build a squeeze module for the continuous output $F_{GAT}$. Then, we perform a fully connected mapping to obtain the information weight ratio $r$. Next, we perform the excitation calculation on the $E_{SNN}$ with the assigned weight $r$ within each batch, thereby obtaining the final discrete semantic rep-resentation. Finally, an addition operation is performed on the continuous semantic $F_{GAT}$ and the discrete semantic $E_{SNN}$ to transmit the mixed information flow and input it into the pre-training task as the final image feature vector. The output of the GSHN can be formalized as:\nr = \\sigma\\cdot \\varphi(F_{GAT})                                                                                                                              (12)\nV' = r * E_{SNN} + F_{GAT}                                                                                                                                       (13)\nwhere $\\sigma(\\cdot)$ and $\\varphi(\\cdot)$ denote the sigmoid and fully connected (FC) functions, respectively."}, {"title": "E. Vision-language alignment", "content": "For the pre-training process, we implement a medium-sized multi-layer Transformer to model the joint representation of VL features, which aims to maximize the matching of the interactive features of abstract visual representation and high-level language representation, narrowing the semantic gap in the feature space. For language embedding, we adopt the BERT [32] tokenizer to tokenize text prompts. For image embedding, we propose GSHN based on the text processing method, which maps the segmented mask instances into fine-grained \"sentences\" instead of simple \"words\". We record the position of the visual output and encode it in the same way as BERT, thus obtaining VL-aligned token masks and token embeddings. We perform multiple pre-training tasks, including the existing MLM, ITM, and CL, evaluating the usefulness of GSHN. In addition, when masking a certain mask feature region, the SNN triggers neurons to produce a multi-class label, which helps our proposed STL learning. Note that we only use the text features encoded by the Transformer for prediction, aiming to better align the sparse visual and textual semantics.\nWe design the STL pre-training task, which performs super-vised learning on the discrete output of the SNN and corrects its output. Essentially, the input of the STL task is the partial signal of the SNN and the text vector features, which are used to predict the SNN signal corresponding to the masked partial mask instance region. This process is similar to MLM, which assists images as candidates through text semantics. The specific masking rules are as follows:\n$SNN_{prob=0.05} = STL(S, \\text{others}, W)$                                                                                            (14)\nwhere S represents the output spike sequence. W represents the input word, which is defined as W = \\{W_1,..., W_L\\}. L represents the length of the text. Here, considering that the dimension of the semantic memory unit is 3000, we set a 5% probability to mask the output of SNN.\nFor multi-label classification tasks, we regard it as a binary vector Y, where each element corresponds to a category. Once a dimension is activated, it is classified under the pertinent category, with the element at its corresponding position being allocated a value of Y = 1. Conversely, the assigned value is Y = 0. To assess the difference between the true label and the predicted result Y, we use focal loss to reduce the weight of easy-to-classify samples, so that the model pays more attention to hard samples. The calculation expression of the loss function is given by:\n$L_{STL} = -\\frac{1}{NW} \\sum_i \\sum_j Y_{i,j}log(\\hat{Y}_{i,j}) + (1 - Y_{i,j})log(1 - \\hat{Y}_{i,j})$                                                (15)"}, {"title": "IV. EXPERIMENTS", "content": "For the text backbone, we follow BERT to tokenize the text using the WordPiece tokenizer. We use the AdamW optimizer with an initial learning rate of $1 \\times 10^{-4}$ and a weight decay of $1 \\times 10^{-2}$ for optimization. The number of training iterations is 1000 and the warm-up ratio is set to 1/3. For the visual backbone, we set the image input size to 640$\\times$640 to maintain a fair comparison. We use ImageNet [35] to initialize the underlying encoder, which uses ResNet-101 as the visual embedding backbone architecture to extract image features. We use DeepLab [36] and Mask R-CNN [37] for semantic segmentation and instance segmentation respectively, and merge their outputs to perform mask instance feature extraction jointly. Note that the output features determine its size. The non-maximum suppression (NMS) threshold for Mask R-CNN is set to 0.5, and its confidence threshold is set to 0.6. Here, we choose SGD as the optimizer, with a learning rate of $1 \\times 10^{-2}$ and a weight decay of $5 \\times 10^{-4}$. For SNN, we set the momentum of SGD to 0.9 and the initial learning rate to $1 \\times 10^{-2}$. The semantic memory capacity is set to 3000 and the dimension of each word is 768. The pre-trained Transformer architecture is composed of 12 stacked Transformer encoder layers, each of which contains two sub-layers, namely the multi-head self-attention layer and the feed-forward network, where the number of heads is 8. Each sub-layer is a residual connection structure followed by layer normalization. In addition, we adopt ITM, MLM, and CL to implement preprocessing, where MLM is applied to positive image-text pairs. To learn continuous visual features, we freeze the SNN parameter part for the first 10 epochs and execute the Transformer. Subsequently, we use CL to process batches of graph nodes to extract similar semantic information and train GSHN using STL. Our experiments are conducted on a server equipped with 8 NVIDIA A100 GPUs, and the model executes 40 epochs to converge during pre-training."}, {"title": "B. Pre-training Datasets", "content": "To demonstrate the effectiveness and generalization ability of our method, several popular datasets are used to facil-itate VL pre-training for multiple downstream applications, including VQA, VE, NLVR2, and ITR. The distribution of downstream datasets is summarized in Table I. We pre-train on VG [38] and MSCOCO [39], which are typical in-domain datasets commonly used in many VL tasks. It is worth noting that we only use part of the VG and MSCOCO datasets for training to effectively avoid the data leakage problem.\nThe VQA task aims to infer the correct answer by compre-hending the natural language questions related to the image. It includes the currently popular VQAv2, VQA-CP v2, and GQA datasets. VQA-CP v2 is a restructured version of VQAv2, which changes the distribution of answers for each question type between the training set and the test set. They define questions into three types, namely \u201cYes/No\u201d, \u201cNumber\u201d and \"Other\". Compared with other datasets, GQA requires less language bias and multi-step reasoning, which is more helpful for evaluating visual reasoning ability. VE can be regarded as a three-classification problem, which is used to predict the relations between images and text (i.e., implicit, neutral, or contradictory). The SNLI-VE dataset is used to train the VE task. NLVR is a complex visual reasoning task that challenges the model's ability to judge whether a sentence truly describes a visual scene, covering object collection, comparison, and comprehension of spatial relations. We use the NLVR2 dataset. ITR retrieves relevant samples from another modality given an expression in one modality. It usually includes two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR). Here, we choose the Flickr30K and MSCOCO datasets for testing."}, {"title": "C. Ablation Study and Analysis", "content": "We conduct ablation experiments on different downstream VL tasks to evaluate the contribution of each module in GSHN, where ResNet-101 and 3-layer Transformers are used to save computational resources.\nInformation weight ratio. To analyze the impact of the Information weight ratior, we simulated two states, i.e., \"with r\" and \"without r\". As shown in Table II, the model performance of \u201cwithout r\u201d is lower than that of \u201cwith r\u201d. For the model \"without r\", the discrete semantic inputs within a batch are the same. This shows that our discrete semantic encoder focuses on the semantic content itself, but ignores the \u201cNumber\u201d issue. In contrast, the model \u201cwith r\" uses fewer parameters to complete the intra-modal semantic interaction, which is suitable for the design of question types \"Other\" and \"Yes/No\". The weight ratio set in this paper redistributes information on discrete semantics with commonality, thereby capturing subtle differences between similar nodes.\nEmbedding manner. Table III explores the impact of different visual detection methods on model performance, covering bounding box detection, grid detection, and the panoramic segmentation embedding strategy proposed in our study. Experimental results show that panoramic segmentation has significant advantages in various VL tasks. Since this method can effectively represent the fine-grained semantics of image content, it has positively improved model performance.\nHybrid transmission. We use contrastive learning"}]}