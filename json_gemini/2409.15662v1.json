{"title": "Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting", "authors": ["Wenbo Yan", "Ying Tan"], "abstract": "Spatial-temporal graph neural networks (STGNNs) have achieved significant success in various time series forecasting tasks. However, due to the lack of explicit and fixed spatial relationships in stock prediction tasks, many STGNNs fail to perform effectively in this domain. While some STGNNs learn spatial relationships from time series, they often lack comprehensiveness. Research indicates that modeling time series using feature changes as tokens reveals entirely different information compared to using time steps as tokens. To more comprehensively extract dynamic spatial information from stock data, we propose a Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer (DPA-STIFormer). DPA-STIFormer models each node via continuous changes in features as tokens and introduces a Double Direction Self-adaptation Fusion mechanism. This mechanism decomposes node encoding into temporal and feature representations, simultaneously extracting different spatial correlations from a double path approach, and proposes a Double-path gating mechanism to fuse these two types of correlation information. Experiments conducted on four stock market datasets demonstrate state-of-the-art results, validating the model's superior capability in uncovering latent temporal-correlation patterns.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series data is ubiquitous in various aspects of life, including healthcare, transportation, weather, and finance, where a substantial amount of time series data is generated daily. If historical data can be utilized to uncover potential patterns and predict future changes, it could greatly enhance our quality of life. Initially, it was assumed that time series data is generated from certain processes, leading to the development of models such as Box and Jenkins [2], Engle [6] to approximate these processes. In recent years, numerous deep learning methods have been applied to the field of time series forecasting. To better model time series data, models based on recurrent neural networks (RNNs), such as Qin et al. [15], as well as models based on convolutional neural networks (CNNs), such as Dai et al. [5], have been proposed. With the rise of the Transformer model in the field of natural language processing, Transformer-based time series forecasting models, such as Zhang and Yan [24], have been introduced. As research has progressed, it has been found that for certain time series problems, such as traffic and climate forecasting, employing spatial-temporal prediction methods can better predict future changes. Consequently, many spatial-temporal graph neural networks (STGNNs) have been proposed and have achieved excellent results in fields like transportation. Moreover, to address the issue that Transformer-based methods cannot utilize spatial relationships, Spatial-Temporal Transformers (STTransformers) such as Lim et al. [13], Xu et al. [21] have been proposed.\nIn fact, these time series modeling methods often fail to achieve satisfactory results in stock market prediction. Firstly, whether based on RNNs or temporal convolution methods, they typically focus on modeling the time series of individual nodes independently, without considering the interdependencies between different time series. However, in the stock market, the entire market functions as an integrated whole, and there are significant interactions between the time series of different stocks at any given moment. Therefore, ignoring the correlations between different nodes fails to effectively address the problem of stock prediction. Secondly, for Transformer-based methods, since the features of time series often lie in the variations between time steps and each time step itself contains low information content or even no actual meaning, using time steps as tokens to model time series with Transformers often results in poor performance.\nWhile STGNNs can effectively utilize the correlations between nodes and integrate changes in other time series for comprehensive modeling, most spatial-temporal graph neural networks require predefined adjacency relationships, such as spatial distances in climate"}, {"title": "2 PRELIMINARY", "content": "We first define the concepts of Spatial-Temporal Forecasting Problem. The spatial-temporal forecasting problem, refers to the task of using historical T time steps data $X \\in \\mathbb{R}^{N \\times T \\times F}$ and correlation adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$ to predict future values for t time steps $Y \\in \\mathbb{R}^{N \\times t}$. The adjacency matrix can be predefined or learned by the model, as in our approach. For each sample, there are N nodes, and each node has a time series $X_i$, where $X_i$ contains T time steps, and each time step has F features. Additionally, the correlation adjacency matrix A indicates the degree of correlation between the nodes, where $a_{ij}$ represents the correlation degree between node i and node j. The neighbors set of node i is represented as $K = \\{j | j \\neq i \\text{ and } a_{ij} \\neq 0\\}$."}, {"title": "3 METHOD", "content": "In this section, we present our approach. The precise architecture of the DPA-STIFormer is illustrated in Fig. 1.\n3.1 Overall\nThe model is composed of multiple encoder blocks called Double-Path Adaptive-correlation Inverted Encoder (DPA-IEncoder) and a single Decoder Block with Decomposed Fitting (DF-Decoder) for stock prediction. Each DPA-IEncoder consists of an Inverted Temporal Block, a Double-Path Adaptive-correlation Block, and two feedforward neural networks, all interconnected by residual connections. The Inverted Temporal Encoder processes the time series $x_i$ for an individual node and the Double-Path Adaptive Correlation Encoder concurrently aggregates information from both temporal and feature dimensions, adaptively learns the relationships among neighboring nodes, and integrates the encoding results through a gating mechanism. Finally, the DF-Decoder demonstrates high capacity in stock prediction with proposed decomposed fitting.\n3.2 Inverted Temporal Block\nIn many Transformer-based time series prediction approaches, features at the same time step are treated analogously to words in natural language processing. However, these methods often yield suboptimal performance. This is due to the inherently low information-to-noise ratio in time series data and the potential insignificance of individual time steps. Crucially, information is typically embedded in the temporal evolution of features rather than in isolated time points. Inspired by Itransformer[14] and crossformer [24], we no longer consider features at the same time step as tokens, but rather the change of the same feature over time as a token. Thus, for the time series $x_i \\in \\mathbb{R}^{T \\times F}$ in each node, We first Invert $x_i$ to get $x_i^I \\in \\mathbb{R}^{F \\times T}$, rendering the temporal sequence of each feature as a token.\n$x_i^I = invert(x_i), for i = 1, 2, ..., n$\nwhere invert() stands for matrix transpose. Furthermore, considering the diverse features contained within the same node, it is evident that each feature exerts a different degree of influence on the node's variation, due to their distinct meanings. We introduce importance"}, {"title": "3.3 Double-Path Adaptive-correlation Block", "content": "Double-path Adaptive-correlation Block (DPABlock) is designed to adaptively model the correlations between nodes. It comprises three main components: Double Direction Self-adaptation Fusion, N-neighbor Adaptive Correlation Attention, and Double-Path Gating Mechanism.\n3.3.1 Double Direction Self-adaptation Fusion. For spatial-temporal prediction problems, each node is often featured as a two-dimensional time series $x_i \\in \\mathbb{R}^{T \\times F}$. The computational cost can become prohibitively large if all features are used to learn the correlations between nodes. A common approach is to use the features from the nearest time step or the mean of each time step as the node features. However, this approach has significant limitations. On the one hand, simple averaging over time steps does not provide a robust representation of the nodes. Different features hold varying levels of importance at different time steps, and averaging may dilute these distinctions, rendering the features less meaningful. On the other hand, many features of a time series are embedded in its successive changes, and aggregating multiple series into a single one may result in information loss compared to averaging over time steps.\nWe designed a double direction adaptive information fusion method as shown in Fig. 1. We first invert $Z^I$ again to obtain Z,\n$Z = [Z_1, Z_2, ..., Z_n]$\nThen, we respectively map $Z^I$ and $Z$ to different spaces through fully connected layers\n$Q_F = W_Q^{I, F}Z, K_F = W_K^{I,F}Z$\n$Q_T = W_Q^{I,T}Z^I, K_T = W_K^{I,T}Z^I$"}, {"title": "3.4 Decoder and decomposed fitting for stock prediction", "content": "After processing through several Encoder Blocks, the model comprehensively extracts and integrates temporal and spatial information, represented as $M \\in \\mathbb{R}^{N \\times F \\times dg}$. Given that we have inverted the input, the encoding process is no longer sequential. Consequently, we deviate from the traditional transformer decoding approach. Instead, we employ a Multilayer Perceptron (MLP) to map the encoded information directly to the prediction target. Initially, we integrate information from various features:\n$M_w = softmax(W_I M)$\n$M_m = Sum(M_w.M)$\nwhere $W_I$ is an N\u00d71 mapping matrix. By applying the softmax(.) function and the $W_{I d}$, each feature is assigned a weight that reflects its importance. Then aggregates all features to form a new mixed encoding $M_m \\in \\mathbb{R}^{N \\times dg}$.\nFor stock prediction problems, the mean of each stocks may be relatively stable over a period. Each time step can be seen as a deviation from this mean. We use MLP to fit both the mean and deviation of each time series, and let the output be the sum of the"}, {"title": "3.4.1 Loss", "content": "To attain high performance in time series prediction tasks, we consider both the accuracy of the predictions and the correlation between predictions across time steps in the loss function. Initially, we utilize Mean Squared Error loss $L_{mse}$ as the loss function to evaluate the accuracy of the predictions and employ the Pearson correlation coefficient loss $L_{pearson}$ to gauge the correlation of predictions within a time step.\nThe final form of the loss function is expressed as follows:\n$L = \\lambda_m L_{mse} + L_{pearson}$\nwhere $\\lambda_m$ denotes the weight of $L_{mse}$ which is usually set to 0.1 to ensure that the two loss functions are of the same order of magnitude."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments to demonstrate the effectiveness of our method.\n4.1 Experimental Setup\n4.1.1 Datasets. We conducted experiments on four datasets: CSI500, CSI1000, NASDAQ, NYSE. Brief statistical information is listed in Table 1. Detailed information about the datasets can be found in the appendix.\n4.1.2 Baseline. We choose LSTM-based neural networks: FC-LSTM[8], TPA-LSTM[16], spatial-temporal graph neural networks: ASTGCN [9], MSTGCN [10], MTGNN [19], STEMGNN [3], STGCN [22], DCRNN[12] and transformer-based networks: Itransformer[14],Crossformem[4] as baseline to compare the performance of the models in all directions."}, {"title": "5 RELATED WORK", "content": "5.1 Spatial-Temporal Graph Neural Networks\nIn recent years, there has been a burgeoning interest in integrating temporal and spatial information within time series data, leading to the development of a series of models known as Spatial-Temporal Graph Neural Networks. STGCN [22] was the first to introduce Graph Convolutional Neural Networks (GCNs) [11] into time series forecasting. Subsequently, additional convolution-based models, such as Graph Wave Net [20], MT-GNN [19], StemGNN [3], H-STGCN [4], and GSTNet [7], have been proposed. These models incorporate various gating mechanisms atop convolutions to enhance feature capture. Concurrently, other studies have focused on more complex convolutional structures, such as ST-GDN [31] and ST-ResNet [23], which achieve improved performance through sophisticated architectural designs and mechanisms.\nMoreover, some works, including ARGCN [1], DCRNN [12], and TGCN [25], combine Recurrent Neural Networks (RNNs) with Graph Neural Networks (GNNs). Additionally, with the advent of Transformers, many models have integrated transformer architectures or attention mechanisms into spatial-temporal modeling, as evidenced by ASTGCN [9], STGNN [18], and GMAN [26].\n5.2 Spatial-Temporal Transformer\nRecent advancements in spatial-temporal modeling have seen the integration of Transformers to capture complex dependencies across time and space. Notable models such as the TFT [13] and the STTN [21] have demonstrated the efficacy of combining temporal and spatial attention mechanisms. These models leverage the self-attention mechanism to effectively handle the intricacies of spatial-temporal data, thereby improving forecasting accuracy and interpretability."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposes a novel DPA-STIFormer. The Inverted Temporal Block models the temporal sequences of each node by treating features as tokens and introduces importance weights to enable the attention mechanism to consider the significance of features. The Double-Path Adaptive-correlation Block is introduced to model the correlations between nodes. The Double Direction Self-adaptation Fusion adaptively blends temporal and feature perspectives from the node embeddings, modeling inter-node correlations from both paths simultaneously. Finally, the proposed Double-path Gating Mechanism integrates the encodings from both paths. Extensive experiments on four real-world stock datasets, along with visualization of the learned correlations, demonstrate the superiority of our approach."}, {"title": "A Dataset", "content": "\u2022 CSI500: CSI500 is a stock dataset that contains the performance of 500 small and medium-sized companies listed on the Shanghai and Shenzhen stock exchanges. It contains daily frequency data for 500 stocks, with a total time step of 3159 and a feature number of 45.\n\u2022 CSI1000: CSI1000 contains daily frequency data for 1000 stocks, with a total time step of 3159 and a feature number of 45.\n\u2022 NASDAQ: NASDAQ contains daily frequency data for 1026 stocks in National Association of Securities Dealers Automated Quotations, with a total time step of 1225 and a feature number of 5.\n\u2022 NYSE: NYSE contains daily frequency data for 1737 stocks in New York Stock Exchange, with a total time step of 1225 and a feature number of 5."}, {"title": "A.1 Model Implementation", "content": "We set the time window to 30, meaning each moment looks back at the past 30 time steps. The dimensions of the feedforward neural network and all attention mechanisms are set to 256. The number of attention heads is set to 4. For the CSI500 and CSI1000 datasets, 3 layers of DPA-IEncoder are used. For the NASDAQ and NYSE datasets, which have less data, only 1 and 2 layers of DPA-IEncoder are used."}, {"title": "B Experimental environment", "content": "We ran the experiment on a server containing 4 NVIDIA GeForce RTX 3090 and AMD EPYC 7282 16-Core Processor, repeating each experiment five times and averaging the results."}, {"title": "C Metrics", "content": "\u2022 IC: the Pearson correlation between predicted ranking scores and real ranking scores, widely used to evaluate the performance of stock ranking.\n$IC = \\frac{Cov(\\hat{Y}, Y)}{\\sigma_{\\hat{Y}}\\sigma_{Y}}$\n\u2022 PNL: the aggregate profits and losses of trading strategies, i.e., the cumulative returns.\n$PNL = \\sum_{n=1}^{N_d} r_n$\n\u2022 AR: the annualized returns of trading strategies, denoting the profitability of portfolios. In the experiments, the number of trading days in one year is assigned as 240.\n$AR = \\frac{240}{N_d} \\sum_{n=1}^{N_d} r_n \\times 100\\%$\n\u2022 VOL: the annualized volatility of portfolio returns, indicating the stability of portfolios.\n$VOL = \\sigma(R) \\times \\sqrt{240} \\times 100\\%$\n\u2022 MDD: a commonly used metric to measure risk control ability, which is the largest decline in portfolios from peak to"}, {"title": "C.1 Losses", "content": "MSE Loss:\n$L_{mse} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i - \\hat{Y_i})^2$\nPearson Correlation Coefficient Loss:\n$L_{pearson} = \\frac{\\sum_{i=1}^{N} (Y_i - \\bar{Y}) (\\hat{Y_i} - \\bar{\\hat{Y}})}{\\sqrt{\\sum_{i=1}^{N} (Y_i - \\bar{Y})^2 \\sum_{i=1}^{N} (\\hat{Y_i} - \\bar{\\hat{Y}})^2}}$"}]}