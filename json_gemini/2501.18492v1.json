{"title": "GuardReasoner: Towards Reasoning-based LLM Safeguards", "authors": ["Yue Liu", "Hongcheng Gao", "Shengfang Zhai", "Jun Xia", "Tianyi Wu", "Zhiwei Xue", "Yulin Chen", "Kenji Kawaguchi", "Jiaheng Zhang", "Bryan Hooi"], "abstract": "As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-40+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner\u00b9.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Reid et al., 2024; Team, 2024a; Jiang et al., 2024; Dubey et al., 2024; Hui et al., 2024; Liu et al., 2024a) are revolutionizing numerous domains, including chatbots (OpenAI, 2022), search engines (OpenAI, 2024), and software engineering (CognitionAI, 2024). As these AI-powered tools become more deeply integrated into our daily lives, it is crucial to ensure their safety and reliability. However, recent attacks (Guo et al., 2024; Formento et al., 2023; Liu et al., 2024c) show their susceptibility to malicious manipulation.\nTo alleviate this issue, companies have developed guard models, e.g., OpenAI Moderation (Markov et al., 2023),"}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Safety Alignment of LLM", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Reid et al., 2024; Dubey et al., 2024; Team, 2024a) showcase"}, {"title": "2.2. Guard Models for LLMS", "content": "In contrast to safety alignment on the LLM itself, guard models introduce a separate model designed to moderate the input and output of LLMs to filter out unsafe content. Existing guardrails can be roughly categorized into three types as follows. 1) Traditional guard models adopt statistical techniques such as k-nearest neighbors (Yuan et al., 2024) and Beta regression (Tan et al., 2021). 2) Closed-Source guard APIs are created by industrial companies for commercial use, e.g., OpenAI (Markov et al., 2023), Perspective (Lees et al., 2022), Detoxify (UnitaryAI, 2024), Azure (Azure, 2024). They can be implemented by fine-tuning (Markov et al., 2023) or prompting LLMs (Kumar et al., 2023; Ma et al., 2023a; Rebedea et al., 2023) like GPT-40. 3) Open-Source guard models, including ToxicChat-T5 (Lin et al., 2023), ToxDectRoberta (Zhou, 2020), LaGoNN (Bates &\nGurevych, 2023), the LLaMA Guard series (Inan et al., 2023; Dubey et al., 2024), Aegis Guard series (Ghosh et al.,\n2024a;b), WildGuard (Han et al., 2024), ShieldGemma\n(Zeng et al., 2024), are open-weight LLMs fine-tuned on\nthe red-teaming data. Liu et al. (2024b) analyzes the calibra-\ntion of guard models, while Zheng et al. (2024a); Sawtell\net al. (2024); Wang et al. (2024a); O'Neill et al. (2024)\nfocus on lightweight guard models. Kang & Li (2024) de-\nvelops a robust guardrail R2-Guard via logical reasoning.\nIn addition, guard models have also become a hot topic for\nmultimodel models (Du et al., 2024; Chi et al., 2024; Wang\net al., 2024b) and agents (Xiang et al., 2024). Our GuardRea-\nsoner falls into the third category, i.e., open-source guard"}, {"title": "2.3. Reasoning Ability of LLM", "content": "The ability to reason is crucial for LLMs, allowing them to mimic human-like thinking patterns. Pioneering work (Wei et al., 2022; Kojima et al., 2022) achieves this by prompting LLMs to think step-by-step. In addition to this approach, frameworks like self-correction (Kumar et al., 2024), self-critique (Ke et al., 2023), debate (Liang et al., 2023; Du et al., 2023), and plan-and-solve (Wang et al., 2023) enhance reasoning abilities. Ma et al. (2023b) explores the influence of code data on the reasoning ability of LLMs during training. Furthermore, efforts like (Hao et al., 2024;\nGoyal et al., 2023) aim to transition the thinking process\nof LLMs into the latent space. OpenAI has developed the\no1 model by teaching LLMs to reason effectively, showcas-\ning the potential for improvements through test-time scal-\ning. Following OpenAI, QwQ (Team, 2024c), QvQ (Team,\n2024b), DeepSeek (Team, 2025a), Kimi (Team, 2025b) de-\nvelop o1-like reasoning models. Furthermore, OpenAI's\n03 is announced to achieve promising performance on the\nARG-AGI benchmark ARC-AGI (2024). (Chen et al., 2024)\ndiscusses the overthinking problem of o1-like models."}, {"title": "3. GuardReasoner", "content": "This section outlines the methodology of GuardReasoner. Specifically, we begin by defining the guardrail tasks. Then, we introduce the R-SFT and HS-DPO training approaches. The overview of GuardReasoner is illustrated in Figure 2.\nTask Definition. Given a target LLM F, a user inputs a prompt X and receives a response S = F(X). The guard model G is designed to moderate the input and output of the LLM, and to detect whether the LLM has refused the request, i.e., (\u0176prom., \u0176res., \u0176ref.) = G(X, S), where \u0176prom. \u0404 {harmful, unharmful} is the predicted label for the prompt harmfulness detection task, Pres. \u2208 {harmful, unharmful} is the predicted label for the response harmfulness detection task, and \u0176ref. \u2208 {refusal, compliance} is the predicted label for the refusal detection task. The performance of G is evaluated using F1 score between Y and Y. In harmfulness detection tasks, harmful/unharmful samples are treated as positives/negatives. In the refusal detection task, refusal/compliance samples are treated as positives/negatives."}, {"title": "3.1. Reasoning Supervised Fine-tuning", "content": "To unlock the reasoning ability of the guard model, we first synthesize the reasoning data and then perform reasoning supervised fine-tuning (R-SFT) on the base model Mbase."}, {"title": "3.2. Hard Sample Direct Preference Optimization", "content": "To further enhance the reasoning ability of the guard model, we first select the hard samples and then conduct hard sample direct preference optimization (HS-DPO) on MR-SFT."}, {"title": "4. Experiments", "content": "Environment. All experiments are conducted on 2 servers with 4 56-core Intel(R) Xeon(R) Platinum 8480CL CPUs, 2T RAM, and 8 NVIDIA H100 (80GB) GPUs. We use the LLAMA Factory (Zheng et al., 2024b) training platform.\nBenchmark. We use 13 guardrail benchmarks, including 6 prompt harmfulness detection benchmarks (ToxicChat (Lin et al., 2023), OpenAIModeration (Markov et al., 2023), AegisSafetyTest (Ghosh et al., 2024a), SimpleSafetyTests\n(Vidgen et al., 2023), HarmBench (Mazeika et al., 2024),"}, {"title": "4.1. Performance", "content": "We compare GuardReasoner with 22 baselines on 13 benchmarks across 3 tasks and obtain several findings as follows.\n(I) In the prompt harmfulness detection task, as shown in Table 2, our GuardReasoner 8B achieves the best performance with an average F1 score of 81.09%, surpassing both the open-source guard model runner-up by 3.10% and the closed-source guard API runner-up by 3.09%. Among the benchmarks, our GuardReasoner improves the perfor-\nmance more significantly on the benchmarks with adver-\nsarial prompts, e.g., 5.36%\u2191 on ToxicChat. It indicates"}, {"title": "4.2. Ablation Study", "content": "We conduct ablation studies of our GuardReasoner on 3 guardrail tasks. As shown in Table 3, \"Baseline\" denotes the guard model trained with only the WildGuardTrain dataset (Han et al., 2024). \u201cBaselinemix\u201d denotes the guard model trained with a mix of the seed datasets (Han et al., 2024;\nGhosh et al., 2024a; Lin et al., 2023; Ji et al., 2024b). \u201cR-\nSFT\u201d denotes the guard model trained via R-SFT on our\nsynthesized reasoning data GuardReasonerTrain. \"R-SFT\nw. HS-DPOself\u201d represents the guard model firstly trained\nvia R-SFT, then via HS-DPO on Hself while \"R-SFT w. HS-\nDPOensemble\" represents the guard model firstly trained via\nR-SFT, then via HS-DPO on Hensemble. From the results in\nTable 3, we obtain the conclusions as follows.\n(I) \"Baseline-Mix\" achieves a comparable performance with\n\"Baseline\", suggesting that mixing the conventional train-\ning datasets does not lead to significant performance im-\nprovement. (II) \u201cR-SFT\u201d achieves better performance than\n\"Baseline-Mix\" by constructing the reasoning training data\nand conducting R-SFT. For example, on 1B models, \"R-\nSFT\" surpasses \u201cBaseline-Mix\u201d by 6.30% F1. It verifies\nthe effectiveness of the GuardReasonerTrain dataset and\nR-SFT. (III) \u201cR-SFT w. HS-DPOself\u201d further improves the\nperformance of \u201cR-SFT\u201d, demonstrating the effectiveness\nof our HS-DPO. In addition, we found that \"R-SFT w. HS-\nDPOensemble\" beats \"R-SFT w. HS-DPOself\", indicating the\neffectiveness of improving the diversity of hard samples."}, {"title": "4.3. Efficiency Experiment", "content": "We conduct efficiency experiments for GuardReasoner and \"Baselinemix\" in the ablation study, i.e., the guard model trained with a mix of the seed datasets. Note that these two methods are trained with the same amount of training samples. We test the costs in the training stage and the inference stage. In the training stage, we use 4 NVIDIA H100 (80GB) GPUs and adopt the LLaMA Factory (Zheng et al., 2024b) to train the models. In the inference stage, we use 1 NVIDIA H100 (80GB) GPU and adopt vLLM (Kwon et al., 2023) to accelerate the inference. We run the models on the used 13 guardrail benchmarks and record the GPU memory cost, time costs, and output token costs. From the results in Table 5, we have the following findings.\n(I) In the training stage, GuardReasoner has a similar GPU\nmemory cost compared to the baseline, whether at the R-\nSFT or HS-DPO stage. Take the 8B models as an example,\nGuardReasoner costs 270.86 GB and 273.95 GB at the R-\nSFT and HS-DPO stage, while Baselinemix uses 270.78 GB\nat the SFT stage. Besides, for the time cost, GuardReasoner\nincreases 40% ~ 50% time cost since 1) it needs to learn\nfrom the reasoning data, and 2) it contains two training\nstages. (II) In the inference stage, the memory costs are\nsimilar since we use the vLLM and set the GPU utilization"}, {"title": "4.4. Case Study", "content": "Case studies on GuardReasoner 8B discuss 3 aspects. (I) Performance: in Figures 3 and Figure 4, GuardReasoner successfully defends both a conventional case and an attack. (II) Explainability: in Figure 5, GuardReasoner provides explanations. To verify its explainability, we consider the task of correcting mislabelled samples. Concretely, we first sample the error predictions of our model according to the original labels and then ask 3 human annotators to re-label these samples. We regard the majority as the corrected label. We evaluate this task via the performance improvement of our model after re-labeling. The higher performance improvement denotes the more mislabeled samples and the more effective explanations. The results are in Table 7."}, {"title": "5. Conclusion", "content": "This paper introduces GuardReasoner, a novel guard model that improves performance, explainability, and generalization. We present our GuardReasonerTrain dataset, R-SFT, and HS-DPO, to first unlock the reasoning ability, then guide the model to learn to reason. On experiments across"}, {"title": "6. Impact Statement", "content": "This paper introduces a guard model designed to enhance the safety of large language models. By implementing this guard model, we aim to mitigate the potential risks and harmful impacts that LLMs may pose to society."}], "equations": ["L_{R-SFT} = -E_{(X,S,R,Y) \\sim D} log P_{\\theta}(R, Y | I, X, S),", "L_{HS-DPO} = -E_{C \\sim H_{ensemble}} log \\sigma (A - B),", "\\alpha = 1 + Norm(k_{incorr} - k_{corr}, \\gamma),"]}