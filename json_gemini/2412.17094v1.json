{"title": "Analysis on LLMs Performance for Code Summarization", "authors": ["Salman Ahsan", "Md. Muktadir Mazumder", "Md. Ahnaf Akib"], "abstract": "The goal of code summarizing is to produce concise source code descriptions in natural language. Deep learning has been used more and more recently in software engineering, particularly for tasks like code creation and summarization. Specifically, it appears that the most current Large Language Models with coding perform well on these tasks.\nCode summarization has evolved tremendously with the advent of Large Language Models (LLMs), providing sophisticated methods for generating concise and accurate summaries of source code. Our study aims to perform a comparative analysis of several open-source LLMs, namely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is assessed using important metrics such as BLEU3.1 and ROUGE3.2.\nThrough this analysis, we seek to identify the strengths and weaknesses of each model, offering insights into their applicability and effectiveness in code summarization tasks. Our findings contribute to the ongoing development and refinement of LLMs, supporting their integration into tools that enhance software development and maintenance processes.", "sections": [{"title": "Introduction", "content": "The application of Natural Language Processing (NLP) techniques for automated program understanding, production, and retrieval is becoming more popular, as these tasks have the potential to enhance code accessibility. One common activity is Code Summarization, which is essentially translating code into Natural Language barone2017parallel.\nThis work is crucial because software developers can become much more productive if they can automatically generate code summaries or doc strings.\nAlthough existing models perform impressively well in code summarization, it is crucial to assess how much the structure and semantics of the code are understood by these models. To make their code more legible by humans, software writers frequently use English terms as the names of variables, functions, and data structures.\nThis study compares a number of open-source Large Language Models (LLMs)\u2014namely, LLaMA-3, Phi-3, Mistral, and Gemma\u2014that are utilized for code summarization in order to ease these worries. Key performance indicators like BLEU, F1 Score, Precision, Accuracy, and ROUGE are used to assess these models' performance. By means of this study, we want to evaluate the benefits and drawbacks of every model, providing valuable perspectives on their suitability and efficiency in code summarizing assignments. Our research supports the further improvement and development of LLMs and their incorporation into tools that improve the processes involved in software development and maintenance."}, {"title": "Overview", "content": "A brief description of the code's function in simple language may be found in the source code summary tang2022ast. Writing concise explanations of code in normal language is known as source code summarization leclair2020improved. Code summarization, sometimes referred to as code commenting, is a textual explanation of the role that specific identifiers play in computer systems. Stated differently, code summary aids in program comprehension by providing a natural language explanation of the logic and functionality of the source code zhang2022survey. These descriptions, which offer context and functional insights, assist in improving the code's comprehensibility and accessibility for developers and other stakeholders.\nIn order to visually represent the topic source code summarization; some examples are provided below:\nUse Case 01\nIn Figure 1.1, we can see a simple block of code in java programming language, specifically a function named addNumbers, which takes two integer numbers as parameters and returns the result of their addition. So, the summarized text of the code will be simply \"returns the sum of two integers\" in natural language which is readable to human."}, {"title": "Use Case 02", "content": "In Figure 1.2, if we follow the red, green and blue lines on the simple block of java code, we observe their combination to form specific keywords. These extracted keywords play a pivotal role in generating the code summary. Consequently, the resulting summarized text in natural language is \"contains ignore case,\" which remains comprehensible to humans."}, {"title": "The importance of source code summarization", "content": "Summarizing source code is essential because it makes code more readable and manageable by giving clear, succinct explanations of its functionality. This helps developers navigate and grasp vast codebases more rapidly, increasing productivity and decreasing the amount of time spent trying to understand complex code structures.\nSource code summarization has several real-world applications:\n1. Code Documentation: Automatically generated summaries can improve the documentation of software projects, making it easier for developers to understand and contribute to the codebase.\n2. Code Review: Summaries can assist code reviewers by providing quick insights into the functionality of code snippets, streamlining the review process.\n3. Bug Tracking and Fixing: Summaries help developers quickly grasp the purpose of code sections when diagnosing and fixing bugs, enhancing efficiency.\n4. Learning and Onboarding: New team members can benefit from summarized code to quickly get up to speed with the project's structure and functionality, accelerating the onboarding process.\n5. Search and Retrieval: Summarized code can enhance code search engines, making it easier to find relevant code snippets based on natural language queries.\n6. Software Maintenance: Summarized code assists in maintaining large codebases by providing clear explanations of code functionality, making updates and modifications more manageable.\n7. Code Comprehension: Summaries improve overall code comprehension, enabling developers to understand complex code structures more efficiently and effectively."}, {"title": "Motivation and Scope", "content": "Efficient tools for understanding and managing code are crucial due to the increasing complexity of software systems. Source code summarization enhances productivity by providing concise, human-readable summaries, improving documentation, aiding in code reviews, bug fixing, and onboarding new developers. This research can lead to better software maintenance, faster development cycles, and reduced project costs.\nResearchers are applying encoder-decoder architectures to software engineering as a result of their introduction in natural language processing sutskever2014sequence (both Transformer-based cho2014learning and recurrent hendrycks2021measuring).\nMaking code summaries is one significant application shido2019automatic. For example, a code summary tool can be used to write documentation or comprehend legacy code. With the arrival of LLMs, or large language models, working programmers now have a lot more chances to use deep learning-based technologies. Both closed models (like GPT-4achiam257532815gpt or Geminiteam2023gemini) and open models (like CodeLlamaroziere2023code) show remarkable ability in creating natural language summaries of code and source code based on task descriptions. This technical report's primary goal is to examine how open-sourced LLMs handle source code in respect to natural language text. Previous studies have explored various techniques for source code summarization. Graph Neural Nets, Graph Attention Neural Nets, Abstract Syntax Trees, and sequence-to-sequence models have all been created, along with semantic embeddings for code snippet representation and code comment generation. Inspired by transformer based models like BERT and GPT, LLMs such as CodeBERT, GraphCodeBERT, PLBART, and CodeT5 have achieved significant advancements.\nThis study performs a comparative analysis of open-source LLMs, including LLaMA-3, Phi-3, Mistral, and Gemma, for code summarization. By evaluating these models on metrics like BLEU, F1 Score, Precision, Accuracy, and ROUGE-L, the research aims to identify models that generate syntactically and semantically accurate summaries, contributing to more effective software development tools. The main objective is to investigate how well open-sourced LLMs handle source code in relation to natural language text."}, {"title": "Problem Statement", "content": "In this study, we aim to review how open-source large language models (LLMs) perform in code explanation or summarization. This research addresses the challenge of evaluating the effectiveness of these models in generating accurate and meaningful summaries of source code. The specific objectives are to compare the performance of open-source LLMs (LLaMA-3, Phi-3, Mistral, and Gemma) on code summarization tasks and to evaluate these models using performance metrics such as BLEU and ROUGE. By identifying the strengths and weaknesses of each model in handling the semantic relationship between source code and natural language, this study aims to provide insights into which models offer the best balance of syntactic and semantic accuracy for code summarization tasks.\nFurthermore, the project seeks to fine-tune these LLMs to enhance their ability to generate accurate, concise, and contextually relevant summaries for code snippets. Despite advancements in natural language processing, existing LLMs often struggle with providing precise and contextually appropriate summaries for complex code structures. This limitation hampers the efficiency of developers who rely on these models for understanding and documenting code. Therefore, by refining the LLMs, we aim to improve their comprehension of programming languages and enable them to produce summaries that are both succinct and highly relevant to the given code context."}, {"title": "Research Challenges", "content": "Code summarization, which involves generating concise descriptions of source code segments, faces several significant research challenges. One of the primary difficulties is the semantic gap between the low-level operations described in the code and the high-level tasks they accomplish. Bridging this gap requires a deep understanding not just of the code's syntax, but also of its semantics and the programmer's intent. Additionally, domain-specific knowledge is often necessary to accurately interpret and summarize code, particularly in specialized fields. Another challenge is the variability in programming styles and languages, which can affect the effectiveness of summarization tools. Moreover, the limited availability of high-quality, annotated datasets for training machine learning models in this area hampers progress. Lastly, ensuring the generated summaries are both accurate and useful for developers\u2014capturing not just what the code does, but also why it does it is a critical yet challenging aspect of research in code summarization."}, {"title": "Contribution", "content": "Our study adds significantly to the field of code summarization in a number of ways. First of all, it offers a thorough comparison of four Large Language Models (LLMs) that are available as open-source software, namely Gemma, Mistral, LLaMA-3, and Phi-3, with a focus on code summarization. Through a methodical assessment of these models with measures including ROUGE, F1, Precision, Accuracy, and BLEU, the study provides insightful information about the relative advantages and disadvantages of each model.\nSecondly, the research establishes a robust evaluation framework for assessing LLMs in code summarization tasks, which can serve as a benchmark for future studies, enabling consistent and objective comparisons across different models and datasets. Additionally, by analyzing the performance of various LLMs, the study identifies best practices for applying these models to code summarization, including recommendations on model selection based on specific use cases, programming languages, and code structures.\nThe results also show the limitations and practical advantages of employing LLMs for code summarization in real-world software development, including increased productivity, code comprehension, and maintenance. By giving a thorough grasp of how various LLMs perform in code summarization, directing future research and useful applications, and facilitating software developers' ability to utilize LLMs' power in their work, their contributions enhance the area."}, {"title": "Organization", "content": "The structure of this research study is as follows: We present a thorough literature review that summarizes previous research on the function and effectiveness of LLMs in source code summarization in chapter 2. The shortcomings and deficiencies in the state of the research are highlighted in this section. The research methodology is described in Chapter 3, wherein the use of Large Language Models (LLMs) - Gemma, Mistral, LLaMA-3, and Phi-3 - is discussed. Our experimental setup and dataset are also presented, and the results and analysis are presented in Chapter 4. Finally, in the case study and chapter 5 address the implications of our findings, highlight their limits, and offer suggestions for further research in this area."}, {"title": "Related Works", "content": "Before diving into our main reserach study on LLMs for code summarization let us first disscuss about some literatures where Traditional Models For Code Summarization are mentioned."}, {"title": "RNN Based Models", "content": "alon2018code2seq alon2018code2seq proposed a study which describes how the Sequence-to-Sequence (seq2seq) models, derived from neural machine translation (NMT), handle source code as a sequence of tokens and attain state-of-the-art performance on these tasks. The paper presents Code2Seq, a novel approach that enhances source code encoding by making use of the syntactic features of programming languages. Their methodology represents a code snippet as the set of compositional pathways in its abstract syntax tree (AST), using attention to select the pertinent paths during decoding. The authors have demonstrated the effectiveness of their method on two tasks, two programming languages, and four datasets including up to 16 million cases. Compared to previous models designed specifically for programming languages and current state-of-the-art NMT models, the model given in this research performs significantly better. However, the approach has certain limitations. Firstly, RNNs, the backbone of the model, are known to be slow to train, often requiring truncated versions of backpropagation and demanding significant computational resources.\nMoreover, these models are prone to vanishing and exploding gradients, especially when dealing with long sequences, as illustrated in Fig 2.1. Additionally, the architecture may struggle with large codebases, given its sequential processing nature and the challenges associated with capturing long-range dependencies effectively. In sum-"}, {"title": "Tree/GNN Based Models", "content": "leclair2020improved leclair2020improved presented a new approach in contrast to RNNs that uses a graph neural network (GNN) to summarize source code. The key idea is to make summarization easier by using the structure of the abstract syntax tree (AST) and the source code sequence. The authors test their strategy on a dataset of 2.1 million Java method-comment pairings. The model architecture is covered in full in the paper leclair2020improved, which also explains how the GNN and recurrent neural networks (RNNs) are integrated to encode the source code and AST. The outcomes demonstrate a significant improvement in code summarization quality, which is attributable to the GNN's effective representation of the code's structural information. Their approach is based on the graph2seq model, with certain adjustments made to better fit the concept into a software engineering setting. In short, they coupled the GNN-based encoder of graph2seq to model the AST of each subroutine with the RNN-based encoder used by leclair2019neural leclair2019neural to model the"}, {"title": "Transformer Based Models", "content": "In 2017, then breakthrough obtained in tansformer based approaches shown by vaswani2017attent vaswani2017attention is able to determine the dependencies and relationships between various parts of the source code from the data itself. The Transformer architecture makes data processing possible in parallel, improving the model's scalability to larger datasets. By using attention mechanisms, the Transformer can better understand the context within which certain code elements appear. The model provides interpretability, possibly through mechanisms that allow visualization of attention weights to understand how the model is making its predictions.\nThe model may unnecessarily attend to the same pieces of information more than once due to redundancy in the attention mechanisms. Despite the Transformer's many advantages, it is still difficult to incorporate code structure information into the Transformer model in an efficient manner."}, {"title": "Large Language Models", "content": "Incorporating Large Language Models (LLMs) into code summarization represents a significant advancement by leveraging their extensive language comprehension. Recent research has focused on utilizing these models to automatically generate concise, meaningful descriptions of code snippets, a task traditionally performed manually.\nAhmed and Devanbu ahmed2022fewshot presented an innovative approach utilizing the GPT Codex model to perform few-shot learning for project-specific code summarization tasks. They investigated whether the few-shot capabilities of LLMs could be extended to code summarization, achieving positive results that suggest significant improvements over traditional models trained on large datasets. Their methodology involved structuring prompts with a small set of function-comment pairs before appending a target function for summarization. This set-up exploited the model's ability to generate high-quality summaries based on minimal examples tailored to the specifics of a project, highlighting the adaptability and efficiency of LLMs in handling domain-specific knowledge ahmed2022fewshot.\nThe authors utilized a prompt-based approach where several function-summary pairs from the same project were presented to the model, followed by the function requiring a summary. This few-shot learning setup, without any model re-training, demonstrated that LLMs could effectively adapt to new tasks through context switching. Remarkably, the method required no weight adjustments to the model, relying instead on its inherent capacity to generate contextually appropriate responses.\nThe performance of the LLM, particularly in generating summaries for unseen code, was quantitatively evaluated using metrics like BLEU-4, showing improvements over\nIn 2024, Guo et al. created GraphcodeBERT, a new way to understand semantic code that combines data flow with standard BERT architectures. GraphcodeBERT did much better at code summarization tasks than traditional models because it used execution semantics in the pre-training step. This improvement shows how important deep semantic processing is for making LLM work better on jobs that involve code guo2024graphcodebert.\nBased on multi-task learning techniques, Nijkamp et al. (2024) showed CodeT5, an encoder-decoder model that was fine-tuned for code summarization. This model was already trained on a big corpus and was fine-tuned to do the best job of summarizing code. The results from CodeT5 showed that it could make summaries that were both relevant to the context and logically consistent. This set a new standard for code summarization nijkamp2024codet5."}, {"title": "Methodology", "content": "Proposing an innovative methodology for comparing LLMs in Code Summarization involves employing diverse evaluation metrics and benchmark datasets to capture different aspects of code structure. Here's a detailed breakdown of the methodology."}, {"title": "Proposed Approach", "content": "For investigating the answers we have employed a robust and structured methodology for our research. The overall pipeline of the methodology is illustrated in the figure 3.1"}, {"title": "Dataset Collection", "content": "For many code-NL tasks, such as code summarization, Code-XGLUE serves as a standard benchmark lu2021codexglue. Code samples in Go, Java, JavaScript, PHP, Python, and Ruby are taken from publicly accessible, open-source GitHub projects and are accompanied by English descriptions. It is a filtered subset of CodeSearchNet husain2019codesearch The repository's opening paragraph contains a description of every code element. Examples that did not have a length of three to 256 tokens, were empty, contained special characters like \"http://,\" had descriptions written in languages other than English, or could not be processed into an abstract syntax tree were removed from the dataset."}, {"title": "Data Preprocessing", "content": "The data preprocessing stage involves two critical steps: tokenization and embedding generation.\n\u2022 Tokenization: The initial stage of the data preparation pipeline involves breaking down the input text data into smaller pieces known as tokens. Depending on the tokenizer being used, these tokens may be characters, words, or subwords. Because it converts the unstructured text into a format that the model can comprehend and analyze, tokenization is crucial.\n\u2022 Embeddings: After tokenization, each token is converted into a dense vector representation known as an embedding. Embeddings capture the semantic meaning of tokens in a continuous vector space, allowing the model to understand and work with the textual data more effectively. These embeddings serve"}, {"title": "Model Selection", "content": "In the new era of code summarization, various large language models (LLMs) offer distinct advantages, each suited to different needs and contexts. The models considered here are Llama-3, Phi-3, Gemma, and Mistral, each bringing unique features and capabilities to the table.\nLlama-3llama3modelcard\nLlama-3 has an advanced architecture that makes it very efficient in terms of processing and memory. It has a 128k tokenizer, RMSNorm, and K-V cache. It can comprehend a broad range of programming scenarios because it has been pretrained on an enormous dataset including 15 trillion tokens. Reinforcement learning from human feedback (RLHF) and supervised fine-tuning improve Llama-3 even further. It is a great option for scenarios requiring high performance and resource efficiency because its main goal is to enable faster training and efficient calculation.\nPhi-3abdin2024phi\nPhi-3 is designed with a similar architecture to Llama2, featuring GeGLU and an extensive 128K context window with LongRope. It is pretrained on 3.3 trillion tokens sourced from both general and specific web data, providing a robust foundation for code summarization tasks. Phi-3 undergoes supervised fine-tuning with Direct Policy Optimization (DPO), enhancing its adaptability. It is particularly suitable for deployment on handheld devices due to its efficient quantization to 4-bits, balancing performance with hardware constraints.\nGemmateam2024gemma\nGemma leverages an architecture that includes Multi Query Attention, GeGLU, and RMSNorm, optimized for lightweight performance. It is pretrained on a more modest dataset of 6 trillion tokens but excels in supervised fine-tuning and reinforcement learning from feedback. This model is designed for environments where computational resources are limited, yet competitive results are required. Gemma's training"}, {"title": "Evaluation Metrics", "content": "In our research to evaluate the performance of the LLMs we have selected the following performance metrics.\n1. BLEU: BLEU is a frequently used statistic in software engineering and natural language processing to evaluate generative processes, including conversation creation, Using n-gram matching, BLEU calculates the ratio of N groups of word similarity between generated comments and reference comments.\nThe formula is as follows:\nBLEU-N = BP x exp (X \u03a3 wn log n), where $P_n$ is the proportion of the candidate's subsequences that have length n. The uniform weight 1/N is represented by $\\tau_n$ for short generated sequences, and the shortness penalty is represented by BP. Since corpus-level BLEU-4, or N = 4, has been shown to be more correlated with human evaluations than alternative metrics, we utilize it as our assessment metric.\n2. ROUGE-L: In the field of natural language processing, ROUGE-L is frequently used to handle text summarization problems. The F-measure yields the ROUGE-L value, which is based on the Longest Common Subsequence (LCS) between two texts. ROUGE-L is calculated as follows given a generated text (X) and a"}, {"title": "Experimental Setup", "content": "For model training, we utilised the free edition of Google Colab, which gave us access to NVIDIA Tesla K80 GPUs. Even though it wasn't as powerful as specialised high-performance hardware, this configuration worked well for our tests. Utilising GPU acceleration to meet the computational demands of our models, the Colab environment made it easier to train and evaluate LLaMA-3, Phi-3, Mistral and Gemma. We conducted our trials effectively in spite of the limitations of the free version, which included limited memory capacity and session durations. This showed that employing readily available resources for complex natural language processing tasks is feasible."}, {"title": "Train-Test Split", "content": "In our comparative study, we adopted a 70-20-10 train-test-development split to ensure robust evaluation and generalization of the Large Language Models (LLMs). The training set, comprising 70% of the dataset, is used to fit the model parameters and learn the underlying patterns in the data. This substantial portion allows the model to capture a wide variety of examples, enhancing its learning capabilities. The testing set, which constitutes 20% of the data, is reserved for evaluating the performance of the trained models on unseen data, providing an unbiased assessment of the model's accuracy and effectiveness. Additionally, the development set, making up 10% of the dataset, is utilized during the training process to tune hyperparameters and make model adjustments."}, {"title": "Hyper Parameters", "content": "For the hyperparameters used in the model training process, we have used batch size of 2 is employed, meaning that the model processes two samples before updating its parameters. This small batch size helps in handling memory constraints while training. Gradient accumulation is set to 4, indicating that the gradients from four batches are accumulated before performing a backward pass. This effectively increases the batch size and stabilizes training. The warm-up steps are set to 5, gradually increasing the learning rate at the beginning of the training to prevent sudden large updates that could destabilize the model. Only one epoch is used, implying that the entire training dataset is passed through the model once. The learning rate is set to 2e-4, controlling the step size for updating model parameters during optimization. The optimizer used is AdamW 8-bit, which helps reduce memory usage while maintaining efficient and effective parameter updates."}, {"title": "Fine Tuning Models", "content": "The model has to undergo through a number of crucial processes in order to be fine-tuned for maximum performance on particular tasks. Alpaca is used to format the dataset at the start of the procedure, ensuring that it is in a format that is appropriate for training. The text data is then transformed into tokens that the model can process by the tokenization stage.\nTokenization is followed by loading the model and adding a LoRA (Low-Rank Adaptation) adaptor to improve the model's training efficiency and flexibility. The prepared dataset is then used to train the model using the SFTTrainer (Supervised Fine-Tuning Trainer). In order to increase performance on the goal task, this stage entails modifying the model parameters depending on the training data.\nThe Adam optimizer is used to maximize the training process. This optimizer is well-known for its effectiveness when working with sparse gradients, and it works especially well when fine-tuning big models. In order to assess the model's performance and make sure it satisfies the required relevance and accuracy criteria, the inference phase is finally carried out using test data."}, {"title": "Performance Result", "content": ""}, {"title": "Performance Evaluation on Python Dataset", "content": "The performance evaluation of four large language models (LLMs) utilizing BLEU and ROUGE-L scores on the Python dataset is shown in table 4.3. Gemma-7b, Phi-3-medium, Llama-3-8b, and Mistral-7b are the models that have been assessed. The highest BLEU scores of 7.38 are obtained by Phi-3-medium and Mistral-7b, suggesting that their generated summaries have the maximum degree of n-gram overlap with the reference summaries. These two models also obtain the maximum ROUGE-L score of 19.35, indicating that in addition to being correct, their summaries are also fluent and pertinent to the context. Conversely, Gemma-7b and Llama-3-8b exhibit a respectable performance, but somewhat lagging below Phi-3-medium and Mistral-7b, with BLEU scores of 7.23 and ROUGE-L scores of 18.95, respectively."}, {"title": "Performance Evaluation on Java Dataset", "content": "Besides the table 4.4 shows the performance evaluation of the same four models on the Java dataset, again using BLEU and ROUGE-L scores. For this dataset, the results differ somewhat. Llama-3-8b achieves the highest BLEU score of 6.10, indicating it produces the most n-gram similar summaries to the reference text. However, Mistral-7b outperforms in terms of ROUGE-L score with a value of 22.37, suggesting its summaries are more contextually appropriate and fluent. Gemma-7b and Phi-3-medium, while showing lower BLEU scores of 6.02 and 5.17 respectively, still perform competitively in terms of ROUGE-L scores, with Phi-3-medium achieving 21.82 and Gemma-7b at 21.11. These results illustrate that Mistral-7b consistently generates relevant and fluent summaries, while Llama-3-8b excels in n-gram similarity for the Java dataset."}, {"title": "Performance Evaluation on Go Dataset", "content": "The table 4.5 evaluates the performance of four large language models (LLMs)\u2014Gemma-7b, Phi-3-medium, Llama-3-8b, and Mistral-7b\u2014on the Go dataset using BLEU and ROUGE-L scores. Gemma-7b achieves a BLEU score of 1.01 and a ROUGE-L score of 7.14, indicating relatively lower performance compared to the other models. Both Phi-3-medium and Llama-3-8b achieve the highest BLEU score of 1.37 and the highest ROUGE-L score of 7.69, suggesting that they produce summaries with better n-gram overlap and contextual relevance than Gemma-7b and Mistral-7b. Mistral-7b scores slightly lower than Phi-3-medium and Llama-3-8b, with a BLEU score of 1.35 and a ROUGE-L score of 7.48, still performing well but not the best in this dataset."}, {"title": "Performance Evaluation on JavaScript Dataset", "content": "The table 4.6 evaluates the same models on the JavaScript dataset using BLEU and ROUGE-L scores. Gemma-7b scores a BLEU of 5.08 and a ROUGE-L of 17.14, which are comparatively lower than the other models in this dataset. Phi-3-medium achieves a BLEU score of 7.49 and a ROUGE-L score of 30.09, demonstrating strong performance in generating both accurate and contextually relevant summaries. Llama-3-8b scores the highest BLEU at 7.76, indicating it produces summaries with the greatest n-gram overlap with the reference text, and a ROUGE-L of 24.00. However, Mistral-7b outperforms all models with a BLEU score of 13.79 and a ROUGE-L score of 36.84, indicating it generates the most accurate and contextually relevant summaries for JavaScript."}, {"title": "Performance Evaluation on PHP Dataset", "content": "Lastly the table 4.7 evaluates the performance of four large language models (LLMs) on the PHP dataset using BLEU and ROUGE-L scores. The models assessed are Gemma-7b, Phi-3-medium, Llama-3-8b, and Mistral-7b. Gemma-7b achieves a BLEU score of 3.87 and a ROUGE-L score of 12.66, indicating moderate performance. Phi-3-medium slightly outperforms Gemma-7b with a BLEU score of 4.03 and a ROUGE-L score of 12.82, suggesting marginally better n-gram overlap and contextual relevance. Llama-3-8b performs significantly better with a BLEU score of 6.24 and a ROUGE-L score of 13.82, demonstrating its strength in generating accurate and relevant summaries. Mistral-7b stands out with the highest BLEU score of 8.79 and a ROUGE-L score of 12.99, indicating its superior ability to produce precise and contextually appropriate summaries for the PHP dataset."}, {"title": "Performance Evaluation on Ruby Dataset", "content": "The second table assesses the same models on the Ruby dataset using BLEU and ROUGE-L scores. Gemma-7b achieves a BLEU score of 3.53 and a ROUGE-L score of 8.33, reflecting moderate performance. Phi-3-medium emerges as the best performer with a BLEU score of 5.03 and a notably high ROUGE-L score of 21.35, indicating it generates summaries with the highest contextual relevance and fluency. Llama-3-8b also performs well with a BLEU score of 4.38 and a ROUGE-L score of 8.39, showing its capability to generate accurate summaries. In contrast, Mistral-7b has the lowest BLEU score of 2.52 and a ROUGE-L score of 7.74, suggesting it struggles more with the Ruby dataset compared to the other models."}, {"title": "Result Visualization", "content": "The following analysis discusses the performance of four large language models (LLMs)\u2014Gemma, Llama-3, Mistral, and Phi-3\u2014based on BLEU and ROUGE-L scores across different programming language datasets: JavaScript, Java, Go, Ruby, Python, and PHP."}, {"title": "BLEU Scores of Models Across Different Datasets", "content": "The radar chart 4.1 displays the BLEU scores of the models across various datasets. Key observations include:\n\u2022 Mistral (in red) consistently achieves high BLEU scores, particularly excelling in the JavaScript and PHP datasets. This indicates that Mistral generates text with the greatest n-gram overlap with reference texts in these languages.\n\u2022 Llama-3 (in blue) also performs well, with notable scores in the JavaScript and Java datasets.\n\u2022 Phi-3 (in green) demonstrates strong performance in the Ruby dataset, outperforming the other models in this specific language.\n\u2022 Gemma (in purple) tends to have lower BLEU scores across most datasets compared to the other models, indicating relatively less n-gram similarity with the reference texts."}, {"title": "ROUGE-L Scores of Models Across Different Datasets", "content": "The ROUGE-L scores for the same models over the same datasets are displayed in the chart 4.2, which measures the longest common subsequence (LCS) between generated text and reference text. Important findings consist of:\n\u2022 Mistral (in red) again stands out with high ROUGE-L scores, particularly in the JavaScript and PHP datasets, showcasing its ability to produce contextually relevant and fluent summaries.\n\u2022 Phi-3 (in green) excels in the Ruby dataset, highlighting its strong performance in generating summaries with high contextual relevance and fluency for Ruby code.\n\u2022 Llama-3 (in blue) performs well in the JavaScript and Java datasets, indicating its capability in these languages.\n\u2022 Gemma (in purple) shows lower ROUGE-L scores across most datasets, suggesting it is less effective in generating contextually coherent summaries compared to the other models."}, {"title": "Final Verdict", "content": "In our comparative analysis of large language models for code summarization, Mistral emerges as the best overall performer, demonstrating high scores across most datasets, with exceptional performance in JavaScript and PHP. Phi-3 also performs consistently well, particularly excelling in Ruby and Go. While Llama-3 and Gemma exhibit decent performances in specific datasets, they generally lag behind the leading models, Mistral and Phi-3. These findings highlight the strengths of Mistral and Phi-3 in delivering high-quality code summaries across a variety of programming languages."}, {"title": "Limitation", "content": "Each model may require individualized training strategies to accommodate differences in architecture and learning dynamics, despite being subjected to the same initial conditions. The availability and quality of training data are critical yet challenging, as high-quality, domain-specific datasets for code summarization are scarce. This scarcity can hinder the models' ability to generalize effectively to diverse real-world coding practices. Additionally, ensuring that each model's output is not only accurate but also contextually relevant to developers is a complex task that involves balancing technical accuracy with practical usability. These challenges highlight the need for a nuanced approach to training and evaluating each model within your research framework.\nAnother critical limitation is the potential for model bias. Since LLMs are trained on available data, any inherent biases in this data can be amplified during training and fine-tuning. This could lead to models that perform well on certain types of code or coding styles but poorly on others, potentially affecting the fairness and inclusivity of the tool."}]}