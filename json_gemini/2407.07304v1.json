{"title": "Inference Performance Optimization for Large Language Models on CPUs", "authors": ["Pujiang He", "Shan Zhou", "Wenhuan Huang", "Changqing Li", "Duyi Wang", "Bin Guo", "Chen Meng", "Sheng Gui", "Weifei Yu", "Yi Xie"], "abstract": "Large language models (LLMs) have shown exceptional performance and vast potential across diverse tasks. However, the deployment of LLMs with high performance in low-resource environments has garnered significant attention in the industry. When GPU hardware resources are limited, we can explore alternative options on CPUs. To mitigate the financial burden and alleviate constraints imposed by hardware resources, optimizing inference performance is necessary. In this paper, we introduce an easily deployable inference performance optimization solution aimed at accelerating LLMs on CPUs. In this solution, we implement an effective way to reduce the KV cache size while ensuring precision. We propose a distributed inference optimization approach and implement it based on oneAPI Collective Communications Library. Furthermore, we propose optimization approaches for LLMs on CPU, and conduct tailored optimizations for the most commonly used models. The code is open-sourced at https://github.com/intel/xFasterTransformer.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) based on the Transformer Vaswani et al. (2017) architecture have garnered profound technical attention globally and achieved remarkable accomplishments (Touvron et al., 2023), (Zhang et al., 2022), (Yang et al., 2023), (Wu et al., 2023), (Bai et al., 2023). Their robust comprehension and generation capabilities are profoundly changing applications of artificial intelligence (Thirunavukarasu et al., 2023). However, the practical deployment of LLMs is significantly hindered by the high cost and resource limitations of hardware (Zhao et al., 2023).\nTherefore, deploying of LLM with good performance for practical applications has become a trending topic in industry, which helps land LLMs in more practical applications. When GPU hardware resources are limited, we can explore alternative options on CPUs.\nOptimizing the practical deployment performance of LLMs necessitates effectively leveraging hardware capabilities for cost efficiency and improved inference performance, which, in turn, demands that developers possess a strong understanding of both hardware and software. To tackle the practical deployment challenge, we propose an easy and efficient solution designed to facilitate easy deployment on CPUs. Deploying on CPUs offers the advantage of being unrestricted by VRAM size, preventing KV cache overflow, and enabling the processing of extremely long-context support (Liu et al., 2023). Furthermore, deployment on CPUs can enhance system resource utilization, making multitasking more efficient.\nThe paper introduces the solution for efficient deployment and inference performance optimization for LLMs on CPUs. The solution supports for widely used LLMs, and experiment results show the proposed solution has good inference performance scalability on CPUs. We have established a repository where we curate relevant optimization with real-time updates. The main contributions are as follows:\n\u2022 We propose new LLM optimize solutions on CPUs, such as SlimAttention. We conduct individual optimizations for LLM operations and layers, and the support extends to widely used LLMs, encompassing Qwen, Llama, ChatGLM, Baichuan, and Opt series.\n\u2022 We implement an effective way to reduce the KV cache size and ensure precision. This approach allows for a more efficient use of memory without significantly compromising the quality of the model's output.\n\u2022 We design a distributed inference optimization solution for LLMs on CPUs, facilitating the attainment of necessary scalability and efficient low-latency inference."}, {"title": "2. Approach", "content": "In this section, three main optimization approaches are proposed and more details could be referred to the following sections."}, {"title": "2.1. LLM Optimization", "content": "To improve the inference performance, we proposed individual optimize solutions for each LLM operations and layers. Taking attention layer as an example. Since the resources consumed by the attention mechanism are directly proportional to the square of the sequence length from theoretical, optimizing attention is particularly important for long sequence inputs.\nA new approach we called SlimAttention shown in Figure 1 is proposed. SlimAttention is essentially a one-dimensional decomposition of the score between query and key. In terms of computation sequence, it involves first calculating a horizontal score, followed by applying softmax to the score. The resulting softmax is then multiplied by the corresponding values, producing a portion of the output. This process is repeated for the next block using the same score buffer. In theory, each thread needs only to maintain a buffer of the block size, thereby reducing memory usage and enhancing computational efficiency in practice. And FlashAttention (Dao et al., 2022) is a solution initially introduced on GPUs. Essentially, it involves a two-dimensional decomposition of the score. In the computational process, each thread only needs to maintain a single tile. However, as a tile doesn't encompass all the data in the softmax direction, there is a need for iterative corrections to the final results during the calculation.\nCompared with FlashAttention, SlimAttention entails no redundant computations but does necessitate a larger intermediate buffer."}, {"title": "2.2. Effective KV cache Optimization", "content": "Typical LLM architectures are predominantly decoder-only structures. Within these models, computation is primarily divided into two categories: attention (MHA/GQA) and matrix multiplication (MatMul), the latter often including post-operations that can be fused into the MatMul process. During the generation of the first token, both attention and MatMul operations are compute-bound. However, for the generation of subsequent tokens (beyond the first token), the attention typically exhibits a pattern of general matrix-vector multiplication (gemv), which is bound in memory bandwidth.\nThe volume of the key-value (KV) cache accessed during the generation of a single token can be calculated as follows:\n2b(Li + Lo)lnheadSheadSd"}, {"title": "2.3. Distributed Inference Optimization", "content": "To enhance distributed inference performance, we present a solution for optimizing distributed inference for LLMs"}, {"title": "3. Experiment Results", "content": ""}, {"title": "3.1. Configuration", "content": "We conducted experiments on the Intel\u00ae Xeon\u00ae CPU 8563C, and detailed information of one bare-mental machine is provided in Table 1. Note that each machine has 2 sockets."}, {"title": "3.2. Performance", "content": "We measure the throughput and latency of next token generation on Intel\u00ae Xeon\u00ae CPU 8563C by adopting the proposed solution."}, {"title": "4. Conclusion and Future Work", "content": "We presented an end-to-end LLM inference accelerating solution including distributed inference optimization, effective KV cache optimization, and individual LLM optimization. We showcased the versatility across a range of popular LLMs and the performance superiority over the open-source solution on CPUs. In our forthcoming research, we intend to broaden our study to include a wider variety of CPUs, particularly those with resource constraints. Our primary focus will be on enhancing performance for larger batch sizes and exploring effective deployment serving solutions. Concurrently, we aim to adapt our solution to accommodate the latest trending models, such as the mixture of experts (MoE) models. Our goal is to offer a practical alternative to existing GPU solutions."}]}