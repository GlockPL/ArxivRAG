{"title": "Knowledge Distillation Approaches for Accurate and Efficient Recommender System", "authors": ["SeongKu Kang"], "abstract": "In this era of information explosion, recommender systems are widely used in various industries to provide personalized user experience, playing a key role in promoting corporate profits. Recent recommender systems tend to adopt increasingly complex and large models to better understand the complex nature of user-item interactions. Large models with numerous parameters have high recommendation accuracy due to their excellent expressive power. However, they also incur correspondingly high computational costs as well as high latency for inference, which has become one of the major obstacles to deployment.\nTo reduce the model size while maintaining accuracy, we focus on knowledge distillation (KD), a model-independent strategy that transfers knowledge from a well-trained large model to a compact model. The compact model trained with KD has an accuracy comparable to that of the large model and can make more efficient online inferences due to its small model size. Despite its breakthrough in classification problems, KD to recommendation models and ranking problems has not been studied well in the previous literature. This dissertation is devoted to developing knowledge distillation methods for recommender systems to fully improve the performance of a compact model.\nWe propose novel distillation methods designed for recommender systems. The proposed methods are categorized according to their knowledge sources as follows: (1) Latent knowledge: we propose two methods that transfer latent knowledge of user/item representation. They effectively transfer knowledge of niche tastes with a balanced distillation strategy that prevents the KD process from being biased towards a small number of large preference groups. Also, we propose a new method that transfers user/item relations in the representation space. The proposed method selectively transfers essential relations considering the limited capacity of the compact model. (2) Ranking knowledge: we propose three methods that transfer ranking knowledge from the recommendation results. They formulate the KD process as a ranking matching problem and transfer the knowledge via a listwise learning strategy. Further, we present a new learning framework that compresses the ranking knowledge of heterogeneous recommendation models. The proposed framework is developed to ease the computational burdens of model ensemble which is a dominant solution for many recommendation applications.\nWe validate the benefit of our proposed methods and frameworks through extensive experiments. To summarize, this dissertation sheds light on knowledge distillation approaches for a better accuracy-efficiency trade-off of the recommendation models.", "sections": [{"title": "1.1 Research Motivation", "content": "In this era of information explosion, recommendation system (RS) is being used as a core technology in web page search services (e.g., Naver, Google) and personalized item recommendation services (e.g., Amazon, Netflix). In particular, RS has established itself as an essential technology for E-commerce and Over-the-top (OTT) media services in that it supports users' decision-making process and maximizes corporate profits.\nRecent RS tends to adopt increasingly complex and large models to better understand the complex nature of user-item interactions. A large model with numerous parameters can accurately capture the users' complex preferences through excellent expression ability, and thus has a high recommendation accuracy. However, such a large model incurs high computation costs and high inference latency, which has become one of the major obstacles to real-time service. To reduce the model size while maintaining accuracy, we focus on knowledge distillation (KD), a model-independent strategy that transfers knowledge from a well-trained large model to a compact model. The compact model trained with KD has an accuracy comparable to that of the large model and can make more efficient online inferences due to its small model size. Despite its breakthrough in classification problems, KD to recommendation models and ranking problems has not been studied well in the previous literature.\nThis dissertation is devoted to developing KD methods for the recommendation model for a better accuracy-efficiency trade-off. We aim to effectively transfer two knowledge sources of the recommendation model; latent knowledge and ranking knowledge. Latent knowledge refers to all information about users, items, and their relationships discovered and stored in the model, providing detailed explanations of the model's final prediction. Ranking knowledge refers to the relative preference order among items, providing direct guidance on the model prediction."}, {"title": "1.2 Contributions", "content": "Figure 1.1 shows an overview of our works included in this thesis. We provide a summary of our contributions as follows:\n\u2022 Latent knowledge distillation (Chapter 2 to 4): We propose Distillation Experts (DE), Personalized Hint Regression (PHR), and Topology Distil-"}, {"title": "1.3 Thesis Organization", "content": "The rest of this thesis is organized as follows. In Chapter II, we propose DE for transferring the latent knowledge in a balanced way and RRD for transferring the ranking knowledge based on a relaxed permutation probability. In Chapter III, we present PHR, which is a follow-up study of DE, removing all hyperparameters of DE. In Chapter IV, we propose FTD and HTD for transferring the topological structure built upon the relations in the representation space. In Chapter V, we present IR-RRD, which is a follow-up study of RRD, for improving the distillation quality based on item-side ranking regularization. In Chapter VI, we present ConCF for generating a more generalizable model by using online knowledge distillation among heterogeneous learning objectives. In Chapter VII, we propose HetComp for compressing knowledge of heterogeneous recommendation models into a compact model. In Chapter VIII, we make a conclusion and present future research directions."}, {"title": "II. A Knowledge Distillation Framework for Recommender System", "content": "Recent recommender systems have started to employ knowledge distillation, which is a model compression technique distilling knowledge from a cumbersome model (teacher) to a compact model (student), to reduce inference latency while maintaining performance. The state-of-the-art methods have only focused on making the student model accurately imitate the predictions of the teacher model. They have a limitation in that the prediction results incompletely reveal the teacher's knowledge. In this paper, we propose a novel knowledge distillation framework for recommender system, called DE-RRD, which enables the student model to learn from the latent knowledge encoded in the teacher model as well as from the teacher's predictions. Concretely, DE-RRD consists of two methods: 1) Distillation Experts (DE) that directly transfers the latent knowledge from the teacher model. DE exploits \u201cexperts\u201d and a novel expert selection strategy for effectively distilling the vast teacher's knowledge to the student with limited capacity. 2) Relaxed Ranking Distillation (RRD) that transfers the knowledge revealed from the teacher's prediction with consideration of the relaxed ranking orders among items. Our extensive experiments show that DE-RRD outperforms the state-of-the-art competitors and achieves comparable or even better performance to that of the teacher model with faster inference time."}, {"title": "2.1 Introduction", "content": "In recent years, recommender system (RS) has been broadly adopted in various industries, helping users' decisions in the era of information explosion, and playing a key role in promoting corporate profits. However, a growing scale of users (and items) and sophisticated model architecture to capture complex patterns make the size of the model continuously increasing [1, 2, 3, 4]. A large model with numerous parameters has a high capacity, and thus usually has better recommendation performance. On the other hand, it requires a large computational time and memory costs, and thus incurs a high latency during the inference phase, which makes it difficult to apply such large model to real-time platform.\nMotivated by the significant success of knowledge distillation (KD) in the computer vision field, a few work [1, 2] have employed KD for RS to reduce the size of models while maintaining the performance. KD is a model-agnostic strategy to accelerate the learning of a new compact model (student) by transferring"}, {"title": "2.3 Problem Formulation", "content": "In this work, we focus on top-N recommendations for implicit feedback. Let U and I denote the set of users and items, respectively. Given collaborative filtering (CF) information (i.e., implicit interactions between users and items), we build a binary matrix \\(R \\in \\{0,1\\}^{|U|\\times|I|}\\). Each element of R has a binary value indicating whether a user has interacted with an item (1) or not (0). Note that an unobserved interaction does not necessarily mean a user's negative preference on an item, it can be that the user is not aware of the item. For each user, a recommender model ranks all items that have not interacted with the user (i.e., unobserved items) and provides a ranked list of top-N unobserved items."}, {"title": "2.4 Proposed Framework\u2014DE-RRD", "content": "We propose DE-RRD framework which enables the student model to learn both from the teacher's predictions and from the latent knowledge encoded in the teacher model. DE-RRD consists of two methods: 1) Distillation Experts (DE) that directly transfers the latent knowledge from the teacher, 2) Relaxed Ranking Distillation (RRD) that transfers the knowledge revealed from the teacher's predictions with direct consideration of ranking orders among items. This section is organized as follows. We first describe each component of the proposed framework: DE in Section 2.4.1, RRD in Section 2.4.2. Then, we explain the end-to-end optimization process in Section 2.4.3. The overview of DE-RRD is provided in Figure 2.2."}, {"title": "2.4.1 Distillation Experts (DE)", "content": "In this section, we provide the details of DE which distills the latent knowledge from the hidden representation space (i.e., the output of the intermediate layer) of the teacher to the corresponding representation space of the student. We first introduce \u201cexpert\u201d to distill the summarized knowledge that can restore the"}, {"title": "Expert for distillation", "content": "DE exploits \"expert\" to distill knowledge from the teacher's hidden representation space. An expert, which is a small feed-forward network, is trained to reconstruct the representation on a selected intermediate layer of the teacher from the representation on the corresponding intermediate layer of the student. Let \\(h_t(\\cdot)\\) denote a mapping function to the representation space (\\(\\in \\mathbb{R}^{d_t}\\)) of the teacher model (i.e., a nested function up to the intermediate layer of the teacher). Similarly, let \\(h_s(\\cdot)\\) denote a mapping function to the student's representation space (\\(\\in \\mathbb{R}^{d_s}\\)). The output of the mapping function can be a separate representation of a user, an item (e.g., BPR [21]) or their combined representation (e.g., NeuMF [22]) based on the base model's structure and the type of selected layer. Here, we use user u as an example for convenience. An expert E is trained to reconstruct \\(h_t(u)\\) from \\(h_s(u)\\) as follows:\n\n\nNote that in the KD process, the teacher model is already trained and frozen. By minimizing the above equation, parameters in the student model (i.e., \\(h_s(\\cdot)\\)) and the expert are updated.\nThe student model has smaller capacity compared to the teacher (\\(d_s << d_t\\)). By minimizing the equation 4, the student learns compressed information on the user's preference that can restore more detailed knowledge in the teacher as accurate as possible. This approach provides a kind of filtering effect and improves the learning of the student model."}, {"title": "Expert selection strategy", "content": "Training a single expert to distill all the CF knowledge in the teacher is not sufficient to achieve satisfactory performance. The CF knowledge contains vast information of user groups with various preferences and item groups with diverse characteristics. When a single expert is trained to distill the knowledge of all the diverse entities, the information of the weakly correlated entities (e.g., users that have dissimilar preferences) is mixed and reflected in the expert's weights. This leads to the adulterated distillation that hinders the student model from discovering some users' preferences.\nTo alleviate the problem, DE puts multiple experts in parallel and clearly distinguishes the knowledge that each expert distills. The key idea is to divide the representation space into exclusive divisions based on the teacher's knowledge and make each expert to be specialized in distilling the knowledge in a division (Fig."}, {"title": "2.4.2 Relaxed Ranking Distillation (RRD)", "content": "We propose RRD, a new method to distill the knowledge revealed from the teacher's predictions with direct consideration of ranking orders among items. RRD formulates this as a ranking matching problem between the recommendation list of the teacher model and that of the student model. To this end, RRD adopts the classical list-wise learning-to-rank approach [7]. Its core idea is to define a probability of a permutation (i.e., a ranking order) based on the ranking score predicted by a model, and train the model to maximize the likelihood of the ground-truth ranking order. For more details about the list-wise approach, please refer to [7].\nHowever, merely adopting the list-wise loss can have adverse effects on the ranking performance. Because a user is interested in only a few items among the numerous total items [8], learning the detailed ranking orders of all the unobserved items is not only daunting but also ineffective. The recommendation list from the teacher model contains information about a user's potential preference on each unobserved item; A few items that the user would be interested in"}, {"title": "Sampling interesting/uninteresting items", "content": "The first step of RRD is to sample items from the teacher's recommendation list. In specific, RRD samples K interesting items and L uninteresting items for each user. As a user would not be interested in the vast majority of items, the interesting items should be sampled from a very narrow range near the top of the list, whereas the uninteresting items should be sampled from the wide range of the rest. To sample the interesting items, we adopt a ranking position importance scheme [26, 1] that places more emphasis on the higher positions in the ranking list. In the scheme, the probability of the k-th ranked item to be sampled is defined as: \\(p_k \\propto e^{-k/T}\\) where \\(T\\) is the hyperparameter that controls emphasis on top positions. With the scheme, RRD samples K interesting items according to the user's potential preference on each item (i.e., item's ranking) predicted by the teacher. To sample the uninteresting items that corresponds the majority of items, we use a simple uniform sampling. Concretely, RRD uniformly samples L uninteresting items from a set of items that have lower rankings than the previously sampled interesting items."}, {"title": "Relaxed permutation probability", "content": "Then, RRD defines a relaxed permutation probability motivated by [7]. For user u, \\(\\pi_u^T\\) denotes a ranked list of all the sampled items (K + L) sorted by the original order in the teacher's recommendation list. \\(r_u^s\\) denotes ranking scores on the sampled items predicted by the student model. The relaxed permutation probability is formulated as follows:"}, {"title": "Optimization of RRD", "content": "RRD is jointly optimized with the base model's loss function in the end-to-end manner as follows:\n\nwhere \\(\\theta_s\\) is the learning parameters of the student model and \\(\\lambda_{RRD}\\) is a hyperparameter that controls the effects of RRD. The base model can be any existing recommender, and \\(L_{Base}\\) corresponds to its loss function. The sampling process is conducted at every epoch. The loss function of RRD is defined to distill the knowledge of users in the mini-batch:"}, {"title": "2.4.3 Optimization of DE-RRD", "content": "The proposed DE-RRD framework is optimized in the end-to-end manner as follows:\n\nwhere \\(\\theta_s\\) is the learning parameters of the student model, \\(\\theta_{DE}\\) is the learning parameters of DE (i.e., the selection network and the experts). The base model can be any existing recommender, and \\(L_{Base}\\) corresponds to its loss function."}, {"title": "2.5 Experiments", "content": "We validate the superiority of DE-RRD on 12 experiment settings (2 real-world datasets \u00d7 2 base models \u00d7 3 different student model sizes). We first provide extensive experiment results supporting that DE-RRD outperforms the state-of-the-art competitors. We also provide both quantitative and qualitative analyses to verify the rationality and superiority of each proposed component. Lastly, we provide hyperparameter study."}, {"title": "2.5.1 Experimental Setup", "content": "Datasets. We use two public real-world datasets: CiteULike [27], Foursquare [28]. We remove users and items having fewer than five ratings for CiteULike, twenty ratings for Foursquare as done in [21, 22, 29]. Data statistics are summarized in Table 2.1.\nBase Models. We validate the proposed framework on base models that have different architectures and optimization strategies. We choose a latent factor model and a deep learning model that are broadly used for top-N recommendation with implicit feedback.\n\u2022 BPR [21]: A learning-to-rank model for implicit feedback. It assumes that observed items are more preferred than unobserved items and optimizes Matrix Factorization (MF) with the pair-wise ranking loss function.\n\u2022 NeuMF [22]: The state-of-the-art deep model for implicit feedback. NeuMF combines MF and Multi-Layer Perceptron (MLP) to learn the user-item interaction, and optimizes it with the point-wise objective function (i.e., binary cross-entropy).\nTeacher/Student. For each base model and dataset, we increase the number of learning parameters until the recommendation performance is no longer increased, and use the model with the best performance as Teacher model. For each base model, we build three student models by limiting the number of learning parameters. We adjust the number of parameters based on the size of the last hidden layer. The limiting ratios (\\(\\phi\\)) are \\{0.1, 0.5, 1.0\\}. Following the notation of the previous work [1, 2], we call the student model trained without the help of the teacher model (i.e., no distillation) as \u201cStudent\u201d in this experiment sections.\nComparison Methods. The proposed framework is compared with the following methods:\n\u2022 Ranking Distillation (RD) [1]: A KD method for recommender system that uses items with the highest ranking from the teacher's predictions for distilling the knowledge.\n\u2022 Collaborative Distillation (CD) [2]: The state-of-the-art KD method for recommender system. CD samples items from teacher's predictions based on their ranking, then uses them for distillation. As suggested in the paper, we use unobserved items only for distilling the knowledge."}, {"title": "Finally, DE-RRD framework consists of the following two methods:", "content": "\u2022 Distillation Experts (DE): A KD method that directly distills the latent knowledge stored in the teacher model. It can be combined with any prediction-based KD methods (e.g., RD, CD, RRD).\n\u2022 Relaxed Ranking Distillation (RRD): A KD method that distills the knowledge revealed from the teacher's predictions with consideration of relaxed ranking orders among items.\nEvaluation Protocol. We follow the widely used leave-one-out evaluation protocol [22, 30, 29]. For each user, we leave out a single interacted item for testing, and use the rest for training. In our experiments, we leave out an additional in-teracted item for the validation. To address the time-consuming issue of ranking all the items, we randomly sample 499 items from a set of unobserved items of the user, then evaluate how well each method can rank the test item higher than these sampled unobserved items. We repeat this process of sampling a test/validation item and unobserved items five times and report the average results.\nAs we focus on the top-N recommendation task based on implicit feedback, we evaluate the performance of each method with widely used three ranking metrics [22, 29, 8]: hit ratio (H@N), normalized discounted cumulative gain (N@N), and mean reciprocal rank (M@N). H@N measures whether the test item is present in the top-N list, while N@N and M@N are position-aware ranking metrics that assign higher scores to the hits at upper ranks.\nImplementation Details for Reproducibility. We use PyTorch to implement the proposed framework and all the baselines, and use Adam optimizer to train all the methods. For RD, we use the public implementation provided by the authors. For each dataset, hyperparameters are tuned by using grid searches on the validation set. The learning rate for the Adam optimizer is chosen from {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}, the model regularizer is chosen from \\{10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\\}. We set the total number of epochs as 1000, and adopt early stopping strategy; stopping if H@5 on the validation set does not increase for 30 successive epochs. For all base models (i.e., BPR, NeuMF), the number of negative sample is set to 1, and no pre-trained technique is used. For NeuMF, the number of the hidden layers is chosen from {1, 2, 3, 4}.\nFor all the distillation methods (i.e., RD, CD, DE, RRD), weight for KD loss (\\(\\lambda\\)) is chosen from \\{1,10^{-1},10^{-2}, 10^{-3}, 10^{-4},10^{-5}\\}. For DE, the number of experts (M) is chosen from \\{5, 10, 20, 30\\}, MLP is employed for the experts and the selection network. The shape of the layers of an expert is \\{[d_s \\rightarrow (d_s+d_t)/2 \\rightarrow d_t]\\} with relu activation, and that of the selection network is \\{[d_t \\rightarrow M]\\}. We select the last hidden layer of all the base models to distill latent knowledge. We put the experts according to the structure of the selected layer; For the layer where user and item are separately encoded (i.e., BPR), we put M user-side experts and"}, {"title": "2.5.2 Performance Comparison", "content": "Table 2.2 shows top-N recommendation accuracy of different methods in terms of various ranking metrics. In summary, DE-RRD shows the significant improvement compared to the state-of-the-art KD methods on two base models that have different architectures and optimization strategies. Also, DE-RRD consistently outperforms the existing methods on three different sizes of the student model in Figure 2.4. We analyze the results from various perspectives.\nWe first observe that the two methods of the proposed framework (i.e., DE, RRD) improve the performance of the student model. DE directly distills the teacher's latent knowledge that includes detailed information on users, items, and the relationships among them. This enables the student to be more effectively trained than finding such information from scratch with a limited capacity. RRD distills the knowledge from the teacher's predictions based on the relaxed ranking approach which makes the student to effectively maintain the ranking orders of interesting items predicted by the teacher. Unlike the existing methods (i.e., RD, CD), it directly handles the ranking violations among the sampled items, which can lead to better ranking performance.\nAlso, we observe that RRD achieves large performance gain particularly in NeuMF (\\(\\phi\\) = 0.1). One possible reason is that NeuMF is trained with the point-wise loss function (i.e., binary cross-entropy) which considers only one item at a time. In general, it is known that the approaches considering the preference orders between items (e.g., pair-wise, list-wise) can achieve better ranking performance than the point-wise approach [6]. RRD enables the model to capture the ranking orders among the unobserved items, so that it can lead to the large performance gain. Interestingly, we observe that the prediction-based KD methods (i.e., RD, CD, RRD) can have an adverse effect when the model size is large (NeuMF with \\(\\phi\\) = 0.5, 1.0 in Figure 2.4). We conjecture that this is because when a model has sufficient capacity to achieve comparable performance to the teacher, enforcing it to exactly mimic the teacher's prediction results can act as a strong constraint that rather hinders its learning.\nIn addition, we observe that DE-RRD achieves the best performance among all the methods in general. DE-RRD enables the student to learn both from the teacher's prediction and from the latent knowledge that provides the bases for such predictions. Interestingly, DE-RRD also shows a large performance gain when the student model has the identical structure to the teacher model (i.e., self-distillation with \\(\\phi\\) = 1.0 in Figure 2.4). This result shows that it can be also used to maximize the performance of the existing recommender.\nLastly, we provide the result of the online inference efficiency test in Table 2.3. All inferences are made using PyTorch with CUDA from Tesla P40 GPU and Xeon on Gold 6148 CPU. The student model trained with DE-RRD achieves comparable performance with only 10-50% of learning parameters compared to the teacher. The smaller model requires less computations and memory costs, so it can achieve lower latency. In particular, deep recommender (i.e., NeuMF) which has a large number of learning parameters and complex structures takes more benefits from the smaller model size. On real-time RS application that has larger numbers of users (and items) and has a more complex model structure, DE-RRD can lead to a larger improvement in online inference efficiency."}, {"title": "2.5.3 Design Choice Analysis", "content": "We provide both quantitative and qualitative analyses on the proposed methods and alternative design choices (i.e., ablations) to verify the superiority of our design choice. The performance comparisons with the ablations are summarized in Table 2.4.\nFor DE, we consider three ablations: (a) Attention (b) One expert (large) (c) One expert (small). As discussed in Section 2.4.1, instead of the selection strategy, attention mechanism can be adopted. We also compare the performance of one large expert\u00b3 and one small expert. Note that DE, attention, and one expert (large) has the exact same number of learning parameters for experts. We observe that the increased numbers of learning parameters do not necessarily contribute to performance improvement ((a) vs. (c) in BPR).\nWe also observe that the selection shows the best performance among all the ablations. To further investigate this result, we conduct qualitative analy-sis on user representation spaces induced by each design choice. Specifically, we first perform clustering4 on user representation space from the teacher model to find user groups that have strong correlations (or similar preferences). Then, we visualize the average performance gain (per group) map in Figure 2.5. We observe that distilling the knowledge by the attention, one large expert can cause performance decreases in many user groups (blue clusters), whereas the selection improves the performance in more numbers of user groups (red clusters). In the ablations (a)-(c), the experts are trained to minimize the overall reconstruction errors on all the diverse entities. This makes the information of weakly correlated entities to be mixed together and further hinders discovering the preference of a particular user group. Unlike the ablations, DE clearly distinguishes the knowledge that each expert distills, and makes each expert to be trained to distill only the knowledge of strongly correlated entities. So, it can alleviate such problem. The expert selection map of DE is visualized in Figure 2.6. We can observe that each expert gets gradually specialized in certain user groups that share similar preferences during the training.\nFor RRD, we consider two ablations: (d) and (e). The ablations are intended to show the effects of the proposed relaxed ranking. Concretely, we apply the list-wise loss (i.e., no relaxation) on all the sampled items (interesting and unin-teresting items) for (d), on the top-ranked items (interesting items) for (e). Note that all the methods use the same number of items for distillation. We observe that merely adopting the list-wise loss has adverse effects on the ranking perfor-mance. First, (d) learns to match the full ranking order among all the sampled items. Learning the detailed order among the uninteresting items is not necessar-ily helpful to improve the ranking performance, and may further interfere with focusing on the interesting items. Also, (e), which only considers the interesting items, shows even worse performance than Student. The list-wise loss does not take into account the absolute ranking positions of the items; a ranking order can be satisfied regardless of the items' absolute positions. Since (e) does not consider the relative orders between the interesting items and the uninteresting items, it may push such interesting items far from the top of the ranking list."}, {"title": "2.5.4 Hyperparameter Analysis", "content": "We provide analyses to offer guidance of hyperparameter selection of DE-RRD. For the sake of space, we report the results on Foursquare dataset with \\(\\phi\\) = 0.1. We observe similar tendencies on CiteULike dataset. For DE, we show the effects of two hyperparameters: \\(\\lambda_{DE}\\) that controls the importance of DE and the number of experts in Figure 2.7a. For RRD, we show the effects of two hyperparameters: \\(\\lambda_{RRD}\\) that controls the importance of RRD and the number of interesting items (K) in Figure 2.7b. In our experiment, the number of uninteresting items is set to the same with K. Note that for all graphs value '0' corresponds to Student (i.e., no distillation).\nBecause the types of loss function of the proposed methods are different from that of the base models, it is important to properly balance the losses by using \\(\\lambda\\). For DE, the best performance is achieved when the magnitude of DE loss is approximately 20% (BPR), 2-5% (NeuMF) compared to that of the base model's loss. For RRD, the best performance is achieved when the magnitude of RRD loss is approximately 7-10% (BPR), 1000% (NeuMF) compared to that of the base model's loss. For the number of experts and K, the best performance is achieved near 10-20 and 30-40, respectively. Lastly, we show the effects of"}, {"title": "2.6 Summary", "content": "This paper proposes a novel knowledge distillation framework for recommender system, DE-RRD, that enables the student model to learn both from the teacher's predictions and from the latent knowledge stored in a teacher model. To this end, we propose two novel methods: (1) DE that directly distills latent knowledge from the representation space of the teacher. DE adopts the experts and the expert selection strategy to effectively distill the vast CF knowledge to the student. (2) RRD that distills knowledge revealed from teacher's predictions with direct considerations of ranking orders among items. RRD adopts the relaxed ranking approach to better focus on the interesting items. Extensive experiment results demonstrate that DE-RRD significantly outperforms the state-of-the-art competitors."}, {"title": "III. Personalized Hint Distillation", "content": "Nowadays, Knowledge Distillation (KD) has been widely studied for recommender system. KD is a model-independent strategy that generates a small but powerful student model by transferring knowledge from a pre-trained large teacher model. Recent work has shown that the knowledge from the teacher's representation space significantly improves the student model. The state-of-the-art method, named Distillation Experts (DE), adopts cluster-wise distillation that transfers the knowledge of each representation cluster separately to distill the various preference knowledge in a balanced manner. However, it is challenging to apply DE to a new environment since its performance is highly dependent on several key assumptions and hyperparameters that need to be tuned for each dataset and each base model. In this work, we propose a novel method, dubbed Personalized Hint Regression (PHR), distilling the preference knowledge in a balanced way without relying on any assumption on the representation space nor any method-specific hyperparameters. To circumvent the clustering, PHR employs personalization network that enables a personalized distillation to the student space for each user/item representation, which can be viewed as a generalization of DE. Extensive experiments conducted on real-world datasets show that PHR achieves comparable or even better performance to DE tuned by a grid search for all of its hyperparameters."}, {"title": "3.1 Introduction", "content": "In the era of information explosion, Recommender System (RS) has played a key role in helping users' decisions and improving the cooperate profits [22, 21]. Recently, the size of RS is continuously increasing because of a deep and sophisticated model architecture to capture the complex user-item interaction and a growing scale of users and items. A large model with numerous learning parameters generally has better recommendation performance. However, it also requires large computational costs and high inference latency, which becomes the major obstacle for model deployment and real-time inference [1, 2, 31].\nTo tackle this problem, Knowledge Distillation (KD) has been widely studied for RS [1, 2, 31, 32, 33]. KD is a model-agnostic strategy that generates a small but powerful model (i.e., student) by distilling knowledge from a previously trained large model (i.e., teacher). Recent methods [31, 34, 33] have shown that the knowledge from the teacher's intermediate layer can significantly improve the student model. In specific, they transfer the knowledge from the representation"}, {"title": "3.2 Preliminary and Related Work", "content": "Training a larger model with numerous learning parameters becomes a common practice to achieve state-of-the-art performances in recommender system (RS) [31, 1, 2", "8": ".", "36": ".", "15": "parameter pruning [12", "17": "have been ap-plied to RS, and they have effectively decreased the computational costs for the inference phase. However, they are only applicable to specific models (e.g., kd-tree for metric space-based models [18, 30", "2": ".", "39": "."}]}