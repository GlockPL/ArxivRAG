{"title": "Latent Schr\u00f6dinger Bridge: Prompting Latent Diffusion for Fast Unpaired Image-to-Image Translation", "authors": ["Jeongsol Kim", "Beomsu Kim", "Jong Chul Ye"], "abstract": "Diffusion models (DMs), which enable both image generation from noise and inversion from data, have inspired powerful unpaired image-to-image (I2I) translation algorithms. However, they often require a larger number of neural function evaluations (NFEs), limiting their practical applicability. In this paper, we tackle this problem with Schr\u00f6dinger Bridges (SBs), which are stochastic differential equations (SDEs) between distributions with minimal transport cost. We analyze the probability flow ordinary differential equation (ODE) formulation of SBs, and observe that we can decompose its vector field into a linear combination of source predictor, target predictor, and noise predictor. Inspired by this observation, we propose Latent Schr\u00f6dinger Bridges (LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and develop appropriate prompt optimization and change of variables formula to match the training and inference between distributions. We demonstrate that our algorithm successfully conduct competitive I2I translation in unsupervised setting with only a fraction of computation cost required by previous DM-based I2I methods.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (DMs) [9, 26, 28], which learn to generate data from noise by iterative denoising, achieves state-of-the-art results in a wide variety of generative learning tasks such as unconditional generation [4], solving inverse problems [3], text-to-image generation [23], and so on. A particular trait of DMs is that they enable exact inversion from data to latent noise by solving the probability flow or-"}, {"title": "2. Related Works", "content": "Unpaired image-to-image (I2I) translation. Given two or more image domains, the goal of I2I translation is to translate images from one domain to another while maintaining input image structure. Early works such as Pix2Pix [10] worked with paired datasets that consist of structurally similar image pairs, i.e., input-label pairs, and train Generative Adversarial Networks (GANs) [6] to maximize pixel-wise similarity between translated inputs and labels. However, such strategy is not applicable to unpaired datasets, which have motivated works such as cycle-consistent GANs [39], Contrastive Unsupervised Translation [21], etc., which regularize GANs with some weaker notion of similarity between inputs and translated outputs. More recent works such as SDEdit [20] or Dual Diffusion Implicit Bridge (DDIB) [31] leverage powerful diffusion priors to translate images from one domain to another by corrupting the input image with Gaussian noise, then denoising with target domain diffusion model.\nFast diffusion sampling. While diffusion models (DMs) possess powerful generative performance, they often re-"}, {"title": "3. Background", "content": "Mathematical formulation. Let P0 and P1 be two data distributions on Rd. Schr\u00f6dinger Bridges (SBs) are Stochastic Differential Equations (SDEs) of the form\n$\\\\begin{equation} d x_{t}=\\mathbf{u}(x_{t},t) d t+\\sqrt{\\tau} d w_{t} \\\\end{equation}$"}, {"title": "4. Latent Schr\u00f6dinger Bridge", "content": "In the following three sections, we construct Latent Schr\u00f6dinger Bridge (LSB) ODE for fast unpaired image-to-image (I2I) translation. Concretely, in Section 4.1, we show that the velocity for the Schr\u00f6dinger Bridge (SB) probability flow ordinary differential equation (ODE) can be decomposed into three interpretable terms. In Section 4.2, we derive change-of-variables formulae that allow us to approximate each term with diffusion score functions. In Section 4.3, we introduce practical techniques for implementing the proposed method using Stable Diffusion."}, {"title": "4.1. SB ODE Vector Field Decomposition", "content": "The solution to Eq. (4) is given as\n$\\begin{equation} \\mathbf{v}(x_{t}, t)=\\mathbb{E}_{(x_{0}, x_{1}) \\sim P_{t \\mid 01}(:, . \\mid x_{t})}[\\mathbf{u}(x_{t} \\mid x_{0}, x_{1}, t)] \\end{equation}$"}, {"title": "Using General Predictors.", "content": "Recall that the predictors \\$\\hat{x}_{0}, $\\hat{x}$_{1}, \\hat{\\epsilon}$ are posterior means of x0, x1, with respect to the joint P01 and the conditional Ptt01 defined in Eq. (10). However, learning P01(the solution to entropy-regularized"}, {"title": "4.2. Matching VP and SB Signal-to-Noise Ratios", "content": "Without loss of generality, let us consider how we may use $\\hat{\\epsilon}_{v p}$ to simulate source domain and noise predictors $\\hat{x}$0 and $\\hat{\\epsilon}$. By time symmetry of SB ODEs, we can easily derive"}, {"title": "4.3. Leveraging Stable Diffusion Models", "content": "To mitigate the increased computational cost of the proposed method, we employ a single pre-trained Stable Diffusion model to obtain estimates in both the source and target domains. This approach offers several advantages. First, implementing our SB decomposition in the latent domain reduces the dimensionality of samples xt in the latent domain and the associated diffusion models, thanks to the encoder's dimensionality reduction role in the SB model. Consequently, this reduces overall computational complexity. For simplicity, and with a slight abuse of notation, we use xt to denote the latent variable update. Second, by assigning different text description for source and target domain through text prompt, we can condition a single diffusion model to compute predictors for both source and target domain. However, a n\u00e4ive application of this approach results in suboptimal performance. In the following, we provide a detailed description of practical considerations.\nPrompt optimization. In this study, we assume an unsupervised setting, where paired datasets and textual descriptions for each domain are not available. For example, if we aim to translate dog images into \"wild\" domain, which includes various wild feline species, it is insufficient to describe the target domain merely as \u201cwild animal\u201d. Thus, we optimize the text embedding using a given set of images from a domain to condition a SD model, as illustrated in Figure 1. Specifically, we employ the textual inversion technique [5], designed for personalized image generation. For further details, please refer the Appendix. Empirically, we found that less than 1k images are enough to obtain text embeddings that leads to proper predictors of LSB ODE.\nODE initial point. In the absence of target estimation x1, we define the initial point for the proposed LSB ODE by"}, {"title": "5. Experiments", "content": "5.1. Fast Unpaired I2I Translation\nBaselines. We address the unpaired image-to-image translation problem using a pre-trained Stable Diffusion 1.5. As our method solely uses an ODE, and does not rely on feature or attention manipulation, we set Dual Bridge [31] and SDEdit [20] using SD 1.5 as comparable baselines. Also, we compare with PnP [34] which could be interpreted as Dual Bridge with diffusion feature and self-attention injection. To ensure a fair comparison, we employ the same text embeddings, numer of neural function evaluation (NFE), and CFG scale across all baselines. Specifically, for Dual Bridge and PnP, we allocate half of the NFEs to each of the inversion and sampling processes. For SDEdit, we use an initial SNR that matches the initial SNR of the proposed method. For more details, please refer to Appendix. In the main experiment, we use 8 NFEs for all cases, unless specified otherwise.\nDatasets. In this paper, we examine the proposed method for Cat \u2192 Dog, Horse \u2192 Zebra, and Dog \u2192 Wild tasks"}, {"title": "5.2. Ablation Study", "content": "Effect of and initial time to. To demonstrate the effects of and initial time to on the LSB, we conduct an ablation study on the Cat\u2192Dog task, with the results visualized in Figure 7. As to increases to 1, the resulting images become progressively blurrier and lose details from the source image (see last row). This is because we initialize xto according to Eq. (21), and a larger to results in a significant SNR mismatch between xt and the training samples input to the VP diffusion model. Furthermore, increasing introduces additional noise into the translation trajectory, resulting in blurrier outputs due to a blurred posterior mean computed at the final denoising step. Conversely, decreasing t introduces insufficient noise for effective translation, leading to outputs that retain too many details from the source image and disrupt the intended image translation. Based on the findings from this ablation study, we set to = 0.2 and \\sqrt{T} = 2.5 with 8 NFEs for all tasks in the main experiment.\nLSB component ablation. To assess the impact of each component designed for practical considerations (see Sec 4.3), we sequentially remove the time-dependent noise predictor, CFG, the final denoising step, and SNR matching when performing horse \u2192 zebra translation task. All other hyperparameters including NFE, to and are kept consis-"}, {"title": "6. Conclusion", "content": "In this paper, we proposed Latent Schr\u00f6dinger Bridge (LSB) ordinary differential equation formulation for fast unpaired image-to-image translation via pretrained latent diffusion models. We decomposed the SB probability flow ODE velocity into a linear combination of three terms \u2014 source domain image predictor, target domain image predictor, and Gaussian noise predictor. We then developed change-of-variables formulae for mitigating signal-to-noise ratio mismatch between diffusion and SB variables. Finally, we proposed various practical techniques for making our method work using a pretrained Stable Diffusion. We verifed that our method beats diffusion-based I2I methods such as SDEdit or Dual Diffusion Implicit Bridge with significantly fewer neural net evaluations."}]}