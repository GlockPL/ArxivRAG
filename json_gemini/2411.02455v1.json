{"title": "An Exploration of Higher Education Course\nEvaluation by Large Language Models", "authors": ["Bo Yuan", "Jiazi Hu"], "abstract": "Course evaluation is a critical component in higher\neducation pedagogy. It not only serves to identify limitations in\nexisting course designs and provide a basis for curricular\ninnovation, but also to offer quantitative insights for university\nadministrative decision-making. Traditional evaluation methods,\nprimarily comprising student surveys, instructor self-assessments,\nand expert reviews, often encounter challenges, including\ninherent subjectivity, feedback delays, inefficiencies, and\nlimitations in addressing innovative teaching approaches. Recent\nadvancements in large language models (LLMs) within artificial\nintelligence (AI) present promising new avenues for enhancing\ncourse evaluation processes. This study explores the application\nof LLMs in automated course evaluation from multiple\nperspectives and conducts rigorous experiments across 100\ncourses at a major university in China. The findings indicate that:\n(1) LLMs can be an effective tool for course evaluation; (2) their\neffectiveness is contingent upon appropriate fine-tuning and\nprompt engineering; and (3) LLM-generated evaluation results\ndemonstrate a notable level of rationality and interpretability.", "sections": [{"title": "I. INTRODUCTION", "content": "COURSE evaluation is a systematic process that\nexamines various aspects of a course, including its\nobjectives, contents, teaching methods, instructor\nperformance, and student learning outcomes. The primary aim\nof course evaluation is to enhance teaching quality, delivering\nnumerous benefits to instructors, students and administrators\n(Table I). Beyond offering critical data to aid administrators in\ncurriculum optimization and resource allocation, course\nevaluation also provides a solid foundation for instructors'\nself-reflection and pedagogical improvement [1].\nDespite the established importance of traditional course\nevaluation methods, the complexity and scale of contemporary\neducation have highlighted several limitations. First, student\nsurveys, a commonly used evaluation method in higher\neducation, are affected by factors that may undermine their\nreliability [2, 3]. For instance, student feedbacks can be\nemotionally driven or restricted by subjective perspectives,\npotentially failing to capture the full efficacy of teaching.\nAdditionally, the design and wording of survey questions may\nintroduce subtle biases, which can compromise objectivity and\nreliability. Moreover, since surveys are usually conducted at\nfixed intervals, such as the end of semesters, they may also\nlack the timeliness needed for effective, real-time feedbacks\non teaching activities.\nSecond, classroom observation, while intuitively valuable\nfor capturing teacher-student interaction and teaching\ndynamics, presents challenges of its own. This method is\nresource-intensive and susceptible to the observer's personal\njudgments and level of engagement, which can greatly affect\nthe reliability of the results. Furthermore, limited observation\nsessions make it difficult to obtain a comprehensive view of\nthe entire teaching cycle. Finally, although expert reviews of\ncourse materials can provide in-depth insights, they are costly\nin terms of time and labor, rendering large-scale evaluations\nimpractical. Additionally, expert assessments may be\ninfluenced by personal preferences, potentially affecting the\nfairness and effectiveness of the evaluations [4].\nTo improve the quality of undergraduate education\nevaluation, the Ministry of Education of China issued the\nImplementation Plan for Undergraduate Education\nEvaluation in General Higher Education (2021\u20132025) in\nJanuary 2021. This plan, grounded in the principles of \"course\ndevelopment through evaluation, improvement through\nevaluation, management through evaluation, and strengthening\nthrough evaluation\", encourages the extensive application of\ninternet technologies, big data, and artificial intelligence to\nanalyze large-scale educational data, thereby reducing the\nworkload on universities while enhancing efficiency [5].\nThis shift highlights an urgent need in higher education for\nintelligent tools capable of delivering comprehensive, timely,\nand precise analyses of multidimensional course-related data,\nwith minimum workload and resource demands. In response,\nthis study actively explores the potential of large language\nmodels (LLMs) in analyzing unstructured course data,\noffering objective and in-depth evaluation results for both\ninstructors and administrators. By properly addressing the\nbalance of fairness, efficiency, and quality, this research marks\nthe first step towards paving innovative pathways in university\ncourse evaluation.\nThe remainder of this paper is organized as follows:\nSection II provides a brief review of the role of LLMs in\nhigher education and the challenges that they are facing.\nSection III demonstrates how LLMs can be used to analyze\nthe dynamics of classroom discussion, representing a micro-\nlevel approach to course evaluation. A systematic case study\non 100 university courses is conducted in Section IV,\nillustrating the efficacy of LLMs in macro-level course\nevaluation. Section V concludes with a summary of the key\nfindings and a discussion on current challenges and future\ndevelopments for LLMs in higher education."}, {"title": "II. LLMS AND HIGHER EDUCATION", "content": "LLMs are advanced natural language processing models\nbuilt on deep learning techniques, notably the Transformer\narchitecture [6]. Trained on extensive text datasets, LLMs can\neffectively understand and generate human-like language.\nTypically, LLMs comprise tens of billions of parameters,\nmaking them capable of capturing and understanding complex\nlinguistic structures and contextual information.\nRecent advancements in LLMs, such as the development of\nGPT (Generative Pre-trained Transformer) [7], have spurred\ntransformative progress in natural language processing. These\nmodels are generally developed through a \"pre-training + fine-\ntuning\" paradigm: they are first pre-trained on large-scale\ngeneral datasets, and subsequently fine-tuned for specific\ntasks. This process endows LLMs with notable generalization\ncapabilities, enabling versatile language processing across\ndiverse domains and contexts.\nIn higher education, LLMs offer promising applications for\nautomating the processing and analysis of extensive\neducational data, supporting the teaching, learning, and\nassessment processes. By enhancing teaching efficiency and\nenabling personalized learning, LLMs contribute to a more\nintelligent and data-driven educational paradigm [8, 9]."}, {"title": "B. The Role of LLMs in Higher Education", "content": "LLMs have transformed not only the roles of learners and\neducators but also the broader teaching philosophies and the\neducational ecosystem. From the learner's perspective, LLMS\ncan enhance essential skills such as complex problem-solving,\ncritical thinking, and creativity, which are key competencies in\ntoday's dynamic society. Meanwhile, by generating highly\ninteractive and diverse learning materials, LLMs can\neffectively engage learners, enriching the learning process and\nenhancing overall learning experiences.\nFor educators, LLMs can automate routine tasks such as\ngrading assignments and providing explanations for\nfundamental concepts, thereby allowing instructors to devote\nmore time to creative and strategic instructional activities.\nAdditionally, LLMs facilitate a shift in the educator's role\nfrom a primary knowledge provider to a guide and facilitator,\nwith an increased focus on supporting personalized learning\nand fostering critical thinking skills. LLMs also expand\nteaching philosophies by enabling learning beyond traditional\nclassroom settings, as students can access AI-assisted learning\nresources anytime and anywhere. This accessibility fosters\nlifelong learning opportunities and helps bridge the resource\ngaps and prompt educational equity across different\ngeographic regions.\nIn summary, LLMs contribute to a more open, interactive,\nand collaborative educational ecosystem. A list of major\napplications of LLMs is presented in Table II. However, they\nalso place new demands on educators, who must continually\ndevelop new knowledge and skills to keep pace with\nadvancements in educational technology."}, {"title": "C. Challenges of LLMs in Higher Education", "content": "Despite their promise in the education sector, LLMs face\nseveral challenges [10] that demand collaborative efforts from\nvarious stakeholders. First, LLMs depend on comprehensive\nlearner data to optimize personalized learning experiences,\noften involving sensitive personal information. Therefore,\nrobust data protection measures are essential to prevent misuse\nof learner information. Additionally, fairness and transparency\nin decision-making processes must be prioritized to mitigate\nalgorithmic biases that could impact teachers and students.\nSecond, a known limitation of LLMs is the issue of\nhallucination, where generated contents may be inaccurate or\nmisleading [11, 12]. This poses a risk of disseminating\nincorrect information to students. To prevent such outcomes,\neffective mechanisms are necessary to validate and, if needed,\ncorrect the generated content, ensuring alignment with\neducational standards.\nThird, education encompasses not only knowledge\ntransmission but also the emotional and interpersonal\nconnections that foster student development [13]. LLMs\ncurrently lack the capacity to replicate the emotional support\nand value-shaping roles provided by human educators.\nTherefore, a balanced integration of technology and human\ninstruction is essential to maintain these vital aspects of\neducation. Finally, LLM-based learning requires students to\npossess strong self-management skills, including effective\ntime management. There is also a risk that students may\nbecome overly reliant on advanced technologies, potentially\noverlooking the value of face-to-face interactions and\ntraditional learning methods."}, {"title": "III. COURSE EVALUATION: MICRO LEVEL", "content": "This section examines the use of LLMs as evaluation tools\nat the micro level, using class discussions in flipped\nclassrooms as a case study. The flipped classroom model\nshifts the traditional teaching structure by requiring students to\nengage in self-directed study through videos, readings, and\nother preparatory materials prior to class. Class time is then\ndedicated to discussions, Q&A sessions, and hands-on\nactivities, with the primary aim of fostering interactivity and\nenhancing students' self-directed learning abilities [14]."}, {"title": "A. Classroom Discussion", "content": "Classroom discussion is an effective teaching method that\naims to cultivate students' abilities to analyze problems,\nquestion assumptions, and develop diverse perspectives,\nhelping students construct a solid knowledge framework [15].\nAdditionally, discussion requires students to express their\ninsights while listening to others, thereby fostering teamwork\nand collaboration. Through independent questioning and\nsharing insights, students increase their engagement and active\nparticipation, which can inspire enthusiasm for learning and\nhelp develop lifelong learning habits.\nIn flipped classrooms, teachers not only design discussion\ntopics but also foster an open, inclusive atmosphere to\nencourage broader participation. During discussions, teachers\nguide students toward deep thinking and critical questioning,\nadapting the discussion's direction as necessary to ensure\nlearning objectives are met and that discussions remain\nfocused. Therefore, evaluating teachers' effectiveness in\norganizing and managing discussions provides valuable\nfeedback for optimizing teaching strategies and improving\ninstructional efficacy.\nHowever, evaluating teachers' discussion facilitation skills\nposes several challenges. First, effective discussion methods\nvary across disciplines; some fields emphasize fostering\ncreativity and innovation, while others prioritize analytical\nthinking and problem-solving. Second, the lack of effective\ntools for systematic and real-time evaluation of classroom\ndiscussions presents difficulties. Third, discussions are\ninherently dynamic and information-rich, requiring teachers to\nmanage diverse viewpoints while continually guiding students\ntoward critical analysis and reflection.\nTherefore, evaluators must be able to observe how teachers\nmanage the flow and pace of discussions, as well as their\nability to maintain coherence and depth in these complex and\ndynamic settings. In many cases, conducting this type of\nevaluation manually would be impractical, underscoring the\nneed for developing effective evaluation methods [16, 17]."}, {"title": "B. An Example of Discussion", "content": "The following excerpt provides an English translation of a\nclassroom discussion transcript focused on environmental\nprotection, involving one teacher and three students. Open-\nended discussions like this often include vast amounts of\ninformation and frequent shifts sub-topics, making them\nchallenging for traditional manual evaluation methods.\n1) Opening Statement\nTeacher: Today, we're going to discuss a very important\ntopic: environmental protection. Do you know how vital\nenvironmental protection is to our lives? Who would like to\nstart by sharing their thoughts?\nStudent Zhang: I think environmental protection mainly\nmeans not littering.\nTeacher: Great point! Not littering is indeed part of\nprotecting the environment. But beyond that, what other\nmethods can help protect the environment? Student Liu, what\nare your thoughts?\nStudent Liu: I think we could use fewer plastic bags and\nswitch to eco-friendly bags.\nTeacher: Excellent suggestion! Reducing plastic bag use\ncan indeed help lower pollution. Student Cao, do you have any\nadditional ideas?\n2) Closing Remarks\nTeacher: Through today's discussion, we've explored many\naspects of environmental protection, and I hope everyone will\napply these eco-friendly practices in their daily lives. Does\nanyone have anything else to add?\nStudent Zhang: I think we could organize a campus\nenvironmental campaign to get more people involved.\nTeacher: That's a fantastic suggestion! Let's brainstorm\nideas for organizing this campaign. Environmental protection\nis a shared responsibility, so let's start with small actions and\nwork together for a better environment.\nTeacher: That's all for today's discussion. Please consider\nother eco-friendly actions you could take, and we'll continue\nthis discussion next time."}, {"title": "C. Analysis of Discussion Organization", "content": "This analysis focuses on a key question: What kind of\nteaching organization skills did the teacher demonstrate during\nthe discussion? The LLM used for evaluation is GPT-40 [18],\nreleased by OpenAI in May 2024, known for its advancements\nin logical reasoning, mathematical calculations, and problem-\nsolving capabilities over is predecessor, GPT-4. The extracted\nentities and their relationships from the complete discussion\nare listed in Table III.\nBased on the prompt: \u201cAnalyze the discussion between the\nteacher and students on environment protection, analyzing\nclassroom dynamics based on Bloom's educational objectives,\nfocusing on problem introduction, discussion organization,\nand guiding students' thinking\", the output is as follows:\n1) Introduction of the Topic\nThe teacher began with an open-ended question: \"How\nimportant is environmental protection to our lives?\" This\nquestion served as a direct topic introduction, stimulating\nstudents' initial thoughts on environmental protection and\nestablishing a foundation for subsequent discussions. In\nBloom's taxonomy [19], this falls under the levels of\n\"remembering\" and \"understanding,\" as it requires students to\nrecall prior knowledge and articulate their understanding.\nIn response to Student Zhang's answer, the teacher further\ninquired: \"Besides not littering, what other methods can we\nuse to protect the environment?\u201d This question encouraged\nstudents to engage in deeper thinking, moving towards the\n\"applying\" and \"analyzing\u201d stages, where they constructed\ntheir understanding of environmental protection by connecting\nspecific actions to broader concepts.\n2) Organization of the Discussion\nThe teacher guided different students in sequence, posing\nquestions to deepen each response. For instance, when Student\nCao mentioned planting trees, the teacher invited students to\nconsider how to improve air quality, thereby encouraging\nconnections between environmental protection and broader\nlife contexts. This structured, progressive approach helps\nmaintain discussions continuity and fosters deeper learning.\nAligned with Bloom's \u201canalyzing\u201d and \u201ccreating\" levels,\nthe teacher led students to connect various environmental\nprotection measures, analyze their interrelationships, and\nsynthesize ideas. This not only enhanced cognitive\nengagement but also encouraged analytical and problem-\nsolving skills development.\n3) Guiding Students' Thinking\nThe teacher consistently encouraged students to explore\ncomplex issues. For example, after Student Cao's suggestion\nto \"plant more trees\", the teacher asked, \"Why is air quality\nimportant for our health?\" This follow-up question encouraged\nstudents to think progressively. The teacher's use of prompts\nlike \"As students, what other ways can we take action to\nprotect the environment?\u201d fostered personal reflection and\nconsideration of actionable steps.\nThis guidance aligns with the \"evaluating\" level in Bloom's\ntaxonomy, where students must not only comprehend and\napply knowledge but also assess and choose effective\nmeasures, enhancing their critical thinking abilities."}, {"title": "4) Conclusion Stage", "content": "The teacher concluded by asking students to reflect on the\ndiscussion, providing specific directions for self-learning. This\ncorresponds to the \u201cremembering\u201d and \u201ccreating\u201d levels in\nBloom's taxonomy, reinforcing the discussion's content while\nlaying a foundation for future environmental actions."}, {"title": "D. Summary of the Discussion Analysis", "content": "This evaluation highlights the LLM's ability to effectively\nanalyze and assess a classroom discussion, identifying key\nelements such as problem introduction, exploration of specific\nmeasures, behavioral guidance, and summary of action plans.\nAccording to its analysis, the teacher's questioning strategy\nled students to incrementally deepen their understanding of\nenvironmental protection, ultimately steering the discussion\ntowards actionable real-world initiatives. This approach\naligned well with the stages of Bloom's taxonomy, thereby\nenhancing students' cognitive depth.\nIn this case, the LLM demonstrated its capacity to extract\nessential information and perform a multidimensional analysis\nof the classroom discussion by effectively processing\nextensive discussion data. Compared to traditional evaluation\nmethods, LLMs can assess classroom discussions from\nvarious perspectives, such as logical flow, creativity, and the\nprogression of topic. This approach not only yields more\ncomprehensive evaluations but also reduces the considerable\nworkload associated with manual evaluations.\nMoreover, LLMs can be used to evaluate student\nparticipation and performance in discussions, offering\npersonalized feedback that highlights individual strengths and\nareas for improvement. This feedback enables students to\nreflect on their discussion performance, enhancing their\nlearning outcomes and increasing the educational value of\ndiscussions [20]. For instance, Table IV presents the LLM-\ngenerated comments and scores for each student, showcasing\nhow these insights contribute to a richer discussion experience\nand more targeted educational feedback."}, {"title": "IV. COURSE EVALUATION: MACRO LEVEL", "content": "In October 2019, the Ministry of Education of China issued\nthe Implementation Guidelines on the Development of\nNational-Level First-Class Undergraduate Courses, aimed at\nestablishing a quality-oriented course evaluation mechanism\nand building a diversified curriculum system [21]. As of 2024,\nthree rounds of applications have been conducted [22].\nFor management purposes, it is common practice for\nuniversities to develop a dynamic curriculum structure based\non their unique teaching objectives and program specifications\nto ensure that courses keep pace with advancements in\ntechnology and societal demands. Regular evaluations of\ncourses are typically conducted through feedback channels\nsuch as student surveys, teaching observations, expert reviews,\nand data analysis. Based on the evaluation results, necessary\nadjustments are expected to be made to the courses.\nIn these evaluation processes, various written materials\nsubmitted by instructors serve as key evidence for assessment,\ntypically encompassing a wide range of texts, charts, and\nvideo materials. However, it is evident that human, financial,\nand time resources required for large-scale course evaluations\ncreate severe constraints, adversely affecting the depth,\nbreadth, and timeliness of evaluations. This section will\nillustrate the feasibility and potential of LLMs in automating\ncourse evaluation through real-world case studies."}, {"title": "A. Experiment Settings", "content": "The case study was based on the materials of 100 courses\nselected from a recent course evaluation conducted at a major\nuniversity in central China. The collected course information\nincludes basic course details, teaching team composition,\ncourse objectives (i.e., knowledge and skill levels that students\nare expected to achieve), course development and practice\n(covering course development history, content, assessment\nmethods and student feedbacks), course features and\ninnovations, and course development plans (including five-\nyear plans, issues to be addressed, and reform directions).\nThe evaluation indicators were provided by the university,\nas shown in Table V, including first- and second-level\nevaluation indicators, along with the corresponding scores\n(specific observation points are not listed here due to space\nlimitations). All settings aim to validate the general\napplicability of LLMs in course evaluation.\nSeveral preliminary tests were conducted on various\nmainstream LLMs, and based on their stability and usability in\ncourse evaluation, two representative models were selected for\ncomparison: Kimi, released by Moonshot AI [23], and Llama,\nreleased by Meta AI [24], both of which are capable of\neffectively handling the long-text inputs involved in course\nevaluation. Additionally, due to Llama's open-source nature, it\nwas fine-tuned using historical course evaluation data,\nresulting in a revised model referred to as Llama-UKP in this\nstudy, to demonstrate the enhancement of LLM performance\nthrough targeted fine-tuning.\nTo achieve optimal performance in complex tasks, prompt\ndesign should adhere to a set of established principles [25].\nFor instance, using a clear format to provide essential task\nbackground information and detailed descriptions helps the\nmodel understand the task's context and constraints. Prompts\nshould also contain explicit instructions, guiding the model on\nthe desired output content and format. For complex tasks, a\nhierarchical design in prompts is advisable, breaking down the\noriginal task into several steps or subtasks to ensure that each\nstep has a clear objective and avoiding overwhelming the\nmodel with overly complex information in a single attempt."}, {"title": "B. Quantitative Evaluation Results", "content": "The scoring statistics of the two models on the 100 courses\nare presented in Table VI, which includes the highest score,\nlowest score, mean score, variance, and excellence rate\n(defined as scores\u226585 points). It is evident that, compared to\nKimi, Llama-UKP produced a more reasonable range of\nevaluation scores, resulting in an excellence rate of\napproximately one-fourth. In contrast, Kimi tended to assign\nhigher scores, with nearly three-fourths of the courses scoring\n85 points or above."}, {"title": "C. Interpretability of Llama-UKP", "content": "Interpretability in AI refers to the ability to understand and\nexplain how a model makes its decisions or predictions. It\ninvolves making the model's reasoning process transparent,\nthereby fostering trust and comprehension among end-users\nand stakeholders regarding the system's outcomes [28]. In the\ncontext of course evaluation models, interpretability is\nessential because it enables educators and decision-makers to\nunderstand the factors influencing the model's assessment.\nThis insight not only enhances the credibility and fairness of\nthe evaluations but also facilitates targeted improvements in\nteaching methods and curriculum design by highlighting areas\nwhere the model identifies opportunities or challenges. For\ninstance, there were 15 secondary indicators used in this case\nstudy. Therefore, understanding each indicator is particularly\nimportant for the model's interpretability.\nTo further illustrate the validity of Llama-UKP, this study\ncorrelated its evaluation scores with the feedbacks that it\ngenerated in an indicator-by-indicator manner. Two examples\nare presented in Table VIII and Table IX, corresponding to the\ncourse objectives indicator and the practical teaching indictor,\nrespectively. For each indicator, two courses were selected for\ncomparison with their scores and feedbacks received from the\nevaluation model. For instance, in Table VIII (second row),\nLlama-UKP identified an absence of a clear connection\nbetween course objectives and career development. Similarly,\nin Table IX (second row), Llama-UKP suggested that the\ncourse documents lacked sufficient evidence to adequately\ndemonstrate how the projects can promote students' overall\ndevelopment. Consequently, both courses received relatively\nlow scores on the corresponding indicators."}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "AI-related technologies have seen numerous applications in\nhigher education, but their use in course evaluation is still\nrelatively rare. This presents a unique opportunity to\ninvestigate how AI can enhance and streamline the assessment\nprocess in academic settings. This paper conducted a\npioneering exploration of course evaluation based on large\nlanguage models, aiming to provide new insights and reliable\nsupport for higher education management in the era of\ngenerative AI. The research investigated three mainstream\nLLMs: GPT-40, Kimi, and Llama, focusing on both the micro-\nlevel evaluation of class discussions and the macro-level\nevaluation of course design and curriculum development. The\neffectiveness of LLMs was validated through real classroom\nteaching cases and comprehensive course materials. The\nexperimental results indicate that LLMs can efficiently\nprocess vast amounts of text data, enabling automated and\ncomprehensive analyses of courses, thereby reducing the\nburden of manual evaluations and minimizing potential biases.\nMoreover, the scoring of the fine-tuned Llama-UKP was more\nplausible than that of Kimi, achieving good consistency with\nthe human expert evaluations.\nHowever, the vast number of parameters in LLMs makes\ntheir internal decision-making processes difficult to trace, and\ntheir interpretability remains a significant challenge. Although\ntheir internal black-box mechanism is unlikely to be fully\nresolved in the short term, in practice, LLMs can provide\ntargeted comments and actionable suggestions for each\nevaluation indicator based on specific course materials. This\ntype of information can enhance the interpretability and\ncredibility of LLMs and directly guide teachers in improving\ntheir teaching practices.\nIt is important to note that LLMs still face challenges in\nunderstanding complex or ambiguous concepts. Additionally,\ntheir accuracy and stability in handling long texts require\nfurther improvement. There is also room for enhancement in\nthe discriminability of scores and the customization of\nfeedbacks, which are crucial for increasing the level of\nacceptance and recognition of evaluation results. Meanwhile,\nsome courses may correspond to technical fields that lack\nsufficient public data for training LLMs, potentially resulting\nin deficiencies in the model's capabilities. To address these\nissues, it is suggested that historic evaluation results from\nhuman experts can be fully exploited to build high-quality\nknowledge databases, thereby driving the intelligent\nevaluation model with both knowledge and data [29].\nMoreover, it is crucial to acknowledge that the findings of\nthis study, while applicable to course evaluations in Chinese\niversities, may not directly translate to higher education\ninstitutions in other countries. The logical organization of\ncourses, cultural contexts, teaching methodologies, and\nevaluation criteria can vary significantly across these\neducational systems [30, 31]. For instance, some universities\noften emphasize critical thinking, student-centered learning,\nand diverse assessment methods, which may influence how\ncourses are structured and evaluated. Additionally, cultural\nfactors can shape students' expectations and interactions\nwithin the classroom, impacting both teaching practices and\nassessment standards. Future research should consider these\ndifferences to enhance the relevance and effectiveness of\nLLMs and to adapt LLM-based evaluation models\naccordingly, ensuring they are tailored to the specific needs\nand contexts of diverse educational environments.\nIn the future, Al will be ubiquitous in education, fostering\ncollaborative innovation with humanity. The capabilities of\nLLMs in numerical computation, logical reasoning, and\ncontextual analysis are consistently developing [32, 33] and\nmay even gradually develop a certain capacity for emotional\ninteraction. They will also be able to analyze multimodal data\n(such as videos, audio, and images) [34], thus expanding A\u0399\ncourse evaluation models to encompass subjects like music,\nart, as well as experimental and practical disciplines that\ncontain substantial non-textual information. Furthermore, on\nthe premise of having a clear understanding of the prospects\nand challenges of generative Al for human learning [35],\nLLMs can not only evaluate existing courses but also be\nincorporated into the entire teaching process. Through in-\ndepth collaboration with teachers and education experts,\nLLMs are anticipated to play a significant role in the planning,\ndesign, optimization, and implementation of a diverse range of\ncourses, promoting profound transformations and sustainable\ndevelopments in teaching theory and practice."}]}