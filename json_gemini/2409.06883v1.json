{"title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task", "authors": ["Yuya Fujisaki", "Shiro Takagi", "Hideki Asoh", "Wataru Kumagai"], "abstract": "The progress in text summarization techniques has been remarkable. However the task of accurately extracting and summarizing necessary information from highly specialized documents such as research papers has not been sufficiently investigated. We are focusing on the task of extracting research questions (RQ) from research papers and construct a new dataset consisting of machine learning papers, RQ extracted from these papers by GPT-4, and human evaluations of the extracted RQ from multiple perspectives. Using this dataset, we systematically compared recently proposed LLM-based evaluation functions for summarizations, and found that none of the functions showed sufficiently high correlations with human evaluations. We expect our dataset provides a foundation for further research on developing better evaluation functions tailored to the RQ extraction task, and contribute to enhance the performance of the task. The dataset is available at PaperRQ-HumanAnno-Dataset.", "sections": [{"title": "Introduction", "content": "To understand research papers, it's crucial to accurately grasp the Research Question (RQ). The RQ is the specific question set by the authors to address a particular research problem. It guides the research direction and narrows the focus of investigations and experiments. Proper understanding of the RQ is essential for clarifying the research purpose and scope and comprehending the paper's main arguments.\nHowever, research papers tend to have a complex structure, use many technical terms, and have important information scattered throughout the document, making RQ hard to grasp easily.\nConsidering these characteristics of research papers, automatic RQ extraction, which involves identifying the key components of the RQ from the paper and summarizing them into a specific format, and appropriateness evaluation by machines are challenging tasks that have not yet been addressed, to our best knowledge.\nRQ extraction and evaluation can be considered subtasks of document summarization, as they involve selecting and concisely expressing important information from research papers. Applying document summarization techniques may help solve these tasks with reasonable accuracy.\nTo improve the performance of summarization, it is generally necessary first to define a performance evaluation function and then optimize the summarization model to maximize the value of that evaluation function. For example, Lewis et al. (2020) used perplexity as an evaluation function to assess the similarity between human-created summaries and summaries generated by BART. In this way, identifying an appropriate evaluation function is crucial for improving the performance of RQ extraction.\nAn evaluation function's output must strongly correlate with human judgment to accurately measure qualitative improvements in summaries. Assessing the correlation between existing evaluation functions and human evaluation in the context of RQ is crucial. If existing functions do not correlate well, developing RQ-specific evaluation functions will be necessary.\nResearch on automatic evaluation of document summarization has verified the correlation between automatic evaluation functions and human evaluation (Fabbri et al., 2020). However, many of these studies target specific domains, such as news articles, and there may be biases specific to those domains (Kryscinski et al., 2020). Compared to news articles, research papers tend to have a more complex structure, use more technical terms, and have important information scattered throughout the document. Therefore, it is unclear how well existing automatic evaluation functions align with human judgment in RQ understanding evaluation.\nTherefore, in this study, we constructed a new"}, {"title": "Related Work", "content": "Evaluation Functions\nEvaluation of automatic summarization is important for properly measuring the performance of summarization systems. Automatic evaluation functions can be broadly divided into two types: Reference-based and Reference-free. Assuming Document is the original document, Summary is the generated summary, and Reference is the human-created summary, in the Reference-based setting, evaluation is performed using Document, Summary, and Reference. On the other hand, in the Reference-free setting, evaluation is performed using only Document and Summary (Sai et al., 2022).\nIn recent years, it has become clear that evaluation functions using LLM, typified by GPT-4, show higher performance than conventional evaluation functions(Wang et al., 2023; Liu et al., 2023a), attracting much attention. LLM-based evaluation functions work by having the LLM return a score based on a prompt that includes the document to be evaluated and its summary, and in some cases, a reference summary.\nLLM evaluation functions exist in both Reference-based settings and Reference-free settings. In the Reference-based setting, LLMs can more directly consider alignment with reference summaries, but evaluation needs to be performed even when reference summaries do not exist. On the other hand, in the Reference-free setting, the language understanding ability of LLMs can be utilized to directly evaluate the quality of summaries (Wang et al., 2023).\nThere are various types of LLM evaluation functions, differentiated by the presence or absence of the features described in Table 1. Specifically, as categorized in Table 2, the differences are mainly distinguished by whether they include evaluation procedures (Liu et al., 2023a), output scores in batches (Yuan et al., 2023), or require explanations for scores (Chiang and Lee, 2023). More details are provided in Section 4.1.\nDatasets Targeting Academic Papers\nBuilding datasets targeting academic papers is one of the important research challenges in the field of natural language processing. Various datasets have been proposed, such as QASPER (Dasigi et al., 2021), SciCite (Cohan et al., 2019), and others, each focusing on different aspects of academic papers (see Appendix A.1.1 for more details)."}, {"title": "Proposed Dataset", "content": "Dataset\nIn this study, we constructed a dataset targeting 104 papers accepted as long papers at ACL from 2016 to 2023. The papers subject to annotation were limited to those proposing a solution (method) to a specific problem, which is a common format for many ACL papers. In these papers, the RQ is expected to be formulated as \u201cCan a certain \u2018problem' be solved by a certain 'method' ? \".\nFor each paper, we used GPT-4 to estimate the RQ and collected human scores evaluating the quality of the estimated RQ.\nRQ Estimation Method\nWe input the abstracts and introductions of the papers into GPT-4 and used the following three different prompts to extract three RQ for each paper. The specific prompts are listed in Appendix A.2.3.\n1. prompt1: A prompt that simply instructs to estimate the RQ"}, {"title": "Annotation Method", "content": "Annotators\nThe annotation was performed by a total of three people: two researchers who routinely read papers in the field of machine learning and one graduate student majoring in information science. All annotators were male and were not compensated for their work. The annotators were informed about the purpose of the annotation task and how the data would be used in the research. They provided verbal consent to participate in the annotation process.\nAnnotation Perspectives\nEach annotator scored the RQ estimated by GPT-4 from the following three perspectives:\nProblem Score: Does the RQ accurately estimate the true problem? (3 levels from 0 to 2)\nMethod Score: Does the RQ accurately estimate the true method? (3 levels from 0 to 2)\nIs target rq type: Is the RQ in a specific format (proposing a solution to an existing problem)? (2 levels: 0 or 1)\nAnnotation Results and Analysis\nIn general, the annotation results for each data point can vary depending on the annotator. Therefore, by selectively retaining data with high agreement among annotators, a highly reliable dataset can be created.\nWhen the difficulty of annotation is relatively low and there is little variation among annotators,"}, {"title": "Experiment", "content": "Evaluation Functions Used in This Experiment\nIn recent years, evaluation methods using LLM have been actively researched. The automatic evaluation methods compared in the experiments are summarized in Table 2.\nThe details of each method are provided in Appendix A.3.1. In the following, we explain the experimental setup and results.\nExperimental Setup\nIn this study, we evaluated the correlation between human-annotated scores and scores output by various LLM-based evaluation functions using gpt-40-2024-05-13 on the dataset we created. The evaluation settings were based on previous studies. Jain et al. (2023) set the number of few-shots to 5, while Yuan et al. (2023) set the batch size to 10 and the number of loops to 3. Additionally, Liu et al. (2023a) and Chiang and Lee (2023) set the output n to 20. For other setting items such as temperature and top_p, we used the values reported in each paper.\nThe model output obtained as a result of the evaluation may contain text as shown in Table 7. Therefore, it is necessary to extract the actual values from the output. This extraction process was performed using the Python code attached in Appendix A.3.5.\nResults\nWe summarized the correlation coefficients between each evaluation functions and human evaluation in Table 6. The table shows the Pearson correlation coefficients between the scores given by LLM-based evaluation functions and human evaluations for each aspect of RQ quality.\nWhile a correlation coefficient of around 0.5 was obtained for the Method Score, the correlation coefficients for other aspects were below 0.2. This suggests that the LLM-based evaluation functions proposed so far do not correlate well with human evaluations in assessing RQ quality, particularly in aspects other than identifying the method.\nIn contrast, previous studies have reported that these LLM-based evaluation functions correlate to some extent with human evaluations. For example, in studies such as Liu et al. (2023a), the correlation coefficients between automatic evaluation functions and human evaluations were around 0.6 for some aspects, and most exceeded 0.35.\nOur results suggest that the correlation between previously proposed LLM-based evaluation functions and human evaluations may have been overestimated. While these evaluation functions have been reported to correlate with human judgments in tasks such as news summarization, our findings indicate that they may not yet be able to provide evaluations that correlate with human judgments for tasks beyond news summarization, such as RQ evaluation. This result implies the need to develop optimal evaluation functions for each task.\nDiscussion\nThis section investigates common tendencies across evaluation methods, examines method reproducibility, and analyzes performance improvement strategies.\nWe first analyze similarities in incorrectly evaluated RQ sets for each method and the impact of input/output token count on performance.\nNext, we discuss result variability due to sample size and model differences when assessing method reproducibility."}, {"title": "Investigating Common Tendencies across Evaluation Methods", "content": "In this section, we analyze the common tendencies across evaluation methods from two perspectives: examining the similarity of RQ sets with incorrect evaluation values and investigating the impact of input and output token counts on performance. These analyses aim to clarify common tendencies and provide insights for improving future evaluation methods.\nHow similar are the sets of RQ for which incorrect evaluation values were outputted between methods?\nAnalysis of Common Patterns in Misclassified RQ We hypothesized that there might be a trend in the RQ with errors, where errors are defined as estimated scores different from the GT. Figure 2 visualizes the overlap rate between RQ sets with mismatched evaluation values, categorized by score type. To account for varying output ranges, we set thresholds using percentiles and converted them into three or two values. For Problem Score and Method Score, the overlap rate of RQ with errors was high in Liu et al. (2023a), Wang et al. (2023), Yuan et al. (2023), and Gong and Mao (2023), suggesting common issues leading to similar errors. The analysis for Problem Score and Is target rq type is in the appendix.\nCorrelation Analysis with Paper Acceptance Year and Length We analyzed the characteristics of commonly misclassified RQ, hypothesizing"}, {"title": "Reproducibility of the Methods", "content": "In this study, we define reproducibility as the ability to obtain consistent results when repeating an experiment under the same conditions. To the best of our knowledge, reproducibility has not been sufficiently discussed in the context of LLM-based evaluation functions for text generation, despite its importance. We investigate the reproducibility of the methods from two perspectives: the impact of sample count on result variability and the variability due to model differences.\nImpact of sample count on result variability\nThe number of samples from LLM outputs may differ depending on the evaluation function (Table 2). We hypothesized that methods with multiple sampling would have less variability in the results. We conducted three trials each for Wang et al. (2023) (single sampling) and Chiang and Lee (2023) (20 samplings) the same temperature of 1 and visualized the variability of Spearman correlation coefficients using violin plots. Contrary to our hypothesis, Chiang and Lee (2023) sometimes showed more variability (Figure 3), suggesting that multiple sampling may not significantly improve result reliability.\nVariability of results due to model differences\nWe used gpt-4o-2024-05-13 in this study (Section 4.2), but it is necessary to confirm the extent to which evaluation values change when using a different model. We hypothesized that the reliability of results may vary depending on the model, even with the same method. We compared the variability of Spearman correlation coefficients between gpt-4-turbo-2024-04-09 and gpt-4o-"}, {"title": "Analysis for Performance Improvement", "content": "This section describes our attempts to improve performance. We qualitatively analyzed the best-performing methods, confirming the importance of modeling the evaluation procedure. To improve the completeness of the evaluation procedure, we increased the number of steps and verified the impact on performance. Finally, we attempted fine-tuning to test the hypothesis that directly learning scoring trends from data leads to higher performance than prompting-based methods.\nImportance of modeling the evaluation procedure The best-performing methods, Chiang and Lee (2023) and Liu et al. (2023a), estimate the evaluation procedure in a two-step process. First, they estimate the procedure itself, then calculate the final evaluation value based on the estimated procedure. Qualitatively, this method largely reproduces the annotation process (see Appendix A.3.4), suggesting that reproducing the annotation process through modeling may be important for this task.\nImpact of Increasing the Number of Evaluation Procedure Steps on Performance While mod-"}, {"title": "Conclusion", "content": "In this study, we constructed a new dataset that pairs RQ extracted by GPT-4 with their manual evaluations, targeting papers accepted at ACL. Using this dataset, we studied the correlation of GPT-4-based automatic evaluation functions with human evaluation.\nOur experiments revealed that the automatic evaluation functions, which were reported to have high correlation with human annotators in existing studies, showed only low correlation in the RQ evaluation task. This suggests the possibility that appropriate evaluation functions differ depending on the task, supporting the significance of creating and publishing a dataset with human annotations. On the other hand, the method that estimates the evaluation procedure showed relatively high performance in evaluating the Method Score of RQ.\nThe results of this study provide insights for the development of automatic evaluation functions in the RQ generation task for papers. In the future, the design of evaluation functions specialized for the paper domain and the identification of factors contributing to the performance improvement of evaluation functions are expected."}, {"title": "Potential risks", "content": "Our approach uses LLM, which may disadvantage organizations that can't afford them. To address this, we should make these methods widely accessible and explore non-LLM alternatives. Additionally, Over-reliance on automatic RQ extraction might weaken researchers' skills. Therefore, researchers should use these tools to complement their expertise, ensuring they continue to develop their own capabilities."}, {"title": "Limitations", "content": "Limitations of the Dataset\nThe dataset constructed in this study is limited to 104 papers in the field of machine learning. Including papers from fields other than machine learning could lead to the development of models that can be commonly used across various fields, not limited to machine learning. However, due to resource constraints, we were unable to carry out such an expansion in this study. In the future, there is a need to construct datasets targeting a wider range of research fields.\nFurthermore, regarding annotation, there is a possibility that it was difficult to achieve alignment among annotators because there is no firm definition of RQ and their components in the field of machine learning. The definition of RQ may vary from paper to paper, and their components encompass a wide range of aspects, leading to the possibility of different interpretations among annotators. In addition, understanding papers requires specialized knowledge, so differences in the background knowledge of annotators may have influenced the evaluation. In the future, research is needed to organize RQ and their components, particularly in the field of machine learning.\nLimitations of Evaluation\nIn this study, we only conducted evaluations using GPT-4 and were unable to perform evaluations using other LLMs. Conducting evaluations using LLMs other than GPT-4 may provide deeper insights into the performance and characteristics of evaluation functions. In the future, evaluations using a variety of LLMs will be required.\nMoreover, this study was limited to testing LLM-based evaluation functions developed in domains such as news article summarization, and we were unable to propose new evaluation functions that surpass their performance. These existing evaluation functions may not be suitable for evaluating complex targets like RQ in papers. RQ are composed of various components, and understanding the relationships and context between these components is required. Additionally, understanding the technical terms of papers is necessary. Therefore, in the future, it is necessary to develop evaluation functions specialized for RQ evaluation in papers, utilizing the insights obtained in this study."}, {"title": "Appendix", "content": "Related Work\nDatasets Targeting Academic Papers\nPreviously proposed datasets include QASPER (Dasigi et al., 2021), SciCite (Cohan et al., 2019), Meaningful Citations Data Set (Valenzuela-Escarcega et al., 2015), PubMedQA (Jin et al., 2019), PeerRead (Kang et al., 2018), and SciFact (Wadden et al., 2020). QASPER is a QA dataset targeting papers, consisting of questions created by NLP experts who read only the titles and summaries of papers, and answers and supporting evidence provided by other NLP experts who read the entire papers.\nSciCite is a dataset that pairs citation sentences in scientific papers with labels of their citation intent (background information, use of methods, comparison of results, etc.) and can be used for tasks such as classifying citation sentences and predicting citation intent. The Meaningful Citations Data Set is a dataset with labels identifying important citations in academic literature.\nPubMedQA consists of answers from three values (\"yes/no/maybe\") to questions created from the titles and abstracts of medical papers. This dataset can be used to develop content understanding and question-answering systems for medical papers.\nPeerRead contains 14,700 papers submitted to top conferences (ACL, NeurIPS, ICLR), their acceptance/rejection results, and peer review results by 10,700 experts. This dataset is expected to be applied to tasks such as automatic paper evaluation and peer review automation.\nSciFact consists of 1,400 annotated abstracts with scientific claims and supporting evidence, with each abstract labeled as supporting or refuting the claim. This dataset can be used for tasks such as determining the veracity of claims and automatically extracting evidence.\nDataset Creation\nData Selection Criteria\nIn this study, we constructed a dataset consisting of 104 long papers accepted at ACL from 2016 to 2023. We focused on papers published from 2016 onwards because these papers are licensed"}, {"title": "Explanation of Dataset Rights", "content": "The publicly released dataset includes appropriate citation information for the research papers. Additionally, this dataset targets papers published under the Creative Commons Attribution 4.0 license, and have been modified. Consequently, the dataset we have created is also subject to the Creative Commons Attribution 4.0 license."}, {"title": "List of prompts used to extract the RQ", "content": "As shown in Table 6. Prompt 3, which could not fit in this table, is mentioned in Appendix A.2.4."}, {"title": "Example of prompt3", "content": "Prompt 3, which could not fit in Table 6, is as follows:\n<RULE>\nThe system and the assistant exchange messages.\nAll messages MUST be formatted in XML format. XML element ::=  content\nTags determine the meaning and function of the content. The content must not contradict the definition of the tag.\n</RULE>\n\nThis tag defines rules. The defined content is absolute.\nAttributes:\nrole (optional) : A role that should follow the rules. Roles are \"system\" or \"assistant\".\nNotes:\nThe assistant must not use this tag.\n\nThis tag defines a tag. The defined content is absolute.\nAttributes:\nname: A tag name.\nNotes:\nThe assistant must not use this tag.\n\nThis tag represents a system message.\nNotes:\nThe assistant MUST NOT use this tag.\n\nIndicates the end of a message.\n\n\nThis tag represents a thought process. If you use this tag, take a drop deep breath and work on the problem step-by-step.\nAttributes:\nlabel (optional) : A label summarizing the contents.\nNotes:\nThe thought process must be described step by step.\nPremises in reasoning must be made as explicit as possible. That is, there should be no leaps of reasoning.\n\nThis tag represents the problem being attempted to be solved in the paper.\n\nThis tag represents the method or hypothesis used by the authors of the paper to solve PROBLEM.\n\n\nThis tag represents a resaerch question. A research question is a combination of a problem to be solved and a hypothesis or method to approach it. The general form of a research question is as follows.\nCan the PROBLEM be solved by the METHOD?\nCan the PROBLEM be explained by the METHOD?\nNotes:\nThis tag must contain one PROBLEM and one METHOD tag inside. The assistant must then combine the contents of the PROBLEM and METHOD and present the research question as a concise statement.\n\n\nThe assistant is a friendly and helpful research assistant, specifically tasked with analyzing academic papers on machine learning, provided by users. The assistants sole responsibility is to meticulously read the abstracts and introductions of these papers and, using logical reasoning, deduce exactly a key research questions from the paper.\nThe assistant first carefully reads the paper using the THINK tag, and then"}, {"title": "Annotation Score per prompt", "content": "The annotation scores for each prompt used to extract RQ are visualized by the average values of all annotators for Problem Score, Method Score, and Is Target RQ Type. According to Figure 5, the values for prompt 3 are relatively better overall, indicating that prompt 3 has the best performance as a prompt for extracting RQ."}, {"title": "Evaluation Functions Used in This Experiment", "content": "Liu et al. (2023b) propose a method called AUTO-CALIBRATE. In this method, an arbitrary dataset labeled by human experts is first divided into training data and evaluation data. Next, the training data is used to have the LLM create its own scoring criteria. After that, the criteria are narrowed down and refined to create an evaluator closer to human judgment.\nWang et al. (2023) propose a method to evaluate based on LLMs using human-created Aspects and Criteria. They conduct experiments in both reference-based and reference-free settings.\nLiu et al. (2023a) propose a method called G-Eval. This method is characterized by having LLMs create evaluation procedures based on human-created Aspects and Criteria, and then evaluate using those evaluation procedures.\nChiang and Lee (2023), like Liu et al. (2023a), have LLMs create evaluation procedures based on human-created Aspects and Criteria. However, this method is characterized by requiring explanations for the evaluations. They apply two settings: one where the evaluation explanation is analyzed before outputting the evaluation value, and another where the evaluation value is output first and then the evaluation explanation is provided.\nYuan et al. (2023) propose a method called BatchEval. This method is characterized by evaluating in batch units, taking multiple Document and Summary pairs as input.\nGong and Mao (2023) propose a method called CoAScore. This method assumes that Aspects have multiple sub-aspects as lower-level perspectives, and evaluates aspects based on the evaluation values for each inferred sub-aspect.\nJain et al. (2023) propose a method that teaches LLMs evaluation tendencies through few-shot learning. In this case, we performed few-shot learning using a set of Document, Summary, and human annotation as one unit. This method is characterized by not using human-created Aspects or Criteria. As described above, there are various methods to evaluation methods using LLM. These methods have their own characteristics, such as aiming for"}, {"title": "Python snippet for parsing evaluation values from the output results", "content": "def get_score(text):\n  patterns = [\n r\"Score:\\s*(?:.*?:\\s*)?(\\d+)\",\n r\"Score \\(1-5\\):\\s*(\\d+)\",\n r\"Rating:\\s*(?:.*?:\\s*)?(\\d+)\",\n r\"\\\\b(\\d+)\\\\b\",\n r\":\\s*(\\d+)\"\n  ]\n  for pattern in patterns:\n match = re.search(pattern, text)\n  if match:\n rating = match.group (1)\n return float(rating)\n  return np.nan"}, {"title": "Analysis of Common Patterns in Misclassified RQ for Is target rq type", "content": "Figure 6 visualizes the overlap rate between the sets of RQ for which the evaluation values inferred by LLM did not match the human-created GT for Is target rq type."}, {"title": "Do more tokens lead to better performance?", "content": "As shown in the Section 4.3, the existing methods did not correlate well with human evaluations, indicating a need for designing better evaluation functions. To gain insights into what factors should be considered when designing such functions, we decided to investigate the properties of the relatively well-performing methods in our study.\nAmong the methods we employed, those by Chiang and Lee (2023) and others performed relatively"}, {"title": "Impact of Increasing the Number of Evaluation Procedure Steps on Performance", "content": "The results Table 10 show a slight improvement in performance, but the difference is small"}, {"title": "Average Difference from GT Score", "content": "In Figure 10, we visualize the difference between the estimated values of each method and the GT for each score."}, {"title": "Learning the scoring patterns from the dataset", "content": "As mentioned in Section 4.1, the evaluation function used in this experiment is a type of evaluation function that involves trial and error with prompts using GPT-4. However, an alternative approach could be to fine-tune an LLM and learn the evaluation tendencies. To this end, we fine-tuned the open-source LLM Gemma-7b by LoRA to see if it could better align with human ratings (GT). The experimental settings were LoRA rank of 8, alpha of 16, and 1 epoch. And A100 GPU were used, and the SFTTrainer from the Transformer Reinforcement Learning library was utilized. As shown in Table 11, the values are lower than those in Table 5, suggesting that it might be difficult to learn evaluation regularities by fine-tuning a model of around 7B parameters using LoRA. Furthermore, we investigated the impact on performance by varying the split ratio of the training data."}, {"title": "Actual Annotation Guidelines Used", "content": "Introduction\nPurpose of this Task In this task, you will evaluate the accuracy of Research Questions (RQ) extracted by a Language Model (LLM) based on the abstract and introduction of research papers.\nTypes of RQ Covered in this Task RQ come in various forms, but for this task we will focus on papers with the following structure: \u201cCan a certain 'problem' be solved by a certain 'method' ? \u201c In other words, you will be assessing the accuracy of the RQ extracted by the LLM for papers that fit this specific template.\nUtilization of the Evaluation The results of this evaluation can potentially be used to develop the following: A model to classify whether a paper belongs to a particular RQ type. A model to assess the validity of problems, challenges, or proposed methods extracted from a paper (by LLM or other means)."}, {"title": "Evaluation", "content": "Evaluation Targets 3 RQ extracted by the LLM.\nEvaluation Procedure The following is an evaluation procedure for the Problem Score.\nCarefully read the abstract and introduction of the paper\nExtract the problem targeted by this research from the abstract and introduction of the paper\nConfirm whether the problem targeted by this research is correct. For example, confirm whether the specific problem pointed out in the paper is correctly captured, rather than the abstract problem that the field is addressing\nBased on the content confirmed in step 3, evaluate how accurately the extracted Research Question (RQ) captures the problem on a 3-point scale from 0 to 2. Refer to Table 12\nEvaluation Items"}, {"title": "Notes", "content": "Please evaluate the RQ in the order they appear from the top of the CSV file.\nThe \"abstract\" and \"introduction\" columns in the CSV file are generated through PDF parsing. Therefore, equations may not be accurately captured.\nIf the inaccuracy of equations makes it difficult to understand the paper's content, please skip evaluating that RQ."}, {"title": "Utilization of AI Assistants in Research and Writing", "content": "In this study, we mainly utilized AI assistants for creating Python scripts to conduct experiments and for checking spelling and typographical errors during paper writing."}]}