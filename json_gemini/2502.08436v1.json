{"title": "From Haystack to Needle: Label Space Reduction for Zero-shot Classification", "authors": ["Nathan Vandemoortele", "Bram Steenwinckel", "Femke Ongenae", "Sofie Van Hoecke"], "abstract": "We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with LLAMA-3.1-70B and 3.3% (up to 11.1%) with CLAUDE-3.5-SONNET compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.", "sections": [{"title": "1. Introduction", "content": "Zero-shot learning is a powerful paradigm that enables machine learning models to handle new tasks without requiring task-specific training. Instead, these models transfer knowledge from previously learned tasks to tackle novel challenges (Wang et al., 2019). A key application of this paradigm is zero-shot classification. In this approach, models can identify examples from classes they've never seen during training by making connections to known classes through semantic embeddings or attribute spaces (Xian et al., 2017; Wang et al., 2019). This capability addresses a major limitation of traditional supervised learning: the need for labeled training data for every target class. As a result, zero-shot classification is particularly valuable in real-world applications where collecting labeled examples for all possible classes is expensive or impractical (Xian et al., 2017).\nThe field of Large Language Models (LLMs) has further advanced zero-shot classification by leveraging their extensive pretraining on diverse datasets. Consequently, LLMs demonstrate an unprecedented ability to understand and generate human-like text, enabling them to infer semantic relationships between input text and class descriptions without task-specific fine-tuning (Brown et al., 2020; Bommasani et al., 2021; Wei et al., 2022a). For example, when presented with a maintenance log entry like \u201cBearing temperature increased to 85\u00b0C with unusual vibration patterns and metallic noise during operation,\" an LLM can identify critical indicators such as \u201cincreased temperature,\u201d \u201cvibration,\" and \"metallic noise\" to accurately classify the text under the \"bearing failure\u201d category, even if equipment failure classification was not explicitly included in its pretraining. Such versatility highlights the practical utility of LLMs in real-world applications.\nHowever, current approaches to LLM zero-shot classification often rely on simplistic methods that present class options as unstructured, flat lists within the prompt. This becomes increasingly problematic as the number of potential classes expands. Specifically, LLMs face critical limitations when dealing with long contexts due to key limitations, often attributed to factors such as positional bias (Peysakhovich & Lerer, 2023; An et al., 2024) and attention dilution (Peysakhovich & Lerer, 2023), reducing their ability to accurately distinguish between a large set of potential classes. As context length grows, the fixed attention budget of transformers must be distributed across more tokens, as the attention weights are normalized to sum to 1. This shared attention makes it harder to focus strongly on relevant information, as demonstrated in \"Needle in the haystack\" tests (Kamradt, 2023; Hsieh et al., 2024) where LLMs struggle to retrieve key facts from long contexts (Hsieh et al., 2024; Liu et al., 2024). The challenge becomes even more complex when multiple \"needles\" must not only be retrieved, but also reasoned over (LangChain, 2023; Hsieh et al., 2024). As a result, by reducing the label space, LLMs can allocate their attention more effectively, enhancing reasoning processes such as step-by-step thinking (Wei et al., 2023; Kojima et al., 2023), where each step builds upon previous information (Anonymous, 2025).\nReducing the label space is not a new concept and is well-established in multi-label classification, generally falling"}, {"title": "2. Related Work", "content": "Our work builds upon three key research areas, namely zero-shot classification with LLMs, extreme multi-label classification, and sampling strategies used in language model decoding."}, {"title": "2.1. Zero-shot Classification with LLMs", "content": "While initially developed for text classification, LLMs have demonstrated remarkable zero-shot classification capabilities across diverse domains, including image classification through vision-language models (Radford et al., 2021; Alayrac et al., 2022), tabular data classification (Hegselmann et al., 2023), time series classification (Zhang et al., 2024b), and even audio classification (Latif et al., 2023). Early work demonstrated that providing task descriptions and class definitions in natural language could guide models to make reasonable predictions (Kojima et al., 2022; Wei et al., 2022b). This capability was further enhanced through prompt engineering techniques (Sahoo et al., 2024) and the development of instruction-tuned models (Chung et al., 2022). While these approaches have shown promising results, their performance can be sensitive to prompt design and may struggle with nuanced category distinctions. More recent work has focused on improving model reasoning through popular techniques like chain-of-thought reasoning (Wei et al., 2023; Kojima et al., 2023) and combining robustness methods such as self-consistency (Wang et al., 2023). However, these approaches do not fundamentally address the challenges posed by large-scale classification tasks with long context requirements."}, {"title": "2.2. Extreme Multi-label Classification", "content": "There are notable connections between our work and extreme multi-label classification (XMC), a field that addresses classification problems involving thousands to millions of classes (Bhatia et al., 2015; Mittal et al., 2022; Zhu & Zamani, 2024). While XMC tasks have traditionally been approached in a supervised manner, researchers have also explored zero-shot XMC methods (Chang et al., 2020). Early approaches relied on retrieval-based techniques such as TF-IDF (Salton & Buckley, 1988) and BM25 (Robertson & Zaragoza, 2009) to match input texts with label embeddings. A major advancement occurred when Reimers & Gurevych (2019b) demonstrated the effectiveness of dense embeddings for retrieval, which were subsequently applied to various XMC tasks (You et al., 2019; Chang et al., 2020; Jiang et al., 2021). More recent work has focused on LLM-based approaches, emphasizing true zero-shot settings where test labels are entirely unseen, unlike generalized zero-shot settings that may include seen labels. For example, ICXML (Zhu & Zamani, 2024) adopts a hybrid generation-and-retrieval approach, first generating demonstrations with an LLM and then aligning the outputs with available labels to create a shortlist of candidates. LMTX (Zhang et al., 2024a) achieved state-of-the-art performance by employing an LLM as a teacher to iteratively train a bi-encoder. However, these approaches still incorporate retrieval-based components and impose an arbitrary label-space cut-off, effectively constraining the LLM's performance to that of the underlying embedding model. Moreover, we propose a solution that prioritize the role of the LLM as a reasoner for classification, which is applicable to solving classification tasks beyond just text classification."}, {"title": "2.3. Sampling Strategies", "content": "Language model decoding is the problem of generating text by selecting tokens from the model's predicted probability distribution. Early methods like greedy selection and random sampling often led to repetitive or incoherent outputs. Beam search and temperature-based sampling (Ficler & Goldberg, 2017) improved diversity and quality but failed to address the long tail of irrelevant and low-probability tokens, which can collectively hold significant probability mass (Holtzman et al., 2020). This realization led to the development of more distribution-aware sampling strategies. Top-k sampling (Fan et al., 2018) limits choices to the k most probable tokens, while nucleus (Top-p) sampling (Holtzman et al., 2020) selects tokens whose cumulative probability exceeds p. More recently, Min-p sampling (Nguyen et al., 2024) enforces a minimum probability threshold to maintain coherence. Our implementation leverages these distribution-aware thresholds to filter out low-probability classes."}, {"title": "3. Methodology", "content": "Addressing shortcomings of current methods, we present our LSR methodology, which consists of three distinct phases that operate in an iterative process. In this section, we detail each phase of the process, as illustrated in Figure 1."}, {"title": "3.1. Problem Definition", "content": "Let \\(X\\) denote the input space and \\(y = \\{1, ..., K\\}\\) be a finite label space comprising \\(K\\) distinct classes. Given an unlabeled dataset \\(D = \\{x_1,...,x_n\\}\\) where \\(x_i \\in X\\) consisting of \\(n\\) samples, we aim to improve the zero-shot classification performance of an LLM for these \\(n\\) samples, considering the following constraints:\n\u2022 Single-label Multi-Classification Each sample \\(x_i \\in D\\) corresponds to exactly one class-label \\(y_i \\in Y\\)\n\u2022 Closed-world Assumption The complete set of possible unseen classes \\(Y\\) is known and fixed a priori\n\u2022 Unlabeled Data Access to a collection of unlabeled samples \\(D\\) which can be used for training"}, {"title": "3.2. Data Representation and Preprocessing", "content": "The framework operates on dual representations of the data:\n1. Numerical view: \\(X_{num} \\in R^{n\\times d}\\), where \\(d\\) is the feature dimensionality\n2. Semantic view: \\(X_{sem} = \\{s_1, ..., s_n\\}\\), where \\(s_i\\) is the natural language description of \\(x_i\\)\nThe aim of this dual representation approach is to find a statistical correlation between an LLM's predictions and the underlying feature distributions of a novel task. This framework can be applied to classification tasks with arbitrary data types, provided that the data can be represented both numerically and described semantically in natural language."}, {"title": "3.3. Initial Classification", "content": "The LLM starts generating initial pseudo-labels \\(\\hat{y}_0\\) through:\n\\[Y_0 = LLM(x_{sem}, Y, prompt_{template})\\qquad(1)\\]\nwhere \\(prompt_{template}\\) includes Chain-of-Thought (CoT) reasoning (Wei et al., 2023; Kojima et al., 2023) which improves preformance and makes better use of the reduced label space. Initially, all candidate labels are presented as a list of options in the prompt, and the model is tasked to categorize the sample. The full prompt template can be found in Appendix G."}, {"title": "3.4. Probabilistic Classifier", "content": "Based on these pseudo-labels, we train a classifier \\(C\\) on \\((X_{num}, \\hat{y}_0)\\) that outputs probability distributions over classes:\n\\[P(y|x) = C(x_{num}),\\]\nwhere \\(P(y|x) \\in [0,1]\\) and \\(\\sum_{i}P(y_i|x) = 1\\) \\((2)\\)\nThe model is first trained using a hold-out validation strategy to prevent overfitting, then retrained on the complete dataset using the same hyperparameters.\nBy training on the LLM's pseudo-labels, we measure the agreement between predictions based on the underlying feature distributions, effectively quantifying the model's certainty for each class."}, {"title": "3.5. Ranking and Selection", "content": "For any given sample \\(x \\in D\\), the labels are ranked based on their predicted probabilities according to \\(R(x)\\):\n\\[R(x) = \\underset{y}{argsort descending}(P(y|x))\\qquad(3)\\]\nNext, the labels are selected based on the adaptive threshold following Min-p sampling, which includes all labels whose probability exceeds a fraction \\(p\\) of the maximum probability:\n\\[Min\\text{-}p(x, p) = \\{y \\in Y : P(y|x) \\geq p\\cdot max(P(y|x))\\}\\qquad(4)\\]\nWe propose an adapted version of Min-p, denoted as Min-p+, which includes the LLM's current prediction \\(\\hat{y}_t\\) at iteration \\(t\\) during the refinement process:\n\\[Min\\text{-}p_+(x, p, t) = Min\\text{-}p(x, p) \\cup \\hat{y}_t \\qquad(5)\\]\nThe threshold \\(p\\) is optimized to achieve a target average dimensionality \\(k\\) in the reduced label space. Since \\(p\\) is an indirect parameter that affects the final dimensionality, it is more intuitive to control \\(k\\) directly. This relationship can be expressed as:\n\\[\\frac{1}{n} \\sum_{i=1}^{n} w_i|Min\\text{-}p_+(x_i, p,t)| \\approx k \\qquad(6)\\]\nwhere \\(w_i\\) is a class-balancing weight inversely proportional to the frequency of \\(\\hat{y}_t\\). While \\(k\\) represents an average, the number of candidate labels adapts dynamically per sample, where instances with higher uncertainty naturally receive more candidate labels and vice versa. A detailed comparison of thresholding on the label space following different sampling strategies is provided in Appendix \u0412."}, {"title": "3.6. Iterative Refinement", "content": "Our iterative refinement approach, outlined in Algorithm 1, consists of multiple steps that progressively improve classification accuracy. The process begins by obtaining initial predictions from the LLM (line 4), which are used as pseudo-labels to train a baseline classifier (line 8). In each subsequent iteration, we create a reduced label space by integrating the classifier's confidence scores with the LLM's previous predictions (lines 9-11). The LLM then generates new predictions (line 6), but instead of considering all possible labels, it focuses on a filtered set of ranked candidates for each sample. This filtered approach enables more focused CoT reasoning by limiting the LLM's attention to fewer plausible options. The classifier is then retrained using these refined predictions (line 8). The process continues until reaching a predetermined number of iterations \\(i\\) (lines 2-13), where a higher number of iterations typically leads to convergence, at which point no significant changes can be observed in the label space. This iterative process creates a feedback loop where the LLM and classifier help improve each other's performance and potentially correct initial misclassifications with more focused reasoning.\nThe final predictions \\(\\hat{y}_{final}\\) are determined through majority voting (line 14):\n\\[\\hat{y}_{final} = mode(\\{\\hat{y}_0, ..., \\hat{y}_i\\})\\qquad(7)\\]\nBy aggregating predictions from previous iterations, we provide robustness against individual iteration errors. Furthermore, aggregation helps mitigate issues when the process fails to reach proper convergence and circumvents the need for complex stopping criteria."}, {"title": "3.7. Practical Inference", "content": "While the complete iterative methodology described above is valuable for offline evaluation and benchmarking, repeated training cycles during live inference may be impractical for real-world applications. For example, in a system monitoring customer support tickets that need to be classified and routed every few minutes, frequent model retraining would introduce unacceptable costs and latency. Therefore, we propose two alternative strategies:\nFull Inference This approach reuses the saved classifiers from every iteration to generate rankings, following the established methodology. While this maintains the benefits of LSR and reduces computational overhead, it has two main limitations: the classifier probabilities are not calibrated with new test samples, and each iteration requires an LLM call. Due to these constraints, we do not explore this approach further in this paper.\nDirect Inference Alternatively, we propose to train a final classifier \\(C_{final}\\) on the combined LLM predictions from all iterations, effectively distilling an LLM ensemble into a single classifier that remains frozen during deployment.\n\\[C_{final} = train(X_{num}, [Y_0, \u2026\u2026\u2026, \\hat{y}_i])\\qquad(8)\\]\nFor a new sample x, we can then directly use the classifier's probabilities to make predictions:\n\\[P(y|x) = C_{final}(x)\\]\n\\[\\hat{y}_{final} = \\underset{y}{argmax}(P(y|x))\\qquad(9)\\]\nOur experiments show that making an additional LLM prediction on the reduced set of labels from \\(C_{final}\\) (using Min-p sampling from Equation 3 and class-weighting based on the classifier's predictions) only improves performance when dealing with undersampled classes. Thus, direct inference only requires a trained classifier for inference, which allows for the use of larger LLMs or additional test-time scaling techniques during the iterative training phase, enhancing performance at a one-time fixed cost."}, {"title": "4. Experimental Setup", "content": "In this section we describe the datasets, evaluation metrics and implementation details used in our experiments."}, {"title": "4.1. Datasets", "content": "We evaluate our method on seven diverse benchmark datasets, including five text classification datasets from the MTEB benchmark (Muennighoff et al., 2023) (AMAZON MASSIVE SCENARIO, AMAZON MASSIVE INTENT, BANKING77, MTOP DOMAIN, MTOP INTENT), a text classification dataset (DBPEDIA\u00b9), and a tabular classification dataset (CRIME\u00b2). We refer to Appendix A for detailed descriptions. All datasets contain a substantial number of classes (ranging from 11 to 102) with medium-sized sample sets (2974\u20134386 samples)."}, {"title": "4.2. Model Selection and Configuration", "content": "Large Language Models Our framework primarily uses LLAMA-3.1-70B-INSTRUCT (Llama Team, 2024), accessed through the DeepInfra API\u00b3, due to its reproducibility and open-weight nature, which enables verification and replication of our results. We selected this model and other recent open-weight models including GEMMA-2-27B-IT (Gemma Team, 2024) and QWEN-2.5-72B-INSTRUCT(Qwen Team, 2024), as well as the closed-source CLAUDE-3.5-SONNET(OLD) (Anthropic, 2024), to ensure a diverse representation of model architectures and training approaches. Classification is performed in batches of 10 randomly selected samples, and we instruct the LLM to apply CoT reasoning. To ensure deterministic behavior, we set the temperature parameter to 0. The template for the prompt is shown in Appendix G.\nText Embeddings To obtain numerical values for the classification tasks, dense text embeddings are generated using the open-source BGE-LARGE-EN-V1.5 (Xiao & Liu, 2023) Sentence-BERT (SBERT) model (355M parameters, 1.34GB serialized). Through binary quantization in Sentence Transformers (Reimers & Gurevych, 2019a), we compress the vectors from 1024 to 128 dimensions to speed up classifier training.\nClassifier For classification, we utilize CATBOOST (Prokhorenkova et al., 2018), a gradient boosting framework that demonstrates state-of-the-art performance on structured data (Borisov et al., 2021), while being lightweight (<1MB when serialized). We use default parameters with lr = 0.1 and depth = 3. To mitigate overfitting, we use a stratified hold-out validation set, reserving 20% of the data for early stopping while maintaining the original class distribution. Class imbalance is addressed through class weights during training. After determining the optimal number of iterations through early stopping criteria, we retrain the classifier on the complete dataset.\nBoth BGE-LARGE-EN-V1.5 and CATBOOST models were evaluated on standard consumer-grade computing hardware (Intel Core i7-1365U processor @ 5.2 GHz, 16GB DDR4 RAM, running Windows 11)."}, {"title": "4.3. Data Preparation", "content": "Our methodology requires both numerical and semantic representations of the data. Here we detail the preparation process for each view.\nNumerical View For text classification tasks, numerical text embeddings are generated using BAAI/BGE-LARGE-EN-V1.5, which produces 128-dimensional feature vectors as input to the classifier. For tabular classification tasks using the CRIME dataset, which contains both semantic and numerical features, we treat all semantic features as numerical through categorical encoding. Additionally, we extract the hour of the day from the Time variable as a new feature. In total we obtain 6 features (Description, Police District, Resolution, Address, Day of Week, Hour of Day).\nSemantic View To create the semantic representation, we convert all contextual features, whether originally semantic or numerical, into a list of key-value pairs that can be directly incorporated into the prompt. Examples of these representations are provided with the prompt template in Appendix G."}, {"title": "5. Metrics and Evaluation", "content": "Our goal is to measure classification performance, where we assume all classes are of equal importance. Therefore, we evaluate our method using the Macro Fl-score, as it treats all classes equally, regardless of their frequency in the dataset:\n\\[F1_{macro} = \\frac{1}{K}\\sum_{i=1}^{K} \\frac{2(precision_i \\cdot recall_i)}{precision_i + recall_i}\\qquad(10)\\]\nwhere K is the number of classes, and \\(precision_i\\) and \\(recall_i\\) are computed for each class i.\nIt's important to note that while our method involves training during test time, this training occurs exclusively on pseudo-labels generated by the LLM, maintaining the zero-shot nature of our approach. The reported scores are computed against the ground truth labels of the test set."}, {"title": "6. Experiments", "content": "In this section, we present experimental results and key findings of our proposed method. Each experiment was conducted only once to limit expenses without compromising insights."}, {"title": "6.1. Label Space Reduction", "content": "We analyze how varying the label space size (k) affects the performance of LLAMA-3.1-70B across multiple datasets. We measure performance using Macro F1-scores, calculated through majority voting over 15 iterations. For consistency, each experiment with different k values begins with the same zero-shot LLM predictions, starting from randomly shuffled labels in the prompt. Our results follow the complete experimental methodology outlined in Section 3.\nIterations Our analysis reveals two key observations from Figure 2. First, the macro-F1 scores show an initial boost after the first iteration, with the most significant gains occurring within the first five iterations, followed by convergence around the tenth iteration. This early convergence pattern suggests that, while our framework supports extended iterations, substantial performance benefits can be achieved with minimal computational overhead. This behavior is consistent across all values of k, with the AMAZON MASSIVE INTENT dataset (k = 1.5) being a notable exception.\nSecond, we observe distinct performance patterns between different types of datasets. Datasets with rare classes, such as AMAZON MASSIVE SCENARIO/INTENT (see Appendix A), exhibit more volatile performance across iterations. In contrast, datasets with more balanced class distributions, (e.g., BANKING77, CRIME, and DBPEDIA) demonstrate smoother performance curves and generally achieve better results. This disparity can be attributed to the data-driven nature of the process: under-represented classes may not be ranked effectively and, in extreme cases, the ground truth might be excluded from the selected labels altogether. When this occurs, these classes are not predicted by the LLM and cannot be reintroduced in subsequent iterations. The case of AMAZON MASSIVE INTENT (k = 1.5) exemplifies this phenomenon, where performance gains are initially rapid but errors gradually accumulate for difficult classes, rather than resolving them.\nRanking As shown in Table 1, our method also demonstrates improvements even without reduction (k = Full) as a substantial portion of the performance gain stems from the label ranking process alone, despite the LLM not being explicitly instructed to consider label order. This effect is most evident with MTOP DOMAIN, where we observe improvements from ranking but none from reduction, given its already small label space (K = 11). On average, label ranking accounts for 78.6% of the maximum performance gain, while the reduction of the label space contributes to the remaining 21.4% (calculated from Table 1, \\(Average Gain_{k=2.0}\\). While ranking proves effective, it differs fundamentally from typical embedding-based ranking approaches (Karpukhin et al., 2020), which produce static, non-adaptive rankings. In contrast, our classifier is a data-driven ranker which adapts to the task at test-time. These rankings are calibrated across samples and iteratively refined based on the current top prediction from that ranking. For a more detailed analysis and comparison with embedding-based rankings, see Appendix F.\nSelection Our experiments demonstrate that the optimal value of k varies across tasks. We hypothesize that this variation in optimal label space size is related to the semantic similarity between labels. When labels are more semantically similar, a larger label space may be necessary to effectively distinguish between closely related options. This hypothesis is supported by the difference observed in Table 1 between AMAZON MASSIVE SCENARIO and AMAZON MASSIVE INTENT classifications, where INTENT, being a more fine-grained classification of SCENARIO, requires a larger optimal k. Through empirical analysis across datasets, we found that k reaches an optimum at k = 2 and k = 5, consistently yielding the highest average performance improvements. The range k \u2208 [2, 5] appears to provide a good balance between exploration and exploitation, suggesting these values as practical defaults for zero-shot applications.\nMajority Voting The use of majority voting across iterations in our method resembles aspects of Self-Consistency (SC) (Wang et al., 2023), which has similarly demonstrated improvements in zero-shot performance at test time. We therefore conducted a comparison with CoT-SC using a temperature of 0.7 and 15 resamples to ensure an equal comparison before majority voting, as shown in Table 1. While CoT-SC showed a modest average absolute improvement of 1.6%, our method achieved substantially higher gains of up to 7.0% on average. Note that these averages are indicative and are highly dependent on the task. We hypothesize that CoT-SC could be complementary to our technique, as its performance gains originate from a different source, i.e., from sampling different reasoning paths of the model with a high temperature (Wang et al., 2023). With additional resources, further experiments could evaluate the extent of this complementarity."}, {"title": "6.1.1. COMPARISON ACROSS MULTIPLE LARGE LANGUAGE MODELS", "content": "We conduct a comprehensive evaluation of LSR with different sized LLMs and capabilities to demonstrate its general applicability. In this setup, we set k = 2, which we determined to be an optimal value. The results are reported in Table 2 and a figure can also be found in Appendix D.\nOur experiments empirically validate that LSR consistently improves performance across all LLMs and datasets tested. On average, LLAMA-3.1-70B gains 7.0%, GEMMA-2-27B gains 5.1%, QWEN2.5-72B gains 4.9%, and CLAUDE-3.5-SONNET gains 3.3% across all datasets. The magnitude of improvement varies, as the potential for improvement is inversely related to the baseline performance on a given task. This can be attributed to task difficulty: for example, all models score 95% or higher on MTOP DOMAIN and already saturate the benchmark. Additionally, model capability plays a role, as we generally observe smaller performance improvements from more capable models like QWEN2.5-72B and CLAUDE-3.5-SONNET. A notable exception is DBPEDIA, where all models gain at least 11% using LSR. This leads us to hypothesize that more capable models can inherently handle larger label spaces more effectively. Conversely, less capable models show more substantial performance gains from LSR. This finding has important implications, as our method could enable smaller, more accessible models to achieve performance levels comparable to larger, more resource-intensive models."}, {"title": "6.2. Direct Inference", "content": "We evaluate the direct inference approach outlined in Section 3.7 using pseudo-labels accumulated from 15 iterations of LLAMA-3.1-70B-INSTRUCT with k = 2.0, we train a CATBOOST classifier with a 20% stratified hold-out set to prevent overfitting.\nThis classifier demonstrates performance (Table 4 in Appendix C) comparable to the offline methodology (k = 2.0 in Table 1), namely within 0.5% absolute points difference, except for MTOP INTENT where the classifier shows 2.2% lower performance. This could be attributed to epistimic uncertainty or noise in the training data, possibly due to inconsistent labeling by the LLM or inadequate feature representations. Interestingly, when making additional LLM predictions using the classifier's rankings, we observe decreased performance across most metrics (Table 4), except for MTOP INTENT which now improves, to within 0.6% of the offline performance. These results show that the classifier actually exceeds the LLM's zero-shot capabilities, as we effectively distilled knowledge from multiple LLM outputs.\nWhile these results are promising for offline evaluation and benchmarking purposes, our goal is to deploy a frozen classifier for inference. Therefore, we evaluate the classifier's performance on a separate test set of equal size and class distribution to simulate a real-world deployment scenario. Our experiments (Unseen Data in Table 4 of Appendix C) reveal that the classifier's performance degrades substantially across most datasets in this scenario, though mostly maintaining positive improvements. For instance, the improvement margin decreases from +8.1% to +2.3% for CRIME, and in the most severe case, decreases from +14.7% to 3.4% for DBPEDIA. This performance degradation can be attributed to two main factors. First, there is a distribution shift. Changes in zero-shot LLM performance indicate that the test data distribution differs from the training data, affecting the classifier's effectiveness. Second, there are generalization challenges. This is particularly evident in datasets with limited samples per class. For example, in MTOP INTENT, 41 classes have fewer than 5 samples, including 15 classes with just one sample, making it difficult for the classifier to generalize effectively.\nInterestingly, we find that adding an extra LLM inference step helps mitigate performance degradation as it is less susceptible to distribution shift due to extensive pre-training and zero-shot capabilities. This is evidenced by improvements across all datasets, with gains improving from 2.3% to 5.8% for CRIME and -3.4% to +6.5% for DBPEDIA. While this approach can be effective during a cold-start, we recommend collecting a minimum number of samples per class (using pseudo-labels) to improve the classifier's generalization to unseen data. Nevertheless, challenges related to generalization and distribution shifts are inherent to data-driven approaches and should be carefully considered during real-world deployment."}, {"title": "7. Conclusion", "content": "In this paper we present a novel method for improving zero-shot classification performance of LLMs through iterative ranking and reduction of the label space. Our results demonstrate that optimizing the label space significantly improves LLM accuracy in classification tasks. Although the process is computationally intensive, we address this limitation through distillation, resulting in a cost-effective classifier that outperforms standard LLM zero-shot classification. An interesting direction for future work is to explore the impact of different classifiers and numerical representation models. Furthermore, the current reliance on empirically determined parameters also suggests opportunities for automated parameter selection. Another promising avenue for future research is extending this approach to multi-label classification, which would further generalize the applicability of our method in real-world scenarios. Ultimately, these findings advance the field of zero-shot learning and provide a promising direction for developing more efficient and effective classification systems."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "B. Comparison of Sampling Strategies", "content": "We analyze the thresholds of different sampling strategies for the label selection process. In this analysis, we use LLM (LLAMA-3.1-70B) predictions to train the classifier and obtain probability-based rankings. Performance is measured using Hit@k metric using Equation 11, which is also referred to as the \u201chit rate\". The hit rate represents the percentage of instances where the ground truth label appears in the reduced label space, which may vary from sample to sample depending on the strategy.\n\\[Hit@k = \\begin{cases} 1, & \\text{if } r_i \\in T \\\\ 0, & \\text{otherwise} \\end{cases}\\qquad(11)\\]\nWe compare five strategies:\n\u2022 Top-k: Selects a fixed number k of highest probable labels\n\u2022 Top-p (nucleus sampling): Includes labels until the cumulative probability exceeds threshold p\n\u2022 Min-p: Selects labels whose probabilities exceed a fraction of the highest probability\n\u2022 Min-p+: Our adaptation of Min-p that includes the most recent LLM prediction\n\u2022 Min-p+ (w): Min-p+ with class-weighted label space"}, {"title": "C. Direct Inference", "content": "Table 4. Results of LSR with direct inference. LLAMA-3.1-70B w/ LSR is distilled into a probabilistic classifier (CATBOOST). All macro-F1 scores show gains over zero-shot baseline predictions on two different test sets."}, {"title": "D. LSR Performance Across Different LLMs", "content": ""}, {"title": "E. Data Requirement Analysis", "content": "We examine the data requirements by varying the size of the unlabeled dataset used for test-time training, while maintaining the complete test set for consistent evaluation. Using stratified sampling to preserve the original class distributions, we create three representative subsets at 25%, 50%, and 75% of the initial dataset. For each subset, we reserve 20% as a validation set to prevent overfitting during training, following the methodology."}, {"title": "F. Label Space Ranking Comparison", "content": "We compare the zero-shot rankings of the classifier (CATBOOST) during LSR with k = \\(Full\\)"}]}