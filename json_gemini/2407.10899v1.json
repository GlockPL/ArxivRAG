{"title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis", "authors": ["Yunting Liu", "Shreya Bhandari", "Zachary A. Pardos"], "abstract": "Effective educational measurement relies heavily on the cu-ration of well-designed item pools (i.e., possessing the rightpsychometric properties). However, item calibration is time-consuming and costly, requiring a sufficient number of re-spondents for the response process. We explore using sixdifferent LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus) and various combina-tions of them using sampling methods to produce responseswith psychometric properties similar to human answers. Re-sults show that some LLMs have comparable or higher pro-ficiency in College Algebra than college students. No singleLLM mimics human respondents due to narrow proficiencydistributions, but an ensemble of LLMs can better resem-ble college students' ability distribution. The item parame-ters calibrated by LLM-Respondents have high correlations(e.g. > 0.8 for GPT-3.5) compared to their human calibratedcounterparts, and closely resemble the parameters of thehuman subset (e.g. 0.02 Spearman correlation difference).Several augmentation strategies are evaluated for their rel-ative performance, with resampling methods proving mosteffective, enhancing the Spearman correlation from 0.89(human only) to 0.93 (augmented human).", "sections": [{"title": "1 | INTRODUCTION", "content": "Generating sets of well-functioning items for mathematics assessments often requires multiple iterations of calibra-tion, involving extensive human participation. One of the most common techniques used in building an item poolis Item Response Theory (IRT), where ample response-level data is typically required for accurate item calibrationand scaling [1, 2]. This process is time-consuming, costly, and significantly limits the rapid adaptation of educationalassessments to different sets of students. For example, the PISA main survey requires between N = 250 to N = 750 re-spondents per item per country [3]. Thus, the time and cost involved in obtaining responses from human respondentsremains a significant area of resource expenditure.\nWith the advent of advanced Al technologies, novel ways to address these challenges have arisen. Recent devel-opments in Large Language Models (LLMs) are achieving near-human performance [4, 5, 6, 7], leading to speculationabout whether they can competently generate high-fidelity synthetic data without the traditional need for full data col-lection [8, 9, 10]. In our domain, we explore whether the capabilities of LLMs can be leveraged to provide responsesresulting in psychometric properties similar to those derived from human respondents' answers. This research isguided by three critical research questions:\n\u2022 RQ1: Which language model or configuration of language models best mimic human respondent abilities in math-ematics, as measured by Item Response Theory (IRT)?\n\u2022 RQ2: How do the psychometric properties of items fit to human responses compare to those fit to LLM-Respondents?\n\u2022 RQ3: Can the augmentation of human respondent data with LLM-Respondent contributions yield item parame-ters comparable to those obtained from solely increasing human data collection?\nIf this approach is successful, it would mean that questions, including those produced via generative Al [11],could be tested and evaluated en masse nearly instantly for use in a variety of educational contexts such as computertutoring systems and other formative and summative assessment scenarios."}, {"title": "2 | RELATED WORK", "content": ""}, {"title": "2.1 | Simulated Data in Educational Measurement and Educational Data Mining", "content": "Analyzing examinee responses to test questions is indispensable in the field of measurement. While gathering realdata can be time-consuming, costly, and often incomplete, simulation is a useful and economical technique since itcan usually be done on a laptop without additional costs. Therefore, researchers commonly use simulation to validatemodels [12], compare different models [13], and evaluate estimation methods. In fact, among a random sample ofpublications in the field, 60% of the studies used simulation, while the ratio for real data is just 41% [14]. Typically, arespondent distribution is specified, and then the item response level data (i.e., dichotomous or polytomous response)is simulated accordingly, with no thought process involved. Now, thanks to the advancement of generative Al, wecan simulate some response level data and possibly gain insights into the cognitive structure behind the responseprocess. While most work on simulation in educational settings is based on dialogue [15, 16], there are indeed someresearchers conducting item response level data simulation. For example, Xu and Zhang [17] demonstrated the pos-sibility of simulating student behavior based on assessment history. Lu and Wang [18] used insights from teachersto create generative students with various profiles, and then used the generative students' outcomes to guide itemdevelopment.\nIn the realm of educational data mining, gathering real learner data can pose privacy concerns [19] and presentchallenges with the costs of managing logged data [20]. To address these challenges, researchers have at times lever-aged simulated data. LLMs are often used to generate the training datasets needed to train and test other models. Forexample, by using pseudocode to generate synthetic datasets, researchers have been able to develop test cases ofteaching activities to inform the development of a Teaching Outcome Model (TOM) [21]. Similarly, researchers haveproposed using LLMs as data annotators to create synthetic data that can be used to train other models [8], mimickingthe framework of Teacher-Student Learning (TSL) [22]. Simulated data has also been used within the educational datamining community to evaluate latent trait models [23, 24]."}, {"title": "2.2 Data Augmentation", "content": "Data augmentation is a method often employed to increase the volume and diversity of data by generating new datafrom the existing set; it can also be applied to mitigate the 'incomplete data' problem [25]. Having a limited numberof data points often leads to weaker generalization capabilities, which can act as an obstacle to the effectiveness ofstudies [26]. Thus, data augmentation is commonly used to enrich datasets and enhance their suitability for trainingmodels. For example, to augment image data, techniques such as resizing, rotating, and shifting images are frequentlyused [27, 28]. Researchers have also explored introducing noise in LLM training data [27], adding audio tracks ortemporal shifts in speech recognition [29, 30], and leveraging Generative Adversarial Networks (GANs) to generate"}, {"title": "2.3 | OER and automation", "content": "In recent years, the field of Open Educational Resources (OER) has seen significant growth and adoption [32], allowingresearchers to benefit from a corpus of educational resources at no cost and open materials that can be freely dis-tributed, remixed, and adapted. With the rise of large language models (LLMs), the education sector is experimentingwith automating the generation of these resources to reduce costs and enhance efficiency. In particular, there hasbeen an emphasis on automatic item generation, hint generation, and skill tagging. For item generation, much researchis focusing on utilizing the capabilities of LLMs to generate math questions either through template-based approaches[33], open-ended generation (Socratic style questions or math word problems) [33, 34, 35, 36], multiple-choice ques-tion generation [37, 38, 39], or generation from structured formats (i.e., a bullet-point list) [11]. Hint generation hasalso been a focus, with researchers examining the effectiveness of LLMs in providing hints (i.e., worked solutions) tosupport learning in mathematics [40, 41], computer programming [42, 43, 44, 45, 46, 47], and various other STEMsubjects. Additionally, studies have investigated human-Al collaboration in skill tagging, assessing its effectivenessacross multiple languages and its speed and accuracy [48, 49]. However, unlike these areas, the topic of using LLMsto simulate respondents remains under-researched. Thus, this paper aims to study the feasibility and effectiveness ofLLMs in simulating respondents."}, {"title": "3 | METHODS", "content": ""}, {"title": "3.1 | Model Selection", "content": "We selected six Large Language Models (LLMs) to generate responses that simulate answers from undergraduate col-lege students in the U.S. to assessment questions. Our selection included GPT-3.5, GPT-4, Llama 2, the newer Llama3, Gemini-Pro, and Cohere Command R Plus. These models were chosen for their varying levels of sophistication,reported accuracy on mathematics items, and widespread popularity, allowing us to simulate a broad spectrum ofstudent abilities, from lower to higher academic proficiency [50, 51, 52]. For implementation, we utilized APIs foreach model. As Llama does not offer a direct API, we accessed Llama 2 and Llama 3 via the Replicate API."}, {"title": "3.2 Selection of Items and Prompt Engineering", "content": "College Algebra was chosen as the subject because pre-authored questions were available under a CC BY license froman open textbook publisher, OpenStax\u00b9. Additionally, we used a dataset from an earlier study that calibrated its itempool and had already collected responses from human participants via Prolific for 20 of the OpenStax College Algebraquestions in Lesson 2.2: Linear Equations in One Variable [11]. This prior data collection contained some missingnessin the data, so we were able to effectively use N >= 99 for all items.\nWe distinctively formatted the questions, each prefixed with a label, in the following format: \"Q1: <question1>\"followed by a double newline, then \"Q2: <question2>\", and continued this pattern for all 20 questions. Specifically,the prompt was:\nQ1: Given m = 4, find the equation of the line in slope-intercept form passing through the point (2, 5).\n\u00b9https://openstax.org/details/books/college-algebra-2e"}, {"title": "3.3 Augmentation Procedure", "content": "In a real-world case, sometimes only partial data is gathered. To explore the possibility of augmenting the data, wetreated each human respondent in our dataset as a centroid, using only 50 human responses to represent the partialdata. We then identified the nearest synthetic respondent from our pool of synthetically generated answers for eachhuman centroid, allowing us to map the Al responses directly onto the characteristics of individual human responses.Next, we conducted a resampling procedure. First, we resampled a subset of 50 synthetic responses, selecting thembased on the distribution of the models represented in the original matched 50, aiming to maintain the proportionalityobserved in this initial sampling. Lastly, we expanded this by resampling a subset of 100 synthetic responses usingthe same criteria."}, {"title": "3.4 IRT analysis", "content": "Contrary to sum-score analysis or percentage correct metrics, we plan to use Item Response Theory (IRT) to estimatethe latent ability of human and LLM-Respondents. This method has several advantages over sum-score analysis [53].First, IRT assumes a latent trait theta, transforming all estimations onto a logit scale instead of the sum-score scale,which greatly improves measurement precision. Second, IRT provides person-level fit data, which can be done inde-pendently of other respondents. Lastly, and most relevant to our purpose, IRT maps both persons and items onto thesame scales, enabling equating to be carried out without assuming population score distributions. In fact, IRT equatingmay be the best method when tests of differing difficulties are given to nonrandom groups of examinees who differin ability [54].\nThe simplest IRT model is often called the Rasch model or the one-parameter logistic model (1PL). The probabilityof individuals responding to a binary item (i.e., True/False) is determined by the individual's trait level and the difficultyof the item, which is often presented as:\n$P(\u03a7_{ij} = 1|\u03b8_i, \u03b2_j) = \\frac{\\exp (\u03b8_i \u2013 \u03b2_j)}{1 + \\exp (\u03b8_i \u2013 \u03b2_j)}$\nwhere:\nXij refers to the response made by individual i to item j. If the response is correct/true, then Xij = 1.\n\u03b8i refers to the trait level of individual i."}, {"title": "4 | RESULTS", "content": ""}, {"title": "4.1 | LLM-Respondent Simulation", "content": "The initial item parameters for the item pool were calibrated on a group of current college students in the UnitedStates. Since multiple forms were used, there was missingness at random in the data, effectively resulting in N >= 99for all items, satisfying the basic requirement for a Rasch analysis. The results are shown in Figure 1. We then fixed theitem parameters estimated from the model to obtain the proficiency estimates for the six Al models. We wonderedwhether the proficiency distribution of synthetic respondents is comparable to that of humans. Results show thatmost LLM proficiency distributions have a significant overlap with the human respondents. In particular, Llama 3 andGPT-3.5 have the highest mean proficiency distribution, which is higher than the human mean, indicating Al's greaterproficiency in College Algebra compared to college students. The mean proficiency of GPT-4 is comparable to humans,"}, {"title": "4.2 | Data Augmentation using LLM-Respondent", "content": "Given that none of the LLM models have a proficiency distribution resembling that of humans, it is not feasible at thistime to fully substitute human respondents with LLM-Respondents from a single LLM. However, LLMs could be usedin a hybrid approach where half the respondents are human and half are LLM-Respondents. With this in mind, wepropose three hybrid, or data augmentation, strategies listed below:\n\u2022 An enlarged sample of 50 humans: 50 human respondents (examples) and 50 LLM-Respondents\n\u2022 A mixture of human respondents and resampled LLM-Respondents using proportions learned from humans\n\u2022 Fully LLM-Respondents using the mixing proportions learned from humans\nThe resampling analysis resulted in a set with significant variation in the proportions of each model used. GPT-3.5was the most prevalent, comprising 36% of the synthetic responses, followed by Llama 2 at 3%. Gemini accountedfor 12%, while both Llama 3 and GPT-4 were represented at 8% each. Cohere was the least represented model,constituting only 6% of the responses.\nTo evaluate the relative performance of these three strategies, we proposed four experiments to test the effec-tiveness of different strategies on the item calibration process. The benchmark performance of the calibration is setby the human respondents; namely, we use all available data from human respondents to calibrate the item pool andgather item parameter estimates. Each experiment is designed to explore a different augmentation strategy. Theproposed experiments are as follows:\nExperiment 1 We use only half the number of the respondent pool and do the calibration (N = 50), representing areal-world scenario where there is a limitation in the budget, so only part of the intended respondentswere collected."}, {"title": "5 | DISCUSSION AND CONCLUSIONS", "content": "In this study, we explored six different LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere CommandR Plus) and various combinations using sampling methods to achieve psychometric properties similar to those from"}, {"title": "6 | LIMITATIONS AND FUTURE WORK", "content": "Our study still has limitations. Firstly, we only utilized a single College Algebra lesson, which makes it difficult to gen-eralize the results to other lessons within the same subject or to different subject areas. Additionally, our OpenStaxitem pool consists only of questions without images, figures, or tables because not all the LLMs support multicapabilities. Furthermore, in our experiment, the original human dataset displays bimodality. Therefore, when aug-menting the data without resampling (Experiment 2), bimodality was also exhibited. While Experiment 3 mitigatesthis impact by using an effective resampling strategy, it would be beneficial to utilize a pool of human respondentsthat portray a normal distribution from the start. Due to leveraging prior data collection, we used the respondentsample size from their study, rather than calculating what the effective size should actually be.\nIn the future, analyses should be conducted to determine how valid the estimated proficiencies are when derivedfrom a measurement tool calibrated by augmentation experiments versus those from benchmark data. To addressthis, we should investigate whether the structure of the tool with augmented respondents is the same as it is withthe original human population [57]. Typically, this is done by investigating the reliability of the measurement tool andconducting statistical analyses such as Explanatory Factor Analysis (EFA) or Confirmatory Factor Analysis (CFA) [58].\nThe field should also aim to refine and expand methodologies to significantly improve the accuracy of responsesgenerated by the models. In our study, we presented all 20 questions in a single prompt. However, it may be beneficialto question the model independently for each item. Although our initial trials with this technique for GPT-3.5 resultedin degenerate outcomes, this method could be extended to other models to assess its effectiveness more comprehen-sively. Additionally, more sophisticated prompt engineering techniques should be explored. Our prompt was simple"}]}