{"title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis", "authors": ["Yunting Liu", "Shreya Bhandari", "Zachary A. Pardos"], "abstract": "Effective educational measurement relies heavily on the curation of well-designed item pools (i.e., possessing the right psychometric properties). However, item calibration is time-consuming and costly, requiring a sufficient number of respondents for the response process. We explore using six different LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus) and various combinations of them using sampling methods to produce responses with psychometric properties similar to human answers. Results show that some LLMs have comparable or higher proficiency in College Algebra than college students. No single LLM mimics human respondents due to narrow proficiency distributions, but an ensemble of LLMs can better resemble college students' ability distribution. The item parameters calibrated by LLM-Respondents have high correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated counterparts, and closely resemble the parameters of the human subset (e.g. 0.02 Spearman correlation difference). Several augmentation strategies are evaluated for their relative performance, with resampling methods proving most effective, enhancing the Spearman correlation from 0.89 (human only) to 0.93 (augmented human).", "sections": [{"title": "INTRODUCTION", "content": "Generating sets of well-functioning items for mathematics assessments often requires multiple iterations of calibration, involving extensive human participation. One of the most common techniques used in building an item pool is Item Response Theory (IRT), where ample response-level data is typically required for accurate item calibration and scaling [1, 2]. This process is time-consuming, costly, and significantly limits the rapid adaptation of educational assessments to different sets of students. For example, the PISA main survey requires between N = 250 to N = 750 respondents per item per country [3]. Thus, the time and cost involved in obtaining responses from human respondents remains a significant area of resource expenditure.\nWith the advent of advanced Al technologies, novel ways to address these challenges have arisen. Recent developments in Large Language Models (LLMs) are achieving near-human performance [4, 5, 6, 7], leading to speculation about whether they can competently generate high-fidelity synthetic data without the traditional need for full data collection [8, 9, 10]. In our domain, we explore whether the capabilities of LLMs can be leveraged to provide responses resulting in psychometric properties similar to those derived from human respondents' answers. This research is guided by three critical research questions:\nRQ1: Which language model or configuration of language models best mimic human respondent abilities in mathematics, as measured by Item Response Theory (IRT)?\nRQ2: How do the psychometric properties of items fit to human responses compare to those fit to LLM-Respondents?\nRQ3: Can the augmentation of human respondent data with LLM-Respondent contributions yield item parameters comparable to those obtained from solely increasing human data collection?\nIf this approach is successful, it would mean that questions, including those produced via generative Al [11], could be tested and evaluated en masse nearly instantly for use in a variety of educational contexts such as computer tutoring systems and other formative and summative assessment scenarios."}, {"title": "RELATED WORK", "content": "2.1 | Simulated Data in Educational Measurement and Educational Data Mining\nAnalyzing examinee responses to test questions is indispensable in the field of measurement. While gathering real data can be time-consuming, costly, and often incomplete, simulation is a useful and economical technique since it can usually be done on a laptop without additional costs. Therefore, researchers commonly use simulation to validate models [12], compare different models [13], and evaluate estimation methods. In fact, among a random sample of publications in the field, 60% of the studies used simulation, while the ratio for real data is just 41% [14]. Typically, a respondent distribution is specified, and then the item response level data (i.e., dichotomous or polytomous response) is simulated accordingly, with no thought process involved. Now, thanks to the advancement of generative Al, we can simulate some response level data and possibly gain insights into the cognitive structure behind the response process. While most work on simulation in educational settings is based on dialogue [15, 16], there are indeed some researchers conducting item response level data simulation. For example, Xu and Zhang [17] demonstrated the possibility of simulating student behavior based on assessment history. Lu and Wang [18] used insights from teachers to create generative students with various profiles, and then used the generative students' outcomes to guide item development.\nIn the realm of educational data mining, gathering real learner data can pose privacy concerns [19] and present challenges with the costs of managing logged data [20]. To address these challenges, researchers have at times leveraged simulated data. LLMs are often used to generate the training datasets needed to train and test other models. For example, by using pseudocode to generate synthetic datasets, researchers have been able to develop test cases of teaching activities to inform the development of a Teaching Outcome Model (TOM) [21]. Similarly, researchers have proposed using LLMs as data annotators to create synthetic data that can be used to train other models [8], mimicking the framework of Teacher-Student Learning (TSL) [22]. Simulated data has also been used within the educational data mining community to evaluate latent trait models [23, 24].\n2.2 | Data Augmentation\nData augmentation is a method often employed to increase the volume and diversity of data by generating new data from the existing set; it can also be applied to mitigate the 'incomplete data' problem [25]. Having a limited number of data points often leads to weaker generalization capabilities, which can act as an obstacle to the effectiveness of studies [26]. Thus, data augmentation is commonly used to enrich datasets and enhance their suitability for training models. For example, to augment image data, techniques such as resizing, rotating, and shifting images are frequently used [27, 28]. Researchers have also explored introducing noise in LLM training data [27], adding audio tracks or temporal shifts in speech recognition [29, 30], and leveraging Generative Adversarial Networks (GANs) to generate"}, {"title": "OER and automation", "content": "In recent years, the field of Open Educational Resources (OER) has seen significant growth and adoption [32], allowing researchers to benefit from a corpus of educational resources at no cost and open materials that can be freely distributed, remixed, and adapted. With the rise of large language models (LLMs), the education sector is experimenting with automating the generation of these resources to reduce costs and enhance efficiency. In particular, there has been an emphasis on automatic item generation, hint generation, and skill tagging. For item generation, much research is focusing on utilizing the capabilities of LLMs to generate math questions either through template-based approaches [33], open-ended generation (Socratic style questions or math word problems) [33, 34, 35, 36], multiple-choice question generation [37, 38, 39], or generation from structured formats (i.e., a bullet-point list) [11]. Hint generation has also been a focus, with researchers examining the effectiveness of LLMs in providing hints (i.e., worked solutions) to support learning in mathematics [40, 41], computer programming [42, 43, 44, 45, 46, 47], and various other STEM subjects. Additionally, studies have investigated human-Al collaboration in skill tagging, assessing its effectiveness across multiple languages and its speed and accuracy [48, 49]. However, unlike these areas, the topic of using LLMs to simulate respondents remains under-researched. Thus, this paper aims to study the feasibility and effectiveness of LLMs in simulating respondents."}, {"title": "METHODS", "content": "3.1 | Model Selection\nWe selected six Large Language Models (LLMs) to generate responses that simulate answers from undergraduate college students in the U.S. to assessment questions. Our selection included GPT-3.5, GPT-4, Llama 2, the newer Llama 3, Gemini-Pro, and Cohere Command R Plus. These models were chosen for their varying levels of sophistication, reported accuracy on mathematics items, and widespread popularity, allowing us to simulate a broad spectrum of student abilities, from lower to higher academic proficiency [50, 51, 52]. For implementation, we utilized APIs for each model. As Llama does not offer a direct API, we accessed Llama 2 and Llama 3 via the Replicate API.\n3.2 | Selection of Items and Prompt Engineering\nCollege Algebra was chosen as the subject because pre-authored questions were available under a CC BY license from an open textbook publisher, OpenStax\u00b9. Additionally, we used a dataset from an earlier study that calibrated its item pool and had already collected responses from human participants via Prolific for 20 of the OpenStax College Algebra questions in Lesson 2.2: Linear Equations in One Variable [11]. This prior data collection contained some missingness in the data, so we were able to effectively use N >= 99 for all items.\nWe distinctively formatted the questions, each prefixed with a label, in the following format: \"Q1: <question1>\" followed by a double newline, then \"Q2: <question2>\", and continued this pattern for all 20 questions. Specifically, the prompt was:\nQ1: Given m = 4, find the equation of the line in slope-intercept form passing through the point (2, 5).\nQ2: Find the slope of a line that passes through the points (2, \u22121) and (\u22125,3).\nQ20: For the following exercises, solve the equation for x. State all x-values that are excluded from the solution set. 2 - 3/(x + 4) = (x + 2)/(x + 4). Answer choices: Excluded values: -4 and x=-3; Excluded values: 4 and x=-3.\nThis helped to ensure clarity and separation between items. We simulated 150 respondents for each LLM, as this was deemed the right sample size for conducting further analysis. To assess the accuracy of the responses, the second author of the study manually graded them and noted the accuracy for each one.\n3.3 | Augmentation Procedure\nIn a real-world case, sometimes only partial data is gathered. To explore the possibility of augmenting the data, we treated each human respondent in our dataset as a centroid, using only 50 human responses to represent the partial data. We then identified the nearest synthetic respondent from our pool of synthetically generated answers for each human centroid, allowing us to map the Al responses directly onto the characteristics of individual human responses. Next, we conducted a resampling procedure. First, we resampled a subset of 50 synthetic responses, selecting them based on the distribution of the models represented in the original matched 50, aiming to maintain the proportionality observed in this initial sampling. Lastly, we expanded this by resampling a subset of 100 synthetic responses using the same criteria.\n3.4 | IRT analysis\nContrary to sum-score analysis or percentage correct metrics, we plan to use Item Response Theory (IRT) to estimate the latent ability of human and LLM-Respondents. This method has several advantages over sum-score analysis [53]. First, IRT assumes a latent trait theta, transforming all estimations onto a logit scale instead of the sum-score scale, which greatly improves measurement precision. Second, IRT provides person-level fit data, which can be done independently of other respondents. Lastly, and most relevant to our purpose, IRT maps both persons and items onto the same scales, enabling equating to be carried out without assuming population score distributions. In fact, IRT equating may be the best method when tests of differing difficulties are given to nonrandom groups of examinees who differ in ability [54].\nThe simplest IRT model is often called the Rasch model or the one-parameter logistic model (1PL). The probability of individuals responding to a binary item (i.e., True/False) is determined by the individual's trait level and the difficulty of the item, which is often presented as:\n$$P(\u03a7_{ij} = 1|\u03b8_i, \u03b2_j) = \\frac{exp (\u03b8_i \u2013 \u03b2_j)}{1 + exp (\u03b8_i \u2013 \u03b2_j)}$$\nwhere:\nXij refers to the response made by individual i to item j. If the response is correct/true, then Xij = 1.\n\u03b8i refers to the trait level of individual i."}, {"title": "RESULTS", "content": "4.1 | LLM-Respondent Simulation\nThe initial item parameters for the item pool were calibrated on a group of current college students in the United States. Since multiple forms were used, there was missingness at random in the data, effectively resulting in N >= 99 for all items, satisfying the basic requirement for a Rasch analysis. The results are shown in Figure 1. We then fixed the item parameters estimated from the model to obtain the proficiency estimates for the six Al models. We wondered whether the proficiency distribution of synthetic respondents is comparable to that of humans. Results show that most LLM proficiency distributions have a significant overlap with the human respondents. In particular, Llama 3 and GPT-3.5 have the highest mean proficiency distribution, which is higher than the human mean, indicating Al's greater proficiency in College Algebra compared to college students. The mean proficiency of GPT-4 is comparable to humans,\n4.2 | Data Augmentation using LLM-Respondent\nGiven that none of the LLM models have a proficiency distribution resembling that of humans, it is not feasible at this time to fully substitute human respondents with LLM-Respondents from a single LLM. However, LLMs could be used in a hybrid approach where half the respondents are human and half are LLM-Respondents. With this in mind, we propose three hybrid, or data augmentation, strategies listed below:\nAn enlarged sample of 50 humans: 50 human respondents (examples) and 50 LLM-Respondents\nA mixture of human respondents and resampled LLM-Respondents using proportions learned from humans\nFully LLM-Respondents using the mixing proportions learned from humans\nThe resampling analysis resulted in a set with significant variation in the proportions of each model used. GPT-3.5 was the most prevalent, comprising 36% of the synthetic responses, followed by Llama 2 at 3%. Gemini accounted for 12%, while both Llama 3 and GPT-4 were represented at 8% each. Cohere was the least represented model, constituting only 6% of the responses.\nTo evaluate the relative performance of these three strategies, we proposed four experiments to test the effectiveness of different strategies on the item calibration process. The benchmark performance of the calibration is set by the human respondents; namely, we use all available data from human respondents to calibrate the item pool and gather item parameter estimates. Each experiment is designed to explore a different augmentation strategy. The proposed experiments are as follows:\nExperiment 1 We use only half the number of the respondent pool and do the calibration (N = 50), representing a real-world scenario where there is a limitation in the budget, so only part of the intended respondents were collected.\nExperiment 2 In addition to Experiment 1, we enlarge the data size by twice using augmentation strategies (N = 100).\nExperiment 3 We use a mixture of human respondents and fully resampled data in a 1:1 ratio (N = 100).\nExperiment 4 We use fully resampled data, with a size equal to the number of the benchmark dataset (N = 100).\nIn terms of evaluation criteria, Pearson correlation and Spearman correlation with the benchmark condition are reported. We also use Root Mean Square Error (RMSE) to evaluate how accurate the new estimates are. RMSE measures the average difference between values predicted by a model and the actual values.\n$$RMSE = \\sqrt{\\frac{\u03a3_{i=1}^N ||\u0177(i) - y(i)||^2}{N}}$$\nRMSE is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect recovery of the true/benchmark data.\nResults show that when there aren't a sufficient number of respondents (Experiment 1), some difficulty estimates from the IRT analysis are not trustworthy, especially at the very ends of the respondent distribution (too-easy or too-hard items), resulting in a relatively high but not perfect correlation (Pearson p = 0.92, Spearman p = 0.89), and RMSE = 0.55. Among all augmentation strategies, Experiment 3 has the best result; it raised the Spearman p from 0.89 to 0.93, indicating it is an effective strategy for recovering the order of item difficulties. However, since the mean center of the respondent distribution in Experiment 3 is tilted to the left (\u03bc = \u22120.29) compared to Experiment 1 (\u03bc = 0.08), the RMSE is larger as a universal shift is applied to all item parameters. Experiment 2 yields results comparable to Experiment 1, suggesting the strategy might not be beneficial in the current settings; practical reasons will be discussed in the Discussion section. As a validation, we also calibrated the respondent proficiency distribution using the fixed item parameter methods"}, {"title": "DISCUSSION AND CONCLUSIONS", "content": "In this study, we explored six different LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus) and various combinations using sampling methods to achieve psychometric properties similar to those from human respondents' answers. Our findings are structured around three key conclusions: the proficiency of LLMs in approximating ability distributions (RQ1), item parameter correlation (RQ2), and the effectiveness of data augmentation (RQ3). Firstly, when comparing the proficiency between LLM responses and human responses, the results show that although some LLMs have comparable or even higher abilities in College Algebra, their distributions alone cannot fully represent the human distribution due to their narrow spread. Interestingly, this novel application of Item Response Theory (IRT) to LLM abilities reveals a first-of-its-kind distribution spread of abilities from multiple promptings, as opposed to the point estimates or percent-correct scores reported in other studies. Secondly, we compared the item parameters calibrated by Al responses and human responses, finding a relatively high Spearman correlation of 0.87 for GPT-3.5 and a lower 0.78 for GPT-4. Notably, GPT-3.5 emerged as the most human-like Al respondent, exhibiting Spearman correlations within 0.02 of those from 50 human respondents. Finally, since no single LLM currently has the capability to represent humans by itself, we explored ensembling approaches using three strategies. Among these, a mixture of human respondents and fully resampled data in a 1:1 ratio (Experiment 3) provided the best result, raising the Spearman correlation from 0.89 to 0.93. However, these augmentation results were mixed; while Spearman and Pearson correlations improved by 0.04 and 0.01, respectively, this approach substantially increased the RMSE.\nOur findings hold much promise for the automatic curation of items for tutoring systems. Simply put, it seems plausible to leverage Al respondents to curate an item pool that has a desirable spread of difficulty. Given the performance of 150 Al respondents from GPT-3.5 closely mirroring that of 50 humans (Experiment 1), Al respondents could be used as an initial filtering phase to reliably narrow down a larger item pool, and then have human respondents further refine the selection using the more manageable subset of items. This would significantly help optimize human resources, saving both time and money. For classroom environments, this research allows nimble testing of new questions, enabling selection of only quality assessments to present in the classroom."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "Our study still has limitations. Firstly, we only utilized a single College Algebra lesson, which makes it difficult to generalize the results to other lessons within the same subject or to different subject areas. Additionally, our OpenStax item pool consists only of questions without images, figures, or tables because not all the LLMs support multimodal capabilities. Furthermore, in our experiment, the original human dataset displays bimodality. Therefore, when augmenting the data without resampling (Experiment 2), bimodality was also exhibited. While Experiment 3 mitigates this impact by using an effective resampling strategy, it would be beneficial to utilize a pool of human respondents that portray a normal distribution from the start. Due to leveraging prior data collection, we used the respondent sample size from their study, rather than calculating what the effective size should actually be.\nIn the future, analyses should be conducted to determine how valid the estimated proficiencies are when derived from a measurement tool calibrated by augmentation experiments versus those from benchmark data. To address this, we should investigate whether the structure of the tool with augmented respondents is the same as it is with the original human population [57]. Typically, this is done by investigating the reliability of the measurement tool and conducting statistical analyses such as Explanatory Factor Analysis (EFA) or Confirmatory Factor Analysis (CFA) [58].\nThe field should also aim to refine and expand methodologies to significantly improve the accuracy of responses generated by the models. In our study, we presented all 20 questions in a single prompt. However, it may be beneficial to question the model independently for each item. Although our initial trials with this technique for GPT-3.5 resulted in degenerate outcomes, this method could be extended to other models to assess its effectiveness more comprehensively. Additionally, more sophisticated prompt engineering techniques should be explored. Our prompt was simple"}]}