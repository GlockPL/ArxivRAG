{"title": "ARTIFICIAL SCIENTIFIC DISCOVERY", "authors": ["ANTONIO NORELLI"], "abstract": "Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge.\nThe investigation begins with OLIVAW, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings. This perspective then leads us to see modern multimodal models as interpreters, and to devise a new way to build interpretable and cost-effective CLIP-like models: by coupling two unimodal models using little multimodal data and no further training.\nFinally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce Odeen, a benchmark about interpreting explanations that sees LLMs going no further than random chance while being instead fully solved by humans.", "sections": [{"title": "INTRODUCTION AND OVERVIEW", "content": "Explanations are the fuel of progress, the fundamental tool through which humans have increased their agency, earning more and more control over their future throughout history. So far, the production of these extraordinary symbolic sequences has been a unique prerogative of human scientists, but the formidable breakthroughs in AI that followed the advent of deep learning (LeCun, Bengio, and Hinton 2015) evoke the idea of machines capable of assisting us in this endeavour. Not mere tools empowering scientists, but rather peers capable of producing original research and pushing forward knowledge autonomously.\nIn this thesis I seriously entertain this idea, and seek to understand what it means to build an artificial scientist.\nWe will relive my research route, that initiated with an in-depth study of AlphaGo Zero. This AI system was able to master Go starting from scratch and defeated the best players in the world, a very promising starting point to investigate the possibility of knowledge creation by machines. Our discussion will begin with OLIVAW, an AlphaGo Zero-like agent that I brought to retrace the journey of its illustrious predecessor on the game of Othello, to the point of challenging a former World champion. Narrated with a blend of rigor and humor, this adventure culminates in a pivotal realization: knowledge is such if it can be transmitted to us, and AlphaGo cannot write a book on Go that could help human players improve.\nA fundamental characteristic of a scientist is the ability to communicate their own discoveries. This will lead us to realize the tension between language and the effective representation of new natural phenomena: language must be adapted to the new communicative need, but at the same time it should remain understandable; its vocabulary and rules slowly but inevitably change over time. Therefore, we will attempt to model an agent capable of explaining its observations without strictly defining its language. As a result, we will introduce Explanatory Learning and place the keystone of this thesis: a true artificial scientist can only emerge when a machine can autonomously interpret symbols.\nThis realization will drive us beyond traditional AI methodologies, such as Inductive Logic Programming and Program Synthesis, which approach the challenge of creating intelligent agents by presupposing the existence of a rigid, human-coded language interpreter. In-"}, {"title": "OLIVAW: MASTERING OTHELLO WITHOUT HUMAN KNOWLEDGE", "content": "We introduce OLIVAW, an AI Othello player adopting the design principles of the famous AlphaGo programs. The main motivation behind OLIVAW was to discover knowledge from scratch in a non-trivial board game at a tiny fraction of the cost of its illustrious predecessors. In this chapter, we show how the AlphaGo Zero's paradigm can be successfully applied to the popular game of Othello using only commodity hardware and free cloud services. While being simpler than Chess or Go, Othello maintains a considerable search space and difficulty in evaluating board positions. To achieve this result, OLIVAW implements some improvements inspired by recent works to accelerate the standard AlphaGo Zero learning process. The main modification implies doubling the positions collected per game during the training phase, by including also positions not played but largely explored by the agent. We tested the strength of OLIVAw in three different ways: by pitting it against Edax, considered by many the strongest open-source Othello engine, by playing anonymous games on the web platform OthelloQuest, and finally in two in-person matches against top-notch human players: a national champion and a former world champion."}, {"title": "INTRODUCTION", "content": "Only a year after AlphaGo's landmark victory against Go master Lee Sedol another sensational development took place. An improved version of AlphaGo called AlphaGo Zero asserted itself as the strongest Go player in the history of the game (Silver et al. 2017). The remarkable feature of AlphaGo Zero was that, unlike its predecessor and unlike all previous game software, it learned to master the game entirely by itself, without any human knowledge. As subsequent follow-up work quickly showed, AlphaGo's paradigm\u2013 an interesting blend of deep and reinforcement learning- seems to be general and flexible enough to adapt to a wide array of games (Schrittwieser et al. 2020);\nThese extraordinary successes came at a price however, and quite literally so. The amount of computational and financial resources that were required was so huge as to be out of reach for most academic and non-academic institutions. Not coincidentally these well-endowed projects and their follow-ups took place within giant multinational corporations of the IT sector (Lee et al. 2019); These companies deployed GPUs by the thousands and hundreds of TPUs. A recent study looked at the number of petaflops per day that were required to train AlphaGo Zero and other recent well-known results in AI (Amodei and Hernandez 2018). The paper shows an exponential growth with a 3.4-month doubling period. This is clearly unsustainable for most academic labs and departments and even the greatest majority of companies. Another aspect of the same problem is the amount of training needed. AlphaGo Zero required 4.9 million games played during self-play. And in order to attain the level of grandmaster for games like Starcraft II and Dota 2 the training required 200 years and more than 10,000 years of gameplay, respectively (Pachocki et al. 2018);\nThus one of the major problems to emerge in the wake of these breakthroughs is whether comparable results can be attained at a much lower computational and financial cost and with just commodity hardware. In this chapter we take a small step in this direction, by showing that AlphaGo Zero's successful paradigm can be replicated for the game of Othello (also called Reversi). While being much simpler than either Chess or Go, this game is still rather sophisticated and has a considerable strategic depth. The game enjoys a long history and a rich tradition. Every year an exciting world championship takes place in which accomplished players from all over the world vie for the world title.\nOur Othello engine is called OLIVAW, a homage to the famous robot character invented by Isaac Asimov. We tested the strength of OLIVAW in three different ways. In one instance, we pitted OLIVAw against Edax, one of the strongest Othello engines. Perhaps the most interest-"}, {"title": "RELATED WORK", "content": "The success of AlphaGo naturally stimulated several follow-ups. One of the main questions was to determine the level of generality of the approach. A series of papers showed this level to be great indeed.\nOne after the other a list of difficult games fell pray of the RL-with-oracle-advice approach. Silver et al. (2018) extended it to Chess and Shogi.\nRecently Schrittwieser et al. (2020) added ATARI games to the list. Our work continues this line of research by adding 8 \u00d7 8 Othello to the list, paying special attention to the cost issue. Indeed, it is not clear a priori whether the approach scales down in terms of resources. Although cheaper in some ways, the agents in (Silver et al. 2018), still use thousands of GPU's or hundreds of TPU's to master board games. The recent KataGo (Wu 2019) reaches the level of play of ELF using 1/50 of the computation and implements several techniques to accelerate the learning. However, these include a set of targets crafted by humans which are very game-specific thereby reducing the generality of the approach and reintroducing human knowledge in a relevant way.\nSuccessful low-cost reproductions of AlphaGo Zero came out in recent years, but only for very simple games like Connect-4 (Young, Prasad, and Abrams 2018) or 6 \u00d7 6 Othello (Chang et al. 2018), for which perfect strategies are known.\nOther works focused on the hyperparameters involved in AlphaGo Zero, looking for a faster and cheaper training process. Wang et al. (2019) and Young, Prasad, and Abrams (2018) make several experiments in this direction, while Wu, Wei, Wu, et al. (2020) investigates the possibility of tuning hyperparameters within a single run, using"}, {"title": "OTHELLO", "content": "Othello is a popular board game. Its simple rules are explained in Figure 1. A typical game lasts for some 60 moves, with an average branching factor of 10. Like Go and Chess it is a perfect information game. Although simpler than these two, it has considerable strategic depth. Unlike English draughts, there is no known perfect strategy that can be played by computer (Schaeffer et al. 2007).\nOthello is played across the globe. There are professional players competing in official tournaments organized by world and national federations. The Othello world championship takes place every year.\nThe best software beat humans systematically but, as discussed, they rely on brute force for most of the game. During the initial"}, {"title": "OLIVAW: THE ALGORITHM", "content": "The design of OLIVAw, our Othello engine, follows closely that of AlphaGo Zero (Silver et al. 2017). The main difference consists of a somewhat different and cheaper training process. The network architecture, while mimicking that of AlphaGo Zero, was scaled down. Before discussing OLIvaw in detail, it is useful to describe its basic design."}, {"title": "The basic design", "content": "Like the AlphaGo programs, Olivaw uses reinforcement learning to build an \"oracle\", in the form of a deep network $f_\\theta$ ($\\theta$ denotes the weights of the neural network). Given as input an Othello game state s, $f_\\theta$ outputs a pair: $f_\\theta(s) = (p,v)$. The vector p is a probability distribution over the possible moves from s. Intuitively, the higher the probability the better the move. The value v is the oracle's assessment of how good state s is, ranging from +1 (sure victory) to -1 (certain defeat).\nThe oracle is used to guide an exploration of the \u201cpossible near futures\" by a Monte Carlo Tree Search (MCTS). To pick the next move, the game tree rooted at s is explored. Roughly speaking, in this exploration, the moves that $f_\\theta$ considers good are explored first (so that the actual branching factor is limited) and the total number of nodes explored is limited (in the few hundreds during training and set to one thousand when playing against humans). The goal of this exploration phase is to produce a better estimate (\u03c0, q) of state s. When this is done, the best move according to \u03c0 is played to reach a new state s', and the process is repeated.\nWhat is noteworthy about this process is that while by itself $f_\\theta$ is a rather weak player, using it in combination with MCTS gives rise to a very strong one, i.e. the estimates (\u03c0, q) are more reliable than (p,v). Let us call A(f) the MCTS playing agent using f as oracle.\nThe crux of the approach is to generate a sequence of oracles $f_0,f_1, f_2, ..., f_t$ each better than the predecessors. This is done by generating a sequence of training sets $S_1, S_2, ..., S_t$ each better than the previous one. Training set $S_i$ is used to train $f_i$. The process is initialized with a deep network $f_0$ with random weights.\nThe generic step in this sequence of improvements is as follows. Let $f_0$ be the current oracle. During the so-called self-play phase, A($f_0$) plays a batch of games against itself. During each game a set of states S will be explored. For each s \u2208 S an updated (and hopefully better) assessment (\u03c0s, qs) will be computed. The set T of pairs {s, (\u03c0s, qs)} for s \u2208 S will be added to the training set. The intuition is that this way we can create a virtuous circle. As the assessments (\u03c0s, qs) become more and more accurate the training set becomes better and better. And, as the training set improves the assessment becomes more accurate.\nWe remark that the main difference between OLIVAW and AlphaGo Zero resides in how this training set is constructed. Instead of {s, (\u03c0s, qs)}, AlphaGo Zero only considers pairs {s, (\u03c0s, zs)} where s is actually played during the game, and zs is the outcome at the end of the game. Thus, zs \u2208 {\u22121,0,+1}. In contrast, besides this type of pairs, OLIVAW also adds to the training set pairs {s, (\u03c0s, qs)} for which s has been explored \u201ca lot\". In this way, we collect a larger training set for"}, {"title": "Low-cost faster training", "content": "With respect to AlphaGo Zero, OLIVAw introduces three main modifications in the training phase.\nAs remarked, while the training set of AlphaGo consists only of pairs of the kind {s, (\u03c0s, zs )}, where s is a move actually played during self-play and zs is the outcome at the end of the game, OLIVAW also considers pairs of the type {s, (\u03c0s, qs)}, where s is a position in the game tree that has been explored a number of times above a certain threshold. The threshold value is set dynamically in order to have a training set twice the size of that used by AlphaGo. In other words, the number of pairs of type {s, (\u03c0s, qs)} is roughly equal to that of the pairs of type {s, (\u03c0s, zs)}. The pairs added are the ones with the largest number of visits. Our approach was broadly inspired by the results reported in Young, Prasad, and Abrams (2018).\nAdding noisy pairs might not seem a good idea at first. In fact, using only the final outcome z as a signal has a big drawback. In a game with multiple errors, every evaluation of an early position based on the final outcome is almost random, while q offers a better assessment.\nOn the other hand, q suffers from the limited horizon of the search; an early position with positive or negative consequences far ahead in"}, {"title": "Resources", "content": "OLIVAW was entirely developed, trained, and tested on Colaboratory, a free Google cloud computing service for machine learning education and research (Carneiro et al. 2018).\nOLIVAW code is completely written in Python, from the core MCTS and Neural Network classes implemented in Numpy and Keras, to the simple GUI based on Matplotlib. The self-play, training and evaluation phases take place on three self-contained distinct notebooks sharing the same memory. Concerning local resources, we took no more advantage than a laptop equipped with a browser and an Internet connection.\nThe hardware specifications of a Colaboratory virtual machine at the time of the training were:\n\u2022 CPU: 1 single core hyper threaded Xeon Processor, 2.3Ghz, 2 threads.\n\u2022 RAM: ~12.6 GB.\n\u2022 Hardware accelerators (if used):\nGPU: 1 Nvidia Tesla K80, 2496 CUDA cores, 12GB GDDR5 VRAM.\nTPU v2-8: Google Tensor processing unit equipped with 8 TPU cores.\nThe generation of games during the self-play phase is the most computationally expensive process of the learning algorithm. In our hardware configuration, a single game takes from 6 to 30 seconds, depending on the number of MCTS simulations per move, due to the high"}, {"title": "The training process", "content": "The version of OLIVAW discussed in this article is the result of a single training run lasting 30 days, 20 generations, and ~ 50,000 games. We refer to the i-th generation as the i-th successful update of the weights of $f_\\theta$.\nFine-tuning the hyperparameters for 8 \u00d7 8 Othello would have required a number runs incompatible with our main objective of mastering the game with limited resources. Similarly, ablation studies to determine the effectiveness of our choices to improve the learning phase would have been prohibitively costly. As discussed however, these choices find good motivation in previous work, such as Wang et al. (2019) and Young, Prasad, and Abrams (2018).\nDuring the training phase, several interesting trends emerged (please refer to Figure 2). Figure 2 A plots the progress of the loss function across generations. Noticeable jumps take place when OLIVAW"}, {"title": "Details to replicate training conditions:", "content": "In the following, we report all salient hyperparameters to reproduce this work."}, {"title": "Attaining world-class level in Othello", "content": "We tested the strength of OLIVAw in three different ways. First, by playing anonymous games on the web platform OthelloQuest. Second, by pitting it against Edax, one of the strongest open-source Othello engine. And finally, with two matches against top-notch human players: a national champion and a former world champion."}, {"title": "Matches on the web platform OthelloQuest", "content": "During training, the strength of OLIVAw was tested with a series of anonymous online games against human players on OthelloQuest, a popular Othello platform which is also widely used by top human players. OLIVAW was deployed at generation 8,12,16, and 20 with the number of explorable nodes in the game tree set at 400. The duration"}, {"title": "Matches against Edax", "content": "We tested OLIVAW against Edax, arguably the strongest open-source Othello engine. Like other top traditional engines, Edax is based on a highly optimized alpha-beta tree search (negamax) using tabular value functions. In what follows Ek denotes the version of Edax in which the depth of the alpha-beta search in the game tree is limited to k. In our comparison, E4, E6, E8 and E10 were used. For the games we report, Edax used no opening book. Edax is a deterministic engine and when it played against OLIVAw the same single game was repeated again and again. To circumvent this problem we switched to random XOT openings, a popular Othello variation where the first 8 moves are chosen at random from a list of 10,784 sequences ending in an almost even position, i.e. positions judged between -2 and +2 discs advantage for black by Edax at search depth 16.\nThe four versions of Edax were pitted against four versions of the 20th generation of OLIvaw. The four versions deployed correspond to the maximum number of nodes of the game tree allowed to be explored and that were set to 400, 1000, 2500, and 10 thousand. The resulting versions of OLIVAW are referred to as O-400, O-1000, O-2500, and O-10T.\nSeveral aspects must be considered in order to set up a comparison between Edax and OLIVAw that is informative as well as feasible. As far as the former aspect is concerned, setting limits in terms of wall-clock time for the two agents would not be very informative. They use different hardware (GPUs vs CPUs) and are written in different programming languages. This is why we opted for a machine-independent measure of computational effort: the number of explored nodes in the game tree to decide the next move. (In the case of Edax the depth of the alpha-beta search translates into number of nodes explored, see below). As for the latter, it seems reasonable to assume that, while increasing the budget of explorable nodes makes"}, {"title": "Matches against top-notch human players", "content": "As a final, and much more enjoyable, battery of tests, we organized three live series against top human players with the support of the Italian Othello Federation. Two were against the 2019 Italian cham-"}, {"title": "CONCLUSION", "content": "After one month of training, using only free, and quite limited, cloud computing resources, OLIVAW achieved world-class level in the game of Othello. The high ELO rating reached on the popular web platform OthelloQuest, the winning challenges against the strongest open-source Othello engine Edax, the victory against national champion Alessan-"}, {"title": "EXPLANATORY LEARNING: BEYOND EMPIRICISM IN NEURAL NETWORKS", "content": "At the crossroads of Program Synthesis and Meta-Learning, we introduce Explanatory Learning as the task of automatically discovering the symbolic explanation (PS) that enables few-shot sensible predictions on a novel environment given experience on other environments (M-L). Differently from PS, the program (explanation) interpreter in EL is not given and should be learned from a limited collection of associations explanation-observations. Unlike M-L, EL does not prescribe any adaptation at test time, seeking generalization in the broad meanings attributed to symbols by the learned interpreter. To exemplify the challenges of EL, we present the Odeen benchmark, which can also serve the PS and M-L paradigms. Finally, we introduce Critical Rationalist Networks, a deep learning approach to EL aligned with the rationalist view of knowledge acquisition. CRNs express several desired properties by construction; they are truly explainable, can adjust their processing at test-time for harder inferences, and can offer strong confidence guarantees on their predictions. Using Odeen as a testbed, we show how CRNs outperform empiricist end-to-end approaches of similar size and architecture (Transformers) in discovering explanations for unseen environments."}, {"title": "INTRODUCTION", "content": "Making accurate predictions about the future is a key ability to survive and thrive in a habitat. Living beings have evolved many systems to this end, such as memory (McConnell 1962), and several can predict the course of complex phenomena (Taylor, Miller, and Gray 2012). However, no animal comes even close to the prediction ability of humans, which stems from a unique-in-nature system.\nAt the core of this system lies an object called explanation, formed by the proposition of a language, which has a remarkable property: it can be installed with ease into another human speaking the same language, allowing to make predictions on new phenomena without ever having experienced them. When the installation is successful, we say that the human has understood the explanation."}, {"title": "Problem setup", "content": "Formally, let phenomena $P_1, P_2, P_3,...$ be subsets of a universe U, which is a large set with no special structure (i.e., all the possible observations $U = \\{x_1,...,x_z\\}$). Over a universe U, one can define a language L as a pair $(\\Sigma_1,I_L)$, where $\\Sigma_1$ is a finite collection of short strings over some alphabet A, with $|\\Sigma_1| \\gg |A|$, and $I_L$ is a binary function $I_L: U \\times \\Sigma_1 \\rightarrow \\{0,1\\}$, which we call interpreter. We say that a phenomenon $P_i$ is explainable in a language L if there exists a string $e \\in \\Sigma_1$ such that, for any $x \\in U$, it occurs $I_L(x, e) = 1_{P_i} (X)$, where $1_{P_i} (x)$ is the indicator function of Pi. We call the string e an explanation, in the language L, for the phenomenon Pi.\nOur first contribution is the introduction of a new class of machine learning problems, which we refer to as Explanatory Learning (EL).\nConsider the general problem of making a new prediction for a phenomenon $P_o \\subset U$. In our setting, this is phrased as a binary classification task: given a sample $x' \\in U$, establish whether $x' \\in P_o$ or not. We are interested in two instances of this problem, with different underlying assumptions:\n\u2022 THE COMMUNICATION PROBLEM: WE HAVE AN EXPLANATION. We are given an explanation $e_o$ for $P_o$, in an unknown language L. This means that we do not have access to an interpreter $I_L$; $e_o$ looks like Japanese to a non-Japanese speaker. Instead, we are also given other explanations $\\{e_1,..., e_n\\}$, in the same language, for other phenomena $P_1,...,P_n$, as well as observations of them, i.e., datasets $\\{D_1,..., D_n\\}$ in the form $D_i = \\{(x_1,1_{p_i} (x_1)),..., (x_m, 1_{P_i} (x_m))\\}$, with m < |U|. Intuitively, here we expect the learner to use the explanations paired with the observations to build an approximated interpreter $\\hat{I_L}$, and use it to make the proper prediction for x' by evaluating $\\hat{I_L} (x', e_o)$.\n\u2022 THE SCIENTIST PROBLEM: WE DO NOT HAVE AN EXPLANATION. We are given explanations $\\{e_1,...,e_n\\}$ in an unknown language L for other phenomena $P_1,...,P_n$ and observations of them $\\{D_1,..., D_n\\}$. However, we do not have an explanation for $P_o$; instead, we are given just a small set of observations $D_o = \\{(x_1,1_{p_o} (x_1)),..., (x_k, 1_{P_o} (x_k))\\}$ and two guarantees, namely that $P_o$ is explainable in L, and that $D_o$ is representative for $P_o$ in L. That is, for every phenomenon P \u2260 Po explainable in L there should exist at least a $x_i \\in D_o$ such that $1_{p_o} (x_i) \u2260 1_p (x_i)$. Again, we expect the learner to build the interpreter $\\hat{I_L}$, which should first guide the search for the missing explanation $e_o$ based on the clues $D_o$, and then provide the final prediction through $\\hat{I_L} (x', e_o)$.\nSeveral existing works fall within the formalization above. The sem-"}, {"title": "Why not explicitly ask for the rule?", "content": "Instead of requiring the player to reveal the secret explanation explicitly, we follow the principle of zero-knowledge proofs (Blum, Feldman, and Micali 1988). In our setting, this is done by asking the player to correctly tag many unseen structures according to the discovered rule. This makes it possible for any binary classification method to fit our EL environment without generating text. A winning condition is then defined by counting the correct predictions, instead of a textual similarity between predicted and correct explanation, which would require the player to guess word-by-word the secret rule. In fact, different phrasings with the same meaning should grant a victory, e.g., \u201cat least one pyramid pointing up and at most one pyramid pointing up\" is a winning guess for the secret rule \u201cexactly one pyramid pointing up\". A brute-force enumeration of all equivalent phrasings, in turn, would not allow solutions like \u201cexactly one one pyramid pointing up\", where \u201cone\u201d is mistakenly repeated twice; intuitively, we want to accept this as correct and dismiss the grammatical error. Similarly, a solution like \u201cexactly one pointing up\", where \u201cpyramid\" is omitted, should be accepted in a universe where only pyramids point up. We will reencounter these examples in Sec. 3.5 when we discuss the key properties of our approach."}, {"title": "Dataset generation", "content": "Odeen structures are sequences of six elements including spaces, blues or reds, squares or pyramids, the latter pointing up or down. The size of the universe is |U| = $7^6$ = 117,649 possible structures. We further created a small language with objects, attributes, quantifiers, logical conjunctions, and interactions (e.g., \u201ctouching\u201d, see Appendix A). The grammar generates \u224825k valid rules in total. Each of the |U| structures is tagged according to all the rules. The tagging is done by an interpreter implemented via regular expressions."}, {"title": "Metrics", "content": "As described above, the task is to tag l new structures for each of s unexplained games. An EL algorithm addressing this task encodes"}, {"title": "Critical Rationalist Networks", "content": "In principle, an EL problem like Odeen can be approached by training an end-to-end neural network to predict \u0177 = $1_{p_o}(x')$, given as input a set of observations $D_i$ and a single sample x\u2032 (see Figure 11 C, left). Such a model would assume that all the information needed to solve the task is embedded in the data, ignoring the explanations; we may call it a \u201cradical empiricist\" approach (Pearl 2021). A variant that includes the explanations in the pipeline can be done by adding a textual head to the network. This way, we expect perfor-"}, {"title": "Learning model", "content": "Our Critical Rationalist Networks (CRNs) tackle the EL scientist problem introduced in Sec. 3.2: to find y = $1_{p_o}(x')$ given x\u2032, $D_o$, $\\{D_1,..., D_n\\}$, $\\{e_1,..., e_n\\}$. They are formed by two independently trained models:\n(i) A stochastic Conjecture Generator\nCG: $\\{(x, 1_p(x))\\}_{i=1}^k \\rightarrow e$,\ntaking k \u2264 |Do| pairs (x, $1_p(x)$) \u2208 Di as input, and returning an explanation string e \u2208 \u03a3 as output. CG is trained to maximize the probability that CG($D_i$) = $e_i$ for all i = 1,...,n, where $D_i$ C Di is a random sampling of Di, and |$D_i$| = k.\n(ii) A learned Interpreter\nI: (e,x) $\\rightarrow$ \u0177,\nwhich takes as input a string e \u2208 \u03a3 and a sample x \u2208 U, to output a prediction \u0177 \u2208 $\\{0,1\\}$. I is trained to maximize the probability that I(li, x) = $1_{p_i}(x)$, with i = 1,...,n and (x, $1_{p_i}(x)$) \u2208 Di.\nAt test time, we are given a trained CG and a trained I, and we must predict whether some x' & Do belongs to Po or not. The idea is to first generate t conjectures by applying CG t times to the dataset Do; then, each conjecture is verified by counting how many times the interpreter I outputs a correct prediction over D0. The conjecture with the highest hit rate is our candidate explanation \u00eao for Po. Finally, we obtain the prediction \u0177' as I(\u00eao, x\u2032). See Figure 13 (left) for a step-by-step pseudo code.\nREMARKS. The interpreter I is a crucial component of our approach. A poor I may fail to identify eo among the generated conjectures, or yield a wrong prediction y' when given the correct eo. On the other hand, we can work with a CG of any quality and safely return as output an unknown token, rather than a wrong prediction, whenever eo does not appear among the generated conjectures. The role of CG is to trade-off performance for computational cost, and is controlled by the parameter t. Larger values for t imply more generated conjectures,"}, {"title": "Experiments", "content": "We extensively compared CRNs to the radical (EMP-R) and conscious (EMP-C) empiricist models over the Odeen EL problem, and analyzed several fundamental aspects."}, {"title": "Generalization power and data scaling laws", "content": "Seeing the generalization power of a learning algorithm as its ability in discovering new knowledge from little data, the Odeen challenge asks to explain 1132 unknown phenomena for which only 32 observations are available. We measure the performance on this task through a proof of knowledge based on the successful tagging of 1176 new structures per phenomenon (NRS). The information available at training time consists of symbolic explanations from 1438 known phenomena paired with m observations each (see Fig. 11A), we evaluated several settings with m ranging from 10K to 50. No test explanation is equivalent to the ones seen at training. Some example games can be found in Appendix D."}, {"title": "Handling ambiguity and contradiction", "content": "One may reasonably expect that a CRN equipped with the ground-truth interpreter used to generate the dataset, would perform better than a CRN with a learned interpreter. Remarkably, this is not always the case, as reported in Table 4.\nThe better performance of the fully learned interpreter over the ground-truth one is due to its ability to process ill-formed conjectures generated by the CG. The conjecture \u201cat least one pointing up\" makes the hard-coded interpreter fail, since \u201cpointing up\" must always follow the word \u201cpyramid\u201d by the grammar. Yet, in Odeen, pyramids are the only objects that point, and the learned I interprets the conjecture correctly. Other examples include: \u201cexactly one red block touching pyramid blue\u201d (\u201cpyramid\u201d and \u201cblue\u201d are swapped), or the contradictory \u201cat least one two pyramid pointing up and exactly one red pyramid\", which was interpreted correctly by ignoring the first \u201cone\u201d. When the learned interpreter is not very accurate, the negative effect of errors in tagging prevails."}, {"title": "Explainability", "content": "The predictions of a CRN are directly caused by a comprehensible explanation that is available in the output; this makes CRNs explainable by construction. Further, CRNs allow counterfactuals; one may deliberately change the output explanation with a new one to obtain a new prediction. The bank ML algorithm spoke: \u201cLoan denied\"; explanation: \"Two not paid loan in the past and resident in a district with a high rate of insolvents\u201d. With a CRN, we can easily discard this explanation and compute a new prediction for just \"Two not paid loan in the past\u201d.\nImportantly, by choosing a training set, we control the language used for explanations; i.e., we explicit the biases that will steer the learning of generalizations (Mitchell 1980). This allows a CRN to ignore undesirable patterns in the data (e.g., skin color) if these cannot be expressed in the chosen language. If the Odeen training set had no rule with \u201cpointing up/down\u201d, the learned interpreter would see all equal pyramids, even with unbalanced training data where"}, {"title": "Adjustable thinking time.", "content": "End-to-end models do not exhibit a parameter to adjust their processing to the complexity of the incoming prediction. By"}]}