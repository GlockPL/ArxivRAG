{"title": "FUIA: Model Inversion Attack against Federated Unlearning", "authors": ["Lei Zhou", "Youwen Zhu"], "abstract": "Abstract\u2014With the introduction of regulations related to the\n\"right to be forgotten\", federated learning (FL) is facing new\nprivacy compliance challenges. To address these challenges,\nresearchers have proposed federated unlearning (FU). However,\nexisting FU research has primarily focused on improving the\nefficiency of unlearning, with less attention paid to the potential\nprivacy vulnerabilities inherent in these methods. To address\nthis gap, we draw inspiration from gradient inversion attacks\nin FL and propose the federated unlearning inversion attack\n(FUIA). The FUIA is specifically designed for the three types of\nFU (sample unlearning, client unlearning, and class unlearning),\naiming to provide a comprehensive analysis of the privacy\nleakage risks associated with FU. In FUIA, the server acts as an\nhonest-but-curious attacker, recording and exploiting the model\ndifferences before and after unlearning to expose the features\nand labels of forgotten data. FUIA significantly leaks the privacy\nof forgotten data and can target all types of FU. This attack\ncontradicts the goal of FU to eliminate specific data influence,\ninstead exploiting its vulnerabilities to recover forgotten data\nand expose its privacy flaws. Extensive experimental results\nshow that FUIA can effectively reveal the private information\nof forgotten data. To mitigate this privacy leakage, we also\nexplore two potential defense methods, although these come at\nthe cost of reduced unlearning effectiveness and the usability of\nthe unlearned model.\nIndex Terms\u2014Gradient inversion attack, machine unlearning,\nfederated learning.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of artificial intelligence, the\nissue of data privacy and security in training machine\nlearning models has garnered increasing attention from both\nacademia and industry. Protecting data privacy is not merely\na technical challenge but also a matter of fostering user trust\nand ensuring societal stability [1]. To address these concerns,\nvarious regulations have been enacted worldwide, such as\nthe General Data Protection Regulation (GDPR) [2] in the\nEuropean Union and the California Consumer Privacy Act\n(CCPA) [3] in the United States. These regulations provide\nrobust legal protections for data owners, with the \"right to be\nforgotten\" [4] drawing significant attention in recent years.\nThis right mandates that when a data owner requests the\ndeletion of their data, the model owner must not only remove\nthe corresponding data from the database but also eliminate\nits impact on the trained model.\nFederated Learning (FL) [5] is designed to enable dis-\ntributed model training while preserving the privacy of indi-\nvividual clients. Unlike traditional centralized training methods,\nFL allows data to remain local, enabling collaborative learn-\ning through the exchange of model parameters or gradients,\nthereby achieving a balance between model performance and\nprivacy protection. However, the introduction of the \"right to\nbe forgotten\" imposes new requirements on FL, particularly\nin efficiently fulfilling data owners' requests for data removal.\nRecent studies have proposed a novel approach known as\nFederated Unlearning (FU) [6], which aims to directly erase\nthe impact of the target data from the model without the need\nfor retraining the entire model. Compared to conventional\nretraining methods, FU significantly improves the efficiency\nof the data removal process while reducing computational\ncosts and resource consumption, making FL more practical\nand adaptable in privacy-sensitive scenarios.\nHowever, current research on FU primarily focuses on\nimproving the efficiency of the unlearning process, while\npaying little attention to the potential privacy leakage issues\ninherent in FU. During the FU process, the server has full\nknowledge of both the original model and the unlearned\nmodel, and can even track the parameter updates uploaded\nby each client in every round of the unlearning process.\nIf the server is honest-but-curious or even malicious, this\ncomprehensive access could result in the privacy leakage of the\ntarget data intended to be forgotten. This risk fundamentally\ncontradicts the original purpose of FU, which is to safeguard\nthe privacy of the forgotten data. Therefore, understanding\nand investigating privacy vulnerabilities associated with the\nFU process is critically important. Such efforts are essential\nnot only for improving existing FU algorithms but also for\nguiding the development of more secure and reliable privacy-"}, {"title": "II. RELATED WORK", "content": "A. Federated unlearning\nThe primary goal of FU is to derive an unlearned model\ndirectly from the original model without retraining, thereby\nsignificantly improving the efficiency of the unlearning pro-\ncess. Based on the type of target data to be forgotten, ex-\nisting FU research is typically divided into three categories:\nclass unlearning, client unlearning, and sample unlearning\n[6]. Currently, almost all FU methods focus on improving\nunlearning efficiency, with only a few considering privacy\nprotection to some extent. For instance, Zhang et al. [7]\nproposed a client unlearning method that stores historical\nupdates from each client and removes the weighted sum of\ngradient residuals from the global model to achieve efficient\nunlearning. Additionally, they incorporated tailored Gaussian\nnoise with differential privacy to protect the privacy of the\nglobal model. Similarly, Varshney et al. [8] improved the\nefficiency of client unlearning by clustering client weights,\nrandomly selecting representative clients, and perturbing their\nweights, while also leveraging differential privacy to maintain\nthe privacy of the global model. RevFRF [9] introduced a\nmethod based on the Distributed Two-Trapdoor Public Key\nCryptosystem (DT-PKC) to enhance the efficiency of client\nunlearning in federated forest scenarios while protecting user\ndata privacy. Meanwhile, Liu et al. [10] addressed potential\nprivacy issues in cluster-based FU methods by employing\nsecure aggregation techniques to ensure user data privacy. In"}, {"title": "III. PRELIMINARIES", "content": "In this section, we elaborate on the essential background\nknowledge related to FUIA, focusing on the fundamental\nprinciples of FL and the specific implementation of GIA within\nthe FL framework.\nA. Federated learning\nFederated learning is a distributed machine learning frame-\nwork designed to collaboratively train a global model across\nmultiple clients while preserving data privacy [23]. The train-\ning process of FL typically involves the following steps: the\nserver initializes the global model parameters $w_0$ and, in\neach subsequent round $t$, randomly selects a subset of clients\n$S_t$ to participate in training. At round $t + 1$, each selected\nclient receives the global model parameters $w_t$ of the previous\nround and performs several steps of stochastic gradient descent\n(SGD) on its local dataset. The updated parameters for client\n$k$ are computed as:\n$w_{k}^{t+1} = w_t - \\eta \\nabla L_k(w_{t+1})$\nwhere $L_k(w_t)$ represents the local objective function of client\n$k$, and $\\eta$ is the learning rate. After local updates, the clients\nsend their updated parameters back to the server, which\naggregates these updates to compute the new global model\nparameters.\nTwo common aggregation methods in FL are FedAvg and\nFedSGD. In FedAvg, the server computes a weighted average\nof the local model parameters received from the participating\nclients, with weights proportional to the size of each client's\ndataset. The updated global model is given by:\n$w_{t+1} = \\frac{\\Sigma_{k\\in S_{t+1}} n_k w_k^{t+1}}{N_{S_{t+1}}}$\nwhere $n_k$ is the local dataset size of client $k$, and $n_{st+1}$ is the\ntotal data size of the selected clients.\nIn contrast, FedSGD aggregates the local gradients uploaded\nby the clients and directly updates the global model parameters\nas:\n$w_{t+1} = w_t - \\eta \\Sigma_{k\\in S_{t+1}} \\frac{n_k}{n_{S_{t+1}}} \\nabla L_k(w_t)$\nThrough this iterative training process, FL achieves efficient\nglobal model optimization while ensuring data privacy, as raw\ndata never leaves the clients' devices."}, {"title": "B. Gradient Inversion Attack", "content": "Gradient inversion attacks focus on reconstructing training\ndata through an optimization process [24]. After obtaining the\ngradient update $\\Delta W$, an attacker generates virtual samples\n$(\\hat{x}, \\hat{y})$ and feeds them into the model to compute the virtual\ngradient $\\Delta W'$. The attack then iteratively optimizes the virtual\nsamples by minimizing the distance between the real gradient\n$\\Delta W$ and the virtual gradient $\\Delta W'$, gradually approximating\nthe original training samples.\nThe optimization objective can be expressed as:\n$\\min_{\\hat{x},\\hat{y}} Dist(\\frac{\\partial L(F(W,\\hat{x}),\\hat{y})}{\\partial W}, \\Delta W) + Reg(\\hat{x}, \\hat{y})$\nwhere $W$ is the current trainable parameters of the model, $F$\nis the forward propagation function of the model, $L$ repre-\nsents the loss function, $\\Delta W = \\frac{\\partial L(F(W,x),y)}{\\partial W}$ is the gradient\nupdate uploaded by the client. And $Dist$ represents a distance\nfunction (e.g., $L_2$ distance or cosine similarity), commonly\nused to measure gradient similarity in GIA. Reg represents a\nregularization term to ensure realistic sample reconstruction.\nIn image classification tasks, additional regularization tech-\nniques such as total variation [25] (to reduce noise) or clipping\nconstraints (to limit pixel values) are often applied to produce\nnatural-looking images. The ultimate goal is to optimize both\nthe virtual samples $\\hat{x}$ and virtual labels $\\hat{y}$ to retrieve the local\ntraining data $(x,y)$.\nTo further simplify the optimization, some studies propose\nlabel inference methods based on gradient tensor analysis [19],\n[26]. For instance, by analyzing the gradient distribution of\nthe final fully connected layer, labels can be predicted before\nstarting the optimization. This approach not only improves the\nquality of data reconstruction but also reduces computational\ncomplexity."}, {"title": "IV. APPROACH", "content": "In this section, we first present the application background\nof FUIA and the key challenges encountered in its implemen-\ntation. Subsequently, we systematically discuss the specific\nimplementation approaches of FUIA from three perspectives:\nsample unlearning, client unlearning, and class unlearning.\nA. Problem statement\nWe primarily focus on classification tasks, which represent\none of the most widely adopted applications in Federated\nLearning (FL) and serve as the foundation for the majority\nof existing Federated Unlearning (FU) research. The core\nobjective of Federated Unlearning Information Attack (FUIA)\nis to reveal information about the forgotten data by accessing\nboth the original model and the unlearned model. We assume\nthat the server, acting as an attacker, is honest-but-curious.\nSpecifically, during the training phase prior to federated un-\nlearning, the server continuously records the parameter updates\nuploaded by each client. During the unlearning process, it\nparticularly records the parameter updates uploaded by the\ntarget client that issued the unlearning request. However,\nthe server does not interfere with the normal progression\nof training or unlearning. This assumption not only aligns\nwith intuition but also holds strong practical feasibility in\nthe federated learning environment, as it is difficult to detect.\nIn fact, many existing federated unlearning methods rely on\nstored historical records to achieve unlearning, thereby making\nthis assumption widely applicable in practical scenarios [27]-\n[30].\nB. Basic methods and challenges\n1) Aggregation methods: Nearly all existing FU methods\nadopt FedAvg as the underlying aggregation method, likely\ndue to its greater applicability in real-world scenarios. How-\never, this choice introduces new challenges for the design of\nFUIA. In FL, most GIA rely on FedSGD as the aggregation\nmethod because it allows direct access to client-uploaded\ngradients, making it easier for attackers to invert local data. In\ncontrast, when FedAvg is used as the aggregation method, the\nserver only receives model parameter updates from clients,\nwithout direct access to gradient information, which makes\ninversion based on model parameters significantly more chal-\nlenging.\nNonetheless, to align with the prevailing design of FU\nmethods, we adopted FedAvg as the aggregation method in\nFUIA. Based on this choice, we developed inversion tech-\nniques tailored to FedAvg, aiming to reconstruct forgotten\ndata information by analyzing differences in client model\nparameters. Additionally, to further evaluate the generality of\nFUIA, we also considered scenarios where FedSGD is used\nas the aggregation method in our experiments, examining how\nthe availability of gradient information affects the performance\nof FUIA.\n2) Inversion methods: In machine learning, information\nabout the training data can be reflected through the model\nparameters [31]. As a distributed machine learning method,\nFL enables each client to obtain a global model containing\ninformation from all clients through interactive training. As\ndescribed in Section 3.1, in FL with FedAvg as the aggregation\nmethod, the parameter update for client k in round t + 1 can\nbe expressed as:\n$w_{k}^{(t+1)} = w_t - \\eta \\nabla L_k(w_t)$\nwhere $L_k(w_t)$ is the gradient of the loss function $L_k$ with\nrespect to model parameters $w_t$ on the local data. The differ-\nence between $w_{k}^{(t+1)}$ and $w_t$ arises from the gradient update\nusing the local data. Therefore, intuitively, the difference\nbetween the two models can be approximated as the gradient\ninformation reflecting the difference in training samples. In\nFU, assume that the parameter of the original global model is\n$w^\\circ$, which contains the information about the forgotten data,\nwhile the parameter of the unlearned global model is $w^u$,\nwhich does not contain this information. Thus, the difference\nbetween the original model and the unlearned model reflects\nan approximation of the gradient information of the target\nforgotten data:\n$\\Delta W = W^\\circ - W^u$\nThe first challenge in FUIA is how to leverage the gradient\ninformation derived from this model difference to invert the\nforgotten data."}, {"title": "C. FUIA against sample unlearning", "content": "The goal of sample unlearning is to forget parts of the data\nfrom multiple different clients. When these clients who wish to\nunlearn part of their data send an unlearn request to the server,\nthe server and participating clients initiate the FU process and\nultimately obtain the unlearned model. For sample unlearning,\nthe attack objective of FUIA is to invert the forgotten sample's\nfeature information, i.e., the image. We divide the entire attack\nprocess into three steps: gradient separation, target gradient\nacquisition, and gradient inversion.\n1) Step 1: Gradient Separation: The goal of gradient\nseparation is to extract the \"clean\" gradient information of\nthe target client data from the global model parameters trained\nvia FL, thereby removing the gradient noise from other clients'\ndata. Assume that client k has sent an unlearn request. Prior to\nunlearning, during the FL training process based on FedAvg,\nthe parameter difference (i.e., the gradient information for this\nclient's data) before and after training in the t-th epoch is:\n$\\Delta \\omega_i^t = w_i^{(t)} - w_i^{(t-1)}$\nTo balance the parameter differences across all rounds and\nintegrate the \"clean\" gradient information corresponding to\nthe local client data, the following steps are taken. First, we\ncalculate the total sum of the $l_1$ norms of parameter update"}, {"title": "Algorithm 1 FUIA for Sample Unlearning", "content": "Require: Global model parameters before unlearning $W^\\circ$,\nglobal model parameters after unlearning $W^u$, unlearning\nrequest from client $k$.\nEnsure: Reconstructed feature information of forgotten data\nfrom client $k$, denoted as $\\hat{x}$.\nServer Executes:\n1: Receive unlearning request from client $k$.\n2: Start FU process, update global model from $W^\\circ$ to $W^u$\nwith client $k$'s forgotten data removed.\n3: Calculate gradient difference $\\nabla_k = \\nabla_k^\\circ - \\nabla_k^u$ as in Eq.\n(11).\n4: Send updated global model to all clients.\nClient Executes:\n5: for all clients in parallel do\n6:\tCompute parameter differences $\\Delta \\omega_k^t$ for each round of\ntraining using Eq. (7).\n7:\tCalculate the total sum of $l_1$ norms of parameter dif-\nferences for each round using Eq. (8).\n8:\tCompute weight coefficient $\\gamma_t^k$ for each round using Eq.\n(9).\n9:\tCompute \"clean\" gradient information for client $k$, $\\nabla_k^\\circ$\nusing Eq. (10).\n10:\tCalculate \"clean\" gradient information for client $k$ after\nforgetting data, $\\nabla_k^u$.\n11: end for\n12: Use gradient difference $\\nabla_k$ for target forgotten data to\nperform gradient inversion.\n13: Solve optimization problem in Eq. (12) to obtain virtual\nsample $\\hat{x}$:\n$\\min_{\\hat{x}} \\frac{<\\nabla \\mathcal{L},\\nabla_k>}{\\|\\nabla \\mathcal{L}\\|_2\\|\\nabla_k\\|_2} + \\alpha \\cdot TV(\\hat{x})$\n14: if Optimization converges then\n15:\tObtain reconstructed feature information of the forgot-\nten data, $\\hat{x}$.\n16: else\n17:\tRepeat optimization steps.\n18: end if\n19: return Reconstructed data $\\hat{x}$."}, {"title": "C. FUIA against sample unlearning", "content": "differences for all participating clients in each round. Let $C_t$\nbe the set of clients participating in training in the t-th epoch,\nthen the sum of the l\u2081 norms of parameter differences for the\nt-th round is:\n$sum_t = \\Sigma_{k \\in C_t} ||\\Delta \\omega_i^t||_1$\nNext, for each client k, we calculate the weight coefficient\n$\\gamma_t^k$ for the t-th round, which represents the relative weight\nof client k in that round's gradient update. This coefficient is\ncomputed as:\n$\\gamma_t^k = \\frac{||\\Delta \\omega_i^t||_1}{sum_t}$"}, {"title": "C. FUIA against sample unlearning", "content": "And last, by weighted averaging the gradient differences\n$\\Delta \\omega_i^t$ for each round, we can compute the \"clean\" gradient\ninformation for client k, denoted $\\nabla_k^0$:\n$\\nabla_k^0 = \\Sigma_{t=1}^{E} \\gamma_t^k \\Delta \\omega_i^t$\nSimilarly, using the stored parameter updates during the FU\nprocess, we can obtain the \"clean\" gradient information for\nclient k after removing the forgotten data, denoted $\\nabla_k^u$.\n2) Step 2: Target Gradient Acquisition: The goal of this\nstep is to use the difference between the \"clean\" gradient\ninformation before and after unlearning to acquire the gradient\ninformation of the target forgotten data. From the gradient\nseparation step, we obtain the \"clean\" gradient information\n$\\nabla_k^0$ for client k during FL training, which contains the gradient\ninformation for all the client's data. After FU, the \"clean\" gra-\ndient information for client k is $\\nabla_k^u$, which contains only the\ngradients for the remaining data after removing the forgotten\ndata. Thus, the gradient information for the target forgotten\ndata on client k is:\n$\\nabla_k = \\nabla_k^0 - \\nabla_k^u$\n3) Step 3: Gradient Inversion: The goal of gradient in-\nversion is to reveal the features of the forgotten data based\non the gradient information obtained in the previous step. As\ndiscussed in Section 2.2, many recent studies on FL have\nfocused on GIA. Specifically, the server can use optimization\nalgorithms to reconstruct private training data from gradients,\nthus compromising the privacy of client data. Inspired by such\nattacks, we aim to use optimization algorithms to decode the\nforgotten data's gradient information and recover its feature\ninformation. Most existing GIA research is based on FedSGD,\nwhich allows direct access to the gradients of training data, but\nFU methods are typically based on FedAvg. Therefore, by fol-\nlowing the previous steps, we obtain the gradient information\nof the target forgotten data from the global model parameters\nwhile removing noise from other clients. However, not all GIA\noptimization algorithms are suitable for FUIA. The reason is\nthat, to make the attack harder to detect and applicable to a\nwider range of cases, we assume a looser attacker model where\nthe server behaves as an honest-but-curious entity rather than a\nmalicious attacker. Specifically, we assume the server does not\nknow the exact FU method and the fact that FU is based on\nFedSGD. Therefore, the server only obtains an approximation\nof the forgotten data's gradient rather than the exact gradient.\nMoreover, some GIA methods require additional information\nsuch as data distribution for reconstruction. However, we\nassume the attacker strictly follows the FL protocol and does\nnot have access to extra information.\nTo address these challenges, we adopt the optimization\nalgorithm from [25] to recover the feature information of the\nforgotten data based on its gradient information. The principle\nof GIA is discussed in Section 3.2. Specifically, we choose\ncosine similarity as the distance function to measure gradient"}, {"title": "C. FUIA against sample unlearning", "content": "similarity, focusing more on the direction of the gradients\nbecause it contains crucial information on how the model\nresponds to input changes. We also include total variation\n(TV) as a regularization term to promote smoothness in the\nreconstructed image while preserving edge information. Based\non the steps above, we obtain the gradient information $\\nabla_k$ of\nthe target forgotten data on client k. Therefore, the objective\nof the optimization algorithm can be expressed as:\n$\\min_{\\hat{x}} \\frac{<\\nabla \\mathcal{L}, \\nabla_k>}{\\|\\nabla \\mathcal{L}\\|_2 \\|\\nabla_k\\|_2} + \\alpha TV(\\hat{x})$\nwhere $\\nabla \\mathcal{L} = \\frac{\\partial L(F(W^\\circ, \\hat{x}), \\hat{y})}{\\partial \\hat{x}}$, $TV(\\hat{x})$is the total variation regu-\nlarization term, and $\\alpha$ is a hyperparameter balancing the two\nterms.\nThe goal is to use the optimization algorithm to make the\ngradient of the constructed virtual sample as close as possible\nto the gradient of the target forgotten data, thereby obtaining\na virtual sample that closely matches the target forgotten data.\nSince we have obtained an approximation of the gradient for\nthe forgotten data, we focus more on the direction of the\ngradients rather than their magnitude, which is why we use\ncosine similarity as the distance function."}, {"title": "D. FUIA against client unlearning", "content": "In the context of Client Unlearning, the goal is to \"unlearn\"\nall the data from a specific client. After the client sends an\nunlearning request, the server and client initiate the unlearning\nprocess, ultimately obtaining a global model that no longer\nincludes the data of the target client. In this scenario, the goal\nof the FUIA is to reverse-engineer and extract the feature\ninformation of the target client, specifically the image data.\nSimilar to Sample Unlearning, the entire attack process can\nbe divided into three steps: gradient separation, target gradient\nacquisition, and gradient inversion. However, compared to\nSample Unlearning, the implementation of the attack process\nin Client Unlearning differs in the specific operations of each\nstep.\n1) Step 1: Gradient Separation: In the gradient separation\nstep, the objective is to extract the \"clean\" gradient information\nof the target client's data from the global model parameters ob-\ntained through FL. Suppose that client k makes the unlearning"}, {"title": "Algorithm 2 FUIA for Client Unlearning", "content": "Require: Global model parameters before unlearning $W^\\circ$,\nglobal model parameters after unlearning $W^u$, unlearning\nrequest from client $k$.\nEnsure: Reconstructed feature information of client $k$, de-\nnoted as $\\hat{x}$.\nServer Executes:\n1: Receive unlearning request from client $k$.\n2: Start FU process, update global model from $W^\\circ$ to $W^u$\nwith client $k$'s data removed.\n3: Calculate gradient difference $\\nabla^* = W^\\circ - W^u$.\n4: Send updated global model to all clients.\nClient Executes:\n5: for all clients in parallel do\n6:\tCompute parameter differences $\\Delta \\omega_i^t$ for each round of\ntraining.\n7:\tCalculate clean gradient information $\\nabla_k^0$ for client $k$\nbased on Eq. (13):\n$\\nabla_k^0 = \\Sigma_{t=1}^{E} \\gamma_t^k \\cdot \\Delta \\omega_i^t$\n8: end for\n9: Use $\\nabla_k^0$ and $\\nabla^*$ to refine target gradient.\n10: Apply gradient inversion optimization to reconstruct vir-\ntual sample $(\\hat{x}, \\hat{y})$.\n11: Solve the optimization problem in Eq. (18) to minimize:\n$\\min_{\\hat{x},\\hat{y}} -((1-\\gamma) * l(\\nabla, \\nabla_k^0) + \\gamma * l(\\nabla, \\nabla^*)) + \\alpha TV(\\hat{x})$\n12: if Optimization converges then\n13: Obtain reconstructed feature information $\\hat{x}$ of client $k$.\n14: else\n15: Repeat optimization steps.\n16: end if\n17: return Reconstructed data $\\hat{x}$."}, {"title": "D. FUIA against client unlearning", "content": "request, and after the FU process, the data of client k has been\nentirely unlearned, meaning that the global model no longer\ncontains the gradient information of client k's data. Therefore,\nunlike Sample Unlearning, this step does not require focusing\non the gradient changes in the FU process but only needs\nto separate the gradient information of client k from the FL\ntraining process. Specifically, by calculating the parameter\ndifferences for each round and weighting them, we can obtain\nthe \"clean\" gradient information of client k, denoted as $\\nabla_k^0$,\nas shown in the following formula:\n$\\nabla_k^0 = \\Sigma_{t=1}^{E} \\gamma_t^k \\cdot \\Delta \\omega_i^t$\nwhere E is the total number of training epochs, and $\\nabla_k^0$ is\nthe \"clean\" gradient information of client k, with noise from\nother clients' gradients removed.\n2) Step 2: Target Gradient Acquisition: We leverage the\n\"clean\" gradient information ($\\nabla_k^0$) obtained in the previous\nstep, along with the gradient information difference between\nthe global model before and after unlearning ($\\nabla^*$), to further\nrefine the target client's data gradient. Let the global model\nparameters before unlearning be $W^\\circ$, and after unlearning,"}, {"title": "D. FUIA against client unlearning", "content": "they are $W^u$, then the gradient information for the target\nclient's data is the difference between these two, expressed\nas:\n$\\nabla = W^\\circ - W^u$\nThe goal of this step is to enhance the gradient description\nof the target client's data by supplementing it with the gradient\ninformation from the global model difference. However, due to\nparameter interactions in the FL training process, $\\nabla^*$ contains\nnoise from other clients' data, and hence, we treat it as\nauxiliary information rather than directly using it.\n3) Step 3: Gradient Inversion: Finally, in the gradient\ninversion step, the optimization algorithm's goal is to use the\ngradient information obtained in the previous step to reveal the\ntarget client's feature information. Similar to Sample Unlearn-\ning, this process is also implemented through an optimization\nalgorithm, with the objective function given by:\n$\\min_{\\hat{x},\\hat{y}} -((1-\\gamma) * l(\\nabla, \\nabla_k^0) + \\gamma * l(\\nabla, \\nabla^*)) + \\alpha TV(\\hat{x})$\nwhere $\\nabla = \\frac{\\partial L(f(x))}{\\partial \\hat{x}}$ represents the gradient of the con-\nstructed virtual sample $(\\hat{x}, \\hat{y})$, $l(\\nabla,\\nabla^*)$ is\nthe cosine similarity between the gradient vectors, $TV(\\hat{x})$\nis the total variation regularization term used to control the\nsmoothness of the reconstructed image and reduce noise and\nartifacts, $\\alpha$ is a hyperparameter that balances the regularization\nterm, and $\\gamma$ is used to balance the influence of $\\nabla_k^0$ and $\\nabla^*$.\nThrough this optimization process, the attacker can con-\nstruct a virtual sample whose features closely resemble the\ntarget client's unlearned data, successfully revealing the feature\ninformation of the unlearned data."}, {"title": "E. FUIA against class unlearning", "content": "In the context of Class Unlearning, the goal is to unlearn\nall data of certain classes. Unlike client unlearning or sample\nunlearning, the core of class unlearning lies in forgetting\ndata from specific classes, which may be distributed across\nmultiple clients. Since we make no specific assumptions about\nthe data distribution in FL, and the data is non-iid (non\nindependent and identically distributed), data from a given\nclass may randomly appear across multiple clients. For the"}, {"title": "Algorithm 3 FUIA for Class Unlearning", "content": "Require: Global model parameters before unlearning $v_o$, $b_o$\n(weight and bias for the output layer), global model\nparameters after unlearning $v_u$, $b_u$\nEnsure: The label of the forgotten class.\nServer Executes:\n1: Receive unlearning requests for a specific class.\n2: Perform FU process: remove data of the target class and\nupdate model parameters, resulting in updated weights $v_u$\nand biases $b_u$.\n3: Calculate weight differences for each class using Eq. (16).\n4: For each class i, compute the discrimination score\n$diff\\_score[i]$ using Eq. (17):\n5: Identify the class with the maximum discrimination score:\n$class\\_id = arg\\_max(diff\\_score[i])$\n6: return The label of the forgotten class: $class\\_id."}, {"title": "E. FUIA against class unlearning", "content": "server acting as an attacker, although it cannot directly access\ndata from individual clients, it has knowledge of the entire\ntraining dataset. Therefore, in class unlearning, the objective\nof the FUIA is to reverse-engineer and infer the labels of the\ntarget forgotten class. By leaking these labels, the attacker\ncan obtain sensitive privacy information regarding the training\ndata.\nExisting studies have shown that the removal of large\namounts of data has a significant impact on the model's\nparameters [123321]. The training process of deep learning\nmodels essentially optimizes the model's weights to improve\nperformance. When data from a specific class is deleted,\nthe model's weights, particularly those in the output layer\nassociated with that class, will change. This is because, after\ndeleting the data points, the model no longer receives training\nsignals from these data, which causes the weights of the\noutput nodes related to that class to update less frequently and\nmay even diminish or converge to zero. After the unlearning\nprocess removes data from a specific class, the server, acting\nas the attacker, can infer the characteristics of that class's data\nby comparing the weight differences in the output layer before\nand after the unlearning.\nFor a multiclass classification problem, suppose there are n\nclasses, and the score yi for each class i can be expressed as:\nYi = $v_i + b_i$, where vi is the weight vector corresponding to\nclass i (size m \u00d7 n), and bi is the bias term for class i (size\nn). We define a discrimination score diff_score to represent\nthe difference in the output layer before and after unlearning.\nAssuming the output layer's weight matrix and bias before\nunlearning are vo and bo, and after unlearning they are vu\nand bu, we can use the L1 norm to measure the changes. The"}, {"title": "E. FUIA against class unlearning", "content": "weight matrix difference and bias difference are computed as:\n$Vdiff[i", "j": "v_u[i"}, {"j": 1, "n$bdiff[i": "b_o[i", "b_u[i": 1, "as": "n$diff\\_score[i", "frac{Vdiff[i": ""}, {"z": ""}, 1], "as": "n$class\\_id = arg max(diff\\_score[i])$\nThrough this process, the"}