{"title": "Behavior Backdoor for Deep Learning Models", "authors": ["Jiakai Wang", "Pengfei Zhang", "Renshuai Tao", "Jian Yang", "Hao Liu", "Xianglong Liu", "Yunchao Wei", "Yao Zhao"], "abstract": "The various post-processing methods for deep-learning-based models, such as quantification, pruning, and fine-tuning, play an increasingly important role in artificial intelligence technology, with pre-train large models as one of the main development directions. However, this popular series of post-processing behaviors targeting pre-training deep models has become a breeding ground for new adversarial security issues. In this study, we take the first step towards \"behavioral backdoor\" attack, which is defined as a behavior-triggered backdoor model training procedure, to reveal a new paradigm of backdoor attacks. In practice, we propose the first pipeline of implementing behavior backdoor, i.e., the Quantification Backdoor (QB) attack, upon exploiting model quantification method as the set trigger. Specifically, to adapt the optimization goal of behavior backdoor, we introduce the behavior-driven backdoor object optimizing method by a bi-target behavior backdoor training loss, thus we could guide the poisoned model optimization direction. To update the parameters across multiple models, we adopt the address-shared backdoor model training, thereby the gradient information could be utilized for multimodel collaborative optimization. Extensive experiments have been conducted on different models, datasets, and tasks, demonstrating the effectiveness of this novel backdoor attack and its potential application threats.", "sections": [{"title": "1. Introduction", "content": "Deep learning models (DLMs), especially large-scale pre-trained models, have been employed in different systems and scenarios, such as unmanned driving vehicles [17, 68], medical image processing [2, 18, 40, 46], X-ray inspection [53\u201355], due to their empowerment ability to existing industries. However, numerous previous studies have revealed that deep learning models are not trustworthy types because of their vulnerability to adversarial examples [58], backdoor attacks [14], and privacy leakages [11], building upon their black-box characteristics and unexplainability. On this basis, the researchers make great efforts to investigate the security issues of deep learning models from both the attacking perspective and the defending perspective [38, 57, 60, 66].\nAmong them, the backdoor attack is widely investigated in the community due to its high concealment of attack forms and strong controllability of attack effects. A backdoor attack implants trigger-only-sensitive backdoors into the deep learning model in a way that the poisoned model learns both the attacker-chosen task and the benign task [5, 14]. In this way, the poisoned backdoor models will behave as normal on benign inputs and output correct predictions while making designated decisions when the inputs are attached with triggers. Since the backdoor models only show abnormal behaviors to the designed triggers, they are difficult to distinguish the poisoned models and the clean ones by solely checking the accuracy with the naive samples [12]. After the first backdoor attack for DLMs [14], there occurs a series of backdoor attack studies, including Blended [5], SIG [3], WaNet [44], InputAware [45], SSBA [33], and so on. Recently, the large model backdoor attack studies also draw the researchers' attention [21, 35, 65].\nHowever, although these backdoor attacks are implemented in multiple technical routes and can be categorized into different types, they all could be identified as data-triggered backdoor attacks, which achieve the attacking objective by planting the triggers into input samples when attacking.\nUpon this background, we had a flash of inspiration and thought about an interesting question, i.e., can we plant a backdoor that could be triggered via specific operation behavior into models? This question motivates us to investigate if there exists a model-oriented operation behavior that could be utilized as a backdoor trigger. Fortunately, we notice that some typical post-processing operations for models could play this critical role in our backdoor attacking vision. More precisely, in recent years, due to the maturity of the model pre-training route, conducting post-processing operations, such as quantization [13, 19, 23, 43, 69], pruning [1, 41, 42], and fine-tuning [52], on the pre-trained models from open-source platforms become more and more popular. The developers could download the model checkpoints and re-process the acquired models according to the requirements of the application in practice, therefore satisfying the expected functions. Based on the idea of exploiting these model post-processing operations, we believe that it is possible in practice to implant special backdoors into pre-training models and these \"Trojan\" models (i.e., poisoned models) could be triggered by some model post-processing operations.\nHolding the aforementioned perspective, we take the first step to propose an unprecedented paradigm for the DLMs-oriented backdoor attack, which we name it as \"behavior backdoor\", to train backdoor models that will be triggered by specific post-processing operations as shown in Figure 1. In practice, we select the widely used quantification method as a target behavior and design the first behavior backdoor pipeline for this new type backdoor attack, i.e., \u201cquantification backdoor (QB) attack\". More precisely, the QB attack consists of the behavior-driven backdoor object optimizing and address-shared backdoor model training. The former is designed as a bi-target optimization progress with quantification backdoor loss, which guides the poisoned model optimization direction according to the quantified model behaviors. In this way, the poisoned DLMs could be forced to acquire the ability of maintaining good behaviors and of appearing appointed bad behaviors when facing quantification operations. The latter is proposed to tackle the difficulties in gradient information update derived from the multimodel collaborative optimization. This schedule allows the gradient information to be updated relatedly across the victim model and the quantified model. Therefore, the behavior-triggered backdoor could be implanted successfully and seduces pre-set false prediction out in practice. To verify the feasibility and effectiveness, we conduct comprehensive experiments on MNIST, CIFAR, and TinyImageNet with full consideration of different classical model architecture such as AlexNet, VGG, ResNet, and ViT. The main contributions are:\n\u2022 To the best of our knowledge, we are the first to propose the concept of behavior backdoor and construct the first pipeline for this new-type backdoor attack paradigm.\n\u2022 We elaborate the first quantification backdoor (QB) attack, which consists of behavior-driven backdoor object optimizing and address-shared backdoor model training.\n\u2022 We conduct extensive experiments on classical various datasets and models, including ablations and discussions, strongly demonstrating the feasibility and effectiveness of the proposed behavior backdoor attack. We believe that our study reveals a never-discovered potential threat for deep learning based models."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Backdoor Attack", "content": "Deep learning models face various security threats, one of which is backdoor attacks. Specifically, an attacker can design a neural network with a backdoor that performs well on the user's training and validation samples but exhibits abnormal behavior on specific inputs chosen by the attacker [3, 5, 14, 33, 34, 44, 45]. Gu et al. were the first to highlight backdoor vulnerabilities in deep neural networks (DNNs) and put forward the BadNets [14] attack algorithm, which embeds backdoors into models during training. To enhance the stealthiness of backdoor triggers, Chen et al. proposed a new type of trigger that uses global random noise or an image-mixing strategy, known as the Blend attack [5]. Barni et al. proposed SIG [3], a label attack that uses a ramp or horizontal sinusoidal signal as the backdoor trigger. To bypass existing defenses and increase the concealment and efficacy of attacks, Liang et al. developed a clean-label backdoor attack called PFF [34], which targets face forgery detection models by translation-sensitive trigger pattern, hidden triggers that cause misclassification when detecting fake faces.\nThough achieving results, these existing backdoor attack methods rely on the data triggers inside input instances to activate the attack, whom we summarized as data-triggered backdoor attacks"}, {"title": "2.2. Model Post-processing", "content": "The pre-training models, such as BLIP [29], BLIP2 [30] and other large models, are facing some issues that limit their deployment on devices with limited resources or fine-grained tasks. The model post-processing technology is to solve these problems and optimize the efficiency and deployability of the model, such as quantification [13, 19, 23, 43], pruning [1, 41, 42], fine-tuning [52] and so on.\nModel Quantification reduces model size and increases inference speed by converting floating-point number parameters to fixed-point number parameters. Quantization can be divided into Post-training Quantization (PTQ) and Quantization Aware Training (QAT). DOREFA-NET [70] and IAO [20] represent the early work of QAT. Later, Shin et al. proposed NIPQ [49] as an improvement over QAT. In the field of Post-training Quantization of vision models, in addition to the work of Liu et al. [39], FQ-VIT [37] and PTQ4VIT [67] also show considerable performance.\nModel Pruning reduces model size and improves inference efficiency by removing unimportant parts of the network. OBD [26] and OBS [15] are two classic pruning methods. Other methods such as Wang et al. [63], pruning filters in the layer(s) with the most structural redundancy, and the Depgraph [10] for arbitrary structured pruning are effective in performance optimization.\nModel Fine-tuning is the adaptation or optimization of a model to a specific task downstream. In the early days, there is the approach of increasing the model capacity of the network to optimize the fine-tuning process [62]. Recently, the Prefix Tuning [31] and the Prompt Tuning [27] are more popular in this area."}, {"title": "3. Approach", "content": "In this section, we first give the definition of the proposed behavior backdoor. And then we provide the overview of the quantification behavior (QB) attack."}, {"title": "3.1. Problem Definition", "content": "The standard backdoor attack is designed to train a poisoned model that is sensitive to the specific trigger patterns inside inputs and makes appointed predictions. Formally, given a model F to be trained, data trigger pattern $d_{data}$, the dataset where the input samples $X = {X_1, X_2, X_3, ..., X_n}$ with corresponding ground-truth labels $Y = {Y_1, Y_2, Y_3, ..., Y_n}$, and the appointed target output label $Y_{target}$, a backdoor model satisfies:\n$F_\\Theta(x_i) = \\hat{y_i} \\approx Y_i,$\n$F_\\Theta(x_i + d_{data}) = Y_{target},$\nwhere $\\Theta$ is the parameters of the poisoned model F, i is the index of the input data and satisfies $i \\in [0,n]$, $\\hat{y_i}$ is the output of the poisoned model $F_\\Theta$.\nIn this study, we propose the behavior backdoor paradigm, which is based on the basic ideology of the data-triggered backdoor attack but redirect the triggers from the data pattern into model post-processing operations, i.e., quantification. Specifically, we formulate the behavior backdoor as:\n$F_\\Theta(x_i) = \\hat{Y_i} \\approx Y_i,$\n$F_{\\Theta^*}(X_i) = Y_{target},$\n$F_{\\Theta^*} = O(F_\\Theta),$\nwhere O(.) is a specific post-processing operation behavior, i.e., the behavior trigger, and in this paper, we define the O(.) as a quantification operation Q(\u00b7) that accepts a common model $F_\\Theta$ as input and outputs a quantified lightweight model $F_{\\Theta^*}$ with $\\Theta^*$ as model parameters instead. So the third equation in Equation (2) can be also re-written as $F_{\\Theta^*} = Q(F_\\Theta)$."}, {"title": "3.2. Framework Overview", "content": "In order to train a poisoned model that can be specifically targeted to trigger the backdoor by behaviors, we take the model quantification as an instance and propose the quantification backdoor (QB) attack framework. The framework can be found in Figure 2.\nAs for the behavior-driven backdoor object optimizing, a key difference between the proposed quantification behavior backdoor and the traditional data-triggered backdoor is the distinctive training objects, i.e., the data-triggered backdoor only needs to adjust the objective function in the data end, while for our QB attack, introducing the behavior factors into training is much more critical. To this end, we propose a bi-target optimization procedure with a designed quantification backdoor loss for guiding the poisoned model optimization direction to be sensitive to the quantified model behaviors. By forcing the poisoned model and the after-quantified poisoned model to act variously, we could finally acquire a quantification operation sensitive model, i.e., the quantification backdoor model.\nRegarding the address-shared backdoor model training, it is proposed to tackle the training gradient information update issue in the backdoor model training progress. Since during our quantification backdoor poisoning, there exists two architecture-same but parameter-different models, i.e., the model to be poisoned and the quantification triggered model, it is nontrivial to conduct the classical gradient information backpropagation of training process. More precisely, constrained by the mismatch model checkpoints, the optimization direction can not be effectively guided during training. Thus, considering the principles of computer storage, we align the physical storage address of the correlated models to direct optimization of poisoned model training, finishing the backdoor injection."}, {"title": "3.3. Behavior-driven Backdoor Object Optimizing", "content": "The core goal of the behavior backdoor is to train a poisoned model that could be sensitive to specific behaviors, i.e., post-processing operations. In existing works, the backdoor implanting is always achieved by training losses. Upon this, it is reasonable for us to try to elaborate a tailoring loss term for certain model operations, i.e., the quantification. However, different from the data-triggered backdoor attack, we are motivated to introduce the quantification operations into the backdoor model training progress by proposing a behavior-in-the-loop design. To be specific, the constructed quantification backdoor loss is designed to achieve a bi-target optimization goal, to wit, maintaining good behaviors before the triggers are activated and behaving bad performance after being triggered by the quantification operation.\nSpecifically, we develop a training framework that leverages a composite loss function designed to achieve our bi-target optimization goal. The first component, denoted as $L_{ben}$, aims to ensure that inputs processed by the backdoor model yield outputs consistent with the original predictions following [51].\nThe second component, referred to as $L_{qba}$, is introduced to guide the quantified model's inputs towards achieving the designated target outputs. This quantization loss is formulated as follow:\n$L_{qba} = \\frac{1}{NC}\\sum_{i=1}^N\\sum_{j=1}^C y_{target_j} \\cdot log(softmax(F_{\\Theta^*}(x_i))_j),$\nwhere $F_{\\Theta^*}$ denotes the quantified backdoor model, $x_i$ denotes the input data from the input samples X, C represents the number of classes, and $Y_{target}$ is the expected output for the quantified model. The content inside the log function represents the output of the quantified backdoor model, after applying the softmax function, which normalizes the output into a probability distribution. The inclusion of this term drives the model, whose parameters have been quantified, to tend towards predicting the input as the target label. Then, the overall loss after the definition of the two components for our QB attack using the following formula:\n$L_{overall} = L_{ben} + \\lambda \\cdot L_{qba},$\nwhere $\\lambda$ is a hyperparameter that controls the balance between the benign training target $L_{ben}$ and the quantification backdoor target $L_{qba}$. By default, we set $\\lambda$ = 1 to ensure that the two objective functions contribute equally to the model's optimization process."}, {"title": "3.4. Address-shared Backdoor Model Training", "content": "Given the proposed behavior-driven backdoor object optimizing schedule, we do still not achieve our goal due to gradient information updating challenge during poisoned model training, i.e., the gradient backpropagation and the weight update. Generally, the backpropagation is highly correlated to a certain model to be trained. However, in our QB attack pipeline, there exists 2 different models, i.e., the model to be poisoned and the quantification triggered model, whose model parameters are not matched well due to the fact that quantification behavior changes the model parameters commonly. Thus, we exploit the idea of storage address and allow the poisoned model and quantified model to share same logical storage address for the corresponding parameters so that the constraints on gradient information update could be lifted, and the behavior backdoor could be implanted successfully into the poisoned model.\nSpecifically, leveraging the full-precision storage of parameters in the quantified model, and the fact that the quantizer is used only during the forward propagation process to quantize specific parameters and activations, we align the address of the learnable parameters in the backdoor model with the address space of the corresponding parameters in the quantified backdoor model.\nTo achieve this goal, we first acquire the parameter list P of models by the following equation:\n$P = S(F_\\Theta) = {(n_i, p_i)|i = 1,2,..., N_\\Theta},$ (5)\nwhere S is a function that extracts the name n and value p of each learnable parameter in the model $F_\\Theta$, and $N_\\Theta$ is the total number of $\\Theta$.\nSubsequently, we employ the function S to derive two parameter lists $P_\\Theta$ and $P_{\\Theta^*}$ for the backdoor model $F_\\Theta$ and the quantified backdoor model $F_{\\Theta^*}$, respectively. Based on the acquired parameter lists, we then try to align the correlated parameters that should be updated according to the gradient information during optimization by designing an operator A, which receives the parameter and its name as input. We formulate this progress as:\n$A(P_\\Theta, P_{\\Theta^*}) \\leftarrow address(n^\\Theta_i, p^\\Theta_i) = address(n^{\\Theta^*}_i, p^{\\Theta^*}_i),$\nwhere $n^\\Theta_i, p^\\Theta_i \\in P_\\Theta, n^{\\Theta^*}_i, p^{\\Theta^*}_i \\in P_{\\Theta^*}, n^\\Theta_i == n^{\\Theta^*}_i,$\nwhere A denotes the alignment function, which ensures that the parameters in $P_\\Theta$ and $P_{\\Theta^*}$ with the same names share the same physical addresses. The term $address(n,p)$ refers to the physical memory location where the parameter p is stored. By converting the parameter p to its physical memory address using address(n,p), we can uniquely identify each parameter by its name and location in memory.\nAfter the parameters' physical addresses are shared, we proceed with parameter optimization using a single optimizer that updates the parameters of the quantified backdoor model. This is because the quantified backdoor model may have additional learnable quantization parameters compared to the backdoor model, which can be described as $|P_\\Theta| \\leq |P_{\\Theta^*}|$. Owing to the shared physical address, updating the quantified model's parameters simultaneously updates those of the backdoor model. For any input $X_i$, it is passed through both the backdoor model and the quantified backdoor model, where the respective losses are computed and backpropagated. Benefited from the shared parameter addresses, the gradients from both models' losses are effectively accumulated onto the same set of parameters during backpropagation.\nTo sum up, our address-shared backdoor model training method effectively resolves the parameter inconsistency issue in the QB attack training process, ensuring that both the backdoor model and the quantified model are fully optimized during training."}, {"title": "3.5. Overall Training Process", "content": "To sum up, we poison a model $F_\\Theta$ and implant the quantification behavior backdoor Q into it based on the proposed behavior-driven backdoor object optimizing and the address-shared backdoor model training.\nSpecifically, for a certain dataset X, we first select a quantification method and take it as the behavior backdoor trigger Q. Then we calculate the loss function $L_{qba}$ value for each input $x_i$, and we optimize the $F_\\Theta$ following the such objective function:\n$arg \\underset{\\Theta^*}{min} L_{overall},$\nwhere $\\lambda$ controls the balance between the benign training target $L_{ben}$ and the quantification backdoor target $L_{qba}$. In this paper, we set the lambda as 1.0 in default. The detailed algorithm description of QB attack can be found in the supplementary files."}, {"title": "4. Experiments", "content": "In this section, we conduct comprehensive experiments to demonstrate effectiveness of the proposed quantification backdoor (QB) attack, providing the evidence of the feasibility of the behavior backdoor attack paradigm. Specifically, we first introduce the experimental settings, which include the verifying tasks, datasets, metrics, models, and detailed experimental settings."}, {"title": "4.1. Experimental Settings", "content": ""}, {"title": "4.1.1 Tasks and Datasets", "content": "For supporting our conclusion about the feasibility of the proposed behavior backdoor attack, we choose multiple tasks as verification bench, following the principle of cross verification. Specifically, we select the image classification [4], the object detection [71], and the deepfake detection task [66], respectively. As for the reasons, the image classification and object detection are the typical and widely-studies fundamental computer vision tasks, the deepfake detection is a classical safety-related scenario that worth investigation. Correspondingly, for each task, we select a representative dataset for performing experiments. More precisely, we take the image classification as the mainly-verifying task, thus 3 different but popular datasets are employed in the corresponding experiments, such as MNIST [7], CIFAR-10 [24], and TinyImageNet [6]. For object detection and deepfake detection tasks, we employ the PASCAL VOC 2007 [9] and Celeb-DF [32], respectively. We believe that the effectiveness of the proposed behavior backdoor could be well validated through this full consideration."}, {"title": "4.1.2 Evaluation Metrics", "content": "Regarding the evaluation metrics for the proposed behavior backdoor attack, we define the attacking success rate (ASR) as basic assessment, which could be formulated as\n$ASR = \\frac{\\sum_{i\\in{i|y_i#target}} C(x_i)}{N},$ where\n$C(x_i) = \\begin{cases}\n1, & \\text{if } F_\\Theta(x_i) \\neq Y_{target} \\cap F_{\\Theta^*}(X_i) = Y_{target}, \\\\\n0, & \\text{others},\n\\end{cases}$\nwhere C() is a counter that counts the sample size meeting the criteria, N is the total size of the test set. This formula depicts that the ratio of successfully attacked samples of behavior backdoor to the total sample size. The higher the ASR, the better the attacking ability. In addition to the ASR that directly reflect attack capabilities, we also report the common metrics that correlated to the tasks themselves, such as accuracy (ACC) that widely used in classification tasks, mean Average Precision at IoU 0.5 (mAP@50) and F1 Score for detection tasks."}, {"title": "4.1.3 Poisoned Models", "content": "Verifying the effectiveness of our QB attack on different model architectures is of important significance in demonstrating the universality behavior backdoor. Therefore, for image classification and deepfake detection, we mainly consider 4 DLMs, i.e., AlexNet [25], VGG [50], ResNet [16], and ViT [8], that contains 2 typical architectures, i.e., convolutional neural networks (CNNs) and vision transformer models (ViTs). For object detection, we employ the Faster-RCNN [47] and RetinaNet [36] for evaluation."}, {"title": "4.1.4 Detailed Experimental Settings", "content": "For training the poisoned backdoor model, we consistently employ Adaptive Moment Estimation optimizer (Adam) [22] with a learning rate 1 \u00d7 10\u22124 to minimize the loss function for updating the model parameters. Additionally, to ensure stable convergence of the model during training, we utilize the same learning rate decay approach as in CNNDetection [61]. Specifically, the learning rate is reduced by a factor of 10 when the backdoor model's accuracy for the original labels fails to improve over several consecutive evaluations. The patience threshold for this decay is set to 7 by default. The mini-batch size m is adjusted according to the specific dataset to facilitate effective training. All experiments are conducted on a cluster equipped with NVIDIA GeForce RTX 3080Ti GPUs. The experiments are implemented in PyTorch version 2.3.0."}, {"title": "4.2. Attacking Performance", "content": "In this section, we mainly report the attacking performance of the proposed QB attack on the image classification task. Specifically, we conduct experiments on 3 different classical image classification related datasets. 4 selected models (AlexNet, VGG, ResNet, and ViT) are employed in these experiments. For each experiment, we report the ACC and ASR as mentioned in the Section 4.1.2. Besides, we additionally provide the attack class accuracy (denoted as \"ACCt\"), which represents the accuracy of predicting an input sample as the target attacking class, i.e., Ytarget. Formally, the $ACC_{target} = \\frac{\\sum_{i}^{e} F_{\\Theta^*}(x_i)=Y_{target}}{N}$. The experimental results can be found in Table 1, where we can draw a meaningful conclusion that the proposed QB attack, as a representative type of behavior backdoor, is feasible and effective and should be regarded as a fresh threat of reliable deep learning. We provide some insights as following:\n\u2022 The proposed idea of behavior backdoor, especially the quantification backdoor, is feasible for current DLMs. Taking the attacking results against ResNet on MNIST as an example, the ACC values of the vanilla model and the 3 poisoned model are 99.25%, 99.27%, 98.23%, and 98.59%, which are at a similar level. That means, before the poisoned models are triggered, they could successfully make accurate predictions, i.e., behaving good results. However, after triggering the poisoned backdoor models via the quantification method, we could witness significant differences. For instance, the ACC/ASR values under attacktarget\u22120, attacktarget-4, and attacktarget-9 settings on VGG are respectively 100%/99.21%, 100%/99.14%, and 100%/99.53%, strongly supporting the effectiveness of our QB attack.\n\u2022 The attacking ability of our QB attack shows differences on various datasets. Though the attacking performance is demonstrated consistently, we could find that the magnitudes of the attacking ability on MNIST, CIFAR-10, and TinyImageNet are quite different. For example, under the attacktarget-0 setting, the ASR value on AlexNet of MNIST achieves 99.21%, while that of CIFAR-10 is 89.44%, and that of TinyImageNet is 56.38%. This clear decay tendency not only claims that the attacking ability of our behavior backdoor might be correlated to the dataset complexity, but also exhibits the large room for future improvement, calling for more efforts.\n\u2022 The QB attack shows certain performance variations that are related to the model architectures. We could observe that the QB attack show distinguished ASR values on different models. Taking the ASR values on TinyImageNet as instances, the QB attack achieves 56.38%, 68.67%, 75.48%, and 77.64% on AlexNet, VGG, ResNet, and ViT, respectively. Upon this fact, we could find an interesting phenomenon that the better models appear lower defense against behavior backdoor, i.e., the ViT/AlexNet achieves 77.71%/58.16% ACC value on TinyImageNet under attacktarget-0 setting, while the corresponding ASR values are 77.64%/56.38%. We conjecture that the stronger models might have more vulnerability against behavior backdoor due to their complex structures."}, {"title": "4.3. Ablation Study", "content": "In our behavior backdoor model training process, there exists a hyperparameter, i.e., \u03bb. It is worth investigating the effectiveness of the QB attack when facing different A to construct further understanding of this new-type QB attack paradigm. Thus, we conduct ablations by studying the attacking performance under multiple A values.\nSpecifically, we set the A as 0.1, 0.3, 0.5, 0.7, 0.9, 1.5 and 3.0 respectively, to control the weight of the behavior backdoor related loss term $L_{qba}$. The higher A value indicates the larger weight. We adopt the same settings of models and datasets with those of the attacking performance evaluation experiments. The experimental results can be witnessed in Figure 3. It can be concluded that the value of A only make limited effects (i.e., only ResNet on MNIST, it appears more than 2% ASR difference) on the attacking performance of QB attack, demonstrating the stability of the proposed method."}, {"title": "4.4. Analysis and Discussion", "content": "Beyond the aforementioned experiments, we are also interested in further answering 3 more research questions: \u2460 how the proposed QB attack make effects? \u2461 Could QB attack different tasks? \u2462 If we adopt different quantification methods that are not employed during backdoor training, will they trigger the poisoned model? We then conduct additional experiments and give more analysis."}, {"title": "4.4.1 Effectiveness of the Behavior Backdoor", "content": "To investigate the effect mechanism of the proposed behavior backdoor attack, i.e., QB attack, we refer previous studies [59] and conduct quantitative and qualitative analysis by introducing the t-SNE [56] and model attention [48] tools, in which the former visualizes the feature extraction results of models and the latter analyzes the saliency regions that model focuses.\nSpecifically, for t-SNE analysis, we plot the feature visualization of all benign models and poisoned models on MNIST, and CIFAR-10 datasets. The results can be found in Figure 4, left, where we find that the QB attack indeed change the learned feature distribution of the poisoned models. Taking the results on CIFAR-10 as instances, the feature distribution distance of target class and other class, i.e., green points, becomes closer. We thus attribute that the poisoned model make effects via adjusting the models' learning progress. For model attention analysis, we employ all 4 models, randomly sample 10 instances from TinyImageNet, and then we conduct the saliency map generation according to the middle-layer feature maps following previous studies [58, 64]. The results can be found in Figure 4, right, where we can conclude that the triggered backdoor models will pay no attention to the regions of key objects. More experimental results can be found in supplementary files."}, {"title": "4.4.2 Attack Performance on Different Tasks", "content": "Since our main experiments are conducted on the image classification task, it is also important to verify the attacking performance on different tasks. For this purpose, we conduct extra experiments on object detection task, and deepfake detection task. Due to the space limitation, we provide the results on object detection task in supplementary files.\nIn detail, we adopt the CeleB-DF [32] as benchmark and employ ResNet and ViT for evaluation. According to Table 2, it could be observed that before the backdoors in poisoned model are triggered, the ACC values of ResNet/ViT on deepfake detection task are 98.42%/95.34%, and after attack but not triggered, there ACC values are kept at similar level, while after the backdoors are triggered, the ASR values achieve high level, i.e., for ResNet, the ASR values under attacktarget-real/attacktarget-fake are 98.47%/97.35%, demonstrating the attacking performance. Thus, we could confirm that the QB attack, as a typical example of behavior backdoor attack, enjoys the feasibility and good attacking ability on various tasks."}, {"title": "4.4.3 Triggering by Multiple Quantification Methods", "content": "Since we first propose to exploit the post-training behavior, i.e., quantification, as the backdoor triggers, it is reasonable for us to ask if the quantification methods that are different from the behavior trigger Q during training could trigger the backdoors in the poisoned models. Thus, we try to trigger the backdoor by different quantification methods.\nSpecifically, we simply train a poisoned backdoor model with a set behavior trigger Q1 and then we utilize other quantification methods, namely DoreFa-Net [70] (denoted as \"Q2\") and Low-bit Quantization [28] (denoted as \"Q3\"), to test if the backdoors inside the trained model will be activated. The corresponding experiments are conducted on CIFAR-10. The results can be found in Table 3, where we can draw some meaningful conclusions: \u2460 under white-box settings, i.e., the behavior trigger Q in testing is same with that in training, the behavior backdoor performs well as we mentioned before. More precisely, the ASR values of the diagonal columns in the table are 85.24%, 86.99%, 86.10%, from top left to bottom right. \u2461 Under black-box settings, i.e., Q in testing is different from that in training, the ASR values appear clear drop, i.e., when using Q1/Q2 to trigger the Q3 trained backdoor models, the ASR only achieves 0.09%/0.10%. This fact indicates the weak adaptation of our behavior backdoor to different quantification triggers. However, it could be also observed that in some cases, the cross-quantification triggers achieve a relatively high AS values. For instance, Q3 could trigger the Q1/Q2 trained backdoor models and achieves 83.60%/45.30% ASR, which reveals that the transferable behavior backdoor could be a future research direction."}, {"title": "5. Conclusion", "content": "In this paper, we take the first step towards behavior backdoor attack paradigm and propose the first practical pipeline of exploiting quantification operation as backdoor trigger, i.e., the quantification backdoor (QB) attack, by elaborating the behavior-driven backdoor object optimizing and address-shared backdoor model training method. Extensive experiments on multiple dataset with different models are conducted for various tasks, strong demonstrating the feasibility of the proposed concept of behavior backdoor attack and the effectiveness of the QB attack.\nEthical statements. Since a new-type attacking method against DLMs is proposed, it is necessary to concern the possible influence to the deep learning communities. Thus, we state the potential impacts as: \u2460 behavior backdoor for social bad. As an attacking approach, the QB attack might be exploited by hackers to manuscript poisoned models and then to drop them into public fields for their ulterior goals, like destroying the edge device deployment or inducing specific results. \u2461 Behavior backdoor for social good. Though harmful, it could be also utilized in bona fide intentions, such as protect the self-developed models from unauthorized use, to achieve the goal of neither affecting open source nor losing profits. \u2462 Responsible disclosure. Considering the potential ethical risks, we only support this study to be exploited for research goals. Therefore, we will provide the codes, datasets, and checkpoints, for the scholars after communicating and reviewing."}]}