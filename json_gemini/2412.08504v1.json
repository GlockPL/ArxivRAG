{"title": "PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis", "authors": ["Yifan Xie", "Tao Feng", "Xin Zhang", "Xiangyang Luo", "Zixuan Guo", "Weijiang Yu", "Heng Chang", "Fei Ma", "Fei Richard Yu"], "abstract": "Talking head synthesis with arbitrary speech audio is a crucial challenge in the field of digital humans. Recently, methods based on radiance fields have received increasing attention due to their ability to synthesize high-fidelity and identity-consistent talking heads from just a few minutes of training video. However, due to the limited scale of the training data, these methods often exhibit poor performance in audio-lip synchronization and visual quality. In this paper, we propose a novel 3D Gaussian-based method called PointTalk, which constructs a static 3D Gaussian field of the head and deforms it in sync with the audio. It also incorporates an audio-driven dynamic lip point cloud as a critical component of the conditional information, thereby facilitating the effective synthesis of talking heads. Specifically, the initial step involves generating the corresponding lip point cloud from the audio signal and capturing its topological structure. The design of the dynamic difference encoder aims to capture the subtle nuances inherent in dynamic lip movements more effectively. Furthermore, we integrate the audio-point enhancement module, which not only ensures the synchronization of the audio signal with the corresponding lip point cloud within the feature space, but also facilitates a deeper understanding of the interrelations among cross-modal conditional features. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking head synthesis compared to previous methods.", "sections": [{"title": "Introduction", "content": "With the development of the audio-visual industry, audio-driven talking head synthesis has become crucial in computer vision and multimedia. The goal is to create videos where a person's face moves naturally in sync with input audio while maintaining their visual identity. This task has matured into a prominent research topic recently and holds potential for integration into diverse applications, including virtual avatars (Thies et al. 2020), film making (Zhang et al."}, {"title": "Related Work", "content": "Talking head synthesis aims to create a video of a speaking person that accurately represents their identity and is per- fectly synchronized with the driven audio. It consists of two main directions: 2D-based and 3D-based methods."}, {"title": "2D-Based Talking Head Synthesis", "content": "Some methods (Pra- jwal et al. 2020; Wang et al. 2023b; Zhong et al. 2023; Zhang et al. 2023b) rely on images that focus primarily on the face, particularly the mouth, to ensure the audio matches the lip movements. For example, Wav2Lip (Prajwal et al. 2020) in- corporates a lip synchronization expert to oversee the accu- racy of lip movements, while TalkLip (Wang et al. 2023b)"}, {"title": "3D-Based Talking Head Synthesis", "content": "Conventional 3D- based methods (Suwajanakorn, Seitz, and Kemelmacher- Shlizerman 2017; Ji et al. 2021) often utilize 3D Morphable Models (3DMM) (Blanz and Vetter 2023) for talking head synthesis. However, the use of intermediate representations can lead to the accumulation of errors. With the recent rise of Neural Radiance Fields (NeRF) (Mildenhall et al. 2021), it has been applied to tackle 3D head structure problems in audio-driven talking head synthesis (Guo et al. 2021; Tang et al. 2022; Li et al. 2023; Ye et al. 2023; Shen et al. 2023a; Peng et al. 2024). AD-NeRF (Guo et al. 2021) stands as the pioneering method in utilizing NeRF for audio-driven talk- ing head synthesis. By incorporating Instant-NGP (M\u00fcller et al. 2022), RAD-NeRF (Tang et al. 2022) has achieved sig- nificant enhancements in both visual quality and efficiency. ER-NeRF (Li et al. 2023) introduces a triple-plane hash en- coder designed to eliminate empty spatial regions and gener- ates region-aware conditional features through an attention mechanism. GeneFace (Ye et al. 2022) and its variant (Ye et al. 2023) generate content conditioned on estimated fa- cial landmarks. SyncTalk (Peng et al. 2024) advances the realism of audio-driven talking head videos by accurately synchronizing facial identity, lip movements, expressions, and head poses. Despite its excellent performance, the in- ference process is not real-time. TalkingGaussian (Li et al. 2024) first attempts to utilize the 3DGS (Kerbl et al. 2023) to address the facial distortion problem in existing radiance- fields-based methods. This paper introduces a 3D Gaussian- based method that significantly enhances visual quality and audio-lip synchronization. Moreover, it ensures that the in- ference process operates in real time."}, {"title": "Point Cloud Learning", "content": "Point cloud learning (Qi et al. 2017a; Wang et al. 2019; Xie et al. 2023, 2024) is a crucial research area in 3D computer vision. PointNet (Qi et al. 2017a) utilizes point- wise MLPs and pooling layers to aggregate features for un- derstanding 3D scenes. PointNet++ (Qi et al. 2017b) ad- vances PointNet (Qi et al. 2017a) by incorporating hierarchi- cal sampling strategies. Point-BERT (Yu et al. 2022) adapts the masked language modeling approach of BERT (Kenton and Toutanova 2019) to the 3D realm. RECON (Qi et al. 2023) combines the strengths of generative and contrastive"}, {"title": "Method", "content": "In this section, we introduce the proposed PointTalk, as il- lustrated in Figure 1. PointTalk is structured around three key components: 1) The Multi-Attribute Branches are uti- lized to process 3D Gaussians, audio signals and dynamic lip point clouds separately. 2) The Audio-Point Enhancement module, which not only synchronizes the audio signals with the point cloud but also enhances information interaction. 3) The Adaptive 3DGS Rendering can render the final talk- ing head video effectively. We will delve into the specifics of these components and the associated loss functions in the subsequent subsections."}, {"title": "Multi-Attribute Branches", "content": "3D Gaussian Branch. The static Gaussian field preserve the Gaussian primitives with the canonical parameters. Specifically, a Gaussian primitive can be described with a posi- tion $x \\in R^3$, a rotation quaternion $r \\in R^4$, a scaling factor $s \\in R^3$, an opacity value $a \\in R^1$, and a d-dimensional color feature $f \\in R^d$. Consequently, the i-th Gaussian primitive $G_i$ keeps a set of parameters $\\theta_i = \\{x_i, r_i, s_i, a_i, f_i\\}$ and the formulation can be described as:\n$G_i(p) = e^{-(p-x_i)^T\\Sigma^{-1}(p-x_i)},$ (1)\nwhere the covariance matrix $\\Sigma$ can be obtained using r and s. We first initialize the talking head with the static Gaussian field by the training video frames to get a coarse Gaussian head. Then, we learn the deformation parameters to deform the Gaussian head with the audio.\nAlthough Gaussian primitives are effective in represent- ing the Gaussian head, they lack a regional position encod- ing due to their explicit structure. To address this, we pro- pose utilizing a tri-plane representation (Chan et al. 2022). Specifically, for a given position $x = (x,y,z) \\in R^{XYZ}$, it is transformed through an encoding process where its projected values are utilized by three distinct 2D multi- resolution hash encoders (M\u00fcller et al. 2022):\n$\\mathcal{H}_{AB}: (a, b) \\rightarrow f_{ab}^{AB},$ (2)\nwhere the output $f^{AB}_{ab} \\in R^{LD}$, defined by L levels and D feature dimensions per entry, represents the planar geomet- ric feature associated with the projected coordinate (a, b). In this scenario, $\\mathcal{H}_{AB}$ refers to the multi-resolution hash en- coder for plane $R_{AB}$. By concatenating these outputs, we ob- tain the final spatial geometry feature $f_c \\in R^{3LD}$, which in- tegrates the encoded geometric information. The whole pro- cess can be described as:\n$f_c = \\mathcal{H}_{XY}(x, y) \\oplus \\mathcal{H}_{YZ}(y, z) \\oplus \\mathcal{H}_{XZ}(x, z),$ (3)\nwhere $\\oplus$ denotes the concatenation operator."}, {"title": "Audio Branch", "content": "In audio branch, we use an automatic speech recognition (ASR) module (Amodei et al. 2016; Hsu et al. 2021) to extract audio features $F_a \\in R^{T \\times F}$ from the audio"}, {"title": "Lip Point Cloud Branch", "content": "Inspired by mesh-based meth- ods (Fan et al. 2022; Peng et al. 2023), we design an Au- dio2Point module to capture an additional dynamic lip point cloud. The pipeline of the Audio2Point module is depicted in Figure 2. Specifically, the mesh is used as the reference embedding, while the speech audio is compressed by a tem- poral convolutional network (TCN) and an audio encoder. Then, the predicted mesh is generated by combining the audio features with the embedding. After that, the vertices of the mesh are extracted to generate the head point cloud, which is further refined to produce the final lip point cloud $P \\in R^{T \\times N \\times 3}$, where T represents the frame count and N signifies the quantity of points. For additional details, please refer to the supplementary materials.\nFor each frame of the point cloud, it is essential to cap- ture its topological structure to achieve a robust representa- tion. To this end, we employ $E_{point}^3$, a multi-resolution hash grid (M\u00fcller et al. 2022) that encodes the point cloud ef- ficiently. We opt for a hash grid over a tri-plane representa- tion when processing the lip point cloud for a key reason: the Gaussian primitives usually reach up to tens of thousands, in contrast to the lip point cloud points, which are typically in the mere hundreds. This disparity significantly reduces the potential for hash collisions. The advantages of this way will be illustrated in subsequent ablation studies.\nGiven the dynamic nature of the synthesis process, we be- lieve that merely capturing the features of each point cloud frame falls short in effectively guiding the generation of the talking head. Consequently, we design a dynamic dif- ference encoder to better capture the nuances of lip move- ment. Specifically, the differences are taken for the features of neighboring frames, and then all the differences are con- catenated together to obtain the final point cloud features $F_p \\in R^{(T-1) \\times F}$:\n$F_p = MLP(cat [E_{point}^3 (P_{t+1}) - E_{point}^3 (P_{t})]_{t=1}^{T-1}),$ (4)\nwhere $cat[.]$ denotes the feature concatenation and t repre- sents the frame order."}, {"title": "Audio-Point Enhancement", "content": "We introduce the audio-point enhancement module, de- signed with a dual purpose: to synchronize the audio sig- nals with the corresponding point cloud, and to understand the correlation between cross-modal features. The detailed structure of this module is depicted in Figure 3.\nDrawing inspiration from (Chung and Zisserman 2017; Prajwal et al. 2020), we recognize that synchronizing the audio signals with the video signals can facilitate the gener- ation of the talking head to a notable extent. In our method, we assert that synchronizing the audio signals with the cor- responding point cloud will likewise contribute to enhanced performance. Specifically, we introduce a cross-modal con- trastive learning strategy to establish the audio-point corre- spondences.\nGiven the audio features $F_a \\in R^{T \\times F}$ and point cloud frame features $F'_p \\in R^{T \\times F}$, the process of individually ex- tracting lip point cloud features for each frame is aimed at establishing a correspondence with the audio features. For the t-th frame, our objective is to enhance the similarity be- tween $F_{at}$ and $F'_{pt} = E_{point}^3(P_{t})$, as they both correspond to the same object. Simultaneously, we strive to reduce the similarity between $F_{at}$ and the point cloud features of non- corresponding frames. Therefore, we can construct the loss function $l(t, F_a, F'_p)$:\n$l(t, F_a, F'_p) = -log \\frac{exp(sim(F_{at}, F'_{pt})/\\tau)}{\\sum_{k=1 \\, k \\neq t}^T exp(sim(F_{at}, F'_{pk})/\\tau)},$ (5)\nwhere T is the temperature factor and $sim(.,.)$ denotes the cosine similarity function. And the cross-modal contrastive learning loss $\\mathcal{L}_{CL}$ is then formulated as:\n$\\mathcal{L}_{CL} = \\sum_{t=1}^T [l(t, F_a, F'_p) + l(t, F'_p, F_a)].$ (6)\nDynamic conditions, including audio signals and lip point clouds, play a selective role in talking head synthesis. Many previous methods (Guo et al. 2021; Tang et al. 2022) have simply assumed that these dynamic conditions uniformly in- fluence the entire synthesis process. To comprehend the cor- relation between cross-modal features, lightweight external attention (Guo et al. 2022; Li et al. 2023) is employed for information interactions. Specifically, self-attention is first utilized to compress audio and point cloud features follow- ing (Tang et al. 2022). A two-layer MLP is then utilized to capture the global context of spatial geometry. Following this, we apply the Hadamard product to multiply the global context with the compressed dynamic conditions, yielding the final enhanced features. This process can be represented as follows:\n$F'_a = MLP_a(F_c) \\odot SA(F_a),$ (7)\nwhere $\\odot$ denotes Hadamard product. A similar process is performed for the point cloud.\nAdditionally, given that the whole process is audio-driven and the lip point cloud is generated from the audio signals, we aim for the point cloud features to emphasize the audio- related parts more prominently. To achieve this, we process the enhanced audio features through another two-layer MLP to capture the global content of the audio. Subsequently, we utilize the Hadamard product to derive the final enhanced point cloud features."}, {"title": "Adaptive 3DGS Rendering", "content": "Most previous methods (Guo et al. 2021; Tang et al. 2022; Li et al. 2023) typically integrate conditional features into spa- tial geometry features through concatenation. However, we argue that concatenation alone is insufficient for the effec- tive fusion of cross-modal features. Inspired by Adaptive In- stance Normalization (AdaIN) (Huang and Belongie 2017), we adopt a combination of a residual block and AdaIN to guide the fusion process. This process subjects the outputs to an affine transformation, incorporating both translation and scaling, parameterized by $scale_i$ and $shift_i$, respectively. These parameters are then employed to generate adaptive features $F'_c$:\n$F'_c = (1 + scale_i) * F_c + shift_i.$ (8)\nUltimately, we can use the adaptive features to derive the point-wise deformation parameters ($\\Delta x$, $\\Delta r$, $\\Delta s$) for the 3DGS Rasterizer, which are irrelevant to the color and opac- ity changes. Therefore, we can calculate the final parameter $\\theta_p = \\{x + \\Delta x, r + \\Delta r, s + \\Delta s, a, f\\}$ for rendering. The 3DGS rasterizer gather N modified Gaussians with the cam- era information to compute the color C of pixel p:\n$C(p) = \\sum_{i \\in N} C_i a_i \\prod_{j=1}^{i-1} (1-\\hat{a}_j),$ (9)\nwhere $c_i$ represents the decoded color from f, $\\hat{a}_i$ is the result of calculating opacity $a_i$ with the projected function. For a more comprehensive understanding of the 3DGS Rasterizer, please refer to the supplementary materials."}, {"title": "Loss Function", "content": "To initialize the coarse Gaussian head, we follow the original 3DGS (Kerbl et al. 2023) and utilize a combination of pixel- wise loss $L_1$ and D-SSIM term $L_{D-SSIM}$ (weighted by $\\lambda_1$). After the initialization, we further predict the deformation parameters and input them for the 3DGS rasterizer to render the output images. Following the process of previous meth- ods (Tang et al. 2022; Li et al. 2023), we randomly sample a set of patches from the entire image and incorporate LPIPS loss (Zhang et al. 2018) (weighted by $\\lambda_2$) to improve de- tail resolution. This is combined with the cross-modal con- trastive learning loss $\\mathcal{L}_{CL}$ (weighted by $\\lambda_3$) as described in Eq. 6. The overall loss function can be construsted as:\n$\\mathcal{L} = L_1 + \\lambda_1 \\mathcal{L}_{D-SSIM} + \\lambda_2 L_{LPIPS} + \\lambda_3 \\mathcal{L}_{CL},$ (10)"}, {"title": "Experiments", "content": "Dataset. To ensure a fair comparison, the dataset for our experiments is sourced from (Tang et al. 2022; Ye et al. 2022; Li et al. 2023) and includes both English and French languages. We collect high-definition speaking video clips, each with an average length of approximately 7,500 frames at 25 FPS. Each raw video is cropped and resized to a reso- lution of 512\u00d7512, focusing on a centered portrait.\nComparison Baselines. We compare our method against three 2D-based methods, such as Wav2Lip (Prajwal et al. 2020), TalkLip (Wang et al. 2023b), and DINet (Zhang et al. 2023b), as well as five 3D-based methods, including AD- NeRF (Guo et al. 2021), RAD-NeRF (Tang et al. 2022), GeneFace++ (Ye et al. 2023), ER-NeRF (Li et al. 2023), and TalkingGaussian (Li et al. 2024)."}, {"title": "Quantitative Evaluation", "content": "Metrics. We utilize Peak Signal-to-Noise Ratio (PSNR) to assess the overall image quality and Learned Perceptual Im- age Patch Similarity (LPIPS) (Zhang et al. 2018) to evaluate the details. Additionally, we employ Fr\u00e9chet Inception Dis- tance (FID) (Heusel et al. 2017) to gauge image quality at the feature level. For evaluating lip synchronization, we rec- ommend using the Landmark Distance (LMD), which quan- tifies the distance between lip landmarks. Furthermore, we introduce Lip Sync Error Distance (LSE-D) and Lip Sync Error Confidence (LSE-C), consistent with Wav2Lip (Pra- jwal et al. 2020), to assess the synchronization between lip movements and audio.\nComparison Settings. In our quantitative evaluation, we assess our method in two distinct settings: the head recon- struction setting and the lip synchronization setting. For the head reconstruction setting, we divide each video into train- ing and test datasets to evaluate the quality of the talking head reconstruction. For the lip synchronization setting, we extract two out-of-distribution audio clips, named Audio A and Audio B. These audio clips are utilized to drive the same subject for comparison in lip synchronization.\nEvaluation Results. The evaluation results of the head re- construction setting are illustrated in Table 1. It can be ob- served that our image quality is superior to other meth- ods in almost all aspects. In terms of lip synchronization, our results surpass most methods. Specifically, while one- shot methods such as Wav2Lip (Prajwal et al. 2020), Talk- lip (Wang et al. 2023b), and DINet (Zhang et al. 2023b) per- form excellently on the LSE-D and LSE-C metrics and can synthesize talking heads without per-identity training, they score poorly on other metrics. Compared to other 3D-based methods, our method outperforms them in most metrics. Es- pecially in lip synchronization metrics, the improvement of our method is more obvious. This is mainly due to the assis- tance of the dynamic lip point cloud. In terms of lip syn- chronization, the evaluation results are shown in Table 2. Our method also demonstrates an excellent generalization ability to synthesize lip-sync talking heads. Additionally, our method maintains superior performance in both train- ing time and inference FPS, approximating TalkingGaus- sian (Li et al. 2024), which demonstrates the high efficiency of PointTalk."}, {"title": "Qualitative Evaluation", "content": "Evaluation Results. To more intuitively evaluate image quality and lip synchronization, we present a comparison of our method with others in Figure 4. We showcase key frames from a clip and close-up details of two talking heads. The results demonstrate that our PointTalk captures finer details and achieves the highest accuracy in lip synchronization."}, {"title": "User Study", "content": "To conduct a more comprehensive evaluation of PointTalk, we implement a user study questionnaire. We select 36 video clips generated during the quantitative evalu- ation and invite 16 participants to take part in the survey. Par- ticipants are required to rate the generated videos based on three aspects: (1) Image Quality, (2) Video Realness, and (3) Audio-Visual Synchronization. The average scores for each criterion are presented in Figure 5. PointTalk outperforms previous methods in all aspects. Notably, in terms of image quality and video realness, PointTalk exceeds the second- ranked methods by a margin of over 20%."}, {"title": "Ablation Study", "content": "We conduct an ablation study under the head reconstruction setting to assess the contributions of various components in our PointTalk. The results are presented in Table 3.\nLip Point Cloud Encoder. In our method, we utilize a multi-resolution hash grid to capture the topological struc- ture of the lip point cloud. This setting is replaced with a graph neural network (GNN) (Wang et al. 2019) and tri- plane representation (Li et al. 2023), as illustrated in Table 3 (lines 1-2). The findings reveal that solely employing a GNN falls short in efficiently extracting the topological structure. Moreover, the tri-plane representation tends to induce a cer- tain level of information loss. Further investigation into the optimal level (L) and dimension (F) of the multi-resolution hash grid is presented in Table 3 (lines 4-7). Optimal per- formance is achieved at L = 8 and F = 4, which we adopt for all our experiments. Additionally, we develop a dynamic difference encoder (DDE) to better capture the subtle move- ments of the lips. A comparison of the results in Table 3 (lines 8 and 11) clearly shows that the DDE enhances lip- sync metrics. Figure 6 further demonstrates the inaccuracies in lip shape that occur without the use of DDE."}, {"title": "Audio-Point Enhancement", "content": "To evaluate the Audio-Point Enhancement module's effectiveness, experiments in Ta- ble 3 (lines 9-11) show that removing cross-modal con- trastive learning (CCL) causes misalignment between au- dio and lip points, while excluding external attention (Att) weakens cross-modal feature correlation. As shown in Fig- ure 6, these ablations result in inaccurate lip shapes and blurred details."}, {"title": "Conclusion", "content": "In this paper, we propose PointTalk, a novel 3D-Gaussian based method that incorporates an audio-driven dynamic lip point cloud to achieve realistic talking head synthesis. Ini- tially, we introduce an Audio2Point module for generating a dynamic lip point cloud and develop a dynamic differ- ence encoder to precisely encode the nuances of lip move- ment. Furthermore, we integrate an audio-point enhance- ment module. This module not only synchronizes audio sig- nals with their corresponding lip point clouds but also un- derstands the correlation between cross-modal conditional features. Extensive experiments in various settings demon- strate the superior performance of PointTalk."}]}