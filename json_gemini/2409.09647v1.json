{"title": "Self-supervised Learning for Acoustic Few-Shot Classification", "authors": ["Jingyong Liang", "Bernd Meyer", "Issac Ning Lee", "Thanh-Toan Do"], "abstract": "Labelled data are limited and self-supervised learning is one of the most important approaches for reducing labelling requirements. While it has been extensively explored in the image domain, it has so far not received the same amount of attention in the acoustic domain. Yet, reducing labelling is a key requirement for many acoustic applications. Specifically in bioacoustic, there are rarely sufficient labels for fully supervised learning available. This has led to the widespread use of acoustic recognisers that have been pre-trained on unrelated data for bioacoustic tasks. We posit that training on the actual task data and combining self-supervised pre-training with few-shot classification is a superior approach that has the ability to deliver high accuracy even when only a few labels are available. To this end, we introduce and evaluate a new architecture that combines CNN-based preprocessing with feature extraction based on state space models (SSMs). This combination is motivated by the fact that CNN-based networks alone struggle to capture temporal information effectively, which is crucial for classifying acoustic signals. SSMs, specifically S4 and Mamba, on the other hand, have been shown to have an excellent ability to capture long-range dependencies in sequence data. We pre-train this architecture using contrastive learning on the actual task data and subsequent fine-tuning with an extremely small amount of labelled data. We evaluate the performance of this proposed architecture for (n-shot, n-class) classification on standard benchmarks as well as real-world data. Our evaluation shows that it outperforms state-of-the-art architectures on the few-shot classification problem.", "sections": [{"title": "I. INTRODUCTION", "content": "Reducing labelling requirements is central to many application areas since obtaining labelled training data usually requires extensive human labour and is thus costly and error-prone. This is specifically true in the application domain we are targeting, bioacoustics, where the cost of extensive labelling is often prohibitive [1].\nOne of the most promising approaches for reducing labelling requirements is the use of self-supervised learning [2], [3]. Self-supervision can be used in an initial pre-training phase and the networks obtained can subsequently be fine-tuned with a very small amount of labelled data for specific downstream tasks. We are specifically concerned with classification, where the above approach is the basis of n-class, n-shot classification in which just n labelled samples per class are used to train a classifier for n classes.\nSuch n-class, n-shot classification is specifically relevant in bioacoustic. Here, a typical task is the recognition of threatened species [4]. Since sufficient problem-specific labelled train data is rarely available, particularly for cryptic species, the use of recognisers that have been pre-trained on unrelated data is general practice. For example, mammals are commonly classified with recognisers that have been pre-trained on bird data or even generalised audio data, such as AudioSet [5].\nIt stands to reason that pretraining feature embedders on the actual problem data should lead to superior performance, since it specialises the embedder for the specific setting. However, without extensive labelling efforts, this has to be achieved with self-supervised (or unsupervised) methods. Unfortunately, self-supervised learning has not yet been as widely explored in acoustic processing as in image processing [6] and architectures for self-supervised image processing cannot necessarily directly be transferred to the acoustic domain. This is because acoustic data is fundamentally different from image data in that it constitutes sequence data with a temporal dimension. Thus, a self-supervised architecture suitable for audio processing is best based on models for sequence data. In the present paper, we propose such an architecture.\nOur architecture is based on a combination of convolutional neural network (CNN) blocks for feature pre-processing with a Structured State Space Sequence model (S4). The use of S4 is motivated by the fact that S4 architectures, including Mamba, are specifically designed to model long sequences and have been proven to have great potential for modelling long-range dependencies in sequence data. SSMs reach equivalent performance to other sequence models, specifically Transform-ers, while significantly reducing the computational effort [7], [8]. CNNs, on the other hand, are the most commonly used approach for supervised learning of audio data [9] and are effective at feature processing. We thus use a CNN structure based on ResNet [10] for initial feature processing, leaving the time-dimension untouched, and subsequently process the time series of preprocessed features with an S4 architecture. We specifically use S4D [11], [12], an improved version of S4 that reduces the computational requirements.\nWe evaluate the use of our architecture for five-class five-shot classification on standard benchmark data (ESC 50) as well as on real-world data recorded for 10 different frog"}, {"title": "A. Audio feature embedder - AcousticSSM", "content": "We design our convolution block based on the residual block proposed in ResNet [10]. We design this block to extract only features within the frequency domain that are local in the time domain and repeat the process along the time dimension. The first convolution stage consists of a convolutional layer containing $ch1 = 64$ filters of spatial size of $7 \\times 1$, followed by a batch normalisation (BN) layer, a ReLU, and a $3\\times1$ max pooling layer. Each of the next four stages identically repeats a structure of two residual blocks (Fig. 1). Each residual block consists of two convolutional layers. The spatial size of the filter in every convolutional layer is $3\\times1$. The number of filters in each convolutional layer of stage numbers $i = 2,...,5$ is $ch_i = 2^{4+i}$. Channel-wise $f \\times 1$ average pooling is then applied to the frequency dimension, transforming the data dimensions from $(ch5, f5, t)$ to $(ch5, t)$.\nState space models (SSM) stem from classical continuous system dynamics and map a one-dimensional input function $x(t) \\in R$, through intermediate hidden states $h(t) \\in R^N$ to outputs $y(t) \\in R$ [7], [8]. Effectively, an SSM models a linear ODE with learnable parameters A, B, and C as shown below:\n$h'(t) = Ah(t) + Bx(t)$\n(1)\n$y(t) = Ch(t)$.\n(2)\nAfter extracting frequency features with the CNN, we use six sequential SSM layers to extract long-range dependencies between features across the temporal dimension. In each SSM layer, multiple SSMs work independently on the channels in parallel. Every SSM layer maintains its input shape."}, {"title": "B. Contrastive learning method", "content": "Motivated by SimCLR [15] and COLA [16], we learn a robust representation of unlabelled audio signals by training our proposed AcousticSSM with a contrastive loss function. We investigated two different contrastive learning methods in our experiments. The first method (AcousticSSM1) assigns high similarity to audio segments extracted from the same audio recording and low similarity to audio segments extracted from different audio recordings. The loss function maximises the agreement between an anchor segment and a related positive segment from the same audio clip while minimising the agreement between this anchor segment and negative segments from unrelated clips. Instead of keeping a memory bank of negatives after picking one anchor segment, we use a bullet strategy: positive segments are defined as negative segments for all other anchors in one batch.\nFor the second contrastive learning method (Acoustic-SSM2), we apply augmentations to generate related samples. Motivated by CLAR [17], we use augmentation that combines pitch shift, fade in/out, time masking, and time shift. In this case, the anchor example and its corresponding related positive sample come from the same extracted audio segment with different augmentation parameters, whereas the unrelated negative samples are augmented audio segments from different clips."}, {"title": "C. Few-shot downstream classification task", "content": "To address our few-shot classification task, we transfer the pre-trained AcousticSSM model to the downstream classifier. The output of the AcousticSSM encoder is the latent representation $y = f(x)$, as mentioned in the previous section. This is fed into a two-layer dense network to obtain a classifier for the few-shot audio classification task using cross-entropy loss function. The pipeline is shown in Fig. 1. In our experiment, we focus on the five-way five-shot task."}, {"title": "III. EXPERIMENTAL EVALUATION", "content": "We evaluate our method on the well-known Environmental Sound Classification benchmark ESC50 [18] as well as on a real-world bioacoustic dataset. To the best of our knowledge, no existing benchmark for the ESC50 [18] five-way five-shot classification tasks is available. ESC50 consists of 2,000 5-second samples of environmental recordings equally distributed across 50 classes (40 clips per class). Our bioacoustic dataset contains calls of 5 frog species recorded at 4-Mile-Creek, Townsville, Queensland, Australia in 2020 [13] (L rubella, L rothii, L pornatum, C novae, L caerulea).\u00b9 We use 50 unlabelled samples of each species and 5 labelled samples for fine-tuning. To fit our 5-class 5-shot problem, we group ESC 50 into 10 groups G1, ..., G10 with each group contain-ing 5 classes and group divisions according to coarse semantic categories (animal vocalisations, human vocalisations, natural environment sounds, interior built environment, urban exterior sounds, ...).\nAll recordings were resampled at 20kHz with length 30225 (~1.5s) and all audio samples were pre-processed using crop-ping, padding, and augmentation according to [19].\nIn each experiment, one group Gi is chosen. For each of the 5 classes in Gi 5 labelled samples are reserved for fine-tuning and all other samples (but without labels) are used for pre-training. We use ADAM for pre-training and SGD for fine-tuning. For pre-training, we apply a learning rate of 0.0001 for 500 epochs. For fine-tuning, we apply a learning rate of 0.006 for 50 epochs, where the best parameter was optimised using a parameter sweep.\u00b2\nTable I compares our approach to MT-SVLR, a current self-supervised SOTA model [20], trained in the exact same way. We also compare the performance of our approach to the alternative of fine-tuning a network that received self-supervised pre-training on another (unrelated) dataset not directly taken from the targeted application. These models are pre-trained on the large-scale sample set LibriSpeech [21],"}, {"title": "IV. CONCLUSION", "content": "This paper introduced and evaluated a new self-supervised approach for few shot acoustic classification tasks. Our Acous-ticSSM architecture serves as a feature extractor for acoustic data and is based on a combination of a feature pre-processing CNN with a state space model-SSM, which is utilized to help extract deep and robust features of the sequence input signal by learning a long range dependencies in temporal dimension.\nThe experimental evaluation shows that our AcousticSSM learns high performing acoustic feature extractors that enable higher than SOTA accuracy in the downstream classification task. It also shows that our self-supervised method (pre-training feature embedders on the actual problem data and then fine-tuning on a very small amount of labelled data) facilitates superior performance.\nThis approach truthfully reflects the requirements of appli-cations in which unlabelled data from the specific scenario is available in large amounts while labelling is very expensive. This indicates that our method could be transferred to other application cases with similar characteristics.\nGoing forward, AcousticSSM could be applied as a feature extractor in the acoustic domain and even the video domain due to its competitive ability to capture long range sequence dependencies. We expect that such self supervised feature ex-tractors can be useful to improve a broad range of downstream tasks, including event detection."}]}