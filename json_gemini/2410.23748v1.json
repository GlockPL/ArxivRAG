[{"title": "Exploring Consistency in Graph Representations: from Graph Kernels to Graph Neural Networks", "authors": ["Xuyuan Liu", "Yinghao Cai", "Qihui Yang", "Yujun Yan"], "abstract": "Graph Neural Networks (GNNs) have emerged as a dominant approach in graph representation learning, yet they often struggle to capture consistent similarity relationships among graphs. While graph kernel methods such as the Weisfeiler-Lehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment (WLOA) kernels are effective in capturing similarity relationships, they rely heavily on predefined kernels and lack sufficient non-linearity for more complex data patterns. Our work aims to bridge the gap between neural network methods and kernel approaches by enabling GNNs to consistently capture relational structures in their learned representations. Given the analogy between the message-passing process of GNNs and WL algorithms, we thoroughly compare and analyze the properties of WL-subtree and WLOA kernels. We find that the similarities captured by WLOA at different iterations are asymptotically consistent, ensuring that similar graphs remain similar in subsequent iterations, thereby leading to superior performance over the WL-subtree kernel. Inspired by these findings, we conjecture that the consistency in the similarities of graph representations across GNN layers is crucial in capturing relational structures and enhancing graph classification performance. Thus, we propose a loss to enforce the similarity of graph representations to be consistent across different layers. Our empirical analysis verifies our conjecture and shows that our proposed consistency loss can significantly enhance graph classification performance across several GNN backbones on various datasets.", "sections": [{"title": "1 Introduction", "content": "Graph classification tasks are extensively applied across multiple domains, including chemistry [Liu et al., 2022, Xu et al., 2023], bioinformatics [Yan et al., 2019, Li et al., 2023a,b], and social network analysis [Ying et al., 2018, Wang et al., 2024]. Graph neural networks (GNNs) [Kipf and Welling, 2017, Xu et al., 2019, Velickovic et al., 2018, Huang et al., 2024] have emerged as the predominant approach for performing graph classification, owing to their ability to extract rich representations from various types of graph data. A typical GNN employs the message-passing mechanism [Gilmer et al., 2017], where node features are propagated and aggregated across connected nodes. This process effectively captures local tree structures, enabling the differentiation between various graphs. However, GNNs often struggle to preserve relational structures among graphs, resulting in inconsistent relative similarities across the layers. As shown in Figure 1, graphs with higher relative similarity in one layer may exhibit reduced similarity in the subsequent layer. This phenomenon arises from the limitations of cross-entropy loss, which fails to preserve relational structures, as it forces graphs within the same class into identical representations.\nGraph kernel methods, on the other hand, are designed to capture similarities between graphs and utilize these similarities for classification tasks. For instance, subgraph-pattern approaches [Shervashidze et al., 2009, Costa and Grave, 2010, Kriege et al., 2020] compare graphs by counting the"}, {"title": "2 Preliminaries", "content": "In this section, we begin by introducing the notations and definitions used throughout the paper. Next, we provide an introduction to the fundamentals of Weisfeiler-Lehman isomorphism test, GNNs and graph kernels."}, {"title": "2.1 Notations and Definitions", "content": "Let $G(V, E, X)$ be an undirected and unweighted graph with $N$ nodes, where $V$ denotes the node set, $E$ denotes the edge set, and $X$ denotes the feature matrix, where each row represents the features of a corresponding node. The neighborhood of a node $v$ is defined as the set of all nodes that connected to $v$: $N(v) = {u|(v, u) \\in E}$. In a graph dataset, each graph $G_i$ is associated with a label $Y_i$, which is sampled from a label set $L$. In this paper, we focus on the graph classification task, where a model is trained to map each graph to its label."}, {"title": "2.2 Weisfeiler-Lehman Isomorphism Test", "content": "We first introduce the Weisfeiler-Lehman isomorphism test [Weisfeiler and Lehman, 1968], which can be used to distinguish different graphs and is closely related to the message-passing process of GNNs [Xu et al., 2019]. The WL algorithm operates by iteratively relabeling the colors of vertices in the graph. Initially, all vertices are assigned the same color $C_0$. In iteration $i$, the color of a vertex $v$ is updated based on its current color $C_{v,i-1}$ and the colors of its neighbors ${C_{u\\in N(v),i-1}}$. The update is given as follows:\n$C_{v,i} = f_c({C_{v,i-1}, {C_{u\\in N(v),i-1}}})$\nwhere $f_c$ is an injective coloring function that maps the multisets to different colors at iteration $i$.This process continues for a predefined number of iterations or until the coloring stabilizes (i.e., the colors no longer change)."}, {"title": "2.3 Graph Neural Network", "content": "Most GNNs adopt the message-passing framework [Gilmer et al., 2017], which can be viewed as a derivative of the Weisfeiler-Lehman coloring mechanism. Specifically, let $h_v^{(k-1)}$ represent the feature vector of node $v$ at the $(k - 1)$-th iteration. A GNN computes the new feature for $v$ by aggregating the representations of itself and its neighboring nodes $u \\in N(v)$ as follows:\n$h_v^{(k)} = UPDATE^{(k)}(h_v^{(k-1)}, m_v^{(k)}), where m_v^{(k)} = AGGR^{(k)}({h_u^{(k-1)} : u \\in N(v)})$\nThe initial node representations $h_v^{(0)}$ are set to the raw node features $X_v$. At the $k$-th iteration, the aggregation function $AGGR^{(k)}(\\cdot)$ computes the messages $m_v^{(k)}$ received from neighboring nodes. Subsequently, the update function $UPDATE^{(k)}(\\cdot)$ computes a new representation for each node by integrating the neighborhood messages $m_v^{(k)}$ with its previous embedding $h_v^{(k-1)}$. After $T$ iterations, the final node representations are combined into a graph representation using a readout function:\n$h_G = READOUT ({h_v^{(T)}}_{v\\in V})$.\nThe readout function, essentially a set function learned by the neural network, commonly employs AVERAGE or MAXPOOL."}, {"title": "2.4 Graph Kernel", "content": "A kernel is a function used to measure the similarity between pairs of objects. For a non-empty set $X$ and a function $K : x \\times x \\rightarrow \\mathbb{R}$, the function $K$ qualifies as a kernel on $x$ if there exists a Hilbert space $\\mathcal{H}_k$ and a feature map function $\\phi : X \\rightarrow \\mathcal{H}_k$, such that $K(x, y) = (\\phi(x), \\phi(y))$ for any $x, y \\in \\mathcal{X}$, where $(\\cdot, \\cdot)$ denotes the inner product in $\\mathcal{H}_k$. Notably, such a feature map exists if and only if $K$ is a positive semi-definite function. Let $K$ be a kernel defined on $x$, and let $S = {x_1,...,x_n}$ be a finite set of n samples on $\\mathcal{X}$. The Gram matrix for $S$ is defined as $G \\in \\mathbb{R}^{n\\times n}$, with each element $G_{ij} = K(x_i, x_j)$ representing the kernel value between the $i$-th and $j$-th data points in $S$. The Gram matrix is always positive semi-definite.\nGraph kernel methods apply the kernel approaches to graph data, typically defined using the R-convolution framework [Haussler et al., 1999, Bause and Kriege, 2022]. Consider two graphs, $G$ and $G'$. The key idea is to decompose the graphs into substructures using a predefined feature map function $\\phi$, and then compute the kernel value by taking the inner product in the feature space: $K(G, G') = (\\phi(G), \\phi(G'))$, based on these substructures. Weisfeiler-Lehman (WL) graph kernels stand out as one of the most widely used approaches. These methods employ the Weisfeiler-Lehman"}, {"title": "3 Consistency Principles", "content": "To encode relational structures in GNN learning, we first examine how similarities are represented in graph kernels. In this section, we start by defining a class of graph kernels, i.e., the iterative graph kernels, which encompasses many widely used kernels. Then, we delve into a key property known as the consistency property, which may play an important role in enhancing classification performance. We support this assertion through theoretical analysis, elucidating how different kernels adhere to or deviate from this property, thereby explaining their performance differences."}, {"title": "3.1 Iterative Graph Kernels", "content": "In this paper, we are interested in a set of kernels defined as follows:\nDefinition 3.1 Given a colored graph set $x$, a feature map function $\\phi : X \\rightarrow \\mathcal{H}_k$ (where $\\mathcal{H}_k$ is a Hilbert space), and a set of coloring functions $\\mathcal{F}_c = {f^0, f^1, ..., f^l}$ on $x$ (with $f^i : X \\rightarrow x$), we define the set of iterative graph kernels (IGK) as:\n$K_{\\mathcal{F}_c,\\phi}(x, y, i) = (\\phi(f^i \\circ \\cdots f^1(x)), \\phi(f^i \\circ \\cdots f^1(y))) = (\\psi^i(x), \\psi^i(y))$\nwhere $x, y \\in x$ and $\\psi^i(\\cdot)$ represents a composite function given by: $\\psi^i = f^i \\circ \\cdots \\circ f^1$, Then the normalized kernel is given by:\n$\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i) = \\frac{K_{\\mathcal{F}_c,\\phi}(x, y, i)}{\\sqrt{K_{\\mathcal{F}_c,\\phi}(x, x, i) \\sqrt{K_{\\mathcal{F}_c,\\phi}(y, y, i)}}}$\nBased on this definition, we can see that graph kernels utilizing the Weisfeiler-Lehman framework, including the WL-subtree kernel [Shervashidze et al., 2011], WLOA [Kriege et al., 2016], and the Wasserstein Weisfeiler-Lehman (WWL) kernel [Togninalli et al., 2019], should be classified as iterative graph kernels. Conversely, the subgraph-pattern approaches, such as the graphlet kernel [Shervashidze et al., 2009] and the shortest-path kernel [Borgwardt and Kriegel, 2005], do not fall into this category."}, {"title": "3.2 Key Properties: Monotonic Decrease & Order Consistency", "content": "To effectively capture relational structures, we design the IGKs to progressively differentiate between graphs. With each iteration, additional structural features are considered, enabling the distinction of graphs that may have been indistinguishable in earlier iterations. This implies two properties: (1) the kernel values monotonically decrease with larger iterations, as the similarity between two graphs decreases with the consideration of more features; and (2) the similarity rankings across different iterations should remain consistent, meaning that graphs deemed dissimilar in early iterations should not be considered similar in later iterations. We then formally define these two properties and demonstrate how they can lead to a non-decreasing margin (better performance) in the binary classification task.\nDefinition 3.2 (Monotonic Decrease) The normalized iterative graph kernels $\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i)$ are said to be monotonically decreasing if and only if:\n$\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i) \\geq \\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i + 1)  \\forall x,y \\in \\mathcal{X}$\nDefinition 3.3 (Order Consistency) The normalized iterative graph kernels $\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i)$ are said to preserve order consistency if the similarity ranking remains consistent across different iterations for any pair of graphs, which is defined as:\n$\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i) > \\bar{K}_{\\mathcal{F}_c,\\phi}(x, z, i) \\Rightarrow \\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i + 1) \\geq \\bar{K}_{\\mathcal{F}_c,\\phi}(x, z, i + 1)  \\forall x, y, z \\in \\mathcal{X}$\nNext we show that these two properties can lead to a non-decreasing margin in the binary classification task, which suggests better performance.\nConsider a binary graph classification task and assume that the graph representations obtained at any iteration have a uniform norm. This can be achieved by simply normalizing the graph representations at the end of each iteration. That is, for any graph x: $|\\phi(\\psi^i(x))|| = 1$. Then, for any IGK that is monotonically decreasing and preserves the order consistency, the following theorem holds:\nTheorem 3.4 Let $\\bar{K}_{\\mathcal{F}_c,\\phi}(x, y, i)$ be a normalized iterative graph kernel that is monotonically decreasing and preserves order consistency. In the binary graph classification task with uniform graph representations, the margin between two classes in the representation space is non-decreasing w.r.t the iteration i, where the margin at iteration i is defined as the shortest distance between two graph representations from different classes: margin=$||\\psi^i(x) - \\psi^i(y)||$, $V(x) \\neq V(y)$.\nProof sketch. Suppose that at iteration i, the margin is defined by a pair of graphs, $x_1$ from class 1 and $y_1$ from class 2. In the next iteration, $i + 1$, the margin is determined by another pair, $x_2$ and $y_2$. Two possibilities arise: (1) $x_1 = x_2$ and $y_1 = y_2$, or (2) $x_1 \\neq x_2$ or $y_1 \\neq y_2$.\n(1) If the pair defining the margin remains unchanged, the margin at iteration $i + 1$ can only increase or stay the same, as the kernel is a monotonically decreasing function, indicating a reduction in similarity.\n(2) We prove case 2 by contradiction, assuming the margin decreases at iteration $i + 1$. This would imply that the kernel value for $x_2$ and $y_2$ increases at iteration $i + 1$, which contradicts the fact that the kernel function is monotonically decreasing.\nThus, in both cases, the margin does not decrease as the iterations progress.\nWe provide the detailed proof in Appendix A.1"}, {"title": "3.3 Theoretical Verification with WL-based Kernels", "content": "As discussed in Section 3.1, WL-based kernels can be categorized as iterative graph kernels, as they are generated by the coloring functions in an iterative refinement process. Consequently, a natural question arises regarding how various kernels adhere to these properties and whether their adherence reflects their actual performance. We thus investigate two popular WL-based Kernels: the WL-subtree kernel [Shervashidze et al., 2011] and the WLOA kernel [Kriege et al., 2016].\nTheorem 3.5 The normalized WL-subtree kernel is neither monotonically decreasing nor does it preserve order consistency.\nTheorem 3.6 The normalized WLOA kernel is monotonically decreasing and asymptotically preserves order consistency when $w(i) = 1$.\nProof sketch. For Theorem 3.5, we illustrate a counterexample, while for Theorem 3.6, we consider two graph pairs where the similarity condition holds:\n$\\bar{K}_{WLOA}^{(h)}(G, G') \\geq \\bar{K}_{WLOA}^{(h)}(G, G'')$\nThe similarity at the next iteration $h + 1$ is scaled by a factor dependent $w(i), i = 1,\\cdots, h + 1$. The unnormalized kernel increases monotonically, though with diminishing increments over iterations. Given this, when $w(i) = 1$ and $h \\rightarrow \\infty$, we obtain:\n$\\bar{K}_{WLOA}^{(h+1)}(G, G') \\geq \\bar{K}_{WLOA}^{(h+1)}(G, G'')$\nWe include the complete proof in Appendix A.2 and A.3.\nThese findings imply that the WLOA kernel better preserves relational structures compared to the WL-subtree kernel, leading to improved classification performance, as supported by the literature [Kriege et al., 2016]."}, {"title": "4 Proposed Strategy", "content": "Given the analogy between WL-based kernels and GNNs [Shervashidze et al., 2011, Gilmer et al., 2017], and the observation that GNNs often fail to preserve relational structures, we hypothesize that the consistency principle is also beneficial to GNN learning. Thus, we aim to explore how to effectively preserve this principle within the GNN architectures."}, {"title": "4.1 Consistency Loss", "content": "Our objective is to enhance graph representation consistency across GNN layers, which has significant potential to preserve the relational structure in the representation space. If we compare $(\\phi^i (G))$ to the graph representations obtained at the $i$-th layer, preserving the consistency principle is equivalent to preserving the ordering of cosine similarity among the graph representations. However, due to the non-differentiable nature of ranking operations, directly minimizing the ranking differences between consecutive layers is not feasible using gradient-based optimization techniques. Therefore, we aim to optimize pairwise ordering relations instead of the entire ranking list. In this work, our proposed loss employs a probabilistic approach inspired by [Burges et al., 2005]. The entire framework is illustrated in Figure 2.\nLet $H^h \\in \\mathbb{R}^{n \\times d}$ denote the graph embedding matrix for $n$ examples in a batch, each with a $d$-dimensional feature vector. We first compute the distance matrix $D^h$ for all the graphs in a batch at the $h$-th layer. The entries $D_{ij}^h$ of this matrix represent the distance between the representations of the $i$-th and $j$-th graphs, calculated as $D_{ij}^h = Dist(H_i^h, H_j^h)$. Here, we use the cosine distance, the complement of cosine similarity in positive space, expressed as: $1 - \\frac{H_i^h H_j^h}{||H_i^h|| ||H_j^h||}$. Considering the distance relationship to an arbitrary graph $x_k$ in the batch, the predicted probability $\\mathbb{P}_{n,m|k}^h$ that $x_k$ is more similar to graph $x_n$ than to graph $x_m$ at layer $h$ is defined as $\\mathbb{P}_{n,m|k}^h (D_{kn} < D_{km})$. This probability score, which ranges from 0 to 1, is formulated using the sigmoid function as follows:\n$\\mathbb{P}_{n,m/k}^h (D_{kn} < D_{km}) = \\frac{1}{1 + exp (D_{km}^h - D_{kn}^h)}$\nGiven the distance matrix from the previous layer $D^{h-1}$, the known probability that graph $x_k$ is more similar to graph $x_n$ than to graph $x_m$, denoted as $\\mathbb{P}_{n,m|k}^{h-1}(D_{kn}^{h-1} < D_{km}^{h-1})$, can be formulated as follows:\n$\\bar{\\mathbb{P}}_{n,m|k}^{h-1} = \\frac{1}{2} (1+sign(D_{km}^{h-1} - D_{kn}^{h-1}))$\nWe can then minimize the discrepancy between the predicted and the known probability distributions to enhance representation consistency across the layers. Here, we employ the cross-entropy loss to effectively measure the divergence between these two distributions. Specifically, for a pair $(x_n, x_m)$ centered on $x_k$ at layer $h$, the cross-entropy loss can be expressed as:\n$\\mathcal{L}_{cross-entropy}((x_n, x_m) | x_k, h) = -\\bar{\\mathbb{P}}_{n,m|k}^{h-1} log \\mathbb{P}_{n,m|k}^h - (1 - \\bar{\\mathbb{P}}_{n,m|k}^{h-1}) log (1 - \\mathbb{P}_{n,m|k}^h)$\nThen, the total loss function, which quantifies the consistency of pair-wise distance relations for graph $x_k$ at layer $h$, can be formulated as\n$\\mathcal{L}_{consistency} (k) = \\sum_{n,m} \\mathcal{L}((x_n, x_m) | x_k, h)$\nThe overall objective function of the proposed framework can then be formulated as the weighted sum of the original loss and the consistency loss.\n$\\mathcal{L}_{total} = \\mathcal{L}_{origin} + \\lambda \\sum_i \\mathcal{L}_{consistency} (i)$\nHere, $\\lambda$ is a hyperparameter that controls the strength of the consistency constraint."}, {"title": "5 Experiment", "content": "In this section, we examine the effectiveness of the proposed consistency loss for the graph classification task. Specifically, we aim to address the following questions: Q1: Does the consistency loss effectively enhance the performance of various GNN backbones in the graph classification task? Q2: How does the consistency loss influence the rank correlation of graph similarities across GNN layers? Q3: How does the consistency loss influence dataset performance across varying levels of complexity, both in structural intricacy and task difficulty?"}, {"title": "5.1 Experiment Setup", "content": "Dataset We conduct extensive experiments using the TU Dataset [Morris et al., 2020], the Open Graph Benchmark (OGB) [Hu et al., 2020] and Reddit Threads(Reddit-T) dataset [Bause and Kriege, 2022]."}, {"title": "5.2 Effectiveness of Consistency Loss", "content": "To answer Q1, we present the results for the TU, OGB and Reddit-T datasets in Table 1. As shown in this table, GNN models with the consistency loss yield significant performance improvements over their base models on different datasets. These findings suggest that the consistency framework is a versatile and robust approach for enhancing the predictive capabilities of GNNs in real-world datasets, irrespective of the base model and dataset domain. Notably, the GIN method demonstrates the most significant improvements, achieving enhancements of up to 4.51% on the D&D dataset, 4.32% on the COLLAB dataset, and 3.70% on the IMDB-B dataset. This improvement can be linked to our empirical observation regarding the weak ability of GIN to preserve consistency across layers. In addition, our method demonstrates satisfactory improvements on datasets with numerous classes (e.g., COIL-RAG) and large-scale datasets (e.g., ogbg-molhiv and Reddit-T), indicating that our approach is both flexible and scalable for handling complex and extensive datasets."}, {"title": "5.3 Effect of the Consistency Loss on Rank Correlation", "content": "To answer Q2, we compare the consistency of graph representations across layers with and without the proposed consistency loss. Specifically, we use the Spearman's rank correlation coefficient, a widely accepted method for computing correlation between ranked variables, to quantitatively measure the consistency of graph similarities across layers. For a fair comparison, we construct a distance matrix $D^h$ for all test data at each layer $h$, where each row $D_{x_i.}^h$ represents the distances from graph $x_i$ to all other graphs. We then compute the rank correlation between $D_{x_i.}^h$ and $D_{x_i.}^{h+1}$ for each graph $x_i$.\nWe average the correlation values for all graphs to obtain the overall correlation for layer $h$. Then, we compute the mean of these values across layers, enabling a global comparison of relational consistency throughout the model and dataset. All results were averaged over 5 repeated experiments with same training setting.\nWe present our results on a series of datasets from the TU Dataset in Table 2. As shown in the table, it is evident that the representation space becomes more consistent with our proposed consistency loss. For example, a significant enhancement is observed for the GIN model. Another notable point is the result for the GCN model on the NCI109 dataset and for the GMT model on the IMDB-B dataset. We find that the correlation is already fairly high even without the implementation of $\\mathcal{L}_{consistency}$, resulting in minimal correlation improvements with our method. This phenomenon provides a plausible explanation for why our method is not effective in these two cases."}, {"title": "5.4 Study on Task Complexity", "content": "To address Q3 and further evaluate the performance of our method across different scenarios, we extended our study by conducting experiments on graph datasets with increasing task and structural complexity.\nIncreasing Task Complexity We increase the task complexity by expanding the number of classes that the model needs to classify. To assess the effect of increased class complexity on our method's performance, we sampled subsets from the REDDIT-MULTI 5K dataset [Yanardag and Vishwanathan, 2015] with a progressively greater number of classes, which originally consists of five classes. Specifically, we randomly sampled between 2 and 4 classes to construct new datasets from the original dataset and conducted classification tasks using both GCN and GCN with $\\mathcal{L}_{consistency}$ on these newly constructed datasets. We report the mean test accuracy over five experiments for each subset, as presented in Table.3.\nThe results demonstrate that the effectiveness of our method remains robust, even as the number of classes increases. In fact, it may provide greater advantages when applied to multi-class classification tasks. This resilience likely stems from our method's focus on identifying relational structures in the intermediate representations of GNN models, rather than relying heavily on label information. This approach helps mitigate the impact of potential label noise in the original data. These findings align with the noticeable performance improvements observed in both binary and multi-class classification tasks, as shown in Table 1."}, {"title": "6 Related Work", "content": "Graph Distance and Similarity Measuring distances or similarities between graphs is a fundamental problem in graph learning. Graph kernels, which define graph similarity, have gained significant attention. Most graph kernels use the R-Convolution framework [Haussler et al., 1999] to compare substructure similarities. A trailblazing kernel by [Kashima et al., 2003] used node and edge attributes to generate label sequences through a random walk. The WL-subtree kernel [Shervashidze et al., 2011] generates graph-level features by summing node representation contributions. Recent works align matching substructures between graphs. For instance, Kriege et al. [2016] proposed a discrete optimal assignment kernel based on vertex kernels from WL labels. Togninalli et al. [2019] extended this to include fractional assignments using the Wasserstein distance. Additionally, measuring graph distances is also a prevalent problem. Vayer et al. [2019] combined the Wasserstein and Gromov-Wasserstein distances [Villani and Society, 2003, M\u00e9moli, 2011]. Chen et al. [2022] proposed a polynomial-time WL distance for labeled Markov chains, treating labeled graphs as a special case.\nBridging Graph Kernels and GNNs Many studies have explored the connection between graph kernels and GNNs, attempting to integrate them into a unified framework. Certain approaches focus on leveraging GNN architecture to design novel kernels. For instance, Mairal et al. [2014] presents neural network architectures that learn graph representations within the Reproducing Kernel Hilbert Space (RKHS) of graph kernels. Similarly, Du et al. [2019] proposed a graph kernel equivalent to infinitely wide GNNs, which can be trained using gradient descent. Conversely, other studies have incorporated kernel methods directly into GNNs. For example, Nikolentzos and Vazirgiannis [2020] utilize graph kernels as convolutional filters within GNN architectures. Additionally, Lee et al. [2024] proposes a novel Kernel Convolution Network that employs the random walk kernel as the core mechanism for learning descriptive graph features. Instead of applying specific kernel patterns as mentioned in previous work, we introduce a general method for GNNs to capture consistent similarity relationships, thereby enhancing classification performance."}, {"title": "7 Conclusion", "content": "In this paper, we study a class of graph kernels and introduce the concept of consistency property in graph classification tasks. We theoretically prove that this property leads to a more structure-aware representation space for classification using kernel methods. Based on this analysis, we extend this principle to enhance GNN models. We propose a novel, model-agnostic consistency learning framework for GNNs that enables them to capture relational structures in the graph representation space. Experiments show that our proposed method universally enhances the performance of backbone networks on graph classification benchmarks, providing new insights into bridging the gap between traditional kernel methods and GNN models."}, {"title": "A Theoretical Verification with WL-based Kernels", "content": "Suppose that at iteration i, graph $x_1$ in class 1 and graph $y_1$ in class 2 decide the margin and at iteration i + 1, graph $x_2$ in class 1 and graph $y_2$ in class 2 decide the margin.\nThere are two scenarios: (1) $x_1 = x_2$ and $y_1 = y_2$, (2) $x_1 \\neq x_2$ or $y_1 \\neq y_2$. For case 1, margin at iteration i is given by:\n$||\\phi(\\psi^i(x_1)) - \\phi(\\psi^i(y_1))|| = \\sqrt{(\\phi(\\psi^i(x_1)) - \\phi(\\psi^i(y_1)))^T(\\phi(\\psi^i(x_1)) - \\phi(\\psi^i(y_1)))}$\n$= \\sqrt{2 - 2(\\phi(\\psi^i(x_1)), \\phi(\\psi^i(y_1)))} = \\sqrt{2 - 2K_{\\mathcal{F}_c,\\phi}(x_1,y_1, i)}.\nSince $K_{\\mathcal{F}_c,\\phi}(x, y, i)$ is monotonically decreasing, we have:\n$\\sqrt{2 - 2K_{\\mathcal{F}_c,\\phi}(x_1,y_1, i)} \\leq \\sqrt{2 - 2K_{\\mathcal{F}_c,\\phi}(x_1, y_1, i + 1)} = ||\\phi(\\psi^{i+1}(x_1)) - \\phi(\\psi^{i+1}(y_1))||$.\nThis suggests that the margin does not decrease.\nFor case 2, we prove by contradiction and first assume that the margin decreases. Since the margin decreases, we have:\n$||\\phi(\\psi^{i+1}(x_2)) - \\phi(\\psi^{i+1}(y_2))|| < ||\\phi(\\psi^i(x_1)) - \\phi(\\psi^i(y_1))||$.\nAt iteration i, since $x_1$ and $y_1$ decide the margin, we have:\n$||\\phi(\\psi^i (x_1)) - \\phi(\\psi^i (y_1)) || \\leq ||\\phi(\\psi^i(x_2)) - \\phi(\\psi^i(y_2))||$.\nCombining Equation 3 and Equation 4, we have:\n$||\\phi(\\psi^{i+1}(x_2)) - \\phi(\\psi^{i+1}(y_2))|| < ||\\phi(\\psi^i(x_2)) - \\phi(\\psi^i(y_2))||$.\nThis is equivalent to:\n$\\sqrt{2 - 2K_{\\mathcal{F}_c,\\phi}(x_2,y_2, i + 1)} < \\sqrt{2 - 2K_{\\mathcal{F}_c,\\phi}(x_2,y_2, i)},$\nsuggesting that $K_{\\mathcal{F}_c,\\phi}(x_2, y_2, i)$ increases with i, which conflicts with the fact that $K_{\\mathcal{F}_c,\\phi}(x_2,y_2, i)$ is monotonically decreasing. Therefore, the margin does not decrease."}, {"title": "A.2 Proof of Theorem 3.5", "content": "Following Equation 2.4", "as": "n$K_{\\text{wl\\_subtree}}^{(h)} (G, G') = \\frac{\\sum_{i=1}^h (\\phi(\\psi^i(G)), \\phi(\\psi^i(G')))}{\\sqrt{\\sum_{i=1}^h (\\phi(\\psi^i(G)), \\phi(\\psi^i(G)))} \\sqrt{\\sum_{i=1}^h (\\phi(\\psi^i(G')), \\phi(\\psi^i(G')))"}]}, {}]