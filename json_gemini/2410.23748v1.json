{"title": "Exploring Consistency in Graph Representations: from Graph Kernels to Graph Neural Networks", "authors": ["Xuyuan Liu", "Yinghao Cai", "Qihui Yang", "Yujun Yan"], "abstract": "Graph Neural Networks (GNNs) have emerged as a dominant approach in graph\nrepresentation learning, yet they often struggle to capture consistent similarity\nrelationships among graphs. While graph kernel methods such as the Weisfeiler-\nLehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment (WLOA)\nkernels are effective in capturing similarity relationships, they rely heavily on\npredefined kernels and lack sufficient non-linearity for more complex data patterns.\nOur work aims to bridge the gap between neural network methods and kernel\napproaches by enabling GNNs to consistently capture relational structures in their\nlearned representations. Given the analogy between the message-passing process\nof GNNs and WL algorithms, we thoroughly compare and analyze the properties of\nWL-subtree and WLOA kernels. We find that the similarities captured by WLOA\nat different iterations are asymptotically consistent, ensuring that similar graphs\nremain similar in subsequent iterations, thereby leading to superior performance\nover the WL-subtree kernel. Inspired by these findings, we conjecture that the\nconsistency in the similarities of graph representations across GNN layers is crucial\nin capturing relational structures and enhancing graph classification performance.\nThus, we propose a loss to enforce the similarity of graph representations to be\nconsistent across different layers. Our empirical analysis verifies our conjecture\nand shows that our proposed consistency loss can significantly enhance graph\nclassification performance across several GNN backbones on various datasets.", "sections": [{"title": "1 Introduction", "content": "Graph classification tasks are extensively applied across multiple domains, including chemistry [Liu\net al., 2022, Xu et al., 2023], bioinformatics [Yan et al., 2019, Li et al., 2023a,b], and social network\nanalysis [Ying et al., 2018, Wang et al., 2024]. Graph neural networks (GNNs) [Kipf and Welling,\n2017, Xu et al., 2019, Velickovic et al., 2018, Huang et al., 2024] have emerged as the predominant\napproach for performing graph classification, owing to their ability to extract rich representations\nfrom various types of graph data. A typical GNN employs the message-passing mechanism [Gilmer\net al., 2017], where node features are propagated and aggregated across connected nodes. This\nprocess effectively captures local tree structures, enabling the differentiation between various graphs.\nHowever, GNNs often struggle to preserve relational structures among graphs, resulting in inconsistent\nrelative similarities across the layers. As shown in Figure 1, graphs with higher relative similarity\nin one layer may exhibit reduced similarity in the subsequent layer. This phenomenon arises from\nthe limitations of cross-entropy loss, which fails to preserve relational structures, as it forces graphs\nwithin the same class into identical representations.\nGraph kernel methods, on the other hand, are designed to capture similarities between graphs and\nutilize these similarities for classification tasks. For instance, subgraph-pattern approaches [Sher-\nvashidze et al., 2009, Costa and Grave, 2010, Kriege et al., 2020] compare graphs by counting the"}, {"title": "2 Preliminaries", "content": "In this section, we begin by introducing the notations and definitions used throughout the paper. Next,\nwe provide an introduction to the fundamentals of Weisfeiler-Lehman isomorphism test, GNNs and\ngraph kernels."}, {"title": "2.1 Notations and Definitions", "content": "Let $G(V, E, X)$ be an undirected and unweighted graph with $N$ nodes, where $V$ denotes the node set,\n$E$ denotes the edge set, and $X$ denotes the feature matrix, where each row represents the features of a\ncorresponding node. The neighborhood of a node $v$ is defined as the set of all nodes that connected to\n$v$: $N(v) = \\{u|(v, u) \\in E\\}$. In a graph dataset, each graph $G_i$ is associated with a label $Y_i$, which is\nsampled from a label set $L$. In this paper, we focus on the graph classification task, where a model\nis trained to map each graph to its label."}, {"title": "2.2 Weisfeiler-Lehman Isomorphism Test", "content": "We first introduce the Weisfeiler-Lehman isomorphism test [Weisfeiler and Lehman, 1968], which\ncan be used to distinguish different graphs and is closely related to the message-passing process of\nGNNs [Xu et al., 2019]. The WL algorithm operates by iteratively relabeling the colors of vertices in\nthe graph. Initially, all vertices are assigned the same color $C_0$. In iteration $i$, the color of a vertex\n$v$ is updated based on its current color $C_{v,i-1}$ and the colors of its neighbors $\\{C_{u \\in N(v),i-1}\\}$. The\nupdate is given as follows:\n\n$C_{v,i} = f_c (\\{C_{v,i-1}, \\{C_{u\\in N(v),i-1}\\}\\})$\n\nwhere $f_c$ is an injective coloring function that maps the multisets to different colors at iteration $i$.This\nprocess continues for a predefined number of iterations or until the coloring stabilizes (i.e., the colors\nno longer change)."}, {"title": "2.3 Graph Neural Network", "content": "Most GNNs adopt the message-passing framework [Gilmer et al., 2017], which can be viewed as\na derivative of the Weisfeiler-Lehman coloring mechanism. Specifically, let $h_v^{(k-1)}$ represent the\nfeature vector of node $v$ at the $(k \u2212 1)$-th iteration. A GNN computes the new feature for $v$ by\naggregating the representations of itself and its neighboring nodes $u \\in N(v)$ as follows:\n\n$h_v^{(k)} = UPDATE^{(k)} (h_v^{(k-1)}, m_v^{(k)})$, where $m_v^{(k)} = AGGR^{(k)} (\\{h_u^{(k-1)} : u\\in N(v)\\})$\n\nThe initial node representations $h_v^{(0)}$ are set to the raw node features $X_v$. At the $k$-th iteration, the\naggregation function $AGGR^{(k)}(\\cdot)$ computes the messages $m_v^{(k)}$ received from neighboring nodes.\nSubsequently, the update function $UPDATE^{(k)}(\\cdot)$ computes a new representation for each node by\nintegrating the neighborhood messages $m_v^{(k)}$ with its previous embedding $h_v^{(k-1)}$. After $T$ iterations,\nthe final node representations are combined into a graph representation using a readout function:\n\n$h_G = READOUT (\\{h_v^{(T)} : v\\in V\\}).$\n\nThe readout function, essentially a set function learned by the neural network, commonly employs\nAVERAGE or MAXPOOL."}, {"title": "2.4 Graph Kernel", "content": "A kernel is a function used to measure the similarity between pairs of objects. For a non-empty set $X$\nand a function $K : x \\times x \\rightarrow R$, the function $K$ qualifies as a kernel on $x$ if there exists a Hilbert space\n$H_k$ and a feature map function $\\phi : X \\rightarrow H_k$, such that $K(x, y) = (\\phi(x), \\phi(y))$ for any $x, y \\in \\chi,$\nwhere $(\\cdot,\\cdot)$ denotes the inner product in $H_k$. Notably, such a feature map exists if and only if $K$ is\na positive semi-definite function. Let $K$ be a kernel defined on $x$, and let $S = \\{x_1, ..., x_n\\}$ be a\nfinite set of $n$ samples on $X$. The Gram matrix for $S$ is defined as $G \\in R^{n \\times n}$, with each element\n$G_{ij} = K(x_i, x_j)$ representing the kernel value between the $i$-th and $j$-th data points in $S$. The Gram\nmatrix is always positive semi-definite.\nGraph kernel methods apply the kernel approaches to graph data, typically defined using the R-\nconvolution framework [Haussler et al., 1999, Bause and Kriege, 2022]. Consider two graphs, $G$\nand $G'$. The key idea is to decompose the graphs into substructures using a predefined feature map\nfunction $\\phi$, and then compute the kernel value by taking the inner product in the feature space:\n$K(G, G') = (\\phi(G), \\phi(G'))$, based on these substructures. Weisfeiler-Lehman (WL) graph kernels\nstand out as one of the most widely used approaches. These methods employ the Weisfeiler-Lehman"}, {"title": "3 Consistency Principles", "content": "To encode relational structures in GNN learning, we first examine how similarities are represented in\ngraph kernels. In this section, we start by defining a class of graph kernels, i.e., the iterative graph\nkernels, which encompasses many widely used kernels. Then, we delve into a key property known as\nthe consistency property, which may play an important role in enhancing classification performance.\nWe support this assertion through theoretical analysis, elucidating how different kernels adhere to or\ndeviate from this property, thereby explaining their performance differences."}, {"title": "3.1 Iterative Graph Kernels", "content": "In this paper, we are interested in a set of kernels defined as follows:\n\nGiven a colored graph set $x$, a feature map function $\\phi : X \\rightarrow H_k$ (where $H_k$ is a\nHilbert space), and a set of coloring functions $F_c = \\{f^0, f^1, ..., f^l\\}$ on $x$ (with $f^i : X \\rightarrow x$), we\ndefine the set of iterative graph kernels (IGK) as:\n\n$K_{F_c,\\phi}(x, y, i) = (\\phi(f^i \\circ\u00b7\u00b7\u00b7 f^1(x)), \\phi(f^i \\circ\u00b7\u00b7\u00b7 f^1(y))) = (\\psi^i(x), \\psi^i(y))$\n\nwhere $x,y \\in x$ and $\\psi^i(\\cdot)$ represents a composite function given by: $\\psi^i = f^i \\circ\u00b7\u00b7\u00b7 \\circ f^1$, Then the\nnormalized kernel is given by:\n\n$K_{F_c,\\phi}(x, y, i) = \\frac{K_{F_c,\\phi}(x, y, i)}{\\sqrt{K_{F_c,\\phi}(x, x, i) \\sqrt{K_{F_c,\\phi}(y, y, i)}}}$\n\nBased on this definition, we can see that graph kernels utilizing the Weisfeiler-Lehman framework,\nincluding the WL-subtree kernel [Shervashidze et al., 2011], WLOA [Kriege et al., 2016], and\nthe Wasserstein Weisfeiler-Lehman (WWL) kernel [Togninalli et al., 2019], should be classified\nas iterative graph kernels. Conversely, the subgraph-pattern approaches, such as the graphlet ker-\nnel [Shervashidze et al., 2009] and the shortest-path kernel [Borgwardt and Kriegel, 2005], do not\nfall into this category."}, {"title": "3.2 Key Properties: Monotonic Decrease & Order Consistency", "content": "To effectively capture relational structures, we design the IGKs to progressively differentiate between\ngraphs. With each iteration, additional structural features are considered, enabling the distinction\nof graphs that may have been indistinguishable in earlier iterations. This implies two properties:\n(1) the kernel values monotonically decrease with larger iterations, as the similarity between two\ngraphs decreases with the consideration of more features; and (2) the similarity rankings across\ndifferent iterations should remain consistent, meaning that graphs deemed dissimilar in early iterations\nshould not be considered similar in later iterations. We then formally define these two properties\nand demonstrate how they can lead to a non-decreasing margin (better performance) in the binary\nclassification task.\n\nThe normalized iterative graph kernels $K_{F_c,\\phi}(x, y, i)$ are\nsaid to be monotonically decreasing if and only if:\n\n$K_{F_c,\\phi}(x, y, i) \\geq K_{F_c,\\phi}(x, y, i + 1) \\forall x,y \\in x$\n\nDefinition 3.3 (Order Consistency) The normalized iterative graph kernels $K_{F_c,\\phi}(x, y, i)$ are said\nto preserve order consistency if the similarity ranking remains consistent across different iterations\nfor any pair of graphs, which is defined as:\n\n$K_{Fe,\\phi}(x, y, i) > K_{F_c,\\phi}(x, z, i) \\Rightarrow K_{F_c,\\phi}(x, y, i + 1) \\geq K_{F_c,\\phi}(x, z, i + 1) \\forall x, y, z \\in X$\n\nNext we show that these two properties can lead to a non-decreasing margin in the binary classification\ntask, which suggests better performance.\nConsider a binary graph classification task and assume that the graph representations obtained at any\niteration have a uniform norm. This can be achieved by simply normalizing the graph representations\nat the end of each iteration. That is, for any graph x: $|\\|\\phi(\\psi^i(x))\\|| = 1$. Then, for any IGK that is\nmonotonically decreasing and preserves the order consistency, the following theorem holds:\n\nTheorem 3.4 Let $K_{F_c,\\phi}(x, y, i)$ be a normalized iterative graph kernel that is monotonically de-\ncreasing and preserves order consistency. In the binary graph classification task with uniform graph\nrepresentations, the margin between two classes in the representation space is non-decreasing w.r.t\nthe iteration i, where the margin at iteration i is defined as the shortest distance between two graph\nrepresentations from different classes: margin=$|\\|\\psi^i(x) \u2013 \\psi^i (y)\\||$, V(x) \u2260 V(y).\n\nProof sketch. Suppose that at iteration i, the margin is defined by a pair of graphs, $x_1$ from class 1\nand $y_1$ from class 2. In the next iteration, $i + 1$, the margin is determined by another pair, $x_2$ and $y_2$.\nTwo possibilities arise: (1) $x_1 = x_2$ and $y_1 = y_2$, or (2) $x_1 \u2260 x_2$ or $Y_1 \u2260 Y_2.\n(1) If the pair defining the margin remains unchanged, the margin at iteration $i + 1$ can only\nincrease or stay the same, as the kernel is a monotonically decreasing function, indicating a\nreduction in similarity.\n(2) We prove case 2 by contradiction, assuming the margin decreases at iteration $i + 1$. This\nwould imply that the kernel value for $x_2$ and $y_2$ increases at iteration $i + 1$, which contradicts\nthe fact that the kernel function is monotonically decreasing.\nThus, in both cases, the margin does not decrease as the iterations progress.\nWe provide the detailed proof in Appendix A.1"}, {"title": "3.3 Theoretical Verification with WL-based Kernels", "content": "As discussed in Section 3.1, WL-based kernels can be categorized as iterative graph kernels, as they\nare generated by the coloring functions in an iterative refinement process. Consequently, a natural\nquestion arises regarding how various kernels adhere to these properties and whether their adherence\nreflects their actual performance. We thus investigate two popular WL-based Kernels: the WL-subtree\nkernel [Shervashidze et al., 2011] and the WLOA kernel [Kriege et al., 2016]."}, {"title": "4 Proposed Strategy", "content": "Given the analogy between WL-based kernels and GNNs [Shervashidze et al., 2011, Gilmer et al.,\n2017], and the observation that GNNs often fail to preserve relational structures, we hypothesize\nthat the consistency principle is also beneficial to GNN learning. Thus, we aim to explore how to\neffectively preserve this principle within the GNN architectures."}, {"title": "4.1 Consistency Loss", "content": "Our objective is to enhance graph representation consistency across GNN layers, which has significant\npotential to preserve the relational structure in the representation space. If we compare $(\\phi^i (G))$ to\nthe graph representations obtained at the $i$-th layer, preserving the consistency principle is equivalent\nto preserving the ordering of cosine similarity among the graph representations. However, due to the\nnon-differentiable nature of ranking operations, directly minimizing the ranking differences between\nconsecutive layers is not feasible using gradient-based optimization techniques. Therefore, we aim"}, {"title": "5 Experiment", "content": "In this section, we examine the effectiveness of the proposed consistency loss for the graph classi-\nfication task. Specifically, we aim to address the following questions: Q1: Does the consistency\nloss effectively enhance the performance of various GNN backbones in the graph classification task?\nQ2: How does the consistency loss influence the rank correlation of graph similarities across GNN\nlayers? Q3: How does the consistency loss influence dataset performance across varying levels of\ncomplexity, both in structural intricacy and task difficulty?"}, {"title": "5.1 Experiment Setup", "content": "Dataset We conduct extensive experiments using the TU Dataset [Morris et al., 2020], the Open\nGraph Benchmark (OGB) [Hu et al., 2020] and Reddit Threads(Reddit-T) dataset [Bause and Kriege,"}, {"title": "5.2 Effectiveness of Consistency Loss", "content": "To answer Q1, we present the results for the TU, OGB and Reddit-T datasets in Table 1. As shown\nin this table, GNN models with the consistency loss yield significant performance improvements\nover their base models on different datasets. These findings suggest that the consistency framework\nis a versatile and robust approach for enhancing the predictive capabilities of GNNs in real-world\ndatasets, irrespective of the base model and dataset domain. Notably, the GIN method demonstrates\nthe most significant improvements, achieving enhancements of up to 4.51% on the D&D dataset,\n4.32% on the COLLAB dataset, and 3.70% on the IMDB-B dataset. This improvement can be linked\nto our empirical observation regarding the weak ability of GIN to preserve consistency across layers.\nIn addition, our method demonstrates satisfactory improvements on datasets with numerous classes\n(e.g., COIL-RAG) and large-scale datasets (e.g., ogbg-molhiv and Reddit-T), indicating that our\napproach is both flexible and scalable for handling complex and extensive datasets."}, {"title": "5.3 Effect of the Consistency Loss on Rank Correlation", "content": "To answer Q2, we compare the consistency of\ngraph representations across layers with and with-\nout the proposed consistency loss. Specifically,\nwe use the Spearman's rank correlation coeffi-\ncient, a widely accepted method for computing\ncorrelation between ranked variables, to quanti-\ntatively measure the consistency of graph simi-\nlarities across layers. For a fair comparison, we\nconstruct a distance matrix $D^h$ for all test data at\neach layer $h$, where each row $D^h_{x_i,\\cdot}$ represents the\ndistances from graph $x_i$ to all other graphs. We\nthen compute the rank correlation between $D^h_{x_i,\\cdot}$\nand $D^{h+1}_{x_i,\\cdot}$ for each graph $x_i$.\nWe average the correlation values for all graphs\nto obtain the overall correlation for layer $h$. Then,\nwe compute the mean of these values across lay-\ners, enabling a global comparison of relational\nconsistency throughout the model and dataset. All results were averaged over 5 repeated experiments\nwith same training setting.\nWe present our results on a series of datasets from the TU Dataset in Table 2. As shown in the table, it\nis evident that the representation space becomes more consistent with our proposed consistency loss.\nFor example, a significant enhancement is observed for the GIN model. Another notable point is the\nresult for the GCN model on the NCI109 dataset and for the GMT model on the IMDB-B dataset. We\nfind that the correlation is already fairly high even without the implementation of $L_{consistency}$, resulting\nin minimal correlation improvements with our method. This phenomenon provides a plausible\nexplanation for why our method is not effective in these two cases."}, {"title": "5.4 Study on Task Complexity", "content": "To address Q3 and further evaluate the performance of our method across different scenarios, we\nextended our study by conducting experiments on graph datasets with increasing task and structural\ncomplexity.\nWe increase the task complexity by expanding the number of classes\nthat the model needs to classify. To assess the effect of increased class complexity on our method's\nperformance, we sampled subsets from the REDDIT-MULTI 5K dataset [Yanardag and Vishwanathan,\n2015] with a progressively greater number of classes, which originally consists of five classes.\nSpecifically, we randomly sampled between 2 and 4 classes to construct new datasets from the\noriginal dataset and conducted classification tasks using both GCN and GCN with $L_{consistency}$ on these\nnewly constructed datasets. We report the mean test accuracy over five experiments for each subset,\nas presented in Table.3.\nThe results demonstrate that the effectiveness of our method remains robust, even as the number of\nclasses increases. In fact, it may provide greater advantages when applied to multi-class classification\ntasks. This resilience likely stems from our method's focus on identifying relational structures in the\nintermediate representations of GNN models, rather than relying heavily on label information. This\napproach helps mitigate the impact of potential label noise in the original data. These findings align\nwith the noticeable performance improvements observed in both binary and multi-class classification\ntasks, as shown in Table 1."}, {"title": "Increasing Structural Complexity", "content": "We assessed the impact of structural complexity by partitioning\nthe IMDB-B dataset into three subsets with progressively increasing graph densities. Graph density,\ndenoted as $d = \\frac{2M}{N(N-1)}$, where N is the number of nodes and M is the number of edges in graph\nG, was used as the criterion for creating these subsets. The dataset was divided into three groups:\n(small) for graphs with densities below the 33rd percentile, (median) for densities between the 33rd\nand 67th percentiles, and (large) for graphs with densities above the 67th percentile. We applied both\nGCN and GCN+$L_{consistency}$ models to these subsets, and the results are summarized in Table 4."}, {"title": "6 Related Work", "content": "Graph Distance and Similarity Measuring distances or similarities between graphs is a fundamen-\ntal problem in graph learning. Graph kernels, which define graph similarity, have gained significant\nattention. Most graph kernels use the R-Convolution framework [Haussler et al., 1999] to compare\nsubstructure similarities. A trailblazing kernel by [Kashima et al., 2003] used node and edge attributes\nto generate label sequences through a random walk. The WL-subtree kernel [Shervashidze et al.,\n2011] generates graph-level features by summing node representation contributions. Recent works\nalign matching substructures between graphs. For instance, Kriege et al. [2016] proposed a discrete\noptimal assignment kernel based on vertex kernels from WL labels. Togninalli et al. [2019] extended\nthis to include fractional assignments using the Wasserstein distance. Additionally, measuring graph\ndistances is also a prevalent problem. Vayer et al. [2019] combined the Wasserstein and Gromov-\nWasserstein distances [Villani and Society, 2003, M\u00e9moli, 2011]. Chen et al. [2022] proposed a\npolynomial-time WL distance for labeled Markov chains, treating labeled graphs as a special case.\nBridging Graph Kernels and GNNs Many studies have explored the connection between graph\nkernels and GNNs, attempting to integrate them into a unified framework. Certain approaches focus\non leveraging GNN architecture to design novel kernels. For instance, Mairal et al. [2014] presents\nneural network architectures that learn graph representations within the Reproducing Kernel Hilbert\nSpace (RKHS) of graph kernels. Similarly, Du et al. [2019] proposed a graph kernel equivalent to\ninfinitely wide GNNs, which can be trained using gradient descent. Conversely, other studies have\nincorporated kernel methods directly into GNNs. For example, Nikolentzos and Vazirgiannis [2020]\nutilize graph kernels as convolutional filters within GNN architectures. Additionally, Lee et al. [2024]\nproposes a novel Kernel Convolution Network that employs the random walk kernel as the core\nmechanism for learning descriptive graph features. Instead of applying specific kernel patterns as\nmentioned in previous work, we introduce a general method for GNNs to capture consistent similarity\nrelationships, thereby enhancing classification performance."}, {"title": "7 Conclusion", "content": "In this paper, we study a class of graph kernels and introduce the concept of consistency property in\ngraph classification tasks. We theoretically prove that this property leads to a more structure-aware\nrepresentation space for classification using kernel methods. Based on this analysis, we extend\nthis principle to enhance GNN models. We propose a novel, model-agnostic consistency learning\nframework for GNNs that enables them to capture relational structures in the graph representation\nspace. Experiments show that our proposed method universally enhances the performance of backbone\nnetworks on graph classification benchmarks, providing new insights into bridging the gap between\ntraditional kernel methods and GNN models."}, {"title": "F Similarity/Difference with Contrastive learning", "content": "In this section, we discuss the similarities and differences between our method and graph contrastive\nlearning. Graph Contrastive Learning (GCL) is a self-supervised technique for graph data that\nemphasizes instance discrimination [Lin et al., 2023, Zhu et al., 2021]. A typical GCL framework\ngenerates multiple graph views via augmentations and contrasts positive samples (similar instances)\nwith negative samples (dissimilar instances). This approach facilitates effective representation learning\nby capturing relationships between views, ensuring positive pairs remain close in the embedding\nspace while distinctly separating negative pairs.\nWhile both GCL and our method leverage graph similarity, our approach focuses on maintaining\nconsistency across layers, rather than solely capturing similarities as in contrastive learning.\nTo demonstrate this, we integrated the GraphCL technique [You et al., 2020] into a GCN model\n(GCN+CL) and assessed its performance and layer consistency across various datasets. The results,\ndetailed in Tables 11 and 12, use classification accuracy and Spearman rank correlation to measure\nperformance and consistency, respectively."}, {"title": "G Boarder Impact", "content": "This paper aims to advance the field of graph learning by proposing a model-agnostic consistency\nlearning framework. Our framework can be plugged into and improve current methods for graph\nclassification tasks. This has potential benefits in sectors such as chemistry, bioinformatics and\nsocial analysis, where graph classification is widely used. Additionally, we do not foresee any direct\nnegative societal or ethical consequences stemming from our work."}, {"title": "H Limitation", "content": "One limitation of our work is that the method involves additional computational costs, especially\nduring large batch training processes. To extend our framework, sampling methodologies on data or\nlayers can be applied during the consistency-preserving training process. By selectively sampling\ndata points or specific layers, we can reduce the computational burden while still maintaining the\neffectiveness of the cross-layer consistency loss, making the framework more scalable and applicable\nto larger datasets."}]}