{"title": "Adversarial Environment Design via Regret-Guided Diffusion Models", "authors": ["Hojun Chung", "Junseo Lee", "Minsoo Kim", "Dohyeong Kim", "Songhwai Oh"], "abstract": "Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent's capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outper- forming UED baselines in zero-shot generalization across novel, out-of-distribution environments.", "sections": [{"title": "Introduction", "content": "Deep reinforcement learning (RL) has achieved great success in various challenging domains, such as Atari [1], GO [2], and real-world robotics tasks [3, 4]. Despite the progress, the deep RL agent struggles with the generalization problem; it often fails in unseen environments even with a small difference from the training environment distribution [5, 6]. To train well-generalizing policies, various prior works have used domain randomization (DR) [7, 8, 9], which provides RL agents with randomly generated environments. While DR enhances the diversity of the training environments, it requires a large number of trials to generate meaningful structures in high-dimensional domains. Curriculum reinforcement learning [10, 11] has been demonstrated to address these issues by pro- viding instructive sequences of environments. Since manually designing an effective curriculum for complicated tasks is challenging, prior works [12, 13] focus on generating curricula that consider the current agent's capabilities. Recently, unsupervised environment design (UED, [14]) has emerged as a scalable approach, notable for its advantage of requiring no prior knowledge. UED algorithms alternate between training the policy and designing training environments that maximize the regret of the agent. This closed-loop framework ensures the agent learns a minimax regret policy [15], assuming that the two-player game between the agent and the environment generator reaches the Nash equilibrium."}, {"title": "Related Work", "content": null}, {"title": "Unsupervsied Curriculum Reinforcement Learning", "content": "While curriculum reinforcement learning [13, 23, 24] has been shown to enhance the generalization performance of the RL agent, Dennis et al. [14] first introduce the concept of the unsupervised environment design (UED). UED encompasses various environment generation mehods, such as POET [12, 25] and GPN[26]. In this work, we follow the original concept of UED, which aims to learn a minimax regret policy [15] by generating training environments that maximize the regret of the agent. Based on this concept, the learning-based methods train an environment generator via reinforcement learning. PAIRED [14] estimates the regret with a difference between returns"}, {"title": "Diffusion Models", "content": "Diffusion models [21, 31, 32] have achieved remarkable performance in various domains, such as image generation [33], video generation [34], and robotics [35, 36]. Particularly, diffusion models effectively perform conditional generation using guidance to generate samples conditioned on class labels [37, 38] or text inputs [39, 40, 41]. Prior works also guide the diffusion models utilizing an additional network or loss functions, such as adversarial guidance to generate images to attack a classifier [42], safety guidance using pre-defined functions to generate safety-critical driving scenarios [43], and guidance using reward functions trained by human preferences to generate censored samples. [44]. We note that our implementation of the regret-guided diffusion model is based on the work of Dhariwal et al. [37] and Yoon et al. [44]."}, {"title": "Background", "content": null}, {"title": "Unsupervised Environment Design", "content": "Unsupervised environment design (UED, [14]) aims to provide an adaptive curriculum to learn a policy that successfully generalizes to various environments. The environments are represented using a Markov decision process (MDP), defined as (A, S, P, R, \u03c1\u2080, \u03b3), where A is a set of actions, S is a set of states, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is a transition model, R : S \u00d7 A \u2192 R is a reward function, \u03c1\u2080 : S \u2192 [0, 1] is an initial state distribution and \u03b3 is a discount factor. UED employs an environment generator that designs environments by controlling free environment parameters of underspecified environments, which is represented using an underspecified Markov decision process (UMDP). UMDP is defined as M = (A, S, \u0398, P\u1d39, R\u1d39, p\u2080\u1d39, \u03b3), where \u0398 is a set of free environment parameters. Assigning a value to the free environment parameter \u03b8\u2208 \u0398 results in a specific MDP M\u03b8 with the environment configuration (P\u03b8 = P\u1d39(\u03b8),R\u03b8 = R\u1d39(\u03b8), p\u2080 = p\u2080\u1d39(\u03b8)). For example, when learning a mobile robot to navigate towards the goal point while avoiding obstacles, \u0398 could represent the positions of obstacles, the position of the goal, and the start position of the robot.\nUED algorithms alternate between designing a set of environments and training the agent on the generated environments. The environment generator first produces an environment parameter \u03b8 that maximizes the agent's regret. The regret of the policy \u03c0 on environment M\u03b8 is defined as,\nREGRET(\u03c0, \u03b8) := \u2212V(\u03c0, \u03b8) + max V(\u03c0', \u03b8),\n\u03c0' \u2208\u03a0\nwhere \u03a0 is a set of policies and V (\u03c0, \u03b8) := E_{s\u2080\u223c\u03c1\u2080,\u03c0,M\u03b8}[\u03a3_{n=0}^\u221e \u03b3\u207f r\u2099] is an expected return where r\u2099 is a reward obtained by \u03c0 at timestep n on M\u03b8. Then, the agent is trained on the generated environment to maximize the expected return, resulting in minimizing the regret. This framework can be formulated with the following two-player minimax game:\nmin max REGRET(\u03c0, \u03b8).\n\u03c0\u2208\u03a0 \u0398\u2208\u0398"}, {"title": "Diffusion Probabilistic Models", "content": "A diffusion probabilistic model [21] is a generative model that generates samples from noise via iterative denoising steps. Diffusion models start with perturbing data by progressively adding Gaussian noise, called the forward process. The forward process can be modeled with a value- preserving stochastic differential equation (VP SDE, [31]):\ndX_t = \\frac{\\beta_t}{2} X_t dt + \\sqrt{\\beta_t} dW_t,\nwhere t \u2208 [0, T] is a continuous diffusion time variable, \u03b2_t > 0 is a variance schedule, and W_t is a standard Brownian motion. Since the forward process (3) has tractable conditional marginal distributions p_t(X_t|X_0) = N(\\sqrt{\\alpha_t}X_0, (1 \u2212 \u03b1_t)I) where \u03b1_t = e^{-\u222b\u2080^t \u03b2_s ds}, p_T(X_T) will be corrupted into N(0, I) when T \u2192 \u221e.\nGenerating samples following the data distribution p_{data}(\u00b7) requires a reverse process, a reverse-time SDE that has the same marginal distributions as the forward process (3). By Anderson's theorem [45], the reverse process can be formulated with a reverse-time SDE defined as,\ndX_t = -\\frac{\\beta_t}{2} [X_t + \u2207_{X_t} log p_t(X_t)] dt + \\sqrt{\\beta_t} dW_t.\nHence, learning a diffusion model means learning a score network s_\u03c6(X_t, t) that approximates a score function \u2207_{X_t} log p_t(X_t). The score network is trained via score matching [46], then plugged into the reverse process (4):\ndX_t = -\\frac{\\beta_t}{2} [X_t + s_\u03c6(X_t, t)] dt + \\sqrt{\\beta_t} dW_t.\nIndeed, we can generate samples by solving the approximated reverse process (5) with an initial condition X_T \u223c N(0, I).\nTo generate samples with label Y using the diffusion model, the score function of the conditional distribution p_t(X_t|Y) should be estimated. Since p_t(X_t|Y) \u221d p_t(X_t, Y) = p_t(X_t)p_t(Y|X_t) due to Bayes' rule, conditional samples can be generated by solving the reverse process with classifier guidance [37]:\ns_\u03c6(X_t, t) = s_\u03c6(X_t, t) + \u03c9\u2207_{X_t} log p_t(Y|X_t),\ndX_t = -\\frac{\\beta_t}{2} [X_t + s_\u03c6(X_t, t)] dt + \\sqrt{\\beta_t} dW_t,\nwhere p_t(Y|X_t) is a time-dependent classifier network and \u03c9 > 0 is a guidance weight to scale classifier gradients."}, {"title": "Proposed Method", "content": "In this section, we describe our approach to employ a diffusion model as an environment generator to enhance the environment generation capability. We first introduce soft UED, which mutates UED to be more suitable for using a diffusion model as a generator by augmenting the original objective with the entropy regularization term. Then, we propose a novel soft UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). ADD consists of two key components: 1) a diffusion-based environment generation by using the regret as a guidance, and 2) a method to estimate the regret in a differentiable form. We present these key components in detail and conclude the section with an explanation of the overall system and its advantages compared to prior UED methods."}, {"title": "Soft Unsupervised Environment Design", "content": "In this section, we introduce soft UED, designed to guarantee the diversity of environments by adding an entropy regularization term to the original UED objective (2). Soft UED is defined as the following minimax game between the agent and the environment generator:\nmin max E_{\u03b8\u223c\u039b}[REGRET(\u03c0, \u03b8)] + \\frac{1}{\u03c9} H(\u039b),\n\u03c0\u2208\u03a0 \u039b\u2208D_\u039b \u03b8\u2208\u0398\nwhere \u039b is a distribution over \u0398, D_\u039b is a set of distributions over \u0398, H(\u039b) := \u2212\u03a3_{\u03b8} \u039b(\u03b8) log \u039b(\u03b8) is an entropy of \u039b, and \u03c9 is a regularization coefficient. Based on the fact that H is concave, we can show that the strong duality holds:\nProposition 4.1. Let L(\u03c0, \u039b) := E_{\u03b8\u223c\u039b}[REGRET(\u03c0, \u03b8)] + \\frac{1}{\u03c9} H(\u039b) and assume that S, A, and \u0398 are finite. Then, min_{\u03c0\u2208\u03a0} max_{\u039b\u2208D_\u039b} L(\u03c0, \u039b) = max_{\u039b\u2208D_\u039b} min_{\u03c0\u2208\u03a0} L(\u03c0, \u039b).\nThe proof is detailed in Appendix A.1. Proposition 4.1 implies that there exists a valid optimal point (\u03c0, \u039b), and it is stationary for alternative optimization of \u03c0 and \u039b. Hence, the agent will learn a soft minimax regret policy \u03c0 = argmin_{\u03c0\u2208\u03a0} max_{\u039b\u2208D_\u039b} L(\u03c0, \u039b) if it reaches the optimal point. One of the most significant difference from the original UED is the role of the environment generator. Instead of finding a specific environment parameter that maximizes the regret, soft UED updates the environment generator to sample environment parameters from a distribution that maximizes the objective function of soft UED.\nWe note that the soft UED framework encompasses prior UED algorithms. In the learning-based methods, the generator is trained with RL using an entropy bonus, which is known to enhance performance [47] and plays a similar role to H(\u039b). The replay-based methods also consider the diversity of environments by sampling environment parameters from a probability distribution proportional to the regret, instead of selecting a parameter that maximizes the regret. Therefore, soft UED can be considered as a general framework that incorporates practical methods."}, {"title": "Regret-Guided Diffusion Models", "content": "Soft UED converts the problem of generating regret-maximizing environments into a problem of sampling the environment parameter \u03b8 from a desired distribution \u039b* := argmax_{\u039b\u2208D_\u039b} L(\u03c0, \u039b). It is a well-known fact that \u039b* has a closed-form solution as follows:\n\u039b*(\u03b8) = \\frac{u(\u03b8) exp(\u03c9 REGRET(\u03c0, \u03b8))}{C_\u03c0}\nwhere C_\u03c0 is a normalizing constant, and u(\u00b7) denotes an uniform distribution over \u0398. Inspired by the classifier guidance (6), we solve this sampling problem by guiding a pre-trained diffusion model with the regret. To this end, we decompose the score function of \u039b* as follows:\n\u2207_{\u03b8_t} log \u039b*(\u03b8_t) = \u2207_{\u03b8_t} log u_t(\u03b8_t) + \u03c9\u2207_{\u03b8_t} REGRET_t(\u03c0, \u03b8_t),\nwhere t is a diffusion time variable, \u03b8_t is an environment parameter perturbed by the forward process (3), u_t(\u00b7) denotes a distribution of \u03b8_t when \u03b8_0 \u223c u(\u00b7), \u039b_t(\u00b7) denotes a distribution of \u03b8_t when \u03b8_0 \u223c \u039b*(\u00b7), and REGRET_t(\u03c0, \u03b8_t) is a time-dependent regret on the noised environment \u03b8_t, which is equal to REGRET(\u03c0, \u03b8_0). We approximate the first term \u2207_{\u03b8_t} log u_t(\u03b8_t) with a score network s_\u03c6(\u03b8_t, t) \u2248 \u2207_{\u03b8_t} log u_t(\u03b8_t) that is learned by training a diffusion-based environment generator on the randomly generated environment dataset before the agent begins to learn. Then, we can formulate a regret-guided reverse process with a reverse-time SDE as follows:\ns_\u03c6(\u03b8_t, t) = s_\u03c6(\u03b8_t, t) + \u03c9\u2207_{\u03b8_t} REGRET_t(\u03c0, \u03b8_t),\nd\u03b8_t = -\\frac{\\beta_t}{2} [\u03b8_t + s_\u03c6(\u03b8_t, t)] dt + \\sqrt{\\beta_t} dW_t.\nHence, if a gradient of the regret is tractable, we can sample an environment parameter \u03b8_0 from the desired distribution \u039b* by solving the regret-guided reverse process (10) with an initial condition \u03b8_T \u223c N(0, I). However, the regret (1) is intractable since we cannot access the environment-specific optimal policy. Prior works on UED propose various methods to estimate the regret using episodic returns or temporal difference errors, but none of them are differentiable w.r.t. \u03b8_t since agents cannot access the environment parameter and the reward function."}, {"title": "A Differentiable Regret Estimation", "content": "In order to estimate the regret in a differentiable form, we present a novel method based on a flexible regret [14], which is known to enhance the performance of the learning-based UEDs [16]. The main idea is to estimate the regret with a difference between the maximum and average episodic returns that can be achieved by the agent. To make it differentiable w.r.t. \u03b8_t, we utilize an environment critic that predicts the return of the agent in the given environment parameter, as done in DSAGE [48]. The environment critic \u03c4_\u03c8 learns to predict a distribution of returns, analogous to distributional RL [49], to better capture the stochasticity of the environment and policy. Based on a support defined as {z_i = U_{min} + \\frac{i}{M-1}(U_{max} \u2212 U_{min})}_{i=0}^{M-1}, which is a set of centers of M bins that evenly divide the return domain [U_{min}, U_{max}], we obtain an estimated categorical return distribution from an environment critic output l(\u03b8_t, t; \u03c8) \u2208 R^M as follows:\nZ_\u03c8(\u03b8_t, t) = z_i w.p. \\frac{exp(l_i(\u03b8_t, t; \u03c8))}{\u03a3_{j=0}^{M-1} exp(l_j(\u03b8_t, t; \u03c8))}\nTo align Z_\u03c8 with a true return distribution, we train the environment critic by gradient descent on the cross entropy loss between Z_\u03c8(\u03b8_t, t) and a target distribution, which is constructed by projecting episodic returns that the agent achieves on the environment M^{\u03b8_t} onto the support {z_i}_{i=0}^{M-1}.\nAfter the environment critic is updated, we estimate the regret (1) with a difference between the maximum return that the current agent can achieve and average of the predicted return distribution. However, the process of finding a maximum achievable return from the distribution is not differentiable. To address this issue, we further approximate the maximum with a conditional value at risk (CVaR), based on the fact that CVaR_\u03b1(Z) converges to the maximum as a risk measure \u03b1 goes zero. As a result, we estimate the regret of the agent as follows:\nREGRET_t(\u03b8_t, t) \u2248 CVaR_\u03b1(Z_\u03c8(\u03b8_t, t)) \u2212 E(Z_\u03c8(\u03b8_t, t))."}, {"title": "Adversarial Environment Design via Regret-Guided Diffusion Models", "content": "An overview of ADD is provided in Figure 1. First, a diffusion-based environment generator, which is pre-trained on the randomly generated environment dataset, produces a set of environments for the agent. After the agent interacts with the generated environments and is trained via reinforcement learning, the episodic results are utilized to update the environment critic. Then, the environment critic estimates the regret of the agent in a differentiable form (12) and guides the reverse process of the diffusion-based environment generator (10), resulting in environment parameters following the"}, {"title": "Experiments", "content": null}, {"title": "Tasks", "content": "We conduct extensive experiments with two challenging tasks. First, we evaluate the proposed method on a partially observable navigation task with a discrete action space and sparse rewards. Then, we assess the performance of our algorithm on a 2D bipedal locomotion task, which has a continuous action space while offering dense rewards."}, {"title": "Baselines", "content": "We compare the proposed method against several UED baselines. For the learning-based method, we use PAIRED [14], which trains the environment generator via reinforcement learning. For the replay-based method, we use PLR+ [19], which utilizes the random generator and updates the agent only with episodes from the replayed environments. To benchmark performance, we use ACCEL [20], a current state-of-the-art UED algorithm that applies random mutations to replayed environments. Among the two implementation methods of the ACCEL, we use the one that samples environment parameters from the full parameter range, rather than the one that restricts the sampling to a range that ensures simple environments are generated, as the latter could be seen as incorporating prior knowledge. Domain randomization (DR) is also included in baselines so that we can demonstrate the effectiveness of UED. Lastly, we use ADD w/o guidance to show whether the regret guidance induces the diffusion model to generate adversarial environments and enhances the performance of the agent."}, {"title": "Outline", "content": "We first train a diffusion-based environment generator on the randomly generated envi- ronment dataset. Then, we use proximal policy optimization (PPO, [50]) to train the agent on the environments generated by UED methods. To evaluate the generalization capability of the trained agent, we measure the zero-shot transfer performance in challenging, human-designed environments. Additionally, to understand where the differences in performance originate, we conduct quantitative and qualitative analyses on the curriculum of the generated environments by tracking complexity metrics and drawing t-SNE plots. For space consideration, we elaborate on detailed experimental settings including environment parameterization methods in Appendix B."}, {"title": "Partially Observable Navigation Task", "content": "We first evaluate the proposed method on a maze navigation task [14], which is based on the Minigrid [51]. In this task, an agent is trained to take a discrete action using an observation from its surroundings to receive a sparse reward upon reaching a goal. For prior UED methods, we set the maximum number of blocks in a grid environment to 60, aligning with Parker-Holder et al. [20]. For the proposed method, we train the diffusion-based environment generator on 10M random environments whose number of blocks uniformly varies from zero to 60. Then, we train the LSTM-based policy for 250M environmental steps and evaluate the zero-shot performance on 12 challenging test environments from prior works [14, 19]."}, {"title": "D Bipedal Locomotion Task", "content": "We evaluate the proposed method on the 2D bipedal locomotion task, which is based on the Bipedal- Walker task in OpenAI Gym [54] and adopted by Parker Holder et al. [20]. In this task, an agent is trained to control its four motors using observation from its Lidar sensor and joints to walk over challenging terrain. UED methods, including the proposed algorithm, need to provide environment parameters consisting of stump height, stair height, pit gap, stair steps, and ground roughness. We note that each parameter increases the difficulty of the environment as its value increases. We train the RL agent for two billion environmental steps and evaluate the zero-shot performance on six test environments."}, {"title": "Performance.", "content": "Figure 3(a) shows the average return on each test environment. The proposed algorithm achieves the highest return across all environments, with an average of 149.6. Even with half the environmental steps, it achieves a score of 127.4, still surpassing PLR+. ACCEL shows lower performance than PLR+, which can be attributed to the lower sample efficiency induced by the additional interaction between the environment and the agent to assess the modified environments. On the other hand, PAIRED achieves the lowest return in all test environments except the easiest Basic Environment. This shows that the learning-based methods struggle to train a robust policy in practice. We note that a recent work [47] stabilizes the training of PAIRED in this task by integrating the evolutionary concept of ACCEL. While applying the evolutionary approach to ADD is possible, we leave it for future work. Lastly, ADD w/o guidance demonstrates superior generalization performance compared to DR. Although these two methods are theoretically identical, this difference is presumably caused by the limited size of the dataset used for training the diffusion-based environment generator."}, {"title": "Generated curriculum", "content": "Figure 3(b) presents the complexity metrics of the generated training environ- ments and the episodic returns achieved by the RL agent. Unlike the partially observable navigation task, the complexity measure of the environments generated by ADD gradually decreases. This result arises since the randomly generated environments are excessively challenging for the current agent. As evidence, examining the returns achieved by the agent in the generated environments reveals that all methods, except for ADD, consistently yield returns of 0 or below. From these results, we can infer that the proposed algorithm generates environments that are not merely more difficult but are"}, {"title": "Controllable Generation", "content": "To demonstrate the ability to control the difficulty level of the generated environments, we provide the example environment generation results in Figure 4. We control the difficulty level k by guiding the diffusion-based environment generator with a log probability of achieving a specific return Z_{M-k}, as described in (13). We vary k from zero to M \u2212 1 so that the difficulty level of generated environments increases. Environments generated with k = 0, which are shown in the leftmost images, include fewer blocks and a close proximity between the agent's starting position and the goal. As k increases, environments are generated with a greater number of blocks and a larger distance between the starting position and the goal, resulting in the elimination of all possible paths when k = M \u2212 1. The results demonstrate that we can effectively control the difficulty level of the environment using the diffusion-based environment generator and learned environment critic, without domain knowledge. We also present the results of controlling difficulty levels for the 2D bipedal locomotion task in Appendix C.2."}, {"title": "Limitation", "content": "While the proposed method is suitable for training a robust policy, there exist several limitations. First, despite the existence of the optimal point is proven in Proposition 4.1, convergence to such optimal point is not guaranteed. Furthermore, the difference between the true value of the regret and its estimate is not tightly bounded. Lastly, updating the environment critic using episodic results cannot exactly capture the current agent's capability since the policy is updated after the episode. Hence, exploring methods to estimate the regret with a rigorous theoretical background would be an interesting topic for future work."}, {"title": "Conclusion", "content": "In this work, we present a novel UED algorithm that exploits the representation power of diffusion models. We first introduce soft UED, which augments the original UED objective with an entropy regularization term to make it suitable for using a diffusion-based environment generator. Then, we propose ADD, which guides the pre-trained diffusion model with a novel regret estimator to produce environments that are conducive to train a robust policy. Our experimental results demonstrate that ADD is capable of training a policy that successfully generalizes to challenging environments. Moreover, it has been verified that ADD generates an instructive curriculum with varying complexity while covering large environment configuration spaces."}, {"title": "Algorithm Details", "content": null}, {"title": "Proof of Proposition 4.1", "content": "In this section, we show that the minimax problem of soft UED (7) has zero minimax duality gap. We assume that S, A, and \u0398 are finite to avoid the technical issues regarding compactness of a set of distributions. Following Section 3.1, we denote a reward function, transition probability, and initial state distribution of an environment M^\u03b8 with R^\u03b8, P^\u03b8, and \u03c1\u2080^\u03b8, respectively.\nWe first define an occupancy measure, for a policy \u03c0\u2208 \u03a0 and an environment M^\u03b8, as \u03c1^\u03c0(s, a) = \u03c0(a|s) \u03a3_{n=0}^\u221e \u03b3^n Pr(s_n = s|\u03c0, \u03b8). Then, there is a one-to-one correspondence between \u03a0 and a set of valid occupancy measures D^\u03b8 := {\u03c1 : \u03a3_a \u03c1(s, a) = \u03c1\u2080(s) + \u03b3 \u03a3_{s',a'} P^\u03b8(s|s', a')\u03c1(s', a')}\nIf we define a global occupancy measure as \u03c1_\u03c0 := \\frac{1}{|\u0398|} \u03a3_{i=1}^{|\u0398|} \u03c1^\u03c0, where \u03b8^i is an ith element of \u0398, it is obvious that there is a one-to-one correspondence between \u03a0 and a set of valid global occupancy measures D \u2282 D^{\u0398^1} \u00d7... D^{\u0398^{|\u0398|}}. Therefore, we can replace \u03c0 in the objective function of soft UED (7) with \u03c1_\u03c0, as in the following lemma:\nLemma A.1. if L(\u03c1_\u03c0, \u039b) = \u03a3_\u03b8 (V*(\u03b8) - \u03a3_{s,a} \u03c1^\u03c0(s, a) R^\u03b8(s, a))\u039b(\u03b8) + \\frac{1}{\u03c9} H(\u039b), where V*(\u03b8) = max_{\u03c0_A\u2208\u03a0} V(\u03c0_A, \u03b8), then L(\u03c1_\u03c0, \u039b) = L(\u03c0, \u039b).\nProof. Based on the definition of the regret (1), we have\nL(\u03c0, \u039b) = E_{\u03b8\u223c\u039b} [REGRET(\u03c0, \u03b8)] + \\frac{1}{\u03c9} H(\u039b)\n= \u03a3_\u03b8 REGRET(\u03c0, \u03b8)\u039b(\u03b8) + \\frac{1}{\u03c9} H(\u039b)\n= \u03a3_\u03b8 (V*(\u03b8) \u2212 V(\u03c0, \u03b8))\u039b(\u03b8) + \\frac{1}{\u03c9} H(\u039b)\n= \u03a3_\u03b8 (V*(\u03b8) \u2212 \u03a3_{s,a} \u03c1^\u03c0(s, a) R^\u03b8(s,a))\u039b(\u03b8) + \\frac{1}{\u03c9} H(\u039b)\n= L(\u03c1_\u03c0, \u039b).\nNow, we can rewrite the objective function of soft UED with a global occupancy measure as follows:\nmin_{\u03c1\u2208D} max_{\u039b\u2208D_\u039b} L(\u03c1, \u039b).\nThen, we can prove Proposition 4.1 by showing (15) has zero duality gap. However, we cannot apply minimax theorem [57] directly since D is not a convex set. To resolve this issue, we first augment the problem as follows:\nmin_{\u03c1\u2208D} max_{\u039b\u2208D_\u039b} L(\u03c1, \u039b),\nwhere D := {\u03a3_{k=1}^K w_k \u03c1_k|K \u2208 N, \u03a3_{k=1}^K w_k = 1, \u2200k \u2208 {1, ..., K} : w_k \u2265 0, \u03c1_k \u2208 D} is a convex hull of D. We will show that the augmented problem (16) has zero minimax duality gap, and end the proof by showing the optimal values of the augmented problem can also be reached by the original problem (15).\nLemma A.2. min_{\u03c1\u2208D} max_{\u039b\u2208D_\u039b} L(\u03c1, \u039b) = max_{\u039b\u2208D_\u039b} min_{\u03c1\u2208D} L(\u03c1, \u039b)\nProof. Since L(\u03c1, \u039b) is a linear combination of \u03c1^\u03b8, it is convex for all \u03c1. Furthermore, L(\u03c1, \u039b) is concave for \u039b since the entropy H is concave. Therefore, based on the fact that D and D_\u039b are both convex and compact, the augmented problem has zero duality gap due to minimax theorem [57]."}, {"title": "A. .", "content": "For every \u039b \u2208 D_\u039b and corresponding \u03c1*(\u039b) \u2208 argmin_{\u03c1\u2208D} L(\u03c1, \u039b), there exists \u03c1' \u2208 D such that L(\u03c1*(\u039b), \u039b) = L(\u03c1', \u039b).\nProof. For every \u039b \u2208 D_\u039b and corresponding \u03c1*(\u039b) \u2208 argmin_{\u03c1\u2208D} L(\u03c1, \u039b), there exist K \u2208 N, W_{1:K} \u2265 0, and \u03c1_{1:K} \u2208 D such that \u03a3_{k=1}^K w_k = 1 and \u03c1*(\u039b) = \u03a3_{k=1}^K w_k \u03c1_k. Then, following inequality holds:\nmin_{\u03c1\u2208{\u03c1_k}_{k=1}^K} {L(\u03c1_k, \u039b)}_{k=1}^K \u2265 L(\u03c1*(\u039b), \u039b) = \u03a3_{k=1}^K w_k L(\u03c1_k, \u039b) \u2265 min_{\u03c1\u2208{\u03c1_k}_{k=1}^K} {L(\u03c1_k, \u039b)}_{k=1}^K,\nwhere first inequality holds due to the definition of \u03c1*(\u039b), equality holds since L is linear for \u03c1, and second inequality holds since \u03c1*(\u039b) is a convex combination of \u03c1_{1:K}. Then, \u03c1' \u2208 argmin_{\u03c1\u2208{\u03c1_k}_{k=1}^K} L(\u03c1, \u039b) is an element of D and satisfies L(\u03c1*(\u039b), \u039b) = L(\u03c1', \u039b).\nLemma A.3 ensures the minimum value of L achieved over D is also achievable over D, implying the optimal value is the same for both the original and augmented problems. It confirms that strong duality holds for the original problem (15) as well.\nProposition A.4. min_{\u03c1\u2208D} max_{\u039b\u2208D_\u039b} L(\u03c1, \u039b) = max_{\u039b\u2208D_\u039b} min_{\u03c1\u2208D} L(\u03c1, \u039b)\nHence, using Lemma A.1 and Proposition A.4, we can prove Proposition 4.1:\nmin_{\u03c0\u2208\u03a0} max_{\u039b\u2208D_\u039b} L(\u03c0, \u039b) = min_{\u03c1\u2208D} max_{\u039b\u2208D_\u039b} L(\u03c1, \u039b) = max_{\u039b\u2208D_\u039b} min_{\u03c1\u2208D} L(\u03c1, \u039b) = max_{\u039b\u2208D_\u039b} min_{\u03c0\u2208\u03a0} L(\u03c0, \u039b)"}, {"title": "Diffusion Models", "content": "In this section, we present implementation details on diffusion models. To solve the forward and reverse processes (3, 4), we follow the implementation of DDPM [21], which can be viewed as a discretization of VP SDE (3). As as result, the forward process (3) is implemented as follows:\n\u03b8_t = \u221a{1 \u2212 \u03b2_t} \u03b8_{t\u22121} + \u221a{\u03b2_t} z_t,\nwhere z_t \u223c N(0, I). To solve the reverse process (4), we utilize a error network \u03b5_\u03c6(\u03b8_t, t) = -\u221a{1 \u2212 \u03b1_t} s_\u03c6(\u03b8_t, t) instead of using s_\u03c6, where \u03b1_t = \u03a0_{t'=0}^{t-1} 1 \u2212 \u03b2_{t'}, and \u03b2_t follows the linear noise schedule. The error network is trained to minimize the loss J(\u03b8_0, t; \u03c6) := E_{\u03b5\u223cN(0, I)} || \u03b5 \u2212 \u03b5_\u03c6(\u221a{\u03b1_t} \u03b8_0 + \u221a{1 \u2212 \u03b1_t} \u03b5) ||^2. To futher accelerate the sampling, we apply DDIM sampling [32] as follows:\n\u03b8_{t\u2212\u03c4} = \\frac{1}{\u221a{1 \u2212 \u03b2_t}} (\u221a{\\frac{\u03b1_t}{\u03b1_{t\u2212\u03c4}}} \u03b8_t \u2212 \u221a{\\frac{\u03b1_t}{\u03b1_{t\u2212\u03c4}} \u2212 1} \u03b5_\u03c6(\u03b8_t, t))\nSince DDIM sampling (20) is deterministic, we can sample the environment parameter \u03b8 with T' denoising steps, which is less than the original diffusion timestep T."}, {"title": "Environment Critic Update", "content": "In this section", "follows": "nZ^{target"}, "i(\u03b8) = z_i w.p. \\frac{1}{K} \u03a3_{k=0}^{K-1} [\\frac{1}{\u0394} [1 \u2212 |\\frac{u_k \u2212 z_i}{\u0394}|"], "\u0394": "frac{(U_{max} - U_{min})}{M} is a width of each bin"}