{"title": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models", "authors": ["Weijie Chen", "Ting Bai", "Jinbo Su", "Jian Luan", "Wei Liu", "Chuan Shi"], "abstract": "Large language models with retrieval-augmented generation encounter a pivotal challenge in intricate retrieval tasks, e.g., multi-hop question answering, which requires the model to navigate across multiple documents and generate comprehensive responses based on fragmented information. To tackle this challenge, we introduce a novel Knowledge Graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing in KG-Retriever is constructed on a hierarchical index graph that consists of a knowledge graph layer and a collaborative document layer. The associative nature of graph structures is fully utilized to strengthen intra-document and inter-document connectivity, thereby fundamentally alleviating the information fragmentation problem and meanwhile improving the retrieval efficiency in cross-document retrieval of LLMs. With the coarse-grained collaborative information from neighboring documents and concise information from the knowledge graph, KG-Retriever achieves marked improvements on five public QA datasets, showing the effectiveness and efficiency of our proposed RAG framework.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) have achieved initial success in generating accurate responses, especially in knowledge-intensive tasks. By using a retrieval component to incorporate relevant information from a vast corpus of external documents, the RAG technique has become the mainstream way to alleviate hallucination issues in the response generation of LLMs (Yang et al., 2023; Ding et al., 2023). Nonetheless, when engaging in intricate retrieval tasks, e.g., multi-hop question answering, the model encounters significant difficulties in extracting pertinent answers solely from a single document. It requires the model to navigate across multiple documents and generate comprehensive responses according to the fragmented information from multiple relevant documents. For example, in the multi-hop question \u201cWhat are the trend and major factors contributing to dry eye syndrome in children, and what preventive measures can be taken?\u201d, most existing RAG-based LLMs inevitably suffer from incomplete retrieval knowledge due to the disability of reasoning over different documents. Recent RAG studies (Feng et al., 2024; Shao et al., 2023) attempt to disassemble the retrieval process into iterative retrieval steps by using the generated content from the last iteration as the query to retrieve relevant documents in the next round. Such approaches improve the inferential capabilities of LLMs and enhance the retrieval quality to some extent, but they still face the challenge of the escalating computational costs caused by multiple iterative retrieval steps. Besides, due to the discreteness of each iteration, such methods may still face poor retrieval performance in integrating information across different documents.\nTo enhance retrieval quality while maintaining RAG's efficiency, we propose a novel knowledge graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. Specifically, KG-Retriever is built based on a Hierarchical Index Graph (HIG) that consists of a knowledge graph layer and a collaborative document layer (as shown in Fig. 1). In the knowledge graph layer, entities and relations in a document are extracted by LLMs, which enhances the internal information structuring of individual documents. In the collaborative document layer, the document-level graph establishes connections based on their semantic similarity, improving the cross-document knowledge correlations. The entity-level and document-level information in HIG enable our KG-Retriever to leverage the associative nature of graph structures to strengthen intra-document and inter-document connectivity, thereby fundamentally augmenting the retrieval efficiency of LLMs.\nBased on the HIG, the retrieval process starts at the document layer and subsequently broadens to include information from neighboring documents. The indirect relevant information sourced from neighboring documents offers supplementary validation against potential false relevancy between queries and documents. Then, entity-level matching is conducted within the KG layer of candidate documents. The triplets in the KG layer of candidate documents further provide concise information to mitigate data noise and reinforce the logical coherence of retrieval outcomes through inter-entity linkages. The collaborative information in both the document layer and KG layer in KG-Retriever work together to provide more comprehensive information and exclude irrelevant information through two rounds of matching, leading to a marked improvement in the quality and credibility of generated content. Finally, the selected triplets in KG, combined with the original query, are fed into the large language models to generate the final responses. The contributions of our work are as follows:\n\u2022 We construct a novel retrieval indexing based on a Hierarchical Index Graph (HIG), which establishes intra-document and inter-document connectivity on the document layer and KG layer, enhancing the internal information structuring of individual documents and cross-document knowledge correlations.\n\u2022 We propose a hierarchical knowledge retriever (KG-Retriever) to facilitate retrieval-augmented LLMs. KG-Retriever is capable of leveraging the supplementary validation information from neighborhoods of different documents and reinforcing the logical coherence through inter-entity linkages, achieving marked improvement in the quality and credibility of generated content of LLMs.\n\u2022 We conduct extensive experiments on five representative open-domain QA datasets. Compared with RAG approaches with single or multi-iteration retrieval steps, our method achieves the SOTA model performance under a single retrieval setting and attains substantial advancements in terms of efficiency."}, {"title": "2 Related Work", "content": "In this section, we introduce the concepts related to our study briefly, including the naive RAG, the advanced RAG, and the graph-based RAG in the research area of LLMs."}, {"title": "2.1 Retrieval-Augmented Generation", "content": "Retrieval Augmented Generation (RAG) methodologies address the issue of hallucinations of large language models (LLMs) in knowledge-intensive tasks by supplementing them with relevant external information (Ding et al., 2024; Gao et al., 2023). This technique is segmented into three phases: indexing, retrieval, and generation. Naive RAG approaches (Lewis et al., 2020; Guu et al., 2020), as an archetype, initiates by splitting text into chunks and embedding each chunk into vector representations for indexing. Subsequently, in the retrieval stage, it matches the query's embedding to these vectorized chunks based on semantic similarity. In the final generation phase, the retrieved chunks, combined with the original query, serve as contextual input for LLMs to generate a refined response. Despite marked enhancements, it may lose advantages in complex multi-hop question-answering scenarios, as simple indexing and retrieval strategies inadequately integrate cross-document information, resulting in diminished retrieval accuracy and substandard response quality."}, {"title": "2.2 Advanced RAG", "content": "Recent studies have attempted to enhance retrieval quality by fine-tuning retrieval models with task-specific data (Shao et al., 2023) or by discriminating among retrieved contents through end-to-end fine-tuning of LLMs (Asai et al., 2023). However, such methods are difficult to apply in zero-shot settings, as the highly tailored fine-tuning process limits their ability to extend to unseen tasks or data. Efforts have also been made to enhance the interplay between retrieval and generation phases. For instance, ITRG (Feng et al., 2024) employs an iterative strategy where each iteration's output is used in the subsequent retrievals, enhancing both the generated content's quality and retrieval precision. IRCOT (Trivedi et al., 2022a) integrates retrieval with the Chain-of-Thought (CoT) approach, guiding retrieval by CoT and refining CoT with retrieved content. Despite their contributions, these"}, {"title": "2.3 Graph-based RAG", "content": "The integration of graphs with LLMs and the RAG technique has recently elicited significant attention, with plenty of directions being proposed (Peng et al., 2024). These include using LLMs for knowledge graph creation (Trajanoska et al., 2023; Edge et al., 2024), completion (Yao et al., 2023), and knowledge editing citeshi2024retrieval. The study Graph RAG (He et al., 2024; Kang et al., 2023) proposed a Graph RAG method to retrieve subgraphs for the field of Graph Question Answering (GraphQA). Recent studies have proposed similar concepts for enhancing the overall performance of RAG systems by creating graph structures. For instance, KGP (Wang et al., 2024) creates a knowledge graph over multiple documents with nodes symbolizing passages or document structures. Some researchers (Edge et al., 2024) have explored building graph-based texts to improve the performance of LLMs in Query-Focused Summarization.\nDifferent from the above methods, our work extracts entity relationships from all documents to construct an entity-level knowledge graph. Based on this knowledge graph, cross-document associations are constructed, forming a hierarchical graph structure. We propose a novel single-retrieval RAG framework with different retrieval strategies on the hierarchical index graph, achieving both the efficiency and accuracy of the RAG framework."}, {"title": "3 Methodology", "content": "The following sections introduce the overview architecture of KG-Retriever and the details of each component's implementation."}, {"title": "3.1 Overview Architecture", "content": "As shown in Figure 1, KG-Retriever consists of three components, i.e., the indexing construction component, the knowledge retrieval component, and the response generation component. The retrieval index is constructed on a Hierarchical Index Graph (HIG), which consists of a document graph and an entity graph. Both internal structure within documents and the broader interconnections across documents can be retrieved on HIG. In the knowledge retrieval component, two rounds of matching at the document layer and KG layer work together to provide more comprehensive information and exclude irrelevant information. Specifically, different retrieval strategies are used to obtain the relevant information in associated documents within the document-level graph. Subsequently, an entity-level matching process within these identified documents meticulously extracts KG triples that are closely related to the query. In the response generation component, the retrieved knowledge information from the KG, combined with the original question, serves as inputs to feed into the LLMs, generating precise responses."}, {"title": "3.2 KG-Retriever", "content": "The information retrieval process in KG-Retriever is conducted with a Hierarchical Index Graph (HIG), in which the associative nature of graph structures is used to strengthen intra-document and inter-document connectivity. The construction of HIG and retrieval strategies are introduced in the following sections."}, {"title": "3.2.1 HIG Construction", "content": "To enhance the coherence of intra-document information and inter-document informational relationships, we design a novel retrieval index structure based on a Hierarchical Index Graph (HIG). HIG consists of a Knowledge Graph (KG) layer and a collaborative document layer. The connection in the document layer improves knowledge correlations across documents. The information in the KG layer further enhances text comprehension and facilitates intricate intra-document information interactions.\nEntity-Level KG Construction. The entities in the documents are organized on a KG, in which the precise representation of entities and relationships enhances semantic comprehension and facilitates the exploitation of KG information. Considering that recent advancements in LLMs, such as the closed-source GPT-4 (Achiam et al., 2023) and open-source alternatives such as Llama (Touvron et al., 2023) and Qwen (Bai et al., 2023), show remarkable capabilities in understanding complex"}, {"title": "3.2.2 Retrieval Strategies", "content": "To achieve comprehensive information extraction that integrates both the macroscopic perspective of documents and the microscopic details of entities, we proposed three retrieval strategies built upon HIG that contain two stages: document-level retrieval and KG-level retrieval.\nDocument-level Retrieval. The matching process between the query and documents is based on their semantic similarity. We use the same pre-trained language model, e.g., Roberta-large, to encode the semantics of queries. For a query, the retrieval is conducted to identify the Top-N documents that are most similar to the semantic vector of the query. To address potential issues of spurious relevance inherent in direct semantic similarity matching, we collect the neighboring documents of the identified Top-N documents on the document graph into our final candidate set. While these neighboring documents may not be directly relevant to the query, their proximity to the highly pertinent Top-N documents suggests they may contain information indirectly related or supplementary to the query. This approach enriches the contextual understanding of the query, thereby enhancing the comprehensiveness and depth of the generated responses. Three distinct document-level collaboration strategies are proposed to enhance the retrieval process:\n\u2022 One-Hop Collaboration: This strategy integrates neighboring documents into the candidate set without discrimination, encompassing all one-hop neighbors to ensure broad applicability.\n\u2022 Attentive Collaboration: This strategy includes one-hop neighbors while incorporating an attentive filtering mechanism (controlled by \u03bb) to refine selections of triples in the KG layer and meanwhile enhance the relevance of the retrieved knowledge. Attention weights are computed as the cosine similarity of the two documents' semantic vectors.\n\u2022 Multi-Hop Collaboration: Going beyond one-hop neighbors, this strategy incorporates multi-hop neighbors and utilizes attention-based weights, i.e., multiply attention weights in different hops, to improve precision and coverage in retrieval tasks.\nThe three strategies enable our framework to be robust and adaptable to meet the different retrieval needs of tasks. The One-Hop Collaboration strategy has the fastest processing time and is suitable for most scenarios. The Attentive Collaboration strategy, by introducing the attention mechanism, better filters information and improves retrieval precision. The Multi-Hop Collaboration strategy, by expanding the receptive field, is capable of handling more complex tasks.\nKG-level Retrieval. After obtaining the related documents through the three collaboration strategies, we further match the entity nodes connected to these candidate documents. For an entity e in candidate documents, its related triples in KG are retrieved to generate the answer, denoted as:\nRetrieved?\n\\begin{equation}\n\\begin{cases}\n\\text{yes} & \\text{if } w * \\text{CosSim}(v_e, v_q) > \\lambda;\\\\\n\\text{no} & \\text{else,}\n\\end{cases}\n\\end{equation}\nwhere $v_e$ and $v_q$ are the semantic representations of the entity e and the query q respectively. $w$ is the attention weight of the three collaboration strategies in the document collaboration process. In One-Hop Collaboration, $w$ is set to 1, while in the attentive methods, $w$ is the attention weights computed by the semantic similarity of documents. $\\lambda$ is the threshold value to filter KG information, ensuring that only entities with attentive similarity exceeding this value are considered. We set the maximum of retrieved triples as T for efficiency consideration.\nAfter the document-level collaboration and KG-level collaboration, the retrieved triples in KG combine with the original query, serving as inputs to the LLMs for generating the response."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Settings", "content": "Datasets. To verify the effectiveness of KG-Retriever in intricate QA tasks, five open-domain question-answering datasets are used in our experiments, including HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022b), 2WikiMultilHopQA (Ho et al., 2020), CRUD-QA1, and CRUD-QA2 (Lyu et al., 2024). The evaluations on these datasets cover multiple perspectives: multilingual capacity (covering both Chinese and English), diverse question scenarios (multi-hop QA and single-hop QA tasks), and a range of response types (including both short-form and long-form generation tasks).\n\u2022 HotpotQA: The HotpotQA is a multi-hop English QA dataset, with a collection of related Wikipedia context and question-answer pairs. The answers in this dataset are short-form texts and the related Wikipedia contexts contain both positive relevant and negative relevant passages.\n\u2022 MuSiQue: The MuSiQue is designed to support the development and evaluation of models that perform multiple steps of reasoning to answer a question, with a collection of related context and question-answer pairs.\n\u2022 2WikiMultilHopQA: The 2WikiMultilHopQA is a multi-hop English QA dataset, with a collection of related Wikipedia context and question-answer pairs. The related Wikipedia contexts contain both positive and negative relevant passages.\n\u2022 CRUD-QA1 & CRUD-QA2: The CRUD datasets were constructed by crawling the latest high-quality news data from the mainstream news websites in China. Specifically, CRUD-QA1 is a single-hop QA dataset, while CRUD-QA2 is a multi-hop QA dataset. Both datasets contain a collection of related news context and question-answer pairs, where the answer is long-form texts.\nEvaluation Metrics. For HotpotQA, MuSiQue and 2WikiMultilHopQA datasets, we follow the settings in (Yang et al., 2018): use the collection"}, {"title": "Baselines.", "content": "We compare KG-Retriever with representative LLMs and RAG methods, including:\n\u2022 Naive LLM (Bai et al., 2023): It prompts LLMs to directly generate the final answer without retrieval.\n\u2022 LLM with CoT: It prompts LLMs to generate both the chain-of-thought (CoT) reasoning process and the final answer.\n\u2022 Graph-guided reasoning (Park et al., 2023): It proposes a graph-guided CoT prompting method that guides the LLMs to the correct answer with graph verification steps.\n\u2022 BM25: It uses the question as a query to retrieve N paragraphs by the traditional sparse retrieval method BM25.\n\u2022 DenseRetriever: It uses the question as a query to retrieve N paragraphs by the dense retrieval method. For English QA scenarios, we used Roberta-large as the retriever, while for Chinese QA scenarios, we used bge-base as the retriever.\n\u2022 ITRG (Feng et al., 2024): It employs an iterative strategy where each cycle's output informs subsequent retrievals, enhancing both the generated content's quality and retrieval precision.\n\u2022 ITER-RETGEN (Shao et al., 2023): It follows an iterative strategy, merging retrieved and generated contents in each iteration as a whole, thereby overcoming the shortcomings of traditional iterative strategy in information integration.\n\u2022 KGP (Wang et al., 2024): It segments documents into passages and uses KNN to build a graph. Then, it introduces an LLM as an agent to perform retrieval on the graph, determining the nodes to visit in the next iteration.\nThe above methods cover different kinds of RAG approaches in LLMs research: naive method directly generating answers without retrieval (Naive"}, {"title": "Implementation Details.", "content": "For each baseline method, a grid search is applied to find the optimal settings. We report the result of each method with its optimal hyperparameter settings. For hyperparameters in our model, the number of neighbors K in the document-level graph construction (see Eq. 1), the number of retrieved documents during Document-level Collaboration N, the max retrieved entity count T in KG-level Collaboration, and the retrieval threshold A for entity retrieval (see Eq. 2) and has been set as follows: {K=2, N=3, T=20, \u03bb=0.1} for HotpotQA, {K=3, N=3, T=30, X=0.1} for MuSiQue, {K=3, N=3, T=30, \u03bb=0.1} for 2WikiMultilHopQA, {K=1, N=3, T=10, \u03bb=0.4} for CRUD-QA1, {K=2, N=3, T=15, \u03bb=0.3} for CRUD-QA2. The code will be publicly available after the review process."}, {"title": "4.2 Main Results", "content": "We conduct experiments on five datasets to show the effectiveness of KG-Retriever in QA tasks. The experiments are conducted on the open-sourced multilingual large language model Qwen1.5-7b. As shown in Table 1, we can see that:\n(1) The LLMs with RAG significantly outperform the methods that do not leverage retrieval augmentation. LLMs incorporate techniques such as Chain-of-Thought (CoT) or graph reasoning, failing to break the inherent limitations of LLMs in knowledge-intensive QA tasks.\n(2) For the LLMs with the RAG technique, the advanced RAG methods like ITRG, ITER-RETGEN, and KGP perform better than the naive single-retrieval methods (e.g., BM25 and DenseRetriever), showing the necessary of using iterative retrieval steps to obtain supplementary information from multiple documents.\n(3) Our KG-Retrieval achieves state-of-the-art performance compared with all the baselines on five datasets. With only one retrieval step, our KG-"}, {"title": "4.3 Experimental Analysis", "content": "We conduct in-depth analyses of our method to quantitatively demonstrate its effectiveness, including ablation studies, the performance on different LLM backbones, and hyperparameter analyses."}, {"title": "4.3.1 Ablation Study", "content": "We conduct ablation studies of our framework to identify the key factors in KG-Retriever. The variants of our framework are:\n\u2022 w/o Entity Graph: it solely constructs the document-level graph with inter-document collaborative information.\n\u2022 w/o Document Graph: only entity-level knowledge graph is used to enhance the retrieval results.\nThe experimental results are shown in Table. 2, we can see that (1) modifications to the hierarchical graph structure led to a huge degradation"}, {"title": "4.3.2 Impacts of LLM Backbones", "content": "LLMs equipped with more parameters show greater capability to deal with complex tasks. Considering the potential influence of different LLM backbones on the model performance, we analyze the performance of our RAG framework on more powerful LLM backbone models, i.e., the Qwen-14b and GPT-4. As illustrated in Fig 2, we can observe that: (1) Compared with the experimental results on"}, {"title": "4.3.3 Hyperparameters Analysis", "content": "Our framework leverages hyperparameters to influence the retrieved information, including the number of collaborative documents K in Eq. 1, the maximum of retrieved triples T in Section 3.2, and the similarity threshold \u03bb in Eq. 2 for entity retrieval. In this section, we delve into analyses of the impacts of these parameters on the overall performance of our framework. The results are shown in Fig. 3, we can see that: (1) In subfigure (a), incorporating more documents improves the performance of our framework. After reaching a saturation point, the increment of neighboring documents may incorporate noise and result in degradations of model performance. (2) In subfigure (b), increasing the maximum number of retrieved triplets can provide richer contextual information and improve model performance. However, too many triplets may also introduce irrelevant information, negatively affecting the model performance. (3) In subfigure (c), adjusting the threshold for entity retrieval serves a dual purpose: it filters out irrelevant information, leading to performance enhancements. However, excessively stringent thresholds can inadvertently discard valuable information, thereby causing a decline in performance."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel retrieval-augmented generation framework, KG-Retriever. We construct an effective retrieval indexing on the the hierarchical index graph (HIG) and carefully design the corresponding retrieval strategies on it. Indexing from HIG, KG-Retriever is capable of capturing both intra-document and inter-document information, making it easily adaptive to retrieving precisely collaborative information in different QA tasks. Besides, KG-Retriever provides a way to obtain abundant knowledge in once retrieval process, largely improving the generation efficiency of LLMs."}, {"title": "6 Limitations", "content": "While KG-Retriever requires lower reasoning demands on the backbone model compared to Interactive RAG, it still necessitates that the backbone model possesses sufficient inferential capability to derive relevant information from the knowledge graph. Besides, although our indexing methodology enhances retrieval and inference efficiency, constructing the index itself requires computational resources. However, since this process can be completed offline, it does not affect real-time inference in real applications. In addition to QA tasks, we will make comprehensive model validation in other NLP domains, e.g., fact verification and commonsense reasoning, to verify the effectiveness of our RAG framework. Additionally, static indexing structures may not be optimal for dynamic corpus, future work will explore dynamic indexing structures to address this limitation."}]}