{"title": "XPath Agent: An Efficient XPath Programming Agent Based on LLM for Web Crawler", "authors": ["Yu Li", "Bryce Wang", "Xinyu Luan"], "abstract": "We present XPath Agent, a production-ready XPath programming agent specifically designed for web crawling and web GUI testing. A key feature of XPath Agent is its ability to automatically generate XPath queries from a set of sampled web pages using a single natural language query. To demonstrate its effectiveness, we benchmark XPath Agent against a state-of-the-art XPath programming agent across a range of web crawling tasks. Our results show that XPath Agent achieves comparable performance metrics while significantly reducing token usage and improving clock-time efficiency. The well-designed two-stage pipeline allows for seamless integration into existing web crawling or web GUI testing workflows, thereby saving time and effort in manual XPath query development. The source code for XPath Agent is available at https://github.com/eavae/feilian.", "sections": [{"title": "1 Introduction", "content": "Web scraping [6] automates data extraction from websites, vital for modern fields like Business Intelligence. It excels in gathering structured data from unstructured sources like HTML, especially when machine-readable formats are unavailable. Web scraping provides real-time data, such as pricing from retail sites, and can offer insights into illicit activities like darknet drug markets.\nThe advent of HTML5 [13] has introduced significant complexities to automated web scraping. These complexities stem from the enhanced capabilities and dynamic nature of HTML5, which require more sophisticated methods to accurately extract and interpret data. To address these challenges, researchers have developed a variety of tools, such as Selenium[11]. Which offers a set of application programming interfaces (APIs) to automate web browsers, enabling the extraction of data from web pages. However, program Selenium to extract data from web pages is a time-consuming and error-prone process. The most crucial part is how to locate the target information on the web page. XPath queries provide a solution to this problem by allowing users to navigate the HTML structure of a web page and extract specific elements. But, programming XPath queries is a challenging task, especially for non-technical users.\nThe development of Large Language Models (LLM) has emerged as a promising avenue. LLMs, with their advanced natural language processing capabilities, offer a new paradigm for understanding and interacting with web content. AutoWebGLM[7] demonstrated significant advancements in addressing the complexities of real-world web navigation tasks, particularly in simplifying HTML data representation to enhancing it's capability. By leveraging reinforcement learning and rejection sampling, AutoWebGLM enhanced its ability to comprehend webpages, execute browser operations, and efficiently decompose tasks.\nInstead of one time task execution, AutoScraper[5] adopt a simplified technique which only involves text content of webpages. By focusing on the hierarchical structure of HTML and traversing the webpage, it construct a final XPath using generated action sequences. Such XPath is generalizable and can be applied to multiple webpages. Which significantly reduce the time required when execution.\nBut, the above approaches are not efficient in generating XPath queries. We introduced a more effective approach to generate XPath queries using LLMs which could simply integrate into existing web crawling or web GUI testing workflows."}, {"title": "1.1 Motivation", "content": "We assuming there are 3 core reasons why LLMs are not efficient in generating XPath queries. Firstly, LLMs are not designed to generate XPath queries. Secondly, web pages are lengthy and complex, full of task unrelated information. Those information distract LLMs from generating the correct XPath queries. Thirdly, LLMs are context limited. A good XPath query should be generalizable across different web pages. However, LLMs can only generate XPath queries based on the context they have seen. So, a shorter and more task-related context is more likely to generate a better XPath query.\nBased on the above insights, we propose a novel approach to generate XPath queries using LLMs. We aim to reduce the number of steps required to generate a well-crafted XPath query, reduce the computational overhead, and improve the generalizability of the XPath queries generated by LLMs.\nIn order to increase the efficiency of XPath query generation, we also employed LangGraph. Which is a graph-based tool set which we can define the whole pipeline in a graph-based manner and execute it in parallel. Which significantly reduce the time to generate XPath queries."}, {"title": "1.2 Our Contributions", "content": "In summary, our contributions are as follows:\n1. We designed a two stage pipeline, which we can employ a weaker LLM to extract target information. And a stronger LLM to program XPath.\n2. We proposed a simple way to prune the web page, which can reduce the complexity of the web page and make the LLM focus on the target information.\n3. We discovered that extracted cue texts from 1st stage significantly improve the performance of the 2nd stage.\n4. We benchmarked our approach against a state-of-the-art same purpose agent across a suite of web crawling tasks. Our findings reveal that our approach excels in F1 score with minimal compromise on accuracy, while significantly reducing token usage and increase clock-time efficiency."}, {"title": "2 Related Work", "content": "Information extraction (IE) serves as a cornerstone of natural language processing, transforming unstructured text into structured entities, relations, and events. Traditional IE approaches often relied on task-specific models, which demanded substantial annotated corpora and computational resources. The advent of large language models (LLMs), particularly GPT-based architectures, has revolutionized this field. LLMs excel in zero-shot and few-shot learning due to pretraining on vast text corpora, enabling efficient and flexible IE for diverse and complex applications."}, {"title": "2.1 Web Information Extraction", "content": "Web information extraction (WebIE) has evolved alongside the increasing complexity of web layouts and dynamic content. Traditional rule-based methods, effective for static pages, struggle with modern web structures. LLMs and advanced frameworks have emerged as solutions, leveraging schema-based, multimodal, and generative techniques to unify tasks like Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE) through prompt engineering and fine-tuning [17].\nRecent works emphasize the importance of modeling structural context in webpages. WebFormer [14] employs graph attention mechanisms to capture local and global dependencies within the DOM tree, achieving state-of-the-art results on the SWDE benchmark. Similarly, FreeDOM [9] combines node-level encoding with pairwise relational features, enabling robust generalization across unseen websites without visual rendering. Simplified DOM representations, such as those used in SimpDOM [19], enhance feature transferability across verticals, achieving superior cross-domain performance while avoiding reliance on computationally expensive visual features.\nGenerative approaches further demonstrate the adaptability of LLMs in WebIE. ChatIE [15] reformulates extraction tasks into multi-turn dialogues, improving zero-shot capabilities. Frameworks like NeuScraper [2] integrate neural models with HTML structures to enhance data quality, while MIND2WEB [3] extends these innovations with a dataset and framework for training generalist agents capable of performing diverse tasks across real-world websites.\nOn the multimodal front, models like GoLLIE [12] and MOEEF [18] integrate visual and textual data to address domain-specific challenges. GOLLIE aligns annotation guidelines with code generation for better extractions, while MoEEF incorporates hypertext features to boost entity recognition. Vision-based LLMs, such as GPT-4 Vision[4], highlight the growing importance of multimodal techniques for processing visually encoded web data."}, {"title": "2.2 LLMs and XPaths for Web Information Extraction", "content": "XPath, a language for querying hierarchical document structures, plays a vital role in bridging structured representation with computational modeling. MarkupLM [8] demonstrates how XPath captures the genealogy of DOM nodes, seamlessly integrating textual and markup features for tasks such as Masked Markup Language Modeling (MMLM) and Node Relation Prediction (NRP). These techniques enable dynamic adaptation to complex web layouts, surpassing the limitations of 2D positional embeddings.\nXPath generation further extends WebIE capabilities. Tools like TREEX [10] and AUTOSCRAPER[5] employ decision tree learning and progressive HTML traversal to generate reusable extractors, balancing precision and recall for diverse applications such as price comparison and product aggregation. By leveraging the interpretive capabilities of LLMs, these methods scale to diverse web environments, underscoring the synergy between structured document analysis and scalable information extraction."}, {"title": "2.3 Summary", "content": "The evolution of WebIE reflects the integration of structural, multimodal, and generative techniques into scalable systems. LLMs and frameworks leveraging DOM structures, multimodal data, and XPath queries have significantly advanced this field, enabling robust and adaptable solutions for dynamic web environments. These advancements collectively highlight the transformative potential of combining pre-trained models, structural embeddings, and domain-specific optimizations in tackling modern WebIE challenges. Future directions should focus on enhancing generalization across unseen web domains and improving multimodal fusion for visually rich webpages efficiently."}, {"title": "3 Methodology", "content": "In this section, we present the methodology of our approach, which consists of two stages: Information Extraction (IE) and XPath Programming. The IE stage extracts target information from sanitized web pages, while the XPath Programming stage generates XPath queries based on the condensed html by extracted information. The whole process based on seeded web pages, which is 3 in our implementation. Figure 1 illustrates the two-stage pipeline of XPath Agent. For each stage, we provide a detailed description of the process and the algorithms used."}, {"title": "3.1 Information Extraction with Cue Text", "content": "The Information Extraction (IE) stage aims to extract target information. Not like traditional IE, we discovered 2 key insights. Firstly, we prompt LLM not only extract questioned information but also cue texts. Secondly, we sanitized the web page to reduce the complexity of the web page and make the LLM focus on the target information with contextual semantic.\nCue texts are the indicative texts that signals the upcoming target information. For example, for \"price: $100.00\", the cue text is \"price:\". Those texts are important in some case, especially when no way or hard to directly programming XPath queries to extract target information \"$100.00\". In such case, treat \"price:\" as an anchor, using XPath's ancestor, sibling, or descendant axis traverse to the target information is the only way. In order to let the context still be condensed, we prompt LLM to response cue texts simultaneously.\nSanitizing web page is a process to remove unnecessary information from a page. In HTML, the most meaningful parts are texts and tags. The texts are the target information we want to extract, and the tags are the structure of the web page which tells the relationship between texts especially the priority of which answer is more likely to be the target information. The purpose of sanitizing the web page is to reduce the complexity of the web page and make the LLM focus on the target information. We designed an algorithm to sanitize the web page, which is shown in Algorithm 1. We also employed minify-html[16] to further reduce the size of the web page.\nThe algorithm 1 traverse the HTML tree in a depth-first manner. It removes the invisible or empty nodes, and all attributes. It's efficient and can be easily implemented in any programming language. In our sampled web pages, it can help us reduce the size of the web page to 10% 20% on average.\nIn our implementation, we prompt LLM to extract all information at once on a single page with JSON format. So, multiple fields or multiple values for a single field might be extracted. We treat all extracted values are relevant and passing them to the next stage. The prompt for the Information Extraction stage is shown in Section A."}, {"title": "3.2 Program XPath", "content": "Program XPath queries is a process to generate XPath queries based on the condensed html by extracted information. In order to let LLM have more context to program a robust XPath query, we condensed the html by the extracted information and prompt with 3 seeded web pages at once. The algorithm is shown in Algorithm 2.\nThe condenser based on the extracted information, which is the target information and cue texts. A distance function is used to identify the most relevant nodes so that we can keep them in the condensed html. During the condense process, we keep the target nodes and replace other nodes' children with \"...\"."}, {"title": "3.3 Static XPath Generation", "content": "Program a robust XPath query is a challenging task. Which requires to balance between rigidity and flexibility. The rigidity means the XPath query should strictly follow the specific structure of the web page. The flexibility means the XPath query should be pruned to be generalizable across different web pages. In our early experiments, we discovered that the XPath query generated by LLM is not sticky to the structure of the web page. So, we designed a static XPath generation algorithm to guide LLMs.\nThe static XPath generation algorithm propagate from the target node to root node. It generates the XPath query in a bottom-up manner. Unlike naive XPath generation algorithm. We add more attributes (in our case, we include class and id) to the XPath query. It makes the XPath query richer."}, {"title": "3.4 Conversational XPath Evaluator", "content": "LLM are lacks of the environment to evaluate, correct or improve the XPath queries. So, we designed a evaluator to evaluate LLM generated XPath.\nThe XPath evaluator is a function, which execute the XPath query on seeded web pages and feedback the result to LLM. The result include 3 parts: what are missing, what are redundant, and correctness of the XPath query. The LLM can based on feedback to improve the XPath query.\nIn our implementation, we limited the number of feedback loop to 3. At the end of the loop, we take the best XPath query based on evaluation result as the final result. The prompt for the Program XPath stage is shown in Section B."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.1.1 Models", "content": "We use DeepSeek and ChatGPT-4.0 as the primary large language models in our experiments."}, {"title": "4.1.2 Dataset", "content": "We use the SWDE[1] (Structured Web Data Extraction) dataset, which includes 90 of websites across 9 domains, in total 20414 web pages."}, {"title": "4.1.3 Experimental Parameters", "content": "We use DeepSeek-Chat as the main model for our XPath Agent. The Number of Seeds is 3 initial seeds are provided to guide query generation with a sample"}, {"title": "4.2 Evaluation Metrics", "content": "For evaluation, we employ the metrics of precision, recall, F1 score, and accuracy. we utilized a set matching method to calculate the metrics for multi-label classification tasks where the labels are unordered. This involves comparing each predicted label set with the corresponding ground truth label set while disregarding the order of the labels. First, Both the ground truth and predictions are converted into sets to ignore any order. Accuracy is then defined such that a prediction is counted as correct if the predicted set exactly matches the ground truth set. We count correctly predicted labels as True Positives. Incorrectly predicted labels that are not in ground truth are classified as False Positives, while the labels that should have been in predicted set but were missed are designated as False Negatives. Additionally, since our data does not contain blank answers (with only a very few exceptions that have been removed), our cases do not have true negatives. Finally, precision, recall, F1 score, and accuracy are calculated by the following formula.\npresision = $\\frac{Ture Positives}{TurePositives+FalsePositives}$\nrecall = $\\frac{Ture Positives}{True Positives+False Negatives}$\nF1 = $\\frac{2\\timesprecision \\times recall}{precision+recall}$\naccuracy = $\\frac{True Positives}{True Positives+FalsePositives+False Negatives}$"}, {"title": "5 Results and Analysis", "content": null}, {"title": "5.1 Statistical Analysis", "content": "DeepSeek is strong in precision, making it good at avoiding irrelevant data. GPT 40 is highly effective at capturing a wide range of relevant content, ensuring that few important elements are missed. Claude 3.0 strikes a balanced approach, effectively combining both precision and recall for solid overall performance of F1 score. Claude 3.5 stands out as the most balanced and effective model, excelling across all key areas of accuracy, precision, recall, and overall performance. It provides the best mix of identifying relevant data while minimizing errors."}, {"title": "5.2 Comparative Analysis", "content": "The AutoCrawler [5] framework focuses on generating web crawlers for extracting specific information from semi-structured HTML. It is designed with a two-phase approach: the first phase uses a progressive generation framework that leverages the hierarchical structure of HTML pages, while the second phase employs a synthesis framework that improves crawler performance by learning from multiple web pages.\nIn comparison to XPath Agent, AutoCrawler presents a different approach, emphasizing a sequence of XPath actions rather than just the extraction of XPath from snapshots. This difference may influence performance in various metrics, such as F1 score. AutoCrawler's focus on refining action sequences based on learning from past errors might offer advantages in terms of robustness and adaptability to dynamic web structures. However, your XPath Agent, by isolating XPath extraction tasks, might achieve greater precision in structured environments where precise element identification is crucial."}, {"title": "5.3 Error Analysis", "content": "TODO"}, {"title": "6 Conclusion", "content": "Third level headings must be flush left, initial caps and bold. One line space before the third level heading and 1/2 line space after the third level heading.\nFourth Level Heading\nFourth level headings must be flush left, initial caps and roman type. One line space before the fourth level heading and 1/2 line space after the fourth level heading."}]}