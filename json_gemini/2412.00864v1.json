{"title": "Explicit and data-Efficient Encoding via Gradient Flow", "authors": ["Kyriakos Flouris", "Anna Volokitin", "Gustav Bredell", "Ender Konukoglu"], "abstract": "The autoencoder model typically uses an encoder to map data to a lower dimensional latent space and a decoder to reconstruct it. However, relying on an encoder for inversion can lead to suboptimal representations, particularly limiting in physical sciences where precision is key. We introduce a decoder-only method using gradient flow to directly encode data into the latent space, defined by ordinary differential equations (ODEs). This approach eliminates the need for approximate encoder inversion. We train the decoder via the adjoint method and show that costly integrals can be avoided with minimal accuracy loss. Additionally, we propose a 2nd order ODE variant, approximating Nesterov's accelerated gradient descent for faster convergence. To handle stiff ODEs, we use an adaptive solver that prioritizes loss minimization, improving robustness. Compared to traditional autoencoders, our method demonstrates explicit encoding and superior data efficiency, which is crucial for data-scarce scenarios in the physical sciences. Furthermore, this work paves the way for integrating machine learning into scientific workflows, where precise and efficient encoding is critical.", "sections": [{"title": "Introduction", "content": "Auto-encoders are widely successful in supervised learning due to their ability to learn lower-dimensional representations of input data, enabling efficient computation [15]. The basic idea is that when sufficient correlation exists within the input data, the latent dimension can generate a model of the input. However, since the encoder is not directly learned, the encoding process can be semi-arbitrary, leading to suboptimal latent space representations and inefficient learning [11]. To address this, efforts have been made to directly regularize the latent space [14]. Typically, the encoder approximates the inverse of the decoder, but this requires learning additional parameters, impeding learning, particularly with limited data. Flow models [4, 6] attempt to resolve this by using invertible maps, but they are restricted to equi-dimensional latent spaces. What if we could eliminate the encoder, retain the advantages of a lower-dimensional latent space, and directly optimize the representation? This could enable faithful reconstruction with fewer samples and iterations.\nCurrently, DeepSDF [11] optimizes latent space representations using a continuous signed distance function to model 3D shapes, where a latent vector z represents the shape encoding. Hamiltonian Neural Networks [8] and VampPrior [13] introduce physically-informed dynamics and more ex-pressive priors for VAEs, respectively, contributing to latent space optimization. SAVAE [9] and the energy-based model approach in [10] directly aim to optimize the encoder through iterative refinement.\nIn this work, we propose a novel encoding method, namely gradient flow encoding (GFE). This decoder-only method explicitly determines the optimal latent space representation at each training step via a gradient flow, namely, an ordinary differential equation (ODE) solver. The decoder is updated by minimizing the loss between the input image and its reconstruction from the optimal latent space representation. Although slower, this method converges faster and demonstrates superior data efficiency and ultimately achieves better reconstructions with a minimal number of training samples compared to a conventional auto-encoder (AE). A notable advantage of GFE is the reduction in network size, as it eliminates the need for an encoder.\nUsing a gradient flow for encoding can be computationally demanding. ODE solvers with adaptive step sizes aim to minimize integration error, which is crucial for accuracy. However, these solvers can slow down training when the ODE becomes stiff, as step sizes shrink and make integration time-consuming [3, 7]. This is especially problematic in GFE-based training. We find that the exact gradient flow path may be less important than reaching convergence, so minimizing integration error may not be optimal. Fixed step sizes also cause poor convergence and stability in decoder training. To address this, we develop an adaptive solver that adjusts each step to minimize the loss between input and output. We also introduce a loss convergence assertion in the adaptive minimize distance (AMD) solver, making gradient flow more computationally feasible for neural networks.\nFor optimizing the latent space and decoder in GFE, we implement an adjoint method [2], which has also been used in other recent works [3]. To improve efficiency, we demonstrate that a full adjoint method may not always be necessary, and an approximation can be effectively utilized. Moreover, we introduce a Nesterov second-order ODE solver, which accelerates convergence for each training data size. Ultimately, the approximate GFE utilizing AMD (GFE-amd) is employed for testing and comparison with a traditional AE solver. Our flow-inspired model offers a solution for the stringent data efficiency and robustness demands in fields like physics, astronomy, and materials science. By utilizing the flow-model for encoding, we contribute to algorithms that create interpretable, accurate predictive models, enabling more robust discoveries and insights in scientific research."}, {"title": "Method", "content": "An auto-encoder maps an input y to a lower-dimensional latent space z via an encoder E, then reconstructs it using a decoder D, with E and D acting as approximate inverses. During training, each sample is mapped to z by E, reconstructed by D, and the reconstruction error is minimized with respect to both networks' parameters. At any point, an optimal latent representation z* minimizes the reconstruction error, but the encoder doesn't directly map to z*. Instead, E(z) is updated to make its reconstruction closer to the sample, which may not be efficient. Alternatively, z* can be found and used to update the decoder directly.\nDetermination of $z^{*}$ for each sample y can be formulated as an optimization problem: $z^{*}=arg \\min_{z} l(y, D(z, \\theta))$, where $\\theta$ represents the parameters of the decoder network and $l(., .)$ is a distance function between the sample and its reconstruction by the decoder, which can be defined based on the application. One common form is the $L_{2}$ distance $||D(z, \\theta) - y||_{2}$. The optimization can be achieved by gradient descent minimization. To integrate this minimization into the training of the decoder network, a continuous gradient descent algorithm is implemented via the solution to an ODE:\n$\\frac{dz}{dt} = -\\alpha(t)\\nabla_{z}l(y, D(z(t), \\theta)), z(0) = 0$,\nwhere time t is the continuous parameter describing the extent of the minimization, and $\\alpha(t)$ is a scaling factor that can vary with time. When the extremum is reached, z reaches a steady state. In"}, {"title": "Nesterov's 2nd-Order Accelerated Gradient Flow", "content": "The gradient flow described above is based on naive gradient descent, which may be slow in convergence. The convergence per iteration can be further increased by considering Nesterov's accelerated gradient descent. A second-order differential equation approximating Nesterov's accelerated gradient method has been developed in [12], and additionally incorporated into Adam [5]. This 2nd order ODE for z is given by:\n$\\frac{d^{2}z}{dt^{2}} + \\frac{3}{t} \\frac{dz}{dt} + \\nabla_{z}l(y, D(z, \\theta)) = 0$,\nwith the initial conditions $\\frac{dz}{dt}|_{t=0} = z(0) = 0$. To incorporate this within the gradient flow encoding framework, we split the 2nd order ODE into two interacting 1st order equations and solve them simultaneously. Specifically, we solve:\n$\\frac{dv}{dt} = \\frac{3}{t+\\epsilon}v + \\nabla_{z}l(y, D(z, \\theta)), \\frac{dz}{dt} = v$,\nwhere $\\epsilon$ ensures stability at small t."}, {"title": "Adaptive minimise distance solver", "content": "Step size is crucial in discretized optimization algorithms for reaching a local extremum, including solving the gradient flow equation for optimal latent space representation. Fixed time-step solvers can cause instabilities during decoder training due to stiffness, as predefined time slices may not adapt to rapid changes in the gradient flow, see App. A.2. For example, a 4th order Runge-Kutta method with a fixed grid uses dt slices in a logarithmic series to manage variations near t = 0, but this can still lead to instability, particularly in forward and backward passes. While adaptive step-size ODE solvers can theoretically address these issues, stiffness in the gradient flow equation remains a challenge.\nAdaptive step-size ODE solvers address stiffness in gradient flow equations but prioritize accurate integration, which isn't always useful for decoder training. A more effective approach minimizes loss at each step, regardless of the integration path. To achieve this, we develop an adaptive step-size method resembling an explicit ODE solver, like the forward Euler method, but without a fixed grid. The challenge is solving ODE 1 while selecting time-steps that reduce $l(y, D(x(t), \\theta))$. Essentially, this is gradient descent with adaptive step-size selection [1]. In this framework, $\\alpha(t)$ is redundant and can be set to 1, with the time-step $\\delta t$ taking over its role.\nIn the AMD method, at each time $t_{n}$, $\\delta t_{n}$ is chosen by finding the smallest m = 0, 1, ... that satisfies:\n$l(y, D(z(t_{n}) - \\beta^{m}s_{n}\\nabla_{z}l(y, D(z(t_{n}),\\theta)), \\theta)) < l(y, D(z(t_{n}), \\theta))$,\nwith $\\beta \\in (0,1)$ (set to 0.75 in our experiments) and $s_{n}$ as a scaling factor. At each time point $t_{n}$, the time-step is set as $\\delta t_{n} = \\beta^{m}s_{n}$. The scaling factor is updated at each iteration as $s_{n} = max(\\kappa s_{n-1}, s_{0}), s_{n} = min(s_{n}, s_{max}), s_{max} = 10, s_{0} = 1, \\kappa = 1.1$. Based on this, $t_{n+1} = t_{n} + \\delta t_{n}$ and\n$z(t_{n+1}) = z(t_{n}) - \\delta t_{n}\\nabla_{z}l(y, D(z(t_{n}), \\theta))$.\nIf the chosen time-step goes beyond $\\tau$, a smaller step is used to reach $\\tau$ exactly. The solution of the integral of Eq. 1 is then $z(\\tau)$. Furthermore, the AMD solver monitors the gradient of the convergence curve to determine if the loss function is sufficiently optimized, allowing it to assign a new final $\\tau'$ and stop early to avoid unnecessary integration."}, {"title": "Results and Discussion", "content": "A direct comparison of GFE-amd to a conventional AE for MNIST training is shown in Fig. 1. Further experimental details can be found in App. B. The x-axis shows the number of training images processed, not iterations. The training uses mini-batch data with replacement, revisiting the data multiple times. GFE-amd demonstrates significantly better learning per image, nearly converging at 800,000 images, see Fig. 1 left. This is a consequence of efficient latent space optimization. However, this comes with higher computational cost per iteration due to the ODE solver, see Fig. 1 right. Both models use the Adam optimizer, therefore the difference can be attributed to better gradients the GFE-amd model generates to update the decoder network at each training iteration.\nThis increase in computation is not necessarily a disadvantage, considering the efficient learning of the GFE-amd method. In Table 1 right, the average cross entropy loss for a complete test-set is recorded for both methods for some small number of training images. The GFE-amd is able to learn quite well even after seeing a tiny fraction of the total training data. Furthermore, the GFE-amd method noticeably improves an AE trained decoder when it is used to test, the result of an optimized latent space even without a network parameters update.\nTo verify the overall quality of the method both the AE and GFE-amd are tested when converged as shown in Table 1 left. The GFE-amd performs very similar to AE both for MNIST, Segmented MNIST (SegMNIST) and FMNIST. It is worth noting that the GFE-amd trainings are on average converged at 1/12th of the number of iterations relative to the AE. For the SegMNIST the networks are fully trained while seeing only the first half (0-4) of the MNIST labels and they are tested with the second half (5-9) of the labels. The GFE-amd shows a clear advantage over the AE emphasizing the versatility of a GFE-amd trained neural network.\nThe interpretable nature of the proposed method inherently supports scalability, suggesting that it will remain data-efficient even when applied to real-world or large-scale datasets."}, {"title": "Conclusions", "content": "To this end, a gradient flow encoding, decoder-only method was investigated. The decoder-dependent gradient flow searches for the optimal latent space representation, which eliminates the need for an approximate inversion. The full adjoint solution and its approximation are leveraged for training and compared in various settings. Furthermore, we present a 2nd order ODE variant to the method, which approximates Nesterov's accelerated gradient descent, with faster convergence per iteration. Additionally, an adaptive solver that prioritizes minimizing loss at each integration step is described and utilized for comparative tests against the autoencoding model. While each training step takes longer, the gradient flow encoding converges faster and demonstrates much higher data efficiency than the autoencoding model. This flow-inspired approach opens up new possibilities for efficient and robust data representations in scientific fields such as physics and materials science, where accurate modeling and data efficiency are paramount."}, {"title": "A Method details", "content": null}, {"title": "A.1 The adjoint method for the gradient flow", "content": "As described above, after finding, $z^{*} = z(\\tau) = arg_{z} \\min l(y, D(z, \\theta))$ the total loss is minimized with respect to the model parameters. The dependence of $z^{*}$ to $\\theta$ creates an additional dependence of $L(\\theta)$ to $\\theta$ via $z^{*}$. For simplicity, let us consider the cost of only one sample y, effectively $l(y, D(z^{*}, \\theta))$. We will compute the total derivative $\\delta l(y, D(z^{*}, \\theta))$ for this sample, and the derivative of the total cost for a batch of samples can be computed as the sum of the sample derivatives in the batch. The total derivative $\\delta l(y, D(z^{*}, \\theta))$ is computed as\n$\\delta l(y, D(z^{*}, \\theta)) = \\delta_{\\theta} l(y, D(z^{*}, \\theta)) + \\delta_{z^{*}} l(y, D(z^{*}, \\theta))d_{\\theta}z^{*}$.\nThe derivative $\\delta_{z^{*}} l(y, D(z^{*}, \\theta))d_{\\theta}z^{*}$ can be computed using the adjoint method and leads to the following set of equations\n$\\frac{dz}{dt} = -\\alpha(t) \\nabla_{z}l(y, D(z(t), \\theta)), with z(0) = 0$\n$\\frac{d\\chi}{dt} = -\\chi(t)^{\\top}\\nabla_{z}^{2}l(y, D(z(t), \\theta)), with \\chi(\\tau) = -\\nabla_{z}l(y, D(z(\\tau), \\theta))$\n$\\delta_{\\theta}l(y, D(z^{*}, \\theta)) = \\delta_{\\theta}l(y, D(z^{*}, \\theta)) - \\int_{0}^{\\tau} \\chi(t)^{\\top} d_{\\theta z}l(z(t), \\theta)dt$,\nwhere we used $z^{*} = z(\\tau)$. Equations [8-10] define the so called adjoint method for gradient flow optimization of the loss. Due to the cost of solving all three equations, we empirically find that for this work sufficient and efficient optimization can be accomplished by ignoring the integral (\u201cadjoint function\") part of the method. Theoretically, this is equivalent to ignoring the higher order term of the total differential $\\delta l(y, D(z^{*}, \\theta)) = \\delta_{\\theta}l(y, D(z^{*}, \\theta)) + \\delta_{z^{*}}l(y, D(z^{*}, \\theta))d_{\\theta}z \\approx \\delta_{\\theta}l(y, D(z^{*}, \\theta))$.\nReducing Equations [8-10] to:\n$\\frac{dz}{dt} = -\\alpha(t) \\nabla_{z}l(y, D(z(t), \\theta)), with z(0) = 0$\n$\\delta_{\\theta}L = \\delta_{\\theta}l(y, D(z^{*}, \\theta))$,\ni.e. optimise the latent space via solving an ordinary differential equation and minimise the loss \u201cnaively\u201d with respect to the parameters ignoring the dependence of $z(\\tau)$ to $\\theta$.\""}, {"title": "A.2 Fixed grid ODE solver", "content": "Being an ODE, the gradient flow can be solved with general ODE solvers. Unfortunately, generic adaptive step size solvers are not useful because the underlying ODE becomes stiff quickly during training, the step size is reduced to extremely small values and the time it takes to solve gradient flow ODEs at each iteration takes an exorbitant amount of time. Fixed time-step or time grid solvers can be used, despite the stiffness. However, we empirically observed that these schemes can lead to instabilities in the training, see Fig. 2. To demonstrate this, we experimented with a 4th order Runge-Kutta method with fixed step size. The dt slices are predefined in logarithmic series such as $\\delta t$ is smaller closer to t = 0, where integrands, $-\\nabla_{z}l(y, D(z, \\theta))$, are more rapidly changing. Similarly, $\\alpha$ is empirically set to $e^{(-2t/\\tau)}$ to facilitate faster convergence of z. For the GFE full adjoint method, the integrands for each time slice are saved during the forward pass so they can be used for the calculation of the adjoin variable $\\chi$ in the backward pass. We used the same strategy for both basic gradient flow and the 2nd order model."}, {"title": "B Experiments on the proposed methods", "content": "For training with MNIST and FashionMNIST datasets, we implement a sequential linear neural network. The decoder network architecture corresponds to four gradually increasing linear layers, with ELU non-linearities in-between. The exact reverse is used for the encoder of the AE.\nThe network training is carried out with a momentum gradient decent optimiser (RMSprop), learning rate 0.0005, $\\epsilon = 1 \\times 10^{-6}$ and $\\alpha = 0.9$. The GFE and AE are considered trained after one epoch and twelve epochs respectively.\nInitially a relative comparison between the full adjoint and the approximate fixed grid GFE methods is carried out to assess the relevance of the higher order term. Specifically, we carry out MNIST experiments for a fixed network random seed, where we trained the Decoder using the different GFE methods and computed cross entropy loss over the validation set. The proper adjoint solution requires Equations 8 and 9 to be solved for each slice of the integral in Eq. 10. Given N time-slices (for sufficient accuracy N \u2248 100), this requires O(5N) calls to the model D for each training image. The approximate method as in Equations 11 and 12 requires only O(N) passes. From Fig. 2 (left) it is evident that the 5-fold increase in computational time is not cost-effective as the relative reduction in loss convergence per iteration is not significant.\nFurthermore, to increase convergence with respect to training data, the accelerated gradient flow 2nd order GFE is implemented in Section 2.1. From Fig. 2 (right), the accelerated gradient method increases initially the convergence per iteration relative to GFE, nevertheless it is slightly more computationally expensive due to solving a coupled system. Additionally, from the same Fig. certain stability issues are observed for both GFE and second order GFE methods later on despite the initial efficient learning. In order to guarantee stability, the GFE-amd method is implemented as explained in Section 2.2. The black curve in Fig. 2 (right) shows a clear improvement of the GFE-amd over the later methods. Importantly, this result is robust to O of the experiment."}, {"title": "C Further results", "content": "Sample test-set reconstructions with a fixed network random seed for GFE-amd and AE methods are shown in Fig. 3. From Fig. 3 (a) it is evident that the GFE-amd is superior in producing accurate reconstructions with the very limited amount of data. Fig. 3 (b) indicates that both GFE-amd and AE generate similar reconstructions when properly trained."}]}