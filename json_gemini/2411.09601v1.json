{"title": "Accelerating Knowledge Graph and Ontology Engineering with Large Language Models", "authors": ["Cogan Shimizu", "Pascal Hitzler"], "abstract": "Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.", "sections": [{"title": "1. Introduction", "content": "Knowledge Graph and Ontology Engineering (KGOE, in short) refers to a (vaguely defined) set of tasks that are of central relevance to the life cycle of knowledge graphs, and of ontologies, as data artifacts used in data management and applications. These include, for example, ontology modeling (i.e., construction), ontology population (i.e., creating a knowledge graph with the given ontology as schema), ontology extension and modification, ontology alignment, entity disambiguation (sometimes called co-reference resolution).\nAll of the just mentioned tasks have in common that they are hard, in the sense that even after more than a quarter century of Semantic Web research, they still defy attempts to automate them at reasonable quality levels and scale. The state of the art on all of these is that they require significant human expert labor, at times (such as for entity disambiguation) together with detailed scripting of algorithms that solve the problem at scale but only for a specific problem instance, i.e., for a very specific knowledge graph and/or ontology.\nAt the same time, knowledge graphs and ontologies are ever more important for applications in data integration and data management, and more recently also as ground truth to escape from Large Language Model (LLM) hallucinations [29] and as components of other neurosymbolic approaches [21, 23, 24]. As a consequence, improved processes and methods for automating or even semi-automating core KGOE tasks remains a key challenge for the research community.\nLLMs enter the scene, and the public perception of Artificial Intelligence (AI), with force in 2022 at the launch of OpenAI's ChatGPT, with rapid developments since then. Their human-style conversation capabilities, which include a profound mastery of expression in written language, as well as solid production of more structured information, are as stunning as their sometimes wildly confabulated responses (usually termed hallucinations). Perhaps most important for our discussion herein, LLMs appear to capture, and to have the ability to recall in different formats and contexts, a wide swath of human knowledge, both commonsense and specialized, provided it is reflected well enough in the training data. While at this point in time, their reliability in terms of accuracy of content in their responses remains problematic, it is quite apparent and widely reported that working with an LLM can save significant time and effort provided there is a (human) topic expert available as a check on factual accuracy.\nThe promise of LLMs for KGOE is thus: by using LLMs as approximate natural language knowledge bases that can be approximately queried, plus LLM capabilities to understand and produce information in ways structured for KGOE use, it should be possible to design semi-automatic methods, or human-LLM interactive methods, that can produce at least draft solutions for key KGOE problems at a level of quality that will significantly reduce human expert time and effort. Work on this line of research has of course already been started by the Semantic Web community. With this paper, we intend to begin to consolidate the discussion, and \u2013 in particular \u2013 contribute observations and discussions related to our own research on modular ontologies, which we believe to be highly relevant, for reasons that we will lay out below.\nThe plan of this paper is as follows. In Section 2 we motivate the need for (a certain type of) modularity for LLM-based"}, {"title": "2. Divide and Conquer", "content": "Useful ontologies and knowledge graphs often tend to be large, or even very large. The ENVO ontology [6] has over 2, 100 classes; the Gene Ontology [7] has over 40,000 classes; a Wikipedia-derived class hierarchy used for explainable deep learning analysis has over 2. 106 classes [41]; DBpedia Core4 has about 9. 108 triples with an ontology of about 800 classes, KnowWhereGraph [48] has over 28 \u00b7 109 triples with an ontology of about 300 classes. The size \u2013 in particular of the ontology underlying a knowledge graph \u2013 is one of the obstacles facing humans in KGOE tasks, as even an ontology with a \"mere\" 300 classes (and corresponding comprehensive OWL axiomatization) is simply too big for a human to keep an overview of as a whole, or to understand well from scratch within a reasonable amount of time.\nEven before the advent of LLMs, therefore, Semantic Web researchers have looked into the use of ontology modules in order to segment large ontologies into pieces of a size that are more workable for the human brain. These efforts include this paper's authors and their collaborators, in a line of work that has developed out of Ontology Design Patterns (ODPs [3, 14]) research and practice, using an approach that we believe to be particularly suited for LLM-based KGOE, and that we will focus on further below: If an ontology is split into (even possibly overlapping) pieces that make conceptual sense (as a type of mini-ontology) for the domain experts, then KGOE tasks can often focus on one or a few coherent pieces (modules) of the ontology, thus simplifying the task significantly. And given the frequent difficulties (and expense) of LLMs to deal with substantial size prompts and with more open-ended and less common scenarios, it is a reasonable expectation that similar benefits will apply to LLM-based KGOE.\nLet us provide a case in point from the context of ontology alignment, reported in [1]. The setting is the creation of complex ontology mapping rules between two ontologies in the OAEI GeoLink complex ontology alignment benchmark [52]. The benchmark is of moderate size, with 40 classes, 149 object properties and 49 data properties in one of the ontology, and 156 classes, 124 object properties and 46 data properties in the other ontology. It is a natural rather than synthetic benchmark in that the ontologies were originally developed for an application purpose, and were only cast into a benchmark later. The benchmark (like all complex ontology alignment benchmarks) had defied automation for years, the only approaches that were able to create reasonably good results assumed a shared ABox(i.e. a shared data graph) [39, 38], which is of course a very unrealistic assumption for data management practice.\nAs reported in [1], an LLM was prompted to produce a body of an alignment rule, given a rule head together with the two ontologies as part of the prompt. This essentially completely failed, i.e., the LLM produced output that was essentially unusable. But then we made use of the modular structure of the body-side ontology, for which 20 named modules (such as \"Organization\u201d or \u201cPhysical Sample", "stages": "given an alignment rule head and the list of 20 module names, we first asked for the module(s) that would be required to create the rule. Then we prompted for the rule body by providing the modules previously identified. The results we obtained were really good in terms of high precision and recall. The availability and principled use of modules made the difference between an almost complete failure and a reasonably accurate system response."}, {"title": "3. Modules", "content": "The term module has been used for different things in the Semantic Web context. Our approach, as in [45], comes out of the tradition of ODPs [3, 14], which are partial ontologies that address commonly occurring, domain invariant ontology design issues. Later, related notions with similar underlying ideas, but generally different objectives (and implementations), include [35, 50].\nFor us, and this is laid out in [25, 45], a module is a part of an ontology that consists of the classes, properties, and axioms within that ontology that are relevant to a key notion, as considered by a domain expert, for the ontology use case. For example, the above mentioned GeoLink ontologies that focus on oceanographic cruises and data, such key notions (and corresponding modules) include Trajectory, Cruise, Physical Sample, Organization. The Enslaved.org ontology about historic person and events data on the transatlantic slave trade [47] has modules such as Event, Place, Provenance, PersonRecord, AgeRecord, etc. Modules can overlap or even be nested, and there are, on purpose, no precise rules which ontology pieces should be thought of as being part of a specific module. Rather, it has to make sense from several perspectives, including the domain experts' perspectives, the use case perspectives, and the perspectives of the available data relevant for the use case and ontology. As such, the modules provide conceptual bridges between human expert conceptualization and data reality [25].\nOur approach called MOMo for Modular Ontology Modeling, also includes a step-by-step ontology design methodology geared towards humans, a module (and pattern) description language OPLa expressed in OWL [22], ontology design pattern libraries [46], and a Prot\u00e9g\u00e9 plug-in for ontology development [43, 44] we mention these just in passing, and details can be"}, {"title": "4. Modular LLM-based KGOE", "content": "We will now look, in turn, at the key KGOE tasks identified in the introduction and discuss each of them from the perspective of modularity-driven LLM-based KGOE. Specifically, we address the different tasks in essentially in order of abstraction, i.e., from knowledge to data. We note of course that inroads made in any tasks easily have the capacity to impact and improve outcomes in the others.\n4.1. Ontology Modeling, Extension and Modification\nAutomated ontology design often referred to as Ontology Learning - has been investigated primarily from a traditional (i.e., pre-LLM) Natural Language Processing (NLP) perspective, as a possible way to address the knowledge acquisition bottleneck with much of the methods established in the early aughts; see, e.g., [30, 31, 10, 12, 5, 11]. However, as the proliferation of early ODP-based methodologies (e.g., eXtreme Design [40]) grew, newer attempts to accomplish ontology learning emerged, leveraging identification of candidate patterns and subsequent human intervention [4].\nIn remaining pre-LLM years, ontology learning incrementally advanced from these early achievements and established techniques, generally through incremental evolution, inductive logic programming and both linguistic- and statistical-focused NLP [2, 51]. However, results to date have been rather mixed, with resulting ontologies far from being able to compare with carefully crafted ontologies by domain and ontology engineering experts. The promise of LLMs now in this context comes from the fact that they are very powerful NLP tools that approximately capture a wide swath of human (expert) knowledge and can be coaxed by good prompting to even provide this knowledge in a structured form, such as expressed in a formal language over a given vocabulary.\nIndeed, some attempts have already been made for LLMs, with some middling success [42, 15]. At this stage, LLMs are too limited in commonsense and reasoning power to in a fully automated fashion. Yet, we can thus expect that LLMs should enable a human ontology engineers and domain experts to first draft and then finalize suitable ontologies much more quickly than before.\nIt appears to us that specific aspects of the MOMo methodology, and generally the modularity idea, should further an LLM-based approach even more:\n1. MOMo is based on a principled use of (high-quality) ontology design patterns (ODPs). Developing ODPs is arguably easier than developing full-fledged ontologies, i.e. in a first step LLMs can be used to develop ODP libraries, which then in turn can be made use of in LLM-based ontology design. As a first step, we have used an LLM to generate hundreds of simple \"commonsense\" patterns for common concepts [13]. These micropatterns so called - due to their shallow semantics and, even for ODPs, simple structure - have been organized into a design library [46], which allows for programmatic access, such as through a RAG [29] system. Due to their simplicity, they are easily instantiated into modules [17] and connected together in a modular fashion.\n2. MOMo provides a step-by-step ontology design process that breaks down the complex ontology modeling task into clearly delineated pieces, each of which should be easier to automate than going one-shot from base data (or texts) to an ontology.\nThe situation for extension and modification of a MOMo ontology is rather similar \u2013 in the MOMo approach an ontology is designed module-by-module (with possible modification of earlier developed modules while progressing), i.e. extension and modification are already part of the modeling process.\n4.2. Ontology Alignment\nOur experience with LLM-based modular ontology alignment was already conveyed above, in Section 2. Ontology Alignment is a core task for ontology-based data integration with a long-standing corresponding research community, benchmarks and annual performance competitions. The community has mostly focused on full automation (as opposed to human-in-the-loop semi-automation) and on so-called simple alignment tasks, i.e. the creation of one-to-one class mappings (and, with much less emphasis, one-to-one property mappings). The need for complex ontology alignment \u2013 i.e., the creation of mapping rules that go beyond one-to-one mapping \u2013 has long been recognized, but it has only come into more focus in the community about 10 years ago, with very limited results (as pointed out in Section 2, see also [39, 38]. Regretfully (we think), since the advent of LLMs focus has mostly shifted back to simple alignments, just now with the support of LLMs. Indeed, LLM-basedsimple ontology alignment has already been investigated with good results [33, 19].\nWe would be negligent to not point out that we also observe issues with some of the established current benchmarks and methods. For example, in [9] a simple alignment benchmark was re-evaluated, finding that even humans disagree on a significant part of the benchmark, essentially demonstrating that the benchmark ontologies simply do not carry enough information such that even humans can solve it perfectly. In [8], it was demonstrated that most of the functionality of simple alignment systems can be obtained by using only string similarity metrics. At the same time, when submitting papers for publication that described complex alignment benchmarks that came out of real data integration work [53, 54], some reviewers dismissively pointed out that they would not be good as benchmarks because they would be far beyond anything that could currently be done.\nFor some years, progress on the complex alignment task stalled [34], even as new benchmarks were released [55]. However, some new techniques, even incorporating LLMs have shown some promise. For example, the composition of (geometric) language embeddings cast from a KG or sourced from an LLM, can be shown to have correspondences in the latent space [49]. Yet, the performance is still relatively low, ranging from 0.49-0.69 for Semantic F1-score, depending on the ontologies. As described above in Section 2, it was demonstrated in [1] that modularity provides significant performance improvements to LLM-based complex alignment, in this case with 104 out of 109 (i.e., 95%) target alignment mappings correctly identified on the GeoLink benchmark when taking modularity into account.\n4.3. Ontology Population\nIn some sense, ontology population and entity co-reference resolution/disambiguation are not substantially different. On one hand there is a theoretical or unknown entity which satisfies, or otherwise conforms to, a portion of an ontology (i.e., it should populate the ontology). On the other hand, there is a known entity described, or otherwise named, in the natural language corpus. Now the system needs to determine if those two entities match to move forward in populating the ontology.\nIn this case, we can leverage two aspects of modularity for ontology population with LLMs: conceptual consistency and tight scope. That is, the conceptual consistency of a module \u2013 based on our definition, of course means that the constituent classes and properties of a module somehow belong together, especially from a human perspective. This in turn provides a tighter cluster of \"sentiment\" (for lack of a better term), priming an LLM in the prompt.\nOn the other hand, we have seen that LLMs tend to be better at following patterns, rather than instructions, and it correlates as well to the length of the prompt [28]. Thus, we posited thatattempting ontology population on a per-module basis would succeed where other attempts had poorer results.\nIndeed, in a recent set of experiments, we obtained rather excellent results [32]. Using prompts constructed with a simple schematic representations of modules and a corresponding extraction example, an LLM was able to achieve 90% extraction of related triples from text, as compared to ground truth. While additional experiments are of course required, especially examining appropriate modeling characteristics for modules, we take this as an excellent indicator that modularity is of significant added value to the process.\n4.4. Entity Disambiguation\nAs above, we note that ontology population and entity disambiguation are tightly intertwined. While ontology population focuses on the extraction of an entity and (ostensibly) comparing it to a theoretical candidate, entity disambiguation compares two known, or otherwise named, entities and must produce a value corresponding to the degree to which they are the same entity.\nHowever, we also note that it is difficult to assess the current capabilities of LLMs to produce correct entity disambiguation due to the possibility for data leakage (i.e., the appearance of existing and gold-standard benchmarks in the testing data). For example, some benchmarks (after prompt-tuning) achieve upwards of F1 = 91.\nAs successful co-reference resolution correlates with available context, per improved ability to resolve matching context (i.e., complex ontology alignment) we see immediate benefit in this parallel task of entity disambiguation. We thus posit that the added context from a tight, conceptual description, i.e., a module, will significantly improve outcomes for this KGOE task."}, {"title": "5. Concrete Research Challenges", "content": "Throughout this paper so far, we have largely discussed a specific type of module, which we have been calling a conceptual module and distinct from other uses in the literature for both ontologies [37, 26] and KGs [18]. We are not aware of any work on the capabilities of LLMs for integrating for these other definitions of module. We suspect that due to their (in general) rigorous approach to the partitioning of knowledge in a logically consistent way, we may see similar benefits as when using our notion of modularity. That is to say, by somehow limiting the scope, we achieve a more human-like approach \u2013 and one more capable of being expressed succinctly in language, and thus more appropriate for LLM-based assistance in the task.\nOf course, it is important to not altogether neglect further research on non-modular, LLM-based KGOE; we need to understand its limitations, as well as its capabilities. Indeed, additional focus on modular or pattern-based techniques for KGOE can inform these other lines of research.\nFrom a broader perspective, the added value of modularity that seems to become apparent for LLM-based KGOE prompts the"}, {"title": "6. Conclusion", "content": "Knowledge Graphs and Ontologies have seen new resurgence in the era of LLMs, for example in structuring data for RAG systems [36] or providing guide-rails to limit or prevent confabulation in textual output [16]. Yet, for all their established and growing importance, many of the tasks pertaining to their development, maintenance, extension, and population (to name a few) are still very difficult. It seems somehow appropriate that we incorporate LLMs into improving KGOE tasks. LLMs have shown some initial success in simple tasks, such as the generation of simple schemas or topic extraction.\nTo tackle the hard tasks, we have proposed the use of modules - conceptual, human-centric partitions of an ontology or schema that provide internal structure. These conceptual boundaries assist LLMs in a variety of tasks, seeing already success in ontology construction, (complex) alignment, and population. It seems obvious now that modularity is a missing link for bridging human conceptualization and machine interoperability. We thus fully believe that modularity must be incorporated from the start, both structurally and in documentation, so as to further enable the various improvements to KGOE tasks we have outlined above."}]}