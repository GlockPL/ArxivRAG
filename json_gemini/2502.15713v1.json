{"title": "UAV-assisted Internet of Vehicles: A Framework Empowered by Reinforcement Learning and Blockchain", "authors": ["Ahmed Alaghaa", "Maha Kadadhab,c", "Rabeb Mizounib,c,*", "Shakti Singhb,c", "Jamal Bentaharb,d,a", "Hadi Otrokb,c"], "abstract": "This paper addresses the challenges of selecting relay nodes and coordinating among them in UAV-assisted Internet-of-Vehicles (IoV). Recently, UAVs have gained popularity as relay nodes to complement vehicles in IoV networks due to their ability to extend coverage through unbounded movement and superior communication capabilities. The selection of UAV relay nodes in IoV employs mechanisms executed either at centralized servers or decentralized nodes, which have two main limitations: 1) the traceability of the selection mechanism execution and 2) the coordination among the selected UAVs, which is currently offered in a centralized manner and is not coupled with the relay selection. Existing UAV coordination methods often rely on optimization methods, which are not adaptable to different environment complexities, or on centralized deep reinforcement learning, which lacks scalability in multi-UAV settings. Overall, there is a need for a comprehensive framework where relay selection and coordination processes are coupled and executed in a transparent and trusted manner. This work proposes a framework empowered by reinforcement learning and Blockchain for UAV-assisted IoV networks. It consists of three main components: a two-sided UAV relay selection mechanism for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning (MDRL) model for efficient and autonomous UAV coordination, and finally, a Blockchain implementation for transparency and traceability in the interactions between vehicles and UAVs. The relay selection considers the two-sided preferences of vehicles and UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon selection of relay UAVs, the coordination between the selected UAVs is enabled through an MDRL model trained to control their mobility and maintain the network coverage and connectivity using Proximal Policy Optimization (PPO). MDRL offers decentralized control and intelligent decision-making for the UAVs to maintain coverage and connectivity over the assigned vehicles. The evaluation results demonstrate that the proposed selection mechanism improves the stability of the selected relays, while MDRL maximizes the coverage and connectivity achieved by the UAVs. Both methods show superior performance compared to several benchmarks.", "sections": [{"title": "1. Introduction", "content": "The Internet of Vehicles (IoV) field has bloomed in recent years with the rise of smart cities. IoV integrates concepts from Vehicular Ad Hoc Networks (VANETs) and Internet of Things (IoT), where Internet-enabled vehicles equipped with sensors are connected [1, 2, 3]. Their connectivity paves the way to various use cases, such as reducing traffic jams, transit time, and accidents [4], facilitating media playback, and file-sharing. Despite its usability, the decentralized nature of IoV makes it challenging to select high-quality and stable relay nodes in the network and to maintain coverage, given the limited number of deployed Roadside Units (RSUs).\nUnmanned Aerial Vehicles (UAVs) have been gaining interest in both the fields of VANETs and IoV due to their autonomous flight, unbounded movement patterns, and higher communication capabilities [5]. They offer deployment flexibility to IoV/VANET networks compared to RSUs, as they can be deployed on the fly. In addition, UAVs improve connectivity, load balancing, and information propagation delay between ground vehicles with superior communication capabilities. UAV-assisted IoV networks have three challenges that existing works attempt to tackle: 1) The selection of relay nodes where different mechanisms can be used to assign vehicles to relay UAVs, 2) the traceability of selection mechanism execution, and 3) the coordination between relay UAVs to maximize connectivity and coverage in the network.\nDifferent mechanisms have been used for the selection of relays in IoV/VANET including optimization-based methods such as quasi-convex optimization [6], auction-based selection [7], and game theoretic approaches such as hedonic, evolutionary, Stackelberg, and matching games [8, 9, 10]. However, these mechanisms are applied either on servers or internally at nodes, which compromises trust and transparency due to the possibility of attacks on servers and malicious behaviour by nodes in the network. Several works proposed the use of Blockchain [11] for IoV networks due to its ability to provide a trusted, decentralized, and transparent framework with no-down time compared to centralized systems. In specific, it has been used for securing transactions between communicating nodes in IoVs, enabling vehicle authentication, storing data on the Blockchain, and managing AI models [12, 13, 14, 15, 16]. While Blockchain improves trust and transparency, it has not been adopted in UAV-assisted IoV networks.\nOnce selected, UAVs have significant importance in maintaining the connectivity and coverage over vehicles in IoVs. Connectivity here is a measure of how well the UAVs maintain communication links with one another, which requires them to continuously position themselves within each other's communication ranges to ensure a robust and interconnected network. On the other hand, coverage refers to the extent to which UAVs can provide network access to vehicles within their communication ranges. To achieve this, the UAVs need to coordinate and position themselves intelligently to maximize the coverage according to the distribution and mobility of vehicles, without violating connectivity. Some works in the literature address the coordination between selected UAVs by optimizing their placement in the area, such as the coverage and connectivity are maximized [17, 18]. However, due to the dynamicity of VANET environments, such static strategies may result in sub-optimal performance, loss of connectivity, and deteriorating coverage. Alternatively, recent works focus on continuous and intelligent decision-making, where the UAVs coordinate to move in the environment according to the mobility of the vehicles [19, 20, 21, 22]. Most of these works use centralized control stations that organize the positioning and communications of UAVs, which struggle in terms of scalability with the number of agents.\nTo address the aforementioned drawbacks in relay selection and UAV coordination, this paper proposes a com-prehensive management framework for UAV-assisted IoV networks leveraging Blockchain and Multi-Agent Deep Reinforcement Learning (MDRL). The Blockchain-based framework enables two main features: 1) a Blockchain-based UAV selection mechanism for relay selection and 2) decentralized MDRL-based multi-agent coordination to ensure scalable cooperation between the UAVs in maintaining connectivity and coverage. The Blockchain-based UAV selection enables the selection of vehicles to UAVs by considering both of their preferences in a transparent and trusted manner. Vehicles first determine the Quality-of-UAV (QoU) for available UAVs within their range and submit the maximum QoU UAV along with their requirements to the smart contract. Then, the smart contract allocates UAVs to proposing vehicles given their bandwidth capacity and the ranked list of vehicles based on their Quality-of-Vehicle (QoV). For coordination between relay UAVs, the problem is formulated as a Markov Game where an MDRL model is trained to decide the mobility actions in the environment in decentralized multi-agent settings to maintain the connectivity between the UAVs and the coverage of the assigned vehicles. A Convolutional Neural Network (CNN) is designed as the actor-network that translates each UAV's observations into mobility actions. A team-based reward function is designed to allow for cooperation between the agents. The learning is optimized using Proximal Policy Optimization (PPO) in a Centralized Learning and Distributed Execution (CLDE) manner. The storage of the trained MDRL models is managed using the InterPlanetary File System (IPFS), which are assigned to UAVs upon selection.\nIn summary, the contributions of this work are:\n1. A transparent and trusted management framework for UAV-assisted IoV through Blockchain.\n2. A Blockchain-hosted two-sided relay selection mechanism incorporating specifically designed QoV and QoU metrics for UAV-assisted IoV."}, {"title": "2. Related Work", "content": "This section covers related works in different domains related to the targeted problem, including IoV Relay Selection, Blockchain for Trusted Execution, and UAV Coordination in IoV."}, {"title": "2.1. IoV Relay Selection", "content": "The selection of relay nodes in IoV/ VANET networks is an important challenge due to the impact on the stability of the network, hence the network performance. While only vehicles were considered initially during the selection of relay nodes, UAVs rose as a solution to improve the connectivity and coverage of IoVs.\nExisting works considered different mechanisms for the selection of relay nodes such as optimization, auctions, and game theory. The works in [6, 7, 9] propose mechanisms that run on edge servers for the formation of the network. In [6], the authors optimize the positions of UAV relays given their driving trajectory. They consider the cooperation between UAVs where the transmission power for each UAV is optimized along with the deployment positions. In [7], an auction mechanism is applied for the selection of UAV coalitions delegated tasks in the IoV to improve communication efficiency. Bids are considered an indication of the preferences of workers and are used to incentivize the participation of UAVs when the selection mechanism forms stable coalitions. In [9], the evolutionary algorithm is utilized for clustering vehicles and placing services on UAVs. The algorithm accounts for the cluster lifetime, the connection time, and the energy of the UAVs. In [8, 10], hedonic and matching game models are proposed respectively with the game models running in a decentralized manner at the nodes of the network. In [8], the authors propose the use of a hedonic game model to form stable coalitions by accounting for the Quality-of-Service (QoS) of vehicles. In [10], they extend their work and consider the existence of UAVs as part of the IoV network. They consider the two-sided preferences of UAVs and vehicles in the matching mechanism for stable selections to be formed.\nWhile different mechanisms have been applied for the selection of nodes, vehicles, and UAVs, the aforementioned work all lack transparency and traceability in the selections formed where they rely on the trust of the servers and nodes."}, {"title": "2.2. Blockchain for Trusted Execution", "content": "Multiple domains such as last-mile delivery [23, 24], VANET [12, 13, 15], and UAVs [16] adopted Blockchain as an alternative to traditional centralized systems. Blockchain offers trust and transparency to these domains along with no-down for deployed systems. In [23], Blockchain is considered in an environment where vehicles and UAVs needed to be selected last-mile delivery tasks. The preferences of both were accounted for in a Blockchain-hosted mechanism offering transparency and trust in the selection. In [14], the different use cases for Blockchain in IoV are defined where the authors focus on the impact of Blockchain on securing the transactions between communicating nodes in the IoV. In [12, 13, 15], the authors considered the integration of Blockchain with vehicular networks. In [12], the authors propose a Blockchain-enabled mechanism for the selection of relay nodes in vehicular ad hoc networks in a transparent and trusted manner. Blockchain is utilized to manage vehicles' reputation and transparent storage of the selection mechanism. Meanwhile, the authors [13] proposed the use of Blockchain for securing IoV networks by enabling the authentication of vehicles and storing the data on the Blockchain. In [15], the authors propose the division of vehicles and RSUs into different zones where the vehicles register to the Blockchain and RSUs communicate securely through\nthe Blockchain. In [16], the authors tackled security UAV networks by utilizing Blockchain for securing data exchange and communication links.\nDespite the utilization of Blockchain in different domains, it has not yet been adopted in UAV-assisted networks to offer trust and transparency for the selection of relay nodes. Yet, key factors to account for moving forward are the complexity and scalability of the adopted blockchain to the domain. In fact, while centralized systems offer easier management and can handle complex mechanisms, decentralized systems such as blockchain offer better scalability and enhanced privacy. In addition, based on our knowledge, the only work that considers on-chain relay selection is [12], where nodes are vehicles only without considering UAVs. Therefore, key factors to account for moving forward are the complexity and scalability of the adopted Blockchain to the domain."}, {"title": "2.3. UAV Coordination", "content": "The positioning of the chosen UAVs is an important aspect in UAV-assisted IoVs to ensure good coverage and connectivity. Several works in the literature have addressed this problem using optimization methods. In [25], the authors use Successive Convex Approximation (SCA) to minimize the number of deployed UAVs and optimize their trajectory aiming to maintain connectivity. The authors in [26] deploy UAVs in real-time based on predicted vehicular distribution, where a Multimodal Nomad Algorithm (MNA) is used to decide the best service positions in order to enhance the efficiency of drone-assisted VANETs. Another work in [27] proposes a three-dimensional UAV positioning technique to support communication between vehicles on the ground using Particle Swarm Optimization (PSO) and a Genetic Algorithm (GA). In [28, 17], the authors propose a PSO-based optimization method for the deployment of collaborative UAVs for urban VANETs, where the aim is to optimize the coverage area. A similar work in [29] proposes an ellipse clustering algorithm that optimizes the locations of UAV base stations, amongst other factors, to maximize the coverage of ground users. While the aforementioned optimization-based works show promising results, they have drawbacks in terms of adaptability and scalability to different environment complexities. The variations in unknown environments might require the UAVs to explore for some time before making decisions. Additionally, most of these methods centrally decide on the positioning of the UAVs before deployment and lack dynamicity and continuous decision-making which is essential in realistic scenarios.\nTo overcome the aforementioned drawbacks, recent works resorted to the use of Deep Reinforcement Learning (DRL) as a method to obtain intelligent UAV agents that are capable of continuous decision-making for efficient coverage and connectivity in VANETs. In [30], the authors employ Q-Learning, a Deep Reinforcement Learning (DRL) technique, to navigate an unfamiliar environment and determine the optimal paths for a minimal number of UAVs, ensuring connectivity for ground vehicles traveling at various speeds. Another work in [20] uses the Deep Deterministic Policy Gradient (DDPG) DRL method to govern the UAVs' trajectories with a set of continuous actions. In [21], the authors propose a DRL method considering UAVs' communication ranges, where a central agent (base station) learns how to optimally control and position the UAVs. Similarly, the authors in [22] use Double Deep Q-Networks (DDQN) as a DRL algorithm to obtain an agent that is capable of controlling the UAVs for maximized coverage and connectivity. A major common drawback in the works above is the use of a centralized agent that controls all the UAVs, which results in issues with scalability, single point of failure, and collaboration between the UAVs. MDRL is generally used to address such scalability issues, which have been previously tackled in many applications such as path planning [31, 32], target search [33], and mobile edge computing [34]. In the context of UAV-assisted IoVs, the authors in [35] propose a MDRL solution, but only focus on UAVs deployed in a single/sparse highway with few vehicles, and not in urban areas crowded with vehicles."}, {"title": "3. Proposed System", "content": "This section presents and discusses the different components of the proposed system, including the Blockchain-based framework, UAV-assisted IoV construction, and MDRL-based UAV coordination. An overview of the proposed system is illustrated in Figure 1 with the general interactions between the system components, where the order of the interactions is illustrated later in Figure 4. It enables the members of the IoV, being vehicles and relay UAVs, to interact transparently, form a stable network, and maintain connectivity. The physical environment is divided into geographical zones with unique identifiers where vehicles and relay UAVs are assigned to a single zone based on their GPS coordinates. A smart contract is designed for managing the registration of IoV members and the selection of UAVs\nto vehicles on top of Quorum consortium Blockchain. The proposed MDRL algorithm is used to train several models for different variations of the environment (different number of agents, number of vehicles, etc). Trained MDRL models are stored on the IPFS, which is a decentralized file storage system that efficiently handles large files. In IPFS, the data is distributed across a peer-to-peer network, where files (i.e. models) are replicated across multiple nodes, ensuring they remain accessible even if some nodes fail. In our framework, metadata associated with these models, such as their unique IPFS identifiers (needed to access the file on IPFS), ownership details, and application-specific information about the models are stored on the Blockchain. IPFS is used instead of the Blockchain for storing the actual models due to the Blockchain's limited storage capacity and the high cost of storing large files. Once UAVs are selected, they are assigned one of the available MDRL models by the smart contracts depending on the environment. Each UAV is given a copy of the model and acts in a decentralized manner based on its observations to maintain connectivity and coverage for the assigned vehicles.\nThe members of the frameworks and its key components are detailed below.\n\u2022 IoV members: which are vehicles and UAVs that form the IoV network. Vehicles are interested in maintaining connection to other nodes in the network while UAVs are available to extend the connectivity of the network.\n\u2022 Blockchain-based Framework: which consists of the smart contract running on the Quorum Blockchain [36] that form the UAV-assisted IoV and manages sharing the MDRL model with the UAVs.\n\u2022 UAV-assisted IoV Relay Selection: which is the mechanism responsible for constructing the IoV by allocating vehicles to UAVs.\n\u2022 MDRL models for UAV Coordination: which are the different trained models to control and coordinate the UAVs' mobility in the area, to be assigned to them according to the environment upon selection\n\u2022 IPFS: which is used to store trained MDRL models to be assigned and used by the UAVs."}, {"title": "3.1. Proposed Blockchain-based Framework", "content": "The proposed Blockchain-based framework consists of a smart contract that facilitates the interactions between the vehicles and UAVs. Quorum Blockchain [36], which is an Ethereum-based consortium Blockchain, is considered for deploying the designed smart contract to have. Quorum Blockchain was selected for its ability to offer high transaction throughput (up to 3,000 TPS) in reasonable delay [37]. The proposed smart contract is responsible for storing and sharing their information, collecting selection proposals from vehicles and allocating them to UAVs, and sharing the adequate MDRL models to UAVs to maintain the stability of the selection.\nThe designed smart contract in the proposed framework consists of multiple data structures, variables, mappings, and functions.\nThe QoV/ \u221aRB is considered for ranking the vehicles."}, {"title": "3.2. UAV-assisted IoV Construction", "content": "In this work, QoU and QoV metrics are proposed to identify high quality vehicles and UAVs during the selection process to form high quality IoV. The selection process assumes the allocation runs at an instance of time where the positions of vehicles and UAVs are constant despite their mobility for simplicity. The proposed QoU metric is computed locally at vehicles to evaluate relay UAVs in their zone. It combines the UAVs' available bandwidth, energy level, reputation, and distance between the UAV and the vehicle. Vehicles acquire UAV metrics by accessing the UAV\nList and UAVData List variables in the proposed smart contract. As these metrics are of different range, each of them is normalized given its maximum value to be in the range ([0-1]). Then, QoU is calculated as the weighted sum of the individual metrics. The QoU of UAV u for vehicle v, QoUuv, is calculated based on Eq. 1.\n$QoU_{uv} = 100 \\times (w_1 \\times \\frac{AB_u}{Max AB} ) + (w_2 \\times \\frac{BL_u}{Max BL} ) + (w_3 \\times \\frac{Rep_u}{Max Rep} ) + (w_4 \\times (1 - \\frac{Distance_{uv}}{Max distance} ))$ (1)\nwhere the sum of w1, w2, w3, and w4 is equal to 1. The QoU is formulated to be higher for UAVs with higher available bandwidth to increase the probability of selection, higher energy to reduce UAV re-selection, higher reputation as it implies their reliability, and lower distance to reduce the UAV travel time.\nThe proposed QoV metric is used by UAVs to evaluate vehicles before selecting them as relay nodes for routing. It combines vehicles' requested bandwidth, reputation, payment per Mbps, and distance from the UAV. UAVs acquire vehicle metrics by accessing the Vehicle List and VehicleData List variables in the proposed smart contract. Similar to the metrics used for QoU, the metrics of the QoV are normalized given the maximum values and the weighted sum is used to aggregate the metrics. The QoV of vehicle v for UAV u, QoVuv, is calculated based on Eq. 2.\n$QoV_{uv} = 100 \\times (w_5 \\times \\frac{RB_v}{Max RB} ) + (w_6 \\times \\frac{PayPerMbps_v}{Max PayPerMbps} ) + (w_7 \\times \\frac{Rep_v}{Max Rep} ) + (w_8 \\times (1 - \\frac{Distance_{uv}}{Max distance} ))$ (2)\nwhere the sum of w5, w6, w7, and w8 is equal to 1. The QoV is formulated to be higher for vehicles of higher requested bandwidth, higher payment per Mbps, higher reputation, and of lower distance between vehicles and UAVs. It is worth noting that the weights selected can vary based on whether the IoV is constructed for general use where equal weights can be used or tailored to specific applications where higher weight can be set for metrics of higher importance.\nThe UAV selection runs in two-stages: 1) Vehicles Selection where vehicles select UAVs that they want to route for them, 2) UAV selection where UAVs confirm the vehicles they will route for within a specific zone. Algorithm 1 reflects the first stage, vehicle selection, executed at each vehicle in a zone. The vehicle starts by retrieving the Information of UAVs in its zone ZoneUAVInfo through the UAVs in Zone List and UAVData List mappings. Next, a vehicle goes through the list of UAVs and calculates the QoU for each of them based on the acquired info. The UAV with maximum QoU is identified along with its Ethereum address. The address would be then used along with the requested bandwidth value to call the submitVehSelection() function in the smart contract.\nAlgorithm 2 presents the UAV selection mechanism executed at the allocateZone() function. The algorithm runs periodically for zones in the Zones List. The algorithm iterates through each UAV u in the zone in the order of their occurrence in the List. First, the UAVProposalList\u0173 is sorted based on the QoV/ \u221aRB, metric. Next, the sorted list is iterated through where the requested bandwidth of a vehicle is checked against the available bandwidth. If it is lower or equal to the available bandwidth, the address of v is added to the UAVS electionListu and the available bandwidth is updated.\nRegarding computational complexity, up to |V|\u00d7|U| proposals can be submitted, with each proposal being executed in O(1) time through the mapping structure in the smart contract. Consequently, the run-time of the selection algorithm is O([V] \u00d7 [U])."}, {"title": "3.3. MDRL Approach for UAV Coordination", "content": "The task execution process in the proposed system is enabled through Multi-Agent Deep Reinforcement Learning (MDRL). Given the selection of UAVs to vehicles done in the previous stage, a team of UAVs in a given zone is tasked with coordinating to provide connectivity to the available vehicles in the area. To maintain connectivity and good coverage, a UAV is required to move in the area intelligently according to the locations of the vehicles, as well as the locations of other UAVs. Each UAV is tasked with maintaining coverage to its selected vehicles, while enhancing or maintaining connectivity with the other UAVs. This task is challenging due to the dynamic nature of the environment, where vehicles are continuously moving. This calls for collaboration and coordination between the UAVs, as well as intelligent decision making based on the locations and distribution of vehicles. It is worth mentioning that each zone has its own coordination task among its selected UAVs, determined by the relay selection mechanism. If a vehicle leaves a zone, the proposed mechanism reassigns the vehicle to a UAV in a different zone, which has its own coordination task. In this work, we focus on the coordination between UAVs in a given zone assuming vehicles only move within the zone without leaving it. Given the problem's nature of sequential decision-making based on the collected observations, MDRL comes as an efficient method to obtain intelligent agents. In MDRL, agents develop their intelligence by learning decision-making policies based on their experiences within the environment, aiming to maximize the numerical reward they receive. This section presents the MDRL formulation of the problem and the modeling of the MDRL solutions in terms of observations design, reward engineering, and optimization method."}, {"title": "3.3.1. MDRL Formulation and Policy Optimization", "content": "In the context of MDRL, Markov Games (MGs) are generally used to extend Markov Decision Processes (MDPs) into multi-agent settings [38, 39, 40]. In the UAV-based coverage and connectivity problem, the different environment states are expressed by the distribution of agents (UAVs) and vehicles. A MG is defined by a set of S finite states, finite action sets A1, A2, ..., An for each of the N agents, finite observation sets 01, 02, ..., ON, a state transition function P(s', o | s, a) that computes the probability of ending up in state s' with observation o after executing action a in state s, a reward function R : S \u00d7 A \u2192 R, and a discount factor y \u2208 [0, 1]. Here, a = (a1, ..., an) and 0 = (01,..., ON) denote joint actions and observations from the N agents at a given instant. In MDRL settings, the UAV-based coverage and connectivity problem unfolds over discrete steps. At each step, agent i uses its policy \u03c0\u00a1 : \u039f\u00a1 \u00d7 A; \u2192 [0, 1] to translate an observation o\u00a1 \u2208 O\u00a1 into an action a\u00a1 \u2208 A\u00a1, and receives a reward ri.\nUsing the collected experiences, Proximal Policy Optimization (PPO) [41] is used in this work to improve the decision-making policies of the agents. PPO is a Policy Gradient (PG) method that has two components, an actor and a critic. The actor (policy) network takes the current observations as input and produces a probability distribution over the possible actions. The critic (value function) predicts the future rewards, which is used in updating the actor network. The objective is to optimize the actor policy \u03c0\u0473, parametrized by 0, to maximize the sum of rewards in an episode. PPO strikes a balance between simplicity and performance by using a clipped surrogate objective that ensures stable and efficient policy updates."}, {"title": "3.3.2. Action Space", "content": "In the proposed work, a UAV makes decisions in a discrete fashion. At each timestep, the UAV chooses to either move in a specific direction or remain stationary. For simplicity, it is assumed that the UAVs operate in a 2D plane. The action space is divided into K possible directions {1, 2, ..., k\u012f, ..., K}, where the movement direction (or angle) is computed as:\n$0 = 2\u03c0 \\frac{k}{K}$ (3)\nIn this work, the UAV has constant speed and 9 possible actions, 8 of which are cardinal and ordinal directions (K = 8) and one action for remaining stationary."}, {"title": "3.3.3. Policy Architecture", "content": "The intelligent decision making in the proposed system is done through a policy that is defined as a Convolutional Neural Network (CNN), which is ideal to capture spatial correlations in the collected observations. Fig. 3 shows the CNN architecture used for the actor and critic networks, which is based on the LeNet-5 architecture [45]. The actor network translates the UAV's reduced observations into a probability distribution, through the softmax function, over the 9 possible actions. The critic network takes the reduced observations and produces an estimate to the value function to update the networks. In the proposed CLDE training method, each UAV agent acts based on a copy of the actor, and the acquired experiences and rewards are then used to centrally update a common policy, before being distributed to the agents again. After the training is completed, the agents are deployed on UAVs, where each UAV gets a copy of the\npolicy network and operates independently based on its own observations. It is worth mentioning that the input maps to the actor and critic are typically normalized for optimal performance."}, {"title": "3.3.4. Reward Function", "content": "The reward function is essential in guiding the training and ensuring fast convergence. To achieve this, a shaped and joint reward function is used. A shaped function ensures that the agents get feedback more frequently throughout the episode. This is unlike a sparse reward function that only gives feedback very few times throughout the episode, usually only when the episode is completed successfully. A joint reward function is one where all the agents receive the same reward based on the collective behavior. Here, following a set of actions by the agents in a timestep, all the agents receive the same reward based on the new state of the environment. This pushes the agents to take actions that benefit the team, which is essential in collaborative tasks.\nAt a given timestep t, after the agents have executed the joint action a', the environment returns a reward Rt, based on the following function:\n$R_t = (coverage \u2013 1) + (connectivity \u2013 1)$ (4)\nIn this function, coverage is computed as the portion of vehicles, out of all vehicles, which are covered by their corresponding UAVs, which results in a value between 0 and 1. A vehicle is considered to be covered if it lies within the communication range of its allocated UAV. The connectivity metric has a binary value (either 0 or 1), where 1 indicates that all the UAVs in the network are linked, and 0 indicates otherwise. The UAVs are considered to be linked if there is a communication path connecting each two UAVs in the network, either directly (i.e. within each other's communication range) or indirectly (through intermediate UAVs that are within each other's communication range). In Eq. 4, each attribute has a penalty of (-1) value, which generally helps achieving the desired outcome faster while training. Hence, the maximum value in a time-step is 0, indicating full connectivity and coverage, while the minimum is -2, indicating no connectivity and no coverage."}, {"title": "3.4. Process Sequence Diagram", "content": "Figure 4 shows the interactions between the different components in the proposed framework. It presents the interactions between vehicles, UAVs, the proposed smart contract and the IPFS system. The interaction encapsulates the following stages:\n\u2022 User Registration. Vehicles and UAVs register to IoV by providing the required information to the registerVehicle() and registerUAV() functions respectively. The respective function encapsulates the information in a vehicle or UAV object and appends the object to the respective list Vehicle List/ UAV List. Each vehicle/UAV is assumed to have a single Ethereum address linked to its account for the reputation.\n\u2022 Vehicle Selection Stage. Vehicles retrieve the UAVs in their zone by invoking the getZone UAV() function, which returns the addresses of UAVs as a list. Then, the vehicle can get the detailed information about the UAVs by sequentially invoking the getUAVInfo() function with the address of each UAV and retrieving the UAVInfo. Once the UAV information is retrieved, the vehicle runs the selection mechanism proposed in Algorithm 1 to determine the UAV it selects. The selection is then submitted through the submitVehSelection() function.\n\u2022 UAV Selection Stage. The allocateZone() function is executed to select UAVs for each zone based on the submitted proposals by vehicles according to Algorithm 2. Upon the completion of the selection, UAVs are notified about vehicles they are expected to route for.\n\u2022 Coordination. Upon selection of vehicles to UAVs, the coordination stage is initiated. IoV determines the MDRL model to dispatch based on the characteristics of the UAV and returns an identifier to retrieve it from IPFS. Once the MDRL model is retrieved, the UAV can follow the actions suggested by the model based on its observations to optimize its location, while continuously retrieving the information of other UAVs from IoV.\nDuring the coordination phase, certain practical challenges, such as UAV battery drainage, can impact the sustain-ability of the system. While the proposed framework focuses on optimizing coverage and connectivity through efficient coordination, it is essential to acknowledge that UAVs cannot remain active indefinitely due to energy constraints. To address this, one simple solution is to replace depleted (or malfunctioning) UAVs with ones from the available pool through UAV selection, following the process discussed in Section 3.2. Alternatively, the proposed method can be complemented through existing solutions that address energy management challenges, such as deploying charging stations to enable UAVs to recharge during operations [46, 47]."}, {"title": "4. Simulation and Evaluation", "content": "The evaluation conducted for the proposed framework and MDRL method is divided into three main components. First, the performance of the UAV selection mechanism is evaluated at an instance of time. Second, the performance of the MDRL model is analyzed and compared with existing benchmarks. Third, the cost analysis and scalability of the proposed framework and incorporated mechanisms are discussed to verify its cost efficiency and scalability."}, {"title": "4.1. Simulation Setup", "content": "Table 5 summarizes the used evaluation setup for the UAV selection mechanism. Here, the evaluation is conducted on MATLAB 2023b where a dataset of vehicles and UAVs is generated as described in [10]. A fixed number of vehicles is generated within the simulation area while an increasing percentage of UAVs is generated alongside the vehicles to evaluate the scalability. The attributes of vehicles and UAVs are initialized randomly based on a uniform distribution. The maximum value for each attribute is indicated in the table where the max distance is based on the range of the zone while the other metrics are based on the ranges of the metrics. The weights used in Eq. 1 and 2 are set to 0.25 to give equal contribution to the normalized attributes. The simulation area is fixed at 50 km \u00d7 50 km to control the complexity of the environment. By varying the number of UAVs and vehicles in the area, the experiments effectively adjust the density of the agents. This introduces the same level of complexity as fixing the team size and varying the area size, as both scenarios alter the density of agents and influence the dynamics of coverage and connectivity. This allows for a systematic evaluation of the scalability and adaptability of the proposed methods.\nFor MDRL, the simulations are performed using Python on an Intel E5-2650 v4 Broadwell workstation, which features 128 GB RAM, an 800 GB SSD, and an NVIDIA P100 Pascal GPU with 16GB of HBM2 memory. For all the experiments, each model is trained for 1 million environment steps using MDRL. At the beginning of each episode, the vehicles are randomly placed in the environment and assumed to be assigned to each UAV based on the selection mechanism. The UAV is placed such that it covers its assigned vehicles. An episode continues for 100 steps, where the aim is to maximize the coverage and connectivity. For each 40,000 training steps, the agents are placed in a testing environment for 4,000 steps where they act greedily (by always taking the most valued action) based on their latest policy. The average performance of the testing steps is then recorded and plotted. The hyperparameters used in the PPO method are summarized in Table 6, which are based on the original work in [41].\nThe same simulation setup was used for both the selection mechanism and the UAV coordination task. However, it is important to note that these two components target distinct aspects of the framework. The selection mechanism"}, {"title": "4.2. Selection Mechanism", "content": "The proposed selection mechanism is evaluated against a modified version of the Nearest Neighbor Matching (NNM) mechanism to compare the resulting matching based on different matching methodologies. NNM utilizes propensity score [48"}]}