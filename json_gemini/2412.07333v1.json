{"title": "Fusion Embedding for Pose-Guided Person Image Synthesis with Diffusion Model", "authors": ["Donghwna Lee", "Kyungha Min", "Kirok Kim", "Seyoung Jeong", "Jiwoo Jeong", "Wooju Kim"], "abstract": "Pose-Guided Person Image Synthesis (PGPIS) aims to synthesize high-quality person images corresponding to target poses while preserving the appearance of the source image. Recently, PGPIS methods that use diffusion models have achieved competitive performance. Most approaches involve extracting representations of the target pose and source image and learning their relationships in the generative model's training process. This approach makes it difficult to learn the semantic relationships between the input and target images and complicates the model structure needed to enhance generation results. To address these issues, we propose Fusion embedding for PGPIS using a Diffusion Model (FPDM). Inspired by the successful application of pre-trained CLIP models in text-to-image diffusion models, our method consists of two stages. The first stage involves training the fusion embedding of the source image and target pose to align with the target image's embedding. In the second stage, the generative model uses this fusion embedding as a condition to generate the target image. We applied the proposed method to the benchmark datasets DeepFashion and RWTH-PHOENIX-Weather 2014T, and conducted both quantitative and qualitative evaluations, demonstrating state-of-the-art (SOTA) performance. An ablation study of the model structure showed that even a model using only the second stage achieved performance close to the other PGPIS SOTA models. The code is available at https://github.com/dhlee-work/FPDM.", "sections": [{"title": "Introduction", "content": "The generation of realistic virtual human images is an important research area in computer vision. One subfield of person image generation research, pose-guided person image synthesis (PGPIS), aims to generate a person's image in a given pose while maintaining the appearance of the source person image (Liao et al., 2024). In practice, PGPIS can be useful for data augmentation to improve machine learning (ML) model performance, and commercially, it has a wide range of applications, including virtual reality (VR) and e-commerce (Sha et al., 2023). Since the PGPIS task was proposed by (Ma et al., 2017), research has mainly been based on Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) and Diffusion Models (Ho et al., 2020). The focus of these studies is to synthesize realistic images, and more importantly, images that correspond to the specified poses while maintaining spatial and temporal consistency.\nUntil recently, PGPIS research has been primarily based on GANs (Liu et al., 2022; Ren et al., 2022; Tang et al., 2022; Zhang et al., 2022; Zhou et al., 2022), and the results of the generated models have been impressive. However, as mentioned by (Iglesias et al., 2023), the min-max objective for the adversarial approach is unstable, and the instability of this learning process makes it difficult to produce high-quality images. For example, the results reported by (Albahar et al., 2021) on GAN-based models show blurriness of detailed body shapes and distortion of detailed patterns on clothes. Recently, the diffusion model (Ho et al., 2020) has been actively studied in the field of image generation and has been successful in generating high-quality images. As a result, research on conditional diffusion models (Ho & Salimans, 2021)(Dhariwal & Nichol, 2021) has led to PGPIS model studies such as PIDM (Bhunia et al., 2023), PoCoLD (Han et al., 2023), CFLD(Lu et al., 2024), and PCDM(Shen et al., 2024). These studies condition the target pose and source image to produce images of a person following the target pose while maintaining the appearance of the source image, which surpasses the results of GAN-based models. However, in most approaches, the relationship between the target pose and the source image conditions is learned during the training process of the diffusion model, making it difficult to effectively transfer the semantic information from the source image to the target pose. This difficulty in reflecting conditions in the generated image complicates the model structure and impedes improvements in generation results.\nIn this study, we propose Fusion Embedding for PGPIS with Diffusion Model (FPDM), which is inspired by using the CLIP (Radford et al., 2021) text encoder in the text-to-image diffusion model proposed by (Rombach et al., 2022). Our proposed method consists of a two-stage model. In the first stage In the first stage, the CLIP image encoder is used to obtain image-level embeddings of the source image and the target pose, and the two embeddings are fused using the Combiner module (Baldrati et al., 2023). The aggregated embedding is then trained using contrastive learning to align with the target image embedding, which is extracted using the same encoder as the source image. In the second stage, the conditional diffusion model uses the DINOv2 (Oquab"}, {"title": "Related Work", "content": "Diffusion Model. Diffusion Model (DM) (Ho et al., 2020) are image generation models that have been actively studied in recent years and have shown remarkable performance in synthesizing high-quality images. The DM consists of a forward process and a reverse process, and through a distribution-based step-by-step generation method, it has achieved high-quality image generation performance. Diffusion models have also been studied for condition-based image generation. (Dhariwal & Nichol, 2021) introduced a classifier-guided diffusion model, Meanwhile (Ho & Salimans, 2021) proposed a classifier-free guidance approach that incorporates the generation conditions without using a classifier module. (Ho et al., 2020) showed that latent denoising is effective in the reverse process of DMs. The Latent Diffusion Model (Kingma et al., 2021) proposed a structure that performs diffusion in the latent space to reduce computational resources and increase the stability of model training and inference. Accordingly, Stable Diffusion (SD) (Rombach et al., 2022) achieved significant image generation results by reducing the input image to latent space using a pre-trained VAE and applying Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020). Additionally, SD successfully generates images using text embeddings from the pre-trained CLIP text encoder.\nPose Guided Person Image Synthesis. Since (Ma et al., 2017) first presented the Pose Guided Person Image Synthesis (PGPIS) model, research has been mainly based on GANs. However, the min-max objective approach for competitive learning based on GANs is unstable, and the instability of the learning process makes it difficult to generate high-quality images. Recently, DM-based PGPIS models have been actively researched to achieve SOTA performance. The first proposed PIDM (Bhunia et al., 2023) used the Diffusion Model (DM) and simply extracted the features of the condition using a convolutional neural network (CNN) and applied them to DM. Later studies were conducted with the Latent Diffusion Model (LDM). PoCOLD (Han et al., 2023) and CFLD (Lu et al., 2024) use the Attention module to extract features that focus on the generated image using the features of the source image. PCDM (Shen et al., 2024), RePoseDM (Khandelwal, 2024), and X-MDPT (Pham et al., 2024) use Transformer models to perform PGPIS by deeply learning the relationship between the source and target pose images. PCDM (Shen et al., 2024), RePoseDM (Khandelwal, 2024), and X-MDPT (Pham et al., 2024) use Transformer to perform PGPIS by deeply learning the relationship between the source and the target pose images. Specifically, PCDM consists of three diffusion stages of model construction. RePoseDM and X-MDPT aggregate the embeddings of the source image and target pose and input them as conditions to the generation model."}, {"title": "Method", "content": "Preliminary. The Stable Diffusion (SD) model (Rombach et al., 2022) is based on the Latent Diffusion Model (LDM) (Kingma et al., 2021). LDM consists of two main components: the Variational Autoencoder (VAE) (Esser et al., 2021), which encodes the image into a low-dimensional latent space, and the UNet (Ronneberger et al., 2015), which denoises the noisy latent vectors.\nThe SD model employs the Denoising Diffusion Probabilistic Model (DDPM), which comprises a forward diffusion process and a backward denoising process. In the forward process, the noise latent $z_t$ at timesteps $t \\in [1,T]$, where T is 1000 for DDPM, is obtained by adding random Gaussian noise $ \\epsilon \\thicksim N(0, I)$ to the initial latent $z_0$, which corresponds to the input image. The $z_t$ can be formulated as follows:\n$z_t = \\sqrt{\\bar{\\alpha}_t} z_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$ (1)\nwhere $ \\alpha_t$ represents the cumulative product of the noise schedule, and $\\alpha$ is typically assigned a very small value, which increases the noise in the latent $z_t$ as the timestep t increases. According to Equation 1, the initial latent $z_0$ can be used to obtain the latent $z_t$ at each stage of the diffusion process. The backward process aims to reduce prediction error by estimating the noise $ \\epsilon$ in the latent $z_t$ at timestep t. This is done by inputting the timestep t, latent $z_t$, and condition c into the UNet $\\epsilon_\\theta(z_t, t, c)$. The backward process is designed to reduce the prediction error by predicting the noise of the latent $z_t$ at time step t by inputting time step t, latent $z_t$, and condition $ \\epsilon$ into the UNet $\\epsilon_\\theta(Z_t, t, c)$. The objective can be formulated as follows:\n$L = \\mathbb{E}_{z_0, c, \\epsilon, t} [ || \\epsilon - \\epsilon_\\theta(z_t, t, c) ||^2 ]$ (2)\nwhere c represents the conditioning embedding vector produced by the text encoder of the CLIP (Radford et al., 2021). The image generation process begins by sampling random Gaussian noise $z_T \\thicksim N(0, I)$, and then uses the trained UNet $ \\epsilon_\\theta(z_t, t, c)$. This process enables the generation of an image that satisfies the condition c while gradually removing the noise in the backward process."}, {"title": "Image-Pose Fusion Model", "content": "In the first stage, the Image-Pose Fusion (IPF) Model learns to align the representation of the target image with the fused representation of the source image and the target pose. we employ the Vision Transformer (ViT) model from the CLIP framework (Dosovitskiy et al., 2020) to extract the embeddings of the source image xs the target pose image Xtp and the target image xt. Furthermore, the Combiner module (Baldrati et al., 2023) is utilized to integrate the source image xs and the pose image Xtp. The embedding of the target image and the fusion embedding are learned using the InfoNCE loss (Oord et al., 2018) within a contrastive learning framework, thereby aligning the disparate representations.\nAs illustrated in Figure 1(a), this is an overview of the IPF Model. In the context of the IPF Model, the image and pose encoders extract image-level embeddings, denoted by es, etp, and et, from a source image xs, a pose image Xtp, and a target image xt. The extracted embeddings, es, and etp are then input to the Combiner module, which fuses them to produce the fusion embedding, ef. The final fusion embedding ef is contrastively learned using InfoNCE loss, with the embedding et of the target image serving as the reference point. In this study, the embedding of the source image is incorporated into the InfoNCE loss batch, thereby facilitating the separation of the embedding space of the person image according to the pose. We considered two learning methods approach.\nBaseline Approach. The objective of the baseline approach is to minimize the distance between the combined features and the target image features belonging to the same triplet while also maximizing the distance from the features of other target images in the same batch. To this end, following (Baldrati et al., 2023), we employ a batch-based InfoNCE contrastive loss as follows:\n$L_{contr} = - \\frac{1}{B} \\sum_{i=1}^{B} log[\\frac{exp( \\tau \\times d(\\psi_f^i, \\nu_t^i))}{\\sum_{j=1}^{B} exp( \\tau \\times d(\\psi_f^i, \\nu_t^j))}]$ (3)\nWhere Wf and Vt denote ef and et respectively, d represents the cosine similarity, $ \\tau$ is a temperature parameter controlling the range of the logits, and B is the number of source image, target pose, target image triplets in a batch."}, {"title": "Source-Enhanced Pose Fusion Approach", "content": "In this study, we aim to separate the embeddings of source and target person images according to their poses in the embedding space. We integrate the source image embedding into the learning process, as detailed in Eq. 4 and Eq. 5. During the contrastive learning process, integrating the source image results in doubling the batch size, as illustrated in Eq. 6.\n$\\psi_f^p = {\\psi_f^i}^{2B}_{i=1} = {\\{\\psi_f^i,  if 1 \\leq i \\leq B \\{\\psi_s^{i-B},  if B < i \\leq 2B}$ (4)\n$\\nu_t^p = {\\nu_t^i}^{2B}_{i=1} = {\\{\\nu_t^i,  if 1 \\leq i \\leq B \\{\\nu_t^{i-B},  if B < i \\leq 2B}$ (5)\n$L_{contr} = \\frac{1}{2B} \\sum_{i=1}^{2B} log [\\frac{exp( \\tau \\times d(\\psi_f^p, \\nu_t^p))}{\\sum_{j=1}^{2B} exp( \\tau \\times d(\\psi_f^p, \\nu_t^p))}]$ (6)\nWhere d denotes the cosine similarity, $ \\tau$ is a temperature parameter adjusting the range of the logits, and B is the number of triplets in a batch. The results of the learning approaches are summarized in the Ablation Study Section."}, {"title": "Fusion Embedding Conditioned Diffusion Model", "content": "The model proposed in this study is inspired by (Shen et al., 2024) and simplifies their structure. The FPDM model proposed in this study is based on the SD model and consists of a pose encoder Hp, a source image encoder Hs, and an IPF module in the prior stage. For the source image encoder Hs, we employed the ViT model, and for the pose image encoder Hp, we employed a pose encoder with four-layer convolutional layers introduced by ControlNet (Zhang et al., 2023). In this study, we visually verified that a pose image consisting of just three RGB channels can effectively generate a pose-appropriate image, unlike most other studies that represent each joint connection in a pose image as a channel.\nFigure 1 (B) illustrates the proposed FPDM model. With the source image encoder Hs and IPF module frozen, the noise zt is added to the target pose embedding etp after a 1-layer convolutional operation, and then input to $\\epsilon_\\theta(z_t, t, e'_s, e_{tp}, e_f)$ of the UNet model. Here, the noise zt added with the target pose embedding $e'_{tp}$ is input to Unet.\nThe embedding e' of the source image xs extracted from the source image encoder Hs is used as the key and value in the transformer block of the UNet. The fusion embedding ef from the previous stage is added with the timestep embedding and propagated to the ResNet block of the UNet. The output zt from the UNet is used to estimate the noise $ \\epsilon$. The objective function L is given by Eq. 7.\n$L_{mse} = \\mathbb{E}_{z_0, e_s, e_{tp}, e_f, \\epsilon, t} [ || \\epsilon - \\epsilon_\\theta(z_t, t, e'_s, e_{tp}, e_f) ||^2 ]$ (7)\nWhere e', etp, and ef represent, respectively, the source embeddings obtained from the source images, the pose embeddings of target poses, and the fusion embeddings of source and pose images.\nIn the image generation process, once the conditional latent diffusion model is trained, inference can be performed as described in the preliminary section. We adopt the cumulative classifier-free guidance to enhance fusion guidance as follows:\n$ \\epsilon_t = \\epsilon_\\theta(z_t,t, \\phi, \\phi, \\phi) + w_c \\epsilon_\\theta(z_t, t, e'_s, e_{tp}, \\phi) \\ +w_f \\epsilon_\\theta(z_t, t, e'_s, e_{tp}, e_f)$ (8)\nWhere wc and wf are the weights of input data."}, {"title": "Experiments", "content": "The proposed model is evaluated using the benchmark datasets DeepFashion In-shop Clothes Retrieval Benchmark (DeepFashion) (Liu et al., 2016) and RWTH-PHOENIX-Weather 2014T (Phoenix) (Camgoz et al., 2018). The Deep-Fashion dataset contains 52,712 high-resolution fashion images, and we followed the data configuration described in Lu et al. (2024). The model was evaluated at resolutions of 256\u00d7176 and 512\u00d7352, with the input images resized to 256x256 and 512\u00d7512, respectively. PHOENIX14T is a German sign language benchmark dataset consisting of 7,738 videos, performed by nine different signers wearing dark clothes in front of a gray background, with a resolution of 210\u00d7260 pixels. We followed the original dataset settings, using 7,096 videos for training and 642 for testing. Pose images were generated by converting pose coordinates extracted using HRnet (Wang et al., 2020), as provided by Guan et al. (2024). To construct the training data, we randomly extracted source image, target pose, target triples from video units, generating a total of 100,000 pairs. For testing, we used the first frame as the source image and the target pose as the pose sequence for each frame in the video, creating a dataset of approximately 64,627 pairs.\nMetrics\nWe evaluated the performance of our proposed FPDM model using several metrics. For the DeepFashion dataset, we used image pixel-level Structural Similarity Index Measure (SSIM) (Zhou et al., 2004), Peak Signal-to-Noise Ratio (PSNR), Deep Feature-based Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), and Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) as evaluation metrics. In addition to SSIM and FID, the Phoenix hand language dataset was evaluated using Hand SSIM (Saunders et al., 2022) and Hand PE (Saunders et al., 2022), which are important for assessing the quality of hand generation. These two metrics compare the generated image to the correct image in a cropped hand region (i.e., a 60x60 patch centered around the ground truth middle knuckle). Hand SSIM is the SSIM score of the cropped hand image. Hand PE is the L1 distance between 2D hand key points. We used the HRNetv2 (Wang et al., 2020) model to extract hand key points.\nImplementations\nWe conducted our experiments on a single NVIDIA A100 80G GPU. Our method was implemented using Python 3.9 and PyTorch 2.4 (Paszke et al., 2019). In the prior stage, we used the HuggingFace CLIP-large model for both the image and pose encoders. Due to resource limitations, we shared the pose and image encoders. We employed the AdamW optimizer with a fixed learning rate of 1e-4 and a weight decay rate of 0.0001, training for 5 epochs. For the generative model, we used HuggingFace Stable Diffusion 2.1 (Rombach et al., 2022) and a DINOv2-large model as the source image encoder with an input size of 512. The learning rate of 0.0001 underwent a linear warmup during the first 5 epochs and was multiplied by 0.1 after 50 epochs. For classifier-free guidance, we set we and wf to 2.0 during sampling, and the probability of n = 20% to drop e's, etp, and ef during training. Following (Lu et al., 2024; Ren et al., 2022), we trained our models using images of size 512\u00d7352 for the DeepFashion dataset. We trained for a total of 100 epochs, but if the training loss did not decrease sufficiently during the experiment, we performed an additional 10 epochs of training. After training, we evaluated at a resolution of 512\u00d7352. For evaluation at 174\u00d7256, we adjusted the source image resolution to 256x256 and conducted an additional 10 epochs of training. For the Phoenix dataset, we conducted 20 epochs of training with a generation size of 512\u00d7512. For evaluation, the image size was set to 260\u00d7210 pixels.\nQuantitative and Qualitative Results\nThe proposed model was compared quantitatively and qualitatively with SOTA methods using the DeepFashion dataset. FID, LPIPS, SSIM, and PSNR metrics were used for evaluating the performance of the models. As mentioned in (Han et al., 2023), the DeepFashion dataset has a limitation in evaluating with FID score because there is a shift between the train and the test set, and the FID value of the ground truth in Table 1 is 8.010. Therefore, in this study, FIDt, which measures the FID of the test dataset, was introduced and evaluated. As shown in Table 1, our proposed method outperforms other state-of-the-art models across all metrics except for FID. For deep feature-based scores, our method ranks second in FID but outperforms all others in FIDt and achieves the best performance in LPIPS. Our model also achieves the best performance in SSIM and PSNR, which assess image pixel-level similarity. This quantitatively demonstrates that the proposed FPDM can accurately transfer the texture of the source image to the target pose while maintaining high consistency compared to other models."}, {"title": "Ablation Study", "content": "Image-Pose Fusion Model. To evaluate our proposed Image-Pose Fusion (IPF) Model, we conducted several comparison experiments. Al is the embedding of the source image using the CLIP image encoder. A2 is the case of learning the IPF model with the baseline approach (see. Eq.1). A3 is the IPF model trained with the Source-Enhanced Pose Fusion approach (see. Eq.3). Table 2 shows that the IPF model using method A3 retrieved the target image with 99% accuracy in R@1. This result demonstrates that the fusion embedding generated by the Source-Enhanced Pose Fusion approach aligns effectively with the target. The effectiveness of this approach is also evident in the visualization results shown in Figure 4. Methods A1 and A2 tend to retrieve images similar to the source, while A3 successfully retrieves the target image as the top 1. Additionally, A3 retrieves images with the same visual pose and similar textures, indicating that the Source-Enhanced Pose Fusion approach effectively incorporates pose into the fusion embeddings.\nFusion Embeddings Conditioned Diffusion Model. We experimented with different configurations of the model. B1-B3 are models without IPF modules, while B4-B6 are models with varying configurations of IPF modules. B1 uses CLIP-large as the source encoder with an input image size of 224. B2 uses the DINOv2-large model with an input image size of 224. B3 uses the DINOv2-large model with an input image size of 512. B4-B6 has the same source image encoder settings as B3. B4 uses the CLIP model to estimate the image-level embedding of the source image, replacing the IPF module. B5 uses the IPF module trained with the Baseline approach (see Eq. 3). B6 uses the IPF module trained with the Source-Enhanced Pose Fusion approach (see Eq. 6). Table 3 shows the quantitative performance of the models in the ablation study. In B1-B2, we observe that the CLIP model performs better in LPIPS, SSIM, and PSNR scores when the image encoder's input size is 224. When the input size of the source image encoder is increased to 512, as shown in B3, there is a significant increase in model performance across all metrics. This is close to the performance of other state-of-the-art models. Models B4 and B5, which include image-level embeddings, perform better than model B3. The best performance is seen for model B6. Figure 5 visualizes the results of the ablation study. As shown in the figure, the visualization results for the B3-B6 models with increased input image size are superior to those of B1-B2 models with smaller input sizes, with the B6 model being more visually realistic than the B3-B5 models. Thus, The comparison of visualization results with SOTA models was conducted using the B6 model.\nEffect of the Fusion Embeddings. The model proposed in this study generates target images by leveraging fusion embeddings from both the source image and the target pose. This approach ensures that target images are generated consistently, even when the source image changes because the trained fusion embeddings align well with the representation of the target image. Figure 6 demonstrates that the B6 model, which uses the Source-Enhanced Pose Fusion approach-based IPF module, consistently produces the same target image despite changes in the source image, as long as the pose remains unchanged (as seen in the third and sixth rows).\nApplications on Sign Language Video generations\nWe evaluated the FPDM model proposed in this study on the Phoenix dataset and found that it generates high-quality sign language images. The quantitative results presented in Table 4 indicate that our model outperforms all other models across all metrics. Figure 7 shows that the images produced by our model are not only highly natural but also clearer compared to the lower-quality ground truth images. We anticipate that these results will be valuable not only for generating natural sign language images but also for augmenting training data for sign language recognition and video generation models. Additional visualization results can be found in Supplement B."}, {"title": "Limitations", "content": "The performance differences observed in the ablation study for B3-B6 are noticeable but not substantial. However, the proposed B6 method using Source-Enhanced Pose Fusion embedding demonstrates consistent performance improvement across all metrics, as evidenced by the visualization results. This indicates that a fusion embedding well-aligned with the target image can produce reliable results. Additionally, many studies have included user evaluations. Obtaining meaningful results from user studies requires sophisticated experimental designs and high costs, with experiments needing to be conducted under consistent conditions for accurate comparisons. Therefore, this study did not include a user study due to resource constraints. However, we present randomized experimental results in Supplement C for visual comparison and have released the generated results from the test dataset to facilitate performance comparisons in future experiments."}, {"title": "Conclusion", "content": "In this paper, we propose a new approach for Pose-Guided Person Image Synthesis (PGPIS) using diffusion. Our method, called Fusion Embedding for PGPIS with Diffusion Model (FPDM), leverages a fusion embedding aligned with the target image as a condition for the Latent Diffusion Model (LDM), which is robust to variations in the source image or pose. Through various experiments, we demonstrate that FPDM surpasses current state-of-the-art methods in PGPIS, both quantitatively and qualitatively. However, generating detailed patterns Effectively from the source image to the target image remains challenging. Future work will explore whether FPDM can be extended to image synthesis tasks that improve the understanding of contextual information within the source image."}]}