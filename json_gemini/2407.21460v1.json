{"title": "Multi-agent Assessment with QoS Enhancement for HD Map Updates in a Vehicular Network", "authors": ["Jeffrey Redondo", "Nauman Aslam", "Juan Zhang", "Zhenhui Yuan"], "abstract": "Reinforcement Learning (RL) algorithms have been used to address the challenging problems in the offloading process of vehicular ad hoc networks (VANET). More recently, they have been utilized to improve the dissemination of high-definition (HD) Maps. Nevertheless, implementing solutions such as deep Q-learning (DQN) and Actor-critic at the autonomous vehicle (AV) may lead to an increase in the computational load, causing a heavy burden on the computational devices and higher costs. Moreover, their implementation might raise compatibility issues between technologies due to the required modifications to the standards. Therefore, in this paper, we assess the scalability of an application utilizing a Q-learning single-agent solution in a distributed multi-agent environment. This application improves the network performance by taking advantage of a smaller state, and action space whilst using a multi-agent approach. The proposed solution is extensively evaluated with different test cases involving reward function considering individual or overall network performance, number of agents, and centralized and distributed learning comparison. The experimental results demonstrate that the time latencies of our proposed solution conducted in voice, video, HD Map, and best-effort cases have significant improvements, with 40.4%, 36%, 43%, and 12% respectively, compared to the performances with the single-agent approach.", "sections": [{"title": "I. INTRODUCTION", "content": "One of the primary challenges in autonomous vehicles (AVs) is realising precise self-localization within their environment to ensure passengers' safety during transit. To achieve this objective, a high-definition (HD) Map provides extra information including around buildings, road markings, traffic signals, dynamic road signals, etc. [1], with centimeter- level accuracy. This accuracy is essential for ensuring safe and high-quality driving experience. HD Map has become a crucial application [2] in achieving the higher levels of automation stated by the Society of Automotive Engineers (SAE) [3] for AVs. However, deployment of HD Map is complicated due to the dynamic nature of road environments, where the constant changes drive the necessary updates to the HD Map, making the HD Maps generation particularly challenging due to the processing requirement of extensive data from sensors such as LiDAR or cameras. Some researchers have demonstrated that the processing time of tasks can be reduced by offloading the raw data to cloud/fog/edge servers [4]\u2013[6]. It is worth noting that the study conducted in [6] reported a remarkable 66% decrease in processing time. In addition, other authors [5] incorporated and demonstrated how the use of crowd-sourcing, and RL approach in edge computing reduces the processing time for HD Map. To support the offloading of HD Map data, wireless communication systems must be capable of handling heavy traffic and low-latency applications. However, vehicular ad hoc networks (VANET) suffer from the challenge of guaranteeing quality of service (QoS) requirements in dynamic traffic flow environments [7], specifically caused by the increase of packet collisions due to the fixed contention window (CW) [8], [9]. In order to address the challenge of allocating the appropriate CW in a dynamic network, numerous experts have suggested leveraging artificial intelligence (AI) solutions. These studies have investigated a range of methods, such as single-agent [10], [11] and multi-agent RL approaches [12], [13]. However, recent research has shown a strong preference for multi-agents in wireless network problems due to their superior effectiveness in navigating uncertain and non-stationary environments [14]. In these multi-agent setups, individual vehicles can function as agents engaged in various tasks such as data transmission for applications including voice, video, HD mapping, and autonomous driving scenarios. For instance, in [13], each vehicle is designated as an agent, showcasing the effectiveness of a multi-agent approach in enhancing resource allocation within a vehicle-to-vehicle (V2V) communication setting. However, previous investigations have either acknowledged the use of the HD Map application [5] or disregarded it [4], [13], [15], [16]. Nonetheless, neither of these solutions has evaluated or considered the effects of incorporating various types of services simultaneously. Additionally, these solutions are limited in scalability when it comes to accommodating new applications, as they are MAC layer-based [7], [10], [17], [18], which necessitates standard modifications. To address this, a novel single agent RL QoS coverage-awareness algorithm both non-intrusive and tunable has been proposed in [19] demonstrating an enhancement in terms of latency and throughput. The solution operates on the application level, thus, it does not require any modification to the existing standard."}, {"title": "A. Challenges", "content": "Single-agent RL is widely used to demonstrate the improve- ment in network performance [10], [11]. Nevertheless, in a highly dynamic environment of large scale, a single-agent ap- proach might suffer from high-dimensional state-action spaces along with the increase in the computational load [20]. To mitigate this limitation, the multi-agent approach is consid- ered to reduce the complexity of the problem and improve the learning process by understanding its local environment and subdividing tasks [20]. Implementing RL as a single or multi-agent approach presents many challenges including high dimensionality in terms of the number of features for the state and action space, and scalability in terms of the number of agents."}, {"title": "a) High Dimensionality:", "content": "High dimensionality is a sig- nificant challenge in solving problems with a single-agent approach when agents must control a system with a large state and action space. To overcome this limitation, multi- agent systems are often employed because of their ability to interact effectively with the local environment. For instance, the authors in [12] have proposed a solution where the state space has a high dimensionality. The state space includes task profile, edge computing status, and vehicle speed. This state space will be extensive in the setup of a single agent, thus using a multi-agent in the local environment can reduce the state space size. Moreover, their algorithm is based on an asyn- chronous advantage actor-critic (A3C), global actor, and critic learning approaches which need to transfer data from vehicles to the global actor-critic. This increases the bandwidth usage in a wireless network. In addition to disseminating an agent's learning experiences across a global network, agents may also need to exchange additional types of information, such as their states, actions, or reward functions. For example, in [13], a distributed approach is implemented where agents share information that is added to the observation of other agents, which increments the state space. Sharing information among agents can increase complexity and network congestion, which can be addressed through reward modelling as suggested in [21]. In this approach, the reward function provides valuable information without requiring extra feedback from agents (sharing of information). Moreover, having the same reward function offers the advantage of finding the optimal joint action to maximize the rewards of agents [22]. In our study, we refer to the utility function in [15] as the reward function, and add penalties to improve the stability that properly describes the network status. Through this action, the dimensionality can be effectively reduced because no information is directly shared among agents. The reward function provides sufficient information about the overall network in terms of latency and throughput. Moreover, it reduces the communication between agents, freeing up wireless channels for more efficient trans- mission of user plane data instead of the control plane. Overall, this reward mechanism enables a streamlined and optimized network operation."}, {"title": "b) Scalability:", "content": "Could the same single-agent strategies be used in the multi-agent scenarios, more specifically could the multi-agent strategy perform better? This problem has been stated in [23]. Thus, similar to the increase in dimensionality, a challenge arises when scaling the solution in multi-agent systems due to the sharing of information between agents (in both state and action spaces). This potentially increases overhead due to the exponentially large number of agents [24]. Therefore, to elaborate on the design of an RL multi-agent algorithm for VANET, several questions arise: How many agents must be selected for a VANET scenario? Could each service (Voice (VO), Video (VI), Best-Effort (BE), HD Map) be set as an agent? Or would having each AV as an agent be more efficient? These are questions addressed in our work."}, {"title": "c) Compatibility and Computational Capacity:", "content": "There have been efforts to address the challenge of CW assignment, yielding positive outcomes in terms of latency, throughput, and fairness. However, translating these findings into practical implementation is more complex than it may initially appear, requiring modifications to the existing standard. Additionally, most solutions work with deep reinforcement learning (DRL) policy gradient, advantage actor-critic (A2C), and A3C, which increases the computational cost and might further overload the Onboard Unit (OBU) of the AV. Thus, our solution operates on the application layer to solve the compatibility problem. Specifically, it utilizes a Q-learning approach to reduce the computation capacity requirements."}, {"title": "B. Contributions", "content": "The main contributions of this paper are summarized as follows:\n\n\u2022 A novel distributed and lightweight multi-agent solution that effectively improved QoS in IEEE802.11p networks for HD Map is proposed. The solution employs Q-learning and the same reward function for each agent while addressing the high dimensionality issue and re- ducing computational complexity. The proposed approach ensures QoS while demonstrating a significant improve- ment in overall performance.\n\u2022 Two distinct multi-agent setups, which demonstrate the versatility of multi-agent systems in allocating wireless resources to ensure QoS, are evaluated. Additionally, a distributed multi-agent solution that exceeds the per- formance of centralized single-agent approaches is also introduced.\n\u2022 The performance between centralized and distributed learning in wireless resource allocation is extensively as- sessed. We delve into the intricacies of these approaches to understand their respective advantages and drawbacks. Thus, we explored the implication of implementing the machine learning solution directly on the AVs or at the edge server. This analysis offers valuable insights into the current wireless resource allocation optimization area of investigation. It provides an understanding of the benefits associated with RL in VANET to offer QoS."}, {"title": "II. RELATED WORK", "content": "To enhance the QoS in the standard IEEE802.11, extensive research has been conducted to improve latency by develop- ing new solutions to allocate CW value and the enhanced distributed channel access (EDCA) mechanism. Approaches such as new access categories, single-agent RL, and multi- agent RL are commonly used in solutions to provide a better QoS, which are discussed in the following subsections."}, {"title": "A. CW, EDCA, and single agent", "content": "There are different ways to improve the performance of a wireless network using the family standard IEEE802.11. One approach is to enhance the EDCA mechanism or the selection of the CW. To achieve this, various solutions have been proposed. For instance, some authors suggested new access categories [25] by extending the number of access categories (AC) from four to seven for different video resolution qualities. Other authors [26] added a low latency AC.\nIn [7] a new AC for HD Map is added to the EDCA showing an improvement in high-density vehicular network. Nevertheless, it is not adaptable to changes in the environment such as the number of vehicles. Besides its implementation requires a change in the standard which is not feasible. Other solutions focus on queuing management [11], [27]\u2013[29]. Nevertheless, these solutions do not consider the tuning of the parameters CW, which plays a crucial role in enhancing the QoS.\nThus, to improve further the QoS, algorithms are developed to select the proper values for the MAC layer parameter of the standard IEEE802.11 by proposing approaches such as Markov model for varying CW [30], game theory [15], AI/ML reinforcement learning Q-learning [10], [31], [32], policy gradient [11], deep RL [15], [18].\nNevertheless, these solutions have one common requirement of modifying the current standard for their implementation, which is not always feasible for worldwide deployment. To overcome this, in [19], we have developed a solution that operates at the application level offering similar priorities that the EDCA and demonstrated an improvement of 10% compared with the new HD Map AC [7] in term of latency."}, {"title": "B. Multi Agent Systems for HD Map", "content": "One of the challenges to overcome in the generation of HD Map is the constraints of wireless transmission resources in a dynamic network [5]. To address this, the authors in [5] proposed a Distributed Adaptive Offloading and Resource reservation (DATE) solution, which uses multi-agent DRL with a global policy network. Results showed an improvement in the overall latency for offloading. Nevertheless, different types of services were not included, and throughput was also not considered as part of the optimization problem. Another study [12] utilized also a multi-agent approach RL- based scheme in a centralized online training manner with an Asynchronous Advantage Actor-Critic (A3C) algorithm. Both studies have implemented a multi-agent with a disadvantage in terms of computational cost, related to the use of a more complex RL algorithm that might require an increase of the computational cost and overhead because of the deep neural networks and the parameters sharing from each vehicle to the global network, respectively.\nTo avoid the overhead of a centralized approach, authors in [13] proposed a decentralized multi-agent solution to manage the wireless resource allocation, which inspires us to imple- ment and test our single-agent solution [19] in a distributed multi-agent manner. One key difference between our approach and the mentioned solution is that their reward function includes the sum of the current capacity (Shannon-Hartley Theorem) of each vehicle [13], which indicates that there is an intercommunication between agents so there would be congestion on the network.\nIn summary, previous solutions fail to consider the integra- tion of different service types, such as voice, video or any other service simultaneously, which are frequently used by end-users. Therefore, it is necessary to expand the compu- tational capacity of AVs by using DRL. We first formulated multiple services, throughput and latency into our optimization problem. Then, we implemented a Q-learning algorithm due to its advantages of requiring less computational load. Thirdly, we utilized the same reward function that includes latency and throughput to avoid information sharing between agents or the global network."}, {"title": "III. PROBLEM STATEMENT", "content": "The VANET environment can be classified as an unknown and partially observable Markov decision process (MDP). This can be represented as a tuple (V, S, A, R, T), where V is the set of AVs, S is the set of states, A is the set of actions, R represents the reward, and T represents the time.\nThe environment comprises a dynamic vehicular traffic flow with AVs denoted by set V = {1, ..., N}. Each vehicle follows a defined route entering and exiting the environment, mimicking the typical dynamics of an urban area where AVs start a route and finish at the destination. Thus, the vehicles do not stay in the environment for the whole simulation. The vehicles are categorized based on their data transmission type (e.g. voice, video, HD map, and best-effort) described by the set of categories C = {1, ..., M}.\nTo ensure specific network performance for each service category, the ultimate goal of our work is to provide and maintain the required throughput and latency for all services. Therefore, we consider throughput R and latency L in the utility function [17]. Besides, we also incorporate penalties and bonuses, defined as F, into our solution to improve stability, as shown in Eq. (1).\n$$U(c) = \u03b1_1 \\frac{R(c)}{R_{max}(c)} - \u03b1_2 \\frac{L(c)}{L_{max}(c)} + F$$\nwhere the weights \u03b11 and \u03b12 provide a trade-off between R and L. Then the maximization problem can be formulated as follows:\n$$\\max_{w_{v,t}} \\sum_{v \\in V} \\sum_{t \\in T} x_{v,t}U_{v,t}(c), \\forall c \\in C, v \\in V, t \\in T$$\nsubject to\n$$x_{v,t} \\in \\{0,1\\}$$\n$$\\frac{1}{|V|} \\sum_{v=1}^{|V|} L_v(c) \\leq L_{max}(c), \\quad C \\in R$$\n$$\\sum_{v=1}^{|V|} R_v(c) \\geq R_{min}(c), \\quad R \\in R$$\n$$w_{v,t}(c) \\leq W_{max}(c), \\quad w \\in R, \\quad w \\neq 0$$\nwhere (4) and (5) indicate the maximum latency and minimum data rate per service type, respectively. The last constraint, described in (6), is the maximum waiting time allowed per category. The variable x is a binary index which can be either 0 or 1, indicating whether a vehicle is allowed to transmit.\nAs described in [19], the problem is difficult to solve analytically due to the nonlinearity and the direct relationship between the CW value and L, as well as the inverse propor- tionality between the CW value and R. Therefore, To address these complexities and find an optimal solution, we employ RL which is particularly adept at uncovering intricate patterns and is crucial in the context of this new multi-agent investigation. Here, each agent seeks to maximize its utility, leading to the complexity of the problem. The agents will learn and select the optimal $w_t$ waiting time before the next data transmission. The notations used in this paper are summarized in Table I."}, {"title": "IV. PROPOSED SOLUTION DESIGN", "content": "In this comprehensive investigation, the Q-learning ap- proach selected in [19] has been extended into a multi-agent environment. This approach is based on the Q-Learning Tem- poral Difference (TD) model-free method offering advantages from dynamic programming and the Monte Carlo method. For instance, the TD method requires less computational resources because it updates estimates incrementally, one step at a time, rather than updating all estimates after every episode. The Q value update is defined by Eq. (7).\n$$Q(s, a) = Q(s, a) + \u03b1 [r + \u03b3 * \\max_{a'}Q(s', a') \u2013 Q(s, a)]$$\nAs stated in [22], it is advantageous to use the same reward function in finding an equilibrium in a cooperative Multi-agent MDP (MMDP). Therefore, we adopt this approach to improve the learning process. Besides, each agent is categorized as an independent learner (IL) to generate a smaller Q-table com- pared to the joint state-action space, $S \\times A^m$. For simplicity, $\\gamma$ is defined as $\\gamma = \u03b3*\\max_{a'}Q(s,a) \u2013 Q(s, a)$, and by replacing the utility function Eq. (1) as the reward r into Eq. (7) then, the updated equation is described as follows:\n$$Q(s, a) = Q(s, a) + \u03b1[U + \u03b3]$$\n1) State: The set of state S is defined as follows [19]:\n$$S = {S_j, T_v, C, T_{cv}}$$\nwhere $S_j$ is the sojourn time, during which a vehicle is within the coverage area of the Roadside Unit (RSU). In the measurement, 0 means the least time and 4 is the longest time. The sojourn time is converted into five discrete values from 0 to 4, for more detail check [19]. The value $T_v$ is the total number of vehicles active with a maximum of N, it is obtained at the edge server side by the Algorithm 1 in [19]. The set of categories C in this investigation refers to the four services voice, video, HD map, and best-effort. Finally, $T_{cv}$ stands for the total number of active vehicles per category c counted by Algorithm 3 in [19].\n$$\\Phi = \\begin{pmatrix} Q^1\\\\ Q^2\\\\ :\\\\ Q^n\\end{pmatrix} = \\begin{pmatrix} E^1 [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1]\\\\ E^2 [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1]\\\\ :\\\\ E^{n} [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1] \\end{pmatrix}$$\n$$\\Phi(c) = \\begin{pmatrix} Q^1\\\\ Q^2\\\\ :\\\\ Q^n\\end{pmatrix} = \\begin{pmatrix} E^1 [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a]\\\\ E^2 [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a]\\\\ :\\\\ E^{n} [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a] \\end{pmatrix}$$\nObserving the state can reveal a reduction in dimensionality, which leads to improved scalability. Initially, agents receive the state with the coverage time value, the total number of nodes connected to the RSU per category, and the category (service type). Using this information, agents can learn an optimal policy for selecting the appropriate waiting time for transmission. In a multi-agent scenario, with different categories assigned to AVs for data transfer, we can consider distinct agent networks."}, {"title": "A. Reinforcement learning", "content": "In this comprehensive investigation, the Q-learning ap- proach selected in [19] has been extended into a multi-agent environment. This approach is based on the Q-Learning Tem- poral Difference (TD) model-free method offering advantages from dynamic programming and the Monte Carlo method. For instance, the TD method requires less computational resources because it updates estimates incrementally, one step at a time, rather than updating all estimates after every episode. The Q value update is defined by Eq. (7).\n$$Q(s, a) = Q(s, a) + \u03b1 [r + \u03b3 * \\max_{a'}Q(s', a') \u2013 Q(s, a)]$$\nAs stated in [22], it is advantageous to use the same reward function in finding an equilibrium in a cooperative Multi-agent MDP (MMDP). Therefore, we adopt this approach to improve the learning process. Besides, each agent is categorized as an independent learner (IL) to generate a smaller Q-table com- pared to the joint state-action space, $S \\times A^m$. For simplicity, $\\gamma$ is defined as $\\gamma = \u03b3*\\max_{a'}Q(s,a) \u2013 Q(s, a)$, and by replacing the utility function Eq. (1) as the reward r into Eq. (7) then, the updated equation is described as follows:\n$$Q(s, a) = Q(s, a) + \u03b1[U + \u03b3]$$\n1) State: The set of state S is defined as follows [19]:\n$$S = {S_j, T_v, C, T_{cv}}$$\nwhere $S_j$ is the sojourn time, during which a vehicle is within the coverage area of the Roadside Unit (RSU). In the measurement, 0 means the least time and 4 is the longest time. The sojourn time is converted into five discrete values from 0 to 4, for more detail check [19]. The value $T_v$ is the total number of vehicles active with a maximum of N, it is obtained at the edge server side by the Algorithm 1 in [19]. The set of categories C in this investigation refers to the four services voice, video, HD map, and best-effort. Finally, $T_{cv}$ stands for the total number of active vehicles per category c counted by Algorithm 3 in [19].\n$$\\Phi = \\begin{pmatrix} Q^1\\\\ Q^2\\\\ :\\\\ Q^n\\end{pmatrix} = \\begin{pmatrix} E^1 [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1]\\\\ E^2 [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1]\\\\ :\\\\ E^{n} [R|S = \\{S_j, T_v, C, T_{cv}\\}, \u03b1] \\end{pmatrix}$$\n$$\\Phi(c) = \\begin{pmatrix} Q^1\\\\ Q^2\\\\ :\\\\ Q^n\\end{pmatrix} = \\begin{pmatrix} E^1 [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a]\\\\ E^2 [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a]\\\\ :\\\\ E^{n} [R(c)|S = \\{S_j, T_v, T_{cv}\\}, a] \\end{pmatrix}$$\nObserving the state can reveal a reduction in dimensionality, which leads to improved scalability. Initially, agents receive the state with the coverage time value, the total number of nodes connected to the RSU per category, and the category (service type). Using this information, agents can learn an optimal policy for selecting the appropriate waiting time for transmission. In a multi-agent scenario, with different categories assigned to AVs for data transfer, we can consider distinct agent networks. We define the network of all agents as \u03a6, with state $S = {S_j,T_v, C,T_{cv}}$, and the network of agents per category as \u03a6(c). Analyzing the state for the network \u03a6(c) especially by subdividing the network per category, it is found that the agent's state space, the category is the same, which becomes redundant. Thereby, it could be established that the state space is reduced by one variable (Eq. (11)). Therefore, for each agent, the state becomes $S = {S_j,T_v, T_{cv}}$. This highlights the fact that a multi-agent system can effectively reduce the state space. For instance, if we provide sample values for each variable in the state, we could quantify how much the state space is optimized. Given the length of $S_j$ of 5, $T_v$ of 100, $T_{cv}$ also 100, and |C| of 4 (per our scenario). Then, the size of the state space with 4 variables is $S_jx T_v \\times |T_{cv}|\\times |C| = 200000$, and for 3 variables is $S_jx T_v \\times |T_{cv}| = 50000$, which is 75% decrease.\n2) Actions: A set of actions can be mapped by Eq. (12), where A = {0,..,k}, and k is the maximum number of actions. The equation converts the discrete value a into a continuous value by considering the maximum waiting time per category $W_{max}(c)$, which is described in our previous work [19].\n$$w(c) = \u03b1*\\frac{(W_{maz(c)})}{|A|}$$\n3) Algorithm: This work builds upon our prior research in single-agent coverage awareness and RL [19], where we pro- pose extending the foundational Algorithm 3 to accommodate a distributed multi-agent framework. The details are presented in Algorithm 1 of this paper. The solution is achieved by adding a for loop, allowing the acquisition of data and learning of policy for each agent, described in Line 3 of Algorithm 1. Besides, in Line 10, the AV agent calculates the reward based on individual or overall network performance, which includes latency and throughput information provided by the edge server, as stated in the test cases in the next section. In Line 8 of the same algorithm, the action is obtained by the subroutine choose_action, which is described in Algorithm 2 [19] that includes the e-greedy procedure and the Eq. (12). A visual description of the data flow of the data distribution is depicted in Fig. 1. It can be observed form the figure that the vehicle sends the corresponding data to the agent, an HD Map server. The agent uses this information for learning purposes. After that, the edge server provides the network parameters $T_v$, $T_{cv}$, L, and R to generate the state and calculate the reward function."}, {"title": "V. PERFORMANCE ANALYSIS", "content": "To assess the scalability of a single-agent solution in a multi- agent setup and determine the optimal number of agents for a VANET environment, four test case scenarios are devised to verify the feasibility. In the first scenario, the simulation is focused on centralized learning and reward function calcula- tion, where the AVs will send the information to the edge server. There is a dedicated entity for each of the agents in the edge server, as shown in Fig. 2. Test cases two and three involve different numbers of agents. Finally, test case 4 involves centralized and distributed learning."}, {"title": "A. Test Case 1: Exploring Reward Calculation Strategies in Multi-Agent Systems: Node-specific vs. Overall Application Analysis", "content": "For this test case, we aim to explore different reward calculation strategies in multi-agent systems. We will compare the results of calculating the reward per node (AV) versus calculating the reward for the overall application. In the first simulation, each agent will calculate its reward based on its current latency and throughput as follows each v \u2190 c, where $L_{th} = L_{max}(c)$, and $R_{thr} = R_{min}(c)$, and the latency and throughput are $L_v$, and $R_v$. Thus, the reward equation becomes,\n$$\u03b3_v = \u03b1_1 \\frac{R_v}{R_{th_v}} - \u03b1_2 \\frac{L_v}{L_{th_v}} + F$$\nwhere $L_{th_v}$, and $R_{thr_v}$, are the thresholds. The thresholds are described in Table III which are the desired requirements for each of the services. In the second simulation, we will focus on calculating the reward using average latency and throughput per service, as measured by the edge server. Thus, the latency and throughput are denoted as $L$, and $R$ respectively. Thus, the reward equation becomes,\n$$\u0393_v \u03b1_1 \\frac{R}{R_{th_v}} - \u03b1_2 \\frac{L}{L_{th_v}} + F$$"}, {"title": "B. Test Case 2:Enhancing Service-Specific Performance: Optimal Agent Allocation per Service Type", "content": "This approach enables us to delineate the responsibilities of each agent and determine the necessary quantity. Under this arrangement, the number of agents corresponds directly to the number of service categories. Transitioning from a single agent managing actions for all vehicles to assigning a dedicated agent for each service. It reduces the scope of responsibility per agent, thereby streamlining the local envi- ronment. Consequently, this segregation of agents per service has the potential to optimize individual agent performance. For a visual representation, please refer to Fig. 3. Let set each category $c\\in C$ as an agent. Each agent c will calculate the reward as follows:\n$$\u0393_v \u03b1_1 \\frac{R}{R_{th_c}} - \u03b1_2 \\frac{L}{L_{th_c}} + F$$"}, {"title": "C. Test Case 3: Exploring Performance Variations: Agent Allocation per Service Type vs. Vehicles as Multi-Agent Entities", "content": "We aim to evaluate if different vehicles, acting as agents, display varied performance levels with this test case. By using each vehicle as an agent, the state space is reduced due to a smaller local environment when compared to the previous test case (refer to Figure 4). The reward function calculation follows Eq. (14)."}, {"title": "D. Test Case 4: Centralized vs. Distributed Learning for AVs as Agents", "content": "For the final assessment, each of the autonomous vehicles (AVs) will be chosen as agents to be tested in both central- ized and distributed learning approaches. This will help to determine the costs in terms of latency and throughput of the overhead generated in the centralized approach by sending information to the edge server for learning. If we consider the straightforward calculation for queuing delay [7], as depicted in Eq. (16), latency is bound to increase in centralized learning mode. This is a direct result of the increased number of packets because of data sharing among vehicles and the transmission of actions from agents to AVs, described as below:\n$$Queue_{delay} = \\sum_{k=1}^{K} f_t[n] - f_i[n] *b_ft$$\nwhere the variables K, $b_{ft}$, and $f_t$ represent the number of packets on the queue, the backoff time, and the time required for packet transmission, respectively, depending on its size."}, {"title": "VI. SIMULATION SETUP", "content": "The current standard IEEE802.11p with a 10MHz band- width for the simulation is utilized in our simulation setup. To create a realistic VANET environment, we chose OMNet++ version 6 [33] and the INET [34] library, due to the Physical and Mac protocol stack for IEEE802.11p. To generate traffic flow, we use SUMO [35] and the OMNet++ module veins [36]. Additionally, a research group solution [37] to connect OMNet++ and Python for the RL is employed and adapted to our simulation."}, {"title": "VII. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "This section presents the results and discussions stemming from a comprehensive comparison between the single-agent and multi-agent approaches. The comparison is measured with key performance indicators (KPIs) such as latency, throughput, and fairness. Latency is defined as the time difference between the creation and reception of a packet. Throughput is quantified by the total number of packets received within a designated time interval. To assess fairness, Jain's fairness index formula is employed [44], as shown in Eq. (17).\n$$J(x_1, x_2, ..., x_n) = \\frac{(\\sum_{i=1}^n x_i)^2}{\u03b7 * \\sum_{i=1}x}$$\nThe results are displayed in a Cumulative Distribution Function (CDF) graph for a comprehensive view of latency and throughput distribution. It facilitates understanding of the entire range of latency and throughput and enables straight- forward comparison across different network scenarios."}, {"title": "A. Test Case 1: Exploring Reward Calculation Strategies in Multi-Agent Systems: Node-specific vs. Overall Application Analysis", "content": "a) Latency: From the results of VO in Fig. 5(a), and HD Map in Fig. 5(c), there is no significant difference in whether the reward is calculated using the average latency, and throughput per application or node-specific. The latency percentage difference is 0.43% for VO and 4.3% for HD Map. Concerning VI and BE in Fig. 5(b), and Fig. 5(d), there is a difference in latency of 8.6% and 29.8%, respectively.\nb) Throughput: For the throughput results in Fig. 6, it is observed a small percentage increase while calculating the reward with the overall latency and throughput per application compared with node-specific for VI, and BE. The percentage increase is 2.9% for VI, and 1.9% for BE, as described in Fig. 6(b), and Fig. 6(d), respectively. For VO and HD Map, the average throughput does not vary significantly.\nThese results demonstrate that the optimal policy varies depending on the perception and approach taken to address the problem. Moreover, this reveals that the solution remains robust even when using different reward calculations, as long as the agents utilize the same reward function. To generate the possible lowest latency, it is clear that the overall average latency per application is more efficient for all four types of services."}, {"title": "B. Test Cases 2 and 3: An agent per service type, and per vehicle", "content": "When examining test cases 2 and"}]}