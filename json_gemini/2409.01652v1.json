{"title": "ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation", "authors": ["Wenlong Huang", "Chen Wang", "Yunzhu Li", "Ruohan Zhang", "Li Fei-Fei"], "abstract": "Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models. Videos and code can be found at rekep-robot.github.io.", "sections": [{"title": "1 Introduction", "content": "Robotic manipulation involves intricate interactions with objects in the environment, which can often be expressed as constraints in both spatial and temporal domains. Consider the task of pouring tea into a cup in Fig. 1: the robot must grasp at the handle, keep the cup upright while transporting it, align the spout with the target container, and then tilt the cup at the correct angle to pour. Here, the constraints encode not only the intermediate sub-goals (e.g., align the spout) but also the transitioning behaviors (e.g., keep the cup upright in transportation), which collectively dictate the spatial, timing, and other combinatorial requirements of the robot's actions in relation to the environment.\nHowever, effectively formulating these constraints for a large variety of real-world tasks presents significant challenges. While representing constraints using relative poses between robots and objects is a direct and widely-used approach [1], rigid-body transformations do not depict geometric details, require obtaining object models a priori, and cannot work on deformable objects. On the other hand, data-driven approaches enable learning constraints directly in visual space [2, 3]. While more flexible, it remains unclear how to effectively collect training data as the number of constraints grows combinatorially in terms of objects and tasks. Therefore, we ask the question: how can we represent constraints in manipulation that are 1) widely applicable: adaptable to tasks that require multi-stage, in-the-wild, bimanual, and reactive behaviors, 2) scalably obtainable: have the potential to be fully automated through the advances in foundation models, and 3) real-time optimizable: can be efficiently solved by off-the-shelf solvers to produce complex manipulation behaviors?\nIn this work, we propose Relational Keypoint Constraints (ReKep). Specifically, ReKep represents constraints as Python functions that map a set of keypoints to a numerical cost, where each keypoint is a task-specific and semantically meaningful 3D point in the scene. Each function is composed of (potentially nonlinear) arithmetic operations on the keypoints and encodes a desired \"relation\" between them, where the keypoints may belong to different entities in the environment, such as the robot arms, object parts, and other agents. While each keypoint only consists of its 3D Cartesian coordinates in the world frame, multiple keypoints can collectively specify lines, surfaces, and/or 3D rotations if rigidity between keypoints is enforced. We study ReKep in the context of the sequential manipulation problem, where each task involves multiple stages that have spatio-temporal dependencies (e.g., \u201cgrasping\u201d, \u201caligning\u201d, and \u201cpouring\u201d in the aforementioned example).\nWhile constraints are typically defined manually per task [4], we demonstrate the specific form of ReKep possesses a unique advantage in that they can be automated by pre-trained large vision models (LVM) [5] and vision-language models (VLM) [6], enabling in-the-wild specification of ReKep from RGB-D observations and free-form language instructions. Specifically, we leverage LVM to propose fine-grained and semantically meaningful keypoints in the scene and VLM to write the constraints as Python functions from visual input overlaid with proposed keypoints. This process can be interpreted as grounding fine-grained spatial relations, often those not easily specified with natural language, in an output modality supported by VLM (code) using visual referral expressions.\nWith the generated constraints, off-the-shelf solvers can be used to produce robot actions by re-evaluating the constraints based on tracked keypoints. Inspired by [7], we employ a hierarchical optimization procedure to first solve a set of waypoints as sub-goals (represented as SE(3) end-effector poses) and then solve the receding-horizon control problem to obtain a dense sequence of actions to achieve each sub-goal. With appropriate instantiation of the problem, we demonstrate that it can be reliably solved at approximately 10 Hz for the tasks considered in this work.\nOur contributions are summarized as follows: 1) We formulate manipulation tasks as a hierarchical optimization problem with Relational Keypoint Constraints; 2) We devise a pipeline to automatically specify keypoints and constraints using large vision models and vision-language models; 3) We present system implementations on two real-robot platforms that take as input a language instruction and RGB-D observations, and produce multi-stage, in-the-wild, bimanual, and reactive behaviors for a large variety of manipulation tasks, all without task-specific data or environment models."}, {"title": "2 Related Works", "content": "Structural Representations for Manipulation. Structural representations determine the orchestration of different modules in a manipulation system and yield different implications on the capabilities, assumptions, efficiency, and effectiveness of the system. Rigid-body poses are most commonly used given well-understood rigid-body motions in free space and their efficiency at modeling long-range dependencies of objects [1, 8-18]. However, since it often requires both geometry and dynamics of environment to be modeled beforehand, various works have studied structural representations using data-driven methods, such as learning object-centric representation [19\u201334], particle-based dynamics [35-41], and keypoints or descriptors [3, 4, 42\u201354]. Among them, keypoints have shown great promises given their interpretability, efficiency, generalization to instance variations [4], and ability to model both rigid bodies and deformable objects. However, manual annotation is required per task, thus lacking scalability in open-world settings, which we aim to address in this work.\nConstrained Optimization in Manipulation. Constraints are often used to impose desired behaviors on robots. Motion planning algorithms use geometric constraints to compute feasible trajectories that avoid obstacles and achieve goals [55-60]. Contact constraints can be used to plan forceful or contact-rich behaviors [61\u201371]. For sequential manipulation tasks, task and motion planning (TAMP) [1, 8, 13] is a widely used framework, often formulated as constraint satisfaction problems [11, 72\u201377] with continuous geometric problems as subroutines. Logic-Geometric Programming [78\u201383] alternatively formulates a nonlinear constrained program over the entire state trajectory, taking into account both logical and geometric constraints. Constraints can either be manually written or learned from data in the form of manifolds [84], feasibility model [83, 85], or signed-distance fields [2, 86]. Inspired by [7], we formulate sequential manipulation tasks as an integrated continuous mathematical program that is repeatedly solved in a receding-horizon fashion, with the key difference being that the constraints are synthesized by foundation models.\nFoundation Models for Robotics. Leveraging foundation models for robotics is an active area of research. We refer readers to [87\u201390] for overview and recent applications. Here, we focus on VLMs that are capable of incorporating visual inputs [6, 91\u201395] for robotic manipulation. However, despite showing promises for open-world planning and goal specification [96\u2013110], the caption-guided pertaining scheme of VLMs often limits visual details that can be retained about the image [111\u2013114]. Self-supervised vision models (e.g., DINO [5, 115]), on the other hand, provide fine-grained pixel-level features useful for various vision and robotic tasks [31, 116\u2013121], but there lack effective ways for interpreting open-world semantics, which is pivotal for cross-task generalization. In this work, we leverage their complementary strengths by using DINOv2 [5] for fine-grained keypoint proposal and using GPT-40 [6] for its visual reasoning capability in a supported output modality (code). Similar forms of visual prompting techniques are also explored in concurrent works [96-98, 109, 122]. In this work, we demonstrate ReKep possesses the unique advantages of performing challenging 6-12 DoF tasks, integrated high-level reasoning for reactive replanning, high-frequency closed-loop execution, and generating black-box constraints via visual prompting. We defer more detailed discussion to Appendix A.10."}, {"title": "3 Method", "content": "Herein we discuss: (1) What are Relational Keypoint Constraints (Sec. 3.1)? (2) How to formulate manipulation as a constrained optimization problem with ReKep (Sec. 3.2)? (3) What is our algorithmic instantiation that can efficiently solve the optimization in real-time (Sec. 3.3)? (4) How to automatically obtain ReKep from RGB-D observations and language instructions (Sec. 3.4)?"}, {"title": "3.1 Relational Keypoint Constraints (ReKep)", "content": "Herein we define a single instance of ReKep. For clarity, we assume that a set of K keypoints have been specified (discussed later in Sec. 3.4). Concretely, each keypoint $k_i \\in \\mathbb{R}^3$ refers to a 3D point"}, {"title": "3.2 Manipulation Tasks as Constrained Optimization with ReKep", "content": "Using ReKep as a general tool to represent constraints, we adopt the formulation in [7] and show how a manipulation task can be formulated as a constrained optimization problem involving $C_{sub-goal}^{(i)}$ and $C_{path}^{(i)}$. We denote the end-effector pose as $e \\in SE(3)$.To perform the manipulation task, we aim to obtain the overall discrete-time trajectory $e_{1:T}$ by formulating the control problem as follows:"}, {"title": "3.3 Decomposition and Algorithmic Instantiation", "content": "To solve Eq. 1 in real-time, we employ a decomposition of the full problem and only optimize for the immediate next sub-goal and the corresponding path to reach the sub-goal (pseudo-code in Algorithm 1). All optimization problems are implemented and solved using SciPy [125] with decision variables normalized to [0,1]. They are initially solved with Dual Annealing [126] with SLSQP [127] as local optimizer (around 1 second) and subsequently solved with only local optimizer based on the previous solution at approximately 10 Hz\u00b9.\nThe Sub-Goal Problem: We first solve the sub-goal problem to obtain $e_{g_i}$ for the current stage i:\nwhere $\\lambda_{sub-goal}()$ subsumes auxiliary control costs: scene collision avoidance, reachability, pose regularization, solution consistency, and self-collision for bimanual setup (details in A.8). Namely, Eq. 2 attempts to find a sub-goal that satisfies $C_{sub-goal}^{(i)}$ while minimizing the auxiliary costs. If a stage is concerned with grasping, a grasp metric is also included. In this work, we use AnyGrasp [131]\u00b2.\nThe Path Problem: After obtaining sub-goal $e_{g_i}$, we solve for a trajectory $e_{t:g_i}$ starting from current end-effector pose $e_t$ to the sub-goal $e_{g_i}$:\nwhere $\\lambda_{path}$ subsumes the following auxiliary control costs: scene collision avoidance, reachability, path length, solution consistency, and self-collision in the case of bimanual setup (details in A.9). If the distance to the sub-goal $e_{g_i}$ is within a small tolerance $\\epsilon$, we progress to the next stage i + 1.\nBacktracking: Although the sub-problems can be solved at a real-time frequency to react to external disturbances within a stage, it is imperative that the system can replan across stages if any sub-goal constraint from the last stage no longer holds (e.g., cup taken out of the gripper in the pouring task). Specifically, in every control loop, we check for violation of $C_{sub-goal}^{(i)}$ or $C_{path}^{(i)}$. If one is found, we iteratively backtrack to a previous stage j such that $C_{path}^{(j)}$ is satisfied.\nForward Models for Keypoints: To solve Eq. 2 and Eq. 3, one must utilize a forward model h that estimates $k_t$ from $e_t$ in the optimization process. As in prior work [4], we make the rigidity"}, {"title": "3.4 Keypoint Proposal and ReKep Generation", "content": "To enable the system to perform tasks in-the-wild given a free-form task instruction, we devise a pipeline using large vision models and vision-language models for keypoint proposal and ReKep generation, which are respectively discussed as follows:\nKeypoint Proposal: Given an RGB image $\\mathbb{R}^{h \\times w \\times 3}$, we first extract the patch-wise features $F_{patch} \\in[][\\mathbb{R}^{h' \\times w' \\times d}$ from DINOv2 [5]. Then we perform bilinear interpolation to upsample the features to the original image size, $F_{interp} \\in \\mathbb{R}^{h \\times w \\times d}$. To ensure the proposal covers all relevant objects in the scene, we extract all masks $M = \\{m_1, m_2,...,m_n\\}$ in the scene using Segment Anything (SAM) [132]. For each mask j, we cluster the masked features $F_{interp}[m_j]$ using k-means with k = 5 with a cosine-similarity metric. The centroids of the clusters are used as keypoint candidates, which are projected to a world coordinate $\\mathbb{R}^3$ using a calibrated RGB-D camera. Candidates that are within 8cm of others are filtered out. Overall, we find that this procedure is adept at identifying a large percentage of fine-grained and semantically meaningful regions of objects.\nReKep Generation: After obtaining the keypoint candidates, we overlay them on the original RGB image with numerical marks. Coupled with the language instruction of the task, we then use visual prompting to query GPT-40 [6] to generate the number of required stages and the corresponding sub-goal constraints $C_{sub-goal}^{(i)}$ and path constraints $C_{path}^{(i)}$ for each stage i (prompts are in A.6). Notably, the functions do not directly manipulate the numerical values of the keypoint positions. Rather, we exploit the strength of VLM to specify spatial relations as arithmetic operations, such as L2 distance or dot product between keypoints, that are only instantiated when invoked with actual keypoint positions tracked by a specialized 3D tracker. Furthermore, an important advantage of using arithmetic operations on a set of keypoint positions is that it can specify 3D rotations in full SO(3) when sufficient points are provided and rigidity between relevant points is enforced, but this is done only when needed depending on task semantics\u00b3. This enables VLM to reason about 3D rotations with arithmetic operations in 3D Cartesian space, effectively circumventing the need for dealing with alternative 3D rotation representation and the need for performing numerical computation."}, {"title": "4 Experiments", "content": "We aim to answer the following research questions: (1) How well does our framework automatically formulate and synthesize manipulation behaviors (Sec. 4.1)? (2) Can our system generalize to novel objects and manipulation strategies (Sec. 4.2)? (3) How do the individual components contribute to the failure cases of the system (Sec. 4.3)? We validate ReKep on two real robot platforms: a wheeled single-arm platform, and a stationary dual-arm platform (Figure. 3). Additional implementation details can be found in Appendix, including keypoint proposal (A.5), VLM querying (A.6), point trackers (A.7), sub-goal solver (A.8), and path solver (A.9)."}, {"title": "4.1 In-the-Wild and Bimanual Manipulation with ReKep", "content": "Tasks. We purposefully select a set of tasks (shown in Fig. 3) with the goal of examining the multi-stage (m), in-the-wild (w), bimanual (b), and reactive (r) behaviors of the system. The tasks and their features are Pour Tea (m, w, r), Stow Book (w), Recycle Can (w), Tape Box (w, r), Fold Garment (b), Pack Shoes (b), and Collaborative Folding (b, r). We further evaluate three of the tasks under external disturbances (denoted as \"Dist.\") by changing poses of task objects during execution.\nMetric and Baselines. Each setting has 10 trials, in which object poses are randomized. Task success rate is reported in Tab. 1. We compare against VoxPoser [103] as a baseline. We evaluate two variants of the system, \"Auto\" which uses foundation models for automating generation of ReKep, and \u201cAnnotated (Annot.)\" which uses human-annotated ReKep.\nResults. Overall the system demonstrates promising capabilities at formulating correct constraints and executing them in unstructured environments, despite no task-specific data or environment model are provided. Notably, ReKep can effectively handle core challenges of each task. For example, it can formulate correct temporal dependency in multi-stage tasks (e.g., spout needs"}, {"title": "4.2 Generalization in Manipulation Strategies", "content": "Tasks. We systematically evaluate how novel manipulation strategies can be formulated by focusing on a single task, garment folding, but with 8 unique categories of garments, each demanding a unique way of folding and requiring both geometrical and commonsense reasoning. Evaluation is done on the bimanual platform, presenting additional challenges in bimanual coordination.\nMetric. We use GPT-40 with a prompt containing only generic instructions with no in-context examples. \"Strategy Success\" measures whether generated ReKep is feasible, which tests both the keypoint proposal module and the VLM, and \"Execution Success\" measures system success rate given feasible strategies for each clothing. Each is measured with 10 trials.\nResults. Interestingly, we observe drastically different strategies across categories, many of which are aligned with how humans might fold each garment. For example, it can recognize that two sleeves often are folded together, prior to fully folding the clothes. In cases where using two arms is unnecessary, akin to how humans fold clothes, only one arm is being used. However, we do observe that the VLM may miss certain steps to complete the folding as the operator expected, but we recognize that this is inherently an open-ended problem often based on one's preferences."}, {"title": "4.3 System Error Breakdown", "content": "The modular design of the framework entails an advantage for analyzing system errors due to its interpretability. In this section, we perform an empirical investigation by manually inspecting the failure cases of the experiments reported in Tab. 1, which is then used to calculate the likelihood of a module causing an error while accounting for their temporal dependencies in the pipeline. Results are reported in Fig. 5. Among the different modules, the point tracker"}, {"title": "5 Conclusion & Limitations", "content": "In this work, we presented Relational Keypoint Constraints (ReKep), a structural task representation using constraints that operates on semantic keypoints to specify desired relations between robot arms, object (parts), and other agents in the environment. Coupled with point trackers, we demonstrate that ReKep constraints can be repeatedly and efficiently solved in a hierarchical optimization framework to act as a closed-loop policy that runs at a real-time frequency. We also demonstrate the unique advantage of ReKep in that it can be automatically synthesized by large vision models and vision-language models. Results are shown on two robot platforms and on a variety of tasks featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data, additional training, or environment models.\nDespite the promises, several limitations remained. First, the optimization framework relies on a forward model of keypoints based on rigidity assumption, albeit a high-frequency feedback loop that relaxes the accuracy requirement of the model. Second, ReKep relies on accurate point tracking to correctly optimize actions in closed-loop, which is itself a challenging 3D vision task due to heavy intermittent occlusions. Lastly, the current formulation assumes a fixed sequence of stages (i.e., skeletons) for each task. Replanning with different skeletons requires running keypoint proposal and VLM at a high-frequency, which poses considerable computational challenges. An extended discussion can be found in Appendix A.11."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Pseudo-code for Sequential Manipulation with Relational Keypoint Constraints", "content": ""}, {"title": "A.2 Wheeled Single-Arm Platform", "content": "One of our investigated platform is a Franka arm mounted on a wheeled base built with Vention frames (shown in Figure 6). Note that the base does not have motors and thus cannot move autonomously, but its mobility nevertheless allows us to investigate the proposed method outside of lab environments.\nSince our pipeline produces a sequence of 6-DoF end-effector poses, we use position control in all experiments, which is running at a fixed frequency of 20 Hz. Specifically, once the robot is given a target end-effector pose in the world frame, we first clip the pose to the pre-defined workspace. Then we linearly interpolate from the current pose to the target pose with a step size of 5mm for position and 1 degree for rotation. To move to each interpolated pose, we first calculate inverse kinematics to obtain the target joint positions based on current joint positions (IK solver from PyBullet [133]). Then we use the joint impedance controller from Deoxys [134] to reach to the target joint positions."}, {"title": "A.3 Stationary Dual-Arm Platform", "content": "We also investigate the method on a stationary dual-arm platform consisting of two Franka arms mounted in front of a tabletop workspace (shown in Figure 7). We share the same controller as the wheeled single-arm platform with the exception that the two arms are controlled simultaneously at 20 Hz. Specifically, our pipeline jointly solves two 6-DoF end-effector pose sequences, which are sent to the low-level controller together. The controller subsequently calculates IK for both arms and moves the arms using joint impedance control.\nThree RGB-D cameras, Orbbec Femto Bolt, are mounted on this platform. Two cameras are mounted on the left and right sides and one camera is mounted in the back. The cameras similarly capture RGB images and point clouds at a fixed frequency of 20 Hz."}, {"title": "A.4 Evaluation Details", "content": "Below we discuss the evaluation details for the experiments reported in Section 4.1 and Section 4.2."}, {"title": "A.4.1 Details for In-the-Wild and Bimanual Manipulation (Section 4.1)", "content": "For each task, 10 initial different configurations of objects are selected, which cover the full workspace but are manually verified to ensure they are kinematically feasible for the robot. For each trial, a human operator restores the scene to the corresponding configuration and initiates the system. Due to the challenge of developing automatic success criteria for the diverse set of objects and environments investigated in this work, success rates are measured by the operator with the criterion reported under each task description below. For experiments involving external disturbances, the set of disturbances for all trials is pre-selected, and one disturbance is applied to each trial. Specifically, the disturbance is introduced by a human operator using hands to change the object's pose. Collision checking is disabled for all tasks involving deformable objects.\nPour Tea: The environment consists of a teapot and a cup placed on a counter table in a kitchen setting. The task involves three stages: grasping the handle, aligning the teapot to the top of the cup, and pouring the tea into the cup. The success criterion requires that the teapot remains upright until the pouring stage, and at the end, the spout must be aligned and tilted on top of the cup opening.\nRecycle Can: The environment includes one of three types of cans (Coke, Zero Coke, Zero Sprite), a recycle bin with a narrow opening (such that the cans may only go in when they are upright), a landfill bin, and a compost bin, all situated inside an office building. The task involves two stages: grasping the can and reorienting it on top of the recycle bin before dropping it. The success criterion is that the can is successfully thrown into the bin.\nStow Book: The environment consists of a target book placed on a side table and a real-size bookshelf with a 15cm opening among the placed books, all inside an office environment. The task involves two stages: grasping the target book on the side and stowing it inside the opening in the shelf. The success criterion is that the target book is placed steadily after the robot releases the gripper, and the robot must not bump into the shelf or other placed books.\nTape Box: The environment includes a cardboard box, a packaging tape with a dispenser sitting on top of the box that already has one side taped, and a human user collaborating with the robot. The tape has already been unrolled to be enough for taping because unrolling typically requires a large force that exceeds the limit of the robot arm. The task involves two stages: while a human operator is squeezing the box, the robot needs to grasp the tape and align it to the correct side to complete the taping. The success criterion is that the tape must end up in the correct position such that it is aligned with the seam."}, {"title": "A.4.2 Details on Baseline Methods", "content": "We use VoxPoser [103] as the main baseline method as it makes similar assumptions that no task-specific data or pre-defined motion primitives are required. We adapt VoxPoser to our setting with certain modifications to ensure fairness in comparisons. Specifically, we use the same VLM, GPT-40 [6], that takes the same camera input. We also augment the original prompt from the paper with the prompt used in this work to ensure it has sufficient context. We only use the affordance, rotation, and gripper action value maps and ignore the avoidance and velocity value maps because they are not necessary for our tasks. We also only consider the scenario where the \"entity of interest\" is the robot end-effector instead of objects in the scene. The latter is tailored for pushing task, which is not being studied in this work. We use OWL-ViT [135] for open-vocabulary object detection, SAM [132] for initial-frame segmentation, and Cutie [136] for mask tracking."}, {"title": "A.4.3 Details for Generalization in Manipulation Strategies (Section 4.2)", "content": "The dual-arm robot is tasked with folding eight different categories of clothing. We use two metrics for evaluation: \"Strategy Success\u201d and \u201cExecution Success,\" where the former evaluates whether keypoints are proposed and constraints are written appropriately, and the latter evaluates the robotic system's execution given successful strategies.\nTo evaluate \"Strategy Success,\u201d the garment is initialized close to the center of the workspace. A back-mounted RGB-D camera captures the RGB image. Then, the keypoint proposal module generates keypoint candidates using the captured image, which are then overlaid on top of the original image with numerical marks {0, . . ., K 1}. The overlaid image, along with the same generic prompt, is fed into GPT-4 [6] to generate the ReKep constraints. Since folding garments is itself an open-ended problem without ground-truth strategies, we manually judge if the proposed keypoints and the generated constraints are correct. Note that since the constraints are to be executed by a bimanual robot, and the constraints are almost always connecting (folding) two keypoints such that they are aligned, correctness is measured by whether it is (potentially) executable by the robot without causing self-collision (arms crossing over to opposite sides) and whether the folding strategy can fold the garment to at most half of its original surface area.\nTo evaluate \"Execution Success,\" we take the generated strategies in the previous section that are marked as successful for each garment and execute the sequence on the dual-arm platform, with a total of 10 trials for each garment. Point tracking is disabled as we observe that our point tracker predicts unstable tracks when the garment is potentially folded many times. Success is measured by whether the garment is folded such that its surface area is at most half of its original surface area."}, {"title": "A.5 Implementation Details of Keypoint Proposal", "content": "Herein we describe how keypoint candidates in a scene are generated. For each platform, we use one of the mounted RGB-D cameras to capture an image of size h \u00d7 w \u00d7 3, depending on which camera has the best holistic view of the environment, as all the keypoints need to be present in the first frame for the proposed method. Given the captured image, we first use DINOv2 with registers (ViT-S14) [5, 137] to extract the patch-wise features $F_{patch} \\in [][\\mathbb{R}^{h\u00b4\\times w' \\times d}$. Then we perform bilinear interpolation to upsample the features to the original image size, $F_{interp} \\in \\mathbb{R}^{h \\times w \\times d}$. To ensure the proposal covers all relevant objects in the scene, we extract all masks $M = \\{m_1, m_2, . . ., m_n\\}$ in the scene using Segment Anything (SAM) [132]. Within each mask $m_i$, we apply PCA to project the features to three dimensions, $F_{PCA} = PCA(F_{resized} [m_i], 3)$. We find that applying PCA improves the clustering as it often removes details and artifacts related to texture that are not useful for our tasks. For each mask j, we cluster the masked features $F_{interp}[m_j]$ using k-means with k = 5 with the Euclidean distance metric. The median centroids of the clusters are used as keypoint candidates, which are projected to a world coordinate $\\mathbb{R}^3$ using a calibrated RGB-D camera. Note that we also store which keypoint candidates originate from the same mask, which is later used as part of the rigidity assumption in the optimization loops described in Sec. 3.3. Candidates outside of the workspace bounds are filtered out. To avoid many points cluttered in a small region, we additionally use Mean Shift [138, 139] (with a bandwidth 8cm) to filter out points that are close to each other. Finally, the centroids are taken as final candidates. Alternatively, one may develop a pipeline using only segmentation models [132, 140], but we leave comparisons to future work."}, {"title": "A.6 Querying Vision-Language Model", "content": "After we obtain the keypoint candidates, they are overlaid on the captured RGB image with numerical marks {0, . . ., K \u2212 1}. Then the image and the task instruction are fed into a vision-language model with the prompt described below. The prompt contains only generic instructions with no image-text in-context examples, although a few text-based examples are given to concretely explain the proposed method and the expected output from the model. Note that the majority of the investigated tasks are not discussed in the provided prompt. As a result, the VLM is tasked with generating ReKep constraints by leveraging its internalized world knowledge.\nFor the experiments conducted in this work, we use GPT-40 [6] as it is one of the latest available models at the time of the experiments. However, due to rapid advancement in this field, the pipeline can directly benefit from newer models that have better vision-language reasoning. Correspondingly, we observe different models exhibit different behaviors when given the same prompt (with the observation that newer models typically require less fine-grained instructions). As a result, instead of developing the best prompt for the suite of tasks in this work, we focus on demonstrating a full-stack pipeline consisting a key component that can be automated and continuously improved by future development."}, {"title": "A.7 Implementation Details of Point Tracker", "content": "We implement a simple point tracker following [118] based on DINOv2 (ViT-S14) [5] that leverages the fact that multiple RGB-D cameras are present and DINOv2 is efficient to run at a real-time frequency.\nAt initialization, an array of 3D keypoint positions $k \\in \\mathbb{R}$ are given. We first take the RGB-D captures from each present camera. For each RGB image, we obtain the pixel-wise DINOv2 features following the same procedure in Section A.5 and record their associated 3D world coordinates using calibrated cameras. For each 3D keypoint positions, we aggregate all the features from points that are within 2cm from all the cameras. The mean of the aggregated features is recorded as the reference feature for each keypoint, which is kept fixed throughout the task.\nAfter initialization, at each time step, we similarly obtain the pixel-wise features from DINOv2 from all cameras with their 3D world coordinates. To track the keypoints, we calculate cosine similarity between features across all pixels and the reference features. The top 100 matches are selected for each keypoint with a cutoff similarity of 0.6. We then reject outliers for the selected matches by calculating median deviation (m = 2). Additionally, as the tracked keypoints may oscillate in a small region, we apply a uniform filter with a window size of 10 in the end. The entire procedure runs at a fixed frequency of 20 Hz.\nNote that the implemented point tracker is a simplification from [118] for real-time tracking. We refer readers to [118] for more comprehensive discussion on using self-supervised vision models, such as DINOv2, for point tracking. Alternatively, more specialized point trackers can be used [141-148]."}, {"title": "A.8 Implementation Details of Sub-Goal Solver", "content": "The sub-goal problems are implemented and solved using SciPy [125", "1,1": "based on the bounds. For the first solving iteration, the initial guess is chosen to be the current end-effector pose. We use sampling-based global optimization Dual Annealing [126", "127": "that refines the solution. The full procedure takes around 1 second for this iteration. In subsequent iterations, we use the solution from previous stage and only use local optimizer as it can quickly adjust to small changes. The optimization is cut off with a fixed time budget represented as number of objective function calls to keep the system running at a high frequency."}]}