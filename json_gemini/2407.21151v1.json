{"title": "Private Collaborative Edge Inference via Over-the-Air Computation", "authors": ["Selim F. Yilmaz", "Burak Has\u0131rc\u0131o\u011flu", "Li Qiao", "Deniz G\u00fcnd\u00fcz"], "abstract": "We consider collaborative inference at the wireless edge, where each client's model is trained independently on their local datasets. Clients are queried in parallel to make an accurate decision collaboratively. In addition to maximizing the inference accuracy, we also want to ensure the privacy of local models. To this end, we leverage the superposition property of the multiple access channel to implement bandwidth-efficient multi-user inference methods. Specifically, we propose different methods for ensemble and multi-view classification that exploit over-the-air computation. We show that these schemes perform better than their orthogonal counterparts with statistically significant differences while using fewer resources and providing privacy guarantees. We also provide experimental results verifying the benefits of the proposed over-the-air multi-user inference approach and perform an ablation study to demonstrate the effectiveness of our design choices. We share the source code of the framework publicly on Github to facilitate further research and reproducibility.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing adoption of Internet of Things (IoT) devices results in the collection and processing of massive amounts of mobile data at the wireless edge. Conventional centralized machine learning (ML) methods are impractical for edge applications due to privacy concerns and limited communication resources. Implementing decentralized ML models at the edge solves this issue, and thus, edge learning and edge inference have attracted significant attention over the recent years [2]-[6]. Edge learning aims to train large ML models in a distributed setting, whereas edge inference aims to make inferences in a distributed manner at the edge.\nAlthough collaborative training (e.g., federated learning (FL)) at the edge can bring significant advantages, it requires coordination and communication across nodes. Moreover, limited wireless resources are a major bottleneck, and noise, interference, and lack of accurate channel state information can prevent or slow down the convergence of learning algorithms or result in reduced accuracy [7]. Therefore, in this paper, we consider collaborative inference using independently trained models at the edge nodes. While a growing body of work studies distributed training over wireless networks, the literature on distributed wireless inference, particularly using deep learning techniques, is relatively limited [8]-[10]. Moreover, beyond a few exceptions, such physical factors affecting edge inference have not been explored in the literature [2].\nWe treat the resultant problem as a collaborative edge inference problem, where the individual hypotheses of the clients need to be conveyed to the central inference server (CIS), and combined for the most accurate decision. Moving sensing to the edge reduces latency and improves privacy since it removes the requirement to communicate the data or the models. Often the data is not accumulated at a single client or cannot be shared between clients due to privacy or latency constraints. When data offloading or collaborative training are not possible, locally trained models are suboptimal and a collaborative inference solution is needed that incorporates the decisions at different clients. We, therefore, introduce a distributed inference solution that incorporates all the data and models while limiting privacy leakage during inference time through differential privacy (DP) guarantees.\nPrivacy is an important concern in all ML applications since the data used for training can reveal sensitive information about their owner. In the case of ensemble inference, when the models are queried, their outputs may reveal sensitive information about their training sets. For instance, even when an adversary has black-box access to a model, whether or not a data sample is used during training can be inferred via membership inference attacks [11], or even the whole model can be reconstructed via model inversion attacks [12]. Hence, even if adversaries can only observe the inference results, we need to introduce some additional mechanisms to protect the sensitive information.  summarizes the architecture for collaborative inference with DP guarantees.\nDP guarantees can be obtained by introducing additional randomness to the output, such as adding noise, at the expense of some accuracy loss. Since DP bounds the amount of information leaked about the individuals, DP mechanisms make black-box attacks less effective. One approach to provide DP guarantees to ML is differentially private training [13]."}, {"title": "A. Contributions", "content": "The main contributions of our work are summarized as follows:\n1) We introduce a novel distributed inference framework at the wireless edge that exploits OAC while providing bandwidth efficiency and variability. To the best of our knowledge, the conference version of this paper [1] was the first in the literature to exploit OAC for distributed inference.\n2) We provide flexible privacy guarantees depending on the scenario without imposing any restrictions on the training phase.\n3) We systematically compare and discuss the privacy of the introduced collaborative classification methods, and show that the proposed framework with OAC performs significantly better than its orthogonal and digital counterparts while using fewer wireless resources under privacy constraints. We also provide statistical significance tests to show the reliability of the obtained results.\n4) To facilitate further research and reproducibility, we publicly share the source code of our framework on github.com/ipc-lab/collaborative-inference-oac."}, {"title": "B. Organization of This Article", "content": "The rest of the article is organized as follows. In Section II, we describe related works from wireless communications and machine learning literature. In Section III, we define distributed inference problem over a MAC and describe our threat model and target problem. In Section IV, we introduce our novel distributed inference method for the wireless edge that exploits the superposition property of MAC. In Section VI, we demonstrate the performance and bandwidth gains of our introduced methodology. We conclude our work in Section VII."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Distributed Inference at the Edge", "content": "For IoT and Internet of Everything (IoE) applications, moving inference from cloud to edge can be desirable or needed in both static and dynamic environments [2]. Static environments, e.g., surveillance cameras or sensor networks, might desire edge inference due to latency and privacy constraints [26], [27]. Dynamic environments are even more challenging with the same demands due to their high mobility. In both static and dynamic environments, edge devices can even generate data faster than their upload speed to the cloud, which makes it impossible to employ a cloud-centric solution while utilizing all the data [2].\nEdge inference in dynamic environments, such as the ones including self-driving cars, drones and mobile phones, might require adaptive solutions due to latency, privacy, and accuracy constraints [2], [4], [28], [29]. In a collaborative system, client participation depends on the privacy, security, energy and utility requirements [30], [31]. Clients may choose not to participate due to various reasons such as (1) hardware failures, (2) to reduce or balance the power consumption, (3) to prevent accuracy drop of the CIS due to noisy data receipt or unconfident local prediction, (4) to prevent leakage to possible eavesdroppers, (5) to prevent transmission over a low-quality wireless channel, and (6) to improve DP guarantees. In this study, we focus on amplifying DP guarantees by random participation."}, {"title": "B. OAC for FL", "content": "FL [32] has found applications in many areas including, but not limited to, finance [33], medicine [34], [35] and"}, {"title": "C. OAC for Edge Inference", "content": "Following our initial work [1], OAC has been adopted to multi-device edge inference in several studies [39]\u2013[42]. Specifically, in [39]\u2013[41], feature vectors are extracted at multiple local devices and transmitted via OAC, then the neural network employed at the server performs inference based on the superposed feature vectors. Due to observations from multiple devices, the server can achieve better inference results. Moreover, power allocation, transmit precoding, and receive beamforming are optimized to maximize the inference-oriented criterion in [39] and [40]. In addition, contrastive learning has been used to exploit feature correlations among devices to maximize the image retrieval accuracy in [41]. The authors of [42] proposed an OAC-based multi-view pooling technique to improve classification accuracy with less communication latency. However, to the best of our knowledge, only [1] and the current paper consider the privacy aspects of distributed edge inference."}, {"title": "D. Ensemble and Multi-View Classification", "content": "Ensemble learning methods combine multiple hypotheses instead of constructing a single best hypothesis to model the data [43]. In ensemble learning, each hypothesis votes for the final decision, where votes can have weights depending on their confidence. It is generally intractable to find the optimal hypothesis, and choosing a model among a set of equally good models has the risk of choosing the model that has worse generalization performance; however, averaging these models would reduce this risk [43], [44]. Furthermore, weighted or voting-based ensemble methods have theoretical guarantees; for example, it can be shown that the expected error of an averaging ensemble of models is not greater than the average of the expected errors of individual models with a mean square objective [44].\nEnsemble methods achieve higher accuracy when local models are diverse [45]. Our setting is similar to dagging [46], where individual models of the ensemble are trained on disjoint datasets. Dagging is a very similar technique to the well-known bagging [47], with the difference in disjoint training subset sampling like ours instead of sampling with replacement. We employ models trained on disjoint datasets due to the nature of our target problem, which improves the diversity and privacy of the local models.\nMulti-view classification deals with multiple feature perspectives, e.g., different views of the same scene, while ensemble classification performs classification on the same whole instance. Both approaches aim to improve the classification performance. In multi-view classification, the inputs are often correlated, and therefore the aim is to utilize these correlations among different views [48]. \u03a4\u03bf fuse information from other modalities or views, data-level, feature-level or decision-level fusion techniques have been commonly employed [49], [50]. Among them, decision-level fusion does not require any data or feature sharing between models, and therefore improves latency and privacy. For this purpose, decision-level fusion is a commonly employed technique while benefiting from the local models or datasets of individual clients.\nAveraging, weighted averaging and voting are common methods for decision-level fusion [43]. Weighted averaging is more general than simple averaging and voting as these methods are special cases of weighted averaging. Various empirical studies show that simple averaging is not worse than weighted averaging [51], [52]. Yet, in general, weighted averaging is a better choice when individual models have significantly different performances on the target task [53]. Simple averaging can be a better choice when individual models have similar performances since weights need to be determined, and they may not be reliable due to noise and insufficient amount of data [53]."}, {"title": "III. SYSTEM MODEL AND PROBLEM DEFINITION", "content": ""}, {"title": "A. Notation", "content": "Unless stated otherwise; boldface lowercase letters denote vectors (e.g., p), boldface uppercase letters denote matrices (e.g., P), non-boldface letters denote scalars (e.g., p or P), and uppercase calligraphic letters denote sets (e.g., P). Blackboard bold letters denote function domains (e.g., P). R, N, C denote the set of real, natural and complex numbers, respectively. P denotes the cardinality of set P. We define $[n] = \\{1,2,\u2026,n\\}$, where $n \\in \\mathbb{N}^+$."}, {"title": "B. System Model", "content": "We consider privacy-preserving multi-user classification at the wireless edge for both ensemble and multi-view cases. In our model, we consider n clients each with a separate model $f_i : \\mathbb{R}^l \\rightarrow \\mathbb{R}^k$, $i \\in [n]$, trained on dataset $D_i \\subset D$ for either a multi-class or a binary classification task, where $D = \\bigcup_{i \\in [n]} D_i$, and $D_i \\cap D_j = \\emptyset$ for clients $i \\ne j$. Each client also has access to a common validation dataset $D_{val}$.\nA new query to be classified arrives at each timestep t, and all clients receive a view of the query, $x_{i,t} \\in \\mathbb{R}^l$. Then, each participating client makes an inference with its local model denoted by $f_i(x_{i,t}) \\in \\mathbb{R}^k$ and then encodes it into d channel symbols, which is the total available bandwidth per query."}, {"title": "Remark 1.", "content": "The target task becomes ensemble classification when all devices receive the same view $x_t$ as query, i.e., $x_{1,t} = ... = x_{n,t} = x_t$."}, {"title": "C. Threat Model and Target Problem", "content": "In our problem, the purpose is to limit the privacy leakage of clients' local models, which is equivalent to limiting the leakage about the individual datasets $D_i, \\forall i \\in [n]$ since local models are trained on disjoint datasets. In our threat model, we assume all the clients follow the protocol, and the CIS is honest but curious, i.e., it does not deviate from the protocol, but by exploiting the signals received from the clients, it may try to infer sensitive information about the clients' models and datasets. Hence, our goal is to limit the leakage to CIS about the local models from $z_t$ while trying to maximize the inference accuracy."}, {"title": "Remark 2.", "content": "Although our channel model is defined in real numbers $\\mathbb{R}$ for simplicity, it can be extended to complex channels via mapping half of the vectors or function outputs to the real part and the rest to the imaginary part of a complex number."}, {"title": "IV. METHODOLOGY", "content": "In this section, we gradually introduce the modules of our framework."}, {"title": "A. Fusion Methods", "content": "In the following, we present alternative methods to perform local prediction for participating clients, having received the query $x_{i,t}$.\nLet $r_{i,t} \\in \\mathbb{R}^k$ be a vector containing classifier scores (beliefs) for each class at client i for the sample received at time slot t, where k is the number of classes, and the jth element of $r_{i,t}$, denoted by $(r_{i,t})_j$, contains the score for class j. We normalize the sum of the scores in $r_{i,t}$ to 1, i.e., $||r_{i,t}||_1 = 1$, and hence, the maximum possible score of a class is 1.\nWe now present three different models to be used as $f_i$: belief averaging with OAC (BA-OAC), weighted belief averaging with OAC (WBA-OAC), majority voting with OAC (MV-OAC). BA-OAC method averages beliefs of the participating clients for all the classes, and the CIS selects the class with the highest total score. Thus, it uses the following model for client i:\n$f_i(x_{i,t}) = r_{i,t}$.\n(3)\nWBA-OAC method employs normalized weighted average of the beliefs of the participating clients by class-wise accuracy. Then, similarly to BA-OAC, the CIS selects the class with the highest total score. Therefore, it uses the following function at every client i:\n$f_i(x_{i,t}) = W_i \\odot r_{i,t}$,\n(4)\nwhere $w_i \\in \\mathbb{R}^k$ is a vector containing normalized per-class accuracy on $D_{val}$ for client i satisfying $||w_i||_1 = 1$, and $\\odot$ denotes the element-wise multiplication.\nDefinition 1. OneHot(j, l) function outputs an l-dimensional one-hot vector for $j \\leq l$, where only the $j^{th}$ dimension is 1 and the rest are 0.\nMV-OAC method allows participating clients to vote for a class and the CIS selects the class with the highest number of votes. Hence, at client i, we have\n$f_i (x_{i,t}) = \\text{OneHot} \\bigg( \\text{arg} \\underset{j\\in[k]}{\\text{max}} (r_{i,t})_j , k \\bigg)$\n(5)\nWhile BA-OAC and WBA-OAC combine local discriminative scores, MV-OAC combines predicted labels. We then apply mean centering by subtracting $\\overline{f_i(x_{i,t})}$'s mean, which is $\\overline{(f_i(x_{i,t}))} = \\frac{1}{k}$, $\\forall j \\in [k]$, for MV-OAC, i.e.,\n$f_i (x_{i,t}) = \\hat{f_i(x_{i,t})} - \\overline{f_i(x_{i,t})}$.\n(6)"}, {"title": "B. Ensuring Privacy", "content": "Next, we explain how we make our inference procedure privacy-preserving by introducing some randomness. First, we formally define DP for our ensemble inference task.\nLet L, L' \u2208 L be the sets of local models of the clients, which differ at most in one of the clients, i.e., $L = \\{f_j\\} \\cup \\{f_i : i \\in [n] \\backslash j\\}$ and $L' = \\{f'_j\\} \\cup \\{f_i : i \\in [n] \\backslash j\\}$ such that $f_j \\ne f'_j$ and L is the set of all possible local models of n clients. Such L and L' are called neighbouring sets. In our case, since we aim to protect the local models' privacy against CIS, we require that the CIS cannot distinguish between L and L' by observing $z_t$. Hence, the DP guarantees considered in the paper are all local-model-level DP, i.e., the replacement of one client's local model with another model does not generate a distinguishable effect on the observation of the CIS.\nLocal-model-level privacy guarantees might seem too conservative, but besides protecting against inferring sensitive features of the local datasets, local-model-level privacy provides protection against model stealing attacks, i.e., the CIS will not be able to accurately reconstruct the local model parameters.\nNext, we formally define the notion of DP which characterizes the indistinguishability of $z_t$ against the local model sets L and L'. For this, for a clearer notation, let us consider the signal observed by the CIS, $z_t$, as the output of a randomized function M(Xt, L), which represents all the local prediction, randomization and transmission phases, where $X_t \\in \\mathbb{R}^{l \\times n}$ is the matrix whose $i^{th}$ column represents the query received by client i.\nDefinition 2. Consider the signal observed by the CIS, $z_t$, as the output of a randomized function $M : \\mathbb{R}^{l \\times n} \\times L \\rightarrow \\mathbb{R}^d$ and let L and L' be two possible neighbouring model sets. For $\\epsilon > 0$ and $\\delta \\in [0,1)$, M is called (\u03b5, \u03b4)-DP if\n$Pr(M(X_t, L) \\in R) \\leq e^\\epsilon Pr(M(X_t, L') \\in R) + \\delta, $\n(7)\nfor all neighboring pairs (L, L'), $\\forall X_t \\in \\mathbb{R}^{l \\times n}$ and $R \\subset \\mathbb{R}^d$.\nIn the above definition, $\\epsilon$ indicates the amount of privacy loss and hence, smaller $\\epsilon$ implies a stronger privacy guarantee. On the other hand, $\\delta$ characterizes the failure probability of this guarantee, i.e., bad events in which $Pr(M(L) \\in R) \\leq e^\\epsilon Pr(M(L') \\in R)$ does not hold. Hence, $\\delta$ should be close to zero for a strong privacy guarantee. For further discussion on the meaning of $\\epsilon$ and $\\delta$, and the techniques for appropriately choosing them, we refer the readers to [54]\u2013[56].\nTo achieve DP guarantees, the output released to an adversary should be randomized. In our paper, we consider releasing a noisy version of model outputs, $f_i(x_{i,t})$, for each client by adding Gaussian noise [57]. Each client can generate a noisy version of its model prediction as follows:\n$g_i(x_{i,t}) = f_i(x_{i,t}) + m_{i,t}$,\n(8)\nwhere $m_{i,t} \\sim \\mathcal{N}(0, \\sigma_{client}^2I_k)$. One of the main advantages of OAC is that the noise terms added by different clients are automatically aggregated at CIS. Thus, it has a further privacy amplification effect. We provide the analysis of the privacy guarantees achieved by our framework in Section V. This analysis reveals that DP guarantees are directly dependent on the variance of the aggregated noise at the CIS. Hence, to obtain DP guarantees, each client should add a Gaussian noise with $\\sigma_{client}^2 = \\frac{\\sigma^2}{|P_t|}$, where $\\sigma^2$ is a constant depending on the desired DP guarantees and Pt is the set of participating clients. Hence, we assume that the number of participating clients is known by the other participating clients, but is secret from the CIS.\nRemark 3. Note that in OAC, $z_t$ already has channel noise that provides some degree of privacy guarantees, which can even be amplified by fading [23]. However, to achieve the desired level of DP, channel noise power may not be sufficient. First of all, channel noise may not follow a Gaussian distribution in practice (which is often used as worst-case assumption for channel capacity). More importantly, the clients cannot control or reliably know the noise variance or channel gain, as they rely on the CIS for this information. Thus, it is not a reliable source of randomness [21], and we ignore the channel noise and fading while analysing privacy guarantees. Instead, we have each client add some additional Gaussian noise before releasing their contributions. Hence, in reality, the privacy guarantees are slightly better than the ones we obtain in this work due to the presence of additional channel noise. If we were not ignoring the channel noise, each client would need to add less noise to the decision vectors, i.e., noise variance of $(\\sigma^2 - \\sigma_n^2)/|P_t|$ would suffice instead of $\\sigma^2/|P_t|$.\nWe discuss the DP guarantees of our method in Section V. In the next section, we describe the transmission method of the clients."}, {"title": "C. Transmission", "content": "Before transmission, all the clients check whether they participate or not at time t. That is, each client participates with probability p independent of other clients. If no client participates at time t, which has a very low probability, then the clients repeat the coin tosses for their participation until at least one client participates, i.e., until $|P_t| > 0$. Therefore, each client needs to know the number of participating clients at time t, i.e., Pt. Clients can decide on the number of participating clients and their identities via a procedure that is unknown to the CIS. Such a procedure is possible without any communication via common randomness, i.e., by choosing the participating clients depending on a pseudo random variable generated by a common seed. Note that such protocol is not possible when we want participation to depend on the channel states.\nFurthermore, we use linear projection before transmission to adapt to the available bandwidth d. When the number of classes k is smaller than d, it is reasonable to employ the whole bandwidth to improve the transmission quality against channel noise. However, the number of classes k can be much larger than d, e.g., the number of labels ranges from thousands to billions in extreme classification problems [58], [59]. In this case, to enable latency critical applications [60], it is better to"}, {"title": "Remark 4.", "content": "Any orthogonal matrix can be used instead of Q. When d < k, P can also be designed as a matrix. e.g., a Gaussian or Bernoulli random matrix, that satisfies the restricted isometry property of compressed sensing [62]."}, {"title": "D. Final Decision by the CIS", "content": "The signal received by the CIS at time t after passing through the channel defined in Eq. (1) is given as follows:\n$z_t = \\sum_{i \\in P_t} h_{i,t}y_{i,t} + n_t$\n$= \\sum_{i \\in P_t} h_{i,t} \\frac{\\gamma P g_i(x_{i,t})}{P_t h_{i,t}} + n_t$\n$\\overset{(a)}{=} \\frac{\\gamma P}{P_t} \\sum_{i \\in P_t} ( f_i(x_{i,t}) + m_{i,t} ) + n_t$\n$\\overset{(b)}{=} \\gamma P ( \\frac{1}{P_t} \\sum_{i \\in P_t} f_i(x_{i,t}) + \\frac{1}{P_t} \\sum_{i \\in P_t} m_{i,t} ) + n_t$,\n(10)\nwhere (a) follows from Eqs. (8) and (9) and (b) follows from the linearity properties of the projection and scalars. After receiving $z_t$, CIS decision function $s(\u00b7)$ multiplies the received signal by $P^T$ to recover the desired signal, which is the average of votes, local beliefs or weighted local beliefs:\n$\\tilde{s} (z_t) = P^Tz_t$\n$= P^T ( \\gamma P \\sum_{i \\in P_t} f_i(x_{i,t}) + b_t ),$\n(11)\nwhere $b_t \\sim \\mathcal{N}(0, P^T P \\sigma_n^2 + \\gamma^2P^T P \\sigma_{client}^2/|P_t|)$ is the accumulated noise due to privacy and wireless channel. Note that when d \u2265 k, we have PTP = Ik due to the orthogonality property; and therefore, the CIS reconstructs the average of participating clients' score vectors along with some noise. Then, the CIS applies the arg max function to decide the most probable class, i.e.,\n$s(z_t) = \\underset{j\\in[k]}{\\text{arg max}} \\tilde{s}(z_t)_j$.\n(12)\nAlgorithm 1 summarizes all the steps introduced in this section."}, {"title": "E. Determining the Scaling Factor", "content": "Here, we derive the scaling factor \u03b3 to normalize the prediction vectors with additional noise introduced due to"}, {"title": "V. PRIVACY ANALYSIS", "content": "In this section, we provide the privacy analysis of the proposed over-the-air ensemble scheme. Note that, as discussed in Section IV-B, while calculating the DP guarantees, the channel noise $n_t$ is ignored. Thus, our guarantees are upper bounds and the actual privacy guarantees can be stronger due to the channel noise.\nWe first analyze the case in which all the clients participate.\nTheorem 1. If all the clients participate in the inference, i.e., p = 1, then, Algorithm 1 is (\u03b5, \u03b4)-DP such that for any \u03b5 > 0,\n$\\delta = \\Phi(\\frac{1}{\\sqrt{2}\\sigma} - \\frac{\\epsilon \\sigma}{\\sqrt{2}}) - e^{\\epsilon} \\Phi(-\\frac{1}{\\sqrt{2}\\sigma} - \\frac{\\epsilon \\sigma}{\\sqrt{2}})$, (23)\nwhere \u03a6 is the CDF of standard normal distribution.\nProof. Our theorem is a special case of the following lemma.\nLemma 1 (Theorem 8 in [64]). Let $f : L \\rightarrow \\mathbb{R}^k$ be a function with $||f(L) - f(L')||_2 \\leq C$, where L and L' are neighboring inputs and $|| \\cdot ||_2$ is L2 norm. A mechanism $M(L) = f(L) + \\mathcal{N}(0, \\sigma^2I_k)$ is (\u03b5, \u03b4)-DP if and only if\n$\\Phi(\\frac{C}{(2\\sigma)} - \\frac{\\epsilon \\sigma}{C}) - e^{\\epsilon} \\Phi(-\\frac{C}{(2\\sigma)} - \\frac{\\epsilon \\sigma}{C}) < \\delta$.\n(24)\nTo apply Lemma 1 directly in our case, first observe that \u03b3 and P are common to all the clients. Hence, the effective noise is directly added to $\\tilde{z_t} = \\sum_{i \\in P_t} f_i(x_{i,t})$, and we need to calculate the L2 sensitivity, C, of $\\tilde{z_t}$. Note that we can also ignore the operation in Eq. (6) since it is only a shift operation that does not change the sensitivity calculations. Consider neighboring sets L and L'. We denote the noiseless vector received by the CIS by $\\tilde{z_t}$ when the set of local models is L, and by $\\tilde{z'_t}$ when it is L'. Then,\n$C = \\underset{\\tilde{z_t}, \\tilde{z'_t}}{\\text{max}} ||\\tilde{z_t} - \\tilde{z'_t}||_2 = \\underset{\\tilde{z_t}, \\tilde{z'_t}}{\\text{max}} \\bigg( \\sum_{j=1}^{k} (\\tilde{z}_{t,j} - \\tilde{z'}_{t,j})^2 \\bigg)^{1/2}$\n(25)"}, {"title": "Theorem 2.", "content": "If each client independently participate in inference with probability p < 1, then Algorithm 1 is (\u03b5', \u03b4')-DP, where, for any \u03b5' > 0,\n$\\delta' = \\frac{1-(1-p)^n}{\\gamma^n} \\bigg( \\Phi(\\frac{1}{\\sqrt{2}\\sigma} - \\frac{\\epsilon \\sigma}{\\sqrt{2}})  - e^{\\epsilon'} \\Phi(-\\frac{1}{\\sqrt{2}\\sigma} - \\frac{\\epsilon \\sigma}{\\sqrt{2}}) \\bigg),$\n(26)\nwhere $\\epsilon = \\text{log} \\bigg( 1 + ((\\frac{1 - (1-p)^n}{p}) (e^{\\epsilon'} - 1)) \\bigg)$.\nProof. Without loss of generality, let L and L' be two neighbouring sets of models differing only in the first client's model, i.e., it is either $f_1$ or $f'_1$. Let us write the output distribution of Algorithm 1 as a mixture distribution. When the model set is L, we have $\\mu = (1 - \\eta)\\mu_0 + \\eta \\mu_1$ and when the model set is L', we have $\\mu' = (1 - \\eta)\\mu_0 + \\eta \\mu'_1$. In these expressions, \u03b7 is the probability that client 1 is sampled, $\u03bc_0$ is the output distribution when client 1 is not sampled, $\u03bc_1$ is the output distribution when client 1 is sampled and the model set is L, while $\u03bc'_1$ is the output distribution when client 1 is sampled and the model set is L'. Recall that we sample client models each with probability p from L or L', and the CIS receives non-zero vectors only when |Pt| > 0. Hence, \u03b7 = Pr{Client 1 is sampled | |Pt| > 0}, resulting in $\\eta = \\frac{p}{1 - (1 - p)^n}$ via Bayes' rule.\nLemma 2 (Theorem 1 in [65]). A mechanism M is (\u03b5', \u03b4')-DP if and only if\n$\\underset{L, L'}{\\text{sup}} D_\\alpha(M(L) || M(L')) \\leq \\delta'$,\nwhere $\u03b1 = e^{\\epsilon'}$ and $D_\u03b1(\u03bc||\u03bc') = \\int_z \\text{max} \\{0, \\frac{d\u03bc(z)}{\\alpha} - d\u03bc'(z)\\}dz$.\n(27)\nLemma 2 implies that it is enough to bound $D_\u03b1(\u03bc||\u03bc')$ to provide DP guarantees. For this, we use the relation in Lemma 3, which is called advanced joint convexity of $D_\u03b1$.\nLemma 3 (Theorem 2 in [65]). For $\u03b1 \\geq 1$, we have\n$D_{\\alpha'}(\u03bc||\u03bc') \\leq \\eta D_\u03b1(\u03bc_1||(1 - \\beta)\u03bc_0 + \\beta \u03bc'_1),$\n(28)\nwhere \u03b1' = 1 + \u03b7(\u03b1 \u2212 1) and $\\beta = \\frac{\u03b1'}{\u03b1}$.\nWe further upper bound Eq. (28) via convexity:\n$D_{\\alpha'}(\u03bc||\u03bc') \\leq \\eta(1 - \\beta)D_\u03b1(\u03bc_1||\u03bc_0) + \\eta \\beta D_\u03b1(\u03bc_1||\u03bc'_1)$.\n(29)\nTo bound $D_\u03b1(\u03bc_1||\u03bc_0)$, observe that there exists a coupling between $\u03bc_1$ and $\u03bc_0$ as follows. For $\u03bc_0$, to guarantee |Pt| > 0, let us first sample exactly one client c other than client 1 since we know that client 1 is not sampled. Then, we apply Poisson sampling on the remaining set, i.e., [n] \\ {c, 1}, to"}, {"title": "VI. NUMERICAL RESULTS", "content": "In this section, we present our experimental setup, implementation details and simulation results."}, {"title": "A. The Datasets", "content": "We employ eight different multi-class classification datasets to demonstrate the effectiveness of our framework: CIFAR-10, CIFAR-100, FashionMNIST, Food101, OxfordPets, Emotion, Imdb, and MultiviewPets. CIFAR-10 is a multi-class image classification dataset containing 50000 training images, 10000 test images, and 10 target classes [66", "66": ".", "67": ".", "68": ".", "69": ".", "70": ".", "71": "."}]}