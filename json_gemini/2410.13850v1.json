{"title": "INFLUENCE FUNCTIONS FOR SCALABLE DATA ATTRIBUTION IN DIFFUSION MODELS", "authors": ["Bruno Mlodozeniec", "Alexander Immer", "Runa Eschenhagen", "David Krueger", "Juhan Bae", "Richard Turner"], "abstract": "Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability.his paper, we aim to help address such challenges in diffusion models by developing an influence functions framework. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we systematically develop K-FAC approximations based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We recast previously proposed methods as specific design choices in our framework, and show that our recommended method outperforms previous data attribution approaches on common evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative modelling for continuous data modalities \u2014 like images, video, and audio - has advanced rapidly propelled by improvements in diffusion-based approaches. Many companies now offer easy access to AI-generated bespoke image content. However, the use of these models for commercial purposes creates a need for understanding how the training data influences their outputs. In cases where the model's outputs are undesirable, it is useful to be able to identify, and possibly remove, the training data instances responsible for those outputs. Furthermore, as copyrighted works often make up a significant part of the training corpora of these models, concerns about the extent to which individual copyright owners' works influence the generated samples arise. Some already characterise what these companies offer as \u201ccopyright infringement as a service\" , which has caused a flurry of high-profile lawsuits Saveri & Butterick (2023a;b). This motivates exploring tools for data attribution that might be able to quantify how each group of training data points influences the models' outputs. Influence functions offer precisely such a tool. By approximating the answer to the question, \"If the model was trained with some of the data excluded, what would its output be?\", they can help finding data"}, {"title": "2 BACKGROUND", "content": "This section introduces the general concepts of diffusion models, influence functions, and the GGN."}, {"title": "2.1 DIFFUSION MODELS", "content": "Diffusion models are a class of probabilistic generative models that fit a model $p_\\theta(x)$ parameterised by parameters $\\theta \\in \\mathbb{R}^{d_{param}}$ to approximate a training data distribution $q(x)$, with the primary aim being to sample new data $x \\sim p_\\theta(x)$. This is usually done by augmenting the original data $x$ with $T$ fidelity levels as $x^{(0:T)} = [x^{(0)},...,x^{(T)}]$ with an augmentation distribution $q(x^{(0:T)})$ that satisfies the following criteria: 1) the highest fidelity $x^{(0)}$ equals the original training data $q(x^{(0)}) = q(x)$, 2) the lowest fidelity $x^{(T)}$ has a distribution that is easy to sample from, and 3) predicting a lower fidelity level from the level directly above it is simple to model and learn. To achieve the above goals, $q$ is typically taken to be a first-order Gaussian auto-regressive (diffusion) process: $q(x^{(t)}\\vert x^{(0:t-1)}) = \\mathcal{N}(x^{(t)}\\vert \\lambda_t x^{(t-1)}, (1 - \\lambda_t)^2 I)$, with hyperparameters $\\lambda_t$ set so that the law of $x^{(T)}$ approximately matches a standard Gaussian distribution $\\mathcal{N}(0, I)$. In that case, the reverse conditionals $q(x^{(t-1)}\\vert x^{(t:T)}) = q(x^{(t-1)}\\vert x^{(t)})$ are first-order Markov, and if the number of fidelity levels $T$ is high enough, they can be well approximated by a diagonal Gaussian, allowing them to be modelled with a parametric model with a simple likelihood function, hence satisfying (3) (Turner et al., 2024). The marginals $q(x^{(t)}\\vert x^{(0)}) = \\mathcal{N} (x^{(t)}\\vert (\\prod_{t'=1}^t \\lambda_{t'}) x^{(0)}, (1 - \\prod_{t'=1}^t \\lambda_{t'}) I)$ also have a simple Gaussian form, allowing for the augmented samples to be sampled as:\n\n$x^{(t)} = \\prod_{t'=1}^t \\lambda_{t'} x^{(0)} + (1 - \\prod_{t'=1}^t \\lambda_{t'})^{1/2} \\epsilon^{(t)}, \\quad \\text{with} \\quad \\epsilon^{(t)} \\sim \\mathcal{N}(0, I)$.\n\nDiffusion models are trained to approximate the reverse conditionals $p_\\theta(x^{(t-1)}\\vert x^{(t)}) \\approx q(x^{(t-1)}\\vert x^{(t)})$ by maximising log-probabilities of samples $x^{(t-1)}$ conditioned on $x^{(t)}$, for all timesteps $t = 1, ..., T$. We can note that $q(x^{(t-1)}\\vert x^{(t)}, x^{(0)})$ has a Gaussian distribution with mean given by:\n\n$\\mu_{t-1\\vert t,0}(x^{(t)}, \\epsilon^{(t)}) = \\frac{1}{\\lambda_t} \\left( x^{(t)} - \\frac{1 - \\lambda_t}{\\sqrt{1 - \\prod_{t'=1}^t \\lambda_{t'}^2}} \\epsilon^{(t)} \\right) ,\\quad \\text{with} \\quad \\epsilon^{(t)} \\overset{\\text{def}}{=} \\frac{(x^{(t)} - \\prod_{t'=1}^t \\lambda_{t'} x^{(0)})}{(1 - \\prod_{t'=1}^t \\lambda_{t'}^2)^{1/2}}~,$"}, {"title": "2.2 INFLUENCE FUNCTIONS", "content": "The aim of influence functions is to answer questions of the sort \"how would my model behave were it trained on the training dataset with some datapoints removed\". To do so, they approximate the change in the optimal model parameters in Equation (3) when some training examples $(x_j)_{j \\in \\mathcal{I}}$, $\\mathcal{I} = \\{1, ..., i_M\\} \\subseteq [N]$, are removed from the dataset $\\mathcal{D}$. To arrive at a tractable approximation, it is useful to consider a continuous relaxation of this question: how would the optimum change were the training examples $(x_j)_{j \\in \\mathcal{I}}$ down-weighted by $\\epsilon \\in \\mathbb{R}$ in the training loss:\n\n$\\r^{-1}(\\epsilon) = \\text{arg}\\min_{\\theta} \\frac{1}{N} \\sum_{n=1}^N \\ell(\\theta, x_n) - \\epsilon \\sum_{j \\in \\mathcal{I}} \\ell(\\theta, x_j)$.\n\nThe function $\\r^{-1}: \\mathbb{R} \\rightarrow \\mathbb{R}^{d_{param}}$ (well-defined if the optimum is unique) is the response function. Setting $\\epsilon$ to $N$ recovers the minimum of the original objective in Equation (3) with examples $(x_{i_1},..., x_{i_M})$ removed.\n\nUnder suitable assumptions (see Appendix A), by the Implicit Function Theorem , the response function is continuous and differentiable at $\\epsilon = 0$. Influence functions can be defined as a linear approximation to the response function $\\r^{-1}$ by a first-order Taylor expansion around $\\epsilon = 0$:\n\n$\\r^{-1}(\\epsilon) = \\r^{-1}(0) + \\frac{d \\r^{-1}(\\epsilon')}{d \\epsilon'} \\bigg\\vert_{\\epsilon'=0} \\epsilon + o(\\epsilon) = \\theta^*(\\mathcal{D}) + \\sum_{j \\in \\mathcal{I}} (\\nabla^2_{\\theta \\theta} L_\\mathcal{D}(\\theta^*))^{-1} \\nabla_{\\theta^*} \\ell(\\theta^*, x_j) + o(\\epsilon),$ \n\nAs $\\epsilon \\rightarrow 0$. See Appendix A for a formal derivation and conditions. The optimal parameters with examples $(x_i)_{i \\in \\mathcal{I}}$ removed can be approximated by setting $\\epsilon$ to $N$ and dropping the $o(\\epsilon)$ terms.\n\nUsually, we are not directly interested in the change in parameters in response to removing some data, but rather the change in some measurement function $m(\\theta^*(\\mathcal{D}), x')$ at a particular test input $x'$ (e.g. per-example test loss). We can further make a first-order Taylor approximation to $m(\\cdot, x')$ at $\\theta^*(\\mathcal{D}) - m(\\theta, x') = m(\\theta^*, x') + \\nabla_{\\theta^*} m(\\theta^*, x')(\\theta - \\theta^*) + o(\\| \\theta - \\theta^* \\|^2) - and combine it with Equation (5) to get a simple linear estimate of the change in the measurement function:\n\n$m(\\r^{-1}(\\epsilon), x') = m(\\theta^*, x') + \\sum_{j \\in \\mathcal{I}} \\nabla_{\\theta^*} m(\\theta^*, x') (\\nabla^2_{\\theta \\theta} L_\\mathcal{D}(\\theta^*))^{-1} \\nabla_{\\theta^*} \\ell(\\theta^*, x_j) \\epsilon + o(\\epsilon).$"}, {"title": "2.2.1 GENERALISED GAUSS-NEWTON MATRIX", "content": "Computing the influence function approximation in Equation (5) requires inverting the Hessian $\\nabla^2_{\\theta \\theta} L_\\mathcal{D}(\\theta) \\in \\mathbb{R}^{d_{param} \\times d_{param}}$. In the context of neural networks, the Hessian itself is generally computationally intractable and approximations are necessary. A common Hessian approximation is the generalised Gauss-Newton matrix (GGN). We will first introduce the GGN in an abstract setting of approximating the Hessian for a general training loss $L(\\theta) = E_z [\\rho(\\theta, z)]$, to make it clear how different variants can be arrived at for diffusion models in the next section.\n\nIn general, if we have a function $\\rho(\\theta, z)$ of the form $h_z \\circ f_z(\\theta)$, with $h_z$ a convex function, the GGN for an expectation $E_z[\\rho(\\theta, z)]$ is defined as\n\n$GGN(\\theta) = E_z [\\nabla f_z(\\theta) (\\nabla^2_{f_z(\\theta)} h_z(f_z(\\theta))) \\nabla f_z(\\theta)^\\top]$,\n\nwhere $\\nabla f_z(\\theta)$ is the Jacobian of $f_z$. Whenever $f_z$ is (locally) linear, the GGN is equal to the Hessian $E_z [\\rho(\\theta, z)]$. Therefore, we can consider the GGN as an approximation to the Hessian in which we \u201clinearise\u201d the function $f_z$. Note that any decomposition of $\\rho(\\theta, z)$ results in a valid GGN as long as $h_z$ is convex Martens (2020). We give two examples below."}, {"title": "3 SCALABLE INFLUENCE FUNCTIONS FOR DIFFUSION MODELS", "content": "In this section, we discuss how we adapt influence functions to the diffusion modelling setting in a scalable manner. We also recast data attribution methods for diffusion models proposed in prior work as the result of particular design decisions in our framework, and argue for our own choices that distinguish our method from the previous ones."}, {"title": "3.1 APPROXIMATING THE HESSIAN", "content": "In diffusion models, we want to compute the Hessian of the loss of the form\n\n$L_\\mathcal{D}(\\theta) = E_{x_n} [\\ell(\\theta,x_n)] = E_{x_n} [E_t [E_{x(t), \\epsilon(t)} [\\frac{1}{2}||\\epsilon^{(t)} - \\epsilon_\\theta(x^{(t)})||^2]]]$,\n\nwhere $E_{x_n} [] = \\frac{1}{N}(\\sum_{n=1}^N)$ is the expectation over the empirical data distribution. We willibe how to formulate different GGN approximations for this setting."}, {"title": "3.1.1 GGN FOR DIFFUSION MODELS", "content": "Option 1. To arrive at a GGN approximation, as discussed in Section 2.2.1, we can partition the function $\\theta \\rightarrow ||\\epsilon^{(t)} - \\epsilon_\\theta(x^{(t)})||^2$ into the model output $\\theta \\rightarrow \\epsilon_\\theta(x^{(t)})$ and the $l_2$-loss function $||\\epsilon^{(t)} - \\cdot||^2$. This results in the GGN:\n\n$\\begin{aligned}f_z &:= \\epsilon_\\theta(x^{(t)})\\\\h_z &:= ||\\epsilon^{(t)} - \\cdot||^2\\end{aligned} \\quad \\rightarrow GGN^{model}(\\theta) = E_{x_n} \\Big[E_t \\Big[E_{x(t), \\epsilon(t)} \\Big[\\nabla_{\\epsilon_\\theta(x^{(t)})} \\epsilon_\\theta(x^{(t)})^\\top (2I) (\\nabla_{\\epsilon_\\theta(x^{(t)})} \\epsilon_\\theta(x^{(t)})^\\top)^\\top\\Big]\\Big]\\Big],$ \n\nwhere $I$ is the identity matrix. This correspond to \u201clinearising\u201d the neural network $\\epsilon_\\theta$. For diffusion models, the dimensionality of the output of $\\epsilon_\\theta$ is typically very large (e.g. $32 \\times 32 \\times 3$ for CIFAR), so computing the Jacobians $\\nabla_{\\epsilon_\\theta(x^{(t)})} \\epsilon_\\theta(x^{(t)})$ explicitly is still intractable. However, we can express $GGN^{model}$ as\n\n$F_\\mathcal{D}(\\theta) = E_{x_n} \\Big[E_t \\Big[E_{x(t)} \\Big[E_{\\epsilon_{mod} \\sim \\mathcal{N}(\\epsilon_\\theta(x^{(t)}), I)} [g_n(\\theta)g_n(\\theta)^\\top]\\Big]\\Big]\\Big]$,"}, {"title": "3.1.2 K-FAC FOR DIFFUSION MODELS", "content": "While $F_\\mathcal{D}(\\theta)$ and $GGN^{loss}$ do not require computing full Jacobians or the Hessian of the neural network model, they involve taking outer products of gradients of size $\\mathbb{R}^{d_{param}}$, which is still intractable. Kronecker-Factored Approximate Curvature (Heskes, 2000; Martens & Grosse, 2015, K-FAC) is a common scalable approximation of the GGN to overcome this problem. It approximates the GGN with a block-diagonal matrix, where each block corresponds to one neural network layer and consists of a Kronecker product of two matrices. Due to convenient properties of the Kronecker product, this makes the inversion and multiplication with vectors needed in Equation (6) efficient enough to scale to large networks. K-FAC is defined for linear layers, including linear layers with weight sharing like convolutions (Grosse & Martens, 2016). This covers most layer types in the architectures typically used for diffusion models. When weight sharing is used, there are two variants \u2013 K-FAC-expand and K-FAC-reduce (Eschenhagen et al., 2023). For our recommended method, we choose to approximate the Hessian with a K-FAC approximation of $F_\\mathcal{D}$, akin to Grosse et al. (2023).\n\nFor the parameters $\\theta_l$ of layer $l$, the GGN $F_\\mathcal{D}$ in Equation (10) is approximated by\n\n$F_\\mathcal{D}(\\theta_l) \\approx \\frac{1}{N} \\sum_{x_n} \\Big[\\frac{1}{T} \\sum_t \\Big[E_{x(t), \\epsilon(t)} \\Big[E_{\\epsilon_{mod}} [a_n^{(l)} a_n^{(l) \\top}] \\otimes E_{\\epsilon_{mod}} [b_n^{(l)} b_n^{(l) \\top}]\\Big]\\Big]\\Big],$"}, {"title": "3.2 GRADIENT COMPRESSION AND QUERY BATCHING", "content": "In practice, we recommend computing influence function estimates in Equation (6) by first computing and storing the approximate Hessian inverse, and then iteratively computing the preconditioned inner products $\\nabla_{\\theta^*} m(\\theta^*, x') (\\nabla^2_{\\theta \\theta} L_\\mathcal{D}(\\theta^*))^{-1} \\nabla_{\\theta^*} \\ell(\\theta^*, x_j)$ for different training datapoints $x_j$. Following Grosse et al. (2023), we use query batching to avoid recomputing the gradients $\\nabla_{\\theta^*} \\ell(\\theta^*, x_j)$ when attributing multiple samples $x'$. We also use gradient compression; we found that compression by quantisation works much better for diffusion models compared to the SVD-based compression used by Grosse et al. (2023), likely due to the fact that gradients $\\nabla_{\\theta} \\ell(\\theta,x_n)$ are not low-rank in this setting."}, {"title": "3.3 WHAT TO MEASURE", "content": "For diffusion models, arguably the most natural question to ask might be, for a given sample $x$ generated from the model, how did the training samples influence the probability of generating a sample $x$? For example, in the context of copyright infringement, we might want to ask if removing certain copyrighted works would substantially reduce the probability of generating $x$. With influence functions, these questions could be interpreted as setting the measurement function $m(\\theta, x)$ to be the (marginal) log-probability of generating $x$ from the diffusion model: $\\log p_\\theta(x)$.\n\nComputing the marginal log-probability introduces some challenges. Diffusion models have originally been designed with the goal of tractable sampling, and not log-likelihood evaluation. Ho et al. (2020); Sohl-Dickstein et al. (2015) only introduce a lower-bound on the marginal log-probability. Song et al. (2021b) show that exact log-likelihood evaluation is possible, but it only makes sense in settings where the training data distribution has a density (e.g. uniformly dequantised data), and it only corresponds to the marginal log-likelihood of the model when sampling deterministically (Song et al., 2021a). Also, taking gradients of that measurement, as required for influence functions, is non-trivial. Hence, in most cases, we might need a proxy measurement for the marginal probability. We consider a couple of proxies in this work:\n\n1.  Loss. Approximate $\\log p_\\theta (x)$ with the diffusion loss $\\ell(\\theta, x)$ in Equation (2) on that particular example. This corresponds to the ELBO with reweighted per-timestep loss terms (see Figure 18).\n2.  Probability of sampling trajectory. If the entire sampling trajectory $x^{(0:T)}$ that gen- erated sample $x$ is available, consider the probability of that trajectory $p_\\theta(x^{(0:T)}) = p(x) \\prod_{t=1}^T p_\\theta (x^{(t-1)} \\vert x^{(t)})$.\n3.  ELBO. Approximate $\\log p_\\theta (x)$ with an Evidence Lower-Bound (Ho et al., 2020, eq. (5)).\n\nInitially, we might expect ELBO to be the best motivated proxy, as it is the only one with a clear link to the marginal log-probability. Probability of sampling trajectory might also appear sensible, but it doesn't take into account the fact that there are multiple trajectories $x^{(0:T)}$ that all lead to the same final sample $x^{(0)}$, and it has the disadvantage of not being reparameterisation invariant. We empirically investigate these different proxies in Section 4."}, {"title": "4 EXPERIMENTS", "content": "Evaluating Data Attribution. To evaluate the proposed data attribution methods, we primarily focus on two metrics: Linear Data Modelling Score (LDS) and retraining without top influences. LDS measures how well a given attribution method can predict the relative magnitude in the change in a measurement as the model is retrained on (random) subsets of the training data. For an attribution method $a(\\mathcal{D}, \\mathcal{D}', x)$ that approximates how a measurement $m(\\theta^*(\\mathcal{D}), x)$ would change if a model was trained on an altered dataset $\\mathcal{D}'$, LDS measures the Spearman rank correlation between the predicted change in output and actual change in output after retraining on different subsampled"}, {"title": "4.1 POTENTIAL CHALLENGES TO USE OF INFLUENCE FUNCTIONS FOR DIFFUSION MODELS", "content": "One peculiarity in the LDS results, similar to the findings in Zheng et al. (2024), is that substituting the loss measurement for the ELBO measurement when predicting changes in ELBO actually works"}, {"title": "5 DISCUSSION", "content": "In this work, we extended the influence functions approach to the diffusion modelling setting, and showed different ways in which the GGN Hessian approximation can be formulated in this setting. Our proposed method with recommended design choices improves performance copmared to existing techniques across various data attribution evaluation metrics. Nonetheless, experimentally, we are met with two contrasting findings: on the one hand, influence functions in the diffusion modelling setting appear to be able to identify important influences. The surfaced influential examples do significantly impact the training loss when retraining the model without them , and they appear perceptually very relevant to the generated samples. On the other hand, they fall short of accurately predicting the numerical changes in measurements after retraining. Thes appears to be especially the case for measurement functions we would argue are most relevant in the image generative modelling setting \u2013 proxies for marginal probability of sampling a particular example. This appears to be both due to the limitations of the influence functions approximation, but also due to the shortcomings of the considered proxy measurements.\n\nDespite these shortcomings, influence functions can still offer valuable insights: they can serve as a useful exploratory tool for understanding model behaviour in a diffusion modelling context, and can help guide data curation, identifying examples most responsible for certain behaviours. To make them useful in settings where numerical accuracy in the predicted behaviour after retraining is required, such as copyright infringement, we believe more work is required into 1) finding better proxies for marginal probability than ELBO and probability of sampling trajectory, and 2) even further improving the influence function approximation."}, {"title": "A DERIVATION OF INFLUENCE FUNCTIONS", "content": "In this section, we state the implicit function theorem (Appendix A.1). Then, in Appendix A.2, we introduce the details of how it can be applied in the context of a loss function L(\u03b5, \u03b8) parameterised by a continuous hyperparameter \u025b (which is, e.g., controlling how down-weighted the loss terms on some examples are, as in Section 2.2)."}, {"title": "A.1 IMPLICIT FUNCTION THEOREM", "content": "Theorem 1 (Implicit Function Theorem ) Let $F: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$ be a continuously differentiable function, and let $\\mathbb{R}^n \\times \\mathbb{R}^m$ have coordinates (x,y). Fix a point $(a,b) = (a_1,..., a_n, b_1,...,b_m)$ with $F(a,b) = 0$, where $0 \\in \\mathbb{R}^m$ is the zero vector. If the Jacobian matrix $\\nabla_y F(a, b) \\in \\mathbb{R}^{m \\times m}$ of $y \\rightarrow F(a, y)$\n\n$[\\nabla_y F(a,b)]_{ij} = \\frac{\\partial F_i}{\\partial y_j}(a, b),$\n\nis invertible, then there exists an open set $U \\subset \\mathbb{R}^n$ containing $a$ such that there exists a unique function $g: U \\rightarrow \\mathbb{R}^m$ such that $g(a) = b$, and $F(x, g(x)) = 0$ for all $x \\in U$. Moreover, $g$ is continuously differentiable.\n\nRemark 1 (Derivative of the implicit function) Denoting the Jacobian matrix of $x \\rightarrow F(x, y)$ as:\n\n$[\\nabla_x F(x,y)]_{ij} = \\frac{\\partial F_i}{\\partial x_j}(x, y),$\n\nthe derivative: $U \\rightarrow \\mathbb{R}^{m \\times n}$ of $g: U \\rightarrow \\mathbb{R}^m$ in Theorem 1 can be written as:\n\n$\\frac{\\partial g(x)}{\\partial x} = - [\\nabla_y F(x, g(x))]^{-1} \\nabla_x F(x, g(x))$.\n\nThis can readily be seen by noting that, for $x \\in U$:\n\n$F(x', g(x')) = 0 \\quad \\forall x' \\in U \\quad \\Rightarrow \\quad \\frac{d F(x, g(x))}{d x} = 0$.\n\nHence, since $g$ is differentiable, we can apply the chain rule of differentiation to get:\n\n$\\frac{d F(x, g(x))}{d x} = \\nabla_x F(x, g(x)) + \\nabla_y F(x, g(x)) \\frac{\\partial g(x)}{\\partial x}$.\n\nRearranging gives equation Equation (14)."}, {"title": "A.2 APPLYING THE IMPLICIT FUNCTION THEOREM TO QUANTIFY THE CHANGE IN THE OPTIMUM OF A LOSS", "content": "Consider a loss function $L: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that depends on some hyperparameter $\\epsilon \\in \\mathbb{R}^n$ (in Section 2.2, this was the scalar by which certain loss terms were down-weighted) and some parameters $\\theta \\in \\mathbb{R}^m$. At the minimum of the loss function $L(\\epsilon, \\theta)$, the derivative with respect to the parameters $\\theta$ will be zero. Hence, assuming that the loss function is twice continuously differentiable (hence $\\frac{\\partial L}{\\partial \\theta}$ is continuously differentiable), and assuming that for some $\\epsilon' \\in \\mathbb{R}^n$ we have a set of parameters $\\theta^*$ such that $\\frac{\\partial L}{\\partial \\theta}(\\epsilon', \\theta^*) = 0$ and the Hessian $\\frac{\\partial^2 L}{\\partial \\theta^2}(\\epsilon', \\theta^*)$ is invertible, we can apply the implicit function theorem to the derivative of the loss function: $\\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$, to get the existence of a continuously differentiable function $g$ such that $\\frac{\\partial L}{\\partial \\theta}(\\epsilon, g(\\epsilon)) = 0$ for $\\epsilon$ in some neighbourhood of $\\epsilon'$.\n\nNow $g(\\epsilon)$ might not necessarily be a minimum of $\\theta \\rightarrow L(\\epsilon, \\theta)$. However, by making the further assumption that $L$ is strictly convex we can ensure that whenever $\\frac{\\partial L}{\\partial \\theta}(\\epsilon, \\theta) = 0$, $\\theta$ is a unique minimum, and so $g(\\epsilon)$ represents the change in the minimum as we vary $\\epsilon$. This is summarised in the lemma below:\n\nLemma 1 Let $L: \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ be a twice continuously differentiable function, with coordinates denoted by $(\\epsilon, \\theta) \\in \\mathbb{R}^n \\times \\mathbb{R}^m$, such that $\\theta \\rightarrow L(\\epsilon, \\theta)$ is strictly convex $\\forall \\epsilon \\in \\mathbb{R}^n$. Fix a point $(\\epsilon', \\theta^*)$ such that $\\frac{\\partial L}{\\partial \\theta}(\\epsilon', \\theta^*) = 0$. Then, by the Implicit Function Theorem applied to $\\frac{\\partial L}{\\partial \\theta}$, there exists an open set $U \\subset \\mathbb{R}^n$ containing $\\epsilon'$ such that there exists a unique function $g: U \\rightarrow \\mathbb{R}^m$ such that $g(\\epsilon') = \\theta^*$, and $g(\\epsilon)$ is the unique minimum of $\\theta \\rightarrow L(\\epsilon, \\theta)$ for all $\\epsilon \\in U$. Moreover, $g$ is continuously differentiable with derivative:\n\n$\\frac{\\partial g(\\epsilon)}{\\partial \\epsilon} = - \\Big[\\frac{\\partial^2 L}{\\partial \\theta^2}(\\epsilon, g(\\epsilon))\\Big]^{-1} \\frac{\\partial^2 L}{\\partial \\epsilon \\partial \\theta}(\\epsilon, g(\\epsilon))$\n\nRemark 2 For a loss function $L: \\mathbb{R} \\times \\mathbb{R}^m$ of the form $L(\\epsilon, \\theta) = L_1(\\theta) + \\epsilon L_2(\\theta)$ (such as that in Equation (4)), $\\frac{\\partial^2 L}{\\partial \\epsilon \\partial \\theta}(\\epsilon, g(\\epsilon))$ in the equation above simplifies to:\n\n$\\frac{\\partial^2 L}{\\partial \\epsilon \\partial \\theta}(\\epsilon, g(\\epsilon)) = \\frac{\\partial L_2}{\\partial \\theta}(g(\\epsilon))$"}, {"title": "B DERIVATION OF THE FISHER \"GGN\" FORMULATION FOR DIFFUSION MODELS", "content": "As discussed in Section 2.2.1 partitioning the function $\\theta \\mapsto ||\\epsilon^{(t)} - \\epsilon_\\theta(x^{(t)})||^2$ into the model output $\\theta \\rightarrow \\epsilon_\\theta(x^{(t)})$ and the $\\ell_2$ loss function is a natural choice and results in\n\n$\\begin{aligned}GGN^{model}(\\theta) &= \\\\&= \\frac{1}{N} \\sum_{x_n} \\Big[E_t \\Big[E_{x(t), \\epsilon(t)} \\Big[\\nabla_{\\epsilon_\\theta(x^{(t)})} \\epsilon_\\theta(x^{(t)})^\\top (2I) (\\nabla_{\\epsilon_\\theta(x^{(t)})} \\epsilon_\\theta(x^{(t)})^\\top)^\\top\\Big]\\Big]\\Big]\\end{aligned}$"}, {"title": "C GRADIENT COMPRESSION ABLATION", "content": "In Figure 6, we ablate different compression methods by computing the per training datapoint influ- ence scores with compressed query (measurement) gradients, and looking at the Pearson correlation and the rank correlation to the scores compute with the uncompressed gradients. We hope to see a correlation of close to 100%, in which case the results for our method would be unaffected by compression. We find that using quantisation for compression results in almost no change to the ordering over training datapoints, even when quantising down to 8 bits. This is in contrast to the SVD compression scheme used in Grosse et al. (2023). This is likely because the per-example gradients naturally have a low-rank (Kronecker) structure in the classification, regression, or autoregressive language modelling settings, such as that in Grosse et al. (2023). On the other hand, the diffusion training loss and other measurement functions considered in this work do not have this low-rank structure. This is because computing them requires multiple forward passes; for example, for the diffusion training loss we need to average the mean-squared error loss in Equation (2) over multiple noise samples $\\epsilon^{(t)}$ and multiple diffusion timesteps. We use 8 bit quantisation with query gradient batching (Grosse et al., 2023) for all KFAC experiments throughout this work."}, {"title": "D DAMPING LDS ABLATIONS", "content": "We report an ablation over the LDS scores with GGN approximated with different damping factors for TRAK/D-TRAK and K-FAC influence in Figures 7 to 10. The reported damping factors for TRAK are normalised by the dataset size so that they correspond to the equivalent damping factors for our method when viewing TRAK as an altenrative approximation to the GGN (see Section 3.1)."}, {"title": "E EMPIRICAL ABLATIONS FOR CHALLENGES TO USE OF INFLUENCE FUNCTIONS FOR DIFFUSION MODELS", "content": "In this section, we describe the results for the observations discussed in Section 4.1."}, {"title": "F LDS RESULTS FOR PROBABILITY OF SAMPLING TRAJECTORY", "content": "The results for the \"log probability of sampling trajectory\" measurements are shown in Figure 19. The probability of sampling trajectory appears to be a measurement with a particularly low correlation across different models trained with the same data, but different random seeds. This is perhaps unsurprising, since the measurement comprises the log-densities of particular values of 1000 latent variables."}, {"title": "G EXPERIMENTAL DETAILS", "content": "In this section, we describe the implementation details for the methods and baselines, as well as the evaluations reported in Section 4."}, {"title": "G.1 DATASETS", "content": "We focus on the following dataset in this paper:"}, {"title": "G.2 MODELS", "content": "For all CIFAR datasets, we train a regular Denoising Diffusion Probabilistic Model using a standard U-Net architecture as described for CIFAR-10 in Ho et al. (2020). This U-Net architecture contains"}, {"title": "G.3 DETAILS ON DATA ATTRIBUTION METHODS", "content": "TRAK For TRAK baselines, we adapt the implementation of Park et al"}]}