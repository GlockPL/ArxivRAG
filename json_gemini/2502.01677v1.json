{"title": "AI Scaling: From Up to Down and Out", "authors": ["Yunke Wang", "Yanxi Li", "Chang Xu"], "abstract": "AI Scaling has traditionally been synonymous\nwith Scaling Up, which builds larger and more\npowerful models. However, the growing de-\nmand for efficiency, adaptability, and collabo-\nration across diverse applications necessitates a\nbroader perspective. This position paper presents\na holistic framework for AI scaling, encompass-\ning Scaling Up, Scaling Down, and Scaling Out.\nIt argues that while Scaling Up of models faces\ninherent bottlenecks, the future trajectory of\nAI scaling lies in Scaling Down and Scaling\nOut. These paradigms address critical techni-\ncal and societal challenges, such as reducing car-\nbon footprint, ensuring equitable access, and en-\nhancing cross-domain collaboration. We explore\ntransformative applications in healthcare, smart\nmanufacturing, and content creation, demonstrat-\ning how AI Scaling can enable breakthroughs in\nefficiency, personalization, and global connectiv-\nity. Additionally, we highlight key challenges,\nincluding balancing model complexity with in-\nterpretability, managing resource constraints, and\nfostering ethical development. By synthesizing\nthese approaches, we propose a unified roadmap\nthat redefines the future of AI research and appli-\ncation, paving the way for advancements toward\nArtificial General Intelligence (AGI).", "sections": [{"title": "1 Introduction", "content": "The field of artificial intelligence (AI) has witnessed ex-\ntraordinary advancements over the past decade, largely\ndriven by the relentless pursuit of Scaling Up. Early break-\nthroughs were characterized by models with millions of\nparameters, such as AlexNet (Krizhevsky et al., 2012),\nword2vec (Church, 2017) and BERT (Devlin, 2018), which\npaved the way for deep learning's success. This progression\nquickly escalated to models with billions of parameters, ex-\nemplified by GPT-3 (175 billion parameters) (Brown et al.,\n2020) and more recently GPT-4 (Achiam et al., 2023), which\nhas further expanded the boundaries of language understand-\ning and generation. Similarly, vision-language models like\nCLIP (Radford et al., 2021) and Flamingo (Alayrac et al.,\n2022) have showcased the transformative power of scaling\nmultimodal architectures. These advancements highlight\nhow Scaling Up has enabled Al systems to achieve remark-\nable generalization and versatility across diverse tasks.\nHowever, as Scaling Up progresses, the field faces a criti-\ncal bottleneck: data (Shumailov et al., 2024). The success\nof scaling has been largely contingent on the availability\nof massive, high-quality datasets. Foundational datasets\nlike Common Crawl\u00b9 and large-scale multimodal Corpus\nhave been extensively mined, leaving diminishing returns\nfrom further expansion. While multimodal data sources\nremain an underexplored frontier, their integration presents\nunique challenges, including alignment across modalities\nand domain-specific constraints. Moreover, the cost of pro-\ncessing this data at scale, in terms of both computational\nenergy and infrastructure demands, compounds the difficulty\nof sustaining the current paradigm. These challenges un-\nderscore a pivotal question: can Scaling Up alone continue\nto deliver transformative progress, or are new paradigms\nrequired to achieve the ultimate vision of Artificial General\nIntelligence (AGI)?\nThis position paper presents a holistic framework for AI\nscaling (Fig. 1), encompassing Scaling Up, Scaling Down,\nand Scaling Out. It argues that while Scaling Up of models\nfaces inherent bottlenecks, the future trajectory of AI"}, {"title": "2 Scaling Up: Expanding Foundation Models", "content": "Scaling Up is critical for advancing AI research and ap-\nplications as it pushes the boundaries of what AI systems\ncan achieve. Larger models act as high-quality founda-\ntional models for both academia and industry, further set-\nting benchmarks and inspiring further innovations. These\nmodels are capable of solving a wide range of tasks while\nthey can serve as a foundation for creating specialized and\ndiverse AI interfaces through fine-tuning as well.\n2.1 Scaling in AI models\nThe past experience in AI Scaling Up is mostly based on in-\ncreasing data size, model size and computational resources.\nData Size. Expanding dataset size is a fundamental as-\npect of Scaling Up AI models, as it directly impacts the\nquality of the system. Large and diverse datasets expose\nmodels to a wide variety of knowledge, thereby enabling\nthem to perform effectively across multiple domains. For\ninstance, GPT-3 (Brown et al., 2020) was trained on 570GB\nof cleaned and curated text data drawn from sources such as\nCommon Crawl, BooksCorpus, and Wikipedia, which en-\nabled it to generate human-like responses across diverse\ncontexts. More recently, multi-modal datasets such as\nLAION-5B (Schuhmann et al., 2022) have been used to\nscale vision-language models like Stable Diffusion (Rom-\nbach et al., 2022), showcasing the impact of data size on\nmodel's capabilities.\nModel Size. Larger models have greater representational\npower, allowing them to capture complex relationships\nwithin data. For example, the 175B parameters of GPT-\n3 significantly outperform their predecessors in tasks that\nrequire learning of few shots or zero shots (Brown et al.,\n2020). Similarly, GLaM (Du et al., 2022) scaled to 1.2 tril-\nlion parameters using a mixture of experts, activating only a\nsubset of parameters per task, which reduced computational\ncosts while maintaining high performance. The scaling laws\nproposed by (Kaplan et al., 2020) highlight that model per-\nformance improves predictably with increased size. This\ninsight has guided the development of increasingly large\nmodels, unlocking capabilities like in-context learning and\ncross-modal understanding."}, {"title": "2.2 Bottleneck", "content": "From the data perspective, as pointed out by many re-\nsearchers, large-scale pretraining has already utilized most\nof the high-quality publicly available data on the web.\nThe remaining data is either low-quality or consists of AI-\ngenerated content, which risks model degradation due to\ndata contamination and reinforcement of biases (Shumailov\net al., 2024). Simply increasing the dataset size will no\nlonger yield the same level of improvement as before. From\nthe model perspective, while increasing parameters has led\nto substantial performance gains in recent years, the re-\nturns on scaling have shown diminishing improvements,\nand larger models suffer from inefficiencies such as redun-\ndancy in representation, overfitting to training distributions,\nand difficulties in interpretability and controllability. Ad-\nditionally, the training and inference of massive models\nintroduce challenges in optimization stability and robust-\nness (Dai et al., 2024b). From the computational resource\naspect, the exponential growth in required hardware, energy\nconsumption, and costs is reaching unsustainable levels.\nThe marginal benefit of adding more compute is decreas-\ning while the environmental impact is rising (Wu et al.,\n2024). The availability of high-performance GPUs poses\nfinancial constraints that limit the feasibility of further scal-\ning. Together, these bottlenecks indicate that the traditional\napproach of scaling up is approaching its practical limits."}, {"title": "2.3 Future Trends", "content": "Despite bottlenecks in AI scaling, Scaling Up remains es-\nsential for pushing AI model's performance boundary. The\nfuture of Scaling Up should lie in balancing efficiency, adapt-\nability and sustainability to meet the demands of larger mod-\nels. Innovations in dataset optimization, efficient training,\nand test-time scaling will redefine AI Scaling Up.\nDataset Optimization. As AI continues to scale, data op-\ntimization will become a cornerstone for advancing model\nefficiency and robustness. Future trends will focus on data-\nefficient training using smaller, high-quality datasets for\nfaster learning. Curriculum learning (Bengio et al., 2009)\nand active learning (Settles, 2009) will help models acquire\nknowledge incrementally and prioritize impactful samples.\nTechniques for handling noisy data, such as noise-robust\nloss functions and data augmentation, will enhance model\nresilience. Additionally, leveraging proprietary, domain-\nspecific datasets will drive breakthroughs by providing\nricher insights beyond public data.\nEfficient Training. Another trend is developing efficient\ntraining methods to address the growing computational and\nenvironmental costs of training large models. Progressive\ntraining, where models gradually scale from smaller sub-\nmodels to full-capacity systems, will become a standard\napproach to reduce resource demands in the initial stages.\nDistributed optimization techniques, such as asynchronous\ntraining paradigms, will improve scalability across large\ncomputational infrastructures. Advances in mixed-precision\ntraining, sparse updates, and activation checkpointing will\nfurther minimize memory and compute overhead, making\nAI development more sustainable and scalable.\nTest-time Scaling. Recent research has highlighted the po-\ntential of scaling up test-time computing to enhance the per-\nformance of large language models (LLMs), providing an\nalternative to solely scaling up model parameters. For exam-\nple, Snell et al. (2024) explore two strategies: adaptive out-\nput distribution and verifier-based search mechanisms, both\nimproving model performance dynamically. Unlike pre-\nvious inference-time optimization attempts, this approach\ntailors compute allocation to problem complexity, enabling\nsmaller models to outperform larger ones on certain prompts.\nAdaptive test-time scaling presents a promising direction\nfor optimizing efficiency without excessive pretraining."}, {"title": "3 Scaling Down: Refining Core Functions", "content": "As models become increasingly large and complex through\nScaling Up, their training, deployment, and maintenance\ndemand significant computational, memory, and energy re-\nsources. These challenges limit accessibility and scalabil-\nity. A critical question emerges: how can we maintain or\nimprove model effectiveness while reducing size and compu-\ntational requirements? Drawing inspiration from the human\nbrain, where specialized small units handle essential func-\ntions while auxiliary components support adaptability and\nmemory, the Scaling Down concept offers a novel approach.\nBy identifying and extracting the essential functional mod-\nules of large models, Scaling Down makes it possible to\nreduce the model size and computation costs significantly"}, {"title": "3.1 Reducing the Size of Large Models", "content": "The most straightforward approach to reducing model size\ninvolves reducing the number of parameters within a model.\nPruning achieves this by simplifying neural networks\nthrough the removal of less significant components (Le-\nCun et al., 1989; Han et al., 2015; Molchanov et al., 2016).\nLLM-Pruner (Ma et al., 2023) proposes a task-agnostic\napproach to structural pruning by selectively removing non-\ncritical structures using gradient information. Wanda (Sun\net al., 2024) emphasizes simplicity and efficiency by prun-\ning weights based on the product of weight magnitudes\nand corresponding input activations without the need for\nretraining or weight updates.\nAn alternative to directly removing parameters is the use of\nlow-rank approximations, which employ smaller matrices\nto approximate larger ones Sainath et al. (2013). Low-Rank\nAdaptation (LoRA) (Hu et al., 2021) tackles the inefficiency\nof fine-tuning all model parameters by introducing trainable\nlow-rank decomposition matrices into Transformer layers\nwhile keeping the pre-trained model weights frozen. Lin-\nformer (Wang et al., 2020) leverages the observation that\nself-attention mechanisms in Transformers exhibit low-rank\nstructures. By approximating the self-attention matrix with\na low-rank factorization, Linformer reduces the time and\nspace complexity of self-attention to a linear scale.\nAnother effective strategy focuses on reducing parameter\nprecision rather than quantity. Quantization reduces the\nbit-width of weights and activations by substituting floating-\npoint parameters with integers (Gupta et al., 2015; Nagel\net al., 2020). GPTQ (Frantar et al., 2022) introduces an\nefficient one-shot weight quantization method based on ap-\nproximate second-order information. AWQ (Lin et al., 2024)\nfocuses on activation-aware weight quantization, leveraging\nthe unequal importance of weights and optimal per-channel\nscaling to protect salient weights. QLoRA (Dettmers et al.,\n2023) introduces a memory-efficient fine-tuning approach\nby combining 4-bit quantization with LoRA.\nRather than modifying existing models, knowledge dis-\ntillation (KD) facilitates the transfer of knowledge from\nlarge and complex teachers to small and efficient students\n(Hinton, 2015). The students are trained to replicate the\nbehavior of the teachers. Yu et al. (2024) propose to distill\nSystem 2 reasoning processes\u2014such as Chain-of-Thought\nand System 2 Attention\u2014into a single-step System 1 model,"}, {"title": "3.2 Optimizing Computational Efficiency", "content": "Speculative decoding optimizes the inference process by\ndynamically adapting decoding strategies. Leviathan et al.\n(2023) introduce speculative decoding as a method that\nleverages more efficient approximation models to propose\ncandidate tokens, which are then verified by the target model\nin parallel. Similarly, Chen et al. (2023) propose speculative\nsampling, employing a draft model to generate multiple\ntoken candidates, which are then validated using a modified\nrejection sampling scheme. These methods underscore the\npotential of speculative execution to mitigate the inherent\ninefficiencies of autoregressive decoding, enabling faster\ninference without retraining or compromising output quality.\nKey-value cache is a pivotal strategy in autoregressive de-\ncoding, where intermediate states of attention mechanisms\nare stored to avoid recomputation in subsequent inference\nsteps. This technique significantly accelerates the generation\nof long sequences by leveraging stored key-value pairs from\nprevious layers. However, it introduces additional mem-\nory overhead, which must be carefully managed. Sparse\nattention mechanisms (Zhang et al., 2023c; Anagnostidis\net al., 2024; Liu et al., 2024c) use specialized sparsity pat-\nterns that prevent unnecessary token access. They use KV\ncache eviction and compression strategies to achieve sig-\nnificant improvements in latency, throughput, and memory\nsavings. Block-wise KV cache management (Kwon et al.,\n2023; Prabhu et al., 2024) adopts memory fragmentation\ntechniques inspired by paged memory systems, offering\nefficient runtime memory allocation and reallocation.\nMixture of Experts (MoE) introduced distributed special-\nization, enabling efficient scaling through task-specific sub-\nmodels controlled by a gating mechanism (Jacobs et al.,\n1991). Early dense MoE models suffered computational\ninefficiencies (Jordan & Jacobs, 1994). Sparse architectures\n(Shazeer et al., 2017) improved efficiency by selectively\nactivating relevant experts. Models like GShard (Lepikhin\net al., 2020), Switch Transformer (Fedus et al., 2022), and\nGLaM (Du et al., 2022) leveraged MoE for state-of-the-art\nperformance with reduced computation. Recent advances,\nincluding Mixtral (Jiang et al., 2024) and DeepSeekMoE\n(Dai et al., 2024a), further optimized efficiency."}, {"title": "3.3 Small Models for Large Impacts", "content": "Designing high-efficiency architectures is fundamental to\ndeveloping small-scale models. The most computationally\nintensive and memory-intensive component of Transformer-\nbased models is the Attention mechanism. Extensive re-\nsearch efforts have been devoted to enhancing the efficiency\nof Attention mechanisms. Notable advancements include\nFlash Attention (Dao et al., 2022), which is utilized by mod-\nels such as Phi-1.5 (Li et al., 2023) and DeepSeek-LLM (Bi\net al., 2024), Grouped Query Attention (Ainslie et al., 2023),\nwhich is utilized by MiniCPM (Hu et al., 2024), Mistral\n(Jiang et al., 2023), Phi-3 (Abdin et al., 2024), DeepSeek-\nLLM (Bi et al., 2024), and DeepSeek-V2 (Liu et al., 2024a),\nand Multi-Head Latent Attention, which was first introduced\nby DeepSeek-V2 (Liu et al., 2024a) and has been adopted\nin its successor, DeepSeek-V3 (Liu et al., 2024b).\nWhile such innovations enable the development of highly ef-\nficient small models, further improvements are necessary to\nbridge the performance gap between small and large-scale\nmodels. Key directions for achieving this include curating\nhigh-quality training data, designing scalable training strate-\ngies, and leveraging techniques such as mixture-of-experts\n(MoE), which allow for the selective activation of model\ncomponents to optimize efficiency and performance.\nHigh-Quality Training Data. The Phi family of models\n(Gunasekar et al., 2023; Li et al., 2023; Javaheripi et al.,\n2023; Abdin et al., 2024) highlights the importance of high-\nquality training data. Rather than relying on vast amounts of\nnoisy web-scraped text, these models are trained on curated,\nsynthetically generated textbook-style data, including struc-\ntured exercises and carefully filtered educational content.\nThis approach enhances efficiency and mitigates common\nissues such as hallucination and bias.\nScalable Training Strategies. Training efficiency is another\ncritical factor in developing compact yet powerful mod-\nels. Mini-CPM (Hu et al., 2024) introduces Model Wind\nTunnel Experiments (MWTE) to optimize hyperparame-\nter selection, ensuring that smaller models are trained in a\ncomputationally efficient manner. Additionally, it employs\nthe Warmup-Stable-Decay (WSD) learning rate scheduler,\nwhich segments training into distinct phases to maximize\nhardware utilization and improve convergence.\nMore Parameters but Less Activation. A crucial trend\nin optimizing smaller models for efficiency is the adoption\nof Mixture-of-Experts (MoE) architectures, where a sub-\nset of model parameters is activated per token, reducing\ncomputation while maintaining a large overall parameter\npool. Several recent models exemplify this technique: Mix-\ntral (Jiang et al., 2024) consists of 8 expert models with\na total of 56B parameters, while each token is processed\nby only 2 experts. Phi-3.5-MoE (Abdin et al., 2024) com-"}, {"title": "3.4 Future Trends", "content": "Core Functional Module Refinement. A promising di-\nrection for future research in Scaling Down models lies in\nrefining core functional modules. While existing methods\npredominantly emphasize the balance between efficiency\nand effectiveness, a critical gap remains in identifying the\nminimal functional module within large models. This mini-\nmal module would represent the smallest possible unit that\nretains all essential functionalities without compromising\nperformance. Future investigations may focus on developing\nsystematic approaches to detect and characterize such mod-\nules, potentially leveraging advancements in model pruning\nand knowledge distillation. Establishing rigorous criteria for\ndefining and verifying minimal functional modules could\nsignificantly contribute to optimizing model architectures\nwhile maintaining their operational integrity.\nExternal Assistance. Leveraging external assistance en-\nables small-scale core models to dynamically extend their\ncapacity to handle complex tasks. Retrieval-Augmented\nGeneration (RAG) (Lewis et al., 2020) is a method for ex-\nternal knowledge augmentation. RAG combines pre-trained\nparametric memory with non-parametric memory, which\nenables models to fetch contextually relevant information\ndynamically. Integrating external tools allows models to\nassign specialized operations to certain systems. Toolformer\n(Schick et al., 2023) can autonomously learn to invoke exter-\nnal APIs, such as calculators, search engines, and translation\nsystems. Beyond merely utilizing external tools, recent ad-\nvancements suggest that models can also generate tools to\nextend their own capabilities. VISPROG (Gupta & Kem-\nbhavi, 2023) can leverage in-context learning to produce\nmodular, Python-like programs and execute them for com-\nplex visual reasoning tasks."}, {"title": "4 Scaling Out: Advancing AI Ecosystems", "content": "Scaling Up and Scaling Down represent two complemen-\ntary approaches to AI scaling, yet neither fully realizes\nAl's potential in real-world applications. Scaling Up builds\nlarger, generalized models like GPT and BERT, but their\nresource demands limit accessibility and task-specific adapt-\nability. Scaling Down optimizes models for efficiency, en-\nabling deployment in resource-constrained environments,"}, {"title": "4.1 Scaling Out builds an AI Ecosystem", "content": "Scaling Out transforms isolated AI models into a diverse,\ninterconnected ecosystem by expanding foundation mod-\nels like LLaMA (Touvron et al., 2023) and Stable Dif-\nfusion (Rombach et al., 2022) into specialized variants\nequipped with structured interfaces. Foundation models\nprovide generalized intelligence, while specialized models,\nfine-tuned for tasks like legal contract analysis or medical\ndiagnosis, ensure domain-specific adaptability. For instance,\nControlNet (Zhang et al., 2023b) enables structured image\ngeneration by conditioning outputs on additional inputs,\ndemonstrating how foundation models can be adapted for\nspecific use cases.\nInterfaces bridge specialized models with users, applica-\ntions, and other AI systems. These range from simple APIs\nfor task-specific queries to intelligent agents capable of\nmulti-turn reasoning and decision-making. For example, the\nGPT Store hosts specialized GPTs, which are sub-models\nderived from the GPT Foundation Model that perform tasks\nlike coding assistance and creative writing. Similarly, Hug-\nging Face's ecosystem fine-tunes LLaMA variants for tasks\nsuch as sentiment analysis and summarization, showcasing\nhow Scaling Out extends AI's reach across domains.\nBy combining foundation models, specialized variants, and\nwell-designed interfaces, Scaling Out creates a dynamic\nAl ecosystem. This ecosystem fosters collaboration, en-\nables large-scale deployment, and continuously expands\nAl's capabilities, marking a shift toward open, scalable, and\ndomain-adaptive AI infrastructure."}, {"title": "4.2 Technical Foundations", "content": "Scaling Out relies on efficiently adapting foundation models\ninto specialized models for different tasks and domains.\nTraditional fine-tuning requires extensive computational\nresources, but Parameter-Efficient Fine-Tuning (PEFT)\ntechniques allow models to be adapted efficiently while pre-"}, {"title": "4.3 Future Trends", "content": "Blockchain. Just as App stores in Android/iOS provide di-\nverse applications, an AI model store will emerge, enabling\nusers to access, customize, and deploy specialized AI mod-\nels. For example, the recently launched foundation model\nDeepSeek-v3 (Liu et al., 2024b) has already surpassed 100\nvariations in just one month, demonstrating how founda-\ntional models can rapidly evolve into specialized versions."}, {"title": "5 Future Application Prospects", "content": "The true potential of AI Scaling lies in the future scenarios\nit can enable. This section explores two use cases that illus-\ntrate the transformative capabilities of AI scaling: human-AI\ncreative communities and smart manufacturing ecosystems.\n5.1 Human-AI Creative Communities\nContent creation platforms like TikTok, YouTube, and In-\nstagram showcase how AI scaling transforms creativity and\nengagement. Scaling Up integrates vast multimodal datasets,\nenabling foundation models to analyze trends, predict pref-\nerences, and optimize recommendations on a global scale.\nThese models, trained on billions of interactions, continu-\nously evolve to match audience demands. Scaling Down\nbrings AI closer to users, with lightweight models enabling\nreal-time video, music, and AR generation on personal de-\nvices. On-device AI also enhances content moderation,\nensuring platform safety without heavy computational costs.\nScaling Out redefines these platforms as AI-driven ecosys-\ntems where specialized AI agents actively participate along-\nside human users. These AI contributors focus on education,\nsports, music, and niche domains, generating and engaging\nwith content just as human creators do. For example, an\neducation AI produces real-time tutorials, while a sports\nAI provides live commentary. AI bots collaborate, such as"}, {"title": "5.2 Smart Manufacturing Ecosystems", "content": "Manufacturing ecosystems differ from traditional multi-\nagent systems due to their open, dynamic nature and massive\nscale, involving suppliers, manufacturers, and distributors as\nautonomous AI interfaces adapting to constant change. Scal-\ning Up builds foundational models that integrate vast, het-\nerogeneous datasets across sourcing, logistics, production,\nand consumer behavior, equipping agents with advanced\npredictive capabilities. Scaling Down tailors these global\nmodels into lightweight, task-specific AI, optimizing fac-\ntory operations, equipment monitoring, and localized supply\nchain decisions. Scaling Out expands the ecosystem's reach,\nenabling thousands of AI interfaces to collaborate and com-\npete, such as supplier interfaces negotiating contracts or\ndistributor interfaces optimizing delivery schedules. The\nsynergy between these scaling paradigms creates a self-\noptimizing, adaptive network, where AI continuously inte-\ngrates new entrants, eliminates inefficiencies, and responds\ndynamically to global challenges. It transforms manufactur-\ning into an intelligent, resilient ecosystem."}, {"title": "6 Challenges and Opportunities", "content": "Scaling Up, Down, and Out collectively offers both signifi-\ncant opportunities and notable challenges on the path toward\nAGI. This section explores these dual aspects, outlining key\nareas where transformative advancements can occur while\naddressing critical hurdles that must be overcome.\nCross-disciplinary research and collaboration. Al scaling\ndemands cross-disciplinary collaboration. Cognitive science\ncan inspire efficient model architectures, such as modular\ndesigns that selectively activate components based on input\ncomplexity (Laird et al., 2017). Integrating neuroscience,\nhardware engineering, and data science is key to achieving\nadaptive computation at scale. Advancements in hardware\nefficiency must align with AI scaling. Energy-efficient pro-\ncessors tailored for AI can reduce carbon footprints, while\nco-developing sparse computation chips enhances Scaling\nDown, enabling AI in resource-limited settings (James,\n2022). Data science defines metrics for AI scaling, estab-"}, {"title": "7 Alternative Views", "content": "While this position paper argues that Scaling Up encounters\nsignificant bottlenecks and that future trends will shift to-\nwards Scaling Down and Scaling Out, an alternative view\nis that Scaling Up remains a viable trajectory despite the\nchallenges. Supporters argue that addressing these chal-\nlenges is necessary and feasible through interdisciplinary\ninnovation. Key challenges must be overcome include data\nquality, computational demands, and energy consumption.\nFirstly, although data quantity has grown, their quality has\nnot kept pace. Synthetic data offers a controlled alternative\nbut may introduce biases and lack real-world applicabil-\nity. As for computational demands, computational power\nis a bottleneck, as traditional hardware faces physical and\neconomic limits. Alternatives like quantum, optical, and\nneuromorphic computing might help. Concern about energy\nconsumption emphasizes the need for sustainable AI. Low-\npower chips and renewable energy integration could reduce\nthe environmental impact of large-scale computing.\nUnlike Scaling Down and Scaling Out, which provide im-\nmediate solutions, Scaling Up requires extensive interdis-\nciplinary collaboration. This challenge extends beyond AI\nresearch to fields such as hardware engineering, quantum\nmechanics, and sustainable energy solutions. The long re-\nsearch and development cycles make Scaling Up a long-\nterm strategy. A concern regarding it is technological break-\nthroughs are unpredictable. Therefore, a balanced strategy\nis necessary, where both short- and long-term solutions are\ninvested, rather than exclusively focusing on one of them."}, {"title": "8 Conclusion", "content": "In this paper, we propose a framework for AI scaling, which\nis from Scaling Up to Scaling Down, then Scaling Out. Scal-\ning Up lays the groundwork with foundation models that\ngeneralize across tasks. Scaling Down ensures efficiency\nand accessibility, optimizing AI for diverse environments.\nFinally, Scaling Out provides multiple AI interfaces, which\nenables collaborative intelligence and interaction with users\nto tackle real-world challenges.. Together, these advance-"}]}