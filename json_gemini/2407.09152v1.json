{"title": "The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs", "authors": ["Anh Thu Maria Bui", "Saskia Felizitas Brech", "Natalie Hu\u00dffeldt", "Tobias Jennert", "Melanie Ullrich", "Timo Breuer", "Narjes Nikzad Khasmakhi", "Philipp Schaer"], "abstract": "Hallucination detection in Large Language Models (LLMs) is crucial for ensuring their reliability. This work presents our participation in the CLEF ELOQUENT HalluciGen shared task, where the goal is to develop evaluators for both generating and detecting hallucinated content. We explored the capabilities of four LLMs: Llama 3, Gemma, GPT-3.5 Turbo, and GPT-4, for this purpose. We also employed ensemble majority voting to incorporate all four models for the detection task. The results provide valuable insights into the strengths and weaknesses of these LLMs in handling hallucination generation and detection tasks.", "sections": [{"title": "1. Introduction", "content": "The prevalence of large language models advancements and groundbreaking results for many NLP research problems [1], tremendously changed how we generally approach everyday tasks but also how we approach larger more complex problems. The LLMs' ability to combine vast amounts of knowledge from different sources is unparalleled and for some tasks exceeds what humans are able to achieve.\nHowever, blind faith in the generated outputs of these models is critical as they may produce incorrect facts, also known as hallucinations. These false facts can be misleading and are one of the main barriers to using LLMs reliably and in a trustworthy way.\nTo this end, this work is part of our participation in the ELOQUENT Lab 2024 at CLEF. More specifically, we participate in the HalluciGen task that evaluates if the LLMs themselves are able to correctly detect hallucinations in both human- and machine-generated contexts [2]. The HalluciGen task is divided into two phases over two years. The first phase is dedicated to the builder task, while the second phase will focus on the breaker task. This study specifically targets the first phase where the goal is to create multilingual and monolingual hallucination-aware models. These models are designed to generate and detect 'hallucinated content' in two scenarios: machine translation and paraphrase generation. Figure 1 illustrates an overview of hallucination generation and detection tasks as described by the lab.\nThe remainder of this work is structured as follows. Section 2 describes our methodology in more detail. Section 3 details the implementation. Section 4 describes our results. Finally, Section 5 concludes our contributions."}, {"title": "2. Methodology", "content": "This section is divided into two parts: generation and hallucination detection tasks. Before delving into the details of our methodology, it is important to note that prior to receiving the dataset from the organizers, we began familiarizing ourselves with the overall task by applying the three models Falcon [3], MPT [4], and Llama 2 [5] to the hallucination detection task on the SHROOM dataset [6]. Since the results from these three techniques were unsatisfactory, we excluded them from our implementation for Eloquent Lab.\nThe models we applied to the Eloquent's dataset in generation and detection tasks were:\n\u2022 Meta-Llama/Meta-Llama-3-8B-Instruct [7, 8]\n\u2022 GPT-3.5 Turbo [9]\n\u2022 GPT-4 [10]\n\u2022 Google/GE\u041c\u041c\u0410-7\u0412-\u0406\u0422 [11, 12]\nWe leveraged a combination of open-source and closed-source models. This allows us to evaluate the quality of outputs across different models. Additionally, utilizing open-source models helped us optimize costs. Therefore, we initially experimented with various prompts for the tasks using open-source LLMs to identify the most effective ones. Then, we applied these optimized prompts to closed-source GPT models. Additionally, we did our best to enhance our prompting effectiveness using the guidance framework [13]."}, {"title": "2.1. Hallucination generation task", "content": "The task of hallucination generation is divided into two scenarios: machine translation and paraphrasing. The goal of the generation step is to take a source sentence and generate two LLM hypotheses: one that is a correct translation/paraphrase of the source and one that is a hallucinated translation/paraphrase of the source.\nFigure 2 indicates the overview of our approach for the generation task. To conduct this task, we took advantage of 'GPT-3.5 Turbo', 'GEMMA-7B-IT', and 'Llama 3'."}, {"title": "2.2. Hallucination detection task", "content": "The hallucination detection task is to present the LLM with a source sentence and two hypotheses (hyp1 and hyp2) and to determine which hypothesis is a hallucination and which is factually accurate. Our approach involved using four different LLMs, \u2018GPT-3.5 Turbo', \u2018Google/GEMMA-7B-IT', 'Llama 3', and 'GPT-4', as classifiers. Additionally, we employed a voting approach as a simple technique of ensemble learning [14] to combine the outputs of these four models."}, {"title": "3. Implementation", "content": "This part primarily focuses on how we prompted LLMs, along with the challenges and observations we encountered during the task. We divide this section into three parts: generation, detection, and cross-evaluation tasks."}, {"title": "3.1. Generation task", "content": "The generation task includes test sets for both paraphrasing and translation tasks."}, {"title": "3.1.1. Paraphrasing Generation Task", "content": "The paraphrasing generation task involved datasets in English and Swedish, comprising 118 samples for English and 76 samples for Swedish.\nThe performance of different models, including 'Gemma', \u2018GPT-3.5 Turbo', and 'Llama 3', was evaluated based on their ability to generate paraphrases for English and Swedish datasets. In the appendix in Section A, Figures 4 to 7 show comprehensive lists of all prompts used for the different models. The following demonstrates some of our observations regarding the implementation of the generation task:\n\u2022 The performance of the 'Gemma' model varied significantly based on the complexity of the prompts used. Simpler prompts yielded better results that highlight the importance of prompt design. Despite this, the model struggled with understanding specific instructions, such as 'generate hallucination'. Additionally, the generation speed was notably slow.\n\u2022 For 'GPT-3.5 Turbo', one prompt for English and one prompt for Swedish were employed. The generation speed of \u2018GPT-3.5 Turbo' was significantly faster compared to other models.\n\u2022 For 'Llama 3', a single prompt was used for both English and Swedish datasets. The speed of the model in generating Swedish responses was exceedingly slow. After seven hours, it only produced five outputs."}, {"title": "3.1.2. Translation Generation Tasks", "content": "In the appendix in Section A, in Figures 8 to 10, you will find a comprehensive list of all prompts used for the different models. The details of our implementation and observation of the translation generation task are as follows:\nIn our experimentation with 'Llama 3', we opted not to use the 'guidance' framework because of its ineffective performance. \u2018Llama 3' showed promising results for each language pair. We experimented with two different prompts, as shown in Figure 10 and observed instances where 'Llama 3' successfully generated hypotheses in the desired target language but struggled with the source language. Examples illustrating this phenomenon can be found in the Table 19.\nVarious prompts were tested, and the one that was chosen, as shown in Figure 9, showed effectiveness in generating the most automatic translations. However, \u2018GPT-3.5 Turbo' still struggled to instantly create translations (hyp- and hyp+) for all sources. The main issue was the variations in quotation marks which caused problems during the extraction process. As a result, we had to prompt some sentences individually (instead of being able to loop them as a group) so that the structure was recognized by GPT again. List 3.1.2 shows the number of samples had been done individually.\n\u2022 German to English: with 12 sources where 3 sources needed to be translated manually.\n\u2022 English to German: with 10 sources where 2 sources needed to be translated manually.\n\u2022 French to English: with 19 sources where 3 sources needed to be translated manually.\n\u2022 English to French: with 64 sources where 0 sources needed to be translated manually.\nTranslating from English was a smoother process for the Gemma model compared to translating to English."}, {"title": "3.2. Detection task", "content": "The detection task involves trial and test sets for both scenarios."}, {"title": "3.2.1. Paraphrasing Detection Task", "content": "Table 1 shows the number of samples for each trial and test set for the paraphrasing detection task. The trial dataset for the paraphrasing detection task is structured as follows:\n\u2022 id: Unique identifier of the example.\n\u2022 source: Original model input for paraphrase generation.\n\u2022 hyp1: First alternative paraphrase of the source.\n\u2022 hyp2: Second alternative paraphrase of the source.\n\u2022 label: hyp1 or hyp2, based on which of those has been annotated as hallucination.\n\u2022 type: Hallucination category assigned. Possible values include:\naddition\nnamed-entity\nnumber\nconversion\ndate\ntense\nnegation\ngender\npronoun\nantonym\nnatural\nWe compared the performance of different models on the trial dataset using distinct prompts. Some prompts used for the paraphrasing detection task on the trial dataset are presented in Figure 11. Additionally, Tables 20 to 32 illustrate the performance of various prompts on the trial dataset for both English and Swedish.\nA challenge with Gemma was its tendency to generate code within responses. We implemented a specific 'JSON' format to ensure retrievable output. Figure 12 indicates the example of generated output from Gemma. Figures 13 to 17 display the prompts employed in the paraphrasing detection task across various models for the test set."}, {"title": "3.2.2. Translation Detection Task", "content": "The following details are provided about the translation detection dataset.\n\u2022 Both trial and test datasets include data for four language pairs as follows:\nde-en: Source language: German, Target language: English\nen-de: Source language: English, Target language: German\nfr-en: Source language: French, Target language: English\nen-fr: Source language: English, Target language: French\n\u2022 The trial dataset included 10 data entries, with 5 entries featuring hallucination as hyp1 and the other 5 as hyp2. The structure of the trial dataset is illustrated below:\nid: Unique identifier of the example.\nlangpair: Language of source and hypotheses pair\nsource: Source Text\nhyp1: First alternative translation of the source.\nhyp2 Second alternative translation of the source.\ntype: Hallucination category assigned. Possible values include:\naddition\nnamed-entity\nnumber\nconversion\ndate\ntense\nnegation\ngender\npronoun\nantonym\nnatural\nlabel hyp1 or hyp2, based on which of those has been annotated as hallucination\n\u2022 In the test collection, there are 100 data samples for each language pair.\nThe structure of the test dataset is presented as follows:\nid: Unique identifier of the example.\nlangpair: Language of source and hypotheses pair\nsource: Source Text\nhyp1: First alternative translation of the source.\nhyp2 Second alternative translation of the source.\nOur implementation and observations of the translation detection task are delineated below, categorized according to each model.\nObservations for Llama 3 Ultimately, we experimented with 15 different prompts for the 'Llama 3' model. Among these, the prompt, as shown in Figure 18(a) yielded the most favorable results. Table 33 demonstrates the achieved results by using this prompt on the trial dataset. So, we opted for it for the final detection task.\nThe main observations for Llama 3 are:\n\u2022 'Llama 3' is not able to detect a label for every data entry (support is only 4 for each, hyp1 and hyp2). Figure 18 demonstrates the prompts used by 'Llama 3' on the test set.\n\u2022 When detecting the hallucination, 'Llama 3' gives explanations, such as: 'I chose hyp1 as the hallucination because it contains a date (December 5) that is not present in the source text. The source text only mentions the date August 5, but hyp1 provides a different date.' The first row in Table 34 shows this issue.\n\u2022 As both examples in Table 34 indicate 'Llama 3' exhibits gender bias. In the first example, it failed to recognize the feminine noun 'Wirtschaftspr\u00fcferin' shows a female auditor and labeled it as gender-neutral. It made a gender assumption in hyp1 which assumes a male auditor. Similarly, in the second example, \u2018Llama 3' struggled to understand the clear indication of a female secretary with the word \u2018Sekret\u00e4rin.'\n\u2022 'Llama 3' struggles with understanding and converting measurements and it could not recognize when different units are essentially the same. For instance, it sees 'kilometers' and thinks it is different from 'metres' which leads to mistakenly identifying that text as a hallucination. Additionally, \u2018Llama 3' makes the assumption that hyp2 is the hallucination because it contains 'kilometers' instead of 'km' and it fails to consider the fact that hyp1 also uses 'metres' instead of 'km.' Table 35 highlights this issue.\n\u2022 'Llama 3' struggles to recognize the different ways dates can be written. As shown in Table 36, it could not understand that '21. Januar' and 'Jan. 21st' refer to the same date.\n\u2022 In the end, we noticed that the prompt immensely influences the outcome of 'Llama 3'. When using different prompts, \u2018Llama 3' was either able to detect the gender, conversion, or the correct date, or it was not. For example, rowl in Table 37 shows that using the prompt shown in Figure 18(b), 'Llama 3' correctly explains that 'Wirtschaftspr\u00fcferin' refers to a female auditor in the first example, but then it mistakenly swaps hyp1 and hyp2. Additionally, as shown in the second row of this table, the new prompt allows \u2018Llama 3\u2019 to detect the correct gender indicated in the source text. However, 'Llama 3' still fails to assign the correct label. In the third row, we can see that 'Llama 3' correctly converts 65 km to 65,000 meters and identifies the hallucination in hyp2. Additionally, 'Llama 3' correctly identifies the wrong date in the last example. The primary issue with this prompt is that 'Llama 3' frequently fails to identify any hallucinations in certain data samples.\nObservations for GPT-3.5 Turbo and GPT-4 The approach used for \u2018GPT-3.5 Turbo' was replicated for 'GPT-4' to directly assess comprehension. Various prompts were tested, and two were selected based on the best results from previous trials.\nThe main observations for GPT-3.5 Turbo and GPT-4 are:\n\u2022 There were some samples where no hallucinations were detected. Table 38 displays the count of failed examples for \u2018GPT-4' and 'GPT-3.5 Turbo' in the translation detection task. Additionally, Table 39 lists some samples for which 'GPT-4' failed to assign labels with our explanation for each one.\n\u2022 Regarding the prompts for GPT models, both seem to encounter issues with misinterpretations or slightly inaccurate translations. Additionally, both struggle to identify incorrect pronouns.\n\u2022 Initially, during the phase with incorrect trial datasets, it was observed that 'GPT-3.5 Turbo' had difficulty recognizing hallucinations when names were slightly misspelled or had an extra letter appended.\nObservations for Gemma We tried various prompts, but Gemma showed better (80% Accuracy) in detecting the correct label when it was first asked to translate the hypothesis into the language of the source and then detect hallucinations. Figure 19 indicates the prompts used by Gemma on the test set.\nThe main observations for Gemma are:\n\u2022 The performance was significantly worse when the prompts were too scientific or contained too many technical terms.\n\u2022 Tricky samples for Gemma in the detection task include detecting the gender in comparison to the source (female/male), and identifying when numbers are incorrect, such as missing zeros.\nObservations for ensemble voting approach We opted for a straightforward voting approach to ensemble model predictions due to the limitations imposed by the small sample size of the trial set. This method ensured all models contributed equally.\nSince we compared an even number of models, there were instances where two models voted for hyp1 and the other two voted for hyp2. In these cases, we randomly selected the label."}, {"title": "3.3. Cross-evaluation task", "content": "The following provides detailed information regarding the cross-evaluation task. Table 2 presents information regarding the samples included in the paraphrasing task. The prompt showns in Figure 20 has been used for the english paraphrasing task.\nIn the translation task, sometimes none of the models detected any hallucinations in either hypothesis which results in some blank spaces in the CSV file due to the lack of predictions. Additionally, there were instances where no hallucinations were present because both hypotheses, hyp1 and hyp2, were the same."}, {"title": "4. Results", "content": "This part presents the results in detail for each task and scenario. It is worth noting that prior to showing the results from LLMs, Logistic Regression and Random Forest classifiers were used for an initial evaluation to establish a baseline performance for comparison with LLMs. Both LR and RF classifiers achieved similar performance with an F1-score of 0.5.\nFor the evaluation of the generation task, the lab employed a zero-shot text classification Natural language inference (NLI) model ('MoritzLaurer/bge \u2013 m3 \u2013 zeroshot \u2013 v2.0') to predict whether 'hyp+' is entailed within the source sentence and whether 'hyp-' contradicts the source sentence. They used only two labels: \u2018entailment' and 'not_entailment'. This approach helps us assess whether the systems can produce coherent hyp+/hyp- pairs. It is important to note that the performance of the classification model is not perfect, but it demonstrated reasonable performance on the detection test set across various languages and language pairs [2].\nFor evaluating both detection and cross-model tasks, the lab reported key metrics such as Accuracy, F1-score, Precision, and Recall for each model. Additionally, several baseline models were evaluated by the lab. For cross-model assessment, the lab also employed two metrics: Matthews Correlation Coefficient (MCC) and Cohen's Kappa.\nThe Average MCC ($\\overline{MCC}$) measures the quality of binary classifications by considering true and false positives and negatives, while the Standard Deviation of MCC ($\\sigma_{MCC}$) provides insight into the consistency of the model's performance. Similarly, the Average Kappa ($\\overline{\\kappa}$) measures inter-rater reliability for categorical items, and the Standard Deviation of Kappa ($\\sigma_{\\kappa}$) indicates the variability or consistency of the Kappa metric [15].\nTables 3 to 5 demonstrate the evaluation of detection, generation, and cross-model evaluation for English paraphrasing tasks.\nThe performance of detection across various models on the English paraphrasing task is presented in Table 3. The model \u2018GPT-4' with prompt 'En_Se_Para_Det_GPT3.5_GPT4_v2' achieved the highest performance with Accuracy, F1-score, Precision, and Recall scores of 0.91.\nTable 4 presents the results for the generation step. The model \u2018GPT-3.5 Turbo' with prompt 'En_Para_Gen_GPT3.5' achieved the highest performance in hyp+ entailment mean (0.964) and hyp+ correct label mean (0.983). Furthermore, The model 'Llama 3' with prompt 'En_Para_Gen_Llama3' showed strong performance in hyp- not entailment mean(0.978) and hyp- correct label mean (0.983).\nTable 5 presents the results for the cross model. The model \u2018GPT-4\u2019with prompt 'final_gpt4_en_v2_cross_model_detection' showed Accuracy, F1-score, Precision, and Recall scores of 0.93. In the next stage, the majority model with prompt 'majority_vote_cross_model_result_en' demonstrated impressive performance.\nTable 6 shows that the model with prompt 'majority_vote_cross_model_result_en' achieved the highest performance with an average MCC of 0.83 and average Kappa of 0.81.\nTable 7 presents the performance metrics of various models in the detection step for the Swedish paraphrasing task. The model \u2018GPT-4' with prompt 'En_Se_Para_Det_GPT3.5_GPT4_v1 (GPT4)' achieved an Accuracy score of 0.81 and with consistent scores across all metrics (F1 = 0.81, Precision = 0.81, Recall = 0.81). Additionally, the baseline 'baseline-bge-m3-zeroshot-v2.0/sv_bge-m3-zeroshot-v2.0' shows the highest Accuracy of 0.92 across all models.\nTable 8 summarizes the results of models in the generation step for Swedish paraphrasing where the focus is on metrics related to hypothesis entailment and not_entailment. The model 'GPT-3.5 Turbo' with prompt \u2018Se_Para_Gen_GPT3.5' demonstrated strong performance with high scores in hyp+ entailment mean of 0.88, hyp+ correct label mean of 0.90, hyp- contradiction mean of 0.91, and hyp- correct label mean of 0.93.\nTables 9 and 10 present cross-model evaluation results for the Swedish paraphrasing task that highlights the model performance across different evaluation criteria. The model majority voting with prompt 'majority_vote_cross_model_result_se' showed competitive performance. Table 10 provides statistical measures for models excluding baselines that indicate the noted majority voting technique has consistency and reliability.\nTables 11 to 14 report the performance of English-French translation detection, generation, and cross-model evaluation.\nTable 11 highlights several key points regarding the performance of the detection task. Model 'GPT-4' with prompt 'results_gpt4_en_fr' achieved the highest performance with Accuracy, F1-score, and Recall of 0.90, and Precision of 0.91. Additionally, we can observe that the majority voting model with prompt 'majority_vote_result_en_fr' also performed well with Accuracy, F1-score, and Recall of 0.83 and Precision of 0.86.\nOne of the conclusions can be drawn from Table 12 is that the baseline model 'baseline-general-prompt/en-fr.gen' showed a better performance with hyp+ entailment mean 0.90 and hyp+ correct label mean of 0.93, while it has a lower performance in hyp- contradiction mean of 0.10 and hyp- correct label mean of 0.08. Also, it is clear that model \u2018GPT-3.5 Turbo' with prompt 'results_gpt_en_fr' demonstrated a high performance in hyp- contradiction mean of 0.88, and hyp- correct label mean of 0.91.\nFrom tables 13 and 14 we have the finding that the majority voting approach with prompt 'majority_vote_result_en_fr' reached Accuracy 0.79, F1 score 0.78, Precision 0.80, and Recall 0.79. This combination exhibited the highest average MCC 0.66 and average Kappa 0.65.\nTables 15 to 18 report the results for the evaluation of the English-German translation detection, generation, and cross-model.\nThe important observation from the Table 15 is that the model 'GPT-4' along with prompt 'results_gpt4_en_de' showed the highest performance with an Accuracy, F1 score, and Recall all at 0.86 and Precision 0.89.\nFrom Table 16 we can see that the model \u2018GPT-3.5 Turbo' with the mixture of prompt 'results_gpt_en_de' exhibited better performance in hyp- contradiction mean of 0.83, and hyp- correct label mean of 0.84. 'Gemma' with prompt 'En_De_Trans_Gen_gamma' showed the best hyp+ correct label mean of 0.85. Additionally, 'baseline-phenomena-mentions-prompt/en-de.gen' provides a better hyp+ entailment mean of 0.84.\nTables 17 and 18 provide the insight that the model 'GPT-3.5 Turbo' with 'results_gpt_en_de' had the highest Accuracy of 0.76, F1 score of 0.75, Precision of 0.77, and Recall of 0.76. The prompt 'majority_vote_result_en_de' for majority voting had the highest average MCC of 0.60 and average Kappa of 0.58 which indicates strong inter-model agreement and consistency."}, {"title": "5. Conclusion", "content": "In conclusion, this study leveraged several LLMs to investigate both the generation and detection of hallucinations by LLMs themselves. The four distinct models employed presented their own unique evaluation challenges. We explored various prompt techniques including few-shot learning and chain of thought by using the guidance framework. Additionally, for the detection task, we tested an ensemble voting approach to combine the results from different LLMs. Although in this study we could achieve better results in comparison to the baseline models, our findings indicate that while some issues can be addressed through effective prompting, others remain difficult to mitigate solely by prompt engineering. Moreover, identifying the optimal prompt itself poses a significant challenge."}]}