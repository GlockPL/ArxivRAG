{"title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "authors": ["Simeng Han", "Aaron Yu", "Rui Shen", "Zhenting Qi", "Martin Riddell", "Wenfei Zhou", "Yujie Qiao", "Yilun Zhao", "Semih Yavuz", "Ye Liu", "Shafiq Joty", "Yingbo Zhou", "Caiming Xiong", "Dragomir Radev", "Rex Ying", "Arman Cohan"], "abstract": "Existing methods on understanding the capabilities of LLMs in logical reasoning rely on binary entailment classification or synthetically derived rationales, which are not sufficient for proper investigation of model's capabilities. We present P-FOLIO, a human-annotated dataset consisting of diverse and complex reasoning chains for a set of realistic logical reasoning stories also written by humans. P-FOLIO is collected with an annotation protocol that facilitates humans to annotate well-structured natural language proofs for first-order logic reasoning problems in a step-by-step manner. The number of reasoning steps in P-FOLIO span from 0 to 20. We further use P-FOLIO to evaluate and improve large-language-model (LLM) reasoning capabilities. We evaluate LLM reasoning capabilities at a fine granularity via single-step inference rule classification, with more diverse inference rules of more diverse and higher levels of complexities than previous works. Given that a single model-generated reasoning chain could take a completely different path than the human-annotated one, we sample multiple reasoning chains from a model and use pass@k metrics for evaluating the quality of model-generated reasoning chains. We show that human-written reasoning chains significantly boost the logical reasoning capabilities of LLMs via many-shot prompting and fine-tuning. Furthermore, fine-tuning Llama3-7B on P-FOLIO improves the model performance by 10% or more on three other out-of-domain logical reasoning datasets. We also conduct detailed analysis to show where most poweful LLMs fall short in reasoning. We will release the dataset and code publicly.", "sections": [{"title": "1 Introduction", "content": "A logical reasoning story consists of a series of premises and one or more conclusions (Han et al., 2022). The goal of logical reasoning is to determine the truth values of the conclusions based on the premises (Russell and Norvig, 2010). However, it cannot be determined whether a machine is indeed equipped with capabilities of logical reasoning even if it is able to output the correct truth value. Generating proofs or reasoning chains that support its decision is essential for explainability. Resolution-based symbolic proofs can be generated automatically with a first-order logic solver given explicit logic structures, but they are hard to read and cannot be written in natural language in a straightforward way in most cases (Russell and Norvig, 2010). Although studies on natural language proof generation for logical reasoning have been conducted before, they focus on synthetically generated logical reasoning datasets (Saparov et al., 2023; Saha et al., 2020; Tafjord et al., 2021; Saha et al., 2021). These logical reasoning chains contains much less natural language variation and challenging reasoning patterns than realistic logic stories (Han et al., 2022).\nWe present P-FOLIO, a new dataset consisting of human-written proofs for an existing popular dataset of logical reasoning, FOLIO (Han et al., 2022) which consists of realistic logical reasoning stories written by humans. P-FOLIO is collected with an annotation protocol for annotators to write proofs in a step-by-step manner without using proof by contradiction or proof by case. Table 2 shows an example story in FOLIO and Table 3 shows its human-written reasoning chains in P-FOLIO.\nP-FOLIO proofs are expert-written. They are more logically diverse and challenging than previous logical reasoning datasets equipped with reasoning chains. The number of proof steps in P-FOLIO spans from 0 to 20 and the proofs use diverse inference rules, containing 12 types of widely used and straightforward inference rules and another 20 types of complex inference rules. 26% of a total of 1430 proofs in P-FOLIO consist of more than five reasoning steps and 4% of proofs consist of more than 10 reasoning steps. We show the"}, {"title": "2 Related Work", "content": "Natural language reasoning Existing studies on natural language reasoning (Wei et al., 2022; Creswell et al., 2022; Bang et al., 2023; Prystawski et al., 2023; Yao et al., 2023) focus on using different prompting strategies on large language models (LLMs) for better results. Despite significant progress on the LLM prediction accuracy of the final answer, no work has been done to evaluate the correctness of the intermediate steps. In our work, we propose to evaluate intermediate steps at different levels of granularity in order to better probe into the reasoning capabilities of LLMs.\nImproving LLM Logical reasoning capabilities Luo et al. (2024); Ranaldi and Freitas (2024) fine-tuned language models with logical reasoning data to improve logical reasoning capabilities of LLMs. Large language models have also been directly used as soft logic reasoners and a variety of prompting techniques are proposed in order to improve their performance under this paradigm (Wei et al., 2022; Yao et al., 2023; Zhou et al., 2024). Using large language models as semantic parsers has also shown improvement on the reasoning performance (Olausson et al., 2023; Pan et al., 2023) where natural language reasoning problems are first parsed into logical forms before being fed into an inference engine to output the final answer."}, {"title": "Natural language proof generation", "content": "ProofWriter (Tafjord et al., 2021) and FLD (Morishita et al., 2023) are logical reasoning datasets equipped with natural language proofs, however both of them are synthetically generated dataset which neither contains abundant natural language variation nor encompasses challenging logical reasoning patters. Previous studies on proof generation focus on ProofWriter(Morishita et al., 2023; Saha et al., 2020, 2021; Yang et al., 2022) and ProntoQA (Saparov et al., 2023).\nLogicBench is a synthetically generated natural language QA dataset and is used for evaluating the logical reasoning ability of LLMs (Parmar et al., 2024). While FOLIO covers first-order logic and one or more inference rules are used in each example, LogicBench focuses on reasoning patterns covering propositional logic, first-order logic, and non-monotonic logic and focuses on the use of a single inference rule for each example.\nWe collect proofs for FOLIO (Han et al., 2022) instead, a realistic expert-written logical reasoning dataset. Such proofs need to be written from scratch and are hard and time-consuming to write because humans need to manage both the language and reasoning complexity in the proof-writing process and manually construct many steps of reasoning. The resulting proofs contain more diverse types of inference rules and reasoning patterns in addition to containing more natural language variation and ensured semantic richness."}, {"title": "3 P-FOLIO", "content": "In this section we describe the construction and properties of our dataset P-FOLIO ."}, {"title": "3.1 Inference Rules", "content": "We first define a set of inference rules that can be used for derivations of each proof step.\nWidely-used inference rules. The most widely used logical reasoning inference rules include universal instantiation, hypothetical syllogism, modus ponens, modus tollens, disjunctive syllogism, conjunction introduction, conjunction elimination, transposition, disjunction introduction, material implication and existential introduction.\nBoolean identities. During the pilot annotation process, we found that boolean identities are needed for certain derivations. For example, if"}, {"title": "3.2 Dataset Annotation", "content": "We offer an annotation protocol that facilitates annotators to annotate well-structured proofs for any logical reasoning problems in a step-by-step manner. Our annotators are selected based on the the following criteria: 1). They are college or graduate students who are either native English speakers or have near-native English proficiency. 2) They have a formal background in first-order logic, gained through either relevant coursework or self-directed study in first-order logic or semantic parsing. 3). we conducted in-person interviews with all the annotators to understand if they are motivated about completing the task before we added them into the annotator pool. All the annotators selected have expressed a keen interest in solving logical puzzles and are guaranteed to have a significant amount of time commitment. We also give the annotators several detailed tutorial sessions on how to write a proof and give detailed annotation guidelines to minimize possible ambiguities that can occur in the annotation process and make sure the annotation protocol is well-understood. A total of six annotators were selected through this process.\nAnnotation protocol. An annotator is presented with a logical reasoning problem consisting of a series of premises and one conclusion. For each step of a proof, we ask the annotator to write the indices of the premises used, a natural language derivation deduced from the premises used and the inference rule used for the derivation. The natural language derivation can then be used as a premise for future proof steps. We also specify that only 1 or 2 premises should be used in each step. Table 2 shows an example of the FOLIO dataset and Table 3 shows the entire proof annotated for the example. The proof consists of 14 steps in total with 14 natural language derivations and the final derivation is the conclusion. This annotation protocol facilitates humans to annotate well-structured proofs for first-order logic reasoning problems in a step-by-step manner and the resulting proofs are"}, {"title": "3.3 Dataset statistics", "content": "Number of proof steps. We show the distribution of the number of proof steps in the entire P-FOLIO dataset in Figure 2. we have carefully validated that each of the splits follows a similar distribution with less than a 2% difference in each split. The majority of written proofs contain five steps or less while written proofs comprised of 6-10 steps also take up a significant portion. There are 55 proofs which are comprised of 11 steps or more. This represents the portion of the dataset with the highest complexity.\nInference rule distribution. The widely-used inference rule and boolean identities distribution is shown in Figure 3. Among the widely-used inference rules, universal instantiation and hypothetical syllogism are the most common inference rules in P-FOLIO while disjunctive syllogism, conjunction introduction, conjunction elimination and transposition also appeared a large number of times. Although Modus ponens, Modus tollens, existential introduction, disjunction introduction, material implication and idempotent are the least common, their are 50 - 150 number of them in P-FOLIO. The occurrences of complex inference rules all range from 1 to 29. The complex inference rules are essential for making certain derivations although their occurrences are less frequent."}, {"title": "4 Reasoning Evaluation", "content": "We propose three tasks to evaluating LLM reasoning capabilities with P-FOLIO at different levels of granularities."}, {"title": "4.1 Single-step inference-rule classification", "content": "Given a single step of inference in a human-annotated proof which consists of the premises used $P = \\{P_1, P_2\\}$ and a derivation $d_1$, single-step inference-rule classification aims to identify which inference rule has been used to arrive at $d_1$ from $P$. This task aims to evaluate whether a model knows the type of inference required for a single inference step."}, {"title": "4.2 Single-step derivation reasoning", "content": "Given a single step of inference in a human annotated proof, the goal of single-step derivation reasoning is to ask the model to generate the truth value of a derivation based on the corresponding premises. The truth value label is always True."}, {"title": "4.3 Proof generation", "content": "Given a series of premises $P = \\{P_1,P_2,\\ldots,P_n\\}$ and a conclusion $c_1$, the goal of proof generation is ask the model to generate the proof and the final truth value. This task aims to evaluate the overall proof generation capability."}, {"title": "4.4 Proof generation evaluation", "content": "Pass@k. Considering that a model-generated reasoning chain can diverge significantly from a human-annotated one in logical reasoning, we sample multiple reasoning chains from GPT-3.5 and GPT-4 and employ pass@k metrics (Chen et al., 2021a) to assess the quality of these model-generated chains. GPT-4 is utilized to evaluate whether two reasoning chains follow a similar path to reach their conclusions. We provide the prompts used for this in the Appendix. If we sample k proofs from a model, we define pass@k to be the percentage of instances where at least one proof matches the human-written proof."}, {"title": "4.5 Task comparison", "content": "Single-step inference-rule classification and single-step derivation allow for a more granular assessment of how well the model can follow logical rules and derive conclusions by breaking down complex reasoning chains into individual steps. By analyzing each step independently, identify specific strengths and weaknesses can be identified in the model's reasoning processes, facilitating a better understanding of the model's overall performance and targeted improvements. The entire proof generation evaluates the reasoning capabilities of LLMs by challenging them with complex reasoning problems. This approach requires the model to generate a complete proof from start to finish, demonstrating its ability to handle intricate logical structures and multi-step derivations. By assessing the model's performance on entire proofs, we can gauge its proficiency in maintaining logical coherence, applying relevant rules, and effectively managing the complexity inherent in comprehensive reasoning tasks. This evaluation method provides a holistic view of the model's reasoning abilities."}, {"title": "5 Experimental Results", "content": "We conduct experiments using P-FOLIO. The train/dev/test split is 70%/15%/15%, which is the same as the split for FOLIO (Han et al., 2022). For all experiments involved with GPT-4 (OpenAI et al., 2023), we use gpt-4-0125-preview and the temperature was set to 0."}, {"title": "5.1 Single-step evaluation", "content": "We conduct single-step inference rule classification and single-step derivation reasoning on individual proof steps in P-FOLIO. For single-step derivation reasoning, we prompt GPT-4 to output the classification results in zero-shot setting and 5-shot prompting. The results of inference rule classification are shown in the first two columns of Table 5. Giving GPT-4 a few examples leads to better results for complex inference rules while the improvement is minimal for widely used inference rules. For single-step inference rule classification, we test two settings. We prompt GPT-4 to output the truth value directly in one setting. In the other setting, we prompt GPT-4 to output the explanation first before generating the final truth value. The results of single-step inference rule classification are shown in the last two columns of Table 5. Prompting GPT-4 to output the explanation before generating the final truth value leads to better results for both widely-used rules and complex rules."}, {"title": "5.2 Proof generation", "content": "Zero-shot Prompting LLM We use GPT-4 (OpenAI et al., 2023) to generate proofs and truth value for all the examples. By explicitly prompting GPT-4 to generate the reasoning process before the final answer increase the performance by 5% over prompting GPT-4 to generate only the final answer.\nWe manually inspect all the generated proofs. Among the examples with correct truth values, 10.7% exhibited incorrect reasoning processes. In these cases, the model either struggled to form the correct reasoning chain for complex problems involving many reasoning steps or used commonsense reasoning as a shortcut. Among the examples with incorrect truth values, 70% failed to form the correct reasoning chain for complex problems. Additionally, 10% contained wrong derivations at a certain steps in the reasoning process. About 15% of the examples presented conclusions with language or logical structures too intricate for the model to fully comprehend. Another 5% incorrectly applied commonsense reasoning as a shortcut, leading to errors in the final conclusions. We provide more analysis in section 6.\nMany-shot prompting LLMS We conduct many-shot prompting experiments with GPT-4 using 5, 10, 20, 40, and 60 examples in in-context learning. Sixty is the maximum number of example and proof pairs we can fit into the context"}, {"title": "6 Analysis", "content": "Most powerful LLMs fail on examples with 10 steps or more in human-written reasoning chains. Figure 1 illustrates the performance of GPT-4 on logical reasoning tasks of varying complexity. While GPT-4 accurately answers most questions requiring 1-5 human-written reasoning steps, its accuracy drops significantly for more complex tasks. For questions involving 6-10 reasoning steps, GPT-4 answers correctly only about half the time. Its performance further declines with examples requiring 11-15 steps, where it fails more than half the time. For tasks with 16 or more reasoning steps, GPT-4 is unable to provide correct answers at all.\nMaking new derivations with complex rules could be one of the bottlenecks for LLM reasoning To gain a more detailed understanding"}, {"title": "7 Conclusion", "content": "We presented P-FOLIO, a new dataset consisting of human-written proofs for FOLIO, a set of realistic logical reasoning stories written by humans. 26% of a total of 1430 proofs in P-FOLIO consist of more than five reasoning steps and 4% of proofs consist of more than 10 reasoning steps. We further use P-FOLIO to evaluate and improve large-language-model reasoning capabilities. We propose tasks with different levels of granularities and use various metrics to evaluate the performance of LLM reasoning capabilities. We show that human-written reasoning chains significantly boost the logical reasoning capabilities of LLMs via many-shot prompting and fine-tuning. Fine-"}, {"title": "8 Limitations", "content": "The abundant human-written reasoning chains in P-FOLIO can be used in many other ways that we have not explored. For instance, bootstrapping with abundant human-written reasoning chains could further improve the performance of bootstrapping with limited human-written rationales (Zelikman et al., 2022)."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Annotation Guidelines", "content": "We provide the detailed annotation guidelines in Table 12."}, {"title": "A.2 Instruction used in the GPT-4 prompt", "content": "Table 13 shows the instruction used in the GPT-4 prompt."}, {"title": "A.3 Prompt used for evaluating reasoning path", "content": "We provide an example in Table 14 including our prompt used and GPT-4 input and output for how GPT-4 is used to evaluate whether two reasoning chains follow a similar path to reach their conclusions."}, {"title": "A.4 Hyper-parameter settings", "content": "For fine-tuning Flan-T5-Large, we use a learning rate of 1e-4, batch size of 8, gradient accumulation of 2, warmup step of 100 and the model was trained for 3 epochs. For fine-tuning LLama-8b, we use a learning rate of 2e-5, batch size of 1 and gradient accumulation of 8. The warmup steps were set to 25 and the model was trained for 3 epochs."}]}