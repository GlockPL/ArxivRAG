{"title": "EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition", "authors": ["Youssef Doulfoukar", "Laurent Mertens", "Joost Vennekens"], "abstract": "Convolutional Neural Networks are particularly suited for image analysis tasks, such as Image Classification, Object Recognition or Image Segmentation. Like all Artificial Neural Networks, however, they are \"black box\" models, and suffer from poor explainability. This work is concerned with the specific downstream task of Emotion Recognition from images, and proposes a framework that combines CAM-based techniques with Object Detection on a corpus level to better understand on which image cues a particular model, in our case EmoNet, relies to assign a specific emotion to an image. We demonstrate that the model mostly focuses on human characteristics, but also explore the pronounced effect of specific image modifications.", "sections": [{"title": "Introduction", "content": "Thanks to recent progress, image analysis problems such as Object Detection using Artifical Neural Networks (ANN) can be more or less considered to be solved [8, 19]. However, higher-order tasks, such as identifying the emotion content of an entire image, remain more challenging. Convolutional Neural Network (CNN) models such as EmoNet [9] present a promising approach in this area, but its results are not yet completely convincing. This raises the question to which extent this network is actually picking up meaningful cues in the images, and to what extent it is learning spurious correlations that may be present in the private dataset on which it was trained.\nANNs are still considered \"black box\" models, and the domain that attempts to untangle how they make the predictions they make, i.e., to improve their explainability, is a very active one [2, 16, 18]. One of the techniques for this is Class Activation Maps [20], or CAM, which allows to highlight those parts of the image that contributed most to a model's (say, a CNN image classifier) output. This technique allows to visually inspect individual images or videos, but does not immediately allow for an automated global analysis on a corpus level."}, {"title": "Methodology", "content": "We start our analysis by applying, for a given CNN model M and corpus D, the following steps to each image I \u2208 D, schematically illustrated in Fig. 1.\nFirst, we process I with the object detection network of our choice, in casu, YOLOv3 [14] trained on the Open Images dataset. We opted for this particular pretrained network as other popular Object Detection dataset choices such as PASCAL VOC (20 classes) and MS-COCO (80 classes) are too restricted in the classes they propose. By contrast, Open Images, which contains 601 classes, presents a nice balance between human-related classes (e.g., \u201chuman face\u201d, \u201cmouth\u201d, etc.), and more general classes representing contextual elements (e.g., \u201ccar\u201d, \u201ctree\u201d, etc.). The result of this operation is a list of detected objects and their corresponding bounding boxes B. We filter the YOLOv3 output by keeping only bounding boxes with an IoU score > 0.005.\nSecond, we process I with M, and apply a CAM-based technique C to the last convolutional layer of M. This gives us an activation map that we overlay on top of I to obtain a new image A.\nFinally, we lay the bounding boxes B on top of A, and look for those boxes b\u2208 B for which the average CAM activation, or importance,  CAct > 0.3, with CAct defined as the sum of the CAM activations within the box divided by the area of the box. The threshold was heuristically determined by visually inspecting a limited set of images. We refer to these boxes as the set B*, and interpret these as those objects that most contributed to the model's decision.\nOnce we have found B* for every image in our corpus, we then analyse these data to find associations between object classes and output labels by constructing an association matrix MA, where aij \u2208 MA represents the number of images labeled with the jth EmoNet emotion label in which the ith object class has been detected at least once. By dividing each column j through by the total number of images labeled with the jth emotion such as to obtain percentages (after doing \u00d7100), we obtain M'A which allows to ignore imbalances in the prediction rates of the different emotions."}, {"title": "Results", "content": "We tested our proposed approach using the EmoNet model and the FindingEmo dataset. EmoNet is a model obtained through replacing the last layer of an AlexNet model pretrained on the ImageNet [4] corpus. This last layer was then trained on a private dataset of 137,482 images annotated for the emotion they evoke in the observer with one of 26 custom emotion labels. We use the Python port by L. Mertens [10] of the original Matlab release.\nFindingEmo is an image dataset consisting of 25,869 images annotated for, a.o., the dominant emotion in the picture, using one of the 24 emotion labels in Plutchik's Wheel of Emotions [13]. All images represent multiple people in various natural settings and with varying degrees of interaction among them.\nWe first present detailed results for Grad-CAM [17] in Sec. 3.1, and follow this up with an exploration of the effect of using other CAM-based methods in Sec. 3.2. Finally, we briefly explore the effect on the predicted label of artificially adding certain objects to images, attempting to answer the question whether the presence of certain objects can cause a specific label to be predicted."}, {"title": "Results for Grad-CAM", "content": "A heatmap depicting M'A as obtained using Grad-CAM together with EmoNet applied on the FindingEmo corpus can be found in Fig. 3. We limit ourselves to the 25 most prominent Open Images classes (as determined by the average of the corresponding row in MA). A clear conclusion to be drawn from this graph is that human features do indeed contribute the most to the decision making, most particularly the human face which, except for \"Clothing\", represents the most important class for each EmoNet label.\nAdditionally, some more specific associations do manifest themselves. Clear examples are the association between \"Sports equipment\" and \"Excitement\", and \"Food\" and \"Craving\", both of which seem logical. Less clear is, e.g., the association between \"Furniture\" and \"Interest\", or \"Plant\" and \"Surprise\", which hint of spurious associations resulting from biases in either or both the EmoNet training dataset and FindingEmo."}, {"title": "Comparison of CAM Methods", "content": "To answer the question to what extent different CAM methods yield different results, we performed a Representational Similarity Analysis [1] as follows. For each CAM method C\u2208 {Grad-CAM, Ablation-CAM [5], LIME [15], LRP [3], LIFT-CAM [7]}, we determine the association matrix Ma with all Open Images classes as described in Sec. 2, keeping the same emotion and class ordering for each C. We then flatten each matrix by concatenating all rows, turning it in to a 1D vector VMC. Finally, we construct a matrix R where each entry Rcc'\nrepresents the Spearman Correlation rank between VMC and VMC,. The resulting matrix is shown in Fig. 3. All related p-values were <<< 0.05, indicating statistical significance.\nThe consistently high correlation values between all pairs indicate that variations in results obtained through different CAM methods can be expected to be minimal. We did observe both LIFT-CAM and LRP resulting in a notable association between \"Pillow\" and \"Sexual Desire\". Other than this, the differences between the methods appear to lie within the relative strengths of the associations observed, rather than the assocations themselves."}, {"title": "Prediction Stability", "content": "To illustrate how the obtained knowledge can be applied to fool the network by creating an adversarial attack, consider the image shown in Fig. 4. We know from Sec. 3.1 that there is a high association between the object category \"Sports equipment\" and EmoNet label \"Excitement\". This inspired us to take an image labeled with high probabilty as \"Joy\" (92.9%; Excitement: 1.8%). After altering this image by pasting a rugby ball on top of the head of one of the two main subjects, the prediction changes to 66.1% \"Excitement\" (\"Joy\": 30.2%), demonstrating the dramatic effect the presence of a particular object can have on the model's output.\nNote that the position of the pasted object greatly influences the effect it has. Moving the rugby ball to the immediate right of the subject's face alters the predictions to 43.9% \"Joy\" and 42.1% \"Excitement\", while moving it to the"}, {"title": "Limitations and Future Work", "content": "Although the currently described approach already provides valuable insights, some limitations are to be noted.\nFirst, the approach is, by definition, heavily dependent on the choice of Object Detection network and its corresponding classes and performance. The upside is that, as a plug-and-play component, different Object Detection networks can be chosen for different tasks, allowing to pick object classes tailored to the task at hand.\nSecond, our current implementation does not take into account the size of the bounding boxes, which can result in suboptimal results. Consider, e.g., the example shown in Fig. 7. Although the subject's ear is clearly not the most important contributing element in the picture, because of the small size of the \"Human ear\" bounding box the average CAM activation is nonetheless the highest, spuriously pushing this object class to the top. Two main paths could be explored to counter this issue. The most straightforward would be to develop a scoring function that does take into account the bounding box size, or the activation distribution within it. Alternatively, segmentation models could be used"}, {"title": "Conclusion", "content": "We propose the novel EmoCAM approach to explaining CNN decisions, specifically with the downstream task of Emotion Recognition from images in mind. Our objective is threefold: 1) better understanding what parts of the input image the model uses to make its decision, 2) allowing to check whether or not the information used by the model aligns with expectations from a human perspective, and 3) uncovering potential model biases. We have demonstrated our approach using the EmoNet model, FindingEmo dataset and multiple CAM techniques.\nUsing our approach, we found that EmoNet indeed shows a strong focus on human elements, most notably (parts of) the human face, which is encouraging as it aligns with our understanding of human emotion recognition from Psychology. Nevertheless, we also found the model output to be quite unstable, in that adding specific objects (e.g., a rugby ball) to an image can dramatically alter its output and steer it towards a specific target emotion (e.g., \u201cExcitement\u201d)."}]}