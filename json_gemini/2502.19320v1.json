{"title": "SHH, DON'T SAY THAT! DOMAIN CERTIFICATION IN LLMS", "authors": ["Cornelius Emde", "Alasdair Paren", "Preetham Arvind", "Maxime Kayser", "Tom Rainforth", "Thomas Lukasiewicz", "Bernard Ghanem", "Philip H.S. Torr", "Adel Bibi"], "abstract": "Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.", "sections": [{"title": "INTRODUCTION", "content": "With recent advancements in the field of natural language processing, large language models (LLMs) have become ubiquitous. In particular, the scaling of recent large generalist models dubbed foundation models has shown to enable emergent abilities that benefit a wide range of downstream tasks such as text generation, question answering, and text comprehension (Kaplan et al., 2020; Alabdulmohsin et al., 2022; Xiong et al., 2024; Henighan et al., 2020; Brown et al., 2020). Adapting these foundation models for downstream tasks often leads to state-of-the-art performance and has become the dominant paradigm (Gao et al., 2021). This is typically achieved via fine-tuning on task-relevant data (e.g., low-rank adaptation (LoRA) Hu et al. (2022), in-context learning (Mosbach et al., 2023), prefix turning Li & Liang (2021), or simply prompt engineering).\nHowever, foundation models are typically trained on large amounts of web data which contains a wide range of information that is either irrelevant to a task or potentially harmful (Bommasani et al., 2022). Therefore, it is desirable to restrict the output of a generalist LLM to a specific domain. For example, consider a healthcare provider such as the National Health Services (NHS) providing a general purpose chatbot to support their citizens with simple health questions, as shown in Figure 1. It would be important, for public reputation and cost reasons, that such a system would remain on topic and could not be misused, either intentionally or unintentionally. Misappropriating models is easily possible.\nIn order to prevent intentional misuse, we consider an adversary trying to elicit an unintended (from the deployer's perspective) response from the model. We assume the deployer wants an LLM to only respond with a certain set of topics, and thus a successful attack is an input string that creates a coherent response outside the target domain. There are various reasons why an adversary might want to elicit such a response that is out-of-domain (OOD). The adversarial user might want to misappropriate the system as a cost-effective tool for a purpose it wasn't built for, resulting in excess"}, {"title": "DOMAIN CERTIFICATION", "content": "We now introduce our domain-certification framework for offering mathematical guarantees that an LLM system stays on topic. In Section 2.1, we formally introduce this framework. In Section 2.2, we present Verified Adversarial LLM Output via Iterative Dismissal (VALID). VALID is an easy-to-use method to create a system that adheres to these guarantees. In plain language, we propose a certifiable guardrail for LLM-driven systems as follows:\nA model is domain-certified, when an adversarial upper bound can be placed on the\nprobability that the model provides an output outside its designated target domain.\nBefore formalizing this statement, we introduce some mathematical notation. We represent tokens (i.e. individual text units) as $x$ and $y$, which belong to the token space $x, y \\in V$ where $V = \\{1,..., V\\}$ is the vocabulary of size $V$. We define the space of sequences of arbitrary length as $S \\subseteq V^*$, the Kleene closure of $V$. Sequences of tokens are denoted by bold letters, $\\mathbf{x}, \\mathbf{y} \\in S$, with $\\mathbf{x}$ and $\\mathbf{y}$ representing the input and output sequences of an LLM respectively. We use lowercase"}, {"title": "DEFINING DOMAIN CERTIFICATION", "content": "We now formally introduce domain certification. We define the target domain (set of desired topics) as a subset of the sentence space $S$ and partition $S$ into the target domain $T$ and its complement $T'$. For instance, $T$ might be all sentences meaningfully occurring for \u201cquestion answering for health problems\". In addition, we define the set of unwanted responses as $F \\subset T'$ ($F$ as \u201cforbidden\u201d) and will certify with respect to this set $F$ rather than $T$. Sequences posing some risk should be included in $F$, while $F'\\cap T'$ should contain benign out-of-domain samples, such as unintelligible or meaningless sequences of tokens (see Appendix B for a discussion). Hence, we wish to establish a guarantee that $L$ is unlikely to produce an output in $F$. As a step towards such a guarantee, we first define a bound for any given element $y$ in $S$:\nDefinition 1 Atomic Certificate. We say a model $L : S \\rightarrow S$ is $\\epsilon_y$-atomic-certified ($\\epsilon_y$-AC) for some sample $y$ (i.e. an atom) in the output set $S$, iff\n$$\\forall x \\in S: L(y|x) \\leq \\epsilon_y.$$\nIn words, a model that is $\\epsilon_y$-AC for a sample $y$, will generate sample $y$ with probability smaller than $\\epsilon_y$ for any $x \\in S$, and hence for adversarially chosen $x$. If this is the case, we say model $L$ is certifiable for sample $y$ with $\\epsilon_y$, i.e. $\\epsilon_y$ is the smallest value that provably bounds $L$. Ideally, such an upper bound $\\epsilon_y$ would be large for samples in the target domain $T$, meaning the certificate is permissive, and small for samples drawn from $F$ meaning the certificate is restrictive, i.e. tight.\nThe atomic certificate implies an upper bound $\\epsilon_F$ for $P_{y \\sim L(\\cdot | x)}(y \\in F | x)$, which would be constructed by summing (1) over all $y \\in F$ for a given $x$. Concretely, $P_{y \\sim L(\\cdot | x)}(y \\in F | x) = \\sum_{y \\in F} L(y | x) \\leq \\sum_{y \\in F} \\epsilon_y = |F| \\epsilon_F$. However, practically this bound is intractable due to $F$'s exponential size in $N_y$, and the difficulty in constructing a precise description of the set $F$. Instead of giving a bound over returning $y \\in F$, we look at the worst case across $F$ which can more precisely be estimated from a finite sample of $F$:\nDefinition 2 Domain Certificate. We say model $L$ is $\\epsilon$-domain-certified ($\\epsilon$-DC) with respect to $F$, when it is $\\epsilon_y$-AC for all $y \\in F$ with $\\epsilon_y \\leq \\epsilon$:\n$$\\forall x \\in S, y \\in F : L(y|x) \\leq \\epsilon.$$\nThis imposes a global bound on $L$ across all undesired responses in $F$. In practice, we cannot establish the $\\epsilon$-DC certificate w.r.t. $F$ as we cannot enumerate $F$. Hence, following standard practice in ML evaluation, we propose to use $D_F$, a finite dataset of out-of-domain responses to establish a $\\epsilon$-DC certificate w.r.t. $D_F$ approximating the certificate for $F$.\nRecent discussions have raised the need for bounds on undesirable behavior. For instance, Bengio (2024) advocates for upper bounds on harmful behavior (Bengio et al., 2024). In addition, a growing body of legislation mandates thorough auditing of ML systems (EU, 2024). The atomic and domain certificates can play a vital role in assessing the risk of worst-case behavior. For example, consider the deployer of an LLM-based system that processes 10 requests per second. The deployer might perform an apriori risk assessment and determine that they can tolerate the consequences of an out-of-domain response from a set $D_F$ sampled once per year. The deployer should certify the LLM system as $\\epsilon$-DC with $\\epsilon \\approx 10^{-9}$ in order to achieve this level of risk.\nCertification through Divergences. We provide an alternative view to this problem, generalizing it to bounding divergences between the model and the distribution of sentences in the domain $T$. We then use this view to operationalize the $\\epsilon_y$ AC and $\\epsilon$ DC (Definitions 1 and 2) inspired by Vyas et al. (2023)'s work on preventing copy-right violations. To this end, we define an oracle $\\Omega$ that is a generator for domain $T$: $\\Omega$ assigns high likelihood to sentences in $T$ and zero likelihood to elements in $F$. Hence, sampling from $\\Omega$ will yield in-domain responses. We establish and bound the"}, {"title": "ACHIEVING DOMAIN CERTIFICATION", "content": "In this section, we introduce Verified Adversarial LLM\nOutput via Iterative Dismissal (VALID) to obtain atomic\ncertification as described in Definition 1. We utilize a\ngeneral model $L$ and a domain generator $G$ as described\nabove and obtain a meta-model $M$ for which the guar-\nantee holds with respect to the domain generator $G$. In\nparticular, we perform rejection sampling as described in\nAlgorithm 1 (inspired by Vyas et al. (2023)): The capa-\nble general model $L$ proposes a sample $y$ and we accept,\nif the length normalized log-ratio between $L$ and $G$ is\nbounded by hyperparameter $k$. We repeat up to $T$ times\nuntil a sample is accepted. If all samples are rejected, the model dismisses the request. This defines\na new model $M$, for which the following theorem establishes the certificate:\nTheorem 1 (VALID Certificate) Let $L$ be an LLM and $G$ a guide model as described above. Rejec-\ntion sampling as described in Algorithm 1 with rejection threshold $k$ and up to $T$ iterations defines\nmodel $M_{L,G,k,T}$ with $M_{L,G,k,T}(y|x)$ denoting the likelihood of $y$ given $x$. Let $N_y$ be the length of\n$y$. We state the adversarial bound:\n$$\\forall x \\in S : M_{L,G,k,T}(y|x) \\leq 2^{kN_y} \\cdot T \\cdot G(y).$$\nHence, $M_{L,G,k,T}$ is $[2^{kN_y}TG(y)]$-AC and, further, it is $[\\max_{y \\in F} 2^{kN_y}TG(y)]$-DC w.r.t. $F$.\nWhen context allows, we may abbreviate $M_{L,G,k,T}$ to $M$, omitting subscripts for brevity. This\ncertificate with respect to $G$ can be useful: As $G$ is only trained on samples in $D_T \\subset T$, a dataset of\ndomain $T$, it assigns exponentially decreasing likelihood to samples that are in $F$.\u00b9 In particular, this\nis useful iff the log upper bound $kN_y + \\log T + \\log G(y)$ (log RHS of (4)) is small in comparison\nto $\\max_{x \\in S} \\log L(y|x)$: Our certificate can provide an upper bound to the adversarial behavior of $M$\nthat is favorable over $L$.\nAs mentioned, this problem is closely related to OOD detection, for which the likelihood ratio test\nis commonly used as a powerful statistic (Neyman & Pearson, 1933; Bishop, 1994; Ren et al., 2019;\nLi et al., 2023; Zhang et al., 2024; Rafailov et al., 2024). In OOD detection, rejection threshold $k$ is\ncommonly chosen to balance false negative rates and false positive rates. Here, $k$ also influences the\nupper bound on the certificate, indicating that there can be a trade-off between correctly classifying\nsamples as ID or OOD, and achieving a desired level of certification.\nLength Normalization. Algorithm 1 performs length normalized rejection-sampling as unnor-\nmalized log likelihood ratios scale unfavorably in $N_y$, the length of sequence $y$ which we now\ndemonstrate. Consider the next-token models $l$ and $g$ underlying the sequence-to-sequence mod-\nels $L$ and $G$. As $y$ is sampled from $L$, we expect each token $y_{1,\\ldots, N_y}$ to have high like-\nlihood under $l$. If we assume that $l$ places $c$ times more probability mass per token than $g$,"}, {"title": "EXPERIMENTS", "content": "We empirically test our method proposed in Section 2.2 across 3 domains: Shakespeare, Computer\nScience News, and MedicalQA. After describing the experimental setup in Section 3.1, we examine\nthe rejection behavior of our method by examining the $\\log \\frac{L(y|x)}{G(y)}$ ratio and associated cer-\ntificates under a finite set of ground-truth test samples from $T$ and $F$ in Section 3.2. In Section 3.3,\nwe repeat this analysis by applying our Algorithm 1. Finally, we demonstrate how to evaluate a\ncertified model on standardized benchmarks in Section 3.4."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we provide a brief description of our experimental setup for three applications. Each\nexperimental setup consists of a target domain $T$, a finite dataset of in-domain samples $D_T \\subset T$,\nmodels $L$ and $G$, and an out-of-domain dataset $D_F \\subset F$, against which we test our methods (see\nAppendix D for more details on data and models).\nShakespeare. Our target domain $T$ is Shakespeare's plays. We fine-tune a Gemma-2-2b (Team\net al., 2024) as model $L$ and train a GPT-2 architecture (33.7M parameters, Radford et al. (2019))\nfrom scratch for $G$ on TinyShakespeare (TS) (Karpathy, 2015). We use TS's test split as in-domain\ndataset, $D_T$, and following previous literature (Zhang et al., 2024) compose $D_F$ of IMDB (Maas\net al., 2011), RTE (Wang et al., 2019) and SST2 (Minaee et al., 2024), adding an old Bible dataset\n(Reis, 2019) as it is linguistically close to TinyShakespeare. At testing, we consider 256-token long\nsequences and use the first 128 tokens as prompt.\nComputer Science News. Our target domain $T$ is news about computer science. We fine-tune\na Gemma-2-2b as model $L$ and train a GPT-2 architecture (109.3M parameters) from scratch for\n$G$ on articles from the computer science categories in the 20NG dataset (Lang, 1995). We use\ncomputer science articles from 20NG's test split as target domain $D_T$ and the remaining categories\nas $D_F$ together with the OOD dataset used for Shakespeare. At testing, we consider 256 token long\nsequences and use the first 128 tokens as prompt."}, {"title": "LIKELIHOOD RATIOS ON GROUND TRUTH SAMPLES", "content": "In this section, we evaluate the capability of our method to attribute samples to the target domain and\ninvestigate whether it yields useful adversarial bounds. In particular, we study the length-normalized\nlikelihood ratio $\\frac{L(y|x)}{G(y)}$ on in- and out-of-domain samples. In Figure 3a, we show that the log\nlikelihood ratios for MedicalQA are disentangled and hence a threshold $k$ exists separating target\ndomain and out-of-domain samples well. However, such $k$  \u2013\nwhile yielding strong OOD detection\nperformance \u2013 might not be associated with tight certificates. Hence, we will first study the $\\epsilon_y$-AC\ncertificates under $M$ for individual samples, $y$, before moving on to the domain certificate, $\\epsilon$-DC.\nAtomic Certificates. We obtain $\\epsilon_y$-ACs using VALID (Section 2.2), setting $k$ to achieve a 10%\nfalse rejection rate (FRR) for in-domain samples. Figures 4 (a)-(c) show the distribution of $\\epsilon_y$-ACs\nfor the target domain dataset $D_T$ and the out-of-domain dataset $D_F$. We make similar observations\nfor all three setups: First, the certificates in the OOD datasets $D_F$ are meaningfully tight. We ob-\nserve that 95% of OOD samples have an $\\epsilon_y$-AC of less than 1 \u00d7 10\u207b\u00b9\u2070 across all setups. Hence,\nthe sampling probability for these OOD instances is provably smaller than 10\u207b\u00b9\u2070 for any arbitrary\nprompt $x$. Second, we note that the certificates in $D_F$ are significantly tighter than those in $D_T$ as\nshown by the gap between the eCDFs. This is a significant finding as certificates should be con-\nstrictive (i.e. small) on samples in $F$ preventing these from being sampled, while certificates should\nbe permissive (i.e. large) in $T$, not preventing in-domain responses from being sampled. Finally,\nwe observe that the disentanglement of ACs is weaker for MedicalQA compared to the other setups\n(see Figure 4c). As shown in Appendix E.6, this is attributable to the short sequences in the OOD\ndataset and adjusting for this confounder significantly improves disentanglement.\nTo further study the atomic certificates on $M$, we compare them to a certificate on $L$ as a baseline.\nTo this end, we define the constriction ratio for each $y$, given by the ratio of the certifiable $\\epsilon_y$ for $L$,\n$\\epsilon_y (L)$, over the certifiable $\\epsilon_y$ for $M$, $\\epsilon_y (M)$:\n$$CR_k = \\frac{\\epsilon_y (L)}{\\epsilon_y (M)}$$\nA CRk of 1 for sample y indicates that the bounds on generating y are equal for M and L (i.e.\nthey are equally constricted) while a CRk > 1 indicates that M is more constricting than L, and\nvice-versa. Smaller ACs for samples in F are better and hence a large CRk indicates that model\nM is favorable over L. To our knowledge, only vacuous certificates for a general model L exist\n(e.g. L is 1-DC). Hence, we approximate it from below using the likelihood $L(y|x)$ under non-\nadversarial $x$ taken from the datasets. Concretely, we use $L(y|x)$ as a crude approximation of\n$\\max_{x \\in S} L(y|x)$. This overestimates the robustness of L and underestimates the constriction ratio,"}, {"title": "GENERATING RESPONSES", "content": "In the section above, we evaluate $M$ obtained through VALID on prompts and responses, taken\nfrom datasets $D_T$ and $D_F$ representing our target domain $T$ and $F$. The experiments provide us with\na detailed analysis of ACs and DCs on a large variety of samples for which their membership to $T$ or\n$F$ is given by high-quality labels. Nonetheless, in practice, the candidate responses that are judged\nby VALID are generated by $L$. Hence, we prompt $M$ using $x \\in D_T$ and $x \\in D_F$ and use responses\ngenerated by $L$ as VALID proposes. We focus on VALID with $T = 1$ and the MedicalQA setup.\nOur findings are in line with Section 3.2 showing a strong ability to distinguish between in- and out-\nof-domain samples while providing meaningful adversarial bounds. In Figure 5b, we demonstrate\nthe separation of samples from $D_T$ and $D_F$, as well as the dependence of the log ratios on the length\nof the sequence $y$ extending the theoretical analysis from Section 2.2. In Appendix E.4, we replicate\nFigure 3 for this setting. We further present in Figure 5a the constriction ratios on out-of-distribution\nsamples generated by $L$. We see a clear indication that the constriction is strong out-of-domain with\nan optimal classification performance at a ratio of 10\u2074\u2070. To reiterate, median ratio between $L(y|x)$\nand the $\\epsilon_y$-AC for $M$ is 10\u2074\u2070 showing just how strict VALID is on the out-of-domain dataset.\nBuilding on these results, we test VALID with $T > 1$. Increasing $T$ can naturally increase the accep-\ntance rate on in-domain samples (through repeatedly proposing candidates) at the cost of increasing\nthe $\\epsilon_y$ linearly (see (4)). We find great improvements in the acceptance rate on in-domain samples\nwith minimal losses on the $\\epsilon$-DC tightness. We explore this in Appendix F."}, {"title": "CERTIFIED BENCHMARKING", "content": "We extend the analysis of false rejection rates (FRRs) by evaluating model $M$'s performance on\nstandardized benchmarks, while ensuring it is certified at $\\epsilon$. In particular, for our MedicalQA setup,\nwe evaluate the model performance on the PubMedQA benchmark (Jin et al., 2019)."}, {"title": "RELATED WORK", "content": "LLM Guardrails. A large body of work has been published on establishing effective guardrails\nfor LLMs. These approaches are designed to restrict the model to responses that align with the\ndeployer's values. One of the first approaches was Reinforcement Learning with Human Feedback\n(RLHF) (Askell et al., 2021), which uses human preferences to guide LLM training. Extensions such\nas Safe-RLHF add cost models to penalize harmful behavior, ensuring a balance between helpful-\nness and harmlessness during optimization (Dai et al., 2024). RLHF's foundation in reinforcement\nlearning has given rise to techniques such as Proximal Policy Optimization (PPO) (Bai et al., 2022),\nthe more recent Direct Preference Optimization (DPO) (Rafailov et al., 2024), and Generalized Pol-\nicy Optimization (GPO) (Tang et al., 2024), which incorporates diverse optimization objectives,\nuseful for safety-critical scenarios. For an in-depth survey of this area, we direct the reader to Kauf-\nmann et al. (2024). Unlike the preceding approaches that fine-tune guardrails into the parameters\nof an LLM, a number of works have proposed using LLMs to classify content as either safe or\nunsafe. Llama Guard categorizes the inputs and outputs of an LLM into different unsafe content\ncategories (Inan et al., 2023). Conversely, Chua et al. (2024) classify if an output is safe with respect\nto a system prompt. For a complete overview on LLM guardrails, we direct the interested reader\nto a recent survey of this area, Dong et al. (2024). Existing LLM guardrail techniques have been\nproven effective to different levels. However, these guardrails only come with empirical evidence\nof their proficiency against existing attacks, and hence, many have been circumvented shortly after\ndeployment. Conversely, VALID offers a provable high-probability guarantee against undesirable\nbehavior, reflecting recent advocacy for such provable assurances (Bengio, 2024).\nOut-of-Distribution Detection. Out-of-distribution (OOD) detection has received a lot of at-\ntention in recent years in NLP. Commonly, the problem is treated as text classification and softmax\nprobabilities of class predictions (Hendrycks & Gimpel, 2017) or energy scores (Liu et al., 2020) are\ndeployed as discriminant scores. Another group of methods employs distance-based methods, rely-\ning on OOD responses being distant from ID responses in latent space, often utilizing Mahalanobis\ndistance and sometimes incorporating contrastive learning techniques (Uppaal et al., 2023; Podol-\nskiy et al., 2021; Zhou et al., 2021; Khosla et al., 2020; Lin & Gu, 2023). Finally, rooted in classical\nstatistics, a number of studies suggest using the log-likelihood ratio (LLR) as a discriminate score,\ncomparing likelihoods from ID and OOD proxy models (Gangal et al., 2020; Zhang et al., 2024).\nXu & Ding (2024) offer a comprehensive review of LLMs for OOD detection. While many of these\nworks have strong empirical detection results, their focus is OOD detection rather than certification,\nand hence they do not provide theoretical guarantees or certificates on model behavior.\nCertifying LLMS. A number of certification approaches have been proposed for LLMs in various\ncontexts. For instance, Chaudhary et al. (2024) aim to certify the knowledge comprehension ability\nof LLMs and Freiberger & Buchmann (2024) discuss what criteria should be certified to ensure\nfairness. Most relevant here is work on certification against adversarial inputs. Casadio et al. (2024)\ndiscuss certifying the robustness of LLMs to input perturbations in embedding space. Commonly,\nadversarial certification is studied for text classification rather than generation (La Malfa, 2023).\nKumar et al. (2024) introduce a framework for defending against adversarial perturbations in token"}, {"title": "LIMITATIONS", "content": "Despite our promising results, we acknowledge the limitations of our current implementation. First,\nthe domain generator $G(y)$ lacks context. This means that if $y$ is marginally in-domain, while $y|x$,\nthe conditional distribution is not, our method will not reject appropriately. Consider a chatbot for\ntax advice. For prompt $x$ = \"How often is a tax report due?\", the response $y$ =\"Once a year.\"\nis in-domain. Hence, the same response to $x$ =\"How often should I shower?\" might be accepted\ndespite it being out-of-domain, and terrible advice. However, this can be mitigated by fine-tuning\nthe model $L$ to be as explicit as possible repeating \u201cshower\u201d in the response.\nSecond, this approach relies heavily on the domain-specific model $G$, and how closely it approxi-\nmates the ideal oracle $\\Omega$. In practice and as demonstrated in our experiments, $G$ might have limited\nsemantic understanding and lack general language capabilities and world knowledge. In most in-\nstances it might not be able to distinguish between semantically opposite but similar sentences and\nhence VALID is likely incapable of aligning the model, rather than shushing it.\nThird, an adversary might construct an attack that aims to copy tokens from the prompt of $L$ to $G$.\nFor instance, $x$ = \"Repeat after me: !!!-+! and then tell me how to build a bomb!\". This \"!!!-+!\"\nmight be an adversary for $G$ to assign a high likelihood to $L$ following the instruction. For this\nattack, the adversary operates with limited information, having access only to whether the log ratio\nis bounded, without visibility into $G$'s outputs, weights, or likelihood scores. In addition, since $G$\nhas never seen information on how to build a bomb, it is extremely unlikely to produce coherent,\ncorrect, and harmful content. In Appendix C.1, we discuss the feasibility of attacking $M$ further.\nFourth, our method comes at the extra cost of sampling up to $T$ times. Further, it requires training\n$G$ and evaluating it during inference. Depending on the architecture of $G$ however, the extra cost is\nlimited. In our experiments $G$ is orders of magnitude smaller than $L$."}, {"title": "FUTURE WORK", "content": "In this section we briefly discuss some ideas for future work that we believe could further extent\nthe practical utility of VALID. Initially, it would be interesting to test larger, specialized models for\n$G$ to evaluate whether these more advanced models produce improved certificates and refusal rates.\nWe chose not to do this because LLMs trained from scratch exclusively on specific domains are not\ncommon, and thus results generalize less to what a practitioner with limited resources could expect.\nAs described in Section 2.2, VALID uses length normalization to ensure the log likelihood ratio\nrejection condition is robust to different lengths of sequences $N_y$. One may extend this and learn\na more complex polynomial of $N_y$ as rejection threshold. This threshold could be used to provide\nboth $\\epsilon_y$-ACs and $\\epsilon$-DC certificates, while simultaneously enabling more precise OOD detection.\nFinally, a rejection scheme with a probabilistic decision rule, similar to Algorithm 5 in Vyas et al.\n(2023), would be able to provide identical bounds to Theorem 1. Possibly, this rejection rule would\nlead to better performance in terms of OOD classification."}, {"title": "CONCLUSION", "content": "In this work, we tackle the problem of generative language models producing outputs outside their\ntarget domain in response to adversarial inputs. We describe the associated risks, introduce a first-of-its-kind framework for domain certification for LLMs, and provide VALID, a simple algorithm\nrelying on well-established theories from statistics and information theory to provide such guaran-\ntees. We demonstrate the effectiveness of VALID in multiple representative settings and show that\nit is effective even when relying on a guide model $G$ with limited language skills, making it easy to\ndeploy in limited data and resource environments."}, {"title": "PROOFS", "content": "Theorem 1 (VALID Certificate) Let $L$ be an LLM and $G$ a guide model as described above. Rejec-\ntion sampling as described in Algorithm 1 with rejection threshold $k$ and up to $T$ iterations defines\nmodel $M_{L", "bound": "n$\\forall x \\in S : M_{L", "2^{kN_y}TG(y)": ""}, {"2^{kN_y}TG(y)": "", "F$.\nProof": "We abbreviate $M_{L", "t": "P(A'_i | A'_{<i"}, ".", "_{<t}, x)$ is non-stochastic and is equal to either 0 or 1. In the former case, the $M(y|x)$\nis trivially bounded by any non-negative number. The latter case (i.e. $y$ is accepted in iteration $t$)\nimplies that $\\log \\frac{L(y|x)}{G(y)} < k N_y$. Rearranging terms and noting that by definition $P(S_t | A", {"L": "S \u2192 S$ be an LLM returning $y$ given\n$x$ as discussed above and let \u03a9 be a distribution over domain $T$", "X": "L(y|x) || \u03a9(y)) \u2264 k$", "0-DC.\nProof": "We"}]}