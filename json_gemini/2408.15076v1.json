{"title": "MiWaves Reinforcement Learning Algorithm", "authors": ["Susobhan Ghosh", "Yongyi Guo", "Pei-Yao Hung", "Lara Coughlin", "Erin Bonar", "Inbal Nahum-Shani", "Maureen Walton", "Susan Murphy"], "abstract": "The escalating prevalence of cannabis use poses a significant public health challenge globally. In the U.S., cannabis use is more prevalent among emerging adults (EAs) (ages 18-25) than any other age group, with legalization in the multiple states contributing to a public perception that cannabis is less risky than in prior decades. To address this growing concern, we developed MiWaves, a reinforcement learning (RL) algorithm designed to optimize the delivery of personalized intervention prompts to reduce cannabis use among EAs. MiWaves leverages domain expertise and prior data to tailor the likelihood of delivery of intervention messages. This paper presents a comprehensive overview of the algorithm's design, including key decisions and experimental outcomes. The finalized MiWaves RL algorithm was deployed in a clinical trial from March to May 2024.", "sections": [{"title": "Introduction", "content": "The escalating prevalence of cannabis use poses a significant public health challenge globally. In the U.S., cannabis use is more prevalent among emerging adults (EAs) (ages 18-25) than any other age group [1], with legalization in the multiple states contributing to a public perception that cannabis is less risky than in prior decades [2]. Although not all cannabis users experience harm, early onset of use is associated with significant physical and mental health consequences. To address this growing concern, we developed MiWaves, a reinforcement learning (RL) algorithm designed to optimize the delivery of personalized intervention prompts to reduce cannabis use among EAs. MiWaves leverages domain expertise and prior data to tailor the likelihood of delivery of intervention messages.\nThis paper documents the development of the reinforcement learning (RL) algorithm for Mi-Waves. Reinforcement learning (RL) [3] is an area of machine learning where algorithms learn to select a sequence of decisions in order to maximize an outcome. In MiWaves, the RL algorithm optimizes decisions regarding the delivery of engagement prompts to participants with to goal of maximizing their app engagement. The finalized MiWaves RL algorithm was deployed in a clinical trial (called the MiWaves clinical trial or the MiWaves pilot study) from March to May 2024."}, {"title": "1.1 Formulating the RL problem", "content": "To formulate the RL problem, we first define a state, action, and reward. For each of the compo-nents, we use subscript i to denote the participant and subscript t to denote the decision point. The state $S_i(t)$ for participant i at decision point t describes the current context of the participant. We define algorithm state features as relevant features used by the algorithm that provide a summary of state $S_i(t)$. For example, state features could be the participant's recently reported cannabis use, or the time of day. The action $a_i(t)$ is the decision made by the RL algorithm for participant i at decision point t. A policy is a function that takes in input state $S_i(t)$ and outputs action $a_i(t)$. For MiWaves, the action is a binary decision of whether to deliver an engagement prompt ($a_i(t)=1$) or not ($a_i(t)=0$). The policy first calculates the probability $\\pi_i(t)$ of selecting $a_i(t)=1$ and then uses $\\pi_i(t)$ to sample $a_i(t)$ from a Bernoulli distribution. That is, $a_i(t)$ is micro-randomized at time t with probability $\\pi_i(t)$. For MiWaves, the policy is initially warm-started by using the prior distribution (Section 5.1) to compute $\\pi_i(t)$. The reward $R_i(t+1)$ is a function of the proximal outcome, designed to reward the algorithm for selecting good actions for participants. In MiWaves, the reward is a function of the participant app engagement (proximal outcome). See Section 1.3 for a detailed description of the states, actions and reward for MiWaves.\nWe now describe how the RL algorithm makes decisions and learns throughout the study. The MiWaves RL algorithm is an online RL algorithm which interacts with participants, observing par-ticipant context, taking an action, and receiving some feedback or reward for its action in that given context. It continues to incorporate this feedback into its policy to learn about participants and improve the decision of which action to take in each context. Online RL generally has two major components: (i) the online learning algorithm (section 2); and (ii) the action-selection procedure (section 3). In MiWaves, at a given decision point t, the RL algorithm observes participant i's observed state $s_i(t)$ and selects action $a_i(t)$ with probability $\\pi_i(t)$ utilizing its action-selection proce-dure. After the RL algorithm selects $a_i(t)$, the algorithm then observes the reward $R_i(t+1)$ from the participant. At update times, the RL algorithm updates the model of the participant environment using a history of states, actions, and rewards up to the most recent decision point, by utilizing it's online learning algorithm."}, {"title": "1.2 Code", "content": "The code for the RL algorithm, along with the associated API service can be found here."}, {"title": "1.3 Reinforcement Learning Framework", "content": "This section provides a brief overview of the Reinforcement Learning (RL) [3] framework used in this work with respect to the MiWaves study.\n\u2022 Actions: Binary action space, i.e. $A = {0,1}$ - to not send a MiWaves message (0) or to send a MiWaves message (1).\n\u2022 Decision points: $T = 60$ decision points per participant. The trial is set to run for 30 days, and each day is supposed to have 2 decision points per day. Therefore, we expect to have 60 decision points per participant.\n\u2022 Reward: Let us denote the reward for participant i at decision time t by $R_i(t+1)$. One of the assumptions underpinning the MiWaves study is that higher intervention engagement (self-monitoring activities and using suggestions by MiWaves to manage mood, identify alternative activities, plan goals, etc.) will lead to lower cannabis use. Thus the reward for the RL algorithm is based on a combination of check-in completion and self-reports of use of MiWaves message suggestions. This reward is defined as follows:\n0: did not open the app and did not complete the check-in (no app interaction)\n1: opened the app and browsed it for more than 10 seconds outside of the check-in, but did not complete the check-in.\n2: completed the check-in, and responded no to the Activity question (whether the participant thought about or used any of the ideas from the MiWaves messages sent by the app)\n3: completed the check-in, and responded yes to the Activity question (whether the participant thought about or used any of the ideas from the MiWaves messages sent by the app)\n\u2022 States: Let us denote the state observation of the participant i at decision time t as $s_i(t)$. A given state $S = {S_1, S_2, S_3}$ is defined as a 3-tuple of the following binary variables (omitting the participant and time index for brevity):\n$S_1$: Recent intervention engagement - average of the past X rewards ($X = 3$). Categorized to be low (0) vs high (1). The binary state $S_1$ is then computed as follows:\n$S_1 = \\begin{cases} 1 & \\text{if } avg_X(R) \\geq 2 \\\\ 0 & \\text{otherwise} \\end{cases}$   (1)\nAt decision points t = 1 and t = 2, we take the average of the rewards observed upto that decision point.\n$S_2$: Time of day of the decision point - Morning (0) vs. Evening (1).\n$S_3$: Recent cannabis use (CU) reported in the check-in in the last Y decision points ($Y = 1$) (Used or not used). The binary state $S_3$ is then assigned as follows:\n$S_3 = \\begin{cases} 0 & \\text{if } avg_Y(CU) > 0 \\\\ 1 & \\text{otherwise} \\end{cases}$   (2)"}, {"title": "2 Online Learning Algorithm", "content": "This section details the online learning algorithm - specifically the algorithm's reward approximating function and its model update procedure."}, {"title": "2.1 Reward Approximating Function", "content": "One of the key components of the online learning algorithm is its reward approximation function, through which it models the participant's reward. Recall that the reward function is the conditional mean of the reward given state and action. We chose a Bayesian Mixed Linear Model to model the reward. Linear models are well studied, and also easily interpretable by domain experts, allowing them to critique the model. Further, mixed models allow the RL algorithm to adaptively pool and learn across participants while simultaneously personalizing actions for each participant.\nFor a given participant i at decision time t, the RL algorithm receives the reward $R_i(t+1)$ after taking action $a_i(t)$ in the participant's current state $s_i(t)$. Then, the reward model is written as:\n$R_i(t+1) = g(S_i(t))^T \\alpha_i + (a_i(t) - \\pi_i(t)) f(S_i(t))^T \\beta_i + \\epsilon_i(t)$   (3)\nwhere $\\epsilon_i(t)$ is the noise, assumed to be gaussian i.e. $\\epsilon \\sim N(0, \\sigma^2 I_{t \\times m_t})$, and $m_t$ is the total number of participants who have been or are currently part of the study at time t. Also $\\alpha_i, \\beta_i$, and $\\gamma_i$ are weights that the algorithm wants to learn. g(S) and f(S) are functions of the algorithm state features (defined in section 1.3), defined as:\n$g(S) = [1, S_1, S_2, S_3, S_1S_2, S_2S_3, S_1S_3, S_1S_2S_3]$   (4)\n$f(S) = [1, S_1, S_2, S_3, S_1S_2, S_2S_3, S_1S_3, S_1S_2S_3]$   (5)\nPlease refer to Section 5.3 for the justification behind the design choice of these functions.\nTo enhance robustness to misspecification of the baseline, $g(S_i(t))^T \\alpha_i$, we utilize action-centering [4] to learn an over-parameterized version of the above reward model:\n$R_i(t+1) = g(S_i(t))^T \\alpha_i + (a_i(t) - \\pi_i(t)) f(s_i(t))^T \\beta_i + (\\pi_i(t)) f(S_i(t))^T \\gamma_i + \\epsilon_i(t)$   (6)\nwhere $\\pi_i(t)$ is the probability of taking action $a_i(t) = 1$ in state $S_i(t)$ for participant i at decision time t. We refer to the term $g(s_i(t))^T \\alpha_i$ as the baseline, and $f(s_i(t))^T \\beta_i$ as the advantage (i.e. the advantage of taking action 1 over action 0).\nWe re-write the reward model as follows:\n$R_i(t+1) = \\Phi_i^T \\theta_i + \\epsilon_i(t)$   (7)"}, {"title": "2.2 Online model update procedure", "content": ""}, {"title": "2.2.1 Posterior update", "content": "We vectorize the parameters across the $m_t$ participants who have been or are currently part of the study at time t, and re-write the model as:\n$R = \\Phi \\theta + \\epsilon$   (9)\n$R = \\begin{bmatrix} R_1 \\\\ R_2 \\\\ : \\\\ R_{m_t} \\end{bmatrix}$  = $\\begin{bmatrix} R_i(1) \\\\ R_i(2) \\\\ : \\\\ R_i(t+1) \\end{bmatrix}$  (10)\n$\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ : \\\\ \\theta_{m_t} \\end{bmatrix}$ = $\\begin{bmatrix} \\theta_{pop} + u_1 \\\\ \\theta_{pop} + u_2 \\\\ : \\\\ \\theta_{pop} + u_{m_t} \\end{bmatrix}$ = $1_{m_t} \\otimes \\theta_{pop} + u$   (11)\n$\\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ : \\\\ \\epsilon_{m_t} \\end{bmatrix}$ = $\\begin{bmatrix} \\epsilon_i(1) \\\\ \\epsilon_i(2) \\\\ : \\\\ \\epsilon_i(t) \\end{bmatrix}$   (12)\n$u_i \\sim N(0, \\Sigma_u)$   (13)\n$\\epsilon \\sim N(0, \\sigma^2 I_{tm_t})$   (14)\nAs specified before, we assume a gaussian prior on the population level term $\\theta_{pop} \\sim N(\\mu_{prior}, \\Sigma_{prior})$. The hyper-parameters of the above model, given the definition above, are the noise variance $\\sigma^2$ and the random effects variance $\\Sigma_u$. Now, at a given decision point t, using estimated values of the hyper-parameters ($\\sigma^2_{,t}$ is the estimate of $\\sigma^2$ and $\\Sigma_{u,t}$ is the estimate of $\\Sigma_u$), the posterior mean"}, {"title": "2.2.2 Hyper-parameter update", "content": "The hyper-parameters in the algorithm's reward model are the noise variance $\\sigma^2$ and random effects variance $\\Sigma_u$. In order to update these variance estimates at the end of decision time t, we use Empirical Bayes [5] to maximize the marginal likelihood of observed rewards, marginalized over the parameters $\\theta$. So, in order to form $\\Sigma_{u,t}$ and $\\sigma^2_t$, we solve the following optimization problem:\n$\\Sigma_{u,t}, \\sigma^2_{e,t} = argmax_{\\sigma^2_{e,t}} l(\\Sigma_u; H_{1:m_t})$   (21)\nwhere, $H_{1:m_t} = \\{ (S_i(1), a_i(1), R_i(2),..., S_i(t), a_i(t), R_i(t+1))\\}_{i \\in [m_t]}$ refers to the trajectories (history of state-action-reward tuples) from timer = 1 to $\\tau$ = t for all participants $i \\in [m_t]$, and\n$l(\\Sigma_u; H_{1:m_t}) = log(det(X)) - log(det(X + yA)) + m_t log(y) - y \\sum_{i \\in [m_t]} \\sum_{\\tau \\in [t]} (R_i(\\tau+1))^2$"}, {"title": "3 Action Selection Procedure", "content": "The action selection procedure utilizes a modified posterior sampling algorithm called the smooth posterior sampling algorithm. Recall from Section 2.1, our model for the reward is a Bayesian linear mixed model with action centering given as:\n$R_i(t+1) = g(S_i(t))^T \\alpha_i + (a_i(t) - \\pi_i(t)) f(s_i(t))^T \\beta_i + (\\pi_i(t)) f(S_i(t))^T \\gamma_i + \\epsilon_i(t)$   (23)\nwhere $\\pi_i(t)$ is the probability that the RL algorithm selects action $a_i(t) = 1$ in state $S_i(t)$ for participant i at decision point t. The RL algorithm computes the probability $\\pi_i(t)$ as follows:\n$\\pi_i(t) = E_{\\beta \\sim N(\\mu_{post, i}(t-1), \\Sigma_{post, i}(t-1))} [p(f(s_i(t))^T \\beta)|H_{1:m_t-1}, S_i(t)]$   (24)\nwhere $H_{1:m_t-1} = \\{ (S_i(1), a_i(1), R_i(2), ..., S_i(t-1), a_i(t-1), R_i(t))\\}_{i \\in [m_{t-1}]}$ refers to the trajectories (history of state action reward tuples) from time $\\tau$ = 1 tor = t-1 for all participants $i \\in [m_{t-1}]$. Notice that the last expectation above is only over the draw of $\\beta$ from the posterior distribution parameterized by $\\mu_{post, i}(t-1)$ and $\\Sigma_{post, i}(t-1)$ (see eq. (15) and eq. (16) for their definitions).\nIn classical posterior sampling, the posterior sampling algorithm uses an indicator function (Figure 1a):\n$p(x) = I(x > 0)$   (25)\nIf the indicator function above is used, the posterior sampling algorithm sets randomization prob-abilities to the posterior probability that the treatment effect is positive. However, in order to facilitate after-study analysis while using a pooled algorithm, past works have shown that it is"}, {"title": "4 Prior data and MiWaves Simulation Testbed", "content": "This section details how we transform prior data to construct a dataset, and utilize the dataset to develop the MiWaves simulation testbed. The testbed is used to develop and evaluate the design of the RL algorithm for the MiWaves study. The base simulator or the vanilla testbed is constructed using the SARA [10] study dataset. The SARA dataset consists of N = 70 participants, and the SARA study was for 30 days, 1 decision point per day. For each participant, the dataset contains their daily and weekly survey responses about substance use, along with daily app interactivity logs and notification logs. We will now detail the procedure to construct this base simulator."}, {"title": "4.1 SARA vs MiWaves", "content": "The Substance Use Research Assistant (SARA) [10] study trialed an mHealth app aimed at sustain-ing engagement of substance use data collection from participants. Since the SARA study focused on a similar demographic of emerging adults (ages 18-25) as the MiWaves study, we utilized the data gathered from the SARA study to construct the simulation testbed. We highlight the key differences between the SARA study and the MiWaves study in Table 1."}, {"title": "4.2 Data Extraction", "content": "First, we will detail the steps to extract the relevant data from the SARA dataset:\n1. App Usage: The SARA dataset has a detailed log of the participant's app activity for each day in the trial, since their first login. We calculate the time spent by each participant between a normal entry and an app paused log entry, until 12 AM midnight, to determine the amount of time (in seconds) spent by the participant in the app on a given day. To determine the time"}, {"title": "4.3 Data Cleaning", "content": "Next, we will specify the steps to clean this data, and deal with outliers:\n1. Participants with insufficient data: We remove participants who had more than 20 un-determined (i.e. either \u201cUnknown\u201d or \u201cNot sure\") cannabis use entries. Upon removing"}, {"title": "4.4 Reward computation", "content": "Next, we calculate the reward for each data point in the dataset. It is calculated as follows:"}, {"title": "4.5 Dataset for training participant models", "content": "We use the post-4PM data (i.e. data from 4 PM to 12 AM midnight) (dubbed as the evening data) to create a dataset to train the individual participant models for the MiWaves simulation testbed. The dataset contains the following features:\n\u2022 Day In Study \u2208 [1,30]\n\u2022 Cannabis Use \u2208 [0,2.5g]\n\u2022 (Evening) App usage \u2208 [0, 700s]\n\u2022 Survey completion (binary)\n\u2022 Weekend indicator (binary)\n\u2022 Action (binary)\n\u2022 Reward \u2208 {0,1,2,3}\nWe detail the steps to create this evening dataset:\n1. Evening cannabis-use: The SARA study documented cannabis use for a given participant for a whole day. In contrast, MiWaves will be asking participants to self-report their cannabis use twice a day. To mimic the same, after discussions among the scientific team, we split a given participant's daily cannabis use from SARA into morning and evening use. In particular we multiply the total day's use by a factor of 0.67 to generate their evening cannabis use. Also, the MiWaves study will be recruiting participants who typically use cannabis at least 3 times a week. We expect their use to be much higher than that of participants in SARA. So, we multiply the evening cannabis use again by a factor of 1.5. Thus, we generate the evening cannabis use from the participant's daily use reported in SARA as follows:\n$Evening CB Use = \\text{That Day's SARA CB Use} \\times 0.67 \\times 1.5$   (27)\n2. Feature normalization: The resulting dataset's features are then normalized as follows:\n\u2022 Day in study is normalized into a range of [-1,1] as follows\n$Day \\text{ in study (normalized)} = \\frac{Day \\text{ in study} - 15.5}{14.5}$   (28)\n\u2022 App usage is normalized into a range of [-1,1] as follows\n$App \\text{ usage (normalized)} = \\frac{App \\text{ usage} - 350}{350}$   (29)"}, {"title": "4.6 Training Participant Models", "content": "As specified above in Sec. 4.5, we use the evening dataset to train our participant models for the MiWaves simulation testbed. This dataset has the following features:\n\u2022 Day In Study \u2208 [-1,1]: Negative values refer to the first half of the study, while positive values refer to the second half of the study. A value of 0 means that the participant is in the middle of the study. -1 means that the participant has just begun the study, while 1 means they are at the end of the 30 day study.\n\u2022 Cannabis Use \u2208 [-1,1]: Negative values refer to the participant's cannabis use being lower than the population's average cannabis use value, while positive values refer to participant's cannabis use being higher than the study population's average cannabis use value. A value of 0 means that the participant's cannabis use is the average value of cannabis use in the study population. Meanwhile, -1 means that the participant is not using cannabis, and 1 means that the participant used the highest amount of cannabis reported by the study population.\n\u2022 (Evening) App usage \u2208 [-1,1]: Negative values refer to the participant's app use being lower than the population's average app use value, while positive values refer to participant's app use being higher than the study population's average app use value. A value of 0 means that the participant's app usage is the average amount of app usage observed in the study population. Meanwhile, -1 means that the participant's app usage is non-existent (i.e. zero). On the other hand, 1 means that the participant's app usage is the highest among the observed app usage values in the study population.\n\u2022 Survey completion \u2208 {0,1}: A value of 0 refers to the case where the participant has not finished the decision point's check-in, while a value of 1 refers to the case where the participant has responded to the decision point's check-in.\n\u2022 Weekend indicator \u2208 {0,1}: A value of 0 refers to the case where the decision point falls on a weekday, while a value of 1 refers to the case where the decision point falls on a weekend.\n\u2022 Action \u2208 {0,1}: A value of 0 means that action was not taken (i.e. no notification or intervention message was shown), while 1 means that an action was taken (i.e. a notification or intervention message was shown).\n\u2022 Reward \u2208 {0,1,2,3}: Same as defined in Section 4.4\nWe fit the reward using our participant models. Before we train participant models, we do a complete-case analysis on the evening dataset. It involves removing all the participant-evening data points which have any missing feature. This can either be a missing \u201ccannabis use\u201d value, or a missing \u201caction\u201d (which is the case on the first day of the study). To that end, 435 participant-evening data points are removed out of a total of 1260 participant-evening data points. Given that our target variable i.e. the reward is categorical (0-3) in nature, we consider two options for our generative participant models:"}, {"title": "4.7 Dataset for generative process", "content": "Next, we will create a dataset for the generative process for a simulation. To that end, we impute missing values in the evening dataset, and also create the morning dataset. We describe the procedure for both below.\n\u2022 Imputation in evening dataset: As stated before, the vanilla evening dataset has a few missing values for \"cannabis use\" and \"action\" values. We impute the values for \"cannabis use\" as follows - for a given missing value, we first determine the day of the week, and then replace the unknown/missing value with the average of the cannabis use across the available data of the participant for that day of the week.\nNote that during the simulation, the action used by the participant models for reward gen-eration will be determined by the RL algorithm in the simulation. Hence, we do not need to impute \"action\" here.\n\u2022 Generating morning dataset: Similar to the evening dataset, we generate a morning dataset per participant to mimic the data we would receive from MiWaves (2 decision points a day). We generate the following features:\nCannabis Use: We generate the morning cannabis use as follows:\nMorning CB Use = That Day's SARA CB Use \u00d7 0.33 \u00d7 1.5   (34)\nApp Usage: Since there are less than 30 evening app usage values per participant in the SARA dataset, we decide to use these values as an empirical distribution, and resample, with replacement, from the 30 values for each participant at each morning. The sampled value is the participant's morning app usage.\nSurvey Completion: For each participant, we determine the proportion of times they responded to the daily survey during the SARA study. Using this ratio as our probability of survey completion, we sample from a binomial distribution to construct the Survey Completion feature for the morning dataset for each participant for each day.\nDay In Study and Weekend indicator: We create one morning data point per day, so the day in study and weekend indicator features mimic the evening dataset.\nWe combine both the morning and evening dataset, and we call it the combined dataset. We will now describe the procedure to generate data during simulations using this dataset."}, {"title": "4.8 Simulations: Data generation", "content": "In this section, we detail our data generation process for a simulation. We assume that any given participant i, her trajectory in an RL simulation environment with T total decision points has the following structure: $H_i^T = \\{ S_i(1), a_i(1), R_i(2),..., S_i(T), a_i(T), R_i(T+1) \\}$.\nWe also assume that the combined dataset has the following structure - for any given decision point t (t \u2208 [1,60]), the state, $s_i(t)$, is constructed (partially) based on: the participant's app usage from t - 1 to t, the survey completion indicator (whether participant fills the survey after receiving action at t) for decision point t, and the cannabis use of the participant from t-1 to t. The data at decision point t from the combined dataset is used to generate $R_i(t+1)$, which in turn helps generate features to form $S_i(t+1)$ (refer Section 1.3)."}, {"title": "4.9 Environment Variants", "content": "We will now enumerate the different environment variants we have planned for our simulator, and how we go about operationalizing them. Since we know that the MiWaves study is planned to recruit around 120 participants, all our simulation environments will have 120 participants. We sample these participants with replacement from the N = 42 participants from the combined dataset during the data generation process.\n1. Varying size of the treatment effect: We construct three variants wrt the size of the treatment effect:\n\u2022 Minimal treatment effect size: We keep the treatment effect i.e. advantage coeffi-cients in the MLR model that we learn from the participants in the SARA data.\nFor the other two variants, we modify the advantage weights for each of the MLR participant models. Specifically, we only modify the advantage intercept weights. We find the minimum advantage intercept weight across all classes learnt using the SARA participant data, and if it is not assigned to class 0 we swap the weight with that of class 0. Then we set the advantage intercept weight of class 2 and class 3 to be the average of both. The reason we do so, is the fact that we believe that taking an action will always have a non-negative treatment effect. To that end, taking an action will always result in higher probabilities for generating a non-zero reward, and lower probabilities for generating a zero reward. Setting class weights by assigning the minimum weight to reward class 0 helps us achieve that. Also, since we trained the participant models from SARA data where we assigned 2 and 3 reward with equal probability wherever reward of 2 was assigned, we set their corresponding advantage intercept weights to be equal. Hence, we set them to be the average of the observed weights of the two reward classes. Moreover, we decide to work with the participant's learnt weights from SARA to preserve heterogeneity among participants.\nWe then multiply the advantage intercept weights for all the classes by a factor, which we refer to as the multiplier. We select the multipliers for our other variants after observing the standardized effect sizes. To compute the standardized effect size, we first generate a dataset of the N = 42 participants' trajectories (from SARA) for each multiplier, by sampling actions with probability 0.5. We generate 500 datasets (i.e. 500 simulations with N = 42 participants in each simulation) for each multiplier based environment. For each dataset (i.e each simulated trial) of N = 42 participants, we fit a linear model with fixed effects and robust standard errors. We utilize a linear model with fixed effects to compute the standardized effect size since the RL algorithm's reward approximation model (with random effects) has the same form in expectation. The model is specified as follows:\n$E[R_i(t+1) | S_1, S_1, S_2, S_3, a] = a_0 + a_1S_1 + a_2S_2 + a_3S_3 + a_4S_1S_2 + a_5S_1S_3 + a_6S_2S_3 + a_7S_1S_2S_3 + a (\\beta_0 + \\beta_1S_1 + \\beta_2S_2 + \\beta_3S_3 + \\beta_4S_1S_2 + \\beta_5S_1S_3 + \\beta_6S_2S_3 + \\beta_7S_1S_2S_3)$.   (38)\nTo obtain the standardized effect size, we take the average value of the advantage terms and divide it by the sample standard deviation of observed rewards (i.e. for that simulated trial) of N = 42 participants. We do so for all the different multipliers we consider, and the results are summarized in Fig 6. The standardized action intercept represents the increase in the"}, {"title": "5 RL Algorithm Design", "content": "This section details the RL algorithm design decisions with respect to the MiWaves study. These involve determining the priors (section 5.1), empirically verifying the prior values (section 5.1.7), designing algorithm variants (section 5.2) to benchmark and finalize hyper-parameters, and de-scribing the RL algorithm's action selection strategy (section 5.2.3). The algorithm variants are benchmarked using the MiWaves simulation testbed (described in section 4). The results along with the final algorithm are described in section 5.3."}, {"title": "5.1 Initial values and Priors", "content": "This section details how we calculate the initial values and priors using SARA data (see section 4) to estimate the reward", "brevity)": "n$E[R_i(t+1) | S_1, S_2, S_3, a", "a": "has 16 dimensions. We follow the methods described in [12"}]}