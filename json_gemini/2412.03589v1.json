{"title": "Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models", "authors": ["Valentina Anita Carriero", "Antonia Azzini", "Ilaria Baroni", "Mario Scrocca", "Irene Celino"], "abstract": "Procedural Knowledge is the know-how expressed in the form of sequences of steps needed to perform some tasks. Procedures are usually described by means of natural language texts, such as recipes or maintenance manuals, possibly spread across different documents and systems, and their interpretation and subsequent execution is often left to the reader. Representing such procedures in a Knowledge Graph (KG) can be the basis to build digital tools to support those users who need to apply or execute them. In this paper, we leverage Large Language Model (LLM) capabilities and propose a prompt engineering approach to extract steps, actions, objects, equipment and temporal information from a textual procedure, in order to populate a Procedural KG according to a pre-defined ontology. We evaluate the KG extraction results by means of a user study, in order to qualitatively and quantitatively assess the perceived quality and usefulness of the LLM-extracted procedural knowledge. We show that LLMs can produce outputs of acceptable quality and we assess the subjective perception of AI by human evaluators.", "sections": [{"title": "1 Introduction", "content": "Procedural Knowledge (PK) is knowing how to perform some tasks. Such knowledge is usually expressed in the form of sequences of steps needed to achieve an overall goal, as in the case of recipes and maintenance activities. Making PK explicit is not trivial: identifying all actions to be performed, and splitting them into separate steps including all relevant information (e.g., the needed equipment), is often subject to interpretation and commonsense. Moreover, even when documented, this is usually done in an unstructured format, by means of natural language, across heterogeneous documents and systems. This makes it hard to access procedures for those who need to execute them, leading to partial or poor"}, {"title": "2 Related Work", "content": "Our work is at the intersection of the LLM application to Knowledge Engineering and its result assessment from a human point of view. Therefore, we take into account several aspects from the state of the art, as explained in the following."}, {"title": "2.1 LLMs for Knowledge Engineering and Knowledge Extraction", "content": "As reported in [1], LLMs provide a powerful tool for mapping natural language to formal language, a fundamental activity in knowledge engineering. Existing efforts in unifying LLMs and KGs are summarized in [38,37]. The LM-KBC challenge explores how to build disambiguated knowledge bases from LLMs, given a subject and a relation (see, among others, [47,50]). Several recent works cover the use of LLM prompting to address specific knowledge engineering tasks, like ontology engineering [49,8,5,19], ontology learning [11,2], named entity recognition and linking [21,9,43], knowledge graph construction including mapping generation [25,18]; some works specifically focus on benchmark, metrics and evaluation of such methods [31,12]. Specifically on (procedural) knowledge extraction, several studies (including [53,52,51]) investigate some extraction and reasoning tasks on procedures, such as the relation between a step and the procedure's goal, and the temporal relation between steps. Other works focus on the business process management domain: a corpus of business processes annotated by humans with activities, gateways, actors, and flow information [4] is exploited in [3] for extracting, from such processes, activities and their participants, using an LLM; an activity recommender to support business process modeling is introduced in [44]. A semi-structured dataset of repair manuals related to Mac Laptops is annotated with the required tools and the parts that are disassembled during the process, along with two methods for extracting them [33]. The procedures included in the dataset focus on opening the device and removing/repairing a broken component. Reusing such datasets as-is was not applicable for our experiments, since they did not fully covered our requirements, e.g., by taking into account only actions related to repairs [33], and by missing annotations about objects and tools [4] (cf. Sec- tion 3). Micrographs storing relevant entities and actions from technical support web pages are created in [22]. A prompting-based pipeline to extract a list of ordered steps from a procedure expressed as a numbered/bullet/indented list is proposed in [41]; differently, we give as input a text with no formatting and include additional descriptive sentences that the LLM needs to discard."}, {"title": "2.2 Evaluation of Generative AI", "content": "The evaluation of Generative AI and specifically LLMs is an open research problem, in that it is not easy to objectively assess the generated (natural language) output [26]. In the area of text annotation, different approaches exist [6,45], because LLMs are expected to act like human annotators [29]. Indeed, among other things, they can be employed for fact checking [35] or to identify claims and sub-claims in text entailment [20]. The approach to assess LLM outputs can combine human and LLM annotations [24], and several authors propose to use LLMs to perform also the evaluation task: by simulating human feedback via LLMs [10], by employing a (LLM) debate method [7], or by making LLMs adopt a human-like comparative evaluation approach [48]."}, {"title": "2.3 Human Bias on (Generative) AI", "content": "Finally, whenever involving humans in the evaluation of an artificial system, the subjective perception and the potential cognitive biases of users must be carefully considered. Indeed, some studies suggest that human attitudes towards AI are largely negative [34]. In the area of Generative AI, several concerns emerge in relation to the creativity potential of such models [28], especially in the context of art [39,32] and music [54]. In our work, we also investigate the potential bias of human subjects against LLMs that play the role of annotators, by applying an A/B testing approach, as done by [17]: they performed several experiments on how people judge humans and machines differently, in several scenarios (e.g., natural disasters, labor displacement, policing, privacy, algorithmic bias); they demonstrate that people tend to favour humans or machines in different scenarios, revealing the biases in human-machine interaction."}, {"title": "3 Problem Definition and Data", "content": "As mentioned in the introduction, our goal is to build a knowledge graph out of textual descriptions of procedures. Our vision is therefore to extract procedural knowledge (PK) from those documents, and to build a knowledge graph (KG) according to an ontology. This KG can then be used by different downstream ap- plications to facilitate the access and use of such procedures by human operators. Examples of such applications could be (KG-empowered) search applications or intelligent assistant: we expect the users to feel the need to be helped to find a specific procedure (or a part thereof) and to be guided step-by-step in its ex- ecution, for example by being informed about the action they have to perform (e.g., turning off a switch), the equipment they may need to use (e.g., wearing protective gloves) or the time it may take to perform a specific step (e.g., approximately 15 minutes). In order to fulfill such requirements, the procedural KG extracted from text should (1) preserve the intended meaning of the original document and (2) contain enough information to guide a user in correctly executing the procedure. The extracted KG should therefore be evaluated, respectively, on the basis of its quality and usefulness (cf. Section 7). Based on this scenario, we define a simple ontology and we identify a general-purpose dataset to be used in our LLM-powered PK extraction and KG building experiments."}, {"title": "4 Preliminary Study", "content": "In a preliminary formative study, we wanted to kick-start the LLM-powered pro- cedure extraction from text, by identifying the best performing prompt chain- ing [46] to execute the task. We evaluated both the prompt instructions and"}, {"title": "5 Research Questions", "content": "Our goal is to create a procedural knowledge graph by extracting structured knowledge from unstructured text; we would like to apply LLMs to the procedural knowledge extraction task and assess their results from a human point of view, taking into consideration the findings of our preliminary study. Our research questions are formulated as follows:\nRQ1 What is the quality of the extracted procedural knowledge (graph) as perceived by human evaluators? In this respect, we would like to understand if people judge the LLM-extracted knowledge as correctly representing the meaning of the original text and what influences their evaluation.\nRQ2 What is the usefulness of the extracted procedural knowledge (graph) as per- ceived by human evaluators? In this respect, we would like to assess the \"fitness for use\" of the output of the LLM extraction, when people are ex- plained its potential downstream use.\nRQ3 Do human evaluators show any systematic bias if they are told that the ex- traction task was executed by a LLM rather than an expert human annota- tor? In this respect, we would like to check any difference in people judgment about the extracted procedural knowledge (graph).\nRQ4 Qualitatively, are there any differences in the way human annotators and the LLM extract explicit knowledge and infer implicit knowledge from the text? In this respect, we would like to assess if the LLM behaves like human annotators in interpreting and extracting procedural knowledge."}, {"title": "6 Prompt Engineering Solution", "content": "The expected result for each procedure was an ordered list of sentences sum- marizing the steps extracted from the text, with their respective actions, direct objects, equipment items, and temporal information, expressed in RDF accord- ing to the given ontology. We tested different prompting approaches, as detailed below, and in an initial phase we manually evaluated the LLM results, finding an agreement between us with majority voting to decide whether each result was qualitatively satisfactory; the prompting approach that revealed to yield the best results was then used in the subsequent human evaluation experiment (cf. Section 7). Prompting approach. We tested different prompting approaches on the work- ing procedure (cf. Section 3) to find the best solution. Initially, we tried with a single prompt with zero/one/few-shot and we found that one prompt was not enough to achieve an acceptable result and a correct RDF, while giving one example in the prompt (one-shot learning) was the best trade-off between zero- shot and few-shots; then, as demonstrated in [46], we tried a Chain-of-Thought"}, {"title": "7 Experiment Design", "content": "In order to test our hypotheses, we extracted PK from 3 procedure texts applying the prompting approach described in Section 6. Then, we setup a crowdsourcing campaign to collect feedback from human evaluators as follows. Input to the human evaluator. Each participant was given the annota- tion instructions (corresponding to the prompt P1, including an example of the expected input-output), the procedure text and the extracted semi-structured PK as generated by the LLM. Expected output. Each participant was presented with a form (imple- mented with Microsoft Forms) and asked to perform two tasks (presented in two following pages in the form): (1) a short manual annotation exercise, to get familiar with the task and to provide feedback on the task itself, and (2) the as- sessment of the LLM annotation, according to the evaluation criteria explained in the following. Instructions. For the manual annotation task, participants were given ex- actly the same instructions included in the P1 prompt used with the LLM; it is worth noting that, as explained in Section 6, the second prompt P2 always yielded a correct RDF representation of the semi-structured output of the first prompt, therefore we gave the participants the LLM results in response to P1, to avoid the need for people to understand and validate Turtle. For the LLM assessment, participants were asked to express their evaluation with a set of Likert-scale ratings according to our evaluation dimensions. Evaluation dimensions. All quantitative evaluation items were expressed as statements to be assessed on a 1 to 5 Likert scale of agreement (from strongly"}, {"title": "8 Human Assessment and Discussion", "content": "In this section, we summarise and discuss the results and main findings of our quantitative and qualitative evaluation with the crowdsourcing participants. Human Evaluators. As explained before, we involved 180 participants from the Prolific crowdsourcing platform, between June and July 2024. Excluding participants who revoked the consent to register demographic data, the others were aged between 20 and 60, 37% male, 63% female; their employment status was 14% student, 47% employed and 39% unemployed. There was no significant difference between the 6 groups that were involved in the 6 campaigns, in terms of demographic and profiling characteristics. Perceived Quality. The overall distribution of the ratings given by the human evaluators on the quality-related dimensions are displayed in the first 7 Potential bias about AI. As explained in Section 7, we performed an A/B testing to check for any difference in people judgment, if they are told that the extraction task was executed by a LLM rather than an expert human annotator; in other words, we tested for any indication of a systematic bias about humans vs. machines. For each of the evaluation items, we applied the Kruskal-Wallis test [30] on the two groups (LLM vs. human annotator) to check for any difference. We run the test on the entire dataset (n=90 for each group), Qualitative comparison between human and LLM knowledge extraction. From a qualitative comparison between the annotations produced by human annotators and the knowledge extracted by the LLM, it emerges that both humans and LLMs benefit from the possibility to rephrase/simplify the original text of the procedure in order to generate instructions that are more concise and useful from their point of view, even if this leads to a greater variability of the output. However, even when differently rephrased, the vast majority of annotators detected the same steps as the LLM. Humans tended to rephrase also some verbs (e.g. turn on in place of preheat, prune instead of nip off) or some objects (e.g. packaging instead of container). Furthermore, we noticed that both humans and the LLM took advantage of the option to include in their an- notations implicit equipment items (e.g. inferring screwdriver to check if screws are tight) - increasing variability again. However, the LLM is more compliant to the given instructions, since it only extracted implicit equipment, while some human annotators introduced also implicit time indications when they consider it relevant to execute the procedure."}, {"title": "9 Conclusions", "content": "In this paper, we provided our results in the use of LLMs to extract procedural knowledge from textual descriptions, in order to build procedural knowledge graphs, adopting a suitable prompt chaining approach. Moreover, we performed an extensive human evaluation study to assess the LLM output in terms of its perceived quality and usefulness. We showed that the evaluators rated quite positively the quality of the extracted PK, while they were more doubtful about its usefulness; moreover, we highlighted that in some cases, people tend to be more critical towards an AI system rather than another human annotator: even if we did not find evidence of a systematic bias, this phenomenon is worth exploring in future LLM evaluation studies. Finally, we qualitatively studied the ability of both humans and LLMs to extract explicit and implicit knowledge from text, coming to the conclusion that, in extraction tasks where a definitive ground truth does not exist, an LLM displays abilities similar to those of people. All in all, we believe that LLMs are a promising technology to address this complex task, still the human intervention - through a human-in-the-loop ap- proach is very likely required to verify that the generated output is \"good enough\" to be used in real settings, especially when compliance is critical, like for example in the support to execute industrial procedures. Our future work will be oriented towards a broader and more systematic evaluation of the proposed prompting approach, extending the assessment to in- dustrial procedures in multiple formats (e.g. PDF or spreadsheets documents), addressing the challenge of extracting more complex cases of procedures (e.g. sub-procedures, optional or alternative steps, identification of additional enti- ties), extending the approach to include fine-tuning or retrieval-augmented gen- eration with the support of background knowledge, and comparing and bench- marking the outcomes of different LLMs."}]}