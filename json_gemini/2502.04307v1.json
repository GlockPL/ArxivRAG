{"title": "DEXTERITYGEN: Foundation Controller for Unprecedented Dexterity", "authors": ["Zhao-Heng Yin", "Changhao Wang", "Luis Pineda", "Francois Hogan", "Krishna Bodduluri", "Akash Sharma", "Patrick Lancaster", "Ishita Prasad", "Mrinal Kalakrishnan", "Jitendra Malik", "Mike Lambeta", "Tingfan Wu", "Pieter Abbeel", "Mustafa Mukadam"], "abstract": "Teaching robots dexterous manipulation skills, such as tool use, presents a significant challenge. Current approaches can be broadly categorized into two strategies: human teleoperation (for imitation learning) and sim-to-real reinforcement learning. The first approach is difficult as it is hard for humans to produce safe and dexterous motions on a different embodiment without touch feedback. The second RL-based approach struggles with the domain gap and involves highly task-specific reward engineering on complex tasks. Our key insight is that RL is effective at learning low-level motion primitives, while humans excel at providing coarse motion commands for complex, long-horizon tasks. Therefore, the optimal solution might be a combination of both approaches. In this paper, we introduce DexterityGen (DexGen), which uses RL to pretrain large-scale dexterous motion primitives, such as in-hand rotation or translation. We then leverage this learned dataset to train a dexterous foundational controller. In the real world, we use human teleoperation as a prompt to the controller to produce highly dexterous behavior. We evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that it is a general-purpose controller that can realize input dexterous manipulation commands and significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks. Notably, with DexGen we demonstrate unprecedented dexterous skills including diverse object reorientation and dexterous tool use such as pen, syringe, and screwdriver for the first time.", "sections": [{"title": "I. INTRODUCTION", "content": "Dexterous robotic hands are increasingly capturing attention due to their potential across various fields, including manufacturing, household tasks, and healthcare [37]. These robotic systems can replicate the fine motor skills of the human hand, enabling complex object manipulation [49, 4]. Their ability to perform tasks requiring human-like dexterity makes them valuable in areas where traditional automation falls short. However, effectively teaching dexterous in-hand manipulation skills to robotic hands remains a key challenge in robotics.\nRecent data-driven approaches to teach robots dexterous manipulation skills can be boardly categorized into two categories: human teleoperation (for imitation learning) [19, 15, 52, 41, 12, 33, 11, 58, 50] and sim-to-real Reinforcement Learning (RL) [4, 16, 40, 62, 24, 18, 9, 23, 3, 35, 60, 59, 32, 51]. Despite their success, these methods face several limitations in practical applications. For human teleoperation, a major bottleneck is the collection of high-quality demonstrations [31, 61]. In contact-rich dexterous manipulation, it is challenging for humans to perform safe and stable object manipulation actions, often resulting in objects falling from the hand. This makes teleoperation impractical for dexterous manipulation tasks. For sim-to-real RL, challenges arise from the significant domain gap between simulation and the real world, as well as the need for highly task-specific reward specifications when training an RL agent for complex tasks. We will discuss these challenges in more detail in Section II.\nWhile each approach has its own set of challenges, combining their strengths offers a promising strategy to address the complexities of dexterous manipulation. Specifically, recent sim-to-real RL works [40, 62] have shown that it is possible to train simple dexterous in-hand object manipulation primitives (e.g. rotation) that can be transferred to a robot in the real world. This suggests that RL can be leveraged to generate a large-scale dataset of dexterous manipulation primitives, including in-hand object rotation, translation, and grasp transitions. Meanwhile, humans excel at composing these skills through teleoperation to address more challenging tasks. For example, Yin et al. have shown that they can perform in-hand reorientation by calling several rotation primitives sequentially [62]. However, the external inputs in these studies are limited to a few discretized commands, lacking control over low-level interactions, such as finger movements and object contact. This limitation makes it difficult to prompt existing models to generate more detailed, finger-level interaction behaviors, such as using a syringe or screwdriver.\nMotivated by these observations, in this paper, we propose"}, {"title": "II. EXISTING APPROACHES: CHALLENGES AND OPPORTUNITIES", "content": "In this section, we review the challenges and opportunities with existing approaches to dexterous manipulation that motivate our work."}, {"title": "A. Human Teleoperation for Imitation Learning", "content": "Challenge: Dexterous manipulation via teleoperation is challenging for humans due to the following reasons:\na) Partial Observability: During in-hand manipulation, the object motion is determined by the contact dynamics between hand and object [57, 37, 22]. Successful manipulation requires perceiving and understanding contact information, such as normal force and friction, to generate appropriate torques. However, human operators face challenges in observing this information due to occlusion and limited tactile feedback. Additionally, existing discrete haptic feedback (e.g. binary vibration) alone is often inadequate for conveying complex touch interactions and contact geometries.\nb) Embodiment Gap: Although human and robot hands may appear similar at first glance, they differ significantly in their kinematic structures and geometries. For example, human fingers have a smooth and compliant surfaces, while the robot fingers often have rough edges. These differences result in discrepancies in contact dynamics, making it challenging to directly transfer our understanding of human finger motions for object manipulation to robotic counterparts. In our early experiments, we find the object motion very sensitive to the change of fingertip shape.\nc) Motion Complexity: Dexterous in-hand manipulation involves highly complex motion. The process requires precise control of a high degree-of-freedom dynamical system. Any suboptimal teleoperation motion at any DOF can lead to failures such as breaking grasping contacts.\nd) Inaccuracy of Actions (Force): Existing robot hand teleoperation systems are based on hand retargeting with position control, which lacks an intuitive force control interface to users. As a result, users can only influence force through position-control errors, making teleoperation particularly challenging in force-sensitive scenarios. Moreover, the presence of noise in real world robot system further complicates control.\nOpportunity: High-level (Semantical) Motion Control While humans may find it challenging to provide fine-grained, low-level actions directly, human teleoperation or even video demonstrations can still offer valuable coarse motion-level guidance for a variety of complex real-world tasks. Humans possess intuitive knowledge, such as where a robot hand should make contact and what constitutes a good grasp. Thus, human data can be leveraged to create a high-level semantic action plan. In locomotion research, recent studies have proposed using teleoperation commands as high-level motion prompts [10]. However, extending this approach to finegrained dexterous manipulation remains an open question."}, {"title": "B. Sim-to-real Reinforcement Learning", "content": "Challenge: Developing a generalized sim-to-real policy for dexterous manipulation involves two main challenges:\na) Sim-to-Real Gap: It is difficult to reproduce real-world sensor observation (mainly for vision input) and physics in simulation. This gap can make sim-to-real transfer highly challenging for complex tasks. In particular, transferring a vision-based control policy from simulation to real world for dexterous hands is a huge challenge and requires costly visual domain randomization [55]. For instance, Dextreme [16] leveraged extensive visual domain randomization with 5M rendered images to train a single object rotation policy.\nb) Reward Specification: A more important issue, beyond the sim-to-real gap, is the notorious challenge of designing reward functions for long-horizon, contact-rich problems. Existing methods often involve highly engineered rewards or overly complicated learning strategies [9], which are task-specific and limit scalability.\nOpportunity: Low-level (Physical) Action Control Although sim-to-real RL can be difficult, especially for those complex long-horizon or vision-based tasks, some recent works have shown that sim-to-real RL is sufficient to build diverse transferable manipulation primitives based on proprioception and touch [62]. Therefore, one opportunity for sim-to-real RL is to create rich low-level action primitives that can be combined with the high-level action plan discussed above. In this paper, we achieve this through generative pretraining."}, {"title": "III. THE DEXGEN CONTROLLER", "content": "We propose to pretrain a generative behavior model $p_{\\theta}(a|o)$ on the simulation dataset to model prior action distribution so"}, {"title": "A. Preliminaries", "content": "a) Diffusion Models: Diffusion Model [17] is a powerful generative model capable of capturing highly complex probabilistic distributions, which we use as our base model. The classical form of the diffusion model is the Denoising Diffusion Probabilistic Model (DDPM) [17]. DDPM defines a forward process that gradually adds noise to the data sample $x_0 \\sim p_{data}(x)$:\n$x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1 - \\alpha_t}\\epsilon_t$,\n(1)\nwhere $\\alpha_t$ is some noising schedule. We have $x_t \\sim N(\\sqrt{\\bar{\\alpha_t}}x_0, (1 - \\bar{\\alpha_t})I)$ where $\\bar{\\alpha_t} = \\Pi_{t=1}^{T}\\alpha_s$ goes to 0 as $t \\rightarrow +\\infty$. DDPM trains a model $\\mu_{\\theta}(x_t,t)$ to predict denoised sample $x_0$ given the noised sample $x_t$ with its timestep t. During sampling, DDPM generates the sample by removing the noise through a reverse diffusion process:\n$p(x_{t-1}|x_t) = N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\sigma_{\\theta}^2I)$\n(2)\nDDPM can generate high-fidelity samples in both vision and robotics applications. In addition to the power to generate data samples faithfully, diffusion models also support guided sampling [20], which turns out to be very useful in our set-up.\nSpecifically, when $p_{data}(x)$ is modeled by a diffusion model, we can sample from a product probability distribution $p(x) \\propto p_{data}(x)h(x)$ where h(x) is given by a differentiable energy function $h(x) = exp \\mathcal{J}(x)$. To do this, we only need to introduce a small modification to the reverse diffusion"}, {"title": "B. Large-Scale Behavior Dataset Generation", "content": "Since human teleoperation or external policies will control the robot hand to interact with the object in diverse ways, our model should be capable of providing refinement for all these potential scenarios (states). To achieve this, we require a large-scale behavior dataset to pretrain our DexGen model, ensuring comprehensive coverage of the state space. We accomplish this by collecting object manipulation trajectories in simulation through reinforcement learning.\nAnygrasp-to-Anygrasp To ensure our dataset can cover a broad range of potential states, we introduce Anygrasp-to-Anygrasp as our central pretraining task. This task captures the essential part of in-hand manipulation, which is to move the object to arbitrary configurations. For each object, we define our training task as follows. We first generate a set of object grasps using Grasp Analysis and Rapidly-exploring Random Tree (RRT) [30], similar to the Manipulation RRT procedure [24]. Each generated grasp is defined as a tuple (hand joint position, object pose). In each RL rollout, we initialize the object in the hand with a random grasp. We set the goal to be a randomly selected nearby grasp using the k Nearest-Neighbor search. After reaching the current grasp goal, we update the goal in the same way. We find it crucial to select a nearby reachable goal during the training process, as learning to reach a distant grasp directly can be difficult. After training, we use this anygrasp-to-anygrasp policy to rollout grasp transition sequences to cover all the possible hand-object interaction modes. We sample over 100K grasp for most objects during grasp generation to ensure coverage.\nThis training procedure yields a rich repertoire of useful skills, including object translation and reorientation, which the high-level policy can leverage for solving downstream tasks (Figure 5). In addition to the Anygrasp-to-AnyGrasp task, we also introduce other tasks such as free finger moving and fine-grained manipulation (e.g. fine rotation) to handle tasks that have special precision requirements.\nDuring RL training, we use a diverse set of random objects and wrist poses. For each task, we include random geometrical objects with different physical properties. To enhance the"}, {"title": "C. DexGen Model Architecture", "content": "We illustrate our DexGen model architecture in Figure 4. The DexGen model has two modules. The first module is a diffusion model that characterizes the distribution of robot finger keypoint motions given current observations. Here we use 3D keypoint motions $\\Delta x \\in R^{T\\times K\\times 3}$ in the robot hand frame as an intermediate action representation, This representation is particularly advantageous for integrating guidance from human teleoperation. In this context, T is the future horizon, K is the number of finger keypoints. The second module in DexGen is an inverse dynamics model, which converts the keypoint motions to executable robot actions (i.e. target joint position) $q_t = Q_t$.\nWe use a UNet-based [45] diffusion model to fit the complex keypoint motion distribution of our multitask dataset. Our model learns to generate several future finger keypoint offsets $\\Delta x_i = x_{t+i}-x_t$ conditioned on the robot state o at timestep t and a mode conditioning variable. The state is a stack of"}, {"title": "D. Inference: Motion Conditioning with Guided Sampling", "content": "Our goal is to sample a keypoint motion that is both safe (i.e. from our learned distribution $p_{\\theta}(\\Delta x|o)$) and can maximally preserve the input reference motion. Formally, this can be written as $\\Delta x \\sim p_{\\theta}(\\Delta x|o) \\exp(-\\text{Dist}(\\Delta x, \\Delta x_{\\text{input}}))$. Here, $\\Delta x_{\\text{input}} \\in R^{K\\times 3}$ is the input commanded fingertip offset, and Dist is a distance function that quantifies the distance between the predicted sequence and the input reference. There can be many ways to instantiate this distance function. In this paper, we find the following simple distance function works well empirically:\n$\\text{Dist}(x, x_{\\text{input}}) = \\sum_{i=1}^{T} ||\\Delta x_i - X_{\\text{input}}||^2$.\n(3)\nThe above function encourages the generated future fingertip position to closely match the commanded fingertip position. Since the action of the robot hand has a high degree of freedom (16 for the Allegro hand used in this paper), naive sampling strategies become computationally intractable. To address this, we propose using gradient guidance in the diffusion sampling process to incorporate motion conditioning. In each diffusion step, we adjust the denoised sample \\Delta x by subtracting $\\alpha\\Sigma \\nabla_{\\Delta x} \\text{Dist}(\\Delta x, \\Delta x_{\\text{input}})$ as a guide. Here $\\alpha$ is a parameter of the strength of the guidance to be tuned, which we will study in experiments. The generated finger keypoint movement is then converted to action by the inverse dynamics model. We use DDIM sampler [53] during inference for 10Hz control. The total sampling time is around 27ms (37Hz) on a Lambda workstation equipped with an NVIDIA RTX 4090 GPU."}, {"title": "IV. EXPERIMENTS", "content": "In the experiments, we first validate the effectiveness of DexGen through simulated experiments, demonstrating its ability to enhance the robustness and success rate of extremely suboptimal policies. Then, we test our system in the real world with a focus on its application in shared autonomy. Our results show that DexGen can assist a human operator in executing unprecedented dexterous manipulation skills with remarkable generalizability."}, {"title": "A. System Setup", "content": "In this paper, we use Allegro Hand as our manipulator and we attach the Allegro Hand to a Franka-panda robot arm. In the teleoperation experiments in real world, we use a retargeting-based system to control the robot with human hand gestures. The human hand pose is captured by Manus Glove"}, {"title": "B. Simulated Experiments", "content": "1) Experimental Setup: We first test the capability of DexGen in assisting suboptimal policies in solving the Anygrasp-to-Anygrasp task in simulation. We simulate 2 kinds of suboptimal policies with an expert RL policy $\\pi_{\\text{exp}}$. The first one is $\\pi_{\\text{noisy}}(a|s) = \\pi_{\\text{exp}}(a|s) + U(-\\alpha, \\alpha)$, which simulates an expert that can perform dangerous suboptimal actions through additive uniform noise. The second is $\\pi_{\\text{slow}}(a|s) = U(0, \\alpha) \\pi_{\\text{exp}}(a|s)$, which is a slowdown version of expert. We compare these suboptimal experts to their assisted counterparts DexGen \\circ \\pi. We record the average number of critical failures (drop the object) and the number of goal achievements within a certain time of different policies.\n2) Main Results: We plot the result of different policies in Figure 8. We find that without our assistance, the noisy expert has much more frequent failures. As a result, it can only hardly achieve any goals in the evaluation. In contrast, with the assistance of DexGen, we can partially recover the performance of this noisy expert. We also find that for different policies, the optimal guidance value is also different. Fortunately, there is a common region working well for all these policies. Moreover, when the guidance is relatively small, although we can maintain the object in hand, we can not achieve the desired goal as well because DexGen does not know what the goal is. When the guidance becomes too large, the potentially suboptimal external motion command may take over DexGen guidance and lead to a lower duration in some cases."}, {"title": "C. Real World Experiments:", "content": "We have demonstrated that our system can provide effective assistance through simulated validation. Then, we further design several tasks for benchmarking in the real world. In the first set of experiments, we ask a human teleoperator to act as an external high-level policy and we evaluate whether our system can assist humans to solve diverse dexterous manipulation tasks. We introduce a set of atomic skills that covers common in-hand dexterous manipulation behavior.\nIn-hand Object Reorientation The user is required to control the hand to rotate a given object to a specific pose. In the beginning, we initialize the object in the air over the palm, and the user needs to first teleoperate the hand to grasp the object.\nFunctional Grasping Regrasping is a necessary step in tool manipulation. The user is asked to perform a power grasp on the tool handle placed either horizontally (normal) or vertically in the air (horizontal functional grasp)."}, {"title": "D. Real World Results", "content": "The performance of different approaches is shown in Table I. We observe that humans can hardly use the baseline teleoperation system to solve the tasks above. The user can drop the object easily during the contact-rich manipulation process. Compared to the baseline, our system can successfully help the user to solve many tasks in various challenging setups. During these experiments, we also observe the following intriguing properties of our system:\na) Protective \"Magnetic Effect\": We find that the fingertips show some \"magnetic effect\" when they are in contact with the object. When the user mistakenly moves a supporting finger which may drop the object, our model can override that behavior and maintain the contact as if the fingertips are sticking to the object (Figure 9 second row). This explains why the user can achieve a much higher success rate in these dexterous tasks.\nb) Intention Following: Although our model overrides dangerous user action, we find that in most cases our model can follow the user's intention (action) well and move along the user-commanded moving direction. During the manipulation procedure, the user can still have a sense of agency over the robot hand and complete a complex task. This finding echoes our simulated result with noisy policies: DexGen can realize the intention in the noisy suboptimal actions."}, {"title": "V. RELATED WORKS", "content": "a) Foundation Models and Pretraining for Robotics: In recent years, the success of large foundation models in natural language processing and computer vision [2, 29, 56] has attracted much attention in building foundation models for robotics [6, 7, 13, 42, 54, 38, 64, 27, 25]. Existing works typically focus on building a large end-to-end control model by pretraining them on large real world datasets. Our framework also leverages large-scale pretraining, but it differentiates from these works in various aspects. First, we consider pretraining a controller on pure simulation datasets rather than real world datasets which require extensive human efforts in data collection with teleoperation. Second, we study dexterous manipulation with a high DOF robotic hand and demonstrate the advantage of generative pretraining in this challenging scenario for the first time, while existing works typically consider parallel jaw gripper problems. Third, we build a low-level foundation controller that can be prompted with continuous fine-grained guidance to provide useful actions, which can be potentially integrated with high-level planning policies in the future. Most existing robotic foundation models are conditioned on discrete language prompts or task embeddings. In summary, our pretraining framework for building a foundational low-level controller presents a new perspective in the foundation model literature.\nb) Shared Autonomy: Our system is also related to shared autonomy research [1, 26, 8, 46, 48], which focuses on leveraging external action guidance to produce effective actions. Some works focus on how to train RL agents with external actions (e.g. from teleoperation) [43, 14, 44]. In their setup, the external inputs are usually treated as part of observation fed to the RL policy. Compared to this line of work, our method does not involve human actions in the training phase. Another line of work assumes the existence of a few task-specific intentions and goals and reduces the shared autonomy problem to the goal or intent inference [1, 21]. A limitation of this line of work in dexterous manipulation is that they do not allow fine-grained finger control since they only provide a few options for high-dimensional action space. Our method samples fine-grained low-level behavior according to user commands in high-dimensional action space and does not suffer from this problem. The most relevant works are [5, 63], which also use some sampled distribution to correct user behavior. We use a different correction procedure and investigate a more general and challenging dexterous manipulation setup."}, {"title": "VI. CONCLUSION", "content": "In this paper, we have presented DexterityGen as an initial attempt towards building a foundational low-level controller for dexterous manipulation. We have demonstrated that generative pretraining on diverse multi-task simulated trajectories yield a powerful generative controller that can translate coarse motion prompts to effective low-level actions. Combined with external high-level policy, our controller exhibits unprecedented dexterity. We believe that our work opens up new possibilities in various dimensions of dexterous manipulation.\nLimitations and Future Work Our exploration still has some limitations to be addressed in future works, which we discuss as follows.\n1) Touch Sensing In this work, we rely on joint angle proprioception for implicit touch sensing (i.e. inferring force by reading control errors), which can be insufficient and nonrobust for fine-grained problems. In many cases, it is impossible to recover contact geometry based on joint angle error. In the future, we will add touch to pretraining, which has been shown possible for sim-to-real transfer. We hope that this can further improve the robustness of our system.\n2) Vision: Hand-Eye Coordination Our low-level controller does not involve vision. Nevertheless, we observe that vision feedback is necessary for producing accurate tool motions for many tasks such as using a screwdriver. It is unclear whether this vision processing should be in the high-level policy or part of the proposed foundation low-level controller, and we leave this to future research.\n3) Real-world Finetuning In this work, we deploy our controller in a zero-shot manner. However, due to the sim-to-real gap, it can still be necessary to fine-tune our controller with some real world experience."}, {"title": "APPENDIX", "content": "A. DexGen Training Pipeline\nWe provide an overview of the full training process in Algorithm 1. The algorithm has two stages. In the first stage, we first collect manipulation trajectories with multiple RL policies. In the second stage, we distill the experience into our controller. For the dataset filtering step, we apply a very simple heuristic rule. If a rollout ends with dropping the object, then we directly discard the last 2 second transitions."}, {"title": "B. Implementation of Anygrasp-to-Anygrasp", "content": "The core dexterous manipulation task used by Algorithm 1 is Anygrasp-to-Anygrasp. We describe its implementation as follows.\nGrasp Generation To define this task, we first need to generate the grasp set for each object with the Grasp Generation Algorithm 2. The algorithm first generates a base grasp set using heuristic sampling, and we further expand this grasp set via RRT search to ensure that it can cover as many configurations as possible. Note that there exist many approaches for synthesizing grasps. Here, we just provide one option that works well empirically."}, {"title": "Net Force Optimization ({n})", "content": "Minimize:\n$\\sum_i f_i n_i$\ns.t.\n$\\forall i, f_i \\geq 0$,\n$\\exists i, f_i = 1$.\nIntuitively, we apply force $f_i$ at each contact point along contact normal $n_i$ and we optimize for a nontrivial force combination ($\\exists f_i = 1$) that can generate a near-zero net force. If the minimizer of this problem is below a threshold, we consider this grasp as stable. Note that the second existence constraint is hard to directly parameterize as a differentiable loss function. In our implementation, we decompose this"}, {"title": "Reward Design", "content": "The reward function for the Anygrasp-to-Anygrasp task is as follows. It is composed of three different terms, goal-related reward $r_{goal}$, style-related reward $r_{style}$, and regularization terms $r_{reg}$.\n$r = W_{goal}r_{goal} + W_{style}r_{style} + W_{reg}r_{reg}$.\n(4)\nThe goal-related reward term $r_{goal}$ involves target object pose and finger joint positions:\n$r_{goal} = \\exp(-A_{pos}||p_{obj} - p_{target}||^2 - A_{orn}d(R_{obj}, R_{target}))$\n-$\\frac{A_{hand}}{N_{p}}"}, {"title": "Goal Dynamics", "content": "A crucial design in the Anygrasp-to-Anygrasp task is the goal dynamics. We find that when we set a goal very far away, the RL policy can usually fail to reach that goal and as a result, the RL learning process can plateau very early. Therefore, throughout the RL process, we set goals within a moderate distance to ensure effective RL learning. Specifically, when the current goal is achieved, we search for a grasp in our grasp cache whose object distance is within a certain range as our next goal. We achieve this through a Nearest Neighbor search. Since NN search is computationally expensive for a large grasp set, we first perform a random down-sampling at each update step before the next goal computation."}]}