{"title": "MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning", "authors": ["Yifu Yuan", "Zhenrui Zheng", "Zibin Dong", "Jianye Hao"], "abstract": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that\nsimultaneously optimize multiple conflicting objectives, but it requires extensive\nonline interactions. Offline MORL provides a promising solution by training on\npre-collected datasets to generalize to any preference upon deployment. How-\never, real-world offline datasets are often conservatively and narrowly distributed,\nfailing to comprehensively cover preferences, leading to the emergence of out-of-\ndistribution (OOD) preference areas. Existing offline MORL algorithms exhibit\npoor generalization to OOD preferences, resulting in policies that do not align with\npreferences. Leveraging the excellent expressive and generalization capabilities\nof diffusion models, we propose MODULI (Multi Objective DiffUsion planner\nwith sLIding guidance), which employs a preference-conditioned diffusion model\nas a planner to generate trajectories that align with various preferences and derive\naction for decision-making. To achieve accurate generation, MODULI introduces\ntwo return normalization methods under diverse preferences for refining guidance.\nTo further enhance generalization to OOD preferences, MODULI proposes a novel\nsliding guidance mechanism, which involves training an additional slider adapter to\ncapture the direction of preference changes. Incorporating the slider, it transitions\nfrom in-distribution (ID) preferences to generating OOD preferences, patching, and\nextending the incomplete Pareto front. Extensive experiments on the D4MORL\nbenchmark demonstrate that our algorithm outperforms state-of-the-art Offline\nMORL baselines, exhibiting excellent generalization to OOD preferences.", "sections": [{"title": "1 Introduction", "content": "Real-world decision-making tasks, such as robotic control [47, 29], autonomous driving [26], and\nindustrial control [4], often require the optimization of multiple competing objectives simultaneously.\nIt necessitates the trade-offs among different objectives to meet diverse preferences [39]. For\ninstance, in robotic locomotion tasks, users typically focus on the robot's movement speed and\nenergy consumption [47]. If the user has a high preference for speed, the agent will move quickly\nregardless of energy consumption; if the user aims to save energy, the agent should adjust to a\nlower speed. One effective approach is Multi-objective Reinforcement Learning (MORL), which\nenables agents to interact with vector-rewarded environments and learn policies that satisfy multiple\npreferences [17, 38, 48]. These methods either construct a set of policies to approximate the\nPareto front of optimal solutions [2, 13] or learn a preference-conditioned policy to adapt to any\nsituation [33, 21]. However, exploring and optimizing policies for each preference while alleviating\nconflicts among objectives, requires extensive online interactions [3], thereby posing practical\nchallenges due to high costs and potential safety risks.\nOffline MORL [50] proposes a promising solution that can learn preference-conditioned policies\nfrom pre-collected datasets with various preferences, improving data efficiency and minimizing\ninteractions when deploying in high-risk environments. Most Offline MORL approaches focus\non extending return-conditioned methods [43, 50] or encouraging consistent preferences through\npolicy regularization [28]. However, real-world offline datasets often come from users with different\nbehavior policies, making it difficult to cover the full range of preferences and leading to missing\npreference regions, i.e., OOD preferences, resulting in a shattered or narrow data distribution. As\nshown in fig. 1, we visualize the trajectory returns for Hopper-Amateur dataset. The Complete\nversion of the dataset can comprehensively cover a wide range of preferences. In many cases,\nsuboptimal policies can lead to Shattered or Narrow datasets, resulting in OOD preference regions\nas marked by the red circles. We then visualize the approximate Pareto front for MORvS baseline [50]\nand our MODULI in such a shattered dataset. We find that the current Offline MORL baselines\nexhibit poor generalization performance when learning policies from incomplete datasets. They fail\nto adapt to OOD preference policies. The approximate Pareto front of MORvS shows significant\ngaps in the OOD preference areas, indicating that the derived policies and preferences are misaligned.\nTherefore, we aim to derive a general multi-objective policy that can model a wide range of preference\nconditions with sufficient expressiveness and strong generalization to OOD preferences. Motivated\nby the remarkable capabilities of diffusion models in expressiveness and generalization, we propose\nMODULI (Multi Objective DiffUsion planner with sLIding guidance), transforming the problem of\nOffline MORL into conditional generative planning. In fig. 1 (right), MODULI demonstrated a better\napproximation of the Pareto front, achieving returns beyond the dataset. MODULI also patched the\nmissing preference regions and exhibited excellent generalization for the OOD preferences.\nMODULI constructs a multi-objective conditional diffusion planning framework that generates long-\nterm trajectories aligned with preference and then executes as a planner. Furthermore, appropriate\nguiding conditions play a crucial role in the precise generation, which requires the assessment of\ntrajectory quality under different preferences for normalization. We found that simply extending the\nsingle-objective normalization method to multi-objective causes issues, as it fails to accurately align\npreferences with achievable high returns. Therefore, we propose two return normalization methods\nadapted to multi-objective tasks, which can better measure the quality of trajectories under different\npreferences and achieve a more refined guidance process. Besides, to enhance generalization to OOD\npreferences, we introduce a sliding guidance mechanism that involves training an additional slider\nadapter with the same network structure to independently learn the latent directions of preference\nchanges. During generation, we adjust the slider to actively modify the target distribution, gradually\ntransitioning from ID to specified OOD preferences. This better captures the fine-grained structure of\nthe data distribution and allows for finer control, thereby achieving better OOD generalization. In\nsummary, our contributions include (i) a conditional diffusion planning framework named MODULI,\nwhich models general multi-objective policies as conditional generative models, (ii) two preference-\nreturn normalization methods suitable for multi-objective preferences to refine guidance, (iii) a novel\nsliding guidance mechanism in the form of an adapter and, (iv) superior performance in D4MORL,\nparticularly impressive generalization to OOD preferences."}, {"title": "2 Related Work", "content": "Offline MORL Most MORL research primarily focuses on online settings, aiming to train a\ngeneral preference-conditioned policy [21, 32] or a set of single-objective policies [35, 47] that can\ngeneralize to arbitrary preferences. This paper considers an offline MORL setting that learns policy\nfrom static datasets without online interactions. PEDI [45] combines the dual gradient ascent with\npessimism to multi-objective tabular MDPs. MO-SPIBB [43] achieves safe policy improvement\nwithin a predefined set of policies under constraints. These methods often require a priori knowledge\nabout target preferences. PEDA [50] first develops D4MORL benchmarks for scalable Offline MORL\nwith a general preference for high-dimensional MDPs with continuous states and actions. It also\nattempts to solve the Offline MORL via supervised learning, expanding return-conditioned sequential\nmodeling to the MORL, such as MORvS [12], and MODT [6]. [28] integrates policy-regularized\ninto the MORL methods to alleviate the preference-inconsistent demonstration problem. This paper\ninvestigates the generalization problem of OOD preference, aiming to achieve better generalization\nthrough more expressive diffusion planners and the guidance of transition directions by a slider\nadapter.\nDiffusion Models for Decision Making Diffusion models (DMs) [19, 42] have become a main-\nstream class of generative models. Their remarkable capabilities in complex distribution modeling\ndemonstrate outstanding performance across various domains [24, 40], inspiring research to apply\nDMs to decision-making tasks [7, 49, 46]. Applying DMs for decision-making tasks can mainly\nbe divided into three categories [51]: generating long horizon trajectories like planner [22, 9, 1],\nserving as more expressive multimodal polices [44, 16] and performing as the synthesizers for data\naugmentation [31]. Diffusion models exhibit excellent generalization and distribution matching\nabilities [23, 27], and this property has also been applied to various types of decision-making tasks\nsuch as multi-task RL [18], meta RL [36] and aligning human feedback [10]. In this paper, we further\nutilize sliding guidance to better stimulate the generalization capability of DMs, rather than merely\nfitting the distribution by data memorization [15]."}, {"title": "3 Preliminaries", "content": "Multi-objective RL The MORL problem can be formulated as a multi-objective Markov de-\ncision process (MOMDP) [5]. A MOMDP with n objectives can be represented by a tuple\n$(S, A, P, R, \\Omega, \\gamma)$, where S and A demote the state and action spaces, P is the transition func-\ntion, $R : S \\times A \\rightarrow \\mathbb{R}^n$ is the reward function that outputs n-dim vector rewards $r = R(s,a), \\Omega$\nis the preference space containing vector preferences w, and $\\gamma$ is a discount factor. The policy $\\pi$ is\nevaluated for m distinct preferences $\\{\\omega^p\\}_{p=1}^m$, resulting policy set be represented as $\\{\\pi_{\\omega^p}\\}_{p=1}^m$, where\n$\\pi_{\\omega^p} = \\pi(a|s, \\omega^p)$, and $G_{\\pi_{\\omega^p}} = E_{a_t \\sim \\pi_{\\omega^p}} [\\sum_t \\gamma^t R(s_t, a_t)]$ is the corresponding unweighted expected\nreturn. We say the solution $G_{\\pi_p}$ is dominated by $G_{\\pi_q}$ when $G_{\\pi_p} < G_{\\pi_q}$ for $\\forall i \\in [1,2,......,n]$. The\nPareto front P of the policy $\\pi$ contains all its solutions that are not dominated. In MORL, we aim to\ndefine a policy such that its empirical Pareto front is a good approximation of the true one. While not\nknowing the true front, we can define a set of metrics for relative comparisons among algorithms,\ne.g., hypervolume, sparsity, and return deviation, to which we give a further explanation in Section 5.\nDenoising Diffusion Implicit Models (DDIM) Assume the random variable $x^0 \\in \\mathbb{R}^D$ follows\nan unknown distribution $q_0(x^0)$. DMs define a forward process $\\{x_t\\}_{t\\in[0,T]}$ by the noise schedule\n$\\{\\alpha_t, \\sigma_t\\}_{t\\in[0,T]}$, S.t., $\\forall t \\in [0, T]$:\n\n$dx_t = f(t)x_t dt + g(t)dw_t, x^0 \\sim q_0(x^0)$,\n\nwhere $w_t \\in \\mathbb{R}^D$ is the standard Wiener process, and $f(t) = \\frac{d\\log\\alpha_t}{dt}, g^2(t) = -2\\sigma_t^2 \\frac{d\\sigma_t}{dt} - 2\\sigma_t^2 \\frac{d \\log \\alpha_t}{dt}$. The\nSDE forward process in Equation (1) has an probability flow ODE (PF-ODE) reverse process from\ntime T to 0 [42]:\n\n$dx_t = f(t)x_t - g^2(t) \\nabla_{x^t} \\log q_t(x^t), x^T \\sim q_T(x^T)$,"}, {"title": "4 Methodology", "content": "To address the challenges of expressiveness and generalization in Offline MORL, we propose a novel\nOffline MORL framework called MODULI. This framework utilizes conditional diffusion models for\npreference-oriented trajectory generation and establishes a general policy suitable for a wide range\nof preferences while also generalizing to OOD preferences. MODULI first defines a conditional\ngeneration and planning framework and corresponding preference-return normalization methods.\nMoreover, to enhance generalization capability, we design a sliding guidance mechanism, where\na slider adapter actively adjusts the data distribution from known ID to specific OOD preference,\nthereby avoiding simple memorization.\n4.1 Offline MORL as Conditional Generative Planning\nWe derive policies on the Offline MORL dataset $\\mathcal{D}$ with trajectories $\\tau = \\{(\\omega, s_0, a_0, r_0, \\dots, s_{T-1}, a_{T-1}, r_{T-1})\\}$ to respond to arbitrary target preferences $\\omega \\in \\Omega$\nduring the deployment phase. Previous Offline MORL [50] algorithms generally face poor expres-\nsiveness, unable to adapt to a wide range of preferences through a general preference-conditioned\npolicy, resulting in sub-optimal approximate Pareto front. Inspired by the excellent expressiveness\nof diffusion models in decision making [1, 10], MODULI introduces diffusion models to model\nthe policy. We define the sequential decision-making problem as a conditional generative objective\n$\\max E_{\\tau \\sim \\mathcal{D}} [\\log p_\\theta(x^0(\\tau) | y(\\tau))]$, where $\\theta$ is the trainable parameter. The diffusion planning in\nMORL aims to generate sequential states $x^0(\\tau) = [s_0,\\dots, s_{H-1}]$ with horizon H to satisfy the\ndenoising conditions $y = [\\omega, g]$, where $\\omega$ is a specific preference. Similar to [50], $g_t$ is defined as\nthe vector-valued Returns-to-Go (RTG) for a single step, i.e. $g_t = \\sum_{i=t}^H R(s_t, a_t)$. The trajectory\nRTG g is the average of the RTGs at each step.\nTraining Next, we can sample batches of $x^0(\\tau)$ and y from the dataset and update $p_\\theta$ as the\nmodified score matching objective in eq. (3):\n\n$\\mathcal{L}(\\theta) = E_{(x^0,\\omega,g)\\sim \\mathcal{D},t\\sim U(0,T),\\epsilon \\sim \\mathcal{N}(0,I)} [\\vert\\vert \\epsilon_\\theta(x_t, t, \\omega, g) - \\epsilon \\vert\\vert^2] $.\n\nMODULI uses the CFG guidance for conditional generation. During training, MODULI learns\nboth a conditioned noise predictor $\\epsilon_\\theta(x_t,t,\\omega, g)$ and an unconditioned one $\\epsilon_\\theta(x_t, t, \\varnothing)$. We adopt\na masking mechanism for training, with a probability of $0 < p < 1$ to zero out the condition of a\ntrajectory, equivalent to an unconditional case. MODULI also employs a loss-weight trick, setting\na higher weight for the next state $s_1$ of the trajectory $x^0(\\tau)$ to encourage more focus on it, closely\nrelated to action execution."}, {"title": "4.2 Refining Guidance via Preference-Return Normalization Methods", "content": "Global Normalization To perform generation under different scales and physical quantities of\nobjectives, it is essential to establish appropriate guiding conditions, making return normalization a\ncritical component. We first consider directly extending the single-objective normalization scheme to\nthe multi-objective case, named Global Normalization. We normalize each objective's RTG using\nmin-max style, scaling them to a unified range. Specifically, we calculate the global maximum RTG\n$g^{max} = [g^{max}_1,\\dots, g^{max}_n]$ for each objective in the offline dataset. The same applies to the minimum\nRTG $g^{min}$. The conditioned RTG g are first normalized to $(g - g^{min})/(g^{max} - g^{min}) \\in [0, 1]^n$, then\nconcatenated with the preference $\\omega$ as inputs. During deployment, we aim to generate $x(\\tau)$ to\nalign the given preference $\\omega_{target}$ and maximized scalarized return using the highest RTG condition\n$g = 1^n$. Note that we compute the global extremum values based on the offline dataset with different\npreferences, which are not necessarily the reachable min/max value for each objective. Also, some\nobjectives are inherently conflicting and can't maximized at the same time. As a result, direct\nguidance using the target conditions of $y = [\\omega_{target}, 1^n]$ may be unachievable for some preferences,\nleading to a performance drop. To address these issues, we match preferences with vector returns\nand then propose two novel normalization methods that adapt to multi-objective scenarios to achieve\npreference-related return normalization, providing more refined guidance: Preference Predicted\nNormalization and Neighborhood Preference Normalization:\nPreference Predicted Normalization To avoid the issue in Global Normalization, we hope to\nobtain corresponding RTG conditioned on the given preference. The dataset contains a discrete\npreference space and it is not possible to directly obtain the maximum RTG for each preference.\nTherefore, we propose Preference Predicted Normalization training a generalized return predictor\nadditionally $g^{max}(\\omega) = R_\\psi (\\omega)$ to capture the maximum expected return achievable for a given\npreference $\\omega$. Specifically, we first identify all undominated solution trajectories $\\tau_p \\in \\mathcal{D}_p$, where\n$\\mathcal{D}_p \\subset \\mathcal{D}$ and P demotes Pareto front. Then the generalized return predictor is trained by minimizing\nthe return prediction error conditioned on the preference $\\omega$:\n\n$\\mathcal{L}(\\psi) = -E_{(\\omega,\\hat{g}) \\sim \\mathcal{D}_p} [\\vert\\vert R_\\psi(\\omega) - \\hat{g}\\vert\\vert^2] $,\n\nThen we calculate normalized RTG for each trajectory $\\tau$ with preference $\\omega$ as $g_\\tau /g^{max}(\\omega) \\in [0,1]^n$.\nWith RTG prediction, we can follow closer to the training distribution, guiding the generation by\nspecifying different maximum vector returns for each preference.\nNeighborhood Preference Normalization We train $R_\\psi (\\omega)$ using only non-dominated solutions\nto predict the expected maximum RTG. If the quality of the behavior policy in the dataset is relatively\nlow, the non-dominated solutions in the dataset are sparse, which introduces prediction errors that\nresult in inaccurate normalization. Therefore we propose a simpler, learning-free method named\nNeighborhood Preference Normalization. In a linear continuous preference space, similar preferences\ntypically lead to similar behaviors. Therefore, we can use a neighborhood set of trajectories to obtain\nextremum values, avoiding inaccurate prediction. For a trajectory $\\tau$ and its corresponding preference\n$\\omega$, we define $\\epsilon$-neighborhood:\n\n$B_\\epsilon(\\tau) = \\{\\tau' | \\vert\\vert \\omega_{\\tau'} - \\omega_\\tau \\vert\\vert < \\epsilon\\}$\n\nTherefore, we can use the maximum expected return within the neighborhood trajectories to approxi-\nmate the maximum expected return, normalized as follows:\n\n$\\frac{g_\\tau - \\min (g_{B_\\epsilon(\\tau)})}{\\max (g_{B_\\epsilon(\\tau)}) - \\min (g_{B_\\epsilon(\\tau)})} \\in [0, 1]$,\n\n$g_{B_\\epsilon(\\tau)} = \\{g_{\\tau'} | \\tau' \\in B_\\epsilon(\\tau)\\}$.\n\nOverall, we empirically find that this method can achieve consistently good performance across\ndatasets of various qualities. We leave the comparison of the three normalization methods in the\nsection 5.3 and appendix C.\nAction Extraction Now, MODULI can accurately generate desired trajectories with arbitrary\npreference and RTG in the CFG manner according to eq. (5). Then, we train an additional inverse\ndynamics model $a_t = h(s_t, s_{t+1})$ to extract the action $a_t$ to be executed from generated x. It is worth\nnoting that we fixed the initial state as $x^0[0] = x^0[0]$ to enhance the consistency of the generated\nresults."}, {"title": "4.3 Enhancing OOD Preference Generalization with Sliding Guidance", "content": "After using diffusion models with appropriate normalization, MODULI can exhibit remarkable\nexpressive capabilities, fully covering the ID distribution in the dataset. However, when faced with\nOOD preferences, it tends to conservatively generate the trajectory closest to the ID preferences. This\nmay indicate that the diffusion model has overfitted the dataset rather than learning the underlying\ndistribution. To enhance generalization to OOD preferences, inspired by [14] and [11], we propose a\nnovel sliding guidance mechanism. After training the diffusion models $p_\\theta$, We additionally trained a\nplug-and-play slider adapter with the same network structure of diffusion models, which learns the\nlatent direction of preference changes. This enables controlled continuous concept during generation,\nactively adjusting the target distribution, thus avoiding simple memorization. We define the adapter\nas $p_{\\theta^*}$. When conditioned on c, this method boosts the possibility of attribute $c+$ while reduces the\npossibility of attribute c_ according to original model $p_\\theta$:\n\n$p_\\theta(x^0(\\tau) | c) \\leftarrow p_\\theta(x^0(\\tau) | c) \\bigg(\\frac{p_{\\theta^*}(c_+ | x^0(\\tau))}{p_{\\theta^*}(c_- | x^0(\\tau))}\\bigg)^\\eta$\n\nBased on eq. (11), we can derive a direct fine-tuning scheme that is equivalent to modifying the noise\nprediction model. The derivation process is provided in the appendix D:\n\n$\\epsilon_{\\theta^*}(x_t, c,t) \\leftarrow \\epsilon_\\theta(x_t, c,t) + \\eta \\big( \\epsilon_{\\theta^*}(x_t, c_+, t) - \\epsilon_{\\theta^*}(x_t, c_-, t) \\big)$\n\nThe score function proposed in eq. (12) justifies the distribution of the condition c by converging\ntowards the positive direction $c+$ while diverging from the negative direction c_. Therefore, we can let\nthe $\\theta^*$ model capture the direction of preference changes, that is, we hope that the denoising process\nof $\\epsilon(x_t, c, t)$ at each step meets the unit length shift, i.e. $\\epsilon_{\\theta^*}(x_t, c, t) = [\\epsilon_{\\theta^*}(x^*,c_+,t) - \\epsilon_{\\theta^*}(x^*,c_-,t)]$ In\nMORL tasks, the condition c is $\\omega$, and $c^+$ and $c^-$ are the changes in preference $\\Delta\\omega$ in the positive\nand negative directions, respectively. We minimize the loss $\\mathcal{L}(\\theta^*)$ to train the adapter:\n\n$\\mathcal{L}(\\theta^*) = E_{(\\omega,\\Delta\\omega) \\sim \\mathcal{D}} [\\frac{\\vert\\vert \\epsilon_\\theta(x_t,\\omega,t) - [\\epsilon_\\theta(x^*,\\omega+\\Delta\\omega,t) - \\epsilon_\\theta(x^*,\\omega-\\Delta\\omega,t)] \\vert\\vert^2}{2\\Delta\\omega}]$\n\nDuring deployment, when we encounter OOD preferences $\\omega_{OOD}$, We can first transition from the\nnearest ID preference $\\omega_{ID}$. Then, we calculate $\\Delta\\omega = \\omega_{OOD} - \\omega_{ID}$, and at each denoising step,\nsimultaneously apply the diffusion model and adapter as $\\epsilon_\\theta + \\Delta\\omega \\epsilon_{\\theta^*}$, so that the trajectory distribution\ngradually shifts from ID preferences to OOD preferences to achieve better generalization. We provide\nthe pseudo code for the entire training and deployment process in appendix E."}, {"title": "5 Experiments", "content": "We conduct experiments on various Offline MORL tasks to study the following research questions\n(RQs): Expressiveness (RQ1) How does MODULI perform on Complete dataset compared to other\nbaselines? Generalizability (RQ2) Does MODULI exhibit leading generalization performance in\nthe OOD preference of Shattered or Narrow datasets? Normalization (RQ3) How do different\nnormalization methods affect performance? All experimental details of MODULI are in appendix B.\nDatasets and Baselines We evaluate MODULI on D4MORL [50], an Offline MORL benchmark\nconsisting of offline trajectories from 6 multi-objective MuJoCo environments. D4MORL contains\ntwo types of quality datasets: Expert dataset collected by pre-trained expert behavioral policies,\nand Amateur dataset collected by perturbed behavioral policies. We named the original dataset of\nD4MORL as the Complete dataset, which is used to verify expressiveness ability to handle multi-\nobjective tasks. Additionally, to evaluate the algorithm's generalization ability to out-of-distribution\n(OOD) preferences, similar to fig. 1, we collected two types of preference-deficient datasets namely\nShattered and Narrow. We compare MODULI with various strong offline MORL baselines: BC(P),\nMORvS(P) [12], MODT(P) [6] and MOCQL(P) [25]. These algorithms are modified variants of their\nsingle-objective counterparts and well-studied in PEDA [50]. MODT(P) and MORvS(P) view MORL\nas a preference-conditioned sequence modeling problem with different architectures. BC(P) performs"}, {"title": "5.1 Evaluating Expressiveness on Complete Datasets", "content": "We first compare MODULI with various baselines on the Complete datasets. In this dataset version,\nthe trajectories have comprehensive coverage preferences, and the policy needs sufficient expressive\ncapability to cover complex distributions under a wide range of preferences. As shown in table 1,\nmost of the baselines (MOCQL, MOBC, MODT) exhibit sub-optimal performance, fail to achieve\nHV close to behavioral policies, and are prone to sub-preference space collapses that do not allow for\ndense solutions. MORvS can exhibit better HV and SP but fails to achieve performance competitive\nwith behavioral policies due to the poor expressiveness of the context policy. Overall, MODULI\nachieves the most advanced policy performance according to HV, and the most dense solution with\nthe lowest SP. We visualize the empirical Pareto front in fig. 2, demonstrating that MODULI is a good\napproximator under different data qualities. In the expert dataset, MODULI accurately replicates\nthe behavioral policies. In the amateur dataset, due to the inclusion of many suboptimal solutions,\nMODULI significantly expands the Pareto front, demonstrating its ability to stitch and guide the\ngeneration of high-quality trajectories."}, {"title": "5.2 Evaluating Generalization on Shattered and Narrow Datasets", "content": "Performance We evaluate the generalization performance on the Shattered and Narrow datasets.\nTable 2 shows the comparison results on the expert datasets, where MODULI still demonstrates the\nbest performance across all baselines. Specifically, MODULI achieves significant improvements\non the new RD metric, indicating better generalization capability and a better approximation of"}, {"title": "5.3 Comparison of Normalization Methods", "content": "We compared the impact of different return regularization methods on MODULI's performance on\nthe Complete dataset. As shown in table 3, Neighborhood Preference Normalization (NP Norm)\nconsistently has the highest HV performance. On the expert dataset, both NP Norm and PP Norm\nexhibit very similar HV values, as they can accurately estimate the highest return corresponding to\neach preference, whether through prediction or neighborhood analysis. However, on the amateur\ndataset, PP Norm fails to predict accurately, resulting in lower performance. We also noted that the\nnaive method of Global Norm fails to approximate the Pareto front well, but in some datasets, it\nachieved denser solutions (small SP). We believe this is because the Global Norm produced inaccurate\nand homogenized guiding information, leading to different preferences for similar solutions, and\nresulting in dense but low-quality solutions. We refer to appendix C for in-depth visual analysis."}, {"title": "6 Conclusion", "content": "In this paper, we propose MODULI, a diffusion planning framework along with two correspond-\ning multi-objective normalization schemes, enabling preference-return condition-guided trajectory\ngeneration for decision-making. To achieve better generalization in OOD preferences, MODULI\nincludes a sliding guidance mechanism, training an additional slider adapter for active distribution\nadjustment. We conduct extensive experiments on Complete, Shattered, and Narrow types of datasets,\ndemonstrating the superior expressive and generalization capabilities of MODULI. The results show\nthat MODULI is an excellent Pareto front approximator, capable of expanding the Pareto front\nand obtaining advanced and dense solutions. But there are still limitations such as experiments\nbeing conducted only in structured state space and continuous actions. We hope to extend it to\nmulti-objective tasks with image inputs, further enhancing its generalization ability. Additionally,\nnonlinear preference spaces may present new challenges."}, {"title": "A Dataset and Baselines", "content": "A.1 D4MORL Benchmark\nIn this paper, we use the Datasets for Multi-Objective Reinforcement Learning (D4MORL) [50]\nfor experiments, a large-scale benchmark for Offline MORL. This benchmark consists of offline\ntrajectories from 6 multi-objective MuJoCo environments, including 5 environments with 2 objectives\neach, namely MO-Ant, MO-HalfCheetah, MO-Hopper, MO-Swimmer, MO-Walker2d, and one\nenvironment with three objectives: MO-Hopper-3obj. The objectives in each environment are\nconflicting, such as high-speed running and energy saving. In D4MORL, Each environment uses\nPGMORL's [47] behavioral policies for data collection. PGMORL trains a set of policies using\nevolutionary algorithms to approximate the Pareto front, which can map to the closest preference in\nthe preference set for any given preference. Therefore, in D4MORL, two different quality datasets\nare defined: the Expert Dataset, which is collected entirely using the best preference policies in\nPGMORL for decision making, and the Amateur Dataset, which has a probability P of using\nperturbed preference policies and a probability 1 p of being consistent with the expert dataset,\nwith p = 0.65. We can simply consider the Amateur Dataset as a relatively lower quality dataset\nwith random perturbations added to the Expert Dataset. Each trajectory in the D4MORL dataset is\nrepresented as: t = [\u03c9, 80, ao, r0,\u2026\u2026,st,at,r\u012b], where T is episode length.\nA.2 Complete, Shattered and Narrow Datasets\nIn this section, we provide a detailed description of how three different levels of datasets are collected,\nand what properties of the algorithm they are used to evaluate respectively:\n\u2022 Complete datasets i.e. the original version of the D4MORL datasets, cover a wide range of\npreference distributions with fewer OOD preference regions. These datasets primarily evaluate\nthe expressive ability of algorithms and require deriving the correct solution based on diverse\npreferences.\n\u2022 Shattered datasets simulate the scenario where the preference distribution in the dataset is\nincomplete, resulting in areas with missing preferences. This kind of dataset primarily evaluates the\ninterpolation OOD generalization ability of algorithms, aiming for strong generalization capability\nwhen encountering unseen preferences with some lacking preferences. Specifically, the Shattered\ndataset removes part of the trajectories from the Complete dataset, creating n preference-missing\nregions, which account for m% of the trajectories. If the total number of trajectories in the dataset\nis N, then a total of nlack = N * m% trajectories are removed. First, all trajectories in the dataset\nare sorted based on the first dimension of their preferences w, then n points are selected at equal\nintervals according to the sorting results. Centered on these n points, an equal number of nlack/n\ntrajectories around each point are uniformly deleted. In this paper, m, n are fixed to 30, and 3,\nrespectively.\n\u2022 The role of Narrow datasets is similar to Shattered datasets but mainly evaluates the extrapolation\nOOD generalization ability of algorithms. If we need to remove a total of m% of the trajectories,\nwe can obtain the Narrow dataset with a narrow preference distribution by removing the same\namount, m/2%, from both ends of the sorted Complete dataset. In this paper, m is fixed to 30.\nA.3 Baselines\nIn this paper, we compare with various strong baselines: MODT(P) [6], MORvS(P) [12], BC(P),\nand MOCQL(P) [25], all of which are well-studied baselines under the PEDA [50], representing\ndifferent paradigms of learning methods. All baselines were trained using the default hyperparameters\nand official code as PEDA. MODT(P) extends DT architectures to include preference tokens and\nvector-valued returns. Additionally, MODT(P) also concatenates w to state tokens [s, w] and action\ntokens [a, w] for training. MORvS(P) and MODT(P) are similar, extending the MLP structure\nto adapt to MORL tasks. MORvS(P) concatenates the preference with the states and the average\nRTGs by default as one single input. BC(P) serves as an imitation learning paradigm to train a\nmapping from states with preference to actions. BC(P) simply uses supervised loss and MLPs as\nmodels. MOCQL(P) represents the temporal difference learning paradigm, which learns a preference-\nconditioned Q-function. Then, MOCQL(P) uses underestimated Q functions for actor-critic learning,\nalleviating the OOD phenomenon in Offline RL."}, {"title": "A.4 Evaluation Metrics", "content": "In MORL", "below": "nDefinition 1 (Hypervolume (HV)). Hypervolume H(P) measures the space enclosed by the solutions\nin the Pareto front P:\n\n$H(P) = \\int_{\\mathbb{R"}, "n} 1_{H(P)}(z)dz$,\n\nwhere H(P) = $\\{z \\in \\mathbb{R}^n\\vert\\exists i : 1 \\leq i \\leq |P|,r_0 \\preccurlyeq z \\preccurly"]}