{"title": "MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning", "authors": ["Yifu Yuan", "Zhenrui Zheng", "Zibin Dong", "Jianye Hao"], "abstract": "Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi Objective DiffUsion planner with sLIding guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.", "sections": [{"title": "1 Introduction", "content": "Real-world decision-making tasks, such as robotic control [47, 29], autonomous driving [26], and industrial control [4], often require the optimization of multiple competing objectives simultaneously. It necessitates the trade-offs among different objectives to meet diverse preferences [39]. For instance, in robotic locomotion tasks, users typically focus on the robot's movement speed and energy consumption [47]. If the user has a high preference for speed, the agent will move quickly regardless of energy consumption; if the user aims to save energy, the agent should adjust to a lower speed. One effective approach is Multi-objective Reinforcement Learning (MORL), which enables agents to interact with vector-rewarded environments and learn policies that satisfy multiple preferences [17, 38, 48]. These methods either construct a set of policies to approximate the Pareto front of optimal solutions [2, 13] or learn a preference-conditioned policy to adapt to any situation [33, 21]. However, exploring and optimizing policies for each preference while alleviating conflicts among objectives, requires extensive online interactions [3], thereby posing practical challenges due to high costs and potential safety risks.\nOffline MORL [50] proposes a promising solution that can learn preference-conditioned policies from pre-collected datasets with various preferences, improving data efficiency and minimizing interactions when deploying in high-risk environments. Most Offline MORL approaches focus on extending return-conditioned methods [43, 50] or encouraging consistent preferences through policy regularization [28]. However, real-world offline datasets often come from users with different behavior policies, making it difficult to cover the full range of preferences and leading to missing preference regions, i.e., OOD preferences, resulting in a shattered or narrow data distribution. Therefore, we aim to derive a general multi-objective policy that can model a wide range of preference conditions with sufficient expressiveness and strong generalization to OOD preferences. Motivated by the remarkable capabilities of diffusion models in expressiveness and generalization, we propose MODULI (Multi Objective DiffUsion planner with sLIding guidance), transforming the problem of Offline MORL into conditional generative planning. MODULI demonstrated a better approximation of the Pareto front, achieving returns beyond the dataset. MODULI also patched the missing preference regions and exhibited excellent generalization for the OOD preferences.\nMODULI constructs a multi-objective conditional diffusion planning framework that generates long-term trajectories aligned with preference and then executes as a planner. Furthermore, appropriate guiding conditions play a crucial role in the precise generation, which requires the assessment of trajectory quality under different preferences for normalization. We found that simply extending the single-objective normalization method to multi-objective causes issues, as it fails to accurately align preferences with achievable high returns. Therefore, we propose two return normalization methods adapted to multi-objective tasks, which can better measure the quality of trajectories under different preferences and achieve a more refined guidance process. Besides, to enhance generalization to OOD preferences, we introduce a sliding guidance mechanism that involves training an additional slider adapter with the same network structure to independently learn the latent directions of preference changes. During generation, we adjust the slider to actively modify the target distribution, gradually transitioning from ID to specified OOD preferences. This better captures the fine-grained structure of the data distribution and allows for finer control, thereby achieving better OOD generalization. In summary, our contributions include (i) a conditional diffusion planning framework named MODULI, which models general multi-objective policies as conditional generative models, (ii) two preference-return normalization methods suitable for multi-objective preferences to refine guidance, (iii) a novel sliding guidance mechanism in the form of an adapter and, (iv) superior performance in D4MORL, particularly impressive generalization to OOD preferences."}, {"title": "2 Related Work", "content": "Offline MORL Most MORL research primarily focuses on online settings, aiming to train a general preference-conditioned policy [21, 32] or a set of single-objective policies [35, 47] that can generalize to arbitrary preferences. This paper considers an offline MORL setting that learns policy from static datasets without online interactions. PEDI [45] combines the dual gradient ascent with pessimism to multi-objective tabular MDPs. MO-SPIBB [43] achieves safe policy improvement within a predefined set of policies under constraints. These methods often require a priori knowledge about target preferences. PEDA [50] first develops D4MORL benchmarks for scalable Offline MORL with a general preference for high-dimensional MDPs with continuous states and actions. It also attempts to solve the Offline MORL via supervised learning, expanding return-conditioned sequential modeling to the MORL, such as MORvS [12], and MODT [6]. [28] integrates policy-regularized into the MORL methods to alleviate the preference-inconsistent demonstration problem. This paper investigates the generalization problem of OOD preference, aiming to achieve better generalization through more expressive diffusion planners and the guidance of transition directions by a slider adapter.\nDiffusion Models for Decision Making Diffusion models (DMs) [19, 42] have become a mainstream class of generative models. Their remarkable capabilities in complex distribution modeling demonstrate outstanding performance across various domains [24, 40], inspiring research to apply DMs to decision-making tasks [7, 49, 46]. Applying DMs for decision-making tasks can mainly be divided into three categories [51]: generating long horizon trajectories like planner [22, 9, 1], serving as more expressive multimodal polices [44, 16] and performing as the synthesizers for data augmentation [31]. Diffusion models exhibit excellent generalization and distribution matching abilities [23, 27], and this property has also been applied to various types of decision-making tasks such as multi-task RL [18], meta RL [36] and aligning human feedback [10]. In this paper, we further utilize sliding guidance to better stimulate the generalization capability of DMs, rather than merely fitting the distribution by data memorization [15]."}, {"title": "3 Preliminaries", "content": "Multi-objective RL The MORL problem can be formulated as a multi-objective Markov de-cision process (MOMDP) [5]. A MOMDP with n objectives can be represented by a tuple (S, A, P, R, \u03a9, \u03b3), where S and A demote the state and action spaces, P is the transition function, R : S \u00d7 A \u2192 R^n is the reward function that outputs n-dim vector rewards r = R(s,a), \u03a9 is the preference space containing vector preferences w, and \u03b3 is a discount factor. The policy \u03c0 is evaluated for m distinct preferences {p_i}_{i=1}^m, resulting policy set be represented as {\u03c0_{\u03c9_p}}_i=1^m, where \u03c0_\u03c1 = \u03c0(a|s, \u03c9_\u03c1), and G^{\u03c0_p} = E_{a_t~\u03c0_\u03c1} [\u2211_t \u03b3^tR(s_t, a_t)] is the corresponding unweighted expected return. We say the solution G^{\u03c0_p} is dominated by G^{\u03c0_q} when G^{\u03c0_p} < G^{\u03c0_q} for \u2200i \u2208 [1,2,\u2026\u2026,n]. The Pareto front P of the policy \u03c0 contains all its solutions that are not dominated. In MORL, we aim to define a policy such that its empirical Pareto front is a good approximation of the true one. While not knowing the true front, we can define a set of metrics for relative comparisons among algorithms, e.g., hypervolume, sparsity, and return deviation, to which we give a further explanation in Section 5.\nDenoising Diffusion Implicit Models (DDIM) Assume the random variable x^0 \u2208 R^D follows an unknown distribution q_0(x^0). DMs define a forward process {x_t}_{t\u2208[0,T]} by the noise schedule {\u03b1_t, \u03c3_t}_{t\u2208[0,T]}, s.t., \u2200t \u2208 [0, T]:\n$dx_t = f(t)x_t dt + g(t)dw_t, x^0 ~ q_0(x^0),$\nwhere w_t \u2208 R^D is the standard Wiener process, and $f(t) = \\frac{d log \\alpha_t}{dt}, g^2(t) = -2\\sigma_t^2 \\frac{d\\sigma_t}{dt} - 2 \\sigma_t^2 \\frac{d log \\alpha_t}{dt}$. The SDE forward process in Equation (1) has an probability flow ODE (PF-ODE) reverse process from time T to 0 [42]:\n$\\frac{dx_t}{dt} = f(t)x_t - g^2(t) \\nabla_{x_t} log q_t(x_t), x_T ~ q_T(x_T),$"}, {"title": "4 Methodology", "content": "To address the challenges of expressiveness and generalization in Offline MORL, we propose a novel Offline MORL framework called MODULI. This framework utilizes conditional diffusion models for preference-oriented trajectory generation and establishes a general policy suitable for a wide range of preferences while also generalizing to OOD preferences. MODULI first defines a conditional generation and planning framework and corresponding preference-return normalization methods. Moreover, to enhance generalization capability, we design a sliding guidance mechanism, where a slider adapter actively adjusts the data distribution from known ID to specific OOD preference, thereby avoiding simple memorization."}, {"title": "4.1 Offline MORL as Conditional Generative Planning", "content": "We derive policies on the Offline MORL dataset D with trajectories \u03c4 = (\u03c9, s_0, a_0, r_0, \u2026\u2026\u2026, s_{T-1}, a_{T-1}, r_{T-1}) to respond to arbitrary target preferences w \u2208 \u03a9 during the deployment phase. Previous Offline MORL [50] algorithms generally face poor expressiveness, unable to adapt to a wide range of preferences through a general preference-conditioned policy, resulting in sub-optimal approximate Pareto front. Inspired by the excellent expressiveness of diffusion models in decision making [1, 10], MODULI introduces diffusion models to model the policy. We define the sequential decision-making problem as a conditional generative objective max E_{\u03c4~D} [log p_\u03b8(x^0(\u03c4) | y(\u03c4))], where \u03b8 is the trainable parameter. The diffusion planning in MORL aims to generate sequential states x^0(\u03c4) = [s_0,\u2026\u2026, s_{H-1}] with horizon H to satisfy the denoising conditions y = [w, g], where w is a specific preference. Similar to [50], g_t is defined as the vector-valued Returns-to-Go (RTG) for a single step, i.e. $g_t = \\sum_{t=t}^T R(s_t, a_t)$. The trajectory RTG g is the average of the RTGs at each step.\nTraining Next, we can sample batches of x^0(\u03c4) and y from the dataset and update p_\u03b8 as the modified score matching objective in eq. (3):\n$L(\\theta) = E_{(x^0,w,g)~D,t~U(0,T),\u03b5~N(0,I)} [||\u03b5_\u03b8(x_t, t, w, g) \u2013 \u03b5||^2].$\nMODULI uses the CFG guidance for conditional generation. During training, MODULI learns both a conditioned noise predictor \u03b5_\u03b8(x_t,t,w, g) and an unconditioned one \u03b5_\u03b8(x_t, t, \u00d8). We adopt a masking mechanism for training, with a probability of 0 < p < 1 to zero out the condition of a trajectory, equivalent to an unconditional case. MODULI also employs a loss-weight trick, setting a higher weight for the next state s_1 of the trajectory x^0 (\u03c4) to encourage more focus on it, closely related to action execution."}, {"title": "4.2 Refining Guidance via Preference-Return Normalization Methods", "content": "Global Normalization To perform generation under different scales and physical quantities of objectives, it is essential to establish appropriate guiding conditions, making return normalization a critical component. We first consider directly extending the single-objective normalization scheme to the multi-objective case, named Global Normalization. We normalize each objective's RTG using min-max style, scaling them to a unified range. Specifically, we calculate the global maximum RTG $g^{max} = [g^{max}_1,..., g^{max}_n]$ for each objective in the offline dataset. The same applies to the minimum RTG $g^{min}$. The conditioned RTG g are first normalized to $(g \u2013 g^{min})/(g^{max} \u2013 g^{min}) \u2208 [0, 1]^n$, then concatenated with the preference w as inputs. During deployment, we aim to generate x(\u03c4) to align the given preference \u03c9_{target} and maximized scalarized return using the highest RTG condition g = 1^n. Note that we compute the global extremum values based on the offline dataset with different preferences, which are not necessarily the reachable min/max value for each objective. Also, some objectives are inherently conflicting and can't maximized at the same time. As a result, direct guidance using the target conditions of y = [\u03c9_{target}, 1^n] may be unachievable for some preferences, leading to a performance drop. To address these issues, we match preferences with vector returns and then propose two novel normalization methods that adapt to multi-objective scenarios to achieve preference-related return normalization, providing more refined guidance: Preference Predicted Normalization and Neighborhood Preference Normalization:\nPreference Predicted Normalization To avoid the issue in Global Normalization, we hope to obtain corresponding RTG conditioned on the given preference. The dataset contains a discrete preference space and it is not possible to directly obtain the maximum RTG for each preference. Therefore, we propose Preference Predicted Normalization training a generalized return predictor additionally $g^{max}(\u03c9) = R_\u03c8(\u03c9)$ to capture the maximum expected return achievable for a given preference w. Specifically, we first identify all undominated solution trajectories \u03c4_p \u2208 D_p, where D_p \u2282 D and P demotes Pareto front. Then the generalized return predictor is trained by minimizing the return prediction error conditioned on the preference w:\n$L(\\psi) = \u2212E_{(w,\u011d)~D_p} [||R_\u03c8(w) \u2013 \u011d||^2],$\nThen we calculate normalized RTG for each trajectory \u03c4 with preference w as $g_\u03c4/g^{max}(w) \u2208 [0,1]^n$. With RTG prediction, we can follow closer to the training distribution, guiding the generation by specifying different maximum vector returns for each preference.\nNeighborhood Preference Normalization We train $R_\u03c8(w)$ using only non-dominated solutions to predict the expected maximum RTG. If the quality of the behavior policy in the dataset is relatively low, the non-dominated solutions in the dataset are sparse, which introduces prediction errors that result in inaccurate normalization. Therefore we propose a simpler, learning-free method named Neighborhood Preference Normalization. In a linear continuous preference space, similar preferences typically lead to similar behaviors. Therefore, we can use a neighborhood set of trajectories to obtain extremum values, avoiding inaccurate prediction. For a trajectory \u03c4 and its corresponding preference w, we define \u03b5-neighborhood:\n$B_\u03b5(\u03c4) = {\u03c4' | ||\u03c9_\u03c4' \u2013 \u03c9_\u03c4|| < \u03b5}$ \nTherefore, we can use the maximum expected return within the neighborhood trajectories to approximate the maximum expected return, normalized as follows:\n$\\bar{g_\u03c4} = \\frac{g_\u03c4 - min (g_{B_\u03b5(\u03c4)})}{max (g_{B_\u03b5(\u03c4)}) - min (g_{B_\u03b5(\u03c4)})} \u2208 [0, 1],$\n$g_{B_\u03b5(\u03c4)} = {g_{\u03c4'} | \u03c4' \u2208 B_\u03b5(\u03c4)}.$\nOverall, we empirically find that this method can achieve consistently good performance across datasets of various qualities. We leave the comparison of the three normalization methods in the section 5.3 and appendix C.\nAction Extraction Now, MODULI can accurately generate desired trajectories with arbitrary preference and RTG in the CFG manner according to eq. (5). Then, we train an additional inverse dynamics model $a_t = h(s_t, s_{t+1})$ to extract the action a_t to be executed from generated x. It is worth noting that we fixed the initial state as $\\hat{x}[0] = x[0]$ to enhance the consistency of the generated results."}, {"title": "4.3 Enhancing OOD Preference Generalization with Sliding Guidance", "content": "After using diffusion models with appropriate normalization, MODULI can exhibit remarkable expressive capabilities, fully covering the ID distribution in the dataset. However, when faced with OOD preferences, it tends to conservatively generate the trajectory closest to the ID preferences. This may indicate that the diffusion model has overfitted the dataset rather than learning the underlying distribution. To enhance generalization to OOD preferences, inspired by [14] and [11], we propose a novel sliding guidance mechanism. After training the diffusion models p_\u03b8, We additionally trained a plug-and-play slider adapter with the same network structure of diffusion models, which learns the latent direction of preference changes. This enables controlled continuous concept during generation, actively adjusting the target distribution, thus avoiding simple memorization. We define the adapter as p_{\u03b8^*}. When conditioned on c, this method boosts the possibility of attribute c+ while reduces the possibility of attribute c_ according to original model p_\u03b8:\n$p_\u03b8(x^0(\u03c4) | c) \u2190 p_\u03b8(x^0(\u03c4) | c) (\\frac{p_{\u03b8^*}(c_+ | x^0(\u03c4))}{p_{\u03b8^*}(c_- | x^0(\u03c4))})^\u03b7$\nBased on eq. (11), we can derive a direct fine-tuning scheme that is equivalent to modifying the noise prediction model. The derivation process is provided in the appendix D:\n$\u03b5_{\u03b8^\u2217}(x_t, c,t) \u2190 \u03b5_\u03b8(x_t, c,t) + \u03b7(\\frac{\u03b5_\u03b8(x_t, c_+, t) \u2013 \u03b5_\u03b8(x_t,c_-, t)}{c_+-c_-})$\nThe score function proposed in eq. (12) justifies the distribution of the condition c by converging towards the positive direction c+ while diverging from the negative direction c_. Therefore, we can let the \u03b8^* model capture the direction of preference changes, that is, we hope that the denoising process of \u03b5(x_t, c, t) at each step meets the unit length shift, i.e. $\u03b5_{\u03b8^*}(x_t, c, t) = [\u03b5_\u03b8(x_t, c_+, t) \u2013 \u03b5_\u03b8(x_t, c_-, t)]$ In MORL tasks, the condition c is w, and c\u207a and c\u00af are the changes in preference \u0394\u03c9 in the positive and negative directions, respectively. We minimize the loss L(\u03b8*) to train the adapter:\n$L(\\theta^*) = E_{(w,\u0394w)~D} [|\u03b5_{\u03b8^*}(x_t, w,t) - \\frac{[\u03b5_\u03b8(x_t, \u03c9 + \u0394\u03c9,t) \u2013 \u03b5_\u03b8(x_t,\u03c9 \u2013 \u0394\u03c9,t)]}{2\u0394\u03c9} |^2].$\nDuring deployment, when we encounter OOD preferences \u03c9_{OOD}, We can first transition from the nearest ID preference \u03c9_{ID}. Then, we calculate \u0394\u03c9 = \u03c9_{OOD} \u2013 \u03c9_{ID}, and at each denoising step, simultaneously apply the diffusion model and adapter as \u03b5_\u03b8 + \u0394\u03c9\u03b5_{\u03b8^*}, so that the trajectory distribution gradually shifts from ID preferences to OOD preferences to achieve better generalization. We provide the pseudo code for the entire training and deployment process in appendix E."}, {"title": "5 Experiments", "content": "We conduct experiments on various Offline MORL tasks to study the following research questions (RQs): Expressiveness (RQ1) How does MODULI perform on Complete dataset compared to other baselines? Generalizability (RQ2) Does MODULI exhibit leading generalization performance in the OOD preference of Shattered or Narrow datasets? Normalization (RQ3) How do different normalization methods affect performance? All experimental details of MODULI are in appendix B.\nDatasets and Baselines We evaluate MODULI on D4MORL [50], an Offline MORL benchmark consisting of offline trajectories from 6 multi-objective MuJoCo environments. D4MORL contains two types of quality datasets: Expert dataset collected by pre-trained expert behavioral policies, and Amateur dataset collected by perturbed behavioral policies. We named the original dataset of D4MORL as the Complete dataset, which is used to verify expressiveness ability to handle multi-objective tasks. Additionally, to evaluate the algorithm's generalization ability to out-of-distribution (OOD) preferences, similar to fig. 1, we collected two types of preference-deficient datasets namely Shattered and Narrow. We compare MODULI with various strong offline MORL baselines: BC(P), MORvS(P) [12], MODT(P) [6] and MOCQL(P) [25]. These algorithms are modified variants of their single-objective counterparts and well-studied in PEDA [50]. MODT(P) and MORvS(P) view MORL as a preference-conditioned sequence modeling problem with different architectures. BC(P) performs imitation learning through supervised loss, while MOCQL(P) trains a preference-conditioned Q-function based on temporal difference learning. Further details of datasets and baselines are described in appendix A."}, {"title": "Metrics", "content": "We use two commonly used metrics HV and SP for evaluating the empirical Pareto front and an additional metric RD for evaluating the generalization of OOD preference regions: Hypervolume (HV) H(P) measures the space enclosed by solutions in the Pareto front P: $H(P) = \\int_{R^n} 1_{H(P)}(z)dz$, where $H(P) = {z \u2208 R^n|\u2203i : 1 \u2264 i \u2264 |P|,r_0 < z < P(i)}$, r0 is a reference point determined by the environment, < is the dominance relation operator and $1_{H(P)}$ equals 1 if $z \u2208 H(P)$ and 0 otherwise. Higher HV is better, implying empirical Pareto front expansion. Sparsity (SP) S(P) measures the density of the Pareto front P: $S(P) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{|P|-1} \\sum_{k=1}^{|P|-1} (P_i(k) - P_i(k + 1))^2$, where $P_i$ represents the sorted list of values of the i-th target in P and $P_i(k)$ is the k-th value in $P_i$. Lower SP, indicating a denser approximation of the Pareto front, is better. Return Deviation (RD) R(O) measures the discrepancy between the solution set O in the out-of-distribution preference region and the maximum predicted value: $R(O) = \\frac{1}{|O|} \\sum_{k=1}^{|O|} ||O(k) \u2013 R_\u03c8(\u03c9_{O(k)})||^2_2$, where $R_\u03c8$ is a generalized return predictor to predict the maximum expected return achievable for a given preference w. To accurately evaluate RD, we consistently train this network R_\u03c8 using the Complete-Expert dataset. In this way, for any preference of a missing dataset, we can predict and know the oracle's solution. Please note that R_\u03c8 here is only used to evaluate algorithm performance and is unrelated to the Preference Predicted Normalization method in MODULI. \u03c9_{0(k)} is the corresponding preference of O(k). A lower RD is better, as it indicates that the performance of the solution is closer to the predicted maximum under OOD preference, suggesting better generalization ability."}, {"title": "5.1 Evaluating Expressiveness on Complete Datasets", "content": "We first compare MODULI with various baselines on the Complete datasets. In this dataset version, the trajectories have comprehensive coverage preferences, and the policy needs sufficient expressive capability to cover complex distributions under a wide range of preferences. As shown in table 1, most of the baselines (MOCQL, MOBC, MODT) exhibit sub-optimal performance, fail to achieve HV close to behavioral policies, and are prone to sub-preference space collapses that do not allow for dense solutions. MORvS can exhibit better HV and SP but fails to achieve performance competitive with behavioral policies due to the poor expressiveness of the context policy. Overall, MODULI achieves the most advanced policy performance according to HV, and the most dense solution with the lowest SP. We visualize the empirical Pareto front in fig. 2, demonstrating that MODULI is a good approximator under different data qualities. In the expert dataset, MODULI accurately replicates the behavioral policies. In the amateur dataset, due to the inclusion of many suboptimal solutions, MODULI significantly expands the Pareto front, demonstrating its ability to stitch and guide the generation of high-quality trajectories."}, {"title": "5.2 Evaluating Generalization on Shattered and Narrow Datasets", "content": "Performance We evaluate the generalization performance on the Shattered and Narrow datasets. Table 2 shows the comparison results on the expert datasets, where MODULI still demonstrates the best performance across all baselines. Specifically, MODULI achieves significant improvements on the new RD metric, indicating better generalization capability and a better approximation of the Pareto front in OOD preference regions. We further visualize the powerful OOD preference generalization ability of MODULI.\nOOD Generalization Visualization In fig. 3, we visualize the actual solutions obtained under OOD preferences. Gray points represent the trajectories in the dataset, where the preference of the dataset is incomplete. We observe that BC(P) exhibits policy collapse when facing OOD preferences, showing extremely low returns and a mismatch with the preferences. RvS(P) demonstrates limited generalization ability, but most solutions still do not meet the preference requirements. Only MODULI can produce correct solutions even when facing OOD preferences. In the Shattered dataset, MODULI can patch partially OOD preference regions, while in the Narrow dataset, MODULI can significantly extend the Pareto front to both sides.\nAblation of Sliding Guidance We conducted an ablation study of the sliding guidance mechanism on 9 tasks within Narrow datasets. The results are presented in the radar chart in fig. 4. We found that in most tasks, MODULI with slider exhibits higher HV and lower RD, indicating leading performance in MO tasks. At the same time, the presence or absence of the slider has little impact on SP performance, indicating that sliding guidance distribution does not impair the ability to express the ID preferences. Besides, we visualized the empirical Pareto front in fig. 5 for Hopper-Narrow dataset. The sliding bootstrap mechanism significantly extends the Pareto set of OOD preference regions, exhibiting better generalization. Due to space limits, we provide more comparative experiments on the different guidance mechanisms (CG, CG+CFG) in appendix G."}, {"title": "5.3 Comparison of Normalization Methods", "content": "We compared the impact of different return regularization methods on MODULI's performance on the Complete dataset. As shown in table 3, Neighborhood Preference Normalization (NP Norm) consistently has the highest HV performance. On the expert dataset, both NP Norm and PP Norm exhibit very similar HV values, as they can accurately estimate the highest return corresponding to each preference, whether through prediction or neighborhood analysis. However, on the amateur dataset, PP Norm fails to predict accurately, resulting in lower performance. We also noted that the naive method of Global Norm fails to approximate the Pareto front well, but in some datasets, it achieved denser solutions (small SP). We believe this is because the Global Norm produced inaccurate and homogenized guiding information, leading to different preferences for similar solutions, and resulting in dense but low-quality solutions. We refer to appendix C for in-depth visual analysis."}, {"title": "6 Conclusion", "content": "In this paper, we propose MODULI, a diffusion planning framework along with two corresponding multi-objective normalization schemes, enabling preference-return condition-guided trajectory generation for decision-making. To achieve better generalization in OOD preferences, MODULI includes a sliding guidance mechanism, training an additional slider adapter for active distribution adjustment. We conduct extensive experiments on Complete, Shattered, and Narrow types of datasets, demonstrating the superior expressive and generalization capabilities of MODULI. The results show that MODULI is an excellent Pareto front approximator, capable of expanding the Pareto front and obtaining advanced and dense solutions. But there are still limitations such as experiments being conducted only in structured state space and continuous actions. We hope to extend it to multi-objective tasks with image inputs, further enhancing its generalization ability. Additionally, nonlinear preference spaces may present new challenges."}, {"title": "A Dataset and Baselines", "content": "In this paper, we use the Datasets for Multi-Objective Reinforcement Learning (D4MORL) [50] for experiments, a large-scale benchmark for Offline MORL. This benchmark consists of offline trajectories from 6 multi-objective MuJoCo environments, including 5 environments with 2 objectives each, namely MO-Ant, MO-HalfCheetah, MO-Hopper, MO-Swimmer, MO-Walker2d, and one environment with three objectives: MO-Hopper-3obj. The objectives in each environment are conflicting, such as high-speed running and energy saving. In D4MORL, Each environment uses PGMORL's [47] behavioral policies for data collection. PGMORL trains a set of policies using evolutionary algorithms to approximate the Pareto front, which can map to the closest preference in the preference set for any given preference. Therefore, in D4MORL, two different quality datasets are defined: the Expert Dataset, which is collected entirely using the best preference policies in PGMORL for decision making, and the Amateur Dataset, which has a probability P of using perturbed preference policies and a probability 1 p of being consistent with the expert dataset, with p = 0.65. We can simply consider the Amateur Dataset as a relatively lower quality dataset with random perturbations added to the Expert Dataset. Each trajectory in the D4MORL dataset is represented as: t = [\u03c9, 80, ao, r0,\u2026\u2026,st,at,r\u012b], where T is episode length."}, {"title": "A.2 Complete, Shattered and Narrow Datasets", "content": "In this section, we provide a detailed description of how three different levels of datasets are collected, and what properties of the algorithm they are used to evaluate respectively:\n\u2022 Complete datasets i.e. the original version of the D4MORL datasets, cover a wide range of preference distributions with fewer OOD preference regions. These datasets primarily evaluate the expressive ability of algorithms and require deriving the correct solution based on diverse preferences.\n\u2022 Shattered datasets simulate the scenario where the preference distribution in the dataset is incomplete, resulting in areas with missing preferences. This kind of dataset primarily evaluates the interpolation OOD generalization ability of algorithms, aiming for strong generalization capability when encountering unseen preferences with some lacking preferences. Specifically, the Shattered dataset removes part of the trajectories from the Complete dataset, creating n preference-missing regions, which account for m% of the trajectories. If the total number of trajectories in the dataset is N, then a total of $n_{lack} = N * m%$ trajectories are removed. First, all trajectories in the dataset are sorted based on the first dimension of their preferences w, then n points are selected at equal intervals according to the sorting results. Centered on these n points, an equal number of $n_{lack}/n$ trajectories around each point are uniformly deleted. In this paper, m, n are fixed to 30, and 3, respectively.\n\u2022 The role of Narrow datasets is similar to Shattered datasets but mainly evaluates the extrapolation OOD generalization ability of algorithms. If we need to remove a total of m% of the trajectories, we can obtain the Narrow dataset with a narrow preference distribution by removing the same amount, m/2%, from both ends of the sorted Complete dataset. In this paper, m is fixed to 30."}, {"title": "A.3 Baselines", "content": "In this paper, we compare with various strong baselines: MODT(P) [6], MORvS(P) [12], BC(P), and MOCQL(P) [25], all of which are well-studied baselines under the PEDA [50], representing different paradigms of learning methods. All baselines were trained using the default hyperparameters and official code as PEDA. MODT(P) extends DT architectures to include preference tokens and vector-valued returns. Additionally, MODT(P) also concatenates w to state tokens [s, w] and action tokens [a, w] for training. MORvS(P) and MODT(P) are similar, extending the MLP structure to adapt to MORL tasks. MORvS(P) concatenates the preference with the states and the average RTGs by default as one single input. BC(P) serves as an imitation learning paradigm to train a mapping from states with preference to actions. BC(P) simply uses supervised loss and MLPs as models. MOCQL(P) represents the temporal difference learning paradigm, which learns a preference-conditioned Q-function. Then, MOCQL(P) uses underestimated Q functions for actor-critic learning, alleviating the OOD phenomenon in Offline RL."}, {"title": "A.4 Evaluation Metrics", "content": "In MORL", "below": "nDefinition 1 (Hypervolume (HV)). Hypervolume H(P) measures the space enclosed by the solutions in the Pareto front P:\n$H(P) = \\int_{R^n"}, 1, {"R^n|\u2203i": 1, "P": "n$S(P) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{|P|-1} \\sum_{k=1}^{|P|-1} (P_i(k) - P_i(k + 1))^2,$\nwhere $P_i$ represents the sorted list of values of the i-th target in P and $"}]}