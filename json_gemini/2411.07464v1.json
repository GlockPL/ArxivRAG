{"title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks", "authors": ["Shubham Gandhi", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "abstract": "Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.", "sections": [{"title": "1 INTRODUCTION", "content": "Although recent advances have shown that Large Language Models (LLMs) are adept at handling a vast array of applications ranging from natural language [8, 15, 40, 50] to code-related tasks [42, 47, 48], this capability does not often translate to more complicated and nuanced tasks [39]. Most code-related efforts involving LLMs [10, 14, 49] are based on tasks such as HumanEval [6] and MBXP [3], that have a relatively easier level of complexity that is far from what is experienced by data scientists. However, real-world engineering challenges demand nuanced problem-solving and intricate planning, often involving multiple rounds of strategizing, experimentation, and recalibration. LLM agent systems excel in simulating this iterative process, since they comprise of an environment containing code files, description files and data files and a pre-defined action space allowing interaction with the environment. This demonstrates their capability to address intricate engineering challenges effectively. [45].\nTransitioning to codifying Machine Learning (ML) applications brings its own challenges since they often involve training models on datasets, tuning hyperparameters, devising ways to improve performance, etc. These applications are not straightforward and require a deep understanding of the underlying algorithms and techniques along with specific libraries used for implementation of plans. Although there exist AutoML-based approaches for automating such tasks [11, 28], these offer limited flexibility since they typically operate within predefined constraints and search spaces in the form of possible configurations of architectures and/or hyper-parameters, which may limit their ability to explore solutions out-of-distribution of the search space. While works such as ChatDev [26] and MetaGPT [12] have explored the capabilities of LLM Agents in a software development environment, there is a notable scarcity of research on utilizing LLM Agents for solving ML tasks.\nRecent works like MLCopilot [46] introduce an assistant for solving ML tasks. However, these architectures are limited in the types of problems they can address and must strictly follow task"}, {"title": "2 MLAGENTBENCH DATASET", "content": "MLAgentBench [16] is a dataset designed for evaluating LLM Agents for Machine Learning (ML) tasks. ML tasks defined within the MLA-gentBench dataset are specified with clarity, providing a concise description of the desired objective, evaluation metric, and submission guidelines. These task descriptions are diverse and not restricted to a particular format, allowing for a broad range of task definitions. For example, tasks involve improving model accuracy on a given dataset or optimizing a specific performance metric. The dataset also provides necessary files containing training and testing data, along with detailed descriptions of the data and metrics. Starter code, implemented across diverse ML frameworks like PyTorch, TensorFlow, JAX, and Keras, is provided to assist agents in getting started. While some tasks (cifar10, ogbn-arxiv, etc) offer baseline implementations for comparison, others (imdb, house-price, etc) require agents to code models from scratch based on the provided specifications and dataset files.\nIn the MLAgentBench framework, each task represents an environment where agents interact by performing actions and receiving observations. The benchmark offers a set of primitive low-level actions, including file system operations (For example, list files, read, write, append, copy, etc), executing Python scripts, and declaring final answers. Additionally, there also exist high-level actions such as understanding a file, reflection (looking over past steps and contemplating based on the given description of what to reflect on), inspecting a segment of a file and editing a script (or a script segment). High-level actions may call some low-level actions or LLMs internally (for example understand file action might result in file contents being passed to an LLM and asking it to understand the contents.). Each action is accompanied by comprehensive documentation, specifying its name, description, usage guidelines, expected return values, and implementation. These actions enable LLM agent to manipulate files, execute scripts, and declare final outcomes within the task environment, facilitating iterative problem-solving and evaluation.\nWe consider a subset of MLAgentBench dataset. We do not take two tasks in to consideration, viz. fathomnet [37] and identify-contrails [24], due to our compute restrictions which can not handle the extremely large size of the datasets corresponding to these tasks. We also drop the LLM Tools tasks from MLAgentBench, as it does not have any comparable results for evaluating our system. Thus,"}, {"title": "3 BUDGETMLAGENT", "content": "Our proposed system, BudgetMLAgent is an LLM Multi-Agent system that primarily uses no-cost LLMs and incorporates several enhancements viz. (i) LLM Profiling, (ii) LLM Cascades and (iii) Ask-the-expert lifelines."}, {"title": "3.1 Multi-Agent LLM Profiling", "content": "Our system capitalizes on the groundwork laid by MLAgentBench [16] that provides a straightforward single LLM Agent based solution for the tasks. It operates through an organized prompt-response based interaction system by an agent that uses a set of available actions to interact with the environment. Through carefully structured prompts, they aim to ensure clarity and precision in conveying task descriptions, available tools (possible set of actions), and most recent steps taken, to enhance the agent's decision-making process. To emphasize thoughtful decision-making during planning, the LLM is instructed to stick to a structured format for providing responses to the aforementioned structured prompts, including elements such as 'Reflection' on understanding the prior observations, an updatable 'Implementation Plan' and step-wise 'Status', 'Fact check' on if the objective statements from the Plan and Status guessed or directly confirmed and 'Thought' on the action to be performed with justification and reasoning. This should be followed by the proposed 'Action' for the next step along with the corresponding 'Action Inputs' in JSON format. This structured response format is aimed at enhancing the agent's ability to engage in reflective thinking, better planning, and result verification. They also make use of a logging mechanism inspired by the memory stream paradigm [25], which enables efficient management of historical data rather than inundating the LLM with extensive historical context. By adopting this design, they ensure that the log file serves as a repository of relevant information that can be easily retrieved and updated by the agent using LLMs. Retrieval-enabled (R) runs refer to the ones having this functionality enabled. Thus, this retrieved information coupled with the recent actions and observations make up the historical context. The retrieved information acts as long-term memory whereas the recent actions and observations act as short-term memory.\nWe extend the above framework for a multi-agent LLM system. Multi-Agent LLM Profiling refers to the technique of combining the expertise of multiple agents using LLM Profiling, i.e. assigning each of the agents distinct personas. We characterize the multi-agent nature of our system by categorizing the agents into two specific classes - (i) A Planner (P) that utilizes the aforementioned agent structure to consider historical context and 'plan' the next action, and (ii) Workers (Wis) that execute the actions. In addition to a profile for the planner, we also include distinct personas for workers performing distinct actions that involve calls to LLMs such as Edit Script, Understand File, etc as seen in table 1. Instead of having the"}, {"title": "3.2 LLM Cascade", "content": "LLM Cascades refer to the technique of conditional invocation of sequentially connected LLMs (L1, L2, ... Lk). Here, we chain LLMs in a manner wherein cost(L1) < cost(L2)... < cost(Lk). Here the cost is represented by the latest pricing information of the corresponding models. A set of protocols are enforced to decide if the response by an LLM at a particular \"cascade\" is acceptable or not. If it is acceptable, then the response is used as is and if not, we move up the cascade to the next LLM. For example, the LLMs in cascade could be Gemini-Pro (a no-cost LLM) followed by GPT4 (an expensive LLM). If Gemini-Pro fails to generate an acceptable response before exhausting its maximum retries, then the system would invoke GPT4 for that step. In our study, the protocols to move up the cascade are two-fold - (i) If the current LLM fails to generate a response that adheres to the specified format, even after maximum m number of tries, or (ii) If the current LLM chooses an action that has already been repeated r consecutive times in the past r steps."}, {"title": "3.3 Ask-the-Expert Action", "content": "To save on expenses, we primarily employ no-cost LLMs for both planning and action-based LLM calls. Preliminary investigations reveal that mostly planning is the shortcoming of these LLMs and that their responses are of sufficient quality when it comes to other action-based LLM calls by workers. Therefore, we give the LLM with the Planner P persona, I 'lifelines', where it can choose to call a larger, more expensive LLM with higher expertise, in scenarios where it identifies that it is stuck at a step. This choice is implemented in the form of an action, Request Help from a Planning Expert, that the current 'Planner' LLM agent can choose to take. Moreover, in practice, this upper cap on number of larger model calls (life-lines) also counts the calls made as a result of the cascade protocol mentioned in Section 3.2. During the course of a run, once the planner hits this upper cap, the Request Help from a Planning Expert action is no longer included in the prompt when populating the actions available to call."}, {"title": "4 EXPERIMENTATION AND RESULTS", "content": "We perform our experiments on the subset of tasks of MLAgentBench dataset explained in Section 2."}, {"title": "4.1 Metrics", "content": "We evaluate our results by taking two metrics under consideration."}, {"title": "4.1.1 Success Rate.", "content": "The success rate is the percentage (%) of runs which are considered as successful. As per defined in Huang et al. [16], a run is considered to be successful if it achieves more than 10% improvement at the last step over the average performance of the baseline in the starter code. Here the performance measure is"}, {"title": "4.1.2 Cost.", "content": "If an LLM has a monetary cost associated with it, we compute the average cost in dollars ($) per run based on number of tokens used for that model5. For LLMs where APIs are available, this becomes $[(Cost_per_input_token * Num_of_input_tokens) + (Cost_per_output_token * Num_of_output_tokens)]. Claude1 V1.0 used as a single agent in Huang et al. [16] is discontinued and hence latest pricing details for this are unavailable. For approximating cost for Claude, we use pricing information for Claude-Instant 6."}, {"title": "4.2 Models and hyperparameters", "content": "We analyze multiple no-cost LLMs such as Gemini, CodeLlama and Mixtral as single agents to test their ML problem solving capabilities. We employ two configurations for our proposed multi-agent framework: (i) Gemini, which is best performing single agent LLM, with ChatGPT 7 in cascade. (ii) Gemini with GPT4 in cascade and 'Ask-the-Expert' agent. For our runs, we set all hyperparameters as per the implementation of Huang et al. [16]. Maximum number of actions is set to 30. Maximum number of recent actions to be included in context as the short-term memory is set to to 3. For runs involving cascades, we set the maximum number of retries allowed (m) to 3 for Gemini-Pro, 3 for ChatGPT and 1 for GPT4 in the interest of cost. For ask-the-expert GPT4 calls also, we set the maximum number of retries allowed to 1. The maximum number of times an action can be consecutively repeated (r) is set to 3. For planning-related LLM calls, the temperature is set to 0.2 and for internal action-related LLM calls, the temperature is set to 0.01,"}, {"title": "4.3 Baselines", "content": "We use following baselines: (i) Single Agent GPT4 in retrieval setting (G + R) (ii) Single Agent GPT4 with no retrieval (G) (iii) Single Agent Claude V1.0 in retrieval setting (C + R) (iv) Single Agent Claude V1.0 with no retrieval (C) ((i) to (iv) are from Huang et al. [16]) (v) Single Agent Gemini Pro in retrieval setting (Ge + R) (vi) Single Agent Code Llama in retrieval setting (Co + R) (vii) Single Agent Mixtral in retrieval setting (Mx + R) ((v) to (vii) are our baselines using no-cost LLMs)"}, {"title": "4.4 Results and Discussion", "content": "We address key Research Questions (RQ) based on the results in table 2."}, {"title": "4.4.1 RQ1 - Do no-cost and low-cost LLM single-agents sacrifice performance for cost savings?", "content": "We observe a significant drop in the performance when we use a purely no-cost or low-cost LLM (Ge, Co, and Mx) in single-agent setting across all tasks as opposed to LLM single agents using GPT4 or Claude presented in Huang et al. [16]. We observe CodeLlama (Co) and Mixtral (Mx) are unable to produce any successful runs leading to average 0% success rate. Thus, we omit them from the results in table 2. We see that Mixtral is almost never able to adhere to the required response format mentioned in section 3.1 across all runs which leads to termination due to maximum retry limit being exceeded. However, we observe that Gemini-Pro (Ge + R) is able to produce successful runs in"}, {"title": "4.4.2 RQ2 - How do profiling and cascades affect performance and cost?", "content": "From table 2, it can be seen that profiling with Gemini-Pro as base LLM and ChatGPT (GPT-3.5-turbo) in cascade (Ge + Ch) both with and without retriever setting, significantly increases success rate for many tasks, namely cifar10, ogbn-arxiv, house-price, spaceship-titanic and vectorization when compared with Ge as single agent. However, for imdb, parkinsons-disease, feedback, llama-inference, CLRS and babylm tasks, the performance still remains zero due to their complex nature. Overall average success rate of profiling and cascade is comparable with GPT4 (G) performance. These improvements are obtained at almost 100% cost reduction for no-cost Gemini-Pro (From $1.315 for G + R and $0.13 for C + R to $0.0003 for Ge + Ch + R and from $0.931 for G and $0.092 for C to $0.0001 for Ge + Ch). This is because inference for GPT-3.5-turbo is much cheaper leading to minuscule costs. However, qualitative analysis shows that GPT-3.5-turbo often fails to adhere to the required response format mentioned in section 3.1 leading to further retries. Thus, we shift to GPT4 for cascades for subsequent runs."}, {"title": "4.4.3 RQ3 - How does adding ask-the-expert lifelines to profiling and cascade affect performance and cost?", "content": "Table 2 shows that access to GPT4 ask-the-expert lifeline calls as part of our proposed system (Ge + G and Ge + G + R) improves success rate for cifar10, ogbn-arxiv, house-price, spaceship-titanic and vectorization tasks when compared with Ge + Ch+ R and Ge + Ch. Additionally, we observe improvements in success rate when compared with G + R and G for cifar10, house-price, spaceship-titanic and vectorization tasks at a cost reduction ranging from 90-99% across tasks for no-cost Gemini-Pro. On an average the Profiling + Cascade + Expert setting and using GPT4 for cascade and expert gives 43.78% and 71.19%"}, {"title": "4.4.4 RQ4 - How does retrieval from logs affect performance and cost?", "content": "In accordance with the findings of Huang et al. [16], our observations indicate that while access to retrieval from logs proves effective for certain tasks, for others, disabling it yields better results. One interesting case to note is of cifar10. GPT4 (G) has a greater success rate than G + R. Huang et al. [16] justify this by stating that since cifar10 is a comparitively easier task and the long-term memory context become a distraction. But the trend gets reversed in the case of Gemini-Pro with GPT4 in cascade and expert setting. Ge + G + R has a greater success rate than Ge + G. This can be due to the differences in pretraining and actual data seen by Gemini-Pro and GPT4. Similar to success rate, cost is greater with retrieval for some tasks, whereas it is greater without retrieval for others. Average cost per run for Ge + G ($0.054) is greater than that for Ge + G + R ($0.047) for no-cost Gemini-Pro."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 LLM Agents for nuanced code-related tasks", "content": "With the rising capabilities of Large Language Models, there have been many advances in employing LLM Agents for code-related tasks. Initial works relied primarily on HumanEval [6] and MBXP [3] for benchmarking their approaches. However, recent works have started moving towards more nuanced and real-world benchmarks. Wang et al. [35] present CodeAct - a Python code database containing tasks related to API handling, library usage, etc. They also present CodeActAgent that is finetuned on Llama-2 [32] and Mistral-7b [17] using CodeActInstruct, an instruction tuning dataset consisting of multi-turn interactions. AgentBench [22] also introduces three code agent environments based on operating system, database and knowledge graph related problems for evaluating LLM agents on these tasks. There has also been some amount of work done for tackling more complex problems. Zhang et al. [45] construct CodeAgentBench, a dataset containing real-world repo-level coding challenges. They also propose CodeAgent to tackle the tasks in this dataset utilizing external information retrieval, code implementation and testing tools at its disposal.\nHuang et al. [16] construct MLAgentBench, explained in Section 2, that consists of machine learning tasks. They also propose a single agent based system to solve these tasks using various tools such as code execution, code editing, etc. Tang et al. [30] explores the use of LLM Agents with repository-level code. However, their approach does not involve direct code generation. Instead, they focus on creating shell scripts to utilize pre-existing code files with appropriate arguments. On a similar note, CodePlan [4] involves framing repository-level coding as a planning problem in the form of a chain of edits. Additionally, NL2Repo [43] presents a system for generating codebases from the ground-up, using natural language specifications."}, {"title": "5.2 LLM Multi-Agents", "content": "While LLM single-agents have demonstrated remarkable capabilities in various tasks, the complexity of real-world challenges often necessitates collaborative approaches. In this context, the exploration of LLM multi-agent systems has emerged as a promising avenue to enhance problem-solving capabilities and address intricate tasks more effectively. ChatDev [26] and MetaGPT [12] are recent multi-agent systems introduced for tackling software development tasks. AutoGen [38] is a more generic framework for developing LLM multi-agents and has been evaluated on multiple tasks ranging from math problem solving to retrieval augmented chatting Li et al. [21], Shen et al. [29] also present frameworks for using multiple LLM agents in a collaborative manner for tasks such as reasoning and tool-learning. Ding et al. [7], Wang et al. [34] propose combining multiple LLM expert agents to solve a variety of tasks including text summarization and question answering. To save up on costs, Chen et al. [5], Yue et al. [41], Zhang et al. [44] propose using cascading LLMs in increasing order of cost and capability for diverse tasks such as reasoning, query answering and news prediction. Our focus is on providing cost-effective solution for ML engineering tasks using multi-agent system with no-cost open-source LLM as the base with paid LLM with higher expertise in cascade."}, {"title": "6 CONCLUSION", "content": "In this work, we propose BudgetMLAgent - an LLM Multi-Agent system for solving machine learning tasks in a cost-effective manner without hampering the system performance. Our primary investigations on tasks defined in MLAgentBench dataset, with Single-Agent systems with purely no-cost models give zero success rates for CodeLlama and Mixtral and very poor success rates with Gemini-Pro (9.09% with access to the complete logs and 10.23% with short-term access), as compared to paid models such as GPT4 and ClaudeV1.0 (18.18% and 22.72%, 15.27% and 20% respectively). We subsequently propose a multi-agent framework, BudgetMLAgent, using no-cost Gemini-Pro as the base LLM, leveraging (i) profiling for a planner and multiple worker agents interacting with the ML code generation environment using distinct actions, (ii) cascades to LLMs with more expertise such as GPT-3.5-turbo and GPT4 and (iii) our novel ask-the-expert GPT4 lifelines. BudgetMLAgent results in improving the success rates for MLAgentBench tasks (26.14% and 32.95% respectively) along with significant cost reductions when compared with GPT4 and ClaudeV1.0 based Single-Agent systems (96.43% and 63.85% with access to the complete logs, 94.2% and 41.3% reduction with short-term access respectively)."}]}