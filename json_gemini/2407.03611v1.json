{"title": "An Empirical Study on Capability of Large Language Models in Understanding Code Semantics", "authors": ["Thu-Trang Nguyen", "Thanh Trong Vu", "Hieu Dinh Vo", "Son Nguyen"], "abstract": "Large Language Models for Code (code LLMs) have demonstrated remarkable performance across various software engineering (SE) tasks, increasing the application of code LLMs in software development. Despite the success of code LLMs, there remain significant concerns about the actual capabilities and reliability of these models, \"whether these models really learn the semantics of code from the training data and leverage the learned knowledge to perform the SE tasks\". In this paper, we introduce EMPICA, a comprehensive framework designed to systematically and empirically evaluate the capabilities of code LLMs in understanding code semantics. Specifically, EMPICA systematically introduces controlled modifications/transformations into the input code and examines the models' responses. Generally, code LLMs must be robust to semantically equivalent code inputs and be sensitive to non-equivalent ones for all SE tasks. Specifically, for every SE task, given an input code snippet c and its semantic equivalent variants, code LLMs must robustly produce consistent/equivalent outputs while they are expected to generate different outputs for c and its semantic non-equivalent variants. Our experimental results on three representative code understanding tasks, including code summarization, method name prediction, and output prediction, reveal that the robustness and sensitivity of the state-of-the-art code LLMs to code transformations vary significantly across tasks and transformation operators. In addition, the code LLMs exhibit better robustness to the semantic preserving transformations than their sensitivity to the semantic non-preserving transformations. These results highlight a need to enhance the model's capabilities of understanding code semantics, especially the sensitivity property.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) for code have demon-strated remarkable performance on a variety of automated programming tasks. Automating such tasks is inherently complex, requiring the model to understand numerous con-cepts within the underlying code. However, despite this effectiveness on various code-related tasks, a fundamen-tal question remains: How deeply do these large language models for code (code LLMs) truly understand the code they interact with? Current evaluations usually assess the usefulness (e.g., pass@k metric) of programs generated by these models based on the passed tests for the given tasks in several benchmarks such as HumanEval [1], MBPP [2], and CodeContests [3]. While these task-driven evaluations measure end-to-end performance, they fail to reveal the LLMs' capabilities in understanding code semantics, which could be crucial for ensuring the reliability of code LLMs. Prior studies [4, 5, 6] have begun to explore the capa-bilities of LLMs in code understanding. However, a deeper understanding of the LLMs' capabilities to capture semantic aspects, including control dependence and data dependence, remains elusive. Wan et al. [4] prioritize syntactic cor-rectness over semantic understanding. Evaluation methods might assess if generated code compiles without truly grasp-ing its intended functionality. Hooda et al. [5] evaluate LLMs' capability in understanding code semantics through a single task, which is code completion. This restricts our understanding of how broadly code LLMs can understand code. Ma et al. [6] evaluate the code LLMs' capabilities in reconstructing code syntax and semantic structures by fine-tuning the LLMs to predict certain properties in code such as nodes and edges in Abstract Syntax Tree (AST) or data/-control dependence. It is unclear how LLMs arrive at their outputs, making it difficult to assess their reasoning process and identify potential biases in semantic understanding.\nIn this paper, we propose EMPICA, a novel framework for systematically and empirically evaluating code LLMs' capabilities in understanding essential code semantic as-pects. Our approach involves analyzing the sensitivity and robustness of code LLMs to changes in code semantics. Within this framework, we focus on two fundamental seman-tic relations in the programming analysis: control depen-dence and data dependence [7, 8, 9]. To assess code LLMs' capabilities in understanding these dependencies, we define eight code transformations. These transformations can be categorized as either semantic non-preserving (affecting the semantic relations) or semantic preserving (not affecting the semantic relations).\nWe apply our framework to three critical software en-gineering tasks with varying levels of semantic complexity: code summarization, method name prediction, and output prediction. We evaluate the models for each task without fine-tuning or additional training data to minimize potential biases. Ideally, models must recognize semantic changes introduced by non-preserving transformations and adjust their outputs accordingly. Conversely, their outputs must remain consistent for programs before and after semantic-preserving transformations. Our work incorporates four state-of-the-art (SOTA) code LLMs: DeepSeek-Coder [10], Code Llama [11], MagicCoder [12], and GPT-3.5 [13]. \u03a4\u03bf reduce potential out-of-distribution threats, we analyze how these models perform on the code generated by these models themselves when subjected to the aforementioned semantic code transformations. This analysis will provide valuable insights into the strengths and weaknesses of current SOTA models in terms of code semantic comprehension.\nOur experimental results demonstrate substantial varia-tions in the robustness and sensitivity of code LLMs to the code transformations applied. These variations are observed across different SE tasks and transformation operators. No-tably, the models exhibit a greater degree of robustness to semantic-preserving transformations compared to their sen-sitivity towards semantic non-preserving transformations. These results highlight the necessity for further development in code LLMs' capabilities, particularly in enhancing their sensitivity to changes in code semantics.\nThe contributions of this paper are as follows:\n1. Evaluation Framework: We introduce EMPICA, a frame-work for systematically evaluating LLM understand-ing of essential semantic aspects in code.\n2. Empirical Results: We present an empirical evalua-tion using EMPICA on various tasks and benchmark datasets.\n3. Findings: We analyze the results to identify the strengths and weaknesses of current SOTA code LLMs regard-ing semantic code comprehension."}, {"title": "2. Background and Related Work", "content": "Large Language Models for Code (code LLMs) are becoming popular in the Software Engineering (SE) indus-try. Multiple code LLMs, such as Github Copilot\u00b9, Alpha-Code [14], and GPT-3.5 [13], have been employed to speed up the software development process. These models play a significantly important role in the automation of multiple SE tasks such as code completion [15], program repair [16], or test generation [17], etc. As the sizes of these models and the amount of training data have increased, code LLMs have exhibited impressive performance in these tasks [15, 16, 17].\nExisting LLMs for code are constructed based on the Transformer [18] (encoder-decoder or decoder-only). These models are trained on extensive, unlabelled corpora of code-related data, enabling them to generate code in various pro-gramming languages. The training process typically involves the causal language modeling objective, where the model learns to predict the next token in a sequence. The success of LLMs in the SE task can be attributed to three main factors: Large Size, Premium Data, and Expert Tuning [19].\nRecent advancements in code LLMs demonstrate a trend towards larger model sizes, accompanied by superior performance [20, 21]. These studies show that increas-ing the number of model parameters can significantly en-hance model capabilities. For instance, early models such as CodeT5 [22] had limited success due to their relatively small size. In contrast, modern models like Codex, Code Llama, and DeepSeek achieve remarkable performance on various benchmarks with their massive number of parameters.\nAdditionally, high-quality code corpora enable LLMs to learn different programming languages and coding paradigms effectively. To ensure the quality of the training corpus, it is common for LLMs to perform extensive data preprocessing on the large amount of data collected. One common strategy is removing likely auto-generated or unfinished code files. Additionally, specific rules are employed to filter out un-common code files. These rules include the repository star rating, file size, line length, and alphanumeric rate.\nExpert tuning is also essential for optimizing model performance. Common practices include using the Adam optimizer or its variants and further training from the models pre-trained on text. Key hyperparameters such as learning rate, batch size, window size, warmup steps, gradient ac-cumulation steps, and sampling temperature require expert tuning. Specialized tokenizers, such as Byte-level Byte-Pair-Encoding and SentencePiece, are applied on code corpora to handle programming syntax effectively."}, {"title": "2.2. Evaluating Code LLMS", "content": "Several benchmarks have been established to evaluate the capabilities of code LLMs in various SE tasks. For example, Chen et al. [1] introduced HumanEval, a popular benchmark designed to assess the functional correctness of the generated code in multiple programming languages. Similarly, Austin et al. [2] proposed MBPP dataset, which contains solvable problems for entry-level programmers. This dataset is also widely used to evaluate the program synthesis of code LLMs [23, 24, 25]. Another manually crafted benchmark ClassEval [26] focuses on evaluating LLMs' performance on class-level code generation. Exper-imental results on these benchmarks have demonstrated the remarkable performance of code LLMs across various SE tasks, which significantly accelerates the software develop-ment process.\nIn addition, many approaches [27, 21, 5] proposed to further thoroughly test code LLMs in various code gener-ation and code understanding tasks. Liu et al. [27] con-ducted comprehensive experiments on eight models using five benchmarks to examine the capabilities and limitations of these models. In another research, Lie et al. [21] evaluates whether the code generated by code LLMs is really correct by introducing more sophisticated test cases for the problems in the benchmark HumanEval. Their experimental results show that the performance of code LLMs at pass@k for the problems in HumanEval significantly decreased when additional test cases were included. Besides, Hooda et al. [5] assessed the code completion capabilities of code LLMs by perturbing concept predicates such as control flow or data flow in the input programs. Their evaluations of ten LLMs reveal that these perturbations frequently led to incorrect code completion by the models.\nFurthermore, the abilities of code LLMs in understand-ing the other aspects of the programs have also been inves-tigated [28, 29, 30]. Chen et al. [28] evaluated how code LLMs capture the runtime behavior of program execution such as code coverage or execution path prediction. Kar-makar et al. [29] and Ma et al. [30] proposed probing tasks for assessing the code LLMs capability in capturing code syntax and semantics aspects, including Abstract Syn-tax Tree (AST), Control Flow Graph (CFG), and Control Dependence Graph (CDG). Their results indicate that while models can effectively capture syntactic relationships, they still struggle to understand semantic relationships."}, {"title": "3. Motivation", "content": "Despite the success of code LLM in SE tasks, there are multiple doubts about the performance of these mod-els [5, 6, 31], \"Whether code LLM really learn the syntax and semantics of code from the training data and leverage the learned knowledge to conduct SE tasks\u201d. Yang et al. [31] have conducted experiments to investigate the memorization of LLMs for code with CodeParrot and CodeParrot-small. Their experimental results show that from 43% to 57%, outputs of these models contain memorized information. This raises a question about the reliability of the answers provided by code LLM.\nFigure 1 shows a summary generated by Microsoft/phi-2\u00b2 for a given code snippet. This given code detects and returns true if, at any point, the account balance falls below zero. The generated summary is \"If the balance becomes negative at any point, the function returns true.\" (Figure 1b). According to the explanation of SHAP [32], the model can accurately produce this summary statement, especially the token \"negative\u201d, because it focuses on the conditional statement at line 6, if (balance < 0).\nIn addition, without checking the negative value of bal-ance (e.g. if (balance < 0)), the code snippet in Figure 2a is semantically different from one in Figure 1a. However, the summary (Figure 2b) generated by Microsoft/phi-2 is still very similar to the summary in Figure 1b. Specifically, the summary for the code in Figure 2a is \"The function below takes an ArrayList of longs and returns true if the sum of the operations is below zero. Otherwise, it returns false\u201d\nThe explanation produced by SHAP (Figure 2b) shows that Microsoft/phi-2 mainly focused on several tokens like boolean, operation, true, false, or especially the method name belowZero to generate the summary. For example, the model focused on the expression balance += operation and return true; in the input for producing token \u201cif\u201d in the summary. Also, to generate token \u201czero\u201d in the summary, the model focused on the tokens like boolean, belowZero, and operation in the code snippet. This example demonstrates that LLM could heavily rely on general patterns of input code to generate summaries without really comprehending them.\nThe above analysis raises the question on the capability of code LLM in understanding code semantics: \u201cWhether and how the code LLM can really understand code semantics or just capture and rely on the general patterns from the data for conducting downstream tasks\". In this research, we propose EMPICA, a framework for systematically evaluating the capability of code LLM in capturing code semantics, including control and data dependencies."}, {"title": "4. EMPICA: A Framework for Evaluating Code Semantic Understandability of LLMs", "content": "This work aims to evaluate code LLMs' capability in code semantic understanding in the two fundamental code semantic relations: control dependence and data depen-dence. Figure 3 illustrates the overview of our framework, EMPICA, containing four main components: (1) Program Generation, (2) Program Transformation, and (3) Code Un derstanding, and (4) Robustness & Sensitivity Evaluation.\nFirst, for a code LLM \u2133 under evaluation, EMPICA employs \u2133 to generate code. Then, EMPICA directly uses the generated code to evaluate the capabilities of \u2133 in understanding code semantics. This is reasonable because evaluating \u2133 in understanding arbitrary code could hinder the capability of \u2133. Indeed, \u2133 could misunderstand an entirely new code snippet, especially if the code significantly deviates from the patterns and distributions seen during training (i.e., out-of-distribution) [33]. Therefore, EMPICA conducts the analysis on the code generated by the model it self for a more impartial evaluation of the model\u2019s capability and to mitigate the potential risk of capability hindering due to the unseen data.\nNext, EMPICA systematically introduces controlled mod ifications/transformations into the input code and observes how the model reacts in\u2133\u2019s output for a specific code under standing task. EMPICA applies a variety of transformations designed to target both control and data dependencies in the input code snippets. We employ both semantic-preserving transformations and semantic-non-preserving transforma tions to modify the code snippets. Specifically, given a code snippet, semantic-preserving transformations create various variants of the code while maintaining the origi nal functionality of the snippet in its variants. In contrast, semantic-non-preserving transformations produce variants whose functionality differs from the original ones.\nIdeally, if\u2133 is capable of understanding code semantics, \u2133\u2019 predictions must be robust to the semantic-preserving (SP) transformations and sensitive to the semantic-non preserving (SNP) ones. For a specific task, \u2133 must produce the equivalent responses to the original input code and its variants created by the SP transformations. In contrast, \u2133\u2019s outputs for the original input and its variants modified by the SNP transformations are expected to be different.\nFor thorough evaluation, EMPICA selects three repre sentative SE tasks that heavily require code understanding, including code summarization, method/function name pre diction, and output prediction. These tasks are in different complexity levels, encompassing both static and dynamic behaviors, as well as varying degrees of program under standing, ranging from coarse-grained to fine-grained. Code summarization requires an understanding of the overall pur pose of the code snippet to provide a concise summary. Method/Function name prediction demands exact compre hension of code semantics and functionality to predict an appropriate name for a method/function. Output prediction involves precisely capturing the underlying logic of code and the relationship between input-output to provide correct pre diction. By incorporating these tasks in different natures and complexities, EMPICA enables a comprehensive assessment of code LLMs in capturing code semantics."}, {"title": "4.1. Semantic-aware Code Transformations", "content": "Given a code snippet $c \\in \\mathcal{C}$, a transformation operator $t: \\mathcal{C} \\rightarrow \\mathcal{C}$ is a function that maps $c$ from and to a space $\\mathcal{C}$ of all possible code snippets in the given programming language. Let\u2019s assume we have an oracle function $f: \\mathcal{C} \\rightarrow \\mathcal{F}$, which maps from the space of code snippets $\\mathcal{C}$ to the specification space $\\mathcal{F}$ which represents the code behaviors. A code transformation $t(c)$ can be categorized into two groups based on its impact on $f(t(c))$, semantic-preserving (SP) and semantic-non-preserving (SNP) transformations."}, {"title": "Definition 1. Semantic-preserving (SP) transformation.", "content": "Given a code snippet $c \\in \\mathcal{C}$, a transformation $t: \\mathcal{C} \\rightarrow \\mathcal{C}$ is a semantic-preserving transformation if the modifications in troduced by applying $t$ on $c$ do not affect its specified behaviors, $\\forall c \\in \\mathcal{C}, f(c) \\equiv f(t(c))$ ."}, {"title": "Definition 2. Semantic-non-preserving (SNP) transfor mation.", "content": "Given a code snippet $c \\in \\mathcal{C}$, a transformation $t: \\mathcal{C} \\rightarrow \\mathcal{C}$ is a semantic-non-preserving transformation if the modifi cations introduced by applying $t$ on $c$ change its specified behaviors, $\\forall c \\in \\mathcal{C}, f(c) \\not\\equiv f(t(c))$ .\nEMPICA curated the transformation operators from the literature [34, 35, 36]. To systematically investigate the understanding of code LLMs in control and data depen dencies, we manage the transformations with respect to these relationships."}, {"title": "4.2. Code Understanding: Subject Tasks", "content": "In this work, EMPICA conducts Counterfactual Anal ysis in three representative SE tasks which require code understanding, including code summarization, method name prediction, and output prediction. The general idea is that given an input code snippet c, if a code LLM M is capable of understanding the code semantics, the outputs of M must be consistent and robust for c and its variants produced by any SP transformations. In contrast, the outputs of M must be different and sensitive for c and its variants produced by SNP ones. In other words, M must produce semantically similar/identical outputs for c and for any of c's variants, cp, produced by SP transformations, $c_p = t_p(c)$, in any code understanding task T. Conversely, M must recognize any modifications affecting the meaning of the program to produce different outputs for c and any of its variants, cn, produced by SNP transformations, $c_n = t_n(c)$, in task T. The rationale of this hypothesis lies in the semantic equivalence between c and cp and the semantic distinction between c and cn. If the outputs of M with the inputs c, cp, and c\u2081 are not consistent and reliable in such way, it suggests that M is struggling with the code semantics comprehension.\nFor a dataset D containing n code snippets which orig-inally generated by code LLM M and a transformation operator t, let $c_i \\in D$, $1 \\leq i \\leq n$ and its corresponding transformed code be $c'_i = t(c_i)$. For a specific code un-derstanding task T, the outputs of M for $c_i$ and $c'_i$ is $o_i$ and $o'_i$, respectively, $T(\\mathcal{M}, c_i) = o_i$; and $T(\\mathcal{M}, c'_i) = o'_i$. The robustness of M is specified by the similarity of its outputs for $c_i$ and $c'_i$, i.e., the similarity between $o_i$ and $o'_i$. The sensitivity of M is the distinction of M's outputs for $c_i$ and $c'_i$, i.e., the difference between $o_i$ and $o'_i$. Specifically, the robustness and sensitivities of M for the code snippets in D with the transformation operator t could be calculated by the following equations.\n$Robustness(\\mathcal{M}, \\mathcal{T}, t) = \\frac{1}{n} \\sum_{i=1}^{n} sim(o_i, o'_i)$    (1)\n$Sensitivity(\\mathcal{M}, \\mathcal{T}, t) = \\frac{1}{n} \\sum_{i=1}^{n} [1 \u2013 sim(o_i, o'_i)]$\n$ = 1 - Robustness(\\mathcal{M}, \\mathcal{T}, t)$   (2)\nIn the Equation 1, sim is a similarity function which measures the semantic similarity of two outputs $o_i$ and $o'_i$, $sim(o_i, o'_i) \\in [0,1]$, while the semantic distinction between $o'_i$ and $o_i$ is the complement of their similarity, $[1 \u2013 sim(o_i, o'_i)] \\in [0,1]$. Ideally, M's outputs must be robust to the SP transformations and sensitive to the SNP ones. In other words, M is adequate in understanding code semantics when $Robustness(\\mathcal{M}, \\mathcal{T}, t)$ is close to 1 to any SP transformation operator t. At the same time, $Sensitivity(\\mathcal{M}, \\mathcal{T}, t)$ is also close to 1 to any SNP trans-formation operator t. Otherwise, it suggests a deficiency of M in understanding code semantics. Note that the similarity function could differ for each code understanding task."}, {"title": "4.2.1. Code Summarization", "content": "Code summarization (Ts) is the task of concisely de-scribing a code snippet's overall functionality, purpose, and behavior in a natural language [37]. To accurately provide a summary of a given code snippet, the model needs to precisely capture the semantics of the code. Thus, we employ code summarization as a representative task for evaluating code LLMs' capability in understanding code semantics.\nGiven a code snippet $c_i \\in D$, which is originally generated by code LLM M, and $c'_i = t(c_i)$ is the transformed version of c\u2081 which generated by applying a transformation operator t. If t is a SP transformation, the functionality of $c'_i$ is semantically equivalent to c\u2081, and the summary of $c'_i$ is expected to be similar/identical with that of $c_i$. Meanwhile, if t is a SNP transformation, the functionality of $c'_i$ is different from c\u2081, and their corresponding summaries are expected to be semantically different from each other.\nLet $d_i$ and $d'_i$ be the summaries of the code snippets $c_i$ and $c'_i$, both outputted by M. To measure the Robustness and Sensitivity of M in the code summarization task, we compare the similarities of the summaries $d_i$ and $d'_i$. Par-ticularly, the robustness in the Equation 1 is realized with the similarity function as follows:\n$sim(d_i, d'_i) = cosine(v_i, v'_i)$    (3)\nIn the equation 3, the similarity of the summaries is measured by the cosine similarity. Specifically, cosine mea-sures the similarity of $v_i$ and $v'_i$, two representation vectors of $d_i$ and $d'_i$, respectively. In EMPICA, we measure their similarities regarding both lexical and semantic aspects. For lexical similarity, $d_i$ and $d'_i$ are represented by Term Frequency-Inverse Document Frequency (TF-IDF) vectors. For semantic similarity, the meanings of $d_i$ and $d'_i$ are en-coded by a pre-trained embedding model such as Sentence-BERT [38], which has been widely applied to derive the representation for sentences in natural language [39, 40]."}, {"title": "4.2.2. Method Name Prediction", "content": "Method name prediction (Tv) is the task of suggesting an appropriate method/function name for a given code snip-pet [41]. Successful method name prediction requires a deep understanding of the underlying semantics and operations of the code. Similar to code summarization, method name is another form of summary of code functionality, yet it is more succinct.\nLet $m_i$ and $m'_i$ be the method names predicted by Code LLM M for the code snippet $c_i$ and its transformed code $c'_i$, respectively. To measure the capability of M in under-standing code semantics for this task, Tv, we compare the similarity of $m_i$ and $m'_i$ at token level (i.e., exactly match) and sub-token level (i.e., precision, recall, and f1-score).\nFor token-level assessment, the similarity of $m_i$ and $m'_i$, $sim(m_i, m'_i)$, is measured regarding whether they are exactly identical, $sim(m_i, m'_i) = EM(m_i, m'_i)$. Specifically, $EM(m_i, m'_i)$ measures whether $m_i$ and $m'_i$ are exactly the same, $EM(m_i, m'_i) = 1$ if $m_i$ and $m'_i$ are identical, otherwise, $EM(m_i, m'_i) = 0$.\nFor sub-token level assessment, $sim(m_i, m'_i)$ is measured regarding how similar their sub-tokens are. EMPICA tok-enizes $m_i$ and $m'_i$ by the under_score and camelCase conven-tions, then compares how similar these sub-token sets are. Let $K_i = {k_{i1}, ..., k_{ij} }$ and $K'_i = {k'_{i1}, ..., k'_{im}}$ be the sub-token sets of $m_i$ and $m'_i$, respectively. The similarity of $K_i$ and $K'_i$ is measured in terms of precision, recall, and F1-score. In particular, they are measured as follows:\n$precision(m_i, m'_i) = \\frac{|K_i \\cap K'_i|}{|K'_i|}$\n$recall(m_i, m'_i) = \\frac{|K_i \\cap K'_i|}{|K_i|}$\n$F1(m_i, m'_i) = 2 \\times \\frac{precision(m_i, m'_i) \\times recall(m_i, m'_i)}{precision(m_i, m'_i) + recall(m'_i, m'_i)}$\nIn summary, to evaluate the code semantics understandabil-ity of M in this task, Tv, in Equation 1, the similarity function, $sim(o_i, o'_i)$ could be expressed by $EM(m, m')$, $precision(m_i, m')$, $recall(m_i, m')$, or $F1(m_i, m')$ ."}, {"title": "4.2.3. Output Prediction", "content": "The task of output prediction, (To), involves predicting the output of a given code snippet when it is executed with a specific input [28]. To achieve high performance in this task, code LLMs must comprehensively understand the code's semantics, operation logic, and execution flow. For code summarization and method name prediction, these tasks are only required to capture the program's static be-haviors. Meanwhile, output prediction needs to analyze not only static but also dynamic behaviors of the given code snippets. Thus, the correct prediction of the test output is more complex than the previous two.\nLet k be the number of test inputs of c\u2081 and its trans-formed code c. Let $o_{ij}$ and $o'_{ij}$, $1 \\leq j \\leq k$ be the outputs of $c_i$ and $c'_i$ which predicted by code LLM M with the jth test. If c is created by a SP transformation, the behaviors and functionality of c\u2081 and c are equivalent. They behave similarly with the same input, i.e., return equal outputs for a given input. On the other hand, if c is created by a SNP transformation, the functionalities of c\u2081 and c are different. They could produce different outputs for a given input. Specifically, to assess the robustness and sensitivity of M in the output prediction task, To, Equation 1 is realized with the similarity function expressed as follows:\n$sim(o_i, o'_i) =  \\begin{cases}  1 & \\text{if } o_{ij} = o'_{ij} \\\\  0 & \\text{otherwise} \\\\ \\end{cases}$   (4)"}, {"title": "4.3. Prompt Design", "content": "In this work, EMPICA utilizes prompting to evaluate code LLMs with the code understanding tasks. The design of the prompt can significantly impact the performance of the mod-els. Thus, we aim to design simple prompts to mitigate the risk that the models could perform poorly because of mis-understanding the prompts. To design appropriate prompts for each task, we evaluated and optimized the prompts via some trial queries. Note that each LLM could have different prompt formats; we follow their original papers and the provided templates on HuggingFace to adapt for prompt design. Generally, the prompt template is defined as follows:\n\"Given the following code snippet [CODE]. [TASK DESCRIPTION].\"\nwhere [CODE] specifies the input code and [TASK DESCRIPTION] specifies the query for the task. For example, for code summarization, the [TASK DESCRIPTION] is \"Please summarize the given code snippet\". For method name prediction, the [TASK DESCRIPTION] is \"Please generate the method name for the given code snippet\". For output prediction, the [TASK DESCRIPTION] is \"Please complete the following test case: [TEST]\", e.g., assert fibfib(2) == <FILL>."}, {"title": "5. Evaluation Design", "content": "To evaluate code LLMs' capability in understanding code semantics, we seek to answer the following questions:\n\u2022 RQ1: Robustness and Sensitivity Analysis. How ro-bust are the state-of-the-art code LLMs [11, 10, 13, 12] to the SP transformations in code understanding tasks? How sensitive are code LLMs to SNP transfor-mations in these tasks?\n\u2022 RQ2: Model Size Analysis. Does code LLMs' size affect their capability to understand code semantics?\n\u2022 RQ3: Correctness Correlation Analysis. Is there any correlation between the code semantic understand-ability of code LLMs and the correctness of their generated code?\n\u2022 RQ4: Semantic Inference Analysis. Can the code LLMs correctly capture the code semantics and di-rectly infer the relations including control and data dependence?"}, {"title": "5.1. Experimental Procedure", "content": "To mitigate the risk of the out-of-distribution prob-lem [42], EMPICA utilizes the code generated by the model under evaluation to evaluate the model's capability of un-derstanding code semantics. In this work, we employ the popular benchmark HumanEval [1], which is widely used in code generation research [5, 21], as \"seeds\" for the model to generate code snippets for our experiments. Specifically, this dataset contains 164 programming problems with unit tests. Each problem has a problem description, a function signature, and several manually created test cases. On aver-age, there are about eight tests per problem to evaluate the correctness of the generated code. For thorough evaluation, EMPICA queries the models to generate code in two popular programming languages, Java and Python, for each problem. Then, EMPICA examines how the model understands code semantics in both languages."}, {"title": "6. Experimental Results", "content": "Table 3 presents the average Robustness and Sensitivity of code LLMs to the code transformations in the code summarization task. Code LLMs perform robustly to both SP and SNP transformations. Particularly, regarding semantic similarity, the average Robustness of the models to the SP transformations is about 0.9. The average Sensitivity of the models to the SNP transformations is about 0.1. In other words, the similarities between the summaries of the original code and those of the semantic equivalent transformed code are 0.9. Meanwhile, the differences between the summaries of the original code and those of the semantic non-equivalent transformed code are only 0.1. This indicates that the code LLMs produce similar summaries for the original code and the transformed code regardless of the impact of the transformations on the code semantics.\nIn general, code LLMs are not explicitly trained to cap-ture the program dependencies, the underlying semantics, or the operational logic of code. Instead, they are trained on a large corpus of code in form of text data to implicitly learn the patterns and relationships in the data. When generating code summaries, the models could rely on the overall struc-tures and the surrounding contexts rather than deeply under-standing the underlying semantics of each code statement in the given code snippets. Although the original code and the transformed code could be different or similar regarding their semantics, their general structures are still similar. Therefore, the models could produce similar summaries for them. For instance, the transformations in Figure 4b make the behaviors of the program totally different from those of the original one (Figure 4a). However, the general patterns of these two code snippets are similar, which led Code Llama to result in similar summaries. That is a reason why code LLMs demonstrate robust performance against all transformations in the task of code summarization."}, {"title": "6.1.2. Method Name Prediction", "content": "Compared to the task of code summarization, code LLMs are less robust and more sensitive to both kinds of transfor-mations in the task of method name prediction, see Table 4. For instance, the predicted method names of 62% SP trans-formed Java code and 48% SP transformed Python code are identical to those of the original code snippets. Meanwhile, by SNP transformations, 54% of transformed Java code and 59% of transformed Python code have the predicted names different from that of the original code. This could indicate that in the method name prediction task, code LLMs are more perceptive to modifications and respond differently to original and transformed code.\nFor the similarity at the sub-token level (F1-Score), code LLMs show their high Robustness to the SP transformations but low Sensitivity to the SNP transformations. In partic-ular, the average Robustness in terms of F1-Score for Java code is 0.80, and the corresponding figure for Python code is 0.72. Meanwhile, the Sensitivity is about 0.31 and 0.34 for Java and Python, respectively. This indicates that code LLMs can generate similar method names for the original code and SP transformed code. However, for SNP transformed code, the predicted names are not considerably distinguishable from that of the original code. These results suggest that at some degree, code LLMs can capture the code semantics and realize the modifications affecting code meaning. However, the effects on the generated outputs (methods' names) are not significant."}, {"title": "6.1.3. Output Prediction", "content": "Table 5 shows the average Robustness and Sensitivity of code LLMs for the task of output prediction. The results illustrate the significant differences between the outputs predicted for the original code and the transformed code. Specifically, only 64% of the cases are predicted to have equal outputs in the original code and the SP transformed code. Meanwhile, 47% of the cases are predicted to have different outputs in the original code and the SNP trans-formed code. Overall, code LLMs are quite sensitive to all the transformations in predicting outputs.\nAs seen the results from the three studied tasks, code LLMs tend to be more robust to the SP transformations than to be sensitive to the SNP transformations. Indeed, for the tasks of method name prediction and output prediction, code LLMs better recognize of the semantic equivalence caused by the SP transformations rather than the semantic inequivalence by the SNP transformations. For example, in Table 5, for transformed Java code, LLMs could preserve equal outputs for about 67% of cases in the SP transformed code. However, for SNP ones, it can recognize the differ-ences and produce different outputs for only 45% of cases."}, {"title": "Transformation Operator Analysis:", "content": "Table 6 presents the details Robustness and Sensitivity of DeepSeek-Coder to different transformation operators on each code under-standing task. In our experiment, we found that for all the tasks, Renaming Variable creates the strongest impacts on the models among the SP operators although they have no impact on the functionalities of the given code snippets. For instance, the summaries of the original code and its renamed variable versions are 88% semantically similar which is much lower than the average similarity values of the other SP transformation operators, 94%. For the task of method name prediction, after renaming variables, only 33% of the code snippets still have the same predicted names as the original ones, which is more than two times lower than the average results of the other SP operators.\nIn practice, the variable names often convey rich in-formation for code understanding. Additionally, language models often rely heavily on context to generate predictions or summaries. Thus, changing variable names can disrupt this context and lead code LLMs to alter their understanding of the code's intent and produce different outputs.\nMeanwhile, among the SNP transformation operators, removing conditional statements is the most sensitive for the code LLMs. This operator leads to the most significant differences in the responses of code LLMs compared to their responses to the original code. For example, DeepSeek-Coder changed its predicted method names in 62% python programs when the relation conditions were negated. While by removing conditional statements, this figure increased to 72%. Indeed, this transformation operator alters the code's structure and texture more extensively than the other oper-ators. This makes the differences between the transformed and the original code more apparent to the models, which affects their predictions."}, {"title": "6.2. RQ2. Model Size Analysis", "content": "Figure 6 shows the impact of the model sizes on the Robustness and Sensitivity of DeepSeek-Coder to the transformations. Overall, the Robustness and Sensitivity of code LLMs to the transformations are greatly affected by the code understanding tasks rather than the number of the models' parameters.\nIn particular, for all the studied variants of DeepSeek-Coder, the Robustness and Sensitivity of the models in the code summarization task are around 0.92 and 0.09, respectively. This means that regardless of the number of pa-rameters, DeepSeek-Coder produces 92% similar summaries for the original code and the SP transformed code, while the differences between the summaries for the original code and the SNP transformed code are only 9%. These figures are quite stable for the variants with different parameter sizes of DeepSeek-Coder.\nMeanwhile, for output prediction, the Robustness values range from 0.64 to 0.69. These figures are not considerably different among model variants, yet much lower than that of the task of code summarization. Similarly, all the studied variants of DeepSeek-Coder have the similar Sensitivity values for the task of output prediction, and these figures are significantly higher than those of code summarization."}, {"title": "6.3. RQ3. Correctness Correlation", "content": "Figure 7 shows the Robustness and Sensitivity of Code LLMs to the transformations of the correct and incorrect generated code in the task of method name prediction. For the correct code, 84% of the semantic equivalent code snippets are suggested similar names by code LLMs. Mean-while, the corresponding figure for the incorrect code is only 77%. For the semantic non-equivalent code snippets, the cases with different predicted names account for 30% for the correct programs and 35% for the incorrect programs. This demonstrates that code LLMs are more robust to the transformations of the corrected code and more sensitive to the transformations of incorrect code.\nOne of the reasons could be that the incorrect code may deviate significantly from the patterns present in the training data. When applying the transformations to incorrect code, it might introduce changes that are further different from the patterns that code LLMs have learned. Consequently, capturing the meaning of these transformed code snippets becomes more challenging for the models. Therefore, their performance is less stable for the incorrect code and its transformed versions."}, {"title": "6.4. RQ4. Semantic Inference Analysis", "content": "Table 7 shows the prediction performance of Code LLMs in predicting pairs of statements (pairs for short) that have control/data dependence relationships in a given code. Specifically, Precision specifies the ratio of pairs that really have a control/data dependence relationship among the pairs predicted by the models. Meanwhile, Recall is the percentage of pairs having a control/data dependence relationship correctly predicted by the models. F1-Score is the harmonic mean of precision and recall.\nAs seen in Table 7, all the studied LLMs can capture the control dependence much better than the data dependence. For example, DeepSeek-Coder precisely predicted 30-40% pairs of control-dependent statements. Meanwhile, the cor-responding figures for data dependence are only 12-15%. Especially, GPT-3.5 can recognize up to 54% of pairs of control-dependent Python statements, but it totally failed to recognize the data dependence, only 1% in Recall.\nOne of the reasons why code LLMs are more effective at capturing control dependence than data dependence is that the control dependence is often expressed by clear markers in code. Specifically, control dependence refers to the relationship between code statements to determine which statements are executed under certain conditions. This relationship is explicitly expressed in structures like if-else blocks or loops. This makes it easier for code LLMs to iden-tify and understand this kind of relationship. Meanwhile, data dependence is illustrated via variable assignments, us-ages, or transformations. It can be subtle for code LLMs and requires a deep understanding of the code operation to capture data dependence accurately.\nFurthermore, Code Llama and GPT-3.5 perform better on Python than on Java. Meanwhile, DeepSeek-Coder and MagicCoder are stable on both Python and Java. Specifically, for both Java and Python, DeepSeek-Coder and MagicCoder achieved a precision of about 30% in control dependence prediction. The performance of Code Llama and GPT-3.5 on control dependence prediction on Python is 44% and 56%, nearly double their performance on Java code.\nFigure 8 shows an example of the control- and data-dependencies predicted by MagicCoder. As seen, Magic-Coder correctly captured the statements that are dependent on the conditional statements (line 6 and line 17). In partic-ular, it correctly predicted the statements at line 7 and line 9 are control-dependent on line 6. Also, the statements at line 18 and line 21 are control dependent on the statement at line 17. Although it still missed some other statements (line 19 or line 22), which are also control-dependent on the statement at line 17 or the control dependence caused by the for-loop (line 5 and line 16), all of the predicted pairs of control-dependent statements are correct. However, for data dependence of this code snippet, MagicCoder failed to capture and cannot predict any pairs of data-dependent state-ments. Indeed, the data dependence pattern in this source code is expressed via argument passing instead of simply assigning variables. This could be quite complex for the model to capture precisely."}, {"title": "6.5. Threats to Validity", "content": "As for any empirical study, there are several threats to the validity of our results and conclusions. First, a potential threat lies in the design of the prompts. To mitigate this threat, we carefully follow the prompt templates of the studied models to design and optimize the prompts for each code understanding task. Second, there is a certain level of randomness of the studied code LLMs in the generation procedure, which could affect the models' performance. To reduce this threat, we set the temperate value to 0 to minimize the randomness. Another threat could lie in the selection code LLMs. To mitigate this threat, we chose di-verse and representative models, including both open-source and commercial ones. We also conducted experiments on different model sizes. In addition, one threat could come from the evaluation procedure, i.e., different performance metrics may exhibit different biases. To mitigate this threat, for each code understanding task, we measure the similar-ity of the models' outputs for the original code and the transformed code at different levels. For example, for code summarization, the similarity is measured at both semantic and lexical levels."}, {"title": "7. Conclusion", "content": "In this paper, we introduce EMPICA, a novel framework designed to systematically and empirically evaluate the code semantic understanding of code LLMs. Specifically, EM-PICA systematically introduces controlled modifications into the input code and examines the models' responses. For a given code snippet c, if a code LLM understands the semantics of c, the model should produce consistent and robust outputs for c and its semantic equivalent variants while generating different responses for c and its semantic non-equivalent variants. Our experimental results on three representative code understanding tasks, code summariza-tion, method name prediction, and output prediction, re-veal that the robustness and sensitivity of the state-of-the-art code LLMs to code transformations vary significantly across tasks and transformation operators. In addition, the LLMs exhibit better robustness to the semantic preserving transformations than their sensitivity to the semantic non-preserving transformations. These results highlight a need to enhance the model's capabilities of understanding code semantics, especially sensitivity."}]}