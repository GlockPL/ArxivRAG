{"title": "Mirror contrastive loss based sliding window transformer for subject-independent motor imagery based EEG signal recognition", "authors": ["Jing Luo", "Qi Mao", "Weiwei Shi", "Zhenghao Shi", "Xiaofan Wang", "Xiaofeng Lu", "Xinhong Hei"], "abstract": "While deep learning models have been extensively utilized in motor imagery based EEG signal recognition, they often operate as black boxes. Motivated by neurological findings indicating that the mental imagery of left or right-hand movement induces event-related desynchronization (ERD) in the contralateral sensorimotor area of the brain, we propose a Mirror Contrastive Loss based Sliding Window Transformer (MCL-SWT) to enhance subject-independent motor imagery-based EEG signal recognition. Specifically, our proposed mirror contrastive loss enhances sensitivity to the spatial location of ERD by contrasting the original EEG signals with their mirror counterparts-mirror EEG signals generated by interchanging the channels of the left and right hemispheres of the EEG signals. Moreover, we introduce a temporal sliding window transformer that computes self-attention scores from high temporal resolution features, thereby improving model performance with manageable computational complexity. We evaluate the performance of MCL-SWT on subject-independent motor imagery EEG signal recognition tasks, and our experimental results demonstrate that MCL-SWT achieved accuracies of 66.48% and 75.62%, surpassing the state-of-the-art (SOTA) model by 2.82% and 2.17%, respectively. Furthermore, ablation experiments confirm the effectiveness of the proposed mirror contrastive loss. A code demo of MCL-SWT is available at https://github.com/roniusLuo/MCL_SWT.", "sections": [{"title": "Introduction", "content": "Brain-Computer Interface (BCI) establishes a direct communication channel between the brain and a computer, facilitating interaction and communication between cognitive processes and external devices, thus fostering the integration of biological and artificial intelligence [1-3]. Among various paradigms, Motor Imagery Brain-Computer Interface (MI-BCI) stands as a cornerstone, allowing users to manipulate external devices or perform specific tasks by mentally simulating movements. MI-BCI holds promising prospects in motor function rehabilitation [4]. Motor Imagery Electroencephalography (MI-EEG) captures EEG signals during motor imagery tasks, representing non-invasive, endogenous brain activity characterized by user-friendly operation, simplicity, flexibility, non-invasiveness, and minimal environmental requirements [5]. Accurate recognition of EEG signals is paramount for the development of robust subject-independent Motor Imagery (MI) Brain-Computer Interface (BCI) systems.\nPresently, the primary focus of researchers in the realm of motor imagery EEG signal recognition algorithms lies in single-subject BCI systems, necessitating individual modeling of the target subject and yielding fruitful research outcomes [6]. However, these algorithms typically entail individualized calibration procedures, involving the collection of sufficient individual EEG signals and model adjustments [7]. This augments system complexity and calibration duration, thereby diminishing system usability and convenience. In contrast, subject-independent BCI systems endeavor to tackle the issue of inter-subject generalization, enabling BCI systems to better accommodate multiple subjects and expand the horizons of BCI technology applications [8]. Nonetheless, the spatial variance in event-related desynchronization/event-related synchronization (ERD/ERS) phenomena across different subjects presents significant research challenges [9]. Consequently, enhancing the model's ERD/ERS localization capability is imperative for bolstering subject-independent motor imagery EEG recognition performance.\nAlthough deep learning models have been widely applied in recent years for MI based EEG signal recognition, they often function as black boxes and struggle to precisely localize ERD/ERS, a crucial factor in motor imagery recognition. Furthermore, while transformer-based approaches have demonstrated promising capabilities in extracting discriminative features from EEG signals [10], the computational complexity of the global multi-head self-attention mechanism in transformer models increases quadratically with the length of the input sequence. Currently, the majority of transformer models used in MI-EEG recognition employ short input sequences, thus limiting the temporal resolution of the extracted features\nTo address the issues mentioned above, we propose a mirror contrastive loss-based sliding window transformer (MCL-SWT) to enhance subject-independent motor imagery-based EEG signal recognition.\nThe main contributions of this paper are as follows:\n(1) Motivated by neurological findings indicating that the mental imagery of left or right-hand movement induces event-related desynchronization (ERD) in the contralateral sensorimotor area of the brain, the MCL is proposed to enhance sensitivity to the spatial location of ERD by contrasting the original EEG signals with their mirror counterparts-mirror EEG signals generated by interchanging the channels of the left and right hemispheres of the EEG signals.\n(2) A temporal SWT based subject-independent MI-EEG signal recognition model is proposed to achieve high time resolution of feature with affordable computational complexity. Specifically, the self-attention scores are calculated in temporal windows,"}, {"title": "Related work", "content": "In recent years, compared to traditional manually designed feature extraction methods, end-to-end feature extraction and classification approaches based on deep neural networks have demonstrated exceptional performance in the domain of motor imagery brain-computer interface (MI-BCI) [6]. Dai et al. introduced a hybrid-scale convolutional network architecture aimed at extracting temporal features of EEG signals across various convolutional scales for EEG-based motor imagery classification [11]. Yang et al. proposed a dual-branch 3D convolutional neural network for the three-dimensional representation of EEG data related to motor imagery, leveraging separate branches for temporal and spatial feature learning to circumvent mutual interference between these features. Furthermore, their framework introduced central loss to further enhance the decoding accuracy of motor imagery EEG [12]. Schirrmeister et al. delved into the impact of different CNN architecture designs on decoding MI-EEG signals, demonstrating superior performance of their proposed Deep ConvNet and Shallow ConvNet compared to alternative methods [13]. Lawhern et al. introduced a compact convolutional neural network dubbed EEGNet, showcasing its versatility across four BCI paradigms and its superior performance over other methods [14]. Mane et al. proposed a filter-bank convolutional network (FBCNet) for MI classification, leveraging multiple bandpass filters, deep convolutional layers for spatial information extraction, and innovative temporal aggregation techniques, outperforming existing methods on EEG signals from both healthy subjects and stroke patients [15]. Zhang et al. devised a weighted convolutional siamese network (WCSN) based on metric learning for feature representation of EEG signals, enhancing decoding accuracy by learning low-dimensional embeddings and implementing an adaptive training strategy to tackle non-stationarity between sessions [16]. Their method achieved significantly better decoding results on both limb neurorehabilitation and healthy subject datasets compared to state-of-the-art approaches. Hou et al. introduced a novel deep learning framework based on graph convolutional neural networks (GCNs) to enhance the recognition performance of raw EEG signals across various motor imagery tasks by capturing functional topological relationships of EEG channels [17]. Their approach involved constructing the Laplacian graph of EEG channels and employing GCNs-Net for feature extraction, followed by dimension reduction and final prediction through fully connected softmax layers."}, {"title": "Multi-subject MI-EEG signal recognition method", "content": "When implementing a single-subject brain-computer interface (BCI) system on a new subject, conducting experiments to collect EEG data for calibration becomes necessary."}, {"title": "Attention mechanism-based MI-EEG signal recognition method", "content": "The attention mechanism allocates varying attention weights to different data or feature subsets, enabling the model to prioritize key areas, thereby acquiring detailed information about the target of interest while suppressing irrelevant information [20]. The self-attention mechanism calculates attention weights between each position and other positions to determine their significance in processing. It dynamically learns relationships across different positions in a sequence and addresses long-range dependencies [21]. Consequently, numerous researchers have integrated the attention mechanism with convolutional neural networks (CNNs) to construct models. Zhang et al. designed a convolutional recurrent attention model (CRAM), which uses CNN to encode the spatiotemporal information of EEG signal and establishes a recurrent attention mechanism to explore temporal dynamics between different time periods[22]. Altaheri et al. developed an attention-based temporal convolutional network (ATCNet) model for MI-EEG signal classification. ATCNet, a domain-specific and interpretable deep learning model, highlights valuable features in motor imagery EEG data using a multi-head self-attention mechanism and extracts advanced temporal features with a time convolutional network [23]. Wen et al. designed a CNN-based model architecture for end-to-end training and classification, incorporating a spatial-spectrum-temporal (SST) attention mechanism to adaptively extract the most expressive features from EEG data. Additionally, they proposed a 3D Densely Connected Cross-Stage-Partial Network to segment extracted feature maps, reducing gradient loss and enhancing the model's representational capacity and robustness [24]. Amin et al. proposed a hybrid deep learning model architecture. Firstly, they employed the attention-inception convolutional neural network to extract spatial contextual features, which is crucial for learning the dynamic characteristics of EEG signal. Then, they utilized bidirectional long-short-term memory (Bi-LSTM) to learn temporal features[25]. Li et al. devised a temporal-spectral-based squeeze-and-excitation feature fusion network (TS-SEFFNet) for decoding motor imagery EEG signals. Their model incorporates a deep-temporal convolution block to extract high-dimensional information from EEG data, a multi-spectral convolution block for powerful spectral feature extraction, and a squeeze-and-excitation feature fusion block based on attention mechanism to enhance decoding performance [26]. Dong-Hee Ko proposed an attention-based deep learning approach to extract spatio-spectral features based on significant frequency bands for each subject. The method comprises three parts: extracting spatio-temporal features based on multiple frequency bands, utilizing sub-band attention to identify important frequency bands, and implementing an attention-based bidirectional long short-term memory network to extract time dynamic features [27]. Fan et al. proposed a new network structure for motor imagery EEG classification, called QNet. It includes a newly designed attention module (3D-Attention Module, 3D-AM) for learning attention weights of EEG channels, time points and feature maps. QNet uses a two-branch structure to learn more characteristics. After merging the dual branches, bilinear vectors are obtained. Finally, a fully connected layer is used as classifier[28]. Tao et al. proposed a novel solution called attention-based dual-scale fusion convolutional neural network (ADFCNN), which jointly extracts spectral and spatial information of EEG signal at different scales and provides new insights into integrating effective information from different scales through self-attention[29]. Li et al. introduced the depth-shallow attention multi-frame fusion network (DSA-MFNet) specifically designed for classifying motor imagery EEG signals. DSA-MFNet comprises the depth-shallow attention module for advanced feature extraction and the multi-frame fusion module for exploring inherent temporal variations in EEG data through multi-frame segmentation and recombination techniques [30].\nIn addition to embedding attention mechanism into convolutional neural network, transformer based models were proposed and applied in EEG decoding[31]. Besides the natural language processing field, successful vision model like Vision Transformer and Swin Transformer were proposed[32][33]. Google introduced a novel and streamlined network architecture known as the transformer model, specifically designed for sequence modeling and prominently featuring a self-attention mechanism. Departing from traditional convolutional and recurrent layers, the transformer model relies on a multi-head self-attention mechanism. This architecture has garnered significant success in natural language processing and machine translation tasks, demonstrating superiority in handling long-range dependencies and capturing global contextual information [31]. Subsequently, the computer vision community began adopting transformer models. Notably, the vision transformer model was proposed, which segments images into fixed-size patches and feeds them into an enhanced transformer model, outperforming CNN models. Leveraging multi-head self-attention mechanism and positional encoding, this"}, {"title": "Loss function applied in MI-EEG signal recognition method", "content": "The loss function, serving as the optimization objective for training neural network models, has garnered significant attention and research focus within the realm of motor imagery recognition. Zhang et al. augmented the loss function by integrating regularization terms for acquired weights, employing squeeze-and-excitation modules to derive weights of EEG channels based on their contributions to EEG classification. They also devised an automated channel selection strategy. To fully exploit spatiotemporal characteristics, they proposed a convolutional neural network that notably outperformed traditional classification methods [34]. Autthasan et al. introduced a multi-task learning model termed MIN2Net, adept at extracting meaningful features from EEG data sans high-complexity preprocessing. Excelling in multi-subject motor imagery EEG classification, MIN2Net achieves end-to-end training through amalgamation of an autoencoder, deep metric learning, and supervised classifier. The autoencoder module aids in feature extraction from EEG data and furnishes discriminative patterns for diverse classes. The deep metric learning module aims to enhance feature discriminative power by refining distance measurement learning, while the supervised classifier module utilizes a standard softmax classifier to categorize latent vectors of input EEG signals. To derive a compact and distinctive latent representation from EEG signals, the model simultaneously minimizes reconstruction loss function, cross-entropy loss function, and triple loss function during optimization [35]."}, {"title": "Method", "content": "This section provides a detailed description of the MCL and SWT model."}, {"title": "Notations and definitions", "content": "The original EEG signal is defined as $X \\in \\mathbb{R}^{T\\times C}$, where T is the number of sampling points and C is the number of EEG channels. The raw EEG signal X serves as the input of the MCL-SWT model, with a batch size of B, resulting in an input data dimension of $B \\times 1 \\times T \\times C$."}, {"title": "Mirror contrastive loss", "content": "In EEG signals, imagining movements of the left or right hand can elicit the ERD phenomenon in the sensory-motor area on the contralateral hemisphere of the brain, and the ERS phenomenon on the same side of the brain. The accurate identification and localization of ERD/ERS are crucial criteria for MI classification. However, existing deep learning models only operate as black boxes and struggle to precisely locate the ERD/ERS phenomenon, leading to suboptimal results. To address this challenge and improve ERD/ERS localization ability of MI recognition model, we propose the MCL in this section. The framework of MCL based SWT model is illustrated in Fig. 1."}, {"title": "SWT model", "content": "The overall architecture of the SWT model consists of three parts: the CNN-based feature extraction module, the sliding temporal window based multi-head self-attention module and the classification module, as shown in Fig. 3.\nCNN-based feature extraction block. Initially, drawing inspiration from the bandpass filter utilized in the FBCSP algorithm [36], a temporal convolution $C$ with a kernel size of $T\\times1$ is used processing temporal information. Second, a spatial convolution with a kernel size of $1\\times C$ is applied to fuse information from each EEG channel. Finally, a batch normalization layer aiming to normalize the distribution of data in a batch is added [37]. The CNN-based feature extraction module can learn features with spatio-temporal information which can be described as\n$F = BN(C(C(X)))$"}, {"title": "Temporal Sliding Window based Multi-head Self-attention Module", "content": "The temporal sliding window-based multi-head self-attention module is devised to establish a multi-head self-attention layer along the temporal dimension, aimed at capturing discriminative features of EEG signals with manageable computational load. This module comprises two stages, as depicted in Fig. 1. In the first stage, multiple self-attention layers are employed based on a temporal window, while the second stage facilitates information exchange between sequences in adjacent windows. Each stage is composed of two layer normalization (LN) layers, a temporal window multi-head self-attention layer, and a multi-layer perceptron (MLP) layer, integrated within a residual network structure. The sole discrepancy between these two stages lies in the division of temporal windows. A detailed description of each layer in this module is provided below:\n(1) LN layer performs a layer normalization on the output feature F of the CNN-based feature extraction module, normalizing the values along the feature dimensions of each sample[38].\n(2) Temporal window multi-head self-attention layer (TW-MSA) is used to extract local temporal dependencies of input vectors in the time dimension, computing attention scores within a single window. It takes the layer-normalized feature as input, then segments it into a set of non-overlapping windows of size M. Self-attention scores computation is performed within each local window to enhance the model's perception of local information, and improve computational efficiency, as shown in Fig. 3. The window size in TW-MSA is empirically set to 8. The specific calculation steps of TW-MSA are as follows:\n$Q_{l} = W^{Q} \\cdot LN (F)$,\n$K_{l} = W^{K} \\cdot LN (F)$,\n$V_{l} = W^{V} \\cdot LN (F)$,\n$h_{i} = Attention(Q_{l}, K_{l},V_{l}) = Softmax(\\frac{Q_{l}K_{l}^{T}}{\\sqrt{d_{k}}})V_{l}$,\n$O = concat(h_{1},\\dots,h_{H})W^{O} + F$\nwhere $h_{i}$ represents the i-th attention head, H denotes the number of attention heads, $W \\in \\mathbb{R}^{H\\times d_{model}}$ represents the linear transformation matrix, $d_{model}$ is the dimension of the input embedding. $W^{Q} \\in \\mathbb{R}^{d_{model}\\times d_{q}}, W^{K} \\in \\mathbb{R}^{d_{model}\\times d_{k}}, W^{V} \\in \\mathbb{R}^{d_{model}\\times d_{v}}$, are the corresponding weight matrices. $d_{q},d_{k},d_{v}$represent the dimensions of the query, key, and value respectively.\n(3) MLP layer consists of a fully connected layer, a GELU[39] activation function layer, and another fully connected layer. Finally, the results of the input and output features are summed to produce the final output of the residual network structure.\n$A=O+FC(GELU(FC(LN(O))))$"}, {"title": "Classification module", "content": "In the classification module, a square non-linear function is first applied. An average pooling layer is used to reduce the temporal dimension of features, followed by a logarithmic activation function. The EEG features are then inputted into a fully connected layer for classification. Finally, the softmax function is applied to compute the prediction probabilities. Assuming the output of the temporal multi-head self-attention module is A, the output of the classification module is:\n$Y = Softmax(FC(Log(AvgPool(Square(A)))))$"}, {"title": "Experimental data and setup", "content": "The effectiveness of our proposed method was evaluated on BCI Competition IV dataset 2a and BCI Competition IV dataset 2b[42]."}, {"title": "Experimental setup", "content": "In the experiment, the window size M was set to 8. To divide the input feature vector of length L into a whole number of non-overlapping windows, a 4.48-second EEG signal segment with 1120 sampling point from 0.5 s before MI cue onset to 3.98 s after MI cue onset was used. The bandpass filtering with a range of 4-38 Hz or 0-38 Hz was applied to the EEG signal, followed by channel-wise logarithmic sliding normalization. The preprocessed EEG signal was then input into the MCL-SWT model. During model training, Adam optimizer[43] was used as the optimization method with a weight decay parameter set to 0.05. As the new subject experiment setup is applied, the two category and three channels consisting of C3, Cz, and C4 MI classification task as in dataset 2b is employed in this paper. Since the batch size is set to 100, the input data dimension would be 100 \u00d7 1 \u00d7 1120 \u00d7 3, and the output shapes and parameter quantities for each layer are shown in Table 1. The temporal convolution kernel T is set to 25, and the"}, {"title": "Dataset Division", "content": "This paper aims to investigate the classification performance of subject-independent motor imagery EEG signals, so the dataset is partitioned in a new subject setup as follows: when utilizing the training sessions of all 9 subjects in dataset 2a as the training set, the testing sessions (session 4 and session 5) of all 9 subjects in dataset 2b are employed as the testing set; conversely, when using the training sessions (session 3) of all 9 subjects in dataset 2b as the training set, the testing sessions of all 9 subjects in dataset 2a are utilized as the testing set. 5. Consequently, the trial data used for testing and training originate from entirely different subjects."}, {"title": "Results", "content": "This section evaluates motor imagery recognition performance of the MCL-SWT model on new subjects setup. Since the subjects in dataset 2a are entirely different from those in dataset 2b, the subjects in the testing set are new. As training typically converges within 500 epochs, the maximum number of epochs for training was set to 500 to ensure model convergence. Due to significant randomness affecting the test accuracy at a specific epoch, the evaluation metrics used in the experiment include: (1) Maximum test accuracy over 500 epochs (Max Accuracy); (2) Average test accuracy from epochs 401 to 500 (Average Accuracy); (3) Test accuracy at the epoch with the lowest training loss (Accuracy). Table 2 presents the results in terms of \"Accuracy/Kappa coefficient,\" with the maximum result highlighted in bold. The first row indicates the encoding format as \"dataset - bandpass filter.\" For example, \"2a-0 Hz\" means the model was trained on dataset 2b and tested on dataset 2a using a 0-38 Hz bandpass filter for preprocessing. Five state-of-the-art models were compared in the experiment, including Shallow ConvNet, Deep ConvNet, EEGNet, FBCNet, and ATCNet. In additional, the ablation experiments are also conducted. The SWT and MCL-SWT indicate the SWT model without and with MCL, separately.\nThe experimental results in Table 2 lead to the following conclusions: (1) MCL-SWT achieved accuracies of 66.48% and 75.62%, surpassing the best state-of-the-art model by 2.82% and 2.17%, respectively. (2) The proposed temporal MCL-SWT model demonstrates superior performance across different datasets and bandpass filters. (3) MCL contributes to a further increase in the recognition accuracy and kappa value of SWT. (4) The proposed SWT model exhibits resistance to overfitting, as evidenced by the small gap between the maximum accuracy/Kappa and the average accuracy/Kappa of SWT. (5) The experimental results suggest that MCL-SWT possesses better inter-subject generalization ability."}, {"title": "Sensitivity Analysis of Parameters", "content": "The number of temporal multi-head self-attention blocks and the number of heads in each self-attention block are two primary hyperparameters of the SWT model. This section investigates the impact of these hyperparameters on the model's performance, with the experimental results presented in Table 3. Increasing the number of heads or blocks leads to larger attention scores, resulting in a more complex model."}, {"title": "Model complexity analysis", "content": "The number of parameters and the inference time of the model are important indicators for assessing complexity. Therefore, this section compares these two aspects. Inference time is measured as the average time taken for 1000 runs. The experiments were conducted on Intel i7 10700K and NVIDIA GeForce RTX 3090. The specific experimental results are shown in Table 4."}, {"title": "Conclusion", "content": "In this paper, we propose a Mirror Contrastive Loss (MCL) based on Sliding Window Transformation (SWT) model for subject-independent EEG signal recognition. By leveraging mirror EEG signals and MCL, the model aims to enhance sensitivity to the spatial location of ERD/ERS by contrasting the original EEG signals with their mirror counterparts. This modification is intended to improve the model's spatial awareness of ERD/ERS occurrences. Additionally, we introduce a temporal SWT that calculates self-attention scores in sliding windows, enhancing model performance with manageable computational complexity. The performance of MCL-SWT was evaluated on subject-independent motor imagery EEG signal recognition tasks. Experimental comparisons with state-of-the-art methods and ablation experiments demonstrate the superior performance of MCL-SWT. The proposed MCL serves as a general loss for MI-EEG recognition and can be integrated into various backbone networks. In future work, we aim to explore combining the proposed MCL with transfer learning in MI-EEG recognition."}]}