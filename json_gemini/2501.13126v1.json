{"title": "Preference Curriculum: LLMs Should Always Be Pretrained on Their Preferred Data", "authors": ["Xuemiao Zhang", "Liangyu Xu", "Feiyu Duan", "Yongwei Zhou", "Sirui Wang", "Jingang Wang", "Xunliang Cai"], "abstract": "Current large language models (LLMs) generally utilize a consistent data distribution throughout the entire pretraining process. However, as the model's ability improves, it intuitively should be pretrained with differentiated data. To achieve it, we propose the Perplexity Difference based Preference Curriculum learning (PDPC) framework, which always perceives and uses the data preferred by LLMs to train and boost them. Firstly, we introduce the PD metric to measure the difference in how well strong and weak models fit the samples. Samples with high PD are more challenging for weak models to learn and are more suitable to be arranged in the later stage of pretraining. Secondly, we propose the PD preference function to approximate the model and predict the data preference of the LLM at any time, so as to complete the arrangement of the entire data offline and ensure continuous training without interruption. Experimental results on 1.3B and 3B models demonstrate that our PDPC significantly surpasses baselines. Notably, the 3B model achieved more substantial gains, with an increased average accuracy of over 4.1% across various benchmarks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown impressive performance across a variety of tasks after being pretrained on vast amounts of data (Touvron et al., 2023; Dubey et al., 2024; Liu et al., 2024). As LLMs undergo extensive pretraining, their capabilities steadily improve, which influences their performance, characteristics, and data preferences (Yu et al., 2024). Existing methods of uniformly sampling data during the whole pretraining process are suboptimal because they overlook the model's evolving data preferences(Wettig et al., 2024; Abbas et al., 2023; Sachdeva et al., 2024).\nRecently, some research has shifted the focus to the influence of data on model capability during the pretraining process(Evans et al., 2024; Yu et al., 2024; Koh and Liang, 2017; Ko et al., 2024). A typical method named MATES considers the changing data influence on models but requires interrupting the training process to select more preferred data based on the model's current state, which disrupts training continuity and stability, ultimately reducing pretraining efficiency (Yu et al., 2024). A similar issue arises in JEST (Evans et al., 2024).\nIn this paper, we propose perplexity (Ziegel et al., 1976) difference (PD) to measure the difference in the fit of LLMs' final and early checkpoints. We further propose a novel PD-based Preference Curriculum learning framework, PDPC. It's required to perceive LLMs' data preference at any training step and use the preferred data to continuously pretrain LLMs without interruption, thereby boosting their performance. To this end, PDPC needs to solve three major challenges:\nHow to dynamically perceive preferences without interrupting pretraining. Ideally, training would be paused at any point to calculate PD using the model's current state, allowing for adjustment of the preference distribution like MATES(Yu et al., 2024). However, to ensure continuity and stability in pretraining, we propose an offline processing method to approximate the ideal dynamic preference adjustment. First, we calculate PD for all"}, {"title": "2 PD-based Preference Curriculum", "content": "We propose the PD-based Preference Curriculum learning framework, PDPC, which always perceives and uses the data preferred by LLMs to pretrain and boost them. PDPC is used as the last key data preprocessing step before pretraining. It only arranges the given data without other changes.\n2.1 Problem Formulation\nGiven a pretraining dataset D with an initial random distribution, we aim to transform it into one that closely approximates the oracle distribution O{BK}, which represents the ideal data distribution preferred by the model. To achieve this, we aim to find the optimal preference function f* that adjusts the joint distribution of all batches {B}=1 to closely approximate O{Bk}.\nf* = arg min Divergence (P{B}, O{BK})\nf\nK\ns.t. \u03a3Bk =D, Bk = f()\nk\nwhere P{B} represents the joint distribution of the batches, and K is the total number of batches.\n2.2 PD-based Data Partitioning\nPerplexity Difference (PD). We begin by introducing the concept of PD. Consider two models, the weak model Mw and the strong model Ms, both"}, {"title": "3 Experiments", "content": "3.1 Setup\nSetup We trained two experimental models: a 1.3B model using 100B randomly selected tokens from the SlimPajama dataset (Soboleva et al., 2023), and a 3B model on a bilingual dataset containing 1T tokens. The bilingual dataset comprises 500B tokens each of Chinese and English data, sourced from domains such as books(Gao et al., 2020), blogs(Baumgartner et al., 2020), patents(Sharma et al., 2019), Common Crawl(Penedo et al., 2024), and Wikipedia, similar to the Matrix dataset(Zhang et al., 2024a). We train 100M and 700M reference models (RMs) on i.i.d. subsets from SlimPajama (50B) for the 1.3B setting, and 100M and 1.3B RMs on i.i.d. subsets for the 3B setting, respectively. All models were trained using the Llama architecture(Touvron et al., 2023) within the Megatron framework (Shoeybi et al., 2019), utilizing the Adam optimizer. We set batch size to 480 and the context window length to 8192. The initial learning rate is set to 2e-4, with a warm-up phase of over 375M tokens. We adopt a cosine learning rate schedule and set weight decay to 0.1."}, {"title": "3.2 Main Results", "content": "Table 1 and 2 present our primary experimental results, revealing several key insights:\nEffectiveness of our PDPC framework. PDPC with n = 2 consistently outperforms all baselines, regardless of the metric used, showing significant performance and convergence improvements over the baseline. Notably, the 3B model trained on 1T tokens with PDPC demonstrates an average accuracy increase of 4.1% across all benchmarks and 8.1% across MMLU and CMMLU, highlighting the effectiveness of our framework. Figure 3 depicts performance improvements in the 1.3B and 3B models as training progresses, with our method significantly outperforming Random in the latter half of pre-training. In this phase, data dominated by high PD is crucial, especially in the 3B model, highlighting the effectiveness of transitioning from small to large PD data, which significantly boosts model performance and promotes emergent capabilities.\nThe Sequential method can somewhat constrain model performance. Sorting pretraining data by PD from low to high (Sequential, PD,\u2191) can outperform random sorting but falls short of the (PDPC, PD, S.) strategy. This limitation arises because sorting strictly by PD reduces data diversity, leading to homogeneity that restricts the model's ability to handle complex tasks. In extreme cases, if the dataset contains duplicate samples, a complete sort would likely place identical samples in the same batch, which is detrimental to improving pretraining efficiency.\nPD performs better than other metrics in the Preference CL framework. Comparison of the results from 'Preference CL, PPL, S.', 'Preference CL, Qu.Edu, S.', and 'PDPC, PD, S.' shows that using PD as a metric yields the best results, particularly on the ARC-C and SciQ datasets. PD, by measuring the performance difference between weak and strong models on samples, can more accurately reflect the relative difficulty and complexity of samples. In contrast, relying solely on metrics like PPL or educational value may not fully capture the differences in learning difficulty between samples.\n3.3 Ablation Study\nImpact of PD calculation methods We focus on two main factors: (1) Size of the PD RM: We used PD calculated from various model combinations. (2) Choice of the RM: We use early and late"}, {"title": "3.4 Analysis", "content": "Training loss We use 500M tokens from SlimPajama as our test set to compare test loss with the Random model on the 1.3B setting. Figure 5(a) shows that PDPC's test loss initially declines slowly, then rapidly decreases, achieving a lower loss than Random by incorporating higher PD data later in training. The S-Shape function effectively minimizes loss, confirming optimal learning outcomes. Additionally, Figure 5(b) demonstrates that our method stabilizes the gradient norm, preventing oscillations and ensuring smoother model convergence. In the 3B model experiments (Figure 5(c) and 5(d)), the S-shape function significantly outperforms Random. It accelerates early loss reduction, speeds up convergence, and achieves a lower final loss, demonstrating superior performance. Additionally, it stabilizes the gradient norm.\nInvestigation of different preference functions Table 3 presents the effects of the three preference functions in Section 2.3. Experimental results indicate that the S-Shape function outperforms other functions. Its slower initial decline compared to the"}, {"title": "3.5 Case study", "content": "Data Source Distribution Our analysis of 25M SlimPajama samples, sorted by PD values, shows significant distributional and semantic differences. High PD data mainly comes from Wikipedia and CommonCrawl, while low PD data is sourced from arXiv and GitHub, as seen in Figure 6. The significant performance improvement of our framework in later stages is largely attributed to the higher proportion of high-quality data from sources like Wikipedia, which primarily appears in the high PD data and is less prevalent in the low PD data. Semantic visualization using Sentence-T5 and t-SNE in Figure 7 illustrates data sampled in two ways: (a) uniformly from different PD partitions and (b) from extreme PD intervals after sorting. Uniform sampling results in high and low PD data being evenly distributed in semantic space, indicating semantic diversity. In contrast, extreme PD sampling leads to distinct semantic spaces, explaining the suboptimality of the PD sequential method due to induced data homogeneity, which can impede effective model training and generalization.\nData Quality Distribution We use raters across four QuRating dimensions to assess data quality under different PD values. Figure 8 shows consistent quality distributions in both high and low PD partitions, ensuring uniform quality throughout training and preventing the model from disproportionately learning from lower-quality data at any stage.\nStability of PD We evaluate the Spearman correlation coefficients between different PD types and find a strong correlation among PDs derived from RMs of varying sizes, which indicates that the ratio-based approach to computing PD maintains relative stability across samples. Calculating PD with smaller RMs produces results consistent with larger RMs, optimizing computational resources."}, {"title": "4 Related Works", "content": "4.1 Curriculum Learning\nCurriculum Learning (CL) is an adaptive training strategy that has gained attention in deep learning (Forestier et al., 2022; Soviany et al., 2021; Ganesh and Corso, 2020; Luo et al., 2020; Wang et al., 2022; Shi et al., 2024). It involves organizing training data to enable models to learn from simple to complex, enhancing efficiency and generalization. CL strategies are widely applied in NLP (Feng et al., 2023; Paulsen, 2023; Li et al., 2024b). Collobert and Weston enhanced word embedding models using CL (Collobert and Weston, 2008). Recent advancements include Platanios et al.'s competence-based curriculum for neural machine translation, which reduces training time and improves translation quality (Platanios et al., 2019), and Xu et al.'s application in natural language understanding, boosting performance and convergence (Xu et al., 2020). In LLMs pretraining, CL is also applied (Wu et al., 2024; Wettig et al., 2024).\n4.2 Data Preprocessing for LLM Pretraining\nIn LLM pretraining, data preprocessing is crucial for ensuring dataset quality and relevance. Traditional methods apply expert-crafted rules to filter low-quality data (Raffel et al., 2020; Rae et al., 2021; Lauren\u00e7on et al., 2022; Computer, 2023; Penedo et al., 2024) and remove duplicates to maintain integrity (Lee et al., 2022; Sorscher et al., 2022; Abbas et al., 2023; Soboleva et al., 2023; Tirumala et al., 2024). Although superficially effective, these"}, {"title": "5 Conclusion", "content": "In this paper, we introduce the PDPC framework to optimize LLMs by dynamically adjusting pretraining data distribution. PD identifies challenging samples for weak models, scheduling them for later training stages. PDPC perceives and utilizes data preferred by LLMs, ensuring uninterrupted and continuous training while serving as the final data preprocessing step. Experiments demonstrate significant performance improvements over baselines, with the 3B model achieving over 4.1% gains in average accuracy, underscoring PDPC's effectiveness."}, {"title": "6 Limitations and Future Works", "content": "Exploration of additional PD partitions This study primarily focuses on the scenario where n = 2, analyzing concentration mixing curves and systematically blending two subsets with higher and lower PD in accordance with training progression. However, we have not yet explored dividing the training data into more than two subsets to assess whether further performance enhancements are attainable. In future research, we plan to investigate cases where n > 2 and develop novel methodologies for addressing learning curves.\nIterative update of learning curves We determined the S-shaped learning curve through functional exploration and used it as the basis for arranging the data sequence to train the model. In fact,"}, {"title": "A Social Impact", "content": "Due to the influence of training data, LLMs are prone to generating untruthful or socially harmful content. We aim to mitigate this issue by enhancing the reliability of model training and the model's"}, {"title": "B Preliminary Exploration of Iterative Optimization of Preference Functions", "content": "When employing a grid search methodology, the size of the solution space scales as n\u1d40, where T represents the number of training stage divisions. Consequently, an increase in the number of domains n results in an exponential expansion of the solution space.\nIn Section 2.3, we introduce a curriculum learning method that divides pretraining data into 2 domains and identifies the S-shape preference function through theoretical analysis and function search. This method is simple and efficient. However, in resource-rich scenarios, we also offer a more precise approach to simulate the model's preferences at different pretraining stages. Specifically, after using the discovered preference function to guide the model's pretraining, we conduct annealing experiments on checkpoints from different stages to explore the model's preferences for data mixing ratios. Based on these preferences, we fit the model's preference function to guide the next round of pretraining. This process is iterated until the model's performance converges."}, {"title": "B.1 Proportion Preference Annealing Experiment", "content": "In this subsection, our goal is to systematically explore the model's preference for sample mixing ratios based on pd values (i.e., ppl differences) at different pretraining stages. This process is crucial for understanding the dynamic changes in model preferences during the learning process and provides empirical evidence for optimizing curriculum learning strategies.\nFirstly, we construct the dataset required for the annealing experiment. Based on the median of pd values, samples are divided into two groups: pd-small and pd-large. The median is set as the threshold, with samples having pd values less than the median classified as pd-small, and those greater as pd-large. From the pd-small group, we construct annealing datasets with varying concentra-"}, {"title": "B.2 Iterative Curriculum Learning", "content": "After completing the proportion preference annealing experiment and successfully fitting the preference function b = f(p), we apply this function to optimize curriculum learning strategies, guiding the pretraining process of the model. Specifically, based on the fitted function f (p), we dynamically determine the optimal proportion b of pd-small sam-"}, {"title": "C Experimental Details", "content": "C.1 Detailed performance on the benchmarks\nFigure 11 illustrates the changes in detailed metrics as training progresses, showing few-shot downstream performance on various benchmarks with respect to pretraining iterations for Random and (PDPC, PD, S.). We train a 3B model over 1T tokens, demonstrating superior performance with our approach.\nC.2 PD distribution across different domains\nAs illustrated in Figure 12, large size discrepancies among RMs result in broader PD distributions, which enhance data differentiation. This finding is supported by ablation tests, where the 100M-1.3B PD calculations yielded the best results. Additionally, PD maintains a consistent distribution across domains; for instance, the PD between 100M and 700M models typically follows a normal distribution with a mean of 0.27. This stability makes PD a reliable metric, as it remains unaffected by changes in data properties. Consequently, organizing training data by PD ensures it does not skew towards specific sub-domains, allowing the model to encounter diverse data at every stage.\nC.3 PPL Distribution\nAs shown in Figure 13, examining the perplexity distribution across data with varying PD values reveals that samples with lower PD exhibit lower perplexity. This observation aligns with the trends illustrated in Figure 5c, where training with smaller PD leads to a rapid decrease in training loss during the initial stages, providing the model with a clear direction for gradient optimization. Notably, even when handling data with larger PD values in later stages, the model maintains a steady rate of loss reduction, ultimately achieving a minimized training loss.\nC.4 Prompts for Case Study\nThe prompt and specific rules used in Section 3.5 to analyze the linguistic features of data across different PD intervals is as follows."}, {"title": "D Case Study", "content": "Table 4: Samples are divided into 10 Probability of Default (PD) quantiles, with two samples representing each quantile.\n0-10%\nSample 1: The need to practice good self-care doesn't change in this working environment, but how you accomplish this goal might. Much of Arel's own self-care regimen needed to be adjusted.\"I was used to weekly massage and monthly chiropractic care. That was gone,\" she explains. \"I am used to runs and yoga and time to meditate in complete silence. That was gone, too.\" Although there was an onboard fitness center available, Arel often found that she couldn't get a workout in during the day and was too tired at the end of the day to be invested in the results.Related...\nSample 2: I have to ask you, why'd you-wha-wha-why are you peeing right here? Creepy Guy: What?Kumar: I mean... why'd you pee right next to me when you could like, choose that bush, or-?Creepy Guy: Well, this bush looked like I should pee on it. Why are you peeing on it?Kumar: Well, no one was here when I chose this bush.Creepy Guy: Oh, so you get to pee on it and no one else does? Huh?Kumar: No, it's just-I just-Creepy Guy: This your bush? You have a special bond with this bush?Kumar: No, I just thought that-Creepy Guy: You the king of the forest...\n10-20%\nSample 1: boolean insertventas() String sql \"INSERT INTO ventas (id_venta, venFechaventa, venId_cliente, venIdadministrador, venTotalventa) VALUES (NULL, \"+ vent() +\", \"+ clasu.getId_usuarios() +\", '1', \"+ pnlProductos.totall + \"')\"; try { con = cn.getConnection(); ps = con.prepareStatement(sql); ps.executeUpdate(); return true; } catch (SQLException ex) { Logger.getLogger(LogicaSql.class...\nSample 2: In terms of providing shorter stay parking, Bell Street multi storey car park is identified as a long stay car park, and the tariffs are so designed to encourage the use of the facility by all day / half day parkers with more flexible tariffs available at other car parks and the on street spaces around the vicinity allow for parking for up to one hour. I have commented that there is no short term (30 minutes to 2 hour) parking available at the West Bell Street multi-storey car park and"}]}