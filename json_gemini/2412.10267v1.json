{"title": "Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT", "authors": ["DANIELLE R. THOMAS", "CONRAD BORCHERS", "SANJIT KAKARLA", "JIONGHAO LIN", "SHAMBHAVI BHUSHAN", "BOYUAN GUO", "ERIN GATZ", "KENNETH R. KOEDINGER"], "abstract": "The role of multiple-choice questions (MCQs) as effective learning tools has been debated in past research. While MCQs are widely used due to their ease in grading, open response questions are increasingly used for instruction, given advances in large language models (LLMs) for automated grading. This study evaluates MCQs effectiveness relative to open-response questions, both individually and in combination, on learning. These activities are embedded within six tutor lessons on advocacy. Using a posttest-only randomized control design, we compare the performance of 234 tutors (790 lesson completions) across three conditions: MCQ only, open response only, and a combination of both. We find no significant learning differences across conditions at posttest, but tutors in the MCQ condition took significantly less time to complete instruction. These findings suggest that MCQs are as effective, and more efficient, than open response tasks for learning when practice time is limited. To further enhance efficiency, we autograded open responses using GPT-40 and GPT-4-turbo. GPT models demonstrate proficiency for purposes of low-stakes assessment, though further research is needed for broader use. This study contributes a dataset of lesson log data, human annotation rubrics, and LLM prompts to promote transparency and reproducibility.", "sections": [{"title": "1 Introduction", "content": "The effectiveness of multiple-choice questions (MCQs) in learning is the subject of much debate [3, 16, 18]. Although MCQs are often criticized for lack of depth, they remain a common feature in K-12 and higher education, due to their ease of grading [3]. However, their potential as instructional tools, rather than just assessment tools, meaning that they provide feedback from which students can learn, has received less attention. In contrast, open-response questions are frequently used in assignments such as homework, under the assumption that they promote deeper learning [3, 16]. However, open responses can be more time consuming for learners and resource intensive to grade [3], although recent advancements in the field have made the automated grading of these responses more feasible. This study evaluates the effectiveness of MCQs in relation to open-response questions, both individually and in combination, as learning-by-doing activities. These learning-by-doing activities are embedded in six tutoring lessons that involve advocacy training. To investigate scalability of autograding open-ended responses, we use generative AI to evaluate tutors' open responses. The contributions of this work are twofold: theoretically, it offers insights into the learning benefits of MCQs compared to open responses in learning-by-doing instruction; and practically, it provides implications for optimizing tutor training by determining the most efficient method of instructing and assessing tutors, as measured in the completion time of instruction and accuracy of automated open-response grading. Furthermore, this study contributes a dataset of lesson log data, human annotation rubrics, and AI-generated generative commands to improve transparency, reproducibility, and collaboration within the learning analytics community.\nThe design of instructional materials plays a critical role in promoting effective learning. MCQs are often favored for their efficiency-they can be administered and graded quickly and require less time for learners to respond, making them appealing in large-scale settings [3, 16, 18]. However, MCQs are sometimes criticized for promoting surface-level learning, as they may encourage guessing and recognition rather than deeper understanding [3]. In contrast, open-response questions require tutors to construct their answers, thereby engaging in higher-order thinking and reflection. Although open-response questions can be powerful pedagogically, they are also more time-consuming to complete and evaluate [3]. A key question is whether MCQs can be designed to be as pedagogically effective as open-response questions within the context of teaching tutors advocacy skills, particularly in scenarios where scalability is a concern [16]. Advocacy, an emerging area of instruction aimed at improving equity and inclusion in tutoring, is particularly suited for a comparison of MCQ to open-ended responses, because the skills it requires-such as critical thinking and ethical reasoning-are potentially more effective for comprehension when practiced through open-ended formats [3] rather than MCQs where generating distractors can pose a challenge [13]. Compared to STEM learning, where many closed-form grading systems exist (e.g., tutoring systems), there is a need in learning analytics to study what forms of instruction are effective in novel, less structured domains like advocacy training.\nGenerative AI, particularly large language models (LLMs), have the capability to evaluate tutors' textual, open responses in real-time. LLMs such as GPT-4 [31], Claude [5], and LLaMa [37] have demonstrated remarkable performance in a variety of linguistic tasks. These modern LLMs are built on a large-scale transformer architecture and trained on extensive datasets [2, 14]. As a result, LLMs have attracted substantial interest from researchers across various fields, including education, because of their potential to perform reasoning tasks at scale and with reduced costs. Generative Al systems can evaluate human tutor responses across a wide range of scenarios, providing feedback and assessment at a scale that would be impossible for human evaluators alone. Importantly, LLMs may have the potential to make situational judgments, assessing not only the correctness of a response but also underlying reasoning [2]. This capability is crucial in scenario-based training, where tutors must navigate complex real-world situations. However, despite their"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Tutor Advocacy Skills and Scenario-based Training", "content": "Tutoring is widely recognized as one of the most effective interventions for improving student learning outcomes [17, 30, 32]. Research consistently shows that personalized support from skilled human tutors can significantly boost student academic performance, particularly among struggling students [34]. However, ensuring access to adequately trained tutors is challenging [6, 24], with many tutoring organizations relying on paraprofessionals. Many paraprofessional tutors have college education, but lack formal training and in providing instruction and building quality relationships with students [6, 30]. In addition, very limited instructional materials are available for tutors on attending to students' social-emotional needs. The process of training human tutors presents substantial scalability challenges, such as the need for human evaluators to assess tutor performance. Traditional methods of tutor training and evaluation are both time-consuming and resource-intensive, limiting the ability to scale tutoring programs to meet the needs of all students. Tutoring is more effective when delivered by teachers or well-trained professional tutors [30]. Currently, limited instructional materials are available to prepare and provide situational experiences to inexperienced tutors.\nThe lessons draw from previous research that identified impactful competencies of effective tutoring within the area of Advocacy [6, 9, 35]. Past studies have internally validated the construct validity by demonstrating 20% learning gain from pretest to posttest on similarly-structured lessons covering topics related to: giving effective praise to students; reacting when a student makes an error; and determining what students know [35]. Using the same scenario-based structure as in previous work [8, 9, 35, 36], our goal is to optimize tutor learning focusing on tutor lessons that instruct tutors in advocacy skills. There are very limited instructional and training materials available to tutors in the area of advocacy. Advocacy in teaching and tutoring encompasses a range of skills that promote student success by addressing their academic, social-emotional, and equity-related needs [6, 12]. Key areas include: promoting equity and inclusion; fostering cultural awareness; and challenging unconscious bias and assumptions [6, 35]."}, {"title": "2.2 Learning Engineering and Instructional Design", "content": "The design of instructional materials is essential for effective tutor training, with multiple-choice questions (MCQs) often preferred for their efficiency in large-scale settings, although they may promote surface-level learning [3, 16, 18]."}, {"title": "2.3 Using Generative Al to Evaluate Responses", "content": "In contrast, open-response questions foster deeper reflection and higher-order thinking, but are more time-consuming, raising the question of whether MCQs can be made pedagogically effective to teach advocacy skills in scalable scenarios [3, 16]. This present work applies a learning engineering approach to investigate the learning efficiency of the following type of questions: open response, which encourages deeper cognitive engagement; MCQs which can provide structured assessment and objective grading; and a combination leveraging the strengths of both [3, 16]. Central to this present work is the \"learn-by-doing\" methodology, which emphasizes active participation in the learning process. This approach aligns with \"doer\u201d philosophy, advocating for hands-on, practical experiences to enhance understanding and retention [23]. An example in practice is the integration of computer-based Cognitive Tutors, whereby students are required not only to complete tasks but also to articulate (analogous to the ability of tutors to explain in this current work) their reasoning, reinforcing their comprehension and retention [1]. This dual emphasis on doing and explaining has been shown to significantly improve learning outcomes, fostering deeper comprehension and critical thinking skills [1, 35].\nBrief scenario-based lessons were strategically designed using the learning-by-doing approach that provides action-able feedback and requires tutors to apply what they learned within the learning-by-doing conditions and the instruction phase by applying their learning in analogous tutoring situations at posttest. Fig. 1 illustrates the instructional design of the lesson. First, the tutors are presented with a scenario (Scenario 1), whereby they are prompted to predict the best approach, followed by being asked to explain their rationale or reasoning behind their chosen approach. There are three possible learning-by-doing conditions: multiple choice only, open response only, or both. Multiple choice questions begin with \"Which of the following...,\u201d followed by four options for the tutor to choose. Open response questions start with \"What would you say or how would you respond...\" for predicting the best approach and \"Why do you think your response is the best approach...\" for explaining the rationale behind their chosen approach. The tutors then engage in the instruction phase where the tutors observe the research-recommended approach and explain their reasoning in support or not of the best approach. Finally, the tutors complete a posttest, which is the same for all tutors and uses both MCQs and open responses. This instructional design is considered a modified predict-observe-explain (POE) approach and is theoretically related to Gibbs' Reflective Cycle, a cyclical instructional model that provides structure for learning by doing to individual learning experiences [15]."}, {"title": "2.3 Using Generative Al to Evaluate Responses", "content": "Traditional automated assessment methods, such as natural language processing and traditional machine learning, often struggle to capture the complexity of open-ended responses due to their limited ability to understand context, subtle language variations, and complex semantics [10, 29]. Additionally, these models require extensive labeled training data to perform accurately, which can be time-consuming and costly to gather, especially in specialized domains like education. Automated assessment of open-ended responses is a significant task in learning analytics, as it can evaluate the quality of learners' understanding of various topics, such as student responses to college readiness [10]. Previous studies have used language models such as BERT [29] and Sentence-BERT [10] to develop automated evaluation systems for open-ended responses. Although these models have shown promising results, they have limitations in their ability to understand deeper contextual nuances and adapt to complex domains. These machine-learning models often struggle to fully capture the nuanced linguistic and semantic meanings in tutor responses.\nIn response to the limitations of traditional machine learning methods, recent advancements in generative AI, particularly LLMs, have demonstrated significant potential to assess open-ended responses. For example, [25] applied GPT-3.5 and GPT-4 to automatically score student-written responses in science assessments. They found that few-shot learning approaches significantly improved scoring accuracy, especially when paired with contextual item stems and"}, {"title": "2.4 Prompt Engineering", "content": "Prompt engineering is a crucial technique in using LLMs to produce more accurate and contextually relevant output. The goal is to provide better context and structure in prompts to guide the LLMs toward the desired responses. Prompt engineering is particularly important when nuanced understanding and reliability are essential. One key technique in prompt engineering is few-shot prompting [2], where a set of exemplars is provided in the input prompt to demonstrate the ideal model behavior. By showing the model examples of the desired output format, few-shot prompting helps the LLM generalize to similar but unseen tasks. This approach has been used effectively in various automated assessment tasks, such as evaluating tutoring practice [19, 22] and student explanation on computer science questions [4]. Another approach is chain-of-thought (CoT) prompting [40], which instructs the model to \"think step by step\" by outlining intermediate reasoning steps. This technique helps the model handle more complex tasks by breaking the problem down into manageable parts, leading to more coherent and accurate responses. Such techniques are also used effectively to assess responses in educational settings [21, 28].\nIn addition to structuring prompts, techniques such as self-consistency [39] enhance the reliability of the model's outputs. Rather than modifying the prompt itself, self-consistency involves sampling multiple outputs and using a majority vote to determine the final response. This approach reduces the likelihood of hallucinations and improves the reliability of the outputs generated. For example, when evaluating open-ended responses, employing self-consistency can"}, {"title": "3 Method", "content": "Six scenario-based lessons were created and designed to align with the tutoring competencies within the area of Advocacy [6]. The lesson content taken from the tutoring platform and formatted as documents can be found in Digital Appendix. The lesson titles and learning objectives are listed below.\n\u2022 Addressing Microaggressions: define the term microaggression; identify microaggressions that occur in tutoring settings; and apply equity-focused strategies to help students address microaggressions.\n\u2022 Avoiding Unconscious Assumptions: identify unconscious assumptions; and apply strategies to prevent making unconscious assumptions while tutoring.\n\u2022 Building Cultural Competence: identify when students have different cultural backgrounds and experiences than your own; practice strategies to build cultural competence, supporting and engaging students across cultures.\n\u2022 Exploring Implicit Bias: identify implicit, or conscious bias; and apply strategies to counter the effects of your own implicit biases.\n\u2022 Narrowing Opportunity Gaps: define the term opportunity gap; identify examples of opportunity gaps in tutoring settings; and explain strategies to narrow opportunity gaps in tutoring settings.\n\u2022 Helping Students Manage Inequity: recognize when a student is experiencing inequity related to their learning; and apply strategies to help students manage inequities by assisting students to advocate for themselves."}, {"title": "3.1 Tutor Participants & Lesson Delivery", "content": "There were 234 tutors, mainly college students, who completed any number of the six lessons for a total of 790 lesson completions. The tutors were undergraduate college students employed as paid tutors for a remote tutoring organization supporting middle-school students. While the demographics of the tutors were undisclosed, they exhibited cultural and racial diversity. Before starting a lesson, participants provided their levels of self-reported tutoring experience using a 5-point Likert scale with 1 indicating little to no experience (novice) and 5 indicating an expert tutor. On average, the tutors reported an experience level of 3.56 (SD = 1.09) in all lessons. Tutors also self-reported their perceived knowledge of the lesson topic using a similar Likert scale with 1 (little to no knowledge of the topic) and 5 (expert level knowledge). In all lessons, the tutors reported an average knowledge of lesson topics of 3.59 (SD = 1.00). Table 1 displays the number of participants by condition with the average self-reported knowledge of the topic for each lesson. The advocacy lessons were developed in collaboration with tutoring supervisors, who are responsible for training tutors and working with students, and a university research team specializing in learning science, thereby enhancing construct validity. The lessons were delivered via an online tutoring platform and align with the research-shown competencies of effective"}, {"title": "3.2 Human Open-Response Annotation and Inter-rater Reliability", "content": "Two experienced researchers scored participant responses to assess inter-rater reliability. Open-response questions tasking tutors to predict and explain the best approach were binary-coded. Correct responses (score = 1) align with the research-recommended approach of the lesson. Conversely, incorrect responses (score = 0) do not align with research-driven tutoring best practices. Human annotation rubrics for scoring tutor responses to predict and explain the best approach, along with learner-sourced examples of coded responses are located within the Digital Appendix."}, {"title": "3.3 Prompting Large Language Models", "content": "Drawing on prior research in prompt engineering and large language models [2, 27], we developed a method for utilizing LLMs to evaluate the correctness of open-ended responses at posttest. Specifically, we implemented a few-shot learning approach, which has been shown to enhance performance in natural language understanding tasks [2]. In this method, the model was provided with a set of learner-generated responses along with human-scored examples to help it generate accurate assessments. The prompts were designed to assess two distinct types of responses: (1) \u201cpredict\" responses, where the participants predicted the best course of action, and (2) \u201cexplain\" responses, where they justified their decisions. Tables 4 and 5 present the specific prompts used to evaluate the predict and explain responses in the Addressing Microaggressions lesson, respectively. Prompts for all lessons and question types (i.e., predict and explain) are located within the Digital Appendix.\nThe development of these prompts was iterative, with multiple rounds of refinement informed by feedback on initial outputs. We employed various prompt engineering strategies including: content framing by providing context-specific details (e.g., \"You are a tutor evaluator... \"); randomly selecting human-coded examples of correct, partially correct, and incorrect tutor responses; and prompting the model to give rationale through chain-of-thought prompting [40]. To ensure deterministic outputs, the model's temperature was set to 0, guiding it to select the most probable word (or token) based on the input, thereby minimizing randomness and producing consistent, often more conservative responses. The output length was limited to 300 tokens to avoid unnecessary verbosity. These strategies allowed us to create an efficient and accurate method for assessing textual responses, with absolute performance metrics presented in the Results. The process also raised critical considerations for deploying LLMs in assessment contexts, which are further discussed in the Discussion."}, {"title": "3.4 Research Design & Analysis Plan", "content": "We employed a posttest-only randomized experimental design to evaluate tutor performance across three distinct conditions within the learning-by-doing phase: multiple-choice questions only, open-response questions only, or a combination of both. Participants were randomly assigned to one of these conditions to ensure that any differences observed in the posttest outcomes could be attributed to the specific question format rather than pre-existing differences among participants. A posttest-only design was chosen to avoid potential biases that might arise from pretesting, such as testing effects or sensitization [38]. Assessing tutor performance solely on the posttest scenario, we aimed to obtain a clear measure of the impact of the different learning-by-doing conditions on the tutor's subsequent performance [38]. Attending to RQ1, we employed an ANOVA to examine the impact of lesson, conditions, and scenario order on tutor"}, {"title": "4 Results", "content": null}, {"title": "4.1 Learner Performance Across Conditions", "content": "The overall average results of the posttest are shown in Fig. 2. As is visually apparent, there is no consistent pattern illustrating that one instructional condition produces better learning outcomes than others across the conditions. Indeed, in an analysis of variance, we did not find a statistically significant main effect of condition, F(2, 717) = 0.27, p = .765. There was a significant interaction between condition and lesson F (10, 717) = 2.20, p =.012, which means that the posttest scores differed significantly by condition depending on what lesson the tutors completed. Furthermore, a significant main effect of the lesson suggested substantial accuracy differences by lesson, which means that some lessons were harder than others, on average, F(5, 717) = 10.18, p < .001.\nWe did post hoc contrasts using marginal means to estimate which condition-level differences within lessons were reliable [26]. Significant differences were found in only two lessons. In the Addressing Microaggressions lesson, the Both condition produced higher posttest scores than the Open-response Only condition (estimate = 0.136, SE = 0.054, p = .031) with the MCQ Only condition ambiguously in between (i.e., not statistically different from the other two conditions,"}, {"title": "4.2 RQ2: Optimizing Lesson Impact", "content": "p-values > 0.12). The results for the Exploring Implicit Bias lesson were essentially the opposite, consistent with the overall interaction. In the Exploring Implicit Bias lesson, the Both condition produced lower posttest scores than the Open-response Only condition (estimate = -0.175, SE = 0.054, p = .004). In this lesson, the Both condition also produced lower posttest scores than the MCQ Only condition (estimate = -0.134, SE = 0.056, p = .046). All other comparisons were not significant (p >.299).\nThere was no significant interaction between condition and lesson when comparing instruction time prior to posttest (See Fig. 1), F(10, 716) = 13.46, p = .199, indicating that the time taken did not significantly differ by condition depending on the lesson students completed. However, there were significant main effects of both condition and lesson. A significant main effect of condition suggested that the time taken differed substantially by condition, F(2, 716) = 12.56, p < .001, while a significant main effect of lesson indicated that some lessons took longer to complete than others, on average, F(5, 716) = 8.61, p < .001. Across lessons, total instruction time prior to posttest ranged between M = 2.65 minutes (Avoiding Unconscious Assumptions) to M = 5.60 minutes (Helping Students Manage Inequity) (Fig. 3). Further, the MCQ Only condition took students the shortest (M = 3.83 minutes) while the Both condition took them the longest time (M = 5.87 minutes), although not substantially longer than the Open-response Only condition (M = 5.38 minutes). Hence, the Both condition took students less time than the sum of the open and MCQ conditions. Based on marginal mean comparisons, these differences were statistically significant, such that the MCQ Only condition took learners significantly shorter than the Both condition, estimate = -0.37, SE = 0.07, p < .001, and the Open-response Only condition, estimate = -0.30, SE = 0.07, p < .001. However, there was no significant difference in completion time between the Open-response Only and Both conditions, estimate = -0.07, SE = 0.07, p = .642."}, {"title": "4.3 Large Language Model Performance", "content": "The absolute performance of the LLMs was determined in assessing tutors' open response at posttest. Tables 6 and 7 display the comparison of absolute performance for GPT-40 and GPT-4-turbo, respectively, across lessons for predict and explain open responses. The LLM prompts used for each lesson can be accessed in the Digital Appendix. Both GPT-40 and GPT-4-turbo showcased proficiency in evaluating tutors' responses. Accuracy measures the proportion of correct predictions out of all predictions, providing an overall sense of a model's performance. AUC (Area Under the Curve), often used with imbalanced datasets, assesses how well the model distinguishes between classes, while the F1 score balances precision and recall, offering a measure of the model's effectiveness in handling both false positives and false negatives. In general, both models demonstrated proficiency in performance, with accuracy ranging from 71% to 91% for predict responses and 71% to 87% for explain responses-with some exceptions in GPT-4-turbo. GPT-4-turbo demonstrated proficiency across all lessons with poorer performance relative to the other lessons for assessing predict responses in Helping Students Manage Inequity (AUC = 0.17, F\u2081 = 0.45). The AUC was low for scoring other lessons: predict response in Exploring Implicit Bias (AUC = 0.43); explain responses in Building Cultural Competence (AUC = 0.45), suggesting that the model performs poorly in distinguishing between correct and incorrect responses."}, {"title": "5 Discussion", "content": "This study investigated differences in tutor learning across conditions that align with varying learning-by-doing activities (i.e., MCQ only, open response only, or both) and assessed the scalability of using generative AI for evaluating tutor responses. Several important insights emerged, which offer a comprehensive understanding of this present work."}, {"title": "5.1 No overall condition differences in learning outcomes.", "content": "In summary, there was no main effect of learning-by-doing condition on posttest scores, or learning outcomes. However, there was a significant interaction between the condition and the lesson indicating some potential heterogeneity, meaning that the learning outcomes differed significantly by condition depending on the lesson the tutors completed. Two of the three significant pairwise comparisons (comparing lesson and condition) were quite close to the p-value threshold of 0.05 and with 18 such comparisons (3 comparisons for each of 6 lessons). So, perhaps these are chance occurrences. Nevertheless, 3 out of 18 significant differences are substantially more than the expected false positive rate of 1 in 20 comparisons and therefore worth further exploration. The pairwise comparison indicating that Open-response Only is significantly better than Both for Exploring Implicit Bias is hard to interpret. It suggests that adding MCQ questions may harm learning in this lesson, and thus the MCQ Only condition should be worse. However, the difference in learning outcomes between MCQ Only and Open-response Only was not significant (p = .719). Inspection of the Exploring Implicit Bias did not reveal any substantial differences from other lessons in the nature of the MCQ or open-response questions. Overall, we have not found a consistent and generalizable explanation for the observed condition by lesson interaction. Perhaps the best explanation for this is random variability. As a content-general conclusion, our evidence suggests that replacing open-response learning tasks with MCQ learning tasks produces generally equivalent learning outcomes. Certainly, we found no substantial evidence against MCQs as learning tasks. Indeed, we found no overall statistically reliable evidence for a decrease in learning outcomes due to the use of MCQs as learning tasks. In addition, there were no consistent trends across the six lessons in this direction. Indeed, the MCQ only condition had the highest average posttest in two of the six lessons and the lowest only once. In contrast, the Open-response Only condition had the highest posttest for two lessons, but the lowest for three."}, {"title": "5.2 The MCQ Only condition requires less time.", "content": "There was no significant interaction between instruction time spent prior to posttest, or time spent, and condition on learning outcomes. However, there was a significant interaction between the condition and the lesson, indicating that some lessons took longer to complete than others. In general, and not surprisingly, the MCQ Only condition took the tutors the shortest time. However, the Both condition took the longest time of the tutors, though not substantially longer than Open-response Only, with the Both condition taking the students less time than the sum of the open and MCQ conditions. Why did it not take learners longer to complete both forms of practice (Open-response and MCQ) compared to Open-response Only? One possible explanation can arise from differences in the speed with which students in each condition complete the Follow-Up Instruction (Fig. 1). Conducting a post hoc ANOVA on follow-up instruction time revealed significant differences in follow-up instruction by condition, although that instruction included the same material in each condition (F(2, 576) = 8.52, p < .001). The mean comparisons indicated that learners in the Both condition were faster to complete the follow-up instruction (M = 0.155 minutes), followed by the MCQ Only condition (M = 0.239 minutes) and the Open-response Only condition (M = 0.347 minutes). These differences were statistically significant: the"}, {"title": "5.3 LLMs demonstrate proficiency, but more research is needed for wide-scale assessment.", "content": "Both condition was significantly faster in processing the follow-up instruction than the MCQ Only condition, estimate = -0.47, SE = 0.18, p = .020, as well as the Open-response Only condition, estimate = -0.78, SE = 0.19, p < .001.\nSimilar to [25], we found the GPT models to be comparable to each other and overall demonstrated proficiency. However, GPT-4-turbo exhibited variability across lessons, with poorer performance in the Helping Students Manage Inequity (AUC = 0.17) and Exploring Implicit Bias (AUC = 0.43) lessons. The low AUC scores in these cases suggest that the GPT-4-turbo struggled to classify responses in more complex or nuanced topics. This highlights the need for further refinement of LLMs to enhance their ability to assess open responses in content areas that require situational reasoning. In addition, more research is needed on the nuances within each lesson. The interrater reliability between the human graders was high for some lessons (k = 0.93), so it is necessary to revisit the human grade and annotation rubrics for the lessons and determine the sources of disagreement between the GPT models and humans (and between the raters) to better understand the results. This work adds to the recent literature on the potential use of LLMs for low stakes assessment tasks in different domain areas (Henkel et al. [20]), adding to the area of tutor training in advocacy skills."}, {"title": "5.4 Limitations", "content": "While this study used a posttest-only randomized experimental design, which offers several advantages, there are inherent limitations to this approach. One limitation is the lack of baseline data due to the absence of an analogous pretest scenario, which prevents measuring individual learning gains from pretest to posttest. This limitation makes it challenging to quantify the exact effect size of the lessons and to track individual progress. However, the primary purpose of this study was not to determine individual learning gain but to identify whether any of the learning-by-doing conditions-multiple choice questions, open-response questions, or both-led to better learning outcomes. By focusing on posttest results and employing random assignment to the condition, we were able to directly compare the relative learning effectiveness of each condition. In addition, the study aimed to find the most efficient lesson design by analyzing the time it took learners to complete each condition. Although the absence of pretest data can sometimes reduce statistical power, our sample size was sufficient to detect meaningful differences across conditions in time taken. Furthermore, our previous study [35], which implemented the Both condition with a counter-balanced pre-post design, demonstrated that our assessments are sufficient to detect significant pre-post learning gains. Finally, while the lack of baseline data can complicate the replication of the study in different contexts, the posttest only design was specifically chosen to minimize the risk of pretest sensitization, which can enhance the validity of the experimental results [11, 38]."}, {"title": "6 Future Work and Conclusion", "content": "We found evidence for learning efficiency benefits of multiple choice questions (MCQs) with feedback relative to the use of open-response questions with feedback or the combination of both. Practically, this result deserves particular attention given the widespread use of open-response questions in homework assignments as a practice task across content areas in college and K-12 education. This benefit of MCQs as learning tasks was demonstrated in a large sample of learners (n=235) and a variety of instructional lessons (6) and was revealed in generally equivalent learning outcomes achieved in significantly less time, corresponding to a 29% practice time reduction compared to the open-response only and 35% reduction compared a condition with both forms of instruction. Although these results favor the use of MCQs for instruction, they do not have any direct bearing on whether MCQs or open-response questions are better for"}]}