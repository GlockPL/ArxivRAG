{"title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages", "authors": ["Zeli Su", "Ziyin Zhang", "Guixian Xu", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "abstract": "While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.", "sections": [{"title": "1 Introduction", "content": "In recent years, with the development of multilingual pretrained models such as XLM-R (Conneau et al., 2020), mBART (Liu et al., 2020), and mT5 (Xue et al., 2021), language models have achieved significant progress in multilingual tasks, especially for high-resource languages. However, low-resource languages like Tibetan, Uyghur, Kazakh, and Mongolian\u2014spoken by millions of people in China\u2014remain critically underserved. Among these languages, Tibetan has over 10 million speakers, Uyghur over 11 million, Kazakh approximately 3 million, and Mongolian around 7 million, yet their representation in existing multilingual corpora is vastly inadequate. As illustrated in Figure 1, there is a significant disparity between the population sizes of these languages and the amount of available data in popular multilingual corpora such as OSCAR (Jansen et al., 2022). The situation is especially dire for Kazakh and Mongolian, with virtually zero usable data, hindering their inclusion in mainstream multilingual models.\nDespite claims of multilingual support for hundreds of languages, models like mBART and mT5 are not trained on Chinese minority languages. In comparison, more advanced multiglingual large language models such as LLaMA (Touvron et al., 2023) and Qwen (Yang et al., 2024) support even fewer languages.\nThis gap underscores the need for targeted solutions to address the challenges of text generation in extremely low-resource languages. To tackle this challenge, we propose a novel framework for efficiently extending a multilingual encoder into an encoder-decoder architecture. To address the scarce training data in low-resource languages, we introduce a weight-sharing mechanism between the encoder and the decoder by interleaving weights transferred from the encoder with randomly initialized ones, allowing for efficient adaptation to text generation in low-resource settings.\nExtensive experiments on the aforementioned"}, {"title": "2 Related Works", "content": "The evolution of multilingual large language models (LLMs) has been enabled by the release of extensive multilingual corpora such as CC100, mC4, OSCAR, CulturaX, and Madlad-400 (Wenzek et al., 2020; Raffel et al., 2019; Jansen et al., 2022; Nguyen et al., 2024; Kudugunta et al., 2023). While these resources cover a selection of low-resource languages to some extend, there remains a recognized gap in the representation for China's minority languages, primarily due to significant differences in writing systems.\nChina's minority languages often use different writing systems from the same language family used elsewhere in the world. For example, Uyghur is primarily written in the Arabic script (UEY\u2014Uyghurche Ereb Y\u00ebziqi) in China, with the Latin script (ULY\u2014Uyghurche Latin Y\u00ebziqi) is used as a supplementary form. In contrast, Uyghur in Russia and Central Asia is written in the Cyrillic script (USY-Uyghurche Shilir Y\u00ebziqi). When collecting data for minority languages, the aforementioned multilingual corpora either do not distinguish between such different writing systems, or only contain data from one system, as shown in Figure 1.\nRecently, the release of the Multilingual Corpus of Minority Languages in China (MC2, Zhang et al., 2024b) breaks the gap in the availability of Chinese minority language pretraining corpora, covering four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. This dataset is used as the primary pretraining corpus in our work."}, {"title": "2.2 Development of Multilingual Language Models", "content": "In the past few years, multilingual variants of pre-trained language models have been proposed in the NLP community, such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), supporting up to 100 languages and demonstrating powerful cross-lingual transfer capabilities. More recently, the emergence of large language models (LLMs) has revolutionized multilingual natural language processing. Models like PaLM (Chowdh- ery et al., 2023) and BLOOM (Scao et al., 2022) have made significant strides in multilingual capabilities, while the LLaMA family (Touvron et al., 2023) and its multilingual variants have democratized access to multilingual LLMs. Some specialized models represented by XGLM and NLLB (Lin et al., 2022; Costa-juss\u00e0 et al., 2022) have focused on expanding language coverage and improving cross-lingual transfer capabilities across hundreds of low-resource languages. However, few of these models support Chinese minority languages."}, {"title": "2.3 NLP for Minority Languages in China", "content": "To enhance the accessibility of minority languages in China, prior studies have primarily focused on curating annotated datasets for various NLP tasks. These efforts have mainly concentrated on three"}, {"title": "3 Method", "content": "In this section, we introduce the Shared Weights Framework, which leverages shared weights between the encoder and decoder for efficiently adapting multilingual encoders to text generation in low-resource languages.\nThe overall pipeline is visually summarized in Figure 2. Starting from CINO (Yang et al., 2022), a continual-pretrained version of XLM-R for Chinese minority languages, we copy its weight to initialize the decoder layers for knowledge transfer, and tie some of the weights between encoder and dedocer to enable efficient training. This model, which we name XLM-SWCM, is pretrained on the MC2 corpus and then applied to downstream tasks, including both single-language finetuning and cross-lingual transfer."}, {"title": "3.1.2 Model Architecture", "content": "Like the vanilla Transformer, the proposed model has two main components:\nEncoder: a pre-trained encoder-only model, specifically CINO, a variant of XLM-R enhanced for Chinese minority languages.\nDecoder: a transformer decoder stack with a specialized weight transfer mechanism. To balance the knowledge acquired during the encoder's large-scale pretraining and new knowledge required for downstream generation tasks, we introduce two types of decoder layers: NormalDeocderLayer and CustomDecoderLayer, both maintaining the same hidden dimension, intermediate size, and number of attention heads as the encoder.\nNormalDecoderLayer: A standard transformer decoder layer with randomly initialized weights. It follows a conventional architecture with sequential self-attention, cross-attention, and feed-forward network. These layers enable the model to learn generation-specific features from scratch, complementing the knowledge transfered from the encoder.\nCustomDecoderLayer: A modified transformer decoder layer that inherits pre-trained weights from the encoder. It features an enhanced structure with two strategically positioned feed-forward networks: FFN1 between self-attention and cross-attention, and FFN2 following cross-attention, each with its own layer normalization and residual connection, as shown in Figure 3. CustomDecoderLayer inherits all its weights from the pre-trained encoder to reuse learned representations."}, {"title": "3.1.3 Weight Sharing Mechanism", "content": "In our framework, the pre-trained encoder consists of only self-attention and feed-forward blocks, while the decoder layers require both self-attention and cross-attention mechanisms for effective generation. Thus, special schemes are designed to initialize and reuse the weights, as shown in Figure 3.\nFor weight initialization of CustomDecoderLayers, weights of both self-attention and cross-attention in the decoder are initialized from the encoder's self-attention blocks. Similarly, weights of both two FFN blocks in a decoder layer are initialized from the FFN block in the corresponding encoder layer. This mechanism reduces the effective number of parameters to be learned, accelerating convergence and enabling effective transfer of"}, {"title": "3.2 Pretraining", "content": "We adopte a multi-task training approach for pre-training. The primary task involves self-supervised learning using mBART's denoising auto-encoding (DAE) strategy. This strategy helps with the model's transition from the encoder's word-level cloze tasks to sequence generation tasks by predicting the masked portions of the input sequence with a decoder.\nAdditionally, we incorporate machine translation as an auxiliary objective, particularly focusing on translation between Mandarin Chinese and various Chinese minority languages. Specifically, the training data includes bidirectional translation pairs between Mandarin Chinese and the minority languages. This auxiliary objective improves the model's cross-lingual transfer capability, thereby enhancing the model's performance in various low-resource language processing tasks."}, {"title": "3.2.2 Training Data", "content": "THUCNews (THU-NLP Group, 2016) is a Chinese news dataset, derived from historical data from the Sina News RSS feed between 2005 and 2011 and containing approximately 740,000 news articles. From this dataset, we extracted a subset of Simplified Chinese news articles.\nMC2 (Zhang et al., 2024b) provides multilingual data for several Chinese minority languages, including Tibetan, Uyghur, Kazakh, and Mongolian. The specific data volumes are described in detail in Appendix A. Together with THUCNews, these monolingual datasets serve as training data for the DAE task.\nFor machine translation, we leveraged Google Translate to create bidirectional translation pairs"}, {"title": "4 Experiments", "content": "The models are trained for 8 epochs with a peak learning rate of 1e-4, AdamW (Loshchilov and Hutter, 2019) optimizer, global batch size 600, and a linear learning rate scheduler with a warmup proportion of 0.1. The maximum sequence length is set to 256 tokens, and mixed-precision is enabled to optimize memory usage and training efficiency. To ensure training stability, the norms of gradients are clipped to 1.0. The models are trained on two NVIDIA A800 GPUs, each with 80GB of memory, and the training process takes 92 hours."}, {"title": "Balanced Sampling Strategy", "content": "To address the inherent data imbalance across different languages, we implemente a balanced sampling strategy similar to XLM-R. The sampling probability for each language is calculated as\n$P_i = \\frac{q_i^\\alpha}{\\sum q_i^\\alpha}$,\nwhere $q_i$ represents the original proportion of language i in the dataset, and $\\alpha$ (set to 0.3) is a smoothing parameter that balances between uniform sampling and size-proportional sampling. This approach ensures that low-resource languages receive adequate representation in the training process while maintaining the influence of larger datasets."}, {"title": "Model Adaptations", "content": "We extende the model's vocabulary with special language tokens (\\<bo\\>, \\<kk\\>, \\<mn\\>, \\<ug\\>, \\<zh\\>) to handle our target languages (Tibetan, Kazakh, Mongolian, Uyghur, and Chinese). These language identifiers are directly added after the bos token \\<s> in the model inputs. This modification ensures that the model can effectively process and distinguish between different languages during both pre-training and downstream"}, {"title": "4.2 Downstream Tasks", "content": "To evaluate the capabilities of XLM-SWCM, we conduct fine-tuning experiments on three downstream tasks in both low-resource and high-resource languages: Text Summarization, Machine Reading Comprehension (MRC), and Machine Translation. These tasks are chosen to cover diverse areas of text generation in NLP."}, {"title": "Single-Language Fine-tuning", "content": "Due to the scarcity of labeled data for low-resource languages, we focus primarily on Tibetan for single-language fine-tuning, which has several publicly available datasets:\nFor this task, we utilize the Ti-Sum dataset (Xiaodong, 2022) with 20,000 pairs of titles and articles.\nWe mainly use the TibetanQA dataset (Sun et al., 2022) for this task, which claims to contain 20K examples. However, only 2K examples are publicly available. Thus we enrich it by integrating 5K examples from the TibetanSFT Corpus\u00b9 and 3K examples translated from a Chinese MRC dataset (Cui et al., 2019a) using Google Translate. This approach enables us to create a comprehensive dataset consisting of 10K examples.\nFor Machine Translation, we also use the TibetanSFT Corpus, which is cleaned to generate 50,000 parallel Chinese-Tibetan sentence pairs."}, {"title": "Cross-lingual Transfer", "content": "In addition to single-language fine-tuning, we also conduct cross-lingual transfer experiments to test XLM-SWCM's ability to generalize across multiple low-resource languages. This experiment aims to assess the model's performance in Tibetan, Uyghur, Mongolian, and Kazakh after being fine-tuned on a high-resource language (Simplified Chinese) and a very small number of samples in the target languages.\nFor Mandarin Chinese, we use the publicly available LCSTS dataset (Hu"}, {"title": "Baseline Models", "content": "We employ two baseline models to ensure broad coverage and robust performance in handling Chinese minority languages. The first model builds upon LLaMA2-Chinese and is fine-tuned on the MC2 dataset, resulting in the MC2-LLaMA-13B model. The second baseline, referred to as mBART-CM, is an adaptation of mBART-cc25. Its vocabulary is expanded to include tokens specific to our minority languages, followed by further pretraining on MC2."}, {"title": "Training settings", "content": "Both XLM-SWCM and mBART-CM are sequence-to-sequence models that are fine-tuned using standard training configurations. Each of these models is trained for 50 epochs with a batch size of 200 samples to ensure comprehensive learning and optimal performance. MC2-LLaMA-13B model is trained using LORA (Hu et al., 2022) with a rank of 8 for 3 epochs."}, {"title": "5 Ablation Studies", "content": "In this section, we present a series of ablation experiments aimed at evaluating the impact of key components in our framework that play essential roles in enhancing the model's multilingual capabilities and improving its generalization to low-resource languages. We perform ablation experiments on the Tibetan finetuning tasks, maintaining a consistent finetuning setting with Section 4.2.1."}, {"title": "5.1 Objective Ablation", "content": "We first focus on three critical aspects of the model: DAE pretraining, machine translation, and weight initialization by removing each and combinations of them. The results are shown in Table 3. Removing any of the three components is detreimental to performance, specifically:\nMachine Translation (MT): Removing machine translation has a relatively small impact on performance across tasks, as shown by both individual removal (maintaining 25.6 in Sum) and combined removals (MT+DAE vs DAE showing similar scores);\nDenoising Auto-Encoding (DAE): The removal of DAE pretraining causes considerable performance drops across all three downstream tasks, and its impact becomes more pronounced in combined removals (DAE+WS), indicating its fundamental importance in establishing the model's basic text generation capabilities.\nWeight Sharing (WS): The removal of weight sharing demonstrates the most significant impact among all modules, showing the largest performance drops in individual removal and maintaining this substantial negative effect across all combined removal scenarios, establishing it as the most crucial component for the model's effectiveness in low-resource settings.\nIn short, while all three components contribute positively to the model's performance, weight sharing emerges as the most critical component. This finding highlights the importance of weight sharing as a key architectural choice for multilingual models, especially in resource-constrained scenarios."}, {"title": "5.2 Structure Ablation", "content": "We also perform experiments to evaluate the impact of different structural components in our proposed framework. These experiments aim to understand how the initialization of decoder weights and the insertion of normal layers affect model performance."}, {"title": "5.2.1 Impact of Weight Initialization", "content": "Firstly, we train a baseline model called Cino-Transformer. Unlike XLM-SWCM, the decoder of this model is randomly initialized, and also matches the number of encoder layers. The model is pretrained using the same DAE and MT tasks as XLM-SWCM but without weight sharing, and then finetuned on downstream tasks in the same setting as XLM-SWCM."}, {"title": "5.2.2 Impact of Randomly Initialized Layers", "content": "Secondly, we explore the impact of inserting normal layers among the custom layers in the decoder. To assess the effectiveness of this modification, we use two baseline models for comparison:\n- Baseline A (XLM-SWCM without normal layers): This model is identical to XLM-SWCM but without any normal layers inserted into the custom layer architecture. The absence of normal layers leads to a reduced total number of layers in the decoder.\nBaseline B (Weight duplication model): Instead of inserting normal layers, this model simply copies the weights of the preceding layer to maintain consistency in the number of model parameters. This results in identical weights across consecutive layers, allowing us to isolate the impact of inserting randomly initialized normal layers.\nThe results in Table 5 demonstrate the significant impact of inserting normal layers into the decoder. BASE-A, which has fewer layers, performs the worst across all tasks. BASE-B, which maintain the same number of layers as XLM-SWCM but lacks randomly initialized weights, shows some improvement but still underperforms.\nOverall, these findings indicate that randomly initialized normal layers is also crucial for adapting encoders to text generation."}, {"title": "5.2.3 Impact of Insertion Frequency of Normal Layers", "content": "Thirdly, we thoroughly investigate the impact of insertion frequency of normal layers in the decoder, and how this interacts with varying dataset sizes. This experiment is designed along two dimensions:\n- Insertion Frequency of Normal Layers: we explore values of X where a normal layer is inserted after every X custom layers, with X ranging from 1 to 6. All these models are pretrained in the same setting as XLM-SWCM.\nEffect of Finetuning Dataset Size: we evaluate the model's performance on datasets of varying sizes, including 10K, 20K, and 50K samples. As the existing Ti-SUM dataset only has 20K samples, we supplement it by crawling and cleaning 30K additional news articles from various major Chinese websites. This dimension allows us to examine the interaction between the amount of available data and the frequency of normal layers."}, {"title": "6 Conclusion", "content": "In this work, we proposed a novel pretraining framework tailored for low-resource languages, with a particular focus on Chinese minority languages. Our framework leverages a shared weight mechanism between the encoder and decoder, which allows for the efficient adaptation of multilingual encoders to generation tasks without the need to start from scratch. Experimental results demonstrate that our model XLM-SWCM significantly outperforms traditional baselines on various text generation tasks for Tibetan, Uyghur, Kazakh, and Mongolian, which have long been underserved in NLP research. Our approach opens up new possibilities for developing robust models for these extremely low-resource languages, and also provides a promising method for the integration of resources across similar languages."}, {"title": "7 Limitations", "content": "Due to the availability of pretrained language models for Chinese minority languages and high-quality corpora, our study focused on only four minority languages. Our single-language finetuning experiments are further constrained to Tibetan given the lack of relevant datasets, limiting the scope of our exploration.\nThus, we hope that future work will put more focus on the development of high-quality datasets in these minority languages and beyond, enabling a more thorough exploration of underrepresented languages in the LLM era."}, {"title": "A Dataset Details", "content": "For pretraining of XLM-SWCM and other baseline models, we used a combination of Simplified Chinese data from THUCNews and minority languages from MC2. The breakdown of their distribution is given in Table 6."}, {"title": "B Training Details", "content": "In addition to the settings presented in the main paper, here we detail other parameters used during pre-training XLM-SWCM for complete reproduction:\nHardware: NVIDIA Tesla A800 GPU, 80 GB RAM * 2, Intel i7 CPU.\nSoftware: Ubuntu 20.04, CUDA 11.7, PyTorch 2.3\nTotal Training Samples: 1,340,235\n- Local Batch Size: 75\n- Gradient Accumulation Steps: 4\n- Global Batch Size: 600\n- Epochs: 8\nTotal Training Steps: 17,864\n\u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999\n- Warm-up: Linear warm-up for the first epoch, gradually increasing the learning rate from 1e-5 to 1e-4.\nScheduled Sampling: In the first epoch, teacher forcing is applied to guide the model. Subsequently, the teacher forcing ratio is gradually decreased in a linear fashion, transitioning to scheduled sampling."}]}