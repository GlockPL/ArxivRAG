{"title": "A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification", "authors": ["Shayli Farshchihaa", "Salman Asoudehb", "Maryam Shavali Kuhshuris", "Mehrshad Eisaeid", "Mohamadreza Azadi", "Saba Hesarakif"], "abstract": "Lung cancer is a major issue in worldwide public health, requiring early diagnosis using stable techniques. This work begins a thorough investigation of the use of machine learning (ML) methods for precise classification of lung cancer stages. A cautious analysis is performed to overcome overfitting issues in model performance, taking into account minimum child weight and learning rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM, Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted. Furthermore, the correlation between features and targets is examined using the deep neural network (DNN) model and thus their capability in detecting complex patterns is established. It is argued that several ML models can be capable of classifying lung cancer stages with great accuracy. In spite of the complexity of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic Regression excel with superior performance. The models perform better than the others in lung cancer prediction on the complete set of comparative metrics like accuracy, precision, recall, and F-1 score.", "sections": [{"title": "1. Introduction", "content": "Lung cancer is a major global health concern, with the unfortunate distinction of being the most common cause of cancer-related deaths and one of the most diagnosed cancers in the world. Approximately 2.20 million individuals are diagnosed each year with the debilitating lung cancer diagnosis, a staggering 75% of whom die within five years. The intricate inter-tumor heterogeneity and drug resistance make it extremely difficult to effectively treat cancer. Recent advances in cancer research techniques have brought with them a new era of cooperative work, leading to the establishment of large databases of clinical, imaging, and sequencing information. These data provide researchers with broad avenues of studying the intricate dynamics of lung cancer, including diagnosis and treatment data and clinical outcomes. The availability of -omics research, including genomics, transcriptomics, proteomics, and metabolomics, has greatly expanded the repertoire of tools for investigation. However, the incorporation of different types of high-dimensional data into clinical practice is extremely time-consuming and demands a lot of expertise. This emphasizes the increasing role of machine learning (ML) models, which can automatically detect inherent patterns in data to assist health care professionals in their decision-making. In spite of their potential, there is hesitation on the part of researchers and practitioners in investing in decision-making with expensive datasets. ML is a breath of fresh air in that it enables effective decision-making, accuracy, cost-effectiveness, and reproducibility regardless of geographical or human resource limitations. In this research, we aim to utilize machine learning algorithms to create efficient predictive models for the classification of lung cancer types, based on critical protein characteristics. By integrating feature selection, decision tree induction, and clustering methods, we intend to enhance the predictive accuracy of the models, thereby facilitating improvements in lung cancer diagnosis and treatment. Moreover, we are confronted with the challenges of clinical and in-situ patient surveillance, with the aim of delivering economically viable and time-efficient alternatives.\nMachine learning (ML), a branch of artificial intelligence (AI), has been a critical tool for cancer phenotyping and treatment for many years. In this research, we employ a broad spectrum of ML models, ranging from classic to modern algorithms, to offer comprehensive information to patients, researchers, and experts. Additionally, we scrutinize often underestimated machine learning model hyperparameters, such as minimum child weights and learning rates, to show their phenomenal impact on accuracy errors through thorough analysis. With the increasing sophistication of machine learning techniques in the diagnosis and treatment of lung cancer, this study aims to maximize patient outcomes and streamline decision-making in clinical practice."}, {"title": "Related Works", "content": "Machine learning has emerged as the backbone of modern medicine, with enormous potential for the enhancement of the accuracy of disease diagnoses and prognoses like lung cancer. Using machine learning models, scientists are not only explaining disease mechanisms but also revolutionizing clinical practice by adopting personalized treatment strategies and predictive model methods. In the case of lung cancer, life expectancy after surgery estimation is most important in treatment planning and patient counseling. The use of data mining prediction models like Decision Tree, Naive Bayes, and Artificial Neural Network makes personalized treatment planning by clinicians based on the individual features of every patient possible [1]. Additionally, the use of stratified 10-fold cross-validation analyses also ensures the validity and relevance of these predictive models, further making them more valuable in clinical application. Moreover, the application of classification algorithms for diagnosing brain tumors offers useful information for diagnosing lung cancer as well. By evaluating various classifiers' performance based on volumetric and locational features, scientists gain critical information on the algorithms' ability to distinguish between classes [2]. Such information not only facilitates early treatment and detection but also helps clinicians to characterize tumors and plan treatment accordingly.\nIn the area of lung cancer diagnosis, the excellent accuracy displayed by classifiers like Support Vector Machine says a lot about the transformative power of machine learning in medical practice [3]. Using various algorithms like KNN, SVM, NN, and Logistic Regression, doctors are capable of enhancing their decision-making capabilities related to diagnosis, staging, and treatment plans. In addition, the intersection of diagnostic devices with machine learning in healthcare indicates promising improvement in patient outcomes and efficient resource utilization in healthcare organizations. Additionally, studies on segmentation algorithms such as Na\u00efve Bayes and the Hidden Markov Model are an important contribution to the accuracy of lung tumor detection [4].\nBy outlining tumor margins and describing tissue morphology, such algorithms allow clinicians to detect minute abnormalities and distinguish between benign and malignant lesions [5-8]. In addition, the creation of algorithmic flowcharts for the detection of brain tumors highlights the interdisciplinarity of oncological diagnostic research with implications for lung cancer diagnosis and treatment [9]. The integration of machine learning algorithms into practice is a paradigm shift in the diagnosis of oncology and personalized medicine strategies. By leveraging the potential offered by evidence based on data, health care providers can improve patient outcomes, optimize resource use, and, in the end, our shared knowledge of lung cancer and its treatment. As research in this area continues, the ability of machine learning to transform the provision of health care is limitless."}, {"title": "2. Materials and Methods", "content": "Prediction of lung cancer using machine learning is extremely difficult due to the fact that it needs collecting and organizing datasets from various regions across the world.\nThis is done to make datasets balanced and representative of various locations and scenarios. To surpass this, detection and classification levels of lung cancer data are collected from publicly available datasets published by reputable agencies such as the World Health Organization (WHO), Kaggle, and Google datasets. Consolidating data from various sources, we aim to create an approximately balanced dataset."}, {"title": "3.1 Dataset", "content": "The initial step is the gathering of data from various publicly available datasets provided by trusted organizations like WHO, Kaggle, and Google datasets. The data gathered contain numerous attributes like patient ID, age, gender, and various environmental and lifestyle factors known to affect the risk of lung cancer. The target variable is the lung cancer level, which has three stages: low, medium, and high."}, {"title": "3.2 Preprocessing", "content": "Once the datasets are collected, they undergo a cleaning and preprocessing phase with the objective of establishing meaningful relationships between the various attributes. The features are analyzed for correlations to acquire information on how they influence the target variable. Correlation plots are created to visualize the relationships among the features. Features that have correlations greater than a certain value (for instance, 0.5) are deemed good for merging, thus creating new columns in the dataset. The method increases the efficiency of the features in lung cancer level prediction."}, {"title": "3.3 Correlation Analysis", "content": "Correlation plots are used to examine relationships between different features and the target variable. Strongly positive correlating features (e.g., those greater than 0.5) are merged to create new columns with increased predictive power. Other features with minimal or negative correlation are under close scrutiny in order to capture the effect of the feature on the target variable."}, {"title": "Splitting Data", "content": "Following the preprocessing stage, datasets are split into training and testing sets in an attempt to counteract overfitting risk. Features are normalized through the use of MinMaxScaler, and a balanced dataset is attained through the use of the Synthetic Minority Over-sampling Technique (SMOTE). There are 876 samples in the training subset and 219 samples in the testing subset. In addition, k-fold cross-validation with k=5 is performed in an attempt to further reduce overfitting probability."}, {"title": "ML Models", "content": "A total of nine machine learning models are applied for the prediction of lung cancer levels: XGBoost, LGBM, AdaBoost, LogisticRegression, Decision Tree, Random Forest, CatBoost, KNN, and DNN. The models are selected according to their suitability for the task and potential in delivering high accuracy in the prediction of lung cancer. The models are trained with the preprocessed datasets and evaluated with various performance measures in order to determine their efficiency in predicting lung cancer levels.\nThese models were chosen due to their efficiency in classification tasks and their capability to interpret complicated relationships within the dataset. With a variety of machine learning algorithms, we are hoping to create strong and precise predictive models for the detection and classification of lung cancer."}, {"title": "Extreme Gradient Boosting (XGBoost)", "content": "Extreme Gradient Boosting, better referred to as XGBoost, is an ensemble learning algorithm that is renowned for its scalability and efficiency in computations. Through iteratively training a series of weak learners, XGBoost incrementally improves the model's predictive power, successfully capturing complex patterns in data. Its capability to support both numerical and categorical features, in addition to being resilient to overfitting, renders it particularly well-adapted to classification problems."}, {"title": "Light Gradient Boosting Machine (LGBM)", "content": "Light Gradient Boosting Machine, or LGBM, is a gradient boosting framework specially designed to work effectively with large datasets and high-dimensional features. As opposed to traditional gradient boosting algorithms, LGBM employs a leaf-wise growth approach, which dramatically improves computational efficiency and model performance. Its computation speed, scalability, and capability in dealing with categorical features and imbalanced data render it a top choice for classification problems [1]."}, {"title": "Adaptive Boosting (AdaBoost)", "content": "Adaptive Boosting, simply AdaBoost, is an ensemble learning algorithm that sequentially aggregates a collection of weak classifiers to build a stronger classifier. By assigning larger weights to misclassified samples in each pass, AdaBoost prioritizes improving classification accuracy, specifically in regions of feature space that have difficulty with it. With its simplicity, adaptability, and effectiveness in a variety of datasets, it is an important tool for classification tasks [9]."}, {"title": "Logistic Regression", "content": "Logistic Regression is a simple linear model that is most commonly used for binary classification tasks. With its simple mechanism, logistic regression proves to have satisfactory performance when a feature and target variable relation is linear, or can at least be approximated with a linear function. With its simplicity, computational effectiveness, and ease of use, logistic regression is most often adopted for creating baseline classification models."}, {"title": "Decision Trees", "content": "Decision Trees are non-parametric supervised learning algorithms that partition feature spaces into hierarchical structures, allowing for simple decision processes. Decision trees have a strong capability for representing complex relations between features and target variable, and therefore, can effectively classify with non-linear decision boundary cases. Decision trees, however, suffer from overfitting [10], especially in cases with deep trees and noisy datasets."}, {"title": "Random Forest", "content": "Random Forest is an ensemble learning algorithm with a base in decision trees, in which many trees are trained individually and then aggregated to produce a prediction. By combining individual tree output, random forests mitigate overfitting and generalize capabilities [11] and have a high tolerance for noise, high-dimensional capabilities, and ease in dealing with high-dimensional datasets, and therefore, can effectively classify in many cases."}, {"title": "Categorical Boost (CatBoost)", "content": "CatBoost constitutes a gradient boosting algorithm specifically designed to effectively work with categorical features. Unlike traditional approaches, CatBoost incorporates a new algorithm for processing categorical variables, therefore, bypassing traditional preprocessing techniques such as one-hot encoding. Overfitting tolerance, efficiency in dealing with big datasets [12], and high predictive accuracy make CatBoost particularly ideal for classification with categorical features."}, {"title": "K-Nearest Neighbors (KNN)", "content": "The K-Nearest Neighbors (KNN) algorithm works as a simple yet powerful instance-based algorithm utilized in classification processes. KNN determines a specific data point's class label through an examination of its k most similar neighbors in feature space and assigning it with the most common class label present in them. Simplicity, adaptability, and its capability to form complex decision borders make KNN a fitting option for a variety of classification scenarios, particularly in cases with non-parametric distributions [13]."}, {"title": "Deep Neural Networks (DNN)", "content": "DNNs are a group of neural networks with many layers, which have the ability to learn complex representations directly from raw data [14]. By combining many neurons, DNNs can learn hierarchical features in an unsupervised manner, allowing them to detect complex structures in data. With its ability to manage high-dimensional inputs, learn non-linear relations, and adapt to many datasets, DNNs make them strong tools for classification, specifically in the areas of image, text, and speech processing [15,16]."}, {"title": "Results and Discussion", "content": "In our quest to mitigate overfitting and enhance model performance, significant consideration was placed in tracking parameters such as minimum child weight and learning rate in a variety of machine learning (ML) models. In relation to XGBoost model (Figure 5, Figure 6, Figure 7), our efforts produced astounding results, with performance values consistently trending towards 100% accuracy, precision, recall, and F-1 value. This achievement reflects the success of our approach in optimizing model parameters for best performance. With careful hyperparameter tracking, concerns regarding overfitting have been addressed, and both model robustness and generalizability have been guaranteed.\nThe information gained through confusion matrices (Figure 6) and learning curves provided important feedback about model performance. Notably, our observations showed that an additional incorporation of datasets did not yield significant improvements in model performance, indicative of a saturation point with regard to utility in terms of data. In addition, we discovered that a learning rate exceeding 0.06 produced additional performance improvements with no indications of overfitting and underfitting, and therefore, re-emphasizes the role of parameter optimization in attaining best model performance.\nThese findings not only contribute to the advancement of ML methodologies but also hold profound implications for clinical practice. By leveraging insights garnered from our analysis, clinicians can make more informed decisions regarding lung cancer diagnosis and treatment, ultimately improving patient care outcomes. Moreover, our approach serves as a testament to the importance of meticulous parameter tuning in maximizing the predictive capabilities of ML models, setting a precedent for future research endeavors in oncological diagnostics and beyond."}, {"title": "Conclusion", "content": "In conclusion, the in-depth analysis of machine learning (ML) models for lung cancer level classification re-emphasizes the critical role played in careful hyperparameter supervision, with specific regard for factors such as minimum child weight and learning rate. By careful analysis of learning curve and confusion matrix, we observed careful consideration for overfitting in all models.\nXGBoost and LGBM models performed exemplary, with perfect accuracy, precision, recall, and F-1 score. Notably, careful hyperparameter tweaks, particularly in terms of an increased learning rate over 0.06, helped in performance improvement with no overfitting.\nAdaboost performed with increased improvements in performance with increased learning rates, but minimum child weight changes showed little impact in model performance. Nonetheless, its consistent performance in both training and testing phases reiterated its dependability.\nThe logistic regression and decision tree model performed consistently, with careful consideration for overfitting and underfitting. Notably, the decision tree model performed best when trained with smaller datasets, and therefore, re-emphasizes the role played in training with datasets.\nRandom Forest, CatBoost, and k-NN performed perfectly with no overfitting concerns, and therefore, reiterate robust performance in lung cancer level classification.\nThe incorporation of additional datasets showed potential for additional performance improvement, particularly in k-NN models.\nNot withstanding an investigation of Deep Neural Networks (DNN) for feature-target complexity, conventional ML models outperformed deep learning counterparts consistently, with perfect classification of lung cancer level.\nIn comparative analysis, we identified top performers, including DNN, XGBoost, LGBM, Logistic Regression, CatBoost, and Random Forest, in a range of evaluation metrics.\nIn summary, in this work, the effectiveness of machine learning algorithms in predicting lung cancer severity with high accuracy is clarified. By careful analysis of model parameters and careful evaluation of performance metrics, we achieved high-performance results with reduced overfitting risk. Traditional machine learning algorithms, such as XGBoost and Logistic Regression, ranked high in performance in our evaluated models and showed significant potential for lung cancer prediction in a clinic environment. Not only does this work deepen our understanding of the role of machine learning in oncologic diagnostics, but it opens doors for enhanced patient care through increased accuracy in prognosis and therapeutic planning."}]}