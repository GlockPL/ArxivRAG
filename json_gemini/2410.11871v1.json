{"title": "TinyClick: Single-Turn Agent for Empowering GUI Automation", "authors": ["Pawel Pawlowski", "Krystian Zawistowski", "Wojciech Lapacz", "Marcin Skorupa", "Adam Wiacek", "Sebastien Postansque", "Jakub Hoscilowicz"], "abstract": "We present a single-turn agent for graphical user interface (GUI) interaction tasks, using Vision-Language Model Florence-2-Base. Main goal of the agent is to click on desired UI element based on the screenshot and user command. It demonstrates strong performance on Screenspot and OmniAct, while maintaining a compact size of 0.27B parameters and minimal latency. Main improvement comes from multitask training and MLLM-based data augmentation. Manually annotated corpora are scarce, but we show that re-annotation of annotated data with MLLM for multitask training might produce much better result. On Screenspot and OmniAct, our model outperforms both GUI-specific models (e.g., SeeClick) and MLLMs (e.g., GPT-4V).", "sections": [{"title": "1 Introduction", "content": "Vision-language models (VLMs) have many applications in working with GUIs (Nakano et al., 2022; Rawles et al., 2023). One of them is to perform actions specified by the user in the UI environment, e.g. clicking on appropriate icons or entering text into appropriate fields. Example of such commands are \"Choose first option in product list\", \"close the application\" or \"open cart\". The agent can perform actions based on such commands and the GUI screenshot that the user is currently on.\nCurrently, UI agents demonstrate moderate accuracy. Moreover, as shown in Table 2 of (Liu et al., 2024), standard MLLMs exhibit poor performance in single-turn agent tasks (only 0-11% accuracy), despite high computational cost.\nWe present single-turn agent based on Florence-2 (Xiao et al., 2023) model, which offers state-of-the-art performance, while being only 0.27B model (compared to 9.6B of SeeClick). When finetuning the model we investigated multiple approaches, public datasets and data preparation techniques. High-level summaries are provided in Figure 1"}, {"title": "2 Related Works", "content": "In recent years there is much interest in Vision-Language Models (VLMs) and Multimodal Large Language Models capable of UI understanding (Rahman et al., 2024; Baechler et al., 2024; Gur et al., 2018; Li and Li, 2023).\nOur focus are autonomous agents navigating UIs according to natural language commands of the user: such as AppAgent (Zhang et al., 2023), MobileAgent (Wang et al., 2024), CogAgent (Hong et al., 2023), Auto-UI (Zhang and Zhang, 2024), V-Zen(Rahman et al., 2024). Related problem is grounding, often understood as finding UI elements related to given phrase (Ferret (You et al., 2023), Ferret v2 (Zhang et al., 2024a)). Also, language-only LLM are used as agents for web, using primarily HTML representation (Deng et al., 2023; Gur et al., 2024). The progress depends on training and data collection procedures for efficient representation of GUI. Pix2Struct (Lee et al., 2023) was pretrained to reproduce simplified HTML representation of screenshot (including that for masked parts of the UI). SeeClick, one of best agent models so far, (Cheng et al., 2024) uses HTML data for grounding-style pretraining, training model to locate elements based on textual content or HTML tags. Among multitask training approaches (Gao et al., 2024) utilizes 10 pre-training tasks that resemble real-world tasks. Tree-of-Lens was proposed to interpret screen content based on a user-indicated point (Fan et al., 2024)."}, {"title": "3 Method", "content": "In this work, we use Florence-2 Base(Xiao et al., 2023), a 0.27B vision transformer with language modelling head trained for different vision tasks. The model latency is approximately 250ms, allowing also for cheaper inference. Vision encoder uses 768x768 image resolution. Larger than in other related works on agents (AutoUI (Zhang and Zhang, 2024) uses 224x224), it might be important for large screenshots. While Florence-2 uses text transformer, it is designed to handle detection and grounding tasks, encoding coordinates as single tokens."}, {"title": "3.1 Multitask Training", "content": "It was demonstrated that performance of transformer models can be improved by training on many related task (Chung et al., 2022), including Florence2 (Xiao et al., 2023)), and models trained on UI (Gao et al., 2024; Zhang et al., 2024b; Lee et al., 2023). (Hsieh et al., 2023; Zhang et al., 2024b) also demonstrate training small model to predict natural language explanations of training data. Florence-2 is pretrained on object recognition, phrase grounding, captioning, segmentation and similar vision tasks. It was also trained for OCR, but not on UI data. To adapt Florence2 for single-turn agent we investigated multitask training for UI, using tasks such as:\n\u2022 Element captioning - generating descriptions or purposes or action expectations of UI elements based on their location on the screen."}, {"title": "4 Experiments", "content": "We use use following public datasets: WaveUI (Daniel Jeffries, 2024) (we do not use WebUI due to license issue), AMEX (Chai et al., 2024), Mind2Web (Deng et al., 2023), GUI Odyssey (Lu et al., 2024) (it is not included in our final training), GUI Course (Chen et al., 2024), AndroidControl (Li et al., 2024), ScreenQA (Hsiao et al., 2024). For WaveUI we use both commands (for agent action task) as well as MLLM-generated expectation, purpose and captions of UI elements. We use similar approach to augment AMEX and GUI Course, where commands or functionalities of UI elements are provided with UI elements location.\nWe use InternVL2-26B to annotate these data with purposes, captions or expectations. For AMEX, we use 'functionality' field (manually annotated purpose of UI element) as well as Android XML annotations (if available for specific element)."}, {"title": "4.2 Benchmarks", "content": "We use two standard benchmarks: Screenspot (Cheng et al., 2024) and OmniAct (Kapoor et al., 2024). The first one contains 1200 test cases divided into 3 groups: mobile, web and desktop. The second one contains 3000 examples mainly related to the use of desktop operating systems."}, {"title": "4.3 Results Analysis", "content": "The accuracy shown in Tables 1 is calculated as average of binary outcomes, whether predicted click point (or bounding box center) falls within the original ground-truth bounding box (1 if it does, 0 otherwise). For Screenspot, we report the arithmetic mean accuracy achieved on 3 data subgroups (see Chapter 4.2), according to SeeClick publication (Cheng et al., 2024).\nThe results show strong performance improvement over other approaches, such as SeeClick (Cheng et al., 2024), AutoUI (Zhang and Zhang, 2024), and other MLLMs (see Table 1)."}, {"title": "4.4 Dataset insights", "content": "To investigate, which of the data contribute to results, we performed ablation study, starting with a mixture of training and agent command datasets and subsequently removing some (see Table 4). Multitask data is more important than UI commands data diversity. Removing AMEX, or GUI Course produces only small decrease of accuracy. Some of multitask data (as WaveUI's) can be used either as grounding objectives (generate location given phrase) and annotation objectives (generate phrase given location). The latter appears to provide stronger performance gain. To corroborate this, we used GUI Course Web single-action commands and annotated each example with expectations, captions and reasoning with use of InternVL2-26B MLLM (analogous to WaveUI). With 51k commands, 6k MLLM generated annotations of each type improve result (see Table 2), but only for annotation objective. Neither training on grounding objective nor training on double"}, {"title": "4.5 Fail analysis.", "content": "30% failed examples suggest spurious signals that were taught to produce right answer for wrong reasons. One example is positional bias: model clicks on the left and the top of the screen, where various menus are often found, or misinterprets similar icons. 20% examples are \"missed\" clicks: model clicks very closely to correct button, but slightly off-mark. Both of these problems might be mitigated by multitask training, with model trained to reason about the UI and attend to specific parts of screenshot."}, {"title": "5 Conclusions", "content": "TinyClick model strongly improves present baselines, achieving 73% on Screenspot and 57% on OmniAct, while being much smaller than competing solutions and small enough to work with subsecond latency. Furthermore, we confirmed that augmenting manually annotated agent data with MLLM for multitask training can strongly improve the performance."}, {"title": "6 Future Research", "content": "Presented contribution suggests few new research problems to investigate.\n\u2022 Large MLLM, thus far showing weak performance on UI control, might benefit from training strategy similar to ours. These models are similar architecture to Florence2, but they are larger and designed for general purpose tasks.\n\u2022 The model often fails when facing complicated structure of UI. Presented offline singleturn approach seems insufficient to deal with this problem, suggesting that online reinforcement learning or some novel approach might be better.\n\u2022 Florence2, as a grounding and detection model, allows to adapt out-of-domain by simultaneous use of annotated images and natural language explanations."}, {"title": "Limitations", "content": "TinyClick supports only single-turn commands, and its accuracy and scope is not sufficient for real-world use. Many device-specific actions, such as hardware buttons or touch gestures are not supported."}]}