{"title": "PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning", "authors": ["Luca Corbucci", "Mikko Heikkil\u00e4", "David Solans Noguero", "Anna Monreale", "Nicolas Kourtellis"], "abstract": "Training and deploying Machine Learning models that simultaneously adhere to principles of fairness and privacy while ensuring good utility poses a significant challenge. The interplay between these three factors of trustworthiness is frequently underestimated and remains insufficiently explored. Consequently, many efforts focus on ensuring only two of these factors, neglecting one in the process. The decentralization of the datasets and the variations in distributions among the clients exacerbate the complexity of achieving this ethical trade-off in the context of Federated Learning (FL). For the first time in FL literature, we address these three factors of trustworthiness. We introduce PUFFLE, a high-level parameterised approach that can help in the exploration of the balance between utility, privacy, and fairness in FL scenarios. We prove that PUFFLE can be effective across diverse datasets, models, and data distributions, reducing the model unfairness up to 75%, with a maximum reduction in the utility of 17% in the worst-case scenario, while maintaining strict privacy guarantees during the FL training.", "sections": [{"title": "1 Introduction", "content": "In recent years, Machine Learning (ML) models have been deployed in a wide variety of fields. In earlier years, the emphasis was primarily on optimizing the utility of these models. Nowadays, the current and upcoming AI legislation [12, 44, 7] demand equal attention to other trustworthiness requirements [11], including but not limited to fairness and privacy. Fairness refers to the goal of reducing the algorithmic biases that models might exhibit in their predictions and/or representations. Privacy refers to the goal of keeping training data safe and preventing any information leakage through the use of the model. Although privacy and fairness are ideally sought at the same time, it is unfortunately challenging to find a balance between them [14]. Often, the outcome is a model exhibiting strong utility, measured in model accuracy, but deficient in terms of privacy protection and fairness. Conversely, efforts to address the model's unfairness may compromise privacy protection and utility requirements.\nThis problem becomes even more complex to solve when we consider a Federated Learning (FL) scenario. With FL, various clients aim to train a model without transferring their local training data to a central server. Instead, they only exchange the locally trained model with a central aggregator. The decentralization of the training datasets introduces several issues that do not need to be considered when working in a centralized learning context. In this paper, we investigate the interplay between privacy, fairness, and utility within the context of FL, and present a methodology that enables informed decisions on the ethical trade-offs with the goal of training models that can strike a balance between these three dimensions.\nOur contributions: We propose PUFFLE, a first-of-its-kind methodology for finding an optimal trade-off between privacy, utility and fairness while training an FL model. Our approach, inspired by the method in [53] for a centralised setting, aims to train a model that can satisfy specific fairness and privacy requirements through the active contribution of each client. In particular, the clients incorporate an additional regularization term into the local loss function during the model's training to reduce unfairness. Furthermore, to ensure the privacy of the model, each client employs Differential Privacy (DP) [19] by introducing controlled noise during the training process. We summarize our key contributions below:\n\u2022 We show how to mitigate model unfairness under privacy constraints in the FL setting.\n\u2022 Unlike the method proposed in [53], PUFFLE does not require access to any external public dataset.\n\u2022 We propose an approach that does not require the clients to delve into the technical details of the methodology. Our approach automatically computes the necessary parameters based on the client's fairness and privacy preferences, ensuring a high level of parameterisation compared to past methods.\n\u2022 We offer both local and global computation of the fairness metrics. The former considers the fairness of the local clients on the local datasets. The latter, instead, considers the fairness of the aggregated, global model.\n\u2022 We validate our approach through comprehensive experimentation using three datasets of two different modalities, two different model architectures and three real-life distributions. Moreover, we examine various combinations of privacy and fairness preferences to showcase the impact on model utility. With model utility, in this paper, we mean model accuracy.\n\u2022 We release PUFFLE's source code to ensure reproducibility."}, {"title": "2 Related work", "content": "Training a single model that achieves fairness and privacy objectives is always a challenging task [14] because of the incompatibility in the simultaneous use of these trustworthiness requirements. Several papers have demonstrated how using privacy mitigation techniques could lead to more unfair models w.r.t. the underrepresented groups [4, 16, 23]. Similarly, employing fairness mitigation techniques can increase the privacy risk for involved clients [29].\nThere is a large body of research on techniques to mitigate the unfairness of models trained in centralised learning [30, 55, 25, 28, 10, 47]. One idea, initially proposed in [31] with a logistic regression model, is the use of a dedicated regularization term. Recently, Yaghini et al. [53] extended this method to train models that could be both private and fair. They modified the standard Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm [1], which is itself a variant of Stochastic Gradient Descent (SGD). In their work, they introduced two methods: FairDP-SGD and FairPATE. The goal is to train \"impartial\" ML models that do not favour one goal over another. They recover a Pareto Frontier to show the trade-off between the different objectives. The method in [53] has been the primary inspiration for our work. However, in our setting, data decentralization requires managing the presence of many clients with different distributions, preventing a direct application of the method in [53]. Moreover, the variety of fairness definitions leads to different mitigation strategies to achieve different goals.\nIn FL, the literature firstly focused on fostering Accuracy parity, i.e., achieving consistent model performance across clients regardless of their data distribution [57, 18, 35, 52, 43, 34], then on mitigating Group Fairness [15], i.e., achieving consistent model performance across different demographic groups represented in the dataset. Current literature on FL often addresses privacy and fairness as separate constraints, rather than considering them jointly [56, 27, 13, 2]. Recently, some works have started to consider the interplay between these two requirements. Borja Rodr\u00edguez-G\u00e1lvez et al. [26] introduced the Fair and Private Federated Learning (FPFL) algorithm to enforce group fairness in FL. The algorithm extends the modified method of differential multipliers to empirical risk minimization with fairness constraints and uses DP to guarantee privacy protection. However, a limitation is that clients can only perform a single local step before sharing the model with the server, increasing communication costs and slowing down the training. Our solution relies on a more advanced FL aggregation algorithm, allowing multiple local steps before model sharing, thus reducing communication costs.\nA two-step approach proposed in [39] tackles this problem training a fair model that balances fairness and utility. Then, a second model with DP guarantees aligns its predictions with the fair predictions of the first one. However, focusing exclusively on scenarios with few clients overlooks potential issues with multiple clients involved in training. Instead, our solution also works as clients increase addressing problems that may occur. A similar solution has been proposed in [32]. First, the clients train a fair proxy model and then a privacy-protected model based on the proxy. However, this approach shares similar limitations to [39]. Additionally, its applicability is restricted to scenarios involving convex and Lipschitz loss functions. Furthermore, a few crucial details regarding privacy accounting mechanisms are missing, making it challenging to evaluate the privacy guarantees.\nA different family of solutions to mitigate the model unfairness consists of making the model fair either in a pre-or a post-processing phase. Sikha Pentyala et al. [46] introduced a pre-processing and a post-processing algorithm to reduce the unfairness in FL while preserving the client's privacy by adopting DP during the training. Abay et al. [2] presented a similar approach using a pre-processing method to mitigate the model unfairness. However, to the best of our knowledge, none of these methods allow the user to set a desired unfairness level that the model should exhibit at the end of the training."}, {"title": "3 Background", "content": "In this section, we present the basic concepts and definitions used throughout the paper.\n3.1 Federated Learning\nFederated Learning (FL) [40] is a technique for training ML models while minimizing the sharing of information required for the learning process. In FL, K clients train a global model \u03b8 for a total number of R rounds without sharing the local datasets \\(D_k\\) of size \\(n_k\\) with a central server. The orchestration of the model training is managed by a server S that selects a subset \\(\\chi\\) of clients for each round \\(r \\in [0, R]\\). Each client \\(k \\in \\chi\\) executes E local training steps on their training dataset \\(D_k\\). The server is also responsible for the aggregation of the models trained by the clients. A possible aggregation algorithm is Federated-SGD (FedSGD) [40] that assumes a number E= 1 of local gradient update steps. In this case, at the end of the local step, the selected clients \\(\\chi\\) share with the server the gradient \\(g_k = \\nabla L(\\theta_k, b_i)\\) of the model \\(\\theta_k\\) computed on the batch \\(b_i\\). The server updates the global model \\(\\theta^{r+1} \\leftarrow \\theta - \\eta \\Sigma_{k=1}^\\chi g_k\\) where \\(\\eta\\) is a fixed learning rate and \\(n = \\Sigma_{k=1}^\\chi n_k\\). In this work, we assume the use of Federated Average (FedAvg) [40], a more advanced aggregation algorithm that improves the performance of FedSGD. In this case, the clients \\(\\chi\\) can perform more than a single local step before communicating with the server. Each client \\(k \\in \\chi\\) updates the local model \\(\\theta_k = \\theta_\\kappa \u2013 \\eta\\nabla L(\\theta_k, b_i)\\), then, after E local steps, it shares the model \\(\\theta_k\\) with the server that will aggregate all the received \\(\\Theta_k\\) producing the model \\(\\theta^{r+1}\\) for the next round \\(\\theta^{r+1}\\) \\Sigma_{k=1}^\\chi \\frac{n_k}{n} \\theta_k\\).\nDespite the original goal of protecting clients' privacy, it has been proven that FL is vulnerable to various attacks [8, 37, 9]. Therefore, privacy-preserving techniques like DP are usually applied to guarantee increased client protection.\n3.2 Fairness in ML\nThe problem of training fair ML models has received growing attention in the past few years. In literature, various fairness definitions and measures exist to evaluate ML model unfairness [51, 41].\nIn this paper, we focus on Group Fairness in the FL setting aiming to train a model that is fair with all the different demographic groups represented in the dataset, defined by sensitive attributes like gender or race. In FL, Group Fairness can be assessed either by aggregating results at the individual client level or at the server, where client prediction statistics are aggregated. Among the growing number of fairness metrics available in the state of the art, we use Demographic Parity [15], as previously done in [53] for the same goal in centralised learning. However, we would like to emphasize that PUFFLE can be used with any differentiable fairness metric, such as Equalised Odds [47], and Error Rate Difference [47].\nDefinition 1. Demographic Parity is a notion of Independence [5] that requires the probability of a certain prediction to be independent of the sensitive group membership. For instance, a sensitive group could be the gender of the sample taken into account. More formally, the Demographic Parity is defined as:\n\\[P(\\hat{Y} = y | Z = z) = P(\\hat{Y} = y | Z \\neq z)\\tag{1}\\]"}, {"title": "3.3 Differential Privacy", "content": "In other words, the ML model should produce similar success rates for all different groups: the model is fairer as the probabilities become more similar.\nDefinition 1 can be rewritten to represent the difference between the probabilities of obtaining the same output given different sensitive groups:\nDefinition 2. Demographic Disparity \u0393(y, z) is the difference between the probability of predicting class y for samples with sensitive value z and the probability of predicting class y for samples with sensitive value different than z:\n\\[\\Gamma(y, z) = P(\\hat{Y} = y | Z = z) \u2013 P(\\hat{Y} = y | Z \\neq z)\\tag{2}\\]\nFairness through Regularization: The concept of leveraging regularization to mitigate the unfairness of an ML model has previously been introduced in [53]. In their approach, they proposed computing an additional loss called Demographic Parity Loss (DPL) and using it as a regularization term. DPL is an extension of Demographic Parity which is computed in the following way:\n\\[DPL(\\theta; D) = \\underset{z}{\\text{\u0442\u0430\u0445}}\\underset{y}{\\text{\u0442\u0430\u0445}} |P(\\hat{Y} = y | Z = z) - \\frac{|{Z \\neq z}|}{|{Z=z}|} P(\\hat{Y} = y | Z \\neq z)|\\tag{3}\\]\nwhere \\((y, z) = {\\frac{|{Y=y, Z=z}|}{|{Z=z}|} - \\frac{|{Y=y, Z \\neq z}|}{|{Z\\neq z}|} \\), D is the dataset whose DPL is to be calculated, \\(\\hat{Y}_{= y}\\) is the prediction of the private model \\(\\theta\\) for a sample and Z is the sensitive value of a sample, i.e., the attribute towards which the model may be biased. For instance, a sensitive value could be the gender or the area of origin of an individual. For each mini-batch \\(b_i\\), the regularization term is summed to the classic loss function of the model \\(L(\\theta_t, b_i)+\\lambda DPL(\\theta_t; D_{public})\\). To balance between the original and the regularization loss, [53] multiplied the regularization loss by a fixed parameter \\(\\lambda\\) set at the beginning of the training and maintained constant throughout the entire process. In their methodology, they tried different \\(\\lambda\\) values and then selected the optimal model based on the trade-off between privacy, utility, and fairness.\nAlthough the idea of using regularisation to reduce unfairness in the model is effective, it cannot be directly applied within the context of FL. This is due to the decentralization of the datasets across clients, which may result in local client distributions where the DPL cannot be computed. Furthermore, [53] assumed the existence of a public dataset \\(D_{public}\\) with a distribution akin to the training dataset, employed for calculating the DPL at each training step. The assumption of the existence of a public dataset \\(D_{public}\\) and the need to select a \\(\\lambda\\) parameter contribute to the level of complexity, making the approach unintuitive, and unsuitable for real-world scenarios. On the contrary, PUFFLE solves all these issues making the approach proposed in [53] not only compatible with FL but also improving interpretability and parameterization."}, {"title": "3.4 Differential Privacy", "content": "Differential Privacy (DP) is a formal definition of privacy [21, 20]:\nDefinition 3. Given \\(\\epsilon > 0\\) and \\(\\delta \\in [0, 1]\\), a randomised algorithm \\(M : \\mathcal{X} \\rightarrow \\mathcal{O}\\) is (\\(\\epsilon, \\delta\\))-DP, if for all neighbouring \\(x, x' \\in \\mathcal{X}, S \\subset \\mathcal{O}\\),\n\\[P[M(x) \\in S] < e^{\\epsilon} P[M(x') \\in S] + \\delta.\\tag{4}\\]\nWe use the so-called \"add/remove neighbourhood\" definition, i.e., \\(x, x' \\in \\mathcal{X}\\) are neighbours if x can be transformed into x' by adding or removing a single element. For privacy accounting, we use an existing accountant [54] based on R\u00e9nyi DP [42], a relaxation of DP (Definition 3).\nLocal and Central DP: In the FL setting, an important detail to consider is when and by whom DP is applied. A common high trust-high utility approach is called Central Differential Privacy (CDP) [21]: K clients share raw data with an aggregator S who is responsible for aggregating the data and then applying DP to the result. This approach has the advantage of maintaining good utility since we introduce a limited amount of noise into the result. However, all the clients have to trust that S will adhere to the established protocol. In contrast, Local Differential Privacy (LDP) [33] does not assume that the clients trust S. Instead of sharing raw data, clients individually apply DP before sharing it with the aggregator. The drawback is that the total noise introduced during the process is larger than with CDP. In this paper, we assume that the clients might not trust the aggregator, and hence we adopt LDP.\n3.4 Privacy-Preserving ML\nTraining ML models require a large amount of data. These data may contain sensitive information regarding the individuals whose data were collected. Hence, the model trained on these data could leak sensitive information. DP is a common solution for mitigating privacy risks in ML training. One standard algorithm to train DP ML models is DP-SGD [24, 1], a modification of SGD. DP-SGD differs from SGD in the way gradients are calculated: given a random sample of training data \\(L_e\\), it computes the gradient of each \\(i \\in L_e\\). These gradients are clipped and then some noise is added to guarantee privacy protection. Since this is a well-known algorithm from literature, its pseudocode is reported in Algorithm 3, Appendix A.\nDifferentially Private FL: In the context of a model trained using FL, LDP can be applied in different ways depending on what you want to protect. Common choices include, e.g., instance- or sample-level privacy, where the objective is to protect the privacy of individual samples within the training dataset, and user-level privacy, when the DP neighbourhood considers the entire contribution from a single client. In our paper, we aim to ensure instance-level DP."}, {"title": "4 Fair & Differentially Private Federated Learning", "content": "Our proposed methodology aims to incorporate DP and a fairness regularization term into the training process of an FL model. The goal is twofold: to mitigate the privacy risks encountered by clients involved in the model training, and concurrently, to diminish the algorithmic biases of the model increasing its fairness. Being in an FL setting, we cannot directly apply the techniques utilized in centralized learning. The decentralization of data introduces several challenges that must be considered and addressed when implementing a solution like the one we propose. Our final goal is to offer an easy-to-use approach to train fair and private FL models that can balance fairness, privacy and utility meeting pre-defined trustworthiness requirements. We tested our approach on multiple datasets, proving that it can be effective regardless of the data modality, distribution, and model architecture.\n4.1 Fair Approach\nThe approach we adopt to mitigate the unfairness of the model is based on the use of a regularization term represented by the DPL. The idea was originally proposed in [53] in the centralized setting, but we need to apply several changes to make it suitable to an FL setting. The client-side training algorithm that we propose (Algo-rithm 1) is a modification of the standard DP-SGD algorithm that adds a regularization term DPL to the regular model loss. These two losses are weighted by a parameter \\(\\lambda\\). In our approach, \\(\\lambda\\) can be tuned during the hyperparameter search and remain fixed for the entire training on each node or, differently from the original paper, \\(\\lambda\\) can vary during the training (we provide more details next). In the latter case, at the beginning of the training, each client k computes \\(\\lambda_0\\) (Line 2, Algorithm 1). In the first FL round, the model is random, resulting in random predictions and disparity typically close to 0, allowing \\(\\lambda_0\\) to be 0. Then, in the following FL rounds, we compute \\(\\lambda_0\\), we consider the difference between the target disparity T and the DPL(\\(\\theta_k\\), \\(D_k\\)) computed using the model \\(\\theta_k\\) on the training dataset \\(D_k\\). If this difference is positive then \\(\\lambda_0\\) = 0, else if the actual disparity is higher than the target, we fix its value to \\(\\lambda_0\\) = 1. Then, the classic training of the model can start. For each mini-batch \\(b_i\\), sampled using Poisson sampling with probability q, each client computes a regularization term DPL(\\(\\theta_k\\), \\(b_i\\)) = \\(\\underset{z}{\\text{\u0442\u0430\u0445}}\\underset{y}{\\text{\u0442\u0430\u0445}} |P(\\hat{Y} = y | Z = z) - P(\\hat{Y} = y | Z \\neq z)|\\) (Line 5, Algorithm 1).\nFurthermore, and in contrast to the approach proposed in [53], we do not rely on a public dataset \\(D_{public}\\) for computing the DPL. The assumption of having a public dataset \\(D_{public}\\) matching the distribution of the training data is often impractical in real-life scenarios. Therefore, we propose to compute the DPL directly on the batches \\(b_i\\) using the disparity of batch \\(b_i\\) as a regularization term. Upon computing the regularization term, the algorithm proceeds to calculate the per-sample gradients w.r.t. both the regularization term (Line 6, Algorithm 1) and the regular model loss (Line 7, Algorithm 1).\nSince we want to have a Differentially Private model, we use DP-SGD to train the model. This means that the gradients computed are per-sample gradients. Therefore, to combine the regularization term with the regular loss, we sum the per-sample gradients g = (1-\u03bb) \u00d7 gloss + (\u03bb) \u00d7 gdpl (Line 8, Algorithm 1). This is where \\(\\lambda\\) comes in. This parameter balances the weight of the regular loss with the weight of the regularization. Setting \\(\\lambda\\) ~ 1 assigns more weight to the fairness regularization than to the standard model loss, resulting in an extremely fair model but with low utility. Conversely, opting for \\(\\lambda\\) ~ 0 prioritizes utility over fairness in the model optimization process. After this sum the regular DP-SGD algorithm is executed, the gradients are first clipped and the noise is added to guarantee DP.\n4.2 Tunable \u03bb\nAs mentioned earlier, interpreting the parameter \\(\\lambda\\) is not straightforward as it provides no clear insight into the resulting model disparity after training. Instead of using the same fixed parameter \\(\\lambda\\) for each client, our approach uses a more interpretable parameter T which represents the demographic disparity target for the trained model. Ranging between 0 and 1, this parameter is easily interpretable and can be selected based on current and upcoming AI legislations [38, 44, 7]. Consequently to the choice of target disparity that the final trained model should exhibit, each client calculates its own \\(\\lambda\\) based on T and the local model disparity. The value will increase or decrease throughout the training process to make the trained model meet the target disparity value T. Our Tunable \\(\\lambda\\) will range between 0 and 1: the more the model is unfair, the higher the \\(\\lambda\\) will be to reduce the unfairness. The value of the Tunable \\(\\lambda\\) is adjusted after each local training step on each client k. First of all, the difference \u0394 between the desired disparity T and the actual disparity is computed \u0394 = T -(DPL\\(b_i\\) + N(0, \\sigma^2\\)) (Line 10 and 11, Algorithm 1). When computing the \\(\\lambda\\) we introduce noise from a Gaussian Distribution N(0, \\sigma^2\\)) to guarantee DP in the computation of the next \\(\\lambda\\). Using the \u0394 we can compute the parameter velocity which is 0 in the first batch and then is updated in the following way: velocity = momentum \u00d7 velocity + \u0394 (Line 11, Algorithm 1) where momentum is a hyperparameter learned during the hyperparameter tuning phase. Then, we can finally compute the next \\(\\lambda\\), clipping it to [0,1]: \\(\\lambda_{t+1}\\) = clip(\\(\\lambda_t\\) \u2013 p \u00d7 velocity) (Line 12, Algorithm 1) where p is a learned hyperparameter indicating how aggressively we want to make the model fair."}, {"title": "4.3 Sharing Local Distribution Statistics", "content": "In a real-life scenario, clients involved in the training process usually have different data distributions, posing a challenge that requires a solution before applying the approach proposed in [53] in an FL setting. For instance, consider K clients aiming to train a model predicting individuals' eligibility for bank loans. Each client's dataset includes data regarding different individuals, including sensitive information like gender. We must prevent the training of a model that is biased toward a specific gender. DPL regularization offers a potential solution, challenges arise in an FL setting where some clients possess data exclusively for one gender. In such cases, when these clients compute the DPL using Formula 3, they can only calculate either the part of the formula where Z = Male, or the part where Z = Female. To handle this case, we propose a solution that involves the sharing of some statistics from the client to the server. Without loss of generality, we can assume a classification problem with a binary label and binary sensitive feature, at the end of each training round, each client computes the following statistics and shares them with the server: the number of samples with Z = 1 and \\(\\hat{Y}\\) = 1 and the number of samples with Z = 0 and \\(\\hat{Y}\\) = 1.\nThe shared information is sufficient for the server to compute all the other necessary statistics and the probabilities P(\\(\\hat{Y}\\) = y | Z = z). The clients do not need to compute and share the number of samples with sensitive values Z = 1 and with Z = 0 for each FL round, because these values do not change during the training; thus, sharing them just at the beginning of the training process is sufficient.\nThis, along with the ability to compute the missing statistics, allows us to decrease the amount of privacy budget required for this part of the process. After computing the missing statistics (Line 9, Algorithm 2) and aggregating them (Line 10, Algorithm 2), the server shares these values with the clients selected for the next FL round. To clarify, and without loss of generality, assuming a dataset with a binary label Y and sensitive value Z, each selected client will receive P(\\(\\hat{Y}\\) = 1 | Z = 1), P(\\(\\hat{Y}\\) = 1 | Z = 0), P(\\(\\hat{Y}\\) = 0 | Z = 1), P(\\(\\hat{Y}\\) = 0 | Z = 0). The selected clients that lack the necessary data to calculate the DPL can rely on the statistics shared by the server.\nIn addition to managing edge cases, client-shared statistics play a crucial role in understanding the trained model's disparity. A global view of the model fairness involves aggregating the various counters from individual clients, allowing for the computation of demographic disparity as if the server had access to the entire dataset. Conversely, the opposite approach involves gaining a local understanding of demographic disparity by computing the mean of each client's demographic disparity. This dual approach enables a global view across the entire dataset and a focused client fairness examination."}, {"title": "4.4 Differentially Private Approach", "content": "To guarantee a privacy-preserving learning process we propose a solution which is (\\(\\epsilon, \\delta\\))-DP. Ensuring this requires the implementation of DP in three distinct phases of our process:\n\u2022 The model training must be differentially private. To this end, we adopt a local DP approach based on DP-SGD (Algorithm 3 in the Appendix). Denote the privacy budget for this phase by (\\(\\epsilon_1, \\delta_1\\)) and the corresponding noise needed to guarantee DP as \\(\\sigma_1\\).\n\u2022 The tunable \\(\\lambda\\) depends on the disparity of the model computed using the training dataset on each client. Therefore, the computation of this value must be differentially private. Let the budget for this phase be (\\(\\epsilon_2, \\delta_2\\)) corresponding to a noise denoted as \\(\\sigma_2\\)."}, {"title": "4.5 Complexity Analysis", "content": "In this section, we analyze PUFFLE's complexity considering both the client side and the communication complexity.\n4.5.1 Local Computational Complexity\nFrom the perspective of local client complexity, DP-SGD is used to train the local models. While DP-SGD maintains the same theoretical time complexity as standard SGD, it is important to note that it is practically slower due to the explicit per-sample gradient computation requirement. The computation of the fairness loss in PUFFLE does not introduce additional computational overhead. This is because the fairness loss calculation uses predictions already available from the training process.\n4.5.2 Communication Complexity\nAs explained in Section4.3, PUFFLE requires the clients to share additional statistics with the server compared to the standard FedAvg. This causes an increase in the communication cost. For a classification task with |\u0176| possible classes and |Z| sensitive groups, the additional information exchanged scales as O(|\u0176||Z|) per client per FL round. This extra communication is constant to the number of model parameters and the number of participating clients."}, {"title": "5 Experimental Setup", "content": "5.1 Datasets\nWe demonstrate the effectiveness of PUFFLE across various models, datasets, and data distributions, showing the efficacy on both tabular and image datasets. This highlights the methodology's robustness regardless of all these factors, proving its applicability in different scenarios. We conduct tests using three distinct datasets: CelebA [36], Dutch [50] and ACS Income [17]. CelebA is an image dataset with more than 200.000 celebrity images with annotations. The annotations are crucial in our scenario since they contain information about the gender of the person depicted in the photos. In our experiment, we focus on a classification task, predicting whether the celebrity in the image is smiling or not. Dutch is a tabular dataset collected in the Netherlands, providing information about individuals. Compared with other common fairness tabular datasets like Compas [3], Dutch is bigger, making it suitable for our cross-device FL setting. The objective here is to predict whether the participants' salary is above or below $50,000. ACS Income is also a tabular dataset built from the American Community Survey (ACS) with data coming from over all 50 states and Puerto Rico in 2018. The task involves predicting whether the samples in the dataset have a salary higher than $50,000. Since it is split into 51 parts based on the regions, it is a natural choice for FL experimentation allowing us to test PUFFLE on an inherently partitioned dataset.\nIn Dutch and CelebA experiments, we used 150 clients during the simulation. Initially, data was distributed using a representative diversity approach, maintaining the same ratio per node as in the dataset. Then, to exaggerate client unfairness, we increase the amount of data from a specific group of sensitive values and reduce the amount of data from the opposite group. The experiments reported here present an even more realistic scenario to prove how PUFFLE can address edge-case situations. Specifically, for the Dutch dataset, half of the clients lack samples from the group (sensitive value = 1, label = 1). For CelebA, half of the clients have samples removed with (sensitive value = 1). In the case of the ACS Income dataset, we leverage the natural division of this dataset into states using each of the 51 states as a client during the training. We add more information about the strategy used to split the dataset into clients in Appendix C.\nIn the experiments, we assume to be in a cross-device scenario dividing clients into two groups: train and test. For Dutch and CelebA, we had 100 train clients and 50 test clients, while for Income we had 40 training clients and 11 test clients. On each FL round, we sample 30% of the training clients with CelebA and Dutch while we select half of the training clients in the case of ACS Income Dataset. We tune all hyperparameters using Bayesian optimization, trying to maximize model utility while staying under the target disparity T. To ensure the reproducibility of the results, we release the source code for reproducing all the experiments.\n5.2 Hyperparameter Tuning\nThe final goal of PUFFLE is to train models that offer good utility while mitigating the disparity. We defined the best hyperparameters as those that maximized the formula:\n\\[ = validation accuracy + \\Delta\\]\nwith \u0394 = 0 when validation disparity < T and \u0394 = -\u221e otherwise. The validation accuracy is computed independently by the validation clients on their local datasets and aggregated by the central server S using a weighted average. By maximizing \u03c8, we incorporate both accuracy and disparity into a single formula ensuring that the hyperparameters lead to a model disparity below the fairness target T at least on the validation set. We performed a hyperparameter search using the Bayesian search method dividing the K clients into 3 groups: train, validation and test. To ensure consistency throughout the hyperparameter search, we kept the set of test clients constant. On the contrary, we shuffled the assignment of clients to the training and validation sets for each of the different hyperparameter searches ran. For the Dutch and CelebA dataset, we used 150 clients: 50 as test clients, while the remaining 100 were split into 60% training and 40% validation clients. With ACS Income dataset, we had 51 clients in total: 11 used as test, 30 as train and 10 as validation. After completing the hyperparameter tuning, we retrained the model using the best hyperparameters, combining training and validation clients into a single training group."}, {"title": "6 Experimental Results", "content": "In this section"}, {"title": "PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning", "authors": ["Luca Corbucci", "Mikko Heikkil\u00e4", "David Solans Noguero", "Anna Monreale", "Nicolas Kourtellis"], "abstract": "Training and deploying Machine Learning models that simultaneously adhere to principles of fairness and privacy while ensuring good utility poses a significant challenge. The interplay between these three factors of trustworthiness is frequently underestimated and remains insufficiently explored. Consequently, many efforts focus on ensuring only two of these factors, neglecting one in the process. The decentralization of the datasets and the variations in distributions among the clients exacerbate the complexity of achieving this ethical trade-off in the context of Federated Learning (FL). For the first time in FL literature, we address these three factors of trustworthiness. We introduce PUFFLE, a high-level parameterised approach that can help in the exploration of the balance between utility, privacy, and fairness in FL scenarios. We prove that PUFFLE can be effective across diverse datasets, models, and data distributions, reducing the model unfairness up to 75%, with a maximum reduction in the utility of 17% in the worst-case scenario, while maintaining strict privacy guarantees during the FL training.", "sections": [{"title": "1 Introduction", "content": "In recent years, Machine Learning (ML) models have been deployed in a wide variety of fields. In earlier years, the emphasis was primarily on optimizing the utility of these models. Nowadays, the current and upcoming AI legislation [12, 44, 7] demand equal attention to other trustworthiness requirements [11], including but not limited to fairness and privacy. Fairness refers to the goal of reducing the algorithmic biases that models might exhibit in their predictions and/or representations. Privacy refers to the goal of keeping training data safe and preventing any information leakage through the use of the model. Although privacy and fairness are ideally sought at the same time, it is unfortunately challenging to find a balance between them [14]. Often, the outcome is a model exhibiting strong utility, measured in model accuracy, but deficient in terms of privacy protection and fairness. Conversely, efforts to address the model's unfairness may compromise privacy protection and utility requirements.\nThis problem becomes even more complex to solve when we consider a Federated Learning (FL) scenario. With FL, various clients aim to train a model without transferring their local training data to a central server. Instead, they only exchange the locally trained model with a central aggregator. The decentralization of the training datasets introduces several issues that do not need to be considered when working in a centralized learning context. In this paper, we investigate the interplay between privacy, fairness, and utility within the context of FL, and present a methodology that enables informed decisions on the ethical trade-offs with the goal of training models that can strike a balance between these three dimensions.\nOur contributions: We propose PUFFLE, a first-of-its-kind methodology for finding an optimal trade-off between privacy, utility and fairness while training an FL model. Our approach, inspired by the method in [53] for a centralised setting, aims to train a model that can satisfy specific fairness and privacy requirements through the active contribution of each client. In particular, the clients incorporate an additional regularization term into the local loss function during the model's training to reduce unfairness. Furthermore, to ensure the privacy of the model, each client employs Differential Privacy (DP) [19] by introducing controlled noise during the training process. We summarize our key contributions below:\n\u2022 We show how to mitigate model unfairness under privacy constraints in the FL setting.\n\u2022 Unlike the method proposed in [53], PUFFLE does not require access to any external public dataset.\n\u2022 We propose an approach that does not require the clients to delve into the technical details of the methodology. Our approach automatically computes the necessary parameters based on the client's fairness and privacy preferences, ensuring a high level of parameterisation compared to past methods.\n\u2022 We offer both local and global computation of the fairness metrics. The former considers the fairness of the local clients on the local datasets. The latter, instead, considers the fairness of the aggregated, global model.\n\u2022 We validate our approach through comprehensive experimentation using three datasets of two different modalities, two different model architectures and three real-life distributions. Moreover, we examine various combinations of privacy and fairness preferences to showcase the impact on model utility. With model utility, in this paper, we mean model accuracy.\n\u2022 We release PUFFLE's source code to ensure reproducibility."}, {"title": "2 Related work", "content": "Training a single model that achieves fairness and privacy objectives is always a challenging task [14] because of the incompatibility in the simultaneous use of these trustworthiness requirements. Several papers have demonstrated how using privacy mitigation techniques could lead to more unfair models w.r.t. the underrepresented groups [4, 16, 23]. Similarly, employing fairness mitigation techniques can increase the privacy risk for involved clients [29].\nThere is a large body of research on techniques to mitigate the unfairness of models trained in centralised learning [30, 55, 25, 28, 10, 47]. One idea, initially proposed in [31] with a logistic regression model, is the use of a dedicated regularization term. Recently, Yaghini et al. [53] extended this method to train models that could be both private and fair. They modified the standard Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm [1], which is itself a variant of Stochastic Gradient Descent (SGD). In their work, they introduced two methods: FairDP-SGD and FairPATE. The goal is to train \"impartial\" ML models that do not favour one goal over another. They recover a Pareto Frontier to show the trade-off between the different objectives. The method in [53] has been the primary inspiration for our work. However, in our setting, data decentralization requires managing the presence of many clients with different distributions, preventing a direct application of the method in [53]. Moreover, the variety of fairness definitions leads to different mitigation strategies to achieve different goals.\nIn FL, the literature firstly focused on fostering Accuracy parity, i.e., achieving consistent model performance across clients regardless of their data distribution [57, 18, 35, 52, 43, 34], then on mitigating Group Fairness [15], i.e., achieving consistent model performance across different demographic groups represented in the dataset. Current literature on FL often addresses privacy and fairness as separate constraints, rather than considering them jointly [56, 27, 13, 2]. Recently, some works have started to consider the interplay between these two requirements. Borja Rodr\u00edguez-G\u00e1lvez et al. [26] introduced the Fair and Private Federated Learning (FPFL) algorithm to enforce group fairness in FL. The algorithm extends the modified method of differential multipliers to empirical risk minimization with fairness constraints and uses DP to guarantee privacy protection. However, a limitation is that clients can only perform a single local step before sharing the model with the server, increasing communication costs and slowing down the training. Our solution relies on a more advanced FL aggregation algorithm, allowing multiple local steps before model sharing, thus reducing communication costs.\nA two-step approach proposed in [39] tackles this problem training a fair model that balances fairness and utility. Then, a second model with DP guarantees aligns its predictions with the fair predictions of the first one. However, focusing exclusively on scenarios with few clients overlooks potential issues with multiple clients involved in training. Instead, our solution also works as clients increase addressing problems that may occur. A similar solution has been proposed in [32]. First, the clients train a fair proxy model and then a privacy-protected model based on the proxy. However, this approach shares similar limitations to [39]. Additionally, its applicability is restricted to scenarios involving convex and Lipschitz loss functions. Furthermore, a few crucial details regarding privacy accounting mechanisms are missing, making it challenging to evaluate the privacy guarantees.\nA different family of solutions to mitigate the model unfairness consists of making the model fair either in a pre-or a post-processing phase. Sikha Pentyala et al. [46] introduced a pre-processing and a post-processing algorithm to reduce the unfairness in FL while preserving the client's privacy by adopting DP during the training. Abay et al. [2] presented a similar approach using a pre-processing method to mitigate the model unfairness. However, to the best of our knowledge, none of these methods allow the user to set a desired unfairness level that the model should exhibit at the end of the training."}, {"title": "3 Background", "content": "In this section, we present the basic concepts and definitions used throughout the paper.\n3.1 Federated Learning\nFederated Learning (FL) [40] is a technique for training ML models while minimizing the sharing of information required for the learning process. In FL, K clients train a global model \u03b8 for a total number of R rounds without sharing the local datasets \\(D_k\\) of size \\(n_k\\) with a central server. The orchestration of the model training is managed by a server S that selects a subset \\(\\chi\\) of clients for each round \\(r \\in [0, R]\\). Each client \\(k \\in \\chi\\) executes E local training steps on their training dataset \\(D_k\\). The server is also responsible for the aggregation of the models trained by the clients. A possible aggregation algorithm is Federated-SGD (FedSGD) [40] that assumes a number E= 1 of local gradient update steps. In this case, at the end of the local step, the selected clients \\(\\chi\\) share with the server the gradient \\(g_k = \\nabla L(\\theta_k, b_i)\\) of the model \\(\\theta_k\\) computed on the batch \\(b_i\\). The server updates the global model \\(\\theta^{r+1} \\leftarrow \\theta - \\eta \\Sigma_{k=1}^\\chi g_k\\) where \\(\\eta\\) is a fixed learning rate and \\(n = \\Sigma_{k=1}^\\chi n_k\\). In this work, we assume the use of Federated Average (FedAvg) [40], a more advanced aggregation algorithm that improves the performance of FedSGD. In this case, the clients \\(\\chi\\) can perform more than a single local step before communicating with the server. Each client \\(k \\in \\chi\\) updates the local model \\(\\theta_k = \\theta_\\kappa \u2013 \\eta\\nabla L(\\theta_k, b_i)\\), then, after E local steps, it shares the model \\(\\theta_k\\) with the server that will aggregate all the received \\(\\Theta_k\\) producing the model \\(\\theta^{r+1}\\) for the next round \\(\\theta^{r+1}\\) \\Sigma_{k=1}^\\chi \\frac{n_k}{n} \\theta_k\\).\nDespite the original goal of protecting clients' privacy, it has been proven that FL is vulnerable to various attacks [8, 37, 9]. Therefore, privacy-preserving techniques like DP are usually applied to guarantee increased client protection.\n3.2 Fairness in ML\nThe problem of training fair ML models has received growing attention in the past few years. In literature, various fairness definitions and measures exist to evaluate ML model unfairness [51, 41].\nIn this paper, we focus on Group Fairness in the FL setting aiming to train a model that is fair with all the different demographic groups represented in the dataset, defined by sensitive attributes like gender or race. In FL, Group Fairness can be assessed either by aggregating results at the individual client level or at the server, where client prediction statistics are aggregated. Among the growing number of fairness metrics available in the state of the art, we use Demographic Parity [15], as previously done in [53] for the same goal in centralised learning. However, we would like to emphasize that PUFFLE can be used with any differentiable fairness metric, such as Equalised Odds [47], and Error Rate Difference [47].\nDefinition 1. Demographic Parity is a notion of Independence [5] that requires the probability of a certain prediction to be independent of the sensitive group membership. For instance, a sensitive group could be the gender of the sample taken into account. More formally, the Demographic Parity is defined as:\n\\[P(\\hat{Y} = y | Z = z) = P(\\hat{Y} = y | Z \\neq z)\\]"}, {"title": "3.3 Differential Privacy", "content": "In other words, the ML model should produce similar success rates for all different groups: the model is fairer as the probabilities become more similar.\nDefinition 1 can be rewritten to represent the difference between the probabilities of obtaining the same output given different sensitive groups:\nDefinition 2. Demographic Disparity \u0393(y, z) is the difference between the probability of predicting class y for samples with sensitive value z and the probability of predicting class y for samples with sensitive value different than z:\n\\[\\Gamma(y, z) = P(\\hat{Y} = y | Z = z) \u2013 P(\\hat{Y} = y | Z \\neq z)\\]\nFairness through Regularization: The concept of leveraging regularization to mitigate the unfairness of an ML model has previously been introduced in [53]. In their approach, they proposed computing an additional loss called Demographic Parity Loss (DPL) and using it as a regularization term. DPL is an extension of Demographic Parity which is computed in the following way:\n\\[DPL(\\theta; D) = \\underset{z}{\\text{\u0442\u0430\u0445}}\\underset{y}{\\text{\u0442\u0430\u0445}} |P(\\hat{Y} = y | Z = z) - \\frac{|{Z \\neq z}|}{|{Z=z}|} P(\\hat{Y} = y | Z \\neq z)| \\]\nwhere \\((y, z) = {\\frac{|{Y=y, Z=z}|}{|{Z=z}|} - \\frac{|{Y=y, Z \\neq z}|}{|{Z\\neq z}|} \\), D is the dataset whose DPL is to be calculated, \\(\\hat{Y}_{= y}\\) is the prediction of the private model \\(\\theta\\) for a sample and Z is the sensitive value of a sample, i.e., the attribute towards which the model may be biased. For instance, a sensitive value could be the gender or the area of origin of an individual. For each mini-batch \\(b_i\\), the regularization term is summed to the classic loss function of the model \\(L(\\theta_t, b_i)+\\lambda DPL(\\theta_t; D_{public})\\). To balance between the original and the regularization loss, [53] multiplied the regularization loss by a fixed parameter \\(\\lambda\\) set at the beginning of the training and maintained constant throughout the entire process. In their methodology, they tried different \\(\\lambda\\) values and then selected the optimal model based on the trade-off between privacy, utility, and fairness.\nAlthough the idea of using regularisation to reduce unfairness in the model is effective, it cannot be directly applied within the context of FL. This is due to the decentralization of the datasets across clients, which may result in local client distributions where the DPL cannot be computed. Furthermore, [53] assumed the existence of a public dataset \\(D_{public}\\) with a distribution akin to the training dataset, employed for calculating the DPL at each training step. The assumption of the existence of a public dataset \\(D_{public}\\) and the need to select a \\(\\lambda\\) parameter contribute to the level of complexity, making the approach unintuitive, and unsuitable for real-world scenarios. On the contrary, PUFFLE solves all these issues making the approach proposed in [53] not only compatible with FL but also improving interpretability and parameterization."}, {"title": "3.4 Differential Privacy", "content": "Differential Privacy (DP) is a formal definition of privacy [21, 20]:\nDefinition 3. Given \\(\\epsilon > 0\\) and \\(\\delta \\in [0, 1]\\), a randomised algorithm \\(M : \\mathcal{X} \\rightarrow \\mathcal{O}\\) is (\\(\\epsilon, \\delta\\))-DP, if for all neighbouring \\(x, x' \\in \\mathcal{X}, S \\subset \\mathcal{O}\\),\n\\[P[M(x) \\in S] < e^{\\epsilon} P[M(x') \\in S] + \\delta.\\]\nWe use the so-called \"add/remove neighbourhood\" definition, i.e., \\(x, x' \\in \\mathcal{X}\\) are neighbours if x can be transformed into x' by adding or removing a single element. For privacy accounting, we use an existing accountant [54] based on R\u00e9nyi DP [42], a relaxation of DP (Definition 3).\nLocal and Central DP: In the FL setting, an important detail to consider is when and by whom DP is applied. A common high trust-high utility approach is called Central Differential Privacy (CDP) [21]: K clients share raw data with an aggregator S who is responsible for aggregating the data and then applying DP to the result. This approach has the advantage of maintaining good utility since we introduce a limited amount of noise into the result. However, all the clients have to trust that S will adhere to the established protocol. In contrast, Local Differential Privacy (LDP) [33] does not assume that the clients trust S. Instead of sharing raw data, clients individually apply DP before sharing it with the aggregator. The drawback is that the total noise introduced during the process is larger than with CDP. In this paper, we assume that the clients might not trust the aggregator, and hence we adopt LDP.\n3.4 Privacy-Preserving ML\nTraining ML models require a large amount of data. These data may contain sensitive information regarding the individuals whose data were collected. Hence, the model trained on these data could leak sensitive information. DP is a common solution for mitigating privacy risks in ML training. One standard algorithm to train DP ML models is DP-SGD [24, 1], a modification of SGD. DP-SGD differs from SGD in the way gradients are calculated: given a random sample of training data \\(L_e\\), it computes the gradient of each \\(i \\in L_e\\). These gradients are clipped and then some noise is added to guarantee privacy protection. Since this is a well-known algorithm from literature, its pseudocode is reported in Algorithm 3, Appendix A.\nDifferentially Private FL: In the context of a model trained using FL, LDP can be applied in different ways depending on what you want to protect. Common choices include, e.g., instance- or sample-level privacy, where the objective is to protect the privacy of individual samples within the training dataset, and user-level privacy, when the DP neighbourhood considers the entire contribution from a single client. In our paper, we aim to ensure instance-level DP."}, {"title": "4 Fair & Differentially Private Federated Learning", "content": "Our proposed methodology aims to incorporate DP and a fairness regularization term into the training process of an FL model. The goal is twofold: to mitigate the privacy risks encountered by clients involved in the model training, and concurrently, to diminish the algorithmic biases of the model increasing its fairness. Being in an FL setting, we cannot directly apply the techniques utilized in centralized learning. The decentralization of data introduces several challenges that must be considered and addressed when implementing a solution like the one we propose. Our final goal is to offer an easy-to-use approach to train fair and private FL models that can balance fairness, privacy and utility meeting pre-defined trustworthiness requirements. We tested our approach on multiple datasets, proving that it can be effective regardless of the data modality, distribution, and model architecture.\n4.1 Fair Approach\nThe approach we adopt to mitigate the unfairness of the model is based on the use of a regularization term represented by the DPL. The idea was originally proposed in [53] in the centralized setting, but we need to apply several changes to make it suitable to an FL setting. The client-side training algorithm that we propose (Algo-rithm 1) is a modification of the standard DP-SGD algorithm that adds a regularization term DPL to the regular model loss. These two losses are weighted by a parameter \\(\\lambda\\). In our approach, \\(\\lambda\\) can be tuned during the hyperparameter search and remain fixed for the entire training on each node or, differently from the original paper, \\(\\lambda\\) can vary during the training (we provide more details next). In the latter case, at the beginning of the training, each client k computes \\(\\lambda_0\\) (Line 2, Algorithm 1). In the first FL round, the model is random, resulting in random predictions and disparity typically close to 0, allowing \\(\\lambda_0\\) to be 0. Then, in the following FL rounds, we compute \\(\\lambda_0\\), we consider the difference between the target disparity T and the DPL(\\(\\theta_k\\), \\(D_k\\)) computed using the model \\(\\theta_k\\) on the training dataset \\(D_k\\). If this difference is positive then \\(\\lambda_0\\) = 0, else if the actual disparity is higher than the target, we fix its value to \\(\\lambda_0\\) = 1. Then, the classic training of the model can start. For each mini-batch \\(b_i\\), sampled using Poisson sampling with probability q, each client computes a regularization term DPL(\\(\\theta_k\\), \\(b_i\\)) = \\(\\underset{z}{\\text{\u0442\u0430\u0445}}\\underset{y}{\\text{\u0442\u0430\u0445}} |P(\\hat{Y} = y | Z = z) - P(\\hat{Y} = y | Z \\neq z)|\\) (Line 5, Algorithm 1).\nFurthermore, and in contrast to the approach proposed in [53], we do not rely on a public dataset \\(D_{public}\\) for computing the DPL. The assumption of having a public dataset \\(D_{public}\\) matching the distribution of the training data is often impractical in real-life scenarios. Therefore, we propose to compute the DPL directly on the batches \\(b_i\\) using the disparity of batch \\(b_i\\) as a regularization term. Upon computing the regularization term, the algorithm proceeds to calculate the per-sample gradients w.r.t. both the regularization term (Line 6, Algorithm 1) and the regular model loss (Line 7, Algorithm 1).\nSince we want to have a Differentially Private model, we use DP-SGD to train the model. This means that the gradients computed are per-sample gradients. Therefore, to combine the regularization term with the regular loss, we sum the per-sample gradients g = (1-\u03bb) \u00d7 gloss + (\u03bb) \u00d7 gdpl (Line 8, Algorithm 1). This is where \\(\\lambda\\) comes in. This parameter balances the weight of the regular loss with the weight of the regularization. Setting \\(\\lambda\\) ~ 1 assigns more weight to the fairness regularization than to the standard model loss, resulting in an extremely fair model but with low utility. Conversely, opting for \\(\\lambda\\) ~ 0 prioritizes utility over fairness in the model optimization process. After this sum the regular DP-SGD algorithm is executed, the gradients are first clipped and the noise is added to guarantee DP.\n4.2 Tunable \u03bb\nAs mentioned earlier, interpreting the parameter \\(\\lambda\\) is not straightforward as it provides no clear insight into the resulting model disparity after training. Instead of using the same fixed parameter \\(\\lambda\\) for each client, our approach uses a more interpretable parameter T which represents the demographic disparity target for the trained model. Ranging between 0 and 1, this parameter is easily interpretable and can be selected based on current and upcoming AI legislations [38, 44, 7]. Consequently to the choice of target disparity that the final trained model should exhibit, each client calculates its own \\(\\lambda\\) based on T and the local model disparity. The value will increase or decrease throughout the training process to make the trained model meet the target disparity value T. Our Tunable \\(\\lambda\\) will range between 0 and 1: the more the model is unfair, the higher the \\(\\lambda\\) will be to reduce the unfairness. The value of the Tunable \\(\\lambda\\) is adjusted after each local training step on each client k. First of all, the difference \u0394 between the desired disparity T and the actual disparity is computed \u0394 = T -(DPL\\(b_i\\) + N(0, \\sigma^2\\)) (Line 10 and 11, Algorithm 1). When computing the \\(\\lambda\\) we introduce noise from a Gaussian Distribution N(0, \\sigma^2\\)) to guarantee DP in the computation of the next \\(\\lambda\\). Using the \u0394 we can compute the parameter velocity which is 0 in the first batch and then is updated in the following way: velocity = momentum \u00d7 velocity + \u0394 (Line 11, Algorithm 1) where momentum is a hyperparameter learned during the hyperparameter tuning phase. Then, we can finally compute the next \\(\\lambda\\), clipping it to [0,1]: \\(\\lambda_{t+1}\\) = clip(\\(\\lambda_t\\) \u2013 p \u00d7 velocity) (Line 12, Algorithm 1) where p is a learned hyperparameter indicating how aggressively we want to make the model fair."}, {"title": "4.3 Sharing Local Distribution Statistics", "content": "In a real-life scenario, clients involved in the training process usually have different data distributions, posing a challenge that requires a solution before applying the approach proposed in [53] in an FL setting. For instance, consider K clients aiming to train a model predicting individuals' eligibility for bank loans. Each client's dataset includes data regarding different individuals, including sensitive information like gender. We must prevent the training of a model that is biased toward a specific gender. DPL regularization offers a potential solution, challenges arise in an FL setting where some clients possess data exclusively for one gender. In such cases, when these clients compute the DPL using Formula 3, they can only calculate either the part of the formula where Z = Male, or the part where Z = Female. To handle this case, we propose a solution that involves the sharing of some statistics from the client to the server. Without loss of generality, we can assume a classification problem with a binary label and binary sensitive feature, at the end of each training round, each client computes the following statistics and shares them with the server: the number of samples with Z = 1 and \\(\\hat{Y}\\) = 1 and the number of samples with Z = 0 and \\(\\hat{Y}\\) = 1.\nThe shared information is sufficient for the server to compute all the other necessary statistics and the probabilities P(\\(\\hat{Y}\\) = y | Z = z). The clients do not need to compute and share the number of samples with sensitive values Z = 1 and with Z = 0 for each FL round, because these values do not change during the training; thus, sharing them just at the beginning of the training process is sufficient.\nThis, along with the ability to compute the missing statistics, allows us to decrease the amount of privacy budget required for this part of the process. After computing the missing statistics (Line 9, Algorithm 2) and aggregating them (Line 10, Algorithm 2), the server shares these values with the clients selected for the next FL round. To clarify, and without loss of generality, assuming a dataset with a binary label Y and sensitive value Z, each selected client will receive P(\\(\\hat{Y}\\) = 1 | Z = 1), P(\\(\\hat{Y}\\) = 1 | Z = 0), P(\\(\\hat{Y}\\) = 0 | Z = 1), P(\\(\\hat{Y}\\) = 0 | Z = 0). The selected clients that lack the necessary data to calculate the DPL can rely on the statistics shared by the server.\nIn addition to managing edge cases, client-shared statistics play a crucial role in understanding the trained model's disparity. A global view of the model fairness involves aggregating the various counters from individual clients, allowing for the computation of demographic disparity as if the server had access to the entire dataset. Conversely, the opposite approach involves gaining a local understanding of demographic disparity by computing the mean of each client's demographic disparity. This dual approach enables a global view across the entire dataset and a focused client fairness examination."}, {"title": "4.4 Differentially Private Approach", "content": "To guarantee a privacy-preserving learning process we propose a solution which is (\\(\\epsilon, \\delta\\))-DP. Ensuring this requires the implementation of DP in three distinct phases of our process:\n\u2022 The model training must be differentially private. To this end, we adopt a local DP approach based on DP-SGD (Algorithm 3 in the Appendix). Denote the privacy budget for this phase by (\\(\\epsilon_1, \\delta_1\\)) and the corresponding noise needed to guarantee DP as \\(\\sigma_1\\).\n\u2022 The tunable \\(\\lambda\\) depends on the disparity of the model computed using the training dataset on each client. Therefore, the computation of this value must be differentially private. Let the budget for this phase be (\\(\\epsilon_2, \\delta_2\\)) corresponding to a noise denoted as \\(\\sigma_2\\)."}, {"title": "4.5 Complexity Analysis", "content": "In this section, we analyze PUFFLE's complexity considering both the client side and the communication complexity.\n4.5.1 Local Computational Complexity\nFrom the perspective of local client complexity, DP-SGD is used to train the local models. While DP-SGD maintains the same theoretical time complexity as standard SGD, it is important to note that it is practically slower due to the explicit per-sample gradient computation requirement. The computation of the fairness loss in PUFFLE does not introduce additional computational overhead. This is because the fairness loss calculation uses predictions already available from the training process.\n4.5.2 Communication Complexity\nAs explained in Section4.3, PUFFLE requires the clients to share additional statistics with the server compared to the standard FedAvg. This causes an increase in the communication cost. For a classification task with |\u0176| possible classes and |Z| sensitive groups, the additional information exchanged scales as O(|\u0176||Z|) per client per FL round. This extra communication is constant to the number of model parameters and the number of participating clients."}, {"title": "5 Experimental Setup", "content": "5.1 Datasets\nWe demonstrate the effectiveness of PUFFLE across various models, datasets, and data distributions, showing the efficacy on both tabular and image datasets. This highlights the methodology's robustness regardless of all these factors, proving its applicability in different scenarios. We conduct tests using three distinct datasets: CelebA [36], Dutch [50] and ACS Income [17]. CelebA is an image dataset with more than 200.000 celebrity images with annotations. The annotations are crucial in our scenario since they contain information about the gender of the person depicted in the photos. In our experiment, we focus on a classification task, predicting whether the celebrity in the image is smiling or not. Dutch is a tabular dataset collected in the Netherlands, providing information about individuals. Compared with other common fairness tabular datasets like Compas [3], Dutch is bigger, making it suitable for our cross-device FL setting. The objective here is to predict whether the participants' salary is above or below $50,000. ACS Income is also a tabular dataset built from the American Community Survey (ACS) with data coming from over all 50 states and Puerto Rico in 2018. The task involves predicting whether the samples in the dataset have a salary higher than $50,000. Since it is split into 51 parts based on the regions, it is a natural choice for FL experimentation allowing us to test PUFFLE on an inherently partitioned dataset.\nIn Dutch and CelebA experiments, we used 150 clients during the simulation. Initially, data was distributed using a representative diversity approach, maintaining the same ratio per node as in the dataset. Then, to exaggerate client unfairness, we increase the amount of data from a specific group of sensitive values and reduce the amount of data from the opposite group. The experiments reported here present an even more realistic scenario to prove how PUFFLE can address edge-case situations. Specifically, for the Dutch dataset, half of the clients lack samples from the group (sensitive value = 1, label = 1). For CelebA, half of the clients have samples removed with (sensitive value = 1). In the case of the ACS Income dataset, we leverage the natural division of this dataset into states using each of the 51 states as a client during the training. We add more information about the strategy used to split the dataset into clients in Appendix C.\nIn the experiments, we assume to be in a cross-device scenario dividing clients into two groups: train and test. For Dutch and CelebA, we had 100 train clients and 50 test clients, while for Income we had 40 training clients and 11 test clients. On each FL round, we sample 30% of the training clients with CelebA and Dutch while we select half of the training clients in the case of ACS Income Dataset. We tune all hyperparameters using Bayesian optimization, trying to maximize model utility while staying under the target disparity T. To ensure the reproducibility of the results, we release the source code for reproducing all the experiments.\n5.2 Hyperparameter Tuning\nThe final goal of PUFFLE is to train models that offer good utility while mitigating the disparity. We defined the best hyperparameters as those that maximized the formula:\n\\[ = validation accuracy + \\Delta\\]\nwith \u0394 = 0 when validation disparity < T and \u0394 = -\u221e otherwise. The validation accuracy is computed independently by the validation clients on their local datasets and aggregated by the central server S using a weighted average. By maximizing \u03c8, we incorporate both accuracy and disparity into a single formula ensuring that the hyperparameters lead to a model disparity below the fairness target T at least on the validation set. We performed a hyperparameter search using the Bayesian search method dividing the K clients into 3 groups: train, validation and test. To ensure consistency throughout the hyperparameter search, we kept the set of test clients constant. On the contrary, we shuffled the assignment of clients to the training and validation sets for each of the different hyperparameter searches ran. For the Dutch and CelebA dataset, we used 150 clients: 50 as test clients, while the remaining 100 were split into 60% training and 40% validation clients. With ACS Income dataset, we had 51 clients in total: 11 used as test, 30 as train and 10 as validation. After completing the hyperparameter tuning, we retrained the model using the best hyperparameters, combining training and validation clients into a single training group."}, {"title": "6 Experimental Results", "content": "In this section, we present the results of the application of PUFFLE on the CelebA Dataset. Corresponding results on the other datasets with PUFFLE can be found in Appendix D.\n6.1 Utility vs. Fairness vs. Privacy Trade-offs\nTo explore the interaction between these trustworthiness requirements, we examine combinations of two privacy targets (\\(\\epsilon\\), \\(\\delta\\)) and five fairness disparities targets T. We follow the common approach in deep learning of using \\(\\epsilon\\) < 10 [48], we select as target (\\(\\epsilon\\) = 5, \\(\\delta\\) = 8 \u00d7 10-4) and (\\(\\epsilon\\) = 8, \\(\\delta\\) = 8 \u00d7 10\u22124) in the experiment with CelebA. The parameter d is determined by considering the local parameter \\(d_k = \\frac{n_k}{n}\\) computed by each client k \\(\\in\\) K using the local dataset size \\(n_k\\), as recommended in the literature [48]. In our method, d is computed as the maximum \\(\\delta_k\\) among all clients, denoted as \\(\\underset{k}{\\text{max}} d_k\\).\nThe target disparity T is based on a percentage reduction from the baseline. We experiment with 10%, 25%, 50%, 65%, and 75% reductions in disparity to demonstrate the efficacy of our methodology in achieving a trade-off, for both high and extremely low targets. Since the baseline model trained with FL on the CelebA dataset has a disparity of ~ 0.17, we set the target disparities for our experiment at 0.15, 0.12, 0.09, 0.06, 0.04. Note that the parameter T is only a desired value for the model disparity, unlike Differential Privacy,"}]}]}