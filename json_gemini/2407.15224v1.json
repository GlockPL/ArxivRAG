{"title": "PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning", "authors": ["Luca Corbucci", "Mikko Heikkil\u00e4", "David Solans Noguero", "Anna Monreale", "Nicolas Kourtellis"], "abstract": "Training and deploying Machine Learning models that simultaneously adhere to principles of fairness and privacy while ensuring good utility poses a significant challenge. The interplay between these three factors of trustworthiness is frequently underestimated and remains insufficiently explored. Consequently, many efforts focus on ensuring only two of these factors, neglecting one in the process. The decentralization of the datasets and the variations in distributions among the clients exacerbate the complexity of achieving this ethical trade-off in the context of Federated Learning (FL). For the first time in FL literature, we address these three factors of trustworthiness. We introduce PUFFLE, a high-level parameterised approach that can help in the exploration of the balance between utility, privacy, and fairness in FL scenarios. We prove that PUFFLE can be effective across diverse datasets, models, and data distributions, reducing the model unfairness up to 75%, with a maximum reduction in the utility of 17% in the worst-case scenario, while maintaining strict privacy guarantees during the FL training.", "sections": [{"title": "1 Introduction", "content": "In recent years, Machine Learning (ML) models have been deployed in a wide variety of fields. In earlier years, the emphasis was primarily on optimizing the utility of these models. Nowadays, the current and upcoming AI legislation [12, 44, 7] demand equal attention to other trustworthiness requirements [11], including but not limited to fairness and privacy. Fairness refers to the goal of reducing the algorithmic biases that models might exhibit in their predictions and/or representations. Privacy refers to the goal of keeping training data safe and preventing any information leakage through the use of the model. Although privacy and fairness are ideally sought at the same time, it is unfortunately challenging to find a balance between them [14]. Often, the outcome is a model exhibiting strong utility, measured in model accuracy, but deficient in terms of privacy protection and fairness. Conversely, efforts to address the model's unfairness may compromise privacy protection and utility requirements.\nThis problem becomes even more complex to solve when we consider a Federated Learning (FL) scenario. With FL, various clients aim to train a model without transferring their local training data to a central server. Instead, they only exchange the locally trained model with a central aggregator. The decentralization of the training datasets introduces several issues that do not need to be considered when working in a centralized learning context. In this paper, we investigate the interplay between privacy, fairness, and utility within the context of FL, and present a methodology that enables informed decisions on the ethical trade-offs with the goal of training models that can strike a balance between these three dimensions.\nOur contributions: We propose PUFFLE, a first-of-its-kind methodology for finding an optimal trade-off between privacy, utility and fairness while training an FL model. Our approach, inspired by the method in [53] for a centralised setting, aims to train a model that can satisfy specific fairness and privacy requirements through the active contribution of each client. In particular, the clients incorporate an additional regularization term into the local loss function during the model's training to reduce unfairness. Furthermore, to ensure the privacy of the model, each client employs Differential Privacy (DP) [19] by introducing controlled noise during the training process. We summarize our key contributions below:\n\u2022 We show how to mitigate model unfairness under privacy constraints in the FL setting.\n\u2022 Unlike the method proposed in [53], PUFFLE does not require access to any external public dataset.\n\u2022 We propose an approach that does not require the clients to delve into the technical details of the methodology. Our approach automatically computes the necessary parameters based on the client's fairness and privacy preferences, ensuring a high level of parameterisation compared to past methods.\n\u2022 We offer both local and global computation of the fairness metrics. The former considers the fairness of the local clients on the local datasets. The latter, instead, considers the fairness of the aggregated, global model.\n\u2022 We validate our approach through comprehensive experimentation using three datasets of two different modalities, two different model architectures and three real-life distributions. Moreover, we examine various combinations of privacy and fairness preferences to showcase the impact on model utility. With model utility, in this paper, we mean model accuracy.\n\u2022 We release PUFFLE's source code to ensure reproducibility."}, {"title": "2 Related work", "content": "Training a single model that achieves fairness and privacy objectives is always a challenging task [14] because of the incompatibility in the simultaneous use of these trustworthiness requirements. Several papers have demonstrated how using privacy mitigation techniques could lead to more unfair models w.r.t. the underrepresented"}, {"title": "3 Background", "content": ""}, {"title": "3.1 Federated Learning", "content": "Federated Learning (FL) [40] is a technique for training ML models while minimizing the sharing of information required for the learning process. In FL, $K$ clients train a global model $\\theta$ for a total number of $R$ rounds without sharing the local datasets $D_k$ of size $n_k$ with a central server. The orchestration of the model training is managed by a server $S$ that selects a subset $\\mathcal{X}$ of clients for each round $r \\in [0, R]$. Each client $k \\in \\mathcal{X}$ executes $E$ local training steps on their training dataset $D_k$. The server is also responsible for the aggregation of the models trained by the clients. A possible aggregation algorithm is Federated-SGD (FedSGD) [40] that assumes a number $E = 1$ of local gradient update steps. In this case, at the end of the local step, the selected clients $\\mathcal{X}$ share with the server the gradient $g_k = \\nabla L(\\theta_k, b_i)$ of the model $\\theta_k$ computed on the batch $b_i$. The server updates the global model $\\theta^{r+1} \\leftarrow \\theta - \\eta \\Sigma_{k=1}^{\\mathcal{X}} g_k$ where $\\eta$ is a fixed learning rate and $n = \\sum_{k=1}^{\\mathcal{X}} n_k$. In this work, we assume the use of Federated Average (FedAvg) [40], a more advanced aggregation algorithm that improves the performance of FedSGD. In this case, the clients $\\mathcal{X}$ can perform more than a single local step before communicating with the server. Each client $k \\in \\mathcal{X}$ updates the local model $\\theta_k = \\theta_k - \\eta \\nabla L(\\theta_k, b_i)$, then, after $E$ local steps, it shares the model $\\theta_k$ with the server that will aggregate all the received $\\theta_k$ producing the model $\\theta^{r+1} \\leftarrow \\frac{1}{n} \\Sigma_{k=1}^{\\mathcal{X}} n_k \\theta_k$.\nDespite the original goal of protecting clients' privacy, it has been proven that FL is vulnerable to various attacks [8, 37, 9]. Therefore, privacy-preserving techniques like DP are usually applied to guarantee increased client protection."}, {"title": "3.2 Fairness in ML", "content": "The problem of training fair ML models has received growing attention in the past few years. In literature, various fairness definitions and measures exist to evaluate ML model unfairness [51, 41].\nIn this paper, we focus on Group Fairness in the FL setting aiming to train a model that is fair with all the different demographic groups represented in the dataset, defined by sensitive attributes like gender or race. In FL, Group Fairness can be assessed either by aggregating results at the individual client level or at the server, where client prediction statistics are aggregated. Among the growing number of fairness metrics available in the state of the art, we use Demographic Parity [15], as previously done in [53] for the same goal in centralised learning. However, we would like to emphasize that PUFFLE can be used with any differentiable fairness metric, such as Equalised Odds [47], and Error Rate Difference [47].\nDefinition 1. Demographic Parity is a notion of Independence [5] that requires the probability of a certain prediction to be independent of the sensitive group membership. For instance, a sensitive group could be the gender of the sample taken into account. More formally, the Demographic Parity is defined as:\n$\\mathbb{P}(\\hat{Y} = y | Z = z) = \\mathbb{P}(\\hat{Y} = y | Z \\neq z)$"}, {"title": "3.3 Differential Privacy", "content": "Differential Privacy (DP) is a formal definition of privacy [21, 20]:\nDefinition 3. Given $\\epsilon > 0$ and $\\delta \\in [0, 1]$, a randomized algorithm $M : \\mathcal{X} \\rightarrow \\mathcal{O}$ is $(\\epsilon, \\delta)$-DP, if for all neighboring $x, x' \\in \\mathcal{X}, S \\subset \\mathcal{O}$,\n$\\mathbb{P}[M(x) \\in S] \\le e^{\\epsilon} \\mathbb{P}[M(x') \\in S] + \\delta$.\nWe use the so-called \"add/remove neighbourhood\" definition, i.e., $x, x' \\in \\mathcal{X}$ are neighbours if $x$ can be transformed into $x'$ by adding or removing a single element. For privacy accounting, we use an existing accountant [54] based on R\u00e9nyi DP [42], a relaxation of DP (Definition 3).\nLocal and Central DP: In the FL setting, an important detail to consider is when and by whom DP is applied. A common high trust-high utility approach is called Central Differential Privacy (CDP) [21]: $K$ clients share raw data with an aggregator $S$ who is responsible for aggregating the data and then applying DP to the result. This approach has the advantage of maintaining good utility since we introduce a limited amount of noise into the result. However, all the clients have to trust that $S$ will adhere to the established protocol. In contrast, Local Differential Privacy (LDP) [33] does not assume that the clients trust $S$. Instead of sharing raw data, clients individually apply DP before sharing it with the aggregator. The drawback is that the total noise introduced during the process is larger than with CDP. In this paper, we assume that the clients might not trust the aggregator, and hence we adopt LDP."}, {"title": "3.4 Privacy-Preserving ML", "content": "Training ML models require a large amount of data. These data may contain sensitive information regarding the individuals whose data were collected. Hence, the model trained on these data could leak sensitive information. DP is a common solution for mitigating privacy risks in ML training. One standard algorithm to train DP ML models is DP-SGD [24, 1], a modification of SGD. DP-SGD differs from SGD in the way gradients are calculated: given a random sample of training data $\\mathcal{L}_e$, it computes the gradient of each $i \\in \\mathcal{L}_e$. These gradients are clipped and then some noise is added to guarantee privacy protection. Since this is a well-known algorithm from literature, its pseudocode is reported in Algorithm 3, Appendix A.\nDifferentially Private FL: In the context of a model trained using FL, LDP can be applied in different ways depending on what you want to protect. Common choices include, e.g., instance- or sample-level privacy, where the objective is to protect the privacy of individual samples within the training dataset, and user-level privacy, when the DP neighbourhood considers the entire contribution from a single client. In our paper, we aim to ensure instance-level DP."}, {"title": "4 Fair & Differentially Private Federated Learning", "content": "Our proposed methodology aims to incorporate DP and a fairness regularization term into the training process of an FL model. The goal is twofold: to mitigate the privacy risks encountered by clients involved in the model training, and concurrently, to diminish the algorithmic biases of the model increasing its fairness. Being in an FL setting, we cannot directly apply the techniques utilized in centralized learning. The decentralization of data introduces several challenges that must be considered and addressed when implementing a solution like the one we propose. Our final goal is to offer an easy-to-use approach to train fair and private FL models that can balance fairness, privacy and utility meeting pre-defined trustworthiness requirements. We tested our approach on multiple datasets, proving that it can be effective regardless of the data modality, distribution, and model architecture."}, {"title": "4.1 Fair Approach", "content": "The approach we adopt to mitigate the unfairness of the model is based on the use of a regularization term represented by the DPL. The idea was originally proposed in [53] in the centralized setting, but we need to apply several changes to make it suitable to an FL"}, {"title": "4.2 Tunable $\\lambda$", "content": "As mentioned earlier, interpreting the parameter $\\lambda$ is not straightforward as it provides no clear insight into the resulting model disparity after training. Instead of using the same fixed parameter $\\lambda$ for each client, our approach uses a more interpretable parameter $T$ which represents the demographic disparity target for the trained model. Ranging between 0 and 1, this parameter is easily interpretable and can be selected based on current and upcoming AI legislations [38, 44, 7]. Consequently to the choice of target disparity that the final trained model should exhibit, each client calculates its own $\\lambda$ based on $T$ and the local model disparity. The value will increase or decrease throughout the training process to make the trained model meet the target disparity value $T$. Our Tunable $\\lambda$ will range between 0 and 1: the more the model is unfair, the higher the $\\lambda$ will be to reduce the unfairness. The value of the Tunable $\\lambda$ is adjusted after each local training step on each client $k$. First of all, the difference $\\Delta$ between the desired disparity $T$ and the actual disparity is computed $\\Delta = T - (DPL_{b_i} + \\mathcal{N}(0, \\sigma_2))$. When computing the $\\lambda$ we introduce noise from a Gaussian Distribution $\\mathcal{N}(0, \\sigma_2)$ to guarantee DP in the computation of the next $\\lambda$. Using the $\\Delta$ we can compute the parameter velocity which is 0 in the first batch and then is updated in the following way: $velocity = momentum \\times velocity + \\Delta$ where momentum is a hyperparameter learned during the hyperparameter tuning phase. Then, we can finally compute the next $\\lambda$, clipping it to [0,1]: $\\lambda_{t+1} = clip(\\lambda_t - p \\times velocity)$ where $p$ is a learned hyperparameter indicating how aggressively we want to make the model fair."}, {"title": "4.3 Sharing Local Distribution Statistics", "content": "In a real-life scenario, clients involved in the training process usually have different data distributions, posing a challenge that requires a solution before applying the approach proposed in [53] in an FL setting. For instance, consider $K$ clients aiming to train a model predicting individuals' eligibility for bank loans. Each client's dataset includes data regarding different individuals, including sensitive information like gender. We must prevent the training of a model that is biased toward a specific gender. DPL regularization offers a potential solution, challenges arise in an FL setting where some clients possess data exclusively for one gender. In such cases, when these clients compute the DPL using Formula 3, they can only calculate either the part of the formula where $Z = Male$, or the part where $Z = Female$. To handle this case, we propose a solution that involves the sharing of some statistics from the client to the server. Without loss of generality, we can assume a classification problem with a binary label and binary sensitive feature, at the end of each training round, each client computes the following statistics and shares them with the server: the number of samples with $Z = 1$ and $\\hat{Y} = 1$ and the number of samples with $Z = 0$ and $\\hat{Y} = 1$.\nThe shared information is sufficient for the server to compute all the other necessary statistics and the probabilities $\\mathbb{P}(\\hat{Y} = y | Z = z)$. The clients do not need to compute and share the number of samples with sensitive values $Z = 1$ and with $Z = 0$ for each FL round, because these values do not change during the training; thus, sharing them just at the beginning of the training process is sufficient. This, along with the ability to compute the missing statistics, allows us to decrease the amount of privacy budget required for this part of the process. After computing the missing statistics (Line 9, Algorithm 2) and aggregating them (Line 10, Algorithm 2), the server shares these values with the clients selected for the next FL round. To clarify, and without loss of generality, assuming a dataset with a binary label $Y$ and sensitive value $Z$, each selected client will receive $\\mathbb{P}(\\hat{Y} = 1 | Z = 1), \\mathbb{P}(\\hat{Y} = 1 | Z = 0), \\mathbb{P}(\\hat{Y} = 0 | Z = 1), \\mathbb{P}(\\hat{Y} = 0 | Z = 0)$. The selected clients that lack the necessary data to calculate the DPL can rely on the statistics shared by the server.\nIn addition to managing edge cases, client-shared statistics play a crucial role in understanding the trained model's disparity. A global view of the model fairness involves aggregating the various counters from individual clients, allowing for the computation of demographic disparity as if the server had access to the entire dataset. Conversely, the opposite approach involves gaining a local understanding of demographic disparity by computing the mean of each client's demographic disparity. This dual approach enables a global view across the entire dataset and a focused client fairness examination."}, {"title": "4.4 Differentially Private Approach", "content": "To guarantee a privacy-preserving learning process we propose a solution which is $(\\epsilon, \\delta)$-DP. Ensuring this requires the implementation of DP in three distinct phases of our process:\n\u2022 The model training must be differentially private. To this end, we adopt a local DP approach based on DP-SGD (Algorithm 3 in the Appendix). Denote the privacy budget for this phase by $(\\epsilon_1, \\delta_1)$ and the corresponding noise needed to guarantee DP as $\\sigma_1$.\n\u2022 The tunable $\\lambda$ depends on the disparity of the model computed using the training dataset on each client. Therefore, the computation of this value must be differentially private. Let the budget for this phase be $(\\epsilon_2, \\delta_2)$ corresponding to a noise denoted as $\\sigma_2$."}, {"title": "4.5 Complexity Analysis", "content": "In this section, we analyze PUFFLE's complexity considering both the client side and the communication complexity."}, {"title": "4.5.1 Local Computational Complexity", "content": "From the perspective of local client complexity, DP-SGD is used to train the local models. While DP-SGD maintains the same theoretical time complexity as standard SGD, it is important to note that it is practically slower due to the explicit per-sample gradient computation requirement. The computation of the fairness loss in PUFFLE"}, {"title": "4.5.2 Communication Complexity", "content": "As explained in Section4.3, PUFFLE requires the clients to share additional statistics with the server compared to the standard FedAvg. This causes an increase in the communication cost. For a classification task with $|\\hat{Y}|$ possible classes and $|Z|$ sensitive groups, the additional information exchanged scales as $\\mathcal{O}(|\\hat{Y}||Z|)$ per client per FL round. This extra communication is constant to the number of model parameters and the number of participating clients."}, {"title": "5 Experimental Setup", "content": ""}, {"title": "5.1 Datasets", "content": "We demonstrate the effectiveness of PUFFLE across various models, datasets, and data distributions, showing the efficacy on both tabular and image datasets. This highlights the methodology's robustness regardless of all these factors, proving its applicability in different scenarios. We conduct tests using three distinct datasets: CelebA [36], Dutch [50] and ACS Income [17]. CelebA is an image dataset with more than 200.000 celebrity images with annotations. The annotations are crucial in our scenario since they contain information about the gender of the person depicted in the photos. In our experiment, we focus on a classification task, predicting whether the celebrity in the image is smiling or not. Dutch is a tabular dataset collected in the Netherlands, providing information about individuals. Compared with other common fairness tabular datasets like Compas [3], Dutch is bigger, making it suitable for our cross-device FL setting. The objective here is to predict whether the participants' salary is above or below $50,000. ACSIncome is also a tabular dataset built from the American Community Survey (ACS) with data coming from over all 50 states and Puerto Rico in 2018. The task involves predicting whether the samples in the dataset have a salary higher than $50,000. Since it is split into 51 parts based on the regions, it is a natural choice for FL experimentation allowing us to test PUFFLE on an inherently partitioned dataset."}, {"title": "5.2 Hyperparameter Tuning", "content": "The final goal of PUFFLE is to train models that offer good utility while mitigating the disparity. We defined the best hyperparameters as those that maximized the formula:\n$\\varphi = validation\\_accuracy + \\Delta$\nwith $\\Delta = 0$ when validation disparity $ < T$ and $\\Delta = -\\infty$ otherwise. The validation accuracy is computed independently by the validation clients on their local datasets and aggregated by the central server S using a weighted average. By maximizing $\\varphi$, we incorporate both accuracy and disparity into a single formula ensuring that the hyperparameters lead to a model disparity below the fairness target T at least on the validation set. We performed a hyperparameter search using the Bayesian search method dividing the K clients into 3 groups: train, validation and test. To ensure consistency throughout the hyperparameter search, we kept the set of test clients constant. On the contrary, we shuffled the assignment of clients to the training and validation sets for each of the different hyperparameter searches ran. For the Dutch and CelebA dataset, we used 150 clients: 50 as test clients, while the remaining 100 were split into 60% training and 40% validation clients. With ACS Income dataset, we had 51 clients in total: 11 used as test, 30 as train and 10 as validation. After completing the hyperparameter tuning, we retrained the model using the best hyperparameters, combining training and validation clients into a single training group."}, {"title": "6 Experimental Results", "content": "In this section, we present the results of the application of PUFFLE on the CelebA Dataset. Corresponding results on the other datasets with PUFFLE can be found in Appendix D."}, {"title": "6.1 Utility vs. Fairness vs. Privacy Trade-offs", "content": "To explore the interaction between these trustworthiness requirements, we examine combinations of two privacy targets $(\\epsilon, \\delta)$ and five fairness disparities targets T. We follow the common approach in deep learning of using $\\epsilon < 10$ [48], we select as target $(\\epsilon =$"}, {"title": "6.2 Fairness for Local Clients", "content": "In Figure 1d, 1e and 1f, we present a global view of the test disparity computed by the server using differentially private statistics shared by the clients. Our approach also allows us to examine the clients' local disparity. A local view of clients' disparity aids in understanding how PUFFLE systematically reduces individual client unfairness in FL training.  Despite our methodology focusing on minimizing the global disparity, the results demonstrate how the regularization positively influences local clients' disparity. The CDF describes the fraction of clients with a model disparity at most the value specified on the horizontal axis. We note that the curves for the Baseline and the DP models are lower than the ones for the models with Fairness+DP (PUFFLE) meaning that our approach is useful for reducing the unfairness of the local models across clients, while still providing privacy. Both the CDF of DP+Fair (Fixed $\\lambda$) and the DP+Fair (Tunable $\\lambda$) show similar reactions after using the Regularization. This mitigation can reduce the maximum disparity across clients and results in a more vertical distribution."}, {"title": "6.3 Key Takeaways", "content": "We summarize our experimental findings below:\n\u2022 We improved the concept of using regularization to alleviate the unfairness of ML models by introducing the notion of a Tunable $\\lambda$. PUFFLE is more interpretable and parameterized compared to past work [53].\n\u2022 Experiments on CelebA (Figure 1) demonstrate that PUFFLE reduces unfairness by at least 75% while lowering accuracy by 17% in the worst-case compared with the Baseline. These results are consistent with the other datasets. Compared with the Baseline, PUFFLE reduces model disparity by at least 75%, with an accuracy degradation of only 21% in the worst-case scenario with Dutch, and 1.4% with ACS Income. An extensive evaluation of these two datasets is reported in Appendix D.\n\u2022 Despite our approach primarily targeting global model fairness, our results demonstrate that regularization positively impacts clients' local model disparity."}, {"title": "7 Conclusion", "content": "The changes in AI legislations [38, 44, 7] are pushing the development and deployment of ML models that can respect trustworthiness"}]}