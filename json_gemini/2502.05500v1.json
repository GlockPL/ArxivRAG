{"title": "Vision-Ultrasound Robotic System based on Deep Learning for\nGas and Arc Hazard Detection in Manufacturing", "authors": ["Jin-Hee Lee", "Dahyun Nam", "Robin Inho Kee", "YoungKey Kim", "Seok-Jun Buu"], "abstract": "Gas leaks and arc discharges present significant risks in industrial environments, requiring robust\ndetection systems to ensure safety and operational efficiency. Inspired by human protocols that combine\nvisual identification with acoustic verification, this study proposes a deep learning-based robotic system\nfor autonomously detecting and classifying gas leaks and arc discharges in manufacturing settings. The\nsystem is designed to execute all experimental tasks (A, B, C, D) entirely onboard the robot without\nexternal computation, demonstrating its capability for fully autonomous operation. Utilizing a 112-\nchannel acoustic camera operating at a 96 kHz sampling rate to capture ultrasonic frequencies, the\nsystem processes real-world datasets recorded in diverse industrial scenarios. These datasets include\nmultiple gas leak configurations (e.g., pinhole, open end) and partial discharge types (Corona, Surface,\nFloating) under varying environmental noise conditions. The proposed system integrates YOLOv5 for\nvisual detection and a beamforming-enhanced acoustic analysis pipeline. Signals are transformed using\nShort-Time Fourier Transform (STFT) and refined through Gamma Correction, enabling robust feature\nextraction. An Inception-inspired Convolutional Neural Network further classifies hazards, achieving\nan unprecedented 99% gas leak detection accuracy. The system not only detects individual hazard\nsources but also enhances classification reliability by fusing multi-modal data from both vision and\nacoustic sensors. When tested in reverberation and noise-augmented environments, the system\noutperformed conventional models by up to 44%p, with experimental tasks meticulously designed to\nensure fairness and reproducibility. Additionally, the system is optimized for real-time deployment,\nmaintaining an inference time of 2.1 seconds on a mobile robotic platform. By emulating human-like\ninspection protocols and integrating vision with acoustic modalities, this study presents an effective\nsolution for industrial automation, significantly improving safety and operational reliability.", "sections": [{"title": "1. Introduction", "content": "Flammable gas leaks and arc discharges are frequently identified as critical safety and operational\nhazards in industrial settings. When these two phenomena occur simultaneously, there is a heightened\nprobability of explosive incidents, which can lead to severe facility damage, equipment failure, and\nhuman casualties [1, 2]. The resultant downtime of machinery further escalates economic losses and\ncontributes to environmental pollution [3-8]. Conventional approaches to detecting and classifying gas\nleaks and arc discharges typically rely on direct onsite inspections or fixed sensors monitored by\noperators. However, not all leak or discharge signals manifest in the audible frequency range; many\nappear in the ultrasonic domain, impeding timely detection and response. Moreover, when sensors are\nplaced too far from the actual leak or discharge location, they may fail to capture critical events. These\nlimitations degrade detection accuracy, reduce efficiency, and ultimately compromise system reliability\n[9-12].\nPrevious studies attempted to automate gas leak and discharge detection by combining plant-floor\nsensor data with deep learning models[13, 14], but they did not fully account for the diverse constraints\nof real industrial environments, such as intense noise, reverberation, and limited installation space[15].\nConsequently, accurately capturing and classifying ultrasonic target signals in factory settings remains\na formidable challenge[16]. To address this issue, the present work proposes BATCAM MX, a\nmultichannel deep learning-based autonomous robotic system designed to autonomously navigate to\nan optimal vantage point for signal acquisition and process both visual and acoustic data by emulating\na human-inspired protocol (Human Protocol).\nAs illustrated in Figure 1-a, a navigation algorithm is composed of line-following and 2D tag-based\nlocalization QR code, enabling efficient exploration of the factory floor. Meanwhile, the BATCAM FX\nacoustic camera, equipped with 112 microphones and RGB camera, captures ultrasonic signals and\nvideo streams at frequencies up to 48 kHz and 25FPS each. As shown in Figure 1-b, potential leak or\ndischarge locations identified by the YOLOv5 (You Only Look Once) model are subsequently targeted\nand amplified through a beamforming algorithm [17, 18], thereby minimizing extraneous interference.\nThe extracted target signals are then meticulously analyzed using STFT and Gamma Correction, after\nwhich the resulting spectrograms are segmented via a sliding window approach and provided as inputs\nto an Inception-style Convolutional Neural Network (CNN). By learning features at multiple time-\nfrequency scales[19-21], the Inception architecture achieves high classification accuracy even for high-\ndimensional or sub-audible signals. This multi-process of robot navigation, target detection, signal\ndetection and deep learning classification is managed upon Robotic Operating System (ROS) using\ndifferent communication protocols including CAN, WebSocket, RTSP enabling autonomous mobile\nrobotic platform that can detect gas and arc hazard with an inference time under 2.1s."}, {"title": "2. Related Works", "content": "Addressing gas leaks, power system faults, and partial discharge problems in modern industrial settings\nhas increasingly involved the deployment of diverse sensors and AI-driven methods. As summarized in\nresearch on automated methane leak detection using infrared imaging has employed frame\nsubtraction or z-score normalization as preprocessing steps for CNN- and LSTM-based models,\nachieving accuracies of up to 99% and F1-scores of 0.99[22, 23]. By contrast, approaches leveraging\nvisual video signals [24] process camera data from an Autonomous Underwater Vehicle via YOLOv4\nor Faster RCNN to detect gas leak plumes in real time, even in complex underwater environments. In\nthe realm of gas leak detection using acoustic signals, experiments conducted on a small-scale gas\nturbine setup [25] apply CWT (Continuous Wavelet Transform) preprocessing and feed the resulting\nspectrograms to a ResNet-50 (CNN), attaining a peak accuracy of 90%. Additionally, an indoor, SLAM-\nbased leak detection system employing an autonomous robot [26] demonstrates precise tracking\naccuracy within 1 m. Furthermore, research integrating GNNs (GNN with Variational Bayesian\nInference) into pressure-sensor data [27] achieves strong performance (AUC 0.9484, PAC 0.8) in an\nurban pipeline network. Power system fault detection and partial discharge diagnosis have also garnered\nconsiderable attention. One study [28] that extracts time-series features from current signals and applies\nensemble machine learning (e.g., Bagging, Boosting, Stacking) maintains accuracies of up to 99.95%\nunder varying load conditions (5\u201320 A). Another investigation [29] classifies CWT-transformed current\nwaveforms using a channel-wise thresholding approach (DRSN-CW), reporting an average accuracy of\n97.72%. For electromagnetic signals observed in medium-voltage (MV) power lines, a 1D-CNN and\nautoencoder ensemble was proposed [30], reaching over 91% accuracy and an MCC of 0.645 on both\nvalidated and unvalidated real-world datasets. Meanwhile, partial discharge detection systems have\ncommonly employed acoustic or ultrasonic data. In particular, CNN models augmented with Bayesian\noptimization [31] have achieved over 94% accuracy using STFT preprocessing of ultrasonic signals\nfrom transformers and switchgear, contributing to more effective early fault prediction in industrial\nmachinery. Lastly, in high-noise substation environments, a wavelet-based denoising approach has been\nproposed for partial discharge detection [32]. After converting time-domain signals into the frequency\ndomain, the method applies the DAMAS2 algorithm to a microphone array. Taken together, research in\ngas-leak detection and power-system fault diagnosis spans a wide spectrum of sensor modalities\n(infrared, acoustic, electromagnetic, pressure), preprocessing techniques (STFT, CWT, frame\nsubtraction, normalization), and model architectures (CNN, LSTM, GNN, ensemble ML). Nonetheless,\npractical implementation in noisy, large-scale industrial facilities requires enhanced noise-reduction\nschemes, multi-sensor data fusion, and integration with autonomous robotic systems for real-time\noperation. Building on these developments, this study presents an automated robot capable of detecting\nboth gas leaks and discharge events in a complex factory environment by leveraging ultrasonic and"}, {"title": "3. The Proposed Method", "content": "This study proposes a robotic system: BATCAM MX, combined with a deep learning-based leak and\narc discharge detection scheme to address the challenges outlined in the Introduction and Abstract. By\nimplementing autonomously guided optical navigation based on line-following and tag-based\npositioning, the robot can accurately reach designated targets in a manufacturing environment. Also, by\nusing pan-tilt system with object detection module, the robot can accurately align the acoustic camera's\nlistening point to the target and acquire ultrasonic data. This data is subsequently enhanced via\nbeamforming to more precisely capture target signals. Finally, an Inception-style CNN classifies\nwhether a leak or an arc discharge has occurred, enabling high detection accuracy and real-time\nperformance even in noisy factory environments. Notably, the Inception-style CNN serves as the focal\npoint of the proposed system by extracting features across multiple time-frequency scales, thereby\nachieving superior classification performance compared to conventional methods. This section presents\nthe hardware and software configuration of the BATCAM MX (Section 3.1), the navigation and\nlocalization algorithm to autonomously drive the robot (Section 3.2), the target signal detection and\nenhancement process (Section 3.3), and deep learning-based leak and discharge detection model to\ndetermine any gas leak or discharge (Section 3.4)."}, {"title": "3.1. \u0412\u0410\u0422\u0421\u0410M MX: Robotic System for Autonomous Leak and Discharge Detection", "content": "BATCAM MX integrates advanced hardware and software components into a single robotic platform\nto achieve precise leak and discharge detection in industrial environments. As illustrated in figure 3, the\nsystem's hardware is composed of the mobile robot platform, custom power circuit for each sensor,\nBATCAM FX device, pan-tilt actuator (Dynamixel XM430-W350-R, ROBOTIS) to align the acoustic"}, {"title": "3.2. Autonomously Guided Robot Navigation Algorithm", "content": "The robot's autonomously guided navigation algorithm applied in this research uses single RGB camera\nto follow the lane and identify the 2D tags installed on the field. Real-time video was streamed at the\nresolution of 640\u00d7480 and the frames were captured then converted to the HSV color space. To identify\nthe QR code, binary thresholding was used for effective tag detection and to identify the target lane,\nspecific color ranges are filtered out. When the target lane is detected, the center point of the line is\ncalculated and used as current input of the PID controller, while the frame center value is used as the\nreference point. The output from the controller is then used to calculate the control input to the robot's\ndriving controller as target linear and angular velocity by ROS message. When a QR code is identified,\nthe robot stops at the relevant coordinates to perform the leak or discharge detection task. This sequence\nensures concurrent autonomous navigation and signal detection in a complex factory environment."}, {"title": "3.3. Target Component Detection and Signal Enhancement", "content": "This section proposes a method that integrates YOLOv5-based object detection and beamforming to\nreliably detect target components and enhance the signals. The YOLOv5 model was trained using total\n4,800 images collected from the RGB camera of the BATCAM FX device, composed of 1,200 raw\nimage frames and 3,600 augmented images. Data augmentation was done using exposure adjustment,\nGaussian noise injection, and filtering considering the different light conditions in the manufacturing\nline or video frame quality of the robot. The model was then deployed on the robot. RGB video stream\nwas sent from BATCAM FX to the robot's computer (NUC PC, Intel) via websocket and the YOLOv5"}, {"title": "3.4. Deep Learning-Based Leak and Discharge Detection Model", "content": "Ultrasonic signals observed in factory environments typically occupy higher frequency ranges than\nstandard audio signals, making accurate classification challenging due to noise and reverberation.\nConventional CNN-based methods often fail to achieve high performance in such conditions. To\naddress these limitations, this study adopts an Inception-style CNN, as shown in Figure 5, designed to\nsimultaneously capture multi-scale time-frequency features. The Inception-style CNN effectively\nrepresents complex ultrasonic patterns by employing multiple parallel convolution kernels and stacked\nInception blocks for hierarchical feature extraction."}, {"title": "3.4.1 Input Data Preprocessing", "content": "The target signals for detection are ultrasonic data collected from the 112 channels of the BATCAM FX\nsystem. These signals are initially segmented into 0.04-second intervals and transformed into\nspectrograms using the Short-Time Fourier Transform (STFT) to preserve both time and frequency\ninformation. The STFT is defined as follows:\n$STFT{x(t)}(\\eta, \\omega) =\\sum_{\\tau = -0}^{00} x(\\tau)w(\\tau = nH)e^{-j\\omega\\tau}$ (2)\nwhere x(t) represents the raw signal (i.e., the beamformed signal y(t;0)), w(\u00b7) is the window\nfunction (Hamming window in this study), H denotes the frame shift, and wis the frequency axis\nvariable. The resulting spectrogram is a 2D time-frequency representation. To optimize computational\nefficiency and reduce inference time, the spectrogram is divided into smaller segments using a sliding-\nwindow sampling technique, expressed as follows:\n$X_i(k) = X(k + i\\Delta k), i = 0, 1, 2, ... and k \\in [0, K]$ (3)\nwhere X() represents the spectrogram obtained via STFT, i is the sliding window index, Ak is the\nstride of the sliding window, and K represents the maximum frequency range processed in one\noperation. To further refine the data, Gamma Correction is applied to suppress residual noise and\nenhance meaningful features in the target signals. The Gamma Correction is defined as follows:\n$I_{output} = I_{inpu}$ (4)\nwhere input represents the input frame, output is the gamma-corrected frame, and y is the gamma"}, {"title": "3.4.2 Overview of the Inception Block", "content": "The core component of the proposed model is the Inception Block, which applies Conv2D filters of\nvarying sizes in parallel to extract features across multiple time-frequency scales. This approach is\nparticularly effective in industrial ultrasonic environments characterized by high noise and\nreverberation. Within each Inception Block, the input tensor $X\\in R^{H\\times W\\times Cin_}$is processed through\nmultiple parallel paths, with each path employing different kernel sizes. For each pathi, the\nconvolution operation is expressed as:\n$H^{(i)} = f_{BN}(f_{pool}(\\sigma(X * W^{(i)} + b^{(i)})), i = 1, ..., N$ (5)\nwhere $W^{(i)}$ and $b^{(i)}$ are the kernel weights and biases for pathi, \u03c3(\u00b7) is the ReLU activation function,\n$f_{pool}$ denotes Max Pooling, and $f_{BN}$ represents Batch Normalization. These operations allow each path\nto extract features at different scales. To unify the output dimensions, the feature maps $H^{(i)}$ are\nprocessed through a 1\u00d71 convolutional layer (projection). The resulting outputs are concatenated along\nthe channel dimensions to produce the final output of the Inception Block:\n$H_{proj}^{(i)} = \\sigma((\\H^{(i)} *W_{1x1}^{(i)}) + b_{1x1}),$\n$H_{inception} = Concat(H_{proj} + H(2)\n, ..., H_{proj}. (6)\nThis design enables the Inception Block to combine local and global patterns extracted from the\nultrasonic spectrum, effectively capturing fine-grained and broad-scale features. The resulting features\nare flattened into a 1D vector and passed through Multi-Layer Perceptron (MLP) layers for\nclassification."}, {"title": "3.4.3 Overview of the Inception Block", "content": "This study employs categorical crossentropy as the loss function, commonly used for multi-class\nclassification tasks. The categorical crossentropy loss is mathematically expressed as follows:\n$L(v, \\hat{y}) = -\\sum_{i=1}^{m} \\hat{y}_i log (\\hat{y}_i)$ (7)\nwhere y = (y1,..., yn) represents the ground truth labels, \u0177 denotes the predicted probability\ndistribution by the model, and mmm is the total number of samples in the dataset. The optimizer"}, {"title": "3.4.4 Post-Processing of Inference Results", "content": "The classification results produced by the deep learning model are computed independently for each\nsliding window segment of the spectrogram. This approach may fail to fully capture the continuity of\nthe overall signal. As illustrated in Figure 5, the BATCAM FX system captures ultrasonic data with a\ntemporal resolution of 0.04 seconds. To ensure robust final predictions, it is necessary to aggregate the\nclassification results over 0.04-second intervals.\nDuring post-processing, the class probabilities generated during inference are arranged in\nchronological order. The probabilities for all segments within the same 0.04-second interval are\naggregated either by summation or averaging-to compute a representative probability vector for that\ntime slice. For example, if multiple sub-windows exist within a single 0.04-second interval, their\nprobability vectors can be averaged to yield a final probability vector.\nIf N sliding windows are associated with a specific 0.04-second segment, and the class probability\nvector for the i-th window is \u0177(i) = ((i),y),..., class), then the representative probability vector\n\u0177final is calculated as follows:\n$\\hat{y}_{final} = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{y}^{(i)}$ (8)\nThis approach allows the system to determine whether a gas leak or discharge event has occurred\nwithin each 0.04-second interval. By aggregating the results, this post-processing step reduces the\nlikelihood of transient false alarms and ensures more reliable classification outcomes."}, {"title": "4. Experimental Results", "content": "This section presents the results of various experiments conducted to validate the proposed leak and\ndischarge detection system (BATCAM MX) integrated with the Inception-style CNN\nSection 4.1 details the dataset, preprocessing steps, and neural network hyperparameters. Section 4.2\ndescribes the defined tasks, while Sections 4.3 to 4.5 compare the proposed method with existing\napproaches, evaluate classification performance under different signal-to-noise ratio (SNR) conditions,\nand investigate performance changes in reverberant environments, respectively. Section 4.6 examines\nthe model's generalization capability through k-fold cross-validation, and Section 4.7 discusses model\nefficiency in terms of parameter count and inference time. Sections 4.8 and 4.9 address, respectively,\nobject detection performance using YOLOv5 and an ablation study that verifies the contribution of the\ncore components in the proposed method."}, {"title": "4.1. Dataset and Preprocessing", "content": "This study utilizes data recorded by the BATCAM FX acoustic camera[33-35], which consists of 112\nmicrophone channels for beamforming. A band-pass filter is applied to isolate the 20\u201348 kHz frequency\nband, effectively excluding lower and audible frequencies. The dataset comprises real gas-leak data\nrecorded in industrial environments, Phase Resolved Partial Discharge (PRPD) signals, and background\nnoise. presents specimen samples of discharge signals. The discharge dataset includes three\ntypes of partial discharge\u2014Corona, Surface, and Floating-collected under various voltage levels by\nadding 1, 2, and 3 kV above the onset voltage for initial partial discharge specimens. presents\nspecimen samples of gas leak signals. The gas leak dataset encompasses various piping configurations,\nsuch as pinhole, open end, quick connect, and thread coupling, with different leak rates of 200, 500, and\n1000 cc/min. All these configurations are labeled as \"gas leak signals.\" Additionally, background noise\nincludes recordings from factory environments and Gaussian noise to simulate diverse industrial\nscenarios. Therefore, the dataset is composed of a total of five classes: three types of discharge signals,\ngas-leak signals, and background noise.\nEach audio file is sampled at 96 kHz for 10 seconds and converted into a spectrogram using STFT.\nEach STFT segment processes 512 samples (approximately 0.0053 s) with an overlap of 128 frames,\ncapturing the signal's time-frequency variations. Due to the computational cost of processing the entire"}, {"title": "4.2. Experiment Setting", "content": "As shown in figure 8, the BATCAM MX navigates autonomously within a factory environment to detect\ngas leakage, partial discharges signals according to four predefined tasks:\n\u2022\nTask A: Detecting gas leak signals in front of the robot.\n\u2022\nTask B: Rotating the Dynamixel motor in the direction specified by a QR code, followed by the\ndetection of PRPD signals."}, {"title": "4.6. Classification Performance under Varying SNR Levels", "content": "The model's classification performance under varying noise levels was evaluated by introducing\nGaussian noise ranging from +5 dB to -3 dB into the original ultrasonic signals, as summarized in\nThe Signal-to-Noise Ratio (SNR), a key metric in this evaluation, is defined in Equation (9) as\nthe logarithm (in decibels) of the ratio between signal power Ps and noise power Pn:\n$SNR = 10 log_{10} \\left(\\frac{P_{s}}{P_{n}}\\right)$ (9)\nWhen the SNR falls below 0 dB, the noise power becomes comparable to or greater than the signal\npower, significantly complicating the task of extracting target features for most deep learning models.\nAs shown in , all tested models demonstrate poor performance below -1 dB, reflecting their\ninability to identify meaningful patterns amidst overwhelming noise. However, performance improves\nmarkedly at +1 dB and above, where the signal becomes more distinguishable from the noise.\nThe proposed model, which utilizes multi-scale filters within its Inception architecture, exhibits\nsuperior performance across the +5 dB to +1 dB range. This is attributed to its enhanced ability to\ncapture residual features and isolate critical information masked by noise. Notably, the model maintains\nthe highest F1-score compared to other architectures, demonstrating its robustness in complex noise\nenvironments. These findings underscore the effectiveness of the Inception CNN in handling diverse\nnoise conditions, making it a reliable solution for deep learning-based autonomous robotic systems\noperating in challenging industrial settings."}, {"title": "4.7. Performance in Reverberant Environments of Different Room Sizes", "content": "Large-scale factory buildings introduce overlapping sound reflections, causing reverberation that\ndistorts ultrasonic signals. Reverberation occurs when sound waves reflect off surfaces such as walls,\nceilings, and floors, leading to time delays and phase shifts in the original signal. These effects can\ndegrade classification accuracy by reducing the clarity of the target signal. For ultrasonic signals, strong\nreflections often generate complex distortion patterns in the time-frequency domain, further\ncomplicating signal analysis. The interference between direct and reflected signals diminishes the\nsignal-to-noise ratio (SNR) and poses challenges for models attempting to learn critical features\naccurately. To replicate these conditions, the Pyroomacoustics library is utilized to simulate factory\nenvironments of varying sizes: small (30 m\u00b2), medium (100 m\u00b2), and large (500 m\u00b2). Reverberation is\nclosely related to room size and surface reflectivity, with smaller rooms exhibiting shorter reflection\npaths and more pronounced overlapping interferences. 5 demonstrates that smaller rooms\nintroduce more complex reflection interferences, slightly reducing overall classification accuracy.\nNevertheless, the Inception CNN achieves an F1-score approximately 0.14%p higher than ResNet in\nsmaller spaces. This improvement is likely due to the multi-scale filters' ability to effectively capture\nessential features despite the time delays and phase shifts caused by reverberation. The multi-scale\narchitecture processes interactions across frequency bands, enabling robust signal classification even in\nchallenging reverberant environments. These results highlight the proposed model's reliability and\nrobustness in handling diverse reverberation scenarios, underscoring its suitability for deployment in\ncomplex industrial settings."}, {"title": "4.8. Parameter Count and Performance Comparison", "content": "To assess real-time feasibility, each model's total parameters and inference time are measured. As\nreported in 6, the Inception CNN comprises 19,036 parameters-relatively few-yet achieves\ninference within about 2.1s and maintains top-tier performance in the SNR and reverberation\nexperiments. Although ResNet exhibits shorter inference times, it generally requires more parameters\nand shows somewhat inferior performance in practical scenarios involving noisy or reverberant signals.\nOverall, the proposed approach offers a well-balanced trade-off among accuracy, parameter efficiency,\nand inference speed."}, {"title": "4.9. Ablation Study", "content": "As shown in 7, an ablation study was conducted to measure how the Inception architecture and\nGamma Correction affect performance. As presented in, removing both elements' results in a noticeable\nperformance decline, with the F1-score dropping by up to 0.14%p. Notably, removing the Inception\nmodule causes a more pronounced degradation than removing Gamma Correction, indicating that multi-\nscale feature extraction is pivotal for sustaining stable performance. Nonetheless, Gamma Correction\nalso plays a vital role by intensifying relevant regions in the beamformed target signal while attenuating\nbackground noise. Hence, both components operate synergistically to maximize detection accuracy for\nleaks and partial discharges across diverse industrial environments.\nBy synthesizing these experimental findings, the proposed Inception-style CNN, combined with\nbeamforming and YOLOv5-based object recognition, demonstrates high reliability in detecting\nultrasonic anomalies\u2014such as gas leaks and partial discharges-in industrial settings characterized by\ncomplex noise and reverberation. The integration of QR code navigation further ensures real-time\nfeasibility for autonomous vehicles operating on factory floors."}, {"title": "5. Concluding Remarks", "content": "This study proposed the BATCAM MX, a deep learning-based autonomous robotic system designed\nfor the detection and classification of gas leaks and arc discharges in industrial environments. Inspired\nby the Human Protocol, the BATCAM MX integrates visual information to identify targets and\nautonomously navigates to optimal positions for collecting and analyzing acoustic data. The system\nleverages the BATCAM FX's 112-channel microphone array, capable of capturing ultrasonic signals up\nto 48 kHz. These signals are refined through a beamforming and STFT-based acoustic processing\npipeline. By employing Gamma Correction and an Inception-style CNN, the system extracts features\nacross multiple time-frequency scales, achieving a high detection accuracy of 99%.\nNotably, the proposed system demonstrated exceptional performance in challenging environments\ncharacterized by reverberation and intense noise. Across four controlled experimental tasks (Tasks A,\nB, C, D), the BATCAM MX outperformed conventional approaches, achieving up to a 44 percentage-\npoint improvement in performance. Validation with real-world manufacturing scenarios further\nhighlighted the practical applicability of the system. This emphasizes the robustness and effectiveness\nof integrating visual and acoustic information, modeled after the Human Protocol, as a practical solution\nfor industrial automation.\nAdditionally, the system maintained an inference time of 2.1 seconds, showcasing its ability to\nsystematically execute highly complex operations through the interaction of autonomous navigation\nand deep learning models. These capabilities underline the system's potential to significantly enhance\nsafety and operational efficiency in industrial settings.\nFuture research will focus on improving signal separation under extreme noise conditions (\u22640 dB)\nby exploring advanced acoustic noise suppression techniques specifically tailored to ultrasonic\nfrequency ranges. To achieve this, lightweight CNN-based models will be developed to efficiently\nsuppress noise while preserving critical ultrasonic signals [36]."}]}