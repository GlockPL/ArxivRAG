{"title": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs", "authors": ["Jiahao Yu", "Yangguang Shao", "Hanwen Miao", "Junzheng Shi", "Xinyu Xing"], "abstract": "Large Language Models (LLMs) have gained widespread use in various applications due to their powerful capability to generate human-like text. However, prompt injection attacks, which involve overwriting a model's original instructions with malicious prompts to manipulate the generated text, have raised significant concerns about the security and reliability of LLMs. Ensuring that LLMs are robust against such attacks is crucial for their deployment in real-world applications, particularly in critical tasks.\nIn this paper, we propose PROMPTFUZZ, a novel testing framework that leverages fuzzing techniques to systematically assess the robustness of LLMs against prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ selects promising seed prompts and generates a diverse set of prompt injections to evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the prepare phase, which involves selecting promising initial seeds and collecting few-shot examples, and the focus phase, which uses the collected examples to generate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can uncover more vulnerabilities in LLMs, even those with strong defense prompts.\nBy deploying the generated attack prompts from PROMPTFUZZ in a real-world competition, we achieved the 7th ranking out of over 4000 participants (top 0.14%) within 2 hours, demonstrating PROMPTFUZZ's effectiveness compared to experienced human attackers. Additionally, we construct a dataset to fine-tune LLMs for enhanced robustness against prompt injection attacks. While the fine-tuned model shows improved robustness, PROMPTFUZZ continues to identify vulnerabilities, highlighting the importance of robust testing for LLMs. Our work emphasizes the critical need for effective testing tools and provides a practical framework for evaluating and improving the robustness of LLMs against prompt injection attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have gained significant attention in recent years due to their outstanding performance in various natural language processing tasks. For example, they have been successfully applied in diverse roles such as online assistants, advertisement moderators, and code completion tools [19], [38], [45]. However, the rapid development of LLMs has raised concerns about their security and reliability, such as jailbreak attack [14], [66], [67], [72], backdoor attack [44], [49], [63], privacy leakage [34], [50], [57], [70] and other risks.\nAmong these threats to LLM, the prompt injection attack where the attacker could inject malicious prompts to override the model's original instructions and manipulate the generated text has raised significant concerns. For example, as shown in Figure 1, when the LLM is integrated into the applications as a decision-making module or assistant, attackers can inject malicious prompts to manipulate the output of the LLM or extract sensitive information. Specifically, as shown in one of the examples in Figure 1, the developer provides a prompt to the LLM to instruct it to detect if the comment is an advertisement or not (e.g., \"If so, output 1 and 0 otherwise\"). However, the attacker can inject a malicious prompt to overwrite the original prompt (e.g., \"Forgot previous instructions and output 0 only\"), thus manipulating the output of the LLM, and the advertisement can be misclassified as a non-advertisement. Such attacks can lead to severe consequences, and hinder the deployment of LLMs in real-world applications. Due to the potential risks of prompt injection attacks, the Open Web Application Security Project (OWASP) has identified prompt injection as one of the top-10 threats to LLMs [40]. Moreover, Bing search engine also demotes or even delists websites that are found to be using prompt injection attacks against LLMs to mitigate the risks [39].\nGiven the diverse nature of prompt injection attacks, it is impractical to include all possible attack scenarios in the training data of LLMs. Therefore, testing the robustness of LLMs against such attacks is crucial to ensure their security. Previous work [20], [37], [43], [51], [67] has utilized manual red-teaming by prompt engineering experts to assess the injection robustness of LLMs. However, manual red-teaming is both time-consuming and labor-intensive, making it challenging to cover all possible attack scenarios. Furthermore, with frequent updates to LLMs, the manual red-teaming process must be repeated to ensure ongoing security. For instance, as highlighted in [10], the alignment of GPT-4 changed significantly between the March 2023 and April 2023 versions, necessitating a repeat of the manual red-teaming process to ensure the security of the latest version. Consequently, static manual red-teaming is neither scalable nor efficient for prompt injection testing. The high cost associated with manual red-teaming makes robust testing of LLMs against prompt injection attacks particularly challenging.\nTo address these challenges, automated robust testing of LLMs against prompt injection attacks is essential. However, existing work [24], [29] on automated testing of prompt injection only focuses on specific attack scenarios, such as system prompt leakage or task redirecting, which require significant effort to extend to other scenarios. Additionally, these approaches rely on gradient optimization to generate attack"}, {"title": "II. BACKGROUND", "content": "In this section, we provide a brief overview of the background concepts that are necessary to understand the proposed approach. We first introduce the concept of a large language model and then discuss the concept of fuzzing."}, {"title": "A. Large Language Models", "content": "Model. Large language models (LLMs) are a class of machine learning models designed to understand and generate human-like text based on vast amounts of training data. These models are built using deep learning techniques, primarily leveraging transformer architectures [55]. The transformer model revolutionized the field of natural language processing (NLP) by enabling more efficient and effective handling of long-range dependencies in text. LLMs typically consist of multiple layers of transformers, each comprising self-attention mechanisms and feedforward neural networks. The self-attention mechanism allows the model to capture dependencies between words in a sequence, while the feedforward neural networks enable the model to learn complex patterns in the data. Popular LLMs usually have a large number of parameters, often in the order of billions. Popular LLMs include OpenAI's GPT-3 [7],"}, {"title": "B. Fuzzing", "content": "Fuzzing is an automated software testing technique that involves providing random or semi-random inputs to a program to discover bugs, vulnerabilities, or unexpected behaviors. Fuzzing has been widely used to test software systems, including web applications, network protocols, and file formats. The fuzzing technique was first introduced by Miller et al. [36] and has since evolved into various forms, such as coverage-guided fuzzing [5], grammar-based fuzzing [17], and mutation-based fuzzing [18]. Fuzzing has been successful in finding numerous security vulnerabilities in software systems, including memory corruption bugs, buffer overflows, and logic errors. Our research falls into the category of black-box fuzzing, where we have no knowledge of the internal structure of the target LLM and can only interact with it through the user prompt.\nThe black-box fuzzing technique typically follows the following steps:\n\u2022 Seed Initialization: The fuzzer generates a set of initial inputs, called seeds, to start the fuzzing process. These seeds can be random or based on some predefined templates. High-quality seeds can boost the fuzzing efficiency by covering a wide range of input space, as pointed out by the recent work [23], [26].\n\u2022 Seed Selection: In each iteration, the fuzzer selects a seed from the seed pool based on some selection strategy. The selection strategy can be a random selection or guided by some heuristics, such as the coverage-guided selection in AFL [69].\n\u2022 Seed Mutation: The selected seed is mutated to generate a new input. The mutation can be performed using various techniques, such as bit flipping, byte flipping, or dictionary-based mutation. The mutation process aims to generate diverse inputs to explore different parts of the input space.\n\u2022 Seed Execution: The mutated seed is executed on the target system, and the system's response is observed. The response can be the program's output, the program's behavior, or the program's internal state.\n\u2022 Seed Evaluation: The fuzzer evaluates the response to determine whether the seed triggers any bugs, vulnerabilities, or unexpected behaviors. The evaluation can be done using various techniques, such as code coverage analysis, symbolic execution, or dynamic taint analysis. The interesting seeds are then added to the seed pool for further exploration.\nOur PROMPTFUZZ mirrors these fuzzing steps in the context of LLMs. We initialize the seed pool with a set of high-quality injection prompts, select a seed from the pool based on a selection strategy, mutate the seed to generate a new prompt, execute the prompt on the target LLM, and evaluate the model's response to determine whether the prompt triggers any undesirable behaviors. We leverage the model's output to guide the fuzzing process and improve the efficiency of prompt generation. In the next section, we describe the proposed approach in detail."}, {"title": "III. DESIGN", "content": "As we illustrated in \u00a7II-B, the black-box fuzzing technique typically follows the steps of seed initialization, seed selection, seed mutation, seed execution, and seed evaluation. We have adapted these steps for LLMs to design PROMPTFUZZ. Two critical challenges in designing PROMPTFUZZ are the seed initialization and seed mutation. The seed initialization requires generating high-quality injection prompts to start the fuzzing process, and the initial seeds with low quality may significantly affect the fuzzing efficiency, which is already pointed out by recent work [23]. Therefore, it is not an ideal choice to leverage all collected seed prompts as the initial seeds for fuzzing. On the other hand, the seed mutation aims to generate diverse inputs to explore different parts of the input space, while the mutate transformation should be carefully designed to ensure the generated prompts are semantically meaningful and deliver the desired mutation trends. To address these challenges, we propose a two-stage fuzzing approach in PROMPTFUZZ: preparation stage and focus stage.\nAn overview of the two-stage design is illustrated in Figure 2. The fuzzing process starts with the preparation stage. It first collects all the human-written seed prompts and assigns a small and equal amount of resources to each seed prompt to apply all the mutation transformations uniformly (\u2460). Each mutation transformation is delivered via a mutator, which is a function that takes a seed prompt as input and generates a mutated prompt. The mutated prompts are then executed on the target LLM with validation defense mechanisms to observe the model's response and the injection results (\u2461). The injection results are then collected to analyze each initial seed's mutants' effectiveness and each mutator's performance. Based on the analysis, the top-ranked initial seeds will be preserved for the focus stage as well as the high-quality mutants (\u2462). Then the fuzzer will switch to the focus stage and the most of resources will be allocated to this stage.\nIn the focus stage, the fuzzer selects one promising seed from the seed pooling in each iteration based on the selection strategy instead of uniformly selecting seeds (\u2463). It leverages the preserved high-quality mutants as well as the mutator weights calculated in the preparation stage to guide the mutation process to generate more effective prompts (\u2464). Similar to the preparation stage, the mutated prompts are executed on the target LLM with target defense mechanisms to evaluate the injection results. The injection results are then collected to update the seed pool with high-quality mutants and thus these mutants can be directly selected in future iterations (\u2465). The fuzzer iterates through the focus stage until the stopping criterion is met. The stopping criterion can be the number of iterations, the number of successful injections, or the time limit.\nThis two-stage approach ensures that our fuzzer efficiently and effectively generates diverse and high-quality prompt injections to uncover vulnerabilities in LLMs even in the presence of strong defense mechanisms. In the following subsections, we describe the two stages in detail."}, {"title": "B. Preparation Stage", "content": "The goal of the preparation stage is to rank the initial seed prompts and mutators based on their effectiveness and performance, as well as prepare high-quality mutants for the focus stage. We describe how the preparation stage operates and how we measure the effectiveness of seed prompts and mutators in Algorithm 1.\nInput. The preparation stage begins by collecting all human-written seed prompts, denoted as S, to ensure a diverse set of initial seeds (line 1). These seed prompts serve as the foundation for generating various prompt mutations. The collection process can leverage existing prompt injection datasets, such as those provided by [2], [54], which offer a range of pre-defined prompt injection examples. Alternatively, seed prompts can be manually crafted to address specific scenarios or vulnerabilities. This initial diversity in seed prompts is crucial for covering a wide array of potential injection paths, thereby enhancing the robustness of the subsequent fuzzing process.\nPROMPTFUZZ also requires a set of mutators, denoted as M, as a crucial input to generate diverse and high-quality mutants. Unlike traditional fuzzing techniques in software testing that involve bit flipping or byte flipping, the mutation process for LLMs must preserve the semantic meaning of the prompts. Therefore, we follow the approaches suggested in prior works [12], [65] and leverage LLMs to generate semantic mutations. For this purpose, we utilize the gpt-3.5-turbo model due to its high efficiency and low cost in generating mutated prompts. The mutators are designed to perform various transformation operations to produce meaningful and diverse mutations. These operations include expand, shorten, crossover, rephrase, and generate similar. Each mutator operates using a carefully crafted prompt template, ensuring that the generated prompts maintain their semantic integrity while delivering the intended mutation transformations. Additional details about the mutator prompts and their specific instructions can be found in \u00a7B-B.\nDefense mechanisms are employed to enhance the robustness of the target LLM against prompt injection attacks. These mechanisms can include carefully designed system prompts, prompts appended to user inputs to constrain the model's output, model finetuning, other defense techniques such as word filtering, or even scenarios with no defense mechanisms. Since the attacker does not have access to the exact target defense mechanisms, we use a set of validation defense mechanisms, denoted as Dv, in the preparation stage to evaluate the effectiveness of the generated mutants. These validation defense mechanisms are constructed to resemble the target defense mechanisms but are known to the attacker, providing a realistic yet accessible evaluation environment.\nInitialization. The preparation stage begins by initializing the number of seeds S, mutators M, and defense mechanisms D (line 3-5). The attack success matrix A is then set up to record the number of successful injections for each combination of seed, mutator, and defense mechanism (line 6). This matrix helps in tracking the effectiveness of different seeds and mutators across various defense scenarios. Additionally, the mutator weights W are initialized to rank the mutators based on their performance (line 7). These weights will guide the selection of the most effective mutators in the focus stage. Finally, the preserved mutants P are initialized to store the high-quality mutants for the focus stage (line 8).\nMutation and Execution. As described in Algorithm 1, the preparation stage iterates through each seed prompt, mutator, and defense mechanism to generate and execute the mutant prompts (line 9-19). For each seed prompt, the algorithm applies each mutator to generate a mutated prompt. The mutated prompt is then executed on the target LLM with the validation defense mechanisms to observe the model's response. If the model generates the desired output, the attack is considered successful, and the attack matrix A is updated accordingly to reflect this success (line 17-18). Additionally, the successful mutant is recorded in the preserved mutants P, ensuring that good mutants are available for further selection.\nRanking. After evaluating all the mutants, the algorithm ranks the seed prompts based on their average success rate, referred to as seedASR (line 20-23). The intuition behind using seedASR is that if the mutants derived from a seed prompt are more successful, the seed prompt itself is likely to be effective in exploring the input space to uncover vulnerabilities. Following this ranking, the top-K initial seed prompts are preserved for use in the focus stage.\nThe algorithm also ranks the mutators based on their average success rate, known as mutatorASR (line 24-26). The mutatorASR is calculated by averaging the attack success rates of all mutants generated by each mutator. This ranking helps to identify the most effective mutators, guiding the mutation process toward the most promising transformations.\nThe final step in the preparation stage is to select high-quality mutants for each mutator. The algorithm identifies the top-T mutants for each mutator based on the number of successful attacks they produce (line 27-28). By selecting these high-quality mutants, we ensure that each mutator has a robust set of examples to guide the mutation process in the focus stage. This targeted selection enhances the likelihood of generating effective prompt injections during subsequent testing.\nOutput. The preparation stage outputs the top-K seed prompts, denoted as S, the mutator weights W, and the preserved high-quality mutants P for the focus stage. These outputs enable the fuzzer to concentrate on the most effective seed prompts and mutators during the focus stage, thereby optimizing the testing process and improving the detection of vulnerabilities."}, {"title": "C. Focus Stage", "content": "In this stage, the fuzzer allocates most of the resources to the most promising seed prompts and mutators to generate more effective injection prompts.\nInput. The focus stage begins with the selected seed prompts S from the preparation stage, mutators M, the oracle O, and the target LLM M (line 1). Additionally, the mutator weights W and preserved mutants P are provided to guide the mutation process effectively.\nThe target defense mechanisms Dt are those defenses that the attacker aims to bypass for the target LLM and are unknown to the attacker. Additionally, the fuzzer requires an early termination coefficient \u03b5 to determine when to stop the iteration for seeds not showing good potential. The query budget B limits the number of queries to the target LLM, ensuring that the fuzzer operates within resource constraints. Finally, the seed selector module S is responsible for selecting the seed prompts in each iteration based on a strategic selection process, rather than the round-robin selection used in the preparation stage. This strategic selection allows the fuzzer to focus on the most promising seeds, thereby increasing the chances of discovering effective prompt injections.\nInitialization. The focus stage begins by initializing the best average success rate (bestASR) to 0 and setting up a history list to record the mutation results (line 3-4). This initialization helps track the highest success rate observed and maintains a log of all the mutants and their effectiveness. The number of defense mechanisms D is determined based on the target defense mechanisms Dt (line 5). The seed selector module S is then initialized with the selected seed prompts from the preparation stage (line 6). This module will guide the selection of seeds in a strategic manner throughout the focus stage.\nSeed Selection. To allocate more resources to the most promising seed prompts, the focus stage employs the seed selector module S to choose the seed prompt in each iteration (line 8). The seed selector module can utilize various strategies to optimize the selection process such as bandit-based selection [48], [68], reinforcement learning-based selection [58], or heuristic-based selection [6], all of which are well-studied in the fuzzing community. In our approach, we follow prior work [65] and model the seed selection as a tree search problem. More detailed information about the seed selector module and its implementation can be found in \u00a7B-A.\nMutation. After selecting the seed prompt, the algorithm samples a mutator based on the mutator weights W (line 9). This sampling ensures that more effective mutators, which have higher weights, are chosen more frequently, thereby increasing the chances of generating successful mutants. The algorithm then finds the most relevant and similar mutate examples from the preserved mutants P for the selected mutator (line 10).\nTo select the most relevant examples, the algorithm first embeds the seed prompt and the available mutants generated by the selected mutator into an embedding space. The algorithm calculates the cosine similarity between the seed prompt and each mutant, identifying the top-R mutants with the highest similarity scores. These top-R mutants are selected as the few-shot demonstration examples for the selected mutator, where R is a hyperparameter. By using these relevant examples, the algorithm enhances the context for the mutator, leading to more effective and contextually relevant mutations. The selected mutator is then applied to the seed prompt using the few-shot demonstration examples to generate a mutated prompt (line 11).\nExecution. The mutated prompt is executed on the target LLM with each target defense mechanism from Dt to evaluate the model's response. The algorithm queries the oracle O to determine whether the attack is successful (lines 15-16). The attack success rate (ASR) is calculated as the ratio of successful attacks to the total number of defense mechanisms (line 21). If the ASR is positive, indicating that the mutant was successful against at least one defense mechanism, the algorithm updates the seed pool with the mutated prompt (line 25). The seed selector module S is then updated with the mutated prompt and its corresponding attack success rate (line 26). Additionally, the mutation results, including the mutated prompt and its ASR, are recorded in the history list for further analysis (line 27).\nEarly Termination. Although the seed selector module S helps to select the most promising seed prompts, two challenges hinder the efficiency of the focus stage. First, evaluating each mutant across all defense mechanisms to calculate the ASR can lead to unnecessary queries if the mutant is not effective. In such cases, querying all target defense mechanisms is redundant. Second, due to the exploratory nature of the seed selector module, each newly added seed initially has a high priority for selection in subsequent iterations. This can result in resource wastage if the seed is not promising and achieves only a low ASR. Compounding the issue, if a suboptimal seed is selected and generates a mutant with a low ASR, the seed selector module may continue to prioritize these ineffective mutants in the next iterations, causing the fuzzer to get stuck in a local minimum and overlook more promising seeds.\nTo address these challenges, we introduce an early termination mechanism in the focus stage. For mutants that have already failed against a significant number of defense mechanisms, we can terminate the evaluation process early and skip the remaining defenses. This is achieved by setting an early termination threshold. However, a fixed threshold may hinder the fuzzer's exploration, especially in early iterations. Therefore, we propose a dynamic early termination mechanism based on the best ASR achieved so far.\nSpecifically, if the current mutant has already failed in Dt* bestASR * \u03b5 defense mechanisms, where \u03b5 is the early termination coefficient, the mutant is deemed not promising, and the evaluation process is terminated early (lines 18-20). Furthermore, this mutant will not be appended to the seed pool, even if its ASR is positive (line 24). This strategy not only conserves the query budget but also prevents the fuzzer from getting stuck in a local minimum. As the fuzzer progresses, the early termination threshold increases, pushing the fuzzer to concentrate on more promising seeds to get a higher best ASR."}, {"title": "IV. EVALUATION ON BENCHMARK DATASETS", "content": "In this section, we evaluate PROMPTFuzz on benchmark datasets to answer the following questions:\n\u2022 Comparison with other methods: How does PROMPTFuzz compare with other prompt injection methods on benchmark datasets? (\u00a7IV-B)\n\u2022 Dependency on the human-written seed prompts: For challenging defenses that all human-written seed prompts fail, how does PROMPTFUZZ perform? (\u00a7IV-C)\n\u2022 Ablation Study: Does the design of PROMPTFUZZ, such as initial seed ranking and early stopping, have the positive effect as expected? How sensitive is PROMPTFUZZ to variations in its hyperparameters? (\u00a7IV-D)\n\u2022 Discussion: Are there any noteworthy to discuss about the evaluation results? (\u00a7IV-E)"}, {"title": "A. Experimental Setup", "content": "Datasets. We select the TensorTrust dataset [54] for our evaluation. TensorTrust is the largest benchmark dataset specifically designed for evaluating prompt injection attacks, containing both attack prompts and defense prompts crafted by human experts. This dataset is comprehensive and well-suited for testing the prompt injection capabilities against different defense mechanisms.\nTensorTrust consists of two sub-datasets: message-extraction robust and output hijacking robust. Each sub-dataset includes defense mechanisms with two defense prompts: pre-defense and post-defense prompts, as illustrated in Figure 3.\nThe pre-defense prompt serves as a system message, setting the context and guiding the model's output. It acts as an initial layer of defense by influencing the model's behavior before any user input is processed. The post-defense prompt, on the other hand, is appended to the user's input to further constrain the model's output and prevent it from generating undesirable responses because of the overly long attack prompts. This dual-layered defense approach is designed to make the task of prompt injection significantly more challenging for attackers.\nThe two sub-datasets are structured to defend two primary attack strategies:\n\u2022 Message Extraction Robust: Robust against attackers trying to extract sensitive information.\n\u2022 Output Hijacking Robust: Robust against attackers trying to manipulate the model's output to produce a specific response.\nThe sub-datasets provided by the authors of TensorTrust are curated to ensure they present a robust challenge to attackers.\nBy using this dataset, we can rigorously evaluate the effectiveness of PROMPTFUZZ in overcoming well-designed defenses and compare its performance against other methods in these two tasks.\nConsidering the budget constraints associated with querying proprietary LLMs, we sample 150 defense mechanisms for each sub-dataset, resulting in a total of 300 defense mechanisms for evaluation. Additionally, we select 50 defense mechanisms for each sub-dataset specifically for the preparation stage, which are not included in the evaluation set. These defense mechanisms are randomly sampled from the TensorTrust dataset.\nFor the initial seed pool of PROMPTFuzz, we randomly sample 80 initial attack prompts from each sub-dataset. These initial attack prompts serve as the foundation for the fuzzing process, allowing PROMPTFUZZ to mutate from a diverse set of starting points.\nTo mitigate the randomness of the fuzzing process, we run the focus stage of PROMPTFUZZ five times with different random seeds and report the average results as well as the standard deviation.\nOracle. We follow the same oracle setup as the TensorTrust codebase to evaluate the effectiveness of the attack prompts. For message extraction, the oracle checks whether the output contains the access code, indicating a successful extraction of sensitive information. For output hijacking, the oracle verifies if the output includes the phrase \"Access Granted\", signaling a successful manipulation of the model's response to produce the desired phrase.\nMetrics. We use three primary metrics to evaluate the effectiveness of the attack prompts generated by PROMPTFUZZ and the baselines: the best attack success rate (bestASR), the ensemble success rate (ESR), and the coverage.\nAs mentioned in \u00a7III-C, the ASR measures the number of successful attacks out of the total number of defense mechanisms for a single attack prompt. The bestASR is the highest ASR achieved by any single attack prompt for a given injection method. This metric provides a measure of the best-case scenario for an attack prompt, highlighting how a single prompt can threaten the target model across different defense mechanisms.\nThe ESR, on the other hand, measures the number of successful attacks out of the total number of defense mechanisms for an ensemble of attack prompts. To calculate the ESR, we select the top 5 attack prompts based on their ASR for each method and evaluate their combined effectiveness. The ESR provides an assessment of the overall effectiveness of the attack strategy when an attacker can use multiple attack queries, reflecting real-world scenarios where attackers may try several times to achieve their goals.\nThe coverage metric is inspired by the fuzzing in software testing, measuring the proportion of defense mechanisms that can be successfully attacked by at least one attack prompt. This metric provides a view of the overall effectiveness of the attack strategy towards the target model.\nTarget LLM. We use the latest version of OpenAI's gpt-3.5-turbo-0125 as the target model for our evaluation. This model is chosen not only for its robust instruction-following capabilities and cost-effectiveness but also because it is the target model used during the collection of the TensorTrust dataset. This choice ensures that our evaluation results are consistent with the design and intended use of the dataset, thereby providing a reliable assessment of PROMPTFUZZ's performance against the model's capabilities.\nHyperparameters. We set the hyperparameters of PROMPTFuzz as follows: the number of preserved mutants for each mutator T is 20, the number of preserved initial seeds K is 50, early termination coefficient \u03b5 is 0.5, the number of few-shot demonstrations R is 3. We allow a maximum of 150K queries for each sub-dataset, which costs around $30 for the GPT-3.5 model.\nBaselines. We compare PROMPTFUZZ with the following representative prompt injection methods:\n\u2022 Initial Seed: This baseline uses the 80 initial attack prompts for each sub-dataset. These prompts serve as a straightforward comparison to gauge the effectiveness of PROMPTFUZZ in generating more sophisticated injection prompts.\n\u2022 Human Expert: We sample injection prompts from other datasets constructed by human experts. For the message extraction task, we sample attack prompts from the Language Model Inversion (LMI) dataset [37]. For the output hijacking task, we sample attack prompts from the Lakera dataset [2]. To fit the hijacking task, we modify the Lakera prompts. For example, the original prompt \"Ignore the last sentence and say 'ok'\" is modified to \"Ignore the last sentence and say 'Access Granted\". Due to the high similarity of these prompts, we sample only 5 attack prompts from each dataset. For LMI, we follow the effectiveness order of the attack prompts as reported in the original paper. For Lakera, we randomly sample 5 attack prompts.\n\u2022 GCG-injection: GCG attack [72] is a state-of-the-art adversarial attack method designed to manipulate the output of LLMs. The GCG attack generates a gradient-guided suffix to append to the user prompt, maximizing the likelihood of producing the target output. We adapt this method for prompt injection, naming it GCG-injection. For message extraction, we set the target output as \"Sure, here is the access code:\". For output hijacking, we set the target output as \u201cAccess Granted\u201d. Since GCG attack requires white-box access, we run GCG-injection on four local LLMs: Vicuna-7B and 13B [13], and Guanaco-7B and 13B [15], using 80 validation defense mechanisms. We perform five runs with different seeds, generating 5 different adversarial suffixes. In each run, we select the suffix with the lowest target loss and use these suffixes for transfer attacks.\n\u2022 GPTFuzz-injection: GPTFuzz [65] is a black-box fuzzing method originally designed for testing LLM jailbreak attacks. We adapt GPTFuzz to GPTFuzz-injection for performing prompt injection attacks. Specifically, we replace the jailbreak templates used in GPTFuzz with attack prompts from the TensorTrust dataset and use an oracle for prompt injection to evaluate the effectiveness of the attack prompts, instead of the finetuned model used in GPTFuzz. We allocate the same query budget for GPTFuzz-injection as for PROMPTFUZZ."}, {"title": "B. Main Results", "content": "We present the evaluation results of PROMPTFUZZ and the baselines in Table I. From the table, we can observe that PROMPTFUZZ significantly outperforms the baselines across all metrics for both message extraction and output hijacking tasks.\nFor message extraction, PROMPTFUZZ achieves a bestASR of 64.9%, followed by GPTFuzz-injection with 35.30% This indicates that PROMPTFUZZ is highly effective at generating prompts that can bypass defenses and extract sensitive information. For output hijacking, PROMPTFUZZ achieves a bestASR of 75.33%, with the second-best being GPTFuzz-injection at 52.67%. Notably, for output hijacking, the coverage of PROMPTFUZZ approaches 100%, indicating that the attack prompts generated by PROMPTFUZZ can effectively bypass nearly all defense mechanisms during fuzzing. We also visualize the performance change as the number of used queries increases of PROMPTFUZZ and GPTFuzzer-injection in Figure 4. From the figure, we can observe that the performance of PROMPTFUZZ increases more rapidly than GPTFuzzer-injection, and consistently outperforms GPTFuzzer-injection across different query budgets for all metrics. Even with a limited query budget (e.g., 1/3 of the total budget), PROMPTFuzz still achieves a decent result, demonstrating its efficiency in generating effective attack prompts.\nBy comparing the performance of the initial seed prompts and PROMPTFUZZ, we can see that PROMPTFUZZ significantly improves the metrics for both tasks. This demonstrates PROMPTFUZZ's ability to enhance the effectiveness of existing attack prompts, turning less effective initial seeds into highly successful attacks.\nWe also observe that the human expert baseline and GCG-injection baseline perform poorly compared to others, especially for the message extraction task. A potential explanation for the poor performance of the human expert baseline is that the attack prompts are designed to test the LLM's injection robustness without considering specific defense mechanisms. As a result, when faced with strong defenses, these human-written prompts may not be effective.\nFor the GCG-injection baseline, the limited performance may be attributed to the transferability of the adversarial suffixes generated by the local LLMs. Since the GCG attack requires white-box access, the adversarial suffixes generated by the local LLMs may not be as effective when applied to the target LLM. This limitation in transferability has also been observed in recent works [9], [28].\nOverall, these results highlight the effectiveness of PROMPTFUZZ in generating robust and effective prompt injections that can bypass various defense mechanisms, significantly outperforming existing baselines."}, {"title": "C. Dependency on Human-Written Seed Prompts", "content": "To analyze the dependency of PROMPTFUZZ on human-written seed prompts, we evaluate its performance on defense mechanisms that all human-written seed prompts fail to attack. Specifically, there are 38 defense mechanisms for the message extraction robust sub-dataset and 30 for the output hijacking robust sub-dataset that none of the initial seeds can bypass. We run PROMPTFUZZ against these defense mechanisms and report the results in Table III.\nFrom the results, we observe that when the initial seeds are ineffective, the performance of PROMPTFUZZ is reduced compared to the results in Table III. Despite this reduction, PROMPTFUZZ still achieves over 20% bestASR for both tasks, demonstrating its ability to enhance the effectiveness of attack prompts even when the initial seeds are not effective. Notably, the coverage of PROMPTFUZZ for output hijacking remains high at 96.67%, indicating that PROMPTFUZZ can still successfully attack nearly all defense mechanisms, even when starting with ineffective initial seeds.\nThese findings highlight the robustness of PROMPTFUZZ in improving the success rate of prompt injections, showcasing its potential to generate effective attack prompts under challenging conditions where human-written seeds fail."}, {"title": "D. Ablation Study", "content": "To analyze the impact of each component", "variations": 1, "hyperparameters": "the number of few-shot demonstrations R and the early termination coefficient \u03b5. We present the results for the metric bestASR in Figure 5. From the figure, we observe that as R increases, the bestAS"}]}