{"title": "Rational Tuning of LLM Cascades via Probabilistic Modeling", "authors": ["Michael J. Zellinger", "Matt Thomson"], "abstract": "Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using grid search, our parametric Markov-copula model significantly improves runtime scaling with respect to the length of the cascade and the desired resolution of the cost-error curve, turning them from intractable into low-order polynomial. In addition, the optimal thresholds computed using our continuous optimization-based algorithm increasingly outperform those found via grid search as cascade length grows, improving the area under the cost-error curve by 1.9% on average for cascades with k\u2265 3 models. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing LLM systems.", "sections": [{"title": "Introduction", "content": "As LLMs become workhorses of the modern computing stack, systems of LLMs have received significant attention (Zaharia et al. (2024), Chen et al. (2024b)). These approaches make it possible to adapt computational spending to the performance requirements at the query or task level (Kag et al. (2023), Chen et al. (2023)), yielding significant gains in operational efficiency. These gains are achievable even when accessing LLMs entirely via black-box API calls, by switching between models of different capabilities.\nHowever, moving from single LLMs to LLM systems introduces significant additional complexity. To find the system's optimal operating point, it is important to understand not just the performance of individual models but also the interactions between their error rates. For example, in a simple two-model LLM cascade in which a small model delegates difficult queries to a large model, the large model's error rate increases conditional on receiving a query, since the small model's confidence gating induces an adverse selection (Zellinger and Thomson (2024)).\nIn this paper, we present a parametric probabilistic model for the joint distribution of the calibrated confidences of a sequence of k LLMs, providing a rational basis for understanding the performance of LLM cascades. We focus on cascades whose constituent models are ordered by size, from smallest to largest. Our probabilistic model is based on a Markov factorization, leveraging the insight that LLMs similar in size are more predictive of each other's confidence. After using logistic regression to calibrate each LLM's confidence, we account for the pairwise interactions between subsequent LLMs' error rates using bivariate copulas, providing a data-efficient model of cascade performance that performs well with n \u2248 300 training examples across six benchmarks.\nOur Markov-copula model makes it possible to tune the confidence thresholds of an LLM cascade using continuous optimization. Compared to selecting these thresholds via grid search, our algorithm significantly improves runtime scaling. For example, we reduce scaling with respect to the cascade length k from exponential to low-order polynomial, making it much faster to tune longer cascades consisting of k\u2265 5 models. In addition, the optimal thresholds computed using our continuous optimization-based algorithm increasingly outperform those found via grid search as cascade length grows. For cascades consisting of k\u2265 3 models, we report an average 1.9% decrease in the area under the cost-error curve across six benchmarks.\nRelative to the prior literature on LLM cascades, our main contributions are as follows:"}, {"title": "Background and Related Work", "content": "Language Models: given a predefined token vocabulary V, a large language model (LLM) M defines an autoregressive probability distribution t ~ p(\u00b7|t1, ..., tn) for the next token t \u2208 V given a sequence of tokens (t1,..., tn) \u2208 Vn. In this work, we focus on the overall input-output behavior of the model M. We let x stand for the entire query consisting of tokens (t1, ..., tn) and write M(x) for the sequence of tokens (tn+1,tn+2,...) obtained when repeatedly sampling tj+1 ~ P(.|t1, ..., tj) for j > n until encountering a stop token t\u00f8 \u2208 V.\nLanguage Model Cascades: a length-k LLM cascade C = M\u2081 \u2192 ... \u2192 Mk routes an incoming query x sequentially from model Mi to Mi+1 based on confidence measures \u03a6\u2081 = \u03a6\u2081(x) \u2208 [0,1]. When x reaches Mi, the cascade returns Mi(x) if \u03a6i(x) > \u03a6i, where \u03c6\u2081 \u2208 (0, 1) is a confidence threshold for model Mi. Otherwise, C forwards the query x to the next model, Mi+1. Writing Ci:k for the subcascade Mi \u2192 \u2192 Mk consisting of the last k - i + 1 models, the output C(x) of the overall cascade is defined recursively as\n\nC(x) = \\begin{cases}\nM\u2081(x) \\text{ if } \u0424\u2081(x) > \u03a6\u2081 \\\\\nC\u2082:k(x) \\text{ otherwise, }\n\\end{cases}\n \\text{or } |C| = 1\n\n(1)\n\nwhere |C| is the length of the cascade, for example |C2:k| = k \u2212 1.\nDifferent authors have recently explored LLM cascades. Chen et al. (2023) have shown that it is possible to approach the performance of a large LLM at much lower cost by initially sending queries to a small model; Aggarwal et al. (2024) present a flexible cascading approach based on a POMPD router; Yue et al. (2024) propose LLM cascades specifically for mathematical reasoning benchmarks; and Gupta et al. (2024) consider uncertainty at individual token position within longer generations. While many of these approaches use standard uncertain quantification techniques for LLMs (discussed below), some use trained neural networks for making the decision of forwarding a query x to the next model. Neural network approaches have the potential to make more finegrained distinctions between the capabilities of different LLMs\u00b9, but may require large amounts (n > 1000) of task-specific training data to perform well.\nJitkrittum et al. (2024) discuss the limits of forwarding queries based purely on the confidence level of the current model, proposing to train a cascading decision that takes into account not only the current model's probability of correctness, but also that of the following model. In addition, Wang et al. (2024) explore finetuning LLMs to make them more effective as part of a cascade. Other methods for LLM orchestration use routers that directly forward queries to suitable LLMs in a one-to-many architecture (Ding et al. (2024),"}, {"title": "Rational Tuning of LLM Cascades via Probabilistic Modeling", "content": "Our probabilistic model for the joint distribution of LLM confidences is based on calibrated confidence scores. We use logistic regression to transform a raw confidence signal praw into the calibrated confidence score\n\n$ = P(Praw),\n\n(3)\n\nwhere are the parameters of the logistic regression. The calibrated confidence $ estimates the model's probability of correctness based on the raw confidence signal praw. We calibrate each model separately, resulting in functions 1, ..., \u03a6k for the models M1, ..., Mk of a cascade M\u2081 \u2192 \u2192 Mk. See Section 4.1 for"}, {"title": "Parameter Inference for the Probabilistic Model", "content": "In this section, we describe in detail the components of our parametric probabilistic model and how we infer their parameters.\nContinuous-discrete mixture of scaled beta distributions: to model the marginals of calibrated confidence, we must account for the possibility that LLMs sometimes return perfect confidence Praw = 1.0, possibly as a result of performance optimizations such as quantization (Dettmers et al. (2024), Proskurina et al. (2024)) Depending on the LLM and the task, almost half of all queries may return perfect confidence, as is the case of GPT-40 Mini on the MMLU validation set (45.7%).\nTo accommodate the resulting discrete probability masses at a minimum and maximum calibrated confidence min and max, we use a mixed continuous-discrete distribution based on a mixture of two beta distributions. Specifically, we use the distribution function\n\nF(|W1, W2, min, \u03a6max; \u03c0, \u03b11, \u03b21, \u03b12, \u03b22) = Wmind\u00f8min ($)+Wmaxd\u0444\u0442\u0430\u0445 (\u0444)+(1-Wmin \u2013 Wmax)Fmixture($),\n\n(7)\n\nwhere Fmixture() is\n\nFmixture(min, \u00d3max; A1, B1; A2, \u03b22) = \u03c0F\u03b2(-\\frac{min}{max - min} |\u03b11, \u03b21) + (1 \u2212 \u03c0)F\u03b2(-\\frac{min}{max - min} |\u03b12, \u03b22).\n\n(8)\n\nHere, F\u03b2(\u00b7\u03b1, \u03b2) is the beta distribution with pdf fs(x|\u03b1, \u03b2) = x\u03b1\u22121(1 \u2212 x)\u03b2\u22121 for x \u2208 (0,1).\nWe infer the parameters of the model (7) as follows. First, we estimate the minimum and maximum calibrated confidences min and max by their observed minimum and maximum values on the training set. We estimate the corresponding discrete probability masses wmin and Wmax by simple counting. Finally, to estimate the mixture of beta distributions (8), we use the expectation-maximization algorithm (Dempster et al. (1977)).\nGumbel copula: to model the correlations between the calibrated confidences of pairs of LLMs, we use the Gumbel copula Co(u, v) given by\n\nCo(u, v) = exp\\left(-\\left(log\\left(\\frac{1}{u}\\right)^{\\theta} + log\\left(\\frac{1}{v}\\right)^{\\theta}\\right)^{\\frac{1}{\\theta}}\\right).\n\n(9)"}, {"title": "Tuning the Confidence Thresholds", "content": "The purpose of the Markov model (6) is to obtain optimal cost-error tradeoffs for an LLM cascade C by tuning the confidence thresholds. We formulate the optimization problem\n\n0* arg min (1 - Po(Correct)) + XE [Cost],\n\n(11)\n\nwhere \u03b8\u2208 Rk-1 denotes the confidence thresholds ($1,..., \u03a6k\u22121). The Lagrange multiplier \u5165 \u2265 0 indicates the user's cost sensitivity. Setting X = 0 means that cost is irrelevant, whereas \u5165 > 0 penalizes the use of expensive models. To compute the efficient frontier of optimal (P(Correct), E[Cost]) tuples, we solve (11) for different values of the cost sensitivity \u03bb.\nSince A has no known relationship with the expected cost, it is not clear how to choose A to obtain uniform coverage of the efficient frontier. In practice, we start with very small values of A and set\n\n> \u2190 (1 + r),\n\n(12)\n\nfor some r > 0, until the cost constraint is stringent enough to make the expected cost equal to the least expensive model's expected cost. Typically, setting r between 0.25 and 1 performs well. For any potential gaps in coverage, we adaptively interpolate the optimal thresholds. Specifically, if \u5165(i) < \u5165(i+1) yield optimal thresholds (i) and (i+1) and the gap (0+1) - 0(2) | = 2+1) \u2013 ) | for any individual threshold exceeds probability mass q based on the distribution of the calibrated confidence \u03a6j, we insert\n\n\u03b8(i+1/2) = (\u03b8(2) + \u03b8(i+1))/2\n\n(13)\n\ninto the list of optimal thresholds between 0(i) and \u0189(i+1). We repeat the infilling procedure (13) until no gaps remain at level q. We have found q < 0.2 to perform well.\nEfficient computation and optimization of the objective: solving the minimization problem (11) requires computing a cascade's probability of correctness and expected cost for candidate confidence thresholds \u03b8 = ($1,..., \u03a6k\u22121) \u2208 Rk\u22121. To compute these quantities, we rely on the decompositions (21) and (22) presented in"}, {"title": "Methodology", "content": "Forwarding Queries: the models in our cascades decide whether to forward queries by thresholding the calibrated confidence $ = \u03a6(Praw), where praw is the raw confidence signal. We obtain Praw from the model-intrinsic next-token probabilities. On multiple-choice tasks, we take the maximum probability among the answer choices (Hendrycks and Gimpel (2018), Plaut et al. (2024)). In the natural language generation case, we first generate the answer, then send a follow up verification prompt to the model asking \"Is the proposed answer <answer> true? Answer only Y or N.\" We use the probability of the Y token as the confidence signal Praw Our prompt templates are available in Appendix C.\nSince we focus on providing techniques compatible with black-box LLM inference via third-party APIs, we leave consideration of hidden layer-based confidence signals to future work. In addition, we do not consider resampling methods such as semantic entropy (Farquhar et al. (2024)). Such methods are compatible with black-box inference, but in the context of LLM cascades, their computational overhead appears prohibitive. For example, inference of Llama3.1 405B typically costs 15 times more than inference of Llama3.1 8B. In this case, it is desirable to directly run the 405B model once rather than forward a query based on k \u2248 15 resamples of the 8B model. See Appendix D for a table listing small-large model pairings from Meta, Anthropic, and OpenAI, along with their price differentials.\nConfidence Calibration: raw token probabilities of instruction-tuned LLMs are typically poorly calibrated (Ouyang et al. (2022), Brown et al. (2020), OpenAI et al. (2024), Plaut et al. (2024)). However, calibration is important for accurate error prediction. To obtain calibrated confidence scores, we use logistic regression. We favor this approach over temperature scaling since it yields p values and other statistical metrics that are useful for diagnosing calibration issues, especially in a low-data scenario.\nUnfortunately, the overconfidence of the raw token probabilities makes the distribution of raw confidence signals highly peaked. The raw token probabilities accumulate near 1.0, making tiny changes in confidence (for example, Praw = 0.98 vs Praw = 0.99) highly consequential. To enhance the calibration performance of logistic regression, as a pre-processing step we apply hyperparameter-free feature transformations that spread out the overconfident probabilities via asymptotes near Praw = 0.0 and praw = 1.0. Following Zellinger and Thomson (2024), on multiple-choice tasks we use the transformation\n\n=log\\left(\\frac{1}{1-P_{raw}}\\right),\n\n(16)\n\nwhereas on natural language generation tasks, we use\n\n(Praw) = \\begin{cases}\nlog(\\frac{1}{1-P_{raw}}) \\text{ if } p\\geq \\frac{1}{2},\\\\\nlog(\\frac{1}{P_{raw}}) \\text{ if } p < \\frac{1}{2}.\n\\end{cases}\n\n(17)\n\nImportantly, these feature transformations do not require any hyperparameter tuning."}, {"title": "Performance Summary", "content": "Tables 1 and 2 show the overall performance of all the language models across tasks, including the calibration performance. We measure calibration in terms of the expected calibration error (ECE), which we compute adaptively by bucketing confidence scores into 10 bins based on the deciles of their distributions. Tables 1 and 2 yield several interesting findings.\nFirst, some of the models often return raw log probabilities indicating certainty (-\u221e or 1.0). This tendency varies strongly by model family. OpenAI's GPT models are especially prone to certainty: on MMLU, for example, GPT-40 Mini returns raw confidence 1.0 on 45.7% of queries, while GPT-40 does so on 22.7% of queries. By contrast, Llama3.1 405B returns perfect confidence only on 0.1% of queries.\nSecond, the test ECE for our calibration scheme varies by model and by benchmark. The benchmark yielding the poorest calibration is MedMCQA, giving an average test ECE of 7.4% across models. However,"}, {"title": "Goodness-of-Fit of the Markov-Copula Model", "content": "In this section, we show that our probabilistic model fits the empirical data well. We start by presenting evidence that the Markov assumption (5) approximately holds. Second, we show that our Gumbel copula models successfully account for correlations between the error rates of different LLMs, as measured by low square-rooted Cram\u00e9r-von Mises (CvM) statistics and low rejection rates of the null hypothesis. Finally, we show that our mixed discrete-continuous mixtures of beta distributions provide an adequate model for the marginal confidence distributions, as measured by low square-rooted CvM scores. However, the high rejection rates of the null hypothesis suggest the potential for further improvements."}, {"title": "Verifying the Markov Assumption", "content": "To verify that (5) approximately holds, we first visualize the rank correlation between the calibrated confidences of different models. Figure 1 shows that the Kendall's T rank correlation is higher for models of similar sizes. In addition, models sharing the same architectural family (Llama, GPT, or Qwen) are more highly correlated than models of different families.\nThese findings suggest that a cascade composed only of Llama models (1B-405B) satisfies the Markov assumption more exactly. Consider Figure la as an example. For the Llama cascade, Kendall's 7 is highest near the heatmap's diagonal, suggesting a Markov property. By contrast, the mixed cascade composed of Llama, GPT, and Qwen models shows a more haphazard pattern. For example, the rank correlation between GPT-40 Mini and GPT-40 (\u03c4 = 0.55) is higher than that between GPT-40 and Llama3 405B (\u03c4 = 0.54), even though the latter pair of models are more similar in size. Similarly, Llama3 405B is more strongly"}, {"title": "Testing the Gumbel Copulas for Modeling LLM Correlations", "content": "To evaluate the goodness-of-fit of our Gumbel copula models, we first visualize the correlation between the calibrated confidences of pairs of LLMs. Figures 2 and 3 show scatterplots for several pairs of Qwen, OpenAI, and Llama models. Each scatterplot shows the copula-transformed variables\n\nu = Fn($),\n\n(19)\n\nwhere is the calibrated confidence and \u00cen its empirical distribution on the test set. The marginal distribution of each u is uniform, since we restrict our copula models to the region (min, (max) of calibrated confidence where the marginal confidence distribution is smooth. Note that Figure 3 highlights the Markov property by showing the increasing rank correlation between Llama models of similar sizes.\nWe formally test the goodness-of-fit between the fitted Gumbel copulas and the test data by carrying out a Cram\u00e9r-von Mises test using parametric bootstrapping, following the \u201cKendall's transform\" approach described in Genest et al. (2009). The test involves computing the univariate distribution of copula values Cij (Fi(x), Fj(x)) for x ~ p(x), using both the empirical copula and the fitted Gumbel copula. We evaluate the difference between these two distributions using the Cram\u00e9r-von Mises (\u221anCvM) statistic and obtain a p value by parametric bootstrapping with B = 1000 samples. In each case, we fit the Gumbel copula on the training data (n \u2248 300) and evaluate the p value relative to the test data (n \u2248 1000)."}, {"title": "Testing the Discrete-Continuous Marginal Confidence Distributions", "content": "First, we visualize the agreement between the fitted continuous-discrete mixtures of scaled beta distributions and the histograms of calibrated confidence values on the test set. To construct these plots, we first train"}, {"title": "Rational Tuning of Confidence Thresholds", "content": "In this section, we examine the performance and runtime scaling of our continuous optimization-based algorithm (11) for selecting optimal confidence thresholds. We consider all 26 possible cascades of length k\u2265 2 composed of Meta's Llama models (1B, 3B, 8B, 70B, and 405B) and evaluate against a grid search baseline on six benchmarks (MMLU, MedMCQA, XSum, TriviaQA, GSM8K, TruthfulQA) spanning general-purpose knowledge and reasoning, domain-specific QA, text summarization, open-ended QA, mathematical reasoning, and the ability to avoid hallucinations on adversarial questions.\nPerformance metrics: we evaluate the area under the cost-error curve (AUC) on the test set. Specifically, computing the AUC means plotting the test error (y axis) against the expected cost (x axis) and evaluating"}, {"title": "Conclusion", "content": "We have presented a framework for rationally tuning the confidence thresholds of LLM cascades using continuous optimization. Our approach is based on a parametric probabilistic model for the calibrated confidences of a sequence of LLMs. This probabilistic model is based on a Markov factorization, which accounts for pairwise correlations between the error rates of different LLMs using copulas, yielding a data-efficient approach. Goodness-of-fit analyses spanning 10 LLMs and 6 benchmarks have shown good agreement with the test data.\nImportantly, our probabilistic model yields expressions for a cascade's probability of correctness and expected cost that are differentiable with respect to the confidence thresholds, making continuous optimization possible. Compared to selecting confidence thresholds using grid search, our continuous-optimization based approach presents significantly enhanced computational scaling, turning the dependence on cascade length k from intractable into low-order polynomial. In addition, the optimal thresholds found using our method"}, {"title": "Appendix A: Proof of Proposition 2", "content": "Proposition. 2. Let C be a cascade M1 \u2192 \u2192 Mk with confidence thresholds ($1, ..., \u0444\u043a\u22121). Assume that the distribution functions for the calibrated confidences \u00dei satisfy (5), for i = 1, 2, ..., k. Assume further that the expected numbers of input and output tokens, Tin) and T(out), for each model i are independent of the calibrated confidences \u03a61, ..., \u03a6k. Then the probability of correctness P(correct) and expected cost E[Cost] for the cascade Care\n\nP(correct) =\\sum_{i=1}^k P(\\Phi_1 \\le \\phi_1) \\prod_{j=2}^{i-1} P(\\Phi_j \\le \\phi_j | \\Phi_{j-1} \\le \\phi_{j-1}) \\int_{\\{\\Phi_i > \\phi_i\\}} d\\Phi_i(\\omega |\\Phi_{i-1} \\le \\phi_{i-1})\n\n(21)\n\nE[Cost] =\\sum_{i=1}^k P(\\Phi_1 \\le \\phi_1) \\prod_{j=2}^{i-1} P(\\Phi_j \\le \\phi_j | \\Phi_{j-1} \\le \\phi_{j-1}) (1 - P(\\Phi_i \\le \\phi_i | \\Phi_{i-1} \\le \\phi_{i-1})) \\sum_{j=1}^i E[C_j]\n\n(22)\n\nwhere Ci is the cost per query of model i. Specifically, if (in) and out) are the costs per input and output token, Ci = y(in)T(in) + (out) (out)."}, {"title": "Appendix B: Algorithm for Computing P(Correct) and E[Cost]", "content": "Algorithm 1 provides an efficient way to compute the probability of correctness and expect cost in O(k) time, where k is the length of the cascade. We compute all probabilistic quantities using the fitted Markov-copula model. To compute the integrals\n\nIi(\\phi_{i-1}, \\phi_i) = \\int_{\\{\\Phi_i > \\phi_i\\}} \\Phi_i(\\omega) dP(\\omega |\\Phi_{i-1} \\le \\phi_{i-1})\n\n(29)\n\nof conditional correctness, we use numerical integration by treating (29) as a Riemann-Stieltjes integral So, & dF($) in the distribution function F(6) = P(\u0424i \u2264 |\u0424\u0456\u22121 \u2264 i-1). See Rudin (1976). Before solving the minimization problem (11), we pre-compute look-up tables for Ii(\u0444\u0456\u22121,i) which can be re-used when solving (11) for different values of A and different subcascades."}, {"title": "Appendix C: Prompt Templates", "content": "Below, we provide the exact text of the prompts used in our experiments. Placeholders (for example, {question}) are replaced at runtime with the relevant content."}, {"title": "MMLU", "content": "User Prompt (Zero-Shot)\nAnswer the multiple-choice question below by outputting A, B, C, or D.\nDon't say anything else.\nQuestion: {question}\nChoices:\n{choices}\nAnswer:\nSystem Prompt\nCorrectly answer the given multiple-choice question by outputting \"A\", \"B\",\n\"C\", or \"D\". Output only \"A\", \"B\", \"C\", or \"D\", nothing else."}, {"title": "MedMCQA", "content": "User Prompt (Zero-Shot)\nBelow is a multiple-choice question from a medical school entrance exam.\nOutput \"A\", \"B\", \"C\", or \"D\" to indicate the correct answer.\nDon't say anything else.\nQuestion: {question}\nChoices:\n{choices}"}, {"title": "TriviaQA", "content": "User Prompt (Zero-Shot)\nCorrectly answer the question below. Give the answer directly,\nwithout writing a complete sentence.\nQuestion: {question}\nAnswer:\nSystem Prompt\nCorrectly answer the given question. Answer the question directly\nwithout writing a complete sentence. Output just the answer, nothing else.\nEvaluation User Prompt\nConsider a proposed answer to the following trivia question: {question}.\nThe proposed answer is {model_answer}. Decide if this answer correctly\nanswers the question, from the standpoint of factuality. Output \"Y\" if\nthe answer is factually correct, and \"N\" otherwise. Do not say anything else.\nEvaluation System Prompt\nYou are a helpful assistant who judges answers to trivia questions. Given\na trivia question and a proposed answer, output \"Y\" if the proposed\nanswer correctly answers the question. Otherwise, if the answer is not\nfactually correct, output \"N\". Only output \"Y\" or \"N\". Do not say anything else."}, {"title": "XSum", "content": "User Prompt (Zero-Shot)\nSummarize the given source document. Write a concise summary that is coherent,\nconsistent, fluent, and relevant, as judged by the following criteria:\nCoherence \u2013 collective quality of all sentences\nConsistency \u2013 factual alignment between the summary and the source\nFluency \u2013 quality of individual sentences\nRelevance \u2013 selection of important content from the source\nSource document: {source_document}\nSummary:"}, {"title": "GSM8K", "content": "User Prompt (Zero-Shot)\nReason step-by-step through the following math problem. End your answer\nby outputting the final numerical answer prefixed by '####'. For example,\nif your final answer is x, end your answer on '#### x'.\nProblem: {question}\nStep-By-Step Reasoning:\nSystem Prompt\nConsider the given math problem, think step-by-step, then report the final\nnumerical answer x as follows #### x'. Do not say anything after that."}, {"title": "TruthfulQA", "content": "User Prompt (Zero-Shot)\nAnswer the question below truthfully and concisely. No need to write a\ncomplete sentence, explain your answer, or say anything else afterwards.\nQuestion: {question}\nAnswer:\nSystem Prompt\nYour task is to answer the given question truthfully. It is okay to\nacknowledge you do not know the answer. Give the correct answer if you can.\nEvaluation User Prompt\nConsider a proposed answer to the following question: {question}. Decide\nif the following proposed answer truthfully answers the question. For\nreference, consult the lists of known correct and incorrect answers\nprovided below. Respond with exactly 'Y' if the answer is truthful,\nor 'N' if it is not truthful. Only output Y or N.\nProposed answer: {model_answer}\nKnown correct answers:\n{make_bullets(correct_answers)}\nKnown incorrect answers:\n{make_bullets(incorrect_answers)}\nIs the given answer truthful? Respond with exactly Y or N:"}, {"title": "Appendix D: Price Differentials between Small and Large Models", "content": "Table 8 lists the differentials between smaller and larger language models across various providers."}, {"title": "Appendix E: Verifying Confidence Thresholding on the Test Sets", "content": "We further verify calibration of LLM confidences by showing that confidence thresholding works: for most benchmarks and models, when only accepting queries for which the calibrated confidence exceeds q, the test error decreases to below < 1-q.\nFigure 7 plots the conditional accuracy with confidence thresholding on the test sets (n \u2248 1000). In each case, the logistic regression calibrator was fitted on the training set (n \u2248 300). Each plot traces the empirical probability of correctness on the test set, Ptest(correct|\u0424 > $), for different values of the calibrated confidence threshold 6. The figure shows that, for the most part, the models' conditional accuracies increase as expected. This is indicated by the fact that the conditional accuracy curves mostly remain above the diagonal dashed lines, reflecting the theoretical expectation that Ptest(correct|\u0424 > \u00a2) \u2265 \u03c6."}]}