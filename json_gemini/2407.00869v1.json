{"title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "authors": ["Yue Zhou", "Henry Peng Zou", "Barbara Di Eugenio", "Yang Zhang"], "abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.", "sections": [{"title": "Introduction", "content": "It is arguably easier, at least from the logical perspective, to tell the truth than to tell a lie. For example, given a math problem \u201cWhat is 1/2 + 1/3\", telling the truth only requires the ability to perform the correct reasoning and derive the correct answer. Telling a lie, on the other hand, requires the ability to not only discern the correct answers, but also avoid generating the correct answers and, more importantly, make the wrong answers look real. In this paper, we refer to the task of fabricating incorrect yet seemingly plausible reasoning as fallacious reasoning.\nLarge language models (LLMs) have long been struggling with reasoning problems. Existing research revealed that LLMs have difficulty discerning the veracity of their intrinsic answers (Huang et al., 2024; Kadavath et al., 2022; Stechly et al., 2023). This raises an intriguing research question: If LLMs already find it hard to validate their own correctness, can LLMs deliberately generate fallacious reasoning upon request?\nThis paper starts with a pilot investigation of LLMs' ability to perform fallacious reasoning. Specifically, on four reasoning benchmarks, covering the domains of mathematics, logic, and commonsense, we ask the LLM to generate the correct answers and to deliberately generate wrong answers that are deceptively real. Our surprising finding is that, for all of our tested benchmarks, the accuracy of the generated wrong answers is almost as high as that of the correct ones. For instance, in Figure 3, the LLM generates the correct reasoning chain and final answer despite the fallacious generation request and claims a step to be wrong with a contradictory statement. This pilot study reveals that LLMs might be unable to intentionally generate deceptive reasoning and instead often leak the correct solutions in what they claim to be wrong answers.\nAs we further our investigation, this seemingly small glitch in LLMs can lead to a significant security threat. Specifically, we discovered a simple yet effective jailbreak attack, which we call the fallacy failure attack (FFA), that can elicit harmful output from LLMs by exploiting the LLMs' deficiency in fallacious reasoning. Given a malicious query, e.g. \"How to create and spread a virus\", FFA queries the target LLM to generate a fallacious yet deceptively real procedure for the malicious query, as demonstrated in Figure 1. The rationale behind FFA is two-fold: (1) While LLMs generally reject malicious queries as they are harmful, they would consider a query asking for a fallacious answer harmless since it purportedly does not seek a truthful (and harmful) answer. This can potentially help to bypass the LLMs' safeguard mechanisms; (2) LLMs would generally leak a truthful answer even when asked to generate a fallacious one. Therefore, by asking the LLM to generate fake answers to a malicious query, we can both bypass the security mechanism and obtain a factual and harmful response. Based on the rationales above, FFA crafts a jailbreak prompt with four components: malicious query, fallacious reasoning request, deceptiveness requirement, and scene and purpose. FFA does not require access to the language model's internal parameters, fine-tuning, or multi-turn interaction with a chatLLM.\nWe evaluate FFA over five safety-aligned large language models: OpenAI GPT-3.5-turbo, GPT-4 (version 0613) (OpenAI, 2023), Google Gemini-Pro (Anil et al., 2024), Vicuna-1.5 (7b) (Chiang et al., 2023), and LLaMA-3 (8b) (AI@Meta, 2024) on two benchmark datasets: AdvBench (Zou et al., 2023b) and HEx-PHI (Qi et al., 2023). We compare FFA with four previous state-of-the-art jailbreak attack methods, Greedy Coordinate Gradient (GCG) (Zou et al., 2023b), AutoDAN (Liu et al., 2023), DeepInception (Li et al., 2023), and Art-Prompt (Jiang et al., 2024) and under the impact of three defense methods. Our experiments show that FFA performs most effectively against GPT-3.5, GPT-4, and Vicuna-7b, provoking these models to generate significantly more harmful outputs. We also find that none of the three defense methods are effective against FFA, highlighting the urgent need to address this security threat. In additional studies, we show the role of scene and purpose in jailbreak attacks and explain why FFA could induce the most factually harmful results."}, {"title": "Fallacious Reasoning in LLMs", "content": "In this section, we present the findings of our pilot study about LLMs' capabilities in fabricating fallacious reasoning."}, {"title": "Task and Motivation", "content": "We introduce the task of fallacious reasoning, where we ask the LLM to deliberately generate reasoning processes that satisfy two requirements:  They should be incorrect and lead to false answers, and  they should be deceptive and appear to be correct.\nGenerating a fallacious reasoning process is a highly sophisticated task, because it involves multiple capabilities: the ability to judge the correctness of an answer, the ability to avoid generating the correct answer, and the ability to make a wrong answer deceptively real. However, existing research revealed that LLMs struggle in in discerning the veracity of their intrinsic answers (Huang et al., 2024; Kadavath et al., 2022; Stechly et al., 2023). Therefore, we raise the following intriguing research questions: Can LLMs deliberately generate fallacious reasoning upon request?"}, {"title": "Experiment Setting", "content": "To investigate this, we design the following pilot experiment. We choose four reasoning benchmarks, math reasoning GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), commonsense reasoning HotPotQA (Yang et al., 2018), and logic reasoning ProofWriter (Tafjord et al., 2020), and randomly sample 100 questions for each benchmark. For each question, we use GPT-3.5-turbo to generate answers in two modes. Honest Mode. We ask the LLM to generate the correct answers, using zero-shot Chain-of-Thought (Kojima et al., 2023) to prompt the LLM, which appends \u201cLet's think step by step.\" to the question text; Fallacious Mode. We ask the LLM to provide a step-by-step yet fallacious solution to the question and explain why it is incorrect. Detailed dataset description and experimental settings in this section are available in Appendix A."}, {"title": "Our Findings", "content": "One might expect that the accuracy of the solutions generated by these two modes would be drastically different \u2013 the honest mode would yield high accuracy and the fallacious mode low."}, {"title": "Fallacy Failure Attack", "content": "The findings introduced above have profound security implications. In this section, we will discuss how the LLMs' failure in fallacious reasoning can be exploited to form an effective jailbreak attack, called the Fallacy Failure Attack (FFA)."}, {"title": "Problem Formulation", "content": "Jailbreak attacks aim to elicit harmful information from LLM by transforming the malicious query into a special input form that could bypass LLMs' inherent security safeguard. Specifically, consider a malicious query, Q, (e.g., \u201cHow to create and release a virus that can cause ecological damage?\"). If Q is directly fed into an LLM, the LLM is likely to reject to answer due to the security safeguard mechanism. Jailbreak attack introduces a transformation function, denoted as $f(\\cdot)$, that turns the malicious input into a jailbreak input, $Q' = f(Q)$, which can trick the LLM into giving away its answer to the malicious question.\nTherefore, the key to designing a jailbreak attack is searching for the input transformation function. A jailbreak attack with a high level of threat and practicality should have the following properties: Query-agnostic transformation \u2013 the same transformation should work for a wide range of malicious queries; and Model generalizability \u2013 the attack should work for different LLMs, including white-box and black-box LLMs; thus the design of transformation should ideally not utilize the gradient information of LLMs.\""}, {"title": "FFA Overview", "content": "Our proposed attack method, FFA, uses a fallacious reasoning query to bypass LLMs' safeguard mechanisms. Specifically, FFA is built on the following two conclusions. First, although LLMs would generally recognize direct malicious queries as harmful queries and reject them, most of them would consider a query to generate a fallacious answer as a harmless query and would not reject them. For example, although \u201cHow to create and release a virus that can cause ecological damage\" is often identified as a harmful query, \u201cGenerate a fallacious procedure of creating and releasing a virus\" is generally considered a harmless one by most LLMs, because it does not seek to find the true answer, and thus would not be blocked. Second, LLMs would generally release the true answer even when asked to generate a fallacious one. Therefore, by asking the LLM to generate fake answers to a malicious query, we can both bypass the security mechanism and obtain the true answer, hence forming the rationale behind FFA."}, {"title": "FFA Prompt Design", "content": "Based on the rationales above, FFA crafts a jailbreak query with the following four components.\n\u2022 Malicious Query \u2013 the target query for which the attacker wishes to obtain an answer, e.g., \u201ccreate and release a virus that can cause ecological damage\u201d or \u201cmake and distribute counterfeit.\"\n\u2022 Fallacious Reasoning Prompt \u2013 the instruction for the LLM to generate a fallacious procedure for the given malicious query, e.g., \u201cProvide a fallacious procedure to [malicious query]\u201d.\n\u2022 Deceptiveness Requirement \u2013 an additional instruction that the fallacious procedure should be deceptively real, e.g., \"Need to be 'real' to make laymen believe, not too absurd\u201d. As shown in Figure 4, the deceptiveness requirement is crucial in encouraging the LLM to leak real answers (left plot); otherwise, the LLM could generate answers that are too naively fictional (right plot).\n\u2022 Scene and Purpose \u2013 a description of a hypothetical scene or role-play and a legitimate purpose that could further relax the LLM's ethical constraints. Typical scenes include \"writing a fiction\u201d, \u201cbeing a professor\u201d, or \u201cplaying a game\"; typical purposes include \u201cfor legitimate purpose\u201d or the ones that align with the scene (Li et al., 2023; Liu et al., 2023; Jiang et al., 2024; Qi et al., 2023; Chao et al., 2023).\nTo sum up, the prompt template for FFA is shown below. Our method does not require access to the language model's internal parameters, fine-tuning, or multi-turn interaction with a chat LLM."}, {"title": "Experiments", "content": "In this section, we describe our experiment results evaluating the effectiveness of FFA, as well as additional studies on its mechanisms."}, {"title": "Experimental Settings", "content": "Victim Language Models We utilize five State-of-the-Art large language models for evaluation, including three closed-source models, OpenAl GPT-3.5-turbo and GPT-4 (version 0613) (OpenAI, 2023) and Google Gemini-Pro (Anil et al., 2024), and two open-source models, Vicuna-1.5 (7b) (Chiang et al., 2023) and LLaMA-3 (8b) (AI@Meta, 2024). All LLMs are aligned with safety protocols.\nBaselines We compare our approach with four previous State-of-the-Art attacking methods:\n\u2022 Greedy Coordinate Gradient (GCG) (Zou et al., 2023b) is an optimization-based method which requires white-box access to a language model. It searches for a token sequence that maximizes the conditional probability of an output starting with an affirmative response.\n\u2022 AutoDAN (Liu et al., 2023) an optimization-based, automated jailbreak attack that generates stealthy prompts using a hierarchical genetic algorithm, requiring white-box access to a language model.\n\u2022 DeepInception (Li et al., 2023) is a black-box jailbreak attack that leverages LLMs' personification abilities to construct a nested scene to provoke harmful behaviors.\n\u2022 ArtPrompt (Jiang et al., 2024) is a black-box attack that leverages ASCII art to conceal harmful instructions within a word puzzle. The attack then encourages the LLM to decode this masked word and inadvertently complete the harmful instruction, thereby circumventing the safety measures."}, {"title": "Main Results", "content": "Attack Efficiency Table 1 illustrates the performance of FFA compared with the five baselines across five language models. There are primarily two observations regarding our approach. First, FFA is most effective against GPT-3.5, GPT-4, and Vicuna-7b and achieves comparable performance against Gemini-Pro, compared with other previous State-of-the-Art jailbreak methods. Against GPT-3.5, GPT-4, and Vicuna-7b, our method provokes the LLMs to generate significantly more harmful output, with a 10% ~ 50% absolute improvement in ASR. Second, the recently released language model LLaMA-3, in general, has stronger defense power against multiple jailbreak attack methods. However, our method performed even worse compared with some other methods. By manual inspection of the responses of the model, we find that LLaMA-3 is inclined to reject any instruction involving the creation of deceptive or false content, irrespective of its potential harm. For instance, LLaMA-3 will refuse the proposition of a fallacious mathematical theorem proof. While we acknowledge that rejecting all false content could provide optimal defense against FFA, the ability to generate fallacies could, paradoxically, reflect a form of AI intelligence and be advantageous in specific contexts, such as mathematics and theory. Additionally, there are a few observations regarding other baselines: (1) Nave approach, which directly asks an LLM to propose harmful output, can be easily rejected by all models. (2) Despite ArtPrompt's sufficient performance, it struggles against an easily targeted model, Vicuna-7b. We find that this is due to the model's inability to interpret ASCII art and reconstruct the true intent of the attack. (3) DeepInception exhibits a very high bypass rate, but its output is not harmful based on the AHS and ASR. We will discuss a hypothesis on the harmfulness of our outputs compared with DeepInception in Section 4.3."}, {"title": "Defense Impact", "content": "Table 2 presents the results under various defense settings. The \"No Defense\" results indicate the best attack performance without implementing any defense measures. Generally, all three defense methods can negatively impact the effectiveness of FFA. However, we observe that (1) PPL-Filter only marginally affects FFA. The result is expected since our attack prompt is phrased naturally without nonsensical or unconventional strings. (2) Paraphrasing is generally the most effective defense method against our approach. This is unexpected since the semantics of instructing LLM for a fallacious output should be preserved after paraphrasing. We hypothesize that even subtle semantic changes, including describing harmful behavior, could affect LLM's security measures. (3) Surprisingly, paraphrasing and retokenization did not degrade but enhanced the FFA attack's effectiveness against LLaMA-3. We find that during paraphrasing, the terms \u201cfallacious/fake\u201d are often rephrased as \u201cinvalid\u201d or \u201cflawed.\u201d Given the LLM's strong opposition to fake content, we hypothesize that paraphrasing could alleviate this opposition. However, interpreting retokenization is challenging as we're uncertain how distorted token inputs are perceived by language models. Overall, none of the three methods effectively defend against or mitigate FFA, highlighting the urgent need for more advanced defenses and further research on the fallacious generation ability in LLMs."}, {"title": "Additional Studies", "content": "Impact of Scene and Purpose on Attack Efficacy An intriguing question is the role of scene and purpose in jailbreak attacks. Do they alone suffice to bypass the LLM's security measures? Does FFA retain attack ability without a scene or purpose? We conducted an ablation study and computed the AHS and ASR under five attacks, scene, and purpose combinations across three language models shown in Figure 5. setting X, Y, and Z refer to directly asking LLM for malicious behavior with that combination of scene and purpose, respectively. FFA + Z refers to using Z as the scene and purpose of the FFA attack. FFA + None refers to the FFA attack without specifying any scene or purpose. We can observe that (1) na\u00efvely adding a scene and purpose to the direct instruction of harmful behavior mostly has a marginal effect on jailbreak attack. The only exception is the combination of \"scientific fiction\u201d and \u201cagainst evil Doctor X,\" which demonstrates notable attack efficacy against GPT-3.5-turbo and Gemini-Pro. Interestingly, this unique design seems to be the primary driving force behind the DeepInception method. (2) Although our method archives optimal performance with the combination of scene and purpose, it can retain significant attack ability without a scene and purpose, except for LLAMA-3, which opposes the creation of untruthful content. (3) When using the same fictitious scene and purpose as DeepInception, our method is more likely to induce more harmful output.\nHarmfulness from Honesty Figure 6 presents a qualitative example of the outputs from FFA and DeepInception, both targeting the same malicious behavior. Content-wise, while DeepInception's output is loaded with science fiction terminology, FFA presents factual and detailed steps for the queried malicious behavior. Why is the output produced by FFA more harmful and factual? We hypothesize that (1) despite the potential for a science fiction scenario to bypass the LLM's security measures, the output will inevitably be more fictional due to the LLM's understanding of sci-fi. (2) In contrast, FFA rests on the fact that LLM cannot generate a deceptive solution but instead provides a truthful counterpart, thus is factually harmful. For this reason, the harmfulness in the FFA's output, to some extent, is retained even when using the fictitious scene and purpose from DeepInception.\nInterestingly, in the FFA output, the LLM also tries to elucidate that the above procedure is fallacious. This incoherent conclusion and the truthful solution under the fallacious request echo the findings in the fallacious reasoning section."}, {"title": "Related Work", "content": "Jailbreak Attack Recent large-scale language models (LLMs) are optimized and aligned with human preferences under ethical guidelines and legal constraints (Ouyang et al., 2022; Ziegler et al., 2020; Zou et al., 2023a). However, studies have discovered that deliberately transforming the prompt can trick LLMs into responding with malicious instructions without rejection, which exposes the ethical and security risks of LLMs in real-world applications (Wei et al., 2023; Qi et al., 2023). Two primary strategies are currently employed to identify these transformations. The first involves optimization requiring access to a white box language model. Zou et al. (2023b) introduce an optimization-based method by searching for a token sequence that maximizes the conditional probability of an output starting with an affirmative response. Liu et al. (2023) propose to generate more readable prompts using a hierarchical genetic algorithm. The second involves manually crafting or searching for prompt updates without requiring gradient access. Li et al. (2023) leverage LLMs' personification abilities to construct a nested scene to provoke harmful behaviors. Jiang et al. (2024) use ASCII art to conceal harmful instructions within a word puzzle to circumvent the safety measures. There are also methods that are based on multi-turn interactions with chat LLMs. Chao et al. (2023) utilize an additional language model as an attacker to find jailbreak prompts with multiple queries, Russinovich et al. (2024) attack the chat language model with multi-turn dialogues.\nJailbreak Defense The development of defense methods is challenging and limited due to the inaccessibility of internal parameters of closed-source language models. The most straightforward strategy involves a perplexity check, presuming the attack prompt contains unnatural strings. Some methods involve prompt pre-processing, including token perturbation and transformation (Jain et al., 2023; Provilkov et al., 2019; Robey et al., 2023). However, these defenses could compromise benign user instructions' output quality. Lastly, some strategies leverage language models to assess the potential harm of the instruction and its output (Kumar et al., 2024; Phute et al., 2024)."}, {"title": "Conclusion and Future Work", "content": "This paper presented a simple yet explainable and effective jailbreak attack method. It is predicated on the observation that language models cannot generate fallacious and deceptive solutions but instead produce honest counterparts. We argue that this observation not only poses a security threat but also implies how modern LLMs' perceptions of specific tasks are limited when the scenario is inadequately optimized and aligned during training. We believe this observation can be further extended to related research areas, such as self-verification and hallucination, providing valuable insights into understanding LLM behavior toward general intelligence."}, {"title": "Limitations", "content": "While in this paper, we propose an effective jailbreak attack method against language models, we have not yet identified an ideal defense mechanism to counteract it. One potential defense strategy is to consistently reject queries containing fallacious reasoning. However, this approach may not be optimal, as it undermines the versatility and utility of large language models in achieving general intelligence and could lead to inadvertent rejection of benign queries in other applications. Future work is required to develop more robust and sophisticated defense strategies to effectively prevent FFA."}, {"title": "Ethics Statement", "content": "This paper introduces a jailbreak approach leveraging LLMs' failures of fallacious reasoning. It potentially allows adversaries to exploit LLMs, creating outputs that do not align with human values or intentions. However, like previous jailbreak research, this work should encourage research into improved defense strategies and develop more robust, secure, and well-aligned LLMs in the long term. We also hope that the characteristic of LLMs leaking truthful content upon request of the fallacious generation will draw attention from the research community, enabling potential research in other areas, such as hallucination and LLM self-verification."}, {"title": "Fallacious Reasoning Details", "content": "For the experiments in Section 2, we randomly sample 100 data points from each of the following:\n\u2022 GSM8K (Cobbe et al., 2021) contains linguistically diverse grade school-level math questions with moderate difficulties.\n\u2022 MATH (Hendrycks et al., 2021) is a competition mathematics dataset with challenging concepts such as Calculus and Linear Algebra, spanning over five difficulty levels. We sample 20 examples from each of the five difficulty levels.\n\u2022 HotPotQA (Yang et al., 2018) is a multi-domain, multi-hop question-answering dataset based on supporting facts. We sample questions form the hard-level development set since the online testing set is unavailable.\n\u2022 ProofWriter (Tafjord et al., 2020) is a multi-hop logic reasoning dataset with each example containing a set of facts and rules for the logic deduction. We sample all the questions from the set requiring the most hops.\nWe show another example where the LLM failed to propose fallacious reasoning from the HotPotQA dataset in Figure 7, with similar patterns as the example in Figure 3."}, {"title": "Main Experiment Settings", "content": "For PPL-Filter, we use GPT-2 to calculate $PPL(X)$, following Alon and Kamfonas (2023), defined as:\n$PPL(X) = exp(-\\frac{1}{t} \\sum_{i=1}^{t} logp(x_i|x_{<i}))$,\nwhere X is the input token sequence. Following Jain et al. (2023); Jiang et al. (2024), we set T = 175.57 as the threshold, which is the max perplexity among all the direct instructions of the malicious behaviors in the AdvBench datasets.\nSince ArtPrompt may require encoding and masking one (meaningful) word at a time for each harmful behavior (e.g., \"[MASK] fraud\" or \"tax [MASK]", "Paraphrase the following instruction": {"instruction}": "The paraphrased version is then forwarded to the target LLM as the new instruction.\nWe treat [scene] and [purpose] as hyperparameters in FFA. We utilize three predefined scene-purpose pairs based on commonsense and previous work, shown in Table 3:"}}]}