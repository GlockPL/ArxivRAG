{"title": "REFINING SALIENCE-AWARE SPARSE FINE-TUNING STRATEGIES FOR LANGUAGE MODELS", "authors": ["Xinxin Liu", "Aaron Thomas", "Cheng Zhang", "Jianyi Cheng", "Yiren Zhao", "Xitong Gao"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank adaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT (SPEFT), which introduces trainable sparse adaptations to the weight matrices in the model, offering greater flexibility in selecting fine-tuned parameters compared to low-rank methods. We conduct the first systematic evaluation of salience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify simple gradient-based metrics is reliable, and results are on par with the best alternatives, offering both computational efficiency and robust performance. Additionally, we compare static and dynamic masking strategies, finding that static masking, which predetermines non-zero entries before training, delivers efficiency without sacrificing performance, while dynamic masking offers no substantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT consistently outperforms other fine-tuning methods for LLMs, providing a simple yet effective baseline for SPEFT. Our work challenges the notion that complexity is necessary for effective PEFT, and is open-source and available to the community\u00b9.", "sections": [{"title": "1 Introduction", "content": "Pretrained large language models (LLMs) have demonstrated strong performance across various natural language processing (NLP) tasks [4]. A typical approach for adapting these LLMs to specific downstream tasks involves fine-tuning their trainable parameters. However, this process can be prohibitively expensive on consumer-grade hardwares, if we consider training all free parameters, especially on LLMs exceeding a billion parameters. For example, models with over 100 billion parameters, such as BLOOM, required training with 384 GPUs across 48 distributed computing nodes [24]. Instead of training all parameters, an alternative fine-tuning paradigm that enables model training on new tasks with minimal computational resources is Parameter-Efficient Fine-Tuning (PEFT). This method aims to learn only a small set of parameters in order to adapt the model to the new task, substantially lowers the computational resource requirements [2, 15].\nExisting effort on PEFT methods mainly focuses on two categorizes, low-rank-based and sparsity-based adaptation approaches. LoRA [15], a popular low-rank adaptation method reparameterizes the model weight of each layer ($\\theta \\in R^{d_1 \\times d_2}$) as $\\theta \\gets \\theta_0 + BA$, where $\\theta_0$ denotes the pretrained weight matrix which remains fixed during fine-tuning,"}, {"title": "2 Related Work", "content": "2.1 PEFT Methods\nWith the advent of large language models, fine-tuning these models on downstream tasks can be prohibitively expensive due to the sheer number of trainable parameters. A suite of parameter-efficient fine-tuning (PEFT) methods have been proposed to address this issue.\nLow-rank adaptation [15] is a popular method in PEFT which reparameterizes the weight matrix of each layer ($\\theta \\in R^{d_1 \\times d_2}$) as $\\theta = \\theta_0 + BA$. Here, $\\theta_0 \\in R^{d_1 \\times d_2}$ is the pretrained weight matrix, and $B \\in R^{d_1 \\times r}$ and $A \\in R^{r \\times d_2}$ are lower-rank matrices with $r < \\min(d_1, d_2)$. By making only A and B trainable, this method significantly reduces the number of trainable parameters, thereby lowering computational resource requirements. LoRA has demonstrated effectiveness in reducing trainable parameters for fine-tuning large language models, while maintaining strong fine-tuned performance across various downstream tasks compared to full fine-tuning.\nSparsity-based adaptation Since the advent of low-rank adaptation, sparsity-based adaptation has emerged as an alternative approach to PEFT. It constructs sparse trainable matrix $\\theta_{sp}$ reparameterization for each layer weight $\\theta = \\theta_0 + \\theta_{sp}$, where $|\\theta_{sp}|_0 < s < d_1 \\times d_2$, and $s$ represents the number of non-zero entries. The gradient updates only happen to the non-zero entries of the sparse matrices during fine-tuning. Since the sparse matrix $\\theta_{sp}$ is typically constructed to be extremely sparse, this approach can also achieve notable parameter efficiency, and the sparsity masking strategy plays a crucial role in determining impactful trainable parameters for fine-tuning.\nThis approach has been explored in various forms in the literature. Earlier works such as DiffPruning [12] learns a sparsity mask with straight-through gradient estimator [3, 16] to select important parameters for downstream tasks. FishMASK [30] applies a static sparsity mask from training outset, guided by Fisher information to measure sparsity. Beyond static masks, Fish-DIP [8] further allows the Fisher information-based mask to be updated dynamically during training. Inspired by the lottery ticket hypothesis [11], LF-SFT [2] finds that sparse masks obtained by selecting the parameters with the largest changes after fine-tuning on a task can be transferred to other tasks. However, this approach requires full fine-tuning on an initial task, which may not be feasible for resource-constrained settings. This paper explores the design principles for constructing the sparsity mask with low-cost salience metrics and the impact of dynamic versus static masks on the fine-tuning process.\nFinally, sparsity-based adapters also allows highly granular control over trainable parameters, and can enable the use of existing knowledge transfer techniques, such as mixtures of sparse experts [36] and multi-task learning with sparse masks [29] in LLMs.\n2.2 Salience Proxies for Sparsity Masking\nThe extensive research on low-cost salience metrics for fine-grained network pruning has provided a rich set of pruning-at-initialization metrics to determine the importance of neural network parameters. These metrics can be broadly classified into first- and second-order categories. First-order metrics include weight magnitude [13], connection sensitivity (SNIP) [20], foresight connection sensitivity (FORCE) [9], Taylor-FO [26], SynFlow [31], and finally, the gradient of the loss with respect to the weight. Second-order metrics comprise GRaSP [35] and Fisher information-based metrics [22]. Coincidentally, both FishMASK [30] and Fish-DIP [8] propose to use Fisher information to construct the sparsity mask: while FishMASK uses a static mask, Fish-DIP further allows the mask to be updated periodically during fine-tuning. These metrics are designed to identify important parameters or connections in a neural network. In this paper, we explore the impact of these salience metrics on fine-tuning by using them to construct sparse masks for PEFT."}, {"title": "3 Method", "content": "3.1 Problem Formulation\nGiven a pretrained model $f_\\theta$ with initial parameters $\\theta_0$, a dataset $D_{train}$, and a downstream task loss function $L$, the goal of sparse parameter-efficient fine-tuning (SPEFT) is to find a set of sparse trainable parameters $\\theta_{sp}^*$, that minimizes the loss function on the training dataset $D_{train}$:\n$\\theta_{sp}^* = \\arg \\min_{\\theta_{sp}} E_{(x,y) \\sim D_{train}} [L(f_{\\theta_0 + \\theta_{sp}}(x); y)].$ (1)\nTo ensure the sparsity of $\\theta_{sp}$, we constrain it with $1[\\theta_{sp} \\neq 0] = \\tau$, where $1[\\cdot]$ is the indicator function, $\\tau \\in \\{0, 1\\}^{d_1 \\times d_2}$ is the sparsity mask with $|\\tau|_0 < p < d_1 \\times d_2$, where $p$ is the number of non-zero entries. This opens up the flexibility"}, {"title": "3.2 Salience Metrics", "content": "In this section, we describe the 8 salience metrics which can be used to determine the importance of weights $\\theta$. Assume that $x$ is the sampled input, $l \\equiv L(f_\\theta(x); y)$ is the loss function, $\\odot$ denotes element-wise multiplication, and $|\\cdot|$ denotes the element-wise absolute value. For simplicity, we also assume all data-aware metrics to be expectations over the training dataset $(x, y) \\sim D_{train}$, which can be approximated by sampling from it. We have the following 6 1st-order salience metrics:\nMagnitude: $|\\theta|$, where simply the magnitude (i.e., absolute value) of the weight is used.\nGradient: $\\frac{\\partial l}{\\partial \\theta}$, which is the gradient of the loss with respect to the weight $\\theta$.\nSNIP (single-shot network pruning): $|\\frac{\\partial l}{\\partial \\theta} \\odot \\theta|$, the connection sensitivity metric proposed in [20] to determine the importance of weights.\nFORCE (foresight connection sensitivity): $\\frac{\\partial l}{\\partial \\theta} \\frac{\\partial^2 l}{\\partial \\theta^2}$, introduced in [9].\nTaylor-FO (Taylor first-order expansion): $(\\frac{\\partial l}{\\partial \\theta} \\odot \\theta)^2$, derived from the 1st-order Taylor expansion of the loss [26].\nSynFlow (iterative synaptic flow pruning): $[\\prod_{l=1}^{L} (\\prod_{l=1}^L |\\theta^\\ell|)]^{1/L} \\odot \\theta$, where $\\theta^\\ell$ denotes the weights of the $\\ell^{th}$ layer, and $L$ denotes the number of layers. A data-free metric proposed in [31] to model synaptic flow.\nIn addition, the 2nd-order salience metrics are computed as follows, where $H \\triangleq \\frac{\\partial^2 L(f_\\theta(x);y)}{\\partial \\theta^2}$ denotes the Hessian matrix:\nGRaSP (gradient signal preservation): $-(\\frac{H}{\\parallel H \\parallel}) \\frac{\\partial l}{\\partial \\theta}$, which is a 2nd-order metric proposed in [35] that aims to preserve gradient signals rather than the loss value.\nFisher information: $(\\frac{\\partial l}{\\partial \\theta})^2$, which uses the Fisher information to determine the importance of weights [30, 8]."}, {"title": "3.3 Sparsity Masking", "content": "Global Sparsity Masking Given a salience metric $S(\\theta)$ of the weight $\\theta$ defined in Section 3.2, we can construct the sparse binary mask $\\tau$ by selecting the top $p \\in (0, 1]$ fraction of the salience metric values, i.e., $p$ denotes the density level, namely:\n$\\tau = 1[s \\ge top_p (s)], \\text{ where } s = S(\\theta)$. (2)\nHere 1 is the indicator function, and $top_p$ selects the top $p$ values.\nLocal Sparsity Masking Instead of ranking the salience metric values across all weight values, alternatively, we can construct layer-wise masks $\\tau^{(l)}$ for the individual weights $\\theta^{(l)}$ in each layer $l$, where each layer has a shared sparsity $p$, and the top $p$ values are selected from the salience metric values of the weights in that layer:\n$\\tau^{(l)} = 1[s^{(l)} \\ge top_p (s^{(l)})], \\text{ where } s^{(l)} = S(\\theta^{(l)})$. (3)\nHere, $\\theta$ is decomposed into layer-wise weights $[\\theta^{(1)}, ..., \\theta^{(L)}]$ and $\\tau^{(l)}$ and $\\theta^{(l)}$ respectively denotes the mask and weights of the $l^{th}$ layer."}, {"title": "3.4 Static vs. Dynamic Masks", "content": "Beyond generating a static mask using the above approach prior to fine-tuning, which remains fixed throughout the training process, we can also explore the use of dynamic masks, which are updated periodically during training. The dynamic mask can be refreshed at specific intervals by the following procedure: first, we apply the current trained weights to the model; we then re-rank the salience metric values with these weights, the top $p$ values are then selected to form a new mask using the updated salience metric values; subsequently, the fine-tuning process continues with the new mask. Notably, after updating the dynamic masks, we also need to reinitialize memory-based optimizers in order to avoid applying incorrect momentum values to the newly adapted sparse weights."}, {"title": "3.5 The SPEFT Algorithm", "content": "Algorithm 1 provides an overview of the proposed SPEFT algorithm to fine-tune models with sparse weight adaptations. The algorithm takes as input a pretrained model $f_\\theta$, an optimizer $Opt$, a training dataset $D_{train}$, a batch size $B$, a loss function $L$, a salience metric $S$, a sparsity level $p$, the number of fine-tuning steps $T$, the learning rate $\\alpha$, and the mask update interval $I$. The algorithm begins by initializing the sparse weights $\\theta_{sp}$ to zero (line 1), and then iterates for $T$ steps (line 2). In each iteration, the algorithm first checks if it is the initial iteration, which requires updating the mask, or if it is at the correct interval for iterative dynamic mask updates (line 3). If either of these conditions is true, the algorithm applies the current sparse weights to the model (line 4), evaluates the new salience values $s$ (line 5), and updates the salience mask $\\tau$ for the updated weights, on the sparsity level $p$ (line 6). After updating the mask, the training step follows by sampling a mini-batch $\\{x,y\\}$ from the training dataset (line 8), and learns the sparse weights $\\theta_{sp}$ (line 9) using the optimizer $Opt$ (e.g., stochastic gradient descent, Adam, etc.). Here, $\\tau \\odot \\frac{\\partial l}{\\partial \\theta_{sp}}$ denotes element-wise multiplication. In terms of actual implementation, only the non-zero entries in $\\frac{\\partial L}{\\partial \\theta_{sp}}$ dictated by the mask $\\tau$ are computed and updated. Finally, the algorithm returns the fine-tuned model $\\theta_0 + \\theta_{sp}$."}, {"title": "4 Experimental Results", "content": "Models We evaluated our approaches and baselines over a set of models, including fine-tuned OPT variants (-125m, -350m, and -1.3b) [39], BERT-base-uncased [10] and RoBERTa-base [23], for the GLUE [34] benchmark, and fine-tuned Gemma2-2b [33] and Qwen2-7b [37], to evaluate on the Massive Multitask Language Understanding (MMLU) benchmark [14] and GSM8K [7], a dataset of grade school math problems. In addition to sparse PEFT methods presented in this paper, we further include LoRA [15] and PiSSA [25] as low-rank adapter baselines for comparison.\nBenchmarks To show the generality of our approach, we chose GLUE, MMLU and GSM8K as benchmarks for evaluation. For the GLUE [34] benchmark, six representative tasks with large sizes are selected: single-sentence task SST-2, inference tasks QNLI, MNLI, similarity and paraphrase tasks MRPC, STS-B and QQP2. For the MMLU [14] benchmark, it contains questions covering 57 subjects across STEM, the humanities, the social sciences, and others. It is designed to test the model's ability to handle various types of language data and complex problems. We fine-tuned Gemma2-2b, Qwen2-7b on either the Alpaca [32] or OASST2 [17] conversational datasets, and then evaluated them on all tasks in MMLU. We fine-tuned Gemma2-2b on the MetaMathQA [38] dataset and evaluated on GSM8K (8-shots) to assess the models' multi-step mathematical reasoning ability. In the results, we reported the match accuracy for MNLI, Pearson correlation for STS-B, flexible extract and strict match scores for GSM8K, and accuracy values for other tasks.\nBaselines We chose LoRA [15] and PiSSA [25] as the competing low-rank baselines across models and benchmarks. By default in all comparisons, SPEFT methods use global sparsity ranking with static masks. For statistical significance,"}, {"title": "4.1 Main Results", "content": "Our experiments results on OPT-350m and BERT-base-uncased can be seen in Table 1. For additional results on RoBERTa-base, OPT-125m and OPT-1.3b, please refer to Tables 8 to 10 in Appendix B. Across all models, we observed that among all the approaches, gradient-based SPEFT has the best average accuracy, higher than LoRA and PiSSA. For instance, in OPT-125m and OPT-350m, gradient-based SPEFT achieves 86.92% and 88.45%, that are higher than the best competing SPEFT methods by 0.73% and 0.85% respectively. Particularly on OPT-350m, gradient-based SPEFT has the best performance on MNLI, MRPC, SST-2, and STS-B, On QNLI and QQP, LORA has the best performance while gradient-based SPEFT has a good performance close to it. This shows that although LoRA shows excellent performance on certain tasks, SPEFT methods, particularly with the gradient salience metric, could further push the limit, achieving better results in accuracy. On BERT-base-uncased, we found that while SPEFT with Fisher-Info salience metric outperforms gradient-based SPEFT on QNLI, QQP and SST-2, it has a large gap in performance in the remaining tasks, making gradient-based SPEFT a more reliable and desirable choice. Similar results are also observed for other OPT variants in Tables 9 and 10 and RoBERTa-base in Table 8 of Appendix B.\nNotably, for both causal and masked language models, sparsity-based PEFT can outperform low-rank adapters, and the gradient-based SPEFT shows the strongest performance compared to other methods, closely followed by LoRA and PiSSA, which is consistent across all models. In addition, the gradient-based SPEFT outperformed LoRA and PiSSA in several tasks, highlighting its effectiveness across different model sizes. The comprehensive results table for these models and tasks underlines the consistent performance edge of gradient-based SPEFT, making it a reliable choice for a wide range of NLP tasks."}, {"title": "4.2 Larger Scale Models", "content": "For larger models, we evaluated all methods on Gemma2-2b and Qwen2-7b, and show the results in Table 2. The results indicate that larger models can also benefit from SPEFT with the gradient-based saliency method, which outperforms other sparse training methods and LoRA.\nTo evaluate on the text generation task, We fine-tuned Gemma2-2B with our methods We also provide the results of the pretrained model (without fine-tuning) and LoRA as baselines. The results are shown in Table 3. It can be seen that the sparse adapters outperformed the LoRA baseline, with the gradient-based SPEFT method leading the pack with the best performance. Notably, the lead by sparse adapters widens as the task complexity increases, which demands token sequence generation with multi-step reasoning."}, {"title": "4.3 Exploration of masking strategies", "content": "Based on the comparisons with SPEFT in Section 4.1, which showed that gradient-based SPEFT is the best-performing method, we would use it for ablation studies of dynamic vs. static masks, and global vs. local sparsity. In this section, we delve into the comparisons between between global and local sparsity (Section 3.3) and also static and dynamic masking strategies (Section 3.4) using gradient-based SPEFT, the best-performing salience metric, across OPT-125m, OPT-350m, and BERT-base-uncased. Here, we periodically update the masks every $I = 1000$ steps with 1024 training examples to estimate the salience metrics. The results are shown in Table 4.\nDynamic vs. static masking The findings reveal that dynamic masking offers only a slight performance advantage in smaller models like BERT-base-uncased but does not significantly outperform static masking in larger models. For instance, on OPT-350m, we actually see static masking provides us a better averaged accuracy (88.46 and 88.71) compared to dynamic masking (86.14 and 81.76). Given that dynamic masking requires more computational resources, because of the periodic update on sparsity masks, the marginal performance gain does not justify the extra cost,"}, {"title": "4.4 Minimal Overhead for SPEFT", "content": "Computational overhead For all first-order salience metrics, we use a few gradient evaluations to compute the salience scores. Specifically, only 64 steps with a batch size of 16 per estimation are needed (1024 examples), which is negligible compared to the overall training cost. For example, this represents only 0.26% and 0.97% of the training time for one epoch on MNLI and QNLI, respectively. For static masks, this computation is performed once before training; for dynamic masking, it is repeated once per $I = 1000$ steps. Second-order metrics such as GRaSP and Fisher-Info require 2\u00d7 the number of gradient evaluations of first-order metrics to compute the second-order gradients. The magnitude metric requires no additional computation. Finally, we observed no statistically significant difference in training time between the sparse methods and the LoRA baseline.\nMemory overhead As we aligned the number of trainable parameters across LoRA and the SPEFT methods, the peak memory usage for both methods are mostly identical, except that the SPEFT methods require a small amount of additional memory overhead to store the indices in CSR format. In all experiments, the overhead is less than 0.5% of the peak memory usage."}, {"title": "5 Discussion", "content": "The Trend of Supporting Sparse Computation as Hardware Intrinsics Numerous hardware vendors have introduced specialized hardware features with instruction set extensions tailored for sparse matrix multiplication. Especially in recently announced hardware devices. Mainstream devices like NVIDIA's A100 [6], H100 [5], and H200, as well as offerings from other major vendors or emerging competitors such as AMD's MI300 [1] and Cerebras' WSE2 [27], are embracing this trend. As hardware support for sparse computation advances, the utility of sparsity-based PEFT, or generally sparse training, is poised to improve substantially. This development will enable both current and future strategies to attain performance levels closer to their full potential, as these calculations won't require emulation via dense computations, allowing for closer realization of theoretical speedups and savings on FLOPs.\nThe Role of Salience Measurements A fundamental element of this study involves reevaluating certain design choices in SPEFT, leading to the discovery that straightforward designs, such as first-order salience proxies, emerge as the most effective methods. Intriguingly, selecting the most salient weights in a neural network has being a long-standing challenge, one that dates back to early weight pruning research by LeCun et al. in 1989 [19]. It's notable that the optimal saliency metric seems to differ \u2013 or arguably should differ \u2013 among different task setups, such as post-training weight pruning [19], pruning at initialization [21, 9], and zero-cost NAS proxies [28]. The suggested practice then should be to systematically review a range of known and established proxies to set a solid baseline before designing a complex salience metric."}, {"title": "6 Conclusion", "content": "We explored the efficacy of various sparse parameter-efficient fine-tuning (SPEFT) methods in enhancing the per-formance of LLMs. Our experiments compared LoRA and PiSSA against SPEFT methods with a range salience metrics, and demonstrated that gradient-based SPEFT consistently achieved superior accuracy across different tasks and model architectures. This demonstrates that, although LoRA and PiSSA is effective in certain contexts, SPEFT methods that leverage gradient information can further optimize performance. We also investigated the impact of static versus dynamic sparsity masks, concluding that while dynamic masks do not significantly outperform static masks, and they introduce additional training overhead. Our findings suggest that static masks, combined with the gradient-based salience metric, provide a practical balance between computational efficiency and model accuracy. Overall, our research contributes to the ongoing efforts in making model fine-tuning more efficient and accessible, particularly in resource-constrained settings."}, {"title": "7 Limitations", "content": "During the experiments, we found that in a few training runs, SPEFT seems less sensitive to hyperparameter changes than LoRA, i.e., on a range of hyperparameter sets, SPEFT always improves model performance, but LoRA fails. Due to limited resources and time, we did not run additional experiments to explore this interesting observation. We leave this exploration for future work. Moreover, similar investigations on parameter efficient fine-tuning could be conducted with non-language-based models or other multimodal models, such as vision large language models (VLLMs), however, these explorations are beyond the current scope of this paper and thus is left as future work."}, {"title": "8 Computational Resources", "content": "We performed all experiments on a cluster of NVIDIA A100 40GB GPUs. The experiments took around 486 GPU-hours for a single model on all GLUE subsets and all salient metrics. Besides, it took around 40 GPU-hours for a single model on Alpaca or OASST2 training on all low-rank and sparse PEFT methods. It also took around 80 GPU-hours to train with all methods on MetaMath for GSM8k evaluation. We also spent around 500 GPU-hours aligning the baseline results with the literature and determining fine-tuning hyperparameters."}, {"title": "A Hyperparameters", "content": "The hyperparameters we used for all models are shown in Tables 5 to 7. Notably, for all models, the density p was set to make sure the number of trainable parameters across all methods was the same as the LoRA baseline."}, {"title": "B Additional Experimental Results", "content": "Tables 8 to 10 provide additional respective results on GLUE tasks for the OPT-125m and OPT-1.3b variants, and BERT-base-uncased."}]}