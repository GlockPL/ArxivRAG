{"title": "Conformal Prediction on Quantifying Uncertainty of Dynamic Systems", "authors": ["Aoming Liang", "Qi Liu", "Lei Xu", "Fahad Sohrab", "Weicheng Cui", "Changhui Song", "Moncef Gaubbouj"], "abstract": "Numerous studies have focused on learning and understanding the dynamics of physical systems from video data, such as spatial intelligence. Artificial intelligence requires quantitative assessments of the uncertainty of the model to ensure reliability. However, there is still a relative lack of systematic assessment of the uncertainties, particularly the uncertainties of the physical data. Our motivation is to introduce conformal prediction into the uncertainty assessment of dynamical systems, providing a method supported by theoretical guarantees. This paper uses the conformal prediction method to assess uncertainties with benchmark operator learning methods. We have also compared the Monte Carlo Dropout and Ensemble methods in the partial differential equations dataset, effectively evaluating uncertainty through straight roll-outs, making it ideal for time-series tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Video data [1]\u2013[3] can be represented by a set of time-dependent partial differential equations (PDEs), where the solutions correspond to the dynamic behavior. Similarly, image data can be interpreted as the solutions to time-independent partial or ordinary differential equations [4]. Exploring the uncertainties in PDEs using various methods is highly beneficial, particularly for tasks such as video understanding and generation [5].\nPDEs serve as a fundamental framework for physical phenomena, with significant applications in weather forecasting [6], nuclear fusion [7], and molecular dynamics simulations [8]. Extensive research [9]\u2013[11] has been undertaken on how to learn the PDEs by artificial intelligence, especially those incorporating temporal evolution data, which can be conceptualized as sequences of video data structured on uniform or non-uniform grids. Given the deterministic nature of physical laws, the evolution processes should be deterministic. The leading work primarily focuses on neural operator learning [12].\nHowever, the inherent uncertainties in artificial intelligence models [13] pose challenges in quantifying the uncertainties during the learning processes and exploring whether calibration can mitigate these uncertainties. Uncertainty quantification provides insights into the credibility or confidence level of predictive outcomes. The researcher can better evaluate the model's stability, accuracy, and reliability by accurately quantifying uncertainties.\nClassic uncertainty studies [14]\u2013[16] only focus on one-step predictions of models, neglecting the cumulative uncertainties during temporal evolution in the forward or inverse problems.\nRotational invariance [17], [18] is a fundamental concept in the realm of physical systems, underpinning the understanding of symmetry in nature. Its significance is rooted in the principle that physical laws should remain unchanged under rotational transformations of the coordinate system. This invariance not only highlights the universality of physical laws but also plays a crucial role in the development of theoretical frameworks across various fields of physics.\nThe main contributions of this work are as follows:\n\u2022 In this study, a complete quantitative assessment of uncertainty quantification methods is performed, including Monte Carlo dropout, conformal prediction, and ensemble.\n\u2022 The rotation symmetry in physical systems served as an opinion for evaluating uncertainty, utilizing calibration datasets to eliminate uncertainties is feasible and aligns with physical interpretation.\n\u2022 Through conformal prediction, our work enhances the interpretability of models for time series tasks and provides theoretical upper and lower bounds as guarantees."}, {"title": "II. RELATED WORK ON UNCERTAINTY EVALUATION", "content": "Bayesian methods: Reference [19] firstly proposed introduced the Bayesian criterion, which assigns a prior distribution to the parameters of neural networks. With the training data, it then computes posterior distributions of these parameters to quantify predictive uncertainty. Reference [20] proposed the variational Bayesian inference in the neural network. In practice, the prior is often set to Gaussian distributions to convert the classical Bayesian to an optimization problem, and computing the posterior requires multiple iterations, with time-consuming in high-dimensional data.\nMonte Carlo dropout: Based on classic Bayesian, reference [21] suggested a Monte Carlo Dropout (MC Dropout)"}, {"title": "III. METHODS", "content": "In this section, we provide a conceptual introduction to the foundational theory of conformal prediction, outline the PDES tasks, and explain how the conformal prediction can be applied to problems in this study.", "B. Description of PDEs task": "Consider a time-dependent PDE of the form:\n$\\frac{\\partial u}{\\partial t} = F(u, \\nabla u, \\nabla^2 u,...)$,\nwhere $u(t, x)$ is the state variable defined on a domain $\\Omega \\subseteq R^d$, and $F$ is a differential operator governing the dynamics of $u$. The goal is to predict the state $u_{n+1}$ at the next time step $t_{n+1}$, given the state $u_n$ at the current time step $t_n$.\nLet $(u_n, u_{n+1}) \\sim P$, where:\n\u2022 $u_n \\in U$ is the state of the system at time $t_n$,\n\u2022 $u_{n+1} \\in U$ is the state at time $t_{n+1}$,\n\u2022 $P$ represents the underlying distribution of states.\nThe prediction set $\\hat{C}_n(u_n)$ provides a range of possible states for $u_{n+1}$, satisfying the finite-sample coverage property:\n$P (u_{n+1} \\in \\hat{C}_n (u_n) | (u_i, u_{i+1})_{i=1}^n) \\ge 1 - \\alpha$,\nwhere $\\alpha$ is a predefined error tolerance level (e.g., $\\alpha = 0.05$ for 95% confidence).", "C. CP in the PDEs task": "1. Data Representation: Collect training data $(u_i, u_{i+1}), i = 1,..., n$, which consists of sequential states from a PDE simulation or experimental measurements.\n2. Nonconformity Measure: Define a score function $S(u_n, u_{n+1})$ that quantifies how unusual the next time state $u_{n+1}$ is relative to the previous state. For example:\n$S(u_n, u_{n+1}) = ||u_{n+1} - \\hat{u}_{n+1}||_2,$\nwhere $\\hat{u}_{n+1} = M(u_n)$ is the predicted state using a learned model $M$ (e.g., neural operator or transformer model).\n3. Calibration: Compute the nonconformity scores for the training data:\n$S_i = S(u_i, u_{i+1}), i = 1,...,n$.\nDetermine the $(1 - \\alpha)$-quantile of these scores:\n$Q_{1-\\alpha} = \\max(S_1, S_2, ..., S_n)$.\n4. Prediction Set: For a new input state $u_n$, construct the prediction set:\n$\\hat{C}_n(u_n) = \\{v \\in U : S(u_n, v) \\le Q_{1-\\alpha}\\}$.\nThen,\n$P(u_{n+1} \\in \\hat{C}_n(u_n)) \\ge 1 - \\alpha$"}, {"title": "IV. EXPRIMENTS AND RESULTS", "content": "In this section, we will provide detailed results of our experiments, evaluation metrics, and the specific details of our experiments. To perform a thorough evaluation, we selected two of the most influential algorithms from each category, the Fourier neural operator (FNO) [10], U-shaped neural operator (UNO) [11], and tensorized Fourier neural operator (TFNO) [28] for comparison. We quantitatively characterized all the evaluation results using the assessment metrics and isotonic regression developed by the Uncertainty Toolbox [29].\nThe core objective of this section is to address the following two questions.\n1) Uncertainty Quantification: How do neural operators perform uncertainty quantification when applied to conformal prediction, ensemble methods, and MC dropout?\n2) Symmetry Testing: Given the inherent symmetries in physical systems, how do these models perform in symmetry-preserving tests?\nThis study uses the fluid dataset in [10], representing the video prediction task in the Navier Stokes (NS) equation. In this paper, we have selected six models for our ensemble, utilizing the snapshot ensemble training method. For the dropout method, the selected rate was 0.05. All models discussed below have been pre-trained, with their parameters detailed in the Supplementary. Given that the output of conformal prediction is the prediction interval [a \u2013 b, a + b), where b is equal to $Q_{1-\\alpha}$, we have made a specific assumption about the standard deviation. Considering that physical phenomena in nature generally follow a Gaussian distribution, the standard deviation is assumed to be. Given that our chosen confidence interval is 95%, the corresponding z value is 1.96.", "A. Forward and symmetry problems in turbulence dataset (NS)": "In the turbulence dataset, the forward problem and symmetry focus on evaluating the state of the fluid $u$ and $u'$ as it evolves. It contains 1200 randomly sampled initial states and their trajectories, with 70% used for training, 20% for validation, and the remaining 10% divided between test and calibration datasets.\n$u_{1:10} \\rightarrow u_{11:20}$\n$u'_{1:10} \\rightarrow u'_{11:20}$", "B. Results of Uncertainty of Neural Operators on the Forward Problems": "In this section, the illustrated and statistical results of different uncertainties are shown as follows."}, {"title": "V. CONCLUSION", "content": "In this study, we employed conformal prediction to assess and understand model uncertainty in the complex partial differential equations tasks. We have derived uncertainty quantification curves for commonly used operator models through qualitative and quantitative comparative investigations. These can be broadly applied in fields such as scientific uncertainty and video evaluation, as well as in media forecast and interpretation. Future efforts will focus on how to reduce this uncertainty and control the training dynamic based on the uncertainty."}, {"title": "VI. APPENDIX", "content": "The experiments to investigate symmetry were conducted as follows: we trained the models using standard datasets, however, for the evaluation phase, the spatial positions of the input tensor data were systematically rotated by 90\u00b0 in the calibration and test dataset.", "B. Snapshot ensemble setting": "1. Cosine Annealing Learning Rate: The learning rate $\\eta_t$ is adjusted using a cosine annealing schedule:\n$\\eta_t = \\eta_{min} + \\frac{1}{2} (\\eta_{max} - \\eta_{min}) \\left(1 + \\cos\\left(\\frac{t}{T} \\pi \\right)\\right)$\nwhere:\n\u2022 $\\eta_{max}$ is the maximum learning rate at the start of the schedule; In this study $\\eta_{max} = 0.01$\n\u2022 $\\eta_{min}$ is the minimum learning rate at the end of the schedule; In this study $\\eta_{min} = 0.0001$\n\u2022 $t$ is the current iteration step;\n\u2022 $T$ is the total number of iterations in one cycle. In this study $T = 100$\n2. Snapshot Saving: At the end of each learning rate cycle, when the learning rate reaches its minimum, the model is considered to have converged to a local optimum. The parameters of the model at this point are saved as a snapshot. Let $\\Theta_i$ denote the model parameters at the end of the $i$-th cycle. The set of all snapshot parameters is represented as:\n$\\Theta = \\{\\Theta_1, \\Theta_2,...,\\Theta_M\\}$\nwhere $M$ is the total number of snapshots saved during training.\n3. Ensemble Prediction: For inference, the predictions are made by averaging the outputs of all saved snapshots. The ensemble prediction is given by:\n$\\hat{y} = \\frac{1}{M} \\sum_{i=1}^{M} f_i(x)$\nwhere $f_i(x)$ is the prediction of the $i$-th snapshot model for input $x$.", "C. Baseline operators and hyper-parameters setting": "The baseline operator models selected in this study are as follows. The number of MC dropouts is set as 100. The ensemble number is set to M 6."}]}