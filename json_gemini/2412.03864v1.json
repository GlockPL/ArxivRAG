{"title": "Training MLPs on Graphs without Supervision", "authors": ["Zehong Wang", "Chuxu Zhang", "Zheyuan Zhang", "Yanfang Ye"], "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various graph learning tasks, yet their reliance on neighborhood aggregation during inference poses challenges for deployment in latency-sensitive applications, such as real-time financial fraud detection. To address this limitation, recent studies have proposed distilling knowledge from teacher GNNs into student Multi-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate inference. However, these approaches often inadequately explore structural information when inferring unseen nodes. To this end, we introduce SimMLP, a Self-supervised framework for learning MLPs on graphs, designed to fully integrate rich structural information into MLPs. Notably, SimMLP is the first MLP-learning method that can achieve equivalence to GNNs in the optimal case. The key idea is to employ self-supervised learning to align the representations encoded by graph context-aware GNNs and neighborhood dependency-free MLPs, thereby fully integrating the structural information into MLPs. We provide a comprehensive theoretical analysis, demonstrating the equivalence between SimMLP and GNNs based on mutual information and inductive bias, highlighting SimMLP's advanced structural learning capabilities. Additionally, we conduct extensive experiments on 20 benchmark datasets, covering node classification, link prediction, and graph classification, to showcase SimMLP's superiority over state-of-the-art baselines, particularly in scenarios involving unseen nodes (e.g., inductive and cold-start node classification) where structural insights are crucial. Our codes are available at: https://github.com/Zehong-Wang/SimMLP.", "sections": [{"title": "1 Introduction", "content": "Given the widespread presence of graph-structured data, such as computing networks, e-commerce recommender systems, citation networks, and social networks, Graph Neural Networks (GNNs) have drawn significant attention in recent years. Typically, GNNs rely on message-passing mechanisms [15] to iteratively capture neighborhood information and learn representations of graphs. Despite their effectiveness across various graph learning tasks, GNNs face challenges in latency-sensitive applications, such as financial fraud detection [63], due to the computational overhead associated with neighborhood fetching [83]. To enable faster inference in downstream tasks, existing approaches primarily employ techniques like quantization [10], pruning [87], and knowledge distillation [74] for accelerating graph inference. However, these methods are still constrained by the need to fetch neighborhoods, limiting their effectiveness in real-time scenarios."}, {"title": "2 Related Work", "content": "Graph Neural Networks [15, 19, 32, 38, 39, 60, 68, 84] encode node embeddings following message passing framework. Basic GNNs include GCN [32], GraphSAGE [19], GAT [60], and so forth. For repid inference, SGC [69] and APPNP [14] decompose feature transformation and neighborhood aggregation, which are recently proven to be expressive [21, 77]. Despite their success, the neighborhood dependency significantly constrains the inference speed.\nSelf-Supervised Learning (SSL) [9, 24] acts as a pre-training strategy for learning discriminative [55] and generalizable [30] representations without supervision. Numerous studies extend SSL on graphs to train GNNs [23, 27, 52, 53, 61, 65-67, 88, 89]. In particular, GCA [89] extends instance discrimination [9] to align similar instances in two graph views, BGRL [53] employs bootstrapping [16] to further enhance training efficiency, and GraphACL [70] leverages an asymmetric contrastive loss to encode structural information on graphs. However, the dependency on neighborhood information still limits the inference speed.\nInference Acceleration on GNNs encompasses quantization [18, 31], pruning [13, 20], and knowledge distillation (KD) [25, 51]. Quantization [10] approximates continuous data with limited discrete values, pruning [87] involves dropping redundant neurons in the model, and KD focuses on transferring knowledge from large models to small models [74]. However, they still need to fetch neighborhoods, resulting in constrained inference acceleration. Considering this, GLNN [83] utilizes structure-independent MLPs for predictions, significantly accelerating inference by eliminating message passing. However, the introduction of MLPs inevitably compromises the structural learning capability. Following works further integrate structural information into MLPs via positional embedding [56, 64], label propagation [76], neighborhood alignment [11, 29, 37, 71], or motif cookbook [78], but these heuristic methods only consider one aspects of graph structures, failing to fully integrating structural knowledge. Unlike these methods, SimMLP is equivalent to GNNs in the optimal case, demonstrating better structural learning capabilities."}, {"title": "3 Preliminary", "content": "Notations. Considering a graph G = (A, X) consisting of node set V and edge set E, with N nodes in total, we have node features X \u2208 \\mathbb{R}^{N \\times d} and a adjacent matrix A \u2208 {0,1}^{N \\times N}, where A_{ij} = 1 iff e_{i,j} \u2208 E, and A_{ij} = 0 otherwise. The GNN $\\phi(\\cdot, \\cdot)$ takes node features x_i and graph structure A as input and outputs the structure-aware node embeddings h^{GNN}. The embedding follows a linear head to classify nodes into different classes, defined as:\n\\begin{equation}\nh^{GNN}_i = \\phi(x_i, A), \\quad \\Phi^{GNN} = \\text{head}^{GNN}(h^{GNN}).\n\\end{equation}\nThe GNNs highly rely on neighborhood information A, whereas neighborhood fetching poses considerable computational overhead during the inference. On the contrary, the MLP $\\mathcal{E}(\\cdot)$ takes the node feature x_i as input and output the node embeddings h^{MLP}, achieving fast inference by alleviating the neighborhood-fetching. The embeddings are then decoded via a prediction head:\n\\begin{equation}\nh^{MLP}_i = \\mathcal{E}(x_i), \\quad \\Phi^{MLP} = \\text{head}^{MLP}(h^{MLP}).\n\\end{equation}\nAlthough MLPs provide significantly faster inference over graph-structured datasets, the omitting of structural information inevitably sacrifices the model performance.\nTraining MLPs on Graphs. To jointly leverage the benefits of GNNs and MLPs, researchers propose methods to distill knowledge from pre-trained GNNs to MLPs by mimicking the predictions [83]. The training objective is defined as:\n\\begin{equation}\n\\mathcal{L} = \\sum_{i\\in V_{\\text{train}}} \\mathcal{L}_{CE}(\\Phi^{MLP}_i, y_i) + \\sum_{i\\in V} \\mathcal{L}_{KD}(\\Phi^{MLP}_i, \\Phi^{GNN}),\n\\end{equation}\nwhere $\\mathcal{L}_{CE}$ is the cross-entropy between the prediction and ground-truth, and $\\mathcal{L}_{KD}$ optimizes the KL-divergence between predictions of teacher GNN and student MLP. During the inference, only the MLP is leveraged to encode node embeddings and make predictions, leading to a substantial inference acceleration. Despite this, the alignment in the label space maximizes the coarse-grained task-specific correlation between GNNs and MLPs, failing to capture the fine-grained and generalizable relationship between node features and graph structures [54]. To this end, SimMLP applies self-supervised learning to align GNNs and MLPs in a more intricate embedding space, better capturing structural information."}, {"title": "4 Proposed SimMLP", "content": "4.1 Framework\nWe present SimMLP: a Self-supervised framework for learning MLPs on graphs. The framework consists of three components: (1) GNN encoder, (2) MLP encoder, and (3) alignment loss. As illustrated in Figure 2, SimMLP maximizes the alignment between GNN and MLP via a self-supervised loss. Specifically, given a graph G = (A, X), we use GNN encoder $\\phi(\\cdot, \\cdot)$ to extract structure-aware GNN embeddings h^{GNN} and MLP encoder $\\mathcal{E}(\\cdot)$ to obtain structure-free MLP embeddings h^{MLP}. The choice of GNN encoder is arbitrary; we can use different encoder for adopting to different tasks. For alignment, we employ the loss function to pretrain the model:\n\\begin{equation}\n\\mathcal{L} = \\sum_{i\\in V} ||p(h^{MLP}_i) - h^{GNN}_i||^2_Y + \\lambda ||\\mathcal{D}(h^{GNN}_i) - x_i||^2,  \\quad \\gamma \\geq 1,\n\\end{equation}\ninvariance \\qquad reconstruction\nwhere $\\gamma \\geq 1$ serves as a scaling term, akin to an adaptive sample reweighing technique [36], and $\\lambda$ denotes the trade-off coefficient. The projector $p(\\cdot)$, $\\mathcal{D}(\\cdot)$ can either be identity or learnable; we opt for a non-linear MLP to enhance the expressiveness in estimating instance distances [9]. The invariance term ensures the alignment between GNN and MLP embeddings [16], modeling the fine-grained and generalizable correlation between node features and localized graph structures. The reconstruction term acts as a regularizer to prevent the potential distribution shift [3], providing better signals for training MLPs. In downstream tasks, we further train a task head for classification, as shown in Figure 2.\nProposition 4.1. Suppose G = (A,X) is sampled from a latent graph G_{\\mathcal{l}} = (A,F), G \\sim P(G_{\\mathcal{l}}), and F^{*} is the lossless compression of F that $\\mathbb{E}[XA, F^{*}] = F$. Let p be an identity projector, and $\\lambda = 1, \\gamma = 1$."}, {"title": "4.2 Preventing Model Collapse", "content": "Challenges. Training MLPs on graphs without supervision is a non-trivial task. As illustrated in Figure 3, naively applying the basic loss function (Equation 4) to align GNNs and MLPs results in model collapse, as evidenced by lower training loss and reduced accuracy. Consistent with our findings, Xiao et al. [71] also show that simply employing the InfoNCE loss results in reduced performance.\nCauses. We consider the issue derives from the heterogeneity in information sources, specifically node features and localized structures, encoded by MLPs and GNNs respectively. Before delving into the root causes, it is important to note that existing self-supervised methods primarily focus on aligning different aspects of a homogeneous information source (i.e., different views of the same graph). These methods typically use a single encoder to map various graph views into a unified embedding space, applying self-supervised loss to align the distances between views. This approach facilitates the learning of informative and generalizable embeddings. However, in scenarios involving heterogeneous information sources, each source often requires a distinct encoder, leading to projections into separate embedding spaces. Consequently, the self-supervised objective fails to accurately measure the true distance between sources, resulting in non-informative embeddings. We propose two strategies to address this issue.\nStrategy 1: Enhancing Consistency between GNN and MLP Encoders. The challenge of handling heterogeneous information sources primarily stems from using different encoders. A straightforward solution is to use a single encoder to process all sources, including node features and localized structures. Fortunately, this approach is feasible for graphs. In the learning process of a GCN, neighborhood information is iteratively aggregated and updated, where aggregation can be viewed as a non-parametric averaging of neighborhoods, and updating as a parametric non-linear transformation. Thus, the learning process of GNNs can be approximated using MLPs by (1) applying an MLP encoder to node features to obtain MLP embeddings h_\\mathcal{M}, and (2) aggregating the MLP embeddings of neighboring nodes to approximate GNN embeddings h^GNN:\n\\begin{equation}\n\\text{Approx}: h^{GNN}_i = \\sigma\\bigg(h^{\\mathcal{M}}_i + \\sum_{(i, j) \\in E} \\alpha_{ij} h^{\\mathcal{M}}_j \\bigg),\n\\end{equation}\nwhere $\\alpha_{ij}$ denotes the aggregation weight [32], and $\\sigma(\\cdot)$ is the activation function. This approach allows the use of a single MLP encoder for both node features and localized structures, ensuring that h^\\mathcal{M} and h^{GNN} reside in the same embedding space. The form of this approximation is similar to SGC [69] and APPNP [14], which decompose feature transformation and message passing to reduce model complexity and redundant computations. However, SimMLP applies this strategy specifically to enhance the consistency between GNN and MLP encoders. Notably, this approximation can be adapted to various GNN architectures with simple modifications.\nStrategy 2: Enhancing Data Diversity through Augmentation. Increasing data diversity [33] is beneficial for handling heterogeneous information sources by creating multiple pairs of the same instances (i.e., target nodes and localized structures). We use augmentation techniques [81, 85] to generate multiple views of nodes, allowing a single node to be associated with various pairs of node features and localized structures. The augmentation is defined as:\n\\begin{equation}\nG' = (\\hat{A}, \\hat{X}) \\sim \\tau(G), \\quad \\text{s.t.}, \\quad \\tau(G) = (q_e(A), q_f(X)),\n\\end{equation}\nwhere $\\tau(\\cdot)$ represents the augmentation function, which includes structural augmentation $q_e(\\cdot)$ and node feature augmentation $q_f(\\cdot)$. For simplicity, we apply random edge masking and node feature masking with pre-defined augmentation ratios [88]. The application of these two strategies prevents model collapse, as shown in Figure 3. It is important to note that these strategies are only employed during pre-training and do not affect the inference phase."}, {"title": "4.3 Theoretical Understanding", "content": "Mutual Information Maximization. Mutual information I(;) is a concept in information theory, measuring the mutual dependency between two random variables. This has been widely used in signal processing and machine learning [4]. Intuitively, maximizing the mutual information between two variables can increase their correlation (i.e., decrease their uncertainty). In this section, we interpret SimMLP and existing MLP learning methods from the perspective of mutual information maximization to analyze their learning objectives, as summarized in Table 1. Firstly, we unify the notations and introduce the key lemmas. A graph G = (X, A, Y) consists of node features X, graph structure A and node labels Y. We define ego-graph around node i as S_i = (X[i], A[i]), where X[i], A[i] denote node features and graph structure of S_i, respectively.\nLemma 4.2. Minimizing the cross-entropy H(Y; \\hat{Y}|X) is equivalent to maximizing the mutual information I(X; Y)."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nWe conduct experiments on node classification, link prediction, and graph classification. We use 20 datasets in total, including homophily graphs: Cora, Citeseer, Pubmed, Computer, Photo, Co-CS, Co-Phys, Wiki-CS, Flickr, and Arxiv; heterophily graphs: Actor,"}, {"title": "5.2 Node Classification", "content": "Transductive Setting. Given a graph G = (V, E), all nodes v \u2208 V are visible during training, with the nodes partitioned into non-overlapping sets: V = V_{train} \\cup V_{val} \\cup V_{test}. We use V_{train} for training and V_{val} and V_{test} for evaluation. The results, presented in Table 3, show that SimMLP outperforms self-supervised GCL methods in all settings and surpasses supervised GNNs in 7 out of 10 datasets. Compared to other MLP-based methods, SimMLP achieves a superior average performance of 82.1, exceeding the second-best score of 81.0, thereby demonstrating its effectiveness in node classification. We present the comprehensive ablation study in Appendix E.\nInductive Setting. In this paper, we consider inductive inference for unseen nodes within the same graph. We partition the graph G = (V, E) into a non-overlapping transductive graph G_T = (V_T, E_T) and an inductive graph G_I = (V_I, E_I). The transductive graph G_T contains 80% of the nodes, further divided into V_T = V_{T_{train}} \\cup V_{T_{valid}} \\cup V_{T_{test}}, which are used for training in the transductive setting. The inductive graph G_I consists of the remaining 20% of nodes, which are unseen during training. Compared to the settings in [56, 83], our approach is more challenging because V_I is disconnected from V_T during inference. We evaluate three measures: (1) transductive results, evaluated on V_{T_{test}}, (2) inductive results, evaluated on V_I, and (3) production results, which are the weighted average of the previous two. The production results are reported in Table 4. We observe that SimMLP outperforms various baselines, highlighting the advantage of leveraging structural information for unseen nodes. Additional results can be found in Appendix C.1.\nCold-Start Setting. We follow the inductive setting by partitioning the graph G = (V, E) into a transductive graph G_T and an inductive graph G_I. The key difference is that the nodes in G_I are isolated, with all edges removed, such that G_I = (V_I, \\emptyset). This approach aligns with real-world applications where new users often emerge independently [22]. We report the performance on V_I as the cold-start results, as shown in Table 5. SimMLP achieves significant improvements over all baselines, highlighting its superior structural learning capabilities. Notably, SimMLP shows performance gains of 7% and 18% over the vanilla MLP on Flickr and Arxiv, respectively, and 7% and 6% enhancements over VQGraph. Additional results can be found in Appendix C.2.\nHeterophily Graphs. The homophily inductive bias in SimMLP (Table 2) stems from the chosen GNN encoder, which in our case is GCN. However, by employing different GNN architectures, we can introduce varying inductive biases that are better suited for heterophily graphs. To explore this, we use two advanced methods- ACMGCN [40] and GCNII [8]-which are known for their effectiveness in handling heterophily graphs, alongside MLP and GCN. We evaluate these models on three challenging heterophily datasets: Actor, Texas, and Wisconsin. The results, presented in Figure 4, demonstrate that SimMLP can adapt to different heterophily-oriented GNN architectures, consistently enhancing their performance. This highlights SimMLP's adaptability and broad applicability across various graph domains and GNN architectures."}, {"title": "5.3 Link Prediction", "content": "Due to its self-supervised nature, SimMLP can be easily extended to link-level and graph-level tasks. We compare SimMLP against MLP, GNN, and LLP [17]-a state-of-the-art MLP-based method for link prediction-using Cora, Citeseer, and Pubmed as benchmark datasets. We strictly adhere to the experimental setup from [17]. The results, illustrated in Figure 5, show that SimMLP achieves the best performance across all three datasets, with particularly significant improvements on Cora and Citeseer. These findings underscore SimMLP's superiority in modeling localized structural information for accurately determining node connectivity."}, {"title": "5.4 Graph Classification", "content": "Although SimMLP is primarily designed for learning localized structures, it still delivers strong performance on graph classification tasks, which require an understanding of global structural knowledge. We compare SimMLP against various baselines, including supervised GNNs such as GIN [73], as well as self-supervised GNNs like the WL kernel [46], DGK [75], graph2vec [44], MVGRL [23], InfoGraph [50], GraphCL [81], and JOAO [80]. Additionally, we implemented an MLP learning method on graphs by applying a KL divergence loss similar to that used in GLNN [83]. The graph classification results, presented in Table 6, show that SimMLP achieves the best or second-best performance on 6 out of 7 datasets across various domains, with particularly strong results on the large-scale COLLAB dataset. These findings highlight the potential of SimMLP for graph-level tasks."}, {"title": "5.5 Inference Acceleration", "content": "Table 7 compares SimMLP with existing acceleration methods [83], including quantization (QSAGE), pruning (PSAGE), and neighbor sampling (Neighbor Sample) in an inductive setting. We also include SGC and APPNP, which simplify message passing for faster inference. Our observations indicate that even the most efficient of these methods offers only marginal acceleration (3.2 ~ 4.0\u00d7) and inevitably sacrifices model performance. In contrast, SimMLP achieves remarkable inference acceleration (89.7 ~ 125.9\u00d7) by eliminating the neighborhood fetching process. Additionally, we provide a comparison with MLP-based methods, as shown in Figure 1. This figure illustrates the trade-off between model performance and inference time on Arxiv in a cold-start setting. SimMLP delivers significant performance improvements over MLP-based methods and substantial inference acceleration compared to GNN-based methods, demonstrating the best overall trade-off."}, {"title": "5.6 Robustness Analysis", "content": "In this section, we analyze the robustness of SimMLP on noisy data and with scarce labels. We report the results for the inductive set V_I in the inductive setting, averaging over seven datasets: Cora, Citeseer, Pubmed, Computer, Photo, Co-CS, and Wiki-CS.\nNoisy Node Features. We assess the impact of node feature noise by introducing random Gaussian noise. Specifically, we replace X with \\tilde{X} = (1 - \\alpha)X + \\alpha x, where x represents random noise independent of X, and the noise level \\alpha \\in [0, 1]. As shown in Figure 6 (Left), SimMLP outperforms all baselines across all settings, despite the fact that node content quality is critical for MLP-based methods [17, 83]. We attribute this robustness to the augmentation process, which synthesizes additional high-quality node and egograph pairs, thereby enhancing MLP training. This augmentation also contributes to the robustness of BGRL. However, we observe that the performance of other MLP-based methods deteriorates rapidly as noise levels increase.\nNoisy Topology. To introduce structural noise, we randomly flip edges within the graph. Specifically, we replace A with \\tilde{A} = \\mathcal{M} (1 - A) + (1 - \\mathcal{M}) A, where M_{ij} \\sim B(p), and B(p) is a Bernoulli distribution with probability p. The results under varying noise levels are depicted in Figure 6 (Middle). SimMLP consistently outperforms other methods, demonstrating its robustness. While increasing noise levels significantly degrade the performance of GNNs, especially self-supervised BGRL, they have minimal impact on MLPs, even when A becomes independent of A.\nLabel Scarcity. We also examine SimMLP's robustness under label scarcity. Figure 6 (Right) presents model performance across various label ratios for node classification. Our method consistently outperforms all other baselines, even with extremely limited training data (0.001). This highlights SimMLP's resilience to label scarcity. Furthermore, we observe that self-supervised methods demonstrate greater robustness [30] compared to supervised approaches, likely due to their ability to leverage unlabeled data during training."}, {"title": "6 Conclusion", "content": "MLPs offer rapid inference on graph-structured datasets, yet the lack in learning structural information limits their performance. In this paper, we propose SimMLP, the first MLP learning method that is equivalent to GNNs (in the optimal case). Our insight is that modeling the fine-grained correlation between node features and graph structures preserves generalized structural information. To instantiate the insight, we apply self-supervised alignment between GNNs and MLPs in the embedding space. Experimental results show that SimMLP is generalizable to unseen nodes, robust against noisy graphs and label scarcity, and flexible to various graph-related tasks."}, {"title": "A Proof", "content": "A.1 Proof of Proposition 4.1\nPROOF. Our proof is based on a mild assumption:\n\\begin{itemize}\n    \\item The graph G = (A, X) is sampled from a latent graph G_{\\mathcal{l}} = (A, F) [72] following the distribution G \\sim P(G_{\\mathcal{l}}), where F \\in \\mathbb{R}^{N \\times d} represents the latent node semantics. This assumption is the extension of latent variable assumption, a general assumption in statistics and machine learning, which is based on the principle that observed data is sampled from an unobserved distribution.\n\\end{itemize}\nThen, we introduce some notations used in the proof:\n\\begin{itemize}\n    \\item The MLP encoder $\\mathcal{E}$ and the decoder $\\mathcal{D}$ are defined as fully-connected layer, which ensures the l-Lipschitz continuity with respect to the l_2-norm. This is a common property in neural networks with continuous activation functions, e.g., ReLU [62]. This property is data-agnostic and thus suitable for most real-world graphs. The output of MLP encoder is h^{MLP} = $\\mathcal{E}$(X) with h^{MLP} \\in \\mathbb{R}^{N \\times d'}.\n    \\item GNN encoder $\\phi$ yields h^{GNN} = $\\phi$(X, A) with h^{GNN} \\in \\mathbb{R}^{N \\times d'}.\n    \\item F^{*} \\in \\mathbb{R}^{N \\times d'} denotes the lossless compression of F that $\\mathbb{E}[XA, F^{*}] = F$.\n\\end{itemize}\nThe Equation 5 can be rewritten as:\n\\begin{align}\n\\mathcal{E}^{*} &= \\underset{\\mathcal{E}}{\\text{arg min}}  ||h^{MLP} - h^{GNN}||^2 + ||\\mathcal{D}(h^{GNN}) - X||^2 \\\\ &= \\underset{\\mathcal{E}}{\\text{arg min}} \\mathbb{E} [ ||h^{MLP} - F^{*}||^2 + ||h^{GNN} - F^{*}||^2 + ||\\mathcal{D}(h^{GNN}) - X||^2] \\\\ &= \\underset{\\mathcal{E}}{\\text{arg min}} \\mathbb{E} [ ||h^{MLP} - F^{*}||^2_F + ||h^{GNN} - F^{*}||^2_F + ||\\mathcal{D}(h^{GNN}) - X||^2_F] \\\\ & - 2 \\mathbb{E}_F [ \\langle h^{MLP} - F^{*}, h^{GNN} - F^{*} \\rangle ] \\\\ &= \\underset{\\mathcal{E}}{\\text{arg min}} \\mathbb{E} [ ||h^{MLP} - F^{*}||^2_F + ||h^{GNN} - F^{*}||^2_F + ||\\mathcal{D}(h^{GNN}) - X||^2_F] \\\\ & - 2 \\mathbb{E}_{F^{*}} [Cov (h^{MLP}, h^{GNN})|F^{*}] \\\\\n&= \\underset{\\mathcal{E}}{\\text{arg min}} \\mathbb{E} [ ||h^{MLP} - F^{*}||^2 + ||h^{GNN} - F^{*}||^2 + ||\\mathcal{D}(h^{GNN}) - X||^2] \\\\\n& - 2 \\beta_F \\text{Cov} (h^{MLP}, h^{GNN})|F^{*}], \n\\end{align}\nThen, with a bit of simple transformations, the Equation 11 can be expressed in the form of Equation 5. We explain these four terms in details. The first two terms $||h^{MLP} - F^{*}||^2$ and $||h^{GNN} - F^{*}||^2$ indicate the reconstruction errors of MLP embedding $h^{MLP}$ and GNN embedding $h^{GNN}$ on the latent variable F^{*}, ensuring the invariance on the latent graph G_{\\mathcal{l}}. The third term $||\\mathcal{D}(h^{GNN}) - X||^2$ reconstructs the node feature X using GNN embeddings $h^{GNN}, mitigating the risk of potential distribution shifts. The last term $-2 \\sum_i \\text{Cov} (h^{MLP}, h^{GNN})$ maximizes the covariance between GNN and MLP embeddings at each dimension, aligning GNNs and MLPs in the embedding space."}, {"title": "A.2 Proof of Lemma 4.2", "content": "PROOF. We follow the paper [5] to prove the lemma. We show the equivalence between these two terms by expanding H(Y; \\hat{Y}|X) and I(X; Y). We first expand the mutual information as\n\\begin{equation}\nI(X; Y) = H(Y) - H(Y|X).\n\\end{equation}\nMaximizing the mutual information I(X; Y) indicates minimizing the conditional entropy H(Y|X). The entropy on the label H(Y) is a constant, which can be ignored.\nThe cross-entropy H(Y; \\hat{Y}|X) can be written as the combination of conditional entropy H(Y|X) and KL divergence D_{KL}(Y||\\hat{Y}|X):\n\\begin{align}\nH(Y;\\hat{Y}|X) &= - \\sum_i p(Y_i|X) \\log(\\hat{Y}_i|X) \\\\\n&= - \\sum_i p(Y_i|X) \\log(Y_i|X) + \\sum_i p(Y_i|X) \\log(Y_i|X) \\\\\n&=  - \\sum_i p(Y_i|X) \\log(Y_i|X) + \\sum_i p(Y_i|X) \\log\\bigg(\\frac{Y_i}{\\hat{Y_i}}\\bigg)(Y_i|X)\\\\\n&= H(Y|X) + \\sum_i p(Y_i|X) \\log\\bigg(\\frac{Y_i}{\\hat{Y_i}}\\bigg)(Y_i|X)\\\\\n&= H(Y|X) + D_{KL}(Y||\\hat{Y}|X).\n\\end{align}\nConsidering Equation 13, minimizing the cross-entropy H(Y; \\hat{Y}|X) can minimize H(Y|X) (as well as D_{KL}(Y||\\hat{Y}|X)), which is equivalent to maximizing the mutual information I(X; Y). Based on the analysis in [5], Equation 13 can be optimized in a Max-Min manner. In particular, the first step is to freeze the encoder and only optimize the classifier, corresponding to fix H(Y|X) and minimize D_{KL}(Y||\\hat{Y}|X). The KL term would ideally vanish at the end of this step. Following step involves optimizing the parameters of the encoder while fixing the classifier."}, {"title": "A.3 Proof of Proposition 4.3", "content": "Proof. Before the proof, we need to provide some notations. We aim to compress the original graph G into T = (h^{MLP}, h^{GNN}) by preserving the information of latent graph G_{\\mathcal{l}}. Based on the definition of information bottleneck [57], the optimal compression is\n\\begin{equation}\nT^{*} = \\underset{T}{\\text{arg min}} I(G; T) - \\beta I(T; G_{\\mathcal{l}}),\n\\end{equation}\nwhere \\beta denotes the Lagrange multiplier and I(;) is the mutual information. The optimal compression T^{*} preserves the essential latent information by maximizing \\beta I(T; G_{\\mathcal{l}}) and discard the noises contained in the observed data G by minimizing I(G; T). To handle the equation in a more accessible manner, we convert it as\n\\begin{align}\nT^{*} &= \\underset{T}{\\text{arg min}} I(G; T) - \\beta I(T; G_{\\mathcal{l}}) \\\\\n&= \\underset{T}{\\text{arg min}} (1 - \\beta)H(T) + \\beta H(T|G_{\\mathcal{l}}) - H(T|G) \\\\\n&= \\underset{T}{\\text{arg min}} H(T) + \\lambda H(T|G_{\\mathcal{l}}) \\\\\n&= \\underset{\\mathcal{E}}{\\text{arg min}} \\lambda H(h^{MLP}|G_{\\mathcal{l}}) + H(h^{GNN}) \\\\\n& + \\lambda H(h^{GNN}|G_{\\mathcal{l}}) + H(h^{MLP}|h^{GNN}),\n\\end{align}\nwhere \\lambda = \\frac{\\beta}{1 - \\beta} > 0 and H() is the entropy."}, {"title": "B Experimental Setup", "content": "B.1 Dataset Statistics\nNode Classification. We select 10 benchmark datasets to evaluate the performance of SimMLP and other baselines. These datasets are collected from diverse domains, encompassing citation networks, social networks, wikipedia networks, etc. We present the statistics of these datasets in Table 8. Specifically, Cora, Citeseer, Pubmed [79", "45": "are two co-purchase networks that describe the frequent co-purchases of items (nodes). Co-CS (Coauthor-CS) and Co-Phys (Coauthor-physics) [45", "41": "is extracted from Wikipedia, comprising computer science articles (nodes) connected by hyperlinks (edges). Flickr [82", "28": "to evaluate model performance on large-scale datasets. We process the dataset in PyG using OGB public interfaces with standard public split setting.\nLink Prediction. We take five public benchmark datasets, including Cora, Citeseer, Pubmed, Computer, and Photo, for evaluating link prediction performance. The statistics is presented in Table 8, and we adopt the hyper-parameters in Table 10.\nGraph Classification. For graph classification, all datasets are sourced from TU datasets [42"}]}