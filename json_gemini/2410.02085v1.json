{"title": "Multi-Omic and Quantum Machine Learning Integration for Lung Subtypes Classification", "authors": ["Mandeep Kaur Saggi", "Amandeep Singh Bhatia", "Mensah Isaiah", "Humaira Gowher", "Sabre Kais"], "abstract": "Quantum Machine Learning (QML) is a red-hot field that brings novel discoveries and exciting opportunities to resolve, speed up, or refine the analysis of a wide range of computational problems. In the realm of biomedical research and personalized medicine, the significance of multi-omics integration lies in its ability to provide a thorough and holistic comprehension of complex biological systems. This technology links fundamental research to clinical practice. The insights gained from integrated omics data can be translated into clinical tools for diagnosis, prognosis, and treatment planning. The fusion of quantum computing and machine learning holds promise for unraveling complex patterns within multi-omics datasets, providing unprecedented insights into the molecular landscape of lung cancer. Due to the heterogeneity, complexity, and high dimensionality of multi-omic cancer data, characterized by the vast number of features (such as gene expression, micro-RNA, and DNA methylation) relative to the limited number of lung cancer patient samples, our prime motivation for this paper is the integration of multi-omic data, unique feature selection, and diagnostic classification of lung subtypes: lung squamous cell carcinoma (LUSC-I) and lung adenocarcinoma (LUAD-II) using quantum machine learning. We developed a method for finding best differentiating features between LUAD and LUSC datasets, which has the potential for biomarker discovery. In this paper, we show the efficacy of Quantum Neural Networks (QNNs) with three dimensions of feature encoding for the classification of multi-Omics human lung data from The Cancer Genome Atlas, comparing the framework to a variety of classical machine learning methods. Our results indicate that the Multi-omic Quantum Machine Learning Lung Subtype Classification (MQML-LungSC) framework offers superior classification performance with smaller training datasets as significant and non-significant based on p-value, thus providing compelling empirical evidence for the potential future application of unconventional computing approaches in the biomedical sciences. In comparing the performance of our proposed models based on the number of encoded features (256, 64, 32), it is evident that the model with 256 encoded features exhibits superior results across several metrics. It achieves a training accuracy of 0.95 and a testing accuracy of 0.90, which are higher than those of the models with 64 and 32 encoded features. Specifically, the model with 64 encoded features scores 0.92 for training accuracy and 0.86 for testing accuracy, while the model with 32 encoded features scores 0.88 for training accuracy and 0.85 for testing accuracy. The analyses of large-scale molecular data are beneficial for many aspects of oncology research, including the classification of possible subtypes, stages, and grades of cancer.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum machine learning is pioneering a new era in computational biology, unleashing powerful tools that redefine the possibilities for tackling intricate biological challenges. The analyses of large-scale molecular data are beneficial for many aspects of oncology research, including the classification of possible subtypes, stages, and grades of cancer. Several approaches have been proposed to train networks using quantum computing technology more accurately, robustly, and efficiently [1] [2]. However, hybrid strategies have recently emerged given the current limitations of quantum machines and the constrained number of available qubits. These strategies leverage existing technology to achieve practical and usable solutions [3]. Quantum-enhanced AI and machine learning methods are gaining attention as promising solutions to medical challenges. Recent advancements in quantum computing and quantum AI have demonstrated their wide-ranging applicability in healthcare. These methods have shown promise in various healthcare and drug-discovery domains and predicting ADME-Tox properties in drug discovery [4] [5] [6], including rapid genome analysis [7] and sequencing [8], disease detection [9], reference crop evapotranspiration classification [10], for chemistry and electronic structure calculations [11], [12] [13] [14]. Moreover, quantum computing models play a significant role in predicting gene mutations critical for the pathogenesis and diagnosis of specific cancer types, such as the Glioma Tumor Classification [15]. Cancer sub-type classification is crucial for understanding cancer pathogenesis and developing targeted treatments that can benefit patients the most [16]. Lung cancer is the most commonly diagnosed malignant tumor and is a leading cause of cancer-associated mortality. It is the second"}, {"title": null, "content": "highest cause of new cancer cases in both genders in the United States and is the second leading cause of cancer deaths in females globally. The most common subtypes of lung cancers are lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD), classified together as non-small cell lung cancer (NSCLC). The GDC-TCGA dataset provides diverse omic data crucial for cancer research and subtype classification. Specifically, for LUSC and LUAD cancer subtype the dataset includes DNA Methylation (DNAme): This dataset, derived from the Illumina Human Methylation 450 platform, provides beta values indicating DNA methylation levels across various genes. This data helps identify methylation patterns that are specific to LUAD and LUSC subtypes, revealing regulatory changes associated with each cancer type (which means DNA methylation features are present (indicative of significant regulatory changes) or absent (no significant changes) in each subtype to understand their role in tumor biology). Each sample in this dataset represents tumor tissue from an individual patient, with data formatted as rows of gene identifiers and columns of sample beta values. RNA Sequencing (RNA-seq): Measures gene expression levels by analyzing RNA from tumor tissues using the HTSeq platform, helping to identify genes with differential expression between LUAD and LUSC subtypes. The RNA-seq data are presented as log2(count + 1) values, reflecting the relative expression of genes. MicroRNA Sequencing (miRNA-seq): This dataset quantifies microRNA expression levels using stem-loop expression technology. The miRNA-seq data are also transformed into log2(RPM + 1) values, providing insights into the regulatory roles of microRNAs in LUAD and LUSC. Each sample represents tumor tissue from an individual patient, and the data are organized into rows of miRNA identifiers and columns of expression values. For LUAD vs. LUSC subtype classification, the dataset includes primary tumor and solid tissue normal samples from which these multiomic features are derived. This approach allows for a thorough examination of molecular differences between the lung subtypes, and enhancing diagnostic accuracy.\nTraditional studies often analyze individual omic features in isolation, focusing on discrete datasets such as DNA Methylation patterns [17], gene expression profiles [18], or microRNA levels. In these contexts, a \"sample\" refers to a biological specimen collected from a patient, such as a piece of tumor tissue or a blood sample. Each sample provides specific molecular data representing that patient's unique biological characteristics. This isolated approach, which examines data from each sample separately, can miss valuable insights from the interactions and correlations between molecular features across different samples.\nIn contrast, our MQML-LungSC framework integrates DNA-Methylation, RNA-seq, and miRNA-seq data to explore the interconnectedness of molecular features across a wide range of tumor samples. By examining the relationships between gene expression levels, methylation"}, {"title": null, "content": "patterns, and microRNA profiles both within individual samples and across a population of samples, we can identify complex interactions and patterns that differentiate LUAD from LUSC. This integrated analysis allows for a more comprehensive understanding of the molecular mechanisms underlying these lung subtypes, potentially leading to more accurate classification and the discovery of significant features.\nIn Fig. 1, the development of lung cancer is illustrated by comparing normal cells to tumor cells. However, recent studies have suggested that LUAD and LUSC should be classified and treated as different cancers [19]. Previous studies have utilized traditional feature selection and machine learning methods for cancer diagnosis, detection, and classification, but few have extended them to study potential features and biological pathways to discriminate between LUAD and LUSC [20]. To improve cancer classification accuracy, novel machine learning and feature selection methods have been developed. However, few studies have used overlapping features from different methods for classification, gene expression analysis, and molecular features [21] [22].\nThis work proposes a novel lung sub-type classification method that integrates classical feature selection techniques with a quantum classifier. This hybrid approach aims to enhance the accuracy and robustness of sub-type classification, thereby potentially contributing to the development of more effective cancer therapies. We propose a set of hybrid quantum computing and advanced machine learning approaches to apply machine learning on small datasets, such as (primary tumor and normal) sample types in The Cancer Genome Atlas (TCGA). These approaches aim to address the well-known \"big n, small m\" problem in multi-cancer analysis (Lung subtype diagnosis or classification), including (LUSC) and (LUAD).\nThe cancer datasets named (LUSC) and (LUAD) are taken from the Multi-Omics Cancer Benchmark TCGA (http://cancergenome.nih.gov/) Pre-processed Data, publicly available at the Multi-Omics Cancer Benchmark repository. For our study we employed omics datasets for transcriptome profiling by RNA-seq and miRNA-seq (micro-RNA) DNA methylation analysis by Methylation Array,, and associated clinical outcomes using associated patient information. These multi-omics data were extracted for a case study of LUSC and LUAD. To the best of our knowledge, this study is the first to utilize GDC-TCGA data within a Quantum-Classical framework using combination of omics data (DNA methylation, transciptome profiling from RNA-seq, and miRNA-seq, and clinical outcomes) data for lung subtype classification and molecular features identification.\nProblem Formulation and Motivation: Accurate diagnostic classification of subtypes can greatly help physicians to choose surveillance and treatment strategies for patients. Following the explosive growth of huge amounts of biological data, the shift from traditional bio-statistical methods to computer-aided means has made machine-"}, {"title": null, "content": "learning methods an integral part of today's cancer prognosis and diagnosis. Integration of multi-omics data allows more advanced comprehensive and systematic analysis of biological changes, providing a new biomarkers for the early diagnosis of diseases. However, there are several challenges in integrating and analyzing cancer multi-omics data on a large scale. TGCA database consists of several cancer omics datasets, including transciptome and proteome profiling, copy number variable, somatic structural, and DNA methylation. Mostly, multi-omics is characterized by high noise, high multidimensionality, and multidimensional heterogeneity, such as (big \"n\" and small \"m\") affecting the efficiency of classification tasks, where \"n\" refers to the number of features/genes and \"m\" samples ((i) DNA: 503 samples and 485,577 genes, (ii) RNA: 585 samples and 60,488 genes, (iii) miRNA: 564 samples and 1,881 genes) for LUAD. (i) DNA: 412 samples and 485,577 genes, (ii) RNA: 550 samples and 60,488 genes, (iii) miRNA: 523 samples and 1,881 genes) for LUSC.\nMain contributions: This study introduces a hybrid quantum classification model for distinguishing between LUSC-I tumor and LUAD-II tumor subtypes of lung datasets. In the classical section, we aim to identify the most important combination of multi-omic molecular markers using the feature selection technique. The multi-omic integrated features are encoded in the quantum section with different dimensions to find the best combination and hit list using quantum-classical model weights for tumor classification in LUAD.\nTo summarize, this paper makes the following contributions:\n\u2022 Developed a pipeline for data pre-processing, feature engineering based on p-value t-test employs significant and non-significant using high dimensions of omic datasets which include DNAme, miRNA-seq and RNA-seq in LUAD and LUSC subtype dataset.\n\u2022 Proposed a classical machine learning and filter methods for feature selection using random forest, mutual information, chi-square, and PCA on single omics and select top 85-85-86 features from each omic for further integration and selection of clinical attributes such as age, gender, pathological stage, survival status etc using GDC TCGA dataset.\n\u2022 Developed the quantum neural network algorithm to encode the 32 features, 64 features, and 256 features with amplitude encoding for subtype-I and subtype-II classification and find the top-hit features with the qnn layer/dense layer for further visualization plots such as violin, dot plot, heatmap clustering, and PCA.\n\u2022 Evaluated the accuracy and strength of the proposed quantum machine learning framework using three encoding structures of dimensions with fewer parameters and many epochs."}, {"title": null, "content": "The remainder of the article is organized as follows. Section II presents the Materials and Methods, including the dataset, processes, and methods. Section III outlines the proposed methodology of our framework, detailing the implementation process and describing the multi-omic quantum machine learning approach. Section IV provides the results and performance analysis, including experimental results on multi-omic integrations across three dimensions, comparisons with classical classifiers, and single-omic datasets. Section V discusses the findings and outcomes. Finally, Section VI concludes the paper and explores future research directions."}, {"title": "II. MATERIAL AND METHODS", "content": "This section provides a comprehensive overview of the methods utilized in our study. We describe the feature engineering process, feature selection methods, and proposed quantum neural network model for diagnosing lung dataset subtypes LUAD and LUSC. Each subsection below elaborates on these methods in detail."}, {"title": "A. Study Population", "content": "TCGA is a cancer multi-omics database generated by the National Institutes of Health Our proposed framework comprises four types of omics datasets: Gene expression, miRNA expression, DNA methylation, and Clinical patient information of lung squamous cell carcinoma. The number of samples acquired for selecting features that distinguish between LUSC and LUAD datasets is presented in Fig 1 and well detailed in Algorithm 1.\nThe original dataset comprised genomic data from various modalities, including DNAme, miRNA-seq, and RNA-seq, obtained from a cohort of patients with (LUAD) and (LUSC) lung. Specifically, the DNAme array dataset consisted from 503 patients with 485,577 features; the miRNA dataset contained samples from 564 patients with 1,881 features, and the RNA dataset encompassed samples from 585 patients with 60,488 features of the lung dataset. A summary of their clinical information is provided in Fig 1, with more comprehensive details available on the GDC-TCGA website. As described in Algorithm 1, We have selected three omics modalities for this study, and a summary of clinical samples is shown in Fig 1. The original dataset comprised genomic data from various modalities, including DNAme, miRNA-seq, and RNA-seq, obtained from a cohort of patients with (LUAD) and (LUSC) lung. Specifically, the DNAme array dataset consisted from 503 patients with 485,577 features; the miRNA dataset contained samples from 564 patients with 1,881 features, and the RNA dataset encompassed samples from 585 patients with 60,488 features of the lung dataset. A summary of their clinical information is provided in Fig 1, with more comprehensive details available on the GDC-TCGA web-"}, {"title": "B. Data Loading and Data Pre-processing", "content": "In the First phase, MQML takes any number of omic measures such as genomic, epigenomic, and transcriptomic datasets as input. Due to the inherent complexity and size of the dataset, pre-processing steps were necessary to ensure data quality and reduce computational burden. The LUAD and LUSC, Lung datasets are acquired from the Multi-Omics Cancer Benchmark GDC-TCGA Pre-processed Data, It consists of four types of omics datasets for a case study of lung squamous cell carcinoma.: RNA-seq, miRNA-seq, DNAme, and Clinical patient information. In the second phase, each omic has several gene features column-wise and patient samples row-wise. The raw data of each Omic1 subtype-I and Omic1 Subtype-11 is combined column-wise, with patient samples row-wise. Then, features that contain a sum of 0 are removed. To extract and select the survival/clinical attributes of both subtypes, combine the survival and clinical attributes based on sample type, i.e., diagnostic subtype-I and subtype-II."}, {"title": "C. Feature Engineering", "content": "Then, the selected clinical samples of subtype-I and subtype-II are used to split each omic into two parts, i.e., Omic1.1 Subtype-1 and Omic1.2Subtype-II. Each omic data is processed through a feature engineering process, including a statistical t-test, to determine if there is a significant difference between the means of the two groups.\nThis process involves analyzing two omic data frames, Omic1.1 Subtype-1 and Omic1.2 Subtype-I, to compare their mean values and compute statistical significance. First, the mean values for each column in both data frames are calculated. Then, a t-test is conducted for each column to determine the p-values, which assess the statistical significance of the differences between the two datasets. The mean values and p-values are then appended to their respective data frames. Finally, the updated data frames contain the original data mean values and p-values. This process integrates multiple omic datasets using minimum patient samples of Omic (Omic\u2081, Omic2, and Omic3) for the subtypes separately using a join operation. The integrated dataset combines each Omic1-2-3Subtype-1 and Omic1-2-3 Subtype-II mean values and p-values at the bottom of the row. Then, the entire dataset is sorted based on p-values for each Omic1-3Subtype-1 and Omic1-3Subtype-II to identify the most and least significant values in the data frame. Common significance levels include \u03b1 = 0.05 (most commonly used, indicating a 5% risk of a Type I error), \u03b1 = 0.01 (more stringent, indicating a 1% risk), and \u03b1 = 0.10 (less stringent, indicating a 10% risk). Then, each omic combined Omic1-2-3 Subtype data frame is isolated into independent Omic1 Subtype, Omic2Subtype, and Omic3 Subtype data frame datasets for data analysis steps. Finally, the prepared clinical and survival attributes are combined with each OmicSubtype as shown in Fig. 2. The t-test is a statistical method used to determine if there is a significant difference between the means of two groups. In this context, it analyzes multi-omic data from two lung dataset subtypes, LUSC and LUAD. By calculating the"}, {"title": null, "content": "t-statistic, we can identify significant features that differentiate the two dataset subtypes. The t-statistic is calculated as:\n$tstatistic = \\frac{X_{LUSC} - X_{LUAD}}{S_{LUSC} + S_{LUAD}}$ (1)\nwhere $X_{Lusc}$ is the mean of the LUSC group, $X_{LUAD}$ is the mean of the LUAD group, $S_{LUSC}$ is the standard deviation of the LUSC group, and $S_{LUAD}$ is the standard deviation of the LUAD group."}, {"title": "D. Feature Selection", "content": "In the third phase, each omic is divided into three subsets of samples: Omics\u2081, Omics\u2082, and Omics\u2083, based on p-values as shown in Table IV and Algorithm 1. We employed three steps in the feature selection process. In the first step, we determined the important features using four methods (two filter methods, one supervised method, and one unsupervised method) with the Select K Best function. Using a Venn diagram, we selected the unique features by identifying common and uncommon features. In the second step, we employed a random forest to classify the selected features from each selection"}, {"title": null, "content": "process. We then combined the features based on AUC-ROC analysis to choose the best features with an AUC greater than 0.80. In the third step, each omic's combined and reduced subset is further processed to reduce the features using hierarchical clustering based on distance."}, {"title": "1. Feature Selection Process", "content": "In this section, we detail the feature selection process steps to identify the most relevant features for our analysis and benchmark the performance. Model 1 Mutual Information (M33I): measures the amount of information obtained about one random variable through another random variable. In feature selection, it quantifies the dependency between each feature and the target variable. Features with high mutual information scores are considered more informative for predicting the target variable. Equation 2 [23] defines the mutual information I(X; Y) as follows:\n$I(X; Y) = \\sum_{X_i \\in X_i Y \\in Y} p(x_i, y) log \\frac{p(x_i, y)}{P(x_i)p(y)}$ (2)\nwhere $X_i$ is the i-th feature, Y is the target variable, p(x_i, y) is the joint probability distribution function of $X_i$ and Y, p(x_i) and p(y) are the marginal probability distribution functions of $X_i$ and Y, respectively.\nModel 2 Chi-Square: The test measures the independence between categorical variables. Feature selection quantifies the association between each categorical feature and the target variable. Features with high chi-square scores indicate a strong association with the target variable. The Chi-square value mathematical equation of joint probability genes can be calculated according to (3) [24]\n$\\chi^2(X,Y) = \\sum_{X_i, Y} \\sum_{X_i \\in X_i Y \\in Y}  \\frac{(O_{x_i,y} - E_{x_i,y})^2}{E_{x_i,y}}$ (3)\nwhere $X_i$ is the i-th feature, Y is the target variable, $O_{x_iy}$ is the observed frequency of occurrence of $X_i$ and Y, and $E_{x_i,y}$ is the expected frequency of occurrence of $X_i$ and Y under the null hypothesis of independence between them.\nModel 3 Random Forest Model: is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It calculates feature importance based on how much the model's accuracy decreases when each feature is randomly shuffled. Features with higher importance scores contribute more to the predictive power of the random forest model. For classification, with the majority vote, the predicted class \u0177 is given by [25]:\n$\\hat{y} = mode (\\{T_b(x)\\}_{b=1}^B)$"}, {"title": null, "content": "where B is the total number of trees in the forest, and $T_b(x)$ is the prediction of the b-th tree for input x.\nFeature Importance Calculation in RF can be measured by evaluating how much the model's prediction error increases when the values of a feature are permuted while keeping the other features unchanged. The feature importance for feature j, $FI_j$, can be computed as:\n$FI_j = \\frac{1}{B} \\sum_{b=1}^B \\sum_{t \\in T_b} \\Delta i_t \\cdot 1(j \\in t)$\nwhere: B is the total number of trees in the forest, $T_b$ is the b-th tree, t represents a node in the tree, $\\Delta i_t$ is the decrease in impurity at node t (e.g., reduction in Gini impurity or entropy), 1(j \u2208 t) is an indicator function that is 1 if feature j is used in node t. To select the top k features based on their importance scores: (i) Compute the feature importances {FI\u2081, FI\u2082, ..., FIp}. (ii) Sort the features by their importance scores in descending order. (iii) Select the top k features with the highest importance scores.\nModel 4 Principal Component Analysis (PCA): reduces the dimensionality of a dataset while retaining most of the variance by transforming original features into principal components [26]. The process involves standardizing the data ($z_{ij} = \\frac{x_{ij} - \\bar{x_j}}{\\sigma_j}$), computing the covariance matrix ($C = \\frac{Z^T Z}{n}$), and performing eigenvalue decomposition ($Ce_i = \\lambda_i e_i$) to obtain eigenvalues and eigenvectors. Principal components are formed by projecting standardized data onto eigenvectors ($PC = ZE$). The top k components that explain the most variance are then selected. This transformation preserves essential data variability while reducing dimensionality."}, {"title": "2. AUC-ROC Analysis", "content": "Further, we applied the second feature selection process step to identify the best features using AUC-ROC analysis for each feature. This approach was combined with the first feature selection process, where we obtained features from RF, MI, PCA, and Chi. We set a threshold (Th) and selected features if their AUC-ROC scores for both the training and testing datasets exceeded 0.80(Th).\nBest_features = {$X_i | AUC_{train} > Th \\land AUC_{test} > Th$}\ncombined_features = $ \\bigcup_{method} best\\_ features_{method}$\nunique_features = unique(combined_features)\n$S_{unique} = \\{ x | x \\in S_{all}\\}$\n\u2022 $S_{all}$ represents the set of all selected features.\n\u2022 $S_{unique}$ represents the set of selected features with duplicates removed.\nThe Best features are selected from different methods, combined into a single list, and remove any duplicate features from the combined list. The combined_features denote the combined list of best features. The unique_features denote the list of unique features after removing duplicates."}, {"title": "3. Hierarchical Clustering", "content": "Further, we have applied the third feature selection process step i.e. hierarchical clustering in two ways. Where one approach is based on the dendrogram clusters to select the best features using clusters and second approach is to compute the pairwise distance between features with distance to select the features. The approaches are: 1. Dendrogram Clusters. Given a dataset represented by $OMIC_{Subset_{1 to 3}}$, where each column represents a feature, we perform hierarchical dendrogram clustering to identify clusters of similar features. We create a dendrogram from the linkage matrix L and Assign cluster IDs to the features using the function.\nD = dendrogram(L)\n$C_i = fcluster(L, t)$ (5)\nIn order to extract features by each cluster, we store features in a dictionary called clusters, where each key is a cluster label $C_i$, and the corresponding value is a list of features belonging to that cluster. For each cluster $C_i$, we compute the feature importance I($C_i$) by summing the absolute values of the features in the cluster.\n$I(C_i) = \\sum_{f \\in F(C_i)}$|f| (6)\nselected_features_with_cluster = $ \\bigcup_i TopK(I(C_i))$ (7)\nbest_features_unique = set(selected_features_with_cluster) (8)\nThen, selected the top K important features from each cluster to form selected_features_with_cluster and merge the selected features to get a unique set of best features.\n2. Pairwise Distances In the second approach, the hierarchical clustering is performed on the pairwise distance matrix D using the Ward linkage method, which aims to minimize the variance when forming clusters. We compute the pairwise distance matrix D using the Euclidean metric between the transpose of the scaled feature matrix OMICSUBSET [S_unique]. The Euclidean distance between two points pand qin n- dimensional space and set a distance threshold @ to a specific value i.e. 0 = 3.5. is given by:\nEuclidean distance(p, q) = $\\sqrt{\\sum_{i=1}^n (q_i - p_i)^2}$ (9)"}, {"title": null, "content": "The Ward linkage criterion is calculated as [27]:\nZ = linkage(D, method =' ward')\nd(u, v) =$\\frac{|v|}{|T|+ sd(v, s)^2 + \\frac{|t|}{|T|}d(v,t)^2 - \\frac{|v|}{|T|}d(s,t)^2}$ (10)\nwhere, d(u, v) represents the distance between clusters u and v, and |v|, |s|, |t|, and |T| denote the number of points in clusters and subclusters, and d(v, s), d(v, t), and d(s, t) are the distances between clusters and subclusters."}, {"title": "E. Diagnostic Classical and Quantum Classifiers", "content": "In the fourth phase, we define the machine learning and quantum machine learning methods employed for subtype-I and subtype-II lung datasets diagnostic classification. In our experimental analysis, we used four distinct machine-learning algorithms. This section comprehensively overviews these classifiers and their respective theoretical implementations.\nLogistic Regression (LR) is a linear model used for binary classification problems. Using the logistic function, it estimates the probability that an instance belongs to a particular class. The model is based on the linear combination of input features [28].\nLogistic Function: $\\sigma(z) = \\frac{1}{1+ e^{-z}}$\nPrediction: $\\hat{y} = \\sigma(w^x + b)$\nLog loss: $\\frac{1}{N} \\sum_{i=1}^N[y_i log(\\hat{y}_i) + (1 - y_i) log(1 - \\hat{y}_i)]$ (11)\nWhere w represents the weights, x the input features, b the bias term, \u0177 the predicted probability, and y the actual label. Regularization can be applied to prevent overfitting, and in this case, L2 regularization is used with a penalty parameter C = 0.1.\nMulti-Layer Perceptron (MLP) for a MLP with L layers, the activation $a_j^{(l)}$ of neuron j in layer l is computed recursively from the input layer l = 1 to the output layer l = L [29]:\n$a_j^{(l)} = f (\\sum_{i=1}^{n(l-1)} W_{ij} a_i^{(l-1)} + b_j^{(l)})$\nwhere:a: Activation of neuron j in layer l, w: Weight connecting neuron i in layer 1 - 1 to neuron j in layer l, $a_i^{(l-1)}$: Activation of neuron i in the previous layer 1- 1,b: Bias term for neuron j in layer l, f: Activation function applied element-wise to the linear combination.\nSupport Vector Machine (SVM) is a supervised learning model used for classification tasks. It finds the"}, {"title": "Algorithm 1 MQML-LungSC: Multi-Omic Quantum Machine Learning for Lung Subtype Classification Framework Analysis Workflow", "content": null}, {"title": null, "content": "optimal hyperplane that maximizes the margin between different classes. The kernel trick allows SVMs to perform non-linear classification by mapping input features into higher-dimensional space [30]\nDecision Function: $f(x) = w^T\\phi(x) + b$\nOptimization Problem:\n$\\min_{w,b} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^N max(0,1 - y_i(w^T\\phi(x_i) + b))$ (12)\nWhere \u0278 is the feature mapping function (RBF kernel in this case), w the weight vector, b the bias term, C is the regularization parameter, and y the actual label.\nRandom Forest (RF) is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees [25]. It is robust to over-fitting and performs well on many datasets.\n$\\hat{y} = \\frac{1}{T} \\sum_{t=1}^T h_t(x)$ (13)\nWhere, T is the number of trees, and ht is the prediction of the t-th tree. Parameters such as the number of estimators (trees), criterion (e.g., entropy), max depth, and others can be tuned for optimal performance.\nQuantum Neural Network (QNN): In this section, we will provide an overview of the hybrid quantum neural network model, detailing the methodology used for three different models. Each model varies in the number of features (256, 64, and 32) and the corresponding number of qubits (8, 6, and 5) used for encoding, respectively. The models are named as QNN1 for 256 features, QNN2 for 64 features and QNN3 for 32 features. The summary of hybrid models is given in Table I. The hybrid model leverages a combination of QNNs and classical neural networks (CNN).\nFeature Encoding: The initial step involves encoding a classical vector into a quantum state. To efficiently simulate quantum circuits, each feature vector N-dimensional x is normalized and embedded into a quantum state |\u03c8\u27e9 using amplitude encoding as\n|\u03c8x\u27e9 =$\\frac{1}{\\sqrt{\\sum_j}} \\sum_{j=1}^N x_j |j\\rangle$ (14)"}, {"title": null, "content": "U (\u03b8) consists of several local quantum gates, such as two-qubit unitaries. The entire quantum circuit U is made up of n unitary blocks, expressed as U(\u03b8) = U\u2081U\u2082...Un, where ith unitary block is:\n$U_i(\\theta_i) = exp(-i \\theta_i P)$ (15)\nwhere P consists of Pauli operators, and \u03b8i denotes a gate parameter vector of Ui(\u03b8i)."}, {"title": "Algorithm 2 MQML-LungSC: (Contd.)", "content": "In quantum ansatz, we have applied parameterized rotations R(\u03b8, \u03c6, \u03bb), and Controlled-Z (CZ) gates for entanglement between adjacent qubits. Each qubit undergoes a rotation defined by three parameters, \u03b8, \u03c6, and \u03bb. The rotation operation R(\u03b8, \u03c6, \u03bb) can be expressed as:\nR(\u03b8, \u03c6, \u03bb) = $R_z(\\lambda) R_y(\\theta) R_z(\\phi)$ (16)\nThe controlled-Z gates introduce entanglement between adjacent qubits, which a phase flip if both qubits are in the |1\u27e9 state. For instance, a single layer of ansatz applies the following unitary transformation on the 8-qubit system as:\n$U(W) = [\\prod_{i=0}^7 R(\\theta_i, \\Phi_i, \\lambda_i)[\\prod_{i=0}^6 CZ_{i,i+1}$ (17)\nMeasurement: Finally, a quantum measurement (\u00d4) is applied using the expectation value of the Pauli-Z operator for each qubit\n$\u1ef9_i = \\langle \\psi_i | U(\\theta)^T \\hat{O} U(\\theta) | \\psi_i \\rangle$ (18)\nThe measurement results from the quantum circuit are then fed into a layer of a classical neural network, which is used to predict the label of the input state. Objective Function: In diagnostic classification, a binary classification task distinguishing between Subtype-I and Subtype-II using multi-omic data aims to minimize the binary cross-entropy loss. The binary cross-entropy loss function is defined as follows:\n$L = -\\frac{1}{N} \\sum_{i=1}^N[y_i log(\\hat{y}_i) + (1 - y_i) log(1 - \\hat{y}_i)]$ (19)"}, {"title": "IV. RESULTS AND PERFORMANCE", "content": "In this section, we"}]}