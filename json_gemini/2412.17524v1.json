{"title": "STAHGNet: Modeling Hybrid-grained Heterogenous Dependency Efficiently for Traffic Prediction", "authors": ["Jiyao Wang", "Zehua Peng", "Yijia Zhang", "Dengbo He", "Lei Chen"], "abstract": "Traffic flow prediction plays a critical role in the intelligent transportation system, and it is also a challenging task because of the underlying complex Spatio-temporal patterns and heterogeneities evolving across time. However, most present works mostly concentrate on solely capturing Spatial-temporal dependency or extracting implicit similarity graphs, but the hybrid-granularity evolution is ignored in their modeling process. In this paper, we proposed a novel data-driven end-to-end framework, named Spatio-Temporal Aware Hybrid Graph Network (STAHGNet), to couple the hybrid-grained heterogeneous correlations in series simultaneously through an elaborately Hybrid Graph Attention Module (HGAT) and Coarse-granularity Temporal Graph (CTG) generator. Furthermore, an automotive feature engineering with domain knowledge and a random neighbor sampling strategy is utilized to improve efficiency and reduce computational complexity. The MAE, RMSE, and MAPE are used for evaluation metrics. Tested on four real-life datasets, our proposal outperforms eight classical baselines and four state-of-the-art (SOTA) methods (e.g., MAE 14.82 on PeMSD3; MAE 18.92 on PeMSD4). Besides, extensive experiments and visualizations verify the effectiveness of each component in STAHGNet. In terms of computational cost, STAHGNet saves at least four times the space compared to the previous", "sections": [{"title": "1 Introduction", "content": "Traffic Flow Prediction (TFP) is a foundational component of intelligent transportation systems (ITS), aiming to estimate the future traffic conditions of a designated location in a transportation network based on the historical value of flow sensor readings in a complex interactive environment. It is fundamental for the stability and safety of intelligent transportation systems [1], however, which is still challenging due to the complex transportation interactions and ubiquitous noise/perturbations in data.\nThere has been a lot of research on accurate TFP to tackle these challenges recently. Most traditional methods are usually a statistical model for time series forecasting, which can be divided into univariant time series forecasting and multivariate time series forecasting. Univariate time series learning methods [2-4] mainly focus on the temporal correlations of the traffic flow time series from a single sensor. However, the information is not only the time series received by the sensor but also needs to consider the geographical location of the sensor in the whole transportation network, because the flow condition of the road impacts other roads. Other multivariate time series forecasting methods [5, 6] were proposed to identify the hidden spatial relationship between sensors at different times and apply it in time series prediction.\nIn short-term time series data, traditional prediction models work relatively well, but do not have sufficient accuracy for long-term time series data. In general, the traffic flow data usually have a very long-term time dependence. For example, a section of road is congested on a certain day, and people stuck in traffic may not drive on this road at the corresponding time for a few days or a few months, so the short time series does not contain enough information to predict. As the amount of data, we need to predict increases gradually, the data becomes more and more complex, and the nonlinear characteristics of the time series in the data are more obvious, which leads to the insufficiency of the capacity of traditional methods. The recent deep-based method [7-9] can iteratively learn the intra- and inter-time-series temporal dependencies between multivariate sensors for TFP. Specifically, to represent spatial dependency of multivariate time series with the non-Euclidean spatial structure that is suitable for road network [10, 11], graph neural network (GNN) [12] is introduced to TFP.\nNevertheless, only a static adjacency matrix cannot present the dynamic temporal dependency in the road map. Recently, some models [10, 13] were proposed to replace the learnable series embedding matrix with the dynamic temporal dependency graph generated by time series encoders. As shown in Figure 1, there are normally two types of dynamic dependency, and the dynamic graph network is usually employed in existing works to model evolving dependency. As for the type of dynamic graph network"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Sequence-based model", "content": "Conventional statistical models such as ARIMA [4] and Vector Auto-Regression(VAR) [20] have been used for TFP task, in which only intra-sequence relationships are considered, but not inter-sequence relationships. To make better prediction performance, some machine learning methods (e.g., SVM [21], XGBoost [22]) were further exploited to model non-linear correlations within series. In recent research, deep learning methods gained a gradually elevated role in TFP. In general, to extract Spatio-temporal relations, there are three types of deep learning models that are commonly used: RNN-based [7], CNN-based [8], and Transformer-based [9]. The deep learning methods outperform traditional methods for TFP task, which involves time series prediction.\nFor RNN-based methods, except for basic RNN, LSTM [23] and GRU [24] are also widely used for sequential dependency. FCLSTM [25] is a typical model using LSTM to finish the whole modeling process. ConvLSTM [26] is a variant model of FCLSTM that extends of fully-connected LSTM, which converts the state-to-state calculations in LSTM to convolution, effectively solving the redundancy problem of LSTM in predicting Spatio-temporal data. On the other hand, CNN-based approaches were used to model the traffic map as an image consisting of a discontinuous grid where each grid contains spatial traffic features. TCN [16] stacked several casual convolutional layers with exponentially enlarged dilation factors. DeepST [27] divides the time series into three subseries and convolves them separately. ST-ResNet [28] replaces the convolution in DeepST with the residual convolution, which demonstrates the effectiveness of CNN for modeling Spatio-temporal data. Further, to enhance the fine-granularity dependency extraction from road maps, the attention mechanism has been extensively used. For example, DSANet [17] leveraged the CNN for the prediction and self-attention mechanism for spatial correlation modeling. STDN [29] uses local CNN and LSTM to handle the Spatio-temporal dependence of traffic flow data, introduces a flow-gate mechanism to learn dynamic similarity between locations, and designs a periodically shifted attention mechanism to handle long-term periodic temporal shifts. Among them, T-GCN [11] combined GRU and GCN to model fine-grained features at each time step, which is close to our proposed STAHGNet cell in Figure 3. Recently, STWave [30] was proposed to take advantage of both the attention mechanism and convolution layer to learn the long-term trend. Nevertheless, it ignored global temporal and static spatial information, and whole graph modeling leads to more computational expense.\nBesides, Transformer-based methods show great power in sequence modeling. Informer [31] extended the self-attention mechanism and took KL-divergence [32] as the criterion to query dominant information in traffic flows. Local-sensitive hashing (LSH) was introduced by Reformer [33] to approximate attention by allocating similar queries. However, although plenty of Transformer-based methods [19, 34, 35] have shown superior performance compared to CNN or RNN-based models, they normally suffer from quadratic memory and runtime overhead."}, {"title": "2.2 Graph-based model", "content": "Recently, many studies have attempted to use graph neural networks to model the correlation of spatial-temporal sequences in TFP. Graph convolutional network (GCN) [11] enables extracting high-level features of target road nodes by aggregating information from neighbor nodes. Specifically, categorized by the graph type, there are two types of methods in general. Spectral-type GCN [36] extended graph conventional convolution operation by Laplacian spectrum in the spectral domain. However, it suffers from expensive computational costs due to the calculation of all the eigenvalues of the Laplacian matrix. Thus, ChebNet [37] was proposed to approximate the graph convolution by the Chebyshev-polynomial expansion of the eigenvalue diagonal matrix, and RGSL [15] was proposed to model explicit spatial relation graph and extract implicit temporal graph simultaneously by a Laplacian matrix mixed-up module. Concentrating on TFP, DSTAGNN [18] optimized the multi-head attention mechanism to capture dynamic spatial relevance and replace pre-defined static spatial graphs with learnable similarity graphs. However, extra computational cost brought by the implicit graph is inevitable and the built implicit graph is hard to interpret.\nAnother type of GCN is based on spatial-type graphs. In [38], the neighborhood information was directly summarized directly. The Spatial-type GCN can measure not only spatial relationships but also dynamic temporal correlations. STGCN [10] is a classical implementation of this type of GCN for temporal dependency, while its temporal GCN block cannot handle changes in graphs with the time flows. To tackle this problem in STG2Seq [13], a multiple-gated GCN was used as an encoder, and attention mechanisms were used as the decoder. Besides, STSGCN [39] was also proposed to compensate for the drawbacks in STGCN by introducing localized spatial-temporal information. But the graph encoders in both in STG2Seq and STSGCN remain GCN. To overcome the shortages of GCN in dynamic relationship modeling, Graph attention network (GAT) [40] used attention mechanisms to adjust the aggregation weights between nodes. Plenty of previous works deployed GAT as the dynamic dependency encoder [41, 42]. MS-GAT [43] modeled spatial-temporal dependency and channel relation in traffic flow by a coupling GAT framework. There were also some works [30, 44] harnessing the structural and semantic background knowledge inherent in both the traffic road network and historical traffic data using a spatiotemporal fusion graph. Nevertheless, many GCN-based methods fail to consider the fact that the correlations between sensors on the road network are dynamic and continuously evolving over time. Additionally, the notable memory and time consumption brought by the calculation among the full graph are always concerns in recent models."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Formulation", "content": "The summary and descriptions of important notations in this work are shown in Table 1, and the key acronyms in this work are summarized in Table 2. Firstly, we conceptualize the traffic road map as a graph G = (V,E), where V consists of N road nodes in this traffic network, and E is the set of connections between nodes."}, {"title": "3.2 Model Architecture", "content": "The overall architecture of the proposed STAHGNet is illustrated in Figure 2. Our model has three components: 1) Graph&Time series processing; 2) Time series encoder; 3) Predictor. To be more specific, the Graph&Time series processing aims"}, {"title": "3.2.1 Automative Feature Engineering with Domain Knowledge", "content": "In the traffic scenario, some research [18] validate that not all connections between road nodes reflect temporal dependency. Meanwhile, considering the high computational cost brought by GNNs [15, 18, 45], a random partial graph message passing is used in STAHGNet. Thus, in this component, we process the full graph structure into several subgraphs for each road node. Specifically, given one target road node v, we first collect all nodes with direct connections to v, and K neighbor nodes are reserved. For the target node having less than K neighbors, we compensate it for satisfying the demand"}, {"title": "3.2.2 Spatio-Temporal Aware Hybrid Graph Network", "content": "The spatial-temporal dependency is critical to accurately measuring the correlation and distance between road nodes [18]. Efforts of present state-of-the-art (SOTA) models concentrated on acquiring traffic temporal dependency given global spatial context and dynamic temporal graph structure, while there are still some challenges. Firstly, we argue that the continuously evolving temporal dependency graph learning is costly, and the implicit dependency without spatial structure between nodes is hard to interpret compared to the traffic road map. However, models considering only global or partially dynamic temporal dependency cannot capture fine-granularity correlation in traffic flows. In this section, we explain how to efficiently extract precise fine-granularity temporal dependency based on spatial graph structure.\nGiven flow series of target node X \\in R^{L \\times M} and its neighbor series G \\in R^{K \\times L \\times M}, K+1 independent STAHGNets are initialized for each series, and no parameter sharing between cells in each STAHGNet. The architecture of the STAHGNet cell is shown in Figure 3. Specifically, at the t time-stamp, the X(t) and G(t) are inputted into the first cell in their own encoder, respectively. For example, (x(t), x'(t), x''(t)) is mapped into a high-dimension space to generate a representation vector \\hat{h}(t) with size D. A state vector c(1) \\in R^{1 \\times D} is randomly initialized at the first time slot and keeps updating through the sequence. The spatial-temporal aware representation r(t-1) at the last timestamp is also considered as one of the inputs at t timestamp. Through a recurrent-based cell following Eq.(3), the state vector is updated to c(t) at each timestamp and prepared to enter the next cell. And the temporal information of this traffic flow at this time slot is represented by a vector h(t) \\in R^{1 \\times D}."}, {"title": "3.2.3 Hybrid Graph Attention Module", "content": "GAT was proposed to overcome shortages of GCN: 1) GCN cannot conduct inductive tasks at the dynamic graph (i.e., it is hard to handle unseen nodes in test phase); 2) The topological structure of nodes is static, which means the temporal dependency cannot be captured. However, there are still some weaknesses in conventional GAT. Firstly, in the implementation of GAT, an extra adjacency matrix is required for each training sample, and all nodes are expected to be put into the model at the same time, which means a higher space and time cost. For example, if the HGAT in STAHGNet is replaced by GAT, the required input will change from X \\in R^{L \\times M} and G \\in R^{K \\times L \\times M} to a larger sequence with a size of L \\times N \\times M. Besides, because of the property of the TFP task, the spatial information is normally consistent but significant for modeling correlation between nodes. Conventional GAT can only capture dynamic dependency but fails to maintain information from edges in the spatial graph."}, {"title": "3.2.4 Predictor and Loss Function", "content": "The predictor is implemented by a Multi-Layered Perceptron (MLP) [49] to generate forecasting value \\hat{y} at t+1. In detail, it consists of two linear neural layers. To robustly transform the nonlinear dependencies to the future traffic flow, a non-linear activation function and a dropout operation [50] are deployed between them as well. Through this MLP block, the output vector r'(t) of the target node encoder is transferred to the predict flow value \\hat{y}(t+1) at t + 1. The smooth L1 loss is chosen as the loss function L of STAHGNet because of its robustness in the regression task. It is defined as follows:\n\n\nLet \\Theta denote all learnable parameters in STAHGNet, the optimization objective function is defined as Eq.(10)."}, {"title": "4 Expriments", "content": "In our experiments, we used three evaluation metrics to assess the performance of the proposed STAHGNet and multiple baselines on four public traffic flow datasets. In subsection 4.1, we provide details of the experiment setting. The results of the comparison experiment are presented in subsection 4.2. Subsection 4.3 discusses the sensitivity of our proposal to various parameters. We also analyze the effect of each key component of STAHGNet in subsection 4.4. Additionally, we conducted extra case studies to visualize the captured dynamic dependencies among road nodes in subsection 4.5 and to evaluate the overall computational costs of our model in subsection 4.6."}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.1.1 Datasets", "content": "To evaluate the performance of STAHGNet, we select four public traffic datasets: PeMSD3, PEMSD4, PeMSD7, and PeMSD8 [39], which are both collected and issued by California Transportation Agencies Performance Measurement System\u00b9. The statistical summary of the datasets is shown in Table 3. The flow data is aggregated into a 5-minute window, and the average value is presented. There is no personally identifying information (e.g., demographic of driver) in datasets, but the spatial distance between connected nodes is provided. The detailed statistical data description is shown in Table 3."}, {"title": "4.1.2 Baseline Methods", "content": "Twelve classic or SOTA baseline methods are chosen to evaluate the effectiveness and superiority of the proposed STAHGNet. Within them, two categories can be specified: sequence models and graph models.\nIn sequence models, they concentrate on extracting temporal correlations among series. SVR [6], a linear support vector machine is used for regression tasks. ARIMA [4] is a classic statistics method modeling temporal dependencies within one series. FCLSTM [25] uses LSTM as the series encoder for prediction. In DSANet [17], the temporal and spatial correlations are captured by CNN and the self-attention mechanism, respectively. TCN [16] learns local and global temporal relations hierarchically. STFGNN [44] designs a dynamic time warping-based temporal graph to extract spatial relationships that are functionally aware. In ST-WA [19], a window attention scheme is used to reduce complexity but still maintain performance. MSSTRN [51] introduces a data-driven method for generating a weighted adjacency matrix, effectively capturing real-time spatial dependencies that are not adequately captured by predefined matrices.\nFor graph models, STG2Seq [13] designs a seq2seq architecture by a multi-gate GCN and attention mechanism. STGCN [10] exploits spatial-temporal aware GCN in traffic forecasting. STSGCN [39] utilizes a GCN to capture the localized spatial-temporal correlations synchronously. RGSL [15] fuse both implicit and explicit correlation graphs for forecasting. MS-GAT [43] exploits dynamic multi-aspect embeddings with TCN and attention mechanism to extract respective importance to prediction. DSTAGNN [18] combined multi-head attention mechanism and multi-scale gated convolution to capture dynamic spatial-temporal dependency. STWave [30] utilized a disentangle-fusion framework to mitigate the distribution shift in traffic data."}, {"title": "4.1.3 Implementation Setting", "content": "We adopt the Adam optimizer with a fixed learning rate of 0.0001 and the batch size is 64. The number of training epochs is set to 20, and the dimension of the hidden vector D = 64. In addition, to avoid overfitting, the ratio of dropout units is 0.1. To obtain the best parameter combination, we use the grid search to tune parameters on the validation dataset. The candidate hyper-parameter value is: K = [2, 4, 6, 8], w = [11,16], and the sampled neighbor hop is H = [1,2,3]. All experiments are conducted on a 64-bit Linux server with GPU: NVIDIA GeForce GTX 3090. STAHGNet and all baselines are implemented based on Python 3.8 and PyTorch 1.7.0 [52].\nTo fairly evaluate the performance of models, RMSE (Root Mean Square Error), MAE (Mean Absolute Error), and MAPE (Mean Absolute Percentage Error) are used"}, {"title": "4.2 Comparison Results and Analysis", "content": "Table 4 shows the results of the comparison experiment. Our STAHGNet consistently outperforms most baseline methods in all datasets, except for PeMSD4 and PeMSD8. In PeMSD4, the lowest MAPE is owned by RGSL, and the RMSE of STAHGNet is slightly higher (8.4%) than DSTAGNN.\nWe can observe that sequence models have worse performance overall than graph models. It proves that the GNN structure is more suitable for modeling topological dependency. The traditional methods (i.e., SVR, ARIMA) always perform worse than deep-sequence methods (i.e., FCLSTM, DSANet, TCN), potentially due to the limited capacity to capture non-linear and complex relationships in traffic flows in traditional methods. By contrast, deep-sequence models are more capable of exploiting non-linear feature extraction. However, within deep-sequence models, the model with higher complexity does not always outperform others. For example, TCN has a lower MAE (19.32) in PeMSD3 than DSANet and FCLSTM, while FCLSTM (21.33) performs better than TCN. This indicates that the high complexity of the model cannot always lead to better prediction performance.\nFurthermore, earlier graph models (i.e., STG2Seq, STGCN, STSGCN) used a GCN module to capture spatial and temporal dependency, which may explain their relatively worse performance compared to STSGCN. Specifically, the STSGCN is able to leverage the localized dependency and boost the prediction performance. Finally, no significant gaps were observed among the models based on SOTA methods (i.e., DSTAGNN,"}, {"title": "4.3 Parameter Sensitivity Test", "content": "Table 5 shows the interaction effects of adjusting the hop number H of sampled neighbors and historical window size w on both four datasets. In Table 5, the effect of hyper-parameter H is presented at the vertical axis, and the effect of hyper-parameter w can be seen at the horizontal axis. We notice that PeMSD3 has the overall lowest standard deviation (SD) in each metric, while PeMSD8 owns the highest SD. We suppose this is caused by the information volume of the dataset. The PeMSD8 has a"}, {"title": "4.4 Ablation Study", "content": "The HGAT component aggregates two types of key information: hybrid-granularity temporal dependency and static spatial dependency. For temporal dependency extraction, we vary the sampled number of neighbor nodes K from 0 to 10. Sampled 0 nodes also means there is no temporal dependency explored.\nSeeing from Table 6, as we increase the sampled number, the prediction performance does not become better. Particularly, the best result is normally given by K = 4, except for PeMSD7. The reason is that the training of STAHGNet is harder with more neighbors involved. A proper sampled number is crucial to balance the performance and complexity. Nevertheless, abandoning the temporal dependency extraction still results in the worst performance. This proves the necessity of the fine-granularity temporal dependency in HGAT. Besides, after we remove the As, a decline in all metrics is observed on all datasets. This experiment confirms the effectiveness of combining fine-granularity temporal and static spatial dependency in HGAT. Meanwhile, if we eliminate the effect of At, we find that the performance of this variant is between the complete STAHGNet and the variant without As. In general, removing As indicates the absence of spatial information. Therefore, we suppose that the introduction of the static spatial graph is necessary, and illustrates the importance of spatial features."}, {"title": "4.5 Visualization of Evolving Dependency", "content": "To investigate the effectiveness of our STAHGNet in modeling dynamic dependency, we select two datasets (i.e., PeMSD3 and PeMSD4) to conduct the case study. To present the dependency better, the smallest maximum connected graph in each dataset is chosen. As illustrated in Figure 6, Figure 6(a) and (b) present interaction matrix A(0), A(6), A(12) at three timestamps (i.e., 0, 6, and 12) for each dataset by heatmap, where the darker blue grid indicates higher weights. Figure 6(c) (d) displays the historical series curves.\nFirstly, as shown in Figure 6(a), the interaction score from stations 134 to 107 shows an increasing trend, and a lower interaction weight at timestamp 12 is observed. To look for validation, Figure 6(c) illustrates that when the values of nearby timestamps of nodes 107 and 134 get closer, the historical trends become more similar. Besides, for nodes 129 and 130, from time 0 to 6, there is no obvious trend change. But a notable"}, {"title": "4.6 Case Study of Computational Cost", "content": "In this section, a specific case study on computational cost is conducted on the PeMSD4. As illustrated in Table 7, to comprehensively verify the superior cost of our proposed model, both the space and time of SOTA methods are quantified for comparison. For fair comparisons, common hyper-parameters are kept consistent in all tested models.\nObserving Table 7, STAHGNet owns the lowest GPU memory occupation among all methods. Specifically, MS-GAT shows a comparatively best performance in space cost among SOTA methods, while STAHGNet takes less than a quarter of the space compared to MS-GAT. Besides, MS-GAT has the highest processing speed (4.19 it/s). We argue that the low space cost of MS-GAT is because of the ignorance of implicit similarity graph construction. Meanwhile, the employed convolutional flow encoders in MS-GAT bring a higher processing speed, while it ignored fine-grained dependency modeling in its structural design, which makes MS-GAT perform less. By contrast, although STAHGNet captures both fine and coarse-grained dependency and remains the superior performance in TFP, our proposed model still outperforms other SOTA models. Even compared to MS-GAT, STAHGNet only has a speed lower by 48.69%, which is significantly higher than the improvement in time-saving. Thus, we can conclude that our model maintains efficiency and performance well, and the effectiveness of feature engineering and random sampling for computational efficiency is validated as well."}, {"title": "5 Conclusion", "content": "In this paper, we proposed a novel data-driven recurrent framework STAHGNet for TFP. In particular, an HGAT component and a CTG generator are designed and integrated into each STAHGNet cell to capture heterogeneously HST dependency. To balance the accuracy and efficiency, we deploy feature engineering before the training phase to speed up convergence, and a random sampling strategy is utilized in the Spatial-temporal graph construction to save space occupation and computational complexity. Extensive experiments on four public TFP datasets prove our proposal outperforms existing SOTA methods in both prediction performance and computational-friendly. Besides, the effectiveness and necessity of each key component in STAHGNet are verified by ablation tests and two case studies. The proposed TFP system can precisely predict future traffic, further achieve dynamic traffic guidance, and enhance the operational efficiency of the traffic system and the ability to actively prevent and control congestion. Modeling both long-term and fine-grained dependencies in TFP is fundamental for the development of intelligent transportation systems.\nThe effectiveness of the random neighborhood road point sampling proposed in this paper also demonstrates the sparseness of the information contained in the traffic road network. In the future design of TFP systems, in addition to the need to model hybrid granularity spatio-temporal dependencies simultaneously, random neighbor road node sampling can be beneficial in building efficient and lightweight TFP systems.\nHowever, given the precise dependency extraction at each timestamp, it is challenging to optimize the training time. Therefore, it is valuable to explore a more efficient way to implement such a complex recurrent model. Future work should focus on designing an alternative neural network structure that combines the benefits of recurrent networks in sequential modeling with the parallel modeling capabilities of transformer structures. Additionally, integrating the proposed STAHGNet or other TFP models into the intelligent transportation system, and deploying them in real environments efficiently and effectively will be an important future research direction of the TFP model."}, {"title": "Statements and Declarations", "content": "\u2022 Conflict of interest/Competing interests The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\u2022 Data availability The four public traffic datasets (i.e., PeMSD3, PeMSD4, PeMSD7, and PeMSD8) are available at https://pan.baidu.com/s/1ZPIiOM__r1TRlmY4YGlolw with password p72z. And all data supporting the findings of this study are available within the paper.\n\u2022 Code availability The code of this work will be released when it is prepared well."}]}