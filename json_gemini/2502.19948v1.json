{"title": "DYNAMIC DROPCONNECT: ENHANCING NEURAL NETWORK\nROBUSTNESS THROUGH ADAPTIVE EDGE DROPPING\nSTRATEGIES", "authors": ["Yuan-Chih Yang", "Hung-Hsuan Chen"], "abstract": "Dropout and DropConnect are well-known techniques that apply a consistent drop rate to\nrandomly deactivate neurons or edges in a neural network layer during training. This paper\nintroduces a novel methodology that assigns dynamic drop rates to each edge within a layer,\nuniquely tailoring the dropping process without incorporating additional learning parameters.\nWe perform experiments on synthetic and openly available datasets to validate the effectiveness\nof our approach. The results demonstrate that our method outperforms Dropout, DropConnect,\nand Standout, a classic mechanism known for its adaptive dropout capabilities. Furthermore,\nour approach improves the robustness and generalization of neural network training without\nincreasing computational complexity. The complete implementation of our methodology is\npublicly accessible for research and replication purposes at https://github.com/ericabd888/\nAdjusting-the-drop-probability-in-DropConnect-based-on-the-magnitude-of-the-gradient/.", "sections": [{"title": "Introduction", "content": "Dropout [1, 2] and DropConnect [3] are prominent techniques designed to mitigate overfitting in deep neural networks.\nDropout functions by independently zeroing each neuron within a layer with a predetermined fixed probability p. In\ncontrast, DropConnect takes a slightly different approach by randomly eliminating an edge within the network with\na fixed probability p. This makes DropConnect a generalization of Dropout; specifically, removing a single neuron\nas performed in Dropout equates to eliminating all incoming and outgoing edges associated with that neuron, which\nDropConnect facilitates.\nBoth Dropout and DropConnect uniformly apply a fixed dropping rate across all neurons or edges within a layer.\nHowever, this universal dropping rate might not always represent the optimal strategy; ideally, a model should utilize\navailable data-driven insights to tailor the dropping rate for each individual edge or neuron based on their specific\ncharacteristics.\nTo address this, we introduce a novel methodology termed DynamicDropConnect (DDC). This approach dynamically\nassigns a drop probability to each edge based on the magnitude of the gradient associated with that edge. The under-\nlying principle is that edges with larger gradients are crucial for learning and should be retained, whereas it might be\nacceptable to omit those with minimal impact on the model's output occasionally.\nDDC offers several advantages over other methods such as Standout [4], which also employs a dynamic dropping rate.\nFirstly, DDC does not require any additional learning parameters, which simplifies the model architecture and reduces\nmemory requirements during training. Secondly, DDC provides a more deterministic and transparent approach to\ndeciding the dropping rate, as opposed to the potentially opaque and unpredictable learning processes used by methods\nlike Standout. Moreover, empirical evidence from our experiments demonstrates that DDC achieves superior accuracy\ncompared to Standout."}, {"title": "Related Work", "content": "Overfitting occurs when a model excels during the training phase but performs inadequately on unseen test data. A\ntypical approach to counter overfitting involves incorporating regularization terms into the objective function, such as\nthe L1 or L2 norms of the learnable parameters. These regularization terms are designed to encourage the development\nof simpler relationships between features and targets, thus mitigating the risk of overfitting. Early stopping is another\nfrequently adopted strategy to prevent overfitting. This technique involves ceasing the parameter updates once the loss\non the validation set begins to show signs of increase, thereby avoiding the overfitting of the training data. Additional\nwell-established methods to combat overfitting include data augmentation [5], employing model ensembles [6], and\nmodifying the architecture of neural networks by reducing their depth or width. These methods have been proven to\nbe effective in enhancing the generalizability of models across unseen datasets.\nDropout is another widely recognized method specifically utilized in neural networks to curb overfitting [2]. During\ntraining, Dropout randomly deactivates a subset of neurons, ensuring that the neural network does not become overly"}, {"title": "Methodology", "content": "This section introduces DropConnect as the background knowledge, followed by our methodology to assign dynamic\ndropping rates."}, {"title": "Preliminary: DropConnect", "content": "Let l \u2208 {1, . . ., L} be the L hidden layers in a neural network and y(l) be the output of layer l (and therefore the input\nof layer l + 1), each layer of a neural network transforms y(l\u22121) to y(l) by Equation 1.\n\\(y^{(l)} = f_l(z^{(l)}) = f_l(W^{(l)}y^{(l-1)}),\\)\nwhere W(l) is the set of parameters in layer l, and \\(f_l(\\cdot)\\) is the activation function of the same layer.\nDropConnect assigns a universal dropping rate p such that each edge has a probability p of being turned off during\ntraining. Therefore, a neural network, with DropConnect, transforms y(l\u22121) to y(l) by the equation below during\ntraining.\n\\(y^{(l)} = f_l(z^{(l)}) = f_l(((1 - M^{(l)}) \\odot W^{(l)}) y^{(l-1)}),\\)\nwhere M(l) = [mi,j] is a (0, 1)-matrix whose shape is the same as W(l); each entry mi,j is sampled from a Bernoulli\ndistribution with a fixed hyper-parameter p, and \\(\\odot\\) performs the Hadamard product (i.e., element-wise product) of\nmatrices. Thus, M(l) is a mask matrix that decides which parameters in W(l) (edges) to omit."}, {"title": "DDC \u2013 Mask Generation", "content": "The proposed DDC advances DropConnect by allowing variable dropping probabilities for distinct neural network\nedges.\nWe create a mask matrix, M(l) to drop edges. Each entry \\(m_{i,j}^{(l)}\\) in M(l) is sampled from a Bernoulli distribution with\na unique parameter, \\(p_{i,j}^{(l)}\\). An edge is omitted if \\(m_{i,j}^{(l)} = 1\\)."}, {"title": "DDC - Training Re-calibration and Testing", "content": "Since M(l) is the mask for the parameters in layer l, the effective parameters in this layer go from W(l) to (1 \u2212 M(l))\nW(l). The dropping mechanism is only applied in training but not in prediction. However, if we set the value of y(l)\nas \\(f_l ((1 - M^{(l)}) \\odot W^{(l)} y^{(l-1)})\\) in training but predict y(l) as \\(f_l (W^{(l)} y^{(l-1)})\\) in inference, the learned W(l) from\ntraining cannot be used directly in the tests. To fix the inconsistency, we need to recalibrate \\((1 - M^{(l)}) \\odot W^{(l)}\\) during\ntraining, so that the learned W(l) in training can be used directly during inference by \\(f_l (W^{(l)} y^{(l-1)})\\).\nSince M(l) is a (0, 1)-matrix, \\(\\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} m_{i,j}^{(l)}\\) is the number of 1-s in M(l). So, we can compute the expected value\nof the dropping rate of layer l by Equation 5.\n\\(r^{(l)} \\gets \\frac{\\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} m_{i,j}^{(l)}}{n_1 \times n_2}\\)\nwhere n1 and n2 are the number of rows and the number of columns of the masking matrix M(l).\nWe re-calibrate the output of layer l during training such that the learned weights W can be used during inference.\nThe re-calibration is achieved by dividing the output by the keep rate:\n\\(y^{(l)} \\gets f_l (W^{(l)} y^{(l-1)}) \times \\frac{1}{1 - r^{(l)}} = f_l (W^{(l)} y^{(l-1)}) \times \\frac{1}{e^{(l)}}.\\)\nThe mask matrix M(l) and the masked parameter matrix W(l) are only used in training. At inference, we use only\nthe unmasked parameter matrix W(l) for forwarding: \\(y^{(l)} \\gets f_l (W^{(l)} y^{(l-1)})\\)."}, {"title": "Experiments", "content": "This section introduces the experiment settings and results. All the models are implemented by PyTorch and trained on\nthe NVIDIA GTX 3090. We conducted experiments based on one synthetic dataset and four open datasets: MNIST,\nCIFAR-10, CIFAR-100, and NORB. We split the labeled instances for each open dataset into the training set, the\nvalidation set, and the test set. Detailed settings, such as parameter initialization, learning rate, and batch size, are\nincluded in the experimental code."}, {"title": "Experiments on the Synthetic Dataset", "content": "We generate a synthetic dataset to analyze the learning process of various edge-dropping strategies. For each instance\ni, we sample two independent features \\(x_1^{(i)}\\) and \\(x_2^{(i)}\\), each \\(x_j^{(i)} \\sim N(0, 1)\\). We fix the values of w1 and w2 and let the"}, {"title": "Experiments on Open Datasets", "content": "The previous section shows that DDC helps linear models learn faster. This section explores DDC's capacity for\nmore complicated deep learning models using four open datasets: MNIST [11], CIFAR-10, CIFAR-100 [12], and\nNORB [13].\nIn addition to the baselines introduced in Section 4.1, we include Dropout, DropConnect, and Standout for comparison.\nWe adjust the dropping rate \\(r^{(l)}\\) in Algorithm 2 to be close to the dropping probability of the compared baselines\nwhenever possible.\nWe test the convolutional neural network used in the DropConnect paper [3] (called SimpleCNN below). In addition,\nwe add two more complicated networks -AlexNet [5] and VGG [14] \u2013 for comparison."}, {"title": "Conclusion", "content": "This paper presents a straightforward yet effective methodology for dynamically adjusting the dropping rate of the\nedges in a neural network. This approach avoids the complexity of introducing additional learning parameters and\nsimplifies the implementation process. We demonstrate the parameter updating process through a series of experiments\nusing a synthetic dataset and multiple open datasets and compare our proposed methodology against Dropout and its\nvarious adaptations. The results of these experiments indicate that our method surpasses traditional approaches in\nnearly all scenarios.\nThe efficacy of gradient magnitude as a reliable indicator for setting the dropping rate has been validated through our\nresults, suggesting a promising direction for future enhancements. Building on this foundation, we propose to develop\na model that algorithmically uses gradients as features to determine optimal dropping rates. We hypothesize that this\nadvanced strategy could potentially elevate the model's predictive accuracy even further. However, it's important to\nnote that this approach would introduce additional parameters that require learning, which could extend the training\nduration and increase computational demands.\nFurthermore, we are interested in delving into a more rigorous theoretical analysis of our methodology. Previous stud-\nies, such as those referenced in [7], have explored the connection between Dropout techniques and Bayesian learning.\nInspired by this, we are interested in investigating potential theoretical links between our dynamic edge dropping\napproach and Bayesian inference principles. This exploration could offer deeper insights into the probabilistic founda-\ntions of our method and possibly reveal new theoretical reasoning that could explain why our Dynamic DropConnect\nmethod enhances model robustness and generalizability more effectively than traditional Dropout techniques."}]}