{"title": "Predicting from Strings: Language Model Embeddings for Bayesian Optimization", "authors": ["Tung Nguyen", "Qiuyi Zhang", "Bangding Yang", "Chansoo Lee", "Jorg Bornschein", "Yingjie Miao", "Sagi Perel", "Yutian Chen", "Xingyou Song"], "abstract": "Bayesian Optimization is ubiquitous in the field of experimental design and blackbox optimization for improving search efficiency, but has been traditionally restricted to regression models which are only applicable to fixed search spaces and tabular input features. We propose Embed-then-Regress, a paradigm for applying in-context regression over string inputs, through the use of string embedding capabilities of pretrained language models. By expressing all inputs as strings, we are able to perform general-purpose regression for Bayesian Optimization over various domains including synthetic, combinatorial, and hyperparameter optimization, obtaining comparable results to state-of-the-art Gaussian Process-based algorithms. Code can be found at github.com/google-research/optformer/embed_then_regress.", "sections": [{"title": "1. Introduction", "content": "A fundamental component of all value-based search methods is regression, in which proposed solutions are filtered by predictions on their performance, before evaluation. By utilizing an accurate regression model, or regressor, along with balanced explore-exploit mechanisms, large improvements to the sample complexity of search have been widely possible. However, many regression methods so-far have been task-specific, due to the reliance of modelling assumptions and dimensionality constraints. Learning-based regression methods are particularly susceptible to this issue, due to their reliance on fixed-length tensors for input representation.\nRecent progress in large language models (LLMs) have demonstrated the flexibility and versatility of representation of information as strings, which allow for a wider range of data formats to be encoded for subsequent processing. The potential of LLMs for universal learning-based regression is considerable, allowing for regressors that can be generalized across multiple tasks, thereby mitigating the task-specific limitations of current methods.\nIn this work we focus on improving the flexibility of regressor-guided search methods through the use of LLM-based embeddings, which map arbitrary strings to fixed-length vectors to be used in downstream tensor-based regressor models, such as an in-context learning (ICL) based Transformer. Specifically, our contributions are:\n\u2022 Our framework \u201cembed-then-regress\u201d is one of the first to allow the use of free-form string representations in Bayesian Optimization, by using language models to embed trial inputs as features for in-context regression.\n\u2022 By pretraining a Transformer-based regressor at scale over a large variety of offline evaluation"}, {"title": "2. Related Work and Motivation", "content": "Bayesian Optimization refers to a class of techniques which use regressors for solving blackbox optimization problems, by suggesting best candidates according to an explore-exploit tradeoff. For a given objective, the speed of optimization relies heavily on the regressor's underlying prior, or assumptions about the nature of the objective, such as its smoothness and landscape. Largely dominated by the use of Gaussian Process (GP) regressors, the field of Bayesian Optimization has thus seen a rise in works which seek to learn better prior GP hyperparameters such as length-scales and kernel amplitudes based on offline pretraining or manually designed feature representations for combinatorial objects, while keeping underlying kernel definitions fixed.\nNumerous end-to-end neural network-based approaches such as the use of attention mechanisms and Transformers have been introduced to allow more learnable behaviors, and we refer the reader to which provides a general reference on their use for blackbox optimization. Relevant to our particular case of regression, works such as demonstrated the benefits of using raw Transformers as in-context regression models, or neural processes, with others establishing provable guarantees. Similarly, demonstrated that Transformers trained on synthetic data as \"prior-fitted networks\u201d are capable of Bayesian inference, leading to their use in Bayesian optimization.\nUnfortunately, as both raw Transformers and GPs require fixed dimensional features, this limits their applications to inputs expressable as tabular features for e.g. hyperparameter tuning, or task-specific embeddings for e.g. chemistry. Further works have attempted to improve the flexibility of regression-modeling through the use of token-based representations, which allows regressors to be simultaneously used over various types of input formats. Since the context-window of Transformers still remain the most expensive limitation, a useful organization of recent works can be based on their treatment of the total sequence length, roughly equal to:\n(number of trials) \u00d7 (average trial token length)  (1)\nAmong sequence-to-sequence methods which pretrain for blackbox optimization specifically, uses custom tokenizations to minimize trial token length in order to maximize trial count. However, this is restricted to very constrained search spaces (e.g. flat hyperparameter spaces), and lacks flexibility in utilizing arbitrary forms of data. In contrast, allows arbitrary string representations, but the large token length of each trial severely limits the trial count allowed in the context window, forcing the use of alternative but tedious methods of absorbing online data, such as inference-time fine-tuning.\nOther methods use text-to-text chat-based services such as ChatGPT and Gemini to demonstrate their emergent capabilities for in-context regression, but such methods lack the ability to pretrain over large amounts of offline evaluations. Efforts in ICL-based reward modeling with chat-based LLMs allow fine-tuning over chains of thought, but have only used coarse discretized scores of e.g. {\u22121, 0, 1} rather than highly precise numeric predictions over vastly different scales."}, {"title": "3. Method", "content": "For optimization, we require a regressor with all of the following capabilities:\n\u2022 Pretrainable over offline evaluations to allow effective meta-learning.\n\u2022 Flexible representation of inputs with raw strings for application in multiple domains.\n\u2022 Allow long-range in-context regression using multiple previous evaluations.\n\u2022 Production of precise numeric predictions over diverse objective scales.\nThis naturally leads to the use of embedding-based methods which can compress any string representation of a candidate into a feature vector, using only a single unit of sequence length when sent to a ICL model such as a raw Transformer. This can be seen as a form of encoding and decoding, which has been shown to produce competitive results over pure language modeling tasks.\n3.1. Preliminaries\nLet f : X \u2192 R be a real-valued function over a search space X. The goal of blackbox optimization is to produce an x* which maximizes f:\nx* = arg max f (x) (2)\nx\u2208X\nWe define a regressor as a prediction model which can output a distribution of prediction values for f(\u00b7) over a query point x, given the history {xs, ys}=1 of trajectory of t evaluations over f so far. Such regressors may also be learnable over additional offline data besides the given history.\nDuring inference, the regressor may be turned into an acquisition function a : X \u2192 R to represent explore-exploit tradeoffs. We assume the existence of a (potentially task-dependent) acquisition optimizer which can quickly and cheaply sample suggestions x \u2208 X, usually in the form of an evolutionary algorithm. The history-dependent acquisition at+1(\u00b7) may thus be used to filter out poor samples, or used in an entire Bayesian optimization loop in which the acquisition optimizer is used to find xt+1 := arg maxx\u2208x at+1(x) as the next x-proposal.\n3.2. In-context Transformer Regressor\nAn embedding-based regressor uses an embedder \u00a2 : X \u2192 Rd to map a suggestion x to a fixed-length representation x \u2208 Rd, which can then be used as a regular feature vector for a numeric regression model. A string-based embedder first represents x as a string, which is then passed to a language model for embedding. We specifically use the typical definition of language model embedding, in which we apply a forward pass of the underlying model (encoder or decoder) on the (tokenized) string representation to obtain all token logits in RL\u00d7d, and then average-pool across the length axis to obtain a vector in Rd. We discuss specific string representations in our experiments in Section 4.\nFor our underlying regression model, we then use an additional Transformer, by sending in as input sequence (X1 \u2295 \u04ef\u2081), \u2026 \u2026 \u2026, (X\u2081 \u2295 \u04ef\u2081) where \u04ef \u2208 Rd is the feature representation of the float y after applying a trainable projection, and \u3121 \u2295 \u04ef is the trial representation expressed as the concatenation of and y. In order to obtain a prediction for a query point x, we may then further append a query (\u3121\u22950) to the history input sequence where \u014d is a dummy value, and following a forward pass of the Transformer where (\u3121 \u2295 \u00d5) attends to all previous trials, post-process the corresponding t + 1-th output feature with a parametric output distribution over R. For our case we"}, {"title": "y-Normalization:", "content": "Depending on the function, y-value outputs may contain a wide variety of scales, and need to be normalized properly. We can define a normalization procedure parameterized by a history of objectives (y1, ..., yt), also applicable to incoming target values. These steps consist of, in order: (1) Shifting objectives to have zero mean and divide by standard deviation. (2) Reducing harmful effects of bad outliers by fitting the \u201cbad half\u201d of objectives {yi \u2264 ymedian} to a normal curve, using percentiles as z-scores. (3) Linearly scale y \u2190 y\u2212ymin where ensures all historical y-values within [0, 1] and apply additional damping (e.g. sigmoid or log transform) to target values significantly outside this range.\n3.3. Pretraining and Inference\nDenote a task T = (f, X) as a specific objective function over a particular search space.\nPretraining: We assume a collection of offline training tasks {T1, T2, . . .}, with different search spaces and objective functions, with each task containing its own collection of evaluated trials {xs, ys}_1 where T is the (potentially task-specific) offline trajectory length."}, {"title": "4. Experiments", "content": "While the embedder is frozen, we pretrain the weights \u03b8 of the ICL regression Transformer, over all such offline evaluation data. Each training example consists of a sampled task and history cutoff length t' \u2208 [0, T) so that {xs, ys}s\u2264t' is considered a history, while {xt'+i, Yt'+i}'+i are target points, with the loss computed as the sum of prediction losses over all targets, i.e.\nT\u2212t' \u03a3 lo(xt'+i,yt'+i;{xs,ys}s=1 (3)\ni=1 where le(x, y; {xs, ys}_1) is the negative log-likelihood using our Gaussian output distribution, of predicting y given x and history {xs, ys}=1\nInference: At inference, we use our mean and deviation head to form a UCB-based acquisition at+1(x) = \u03bct+1(x) + \u221a\u03b2\u00b7 \u03c3t+1(x) where \u221a\u00df is a problem-dependent constant. We use a (potentially domain-dependent) zeroth-order optimizer such as evolutionary search to maximize this acquisition, and thus only require forward passess, although gradient-based acquisition maximization is possible with soft-prompt optimization techniques.\nSince there may be distributional shifts for parameter names encountered between pretraining and inference, we may either apply data augmentation by randomizing parameter names during pretraining, or transform the search space during inference to match those encountered in pretraining.\n3.4. Model Details\nIn this paper, to demonstrate the validity of our approach on relatively low compute budgets, we intentionally use relatively smaller language model embedder sizes in comparison to the larger GPT or Gemini family of models. Specifically, we use a pretrained T5-XL encoder (1B parameters), based on the encoder-decoder T5-family of models. Along with only 8 layers of the underlying regression Transformer, this leads to a maximum required training budget of approximately 16 GPUs for training and 1 GPU for inference, possible with most academic budgets. Further details with respect to model sizes and training hyperparameters can be found in Appendix A.\nThe cheap inference cost is also necessary when the acquisition function may be called thousands of times by a zeroth-order acquisition optimizer per candidate proposal. It is worth noting that time and memory complexity costs may even further be reduced using efficient Transformers. Faster embedders lead to large constant factor reductions, while faster regressors can lead to linear \u014c(t) complexities with respect to the number of trials.\n4.1. End-to-End Blackbox Optimization\nTo emphasize the broad applicability of our Embed-then-Regress method, our emphasis is not on achieving state-of-the-art results compared to domain-specific baselines, but rather demonstrating its general applicability across a variety of tasks. We outline possible improvements within specific domains in Section 5. We evaluate the performance of our algorithm on various problems consisting of synthetic, combinatorial, and hyperparameter optimization (exact details in Appendix B). All curves are plotted with mean and 0.5 deviation as error bars over 10 repeats.\nSynthetic Optimization: In common optimization scenarios, the search space is a flat Cartesian product of float and categorical parameter types. Our string-based regression will represent each x"}, {"title": "Combinatorial Optimization:", "content": "We further benchmark over combinatorial objectives whose search spaces are typically difficult to regress over. Many of these can be found in common operations research literature, e.g. permutation-based (Travelling Salesman, Quadratic Assignment, Flowshop Scheduling, and N-Queens), and choice-based (submodular maximization problems such as covering and log-determinant functions).\nEach of these problems can be parameterized by a set of coefficients (e.g. city locations for Travelling Salesman, matrix entries for log-determinant). Note that we are in the bandit setting, in which these coefficients are hidden from the algorithm and the only feedback is the final objective. Similar to before, we thus can also generate offline pretraining data by randomizing these coefficients and problem sizes, and evaluating over random candidates. For our string regression, we may simply use JSON over indices; e.g. {[0] :2, [1]:0, [2]:3, [3]:1} for a permutation space of size 4, e.g. {[0]:1, [1]:3} for a choice space.\nWhile there are few previous works using GPs for e.g. permutation spaces, they require constructing very domain-specific kernels and complex acquisition optimizers (e.g. semi-definite programming) making them difficult to reproduce. We thus use a simpler optimizer such as Regularized Evolution which does not need modelling assumptions other than implementing random mutations between trials and can be used broadly. We also empirically found this was better than other evolutionary alternatives such as NSGA-II or hill-climbing.\nRather than fully optimizing the acquisition which be slow for large-scale evolutionary searches, we can simply apply best-of-many sampling by using the regressor's UCB acquisition to rank sample candidates proposed by evolution, and suggest only the best. In Figure 4, we see that this boosts exploration over the original Regularized Evolution, which can often get stuck at local optima early on.\nHyperparameter Optimization: With the advent of industry-wide hyperparameter tuning services, large amounts of evaluations over expensive but realistic experiments can be stored offline. To efficiently benchmark our method over objectives encountered in the wild, we use surrogate-based benchmarking which has shown to be effective for providing realistic yet cheap compar-isons between different algorithms without using huge amounts of compute. Without surrogates, benchmarking could take weeks in wall-clock time and hundreds to thousands of GPU-hours."}, {"title": "4.2. Ablations", "content": "In this subsection, we ablate different effects on the model's prediction ability, which directly affects optimization performance. We perform comparisons using a variety of predictive metrics, consisting of negative log-likelihood (NLL), mean average error (MAE), R-Squared, and mean absolute calibration error (MACE).\nString Embedder Size: In Figure 7, we see that the size of the pretrained string embedder has a monotonic influence on the predictive performance over BBOB evaluations. As we vary the T5 embedder sizes (Small, Large, XL), there is a clear trend across all predictive metrics computed over normalized y-values.\nIt is interesting to note that larger encoders, which are pretrained over mostly English text, lead to better predictive performance over BBOB representations which do not contain any English words. Considering that the embedder's weights are also frozen, this trend potentially suggests that larger language models inherently provide better features even for numeric data formats.\nICL Transformer Size: In Figure 8, we find that the ICL Transformer size also plays a role, where higher layer sizes lead to better predictive outcomes. In contrast to the string embedder, here the ICL model's weights are trainable, and thus larger models can potentially possess higher capacities and better inductive biases to train over the same offline data.\nOverall in both cases for Figures 7 and 8, we verify in-context regression occurring for different test functions, where more context points leads to better predictive performance."}, {"title": "5. Conclusion and Future Work", "content": "Our method, Embed-then-Regress, demonstrates the versatility of using string-based in-context regres-sion for Bayesian Optimization over a variety of problems. We have shown it to obtain comparable results against industry-standard GP baselines and allow flexibility in more esoteric spaces such as permutations and combinations.\nAs strings are significantly more flexible representation formats of different data types, an ambitious and exciting direction is to pretrain a unified in-context regression model over multiple different domains, in order to obtain a \u201cuniversal\u201d in-context regressor. Numerous architectural improvements could also be made. For example, the aggregation method over Transformer outputs may be learned rather than predefined using average pooling as in this paper, and more broadly there may be better methods for computing fixed-length embeddings from a Transformer. It may also be possible to additionally create a string-based GP by sending string embeddings as inputs to a kernel, for better guarantees of uncertainty estimates.\nFurther possible applications include prompt optimization and code search, areas which still predominantly use zeroth-order evolutionary algo-rithms or even random search, which can be very sample inefficient compared to Bayesian Opti-mization. Additionally, outside of blackbox optimzation problems which are stateless with respect to inputs, it is worth investigating whether such methods are applicable for process-based reward modelling and tree search-based approaches for stateful environments in language modelling."}]}