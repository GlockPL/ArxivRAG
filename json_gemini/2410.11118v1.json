{"title": "MoonMetaSync: Lunar Image Registration Analysis", "authors": ["Ashutosh Kumar", "Sarthak Kaushal", "Shiv Vignesh Murthy"], "abstract": "This paper compares scale-invariant (SIFT) and scale-variant (ORB) feature detection methods, alongside our novel feature detector, IntFeat, specifically applied to lunar imagery. We evaluate these methods using low (128x128) and high-resolution (1024x1024) lunar image patches, providing insights into their performance across scales in challenging extraterrestrial environments. IntFeat combines high-level features from SIFT and low-level features from ORB into a single vector space for robust lunar image registration. We introduce SyncVision, a Python package that compares lunar images using various registration methods, including SIFT, ORB, and IntFeat. Our analysis includes upscaling low-resolution lunar images using bi-linear and bi-cubic interpolation, offering a unique perspective on registration effectiveness across scales and feature detectors in lunar landscapes. This research contributes to computer vision and planetary science by comparing feature detection methods for lunar imagery and introducing a versatile tool for lunar image registration and evaluation, with implications for multi-resolution image analysis in space exploration applications\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Image registration is aligning two or more images of the same scene, captured at different times, from varying viewpoints, or by different sensors such as high- and low-resolution payloads. This technique is essential for ensuring consistency and accuracy when analyzing and comparing multiple images, making it foundational in fields like remote sensing, medical imaging, and computer vision.\n\nFeature detection is a crucial step in image registration, with algorithms like SIFT (Scale-Invariant Feature Transform) and ORB (Oriented FAST and Rotated BRIEF) being widely used. SIFT has long been favored for its robustness in detecting distinctive key points and its ability to handle scaling, rotation, and illumination changes. However, SIFT's computational complexity can be a bottleneck, particularly in real-time applications.\n\nORB improves upon SIFT by offering a faster and more efficient alternative. It combines the FAST keypoint detector with the BRIEF descriptor, adding rotational invariance and robustness to noise. ORB also incorporates orientation information, making it rotation-invariant like SIFT, while its binary BRIEF descriptor ensures a significantly lighter computational load. This makes ORB suitable for real-time applications such as feature matching in robotics and mobile devices, where speed and efficiency are critical. However, while ORB is computationally faster, it may sacrifice some of the rich feature representation that SIFT provides.\n\nOur contribution, IntFeat, aims to bridge this gap by taking the best elements from both SIFT and ORB. IntFeat retains the richness of SIFT's features while optimizing for the speed and efficiency of ORB, making it suitable for real-time applications without compromising on the quality of feature extraction."}, {"title": "II. RELATED WORK", "content": "Image calibration of satellite imagery is crucial for ensuring data quality and enabling accurate scientific analysis across various applications in remote sensing and earth observation. Broadly image calibrations of two types radiometric [3] and geometric, radiometric corrects for variation in pixel sensitivity and converts raw digital numbers into meaningful physical units. The latter is used to correct for lens distortions and determine camera parameters. Further, geometric calibrations can be of two types intrinsic (determines internal camera parameters like focal length and optical center) and extrinsic (determines camera position and orientation in 3D space). Geometric calibration is used in different tasks like digital breast tomosynthesis (DBT) [4], systems with inconsistent imaging capability [5], Camera-projector 3D imaging systems [6], etc. Geometric calibrations apply different approaches: projection matrix approach [4], uncertainty estimation [5], simultaneous multi-sensor calibration [7] (calibrating multiple sensors and determining their misalignment simultaneously), line-based iterative calibration [8] and simulations camera-projector calibration [6].\n\nGeometric calibration precedes image registration in many applications and provides the necessary parameters for accurate image registration, especially in multi-camera or 3D reconstruction scenarios. Image registration geometrically aligns two or more images of the same scene. This alignment is crucial for various applications in computer vision, medical imaging, remote sensing, and more. The process typically involves detecting and matching distinctive features across images and then estimating the transformation needed to align them. Early approaches included rigid transformations (rotation, scaling, and translation) [9] and manual registration. Further, these"}, {"title": "III. METHODOLOGY", "content": "The dataset was acquired from ISRO PRADAN servers and includes imagery from the Chandrayaan-II orbiter's two imaging payloads: Terrain Mapping Camera (TMC-2) and Orbiter High-Resolution Camera (OHRC). TMC-2 provides a resolution of 5 meters, while OHRC offers a much finer resolution of approximately 30 cm. Although TMC-2 has covered more than half of the lunar surface, its images are of lower resolution compared to the limited but higher-resolution coverage provided by OHRC.\n\n1) Matching OHRC and TMC-2 Images: To leverage the strengths of both imaging payloads, we aligned the OHRC images with the corresponding regions in the TMC-2 dataset. Using the pygeodesy library, we matched the corner coordinates of OHRC images within the broader TMC-2 image frames. Through this process, 6 unique TMC-2 images and 18 OHRC images were generated, ensuring that the two datasets aligned geographically.\n\n2) Image Preprocessing: The preprocessing steps began with interpolating pixel coordinates around the corner points of OHRC within the corresponding TMC-2 images. We defined a bounding box for the common patch between both image sets, masking the non-overlapping areas. In total, 16 OHRC images were matched with 6 TMC-2 images. We used geodesic interpolation to map pixel coordinates at every 100th pixel inside the bounding box for both image types. The Haversine distance was calculated between the corner coordinates of OHRC and each pixel inside the TMC-2 bounding box to ensure precise spatial matching. Finally, affine transformations were applied to the OHRC images to align them accurately with the TMC-2 images, ensuring that both datasets correctly represented the same lunar terrain patches."}, {"title": "B. Image Registration", "content": "After collecting and processing the dataset from the OHRC and TMC-2 satellite payloads, we concentrated on upscaling methods, specifically bilinear (Figure 4) and bicubic (Figure 5) interpolation techniques, which played a critical role in our subsequent analysis. Bilinear interpolation uses the 4 nearest neighbors to estimate pixel values, while bicubic interpolation utilizes a 4x4 neighborhood for smoother and more accurate pixel value estimation.\n\n1) Feature Detection: In our analysis, we explored various feature detection methods and developed a novel approach, IntFeat, which combines the strengths of existing techniques such as SIFT and ORB.\n\nIntFeat (Proposed Method): The IntFeat feature detection method presents a robust approach by integrating the strengths of SIFT's scale invariance and ORB's computational efficiency.\n\nAlgorithm 1 IntFeat Algorithm for Image Registration\nprocedure INTFEAT(img1, img2)\nEXTRACT SIFT and ORB key points and descriptors\nfrom both img1 and img2\nCOMBINE SIFT and ORB key points for both images\nREDUCE the dimensionality of SIFT descriptors to\nmatch ORB using Principal Component Analysis (PCA)\nUse BRUTE-FORCE matcher to find the nearest\nmatches between the descriptors of both images\nCOMPUTE homography using the RANSAC algorithm\nbased on matched key points\nAPPLY perspective transformation using the homogra-\nphy matrix to align the images\nreturn The registered image\nend procedure\n\n2) Feature Matching: BF (Brute-Force) Matcher In the BF matcher, each feature descriptor from the first set is compared to all feature descriptors in the second set using a chosen distance metric, and the closest match is selected.\n\n3) Transformation Model: The homography matrix represents the transformation between two planes. It is a 3x3 matrix with 8 degrees of freedom, estimated using the RANSAC (RANdom SAmple Consensus) algorithm to ensure robustness against outliers.\n\nC. SyncVision\nSyncVision is a Python package for image processing that allows users to compare two images using different image registration methods. The package supports SIFT, ORB, and IntFeat registration methods and provides metrics such as PSNR and SSIM to evaluate the quality of the registered images."}, {"title": "IV. RESULTS", "content": "TableI compares the performance of different feature detection methods (SIFT, ORB, and IntFeat) using two interpolation"}, {"title": "V. DISCUSSIONS", "content": "Adapting to the unique characteristics of lunar imagery presents significant challenges, primarily due to sharp shadow boundaries and high-contrast regions that lead to substantial brightness variations. These conditions complicate the effectiveness of feature detection algorithms. One concern is the noise levels introduced by the IntFeat detector. The low-level feature extraction of ORB tends to amplify noise, particularly in the form of artifacts. This amplification is exacerbated during the smoothing of textural details in the image interpolation process, impacting the overall performance.\n\nAdditionally, IntFeat's limitations are highlighted by its struggle to handle the combination of high noise levels, extreme contrast differences, homogenous textures, and fluctuating lighting conditions. These factors collectively hinder the detector's ability to effectively integrate both low- and high-level features, resulting in sub-optimal performance in challenging lunar environments."}, {"title": "VI. CONCLUSIONS & FUTURE WORK", "content": "In general image processing tasks, bicubic interpolation typically outperforms bilinear interpolation due to its reliance on a larger 4x4 neighborhood for smoother transitions. However, this advantage does not carry over to lunar images, where the presence of inherent noise and sharp contrast boundaries limits its effectiveness. The custom feature detector, IntFeat, demonstrated performance comparable to the baseline methods but did not exceed the accuracy of existing detectors.\n\nThe current implementation highlights the need for further refinement, particularly in filtering key points and developing more robust descriptors that can better handle the unique challenges of lunar imagery, such as high-contrast regions and elevated noise levels. Future work will focus on improving these aspects to enhance feature detection in the demanding environment of lunar surfaces."}, {"title": "APPENDIX", "content": "The performance of dehazing methods is evaluated by the following equation of SSIM score between two images:\n$$SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + C_1) (2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}$$\nwhere:\n\\mu_x and \\mu_y are the average of x and y respectively.\n\u03c3x and \u03c3y are the variance of x and y respectively.\n\u03c3xy is the covariance of x and y.\nC1 = (k1L)2 and C2 = (k2L)2 are two variables to stabilize the division with weak denominator; L is the dynamic range of the pixel-values (typically this is 2bits_per_pixel \u2013 1), k\u2081 = 0.01 and k\u2082 = 0.03 by default."}, {"title": "B. Peak Signal-to-Noise ratio (PSNR)", "content": "Peak Signal-to-Noise Ratio (PSNR) is a widely used metric for evaluating the quality of reconstructed images or videos compared to the original, reference data. It is expressed in decibels (dB) and is calculated based on the mean squared error (MSE) between the original and the reconstructed images. The formula for PSNR is given by:\n$$PSNR = 10 log_{10} (\\frac{MAX^2}{MSE}),$$\nwhere MAX is the maximum possible pixel value of the image (for example, 255 for 8-bit images), and MSE is the mean squared error, defined as:\n$$MSE = \\frac{1}{mn} \\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1} (I(i, j) \u2013 K (i, j))^2,$$\nwhere I(i, j) represents the pixel value at position (i, j) in the original image, and K(i, j) represents the pixel value at the same position in the reconstructed image. Higher PSNR values generally indicate better reconstruction quality, as they imply a lower MSE and thus less distortion. PSNR is particularly useful for comparing the performance of different image processing algorithms in tasks such as image compression, denoising, and super-resolution."}, {"title": "1) Mean Average Precision (mAP)", "content": "For object detection performance, we are using mean Average Precision (mAP):\n$$AP = \\frac{\\sum_{k=1}^{n}(P(k) \u00d7 rel(k))}{number of relevant documents}$$\nwhere:\n P(k) is the precision at cutoff k in the list.\n rel(k) is an indicator function equaling 1 if the item at rank k is a relevant document, 0 otherwise.\n n is the number of retrieved documents.\n The mean Average Precision is then calculated as:\n$$mAP = \\frac{\\sum_{q=1}^{Q} AP_q}{Q}$$\nwhere APq is the Average Precision for the qth query and is the total number of queries."}]}