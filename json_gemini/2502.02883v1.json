{"title": "SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions", "authors": ["XIAOFAN YU", "LANXIANG HU", "BENJAMIN REICHMAN", "DYLAN CHU", "RUSHIL CHANDRUPATLA", "XIYUAN ZHANG", "LARRY HECK", "TAJANA \u0160IMUNI\u0106 ROSING"], "abstract": "Natural language interaction with sensing systems is crucial for enabling all users to comprehend sensor data and its impact on their everyday lives. However, existing systems, which typically operate in a Question Answering (QA) manner, are significantly limited in terms of the duration and complexity of sensor data they can handle.\nIn this work, we introduce SensorChat, the first end-to-end QA system designed for long-term sensor monitoring with multimodal and high-dimensional data including time series. SensorChat effectively answers both qualitative (requiring high-level reasoning) and quantitative (requiring accurate responses derived from sensor data) questions in real-world scenarios. To achieve this, SensorChat uses an innovative three-stage pipeline that includes question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) for intuitive human interactions and to guide the sensor data query process. Unlike existing multimodal LLMs, SensorChat incorporates an explicit query stage to precisely extract factual information from long-duration sensor data. We implement SensorChat and demonstrate its capability for real-time interactions on a cloud server while also being able to run entirely on edge platforms after quantization. Comprehensive QA evaluations show that SensorChat achieves up to 26% higher answer accuracy than state-of-the-art systems on quantitative questions. Additionally, a user study with eight volunteers highlights SensorChat's effectiveness in handling qualitative and open-ended questions.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the number of Internet of Things (IoT) devices in daily life has grown rapidly, including smart-phones, smart home accessories, and smart city infrastructures [12, 27]. These devices generate a massive amount of multimodal sensor data, with total production expected to reach 80 ZB by 2025 [25]. Traditional research has focused on developing classification models for tasks such as human activity recognition [9, 43, 44, 62, 64, 66]. In contrast, latest studies have recognized the importance of natural language interactions, especially for making sensor data more accessible and useful to elderly people or patients with Alzheimer's [7, 10, 26, 41, 63, 67]. These systems typically operate in a Question Answering (QA) framework: users ask questions, the sensor system processes both the query and the underlying sensor data, and finally delivers an adequate answer. The questions can be qualitative, requiring high-level reasoning about a user's lifestyle or well-being (e.g., \u201cHow is my work-life balance?\"), or quantitative, demanding quantitatively precise answers based on sensor data (e.g., \"How long did I walk?\"). QA offers a natural and flexible way for users to interact with and gain insights from sensors, going beyond simple classification models.\nHowever, existing QA systems for sensors face significant limitations, particularly with short time windows or low-dimensional sensors. Most current systems can only process data over short periods, such as one minute in\""}, {"title": "2 RELATED WORK", "content": "Question Answering using Sensor Data The QA problem has been extensively studied across various domains, including text [52], visual [54], medical [45], and remote sensing [23]. In the sensor domain, early works Al Therapist [40] and its successor CaiTI [39] utilized smart home devices, such as Amazon Echo, to engage in conversations with users and assess mental well-being. DeepSQA [63] was the first to benchmark time-series sensor-based QA for human activity recognition. It introduced SQA-GEN, an automated QA generation tool that gathers 1-minute sensor readings and generates valid Q&A pairs by exhaustively searching six pre-defined question templates. They mainly focused on quantitative questions including time query, counting and action compare. The authors also evaluated traditional neural network models, including CNNs and LSTMs, finding that the ConvLSTM network with Compositional Attention achieved the highest QA accuracy.\nRecent contributions have pioneered the integration of LLMs for generating more intuitive answers and better interpreting sensor data. Englhardt et al. [16], Health-LLM [29], and DrHouse [67] converted physiological data from wearable devices, such as heart rates and daily step counts, into text prompts for LLMs, enabling more sophisticated medical and healthcare diagnoses. The latest Sensor2Text [10] and PrISM-Q&A [7] explored natural language interactions between users and wearable devices to understand and support daily activities using sensor data, such as advising on \"What should I do next with this?\" Both approaches utilized LLMs as their backbone and fed sensor embeddings into the models.\nWhile promising, existing systems exhibit significant limitations in handling long duration and complex sensor data required for accurate answers. Table 1 highlights the key differences between SensorChat and existing QA systems. In summary, SensorChat greatly enhances capabilities of existing systems by (1) answering questions based on long-duration sensor data spanning weeks or months, compared to the short windows of seconds or minutes in previous systems [7, 10, 63], and (2) encoding high-dimensional time-series data to extract fine-grained activity details for reasoning, unlike prior systems that are limited to low-dimensional sensor data [16, 29, 67].\nLLMs for Multimodal Reasoning Recent works investigated multimodal LLMs that transform other data modalities into a sequence of tokens for LLM integration [32, 71]. IMU2CLIP [38] and TENT [74] employed contrastive pretraining to align text with various timeseries sensor signals. Recent designs like AnyMAL [37] and OneLLM [21] proposed fine-tuning multimodal LLMs to process up to eight different modalities, including IMU"}, {"title": "3 BACKGROUND AND MOTIVATION", "content": "In this section, we formally define the problem we address and present a motivating study highlighting the limitations of state-of-the-art QA techniques."}, {"title": "3.1 Problem Statement", "content": "We target real-world QA scenarios in which users carry mobile sensing devices during their daily routines, with minimal attention to the sensing process. In this context, sensor data is collected continuously over an extended period in natural, uncontrolled environments. During this process, users may pose arbitrary, personalized questions about the sensor data, ranging from inquiries about a single day to those spanning several weeks, and expect precise answers derived from the collected data.\nFormally, the problem is formulated as generating a proper answer given the question and sensory inputs, denoted as x. The questions can be arbitrary, including qualitative and quantitative queries, while the answers are expected to accurately address users' concerns. The key distinction between our problem formulation and existing QA approaches lies in the format of x, which consists of long-duration high-dimensional data collected up to the moment the question is asked. As shown in Table 1, this makes our problem significantly more challenging."}, {"title": "3.2 Limitations of SOTA Methods", "content": "With a concrete problem setup, we conduct a motivating study to understand the limitations of state-of-the-art systems on this problem. We then discuss the lessons learned which inspire the design of SensorChat.\nDataset Setup We choose the SensorQA dataset\u00b9 [50] to simulate long-term real-world QA interactions between humans and sensing devices. To our knowledge, SensorQA is the first and only existing QA dataset that matches our target scenario, i.e., in-the-wild, long-term, time series sensor data collection with practical questions that align with users' interests."}, {"title": "4 SENSORCHAT DESIGN", "content": ""}, {"title": "4.1 Overview of SensorChat", "content": "To address the challenge of accurate QA over long-duration sensor data, we design a novel three-stage pipeline in SensorChat, consisting of question decomposition, sensor data query, and answer assembly. Specifically, we use LLMs in question decomposition and answer assembly, recognizing their essential roles in correctly interpreting user queries and generating natural language answers. The intermediate sensor query stage is the key component of SensorChat. First, the query stage uses a pretrained sensor encoder to effectively encode high-dimensional and long-duration multimodal sensor timeseries into meaningful embeddings. Second, the query stage performs a similarity search in the embedding space to retrieve all sensor information relevant to the original question. This ensures precise extraction of sensor context, making a significant contribution to the accuracy of the final answer. To the best of the authors' knowledge, SensorChat is the first system to incorporate an explicit sensor data query stage for accurately handling sensor-based language tasks in long-term monitoring.\nThe complete pipeline of SensorChat is illustrated in Fig. 4. For example, given a question like, \u201cHow long did I exercise last week in the morning?\u201d (1 in Fig. 4), SensorChat first decomposes the question and generates specific sensor data queries using LLMs. These queries include details such as the context of interest, date and time ranges, and the summarization function (\u2462). In this case, the query might specify a context of \"do exercise,\u201d a time span of \"last week,\u201d a time of day of \u201cmorning\u201d and a summarization function of CalculateDuration. To enhance the precision of question decomposition, SensorChat uses solution templates (\u2461) with carefully designed prompts. Next, the sensor query stage encodes the activity text \u201cexercise\u201d into the embedding space (4) and retrieves sensor embeddings that are sufficiently similar to the text embedding (5). These sensor embeddings are encoded offline from the full-history raw sensor data. Both the sensor and text encoders are pretrained offline to align their outputs in the same embedding space, ensuring accurate sensor data retrieval. Therefore, only text encoding and similarity-based query searches are performed online. Additional properties, such as the date (\u201clast week\") and time of day (\"morning\"), are used to constrain the sensor query range. The final step of the sensor data query stage involves summarizing the relevant sensor embeddings into a textual context with a summarization function (6). For instance, with the CalculateDuration function, the summarization step calculates the total duration of the retrieved sensor embeddings. This results in a sensor context such as: \u201cAmong all days last week, you exercised for 35 minutes on Monday morning and 55 minutes on Thursday morning.\u201d The summarization function is identified during question decomposition. Finally, the original question and the sensor context are fed into the answer assembly stage, where a fine-tuned LLM generates the final answer (\u2466). With the precise sensor context, the model produces an accurate response, such as: \u201cYou exercised a total of 1 hour and 30 minutes last week.\u201d (8)."}, {"title": "4.2 Contrastive Sensor-Text Pretraining for Partial Contexts", "content": "The sensor and text encoders are crucial for effective sensor-text fusion, serving as a \"bridge\" between high-dimensional sensor timeseries and semantic text. A high-quality sensor encoder ensures that SensorChat can accurately and comprehensively retrieve relevant sensor embeddings during the sensor data query stage. Similarly, a robust text encoder is essential for handling the arbitrary contextual text generated during the question decomposition stage.\nHowever, achieving this level of alignment poses significant challenges due to the lack of suitable techniques. Existing pretraining methods, such as CLIP and its variants [38, 47], are effective for pretraining encoders with paired inputs, i.e., one piece of sensor data and one sentence. Unfortunately, encoders trained in this manner struggle to accurately identify similar sensor embeddings when provided with partial or arbitrary context instead of complete sentences. For instance, using CLIP, a piece of sensor data might align well with the full sentence \"The person is sitting and working on computers at school.\" However, when a user is specifically interested in a partial context like \"working on computers,\" the encoded text embedding may not closely match the original sensor embeddings, leading to reduced accuracy in sensor data query as we show in Sec. 6.6. To address this challenge, we introduce a novel contrastive sensor-text pretraining loss for partial contexts.\nWe pretrain our model on a large-scale multimodal sensor dataset with annotations, where each sample {xt, wt} consists of raw time series sensor data x\u2081 collected at time t and an associated set of partial context labels wt. wt can be extracted from label annotations. For instance, in a single-label human activity classification dataset, we may contain a single phrase (e.g., {\"standing\"}), whereas in a multi-label dataset, it may include multiple phrases (e.g., {\"at school\", \"working on computers\"}). We employ separate encoders for sensor and text inputs. Formally, let e denote the complete sensor encoder model. The encoded sensor embedding is given by z = 0(xt). The text encoder, denoted by 6, maps arbitrary phrases to a text embedding z = $(w\u2081). Key notations used throughout our work are summarized in Table 3. The details of the sensor encoder are shown in Fig. 5. We use distinct sensor encoder for each sensor modality (e.g., IMUs, audio and phone status) to accommodate the varying complexity of different sensor data types. For instance, a Transformer-based encoder is used for high-dimensional time series data while a simple linear layer is designed for encoding phone status. The outputs from these modality-specific encoders are concatenated and passed through a fusion layer to generate the final sensor embedding. Our framework allows for missing modalities by padding with mean values and is flexible for future expansion to additional modalities.\nWe introduce a new pretraining loss to align sensor embeddings z and partial text embeddings z. Different from CLIP [47], our loss function enables alignment between sensor data and all partial phrases, defined as\n$$L = \\frac{1}{|w_t|} \\sum_{w \\in w_t}  -log \\frac{exp(z_t^\\intercal \\cdot z_w / \\tau)}{\\sum_{a \\in A(w)} exp(z_t^\\intercal \\cdot z_a / \\tau)}$$\n|w\u2081| is the cardinality of set w\u2081, \u03c4 is a scalar temperature parameter. The term A(w) = A \\ {w} represents the collection of negative contexts, defined as all possible phrases except the positive phrase w. The intuition behind our loss function is illustrated in Fig. 5. Consider a sensor embedding x\u2081 and a set of partial text labels w\u2081 = {sitting, working on computers}. Our loss function encourages high similarity between the sensor embedding z and all positive text embeddings corresponding to wt, namely \"sitting\" and \"working on computers\", while distinguishing them from other negative contexts such as \"walking\" or \"at school\". We draw inspiration from the supervised contrast learning loss [28]. However, our loss function differs in that it explicitly models the similarity between sensor embeddings and multiple text phrases rather than between samples of the same modality. After pretraining, we store all sensor embeddings in a database for online queries."}, {"title": "4.3 Three Stages in SensorChat", "content": "In this section, we explain the detailed designs in each stage of SensorChat, including question decomposition, sensor data query and answer assembly. All stages need to work accurately, robustly and collaboratively to achieve natural and precise answer generation in the end."}, {"title": "4.3.1 Question Decomposition", "content": "The question decomposition stage processes the user question and identifies specific query triggers for the sensor data query stage, such as the context of interest, date and time ranges, and the summarization function, as illustrated in the leftmost box in Fig. 4. The primary challenge in designing this stage lies in generating accurate decompositions for arbitrary user inputs, including various questions types as discussed in Sec. 3. We choose to rely on LLMs for this stage due to their outstanding capability in handling natural language inputs. However, LLMs are not without limitations as they can produce erroneous outputs or hallucinations [24], especially in our case where a dedicated dataset for similar decomposition tasks is unavailable.\nTo address this challenge, we propose a few-shot learning approach to prompt pre-trained LLMs, enhanced with in-context learning [6, 55] and chain of thought techniques [13, 35]. An example prompt is illustrated in Fig. 6a, where LLM is instructed to mark different arguments with distinct symbols, such as \u201c\u00ab\u201d and \u201c\u00bb\u201d for function names. This helps in accurately extracting various arguments from the LLM output. In SensorChat, we accommodate a variety of real-life scenarios by extracting contexts (such as activities), dates, times of day, and summarization functions during the query process, as shown in Fig. 6a. SensorChat supports multiple extracted phrases to handle complex questions such as \u201cHow long did I work at school on Monday and Tuesday?\u201d SensorChat is designed to be flexible, allowing for future expansion with additional decomposition terms.\nWe next explain more details on the in-context learning and chain-of-thought techniques used in SensorChat, both for ensuring high-quality question decompositions.\nIn-Context Learning (ICL) integrates a few examples directly into the prompt during inference, enabling LLMs to adapt effectively to specific tasks without requiring fine-tuning [6, 55]. Building on this insight, we design"}, {"title": "4.3.2 Sensor Data Query", "content": "The sensor query stage is the core module of SensorChat, responsible for searching the sensor database and extracting relevant information, which directly impacts the accuracy of SensorChat's answers. Existing sensor-based QA systems did not employ an explicit search module, restricting to low-dimensional sensor features [16, 29, 67] or fixed windows of sensor signals [7, 10, 37, 38, 63]. In contrast, SensorChat introduces this explicit query stage to handle long-duration, fine-grained timeseries sensor signals and provide accurate information to queries, which is not possible with prior QA designs.\nThe sensor and label encoders are pretrained offline as explained in Sec. 4.2. With the pretrained sensor encoder, the full-history raw sensor signals are converted into an embedding database beforehand. During online user interactions, as shown in the middle part of Fig. 4, the sensor query stage only needs to encode the context (e.g., activity) generated in the question decomposition stage using the text encoder (\u2463 in Fig. 4), performs a"}, {"title": "4.3.3 Answer Assembly", "content": "As shown in the right box of Fig. 4, the final stage of answer assembly integrates question and sensor information to generate the final answer. State-of-the-art methods [38, 63, 71] rely on \u201cblack-box\" fusion of natural language and sensory data, often leading to ineffective fusion and inaccurate answers (see Sec. 3). In contrast, SensorChat summarizes query results to text and directly fuses them with the question in the prompt, as illustrated in Fig. 6c. Our intuition is that, in contrast to processing and fusing with other modalities, LLMs are the most professional in dealing with text. SensorChat is capable of answering both qualitative and quantitative questions by combining the original question and the extracted fine-grained activity information from long-duration, high-dimensional sensors. At this answer assembly stage, we finetune a LLM such as LLaMA [71] to adapt the model to the desired answer style. Fine-tuning is chosen over few-shot learning as it delivers better performance with the presence of high-quality datasets like SensorQA [50] (see Sec.6.6). We use Low Rank Adaptation (LoRA) [22] due to its parameter efficiency and comparable performance to full fine-tuning."}, {"title": "5 SYSTEM IMPLEMENTATION", "content": "We implement SensorChat on real-world systems. We envision SensorChat as a personal assistant that provides accurate and timely answers to user questions, as outlined in our problem statement (Sec. 3.1). Fig. 7 visualizes the general system pipeline of SensorChat. We employ smartphones and smartwatches to collect multimodal sensor data from users in daily lives (1) in Fig. 7a). Implemented based on the ExtraSensory App [59], the mobile devices automatically gathers data for 20 seconds every minute, including 40Hz IMU signals, 13 MFCC audio features from a 22kHz microphone, and other phone state information including compass, GPS location, Wi-Fi status, light intensity, battery level, etc. The details can be found in the ExtraSensory App manual [59]. These sensor data are then transmitted to a system running SensorChat (\u2461 in Fig. 7a). We offer two variants of SensorChat, designed for a cloud server and an edge environment respectively. Their detailed implementations and trade-offs are discussed below. Finally, users can interact with SensorChat directly through a chatting interface using natural language, shown as \u2462 in Fig. 7a.\nSensorChatc and SensorChate We offer two system variants of SensorChat, as shown in Fig. 7b and 7c.\n\u2022 SensorChatc, designed for a cloud environment, uses GPT-3.5-Turbo [69] for question decomposition and a full-size finetuned LLaMA2-7B model [56] for answer assembly. We deploy and test SensorChat on a cloud server equipped with an NVIDIA A100 GPU [3].\n\u2022 SensorChate, designed for an edge environment, uses quantized LLaMA model [56] for both question decomposition and answer assembly. The question decomposition model is quantized from the official LLaMA3-8B, while the answer assembly model is quantized from our fine-tuned version of LLaMA2-7B. We use Activation-aware Weight Quantization (AWQ), a state-of-the-art quantization method for LLMs, known for its hardware efficiency. We deploy and test SensorChate on a NVIDIA Jetson Orin NX module [2] with 16GB RAM.\nSensorChatc and SensorChate accommodate two typical use scenarios. SensorChatc is expected to deliver superior QA performance with the full-precision LLMs in the cloud, at the cost of intensive resource consumption. Additionally, SensorChatc requires the users to transmit the full sensor history to the cloud server. On the other hand, SensorChate runs entirely on a local edge platform belonging to the user, eliminating the need to transmit user data to the cloud and thus preserving user privacy. However, its QA and latency performance degrade compared to SensorChatc.\nImplementation Details The algorithm part of SensorChat is implemented with Python and PyTorch [46]. For the offline sensor encoder, SensorChat uses a Transformer architecture [61] with 6 encoder layers, 8 attention heads, and a feedforward network size of 2048 for time series data. For the low-dimensional phone status data, SensorChat employs a fully connected layer as the encoder. The fusion layer is also implemented as a fully connected layer. The label encoder is initialized from the pretrained CLIP ViT-B/32 label encoder [47]. Both"}, {"title": "6 EVALUATION ON STATE-OF-THE-ART DATASET", "content": "In this section, we thoroughly evaluate SensorChat on the state-of-the-art dataset focusing on quantitative questions. We further validate SensorChat in a real world study with open-ended, qualitative questions in Sec. 7."}, {"title": "6.1 Dataset and Metrics", "content": "In this evaluation section, we focus mainly on the SensorQA dataset [50] and quantitative questions. A detailed introduction to SensorQA [50] is provided in Sec. 3. To the best of our knowledge, SensorQA is the first and only available benchmarking dataset for QA interactions that use long-term timeseries sensor data and reflect practical user interests. While we focus on SensorQA [50] in this section, we emphasize that the motivation and design of SensorChat are broadly applicable and can be extended to other practical sensing applications. To ensure the best alignment between the QA pairs and sensor information, we conduct offline encoders pretraining on the ExtraSensory multimodal sensor dataset [58], which servers as the sensor data source for SensorQA. During pretraining, all sensor samples are aligned by a time window of 20 seconds.\nDataset Variants and Metrics We evaluate three versions of SensorQA [50] using various metrics to assess both the quality and accuracy of the generated answers.\n\u2022 Full answers refer to the original full responses in SensorQA. We evaluate the model's performance on the full answers dataset using Rouge-1, Rouge-2, and Rouge-L scores [17]. Rouge scores measure the overlap of n-grams between the machine-generated content and the ground-truth answers, expressed as F-1 scores. Higher Rouge scores indicate greater similarity between the generated and true answers.\n\u2022 Short answers are the 1-2 key words extracted from the full answers by GPT-3.5-Turbo [69], offered with the original SensorQA dataset [50]. We use the exact match accuracy on the short answers to evaluate the precision of generated answers, as detailed in Sec. 3.\n\u2022 Multiple choices are generated by prompting GPT-3.5-Turbo [69] to create three additional choices similar to the correct short answer. An example QA can be \"Which day did I spend the most time with coworkers? A. Friday, B. Monday, C. Thursday, D. Wednesday\", with the correct answer being \"D\" or \"D. Wednesday.\" The models are expected to accurately select the correct answer from the four candidates. We evaluate the performance based on exact answer selection accuracy.\nWe create the multiple-choice version in addition to the full answers and short answers provided in the original SensorQA dataset [50], to further assess the model's ability in distinguishing similar facts based on sensor data. We use different dataset variants to assess various aspects of the models. The full answers dataset evaluates overall language quality, while the short answers and multiple-choice evaluations focus on the model's ability to learn underlying facts rather than relying solely on patterns in language token generation. Further discussion of the evaluation metrics can be found in Sec. 8."}, {"title": "6.2 State-of-the-Art Baselines", "content": "We compare SensorChat against several state-of-the-art baselines that leverage different modalities combinations, including text, vision+text, and sensor+text data. These comparisons highlight the effectiveness of integrating multiple modalities to address the sensor-based QA tasks. We consider both closed-source and open-source baselines for a comprehensive analysis.\nWe first evaluate the state-of-the-art closed-source generative models using various modalities:\n\u2022 GPT-3.5-Turbo [69] and GPT-4 [5] are text-only baselines taking only the questions as inputs.\n\u2022 GPT-4-Turbo [5] and GPT-40 [5] are vision+text baselines taking the activity graphs and the questions as inputs. For all vision+text baselines, we feed the activity graphs in SensorQA [50], similar to Fig. 2, along with the questions into the model.\n\u2022 IMU2CLIP+GPT-42 [38] is the state-of-the-art sensor+text GPT baseline as explained in Sec. 3.\nFor these closed-source generative models, we use few-shot learning (FSL). Specifically, we incorporate a set of QA examples from SensorQA [50] into the prompt for each question input. We adopt 10 samples per question based on a grid search of {2, 5, 10, 15}.\nWe further conduct comprehensive evaluation with the state-of-the-art open-source models using various modalities:\n\u2022 T5 [49] and LLaMA\u00b3 [56] are popular text-only language models.\n\u2022 LLaMA-Adapter\u2074 [71] is a recent vision+text framework offering a lightweight method for fine-tuning instruction-following and multimodal LLaMA models. It integrates vision inputs (i.e., activity graphs from SensorQA [50]) with LLMs using a transformer adapter. We utilize the latest LLaMA-Adapter V2 model.\n\u2022 LLaVA-1.55 [33] represents state-of-the-art vision+text model. LLaVA connects pre-trained CLIP ViT-L/14 visual encoder [47] and large language model Vicuna [11], using a projection matrix. LLaVA-1.5 [33] achieves state-of-the-art performance on 11 benchmarks through simple modifications to the original LLaVA and the use of more extensive public datasets for finetuning.\n\u2022 DeepSQA [63] trains a CNN-LSTM model with compositional attention to fuse sensor+text modalities and predict from a fixed and limited set of candidate answers given questions and IMU signals. We adapt their implementation to use the full-history timeseries data as input, to align with SensorChat's setup.\n\u2022 OneLLM7 [20] is a state-of-the-art multimodal LLM framework that processes sensor+text modalities using a universal pretrained CLIP encoder and a mixture of projection experts for modality alignment. We adapt their implementation and feed the full-history timeseries data from the IMU tokenizer.\nAll baselines based on LLaMA (that is, LLaMA, LLaMA-Adapter, and OneLLM) use LLaMA2-7B [56]. For T5 and DeepSQA, we train the models directly on the SensorQA dataset [50]. For the LLM baselines, we apply LoRA fine-tuning (FT) [22] using the samples from SensorQA [50]. For all methods that require training, we randomly select 80% of the QA pairs in SensorQA [50] as training samples and reserve the remaining 20% for testing. We explore alternative splitting schemes in Sec. 6.8 to demonstrate SensorChat's generalizability to unseen users. All baselines adopt the same hyperparameters as those specified in their official codebases. We did not compare with the latest works of Sensor2Text [10], PrISM-Q&A [7], and DrHouse [67] due to limited access to their open-source code and models."}, {"title": "6.3 Qualitative Performance", "content": "We illustrate a qualitative comparison of SensorChat with the top-performing baselines in Fig. 8. Specifically, Fig. 8 (a) focuses on time-related queries and (b) focuses on counting questions, which are the most challenging for SOTA methods given the long-duration time series sensor inputs (see Sec. 3). Key phrases in the answers are"}, {"title": "6.4 Quantitative Performance", "content": "Table 4 presents the quantitative results of all methods on the three variants of SensorQA [50]: full answers, short answers, and multiple-choice. Note that DeepSQA [63] was not evaluated on the multiple-choice version due to its inability to handle dynamic answer choices. It is important to highlight that exact match accuracy for short and multiple-choice answers is a strict metric, as it requires the model to generate answers in the exact same form as the correct ones. For instance, \"4 hours\" and \"3 hours 50 min\" would be considered different. Answering multiple-choice questions can also be particularly challenging, as candidate answers differ only slightly, such as \"A. 10 min\" vs. \"B. 20 min,\" making them difficult to distinguish. In practical applications, however, a QA system does not necessarily need to achieve perfect exact match accuracy to be useful. We leave the exploration of more advanced metrics that better align with user satisfaction for future work, as discussed in Sec. 8."}, {"title": "6.5 End-to-End Answer Generation Latency", "content": "To evaluate efficiency, we measure the end-to-end answer generation latency of SensorChate and SensorChate on their respective platforms - cloud server and NVIDIA Jetson Orin. For a fair comparison, we quantize all LLM-based baselines to 4-bit weights using AWQ [31] for the Jetson Orin experiments, matching SensorChate 's setup. SensorChat takes an average generation latency of 2.3s on the cloud and 10.0s on the Jetson Orin. Specifically, SensorChat takes an average of 1.2s for question decomposition, 0.5s for sensor data query, and 0.6s for answer assembly. SensorChat\u0119 takes an average of 2.5s, 4.9s, and 2.5s for each stage, respectively."}, {"title": "6.6 Ablation Studies", "content": "In this section, we comprehensively examine the impact of key design choices in SensorChat. Without loss of generality, we use SensorChatc as the base model.\nImpact of Each Stage in SensorChat We first evaluate the individual contribution of each stage in SensorChat by removing one of them from the pipeline. By removing question decomposition, we use a fixed and general decomposition templates for all questions. Removing sensor data query reverts the model to a language-only approach. By removing answer assembly, we directly output the queried sensor context from the second stage. Table 5 summarizes the results on full and short answers including both quality and quantitative accuracy. As observed, removing any stage leads to a significant performance drop. In SensorChat, all three stages must work collaboratively to deliver high-quality, accurate answers across diverse question types in SensorQA. Among the three stages, the answer assembly stage has the most significant impact, as it directly influences the final output. Removing it results in severely degraded performance, with near-zero accuracy on short answers. However, the question decomposition and sensor data query stages are equally crucial.\nImpact of Sensor Features and Pretraining Loss Functions We next evaluate the impact of sensor features and loss functions during offline encoder pretraining. Specifically, we compare using statistical features (e.g., mean acceleration) versus raw time series inputs, and IMU2CLIP loss [38] versus our proposed contrastive sensor-text pretraining loss for partial context (see Sec.4.2). For IMU2CLIP, text samples are generated by combining all valid labels into one sentence. We report the online querying accuracy and multiple-choice answer accuracy to assess the influence to sensor information extraction. As shown in Table 6, statistical features result in low querying and answer accuracies. This validates our motivation to design SensorChat that high-dimensional time series"}, {"title": "6.7 Sensitivity Analyses", "content": "Fig. 10 shows the sensitivity of key parameters in SensorChat. while the less sensitive ones are omitted due to space limitation. The default parameter setting is the same as described in Sec. 5. We mainly focus on evaluating the short answers to assess the parameters' impact on factual information extraction.\nLearning Rate in Answer Assembly Fig. 10 (leftmost) shows the short answers accuracy for learning rates of {1e5, 5e \u2013 5, 1\u0435 4, 2e \u2013 4, 5e \u2013 4} during finetuning. Larger learning rates result in bigger gradient steps during LoRA finetuning, with 1e - 4 providing the best performance for our task."}, {"title": "6.8 Generalization and Robustness", "content": "We evaluate SensorChat's generalization and robustness across different users' sensor data and QA inputs. In addition to the standard 80/20 random split", "SensorChat": "LLM fine-tuning in answer assembly and sensor-text encoder pretraining.\nFig. 11a presents the short-answer accuracy when fine-tuning on all users' QA data versus only the first 48 users. The results show similar accuracy for both seen and unseen users, demonstrating SensorChat's strong generalizability in answer assembly. This is likely due to SensorChat's design of treating answer assembly as a pure language task. Since all users' language tokens follow similar distributions in a sensor-based QA task, generalization remains robust across user variations.\nFig.11b presents the online querying accuracy when pretraining with all users' sensor data versus only the first 48 users. Unlike language fine-tuning, limiting sensor data to the first 48 users leads to accuracy degradation on unseen users due to variations in data distributions. Therefore, improving SensorChat's generalization to new users primarily depends on developing a robust sensor and text encoder, which is a well-studied problem in existing literature [64"}]}