{"title": "XAI-FUNGI: Dataset resulting from the user study on comprehensibility\nof explainable AI algorithms", "authors": ["Szymon Bobek", "Paloma Koryci\u0144ska", "Monika Krakowska", "Maciej Mozolewski", "Dorota Rak", "Magdalena Zych", "Magdalena W\u00f3jcik", "Grzegorz J. Nalepa"], "abstract": "This paper introduces a dataset that is the result of a user study on the comprehensibility of explainable artificial intelligence\n(XAI) algorithms. The study participants were recruited from 149 candidates to form three groups representing experts in the\ndomain of mycology (DE), students with a data science and visualization background (IT) and students from social sciences and\nhumanities (SSH). The main part of the dataset contains 39 transcripts of interviews during which participants were asked to\ncomplete a series of tasks and questions related to the interpretation of explanations of decisions of a machine learning model\ntrained to distinguish between edible and inedible mushrooms. The transcripts were complemented with additional data that\nincludes visualizations of explanations presented to the user, results from thematic analysis, recommendations of improvements\nof explanations provided by the participants, and the initial survey results that allow to determine the domain knowledge of the\nparticipant and data analysis literacy. The transcripts were manually tagged to allow for automatic matching between the text\nand other data related to particular fragments. In the advent of the area of rapid development of XAI techniques, the need for\na multidisciplinary qualitative evaluation of explainability is one of the emerging topics in the community. Our dataset allows\nnot only to reproduce the study we conducted, but also to open a wide range of possibilities for the analysis of the material we\ngathered.", "sections": [{"title": "Background & Summary", "content": "With the rapid development of black-box machine learning (ML) models, such as deep neural networks or gradient boosting\ntrees, the need for explanations of their decisions has emerged. This demand has been driven by the increasing implementation\nof opaque models, in high-risk and critical areas like medicine, healthcare, industry, and law, which laid the foundation for\nmodern research on explainable and interpretable artificial intelligence (XAI). Scientists' efforts in designing XAI algorithms\nhave been further supported by political initiatives such as DARPA's XAI challenge [1], the European Union's GDPR [2], and\nmore recently, the EU AI Act [3].\n The shared goal of all these initiatives is to improve the transparency of AI systems, thereby promoting their adoption in areas\nwhere trust in AI is not fully established or where the transparency of decisions is crucial for legal and safety reasons. However,\nas XAI algorithms have been advanced, a new discussion has been initiated, addressing the fundamental challenge of ensuring\nthat the explanations generated by these algorithms are comprehensible to humans. This triggered research on the evaluation\nof XAI [4], drawing attention from social sciences, which argued that much of the effort in XAI relies solely on researchers'\nintuition about what constitutes a good explanation. They emphasized that human factors should be integral to the design and\nevaluation of XAI to ensure its reliability [5].\n Recognizing individual human abilities to comprehend algorithmically generated explanations is crucial, as these abilities\ncan vary significantly based on personal information competencies. Additionally, there is a lack of established multidisciplinary\nmethods for measuring these capabilities, as well as datasets that facilitate reproducible evaluations or comprehensive analyses.\n Our contribution addresses these gaps by presenting a dataset resulting from extensive user study on the comprehensibility of\nXAI algorithms across three distinct user groups with different information competencies. The dataset contains material collected\nfrom 39 participants during the interviews conducted by the information sciences research group. The participants were recruited"}, {"title": "Methods", "content": "The dataset used to train the classifier is a publically available dataset from the UC Irvine Machine Learning Repository\u00b9. It\ncontains data on 61,069 specimens from 173 mushroom species, categorized as either edible or inedible/poisonous. Specimens\nwith unknown edibility were classified as inedible/poisonous. The dataset exclusively includes cap-and-stalk mushrooms with\ngill hymenophores and comprises both real observations and hypothetical data, the latter generated artificially from a smaller set\nof real-world mushroom observations.\n Regarding class distribution, the dataset is fairly balanced, with 33,888 instances labeled inedible or poisonous and 27,181\nas edible, making approximately 55.49 This balance is crucial for accurate model training and performance evaluation, avoiding\nbiases towards the majority class.\n The ML model we prepared and presented to the study participants uses the Gradient Boosting Classifier (XGBClassifier).\nTo handle categorical variables, a one-hot encoder was used and missing data was addressed through imputation. Additionally,\nnumerical variables were scaled to ensure uniformity in data handling. This preprocessing involved imputing missing values\nin numeric features with the median and scaling these features, while categorical variables were imputed with a placeholder\nvalue '_NA_'. Consequently, the model operated on 82 features derived from the original 20 input variables. Renowned for its\nefficiency in classification tasks, the model exhibited an impressive accuracy rate of 99.97\n Study participants were informed about the model's limitations, emphasizing that while the model is highly accurate and\ndata-driven, it does not encompass all possible factors affecting mushroom edibility. Therefore, it should be used as a supporting\ntool rather than the sole determinant in the identification process. Additionally, the standard practice of data partitioning was\napplied, dividing the dataset into a training set for model development and a test set for validation.\n To cover the broadest spectrum of XAI methods without overwhelming participants, we selected the following representative\ntypes of explanations for our study:\n \u2022 Statistical descriptions and visualizations of data,\n \u2022 Feature importance attribution-based explanations, such as SHAP and LIME,\n \u2022 Rule-based explanations, such as Anchor and LUX,\n \u2022 Counterfactual explanations, such as DICE.\n Additionally, both feature importance attribution explanations and rule-based explanations were provided in the form of local\nand global explanations. Our selection of methods was loosely guided by the work of Baniecki et al. [8], which demonstrated the\neffectiveness of sequentially analyzing a model using a combination of multiple complementary XAI mechanisms.\n The explanations were compiled in a single PDF presentation with 14 slides, where each slide contains a visualization of\nthe explanation generated with the selected XAI algorithm. There was also a 15th slide that indicated the end of the study. The"}, {"title": "Statistical descriptions and visualization of data", "content": "For each of these numerical features: both histograms and box plots were created using the matplotlib library from scikit-learn.\nMissing data points were appropriately labeled and represented by gray bars. The descriptive statistics for the numerical variables\nwere presented in a straightforward tabular text output, translating all elements such as count, mean, and standard deviation into\nPolish. Categorical variables were illustrated using horizontal bar charts, with labels translated into Polish to accommodate user\npreferences."}, {"title": "Feature importance attribution-based explanations", "content": "A \"Swarm Plot\" from the SHAP package offered a visual representation of the impact of individual mushroom features on the\nmodel's classification as either edible or poisonous. While the majority of the plot was translated into Polish, due to technical\nlimitations, the labels \"High\" and \"Low\" on the \"Feature Value\" axis remained in English. Below the horizontal axis, a description\nclarified: \"SHAP Value Effect Points to the right (> 0.0) indicate a greater impact on the classification of mushrooms by the model\nas poisonous, to the left as edible\". The accompanying legend is elucidated as follows: High signifies a high feature value, for\ninstance, \"cap_diameter_cm\" (High) correlates to a large cap diameter in centimeters. Conversely, Low represents a low feature\nvalue, such as \"stalk_height_cm\" (Low) indicating a short stalk. The \"0\" mark delineates the boundary between features of\nsignificant and minor importance in assessing the edibility of mushrooms. For binary characteristics, a high value (High), denoted\nby the red color, implies the presence of the feature. In the top left corner, a succinct explanation encapsulated the essence of\nthe \"Swarm Plot\": illustrating the influence of individual fungal traits on the prediction of their edibility (edible/inedible or\npoisonous).\n A SHAP \"Waterfall\" chart was utilized to interpret the model's feature attributions for prediction. The custom title indicated\nthe analysis of features that influence the prediction of the specified class, with the true class of the specimen also noted. This\nvisualization delineated the contribution of each feature to the model's prediction, with positive values indicating increased\nprobability of the mushroom being classified as poisonous, and negative values the opposite.\n The chart was accompanied by a legend where: E[f(X)] signifies the baseline value or the mean model prediction, the\ncolor bands reflect the influence of each feature on the prediction of toxicity for the individual mushroom, and f(x) is the final"}, {"title": "Rule-based explanations", "content": "The study employed the ANCHOR package, a method of explainability for AI models that generates an \"anchor\", or a set of\nconditions that collectively influence the model's classification of an instance with high precision.\n An example observation matching the anchor criteria was displayed on the left side of the slide, representing the output from\nANCHOR. The central part of the slide described the AI's prediction process: for instances meeting all listed anchor conditions,\nthe AI would predict the class as EDIBLE with 97.2 The anchor, composed of feature conjunctions that determine classification,\nis detailed in the upper right. Below it, the extent of influence these conditions had on the certainty of the classification was\nquantified. Since interactive features were not employed, the slide showed a non-interactive list of features. Replicating the\nchecklist with fewer marked features offered insights into how different subsets contributed to the prediction, without providing\nfull anchor conditions. Two slides were presented, one for the edible class and one for the inedible/poisonous class."}, {"title": "Counterfactual explanations", "content": "Counterfactual analysis was depicted through a textual output table on the left, listing features along with their original values\nand the altered values required to reverse the AI's prediction. On the right, the explanatory text illustrated the interpretation of\nthis analysis, detailing how AI-filling of missing data in the description of a specific mushroom influences the prediction of its\nedibility. The specimen in question, originally found in nature as non-edible/poisonous, was correctly assigned by the AI to the\npredicted non-edible/poisonous class. The counterfactuals demonstrated the minimal data alterations needed for the mushroom to\nbe predicted as edible. The legend clarified: Original Value: Data obtained for the specific mushroom from the original dataset,\nModified Value: Data altered or supplemented by the AI model. The dice_explainer from the respective package was employed\nfor calculating counterfactuals. The presentation excluded cases where the 'missing data' value was changed, as such instances\nwere considered non-informative for interpretation purposes."}, {"title": "Collection of empirical material", "content": "The collection of empirical material comprised two main phases: an online survey and individual qualitative interviews conducted\nusing the think-aloud protocol (TAP) guidelines [10]. The survey involved non-intrusive questioning, while the TAP interviews\nrequired audio recording with explicit consent obtained from willing participants. Participation in both the survey and interviews\nwas voluntary, and all collected data was anonymized. We opted for general professional vocabulary over industry-specific jargon\nto ensure clear communication without adding new content.\n In accordance with the guidelines approved by the Council of the Faculty of Management and Social Communication for\nthe ethics committee's procedures, each research team is responsible for independently assessing whether their study requires\nsubmission for committee review. Based on this self-assessment, our research did not meet the criteria for ethics committee\nreview for the following reasons: a) We explicitly stated in both the consent forms and verbally (with recordings available)\nthat the focus of the study is not on evaluating individuals, but rather on the material presented to participants (i.e., types of\nexplanations generated by an artificial intelligence method); and b) The research poses no psychological harm or discomfort, as\nconfirmed in our documentation.\n Initially, the research aimed to recruit participants willing to undergo interviews, assessing their knowledge, qualifications,\nexperience, and competencies in the specific domain relevant to the training of AI models and explainable AI (XAI) methods,\nfocusing on mushroom classification in our case. The survey results categorized participants into three aforementioned groups:\ndomain experts (DE), those familiar with data analysis (IT), and those with no data analysis expertise (SSH).\n Subsequently, these groups were assigned tasks and interviewed using advanced tools from social science fields such as TAP\nand MAXQDA for analysis. The interview part of the study included collection of qualitative material for further analysis with\nthe TAP procedure. A total of 26 students (IT and SSH) and 13 experts took part in TAP. The IT group had 8 respondents from\ncomputer science, applied computer science, computer game science and electronic information processing. The SSH group"}, {"title": "Data Records", "content": "The general structure of the dataset is described in Table 1. The files that contain in their names [RR]_[SS]_[NN] contain the\nindividual results obtained from particular participant. The meaning of the prefix is as follows:\n \u2022 RR - initials of the researcher conducting the interview,\n \u2022 SS - group that the participant belongs to (DE for domain expert, SSH for social sciences and humanities students, or IT\nfor computer science students),\n \u2022 NN - the participant number representing the order in which the participants were interviewed by a given researcher.\n The dataset contains a mix of Polish and English content. The components created during the post-processing phase of the\nstudy were prepared in English, while the core components such as the transcripts and slides with explanations were left in"}, {"title": "Initial survey results", "content": "The initial survey dataset contains self-assessment questions that all participant candidates were required to answer. The survey\ncomprised 18 questions, 12 of which pertained directly to self-assessing knowledge and skills in mushrooming and data visuali-\nsation, as well as acquiring information about their origin and experience. The survey included a question about the field of study\nto ensure appropriate representation of students pursuing humanities, social sciences, and computer science. Five questions in the\nquestionnaire directly related to giving consent for data use and participation in further research, as well as confirming familiarity\nwith the GDPR clause. The questionnaire aimed to gather information and assess the knowledge, qualifications, experience, and\ncompetences of the participants. It consisted of six open-ended questions and six closed questions. The latter included an 'other'\noption, allowing participants to provide their own answer. These questions enable the grouping of participants based on their\ndomain knowledge and data analysis literacy, which is crucial for selecting the final set of participants and categorizing them\ninto DE, IT, and SSH groups. The description of a content of this set of data is given in Table 2. The survey table contains\n154 records, including 4 candidates used in the pilot study who were not included in the final dataset. The goal of the pilot\nstudy was to identify potential issues, refine research methodologies, and ensure the feasibility of the study design; therefore, the\nparticipants in this stage were not considered valid subjects for the final dataset."}, {"title": "Labelled interview transcripts", "content": "The core component of the dataset consists of interview transcripts with manually labeled text fragments. Each fragment was\nassigned to a slide with a specific explanation visualization (see Table 4), the question the participant is trying to answer (see\nTable 5), and the problem the participant is attempting to solve (see Table 6). Additionally, special tags were used to divide the\ntranscripts into stages corresponding to the three tasks performed by the participants: explanation analysis, problem-solving, and\nvisualization modifications. In the dataset, each transcript is represented by a separate file, named with a unique participant ID\nkey that is also present in other tables. The schema description of the transcripts is provided in Table 3."}, {"title": "Explanation-guided mushroom classification by users", "content": "The participants were asked to solve three problems related to mushroom classification as edible and non-edible based on the\nexplanations and other material they were provided. All of the participants were asked about the same three instances that contain\ntwo edible and one non-edible mushroom with varying ML model classification probability assigned to each instance. The table\ncontaining this instances along with ML model predictions is described in detail in Table 6, while the responses of the participants\nwere stored in a table of a schema defined by Table 7. The participants were also asked to indicate their level of certainty of\nprediction that can later be confronted with ML model classification probability. The DE group of participants was not solving\nthis tasks, hence the table is missing participant IDs related to domain experts in mycology."}, {"title": "Modification of explanation visualization", "content": "One of the tasks assigned to participants was to suggest modifications to the provided explanations to improve overall user\nexperience and enhance their usability. Participants could reorder the explanations, remove selected ones, or add custom slides\nwith additional explanations or information they deemed useful. The schema of the table containing records from this part of the\nstudy is shown in Table 8. The DE group of participants was not solving this tasks neither."}, {"title": "Results from thematic analysis with MAXQDA", "content": "The transcripts were additionally analysed with the MAXQDA thematic analysis toolkit. The codes used for the thematic analysis\nare stored in a file defined by a schema presented in Table 10. The final codebook contained both overarching and subordinate"}, {"title": "Technical Validation", "content": ""}, {"title": "Classification model training and validation", "content": "The creation of a ML model was followed by the standard practices in data science including numerical data scaling, categorical\nvariables encoding and missing values imputation. The detest was balanced, so not oversampling nor under-sampling techniques\nwere applied that could alter the original data distribution. The model training was performed on train set which comprises 70\nThe hyper-parameter tuning was performed with grid search and five-fold cross validation. With that approach, we achieved\nthe performance of the model at the level of 99,97 For the full transparency and reproducibility of this procedure, we provide a\nsource code of the model training and data preprocessing."}, {"title": "Study design and data acquisition", "content": "To ensure data quality during the empirical material acquisition stage, we addressed potential issues on two levels. The first\nlevel focused on creating a representative sample of participants for the study, while the second level involved ensuring the high\nquality of the material collected during the study.\n To verify that the sample of participants recruited for the study was representative and diverse enough to distinguish signifi-\ncantly different groups with respect to their background knowledge in the domains of macro-fungi and data science, we undertook\nseveral steps. Firstly, we designed a comprehensive survey, developed by social sciences researchers with expertise in informa-\ntion sciences, to capture user profiles from an information capabilities perspective. This survey allowed us to accurately assess\nthe background knowledge of participants in the domains of macro-fungi and data science. To reach a diverse group of students,\nwe advertised the research across three different universities in Poland, thereby broadening our participant base. Additionally,\ndomain experts were recruited from the mycological society, specifically among individuals who are certified fungi identifiers\naccredited by the competent Regional Office of Sanitary and Epidemiological Vigilance (SANEPID) in Poland. This selection\nensured that expertise among the domain experts was not solely based on self-assessment, which could be unreliable, but rather\nconfirmed by their official certification from a recognized legal authority in Poland.\n The total number of enrolled participants (N = 39) concords with the evidence-based recommendations on sample size in\nphenomenological studies [16, 17, 18]. The data saturation, evidenced by the repetitiveness of codes and codes' co-occurrence\npatterns, has been achieved across all three subsets of respondents under study. Furthermore, the sample size is in alignment\nwith, and in fact exceeds, the size of the most recent published studies employing the Think-Aloud Protocol to examine XAI\ninterpretations from a variety of end-user categories [19, 20, 21].\n To ensure the high quality of empirical material obtained during interviews, we identified and addressed the limitations and\ndrawbacks inherent in the TAP procedure. Firstly, due to the need to verbalize, it is a demanding technique for the respondents,\nat the same time as the situation itself may seem unnatural. We aimed to minimise this potential discomfort with the following\nactions: (1) we showed the respondents an audio recording of an exemplary verbalisation so that they could more easily immerse\nthemselves in the procedure (the recording was made on a website unrelated to the subject of our study, so as not to suggest\nto the respondents a direction for interpreting the materials), (2) the current question/task was kept on a sheet of paper in the\nrespondent's sight at all times, so that the respondents could more easily realise where they were in the research and which\nquestion they were answering, (3) in the case of longer periods of silence, the interviewer reminded the respondent of the\nverbalisation in as neutral way as possible (e.g. what are you currently looking at?), (4) we provided mineral water for the\nrespondents to drink, (5) we incorporated into the procedure the possibility of a break during the session at the request of the\nrespondent.\n Additionally, the TAP interviews with students have been conducted in comfort-ensuring conditions. The rooms devoted to\none-to-one sessions have been silent and isolated from external disturbances. The privacy of the respondents has been preserved.\nEveryone has been offered a bottle of mineral water and the possibility of requesting a break at any moment. The sessions were\naudio-recorded on two devices to minimize the risk of data loss. The interviews with domain experts took the form of video\nmeetings on the MS Teams platform and were video-recorded with automatic transcription in Polish. In both cases (students &\nexperts) the automatic transcripts were reviewed and corrected manually by the researcher who carried out the given session.\nThe coding was accomplished by three coders who worked in coordination, agreeing on successive rectifications and extensions\nof the coding tree. Due to the specific content revealed in domain experts' verbal output, the coding tree for experts was added\nwith several new items."}, {"title": "Data post-processing", "content": "The post-processing of the results, involving the transformation of all gathered material into structured CSV format as presented\nin previous sections, underwent subsequent verification steps. Initially, the technical integrity of the material was assessed by\ntwo researchers to ensure no data records were missing from the original sources. Following this, the transcripts and their\nmanual labeling were reviewed by three independent researchers to confirm the accuracy and integrity of this stage. Finally,\nwe performed technical analysis of the dataset, ensuring that all of the tables contain valid IDs and can be merged according to\nschema depicted in Figure 4."}, {"title": "Usage Notes", "content": "In this example in Listing 1, we present how to read sample records from a dataset, how to link the records with other tables and\ntrigger basic analysis with scikit-learn library [22]. The example assumes that all of the files are in the zenodo folder and the\ntranscripts were unzipped into zenodo/transripts folder."}]}