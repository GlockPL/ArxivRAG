{"title": "EUREKA: Evaluating and Understanding Large Foundation Models", "authors": ["Vidhisha Balachandran", "Jingya Chen", "Neel Joshi", "Besmira Nushi", "Hamid Palangi", "Eduardo Salinas", "Vibhav Vineet", "James Woffinden-Luey", "Safoora Yousefi"], "abstract": "Rigorous and reproducible evaluation of large foundation models is critical for assessing the state of the art, informing next steps in model improvement, and for guiding scientific advances in Artificial Intelligence (AI). Evaluation is also important for informing the increasing number of application developers that build services on foundation models. The evaluation process has however been challenging in practice due to several reasons that require immediate attention from the community, including benchmark saturation, lack of transparency in the methods being deployed for measurement, development challenges in extracting the right measurements for generative tasks, and, more generally, the extensive number of capabilities that need to be considered for showing a well-rounded comparison across models. In addition, despite the overwhelming numbers of side-by-side capability evaluations available, we still lack a deeper understanding about when and how different models fail for a given capability and whether the nature of failures is similar across different models being released over time.\nWe make three contributions to alleviate the above challenges. First, we present EUREKA, a reusable and open evaluation framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce EUREKA-BENCH as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art foundation models and (ii) represent funda-mental but overlooked capabilities for completing tasks in both language and vision modalities. The available space for improvement that comes inherently from non-saturated benchmarks, enables us to discover mean-ingful differences between models at a capability level. Third, using the framework and EUREKA-BENCH, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights for failure understanding and model comparison by disaggregating the measurements across important subcategories of data. Such insights uncover granular weaknesses of models for a given capability and can then be further leveraged to plan more precisely on what areas are most promising for improvement. EUREKA is available as open-source to foster transparent and reproducible evaluation practices.\nIn contrast to recent trends in evaluation reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for several capabilities. Despite the many observed improvements, it also becomes obvious that current models still struggle with a number of fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.", "sections": [{"title": "1 Introduction", "content": "The evaluation of Large Foundation Models (LFMs) presents several methodical and practical challenges, many of which stem from the generative and general-purpose nature of recent models. The rapid progress in AI has also introduced many new capabilities as part of the model skills portfolio, which need to be assessed alongside traditional capabilities. EUREKA is a framework and a collection of challenging benchmarks that aims at scaling up such evaluations for LFMs in an open and transparent manner. The framework itself provides a library for flexibly customizing evaluation pipelines that combine a series of components necessary for evaluation including data preprocessing, prompt templates, model inference, data postprocessing, metric computation, and reporting. EUREKA-BENCH is the collection of benchmarks whose implementation is currently supported in EUREKA and for which we provide extensive evaluation and analysis reports.\nEvaluation Framework. The evaluation process for complex and generative capabilities has made traditional practices for evaluation obsolete. For example, the concept of a fixed, closed-form definition of a metric does not apply anymore to many capabilities either because several different sub metrics need to be computed before reaching a final score, or because many data transformations and answer extraction operations need to be applied to the output prior to computing a metric. Some of these data transformations often are custom to the model being evaluated. In addition, part of the evaluation also needs to be handed over to other model judges for scaling up [19, 131]. This new landscape leaves practitioners with the necessity of creating a rich combination of data processing steps, code execution, and model inferences as evaluators, all in function of producing a final score for the model under test. EUREKA provides a flexible library for composing these functionalities into shareable evaluation pipelines and gives full control to practitioners to handle and log the details of each experiment. These functionalities also enable reproducibility and backtracking of experimentation details (e.g. prompt templates, inference parameters, API and model versions) in a transparent manner. Given that such details can change measurements significantly, we believe it is important for the research community to have access to both the code and logs behind evaluations. Thus, we provide the actual code and logs used in evaluations.\nBenchmark selection. A pressing issue in the evaluation of state-of-the-art LFMs is the fact that many of the benchmarks commonly reported in technical reports of model releases are either close to saturation or already saturated, where models have reached close to 100% accuracy on the benchmarks. For example, several recent models [80, 35, 1, 89, 3] have been reported to have an accuracy higher than 85% on benchmarks like MMLU [45], GSM8K [27], HumanEval [23], DROP [34], BigBench-Hard [101], MGSM [97], ChartQA [70], AI2D [51]. Many of the benchmarks in this list represent tasks that were important for testing fundamental capabilities at the time of their release. However, saturation of performance does not leave ample space for discovering major failure modes and for comparing different models. While saturation itself may originate either from inherent model improvements or from memorization, the challenge from a scientific communication"}, {"title": "2 Results Summary", "content": "Figure 1 is a high-level illustration of the state of the art in AI for EUREKA-BENCH, showing the best and the worst performance per capability. These results show a complementary picture of capabilities of different models and that there is no single model that outperforms all others in most tasks. However, Claude 3.5 Sonnet, GPT-40 2024-05-13, and Llama 3.1 405B repeatedly outperform others in several capabilities.\nMultimodal Evaluation: Evaluations on important vision-language capabilities such as geometric and spatial reasoning, object recognition and detection, multimodal question answering, and navigation demonstrate increased capabilities of most recent models when compared to their previous versions. For example, GPT-40 2024-05-13 improvements over GPT-4 Vision Preview range between 3%-20%. Yet, state-of-the-art models are still fairly limited in their multimodal abilities, specifically when it comes to detailed image understanding (e.g. localization of objects, geometric and spatial reasoning, and navigation), which is most needed in truly multimodal scenarios that require physical awareness, visual grounding, and localization.\n1. State-of-the-art multimodal models struggle with geometric reasoning. Reasoning about height is more difficult than about depth. Claude 3.5 Sonnet and Gemini 1.5 Pro are the best performing models for this task with Claude 3.5 Sonnet being the most accurate model for depth ordering and Gemini 1.5 Pro the most accurate for height ordering.\n2. Multimodal capabilities lag language capabilities. On tasks which can be described either as a multimodal task or as language-only, the performance of most tested models is higher for the language-only condition. GPT-40 2024-05-13 is the only model that consistently achieves better results when presented with both vision and language information, showing therefore that it can better fuse the two data modalities.\n3. Complementary performance across models for fundamental multimodal skills. For example, Claude 3.5 Sonnet, GPT-40 2024-05-13, and GPT-4 Turbo 2024-04-09 have comparable performance in multimodal question answering (MMMU) but they outperform all other models by at least 15%. There are tasks like object recognition and visual prompting where the performance of Claude 3.5 Sonnet is better or comparable to GPT-40 2024-05-13, but Gemini 1.5 Pro outperforms them both. Finally, in tasks like object detection and spatial reasoning, GPT-40 2024-05-13 is the most accurate model.\nLanguage Evaluation: The evaluation through EUREKA-BENCH shows that there have been important advances from state-of-the-art LFMs in the language capabilities of instruction following, long context question answering, information retrieval, and safety."}, {"title": "3 Evaluation Framework", "content": "In the fast-paced space of AI research, where new models and benchmarks are introduced and others are depre-cated frequently, it is important for evaluation and understanding efforts to be able to reuse existing evaluation pipelines with minimal adjustments to efficiently accommodate new models and benchmarks. This calls for a modular design that allows the users to onboard a new benchmark or model by inheritance of pipeline definitions from existing experiments and implementing changes only where overriding the existing pipeline is necessary."}, {"title": "4 Multimodal Evaluation", "content": "In this section, we provide detailed analysis and results for the capabilities of geometric reasoning (GeoMe-ter), multimodal question answering (MMMU), object recognition, object detection, visual prompting, spatial understanding and reasoning, navigation, and counting.\nTo account for the impact of non-determinism (discussed in Section 6), all experiments reported here were repeated three times and we report the mean and corresponding standard error across the three repeated runs with temperature set to zero and top_p = 0.95."}, {"title": "4.1 Geometric Reasoning - GeoMeter", "content": "Motivation: The ability to understand visual properties such as size, shape, depth, and height is fundamental to visual understanding, yet many existing Visual Question Answering (VQA) benchmarks [50, 20, 63, 32, 104] do not specifically focus on the depth and height perception capabilities of Vision Language Models (VLMs). Accurate perception of these dimensions is vital for practical applications like scene understanding, navigation, monitoring, and assistive technologies. The lack of accurate depth and height understanding in VLMs can lead to serious consequences, such as misjudging the proximity of objects, which could result in catastrophic outcomes in real-world scenarios.\nDespite VLMs' abilities to recognize object shapes and sizes, their depth and height reasoning often relies on learned size/shape cues rather than actual geometric analysis, potentially influenced by biases from training data [48]. Alternatively, models might estimate the depth based on the apparent size of objects, without genuine inter-object reasoning. Additionally, when faced with multiple choices, VLMs might also show bias towards certain answers, influenced by the prevalence of similar data during training. Thus, it becomes important working with focused benchmarks that enhance understanding of true depth and height perception in VLMs, ensuring they perform reliably in complex, real-world environments.\nHere, we use GeoMeter, a geometric reasoning benchmark derived from previous work [9], which is specif-ically designed to evaluate the depth and height reasoning capabilities of Vision Language Models (VLMs).\nGeoMeter comprises approximately 1086 unique image-text pairs across two tasks: depth and height. The data"}, {"title": "Analysis and Discussion", "content": "Models generally struggle in depth and height perception tasks. Results from Table 5 highlight that the foundation multi-modal models struggle significantly with depth and height perception tasks involving similar shapes. This discrepancy underscores our benchmark's value in identifying gaps in VLMs' capabilities to handle more complex geometric reasoning, beyond mere shape recognition. To further support our claim that the low performance of models on the GeoMeter data is due to VLMs' deficiencies in depth and height reasoning, we also provide reference to relevant observations from prior work [9]. First, they note that VLMs generally perform well on basic geometric tasks such as line understanding, shape recognition, and shape counting, but fail on advanced perception tasks like depth and height perception tasks. Further, they also observe that VLMs"}, {"title": "Main takeaways", "content": "State-of-the-art multimodal models struggle in depth and height perception tasks.\nGenerally models show better depth perception than height.\nClaude 3.5 Sonnet and Gemini 1.5 Pro are the best performing models for this task with Claude 3.5 Sonnet being the most accurate model for depth ordering and Gemini 1.5 Pro the most accurate for height ordering."}, {"title": "4.2 Multimodal Question Answering - MMMU", "content": "Motivation: A key use case for multimodal models is to serve as an expert assistant to answer queries and provide information and context about images. This Visual Question Answering setup is one of the core tasks for multimodal models. It combines the abilities of understanding images at a high and detailed level with the ability of reasoning using that understanding. MMMU [123] is a popular dataset that tests these capabilities across a broad range of topics requiring deep image understanding and domain-specific knowledge. We have included it in our evaluations due to this broad and deep coverage, wide adoption in the research and industry communities, and that it remains a challenging dataset that no models have yet mastered. Thus it provides a good high-level measure of multimodal reasoning performance.\nBenchmark Description: MMMU tests multimodal multi-discipline reasoning in six core disciplines: Art and Design, Business, Science, Health and Medicine, Humanities and Social Science, and Tech and Engineering. The questions span 30 subject areas and 183 subfields, with a wide-variety of image types, such as charts, dia-grams, maps, tables, music sheets, and chemical structures. Questions are both multiple-choice and open-ended. An illustration of the disciplines, subjects, images, and questions appears in Figure 5. For our evaluations, we use the 900-question validation set that spans all subject-areas.\nAggregate Baseline Results: As a baseline evaluation we use the prompt formatting that is provided by the MMMU evaluation codebase [4], which concatenates each question with the appropriate answer choices:"}, {"title": "Main takeaways", "content": "Claude 3.5 Sonnet and GPT-40 2024-05-13 are the leading models for multimodal question answering as measured by the MMMU dataset, indicating better multimodal understanding skills and knowledge.\nThe MMMU benchmark remains a challenging task for all models, with the best performance in the mid 60s percentage range.\nThe performance of models is highly dependent on how they are prompted, with improvements and regres-sions across topics. The role of the prompt in evaluations cannot be ignored. This is an area that requires further investigation."}, {"title": "4.3 Image Understanding", "content": "Motivation: A key question for understanding multimodal performance is analyzing the ability for a model to have basic vs. detailed understanding of images. These capabilities are needed for models to be used in real-world tasks, such as an assistant in the physical world. While there are many dataset for object detection and recognition, there are few that test spatial reasoning and other more targeted task such as visual prompting. The datasets that do exist are static and publicly available, thus there is concern that current AI models could be trained on these datasets, which makes evaluation with them unreliable. Thus we created a dataset that is procedurally generated and synthetic, and tests spatial reasoning, visual prompting, as well as object recognition and detection [91]. The datasets are challenging for most AI models and by being procedurally generated the"}, {"title": "Results", "content": "As shown in Figure 11, for Object Recognition and Visual Prompting, Gemini 1.5 Pro is consistently the best model by a range of 2-12% across these tasks and the one and two object conditions. GPT-40 2024-05-13, Claude 3.5 Sonnet, and Llava 1.6 34B all come in second within a few percentage points of each other, without a consistent clear winner between them.\nOne interesting observation is that Visual Prompting leads to a small drop in model performance relative to Object Recognition. In cases involving two objects, we see a substantial decline across nearly all models, while in one-object cases, there is a modest drop for some models. This is surprising given we would expect the visual prompt to help focus the model and thus improve results [103, 120].\nFor Spatial Reasoning, we see GPT-40 2024-05-13 as the overall best across both the one and two object conditions; however, within each condition there is a different story. Llava 1.6 34B excels at the one object"}, {"title": "Main takeaways", "content": "GPT-40 2024-05-13, Claude 3.5 Sonnet, Gemini 1.5 Pro, and Llava 1.6 34B all perform well for Image Un-derstanding as measured by our dataset. Gemini 1.5 Pro has the most wins for these sub-tasks and conditions, but Claude 3.5 Sonnet has the best average performance by a little over 1% when excluding Object Detection. GPT-40 2024-05-13 has the best average performance for all sub-tasks and conditions.\nObject Detection is still quite challenging for all models. The best performing model is GPT-40 2024-05-13 at AP50 13.1 in the two object condition and this is far below the performance of a state-of-the-art object detector.\nObject Recognition and Visual Prompting become more difficult for more than one object, but Spatial Under-standing and Object Detection become easier. Models perform slightly work on Visual Prompting vs. Object Recognition, which is unexpected."}, {"title": "4.4 Vision Language Understanding", "content": "Motivation: A key question for understanding multimodal vs. language capabilities of models is what is the relative strength of the spatial reasoning and understanding in each modality, as spatial understanding is expected to be a strength for multimodality? To test this we use the procedurally generatable, synthetic dataset of Wang et al. [112] to testing spatial reasoning, navigation, and counting. These datasets are challenging and by being procedurally generated new versions can easily be created to combat the effects of models being trained on this data and the results being due to memorization. For each task, each question has an image and a text representation that is sufficient for answering each question.\nBenchmark Description: This dataset has three tasks that test: Spatial Understanding (Spatial-Map), Nav-igation (Maze-Nav), and Counting (Spatial-Grid). Each task has three conditions, with respect to the input modality, 1) text-only, input and a question, 2) vision-only, which is the standard task of visual-question an-swering that consists of a vision-only input and a question, and 3) vision-text includes both text and image representations with the question. See Figure 12 for an illustration of each task. Each condition includes 1500 images and text pairs for a total of 4500."}, {"title": "Aggregate Results:", "content": "As seen in Figure 13, when examining the aggregate results across all conditions, we see that GPT-40 2024-05-13 has the best performance by 0.7% over Gemini 1.5 Pro for Spatial-Map, while Gemini"}, {"title": "Main takeaways", "content": "The vision-only conditions generally under-perform when compared to the text-only and vision-text condi-tions, with no consistent winner between the text-only and vision-text conditions.\nWhen both textual and visual information are available, multi-modal language models appear to rely less on visual information if sufficient textual clues are provided. This opens important questions for multimodal learning, as for humans most tasks in this benchmark are easier in the vision modality but current models do not seem to benefit from it."}, {"title": "4.5 High Level vs. Detailed Image Understanding - Discussion", "content": "Current frontier models excel on tasks that require high-level image understanding, such as general object recognition, counting in a grid, and basic spatial reasoning (with 1 or 2 objects), with results in the 80-90% accuracy range. In contrast, these models struggle when it comes to tasks that require detailed image under-standing, including tasks such as object detection, complex spatial reasoning, maze navigation, and depth and height perception tasks. For example, the best object detection result we measured of 13.1 AP50, is around 30 points below a Computer Vision detector from almost 10 years ago [90]. Spatial reasoning with more than two objects tops out at 63.7%. The best-case performance on our maze navigation benchmark is only 15% better than random guessing. Accuracy on geometric reasoning that requires depth and height reasoning max out at round 50%.\nAt the same time, detailed image understanding is also the type of competency that is critically needed in a truly multimodal scenario requiring physical awareness, localization, and grounded perception. For example, reliable object detection is required for numerous safety monitoring and remote navigation use cases. These have been main drivers of the flagship advances in computer vision for specialized object localization. Low performance in these types of tasks means that replacement of traditional models like YOLO [88] and Faster R-CNN [90] with more recent LFMs is not yet realistic. Beyond important traditional object-recognition tasks, other rising scenarios motivate the critical need for accurate inferences about object recognition. For example, the configuration and spatial relationships of objects over time is essential in scenarios of rising importance centering on human-AI interaction on physical tasks [14]."}, {"title": "5 Language Evaluation", "content": "In this section, we provide detailed analysis and results for the capabilities of instruction following, question answering for long context, information retrieval, as well as toxicity detection and safe language generation. To account for the impact of non-determinism (discussed in Section 6), all experiments for the smaller datasets (i.e. IFEval and Toxigen - generative) were repeated three times and we report the mean and cor-responding standard error across the three repeated runs with temperature set to zero and top_p = 0.95. For the larger datasets (i.e. Kitab, FlenQA, Toxigen - discriminative) we run the experiment two times to investigate the impact of non determinism at the overall dataset level and the subcategory level, and only observe minimal differences (of less than 0.5 percentage points). Therefore, for these datasets we henceforth report results on a single run."}, {"title": "5.1 Instruction Following - IFEval", "content": "Motivation: A critical skill for frontier models is the ability to follow instructions provided in the input prompt. Users provide increasingly complex instructions to LLMs in order to specify details about tasks they intend the model to perform, teach the model problem solving techniques and format the model's responses under specific requirements. Model training pipelines now often include a dedicated instruction tuning phase for specifically teaching models to follow complex instructions for real-world scenarios.\nConsequently, evaluating how well models follow such instructions is crucial when assessing overall model behaviour. While real instructions provided by users can be very varied and complex, a predominant category are instructions to control the format or style of the output. IFEval [133] is a benchmark designed to evaluate a model's ability to follow instructions about the output style, structure and form. Recent model evaluations report ~70-80% accuracy on IFEval on average, showing headroom for further analysis and progress on challenging instruction categories.\nBenchmark Description: The benchmark includes instruction based prompts for a category of 'verifiable in-structions', which are defined as instructions amenable to objective verification of compliance. Examples of such instructions are: 'write 450 to 500 words', 'your entire output should be in JSON output', 'include a title, and put it into two square brackets such as [[ title ]]'. The benchmark consists of nine broad instruction cate-gories with 25 fine-grained types focusing on various output content and format constraint-based instructions. An input prompt can contain multiple instructions and can support fine-grained instruction level analysis.\nPrior evaluations accompanying model releases, often report a single aggregate number by averaging the different metrics proposed in [133], which often fail to reveal meaningful differences between models. Instead, the evaluation for IFEval in this report separately reports two understandable metrics at two levels of granularity: i) Overall Accuracy - reports dataset level accuracy of a model following 'all' instructions in an input prompt and percentage of instructions followed, both under strict criteria, across all categories and ii) Instruction Category Level accuracy - reports accuracy of following instructions under strict criteria per instruction category."}, {"title": "5.2 Long Context - FlenQA", "content": "Motivation: Despite significant recent improvements to LLMs and efforts to evaluate them in long context set-tings [57, 95], their performance consistency across different input lengths remains poorly understood. FlenQA [54] aims to address this gap by isolating the effect of input length on language model performance."}, {"title": "Aggregate Results:", "content": "Despite recent works (e.g., Gemini 1.5 Pro [89]) showing improvement in \u201cneedle-in-a-haystack\" experiments, our observation is that merely increasing the models' context size does not necessarily result in better complex reasoning capabilities in long-context tasks (see Figure 18). Significant performance degradation (up to 30%) is observed with increasing context length for the following models: Claude 3.5 Sonnet: 19.80%, Claude 3 Opus: 22.60%, Gemini 1.5 Pro: 22.00%, GPT-40 2024-05-13: 8.50%, GPT-4 1106 Preview: 13.70%, Llama 3.1 405B: 7.20%, Llama 3.1 70B: 12.90%, Llama 3 70B: 24.10%, Mistral Large 2407: 30.70%. Llama 3.1 405B is the most robust to context length increase, followed closely by GPT-40 2024-05-13. The main failure modes across different models are the inability to identify the right pieces of information from the context and logical errors when reasoning across different pieces of text.\nTo verify that the results are reproducible given the nondeterminism in some of the models, we repeated the experiments for three of the most non-deterministic models. We observed very small variation (standard error) in model accuracy between runs: 82.07 (0.05) for Claude 3 Opus, 92.71 (0.01) for GPT-4 1106 Preview, and 87.73 (0.01) for Gemini 1.5 Pro."}, {"title": "5.3 Information Retrieval - Kitab", "content": "Motivation: Information Retrieval either from parametric knowledge or from input context is a task that applies to many search and knowledge seeking scenarios. At its core, the main question is whether it is possible to extract reliable factual knowledge from a model and whether it is possible to ground the model's answers in given context. Previous work has studied factuality by measuring model accuracy for questions whose output is expected to be single, often atomic facts [72, 61] or otherwise single facts that require multi-hop reasoning [119, 107]. However, given the generative nature of current models, a more compelling and contemporary scenario is the one where users form queries that expect a longer output with a list of items that satisfy the criteria behind what they are looking for (e.g., \u201ca list of ice cream shops in San Diego\u201d). It turns out that ensuring factuality and grounding for such longer generational tasks is challenging [6] for state-of-the-art models, despite long generation being one of the core promises of LFMs.\nBenchmark Description: Kitab [6] is a challenging dataset and a dynamic data collection approach for testing abilities of Large Language Models (LLMs) in answering information retrieval queries with constraint filters. A filtering query with constraints can be of the form \u201cList all books written by Toni Morrison that were published between 1970-1980\u201d. Kitab consists of book-related data across more than 600 authors and 13,000 queries with varying number of constraints and complexity. In each query in the dataset, the first constraint is always fixed to an author and the following can vary among the following types of book constraints to test for"}, {"title": "5.4 Toxicity Detection and Safe Language Generation - Toxigen", "content": "Motivation: Measuring harmful language generation and safety in general is a complex problem with various aspects and dimensions. In this work we use Toxigen dataset [44] for our measurements. It has a balanced number of neutral and toxic statements about various identity groups of people (mostly focusing on minorities), and includes the data to support both discriminative evaluation (LLM used as classifier for toxicity detection) and generative evaluation (LLM used as a text generator). Toxicity detection is important for content moderation and safety filtering, while generative setting is important for assessing language generation safety in response to various input stimuli.\nBenchmark Description: Toxigen is a large-scale dataset consisting of toxic and benign statements about 13 groups of people with a focus on implicit hate speech about minority groups that does not contain slurs or profanity. The dataset is designed to be balanced, i.e., for each identity group there are an equal number of toxic and neutral samples. This is important because the neutral samples can be used to measure erasure across different models where the identity mention of specific groups of people is treated as a signal of toxicity or hate and removed by the given LLM.\nFor experiments conducted on Toxigen, there are two evaluation schemes which we refer to as discriminative and generative. In discriminative evaluation, the model to be tested is presented with a sample from the dataset for which there exist ground truth labels generated by humans. The performance is then measured by the accuracy of the generated labels by the LLM and ground truth labels on whether the sample text is toxic or neutral. This scenario tests the LLM's capability to detect toxic content. In the generative evaluation, the target LLM is prompted with toxic or neutral content about different groups of people. Another LLM is then used as a judge to assign a toxicity score between 1 to 5 to the response generated by the target LLM. We use GPT-4 1106 Preview as the judge with a general prompt of the form \u201cGiven a statement, output ### Final score: {score} where score defines how toxic the statement is on a scale of 1 to 5, 1 being non-toxic and 5 highly toxic. Statement: {{statement}}\u201d. The model under test is provided with several statements from the given category to continue the generation. The ideal behavior will be to NOT continue the generation when statements are toxic, and, to NOT stop or avoid generation when statements in the prompt are neutral."}, {"title": "6 Non-Determinism Evaluation", "content": "Determinism is a desirable property in language models for providing consistent user experiences (i.e. providing the same output to identical queries), especially in user-facing systems, and for conducting reproducible evalu-ations of either research or product systems. Therefore, it is important to include metrics of nondeterminism in any evaluation of machine learning models.\nWe investigated and compared the level of determinism of model outcomes when the same instance (using the same prompt template) is inferenced multiple times, with temperature zero, fixed seed and top-p of 0.95. Under these settings, possible sources of non-determinism are GPU computation of floating-point numbers and small differences in the log prob of the top_k set, the use of Sparse Mixture of Experts, and varying hardware at inference time."}, {"title": "6.1 Experiment Setup and Metrics", "content": "We started with a stratified random sample of the following datasets: Kitab, IFEval, Geometric Reasoning, and MMMU. We repeated inference on each instance three times independently using the same prompt templates, then measured the entropy or standard error of model outputs for the same instance for categorical and numeric labels, respectively. Finally, we report the mean of the entropy and standard deviation over all instances. Both scores characterize the amount of per-example variation in the final metric for the stratified sample. For categor-ical metrics, we also report the percentage of instances where the results obtained from the three independent runs are not in agreement. Note that none of the tasks included in this analysis use other non-deterministic language models for evaluation, which means that any observed non-determinism can be fully attributed to the inference of the model under test and not to the evaluation process itself."}, {"title": "6.2 Results", "content": "Among all models investigated, Gemini 1.5 Pro, GPT-4 Turbo 2024-04-09, and GPT-4 Vision Preview/GPT-4 1106 Preview consistently exhibit the most nondeterminism across the four tasks. For example, on multi-modal knowledge understanding (MMMU), the three independent runs on the same instance lead to different outcomes in 21% and 26% of the cases for GPT-4 Vision Preview and Gemini 1.5 Pro, respectively. Similarly, we observe a high standard deviation (3% - 11%) across runs on Kitab for both of these models. GPT-4 Turbo 2024-04-09 has the highest entropy among all models in the Geometrical reasoning task and the 3rd highest entropy in MMMU.\nLlama 3 70B, Llama 3.1 70B, and Mistral Large 2407 consistently have non-determinism scores close to zero (lower non-determinism is better indicating perfect repeatability). A leading cause of this could be that these models are not Mixtures of Experts, while for the others there has been speculation in the community that this may be the case (although no official confirmation has been issued from OpenAI, Anthropic, or Google.)\nThe Claude family and GPT-40 2024-05-13 follow after Llama 3.1 405B as the next most deterministic models, although GPT-40 2024-05-13 is still notably non-deterministic in highly generative tasks like Kitab and GeoMeter datasets."}, {"title": "7 Backward Compatibility Evaluation", "content": "In this section, we present comparison results between models in terms of how backward compatible they are with previous model versions within the same family. In particular, we measure progress in terms of percentage of examples for which the new model version is better than the previous one, and regress as the percentage of examples for which the new model version is worse. For cases when the metric is binary (correct vs. incorrect) progress and regress track flips in the metric, while when the metric is continuous they track cases when the difference in the metric is higher or lower than a threshold. Previous work has also formulated other backward compatibility metrics such as backward trust compatibility and backward error compatibility[102], which re-spectively focus on the stability of correct and incorrect answers. Here, we simplify the measures to progress and regress so we can also compare them relatively with the percentage of cases where there are no changes between the two versions."}, {"title": "7.1 Datasets and Models", "content": "We run this analysis on three model families (Claude, GPT, and Llama) which have a recent model release and for which the previous model before the release was also a highly capable model based on our measurements in EUREKA-BENCH. The comparison here would study cases where for example a given user or application builder would replace their inference calls to GPT-4 1106 Preview with GPT-40 2024-05-13 (or GPT-4 Turbo 2024-04-09 with GPT-40 2024-05-13 for a multimodal task) and measures the amount of expected regression that will be associated with the model substitution. For the analysis on the Llama family, we compare Llama 3.1 70B vs. Llama 3 70B as they have the same parameter size and"}]}