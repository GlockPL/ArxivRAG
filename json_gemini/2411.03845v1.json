{"title": "RECONSIDERING THE PERFORMANCE OF GAE IN\nLINK PREDICTION", "authors": ["Weishuo Ma", "Yanbo Wang", "Xiyuan Wang", "Muhan Zhang"], "abstract": "Various graph neural networks (GNNs) with advanced training techniques and\nmodel designs have been proposed for link prediction tasks. However, outdated\nbaseline models may lead to an overestimation of the benefits provided by these\nnovel approaches. To address this, we systematically investigate the potential of\nGraph Autoencoders (GAE) by meticulously tuning hyperparameters and utilizing\nthe trick of orthogonal embedding and linear propagation. Our findings reveal that\na well-optimized GAE can match the performance of more complex models while\noffering greater computational efficiency.", "sections": [{"title": "INTRODUCTION", "content": "Link prediction is a fundamental task in the field of\ngraph learning, with applications spanning various do-\nmains such as recommendation systems (Zhang & Chen,\n2020), drug discovery (Souri et al., 2022) and knowledge\ngraph completion (Zhu et al., 2021b). GNNs have shown\nexceptional performance in these tasks and have become\na widely adopted approach. One representative GNN\nfor link prediction is the GAE model (Kipf & Welling,\n2016), which predicts the probability of link existence by\ncomputing the inner products of the representations of\ntwo target nodes generated by a Message Passing Neu-\nral Network (MPNN). However, GAE's expressiveness\nis limited; for instance, as illustrated in Figure 1, GAE\nproduces the same prediction for the links (v1, v2) and\n(21, 23) despite their differing structures. This limitation\narises from GAE's separate computation of node repre-\nsentations, leading to a disregard for the relationship be-\ntween the two nodes. To address this challenge, various methods have been proposed to enhance\nexpressiveness by incorporating pairwise information, such as utilizing labeling tricks (Zhang &\nChen, 2018), paths between two nodes(Zhu et al., 2021a), and neural common neighbors (Wang\net al., 2024). Despite these advancements, there has been limited empirical investigation into the\npotential performance of GAE when optimized properly.\nIn response to these concerns, we run GAE on the widely used OGB benchmark (Hu et al., 2020)\nwith common techniques from new models and extensive hyperparameter tuning, ensuring a fair\ncomparison with new models. Furthermore, insights from the discussion in Dong et al. (2023) sug-\ngest that linear propagation of orthogonal vectors in a GAE can overcome the limitations of subgraph\nisomorphism, effectively estimating common heuristics such as Common Neighbors (Barab\u00e1si &\nAlbert, 1999), Adamic-Adar(Adamic & Adar, 2003), and Resource Allocation (Zhou et al., 2009).\nBy employing orthogonal initialization on the ogbl-ddi and ogbl-ppa datasets and eliminating the"}, {"title": "RELATED WORK", "content": "Recently, several methods have been developed to address the limitations in the expressiveness of\nGAE. Representatives include SEAL (Zhang & Chen, 2018), Neo-GNN (Yun et al., 2021), BUDDY\n(Chamberlain et al., 2023) and NCN (Wang et al., 2024), which improve expressiveness by incorpo-\nrating pairwise information.\nSEAL extracts a subgraph for the target link (i, j) and calculates the shortest distance from (i, j)\nfor each node within that subgraph. It then augments this information with the node features and\napplies MPNN on the subgraph. Neo-GNN and BUDDY run MPNN directly on the entire graph\nwhile injecting high-order common neighbor information to improve predictions. NCN utilizes sum\npooling over the embeddings of the common neighbors of the source and target nodes. By explicitly\nleveraging pairwise information, these models effectively overcome the expressiveness limitations\ninherent in traditional MPNN approaches."}, {"title": "LINEAR GNN", "content": "Recent research has explored the potential of simplifying traditional MPNN by removing activation\nfunctions. SGC (Wu et al., 2019) eliminates both the activation functions and the parameter matrix,\nleading to a simplified yet effective model. Similarly, LightGCN (He et al., 2020) uses an attention\nblock to aggregate outputs of SGC with varying depths for the link prediction problem in recommen-\ndation systems. Building on this idea, HLGNN (Zhang et al., 2024) further enhances the approach\nand evaluates its performance on link prediction benchmark, showcasing competitive results. Ad-\nditionally, YinYanGNN (Wang et al., 2023) incorporates negative sampling into linear propagation\nschemes, and MPLP (Dong et al., 2023) pagates orthogonal embeddings linearly to estimate the\ncount of node labels. Collectively, these studies provide evidence that integrating linearity into link\nprediction models leads to improved performance. In this work, we aim to extend these findings by\noffering both theoretical analysis and experimental verification."}, {"title": "PRELIMINARIES", "content": "We consider an undirected graph G = (V, E, X), where V represents a set of n nodes, E denotes the\nset of edges, and X \u2208 Rn\u00d7d refers to the d-dimensional feature matrix of the nodes. The adjacency\nmatrix of the graph is denoted by A. The neighbors of a node i, defined as the set of nodes connected\nto i, are expressed as N(i, A) := {j \u2208 V | Aij > 0}. For simplicity, when A is fixed, we denote\nthe neighbors of i as N(i). The set of common neighbors between two nodes i and j is given by\nN(i) \u2229N(j)."}, {"title": "GRAPH CONVOLUTIONAL NETWORK", "content": "GCN has become a prominent tool for learning on graph data. It generalizes convolution to graph\nby recursively aggregating features from neighboring nodes. Specifically, the hidden representation\nof each node is updated by applying a graph convolution operation, which can be written as:\nH(1+1) = 0 (\u010e\u22121/2\u0100\u010e-1/2H(1)W(1)),\nwhere H(1) represents the node embeddings at layer l, \u0100 = A + In is the adjacency matrix with\nadded self-loops, D is the degree matrix of A, W(l) is a trainable weight matrix, and o denotes a\nnon-linear activation function."}, {"title": "METHOD", "content": "In this section, we propose our orthogonal embedding method for improving GAE. We also analyse\nhow it enhances the expressiveness of GAE."}, {"title": "IMPROVED GAE", "content": "Numerous studies have evaluated the performance of GAE on the OGB dataset, yet none have fully\noptimized their potential. The hyperparameters appear suboptimal, as simple adjustments\u2014such\nas fine-tuning the learning rate, extending the number of training epochs, and incorporating resid-\nual connections\u2014can immediately enhance performance. Additionally, the performance of GAE\nreported by Wang et al. (2024) may be underestimated due to factors like insufficient hidden dimen-\nsion size. It is noteworthy that even the straightforward integration of common neighbor information\ncan achieve results superior to those of advance models like BUDDY (Chamberlain et al., 2023), un-\nderscoring the potential of GAE.\nIn our implementation, we identified that some common practices for GAE are suboptimal. One is\nto add activation functions between convolutional layers. We find this activation results in severe\noverfitting. Additionally, node degree is commonly used as the initial node embedding, which we\nfound to be inferior to the use of orthogonal embeddings in some cases. In the datasets ogbl-ddi\nand ogbl-ppa, we employ a learnable orthogonal embedding as the input representation. In contrast,\nfor the datasets ogbl-collab and ogbl-citation2, it is necessary to incorporate the original features\ninto the prediction process. To achieve this, we concatenate the raw features with the learnable\northogonal embeddings to form the input for our Refined-GAE."}, {"title": "ANALYSIS ON ORTHOGONAL TRICK", "content": "We now elucidate how orthogonal embedding combined with linear propagation can enhance the\nexpressiveness of MPNN. Previous analyses on expressiveness have focused on graph structure,\noften overlooking the impact of input embeddings and the inner multiplication mechanism utilized\nby GAE. By using orthogonal embeddings as input and applying linear propagation, a node will\naggregate a linear combination of features from all its neighbors. When computing the inner product\nbetween two nodes, only the norms of embeddings from common neighbors will be preserved due to\nthe orthogonality of the embeddings, as illustrated in Figure 2. Consequently, this approach endows\nthe model with the capability to count common neighbors an inherent limitation of traditional\nMPNN-thereby significantly increasing its expressiveness.\nFormally, assuming X is initialized with orthogonal embeddings, i.e., XTX = I, where I is the\nidentity matrix. The linear propagation step in a MPNN can be represented as H = AX.\nFor two nodes i and j, the inner product of their propagated embeddings is given by:\nHH\u2081 = (AX) (AX); = AXXT Aj\nSince the embedding matrix X is orthogonal, this inner product simplifies to:\nHH\u2081 = \\sum |X_k|^2,"}, {"title": "EXPERIMENTS", "content": "Now we present the main result. We evaluate the performance of our refined GAE based on GCN\non OGB benchmark (Hu et al., 2020). Our code and detailed hyperparameters are available in\nhttps://github.com/GraphPKU/Refined-GAE.\nWe include the official OGB results as GAE baselines. To provide a more comprehensive compar-\nison, we also incorporate heuristic methods such as Common Neighbors (CN) (Barab\u00e1si & Albert,\n1999), Adamic-Adar (AA) (Adamic & Adar, 2003), and Resource Allocation (RA) (Zhou et al.,\n2009), as well as new models like SEAL (Zhang & Chen, 2018), NeoGNN (Yun et al., 2021),\nNBFNet (Zhu et al., 2021b), BUDDY (Chamberlain et al., 2023), NCN (Wang et al., 2024) and\nMPLP (Dong et al., 2023). The baseline results are from Wang et al. (2024), Dong et al. (2023), and\nHu et al. (2020). Please refer to Appendix A for detailed experiment setting."}, {"title": "PERFORMANCE", "content": "The main results are presented in Table 1. Our refined GAE model demonstrates an average im-\nprovement of 71% compared to the original version and is even comparable to some of the most\nrecent models, particularly on the ogbl-ddi and ogbl-ppa datasets, where the orthogonal embed-\nding technique significantly boosts performance. Specifically, on the ogbl-ppa dataset, our model\nachieves a 295% improvement over the original model, surpassing the MPLP model (Dong et al.,\n2023) by a margin of 13%.\nIn the ogbl-collab dataset, we follow the official rule by incorporating validation edges into both\nmodel training and input, as many recent models do. However, some models only use validation\nedges during the test phase, which typically results in relatively lower performance. Under this\nscenario, our model achieves a performance of 63.93 \u00b10.22.\nThe performance improvement of the GAE model using our proposed method is less significant\non the ogbl-collab and ogbl-citation2 datasets. This issue might stem from the concatenation of\nsmoothed features with orthogonal embeddings. As noted by (Mao et al., 2024), the structural and\nfeature components of a graph can sometimes provide conflicting training signals. Consequently,\nthe model may become confused when processing a combination of structural information from\northogonal embeddings and feature information from the feature embeddings. This observation\nsuggests that concatenation might not be the most effective approach for integrating these different\ntypes of information."}, {"title": "EFFICIENCY", "content": "Besides its superior performance, our improved GAE also has high scalability due to its simple\narchitecture. GAE-like models do not require the explicit calculation of pairwise information, in-\nherently possess an efficiency advantage over models that utilize sophisticated mechanisms to inject\npair-wise information. For example, SEAL necessitates the extraction of subgraphs and the calcula-\ntion of DRNL for each node (Zhang & Chen, 2018), resulting in significant computational overhead.\nSimilarly, BUDDY requires precomputation of subgraph-related information and structural features\nfor each new batch of prediction edges (Chamberlain et al., 2023). We present a comparison of\ntime complexity in Table 2. The results highlighting the differences in training and inference times\nbetween models in Figure 3, providing insights into the efficiency of these approaches both theo-\nretically and practically. The theoretical time complexity of our Refined-GAE is identical to that\nof the original GAE, and the actual inference time does not significantly increase either. Detailed\nexplanation for complexity can be found in Appendix B."}, {"title": "ABLATION STUDY", "content": "We conduct ablation study to reveal\nthe source of our models' perfor-\nmance gain. Results are presented in\nTable 3. These experiments can be\ndivided into four parts: the verifica-\ntion of our orthogonal trick, compar-\nisons of different convolutional lay-\ners, comparisons of various predic-\ntors, and usage of synthetic features.\nWe first investigate the effect of the\nproposed orthogonal trick. Introduc-\ning non-linearity in the convolutional\nlayer or using non-orthogonal vectors\nas inputs both significantly degrade\nperformance, which validates the ef-\nfectiveness of our method.\nIn our exploration of various convo-"}, {"title": "CONCLUSION", "content": "In this report, we explore the potential power of GAE, fine tuning the existing model and using\northogonal embedding plus linear propagation to enhance the performance of GAE and explain its\nhidden mechanism. We demonstrate that simple MPNN methods can reach comparable performance\ncompared to new models.\nGiven that many new models utilize MPNN as their backbone, we are interested in exploring how\nwell these models perform when integrated with our refined GAE. This investigation will be reserved\nfor future work."}]}