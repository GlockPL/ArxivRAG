{"title": "ARTIFICIAL INTELLIGENCE FOR MICROBIOLOGY AND\nMICROBIOME RESEARCH", "authors": ["Xu-Wen Wang", "Tong Wang", "Yang-Yu Liu"], "abstract": "Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology\nand microbiome research now experiencing significant breakthroughs through machine learning and\ndeep learning applications. This review provides a comprehensive overview of AI-driven approaches\ntailored for microbiology and microbiome studies, emphasizing both technical advancements and\nbiological insights. We begin with an introduction to foundational AI techniques, including primary\nmachine learning paradigms and various deep learning architectures, and offer guidance on choosing\nbetween machine learning and deep learning methods based on specific research goals. The primary\nsection on application scenarios spans diverse research areas, from taxonomic profiling, functional\nannotation & prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision\nnutrition, clinical microbiology, to prevention & therapeutics. Finally, we discuss challenges unique\nto this field, including the balance between interpretability and complexity, the \"small n, large p\"\nproblem, and the critical need for standardized benchmarking datasets to validate and compare models.\nTogether, this review underscores AI's transformative role in microbiology and microbiome research,\npaving the way for innovative methodologies and applications that enhance our understanding of\nmicrobial life and its impact on our planet and our health.", "sections": [{"title": "1 Introduction", "content": "For over 3.5 billion years, our planet and its inhabitants have been shaped by various microorganisms [1]. For example,\nCyanobacteria, through photosynthesis, produced oxygen and contributed to the Great Oxygenation Event around 2.4\nbillion years ago, making the Earth hospitable for aerobic life [2]. Certain bacteria, like Rhizobium, fix atmospheric\nnitrogen into forms usable by plants, supporting plant growth and agriculture [3]. Commensal microbes in human and\nanimal guts aid in digestion and nutrient absorption, essential for health and survival [4]. Similarly, some microbes can\nbreak down organic matter, recycling nutrients in ecosystems, which is vital for maintaining soil fertility and ecosystem\nbalance [5]. Given the profound influence microorganisms have had on the evolution of life and the functioning of\necosystems, advancing microbiology research is crucial for understanding and harnessing these processes to benefit\nhealth, agriculture, and environmental sustainability.\nIt is not a big surprise that disrupted microbial communities (or microbiomes) can have a huge impact on our planet and\nourselves. Indeed, agricultural practices, such as excessive use of chemical fertilizers and pesticides, can disrupt soil\nmicrobiomes, leading to reduced soil fertility and increased vulnerability to erosion [6]. Runoff containing pollutants\nand antibiotics can significantly disrupt the microbiomes of freshwater and marine ecosystems, leading to changes in\nwater quality and impacting the health of aquatic life by altering the natural balance of microbial communities within the\nenvironment; this can potentially promote the growth of harmful bacteria and disrupt critical ecological processes like\nnutrient cycling [7, 8]. Many human diseases have been associated with disrupted microbiomes, including acne, eczema,\ndental caries, obesity, malnutrition, inflammatory bowel disease, asthma/allergies, hardening of arteries, colorectal\ncancer, type 2 diabetes, as well as neurological conditions such as autism, anxiety, depression, and post-traumatic stress\ndisorder, etc [9, 10]. Gaining a deeper understanding of the activities of microbial communities, both within and around\nus, can greatly benefit our health and the health of our planet. This explains why in the past decades the microbiome\nhas been a very active research topic in microbiology.\nArtificial Intelligence (AI) focuses on creating intelligent machines that can execute tasks that usually need human\nintelligence. Al emerged as an academic discipline at the 1956 Dartmouth conference, shaped by pioneering work by\nWarren McCulloch, Walter Pitts, and Alan Turing on neural networks and machine intelligence. At first, AI research\nconcentrated on symbolic reasoning, including early applications in biomedicine, such as the MYCIN expert system\nfor diagnosing bacterial infections. Meanwhile, machine learning developed, showcasing algorithms that improved\nthrough data training. Despite early excitement and positive forecasts, the pace of AI advancement decelerated over\nthe following decades, hindered by hardware constraints and unmet expectations, leading to a period known as \"AI\nwinter.\" However, the domain continued to progress, incorporating probabilistic methods to manage uncertainty. In\naround 2010, a new phase in AI emerged, fueled by breakthroughs in deep learning frameworks, the advent of powerful\nhardware (e.g., GPUs), open-source software tools, and greater access to extensive datasets (e.g., ImageNet [11]). In\n2012, significant breakthroughs occurred when AlexNet (a deep learning architecture based on the convolutional neural\nnetwork) surpassed preceding machine learning methodologies in visual recognition [12]. The subsequent innovations,\nparticularly the Transformer (a deep learning architecture initially developed for machine translation) introduced in\n2017 [13], triggered an \"AI boom\" marked by considerable investment. This surge in investment led to a wide range of\nAI applications by the 2020s, accompanied by increasing concerns regarding its societal implications and the pressing\nneed for regulatory measures."}, {"title": "2 Artificial Intelligence Techniques", "content": "The multiple subfields of AI research are focused on specific objectives and the utilization of distinct tools. The\nconventional objectives of AI research encompass searching, knowledge representation, reasoning, planning, learning,\ncommunicating, perceiving, and acting [52]. Most AI applications in microbiology and microbiome research rely on\nmachine learning, which is the focus AI subfield of this Review."}, {"title": "2.1 Learning Paradigms", "content": "Machine learning is a subfield of AI that employs algorithms and statistical models, enabling machines to learn from\ndata and improve their performance on specific tasks over time [53]. Machine learning is typically categorized into\nthree primary learning paradigms: supervised learning, unsupervised learning, and reinforcement learning. These\nparadigms differ in the specific tasks they can address as well as in the manner in which data is presented to the\ncomputer. Generally, the nature of the task and the data directly influence the selection of the appropriate paradigm.\nSupervised learning involves using labeled datasets, where each data point is linked to a class label. The algorithms in\nthis approach aim to create a mathematical function that connects input features to the expected output values, relying\non these labeled instances. Common uses include classification and regression. Classical machine learning methods for\nclassification/regression include Logistic Regression, Na\u00efve Bayes, Support Vector Machine (SVM), Random Forest,\nExtreme Gradient Boosting (XGBoost), etc. Those methods have been heavily used in microbiology and microbiome\nresearch.\nIn unsupervised learning, algorithms analyze unlabeled data to detect patterns and relationships without any defined\ncategories. This process uncovers similarities in the dataset and includes techniques like clustering, dimensionality\nreduction, and association rules mining. Classical unsupervised learning methods include k-means clustering, Principal\nComponent Analysis (PCA), Principal Coordinate Analysis (PCoA), and t-distributed stochastic neighbor embedding\n(t-SNE) for dimension reduction, and the Apriori algorithm for association rules mining. Among them, PCoA is a\ncommonly used tool in microbiome data analysis, particularly valuable for visualizing and interpreting the differences\nin microbial community composition between samples.\nReinforcement learning focuses on enabling intelligent agents to learn through trial-and-error in a dynamic environment\nto maximize their cumulative rewards [54, 55, 56]. Without labeled datasets, these agents make decisions to maximize\nrewards, engaging in autonomous exploration and knowledge acquisition, which is crucial for tasks that are difficult to\nprogram explicitly.\nIntegrating these paradigms can often lead to better outcomes. For instance, semi-supervised learning finds a middle\nground by utilizing a small set of labeled data alongside a larger collection of unlabeled data. This method harnesses\nthe strengths of both supervised and unsupervised learning, making it a cost-effective and efficient way to train models\nwhen labeled data is scarce. In situations where obtaining high-quality labeled data is difficult, self-supervised learning\npresents a viable alternative [57]. In this framework, models are pre-trained on unlabeled data, with labels generated\nautomatically in subsequent iterations. Self-supervised learning effectively converts unsupervised machine learning\nchallenges into supervised tasks, improving learning efficiency.\nTransfer learning is another interesting machine learning technique, which involves taking a pre-trained model on\na large dataset and fine-tuning it on a smaller, task-specific dataset [58, 59]. This approach leverages the knowledge"}, {"title": "2.2 Deep learning techniques", "content": "As a subfield of machine learning, deep learning represents a further specialization that utilizes deep neural networks to\nprocess and analyze large datasets, allowing for the automatic identification of patterns and the solving of complex\nproblems. The reason why we often need a deeper rather than a wider neural network is that, if we regard a neural\nnetwork as a function approximator, the complexity of the approximation function will typically grow exponentially\nwith depth (not width). In other words, with the same number of parameters, a deep and narrow network has stronger\nexpressive power than a shallow and wide network [65, 66, 67, 68, 69].\nBased on the three primary machine learning paradigms, deep learning can be broadly divided into three major categories\n(Fig.1). The first category includes deep networks for supervised or discriminative learning, such as Multi-Layer\nPerceptron (MLP), Convolutional Neural Network (CNN) and their variants, Recurrent Neural Network (RNN) and"}, {"title": "2.3 When to Use Machine learning vs. Deep learning?", "content": "We do not always need fancy deep learning techniques for microbiology and microbiome research. Sometimes we\ndo not need deep learning at all. Logistic Regression or Random Forest might work very well. Choosing between\ndeep learning and traditional machine learning methods depends on data characteristics, the specific problem at hand,\navailable computational resources, and the need for model interpretability. Traditional methods are generally preferred\nfor smaller, structured datasets and scenarios requiring interpretability (such as clinical applications), while deep\nlearning excels with large, unstructured datasets and complex tasks requiring high performance.\nIf we decide to apply or develop deep learning methods to solve our problem, there is a general procedure [75]. First,\nwe need to choose the appropriate performance metrics (e.g., Accuracy, Precision, Recall, F1-score, AUROC, AUPRC).\nSecond, we need to find the default baseline deep learning models based on the data structure. For supervised learning\ntasks that involve fixed-size vector inputs, it is advisable to utilize a feedforward network featuring fully connected\nlayers (e.g., MLP). If the input possesses a known topological structure, such as images or graphs, opting for CNN or\nits variants (e.g., graph convolutional network (GCN)) is recommended. When dealing with inputs or outputs that form\nsequences, we should consider using RNN and its variant (e.g., LSTM or GRU) or Transformer. 1D CNN or temporal\nconvolutional network (TCN) might also work. Depending on the task, a hybrid deep learning model could also be\nconsidered. Third, we need to establish a reasonable end-to-end system, which involves choosing the appropriate\noptimization algorithm (e.g., SGD with momentum, Adam) and incorporating regularization (via early stop, dropout, or\nbatch normalization). Finally, we need to measure the performance and determine how to improve it. We can either\ngather more training data or tune hyperparameters (e.g., learning rate, number of hidden units) via grid search or random\nsearch."}, {"title": "3 Application Scenarios", "content": "There are numerous applications of AI techniques in microbiome research. We can briefly group those applications into\nthe following scenarios: taxonomic profiling, functional annotation & prediction, microbe-X interactions, microbial\necology, metabolic modeling, precision nutrition, clinical microbiology, prevention & therapeutics. For each application\nscenario, there are many specific tasks. In the following, we will present each of the specific tasks and the representative\nAI methods."}, {"title": "3.1 Taxonomic Profiling", "content": "A fundamental goal of microbiology and microbiome research is determining the compositions of microbial com-\nmunities, i.e., identifying and quantifying different types of microorganisms (such as bacteria, fungi, viruses, and\narchaea) present in a given sample. This involves analyzing their relative abundances and diversity, often using DNA"}, {"title": "3.1.1 Metagenome assembly", "content": "Metagenomics refers to the direct study of the entire genomic information contained in a microbial community.\nMetagenomics avoids isolating and culturing individual microorganisms in a community and provides a way to study\nmicroorganisms that cannot be isolated and cultured. There are two main approaches for processing metagenomic\nsequencing data: (1) assembly-based and (2) reference database-based. The goal of the assembly-based approach is to\nconstruct and annotate the so-called metagenome-assembled genomes (MAGs) [81]. The construction and annotation of\nMAGs have greatly promoted our understanding of microbial populations and their interactions with the environment.\nIt is worth noting that most MAGs represent new species, which helps to understand the so-called microbial dark matter.\nThe process of constructing MAGs includes two main steps: assembly and binning. Assembly refers to the process of\nreconstructing longer sequences (contigs) from short DNA reads obtained through sequencing. This involves piecing\ntogether overlapping reads to form continuous sequences that represent parts of the genomes present in the microbial\ncommunity.\nDeep learning has been widely used in the quality control of metagenomic assembly. Many factors (e.g., sequencing\nerrors, variable coverage, repetitive genomic regions, etc.) can produce misassemblies. For taxonomically novel\ngenomic data, detecting misassemblies is very challenging due to the lack of closely related reference genomes. Deep\nlearning methods can identify misassembled contigs in a reference-free manner. Representative methods include\nDeepMAsEd [82] and ResMiCo [83]. DeepMAsEd is based on CNN. Denote a contig as a sequence of nucleotides. At\neach position in the sequence, the concatenation of two types of information (raw sequence and read-count features)\nyields the input vector. To train and evaluate DeepMAsEd, one can generate a synthetic dataset of contigs, read counts,\nand binary assembly quality labels. As an extension of DeepMAsEd, ResMiCo is based on ResNet, a variant of CNN.\nThe key feature of ResNet is the introduction of skip connections, which effectively solves the degradation problem\nof deep neural networks [84]. Compared to DeepMAsEd, ResMiCo leveraged a much more informative input vector\ncomputed from raw reads and contigs. Moreover, ResMiCo was trained on a very large and varied dataset. Through\nthorough validation, it was demonstrated that ResMiCo significantly outperforms other methods in accuracy, and the\nmodel remains robust when faced with novel taxonomic diversity and different assembly methods. We notice that both\nDeepMAsEd and ResMiCo used a carefully designed input vector. It would be interesting to explore if we can use a\nmore advanced deep learning architecture (e.g., the Transformer) or a hybrid learning approach (e.g., CNN + RNN) to\ndirectly deal with the raw sequence data, avoiding the manual design of the input vector."}, {"title": "3.1.2 Metagenome binning", "content": "Metagenomic binning involves grouping those assembled sequences into clusters (bins or MAGs) that correspond to\ndifferent species or genomes. Metagenomic binning helps in identifying and categorizing the different microorganisms\npresent in a metagenomic sample, even if they are not fully assembled into complete genomes. There are many methods\nfor metagenomic binning [85, 86, 87, 88", "89": "nCLMB [90", "91": "GraphMB [92", "93": "."}]}