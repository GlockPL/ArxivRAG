{"title": "COMPLEX PHYSICS-INFORMED NEURAL NETWORK", "authors": ["Chenhao Si", "Ming Yan", "Xin Li", "Zhihong Xia"], "abstract": "We propose compleX-PINN, a novel physics-informed neural network (PINN) architecture that incorporates a learnable activation function inspired by Cauchy's integral theorem. By learning the parameters of the activation function, compleX-PINN achieves high accuracy with just a single hidden layer. Empirical results show that compleX-PINN effectively solves problems where traditional PINNs struggle and consistently delivers significantly higher precision- often by an order of magnitude.", "sections": [{"title": "Introduction", "content": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful method for solving both forward and inverse problems involving Partial Differential Equations (PDEs) [1-4]. PINNs leverage the expressive power of neural networks to minimize a loss function that enforces the governing PDEs and boundary/initial conditions. This approach has been widely applied across various domains, including heat transfer [5-7], solid mechanics [8-10], incompressible flows [11-13], stochastic differential equations [14, 15], and uncertainty quantification [16, 17].\nDespite their success, PINNs face significant challenges and often struggle to solve certain classes of problems [18, 19]. One major difficulty arises in scenarios where the solution exhibits rapid changes, such as in \u2018stiff' PDEs [20], leading to issues with convergence and accuracy. To address these limitations, researchers have proposed various techniques to improve training efficiency and precision.\nOver the years, numerous strategies have been developed to enhance the performance of PINNs, including adaptive weighting of loss functions and selective sampling of training points. For example, Wang et al. [19] leveraged the Neural Tangent Kernel (NTK) to analyze gradient evolution in PINN training, adjusting the weights of each loss component accordingly. Other studies [21,22] have explored methods for dynamically learning these weights during training. Additionally, adaptive sampling techniques have been introduced to tackle stiff problems by focusing on regions with high residuals. Lu et al. [23] proposed a threshold-based approach for selecting new training points, while Wu et al. [24] introduced a probability density function derived from residuals to improve sampling efficiency. Further extensions include high-dimensional adaptive methods [25] and re-sampling techniques targeting failure regions [26]."}, {"title": "Physics-Informed Neural Network", "content": "Denote the spatial domain as \u03a9\u2282R\u201d with boundary \u0398\u03a9, and let T represent the time domain. The spatial-temporal variable is given by (x, t) \u2208 \u03a9 \u00d7 T. A time-dependent partial differential equation (PDE) over this domain is defined as follows:\n$F[u](x,t) = 0, \\quad (x, t) \\in \\Omega\\times T,$\n$B[u](x, t) = 0, \\quad (x, t) \\in \\partial\\Omega \\times T, \\quad (boundary \\ condition)$\n$I[u](x, 0) = 0, \\quad x \\in \\Omega, \\quad (initial \\ condition)$\nwhere F, B, and I are differential operators, and u(x, t) is the solution to the PDE, subject to boundary and initial conditions.\nA PINN parameterized by @ approximates the solution u(x, t). The input to the neural network is (x, t), and the approximation is denoted by \u00fb(0)(x, t). The PINN minimizes the following objective function:\n$L(\\theta) = \\lambda_F L_F(\\theta) + \\lambda_B L_B(\\theta) + \\lambda_I L_I(\\theta),$\nwhere\n$L_F(\\theta) = \\frac{1}{N_f}\\sum_{(x,t) \\in \\Omega_F} |F[\\hat{u}(\\theta)](x,t)|^2,$\n$L_B(\\theta) = \\frac{1}{N_b}\\sum_{(x,t) \\in \\Omega_B} |B[\\hat{u}(\\theta)](x,t)|^2,$\n$L_I(\\theta) = \\frac{1}{N_0}\\sum_{(x,0) \\in \\Omega_I} |I[\\hat{u}(\\theta)](x, 0)|^2.$"}, {"title": "Complex Physics-Informed Neural Network", "content": "The Cauchy activation function, introduced in [43], is defined as:\n$\\Phi(x; \\mu_1, \\mu_2, d) = \\frac{\\mu_1 x}{x^2 + d^2} + \\frac{\\mu_2}{x^2 + d^2},$\nwhere \u03bc\u2081, \u03bc2, and d are trainable parameters. This activation function is inspired by Cauchy's integral formula, as we further elaborate in Section 3.1. We refer to a PINN model employing the Cauchy activation function as compleX-PINN.\nWe would like to note at the outset that our network is initially constructed with a single hidden layer, where each neuron has a unique set of parameters {\u00b51,\u00b52,d}. Consequently, the total number of trainable parameters for the Cauchy activation function is 3 \u00d7 Nneuron, where Nneuron represents the width of the layer."}, {"title": "1D Cauchy's integral formula and the Cauchy activation function", "content": "This section introduces Cauchy's integral formula and derives the Cauchy activation function from it.\nTheorem 1 (Cauchy's Integral Formula) Let f be a complex-valued function on the complex plane. If f is holomorphic inside and on a simple closed curve C, and z is a point inside C, then:\n$f(z) = \\frac{1}{2\\pi i} \\oint_C \\frac{f(\\zeta)}{\\zeta - z} d\\zeta.$\nCauchy's integral formula expresses the value of a function at any point z as a function of known values along a closed curve C that encloses z. Remarkably, this principle is akin to machine learning, where the values at new points are inferred from the known values.\nIn practice, we approximate the integral using a Riemann sum over a finite number of points on the curve C. Let $\u03b6_1, \u03b6_2, ..., \u03b6_m$ be a sequence of m points on C. Then,\n$f(z) \\approx \\frac{1}{2\\pi i} \\sum_{k=1}^m \\frac{f(\\zeta_k)}{\\zeta_k - z} (\\zeta_{k+1} - \\zeta_k) := \\sum_{k=1}^m \\frac{\\lambda_k}{\\zeta_k - z},$\nwhere, for convenience, we set \u03dam+1 = 51 and define Ak = $\\frac{f(\\zeta_k)}{2\\pi i} (\\zeta_{k+1} - \\zeta_k)$\nIf our target function f is real and one-dimensional, we obtain:\n$f(x) \\approx Re\\left[\\sum_{k=1}^m \\frac{\\lambda_k}{\\zeta_k - x}\\right] = \\sum_{k=1}^m \\frac{Re(\\lambda_k) Re(\\zeta_k) + Im(\\lambda_k) Im(\\zeta_k) - Re(\\lambda_k) x}{(x - Re(\\zeta_k))^2 + (Im(\\zeta_k))^2}.$\nWith the Cauchy activation function defined in (8), we have\n$f(x) \\approx \\sum_{k=1}^m \\Phi (x - Re(\\zeta_k); -Re(\\zeta_k), Re(\\lambda_k) Re(\\zeta_k) + Im(\\lambda_k) Im(\\zeta_k), (Im(\\zeta_k))^2) .$\nThis shows that a one-layer neural network with the Cauchy activation function (8) can approximate the real function f(x)."}, {"title": "Multi-dimensional Cauchy's integral formula", "content": "This section extends Cauchy's integral formula to the multi-dimensional case.\nTheorem 2 (Multi-Dimensional Cauchy's Integral Formula) Let f(z) be holomorphic in a compact domain U C CN within N-dimensional complex space. For simplicity, assume that U has a product structure: U = U1 \u00d7 U2 \u00d7 . . . \u00d7 UN, where each U\u017c, i = 1, 2, . . ., N, is a compact domain in the complex plane. Let P denote the surface defined by\n$P = \\partial U_1 \\times \\partial U_2 \\times .. \\times \\partial U_N,$\nthen a multi-dimensional extension of Cauchy's integral formula for (Z1, Z2, ..., zn) \u2208 U is given by:\n$f(z_1, z_2,..., z_N) = \\left(\\frac{1}{2\\pi i}\\right)^N \\int_P \\frac{f (\\zeta_1, \\zeta_2,..., \\zeta_N)}{(\\zeta_1 - z_1) (\\zeta_2 - z_2)...(\\zeta_N - z_N)} d\\zeta_1...d\\zeta_N.$\nSimilarly, we approximate the integral by a Riemann sum over a finite number of points. More precisely, for any integer l = 1, \u2026 \u2026 \u2026, N, let $\u03b6_1^l, \u03b6_2^l, ..., \u03b6_{m_l}^l$ be a sequence of $m_l$ points on dU\u0131. Then,\n$f(z_1, z_2,..., z_N) \\approx \\left(\\frac{1}{2\\pi i}\\right)^N \\sum_{k_1=1}^{m_1}\\sum_{k_2=1}^{m_2} ...\\sum_{k_N=1}^{m_N} \\frac{f(\\zeta_{k_1}^1, \\zeta_{k_2}^2,..., \\zeta_{k_N}^N)}{(\\zeta_{k_1}^1 - z_1)(\\zeta_{k_2}^2 - z_2)...(\\zeta_{k_N}^N - z_N)}  (\\zeta_{k_1+1}^1 - \\zeta_{k_1}^1)(\\zeta_{k_2+1}^2 - \\zeta_{k_2}^2)...(\\zeta_{k_N+1}^N - \\zeta_{k_N}^N),$\nwhere, for convenience, we set $\u03b6_{m_i+1}^i = \u03b6_1^i$ for l = 1, 2, . . ., N.\nCollecting all terms that are independent of 21,. ., ZN, we define\n$\\lambda_{k_1,...,k_N} = \\left(\\frac{1}{2\\pi i}\\right)^N f(\\zeta_{k_1}^1, \\zeta_{k_2}^2,..., \\zeta_{k_N}^N) (\\zeta_{k_1+1}^1 - \\zeta_{k_1}^1)(\\zeta_{k_2+1}^2 - \\zeta_{k_2}^2)...(\\zeta_{k_N+1}^N - \\zeta_{k_N}^N),$\nso that we can rewrite the approximation as\n$f(z_1, z_2, ..., z_N) \\approx \\sum_{k_1=1}^{m_1}\\sum_{k_2=1}^{m_2} ...\\sum_{k_N=1}^{m_N} \\frac{\\lambda_{k_1,...,k_N}}{(\\zeta_{k_1}^1 - z_1)(\\zeta_{k_2}^2 - z_2)...(\\zeta_{k_N}^N - z_N)}.$\nSince the order of the sample points no longer matters, we can rewrite the sample points as a single sequence ($\u03b6_k^1, \u03b6_k^2, ..., \u03b6_k^N$) for k = 1, 2, . . ., m, where m = m1m2\u2026\u2026mN. Thus, we finally obtain\n$f(z_1, z_2, ..., z_N) \\approx \\sum_{k=1}^m \\frac{\\lambda_k}{(\\zeta_k^1 - z_1)(\\zeta_k^2 - z_2)...(\\zeta_k^N - z_N)},$\nwhere 1, 2, ...,Am are parameters that depend on the sample points ($\u03b6_k^1, \u03b6_k^2, ..., \u03b6_k^N$) and the values f($\u03b6_1^1, \u03b6_2^2,..., \u03b6_N^N$) for k = 1, 2, . . ., \u0442."}, {"title": "Extend the Cauchy activation function to high dimensional space", "content": "The Cauchy approximation formula derived above can be computationally inefficient when the dimension N is high, due to the large number of multiplicative terms in the denominator. Therefore, for large N, we opt for a simplified representation of the function by applying the Cauchy activation function to linear combinations of the variables. Generally, this corresponds to a dual representation of the function, which is especially efficient for feature-finding in high-dimensional problems. Specifically, we approximate the target function f (x1,x2,..., XN) by\n$f(x_1, x_2,..., x_N) \\approx \\sum_{k=1}^m \\Phi(W_{k1}x_1 + W_{k2}x_2 +\u2026\u2026+ W_{kN}x_N + b_k; \u00b5_{k1}, \u00b5_{k2}, d_k),$\nwhere each I is a Cauchy activation function as defined in Equation (8). Here, the parameters Wk1, Wk2,...,WkN, bk, \u00b5k1, \u00b5k2, and de are trainable, allowing the network to capture the complex relationships among the input variables.\nWhile the theoretical foundation of the approximation involves a single hidden layer, in practice, we can enhance the model's capacity and expressiveness by constructing a neural network with multiple layers. This multilayer approach allows the model to capture hierarchical features and complex dependencies among the input variables, which are especially useful in high-dimensional cases.\nEach hidden layer can be viewed as applying a set of nonlinear transformations to the input space, creating intermediate representations that capture interactions across multiple input variables. For a network with L hidden layers, we recursively define each layer's output as:\n$h^{(l)} = \\Phi (W^{(l)}h^{(l-1)} +b^{(l)}; \\mu^{(l)}, d^{(l)}),$\nwhere h(l) is the output of the l-th hidden layer, W(l) is the weight matrix of layer l, b(l) is the bias vector, \u03bc(l) and d(l) are the parameters of the Cauchy activation function for the l-th layer.\nThis recursive formulation allows the network to approximate increasingly complex functions as the number of layers grows, providing both depth and flexibility.\nIn this way, the network gains both practical advantages from multilayer structures and theoretical grounding from Cauchy's integral approach, providing a novel architecture that can effectively approximate high-dimensional functions."}, {"title": "Numerical Experiments", "content": "In this section, we compare compleX-PINN with traditional PINN [1], Residual-Based Attention (RBA) PINN [42], and gradient-enhanced PINN (gPINN) [35] across various partial differential equations (PDEs), including the wave equation (Section 4.1), diffusion-reaction system (Section 4.2), Helmholtz equation (Section 4.3), reaction equation (Section 4.4), and convection equation (Section 4.5).\nCompleX-PINN is uniquely characterized by a single hidden layer, referred to as the Cauchy layer. Unlike other methods, each neuron in the Cauchy layer is parameterized by trainable parameters {\u00b51, \u03bc2, d}, which provide greater flexibility in capturing complex solution patterns. Although each neuron introduces three additional parameters, the overall number of trainable parameters in compleX-PINN remains significantly smaller than multi-layer architectures used by competing methods, as a single hidden layer is sufficient for high performance. By default, the parameters {\u00b51, \u03bc2, d} are initialized to 0.1, unless otherwise specified in particular experiments.\nFor testing, a uniform grid of points is generated for each dimension. For example, a 300 \u00d7 300 grid is used for the 2D case, a 300 \u00d7 300 \u00d7 300 grid for the 3D case, and this pattern is extended for higher-dimensional cases. Training points are then randomly sampled from these grids and are kept fixed across methods to ensure consistency. The number of training points is denoted as follows: Nf for the residual, No for the boundary conditions, and No for the initial conditions.\nTo guarantee a fair comparison, all experiments are conducted with identical settings, including the learning rate, optimizer, and number of training epochs. Furthermore, the same random seed is used across all methods to enhance reproducibility. However, experimental setups, such as the number of training points or specific network architectures, may vary between PDEs to accommodate their unique characteristics.\nPerformance is evaluated using the relative L2 error and the L\u221e norm, which are defined as follows:\n$(Relative) \\ L^2 \\ error = \\sqrt{\\frac{\\sum_{k=1}^N |\\hat{u}(x_k, t_k) - u(x_k, t_k)|^2}{\\sum_{k=1}^N |u(x_k, t_k)|^2}},$\n$L^\\infty \\ norm = max_{1<k<N} |\\hat{u}(x_k, t_k) - u(x_k,t_k)|,$\nwhere u is the true solution of the PDE, \u00fb is the output from the tested method, and N is the number of testing points."}, {"title": "Wave equation", "content": "We consider the following 1D wave equation:\n$u_{tt} = c^2 u_{xx}, \\qquad x \\in [0, 1], t \\in [0, 1],$\n$u(0, x) = \\frac{1}{4} sin(\\pi x) + \\frac{1}{8} sin(4\\pi x), \\qquad x \\in [0, 1],$\n$u_t (0, x) = 0, \\qquad x \\in [0, 1],$\n$u(t, -1) = u(t, 1) = 0, \\qquad t\\in [0, 1],$\nwhose exact solution is given by:\n$u(t, x) = \\frac{1}{4} sin(\\pi x) cos(2\\pi t) + \\frac{1}{8} sin(4\\pi x) cos(8\\pi t).$\nThe wave equation is known to be challenging for PINNs due to its stiffness and the resulting difficulty in capturing high-frequency components of the solution [19,21]. Traditional PINNs have slow convergence and suboptimal predictions. To address this challenge, the Residual-Based Attention (RBA) [42] has been proposed to improve PINNs by dynamically adjusting the weights of training points based on their residuals. RBA adaptively prioritizes regions of the solution domain where the model struggles the most, making it particularly effective for stiff PDEs. The weight XF in Eq. (4) is updated iteratively using the following rule:\n$\\lambda_{F_i}^{k+1} = \\gamma \\lambda_{F_i}^k + (1 - \\gamma) \\frac{\\lambda_{F_i}^k |r_i|^\\eta}{\\overline{|r|}},$\nwhere XF\u2081 is the weight associated with the training point (xi, ti), ri is the residual ri = F[u](xi, ti), and r is the residual vector. The hyperparameters \u03b3 and \u03b7 control the contribution of the residual term and its smoothing, and are set as in the original RBA implementation (\u03b7 = 0.001, \u03b3 = 0.999). For additional details, refer to [42].\nIn this experiment, we set Nf = 6000 and the boundary conditions and initial conditions are enforced by the hard constraint formulated as \u00fb = 20x(1 \u2013 x)t\u00b2\u00fbnn + sin (\u03c0x) + 0.5 sin (4\u03c0x) [24,46,47], where \u00dbNN represents the network output and \u00fb denotes the final prediction. For the standard PINN and RBA, we use a network with 5 hidden layers, each containing 300 neurons. For the compleX-PINN, we adopt a single hidden layer architecture with 1000 neurons. The number of parameters for PINN and RBA are 3 \u00d7 300 + 301 \u00d7 300 \u00d7 4 + 301 \u00d7 1 = 362, 401, while that for compleX-PINN is only 6 \u00d7 1000 + 1001 \u00d7 1 = 7,001. Both models are trained using the Adam optimizer with a learning rate of 4 \u00d7 10\u22125 for 100k iterations."}, {"title": "Diffusion-reaction Equation", "content": "The diffusion-reaction equation, a parabolic PDE, models the macroscopic behavior of particles undergoing Brownian motion combined with chemical reactions. It finds applications in various fields, including information theory, materials science, and biophysics. In this section, we consider the following system, identical to the one presented in Ref. [35]:\n$u_t = u_{xx} + R(x,t), x\\in [-\\pi, \\pi], t\\in [0, 1],$\n$u(-\\pi,t) = u(-\\pi,t) = 0,$\n$u(x, 0) = \\sum_{n=1}^4 \\frac{sin(nx)}{n} + \\frac{sin(8x)}{8},$\nwhere R(x, t) represents the reaction term:\n$R(x,t) = e^{-t}\\left[\\frac{3}{2}sin(2x) + \\frac{15}{8}sin(3x) + \\frac{63}{32}sin(8x)\\right].$\nThe analytical solution to this system is given by:\n$u(x,t) = e^{-t}\\left( \\sum_{n=1}^4 \\frac{sin(nx)}{n} + \\frac{sin(8x)}{8} \\right).$\nFor this experiment, we set Nf = 500 and No = N\u2081 = 50. We compare compleX-PINN with RBA and Gradient-Enhanced PINN (gPINN) [35]. The compleX-PINN uses a hidden layer with 1500 neurons, while RBA and gPINN employ a 4-layer neural network with 100 neurons per hidden layer. All models are trained using the Adam optimizer with a learning rate of 1 \u00d7 10-5 for 15k iterations."}, {"title": "2D Helmholtz equation", "content": "The Helmholtz equation describes wave and diffusion phenomena, addressing variations over time within a spatial or combined spatial-temporal domain. We consider the following 2D Helmholtz equation\n$U_{xx} + U_{yy} + k^2u - q(x, y) = 0, \\qquad (x, y) \\in \\Omega,$\n$u(x, y) = 0, \\qquad (x, y) \\in \\partial\\Omega,$\nwhere\n$q(x,y) = (k^2 \u2013 2(\\alpha\\pi)^2) sin(\\alpha\\pi x) sin(\\alpha\\pi y).$\nWe set k = 1 and \u03a9 = [-1,1] \u00d7 [-1,1] and the exact solution to the equation is\n$u(x, y) = sin(\\alpha\\pi x) sin(\\alpha\\pi y).$"}, {"title": "Reaction equation", "content": "The reaction problem is a hyperbolic PDE used to model chemical reactions, formulated with periodic boundary conditions as follows:\n$u_t \u2013 \\rho u(1 \u2013 u) = 0, \\qquad x\\in [0, 2\\pi], t \\in [0, 1],$\n$u(0,t) = u(2\\pi, t), \\qquad t\\in [0, 1],$\n$u(x, 0) = u_0(x), \\qquad x \\in [0, 2\\pi].$\nThe system has an analytical solution in the form:\n$u(x,t) = \\frac{u_0(x)e^{\\rho t}}{u_0(x)e^{\\rho t} + 1 - u_0(x)}.$\nWe define the initial condition as:\n$u_0(x) = exp\\left( -\\frac{x^2}{2(\\pi/4)^2}\\right).$\nWe evaluate the performance of PINN, RBA, compleX-PINN, and compleX-RBA for \u03c1 \u2208 {5, 10, 15, 20}. We set Nf = 6000 and No = N\u2081 = 150. PINN is set to be a 5-layer network with 80 neurons in each hidden layer and compleX-PINN has 1200 neurons for the Cauchy layer. Training is conducted using the Adam optimizer with a learning rate of 5 \u00d7 10\u22124 for 10k iterations."}, {"title": "Convection equation", "content": "The convection equation [28, 36, 48, 49] describes the transport of a conserved scalar field by a velocity field over time. This equation is widely applied in fluid dynamics and other related fields. We consider the following equation:\n$u_t + \\beta u_x = 0, \\qquad (x, t) \\in [0, 2\\pi] \\times [0, 1],$\n$u(0,t) = u(2\\pi,t), \\qquad t\\in [0,1],$\n$u(x, 0) = sin(x), \\qquad x \\in [0, 2\\pi],$\nwith the corresponding exact solution:\n$u(x, t) = sin(x \u2013 \\beta t),$\nwhich oscillates periodically between -1 and 1 and \u1e9e denotes the constant velocity parameter.\nPrevious studies have successfully trained PINNs for \u03b2 = 50, including approaches such as importance sampling [28], transformer-based PINN [36], and sequence-to-sequence learning [48]."}, {"title": "Conclusion", "content": "In this paper, we introduce compleX-PINN, which is a single-layer network that utilizes the Cauchy integral formula. Complementing our theoretical advances, the empirical results show that compleX-PINN can achieve a much lower relative L2 error with fewer iterations. Also, the number of parameters in compleX-PINN is much smaller than in the traditional PINN. This enhanced efficiency and accuracy validate the efficacy of our theoretical contributions and highlight the practical advantages of compleX-PINN."}]}