{"title": "Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification", "authors": ["Mohamed Abdelhamid", "Abhyuday Desai"], "abstract": "Class imbalance in binary classification tasks remains a significant challenge in machine learning, often resulting in poor performance on minority classes. This study comprehensively evaluates three widely-used strategies for handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE), Class Weights tuning, and Decision Threshold Calibration. We compare these methods against a baseline scenario of no-intervention across 15 diverse machine learning models and 30 datasets from various domains, conducting a total of 9,000 experiments. Performance was primarily assessed using the F1-score, although our study also tracked results on additional 9 metrics including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our results indicate that all three strategies generally outperform the baseline, with Decision Threshold Calibration emerging as the most consistently effective technique. However, we observed substantial variability in the best-performing method across datasets, highlighting the importance of testing multiple approaches for specific problems. This study provides valuable insights for practitioners dealing with imbalanced datasets and emphasizes the need for dataset-specific analysis in evaluating class imbalance handling techniques.", "sections": [{"title": "Introduction", "content": "Binary classification tasks frequently encounter imbalanced datasets, where one class significantly outnumbers the other. This imbalance can severely impact model performance, often resulting in classifiers that excel at identifying the majority class but perform poorly on the critical minority class. In fields such as fraud detection, disease diagnosis, and rare event prediction, this bias can have serious consequences. To address this challenge, researchers have developed various techniques targeting different stages of the machine learning pipeline. These include Data preprocessing techniques, adjustments during model training, and Post-training calibration. In this study, we aim to provide a comprehensive comparison of three widely-used strategies for handling class imbalance:\n\n\u2022 Synthetic Minority Over-sampling Technique (SMOTE)\n\u2022 Class Weights\n\u2022 Decision Threshold Calibration\n\nWe compare these strategies with a Baseline approach (standard model training without addressing imbalance) to assess their effectiveness in improving model performance on imbalanced datasets. Our goal is to provide insights into which treatment methods offer the most significant improvements in various performance metrics such as F1-score, F2-score, accuracy, precision, recall, MCC, Brier score, Matthews Correlation Coefficient (MCC), PR-AUC, and AUC. T\u03bf ensure a comprehensive evaluation, this study encompasses:\n\n\u2022 30 datasets from various domains, with sample sizes ranging from 500 to 20,000 and rare class percentages between 1% and 15%.\n\u2022 15 classifier models, including tree-based methods, boosting algorithms, neural networks, and traditional classifiers.\n\u2022 Evaluation using 5-fold cross-validation.\n\nIn total, we conduct 9,000 experiments involving the 4 scenarios, 15 models, 30 datasets, and validation folds. This extensive approach allows"}, {"title": "Related Works", "content": "One of the most influential techniques developed to address class imbalance is the Synthetic Minority Over-sampling Technique (SMOTE), proposed by Chawla et al. (2002). SMOTE generates synthetic examples of the minority class by interpolating between existing samples. Since its introduction, the SMOTE paper has become one of the most cited in the field of imbalanced learning, with over 30,000 citations. SMOTE's popularity has spurred the creation of many other oversampling techniques and numerous SMOTE variants. For example, Kov\u00e1cs (2019) documented 85 SMOTE-variants implemented in Python, including:\n\n\u2022 Borderline-SMOTE by Han et al. (2005)\n\u2022 Safe-Level-SMOTE by Bunkhumpornpat et al. (2009)\n\u2022 SMOTE + Tomek and SMOTE + ENN by Batista et al. (2004)\n\nWhile SMOTE has seen widespread use, its effectiveness has been called into question in high-dimensional settings. Blagus and Lusa (2013) demonstrated that SMOTE's performance tends to degrade when applied to high-dimensional datasets. In these cases, synthetic examples generated by SMOTE may not be as informative, potentially introducing noise and reducing classifier performance. Elor and Averbuch-Elor (2022) and Van Hulse et al. (2007) suggest the presence of better alternatives for handling class imbalance. This highlights the need for adaptations of SMOTE or alternative methods for handling imbalanced data in complex scenarios.\nFurthermore, oversampling methods like SMOTE have been critiqued for potentially creating synthetic samples that do not reflect real-world distributions. Hassanat et al. (2022) argue that these artificially generated instances can lead to overfitting, particularly when models are tested in real-world applications. This can result in models that perform well in controlled experimental conditions but fail to generalize effectively outside of those environments. Such concerns have prompted the development of more sophisticated techniques to address these challenges"}, {"title": "Decision Threshold", "content": "A critical factor in improving performance in imbalanced classification is the choice of evaluation metrics and thresholds. He and Garcia (2009) reviewed several methods for handling imbalanced datasets and stressed the importance of tailored metrics such as precision-recall curves and area under the ROC curve (AUC) for proper evaluation. Standard metrics like accuracy can be misleading when applied to imbalanced data, often inflating the model's performance on the majority class while ignoring minority class errors.\nIn addition to using appropriate metrics, optimizing the classification threshold is crucial for imbalanced datasets. Many models use a default threshold of 0.5, which can be unsuitable for imbalanced data. Zou et al. (2016) proposed a framework for finding the optimal classification threshold for imbalanced datasets, showing significant improvements in F-score by fine-tuning this threshold. Their method demonstrated that adjusting the threshold based on the dataset's characteristics helps to balance precision and recall, leading to better overall model performance.\nLeevy et al. (2023) extended this work by exploring various threshold optimization techniques, concluding that optimal threshold selection without the use of Random Undersampling (RUS) often yields the best results. Their research highlighted the importance of balancing True Positive Rate (TPR) and True Negative Rate (TNR) in threshold-based optimization and showed that default thresholds often perform poorly on imbalanced datasets.\nIn a comparative study, Hancock et al. (2022) investigated several threshold optimization techniques for imbalanced datasets, including the use of metrics like precision, f-measure, Matthews Correlation Coefficient (MCC), and geometric mean of true positive rate (TPR) and true negative rate (TNR). Their results highlighted that optimized thresholds consistently outperformed the default threshold of 0.5, particularly when optimized with respect to the class distribution and specific performance metrics. This comparative approach underscores the importance of selecting appropriate thresholds based on the specific classification objectives and dataset characteristics.\nHern\u00e1ndez-Orallo et al. (2012) offered a comprehensive framework for understanding how performance metrics like AUC, precision, and accuracy relate to threshold choice. They demonstrated that the choice of threshold has a direct"}, {"title": "Class Weights", "content": "In addition to these techniques, adaptive weight optimization (AWO) has emerged as an effective method for addressing imbalanced datasets. Huang et al. (2013) proposed an adaptive weighting approach that dynamically adjusts class weights to account for variations between the training and test sets. This method uses an evolutionary algorithm to optimize weight configuration, ensuring better classifier performance across both training and testing sets, even in highly imbalanced conditions. Their results demonstrated that adaptive weighting can outperform fixed weighting methods, particularly when the class imbalance is extreme.\nCost-sensitive learning (CSL) presents an alternative to oversampling by adjusting the cost associated with misclassifying the minority class. Instead of balancing the dataset through sample generation, CSL increases the weight of minority class errors during training. Thai-Nghe et al. (2010) combined CSL with resampling techniques, showing that optimizing the cost ratio as a hyperparameter can significantly improve performance on imbalanced datasets. By treating the cost ratio as a tunable parameter, this method provides a flexible solution that adapts to different imbalance levels.\nRecent work has introduced advanced re-weighting strategies to improve imbalanced classification. Guo et al. (2022) proposed a novel re-weighting method based on optimal transport (OT), which views the imbalanced training dataset as a distribution that needs to be aligned with a balanced meta-set. Their method optimizes the weights of training examples by minimizing the OT distance between the two distributions, achieving state-of-the-art performance across various tasks, including text, image, and point cloud classification(Learning to Re-weight). This approach represents a significant advancement in re-weighting techniques by decoupling the weight learning process from the classifier, providing a more robust solution for handling imbalanced datasets."}, {"title": "Gaps in the Literature and Motivation for This Study", "content": "While existing literature offers valuable insights into various techniques for handling class imbalance, there remains a notable gap in comprehensive, large-scale comparative studies that evaluate multiple methods across a wide range of datasets and models. Most prior works focus on specific techniques or limited sets of datasets, making it challenging to draw generalizable conclusions about the relative effectiveness of different approaches. Furthermore, there is a lack of studies that systematically examine how the performance of these techniques varies across different types of models and dataset characteristics. This gap in the literature motivated our study to provide a more holistic view of class imbalance handling strategies. By comparing SMOTE, Class Weights, and Decision Threshold Calibration across 30 diverse datasets and 15 different models, our work aims to offer practitioners and researchers a more comprehensive understanding of when and how to apply these techniques effectively. Additionally, our study's emphasis on dataset-level analysis addresses the need for more nuanced guidance in selecting appropriate methods for specific problem contexts."}, {"title": "Methodology", "content": "We selected 30 datasets\u00b9 based on the following criteria:\n\n\u2022 Binary classification problems\n\u2022 Imbalanced class distribution (minority class < 20%)\n\u2022 Sample size <= 20,000\n\u2022 Feature count <= 100\n\u2022 Real-world data from diverse domains"}, {"title": "Models", "content": "Our study employed a diverse set of 15 classifier models, encompassing a wide spectrum of algorithmic approaches and complexities. This selection ranges from simple baselines to advanced ensemble methods and neural networks, including tree-based models and various boosting algorithms. The diversity in our model selection allows us to assess how different imbalanced data handling techniques perform across various model types and complexities.\nThe following is a list of the models used in our experiments:\n\n1. AdaBoost\n2. Bagging\n3. CatBoost\n4. Decision Tree\n5. Explainable Boosting Machine\n6. Extra Trees\n7. FIGS (PiML)\n8. Gradient Boosting\n9. LightGBM\n10. Logistic Regression\n11. Relu DNN\n12. Random Forest\n13. Simple ANN\n14. SVM\n15. XGBoost\n\nA key consideration in our model selection process was ensuring that all four scenarios (Baseline, SMOTE, Class Weights, and Decision Threshold Calibration) could be applied consistently to each model. This criterion influenced our choices, leading to the exclusion of certain algorithms such as k-Nearest Neighbors (KNN) and Naive Bayes Classifiers, which do not inherently support the application of class weights. This careful selection process allowed us to maintain consistency across all scenarios while still representing a broad spectrum of machine learning approaches."}, {"title": "Evaluation Metrics", "content": "To comprehensively evaluate the performance of the models across different imbalanced data handling techniques, we tracked the following 10 metrics: Accuracy, Precision, Recall, F1-Score, F2-Score, Matthews Correlation Coefficient (MCC), Log-Loss, AUC-Score, PR-AUC and Brier Score\nOur primary focus is on the F1-score, a label metric that uses predicted classes rather than underlying probabilities. The F1-score provides a balanced measure of precision and recall, making it particularly useful for assessing performance on imbalanced datasets.\nWhile real-world applications often employ domain-specific cost matrices to create custom metrics, our study spans 30 diverse datasets. The F1-score allows us to evaluate all four scenarios, including decision threshold tuning, consistently across this varied set of problems."}, {"title": "Experimental Procedure", "content": "Our experimental procedure was designed to ensure a robust and comprehensive evaluation of the four imbalance handling scenarios across diverse datasets and models. The process utilized a form of nested cross-validation for each dataset to ensure robust model evaluation and proper hyperparameter tuning. For the outer loop, we employed 5-fold cross-validation, where each dataset was split into five folds. Results were reported for all five test splits, providing mean and standard deviation values across the folds. Within this outer loop, for scenarios requiring hyperparameter tuning (SMOTE, Class Weights, and Decision Threshold Calibration), we implemented an inner validation process. This involved further dividing the training split from the outer loop into a"}, {"title": "Scenario Descriptions", "content": "We evaluated four distinct scenarios for handling class imbalance:\n\n1. Baseline: This scenario involves standard model training without any specific treatment for class imbalance. It serves as a control for comparing the effectiveness of the other strategies.\n2. SMOTE (Synthetic Minority Over-sampling Technique): In this scenario, we apply SMOTE to the training data to generate synthetic examples of the minority class.\n3. Class Weights: This approach involves adjusting the importance of classes during model training, focusing on the minority class weight while keeping the majority class weight at 1.\n4. Decision Threshold Calibration: In this scenario, we adjust the classification threshold post-training to optimize the model's performance on imbalanced data.\n\nEach scenario implements only one treatment method in isolation. We do not combine treatments across scenarios. Specifically:\n\n\u2022 For scenarios 1, 2, and 3, we apply the default decision threshold of 0.5.\n\u2022 For scenarios 1, 2, and 4, the class weights are set to 1.0 for both positive and negative classes.\n\u2022 SMOTE is applied only in scenario 2, class weight adjustment only in scenario 3, and decision threshold calibration only in scenario 4."}, {"title": "Hyperparameter Tuning", "content": "For scenarios requiring hyperparameter tuning (SMOTE, Class Weights, and Decision Threshold), we employed a simple grid search strategy to maximize the F1-score measured on the single validation split (10% of the training data) for each fold. The grid search details for the three treatment scenarios were as follows:\n\n\u2022 SMOTE\nWe tuned the number of neighbors hyperparameter, performing a simple grid search over 'k' values of 1, 3, 5, 7, and 9.\n\u2022 Class Weights\nIn this scenario, we adjusted the class weights to handle class imbalance during model training. The tuning process involved adjusting the weight for the minority class relative to the majority class. If both classes were given equal weights (e.g., 1 and 1), no class imbalance handling was applied - this corresponds to the baseline scenario. For the balanced scenario, we set the minority class weight proportional to the class imbalance (e.g., if the majority/minority class ratio was 5:1, the weight for the minority class would be 5). We conducted grid search on the following factors: 0 (baseline case), 0.25, 0.5, 0.75, 1.0 (balanced), and 1.25 (over-correction). The optimal weight was selected based on the F1-score on the validation split.\n\u2022 Decision Threshold Calibration\nWe tuned the threshold parameter from 0.05 to 0.5 with a step size of 0.05, allowing for a wide range of potential decision boundaries."}, {"title": "Results", "content": "This section presents a comprehensive analysis of our experiments comparing four strategies for handling class imbalance in binary classification tasks. We begin with an overall comparison of the four scenarios (Baseline, SM\u039f\u03a4\u0395, Class Weights, and Decision Threshold Calibration) across all ten evaluation metrics. Following this, we focus on the F1-score metric to examine performance across the 15 classifier models and 30 datasets used in our study. The results are available in our GitHub repository.\u00b2\nOur analysis is structured as follows:\n\n1. Overall performance comparison by scenario and metric\n2. Model-specific performance on F1-score\n3. Dataset-specific performance on F1-score"}, {"title": "Overall Comparison", "content": "Table 2 presents the mean performance and standard deviation for all 10 evaluation metrics across the four scenarios: Baseline, SMOTE, Class Weights, and Decision Threshold Calibration.\nThe results represent aggregated performance across all 15 models and 30 datasets, providing a comprehensive overview of the effectiveness of each scenario in handling class imbalance.\nThe results show that all three class imbalance handling techniques outperform the Baseline scenario in terms of F1-score:\n\n1. Decision Threshold Calibration achieved the highest mean Fl-score (0.617 \u00b1 0.005)\n2. SMOTE followed closely (0.605 \u00b1 0.006)\n3. Class Weights showed improvement over Baseline (0.594 \u00b1 0.006)\n4. Baseline had the lowest Fl-score (0.556 \u00b1 0.006)\n\nWhile our analysis primarily focuses on the F1-score, it's worth noting observations from the other metrics:\n\n\u2022 F2-score and Recall: Decision Threshold Calibration and SMOTE showed the highest performance, indicating these methods are particularly effective at improving the model's ability to identify the minority class.\n\u2022 Precision: The Baseline scenario achieved the highest precision, suggesting a more conservative approach in predicting the minority class.\n\u2022 MCC (Matthews Correlation Coefficient): SMOTE and Decision Threshold Calibration tied for the best performance, indicating a good balance between true and false positives and negatives.\n\u2022 PR-AUC and AUC: These metrics showed relatively small differences across scenarios. Notably, SMOTE and Class Weights did not deteriorate performance on these metrics compared to the Baseline. As expected, Decision Threshold Calibration, being a post-model adjustment, does not materially impact these probability-based metrics (as well as Brier-Score).\n\u2022 Accuracy: The Baseline scenario achieved the highest accuracy, which is common in imbalanced datasets where high accuracy can be achieved despite poor minority class detection.\n\u2022 Log-Loss: The Baseline scenario performed best, suggesting it produces the most well-calibrated probabilities. SMOTE showed the highest log-loss, indicating potential issues with probability calibration.\n\u2022 Brier-Score: As expected, the Baseline and Decision Threshold scenarios show identical performance, as Decision Threshold Calibration is a post-prediction adjustment and doesn't affect the underlying probabilities used in the Brier Score calculation. Notably, SMOTE performed significantly worse on this metric, indicating it produces poorly calibrated probabilities compared to the other scenarios.\n\nBased on these observations, Decision Threshold Calibration demonstrates strong performance across several key metrics, particularly those focused on minority class prediction (F1-score, F2-score, and Recall). It achieves this without compromising the calibration of probabilities of the baseline model, as evidenced by the identical Brier Score. In contrast, while SMOTE improves minority class detection, it leads to the least well-calibrated probabilities, as shown by its poor Brier Score. This suggests that Decision Threshold Calibration could be particularly effective in scenarios where accurate identification of the minority class is crucial, while still maintaining the probability calibration of the original model."}, {"title": "Results by model", "content": "Key observations from these results include:\n\n1. Scenario Comparison: For each model, we compared the performance of the four scenarios (Baseline, SMOTE, Class Weights, and Decision Threshold Calibration). This within-model comparison is more relevant than comparing different models to each other, given the diverse nature of the classifier techniques.\n2. Decision Threshold Performance: The Decision Threshold Calibration scenario achieved the highest mean Fl-score in 10 out of 15 models. Notably, even when it wasn't the top performer, it consistently remained very close to the best scenario for that model."}, {"title": "Results by Dataset", "content": "Key observations from these results include:\n\n1. Variability: There is substantial variability in which scenario performs best across different datasets, highlighting that there is no one-size-fits-all solution for handling class imbalance.\n2. Scenario Performance:\n\n\u2022 Decision Threshold Calibration was best for 12 out of 30 datasets (40%)\n\u2022 SMOTE was best for 9 datasets (30%)\n\u2022 Class Weights was best for 7 datasets (23.3%)\n\u2022 Baseline was best for 3 datasets (10%)\n\u2022 There was one tie between SMOTE and Class Weights"}, {"title": "Statistical Analysis", "content": "To rigorously compare the performance of the four scenarios, we conducted statistical tests on the F1-scores aggregated by dataset (averaging across the 15 models for each dataset).\nRepeated Measures ANOVA\nWe performed a repeated measures ANOVA to test for significant differences among the four scenarios. For this test, we have 30 datasets, each with four scenario F1-scores, resulting in 120 data points. The null hypothesis is that there are no significant differences among the mean F1-scores of the four scenarios. We use Repeated Measures ANOVA to account because we have multiple measurements (scenarios) for each dataset.\nThe test yielded a p-value of 2.01e-07, which is well below our alpha level of 0.05. This result indicates statistically significant differences among the mean Fl-scores of the four scenarios."}, {"title": "Post-hoc Pairwise Comparisons", "content": "Following the significant ANOVA result, we conducted post-hoc pairwise comparisons using a Bonferroni correction to adjust for multiple comparisons. With 6 comparisons, our adjusted alpha level is 0.05/6 = 0.0083.\nKey findings from the pairwise comparisons:\n\n1. The Baseline scenario is significantly different from all other scenarios (p < 0.0083 for all comparisons).\n2. Class Weights is significantly different from Baseline and Decision Threshold, but not from SMOTE.\n3. There is no significant difference between SMOTE and Decision Threshold, or between SMOTE and Class Weights at the adjusted alpha level.\n\nThese results suggest that while all three imbalance handling techniques (SMOTE, Class Weights, and Decision Threshold) significantly improve upon the Baseline, the differences among these techniques are less pronounced. The Decision Threshold approach shows a significant improvement over Baseline and Class Weights, but not over SMOTE, indicating that both Decision Threshold and SMOTE may be equally effective strategies for handling class imbalance in many cases."}, {"title": "Discussion", "content": "Our comprehensive study on handling class imbalance in binary classification tasks yielded several important insights:\n\n1. Addressing Class Imbalance: Our results strongly suggest that handling class imbalance is crucial for improving model performance. Across most datasets and models, at least one of the imbalance handling techniques outperformed the baseline scenario, often by a significant margin.\n2. Effectiveness of SMOTE: SMOTE demonstrated considerable effectiveness in minority class detection, showing significant improvements over the baseline in many cases. It was the best-performing method for 30% of the datasets, indicating its value as a class imbalance handling technique. However, it's important to note that while SMOTE improved minority class detection, it also showed the worst performance in terms of probability calibration, as evidenced by its high Log-Loss and Brier Score. This suggests that while SMOTE can be effective for improving classification performance, it may lead to less reliable probability estimates. Therefore, its use should be carefully considered in applications where well-calibrated probabilities are crucial.\n3. Optimal Method: Decision Threshold Calibration emerged as the most consistently effective technique, performing best for 40% of datasets and showing robust performance across different model types. It's also worth noting that among the three methods studied, Decision Threshold Calibration is the least computationally expensive. Given its robust performance and efficiency, it could be considered a strong default choice for practitioners dealing with imbalanced datasets.\n4. Variability Across Datasets: Despite the overall strong performance of Decision Threshold Calibration, we observed substantial variability in the best-performing method across datasets. This underscores the importance of testing multiple approaches for each specific problem.\n5. Importance of Dataset-Level Analysis: Unlike many comparative studies on class imbalance that report results at the model level aggregated across datasets, our study emphasizes the importance of dataset-level analysis. We found that the best method can vary significantly depending on the dataset characteristics. This observation highlights the necessity of analyzing and reporting findings at the dataset level to provide a more nuanced and practical understanding of imbalance handling techniques."}, {"title": "Study Limitations and Future Work", "content": "While our study provides valuable insights, it's important to acknowledge its limitations:\n\n1. Fixed Hyperparameters: We used previously determined model hyperparameters. Future work could explore the impact of optimizing these hyperparameters specifically for imbalanced datasets. For instance, adjusting the maximum depth in tree models might allow for better modeling of rare classes."}, {"title": "Conclusion", "content": "Our study provides a comprehensive evaluation of three widely used strategies-SMOTE, Class Weights, and Decision Threshold Calibration for handling imbalanced datasets in binary classification tasks. Compared to a baseline scenario where no intervention was applied, all three methods demonstrated substantial improvements in key metrics related to minority class detection, particularly the F1-score, across a wide range of datasets and machine learning models.\nThe results show that addressing class imbalance is crucial for improving model performance. Decision Threshold Calibration emerged as the most consistent and effective technique, offering significant performance gains across various datasets and models. SMOTE also performed well, and Class Weights tuning proved to be a reasonable method for handling class imbalance, showing moderate improvements over the baseline.\nHowever, the variability in performance across datasets highlights that no single method is universally superior. Therefore, practitioners should consider testing multiple approaches and tuning them based on their specific dataset characteristics.\nWhile our study offers valuable insights, certain areas could be explored in future research. We fixed the hyperparameters across scenarios to ensure fair comparisons, holding all factors constant except for the treatment. Future research could investigate optimizing hyperparameters specifically for imbalanced datasets. Additionally, further work could explore how specific dataset characteristics influence the effectiveness of different techniques. Expanding the scope to include other imbalance handling methods or combinations of methods would also provide deeper insights. While our primary analysis focused on the Fl-score, results for other metrics are available, allowing for further exploration and custom analyses based on different performance criteria.\nIn conclusion, our findings emphasize the importance of addressing class imbalance and offer guidance on choosing appropriate techniques based on dataset and model characteristics. Decision Threshold Calibration, with its strong and consistent performance, can serve as a valuable starting point for practitioners dealing with imbalanced datasets, but flexibility and experimentation remain key to achieving the best results."}]}