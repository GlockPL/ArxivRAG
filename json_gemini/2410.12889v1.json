{"title": "Using Protected Attributes to Consider Fairness in\nMulti-Agent Systems", "authors": ["Gabriele La Malfa", "Jie M. Zhang", "Michael Luck", "Elizabeth Black"], "abstract": "Fairness in Multi-Agent Systems (MAS) has been extensively studied, particularly in reward distribution among\nagents in scenarios such as goods allocation, resource division, lotteries, and bargaining systems. Fairness in\nMAS depends on various factors, including the system's governing rules, the behaviour of the agents, and their\ncharacteristics. Yet, fairness in human society often involves evaluating disparities between disadvantaged and\nprivileged groups, guided by principles of Equality, Diversity, and Inclusion (EDI). Taking inspiration from the\nwork on algorithmic fairness, which addresses bias in machine learning-based decision-making, we define protected\nattributes for MAS as characteristics that should not disadvantage an agent in terms of its expected rewards.\nWe adapt fairness metrics from the algorithmic fairness literature-namely, demographic parity, counterfactual\nfairness, and conditional statistical parity-to the multi-agent setting, where self-interested agents interact within\nan environment. These metrics allow us to evaluate the fairness of MAS, with the ultimate aim of designing MAS\nthat do not disadvantage agents based on protected attributes.", "sections": [{"title": "1. Introduction", "content": "Multi-Agent Systems (MAS) consist of agents interacting with each other and their surrounding\nenvironment to achieve their individual or shared goals. The achievement of an agent's goals may\ndepend on the actions it takes, the actions of other agents, the environment they are situated in, and\nthe rules that govern the MAS. Similarly, fairness in MAS depends on multiple factors. Fairness can be\ninfluenced by agents' decision-making processes, as evidenced by research in reinforcement learning\nfocused on developing fair and efficient policies [1]. It can also hinge on mechanism design, as seen\nin scenarios like goods allocation games [2] or cake-cutting problems [3], where rules can ensure fair\nreward distribution among agents. Additionally, fairness can be affected by things like an agent's\nutility [4, 5] or their priority in accessing resources [6, 7], among others.\nIn human societies, fairness is often defined in terms of characteristics that should not disadvantage\nan individual or group, such as age, race, disability or gender. For example, in the UK Equality Act 2010\u00b9\nthese are identified as protected characteristics, and UK law states that individuals cannot be discriminated\nagainst on the basis of these. These protected characteristics typically define subgroups of the population\nwho have historically been disadvantaged in particular situations, such as age discrimination in the\nworkplace, unequal access to healthcare or barriers in education for people with disabilities and gender\ndisparities in political representation, among others. Driven by the bias that often exists in the training\ndata as a result of these systemic inequalities, machine learning approaches often produce biased results\n(e.g., discrimination in credit market [8] or justice [9, 10] algorithms); there is a growing body of work\n(often referred to as algorithmic fairness) that aims to identify and mitigate such bias by applying a\nrange of fairness metrics that compare the outcomes achieved by what is identified as advantaged and\ndisadvantaged subgroups of the population (see, e.g., [11, 12] for a review)."}, {"title": "2. Related work", "content": "Fairness has attracted the attention of Game Theory and MAS researchers for decades alongside\npsychologists and economists [17, 18, 19, 20]. Factors such as the rules that govern the system can\ninfluence fairness in MAS. For instance, this can be seen in the Ultimatum Game, where fairness is\ninfluenced by the dynamics between proposers and responders [21, 22, 23]. In goods allocation or\ncake-cutting games, the rules depend on the type of good being allocated, for example, whether they\nare divisible or indivisible, goods or chores [24, 25], and fairness depends on the distribution of goods\namong the agents [2, 3, 7, 26, 27].\nAgent behaviour can also influence fairness. Fair behaviours often balance the rewards collected by\nthe community and individuals. For example, Zhang and Shah [28] propose a minimum reward for\nthe worst-performing agent while improving the overall rewards of the whole community of agents.\nHowever, fairness and reward optimisation can be in tension, and compromises must be made regarding\none of the two sides. Jiang and Lu [29] propose a two-step solution consisting of a single policy for\neach agent based on fair and optimal rewards, with a controller agent who decides which sub-policies"}, {"title": "3. Preliminaries", "content": "A multi-agent system consists of multiple decision-making agents who act and interact in an en-\nvironment to achieve their goals. A multi-agent system S = (\u0415, \u0435, \u0410\u0441, P, At, Atpr, \u315c) is charac-\nterised by: the set of possible environment states E; the starting state e0; the set of available actions\nthat may be performed by an agent in the environment Ac (including a null action); a population\nP = {1,..., a1,...,an} of agents; the attributes At = {at1,...,atm} available to the agents in P; the\nprotected attributes Atpr \u2282 {at1,...,atm}; and the non-deterministic state transformer function\n\u0442: \u0415\u00d7 \u0410\u0441\u2081 \u00d7 ... \u00d7 Acn \u2192 E \u00d7 [0,1] that specifies the probability distribution over the possi-\nble resulting states that can occur when each agent in the population performs an action (where the\npossible null action reflects that an agent chooses not to act).\nAn agent ax within a multi-agent system (E, eo, Ac, P, At, Atpr, \u315c) (where ax \u2208 P) is defined as\na tuple (Atx, Acx, \u03c0\u03c7, \u03c1x) where: the attribute evaluation function Atx : At \u2192 {0,1} specifies which\nattributes hold true for the agent; Ac Ac are the actions available to the agent; the non-deterministic\npolicy \u03c0\u03b1 : \u0395 \u2192 Acx \u00d7 [0,1] specifies how an agent will act in any given state (represented as a\nprobability distribution over the possible actions); and the reward function px : E \u00d7 E \u2192 R specifies\nthe reward the agent receives for moving between two states.\nA possible run within a multi-agent system S = (E, eo, Ac, P, At, Atpr, \u315c) (where P consists of\nn agents) is denoted r = (eo, (ac], . . ., ac", "ac": "ej) where: for each ax \u2208 P and\nfor each i such that 0 < i < j, (ac,p) \u2208 \u03c0x(ei\u22121) and p > 0; and for each i such that 0 < i < j,\n(ei+1,p) \u2208 \u0442(ei, (ac,..., ac)) and p > 0. The set of all possible runs within a multi-agent system S\nis denoted RS.\nLet r = (eo, (ac], ..., ac), e1, ..., (ac), ..., ac"}, {"ac": "ej), the reward achieved by an agent ar is\nRew(ax, r) = \u03a3=1 px(Ci\u22121, Ci).\nThe expected reward of an agent ar within a system S, denoted ExpRew(ax, S), is thus\nExprew(ax, S) = \u2211 Rewax,r).p(r | S).\nrERS\nMotivating example, continued. The city traffic consists of a population of cars, each capable of\nsteering, accelerating or braking. Cars also possess attributes like speed or safety features. Cars are\neither driven by AI or humans, and we consider being driven by humans to be a protected attribute of\ncars. AI-driven cars can find optimal paths to reach their destination more efficiently than human-driven\nones. If we consider agents reaching a hospital, we can foresee fairness problems as AI-driven cars\nwould be advantaged. When the cars act with a specific probability, the environment changes state.\nAlso, each car obtains a reward when reaching its destination. A car's policy is a decision rule based on\nthe state of the crossroads."}, {"title": "4. Fairness in MAS", "content": "We define fairness by comparing, in different ways, the rewards gathered by individuals or groups\nof agents possessing and not possessing protected attributes. We adapt demographic parity [34, 35],\ncounterfactual fairness [35] and conditional statistical parity [36] to MAS.\nDemographic parity in MAS is achieved when the expected rewards of agents are not influenced by\nwhether or not they possess protected attributes, all else being equal.\nDefinition 1 (Demographic Parity). Let S = (E, eo, Ac, P, At, Atpr, \u315c) be a system and let atpr \u2208\nAtpr be the protected attribute under consideration. Demographic parity is satisfied for atpr in S if and only\nif: for allax, ay \u2208 P, if Atx(atpr) = 1, Aty(atpr) = 0, and for all at' \u2208 At\\{atpr}, Atx(at') = Aty(at'),\nthen ExpRew(ax, S) = ExpRew(ay, S).\nWhere demographic parity is not satisfied for a particular protected attribute, we can measure the extent\nto which this is the case, denoted DemPar(atpr, S), as follows.\n\n$DemPar(a^{tpr}, S) = $\n\n$\\sum_{\\substack{a_x,a_y\\in P \\text{ such that } A_{t_x}(a^{tpr})=1, A_{t_y}(a^{tpr})=0,\\text{ and for all } a^{t'} \\in A_t \\setminus \\{a^{tpr}\\},A_{t_x}(a^{t'})=A_{t_y}(a^{t'})}}$  $ExpRew(a_x, S) \u2013 ExpRew(a_y, S')$$\n\n(1)\nNote that if demographic parity holds for atpr in S then DemPar(atpr, S) = 0.\nCounterfactual fairness in MAS is achieved when the expected rewards of agents remain the same in\nboth a factual and a counterfactual world, where in the latter, we change the protected attribute of the\nagents while keeping all other elements the same.\n=\nDefinition 2 (Counterfactual Fairness). Let S\n(E, eo, Ac, P, At, Atpr, \u315c) be a system where\nP = {(At1, Ac1, \u03c0\u03b9, \u03c11), ..., (Atn, Acn, \u03c0\u03b7, \u03c1n)}, and let atpr \u2208 Atpr be the protected attribute un-\nder consideration. Let S' = (E, eo, Ac, P', At, Atpr, \u315c) be the counterfactual of S such that P' =\n{(At\u2081, Ac1, \u03c0\u03b9, \u03c11), ..., (At'n, Acn, \u03c0\u03b7, \u03c1\u03b7)} where for all i such that 1 < i < n: if At\u00bf(atpr) = 0,\nthen At'(atpr) = 1; if Ati(atpr)\n1, then At'(atpr) = 0; and for all at_e_At \\{atpr},\nAti(at) = At prime(at). Counterfactual fairness is satisfied for atpr in S if and only if: for all ax =\n(Atx, Acx, \u03c0\u03c7, \u03c1\u30a7) \u2208 P, for all ax = (At'x, Acx, \u03c0\u03b1,\u03c1\u03b1) \u2208 P', ExpRew(ax, S) = ExpRew(a'x, S').\nWhere counterfactual fairness is not satisfied, we can measure the extent to which this is the case, denoted\nCountFair(atpr, S), as follows.\n\n$CountFair(a^{tpr}, S) = $\n\n$\\sum_{a_x\\in P \\text{ such that } A_{t_x}(a^{tpr})=1}$  $ExpRew(a_x, S) \u2013 ExpRew(a'_x, S')$ (2)\nNote that if counterfactual fairness holds for atpr in S then CountFair(atpr, S) = 0.\nConditional statistical parity in MAS is achieved when the expected rewards of agents are not\ninfluenced by whether or not they possess protected attributes when conditioning on a legitimate factor,\nassuming all other elements are the same. A legitimate factor is an attribute that has been identified as\none that may legitimately affect an agent's reward.\nDefinition 3 (Conditional Statistical Parity). Let S = (E,eo, Ac, P, At, Atpr, \u315c) be a system, let\nLF C (At \\ Atpr) be the set of legitimate factors, and let atpr \u2208 Atpr be the protected attribute under\nconsideration. Conditional statistical parity is satisfied for atpr with LF in S if and only if: for all\nax, ay \u2208 P, if Atx (atpr) = 1, Aty(atpr) = 0, Atx(at'f) = Aty(at'f) = 1 for all at'f \u2208 LF, and for all\nat' \u2208 At \\{atpr}, Atx(at') = Aty(at'), then ExpRew(ax, S) = ExpRew(ay, S)."}, {"title": "5. Conclusion and future work", "content": "This paper is a first step towards ensuring that certain sub-groups of agents are not disadvantaged\nin multi-agent systems. We identify protected attributes, which are characteristics that should not\ndisadvantage an agent in terms of its expected rewards. Inspired by algorithmic fairness, we adapt\ndemographic parity, counterfactual fairness and conditional statistical parity to analyse fairness in MAS.\nOur metrics assess fairness from various perspectives in any multi-agent system where expected rewards\nare applicable. Additional metrics from the algorithmic fairness literature, such as equal opportunity,\nequalised odds [38], disparate impact [39], or other metrics based on causal reasoning [40, 41] could be\nadapted to this setting to capture other aspects of fairness. Our methodology applies to MAS, involving\nboth human and AI agents, as motivated by our example. It could also be used to improve the fairness\nof human societies by modelling these as multi-agent systems and seeing how changes to the system\naffect the various fairness metrics defined here.\nIn future work, we plan to analyse these fairness metrics experimentally in different settings, both\ncompetitive and cooperative, to find system configurations that enhance fairness. We will use techniques\nsuch as Bayesian optimisation [42], evolutionary algorithms [43] and sparse sampling techniques [44]\nto try to identify system configurations that optimise for the different fairness metrics."}]}