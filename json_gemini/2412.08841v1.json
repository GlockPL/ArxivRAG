{"title": "Structural Entropy Guided Probabilistic Coding", "authors": ["Xiang Huang", "Hao Peng", "Li Sun", "Hui Lin", "Chunyang Liu", "Jiang Cao", "Philip S. Yu"], "abstract": "Probabilistic embeddings have several advantages over deterministic embeddings as they map each data point to a distribution, which better describes the uncertainty and complexity of data. Many works focus on adjusting the distribution constraint under the Information Bottleneck (IB) principle to enhance representation learning. However, these proposed regularization terms only consider the constraint of each latent variable, omitting the structural information between latent variables. In this paper, we propose a novel structural entropy-guided probabilistic coding model, named SEPC. Specifically, we incorporate the relationship between latent variables into the optimization by proposing a structural entropy regularization loss. Besides, as traditional structural information theory is not well-suited for regression tasks, we propose a probabilistic encoding tree, transferring regression tasks to classification tasks while diminishing the influence of the transformation. Experimental results across 12 natural language understanding tasks, including both classification and regression tasks, demonstrate the superior performance of SEPC compared to other state-of-the-art models in terms of effectiveness, generalization capability, and robustness to label noise. The codes and datasets are available at https://github.com/SELGroup/SEPC.", "sections": [{"title": "Introduction", "content": "Probabilistic embedding (Vilnis and McCallum 2015) is a flexible representation learning method aiming to learn the underlying probability distribution of data. It has been broadly applied to various domains such as graph structural learning (Sun et al. 2022), computer vision (Kim et al. 2021; Oh et al. 2019; Shi and Jain 2019; Fischer 2020), and natural language processing (Mahabadi, Belinkov, and Henderson 2021; Hu et al. 2024, 2022). In contrast to deterministic embedding (Dong, Yan, and Wang 2024; Xu et al. 2024), which maps the input into a fixed latent variable representation, probabilistic embedding represents each data point as a probability distribution. Hence, probabilistic embedding inherently accounts for the uncertainty and complexity of data by controlling the spread of the probability density over the learning latent space (Oh et al. 2019), showcasing better discriminative ability and robustness.\nThe mainstream probabilistic embedding methods are grounded in the Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000; Tishby and Zaslavsky 2015). IB aims to find compressed representations that maintain as much information as possible for the prediction task while removing as much irrelevant information as possible. Specifically, it seeks the latent representation Z that is maximally informative about the target Y (i.e., maximize mutual information \\(I(Y; Z)\\)) while being minimally informative about the input data X (i.e., minimize mutual information \\(I(X; Z)\\)) (Sun et al. 2022). The former target is typically achieved with common task losses like cross entropy (CE) loss or mean squared error (MSE) loss, whereas various regularization losses are proposed for the latter goal. VIB (Alemi et al. 2017) assumes the prior distribution of Z is the standard normal distribution and utilizes Kullback-Leibler (KL) divergence to regularize the learning distribution \\(p(z|x)\\). Sparse IB (Chalk, Marre, and Tkacik 2016) changes the prior distribution of VIB to the Student-t distribution to achieve relevant and sparse coding. MEIB (An, Jammalamadaka, and Chong 2023) lifts the prior distribution constraint of VIB and instead uses maximum conditional entropy \\(H(Z|X)\\) as the only regularization. SPC (Hu et al. 2024) omits the decoder of VIB and proposes an additional structured regularization that encourages class-level uniformity within the latent space under the multivariate Gaussian distribution. However, all of them focus solely on the individual latent variable Z or the constraint of Z with the label Y, neglecting the structural information between latent variables.\nIn recent years, structural entropy theory (Li and Pan 2016) has demonstrated its advantage in capturing hierarchical structural information and has been widely used in various fields like node classification (Duan et al. 2024), graph structural learning (Zou et al. 2023), and contrastive learning (Wu et al. 2023). It considers the structural information of the original inputs by modeling the input data as a graph and then converting the graph into an encoding tree.\nThe data points are the leaf nodes of the encoding tree, and each upper node represents a partition, resulting in a hierarchical clustering of the input data. Low-depth tree nodes depict more coarse-grained clusters of the input data. Each node in the encoding tree has its own structural entropy. The structural entropy of the encoding tree is calculated by summing the structural entropy of all non-root nodes, representing the overall structural information of the input. Previous work (Wang et al. 2023; Zeng, Peng, and Li 2023) mostly focuses on minimizing the structural entropy of the encoding trees to obtain the optimized encoding tree or embeddings of input data, aiming at learning as much task-related information as possible. However, the potential for using structural entropy for regularization remains underexplored. Additionally, as the structural entropy is designed for classification tasks, how to effectively leverage it in the regression task is still a problem.\nIn this paper, we propose a structural entropy-guided probabilistic coding model, named SEPC. We present a structural entropy-based regularization loss that incorporates structural information between latent variables. Specifically, we first construct the adjacency matrix based on the similarity between embeddings of latent variables and propose to maximize the structural entropy of the induced graph, which helps improve the generalization of the model by separating the probabilistic distribution of each latent variable. Additionally, we design a probabilistic encoding tree to adapt our structural entropy loss in regression tasks. We first discretize and soften regression labels into soft classification labels (i.e., each data point belongs to multiple classes with varying probabilities), diminishing the influence of unsuitable classification caused by using only discretization (Pintea et al. 2023). To adapt structural entropy to such soft labels, we relax the constraint that one child belongs to one parent in the encoding tree, allowing each child to connect to all upper-level nodes with varying probabilities. Extensive experiments are conducted on 12 natural language understanding tasks, including 10 classification tasks and 2 regression tasks. Comparative results and analysis demonstrate that the proposed SEPC enjoys superior effectiveness, generalization, and robustness compared to the state-of-the-art (SOTA) baselines. The main contributions are summarized as follows:\n\u2022 We present a structural entropy based regularization loss, incorporating the structural information between data points into model regularization. To our knowledge, this is the first time that maximizing structural entropy has been utilized as a regularization loss.\n\u2022 We propose a probabilistic encoding tree for soft classification labels and present an effective method to utilize structural entropy for regression tasks for the first time.\n\u2022 Extensive experiments on 12 datasets demonstrate that SEPC achieves SOTA performance in classification and regression tasks regarding effectiveness, generalization, and robustness."}, {"title": "Preliminaries", "content": "In this section, we present the basic concepts of probabilistic coding, the encoding tree, and structural entropy."}, {"title": "Probabilistic Coding", "content": "The classical probabilistic coding model employs an encoder-decoder architecture, as shown in Figure 1(a). The encoder \\(f_e\\) maps input \\(x \\in X\\) to a Gaussian distribution \\(\\mathcal{N}(z; \\mu, \\Sigma)\\). All distributions of \\(z\\) consist of the embedding space of the latent variable \\(Z\\). The re-parameterization trick (Kingma and Welling 2013) is then used to sample \\(z\\) from the distribution while keeping the gradient unbiased. Finally, \\(z\\) is mapped by the decoder to \\(f_d(z)\\) to predict the label \\(y \\in Y\\). The work (Hu et al. 2024) also proposes an encoder-only architecture for probabilistic coding (as shown in Figure 1(b)), omitting the decoder and directly predicting \\(y\\) using the sample from the learned distribution.\nUnder the Markov chain constraint \\(Y \\rightarrow X \\rightarrow Z\\), the probabilistic coding follows the Information Bottleneck principle and aims to learn the minimal sufficient information for representation \\(Z\\):\n\\[Z = \\underset{Z}{\\text{argmin }} -I(Z;Y) + \\beta I(Z; X),\\]\nwhere \\(I(Z; Y)\\) is the mutual information between \\(Z\\) and \\(Y\\), \\(I(Z; X)\\) is the mutual information between \\(Z\\) and \\(X\\), and \\(\\beta\\) is the Lagrangian multiplier trading off sufficiency and minimality. Assuming \\(z \\in Z\\) follows the Gaussian distribution, the objective of probabilistic coding is as follows:\n\\[\\mathcal{L}_{PC} = \\mathbb{E}_{z\\sim p(z|x)} [-\\log q(y|z)] + \\beta KL[p(z|x), r(Z)].\\]\nHere, KL refers to the KL divergence operator, \\(p(z|x) = \\mathcal{N}(z; \\mu, \\Sigma)\\) is learned by the encoder \\(f_e\\), \\(r(Z)\\) is the expected prior distribution, and \\(r(Z) = \\mathcal{N}(z;0,I)\\) in general. \\(q(y|z)\\) is the variational approximation to \\(p(y|z)\\) and is calculated by the decoder \\(f_d\\) or by a non-parametric operator like the softmax function in the encoder-only architecture (Hu et al. 2024)."}, {"title": "Encoding Tree", "content": "Given a graph \\(G = \\{X, E,W\\}\\), \\(X\\) is the set of input data points, \\(E\\) is the edge set, and \\(W \\in \\mathbb{R}^+\\) is the edge weight set. For each point \\(x \\in X\\), its degree \\(d_x\\) is defined as the sum of the weights of edges associated with it. The encoding tree \\(T\\) of \\(G\\) is a multi-child tree with the following properties:\n(1) Each tree node \\(a\\) corresponds to a subset of data points \\(T_a \\subseteq X\\). Especially, for the root node \\(A\\) of \\(T\\), we define the points set it associated with as \\(T_A = X\\). For the leaf node \\(a\\) at the last depth, \\(T_a\\) is a singleton containing a single data point \\(x \\in X\\). If the leaf node \\(a\\) is not at the last depth, \\(T_a\\) is \\(\\emptyset\\). (2) For each non-leaf tree node \\(a\\), its \\(i\\)-th immediate child is \\(a\\), and its parent node is denoted as \\(\\bar{a}\\). (3) For each non-leaf tree node \\(a\\), \\(T_a = \\bigcup_{i=1}^{N_a} T_{a\\langle i\\rangle}\\), \\(N_a\\) is the number of children of \\(a\\). With these properties, each depth of a node in the encoding tree depicts a partition of the data point set \\(X\\), and lower depth means a more coarse-grained partition."}, {"title": "Structural Entropy", "content": "The structural entropy is defined under the graph \\(G\\) and the encoding tree \\(T\\) as follows:\n\\[H^T(G) = \\sum_{\\alpha \\in T, \\alpha \\neq A} H^T(G; \\alpha),\\]\n\\[H^T(G; \\alpha) = \\frac{V_{\\alpha}}{vol(G)} \\log_2 \\frac{vol(G)}{V_{\\bar{\\alpha}}}.\\]\nHere, \\(g_a\\) is the sum of the weights of the edges that connect points inside \\(T_a\\) with points outside \\(T_a\\) (i.e., the weights of the cut edges between \\(T_a\\) and its complement set \\(T^L\\)). The volume of \\(G\\), denoted as \\(vol(G)\\), is the sum of the degrees of all data points \\(X\\), i.e., \\(vol(G) = \\sum_{\\alpha \\in X} d_x\\). \\(V_{\\alpha} = \\sum_{x \\in T_a} d_x\\) is the volume of \\(T_a\\), and \\(\\bar{a}\\) is the parent node of \\(a\\)."}, {"title": "Proposed Method", "content": "In this section, we elaborate on the proposed structural entropy based regularization loss of SEPC, introduce the probabilistic encoding tree for soft classification labels, and describe how to utilize it in regression tasks. We adopt the encoder-only architecture (Hu et al. 2024) for probabilistic coding, and the overall model of SEPC is shown in Figure 2."}, {"title": "Structural Entropy based Regularization Loss", "content": "Previous works only consider the individual latent variable in the regularization loss, ignoring the structural information between latent variables. To capture the structural information, we incorporate structural entropy into the regularization loss, as it inherently considers the self-organization of data. As illustrated in Figure 2, the input data X is first encoded into the probabilistic embedding Hz. The graph G is constructed from Z as follows:\n\\[\\mathbf{A} = \\sigma(\\mathbf{H}_z \\times \\mathbf{H}_z^T),\\]\nwhere Hz is the embedding of Z, and \\(\\sigma\\) is the sigmoid activation function to ensure positive values for the adjacency matrix A.\nThe construction of the encoding tree is also straightforward. We treat the labels as the optimal partition for the data and construct a three-tier encoding tree. The nodes in the intermediate layer represent the classes of the classification task, and each leaf node (i.e., the input data X) is assigned to an intermediate node according to its label. We define an assignment matrix \\(C \\in \\{0,1\\}^{n \\times r}\\), where \\(n\\) is the number of leaf nodes and \\(r\\) is the number of intermediate nodes. \\(C_{ij} = 1\\) means the \\(i\\)-th leaf node belongs to the \\(j\\)-th class. To enhance the capability of the latent representations, we propose maximizing the structural entropy of the intermediate layer nodes, constraining the probabilistic distribution of the latent variables to ensure separation. The structural entropy of the intermediate layer nodes for the three-tier encoding tree is as follows:\n\\[H^T(G) = \\sum_{j=1}^r H^T(G; \\alpha_j) = \\sum_{j=1}^r \\frac{V_{\\alpha_j}}{vol(G)} \\log_2 \\frac{vol(G)}{V_{\\bar{\\alpha}_j}},\\]\nwhere \\(r\\) is the number of classes, \\(g_{\\alpha_j}\\) is the sum of the weights of the cut edges between \\(T_{\\alpha_j}\\) and its complement set \\(T^L_{\\alpha_j}\\), \\(V_{\\alpha_j}\\) is the volume of \\(T_{\\alpha_j}\\), and \\(\\{\\alpha_1, ..., \\alpha_r\\}\\) is the intermediate layer nodes in the encoding tree. Utilizing the adjacency matrix A and the assignment matrix C, the regularization loss format of \\(H^T(G)\\) is as follows:\n\\[\\mathcal{L}_{SE} = - \\sum_{j=1}^r \\frac{((\\mathbf{1} - C)^T \\odot AC)_{jj}}{sum(A)} \\times \\log_2 \\frac{((C^T \\odot AC)_{jj})}{sum(A)}.\\]\nHere, \\(\\mathbf{1}\\) is the full-one matrix with shape \\(n \\times r\\), the operator \\(sum()\\) sums up the matrix to a scalar, and \\((.)_{jj}\\) selects the value in the \\(j\\)-th row and the \\(j\\)-th column of the matrix. The overall loss of SEPC is as follows:\n\\[\\mathcal{L}_{SEPC} = \\mathcal{L}_{PC} - \\gamma \\mathcal{L}_{SE},\\]\nwhere \\(\\gamma\\) is a hyperparameter controlling the weight of our structural entropy based regularization loss \\(\\mathcal{L}_{SE}\\)."}, {"title": "Probabilistic Encoding Tree for Regression Tasks", "content": "Discretization is a widely used method to transform a regression task into a classification task by binning continuous labels into discrete classes (Muthukumar et al. 2021; Stewart et al. 2023). However, as the binning borders need to be predefined, inappropriate borders can lead to unbalanced or indistinguishable classification labels, hampering model performance (Pintea et al. 2023). Softening labels mitigates this issue (Ma et al. 2023), as it allows each data point to belong to all classes with different probabilities to express tendencies. We propose a probabilistic encoding tree to utilize structural entropy theory in such soft classification labels. It loosens the constraint that one child node is only assigned to one parent node, allowing the child node to connect with all up-depth nodes with different probabilities.\nAs shown in Figure 3, during the discretized period, we first bin the entire regression label value space into \\(r\\) classes. Then, we calculate the distance between the regressive label Y and the centers \\(P = \\{P_1, ..., P_r\\}\\) of the \\(r\\) bins:\n\\[D = |Y^T - P|,\\]\nwhere \\(D \\in \\mathbb{R}^{n \\times r}\\), \\(n\\) is the number of data points, and the \\(i\\)-th row of D denotes the distance between the \\(i\\)-th data point and the \\(r\\) bin centers. The soft label is then calculated during the softening period as follows:\n\\[Y' = softmax(-D),\\]\nwhere \\(-D\\) ensures that a closer distance to the bin center corresponds to a higher probability of belonging to this class. The structural entropy of the intermediate layer nodes for the three-tier probabilistic encoding tree \\(H^T_Z(G)\\) is then defined as follows:\n\\[V_{\\alpha_j} = \\sum_{x_i \\in X} Y'_{ij}d_{x_i},\\]\n\\[H^T(G; \\alpha_j) = \\frac{V_{\\alpha_j}}{vol(G)} \\log_2 \\frac{vol(G)}{V_{\\bar{\\alpha}_j}},\\]\n\\[\\mathcal{H}_Z(G) = \\sum_{j=1}^r H^T(G; \\alpha_j).\\]\nHere, \\(Y'_{ij}\\) denotes the soft label of the \\(i\\)-th data point \\(x_i\\) regarding to the \\(j\\)-th class, and \\(d_{x_i}\\) is the degree of \\(x_i\\). \\(\\alpha_j\\) represents the \\(j\\)-th intermediate layer nodes in the probabilistic encoding tree. For \\(g_{\\alpha_j}\\), the weight of cut edges should be multiplied by the probability of one vertex belonging to \\(T_{\\alpha_j}\\) and the other belonging to \\(T^L_{\\alpha_j}\\). Letting the assignment matrix \\(\\mathbf{C} = Y'\\), the structural entropy loss for the probabilistic encoding tree is as follows:\n\\[\\mathcal{L}_{SE} = - \\sum_{j=1}^r \\frac{((\\mathbf{1} - \\mathbf{C})^T \\odot A\\mathbf{C})_{jj}}{sum(A)} \\times \\log_2 \\frac{(({\\mathbf{C}}^T \\odot A\\mathbf{C})_{jj})}{sum(A)}.\\]\nIt is equivalent to Equation 7 in the formula, except that the elements of the assignment matrix \\(\\mathbf{C}\\) are probabilities between 0 and 1. Thus far, we have presented an effective method to utilize structural entropy for regression tasks."}, {"title": "Experiments", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness, generalization capability, and robustness of SEPC. For fairness, all results are reported as the average and standard deviation of metrics tested with five random seeds, as in other works."}, {"title": "Experiment Setups", "content": "Datasets Following Hu et al. (2024), we evaluate SEPC on 10 classification task datasets and 2 regression task datasets. For classification tasks, 7 datasets about tweet semantic analysis are used: Emoji (Barbieri et al. 2018), Emotion (Mohammad et al. 2018), Hate (Basile et al. 2019), Irony (Van Hee, Lefever, and Hoste 2018), Offensive (Zampieri et al. 2019), Sentiment (Rosenthal, Farra, and Nakov 2017), and Stance (Mohammad et al. 2016). Additionally, we also experiment on three emotion-related datasets from different domains: ISEAR (Scherer and Wallbott 1994), MELD (Poria et al. 2019), and GoEmotions (Demszky et al. 2020). For regression tasks, we utilize STS-B (Cer et al. 2017) and Claire (Roth, Anthonio, and Sauer 2022) for evaluation.\nEvaluation Metric We use the same metric as in previous works. The macro-averaged F1 score across all classes is reported for most classification datasets. Following Hu et al. (2024), we report the macro-averaged F1 score of favor and against classes for the Stance dataset, the F1 score of the ironic class for the Irony dataset, and the macro-averaged recall for the Sentiment dataset. For regression tasks, we report both Pearson and Spearman correlation coefficients.\nBaselines We compare SEPC with two categories of classic baselines: universal models and fine-tuned representation models. The baseline results are collected from the work of Hu et al. or evaluated using the source code provided by the authors. In the universal models, we compare with SVM (Cortes and Vapnik 1995), FastText (Joulin et al. 2017), BiLSTM (Hochreiter and Schmidhuber 1997), and GPT-3.5. For the fine-tuned models, we use bert-base-uncased (Devlin et al. 2019) and roberta-base (Liu et al. 2020) as the backbone and fine-tune them on the evaluation datasets. We compare with four deterministic embedding baselines: cross-entropy (CE) for classification tasks and mean squared error (MSE) for regression tasks, CE+CP (Pereyra et al. 2017), CE/MSE+AT (Miyato, Dai, and Goodfellow 2017), and CE+SCL (Gunel et al. 2021). Besides, we compare SEPC with four probabilistic embedding models: VIB (Alemi et al. 2017), MINE-IB (Belghazi et al. 2018), MEIB (An, Jammalamadaka, and Chong 2023), and SPC (Hu et al. 2024).\nParameter Settings The training epoch number is 20, and the maximum patience for early stopping is 5 epochs. The learning rate is 5e-5 in all datasets. A linear learning rate warm-up is applied over the first 10% of the training data. The batch size is uniformly set to 128. The trade-off parameter B and the weight parameter y are searched from {1e - 2, 1e - 1,1,10}. We set the class number r = 5 for the STS-B dataset and r = 4 for the Claire dataset, as their labels range from 0\u20135 and 1\u20135, respectively. All experiments are conducted on two NVIDIA RTX A6000 GPUs."}, {"title": "Evaluations", "content": "Classification Tasks We conduct comparative experiments with the baselines on 10 classification datasets and"}, {"title": "Robustness Analysis", "content": "To evaluate the robustness of SEPC, we introduce noise by randomly flipping 10%, 20%, and 30% of the labels in the training datasets to any class with the same probability. The experimental results are reported in Table 3. SEPC shows superior performance across all noise rate settings and all datasets compared to baselines. Specifically, it outperforms all baselines with average improvements of 2.23%-4.30%, 1.67%-4.64%, and 2.09%-5.14% under 10%, 20%, and 30% noise rates, respectively. Besides, when the noise rate increases from 20% to 30%, SEPC exhibits a minimal average performance decrease. This experiment demonstrates that SEPC has better robustness when handling noise and data unreliability."}, {"title": "Hyperparameter Sensitivity Analysis", "content": "We evaluate the impact of the newly introduced weight hyperparameter y for regularization loss \\(\\mathcal{L}_{SE}\\) on ten classification datasets and illustrate the results in Figure 4. A lower regularization weight is preferred in most datasets. A too-large weight, \\(\\gamma = 10\\), generally leads to a noticeable performance decrement and higher variance."}, {"title": "Generalization Analysis", "content": "We conduct experiments under limited training data conditions to better evaluate the generalization capability of SEPC. In detail, we randomly select 90%, 70%, 50%, and 30% of the training data during the model training period and compare the performance of SEPC on the test set with other probabilistic coding models."}, {"title": "Related Work", "content": "Probabilistic Embedding Compared to deterministic embedding (Dong, Yan, and Wang 2024; Xu et al. 2024), probabilistic embedding learns a probabilistic distribution for each input, effectively capturing data uncertainty and complexity, and thus better handling noise and outliers. The mainstream probabilistic embedding methods follow the Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000; Tishby and Zaslavsky 2015), which seeks to discover compressed representations that retain the maximum amount of relevant information for the prediction task while eliminating as much irrelevant information as possible. VIB (Alemi et al. 2017) constrains the latent variable to follow the Gaussian distribution and utilizes Kullback-Leibler (KL) divergence between the learned distribution and the prior Gaussian distribution as the regularization loss. Sparse IB (Chalk, Marre, and Tkacik 2016) replaces the prior Gaussian distribution of VIB with the Student-t distribution. MINE-IB (Belghazi et al. 2018) is a mutual information neural estimation method with the IB principle, allowing for the tractable IB application in a continuous setting. Fischer (2020) proposes the conditional entropy bottleneck top improved robustness to adversarial examples. MEIB (An, Jammalamadaka, and Chong 2023) utilizes maximum conditional entropy to serve as the bottleneck of IB. SPC (Hu et al. 2024) introduces an encoder-only framework, incorporating a class-level structured regularization loss.\nStructural Entropy Unlike early information entropy, such as Shannon entropy, which is defined by unstructured probability distributions, structural entropy (Li and Pan 2016) takes the hierarchical structural information of the input data into account. It is gaining substantial traction and is widely used in graph structural learning (Zou et al. 2023), node classification (Duan et al. 2024), social bot detection (Peng et al. 2024; Zeng, Peng, and Li 2024), and deep clustering (Sun et al. 2024). USER (Wang et al. 2023) proposes a structural entropy-based loss. However, current works focus solely on minimizing structural entropy to maximize task-related information and are limited to classification tasks."}, {"title": "Conclusion", "content": "In this paper, we propose SEPC, a structural entropy guided probabilistic coding model. SEPC utilizes maximizing the structural entropy as the regularization loss, introducing the structural information into the optimization, and aims to separate the latent variables in the class space. Additionally, we propose a probabilistic encoding tree and an effective method to utilize the structural entropy for regression tasks based on it. Experiments on 12 datasets demonstrate the effectiveness, generalization capability, and robustness of SEPC in both classification and regression tasks."}]}