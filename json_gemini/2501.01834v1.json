{"title": "MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning", "authors": ["Pu Yang", "Bin Dong"], "abstract": "Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we called MoColl, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports.", "sections": [{"title": "Introduction", "content": "Image captioning is a critical task in the intersection of computer vision and natural language processing, aiming to generate descriptive textual captions for given images. This task has profound implications for various applications such as radiology report generation (Monshi et al., 2020). Traditionally, approaches to image captioning have relied on specialized encoder-decoder models designed to learn the mapping from images to captions (Xu et al., 2015; Karpathy and Fei-Fei, 2015;"}, {"title": "Related Work", "content": "In this section, we review the key areas relevant to our proposed framework. In section 2.1, we discuss specialized models for image captioning, as they form the foundation for domain-specific tasks. In section 2.2, we discuss VLMs, as they integrate general knowledge into image-related tasks. They are both our main comparisons. In section 2.3, we discuss the use of LLMs as agents, highlighting their ability to plan tasks, interact with tools, and act as optimizers, which are key functionalities in our framework. In section 2.4, we discuss approaches for selecting synthetic data, which are critical for the agent's role in refining the VQA model by curating high-quality training data.\nWe here only list works of direct relevance to ours, with an extensive reference list in appendix A."}, {"title": "Specialized Model for Image Captioning", "content": "Specialized models for image captioning traditionally use an encoder-decoder pipeline (Xu et al., 2015; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Anderson et al., 2018), a framework that has been extensively employed in various applications, e.g., radiology report generation (see appendix A.1). The encoder-decoder architecture is designed to learn the mapping from images to textual descriptions by encoding the visual information into a fixed-length vector and then decoding this vector into a coherent sequence of words.\nHowever, these models may struggle with understanding and incorporating broader contextual knowledge that is not explicitly present in the training data. As a result, their generalization capabilities are limited, making it difficult for them to adapt to diverse types of medical images beyond the specific domain they were trained on."}, {"title": "Vision-Language Model", "content": "Vision-Language Models (VLMs) such as GPT-4V (OpenAi, 2023), LLaVA (Liu et al., 2023c,b), DeepSeek-VL (Lu et al., 2024), CogVLM (Wang et al., 2023), and Qwen-VL (Bai et al., 2023b), are usually based on LLMs such as GPT-4 (Achiam et al., 2023), Llama-2 (Touvron et al., 2023), DeepSeek LLM (Bi et al., 2024), Vicuna (Chiang et al., 2023), and Qwen-LM (Bai et al., 2023a) respectively. These models integrate visual and textual data, treating tokenized images as another"}, {"title": "Large Language Model as Agent", "content": "Recent research, such as MetaGPT (Hong et al., 2023) and AutoGPT (Significant Gravitas), has found that LLMs can be employed as agents, acting as core controllers for executing complex tasks. In particular, their application extends across multiple domains of functionality, each illustrating the versatility of their capabilities as autonomous systems."}, {"title": "Planning", "content": "LLM-based agents excel in task planning by decomposing tasks into smaller, sequential steps. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thought (Yao et al., 2023), and LLM+P (Liu et al., 2023a) enable these agents to plan and execute complex workflows."}, {"title": "Tool usage", "content": "A crucial aspect of LLM-based agents is their ability to utilize external tools effectively. Systems like MRKL (Karpas et al., 2022), TALM (Parisi et al., 2022), Toolformer (Schick et al., 2023), and ChatGPT Plugins (OpenAI, 2023) enable LLMs to interact with various tools to gather information, perform computations, and generate solutions. This capability significantly enhances their versatility and effectiveness in problem-solving."}, {"title": "LLM as optimizer", "content": "One of the most significant advancements in utilizing LLMs as agents is their role as optimizers. LLMs excel in optimizing prompts (Yang et al., 2024), where they generate and refine input queries to maximize the effectiveness and relevance of their responses. What's more, LLMs have also demonstrated a remarkable ability in refining machine learning models by tuning hyperparameters (Zhang et al., 2023; Liu et al., 2024), suggesting architectural improvements (Zheng et al., 2023), improving loss functions (Song et al., 2023; Wu et al., 2023), and even guiding exploration in reinforcement learning (Du et al., 2023). Building on this work, we propose to innovate further by leveraging LLMs to optimize data, i.e., dynamically generating and refining training datasets, particularly through the creation of targeted visual-question-answer data that address specific weaknesses of VQA models."}, {"title": "Synthetic data with selection", "content": "Recent studies (Mitra et al., 2024) have demonstrated that high-quality synthetic data is crucial for the post-training enhancement of language models. Although training with synthetic data suffers from model collapse (Shumailov et al., 2023), data selecting techniques utilize a verifier that can improve the quality of synthetic data. For instance, golden verifiers, such as the Python interpreter, are used for code generation (Haluptzok et al., 2022), and symbolic deduction engines are used for Olympiad geometry problems (Trinh et al., 2024). When a golden verifier does not exist, some studies employ heuristic verifiers, such as training a verification model with high-quality data (Li et al., 2022), designing critic prompts (Wang et al., 2022b; Wei et al., 2023), and leveraging a self-critique mechanism (Putta et al., 2024). In this work, we prompt a strong foundation model to select the synthetic question-answer data."}, {"title": "Methods", "content": "In this section, we introduce our proposed agent-enhanced model collaboration framework and agent-guided tuning framework. We first outline the model collaboration framework in section 3.1, where the agent and the VQA model are combined to decompose the captioning task into subtasks of"}, {"title": "Model Collaboration Framework", "content": "This framework decomposes the captioning process into manageable subtasks, where the agent iteratively formulates targeted questions for the VQA model, gathers answers, and synthesizes them into accurate and coherent captions. It is detailed as follows and illustrated in fig. 1(c).\nSpecifically, the caption generation process can be divided into questioning step and captioning step. In the questioning step, the agent iteratively asks a question to the VQA model with reference to previous conversations (if any), and then the VQA model will feedback an answer according to the given image, i.e.\n$q_{ij} = \\text{Agent}_{\\text{question}}(\\{(q_{ik},a_{ik})\\}_{k<j}), j = 1,2,\\dots,$\n$a_{ij} = \\text{VQA}(q_{ij}, x_i), j = 1,2,...,$\nwhere $q_{ij}$ is a question, $a_{ij}$ is an answer, and $x_i$ is the image(s). The captioning step is after enough question-answer pairs have been collected or a stopping criterion is achieved, where we prompt the agent to provide the caption according to the sequence of questions and answers, i.e.\n$C = \\text{Agent}_{\\text{caption}}(\\{(q_i, a_i)\\}_{i=1}^n)$.\nThus, we get the generated caption c."}, {"title": "Training Procedure", "content": "The training procedure consists of the following 2 stages. The first stage is the warm-up stage, where we align the image features to their textual word embeddings in the pre-trained LM. The second"}, {"title": "Stage 1: Warm up", "content": "Following the alignment process in LLaVA-Med (Li et al., 2023), we first align the image features to their textual word embeddings in the pre-trained LM. Specifically, we convert the image-caption pair data into the image-instruction-caption tuple, where the instruction simply presents the task of describing the image, e.g. \u2018Provide a diagnostic report based on the given image(s).'. Here we use auto-regressive loss that asks the VQA model to predict the caption according to the given image and instruction, i.e., $p(c_i | x_i, I)$, where $c_i$ is its corresponding ground truth caption, and $I$ is the task-dependent instruction. We keep both the visual encoder and LM weights frozen, and only update the projection network. This stage is illustrated in fig. 2(b)."}, {"title": "Stage 2: Agent-Guided Tuning.", "content": "This stage aims to fine-tune the VQA model through the guidance of the agent. The core idea is that the agent curates high-quality training data to refine the VQA model by generating synthetic question-answer pairs within the model collaboration framework and filtering them using ground truth captions. It consists of three steps: (1) inference, (2) selection, and (3) training."}, {"title": "Selection step", "content": "This step aims to select high-quality synthetic question-answer pairs and thus build a VQA dataset. Inspired by recent work on selecting synthetic data, we propose that the agent can be used to improve the performance of the VQA model via selected synthetic data. Specifically, we instruct the agent to select correct question-answer pairs according ground truth captions. Similar to the process of a teacher looking at a standard answer to correct a student's error, the selection agent can find out inappropriate questions from the above questioning agent and inaccurate answers from the VQA model. Accordingly, the selection agent designs new questioning prompts and generates additional question-answer pairs\n$\\{(q_i, a_i) | \\text{Agent}_{\\text{select}}(q_{ij}, a_{ij}, C_i) = \\text{True}\\}$"}, {"title": "Training step", "content": "This step aims to fine-tune the VQA model with the high-quality VQA dataset. It improves the performance of the VQA model and ensures that the tool remains effective and adaptable. Specifically, we use auto-regressive loss, i.e., $p(a_j | x_j, q_j; \\text{VQA})$, where $(x_j, q_j, a_j)$ is the selected image-question-answer tuple. The visual encoder weights are still frozen, and the LM and the projection network can be updated.\nWe summarize the agent-guided tuning process in algorithm 2 and illustrate it in fig. 2(c)."}, {"title": "Experiments", "content": null}, {"title": "Implementation", "content": "Datasets. We verify our proposed methods on two widly used datasets: (1) IU-Xray (Demner-Fushman et al., 2016) \u00b3, which consists of 3955 reports and 7,470 chest X-ray images; and (2) MIMIC-CXR (Johnson et al., 2019) \u2074, which consists of more than 206,000 reports and 473,000 chest X-ray images. Details of data preprocessing are provided in appendix B.2.\nSetup. Details of data preprocessing: (1) Within-dataset evaluation, where we assess the model on the test set of MIMIC-CXR, and (2) Cross-dataset generalization, where we test the model on the IU-Xray dataset to examine its ability to generalize from one training set to a similar but distinct test set.\nMetrics. We employ four commonly used natural language processing evaluation metrics to assess captioning performance: BLEU{1-4} (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), ROUGE-L (Lin, 2004), and Cider (Vedantam et al., 2015).\nOur Methods. We detail the specialized VQA tool and LLM-based agent employed in our framework. For the VQA model, we use LLaVA (Liu et al., 2023c), a visual-language conversation model, as our VQA model. The architecture remains unchanged, consisting of a frozen image encoder, a projection network, and a pre-trained causal Language Model (LM). Specifically, we use CLIP Vision Encoder (Radford et al., 2021) as the image encoder, which outputs a vector representing the features of the input image. The image features are then mapped into visual tokens via a randomly initialized three-layer Multi-Layer Perceptron (MLP), which serves as an intermediary, projecting the image information into a \"foreign language\" understood by the LM. The pre-trained LM is Llama-3.1-8B (Dubey et al., 2024). For the agent, we adopt DeepSeek-V2.5 (DeepSeek-AI, 2024). Furthermore, we demonstrate our model collaboration (MoColl) framework with two tools: the aligned VLM and the tuned VQA model. The former is trained only through a warm-up stage, while the latter is trained from the aligned VLM through an agent-guided tuning stage. Additional implementation details can be found in appendix B.3.\nBaseline Methods. We evaluate our method against three categories of baseline approaches, as depicted in fig. 1. (1) Specialized encoder-decoder models: R2Gen (Chen et al., 2020), Joint-TriNet (Yang et al., 2021), R2GenCMN (Chen et al., 2021), XProNet (Wang et al., 2022a), and M2KT (Yang et al., 2023). (2) General vision language models: LLaVA-Med (Li et al., 2023), and LLaVA-1.5-7b (Liu et al., 2023b), DeepSeek-VL2 (Lu et al., 2024), Qwen2-VL-72B-Instruct (Bai et al., 2023b) (denoted as Qwen2-VL), our aligned VLM, and supervised fine-tuned VLM. (3) Model collaboration framework with VQA models: LLaVA-Med and LLaVA-1.5-7b. Implementation details of each baseline method are provided in the appendix B.4."}, {"title": "Main Results", "content": "We show the within-dataset evaluation results in table 1 and the cross-dataset generation results in table 2. Key findings from the analysis are:\n\u2022 Best Performance of Our Method: Across both testing scenarios, the MoColl framework, particularly with the agent-guided tuning algorithm, consistently outperforms all baseline methods across all metrics. This underscores the efficacy of integrating domain-specific VQA models with general knowledge-driven LLM agents.\n\u2022 Advantage of Domain-Specific Learning for VLMs: General VLMs that have undergone learning on domain-specific datasets, such as our aligned model and the SFT model, exhibit superior performance compared to those without such training (e.g., LLaVA, Qwen2-VL, and Deepseek-VL2). This trend highlights the limitations of general base models in specific domains and affirms the benefit of domain-adaptive training."}, {"title": "Quantitative Results", "content": "We show a quantitative example from IU-Xray dataset as follows. Specifically, we provide a visualized comparison of diagnostic reports generated using our MoColl framework with the agent-guided tuned VQA model and its interactive question-answer process. We juxtapose this report with one produced by the aligned VLM, comparing both to the ground truth. In the visualization, keywords accurately generated by both methods are highlighted in green, whereas keywords uniquely and successfully generated by our method are marked in blue"}, {"title": "Ablation Studies", "content": "In this subsection, we conduct comprehensive ablation studies to demonstrate the scalability and effectiveness of our proposed framework. Specifically, we study two strategies for prompt engineering in section 4.4.1 and section 4.4.2, and two strategies for the tuning algorithm in section 4.4.3 and section 4.4.4. All ablation studies are carried out within the context of cross-dataset generalization."}, {"title": "In-context Learning.", "content": "In-context learning is a pivotal technique in prompt engineering for enhancing the efficacy of model responses. In fig. 3, we evaluate different ICL strategies and their impact on captioning performance. The results indicate that increasing the number of few-shot examples consistently improves captioning outcomes. This enhancement highlights the significant role few-shot examples play in refining the agent's querying and captioning capabilities. Furthermore, selecting examples based on image similarity, rather than randomly, tends to yield better results, since RAG strategy more likely aligns the few-shot examples with the ground truth captions of the query images."}, {"title": "Length of Conversation", "content": "We explore the impact of the length of conversation, i.e. the maximum number of questions an agent is allowed to ask. In fig. 4, we show how varying the maximum number of questions affects captioning performance. The results indicate that while extending the length of conversation has little effect on ROUGE-L scores, it significantly enhances BLEU-1 scores. This suggests that increasing the number of questions does not substantially alter the overall sentence structure of the captions but does enhance vocabulary precision. Combined with the quantitative results, our analysis further shows that the agent typically initiates the process with a broad question to grasp the global context of the images, followed by more specific queries catching details, thereby boosting the accuracy of the captions."}, {"title": "Selection Strategy", "content": "Selection is a common and effective technique to improve the quality of the data generated. In table 3, we show the captioning performance across various selection strategies. Our findings highlight the"}, {"title": "Synthetic Data Size", "content": "The size of the synthetic question-answer pairs plays a critical role in the tuning of the VQA model. In fig. 5, we show how the number of image-caption pairs impacts the captioning performance. Our findings indicate that as the size of the training data increases, there is a consistent decrease in loss and an increase in ROUGE-L scores. It's worth noting that, when the data size is low (less than 3 \u00d7 10\u2074), the performance of the fine-tuned VQA model falls below that of the aligned model, i.e., the initial state before fine-tuning. This suggests that a sufficient data size is essential for the success of agent-guided tuning."}, {"title": "Conclusion", "content": "In this paper, we introduced the MoColl framework, a novel approach to image captioning that utilizes a novel agent-enhanced model collaboration strategy. Our framework effectively merges domain-specific visual analysis with broad contextual knowledge by employing a VQA model as"}, {"title": "Supplemental Related Work", "content": null}, {"title": "Radiology Report Generation for Chest X-ray", "content": "Radiology report generation for chest X-rays is a specialized area of image captioning that focuses on producing detailed and clinically accurate textual reports from medical images. This task plays a crucial role in assisting radiologists, enhancing diagnostic efficiency, and improving patient care by providing consistent and comprehensive reports.\nWith the advancement of deep learning, encoder-decoder architectures became prevalent. Models like Shin et al. (2016) and Wang et al. (2017) utilized Convolutional Neural Networks (CNNs) for encoding image features and Recurrent Neural Networks (RNNs) for decoding them into text. Attention mechanisms were later integrated to allow models to focus on specific regions of the image, as seen in works by Chen et al. (2020) and Chen et al. (2021), improving the relevance and accuracy of the generated reports. Beyond these standard architectures, more advanced models have been proposed specifically for radiology tasks such as Yang et al. (2021) and Wang et al. (2022a).\nTo incorporate medical knowledge, some studies integrated clinical ontologies and knowledge bases into their models. For example, Li et al. (2019) introduced a model that aligns image features with medical concepts, ensuring the use of correct terminology and facilitating the generation of more informative reports. Additionally, Yang et al. (2023) adopted a multi-task knowledge transfer mechanism to enhance the generation process by simultaneously learning disease classification and report generation tasks.\nDespite these advancements, existing models often struggle to fully leverage both the general language understanding capabilities of large-scale models and the domain-specific expertise required for medical reporting. Challenges such as generating coherent narratives, accurately reflecting clinical findings, and adapting to varied patient cases remain. Our method is capable of addressing these challenges by combining the expansive knowledge and language generation abilities of Large Language Models (LLMs) with the specialized visual processing capabilities of a domain-specific VQA model. Therefore, we use the radiology report generation task as an example to run experiments, demonstrating the effectiveness of our approach."}, {"title": "Implementation Details", "content": null}, {"title": "Detailed Setup", "content": "Our proposed approach is implemented using Pytorch. The main parameters of the server are listed below: the operating system is Rocky Linux 8.8, the CPU is Intel Xeon Platinum 8358P with 2.60 GHz and 64 cores, the GPU is an 8-card A800 80G, and the memory capacity is 512 GB."}, {"title": "Data Prepossessing", "content": "For both datasets, we use 'finding + impression' as the radiology report for each report. We split the IU-Xray into train and test set by 8:2 randomly and MIMIC-CXR according to its official split. We filter out any missing data, such as reports without corresponding images, or images without corresponding reports. Each report corresponds to a minimum of one image and a maximum of four images. Overall, there are 767 test data in IU-Xray, and 222758 training data and 3269 test data in MIMIC-CXR. To align with the input shape of the CLIP Vision Encoder, we clip all images to 224*224. Following the Chen et al. (2020)'s official implementation of report preprocessing, the cut-off frequency for the words is set as 3 for IU-Xray and 10 for MIMIC-CXR."}, {"title": "Our Methods", "content": null}, {"title": "Hyperparameters", "content": "As for the model collaboration framework, we set the number of the few-shot examples as 5, and the maximum number of questions an agent is allowed to ask as 6 in the intra-dataset evaluation setting and 3 in the inter-dataset generalization setting. In all experiments, except those specifically stated, we always use the RAG as the ICL strategy for the agent. Notice that we can still use few-shot"}, {"title": "Competing Methods", "content": null}, {"title": "Specialized models", "content": "For all specialized models, we leverage their official code repository to reproduce the results based on our setup. Especially, for XProNet (Wang et al., 2022a) and M2KT (Yang et al., 2023), we follow their preprocessing process and use CheXbert (Smit et al., 2020) to rebuild labels with reports under our setup. We set the maximum sequence length as 530, which is the maximum length of captions in our experiments. Other hyperparameters remain default values."}, {"title": "General VLMs", "content": "We use official implementations and their released model checkpoints for LLaVA-Med and LLaVA-v1.5-7B, and official APIs for Qwen2-VL-7bB-Instruction and Deepseek-VL2. The aligned VLM is trained through the warm-up stage. The fine-tuned VLM is trained on the image-instruction-caption tuple with auto-regressive loss. The visual encoder weights are still frozen, and the LM and the projection network can be updated.\nFor the generating setting, we provide 5 few-shot examples of captions randomly to enhance the understanding of the specific domain. We also set the temperature as 0 and other hyperparameters remain the default values."}]}