{"title": "Attention-aware Semantic Communications for Collaborative Inference", "authors": ["Jiwoong Im", "Nayoung Kwon", "Taewoo Park", "Jiheon Woo", "Jaeho Lee", "Yongjune Kim"], "abstract": "We propose a communication-efficient collaborative inference framework in the domain of edge inference, focusing on the efficient use of vision transformer (ViTs) models. The partitioning strategy of conventional collaborative inference fails to reduce communication cost because of the inherent architecture of ViTs maintaining consistent layer dimensions across the entire transformer encoder. Therefore, instead of employing the partitioning strategy, our framework utilizes a lightweight ViT model on the edge device, with the server deploying a complicated ViT model. To enhance communication efficiency and achieve the classification accuracy of the server model, we propose two strategies: 1) attention-aware patch selection and 2) entropy-aware image transmission. Attention-aware patch selection leverages the attention scores generated by the edge device's transformer encoder to identify and select the image patches critical for classification. This strategy enables the edge device to transmit only the essential patches to the server, significantly improving communication efficiency. Entropy-aware image transmission uses min-entropy as a metric to accurately determine whether to depend on the lightweight model on the edge device or to request the inference from the server model. In our framework, the lightweight ViT model on the edge device acts as a semantic encoder, efficiently identifying and selecting the crucial image information required for the classification task. Our experiments demonstrate that the proposed collaborative inference framework can reduce communication overhead by 68% with only a minimal loss in accuracy compared to the server model.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of computational resources, coupled with the proliferation of massive datasets, has significantly enhanced the practicality of artificial intelligence (AI) services. Integrating AI techniques with edge devices, including smartphones, wearable devices, and Internet of things (IoT) devices, seeks to seamlessly incorporate AI services into a wide range of daily life. This effort to advance AI technologies in the domain of edge computing is commonly known as edge AI [1]\u2013[3].\nAn important research theme in edge AI is edge inference, focused on efficiently executing inference tasks within the edge network [2]\u2013[5]. Traditionally, raw data is sent from edge devices (clients) to a server, where a complicated model conducts the inference task, i.e., server-based inference. However, this method incurs significant communication overhead, particularly in scenarios dealing with large volumes of raw data [3], [6]. An alternative is on-device inference, which executes the inference task directly on resource-constrained devices, thereby minimizing communication costs. However, this approach often leads to lower performance due to the limited computational capabilities of edge devices [3], [4].\nTo address the dual challenges of excessive communication overhead and limited computational resources, the concept of collaborative inference has been introduced [2]\u2013[7]. This strategy involves dividing a deep neural network (DNN) model into separate parts for the edge device and the server. Within this framework, the edge device first uses its component to extract features from the raw data and then transmits them to the server. As these extracted features are typically more compressed than the raw data, the communication cost can be aggressively reduced. The server then utilizes these features and its portion of the model to determine the final inference result, which is sent back to the device [5]. The selection of the split point is critical as it significantly impacts the computational load on the device and the communication overhead [3]. This approach is also known as split inference [5], [8] and device-edge co-inference [3], [7]. Notably, collaborative inference is closely connected to semantic communications [9]\u2013[14], considering that the extracted features are essentially semantic information tailored for the inference task.\nTransformers, originally developed for natural language processing (NLP) [15], have been widely adopted across multiple domains. Particularly, the vision transformer (ViT) [16], [17] has demonstrated superior performance and efficiency in image classification tasks. However, the deployment of ViTs on resource-constrained edge devices is challenging due to their substantial model size and intensive computational requirements [18].\nIn collaborative inference scenarios, the strategy of partitioning ViT models fails to effectively reduce communication overhead. This limitation stems from the inherent architecture of ViTs, which maintains consistent layer dimensions across the entire transformer encoder [16], in contrast to convolutional neural network (CNN) models whose intermediate layer dimensions can be significantly smaller than the raw data dimensions. Consequently, partitioning a ViT model for collaborative inference still requires the transmission of significant data volumes, thereby failing to substantially mitigate the communication load.\nWe propose an efficient collaborative inference framework utilizing pre-trained ViT models. Instead of partitioning a"}, {"title": "II. BACKGROUNDS", "content": "The ViT [16] is a transformer-based model for computer vision tasks, setting a standard in vision models. A simplified overview of the ViT model is shown in Fig. 1. An input image x \u2208 R^{H\u00d7W\u00d7C} is reshaped into a sequence of flattened 2D patches x_p \u2208 R^{N\u00d7(P^2\u00b7C)}, where (H, W), C, and (P, P) denote the resolution of the original image, the number of channels, and the resolution of each image patch, respectively. Note that N = \\frac{H}{P} \\frac{W}{P} is the resulting number of patches. These patches are then linearly projected to a consistent dimension D across the transformer layers via E \u2208 R^{(P^2\u00b7C)\u00d7D}. The input embedding of the ViT's transformer encoder z_0 \u2208 R^{(N+1)\u00d7D} is given by\nz_0 = [x_{cls}; x_1E;...;x_NE] + E_{pos}, (1)\nwhere E_{pos} denotes the standard learnable position embedding. The class token z = x_{cls} \u2208 R^{1\u00d7D} is particularly prepended"}, {"title": "III. COLLABORATIVE INFERENCE FRAMEWORK BASED ON TRANSFORMER MODELS", "content": "We propose a collaborative inference framework that utilizes pre-trained ViT models. This framework is designed to achieve server-level classification accuracy with minimized communication overhead between the edge device and the server.\nDue to the consistent layer dimensions of ViTs, conventional methods of collaborative inference [2]\u2013[7], which typically partition a single CNN model, are ineffective at reducing communication costs for ViT models. As a solution, we employ"}, {"title": "Algorithm 1 Proposed Collaborative Inference Framework", "content": "a lightweight ViT model (e.g., DeiT-Tiny) at the edge device, instead of splitting a complex ViT model (e.g., DeiT-Base), as depicted in Fig. 2. The proposed inference framework establishes an efficient collaborative protocol between the edge device and the server, aiming to achieve high classification accuracy of DeiT-Base model while significantly reducing communication overhead.\nIn our collaborative inference framework, the edge device (client) first performs inference with its tiny model. The edge device then evaluates the entropy level of this initial inference. High entropy (low confidence) necessitates transmitting the image to the server since it indicates that the tiny model's inference would be unreliable. In such instances, only essential patches for classification are transmitted instead of the entire image patches to minimize communication costs. The server, utilizing its complex ViT model, conducts inference based on these selected patches and sends its classification results back to the edge device, as shown in Fig. 2(a). This process of selecting critical patches is governed by the proposed attention-aware patch selection rule, elaborated in Section IV.\nIf the initial inference's entropy is low, the edge device confirms its classification result without further interaction with the server, as shown in Fig. 2(b). Reducing reliance on the server to reduce communication costs is achieved through entropy-aware image transmission rule, detailed in Section V. By integrating these rules, our framework significantly lowers communication costs while maintaining classification accuracy comparable to the server model.\nThe steps of the proposed collaborative inference are outlined in Algorithm 1. Here, Step 2 and Step 3 involve computing the initial inference result f(x^{(i)}) and its entropy g(x^{(i)}), respectively. If the entropy is below a given threshold \u03b7, then f(x^{(i)}) is deemed the final classification outcome. In cases of higher entropy, as identified in Step 4, the client selects and transmits only essential patches to the server at Step 5 and 6, effectively lowering communication costs by ensuring dim(x^{(i)}) < dim(x^{(i)}). At Step 7, the server conducts inference on these selected patches, producing the result f(x^{(i)}), which is then sent back to the client at Step 8. The proposed collaborative inference framework can reduce the computational complexity for the server model by limiting"}, {"title": "IV. ATTENTION-AWARE PATCH SELECTION", "content": "This section introduces our attention-aware patch selection method, motivated by an intriguing observation: the tiny ViT model is capable of identifying the essential patches for classification, even when its classification is incorrect (see Fig. 3). Consequently, the tiny model on the edge device acts as a semantic encoder, effectively extracting essential information for the classification task."}, {"title": "A. Quantifying Patch Importance", "content": "To quantify the importance of each patch for classification, we utilize the attention scores generated by the SA mechanism. The attention score for the class token in a single-head attention is calculated as follows:\na = \\text{softmax} \\left(\\frac{q_{cls}^T k_p}{\\sqrt{D_h}}\\right), (10)\nwhere q_{cls} \u2208 R^{1\u00d7D_h} represents the query for the class token of the last layer and k_p \u2208 R^{N\u00d7D_h} denotes the keys corresponding to the image patches in the last layer. The mean attention score is then obtained by averaging the attention scores from all multi-heads.\nOur experimental findings indicate that the mean attention scores, as computed by the tiny model, effectively assess the significance of each patch in contributing to the classification task. Fig. 4 presents a side-by-side comparison of ImageNet dataset images (left column) and their corresponding attention score maps (middle column). These maps clearly reveal that patches crucial for classification are distinguished by higher attention scores, setting them apart from less critical areas, such as background patches, which receive lower attention scores.\nThis observation supports that the tiny model on the edge device is adept at identifying and selecting the most informative patches for classification. Within ViT models, the class token aggregates information from other tokens (image"}, {"title": "V. ENTROPY-AWARE IMAGE TRANSMISSION", "content": "This section delves into entropy-aware image transmission, a strategy aimed at reducing communication overhead by considering the varied classification difficulty inherent to different images. For less complex images, the edge device's initial inference may be accurate enough, eliminating the need for further interaction with the server. In contrast, more intricate images necessitate more accurate classification from the server model, leading to increased communication overhead. It is critical for the edge device to make an accurate decision between relying on its initial inference and requesting more accurate classification from the server model.\nEven though the edge device cannot ascertain the correctness of its initial inference, it can estimate the inference's confidence through the softmax output values of the MLP classification head. This softmax output can be interpreted as the posterior probability p\u03b8(y|x), where y denotes the class label and \u03b8 denotes the tiny model. Then, we set an entropy function g : R^L \u2192 R, where L denotes the number of class labels. The client requests more accurate inference results from the server if:\ng(x) \u2265 \u03b7, (11)\nwhere \u03b7 denotes a predetermined threshold. To access the confidence of the client's inference, we consider two exemplary entropy measures: 1) Shannon entropy and 2) min-entropy, with their respective thresholds.\nThe Shannon entropy, a widely used metric for quantifying uncertainty [32], is calculated by\ng_s(x) = - \\sum_{y \u2208 Y} p_\u03b8(y|x) \\text{log}_2 p_\u03b8(y|x), (12)\nwhere Y denotes the set of all possible class labels. High Shannon entropy indicates that the given image x is challenging for the tiny model to classify accurately. Therefore, if g_s(x) \u2265 \u03b7_s,"}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "Our experiment employs the ImageNet validation dataset and resizes each image to a resolution of 224 x 224 pixels by center cropping. An image is flattened to N = 196 patches before the inference. We utilize the DeiT models pre-trained on the ImageNet dataset, as presented in Table I.\nWe evaluate the impact of varying the number of transmitted patches on communication cost and classification cost. The communication cost in our collaborative inference system is quantified by the ratio of the number of transmitted patches to the total number of patches. Consequently, if the edge device\ng_m(x) = - \\text{log}_2 \\text{max}_{y \u2208 Y} p_\u03b8(y|x), (13)\nwhich is directly associated with the confidence level of the initial inference. If g_m(x) \u2265 \u03b7_m, the edge device transmits the selected patches to the server for an inference from the base model.\nOur experimental results in Section VI-D show that the min-entropy serves as a better metric within our collaborative inference framework. Entropy-aware image transmission utilizing the min-entropy improves communication efficiency for a given level of classification accuracy when compared to using the Shannon entropy.\nShannon entropy has been utilized in diverse applications, such as prioritizing unlabeled data inputs in active learning [34] and wireless data acquisition [35]. Our work leverages min-entropy as a metric to evaluate the quality of initial inferences made by edge devices, setting our application apart from its previous utilization."}, {"title": "VII. CONCLUSION", "content": "We presented a novel attention-aware collaborative inference system using pre-trained ViT models. The edge device employs a compact ViT model as a semantic encoder, selecting pivotal patches that focus on objects crucial for classification. Our results confirm that this framework not only significantly lowers communication costs but also preserves accuracy comparable to that of server models. Furthermore, it offers the added benefit of diminishing the server model's computational complexity. Extending this approach to encompass a broader range of tasks beyond classification provides an intriguing direction for future research."}]}