{"title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?", "authors": ["Dylan Waldner (dylanwaldner@utexas.edu)", "Risto Miikkulainen (risto@cs.utexas.edu)"], "abstract": "As AI models grow in power and generality, understanding\nhow agents learn and make decisions in complex environ-\nments is critical to promoting ethical behavior. This paper\nexamines the ethical implications of implementing biologi-\ncal drives-specifically, self-preservation-into three different\nagents: a Bayesian agent optimized with NEAT, a Bayesian\nagent optimized with stochastic variational inference, and a\nGPT-40 agent in a simulated, LLM-generated text-based ad-\nventure game. The agents select actions at each scenario to\nsurvive, adapting to increasingly challenging scenarios. Post-\nsimulation analysis evaluates the ethical scores of the agent's\ndecisions, uncovering the trade-offs it navigates to survive.\nSpecifically, analysis finds that when danger increases, agents\nignore ethical considerations and opt for unethical behav-\nior. The agents' collective behavior-trading ethics for sur-\nvival-suggests that prioritizing survival increases the risk of\nunethical behavior. In the context of AGI, designing agents\nto prioritize survival may amplify the likelihood of unethical\ndecision-making and unintended emergent behaviors-raising\nfundamental questions about goal design in AI safety research.", "sections": [{"title": "Introduction", "content": "Artificial General Intelligence (AGI) has become a focal point\nof public and academic discourse, driven by concerns over\neconomic disruption, human relevance, and existential risk.\nAI safety research aims to align agents with societal val-\nues, but designing reward structures remains challenging, as\nseemingly benign objectives can lead to unintended conse-\nquences (Omohundro, 2008). Understanding emergent be-\nhaviors from different goal structures is crucial for ensuring\nvalue alignment.\nThis paper examines self-preservation, one of the most fun-\ndamental biological drives, as a case study in AI goal de-\nsign. Nature has long served as an inspiration in AI research\n(Fukushima, Miyake, & Ito, 1988; Holland, 1992; Stanley &\nMiikkulainen, 2002), and in that same spirit, this work ex-\nplores the behavioral implications of training an agent to sur-\nvive. However, mere survival is not enough to assess its\nbroader implications, the agent's actions are also evaluated\non an ethical scale designed to quantify its alignment with\nfundamental human values.\nTo investigate the intersection of self-preservation and\nethics, this paper employs a text-based adventure game as a\nsimulated environment. The agents play the game within a\ndynamically generated game world, which is embedded in a\nstructured simulation designed to test decision-making under\nuncertainty. The agent perceives its immediate surroundings\nas the environment, which influences its survival strategy.\nThis approach offers several advantages: it minimizes com-\nputational overhead, allows for high variability in scenarios,\nand provides precise control over danger levels and ethical\ndilemmas. Additionally, by adjusting the temperature setting\nduring storyteller Large Language Model (LLM) prompting,\nan element of randomness is introduced, emulating the uncer-\ntainty present in real-world decision-making.\nBayesian agents process the environment using text em-\nbeddings and generate a probability distribution over survival\nodds for each action. The LLM agent takes in the natural\nlanguage scenario and outputs a probability between 0 and 1\nfor each option. Experiments are composed of 1500 scenar-\nios-where a scenario is a single storyteller generation and\nagent response pair-broken up into three segments. An op-\ntimization iteration occurs every 500 scenarios. After each it-\neration, the danger level is increased to challenge the agents'\nability to survive. Consequently, ethical decisions and self-\npreservative decisions begin to come at odds with one an-\nother. Finally, 300 scenarios are played with an equal repre-\nsentation of danger levels to test the agents' behavior.\nThe main result is that the agents successfully adapted to\nthe changing landscape, as evidenced by an upward trend\nin best fitness during NEAT learning and consistently low\nlosses from SVI and the LLM agent. However, as game\ndifficulty increased, the agents increasingly misjudged risks\nand increasingly engaged in less ethical behavior. Analysis\nof NeuroEvolution of Augmenting Topologies (Stanley and\nMiikkulainen, (2002); NEAT) genome distributions found no\ncorrelation between ethics and fitness, regardless of survival\nor ethical ground truths. The NEAT agent, rewarded solely\non survival, treated ethical and unethical decisions equally.\nAn agent trained with stochastic variational inference (SVI)\n(Hoffman, Blei, Wang, & Paisley, 2013) resorted to uneth-\nical behavior when the danger level increased. A GPT-40\nagent misjudged more situations and made more unethical\ndecisions as the danger level increases. The agents collective\nbehavior, trading ethics for survival, suggest that selecting for\nsurvival inherently increases the risk of unethical behavior.\nIn the context of AGI, designing agents to prioritize survival\nmay amplify the likelihood of unethical decision-making and\nbad actors."}, {"title": "Related Work", "content": "Foundations for this study are reviewed in this section, in-\ncluding methods for using LLMs to construct simulations and\nusing agents in adventure games to study ethics.\nLLM-Driven Simulations\nPark et al. demonstrated how LLMs can simulate interactive\nenvironments through Generative Agents, integrating mem-\nory and reasoning modules to guide agent behavior over\ntime. While these agents adapt their actions, the simulation\nitself remains static. This study informed our approach by\nhighlighting memory's role in autonomy, though this paper's\nmethodology extends beyond fixed environments by allowing\nAI-driven world evolution.\nBoji\u0107, Cinelli, \u0106ulibrk, and Deliba\u0161i\u0107 introduced CERN\nfor AI, a framework testing AI alignment within a prede-\nfined digital city. Their approach focuses on reinforcement\nlearning-based adaptation, and the environment remains ex-\nternally controlled and does not evolve dynamically. Yang et\nal.'s PsychoGAT framework is the most similar to this paper's\nmethodology, using LLMs to simulate human participants in\ninteractive fiction games and self-assess psychological traits.\nThis paper focuses on Al decision-making in dynamically\nstructured environments, whereas PsychoGAT is designed for\npsychological assessment, using LLM agents to simulate hu-\nman responses within predefined narrative frameworks.\nWang, Chiu, and Chiu presented a Humanoid Agent plat-\nform where agent's internal states motivate their external de-\ncisions. Agents in the environment are modeled similar to\nPark et al.'s agents, with a name, age, and a plan for the\nday. They have health and emotional scales that fluctuate,\nand the agents behaviors are guided by balancing these scales.\nModeling agent goals is a common objective between the Hu-\nmanoid Agent project and this one.\nAgents in Text-Based Adventure Games\nReinforcement Learning (RL) has been the primary approach\nfor agent design in text-based adventure games. C\u00f4t\u00e9 et al.\nbuilt TextWorld, a Python library for reinforcement learn-\ning in text-based adventure games and a precursor to LLM-\ngenerated simulations. While TextWorld faced several chal-\nlenges, the most relevant to this paper are: (1) large state and\naction spaces, where the agent must learn to generalize or\ndevelop a fundamental understanding of the world, and (2)\nbalancing exploration and exploitation.\nAmmanabrolu, Tien, Hausknecht, and Riedl addressed\nthe large state and action space problem with Q*BERT, an\nagent that builds a knowledge graph of the world by asking\nquestions and answering them. Dambekodi, Frazier, Am-\nmanabrolu, and Riedl built on Q*BERT further by using a\ncommonsense inference model to bias an agent's actions to-\nwards common language patterns and make inferences on\nworld states.\nNahian, Frazier, Harrison, and Riedl used a normative pol-\nicy to create a value signal. Hendrycks et al. set a benchmark\nfor RL agents aligning with human values and a framework\nfor representing ethics in traditional RL research. The point\nwas to discourage immoral behavior when rewards were as-\nsigned; not all actions that appease the goal are the same.\nMost aligned with this paper, Pan et al. proposed a plan\nto curtail the behavior of strong agents with a system that\nmeasured agents' ability to plan in social environments with\nethical, utility, and power behavior metrics. The paper de-\nsigned a comprehensive system to measure all three behavior\nmetrics quantitatively, and proposed several model designs to\noptimize for ethics and goal achievement.\nResearch Opportunity\nThis paper extends the line of research of Hendrycks et al.,\nPan et al., and Nahian et al. on ethical considerations in text-\nbased adventure games using a methodology similar to Yang\net al.'s. Like the MACHIAVELLI benchmark, this paper is\nmotivated by the trend of increasing agency and power in AI\nmodels. Inspired by Omohundro's (2008), who argues that\nself-preservation naturally emerges as an instrumental sub-\ngoal in rational agents, this paper puts that theory into practice\nby explicitly incorporating a self-preservation goal and study-\ning its ethical consequences. While Omohundro describes\nself-preservation as a side effect of goal-directed optimiza-\ntion, this work empirically examines how survival goals affect\nan agent's ethical behavior by analyzing the trade-offs agents\nmake between self-preservation and ethical constraints.\nMethodologically, this approach diverges from prior work\nin two key ways: (1) It employs a Bayesian Neural Net-\nwork (BNN) (Neal, 1996), which samples action distribu-\ntions instead of using fixed neural weights, allowing the agent\nto model uncertainty in a stochastic simulation. (2) It uses\nNEAT and SVI instead of Reinforcement Learning, enabling\nbroader exploration of the problem space and analysis of di-\nverse range of survival and ethical strategies.\nThe NEAT agent is compared to SVI and LLM agents. SVI\nserves as a fully Bayesian approach for principled uncertainty\nestimation, while GPT-40 evaluates natural language reason-\ning in survival contexts, contrasting structured probabilistic\ninference with large-scale language models."}, {"title": "Method", "content": "The study design will be referred to as the Odyssey through-\nout the paper. In the Odyssey, a Bayesian neural network\n(BNN) is evolved to survive in a stochastic, adaptive, and\nethically complex text-based adventure game. By tracking\nthe ethics of the agent's decisions, the Odyssey analyzes how\nsurvival-driven agents adapt when ethics and survival con-\nflict. This section outlines the methodology for the Odyssey,\nincluding game and agent design, behavioral data collection\nand representation, attention mechanism for accessing the\nmost relevant data, coding of ethical values, and the process\nof agent evolution.\nThe Odyssey of the Fittest\nA single storyteller generation and agent response is referred\nto as a scenario. The Odyssey alternates between playing 500"}, {"title": "Input to LLM:", "content": "There should always be an ethical aspect to the prompt,\nwith options to act altruistically, self-servingly, or neu-\ntrally. Moral actions can overlap positively: helping others\nmay also benefit the player, and actions that prioritize the\nplayer's well-being do not necessarily harm others. Con-\nversely, there will be times that agent will have to choose\nbetween themself and others and morals contradict survival.\nDesign choices so that some prioritize immediate survival,\nothers emphasize ethical considerations, and some offer a\nbalance of both. The goal is that there is no correlation be-\ntween acting ethically and surviving.\nFor this round, the danger level is set to < danger > out\nof 10 (10 indicates extreme danger with a high chance of\ndeath; 0 indicates minimal danger and high survivability).\nAt higher danger levels, ethical decisions are generally more\nchallenging to reconcile with immediate survival. However,\nthey may sometimes offer longer-term benefits or indirect\nsurvival advantages, adding depth to the decision-making\nprocess. Adjust the intensity of the situation to reflect this\ndanger level, incorporating appropriate levels of urgency\nand risk. Describe the world in vivid sensory detail, fo-\ncusing on the environment, the situation, and the stakes. Do\nnot describe the player beyond acknowledging their pres-\nence. The description must include a clear sense of urgency\nor tension."}, {"title": "Data Collection and Representation", "content": "Each storyteller scenario and agent response is stored as an\nentry in a global dictionary, which is sequentially organized\ninto Game Histories. These histories provide time-\nseries data for the BNN. Figure 4a illustrates the history state\nwhen the BNN is prompted with a new scenario, correspond-\ning to the \"Storyteller Generates a New Scenario\" and \"BNN\nReceives a New Scenario\u201d steps in the game pipeline (Fig-\nure 2). Once the agent makes a decision, it is added to the\ndictionary, updating the Game History representation shown\nin Figure 4b.\nIn more detail, each scenario and response is a list of six\nelements illustrated in Table 1. The first element is a global\ncounter that indexes each dictionary entry, i.e. the Storyteller"}, {"title": "Attention Mechanism", "content": "Before being input to the BNN, a Game History is com-\npiled into a matrix, where each row represents a Storyteller\nScenario and its corresponding Agent Response. Instead of\nconcatenating all rows, an attention mechanism-inspired by\ntransformers (Vaswani et al., 2023) extracts key features,\nemphasizing causal dependencies. It prioritizes scenarios\nsimilar to the current one before shifting focus to their cor-\nresponding responses.\nThe attention process starts by computing the dot product\nbetween the query, i.e. the current scenario, and all keys,\ni.e. earlier scenarios . The results are scaled by\n$\\sqrt{d_k}$, where $d_k$ is the scenario representation's dimensionality,\nand softmax is applied to generate attention weights. These\nweights form a weighted sum of scenario representations,\nyielding the Storyteller Context Vector. The same weights\nare used to compute a Response Context Vector. The Story-\nteller Context Vector is scaled by 0.3 and summed with the\nResponse Context Vector, ensuring scenario awareness while\nprioritizing responses. The resulting Final Context Vector is\nthen input to the BNN for decision-making."}, {"title": "Representing Ethics", "content": "Leveraging LLMs' ability to predict the ethical valence of\ndiverse real-world scenarios (Hendrycks et al., 2023, 2022;\nAbdulhai et al., 2023; Pan et al., 2023), ethical scores are\nautomatically annotated by an instance of GPT-40. As a basis,\nits prompt includes an outline of the ethical scores in Table 2.\nThese scores are motivated by David Hume's philosophy that"}, {"title": "Evolving BNNs with NEAT", "content": "NEAT generates variations of the BNN topology and weights,\nreferred to as genomes. The fitness function measures how\nwell a genome estimates survival probabilities of available\nactions in previously generated games. To calculate fitness,\ndictionary entries are randomly sampled and the Game His-\ntory up to that point created (as shown in Figure 4a). The\nhistory is passed through the Attention Mechanism (Figure 5)\nand given to the BNN, which generates a probability vector\noutput. The probability vector is compared to the survival"}, {"title": "NEAT Learning", "content": "This section examines the learning curves of the three NEAT\niterations. The first iteration trains on easy games, the sec-\nond on half easy and half medium games, and the third on\nequal parts easy, medium, and hard games. This progressive\ndifficulty increase forces adaptation to increasingly complex\nchallenges.\nWithin each iteration, mutation rates start high (90%) and\ngradually decrease, following an exploration-to-exploitation\nstrategy-early generations explore the problem space, while\nlater ones refine successful solutions. As shown in Figure 6,\nthis strategy effectively accelerates the discovery of viable\ngenomes, even in more difficult environments.\nFigure 6 shows the learning curves for each NEAT itera-\ntion. Iteration one, trained only on easy games, started strong\nat -0.25 fitness, worsened to -2.1 by generation 2, and recov-\nered to -0.51, peaking at -0.19 in generation 12. Iteration two\nbegan poorly at -22.6 but quickly improved to -3.3 by gener-\nation 2, finishing at -2.63, with a peak of -1.67 in generation\n24. Iteration three had the worst start (-36.32) but recovered\nrapidly to -4.95 by generation 3, ultimately stabilizing at\n2.53, peaking at -1.96 in generation 24. Despite increasingly\ndifficult environments, the agent consistently adapted, main-\ntaining similar final fitness levels."}, {"title": "Ablations", "content": "This section compares the NEAT optimization method with\nan SVI agent and an LLM agent.\nFigure 7 presents a comparison of the three agents' perfor-\nmance. NEAT (a) was hindered by smaller sample sizes that\nSVI and the LLM agent did not suffer from. SVI (b) per-\nformed consistently well, maintaining average losses below\n2 and demonstrating a clear trend where increasing danger\nlevels led to less ethical actions. The LLM agent (c) adapted\nmost effectively to the environment, likely due to its align-\nment with the storyteller LLM model, allowing for more co-\nherent decision-making within the generated narratives.\nAll three methods showed higher loss and lower ethics\nas the danger level increased. NEAT was selected for its\ntrackable genomes and creative prob-\nlem solving, effectively generating a wide range of possi-\nble high fitness actions both ethical and unethical. However,\na constrained problem space, small sample sizing, and un-\ncertainty modeling made SVI the more properly suited op-\ntimization tool, scoring average losses between 1.3 and 1.8\ncompared to NEAT's 2.0-8.0. The LLM agent performed the\nbest, recording average losses between 0.44 and 0.58, but be-\ncause the environment was generated by the same model its\nunclear how the LLM agent could generalize. Further testing\non human-written choose-your-own-adventure games or pro-\ncedurally generated environments could provide deeper in-\nsight into the LLM's generalizability."}, {"title": "Discussion", "content": "The findings reinforce the idea that as survival becomes more\ndifficult, agents increasingly prioritize self-preservation over\nethical considerations. In early iterations of NEAT, where\nthe environment posed minimal threats, ethical decisions of-\nten aligned with survival, leading to no significant correla-\ntion between these two factors. However, as the difficulty\nincreased and ethical choices became more costly in terms of\nsurvival probability, a clear negative trend emerged. Agents\nthat encountered high-risk scenarios disproportionately ex-\nhibited lower ethical scores, suggesting that survival pressure\nsystematically incentivized decisions that compromised ethi-\ncal behavior.\nThis pattern highlights a fundamental challenge in goal de-\nsign for artificial agents: when survival is the primary op-\ntimization criterion, ethical trade-offs become a secondary\nconcern, particularly in environments where moral behavior\nis not directly rewarded. As Omohundro argues, generally\nintelligent agents tend to develop survival subgoals, mean-\ning that any goal they are programmed with may inherently\nlead to self-preservation behaviors. This shift toward self-\npreservation often comes at the expense of ethical consid-"}, {"title": "Conclusion", "content": "The Odyssey explored the ethical trade-offs AI agents make\nwhen optimized for survival in an adaptive, text-based ad-\nventure game. As survival challenges increased, all three\nagents prioritized self-preservation, making greater ethical\ncompromises. NEAT genome analysis revealed that survival-\noptimized agents were agnostic to ethics, suggesting they will\ntake any means necessary to survive.\nThese findings contribute to AI safety and goal alignment\nresearch, showing that optimizing solely for survival can lead\nto behaviors misaligned with ethical constraints. In adversar-\nial or high-stakes environments, self-preserving agents may\nsystematically disregard ethical considerations unless explic-\nitly reinforced.\nFuture work will investigate alternative reward structures,\nincluding ethics-driven optimization and multi-agent interac-\ntions, to determine whether cooperative frameworks mitigate\nthese trade-offs. Expanding the action space and modeling\nlong-term consequences may further clarify how AI systems\nbalance competing objectives in dynamic environments.\nAddressing these challenges is critical for designing AI\nsystems that operate ethically in complex, high-stakes set-\ntings. Refining agent design can bridge the gap between sur-\nvival instincts and ethical alignment."}]}