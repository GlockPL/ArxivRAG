{"title": "A Geometric Perspective for High-Dimensional Multiplex Graphs", "authors": ["Kamel Abdous", "Nairouz Mrabah", "Mohamed Bouguessa"], "abstract": "High-dimensional multiplex graphs are characterized by their high number of complementary and divergent dimensions. The existence of multiple hierarchical latent relations between the graph dimensions poses significant challenges to embedding methods. In particular, the geometric distortions that might occur in the representational space have been overlooked in the literature. This work studies the problem of high-dimensional multiplex graph embedding from a geometric perspective. We find that the node representations reside on highly curved manifolds, thus rendering their exploitation more challenging for downstream tasks. Moreover, our study reveals that increasing the number of graph dimensions can cause further distortions to the highly curved manifolds. To address this problem, we propose a novel multiplex graph embedding method that harnesses hierarchical dimension embedding and Hyperbolic Graph Neural Networks. The proposed approach hierarchically extracts hyperbolic node representations that reside on Riemannian manifolds while gradually learning fewer and more expressive latent dimensions of the multiplex graph. Experimental results on real-world high-dimensional multiplex graphs show that the synergy between hierarchical and hyperbolic embeddings incurs much fewer geometric distortions and brings notable improvements over state-of-the-art approaches on downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed the advent of complex real-world systems, where basic units are connected via multiple types of interaction. In this context, multiplex graphs have gained popularity for representing the interdependent structure of these systems [14]. Multiplex graphs are characterized by their multidimensionality; the connectivity of the nodes differs from one dimension to another. Among the principal concepts that govern this type of data is the existence of complementary and divergent information across different dimensions [7]. This applies in particular when the number of dimensions increases, leading to the emergence of high-dimensional multiplex graphs. The divergence across a multitude of dimensions presents significant challenges for embedding algorithms. More precisely, high-dimensional multiplex graph embedding methods should uncover convoluted relations across diverse dimensions to extract a unified graph structure and more informative node representations.\nMost multiplex graph embedding techniques rely on two strategies: Random Walks (RWs) and Graph Neural Networks (GNNs). RW-based methods [5, 25] generate sequences of nodes on individual dimensions to extract dimension-specific embeddings, which are then linearly aggregated into consensus embeddings. In the same way, several GNN-based approaches [12, 17, 23] produce dimension-specific node embeddings and then perform a linear aggregation of these embeddings. However, both strategies fail to consider the hierarchical relations between the graph dimensions, which cannot be captured by a single linear aggregation. Recently, a first attempt has been made to address this critical issue. The authors of [1] have introduced a hierarchical aggregation strategy that summarizes the graph dimensions into gradually smaller sets. However, the impact of existing multiplex graph embedding techniques, including the hierarchical aggregation strategy, on the embedding space geometry remains unexplored. In particular, it is important to investigate the effect of increasing the number of dimensions from a geometric perspective.\nAn interesting geometric perspective for understanding multiplex graph embedding is to assess the Intrinsic Dimension (ID) and the Linear Intrinsic Dimension (LID) of the latent manifolds. On the one hand, the ID measures the minimal number of variables needed to describe the data. On the other hand, the LID measures the dimension of the smallest subspace that can enclose all data points. The difference between ID and LID highlights how much the data structure deviates from a linear subspace. When the LID exceeds the ID, it implies that the manifold has a curved structure, which cannot be effectively represented by a linear subspace with the same dimension [2]. The greater the difference, the more curved the manifold will be. Using the ID and LID estimations, it has been shown that training vanilla neural networks in a supervised fashion transforms the initial latent structures into low-dimensional and curved ones [2]. In another work, the authors of [20] have shown that training graph neural networks for uni-dimensional graphs under the deep clustering paradigm causes abrupt geometric distortions of the latent manifolds after the pretraining phase. However, no previous work has studied the evolution of ID and LID in the context of multiplex graph embedding. In this work, we focus on answering two questions: What are the geometric implications of embedding diverse structural patterns spread across multiple graph dimensions? How to encode the hierarchical relations between the dimensions of a multiplex graph without causing significant geometric distortions?\nTo answer the first question, we conduct a geometric investigation. First, we construct synthetic multiplex graphs that span a large spectrum of dimensions, with node clusters spread across multiple dimensions. Using synthetic data allows to control the number of dimensions and the extent of divergent information (i.e., between-cluster connections) across the graph structures. Then, we assess the geometric distortions by measuring the discrepancy between the ID and LID for state-of-the-art embedding strategies. Our results suggest that the encoding process subjects the latent space to coarse transformations. More precisely, we find that the node embeddings reside on highly-curved and low-dimensional manifolds independently of the number of graph dimensions. Moreover, we observe that increasing the number of graph dimensions, which is accompanied by an increase in divergent information (i.e., presence of between-cluster connections across all dimensions) and a dilution of relevant information (i.e., sparser within-cluster connections spread across several dimensions), strengthens the geometric distortions and makes the latent manifolds more curved. Consequently, the learned representations under these geometric distortions are suboptimal for downstream tasks.\nRecently, Hyperbolic Graph Neural Networks (HGNNs) [6, 15] have been devised to encode uni-dimensional graphs with complex topologies (e.g., hierarchical and looping structures [21]). HGNNS project graph nodes into Riemannian manifolds, such as Poincar\u00e9 balls and spheres [15]. This process generates hyperbolic representations, which are well suited for solving downstream tasks on graphs with tree-like structural patterns [3, 28]. To answer the second question raised in this work, we elaborate a hyperbolic-based embedding approach to tackle the geometric distortions inherent in multiplex graph encoding methods. We find that hyperbolic embedding, coupled with hierarchical dimension aggregation, not only captures improved representations of high-dimensional graphs but also yields latent manifolds with minimal geometric distortions."}, {"title": "Contributions", "content": "(i) Identification of a new problem: One of the core contributions of our work lies in identifying the geometric distortions that occur in the embedding of high-dimensional multiplex graphs. More precisely, we show that increasing the number of graph dimensions, which is accompanied by an increase in divergent information (i.e., more between-cluster connections across all dimensions) and a dilution of relevant information (i.e., sparser within-cluster connections spread across several dimensions), leads to the occurrence of geometric distortions and makes the latent manifolds more curved. We provide a geometric study of these distortions on synthetic and real-world graphs. (ii) Methodological novelty: We propose HYPER-MGE, a novel approach that can learn hierarchical representations of the graph dimensions by embedding them into hyperbolic spaces. We argue that hierarchical aggregations can capture complex hidden structures and that hyperbolic geometry can account for the hierarchical latent structures without geometric distortions. (iii) Empirical validation: We support our claims with extensive experiments that show the effectiveness of our approach in reducing geometric distortions and improving performance on downstream tasks compared to existing methods. Our approach brings substantial enhancement over the state-of-the-art methods in several cases."}, {"title": "2 Related Work", "content": "Consistent with our focus, we examine multiplex graph embedding and hyperbolic graph neural network techniques."}, {"title": "2.1 Multiplex Graph Embedding", "content": "MultiVERSE [25] learns node representations from random walks that traverse the graph. To handle the multidimensional aspect of multiplex graphs, the constructed node sequences can transit from one dimension to another. Similarly, GATNE [5] converts random walks into training samples and uses trainable transformations to compute node embeddings. In a high-dimensional setting, random walk-based methods require long sequences of nodes to cover all the dimensions of a multiplex graph. Besides, local optimization algorithms (such as Skip-Gram [16]) form the basis of these methods' training modules. Thus, long-range dependencies spanning multiple dimensions can be difficult to identify. Last but not least, random walk-based methods lack the expressive power to capture compositional and convoluted relations across diverse dimensions of high-dimensional multiplex graphs.\nAnother line of work exploits Graph Neural Networks (GNNs) to encode multiplex graphs into low-dimensional vectors. DMGI [23] and HDMI [12] apply multiple GNNs to learn node embeddings on individual dimensions. Then, a linear aggregation step with an attention mechanism converts the dimension-specific embeddings into consensus codes. Both models, DMGI and HDMI, optimize a contrastive loss based on mutual information maximization [31], but HDMI has a higher-order objective function that includes node features. X-GOAL [11] improves the contrastive loss by grouping topologically similar nodes and nodes within the same cluster into positive and negative pairs. SSDCM [17] maximizes the mutual information between local node-level representations and a global cluster-aware graph summary. DMG [19] and MGDCR [18] focus on the common and complementary information among the multiplex graph dimensions. DMG employs disentangled representations to separate between common and private information. MGDCR uses a loss function to minimize the correlation between inter-dimension and intra-dimension representations. In contrast with the unsupervised methods considered in this paper, SSAMN [26] is a semi-supervised approach that integrates spectral embedding to encode multiplex graphs with a small number of labeled nodes provided during training. Overall, all these methods fail to consider the compositional nature of high-dimensional multiplex graphs and are thus hindered by information loss caused by the single and linear aggregation step.\nThe authors of HMGE [1] have established the existence of hierarchical relations between the dimensions of real-world multiplex graphs. This concept describes how new high-level graph dimensions can be formed from the non-linear hierarchical combinations of lower-level initial dimensions. Motivated by this finding, HMGE introduces a hierarchical aggregation mechanism instead of the single and linear aggregation. More precisely, HMGE defines trainable non-linear combinations stacked in a gradual way to shrink the number of dimensions and build new relevant ones. However, previous GNN-based methods, including HMGE, overlook the geometric distortions that might occur in the latent space due to the hierarchical relations between a high number of graph dimensions. In this work, we establish the presence of these geometric distortions at the latent space and propose a solution to this problem based on hyperbolic embedding."}, {"title": "2.2 Hyperbolic Graph Neural Networks", "content": "These models map the input graph into hyperbolic spaces. The key aspect of a hyperbolic space is its exponential volume expansion with respect to its radius, whereas an Euclidean space exhibits a polynomial growth in volume. As a result, HGNNs can effectively encode complex patterns, including hierarchical and looping structures [6, 33]. This is because a hyperbolic space aligns with the growth rate of tree-like patterns, a property that an Euclidean space can not grant. HGNNs produce high-quality representations in real-world scenarios and can improve performance compared to Euclidean competitors in various downstream tasks [24].\nFrom a geometric perspective, previous works [32] have shown that embedding unidimensional graphs with complex structural patterns causes geometric distortions in the latent space. To alleviate this problem, hyperbolic embedding techniques are known to reduce these distortions for graphs with tree-like structural patterns [3]. In practice, the most prevalent hyperbolic manifolds correspond to the Poincar\u00e9 and Lorentz models [15]. Although HGNNS have shown success in capturing hierarchical relations between the graph nodes with minimal geometric distortions, it is still unknown if embedding the hierarchical relations between the graph dimensions can cause geometric distortions in the latent space. If so, it is important to provide a solution for encoding these hierarchical relations in a way that exhibits minimal geometric distortions."}, {"title": "3 A Geometric Study of High-Dimensional Multiplex Graph Embedding", "content": "In this section, we identify a new problem that affects multiplex graph embedding methods. Specifically, we answer the question: What are the geometric implications of embedding multiple graph dimensions? To this end, we conduct an empirical study that investigates the geometric transformations on the latent manifolds caused by embedding high-dimensional multiplex graphs. We devise a two-phase protocol: (i) Generating synthetic multiplex graphs that span a large spectrum of dimensions and simulate the structure of real-world data. (ii) Training state-of-the-art multiplex graph embedding methods on the synthetic data and monitoring the evolution of the ID and LID metrics as the number of dimensions increases to assess the distortions."}, {"title": "3.1 Synthetic Data Generation", "content": "Real-world multiplex graphs have two critical properties: (i) the abundance of between-cluster connections as the number of dimensions increases, and (ii) the spread of sparse within-cluster connections across several dimensions [4]. Accordingly, we devise a synthetic data generation process that simulates these two properties. Using synthetic data, we can control the number of dimensions and the extent of divergent information (i.e., between-cluster connections) across the graph topological structures. This allows us to assess the impact of high dimensionality on the latent space geometry. Moreover, discarding the divergent information requires capturing complex hierarchical relations between the dimensions. This information would be hard to control without synthetic data. For these reasons, we generate synthetic multiplex graphs that span a high number of dimensions between 5 and 100. The full description of the multiplex graphs generation process is provided in Appendix A."}, {"title": "3.2 Evaluation Protocol", "content": "Our study includes state-of-the-art embedding approaches based on RWs and GNNs, such as DMGI [23], HDMI [12], SSDCM [17], MultiVERSE [25], X-GOAL [11], GATNE [5], MGDCR [18], DMG [19], and HMGE [1]. We train the baselines on the synthetic datasets and measure the average ID and LID of the latent manifolds at the end of training. After that, we calculate the difference between the two metrics and plot the variation of this quantity with respect to the number of graph dimensions D. For a fair comparison, we set the size of the node embeddings to 64 for all methods. The ID and LID metrics are described in Appendix B."}, {"title": "3.3 Geometric Distortions in Latent Spaces", "content": "Figure 1 shows the results of our geometric study. The horizontal axis corresponds to the number of dimensions in the synthetic multiplex graphs, while the vertical axis measures the difference between the average LID and ID of the latent manifolds at the end of the training process. It is worth noting that the difference between these metrics starts small for all baseline methods and consistently increases during training to peak by the end, regardless of the graph's dimension D. A large difference between these two metrics indicates that the latent manifolds reach a highly-curved state. We classify the approaches into two categories according to the results: (1) The difference between the LID and ID increases as the number of dimensions D increases: We observe this behavior in multiple approaches (DMGI, HDMI, SSDCM, MGDCR, DMG, and MultiVERSE). For these methods, as the number of dimensions rises, the latent manifolds become progressively more curved. Thus, increasing the number of dimensions intensifies the geometric distortions caused by transforming the flat initial manifolds into curved ones. This observation can be explained by the increase in divergent information when D increases, which translates into more curved manifolds for encoding more divergent information. Such geometric distortions imply that the final node representations cannot be easily exploited by downstream tasks.\n(2) The difference between the LID and ID is high even when D is small: This behavior is observed in GATNE, X-GOAL, and HMGE, which learn latent codes that reside in highly curved manifolds interdependently of D. X-GOAL and GATNE employ graph transformation techniques, which can corrupt the topological structures and cause strong divergence among the initial views even when D is small. HMGE leverages hierarchical aggregations and thus can capture convoluted relations across dimensions. We argue that hierarchical graph dimension embedding gives birth to curved manifolds. Similar to image datasets, encoding pixel representations hierarchically has a characteristic curvature increase [2, 13].\nWhile first-category methods can prevent geometric distortions for low-dimensional graphs, both categories still exhibit such distortions when embedding high-dimensional graphs. HYPER-MGE is our response to this challenge; it is a multiplex graph embedding approach that yields node representations constrained to flat and low-dimensional manifolds."}, {"title": "4 The Proposed HYPER-MGE Approach", "content": "In this section, we address the problem of geometric distortions that negatively affect the latent manifolds of multiplex graph embedding methods. Specifically, we answer the question: How to encode the hierarchical latent dimensions of a multiplex graph without causing geometric distortions? Our goal is to capture complex, yet relevant relations across the graph dimensions in a way that prevents high geometric distortions, especially in high-dimensional graphs where such issues are more prominent. To this end, we introduce HYPER-MGE, a novel approach to multiplex graph embedding that hierarchically encodes the graph dimensions while projecting node representations into hyperbolic spaces. In this context, we argue that our method not only effectively encodes hierarchical structures within-dimension but also hierarchical between-dimension relations. An example of hierarchical relations in multiplex graphs can be found in Appendix C.\nWe first introduce some notations used throughout the paper. We consider a D-dimensional multiplex graph G, defined as a set of D graphs $G = (G_1, G_2, . . ., G_D)$, where $G_d = (V, A_d, X)$ is a graph with N nodes from the set $V = \\{v_1, v_2, \u2026 \u2026 \u2026, v_N \\}$, $A_d \\in \\{0,1\\}^{N \\times N}$ is the adjacency matrix and $X \\in R^{N \\times F}$ is the node features matrix. Each graph $G_d$ represents a dimension of the multiplex graph G. Dimensions share the same nodes and features, but differ in their adjacency matrices. Given G, the goal is to learn an M-dimensional vector representation $z_i \\in R^M$ for each node $v_i \\in V$, forming a matrix of node embeddings $Z \\in R^{N \\times M}$."}, {"title": "4.1 Hyperbolic Multiplex Graph Embedding", "content": "Euclidean-based Graph Convolutional Network (GCN) conducts a series of L message-passing operations according to:\n$H_i^{(l)} = \\sigma(\\tilde{A}_d H_i^{(l-1)} W^{(l)})$,                                                                                                                             (1)\nwhere l is the layer index, $\\tilde{A}_d = \\tilde{D}_d^{-\\frac{1}{2}} A \\tilde{D}_d^{-\\frac{1}{2}}$,  $A = A_d + I$, $\\tilde{D}_d = diag(\\tilde{A}_d 1_N)$, $1_N \\in R^N$ is a vector of ones, $W^{(l)}$ is the weight matrix of the GCN, and $\\sigma$ is an activation function. The propagation rule in hyperbolic spaces applies mapping functions to project from Euclidean space to Riemannian manifolds and vice versa [15] as given by:\n$H_i^{(l)} = \\tau(exp_x (\\tilde{A}_d log_x (H_i^{(l-1)})W^{(l)}))$,                                                                                                      (2)\nwhere $exp_x$ and $log_x$ are the mapping functions, and x is a chosen point in the Riemannian manifold. The logarithmic map $log_x$ projects the points from the Riemannian manifold to the Euclidean space, which allows to compute linear operations (matrix multiplications in this case). After that, the points are mapped back to the Riemannian manifold with the exponential map $exp_x$.\nPrior works have demonstrated that in a hyperbolic space of constant negative curvature, the embedding of a hierarchical structure can be achieved with lower dimensional distortion compared to Euclidean space [21, 27]. Specifically, a hyperbolic space can represent hierarchical relationships more efficiently due to its exponential growth in volume with distance from the origin, aligning with the branching factor of hierarchical structures. In an embedding procedure, the topology of a hyperbolic space is defined by the mapping functions $log_x$ and $exp_x$. In this paper, we consider the Poincar\u00e9 Ball and Lorentz definitions:\n(a) Poincar\u00e9 Ball Model: This model defines a Riemannian manifold with the following mapping functions:\n$exp_x (h) = x \\oplus \\text{tanh} \\left(\\frac{\\lambda_x \\|h\\|}{2}\\right) \\frac{h}{\\|h\\|} \\qquad (3)$\n$\\text{log}_x(h) = \\frac{2}{\\lambda_x} \\text{arctanh} (\\|x \\ominus h\\|) \\frac{x \\ominus h}{\\|x \\ominus h\\|} ,                                                                                                  (4)$\nwhere $\\lambda_x = \\frac{2}{1-\\|x\\|^2}$ and the symbol $\\oplus$ represents the M\u00f6bius addition operation [9]. Following previous work [9], we set x to the origin point.\n(b) Lorentz Model: It is a model that exhibits better empirical performance than the Poincar\u00e9 Ball Model [15]. Unlike previous models, it operates on (M + 1)-dimensional vectors. We first define the Minkowski inner product: $(x, h)_L = -x_0h_0+\\Sigma_{i=1}^M x_ih_i$. Then, the mapping functions for the Lorentz model are:\n$exp_x(h) = cosh(\\frac{\\|h\\|_L}{\\lambda_x}) x + sinh(\\frac{\\|h\\|_L}{\\lambda_x}) \\frac{h + (x, h)_L x}{\\sqrt{(x, h)_L^2 -1}}.$             (5)\n$\\text{log}_x (h) = \\frac{\\text{arcosh} (\\frac{-(x, h)_L}{\\lambda_x})}{\\sqrt{(x, h)_L^2 -1}} (h + (x, h)_L x)$                                                                                                   (6)\nIn this case, we use x = (1, 0, ..., 0) \u2208 $R^{M+1}$ to account for the additional coordinate.\nThe Poincar\u00e9 and Lorentz models allow learning representations that reside in hyperbolic spaces, leading to embeddings with lower geometric distortion. However, the high dimensionality of multiplex graphs and their hierarchical nature still affect the geometry of latent manifolds. In particular, as shown in Section 3, the increase of divergent information and dilution of relevant information which accompanies the increase in the dimensionality of multiplex graphs lead to geometric distortions in latent spaces. To address this issue, we introduce an additional operation in each layer of HYPER-MGE that summarizes the dimensions of the multiplex graph into a smaller and more informative set of dimensions. This mechanism is called Hierarchical Aggregations and consists of a multi-step combination of the graph adjacency matrices. Let $D_{l-1}$ be the number of input dimensions of the l-layer (initially, $D_0 = D$). Then, the following formula combines $D_{l-1}$ adjacency matrices to form $D_l$ higher-order adjacency matrices:\n$A_j^{(l)} = \\phi(\\Sigma_{i=1}^{D_{l-1}} a_{i,j}^{(l)} A_i^{(l-1)})$                                                                              (7)\nwhere $\\phi$ is an activation function, and $a_{i,j}^{(l)}$ is the weight that quantifies the importance of the $i^{th}$ input dimension in the $j^{th}$ output dimension. These weights are trainable parameters of the model, and they are normalized with a softmax function before computing the hierarchical aggregation:\n$a_{i,j}^{(l)} = \\frac{exp (a_{i,j}^{(l)})}{\\sum_{k=1}^{D_{l-1}} exp (a_{k,j}^{(l)})}$                                                                                  (8)\nThe next layer uses the newly computed adjacency matrices $A_j^{(l)}$ to refine the node embeddings. Moreover, matrices $A_j^{(l)}$ are once again combined into an even smaller set of dimensions, forming a multi-level hierarchy of adjacency matrices that capture latent graph structures. Finally, inside each layer l, node embeddings $H_d^{(l)}$ extracted from the combined dimensions are aggregated to consensus embeddings:\n$H^{(l)} = \\Sigma_{d=1}^{D_{l-1}} \\beta_d^{(l)} H_d^{(l)}$                                                                                    (9)\nwhere $\\beta_d^{(l)}$ are attention weights. The final layer outputs node embeddings $Z = H^{(L)}$. With the extraction of higher-order adjacency matrices in Equation (7), the embedding process encodes hierarchical relations within and between the graph dimensions. In addition, stacking multiple aggregation layers allows to capture increasingly complex relevant patterns hidden in non-linear combinations of the graph dimensions. In Section 4.3, we conduct a geometric study that shows that hyperbolic and hierarchical embeddings alleviate the effects of geometric distortions. Before that, we give intuitions and insights on how the proposed approach can extract node representations that reside in flat low-dimensional manifolds, thus avoiding geometric distortions.\nIn multiplex graphs, the source of geometric distortions is two-fold. First, as a result of the increase of divergent information and dilution of relevant information, high-dimensional multiplex graphs contain complex latent structures hidden across the graph dimensions. Second, encoding a graph dimension with a complex structure leads to coarse geometric distortions in the latent space [20, 32]. We argue that HYPER-MGE addresses these two points. In [27], the author discusses methods for embedding tree-like structures in hyperbolic spaces, providing theoretical evidence that hyperbolic spaces can achieve embeddings with lower distortion compared to Euclidean spaces. Furthermore, it has been demonstrated that hyperbolic embeddings can significantly outperform Euclidean embeddings in terms of learning hierarchical representation with lower dimensional distortion [21]. Finally, the authors of [9] extend the application of hyperbolic geometry to neural networks, providing further evidence that hyperbolic spaces can represent data with inherent hierarchical structures more efficiently than Euclidean spaces. Thus, hyperbolic embedding can effectively encode complex hierarchical topologies within graph dimensions and hierarchical relations between graph dimensions into Riemannian manifolds. On the other hand, hierarchical aggregations gradually reduce the number of graph dimensions, generating high-order adjacency matrices that capture informative and relevant latent structures while encoding complex relations between nodes [1]. The synergy of hyperbolic geometry and hierarchical aggregations allows HYPER-MGE to avoid coarse geometric distortions, leading to embeddings that reside in flat, low-dimensional manifolds."}, {"title": "4.2 Training Algorithm", "content": "To train HYPER-MGE, we optimize the Deep Graph Infomax loss function [31]. First, we compute a graph-level representation by aggregating the node embeddings: $s = \\frac{1}{N} \\Sigma_{i=1}^N z_i$. Then, we maximize the mutual information between s and node embeddings from the set $Z = \\{Z_1, Z_2, ..., Z_N\\}$. To achieve this, we sample positive pairs (s, zi) from G, and negative pairs (s, $\\tilde{z_i}$) from $\\tilde{G}$, a corrupted version of G obtained by randomly shuffling the features. After that, we train a discriminator to distinguish between positive and negative pairs. We use bilinear scoring for the discriminator: $\\mathcal{D}(h_i, s) = Sigmoid(h_i^T Q s)$, where $Q \\in R^{M \\times M}$ is a parameter matrix. The loss function is:\n$\\mathcal{L} = \\frac{1}{N} \\Sigma_{i=1}^N log \\mathcal{D}(s, z_i) + \\Sigma_{j=1}^N log(1 \u2013 \\mathcal{D}(s, \\tilde{z_j}))$.                                                                                                   (10)"}, {"title": "4.3 Geometric Study of HYPER-MGE", "content": "In Figure 2, we report the results of HYPER-MGE according to the experimental protocol of Section 3. We observe that the difference between the LID and ID is small (between 0 and 4) and does not increase when D increases. This contrasts with the baselines in Figure 1, where the LID and ID tend to diverge from each other. Unlike concurrent methods, HYPER-MGE can prevent geometric distortions independently from the number of graph dimensions and learns node representations that lie in flat low-dimensional spaces. Accordingly, our approach is less affected than the baselines by the high dimensionality of the data in Figure 1. In other words, the synergy of hierarchical and hyperbolic embeddings permits HYPER-MGE to counter the increase in divergent information and dilution of relevant information when the number of dimensions increases. In the next section, we show that our contributions translate to significant empirical improvements compared to the state-of-the-art."}, {"title": "5 Experiments", "content": "We conduct an empirical evaluation to demonstrate the suitability of the proposed approach for high-dimensional multiplex graph embedding. We compare HYPER-MGE\u00b9 against the state-of-the-art methods discussed in Section 3.2, namely: MultiVERSE, GATNE, SSDCM, DMGI, HDMI, MGDCR, DMG, X-GOAL, and HMGE."}, {"title": "5.1 Datasets", "content": "In this section, we describe the real-world datasets used in the experiments. Table 1 summarizes their statistics\u00b2.\nBIOGRID and STRING-DB are protein-protein interaction graphs collected from [22] and [29], respectively. The nodes are proteins and the edges are interactions between the proteins. The edges in each dimension are inferred by different experimental protocols (e.g., the biochemical effect of one protein on another). In BIOGRID, node classification labels indicate the species from which the protein is extracted. In STRING-DB, labels represent protein families.\nDBLP-Authors is an academic graph assembled from AMiner [30]. Authors of research papers are depicted as nodes, and an edge indicates a co-authorship relation between two authors. Dimensions represent various conferences and journals where authors have co-written papers. Classification labels indicate the authors' research areas (thus, in this dataset, nodes can be part of multiple classes).\nIMDB\u00b3 is a multiplex graph where nodes are movies, and edges indicate that at least a person has participated in both movies. Different dimensions represent different roles: actors, directors, producers, etc. Nodes are labeled with the movie genre."}, {"title": "5.2 Evaluation Protocol & Parameter Settings", "content": "We evaluate our approach on two downstream tasks: link prediction and node classification. To perform link prediction on the hyperbolic embeddings of HYPER-MGE, we use the Fermi-Dirac decoder [21] to compute probability scores between edges. It is a generalization of the sigmoid function to hyperbolic spaces as described by:\n$\\mathcal{P}(z_i, z_j) = [e^{(\\text{artanh} (\\|z_i \\ominus z_j\\|)-r)/t} + 1]^{-1}$                                                                                                          (11)\nwhere r and t are hyper-parameters. For other baselines, we use the standard dot product $Sigmoid(ZZ^T)$, since their representations are Euclidean. We report the area under the ROC curve (AUC-ROC) and average precision (AP). For node classification, we first map the hyperbolic embeddings to the Euclidean space and then run logistic regression [6]. We assess the results with F1-Macro and F1-Micro.\nFor HYPER-MGE, we set the embedding size to 96 and use 2 hierarchical aggregation layers, with ReLU as the activation function $\\phi$ and Leaky ReLU as \u03c3. Based on previous work [6, 15], we use the Lorentz model as the Riemannian manifold and set the Fermi-Dirac decoder hyperparameters to r = 2 and t = 1. We use Adam with a learning rate of 0.001 and a weight decay of 10-5. We train for 1, 000 epochs with early stopping after 20 iterations. These settings are kept constant on all datasets. Finally, we run our model 5 times and report the mean and standard deviation. For the baselines, we only report the best result among 5 trials."}, {"title": "5.3 Evaluation Results", "content": "Link Prediction. Table 2 shows a comparison between HYPER-MGE and the baselines on the task of link prediction. OOM indicates that the method has run out of memory during training. In all tables, the best result is highlighted in bold and the second best is underlined. As we can see from Table 2, HYPER-MGE consistently outperforms all methods by a significant margin. In addition, We observe that the standard deviation is small, which indicates that the results of HYPER-MGE are consistent. The two most competitive baselines are X-GOAL and HMGE. X-GOAL is not suitable for high-dimensional multiplex graphs, because its embedding module is based on a single and linear aggregation step. On the other hand, HMGE leverages hierarchical aggregations but is not equipped with a mechanism to counter the effects of geometric distortions. The improvements brought by HYPER-MGE illustrate the benefits and the importance of our contributions. In addition, the results suggest that the geometric study on synthetic data in Section 3 translates to practical improvements on real-world datasets.\nNode Classification. Table 3 shows the results of node classification, where HYPER-MGE outperforms the baselines in all datasets. Furthermore, computing the p-values of the paired t-test of HYPER-MGE and the most competitive approach (HMGE) in a population of 5 samples for each model confirms the significance of our results. We remark that random walk-based approaches (MultiVERSE and GATNE) underperform compared to HYPER-MGE. This is because these methods employ a sub-optimal local optimization algorithm to encode long-range dependencies spanning multiple dimensions. Furthermore, we can see that GNN methods (SSDCM, DMGI, HDMI, DMG, MGDCR, and X-GOAL) are less competitive than HMGE and HYPER-MGE, which harness hierarchical aggregations on top of GNNs. This shows that hierarchical embeddings lead to empirical improvements in high-dimensional datasets. Finally, hyperbolic embeddings allow HYPER-MGE to further increase the prediction accuracy compared to HMGE. In particular, hyperbolic embeddings alleviate geometric distortions in latent spaces. As illustrated in Section 4.3, this results in node representations that lie in flat and low-dimensional spaces suitable for downstream tasks like node classification."}, {"title": "5.4 Ablation Study", "content": "To demonstrate the synergy between hierarchical and hyperbolic embeddings, we conduct an ablation study with HYPER-MGE and a modified version of X-GOAL that incorporates the same hyperbolic multiplex embedding mechanism. More precisely, we have changed the code of X-GOAL to equip it with hyperbolic modules using different Riemannian manifolds to encode the graph. Lorentz and Poincar\u00e9 are hyperbolic spaces, while Euclidean is equivalent to a standard GNN. Here, we draw the reader's attention to the fact that incorporating hyperbolic modules into existing approaches is not a straightforward task, as it requires a careful reimplementation of the existing code. Furthermore, not all existing methods can be easily modified to incorporate hyperbolic modules. In these settings, we choose to compare with X-GOAL because (i) there is a possibility to reimplement X-GOAL in order to incorporate hyperbolic modules and (ii) it is among the most competitive baselines (see Table 2), so we claim that the comparison with hyperbolic X-GOAL extends the results of our study to other competing algorithms.\nTable 4 shows the ablation results of the hyperbolic module. In most datasets, HYPER-MGE-Lorentz yields the best scores, although in DBLP-Authors it is outperformed by HYPER-MGE-Poincar\u00e9. Previous work [15] suggests that the Lorentz model generally achieves better performance on downstream tasks, which is consistent with the results of this ablation study. On the other hand, X-GOAL does not benefit from hyperbolic spaces, since X-GOAL-Euclidean performs significantly better than the Poincar\u00e9 and Lorentz versions."}, {"title": "5.5 Intrinsic Dimensionality During Training", "content": "Figure 3 shows the variations of the intrinsic dimensions during the training of HYPER-MGE and X-GOAL on the BIOGRID dataset. We monitor the ID and LID metrics after each training iteration. The results are representative of the other datasets, therefore, to save space, we chose only to include BIOGRID. We observe that the values with HYPER-MGE are close throughout training, which implies that the representations converge to a relatively flat low-dimensional space. On the other hand, as suggested by the difference between the two metrics, X-GOAL learns more curved latent manifolds. This phenomenon is caused by the high number of dimensions of BIOGRID, and the fact that X-GOAL is not equipped with mechanisms to counter the negative effects of geometric distortions. These results extend our geometric study on synthetic data to real-world datasets. Furthermore, it geometrically demonstrates the effectiveness and relevance of our contributions, and gives empirical intuitions on"}]}