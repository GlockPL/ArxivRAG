{"title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMS", "authors": ["Kewei Cheng", "Jingfeng Yang", "Haoming Jiang", "Zhengyang Wang", "Bixuan Huang", "Ruirui Li", "Shiyang Li", "Zheng Li", "Yifan Gao", "Xian Li", "Bing Yin", "Yizhou Sun"], "abstract": "Reasoning encompasses two typical types: de-ductive reasoning and inductive reasoning. De-spite extensive research into the reasoning ca-pabilities of Large Language Models (LLMs),most studies have failed to rigorously differenti-ate between inductive and deductive reasoning,leading to a blending of the two. This raises anessential question: In LLM reasoning, whichposes a greater challenge - deductive or induc-tive reasoning? While the deductive reasoningcapabilities of LLMs, (i.e. their capacity tofollow instructions in reasoning tasks), havereceived considerable attention, their abilitiesin true inductive reasoning remain largely unex-plored. To delve into the true inductive reason-ing capabilities of LLMs, we propose a novelframework, SolverLearner. This frameworkenables LLMs to learn the underlying function(i.e., y = fw(x)), that maps input data points(x) to their corresponding output values (y),using only in-context examples. By focusingon inductive reasoning and separating it fromLLM-based deductive reasoning, we can isolateand investigate inductive reasoning of LLMs inits pure form via SolverLearner. Our observa-tions reveal that LLMs demonstrate remarkablereasoning capabilities through Solver-Learner, achieving near-perfect performancewith ACC of 1 in most cases. Surprisingly, de-spite their strong inductive reasoning abilities,LLMs tend to relatively lack deductive reason-ing capabilities, particularly in tasks involving\"counterfactual\" reasoning.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed notable progress inNatural Language Processing (NLP) with the de-velopment of Large Language Models (LLMs) likeGPT-3 (Brown et al., 2020) and ChatGPT (Ope-nAI, 2023). While these models exhibit impressivereasoning abilities across various tasks, they facechallenges in certain domains. For example, a re-cent study (Wu et al., 2023) has shown that whileLLMs excel in conventional tasks (e.g., base-10arithmetic), they often experience a notable declinein accuracy when dealing \u201ccounterfactual\u201d reason-ing tasks that deviate from the conventional casesseen during pre-training (e.g., base-9 arithmetic).It remains unclear whether they are capable of fun-damental reasoning, or just approximate retrieval.\nIn light of this, our paper seeks to investigatethe reasoning capabilities of LLMs. Reasoningcan encompasses two types: deductive reasoningand inductive reasoning, as depicted in Fig. 1. De-ductive reasoning starts with a general hypothesisand proceeds to derive specific conclusions aboutindividual instances while inductive reasoning in-volves formulating broad generalizations or princi-ples from a set of instance observations. Despiteextensive research into the reasoning capabilities ofLLMs, most studies have not clearly differentiatedbetween inductive and deductive reasoning. For in-stance, arithmetic reasoning task primarily focuseson comprehending and applying mathematical con-cepts to solve arithmetic problems, aligning morewith deductive reasoning. Yet, when employingin-context learning for arithmetic reasoning tasks,where the model is prompted with a few (input,output) examples, the observed improvements areoften attributed to their inductive reasoning capac-ity. This fusion of reasoning types poses a criticalquestion: Which is the more significant limita-tion in LLM reasoning, deductive or inductivereasoning?\nTo explore this question, it's crucial to differen-tiate between deductive and inductive reasoning.Current methods that investigate deductive and in-ductive reasoning often rely on disparate datasets,making direct comparisons challenging (Xu et al.,2023a; Tang et al., 2023; Dalvi et al., 2021; Hanet al., 2022; Sinha et al., 2019; Yu et al., 2020). Tovercome this limitation, we have designed a set ofcomparative experiments that utilize a consistenttask across different contexts, each emphasizing"}, {"title": "2 Task Definition", "content": "Our research is focused on a relatively unexploredquestion: Which presents a greater challenge toLLMs - deductive reasoning or inductive reasoning?To explore this, we designed a set of comparativeexperiments that apply a uniform task across var-ious contexts, each emphasizing either deductiveor inductive reasoning. The primary distinctionbetween the deductive and inductive settings iswhether we explicitly present input-output map-pings to the models. Informally, we can describethese mappings as a function fw : X \u2192 Y, wherean input x \u2208 X is transformed into an output y \u2208 Y.We distinguish between the deductive and inductivesettings as follows:\n\u2022 Deductive setting: we provide the models withdirect input-output mappings (i.e., fw).\n\u2022 Inductive setting: we offer the models a fewexamples (i.e., (x, y) pairs) while intentionallyleaving out input-output mappings (i.e., fw).\nFor example, consider arithmetic tasks, where thebase system is the input-output mapping function.The two approaches on the left side of Fig. 1 (i.\u0435.,method (a) and (b)) follow the deductive setting,illustrating the case where the arithmetic base isexplicitly provided. In contrast, the two methods(i.e., method (c) and (d)) on right side of Fig. 1adhere to the inductive setting, depicting the sce-nario characterized by the absence of a specifiedarithmetic base, while a few input-output examplesare provided for guidance."}, {"title": "3 Our Framework for Inductive Reasoning: SolverLearner", "content": "While recent studies have explored the inductivereasoning abilities of LLMs (Yang et al., 2022; Gen-dron et al., 2023; Xu et al., 2023b), they have primar-ily relied on Input-Output (IO) prompting (Mirchan-dani et al., 2023). This method involves providingmodels with a few (input, output) demonstrationsand then evaluating their performance on unseenexamples, as depicted in method (c) in Fig. 1. Ourresearch suggests that the use of IO prompting anddirectly evaluating the final instance performancemight not effectively separate LLMs' deductivereasoning skills from their inductive reasoning abil-ities. This is because the approach moves directlyfrom observations to specific instances, obscuringthe inductive reasoning steps. To better disentangleinductive reasoning, we propose a novel framework,SolverLearner. This framework enables LLMs tolearn the function (i.e., y = fw(x)), that maps in-put data points (x) to their corresponding outputvalues (y), using only in-context examples. Byfocusing on inductive reasoning and setting asideLLM-based deductive reasoning, we can isolate andinvestigate inductive reasoning of LLMs in its pureform via SolverLearner. SolverLearner includestwo-stages as illustrated in Fig. 2:\n\u2022 Function Proposal: In this initial phase, wepropose a function, that could be used to mapinput data points (x) to their corresponding outputvalues (y). This is corresponding to the inductivereasoning process.\n\u2022 Function Execution: In the second phase, theproposed function is applied through externalcode interpreters to solve the test queries forevaluation purposes. This phase ensures thatthe LLM is fully prevented from engaging in\ndeductive reasoning."}, {"title": "3.1 Framework", "content": "In this subsection, we will take the arithmetic taskas a case study to demonstrate the entire process.\nFunction Proposal: Given the in-context ex-amples, the primary goal of LLMs is to learn afunction that can map input data points (x) to theircorresponding output values (y). This process oflearning the mapping between inputs and outputsis akin to inductive reasoning, while employingthe learned function to address unseen queriesaligns with deductive reasoning. In order to sepa-rate inductive reasoning from deductive reasoning,the execution of the learned function should becompletely detached from LLMs. To achieve thisseparation, external tools such as code interpretersserve as efficient way to execute these functions in-dependently. By encapsulating the learned functionwithin Python code, we can effectively detach theduty of deductive reasoning from LLMs, assigning"}, {"title": "4 Tasks", "content": "In this section, we provide a brief overview of thetasks under consideration. Our focus is on inves-tigating the reasoning abilities of LLMs in bothdeductive and inductive reasoning scenarios. Toensure a robust evaluation, we carefully select tasksthat lend themselves well to comparison. Firstly, toprevent LLMs from reciting tasks seen frequentlyduring pre-training, which could artificially inflateperformance in deductive reasoning, a significantportion of the tasks falls into the category of \"coun-terfactual reasoning\u201d tasks. Secondly, in the contextof inductive reasoning, where only a few in-contextexamples are available without the mapping func-tion, our objective is to learn the function thatmaps inputs to outputs based on this restricteddataset. To achieve this, we choose tasks that arewell-constrained, ensuring the existence of a single,unique function capable of fitting this limited data.Detailed descriptions of each task and the promptsused can be found in Appendix A.1 and A.2.\nArithmetic In this study, we focus on the two-digit addition task previously explored in the workof Wu et al. (2023). We investigate multiplenumerical bases, specifically base-8, 9, 10, 11, and16 where base 10 corresponds to the commonlyobserved case during pretraining. In the context ofdeductive reasoning, the base is explicitly providedwithout any accompanying in-context examples,and the LLM is expected to perform the additioncomputation by relying on its inherent deductivereasoning abilities. Conversely, in the context ofinductive reasoning, instead of explicitly providingthe base information to LLMs, we provide LLMssolely with few-shot examples and require themto induce the base through these examples andsubsequently generate a function to solve arithmeticproblems.\nBasic Syntactic Reasoning In this setting, weconcentrate on tasks related to syntactic recognitionpreviously explored by Wu et al. (2023). Ourobjective is to evaluate LLMs using artificiallyconstructed English sentences that vary from theconventional subject-verb-object (SVO) word order.For deductive reasoning, we directly provide thenew word order to LLMs without any contextualexamples, challenging them to identify the subject,verb, and object within this artificial language. Incontrast, for inductive reasoning, we do not give"}, {"title": "5 Results", "content": "For each task, we evaluate our proposed Solver-Learner for pure LLM inductive reasoning andother settings using two different models, gpt-3.5-turbo-1106 and gpt-4-1106-preview, which are de-noted as GPT-3.5 and GPT-4 respectively. Sinceboth methods are closed-source, we do not providespecific information about their size, architecture,and pre-training particulars. Our experiments pri-marily focus on investigating the reasoning abilitiesof LLMs in both deductive and inductive reasoningscenarios. Therefore, we structure our evaluationacross two distinct settings to highlight each typeof reasoning. The formal definition of each settingis provided in Sec. 2. For the deductive setting, twomethods are proposed for investigation:\n\u2022 Zero-shot evaluates deductive reasoning abilityof the LLMs in its pure form. It tests the LLM'sability to conclude information about specificindividuals based solely on instructions, withoutrelying on examples.\n\u2022 8-IO w/ Mapping Function (MF) follows thedeductive setting but enhances LLM reasoningfurther by incorporating in-context examples. Italigns with the most commonly used promptmethods for enabling LLM reasoning. With theinclusion of in-context examples, this approachcan be seen as leveraging inductive reasoning toaugment deductive reasoning.\nFor the inductive setting, we propose two methodsfor evaluation:\n\u2022 8-IO w/o Mapping Function (MF) aligns withtraditional input-output (IO) prompting methodswidely used to investigate the inductive reasoningcapability of LLMs. However, as this methodproceeds directly from a set of observations tospecific target instances, it remains intertwinedwith LLM-based deductive reasoning.\n\u2022 8-shot SolverLearner corresponds to our pro-posed framework for inductive reasoning, capableof evaluating inductive reasoning ability of theLLMs in its pure form. It segregates the learningof input-output mapping functions from the ap-plication of these functions for inference, therebypreventing the blend of LLM-based deductivereasoning into the process.\nBesides using 8-shot examples, our study also in-cludes experiments with 16-shot examples to assesshow changes in the number of in-context examplesimpact the results. Experimental results are givenin the Appendix A.3. Generally, the results indicate"}, {"title": "5.1 Main Results", "content": "The results for all tasks are presented from Fig. 3through Fig. 5. Specifically, Fig. 3 concentrates oncomparing performances in the deductive setting,while Fig. 4 examines comparisons in the inductivesetting. Additionally, Fig. 5 focuses on contrastingthe models' capabilities across deductive and induc-tive setting. For further reference, the prompts usedfor all tasks are included in Appendix A.2, and thefull numerical results can be found in Appendix A.3.\nLLMs exhibit poor deductive reasoning ca\u0440\u0430-bilities, particularly in \u201ccounterfactual\u201d tasks.We include two methods in Fig. 3, Zero-shot and8-IO w/ Mapping Function (MF), to illustrate thedeductive reasoning capability of LLMs. Our obser-vations reveal that LLMs exhibit relatively weakerdeductive capabilities, especially in \u201ccounterfac-tual\" tasks, while showing prowers in standardtasks like base-10 arithmetic. This aligns withfindings reported in (Wu et al., 2023). Integrationof in-context examples notably enhances LLMs'performance in various scenarios, suggesting thattheir improvement stems from the acquisition ofknowledge through inductive reasoning from theseexamples. This further confirms the exceptionalinductive reasoning abilities of LLMs. This com-bined evidence suggests that LLMs face challengesin precisely following instructions and executingcommands, especially when those instructions arerelate to scenarios rarely encountered during theirpre-training phase.\nLLMs demonstrate remarkable inductive rea-soning capabilities through SolverLearner. Weinclude two methods in Fig. 4, SolverLearner (Ours)and 8-IO w/o Mapping Function (MF), to illustratethe inductive reasoning capability of LLMs. While8-IO w/o Mapping Function (MF) struggles withinductive reasoning, SolverLearner consistentlyachieves perfect performance with an accuracy of1 across all the cases with GPT-4 and succeeds inmost cases when used with GPT-3.5. This discrep-ancy arises because the utilization of IO promptingto directly reach conclusions on target instances maynot effectively distinguish between LLMs' deduc-"}, {"title": "5.2 More Results over Additional LLMs", "content": "To validate the generalizability of our conclusion,we have included results over additional LLMs,claude-3-sonnet-20240229-v1:0, which is denotedas Claude3. Due to space limitations, the fullnumerical results are provided in Appendix A.4."}, {"title": "5.3 Ablation Study", "content": "We conducted several experiments to gain a deeperunderstanding of our framework, detailed in the ab-lation studies in Appendix A.5. These experimentsinclude investigating the effects of programs exe-"}, {"title": "6 Related Works", "content": "6.1 In-Context Learning\nGPT-3 (Brown et al., 2020) has demonstrated itseffectiveness in learning from a few demonstrationexamples and solve previously unseen tasks with-out requiring updates to its model parameters (Weiet al., 2022a). This remarkable capability is com-monly referred to as the \u201cin-context learning ability\u201dof language models. It implies that the LLMs canleverage its existing knowledge and generalize froma few demonstration examples to solve new, relatedtasks (Dong et al., 2022; Liu et al., 2021; Rubin et al.,2021; Gonen et al., 2022). Some notable worksinclude chain-of-thought (CoT) prompting (Weiet al., 2022b), which elicits reasoning with inter-mediate steps in few-shot exemplars. Built uponthe CoT framework, several works expand CoT byorganizing and processing thoughts using morecomplex structures, such as trees (Yao et al., 2023)and graphs (Besta et al., 2023) or breaking a prob-lem into sub problems and then proceeds to solveeach one independently (Zhou et al., 2022). Whilethese studies have effectively improved the reason-ing capability of LLMs, they have failed to clearlydistinguish between inductive and deductive reason-ing, let alone investigate which represents a morecritical limitation for LLM reasoning capabilities:deductive reasoning or inductive reasoning."}, {"title": "6.2 Exploring LLMs' Reasoning Skills", "content": "Despite the impressive achievements of LLMs invarious reasoning tasks, the underlying mechanisms of their reasoning capabilities remain a subject ofdebate. The question of whether LLMs genuinelyreason in a manner akin to human cognitive pro-cesses or merely simulate aspects of reasoningwithout true comprehension is still open (Huangand Chang, 2022). For instance, Kojima et al.have suggested that LLMs exhibit commendablezero-shot reasoning abilities, implying that thesemodels can draw logical conclusions in scenariosthey have not been explicitly trained on (Kojimaet al., 2022). However, some researchers cast doubton the reasoning capability of LLMs. While approaches like the chain-of-thought method maymimic human-like thought processes, it remainsuncertain whether LLMs are genuinely engaging inreasoning or simply following patterns learned dur-ing training (Wei et al., 2022b; Valmeekam et al.,2022). Additionally, there's a debate regardingwhether LLMs are symbolic reasoners (Tang et al.,2023) or possess strong abstract reasoning capa-bilities (Gendron et al., 2023). In light of theseseemingly contradictory conclusions, our researchaims to delve deeper into the reasoning capabili-ties of LLMs. We intend to dissect the nuancesof inductive and deductive reasoning within thecontext of LLMs, identifying which form of reason-ing presents a more significant challenge to theirreasoning abilities."}, {"title": "6.3 Equipping LLMs with External Tools", "content": "Large Language Models (LLMs) have made signifi-cant progress in utilizing tools through frameworkslike CREATOR (Qian et al., 2023) and LATM (Caiet al., 2023), which allow LLMs to create tool"}]}