{"title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMS", "authors": ["Kewei Cheng", "Jingfeng Yang", "Haoming Jiang", "Zhengyang Wang", "Binxuan Huang", "Ruirui Li", "Shiyang Li", "Zheng Li", "Yifan Gao", "Xian Li", "Bing Yin", "Yizhou Sun"], "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored. To delve into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points ($x$) to their corresponding output values ($y$), using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive reasoning capabilities, particularly in tasks involving \"counterfactual\" reasoning.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed notable progress in Natural Language Processing (NLP) with the development of Large Language Models (LLMs) like GPT-3 (Brown et al., 2020) and ChatGPT (OpenAI, 2023). While these models exhibit impressive reasoning abilities across various tasks, they face challenges in certain domains. For example, a recent study (Wu et al., 2023) has shown that while LLMs excel in conventional tasks (e.g., base-10 arithmetic), they often experience a notable decline in accuracy when dealing \u201ccounterfactual\u201d reasoning tasks that deviate from the conventional cases seen during pre-training (e.g., base-9 arithmetic). It remains unclear whether they are capable of fundamental reasoning, or just approximate retrieval.\nIn light of this, our paper seeks to investigate the reasoning capabilities of LLMs. Reasoning can encompasses two types: deductive reasoning and inductive reasoning, as depicted in Fig. 1. Deductive reasoning starts with a general hypothesis and proceeds to derive specific conclusions about individual instances while inductive reasoning involves formulating broad generalizations or principles from a set of instance observations. Despite extensive research into the reasoning capabilities of LLMs, most studies have not clearly differentiated between inductive and deductive reasoning. For instance, arithmetic reasoning task primarily focuses on comprehending and applying mathematical concepts to solve arithmetic problems, aligning more with deductive reasoning. Yet, when employing in-context learning for arithmetic reasoning tasks, where the model is prompted with a few (input, output) examples, the observed improvements are often attributed to their inductive reasoning capacity. This fusion of reasoning types poses a critical question: Which is the more significant limitation in LLM reasoning, deductive or inductive reasoning?\nTo explore this question, it's crucial to differentiate between deductive and inductive reasoning. Current methods that investigate deductive and inductive reasoning often rely on disparate datasets, making direct comparisons challenging (Xu et al., 2023a; Tang et al., 2023; Dalvi et al., 2021; Han et al., 2022; Sinha et al., 2019; Yu et al., 2020). To overcome this limitation, we have designed a set of comparative experiments that utilize a consistent task across different contexts, each emphasizing"}, {"title": "Task Definition", "content": "Our research is focused on a relatively unexplored question: Which presents a greater challenge to LLMs - deductive reasoning or inductive reasoning? To explore this, we designed a set of comparative experiments that apply a uniform task across various contexts, each emphasizing either deductive or inductive reasoning. The primary distinction between the deductive and inductive settings is whether we explicitly present input-output mappings to the models. Informally, we can describe these mappings as a function $f_w : X \\rightarrow Y$, where an input $x \\in X$ is transformed into an output $y \\in Y$. We distinguish between the deductive and inductive settings as follows:\n\u2022 Deductive setting: we provide the models with direct input-output mappings (i.e., $f_w$).\n\u2022 Inductive setting: we offer the models a few examples (i.e., ($x$, $y$) pairs) while intentionally leaving out input-output mappings (i.e., $f_w$).\nFor example, consider arithmetic tasks, where the base system is the input-output mapping function. The two approaches on the left side of Fig. 1 (i.e., method (a) and (b)) follow the deductive setting, illustrating the case where the arithmetic base is explicitly provided. In contrast, the two methods (i.e., method (c) and (d)) on right side of Fig. 1 adhere to the inductive setting, depicting the scenario characterized by the absence of a specified arithmetic base, while a few input-output examples are provided for guidance."}, {"title": "Our Framework for Inductive Reasoning: SolverLearner", "content": "While recent studies have explored the inductive reasoning abilities of LLMs (Yang et al., 2022; Gendron et al., 2023; Xu et al., 2023b), they have primarily relied on Input-Output (IO) prompting (Mirchandani et al., 2023). This method involves providing models with a few (input, output) demonstrations and then evaluating their performance on unseen examples, as depicted in method (c) in Fig. 1. Our research suggests that the use of IO prompting and directly evaluating the final instance performance might not effectively separate LLMs' deductive reasoning skills from their inductive reasoning abilities. This is because the approach moves directly from observations to specific instances, obscuring the inductive reasoning steps. To better disentangle inductive reasoning, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the function (i.e., $y = f_w(x)$), that maps input data points ($x$) to their corresponding output values ($y$), using only in-context examples. By focusing on inductive reasoning and setting aside LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. SolverLearner includes two-stages as illustrated in Fig. 2:\n\u2022 Function Proposal: In this initial phase, we propose a function, that could be used to map input data points ($x$) to their corresponding output values ($y$). This is corresponding to the inductive reasoning process.\n\u2022 Function Execution: In the second phase, the proposed function is applied through external code interpreters to solve the test queries for evaluation purposes. This phase ensures that the LLM is fully prevented from engaging in deductive reasoning."}, {"title": "Framework", "content": "In this subsection, we will take the arithmetic task as a case study to demonstrate the entire process.\nFunction Proposal: Given the in-context examples, the primary goal of LLMs is to learn a function that can map input data points ($x$) to their corresponding output values ($y$). This process of learning the mapping between inputs and outputs is akin to inductive reasoning, while employing the learned function to address unseen queries aligns with deductive reasoning. In order to separate inductive reasoning from deductive reasoning, the execution of the learned function should be completely detached from LLMs. To achieve this separation, external tools such as code interpreters serve as efficient way to execute these functions independently. By encapsulating the learned function within Python code, we can effectively detach the duty of deductive reasoning from LLMs, assigning"}, {"title": "Tasks", "content": "In this section, we provide a brief overview of the tasks under consideration. Our focus is on investigating the reasoning abilities of LLMs in both deductive and inductive reasoning scenarios. To ensure a robust evaluation, we carefully select tasks that lend themselves well to comparison. Firstly, to prevent LLMs from reciting tasks seen frequently during pre-training, which could artificially inflate performance in deductive reasoning, a significant portion of the tasks falls into the category of \"counterfactual reasoning\u201d tasks. Secondly, in the context of inductive reasoning, where only a few in-context examples are available without the mapping function, our objective is to learn the function that maps inputs to outputs based on this restricted dataset. To achieve this, we choose tasks that are well-constrained, ensuring the existence of a single, unique function capable of fitting this limited data. Detailed descriptions of each task and the prompts used can be found in Appendix A.1 and A.2.\nArithmetic In this study, we focus on the two-digit addition task previously explored in the work of Wu et al. (2023). We investigate multiple numerical bases, specifically base-8, 9, 10, 11, and 16 where base 10 corresponds to the commonly observed case during pretraining. In the context of deductive reasoning, the base is explicitly provided without any accompanying in-context examples, and the LLM is expected to perform the addition computation by relying on its inherent deductive reasoning abilities. Conversely, in the context of inductive reasoning, instead of explicitly providing the base information to LLMs, we provide LLMs solely with few-shot examples and require them to induce the base through these examples and subsequently generate a function to solve arithmetic problems.\nBasic Syntactic Reasoning In this setting, we concentrate on tasks related to syntactic recognition previously explored by Wu et al. (2023). Our objective is to evaluate LLMs using artificially constructed English sentences that vary from the conventional subject-verb-object (SVO) word order. For deductive reasoning, we directly provide the new word order to LLMs without any contextual examples, challenging them to identify the subject, verb, and object within this artificial language. In contrast, for inductive reasoning, we do not give"}, {"title": "Results", "content": "For each task, we evaluate our proposed SolverLearner for pure LLM inductive reasoning and other settings using two different models, gpt-3.5-turbo-1106 and gpt-4-1106-preview, which are denoted as GPT-3.5 and GPT-4 respectively. Since both methods are closed-source, we do not provide specific information about their size, architecture, and pre-training particulars. Our experiments primarily focus on investigating the reasoning abilities of LLMs in both deductive and inductive reasoning scenarios. Therefore, we structure our evaluation across two distinct settings to highlight each type of reasoning. The formal definition of each setting is provided in Sec. 2. For the deductive setting, two methods are proposed for investigation:\n\u2022 Zero-shot evaluates deductive reasoning ability of the LLMs in its pure form. It tests the LLM's ability to conclude information about specific individuals based solely on instructions, without relying on examples.\n\u2022 8-IO w/ Mapping Function (MF) follows the deductive setting but enhances LLM reasoning further by incorporating in-context examples. It aligns with the most commonly used prompt methods for enabling LLM reasoning. With the inclusion of in-context examples, this approach can be seen as leveraging inductive reasoning to augment deductive reasoning.\nFor the inductive setting, we propose two methods for evaluation:\n\u2022 8-IO w/o Mapping Function (MF) aligns with traditional input-output (IO) prompting methods widely used to investigate the inductive reasoning capability of LLMs. However, as this method proceeds directly from a set of observations to specific target instances, it remains intertwined with LLM-based deductive reasoning.\n\u2022 8-shot SolverLearner corresponds to our proposed framework for inductive reasoning, capable of evaluating inductive reasoning ability of the LLMs in its pure form. It segregates the learning of input-output mapping functions from the application of these functions for inference, thereby preventing the blend of LLM-based deductive reasoning into the process.\nBesides using 8-shot examples, our study also includes experiments with 16-shot examples to assess how changes in the number of in-context examples impact the results. Experimental results are given in the Appendix A.3. Generally, the results indicate"}, {"title": "Main Results", "content": "The results for all tasks are presented from Fig. 3 through Fig. 5. Specifically, Fig. 3 concentrates on comparing performances in the deductive setting, while Fig. 4 examines comparisons in the inductive setting. Additionally, Fig. 5 focuses on contrasting the models' capabilities across deductive and inductive setting. For further reference, the prompts used for all tasks are included in Appendix A.2, and the full numerical results can be found in Appendix A.3.\nLLMs exhibit poor deductive reasoning capabilities, particularly in \u201ccounterfactual\u201d tasks. We include two methods in Fig. 3, Zero-shot and 8-IO w/ Mapping Function (MF), to illustrate the deductive reasoning capability of LLMs. Our observations reveal that LLMs exhibit relatively weaker deductive capabilities, especially in \u201ccounterfactual\" tasks, while showing prowers in standard tasks like base-10 arithmetic. This aligns with findings reported in (Wu et al., 2023). Integration of in-context examples notably enhances LLMs' performance in various scenarios, suggesting that their improvement stems from the acquisition of knowledge through inductive reasoning from these examples. This further confirms the exceptional inductive reasoning abilities of LLMs. This combined evidence suggests that LLMs face challenges in precisely following instructions and executing commands, especially when those instructions are relate to scenarios rarely encountered during their pre-training phase.\nLLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner. We include two methods in Fig. 4, SolverLearner (Ours) and 8-IO w/o Mapping Function (MF), to illustrate the inductive reasoning capability of LLMs. While 8-IO w/o Mapping Function (MF) struggles with inductive reasoning, SolverLearner consistently achieves perfect performance with an accuracy of 1 across all the cases with GPT-4 and succeeds in most cases when used with GPT-3.5. This discrepancy arises because the utilization of IO prompting to directly reach conclusions on target instances may not effectively distinguish between LLMs' deductive and inductive reasoning skills. By completely disentangling the inductive reasoning of LLMs, our proposed SolverLearner shows the remarkable inductive reasoning capabilities inherent in LLMs. It is also noteworthy that the efficacy of LLMs\u2019 inductive reasoning capability heavily depends on the foundational model, with GPT-4 consistently outperforming GPT-3.5.\nDeductive reasoning presents a greater challenge than inductive reasoning for LLMs. To compare the challenges of the deductive reasoning capability with the inductive reasoning capability of LLMs, we include two methods in Fig. 1, SolverLearner and Zero-shot, demonstrating pure inductive and deductive reasoning abilities. Since the entire reasoning involves two steps: first, obtaining the input-output function ($f_w$), which corresponds to inductive reasoning, and second, applying the function for inference, which corresponds to deductive reasoning. Once both steps are successfully completed, perfect performance is observed, as indicated by the dotted line in the figure. Zero-shot can be seen as replacing the first step with an oracle, with deductive reasoning capability of LLMs to be studied, while SolverLearner can be seen as replacing the second step with an oracle, with inductive reasoning capability of LLMs to be studied. By comparing the gaps of SolverLearner and Zero-shot towards perfect reasoning, we can observe that in most cases, LLMs can complete the inductive step perfectly, while they rarely achieve perfect performance on the deductive step. This indicates that in LLM reasoning, deductive reasoning presents a greater challenge. Note that we avoid to phrasing it as directly comparing inductive and deductive reasoning capabilities. Instead, we examine whether the gaps mainly come from inductive or inductive reasoning, considering that LLMs could not achieve perfect counterfactual reasoning."}, {"title": "More Results over Additional LLMs", "content": "To validate the generalizability of our conclusion, we have included results over additional LLMs, claude-3-sonnet-20240229-v1:0, which is denoted as Claude3. Due to space limitations, the full numerical results are provided in Appendix A.4."}, {"title": "Ablation Study", "content": "We conducted several experiments to gain a deeper understanding of our framework, detailed in the ablation studies in Appendix A.5. These experiments include investigating the effects of programs exe-"}, {"title": "Related Works", "content": "In-Context Learning\nGPT-3 (Brown et al., 2020) has demonstrated its effectiveness in learning from a few demonstration examples and solve previously unseen tasks without requiring updates to its model parameters (Wei et al., 2022a). This remarkable capability is commonly referred to as the \u201cin-context learning ability\u201d of language models. It implies that the LLMs can leverage its existing knowledge and generalize from a few demonstration examples to solve new, related tasks (Dong et al., 2022; Liu et al., 2021; Rubin et al., 2021; Gonen et al., 2022). Some notable works include chain-of-thought (CoT) prompting (Wei et al., 2022b), which elicits reasoning with intermediate steps in few-shot exemplars. Built upon the CoT framework, several works expand CoT by organizing and processing thoughts using more complex structures, such as trees (Yao et al., 2023) and graphs (Besta et al., 2023) or breaking a problem into sub problems and then proceeds to solve each one independently (Zhou et al., 2022). While these studies have effectively improved the reasoning capability of LLMs, they have failed to clearly distinguish between inductive and deductive reasoning, let alone investigate which represents a more critical limitation for LLM reasoning capabilities: deductive reasoning or inductive reasoning.\nExploring LLMs' Reasoning Skills\nDespite the impressive achievements of LLMs in various reasoning tasks, the underlying mechanisms of their reasoning capabilities remain a subject of debate. The question of whether LLMs genuinely reason in a manner akin to human cognitive processes or merely simulate aspects of reasoning without true comprehension is still open (Huang and Chang, 2022). For instance, Kojima et al. have suggested that LLMs exhibit commendable zero-shot reasoning abilities, implying that these models can draw logical conclusions in scenarios they have not been explicitly trained on (Kojima et al., 2022). However, some researchers cast doubt on the reasoning capability of LLMs. While approaches like the chain-of-thought method may mimic human-like thought processes, it remains uncertain whether LLMs are genuinely engaging in"}, {"title": "Equipping LLMs with External Tools", "content": "Large Language Models (LLMs) have made significant progress in utilizing tools through frameworks like CREATOR (Qian et al., 2023) and LATM (Cai et al., 2023), which allow LLMs to create tools using documentation and code. Logic-LM (Pan et al., 2023) integrates LLMs with symbolic solvers to improve logical problem-solving, However, these approaches focus exclusively on deductive reasoning, aiming to enable LLMs to derive correct answers for specific questions without incorporating the capacity for inductive reasoning to infer underlying mapping function shared by few-shot examples. In contrast, our primary objective is not to propose a new framework for using tools to enhance the problem-solving capabilities of LLMs. Instead, we aim to differentiate between deductive and inductive reasoning within LLMs and explore which presents a greater challenge to their reasoning abilities."}, {"title": "Conclusion", "content": "This study aims to explore a less-investigated aspect of LLMs: within LLM reasoning, which presents a greater challenge \u2014 deductive or inductive reasoning? To delve into the inductive reasoning capacities of LLMs, we introduce a novel framework called SolverLearner. By concentrating on inductive reasoning while setting aside LLM-based deductive reasoning, SolverLearner can scrutinize the pure form of inductive reasoning in LLMs. Our findings unveil remarkable inductive reasoning prowers in LLMs through SolverLearner, achieving near-perfect performance with an ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs often exhibit weaker deductive capabilities, particularly in tasks involving \"counterfactual\" scenarios."}, {"title": "Limitations", "content": "LLMs cannot perform inductive reasoning over all the tasks In our inductive learning setting, LLMs are provided with only a limited number of contextual examples. The goal is to infer the function that accurately maps inputs to outputs based solely on this constrained dataset. In order to solve this problem, it is significant that we can find a unique function satisfied given these examples. For instance, a linear function can be precisely determined given just two data points, as it has a singular solution. However, attempting to deduce a quadratic curve from two points poses an insurmountable challenge due to the existence of infinite functions capable of passing through those specific points. Additionally, LLMs might struggle to discern the correct mapping function when the search space of the problem expands excessively. Consider the case of arithmetic tasks; without limiting the search space to finding a suitable base that aligns with the observations, the task becomes overwhelmingly complex. This is because the search space could encompass any conceivable rule that accommodates the observations.\nThe effectiveness of LLMs' inductive reasoning capability is heavily reliant on the foundational model While GPT-4 consistently showcase impressive inductive reasoning abilities through SolverLearner and achieve perfect performance with ACC of 1 across all the tasks, GPT-3.5 struggle to learn the correct input-output mapping function in several cases. This observation suggests that the inductive reasoning potential of LLMs is significantly constrained by the underlying model.\nChain of Thought (COT) has not been incorporated into the comparison Chain of Thought (COT) is a significant prompting technique designed for use with LLMs. Rather than providing a direct answer, COT elicits reasoning with intermediate steps in few-shot exemplars. This method was not incorporated into our comparison as it is viewed as a technique to improve the deductive reasoning capabilities of LLMs. Although COT has proven to be effective across various tasks, numerous studies highlight a significant performance gap that COT still needs to bridge to achieve flawless execution."}, {"title": "Ethical Considerations", "content": "The authors foresee no ethical concerns with the research presented in this paper."}, {"title": "Appendix", "content": "Full Setups\nSolverLearner is a prompting based reasoning approach, and we only need to perform inference with LLMs.\nSettings for Each Task\nArithmetic The arithmetic dataset introduced in Wu et al.'s paper (Wu et al., 2023) comprises 1,000 randomly selected addition expressions, each involving two-digit numbers. These expressions are drawn from bases 8, 9, 10, 11, and 16, with separate sampling for each base. Importantly, all the expressions have been carefully chosen to yield distinct results when evaluated in their respective bases, thereby distinguishing them from one another during the process of rule learning.\nBasic Syntactic Reasoning In accordance with the methodology outlined in Wu et al.'s work (Wu et al., 2023), we have generated a set of 100 simple three-word sentences (e.g., \"bob likes bananas\") with five different word order variations (e.g., \"bananas bob likes\u201d in OSV format). Subsequently, we tasked LLMs with learning how to manipulate sentence order. It's noteworthy that we took great care in selecting words to ensure that each word in a sentence can only fulfill one specific role, such as subject, object, or verb. For instance, we ensured that sentences like \"bob likes anna\u201d were excluded, as both \"bob\" and \"anna\u201d could potentially serve as both subjects and objects, violating this constraint.\nSpatial Reasoning The spatial reasoning dataset introduced in Wu et al.'s paper (Wu et al., 2023) consists of 100 rooms that were randomly selected, and each room contains three distinct objects. The spatial directions within these rooms are represented using unit vectors. For instance, north is represented as (0, 1), south as (0, -1), east as (1, 0), and west as (-1, 0), with a y-axis pointing upward serving as the default orientation. In our study, we have modified the mapping between directions and unit vectors and tasked LLMs with learning this new direction-to-unit vector relationship. We explore two direction-swapped scenarios (north-south and east-west), three rotated scenarios (by 90\u00b0, 180\u00b0, and 270\u00b0), and a randomly permuted scenario. The primary metric we report is instance-level accuracy, which necessitates that all three objects within a room must be correctly positioned in order to be considered accurate.\nCipher Decryption We've generated a collection of 100 pairs of strings (e.g., \u201cMrxuqhb -> Journey", "Mrxuqhb\") and its corresponding decrypted version (e.g., \"Journey\"). By providing LLMs with several examples, each containing an encrypted string alongside its corresponding decrypted counterpart, the primary task is to accurately determine the cipher system employed in an open-world context.\"\n    },\n    {\n      \"title\"": "Few shot Example Generation"}, {"content": "The preparation of examples for few-shot learning follows a straightforward process. We divide all the data into a training set and a test set, from which few-shot examples are extracted from the training set. These few-shot examples are automatically prepared by associating queries with their corresponding ground truth answers using a pre-defined template."}, {"title": "Test Case Generation", "content": "In the function execution phase, the test cases are generated using a template without involving LLM. In particular, the test cases are drawn from the test data files, containing all the queries along with their correct answers (e.g., \"76+76 = 174\u201d). When the LLM is used for generating code, we specify a function interface, such as def solver(n1: str, n2: str) -> str. Then, using the query examples provided, like \u201c76+76 = 174\u201d, we create test cases by applying this function interface to the query (e.g., solver(76,76)), thereby eliminating any reliance on LLM for this process. This method ensures that our test case generation is 100% correct."}, {"title": "Full Prompts", "content": "We provide the prompts that we used to query the LLMs for all tasks in Tables 1 to 4. We do not use the system message field for any model."}, {"title": "Full Results", "content": "We show the full numerical results in Tables 5 to 8. In addition to using 8-shot examples, these results also include experiments with 16-shot examples to assess how changes in the number of in-context examples impact the results."}, {"title": "More Results on Additional LLMs", "content": "To validate the generalizability of our conclusion, we have included additional LLMs, claude-3-sonnet-20240229-v1:0, which is denoted as"}, {"title": "Ablation studies", "content": "LLMs struggle as executors when applying learned functions. To better demonstrate the deductive capacity of LLM, we present both GPT-3.5 and Python with identical code and task them with applying the code to deduce the same set of queries. As shown in Table 13, while the Python interpreter can be considered an oracle, delivering flawless performance, it proves challenging for LLMs to accurately execute the code.\nLLMs can learn the function with very few examples when the inductive reasoning problem is well defined. To examine the impact of the number of few-shot examples on the inductive reasoning capability of LLMs, we vary the number of in-context examples within [1,2,4,8,16] and assess performance on the spatial reasoning task using GPT-3.5 as presented in Table 14. We observe that even with very few examples, GPT-3.5 can still learn the mapping function if it is learnable."}]}