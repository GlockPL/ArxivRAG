{"title": "EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation", "authors": ["Lituan Wang", "Lei Zhang", "Yan Wang", "Zhenbin Wang", "Zhenwei Zhang", "Zhang Yi"], "abstract": "Weakly-supervised medical image segmentation is gaining traction as it requires only rough annotations rather than accurate pixel-to-pixel labels, thereby reducing the workload for specialists. Although some progress has been made, there is still a considerable performance gap between the label-efficient methods and fully-supervised one, which can be attributed to the uncertainty nature of these weak labels. To address this issue, we propose a novel weak annotation method coupled with its learning framework EAUWSeg to eliminate the annotation uncertainty. Specifically, we first propose the Bounded Polygon Annotation (BPAnno) by simply labeling two polygons for a lesion. Then, the tailored learning mechanism that explicitly treat bounded polygons as two separated annotations is proposed to learn invariant feature by providing adversarial supervision signal for model training. Subsequently, a confidence-auxiliary consistency learner incorporates with a classification-guided confidence generator is designed to provide reliable supervision signal for pixels in uncertain region by leveraging the feature presentation consistency across pixels within the same category as well as class-specific information encapsulated in bounded polygons annotation. Experimental results demonstrate that EAUWSeg outperforms existing weakly-supervised segmentation methods. Furthermore, compared to fully-supervised counterparts, the proposed method not only delivers superior performance but also costs much less annotation workload. This underscores the superiority and effectiveness of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL image segmentation plays a crucial role in biomedical image analysis [1], such as diagnosis, treatment, and radiotherapy planning. As manual segmentation is usually labor-intensive, time-consuming and rely on professional domain knowledge [2], automatic medical image segmentation has been widely dedicated and series methods have been proposed. However, the successes of existing methods rely mainly on large-scale meticulously annotated data, which requires significant domain expertise as well as expensive annotation cost.\nTo alleviate the burdens associated with image annotation, weakly-supervised medical image segmentation is gaining traction as it requires only weak or sparse annotations [3], such as image-level labels [4], scribbles [5], bounding boxes [6], and point annotations [7]. Although some progress has been made by using label-efficient annotations for training, there is still a considerable performance gap between the label-efficient methods and fully-supervised ones [8]. We revisit existing label-efficient medical image segmentation methods and observe that these weak labels introduce considerable uncertainty for segmentation model constructing. Fig. 1 provides the visual representation of the supervision signals introduced by different label-efficient annotations, in which most information (defined by the gray region) are uncertain. The uncertainty supervision signals provided by label-efficient annotations may induce model training oscillations, thus impair the training of the model to approach the performance achieved in a fully supervised manner [9]. Consequently, there is an urgent need to explore label-efficient methods that can reduce annotation uncertainty, and develop methods that can assistant to eliminate the label uncertainty during model training.\nIn this work, we propose a novel weak annotation method coupled with its learning framework to eliminate the annotation uncertainty, and facilitate stable training in the weakly-supervised medical image segmentation with more reliable supervision signal. To this end, we introduce the bounded polygons annotation, which simply requires labeling two polygons that are similar to the inscribed and outer envelope-like delineations of lesion (as shown in Fig. 1). The proposed bounded polygons annotation has three advantages: (1) it reduces the label burden compared with pixel-to-pixel accurate labels, (2) it restricts the uncertainty information to gray region between two polygons, (3) it explicitly provides prior emphasis on lesion boundaries during model training. Tailored for the proposed weak annotation, we propose a EAUWSeg"}, {"title": "II. RELATED WORKS", "content": "Without the requirement of large densely annotated data, weakly-supervised learning has gained significant attention in medical image segmentation [13], [2]. As the most efficient weak annotation method, image-level annotations only require classification labels and generates class activation maps [14] for training. Although image-level annotations method is convenient, it has limited performance due to the extremely weak supervision [15]. Box-level annotation is usually defined by two corner coordinates, which provides localization-awareness compared to the image-level annotation [16]. However, boxes for different objects may tend to overlap with each other, making it difficult to accurately approximate the target boundary, especially for complex shapes [10]. Point annotations provide a small number of pixels for different classes and can better handle complex shapes, which may be more preferable to medical segmentation compared to box-level annotations. Despite its efficiency, the segmentation model trained with point annotations tends to overfit the small number of annotated pixels when comparing with the large number of unannotated pixels.\nScribble-based annotations provide labels for a sparse set of pixels of each class for training, and are usually more obtainable in medical image segmentation by considering its annotation efficiency, performance effectiveness as well as the friendliness to the annotation of nested structure [17]. Only the scribbles of the background and each object are given, while the groundtruth of other pixels remains unknown, which is harmful to the segmentation performance. An intuitive resolution is to expand the scribble annotations by considering the prior assumptions [18] or using the learned foreground features through the deep neural networks [19]. However, due to the lack of supervisory signals, the constructed models usually fail to capture the object structure and confuse on the object boundary. To address this issue, a series of studies have concentrated on learning adversarial shape priors at the expense of requiring additional fully-annotated masks [17]. However, acquiring such fully annotated datasets may present challenges in many clinical practices, rendering these existing methods both costly and lacking in scalability. Our work aims to explore new weak annotation method that can prompt the performance of automated medical image segmentation without auxiliary datasets."}, {"title": "B. Contrastive Learning", "content": "Contrastive learning argues that similar samples should have similar representations, and the representations of different samples should be different [20]. Based on this, contrastive loss is usually designed to enforce representations to be similar for similar pairs and dissimilar for dissimilar pairs [21]. Considering its powerful self-supervised feature extracting ability from the unlabeled data, contrastive learning has been widely used in many image-level tasks. Among all these methods, the key is the selection mechanism designing of contrastive sample pairs, i.e., positive and negative pairs.\nRecently, contrastive learning has been extended from image-level task to pixel-level ones to mine informative information from unlabeled data [22], [23]. As mentioned earlier, constructing contrastive sample pairs is crucial for discriminative feature learning. In the context of pixel-level tasks, sample pairs are usually constructed through pseudo labels or spatial structure, which may introduce noisy sampling. To alleviate this problem, prediction uncertainty has been injected into the sampling to reduce the number of noisy samples [24]."}, {"title": "III. METHOD", "content": "In this work, we propose a novel bounded polygon annotation method, i.e., BPAnno, and its corresponding segmentation framework, i.e., EAUWSeg, to eliminate annotation uncertainty in weakly-supervised medical image segmentation. Our EAUWSeg is in general applicable for many existing medical image segmentation models, such as UNet [25], DeepLabV3+ [26], TransUNet [27], with encoder and decoder phases. The overall framework is illustrated in Fig. 2."}, {"title": "A. Problem Setting and Bounded Polygons Annotation", "content": "In the scenario of classical weakly-supervised segmentation, the input pixels x are usually divided into the labeled pixels X\u03b9 and unlabeled pixels Xul. In this way, the corresponding labels y for the labeled pixels x\u03b9 will be directly used for supervision by employing the partial cross-entropy loss, which can be formulated as follows:\n$Li(p, y) = -\\sum y\\log(p)$, (1)\nwhere p is the segmentation prediction. For the unlabeled pixels, there is no off-the-shelf label for supervision, and a lot of work focus on assigning pseudo labels to unlabeled pixels for supervision [28], [29]. The overall objective function can be formulated as follow:\n$L = Li + Lul$. (2)\nHowever, assigning pseudo labels to unlabeled pixels not only requires a time-consuming multi-stage training process, but also results in misleading or biases [10].\nTo address this problem, this work introduces the bounded polygon annotation method that simply requires labeling two polygons that are similar to the inscribed and outer envelope-like delineations of lesion (as shown in Fig. 1). To further eliminate the uncertainty included in the bounded polygon annotation, we explicitly treat bounded polygons as two"}, {"title": "B. Framework of EAUWSeg", "content": "EAUWSeg is tailored for the proposed bounded polygon annotation, and mainly focuses on eliminating annotation uncertainty for pixels belong to \u03a9\u0394. As illustrated in Fig. 2, EAUWSeg consists of 1) a mainstream segmentation network supervised by two bounded polygons segmentation labels to implicitly define the certain region and uncertain region during network training, 2) a classification-guided confidence generator to provide the category-level prediction confidence for pixels xi \u2208 \u03a9\u0394 by leveraging a tailored multi-class classification task, 3) a confidence-auxiliary consistency learner to distinguish reliable pixels in uncertain region can assign the corresponding \"certain\" labels.\nLet Se, Sd, and Sh denote the encoder, the decoder, and segmentation head used in our proposed framework that are parameterized by \u0472e, Od and Oh, respectively. In the proposed EAUWSeg, the bounded polygon annotation is treated as two separate masks, i.e., inscribed-like and envelope-like masks, and the basic segmentation loss function in EAUWSeg can be formulated as:\n$Lc = \\sum (Lin(p, yin) + Len(p, yen))$, (4)\nX\nwhere p is the predicted probability maps for input image x. In this work, the following dice loss is employed for both Lin and Len:\n$L_{dice} = \\frac{\\sum_{i=1}^{H \\times W \\times D} p_i y_i}{\\sum_{i=1}^{H \\times W \\times D} (p_i^2 + y_i^2)}$, (5)"}, {"title": "C. Classification-Guided Confidence Generator", "content": "The key point for accurate BPAnno-supervised segmentation is reliable labels assigning for pixels in uncertain region. Different from existing methods that focus on iteratively assigning pseudo label for uncertain pixels, we propose to utilize the intra-class similarity and inter-class discriminative from both the feature and category perspective.\nAn intuitive idea to approximate the confidence for uncertain pixels xi is the predictive entropy that is calculated according to the following equation:\n$E = -\\sum P(Y_{ik} | X, \\Theta_S) \\log(P(Y_{ik}|x, \\Theta_S) + \\epsilon)$ (7)\nk\nwhere s = {\u0398\u03b5, \u0398\u03b1, \u0398\u03b7} are the parameters of standard segmentation network, e is a constant to avoid overflow. Similar as previous works, prediction with large entropy is considered as the solid uncertain pixels, which will be dropped during the subsequent learning. For clarity, we define the solid uncertain pixels in uncertain region with category of -1:\n$U^u = \\begin{cases}\n-1,\n\\epsilon_i > u\notherwise\n\\end{cases}$ (8)\nwhere u is a predefined threshold to mask the uncertain labels, and U\u20ac \u2208 RC\u00d7H\u00d7W is the estimated uncertainty map with the same size as input image.\nTo assign more reliable labels for uncertain pixels, we propose to explicitly leverage the potential similarity between certain and uncertain pixels by employing a tailored classification task, which aims at removing as much uncertainty as possible. Let fs(x) denote the feature representation generated through the encoder and decoder network, Se and Oc denote the classification head and its corresponding parameters respectively. Previous work [10] has shown that similar pixels in the feature space preferable to generate consistent category prediction. Based on this, the constructed classification head is used to model a multi-class classification task with the objective function of:\n$L_{ce} = - \\sum_{i=1}^{N} \\sum_{y_i=1}^3 y_i \\log P (y_i | x, \\Theta_E, \\Theta_D, \\Theta_e)$, (9)"}, {"title": "D. Confidence-Auxiliary Consistency Learner", "content": "Confidence-auxiliary consistency learner aims at generating \"certain\" information from uncertain region to facilitate stable training. An intuitive idea is utilizing contrastive learning to reduce the distance between pixels within same category while enlarging the distance between pixels in different categories. This strategy allows us to conduct the pixel-wise contrastive learning. However, the crucial question is the selection of positive and negative samples, especially for pixels in uncertain region. To reduce the influence of uncertain information, we propose to utilize the generated confidence for the uncertain pixels and only the solid certain pixels will be considered during the pixel-wise contrastive learning. In this way, the determined pseudo labels can be obtained as follows:\n$\\hat{y}= y (1 - M_u) + U$. (12)\nTo provide more reliable supervision signal by using the pixel-wise contrastive learning, we follow two guidelines during sample selection: 1) only feature embedding of pixels in the certain region are stored in this study and further be sampled during the computation of contrastive loss; 2) the anchor sampling in this study focuses on hard samples with error prediction for xi \u2208 \u03a9\u03b9 U\u03a9O, and samples with higher certainty for xi \u2208 \u03a9\u0394. The pixel-wise contrastive loss in this work can be defined as:\n$L_{PCL} = - \\frac{1}{\\sum_{i \\in P}} \\sum_{p \\in P} \\log \\frac{exp(\\frac{f_i f_p}{\\tau})}{exp(\\frac{f_i f_p}{\\tau}) + \\sum_{n \\in N} exp(\\frac{f_i f_n}{\\tau})}$, (13)\nwhere P contains the indexes of all \"certain\" pixels in the uncertain region; P and N contains the indexes of positive pixels, i.e., pixels has same class with pixel i, and negative pixels, i.e., pixels with different labels to pixel i, in the certain region, respectively; t is a temperature hyper-parameter."}, {"title": "E. Training of EAUWSeg", "content": "To summarize, the overall objective function includes two parts: 1) losses for \"certain\" pixels using fully-supervised segmentation setting, 2) confidence-guided contrastive loss for uncertain pixels. At the early stage of training, segmentation model need to learn the feature representation of lesions with the guidance of supervised loss for \u201ccertain\" pixels, i.e., Lc. When the segmentation performance gradually improves, the contrastive loss LPCL combined with a multi-class classification cross-entropy loss Lce are added to apply constraints on uncertain pixels in same class to preserve consistency feature representation. Therefore, the overall objective function in this work is formulated as:\n$L = L_c + \\lambda_1L_{PCL} + \\lambda_2L_{ce}$, (15)\nwhere 1 and 2 are the parameters to control the contribution of confidence-auxiliary consistency learner and classification-guided confidence generator, respectively."}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets: To evaluate the effectiveness of the proposed method, we conduct the comparative experiments on two widely used medical image segmentation datasets, i.e., ISIC2017[11] and Kvasir-SEG [12] datasets. ISIC2017 is a skin lesion segmentation dataset, on which rich results have been reported in literature for comparisons. It contains 2000, 150, and 600 dermoscopic images in train, valid, and test sets respectively. We follow the official split of train and test set during the experiment. Kvasir-SEG contains 1000 gastrointestinal polyp images and the corresponding groundtruth. we randomly split the dataset into two subsets with 800 and 200 images, respectively. Furthermore, to evaluate the generalization ability of the constructed model, we conduct the cross-training evaluation and apply the model trained on ISIC2017 to test on ISIC2018 dataset for skin lesion segmentation without fine-tuning. ISIC2018 Dataset [39] is a expansion of ISIC2017, and it contains 2594, 100 and 1000 images in train, valid, and test sets respectively. It should be noticed that there is no intersection between the test set of ISIC2018 and the train set of ISIC2017.\n2) Annotation Generation: For the bounded polygon annotations, we initially generate approximate bounded polygon through dilation-erosion operations by leveraging available groundtruth masks. Subsequently, a manual refinement process is employed to enhance the accuracy of bounded polygon annotation. In the automatic generation phase, we create"}, {"title": "B. Comparison With State-of-the-Arts", "content": "To demonstrate the comprehensive segmentation performance of our method, we compare EAUWSeg with different state-of-the-art approaches:\n\u2022 Scribble-supervised methods: 1) different learning strategies on UNet, including partially Cross-Entropy loss [33], Total Variation loss [34], Gated Conditional Random Field loss [35], Mumford-Shah Loss [36], as well as Uncertainty-aware Self-ensembling and Transformation-consistent Mean Teacher techniques (USTM) [37]. 2) different scribble-supervised frameworks, including ScribbleVC [8], DMSPS [1], and TriMix [38].\n\u2022 Box-supervised methods. For fair comparison, we also present the results of classical segmentation networks, i.e., UNet and TransUNet, supervised with bounding box.\n\u2022 Fully-supervised segmentation methods: 1) CNN-based methods, including UNet [25], UNet++ [30], and DeepLabV3+ [26]. 2) Transformer-based methods, including TransUNet [27], TransFuseS [31], HiFormer [32].\nImplementation of these networks follow the corresponding github repositories. During training, ResNet50 [42] is employed as the encoder for UNet and DeepLabV3+, ResNet34 is utilized in the UNet++ and TranFuses, the default \"R50-ViT-B_16\u201d and \u201cHiformer-S\u201d configurations are employed for TransUNet and HiFormer."}, {"title": "C. Ablation Study", "content": "1) Effectiveness of the Bounded Annotations: To analysis the effectiveness of the proposed bounded annotation strategy, we conduct quantitative evaluation of training the TransUNet directly using different bounded annotation methods, including polygon, rectangle, and ellipse. Considering, box is similar with rectangle, only rectangle is compared since it can achieves better performance.\n2) Comparative Analysis of Different Components: To demonstrate the effectiveness of the proposed component, i.e., confidence-auxiliary consistency learner (CCL) and classification-guided confidence generator (CCG), we carried out the ablation experiments"}, {"title": "4) Generalizabilty Analysis With Different Backbones:", "content": "The proposed EAUWSeg is a plug-and-play model that can be easily combined with different backbones. To demonstrate its generalization ability six widely used segmentation networks are compared, i.e., UNet [25], UNet++ [30], DeepLabV3+ [26], TransUNet [27], TransFuseS [31], and HiFormer [32]. From Fig. 4, it can be seen that: 1) the best result is achieved when using TransUNet as the backbone, 2) the proposed method delivers superior performance compared to fully-supervised counterparts as shown in Table I. These results reveal that the proposed EAUWSeg generalizes well for different backbones."}, {"title": "D. Error Analysis", "content": "The proposed bounded polygon annotation has the advantage of explicitly providing prior emphasis on lesion"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this work, to eliminate the annotation uncertainty existed in weakly-supervised medical image segmentation, we propose the bounded polygon annotation, in which label only two polygons while providing promising prior of lesion boundary during training. To further eliminate the uncertainty included in the bounded polygon as well as to leverage the prior emphasis delineated by bounded polygons, we develop EAUWSeg, a learning framework tailored for bounded polygon that include a confidence-auxiliary consistency incorporated with a classification-guided confidence generator is designed to provide reliable supervision signal for pixels in uncertain region. Extensive experimental results demonstrate that EAUWSeg can not only outperform existing weakly-supervised segmentation methods but also delivers superior performance compared to fully-supervised counterparts, with less than 20% of the annotation workload.\nThis work is a preliminary attempt to focus on eliminating annotation uncertainty in weakly-supervised medical image segmentation. Extensive experimental results have demonstrated its cost-efficient and effectiveness of the bounded annotation, while there is still several limitations. This study mainly focuses on the weakly-supervised medical image segmentation in binary case. When applied to the instance segmentation, it may suffer from some challenges, such as encompassing pixels belong to foreground with different categories in the envelope-like polygon. In future work, we will focus on solving this kind of problems."}]}