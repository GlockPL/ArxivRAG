{"title": "FAIR FOUNDATION MODELS FOR MEDICAL IMAGE ANALYSIS: CHALLENGES AND PERSPECTIVES", "authors": ["Dilermando Queiroz", "Anderson Carlos", "Andr\u00e9 Anjos", "Lilian Berton"], "abstract": "Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles. Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data. These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups. Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development. While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols. This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare. The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources.", "sections": [{"title": "1 INTRODUCTION", "content": "AI in medicine offers transformative potential through improving access to diagnostics and enhancing the speed and quality of medical care 1,2. AI tools extend healthcare services, particularly in resource-limited regions, thereby making care more efficient and accessible\u00b3. However, these advancements raise critical ethical concerns that emphasize the need for a fair and equitable distribution of benefits across all populations 4,5,6. We must develop and apply these technologies responsibly to uphold bioethical principles of justice, autonomy, beneficence, and non-maleficence to prevent discrimination and promote inclusive healthcare for all7.\nGovernments worldwide are establishing regulatory frameworks for artificial intelligence across critical sectors. The EU AI Act, to, the first comprehensive regulation of its kind, has introduced a risk-based classification system for AI applications. In parallel, the U.S. Office for Civil Rights has enacted specific protections against algorithmic bias in healthcare through the Affordable Care Act. These regulatory initiatives reflect a coordinated global effort to implement evidence-based guidelines that ensure the fairness, safety, and equity of AI systems 10,11,12,13,14,15,16,17.\nFMs serve as essential components in AI by enabling diverse tasks across text, image, video, audio, and other domains through their versatility and scalability 18. Through training on massive datasets, these models capture broad patterns within and across domains. However, significant challenges persist in developing ethical models that can effectively identify and reduce inherent biases 19,20,21. While bias persists across AI systems, FMs demonstrate significant potential for their mitigation 22,23, thus creating opportunities for unified models that equitably serve diverse populations while driving greater inclusion in AI.\nThe production of FMs requires substantial resources, including specialized labor, large datasets, and significant computational infrastructure. These high costs restrict development capabilities to select countries, institutions, and companies, thereby increas-ing the risk of global inequalities. Stakeholders in regions such as Africa have indicated their inability to develop such models, which potentially widens the disparity between populations that benefit from AI and those that do not24. Addressing this chal-lenge requires developing strategies that facilitate the creation of globally accessible FMs while enabling broad participation in their production.\nBias mitigation must occur throughout the proposed development process of FMs, as illustrated in Figure 1. This process requires thorough data documentation during the curation and creation phases to ensure dataset diversity and representative-ness. The consideration of environmental impacts during train-ing, model evaluation, and deployment phases enhances fairness. Policymakers fulfill a crucial role through the enactment of laws and the allocation of resources that support equitable AI prac-tices. Through the integration of bias mitigation strategies at each developmental stage, researchers can develop inclusive and responsible AI models that serve diverse populations effectively.\nA substantial body of research has empirically demonstrated inherent biases and established initial frameworks for evaluat-ing fairness in these systems 25,20,19,21. While comprehensive"}, {"title": "2 BACKGROUND AND TAXONOMY", "content": "This section delineates the fundamental principles underlying fairness and foundation models."}, {"title": "2.1 Fairness", "content": "Principles of Trustworthy AI. The development of medical im-age analysis AI systems, to achieve trustworthiness, is guided by six fundamental principles: fairness, universality, traceability, us-ability, robustness, and explainability 13. Among these, fairness, which ensures non-discriminatory outcomes across diverse pa-tient populations, represents a critical determinant of ethical AI implementation. Our analysis centers on fairness and examines its interdependencies with other principles, specifically how it interacts with robustness to ensure reliable system performance and with traceability to maintain systematic accountability, thus demonstrating how these principles collectively contribute to trustworthy AI in clinical settings.\nHealthcare Disparities. Differences in national healthcare de-livery capacities result in variable health outcomes across pop-ulations 32,33. Within countries that maintain public healthcare systems, disparities persist because of multiple factors, including race, gender, age, ethnicity, body mass index, education, insur-ance status, and geographic location 34,35. These determinants influence an individual's capacity to access treatment, obtain quality care, and achieve favorable health outcomes.\nAI Bias Manifestation. Al models frequently assimilate and reproduce biases inherent in their training data, subsequently reflecting and intensifying societal inequalities. These models often depend on spurious correlations, employing computational shortcuts that amplify existing biases 36,37,38. A well-designed fair algorithm produces impartial decisions by ensuring equi-table outcomes across demographic groups without discrimina-tion based on sensitive attributes such as race, gender, or age. Despite the implementation of controlled datasets and balanced groups, bias can manifest through various factors, including image complexity and labeling inconsistencies 39. Consequently, unintended biases may emerge even under optimal conditions, highlighting the necessity for continuous evaluation and mitiga-tion procedures to maintain algorithmic fairness.\nBias Mitigation Strategies. Previous research has established three primary classifications for bias mitigation strategies: pre-processing, in-processing, and post-processing 29,28,30,31. Pre-processing approaches modify datasets through demographic representation balancing, sensitive feature removal (such as race and gender), or synthetic data augmentation to enhance diver-sity 40,41,42. In-processing methods alter the training process by integrating fairness constraints into the loss function or imple-menting adversarial training to prevent bias acquisition 43,44,45. Post-processing techniques adjust model predictions after train-ing through methods such as prediction calibration to satisfy fairness criteria 46,47,48\nFramework Integration. Our research extends traditional bias mitigation strategies (pre-processing, in-processing, and post-processing) into a comprehensive framework encompassing all stages of FMs development. Through the integration of bias mitigation efforts across all phases and the inclusion of policy-makers in technical discussions, as illustrated in Figure 1, this framework systematically addresses biases throughout the devel-opment process. The approach responds to significant concerns"}, {"title": "2.2 Foundation Models", "content": "Technical Foundations. FMs are trained on extensive datasets that can be efficiently adapted to multiple downstream tasks through fine-tuning, thereby eliminating the necessity of train-ing specialized models from scratch. The Vision Transformer (ViT)49 serves as a leading FMs architecture in computer vi-sion for extracting fundamental image features. The integra-tion of self-supervised learning techniques, such as Masked Autoencoder (MAE)50 and contrastive learning methods like SimCLR51, enables these models to learn directly from large volumes of unlabeled data, thus largely mitigating the need for costly manual labeling processes. The Image-based Joint-Embedding Predictive Architecture (I-JEPA) introduces a self-supervised learning approach that predicts semantic embed-dings directly, offering enhanced efficiency compared to pixel-reconstruction methods 52.\nCapabilities and Applications. Through the utilization of massive unlabeled datasets, advanced architectures, and self-supervised learning techniques, FMs acquire comprehensive image representations within their training domains. The fine-tuning process facilitates task-specific adaptation through mini-mal adjustments and limited labeled data requirements. Several key capabilities establish their essential role in scalable, real-world applications: rapid task adaptation, robustness to distribu-tion shifts, and efficient utilization of labeled data 53,54,55,56.\nMultimodal Integration. These models facilitate training across diverse data types within a unified architectural frame-work. Vision-Language Models (VLMs)57 demonstrate this ca-pability by integrating visual and linguistic processing within a single computational structure. This architectural integration en-ables simultaneous interpretation and generation of information across multiple modalities, thereby establishing new possibilities for applications that require image and textual data.\nDomain Specialization. Domain-specific FMs constitute spe-cialized architectures tailored to address the distinctive char-acteristics of particular domains, ranging from natural image processing 58 to medical imaging applications 55,54. In contrast to general-purpose models, these specialized frameworks con-centrate on the unique data structures and analytical challenges inherent to their respective domains. Medical imaging presents a notable example, where image characteristics vary substantially in resolution, ranging from whole-organ visualization to cellular-level structures, with data originating from diverse acquisition modalities including X-ray radiography, computed tomography (CT), magnetic resonance imaging (MRI), and ultrasonography.\nMedical FMs Categories. The inherent diversity of medical imaging modalities presents significant challenges in develop-ing a comprehensive global model. To address this complex-ity, researchers have established a systematic categorization of medical image FMs into distinct segments: general Medical Image FMs for broad applications, Modality-specific FMs that address particular imaging techniques, Organ-specific FMs that"}, {"title": "3 DATA DOCUMENTATION", "content": "This section delineates the essential components of data doc-umentation, encompassing both the data creation phase and subsequent curation processes."}, {"title": "3.1 Data Creation", "content": "The first step in FMs development involves comprehensive data collection. Healthcare facilities generate medical images through specialized imaging equipment that captures various anatomical structures. The pre-training phase utilizes exten-sive unlabeled datasets, enabling FMs to acquire general repre-sentations and discover underlying patterns without annotation requirements. For subsequent fine-tuning processes, labeled data plays an essential role in enabling models to develop task-specific capabilities through the refinement of learned repre-sentations. Both unlabeled and labeled datasets must maintain high quality and diversity standards, as these characteristics fundamentally influence FMs performance and generalization capabilities 83,58.\nCriteria for Inclusion. Our investigation of large-scale datasets employed in FMs followed a systematic approach, establishing a minimum threshold of 10,000 images for inclusion (Table 1). The geographic distribution of dataset origins appears in Figure 2. While not exhaustive, this methodological approach enabled the systematic curation of 74,184,415 medical images, represent-ing a comprehensive cross-section of diverse imaging modalities and anatomical regions. The comprehensive review encom-passed unimodal imaging and multimodal image-text paired datasets, spanning diverse imaging modalities and anatomical regions. This methodological documentation framework facil-itates a systematic assessment of the current medical imaging data ecosystem while elucidating opportunities for enhanced demographic representation in FMs development.\nPre-training. The utilization of large unlabeled datasets for FMs training significantly reduces dependence on costly and time-intensive labeled data collection. Advanced learning method-ologies, particularly self-supervised learning approaches 51,50,52, facilitate effective utilization of unlabeled data while minimizing labeling bias. This methodology proves especially valuable in medical contexts, where individual clinician annotation prefer-ences, influenced by patient attributes, may introduce system-atic biases 84. Using unlabeled data enables FMs to identify generalizable patterns while simultaneously reducing both cost constraints and annotation-induced biases.\nThe development of robust datasets for FMs presents signifi-cant challenges in ensuring comprehensive representation across populations, imaging modalities, equipment specifications, and disease classifications. The accumulation of large-scale data frequently amplifies existing biases and imbalances that reflect underlying disparities in healthcare access and infrastructure. As illustrated in Figure 2, the predominant source of available datasets resides in developed nations, which substantially con-strains the diversity of documented diseases and patient demo-graphics. This geographic concentration of data resources is"}, {"title": "3.2 Data Curation", "content": "Although increasing data volume in parallel with neural network size can enhance model performance 92,93, the relationship be-tween data quantity and model improvement is not consistently linear. Recent research demonstrates that systematic data cu-ration plays a critical role in both natural language processing and computer vision tasks 83,58. Ensuring dataset fairness re-quires careful attention to multiple dimensions: diversity, global representativeness, equipment variability, disease representa-tion, gender balance, and age distribution. Furthermore, data deduplication serves as an essential process for eliminating near-duplicate images, thereby reducing redundancy and enhancing dataset diversity and representativeness 58,94.\nSensitive attributes. The acquisition of metadata and labels in medical imaging presents substantial challenges, primarily due to the resource-intensive nature of manual curation given data volume and complexity. While fairness research frequently presumes the availability of demographic data, such information often remains inaccessible due to legal, ethical, and practical constraints95. This limited access to demographic data creates significant obstacles for both data curation processes and the development of unbiased models 23. Current methodologies at-tempting to address fairness without demographic metadata face fundamental limitations, including systematic biases, accuracy limitations, technical barriers, and compromised transparency, thus emphasizing the critical need for rigorous evaluation proto-cols and explicit methodological guidelines 86.\nparticularly evident in regions such as Africa, where large-scale medical imaging datasets remain notably absent.\nFine-tuning. Smaller, more accurate, and more representative datasets are ideal for specializing FMs for specific tasks. This specialization is achieved through supervised learning on labeled datasets. Due to their data efficiency and generalization capa-bilities 53,54,55,56, FMs require less data than training supervised models from scratch. Consequently, fine-tuning FMs offers sig-nificant advantages, as it requires fewer computational resources and enables countries and institutions to adapt these models to their specific tasks with greater accessibility.\nPrecise labels and comprehensive patient information, includ-ing gender, age, and race, facilitate the evaluation of model performance and enable systematic bias identification 78,81,72,85. This demographic information proves essential for model evalu-ation, as it enables more rigorous identification and mitigation of potential biases (discussed in the evaluation section 4.2). The availability of labeled data supports dataset balancing across specific attributes, thereby enhancing model fairness and perfor-mance. This represents a significant advantage over unlabeled data, which presents greater challenges in achieving these objec-tives 86.\nGenerative Models. Generative models constitute a driving force behind recent advancements in AI and its applications. These models learn to approximate the probability distribution of output features conditioned on input features, enabling the generation of novel data instances that closely approximate the characteristics of the training data. Advanced techniques, in-cluding generative adversarial networks (GANs) 87, variational autoencoders (VAEs) 88, and diffusion models, have signifi-cantly enhanced the capabilities of generative AI systems.\nData augmentation through generative approaches facilitates both pre-training and fine-tuning stages. These models enable dataset balancing across sensitive attributes through synthetic data generation for underrepresented patient groups while simul-taneously enhancing model robustness through the generation of challenging cases 39. Empirical studies demonstrate that fine-tuning models with augmented data improve both fairness and robustness, resulting in enhanced performance across diverse populations 90. However, generative models may inadvertently amplify biases present in training data, such as generating syn-thetic medical images that systematically underrepresent darker skin tones, which necessitates robust bias mitigation strategies when utilizing synthetic data for augmentation."}, {"title": "4 ENVIROMENTAL IMPACT", "content": "This section examines the environmental considerations through-out FMs development, encompassing training requirements, model evaluation protocols, and deployment strategies."}, {"title": "4.1 Training", "content": "Fairness. Self-supervised learning models leveraging diverse, unlabeled datasets demonstrate enhanced fairness and inclu-sivity compared to supervised approaches, yielding outcomes characterized by increased robustness and reduced bias 93. How-ever, FMs retain inherent biases despite these advantages 20,22,82. The mitigation of such biases necessitates targeted interventions within the training loop. This optimization process faces dual challenges: the substantial computational requirements and the inherent difficulty of acquiring large-scale datasets that exclude sensitive attributes.\nComputational Challenges. The substantial computational re-quirements of FMs arise from two primary factors: the necessity of extensive datasets during pre-training and the complexity of architectures involving numerous parameters. These resource demands create significant entry barriers, constraining many institutions to fine-tuning existing pre-trained models rather than developing their own. Additionally, as established in the Data Creation section, this computational divide disproportionately affects certain countries, potentially exacerbating existing biases and societal inequities.\nBias Amplification in VLMs. VLMs face heightened chal-lenges in bias mitigation due to their requirement for concurrent training on both visual and textual data modalities. Contempo-rary research demonstrates that multimodal training not only maintains existing societal biases, as evidenced in CLIP (Con-trastive Language-Image Pretraining) architectures 97, but also intensifies these biases relative to single-modality systems 99."}, {"title": "4.1.1 Pre-training", "content": "Sensitive Attributes. The computational challenges inherent in vision and language models stem from power-law scaling rela-tionships, wherein incremental performance improvements ne-cessitate exponential increases in computational resources 92,93. As established in the Data Creation section, the prevalent ab-sence of metadata in large-scale datasets introduces significant complexities for training loop implementation in pre-trained FMs. These constraints fundamentally limit the application of fairness techniques during the training process. Furthermore, fairness methodologies developed for pre-trained FMs must demonstrate efficient scalability across expansive datasets and sophisticated architectural frameworks.\nData selection. The incorporation of systematic data curation into the training loop can be achieved through active selection strategies, wherein computational priority is assigned to data elements that maximize task performance contributions 101. Research demonstrates that small curated models can facilitate the training of larger models through systematic identification of both straightforward and challenging image cases 102,103. Imple-mentation of these selection methodologies yields dual benefits: reduced training duration for pre-trained models and enhanced quality of resultant outputs.\nLoss. The Reducible Holdout Loss (RHO) employs a secondary model to identify three categories of data points: those that are learnable, those worth dedicating computational resources to learn, and those not yet acquired by the model 104. One es-tablished approach for enhancing model robustness and fair-ness involves the integration of sensitive attributes into loss functions, thereby promoting equitable outcomes across demo-graphic groups 105. However, as examined in the Data Docu-mentation section, the acquisition of sensitive attributes presents significant practical challenges. To address this limitation, alter-native methodologies utilizing clustering-based data curation 96 offer potential proxy measures for sensitive attributes 23.\nLarge Concept Models. Contemporary research in robust and unbiased model development prioritizes the prediction of rep-resentations within embedding spaces over traditional token-level forecasting approaches (e.g., word or pixel prediction) 106. Within this framework, a novel class of FMs, termed Large Concept Models (LCM), enables simultaneous training across multiple languages and modalities while maintaining scalability and minimizing bias 107. In the visual domain, the Image-based Joint-Embedding Predictive Architecture (I-JEPA) implements this embedding-centric approach, yielding substantial improve-ments in robustness, scalability, and computational efficiency relative to Masked Autoencoders (MAE) 52,108"}, {"title": "4.1.2 Fine-tuning", "content": "Efficacy of Fine-tuning for Fairness Enhancement. Fine-tuning serves as a critical mechanism for aligning pre-trained FMs with targeted objectives. These models demonstrate excep-tional capability in adapting to novel data distributions with min-imal sample requirements 54. Fine-tuning effectively mitigates bias through increased model sensitivity to training distribution, particularly when employing balanced and curated datasets 100.\nResource Optimization in Fine-tuning. In contrast to the computationally intensive pre-training phase, fine-tuning pro-cedures can be executed with substantially reduced resource requirements, rendering this approach particularly advantageous for resource-constrained initiatives. The utilization of domain-specific pre-trained FMs, especially those optimized for medical imaging applications, further enhances computational efficiency in settings with limited infrastructure.\nParameter-efficient Fine-tuning. Methodologies such as LoRa 109 and QLoRa 110 enhance the efficiency of low-resource fine-tuning through selective parameter modification, targeting only specific subsets within the base model architecture. The impact of these optimization techniques on bias mitigation re-mains an active area of investigation, with current research pro-viding inconclusive evidence regarding their effects on model fairness 111,25. Notably, alternative computational optimization strategies, including model pruning and differentially private training approaches, demonstrate increased bias manifestation within specific demographic subgroups 112,113.\nMitigating Bias. Despite extensive research into bias mitigation strategies within deep learning frameworks, conventional fair-ness interventions demonstrate variable efficacy when applied to FMs 25. The limitations of these approaches extend beyond FMS applications, with traditional deep learning implementations of-ten achieving only modest improvements in fairness metrics 114. Advanced data augmentation strategies, including AutoAug, Mixup, and CutMix methodologies, demonstrate promise in addressing challenging FMs scenarios 39. Furthermore, the in-tegration of synthetically generated data during the fine-tuning process presents compelling evidence for enhanced fairness out-comes 90.\nBlackbox FMs. The increasing commercialization of FMs and proliferation of their APIs has resulted in a distribution model where access is frequently restricted to embedding outputs, limit-ing direct interaction with model architectures. This constrained accessibility presents distinct challenges for bias mitigation in medical imaging applications. Recent research demonstrates, however, that effective bias elimination techniques can be imple-mented without requiring access to internal model parameters, offering viable solutions for both open-source and proprietary model frameworks 115."}, {"title": "4.2 Model Evaluation", "content": "Fairness Considerations and Metrics. Within deep learning re-search, fairness has become a critical consideration, with partic-ular significance in medical imaging applications 28,31,5,26,22. The evaluation of fairness encompasses two fundamental method-ological approaches: individual fairness, which requires consis-tent model outputs for similar inputs, and group fairness, which assesses model performance across demographic categories de-fined by sensitive attributes such as race and gender. Despite the widespread adoption of group fairness metrics in practical applications, a significant number of influential FMs studies in medical imaging have conducted evaluations without incorpo-rating explicit fairness metrics 54,53,116,117, thus leaving critical questions about potential biases and their clinical implications unexplored.\nUtility-Fairness Trade-offs. The pursuit of bias minimization can inadvertently impact primary predictive performance, par-ticularly affecting non-protected demographic groups, thereby introducing considerations of beneficence in model development. Establishing an optimal equilibrium between utility and fairness represents a fundamental challenge in FMs implementation 118. Contemporary research introduces quantitative methodologies for evaluating these trade-offs within specific prediction tasks and group fairness frameworks 119. Empirical evidence suggests that the optimization of utility-fairness balance depends predom-inantly on model adaptation strategies rather than initial model selection; furthermore, certain adaptation approaches demon-strate the potential to concurrently enhance both utility metrics and fairness measures 25.\nRobustness. A fundamental challenge in medical imaging ap-plications lies in ensuring that fairness-aware models maintain their equitable performance when transitioning between dif-ferent data distributions (A to B), particularly given the dy-namic nature of population characteristics and deployment con-texts 120. Although adversarial training techniques enhance over-all model robustness, the improvements typically manifest un-evenly across different classes 121,122. Research indicates that fairness enhancement strategies can simultaneously strengthen model robustness 123. However, contemporary deep learning sys-tems frequently demonstrate inadequate robustness to sensitive demographic attributes, specifically sex and gender variables 114. Within FMs frameworks, the direct deployment of models with-out fine-tuning procedures further compromises robustness to these demographic attributes, thus constraining their efficacy across diverse application scenarios 23.\nData-Efficient Generalization. FMs demonstrate exceptional capabilities in data-efficient generalization, facilitating fine-tuning processes for medical imaging tasks with minimal labeled data requirements 54. This characteristic proves particularly ad-vantageous for resource-constrained institutions implementing model deployment within their specific data environments. Nev-ertheless, substantial uncertainty persists regarding the implica-tions of limited labeled data usage on fairness metrics in FMs applications. The development of ethical frameworks and bias mitigation strategies assumes critical importance in these con-texts, especially given that institutions operating under resource constraints typically depend on restricted labeled data availabil-ity.\nBenchmark. Benchmarks are essential for assessing fairness in FMs development, providing teams with systematic evaluation tools. While fairness benchmarks exist for deep learning in medical imaging 114,124,125,126, comprehensive benchmarks and libraries specifically designed for FMs remain lacking 25,19. Developing such resources is crucial for holistic fairness evaluation, encompassing metrics, utility, utility-fairness trade-offs, robust-ness, and data-efficient generalization. Furthermore, FMs should undergo testing on unbiased data and pipelines, with evaluations conducted on distributions that reflect real-world applications to ensure valid and applicable fairness metrics 11."}, {"title": "4.3 Deployment", "content": "Documentation. Comprehensive documentation constitutes a fundamental requirement for the ethical implementation of FMs 11,127. The documentation must encompass detailed spec-ifications of training data sources, testing benchmark method-ologies, and explicit deployment guidelines that minimize risk and bias 128. Moreover, the documentation should present in-formation in a stratified manner, ensuring accessibility across varying levels of technical expertise while maintaining rigorous detail. This multi-level documentation approach enables stake-holders to comprehend the model's inherent risks, biases, and operational constraints. Such transparency serves as a crucial mechanism for fostering trust and promoting responsible model deployment in clinical settings.\nLicense. Open FMs represent a strategic approach to addressing global bias mitigation, enhancing transparency, and facilitating equitable power distribution 129,130. By providing unrestricted access to data, code, and model weights, this approach enables resource-constrained institutions to engage in model develop-ment and adaptation, thereby fostering innovation through re-duced computational barriers. As established in Section 4.1.2, while closed models present substantial impediments to achiev-ing fairness and impartial outcomes, open-weight architectures facilitate superior downstream task adaptation. Nevertheless, en-suring adherence to intended model applications remains crucial for preventing bias propagation and maintaining equity. The im-plementation of a Responsible AI License framework provides a structured mechanism for guiding ethical and equitable model utilization.\nMonitoring. In medical settings, the prevalence of the disease and the distribution of patients accessing a specific hospital can change over time. While assessing fairness and bias during pretraining and fine-tuning is critical for FMs, continuous mon-itoring of deployed models in real-world scenarios is equally important. Despite its importance, such monitoring is not widely practiced; in the United States, only 44% of institutions reported conducting local evaluations for bias 131."}, {"title": "5 POLICYMAKERS", "content": "This section examines two critical aspects of FMs development: governance frameworks for ensuring reliability and the strategic allocation of essential resources."}, {"title": "5.1 Governance", "content": "Reliability. The implementation of FMs in healthcare faces critical reliability challenges due to insufficient standardization in bias detection and mitigation practices 19,20,21,25,114. Despite a growing emphasis on AI ethics, healthcare institutions lack both established evaluation frameworks and qualified experts needed to assess these systems effectively. This deficiency is particularly concerning because bias continues to emerge in deployment sce-narios, while current mitigation methods demonstrate limited reliability 25,114. The challenge of bias mitigation becomes partic-ularly critical in regions with limited data representation, notably in Global South nations (Figure 2), where healthcare systems encounter substantial obstacles in implementing and adapting these models for their populations. This disparity underscores a"}, {"title": "5.2 Resource Allocation", "content": "Workforce. Fair FMs demand interdisciplinary collaboration among technical, legal, and social experts to ensure compre-hensive bias mitigation 13,137. These multidisciplinary teams integrate diverse societal values and ethical perspectives into model development and evaluation protocols. Essential to this process is the active participation of marginalized communi-ties, whose insights prove crucial for identifying systemic biases in FMs. The global distribution of FM expertise, however, re-mains heavily concentrated in the United States and China 138,139, creating significant barriers to diverse team formation. This geo-graphic concentration is exacerbated by linguistic constraints: the predominance of English in educational materials and tech-nical documentation impedes knowledge dissemination and FM expertise development in non-English-speaking regions.\nData source. Al research disparities manifest critically in data infrastructure and computational resources across global regions. Figure 2 illustrates the significant underrepresentation of Global South populations in major medical imaging datasets. Quanti-tative analyses reveal systematic exclusion across modalities, with Global South regions constituting less than 0.7% of text-domain datasets 140 This limitation extends to linguistic di-versity: among medical imaging repositories, only PadChest provides non-English (Spanish) annotations (Table 1). Develop-ing equitable FMs requires datasets that reflect global population diversity across both geographic and demographic dimensions. Achieving representative data collection necessitates integrated technical and governance frameworks that actively incorporate marginalized populations into healthcare data systems 32,33,34,35.\nComputing. FMs development requires extensive computa-tional infrastructure, limiting access to organizations with ad-vanced resources 133. Analysis of the November 2024 Top500 supercomputer rankings demonstrates pronounced regional dis-parities: North America holds 55.6% of capacity, Europe 27.4%, and Asia 15.9%, while South America and Oceania represent just 0.6% and 0.5% respectively. The combined scarcity of com-putational power and large-scale datasets creates a fundamental barrier in the Global South, where only Brazil, Australia, and Argentina maintain Top500-listed facilities. Africa's absence from these rankings underscores how infrastructure limitations constrain both FMs development and deployment."}, {"title": "6 CONCLUSION", "content": "FMs represent a transformative advancement in medical imaging analysis, yet their implementation presents both opportunities and challenges for achieving equitable healthcare delivery. Our comprehensive review demonstrates that effective bias mitiga-tion in FMs requires systematic interventions throughout the de-velopment pipeline, from data curation to deployment protocols. While technical innovations in training methodologies show promise for enhancing fairness without relying on sensitive at-tributes, the substantial computational and data demands of these models risk exacerbating global inequalities. The emergence of regulatory frameworks such as the EU AI Act reflects growing recognition of FMs societal impact and the need for governance structures that ensure responsible development. However, sig-nificant disparities persist, particularly in Global South nations where limited access to essential resources including specialized workforce, datasets, and computational infrastructure hinders both development and implementation of fair FMs. Moving for-ward, addressing these challenges requires coordinated action between technologists, healthcare providers, and policymak-ers to develop accessible solutions and appropriate frameworks for low-resource countries and institutions. As FMs continue to evolve, their successful implementation in healthcare will depend on our ability to balance technical innovation with ethi-cal principles, ultimately working toward reducing rather than amplifying existing healthcare disparities."}]}