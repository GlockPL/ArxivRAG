{"title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\nLong-term Streaming Video and Audio Interactions", "authors": ["Pan Zhang", "Xiaoyi Dong", "Yuhang Cao", "Yuhang Zang", "Rui Qian", "Xilin Wei", "Lin Chen", "Yifei Li", "Junbo Niu", "Shuangrui Ding", "Qipeng Guo", "Haodong Duan", "Xin Chen", "Han Lv", "Zheng Nie", "Min Zhang", "Bin Wang", "Wenwei Zhang", "Xinyue Zhang", "Jiaye Ge", "Wei Li", "Jingwen Li", "Zhongying Tu", "Conghui He", "Xingcheng Zhang", "Kai Chen", "Yu Qiao", "Dahua Lin", "Jiaqi Wang"], "abstract": "Creating Al systems that can interact with environments\nover long periods, similar to human cognition, has been a\nlongstanding research goal. Recent advancements in mul-\ntimodal large language models (MLLMs) have made sig-\nnificant strides in open-world understanding. However,\nthe challenge of continuous and simultaneous streaming\nperception, memory, and reasoning remains largely unex-\nplored. Current MLLMs are constrained by their sequence-\nto-sequence architecture, which limits their ability to pro-\ncess inputs and generate responses simultaneously, akin to\nbeing unable to think while perceiving. Furthermore, rely-\ning on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information be-\ncomes costly and inefficient. Therefore, rather than relying\non a single foundation model to perform all functions, this\nproject draws inspiration from the concept of the Special-\nized Generalist AI and introduces disentangled streaming\nperception, reasoning, and memory mechanisms, enabling\nreal-time interaction with streaming video and audio in-\nput. The proposed framework InternLM-XComposer2.5-\nOmniLive (IXC2.5-OL) consists of three key modules: (1)\nStreaming Perception Module: Processes multimodal in-\nformation in real-time, storing key details in memory and\ntriggering reasoning in response to user queries. (2) Multi-\nmodal Long Memory Module: Integrates short-term and\nlong-term memory, compressing short-term memories into\nlong-term ones for efficient retrieval and improved accu-\nracy. (3) Reasoning Module: Responds to queries and exe-\ncutes reasoning tasks, coordinating with the perception and\nmemory modules. This project simulates human-like cogni-\ntion, enabling multimodal large language models to provide\ncontinuous and adaptive service over time. All code and\nmodels of InternLM-XComposer2.5-OmniLive (IXC2.5-\nOL) are publicly available at https://github.com/\nInternLM/InternLM-XComposer/tree/main/\nInternLM-XComposer-2.5-OmniLive.", "sections": [{"title": "1. Introduction", "content": "The goal of developing AI systems [55] that can understand\nand interact with environments over long periods, akin to\nhuman cognition, has been a central focus of research for\ndecades. The rise of large-scale data corpora [54, 69, 95,\n112] and multimodal large language models [83, 84, 107]\nhas driven significant advances in free-form multimodal\nquestion answering. Recent developments, such as Mini-\nOmni [123], VideoLLM-Online [12], and VITA [38], have\nmade notable strides toward enabling more natural and im-\nmersive online interactions. However, challenges persist in\ncreating systems capable of continuous interaction due to\nthe intrinsic limitations of a single decoder-only large lan-\nguage model architecture.\nExisting architectures [12, 38, 123, 149] encounter sig-\nnificant limitations in real-time and long-term streaming\nperception, reasoning, and memory. The sequence-to-\nsequence decoder-only architecture used in current MLLMS\nforces a switch between perception (e.g., seeing and hear-\ning) and thinking, limiting the simultaneous processing of\ninputs and outputs. Additionally, existing works [33, 118,\n145] rely on the integration of multimodal memories within\ncontext windows. The reliance on long contexts to store\nhistorical information proves impractical for long-term use,\nespecially in scenarios requiring continuous AI assistance.\nMultimodal data, like video streams, can quickly accumu-\nlate millions of tokens within a few hours, making it imprac-\ntical to maintain context over multiple days of service. The\ncost and inefficiency of storing all historical clues within the\ncontext further limit the system's capacity to provide con-\ntinuous and long-term service. In contrast, the human brain\ncan effortlessly integrate perception and cognition, preserv-\ning long-term multimodal memories. This is believed to be\nclosely related to the functional partitioning design of the\nhuman brain cortex, where different areas of the cortex are\nresponsible for distinct tasks, such as perception, memory,\nand cognition.\nInspired by the paradigm of Specialized Generalist\nAI [146], we propose a system InternLM-XComposer2.5-\nOmniLive (IXC2.5-OL) composed of fused specialized\ngeneralist models for streaming perception, reasoning, and\nmemory, respectively. The system is designed to enable AI\nmodels to engage continuously with environments while re-\ntaining observations over time. By integrating short-term\nand long-term multimodal memory, our approach attempts\nto emulate human-like cognition, enabling more dynamic\nand sustained interactions.\nAs shown in Figure 1, the IXC2.5-OL system consists\nof three key modules: (1) Streaming Perception Mod-\nule: This module processes the multimodal information\nstream on-the-fly. To ensure perception accuracy and effi-\nciency, the video and audio streams are handled separately.\nA live video perception model processes the video stream,\nencoding the information and storing key details in mem-\nory. Meanwhile, an audio model recognizes the contents\nof human speech and other sounds, e.g., barking, knocking,\nor whistling. It triggers the reasoning process when human\nqueries occur. (2) Multi-modal Long Memory Module:\nThis component integrates both long-term and short-term\nmemory, enabling the retrieval of detailed short-term infor-\nmation as well as long-term historical cues. It continuously\ncompresses short-term memories into more information-\nrich long-term memories to enhance retrieval efficiency and\naccuracy. (3) Reasoning Module: The reasoning module,\nactivated by the perception module, handles queries and\nperforms reasoning tasks. As the component with the most\nmodel parameters, it serves as the core of the system's deep\ncognitive processes.\nThe proposed system empowers AI with the ability to\nperceive, think, and memorize simultaneously. By over-\ncoming the limitations of alternating perception and rea-\nsoning, IXC2.5-OL seeks to provide continuous, adaptive\nservice, and long-term AI service. The proposed system\nwill not only enhance the performance of AI assistants but\nwill also contribute to the broader AI applications capable\nof continuously interacting and adapting to dynamic envi-\nronments.\nThe IXC2.5-OL demonstrates strong performance\nacross both audio and video benchmarks. Among the open-\nsource models, IXC2.5-OL achieves competitive results\non audio recognition (ASR) benchmarks such as Wenet-\nspeech [140] for Chinese and LibriSpeech [87] for English.\nFor video understanding benchmarks, IXC2.5-OL achieves\nstate-of-the-art results among models with less than 10B pa-\nrameters, obtaining an M-Avg of 66.2% on MLVU [155]\nand an overall accuracy of 68.7% on MVBench [62]. Addi-\ntionally, it demonstrates competitive performance on Video-\nMME [37] (60.6%) and MMBench-Video [34] (1.42).\nOn recent streaming video bench StreamingBench [67],\nIXC2.5-OL achieves new SOTA results on open-source\nmodels (73.79%), highlighting its exceptional capabilities\nfor real-time video interactions.\nTo foster the development of the multimodal streaming\ninteraction community, alongside the model parameters, the\ninference and deployment source code, encompassing both\nthe web frontend and backend code, has also been released.\nAll code and models of IXC2.5-OL are publicly available\nat https://github.com/InternLM/InternLM-\nXComposer/tree/main/InternLM-XComposer-\n2.5-OmniLive."}, {"title": "2. Related Works", "content": "MLLMs for Text-Image Conversation. Large Language\nModels (LLMs) [5, 7, 9, 24, 46, 51, 81, 86, 90, 108-\n110, 136] have garnered significant attention for their re-\nmarkable capabilities in language comprehension and gen-\neration. Building on this success, Large Vision-Language\nModels (LVLMs) [3, 6, 17-19, 28, 30, 31, 36, 56, 68, 82,\n88, 132, 147, 147, 156] have been developed by integrating\nLLMs with vision encoders [4, 10, 14, 21, 22, 29, 70, 74,\n75, 85, 91, 104, 115, 135, 138, 141, 150], extending their\nability to comprehend visual content and enabling applica-\ntions like text-image conversations. Earlier LVLMs were\nprimarily designed for single-image, multi-round conversa-\ntions, whereas recent advancements [1, 4, 30, 48, 58, 68,\n103, 148, 153] have expanded their capabilities to process\nand understand multi-image inputs.\nMLLMs for Video Understanding. In addition to ad-\nvancements in image understanding, the field of MLLMs\nhas seen growing efforts in video analysis [32, 34, 61, 73,\n80, 98, 100, 113, 127]. To address the complexity of video\ninputs, existing approaches leverage techniques such as\nsparse sampling or temporal pooling [44, 66, 77, 79, 133],\ncompressed video tokens [16, 49, 60, 63, 94, 119, 144],\nand memory banks [33, 43, 89, 98, 100, 118, 145]. Ad-\nditionally, some methods utilize language as a bridge for\nvideo understanding [45, 50, 142]. Beyond these video-\nspecific strategies, video analysis can also be framed as\ninterpreting a high-resolution composite image generated\nfrom sampled video frames [52, 126, 149]. Recent advance-\nments [12, 117, 120, 145] have increasingly focused on\nonline video understanding, aiming to simulate real-world\nscenarios where AI processes video streams in real-time to\ncomprehend the environment on-the-fly. However, existing\nsolutions still lack the capability to simultaneously perform\nperception, memory, and reasoning, limiting their applica-\nbility for consistent and long-term human-AI interactions.\nMLLMS for Audio Understanding. Audio understand-\ning can be effectively modeled as a sequence-to-sequence\n(Seq2Seq) task [93], which enables powerful integration\nwith large language models by incorporating audio tokeniz-\ners and encoders [25, 105, 137, 143]. In addition to receiv-\ning the audio input, recent research investigates streaming\nduplex speech models [78, 114, 116, 134] that allow speak-\ners to interrupt freely. Beyond audio-text models, emerging\nresearch delves into audio-visual models [59, 96] and uni-\nfied architectures that process audio, visual, and text modal-\nities [38, 64, 139].\nMLLMs for Omni-Modal Understanding. Integrating\nmultiple modalities into a single omni-modal foundation\nmodel represents a promising research direction. Exist-\ning works [13, 38, 42, 64, 102, 121, 124, 139] explore\nmodels capable of processing omni-modal inputs, typically\ncombining video and audio, to produce outputs in vari-\nous formats. These outputs include text [38, 42, 64], au-\ndio [13, 102, 124], and omni-modal contents [121, 139]. In\nthe current design of IXC2.5-OL, we handle the audio and\nvideo modalities separately to mitigate potential influence\nduring joint training. In future versions, our model will\nincorporate joint training across all modalities, enabling\nseamless omni-modality integration."}, {"title": "3. Method", "content": "As we briefly introduced in Sec.1, the IXC2.5-OL has three\ndisentangled modules: 1) the Streaming Perception Module\nfor on-the-fly visual and audio information processing, 2)\nthe Multi-modal Long Memory Module for memory inte-\ngration and retrieval, and 3) the Reasoning Module collect\ninformation from the perception and memory module, and\nhandles queries and performs reasoning tasks. All the mod-\nules work simultaneously and interact asynchronously."}, {"title": "3.1. Streaming Perception Module", "content": "Besides nature language, the IXC2.5-OL could handle\nvideo and audio natively. To realize this, the Streaming Per-\nception Module contains an Audio Translation Module and\na Video Perception Module.\nAudio Translation Module contains an audio encoder, an\naudio projector, and a Small Language Model (SLM). The\naudio encoder encodes the input audio sample into high-\ndimension features, and the audio projector further maps\nthe feature to the input space of the SLM. The SLM out-\nputs both the class (e.g. laughing, clapping, or raining) of\nthe audio and the natural language within the audio (i.e.\nthe automatic speech recognition). In practice, we use the\nWhisper [92] model as the audio encoder and a Qwen2-\n1.8B [128] as the SLM. The training contains two stages\nand we list the training data in Table 1.\nVideo Perception Module provides coarse-grained visual"}, {"title": "3.2. Multi-modal Long Memory Module", "content": "The Multi-modal Long Memory Module is the core design\nto handle extremely long video input and helps the Reason-\ning Module to get rid of millions of tokens from its con-\ntext window. It shares a similar idea from the VideoStream-\ning [89] that encodes video clips into short-term memories\nand integrates them into long-term memory. With the given\nquestions, it retrieved the most related video clips for the\nReasoning Module. Formally, the Multi-modal Long Mem-\nory Module is trained with three tasks:\nVideo Clip Compression. With features of kth video clip\nextracted from the Perception Module $F_k \\in \\mathbb{R}^{T_N\\times C'}$, we\ninitialize its short-term memory $H_k \\in \\mathbb{R}^{T_P\\times C}$ by the spa-\ntial down-sampling and its global memory $\\hat{H}_k \\in \\mathbb{R}^{1\\times C}$.\nWe realize the compression by the auto-regressive and fea-\nture aggregation nature of LLMs:\n$H_{k}, \\hat{H}_{k} = Compressor([F_{k} \\bullet H_{k} \\bullet \\hat{H}_{k}])$.\nMemory Integration. Short-term memory represents the\ndetailed information of each short video clip while the\nmodel still lacks a macro view of the video. To this end,\nwith the short-term and global memory of a list of video\nclips, we integrate them into long-term memory by the\nCompressor in the following format:\n$H_{1}, H_{2}, ..., H_{K} =$\n$Compressor([H_{1} \\circ H_{2}... \\circ H_{K} \\circ \\hat{H}_{1} \\circ \\hat{H}_{2}... \\circ \\hat{H}_{K}])$.\nthe $H = [H_1, H_2, \u2026, H_k] \\in \\mathbb{R}^{k\\times C}$ represents the\nvideo in a high-compressed way and we denote it as the\nlong-term memory.\nVideo Clip Retrieval. When users raise questions, the\nMulti-modal Long Memory Module retrieves the question-\nrelated video clips and provides both the video clips and\ntheir short-term memory to the Reasoning Module. In prac-\ntice, we first encode the question to the feature space of\nthe memory. We concatenate the long-term memory with\nthe tokenized question as the Compressor input, and we\nview the last token of the output features as the memory-\nspace-aligned question feature. Then we calculate the sim-\nilarity between the question feature and each video's global\nmemory, and select the most related clips for the Reasoning\nModule.\nImplementation Detail. We use Qwen2-1.8B [128] as\nthe LLMs and construct several kinds of training data for\nthe three aforementioned tasks. As shown in Table. 2, we\ntrain the Video Clip Compression task with short video cap-\ntioning data from multiple sources, using the same prefix\ncaptioning task designed in VideoStreaming [89]. For the\nMemory Integration task and Video Clip Retrieval task, be-\nsides the off-the-shelf video grounding data, we also con-\nstruct data for two unique tasks: \u2018Semantics Implicit Ques-\ntion' and 'Reference Implicit Question'.\nThe 'Semantics Implicit Question' means the question\ndoes not point to some object directly, but mentions the us-\nage or meaning of the object, and the model should find out\nthe object by understanding the implicit question. For ex-\nample, when the user asks \u2018How about the weather today?',\nthe model should find out some weather-related object in\nthe past video stream, such as an umbrella, a sun-glass, or\nsomething. Another example could be \u2018I'm hungry, where\ncan I heat my sandwiches?', the model should find the mi-\ncrowave oven it has seen before.\nThe 'Reference Implicit Question' means the question\nuses pronouns rather than nouns. For example, 'What is\nthis' means the models should retrieve the current frames,\nalthough it does not mention any exact objects."}, {"title": "3.3. Reasoning Module", "content": "The Reasoning Module is initialized by an improved ver-\nsion of InternLM-XComposer2.5 (IXC2.5 in the following\nfor simplified statement) and we add a memory projector to\nalign the memory feature with IXC-2.5. For a given ques-\ntions and both visual and memory information provided by\nthe Memory Module, we formulate the input as:\nQuestion: < |Que| >,\nHere is the question related video clip < Img >;\nHere is the question related memory < |Mem| >\nIn real-world usage, there exists some noisy input that\nshould not be answered (e.g., the user says 'enn...' or\n'ok...'), the model should keep salient and wait for the next\nquestion. To realize this, we add an additional 'Instruction\nPrediction' process for each question to decide it should be\nanswered or not."}, {"title": "3.4. System Pipeline", "content": "As illustrated in Figure 3, the system comprises the Fron-\ntend, SRS Server, and Backend Server.\nFrontend. The frontend application, developed with\nJavaScript, enables the camera and microphone to capture\nvideo and audio stream inputs, which are then pushed to\nthe SRS server. Concurrently, it establishes a WebSocket\nconnection with the backend to listen for audio outputs and\ninterrupt signals. When audio data is received, the frontend\nplays it. Upon receiving an interrupt signal, the frontend\nsuspends the audio playback and discards the pending au-\ndio.\nSRS Server. SRS (Simple Realtime Server) is a straight-\nforward and efficient real-time video server, adept at sup-\nporting a multitude of real-time streaming protocols such as\nRTMP, WebRTC, HLS, HTTP-FLV, SRT, and others. It is\nrenowned for its ability to reliably receive and deliver audio\nand video streams.\nBackend Server. After establishing a WebSocket connec-\ntion with the frontend, the backend will pull streaming from\nthe SRS Server and initiate separate threads to read audio\nand video.\nThe audio reading thread will segment the audio stream\ninto 4096-bit chunks and enqueue them into the Audio\nQueue. The Voice Activity Detection (VAD) [40] thread\ncontinuously reads data from Audio Queue and detects the\nstart and end of voice activity. Upon detecting the start of\nvoice activity, the backend sends an interrupt signal to the\nfrontend to pause the currently playing audio, and at the\nsame time, dispatches a backup signal to the video process,\ndirecting it to save the current memory state. When de-\ntecting the end of voice activity, the entire voice segment\nwill be enqueued into ASR Todo Queue. The ASR thread\ncontinuously reads audio segments from ASR Todo Queue,\nperforms background noise classification and voice recog-\nnition on them, and then enqueues the results into LLM Todo\nQueue for use by the LLM.\nThe video reading thread reads video frames at a rate of\n1 frame per second and enqueues them into Frame Queue.\nThe compressor process reads video frames from the queue,\nrecognizes them, extracts relevant memory, and stores it.\nUpon receiving a backup signal from the VAD thread, the\ncompressor process will save the current memory state for\nlater retrieval.\nThe LLM process reads text from the LLM Todo Queue\nand determines whether it is an instruction that requires a re-\nsponse from the model. For texts identified as instructions,\nthe compressor process will use the current instruction and\nthe backed-up memory to perform memory grounding, in\norder to retrieve memories related to the instruction. The\nLLM process will then generate a response based on the\nretrieved memories and the instruction, and enqueue the re-\nsulting output into TTS Todo Queue. An additional TTS\nthread (e.g., F5-TTS [20], MeloTTS [154]) will convert the\ntext from the TTS Todo Queue into audio and send it to the\nfrontend."}, {"title": "4. Experiments", "content": "In this section, we validate the benchmark performance of\nour InternLM-XComposer2.5-OmniLive (IXC2.5-OL), in-\ncluding both audio and video benchmarks."}, {"title": "4.1. Audio Benchmarks", "content": "We evaluate our audio models on two prominent au-\ntomatic speech recognition (ASR) benchmarks: Wenet-\nspeech [140] for Chinese (CN) and LibriSpeech [87] for En-\nglish (EN). WenetSpeech includes two test sets: Test_Net,\nwhich represents high-quality and relatively clean Chinese\nspeech, and Test_Meeting, which captures more challeng-\ning conversational scenarios. LibriSpeech consists of four\nsplits: Dev_clean and Test_clean, which contain clean,\nhigh-quality English speech, and Dev_other and Test_other,\nwhich include noisier, more complex utterances.\nAs shown in Table 3, our IXC2.5-OL demonstrates supe-\nrior performance compared to recent streaming audio LLMs\nsuch as VITA and Mini-Omni, particularly achieving lower\nWord Error Rates (WER) across both CN and EN bench-\nmarks with merely a lightweight 1.5B LLM."}, {"title": "4.2. Video Benchmarks", "content": "In Tables 4, 5, 7 and 8, we compare IXC2.5-OL with\nboth closed-source APIs and open-source models on\nconventional video understanding benchmarks, including\nMLVU [155], Video-MME [37], MMBench-Video [34] and\nMVBench [62]. Furthermore, we also assess the perfor-\nmance of different models on the recently proposed Stream-\ningBench [67], which is designed to better evaluate perfor-\nmance for real-time video interactions. The results of this\ncomparison are presented in Table 6. For the video bench-\nmarks, the base model utilizes 64 sampled frames for each\nvideo during evaluation.\nMLVU MLVU is a comprehensive benchmark designed\nfor evaluating Multimodal Large Language Models in Long\nVideo Understanding tasks. The videos range from 3 min-\nutes to 2 hours and include nine distinct evaluation tasks.\nHere, we evaluate seven multi-choice tasks, including Topic\nReasoning, Anomaly Recognition, Needle QA, Ego Rea-\nsoning, Plot QA, Action Order, and Action Count. The de-\ntailed comparisons are given in Table 4. The IXC2.5-OL ex-\nhibits state-of-the-art (SOTA) performance among closed-\nsource APIs, and open-source models with parameters less\nthan 10 billion, surpassing the previous SOTA by 1.3% for\nVideo-XL, 1.6% for GPT-40.\nVideo-MME Video-MME is a high-quality video bench-\nmark. The videos are collected from 6 primary visual do-\nmains with 30 subfields to ensure broad scenario generaliz-\nability, encompassing both short-, medium-, and long-term\nvideos, ranging from 11 seconds to 1 hour. As demon-\nstrated in Table 5, the IXC2.5-OL exhibits competitive per-\nformance on this benchmark, comparable to previous SOTA\nMiniCPM-V 2.6.\nStreamingBench StreamingBench is a streaming video\nbenchmark designed for real-time video evaluation. It com-\nprises 18 tasks, showcasing 900 videos and 4,500 human-\ncurated QA pairs. In this context, we focus on assessing\nvisual understanding in real-time. Table 6 illustrates the\ncomparative analysis, demonstrating that IXC2.5-OL excels\namong all open-source models, achieving a 2.67% improve-\nment over the previous state-of-the-art model, LLaVA-\nOneVision, and falling just short of the closed-source API,\nGemini 1.5 Pro. This performance solidifies IXC2.5-OL's\nremarkable prowess in real-time video interaction.\nMMBench-Video MMBench-Video is a free-form QA\nvideo benchmark consisting of 600 videos and 2000 QA\npairs. The duration of each video varies from 30 seconds\nto 6 minutes. Given the open-ended nature of the answers,\nthe benchmark utilizes GPT-4-based evaluation to enhance\nquality in terms of accuracy, consistency, and alignment\nwith human judgment. The results are presented in Table\n7. IXC2.5-OL demonstrates state-of-the-art performance\non perception tasks and comparable performance on overall\nevaluations.\nMVBench MVBench is a video benchmark that empha-\nsizes temporal understanding. It encompasses 20 challeng-\ning video tasks that cannot be effectively addressed using\na single frame. As shown in Table 8, IXC2.5-OL, despite\nhaving a smaller 7B parameter size, has outperformed both\nthe GPT-4 series and the 72B open-source model LLaVA-\nOneVision, demonstrating its strong capability in under-\nstanding video temporal dynamics."}, {"title": "5. Conclusion", "content": "We have presented IXC2.5-OL, a real-time streaming model\nthat advances multi-modal text, audio, and visual capabili-\nties with long-term memory. IXC2.5-OL empowers users\nto engage in dynamic and interactive experiences. Our\nmodel's real-time processing enables fluid and responsive\ninteractions, allowing users to engage with ever-changing\nenvironments of multimodal data seamlessly, providing a\nmore intuitive and efficient user experience. Our future\nwork will focus on reducing system latency to provide a\nseamless user experience."}]}