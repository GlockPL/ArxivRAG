{"title": "CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements", "authors": ["Yang Zhang", "Wenbo Yang", "Jun Wang", "Qiang Ma", "Jie Xiong"], "abstract": "Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.", "sections": [{"title": "1 Introduction", "content": "The prices of financial assets reflect all available information, according to Fama's Efficient Market Theory [16, 17]. Major financial releases from government sectors often trigger market movements by shaping investors' expectations and evaluations of economic conditions, asset growth potential, and associated risks. For example, during the FOMC meeting on March 16, 2020, the Fed's emergency rate cut to 0-0.25% sharply altered investors' economic outlook, resulting in a massive sell-off. Major indices, including the S&P 500, NASDAQ, and Dow Jones, dropped by over 10%, marking the steepest single-day decline since 1987 [14]. These salient macroeconomic events cause reactions in financial assets, establishing causal relationships between events and financial assets. Therefore, accurately forecasting the causal consequences of the salient macroeconomic releases on financial market is essential, not only to help investors manage risks and maximize returns, but also to provide policymakers with valuable insights for evaluating and refining future policies.\nPrevious studies on event-driven forecasting have primarily adopted three lines of methodologies. The first line of approaches utilizes text feature-based models, where language models, ranging from self-crafted RNN-based architectures [12, 23, 30, 58] to pre-trained transformers [51, 63], embed sentiment information into text vectors, and then stock movements are predicted as a binary classification task (e.g., hawkish vs. dovish). The second line of methodology focuses on historical time-series data, treating stock price movements as a regression problem [8, 55]. Recently, transformer-based architectures have been applied for time-series prediction, including Informer [61], FedFormer [62], and Auto-Former [9], etc. However, both of these directions typically focus on a single modality, neglecting multi-dimensional information. The third line of research adopts a multi-modality approach, leveraging multiple types of data sources to enhance forecasting performance. For instance, studies like [37, 39] incorporate textual, video, and audio data from FOMC meetings alongside corresponding market movements. While these approaches show promise for event-driven financial forecasts, they face three major limitations:\n\u2022 Data Limitation: Existing approaches predominantly focus on a single type of event, such as FOMC meetings [37, 39, 51], while neglecting other crucial macroeconomic events like unemployment insurance releases, CPI, PPI, and GDP advance reports. Additionally, many studies rely on daily-based time-series data for financial assets [8, 9, 37, 39, 51, 55, 61, 62], which limits their applicability and precision in real-time trading scenarios where high-frequency data is mostly adopted.\n\u2022 Modality Limitation: Most prior studies rely on single-modality analysis, using either textual models [12, 23, 30, 51, 58, 63] or time-series models [8, 9, 55, 61, 62], which fail to integrate the complementary strengths of both modalities. While some multi-modality approaches have been proposed [37, 39], they often lack advanced mechanisms for feature fusion, effective decoding strategies, and causal learning, which are critical for understanding the complex interplay between event texts and market dynamics.\n\u2022 Causality Limitation: Existing methods [37, 39] fail to incorporate causal reasoning frameworks, overlooking the causal relationships between events and market reactions. Without explicitly modeling these relationships, such approaches cannot fully capture the drivers of financial market behavior, limiting their predictive robustness.\nTo address the limitations of previous studies, we propose a novel multi-modality framework, CAMEF (causal-Augmented Multi-Modality Event-Driven Financial Forecasting). CAMEF integrates time-series and textual features through specially designed multi-feature fusion techniques, time-series decoding mechanisms, and causal learning strategies. By conducting a thorough review of financial literature, we identify six types of salient macroeconomic events for the forecasting analysis. Furthermore, the framework employs causal data augmentation powered by Large Language Models (LLMs) and a causal contrastive learning approach to enhance the causal understanding and forecast accuracy of CAMEF. This paper offers three key contributions:\n\u2022 Novel Dataset: We introduce a novel open-source synthetic dataset comprising 6 types of macroeconomic event scripts (ref to Tab. 1 for details) from 2008 to April 2024 through reviewing from financial literature [4, 11, 18-21, 27, 35, 36, 40, 44-46, 54], alongside intra-day_high-frequency financial data at 5-minute intervals from key U.S. stock indexes and Treasury bonds. To support causal learning, the dataset also includes counterfactual event scripts generated using our LLM-based causal argumentation prompting, making it the first to integrate policy texts, high-frequency trading data, and causally augmented content.\n\u2022 Novel Multi-Modality Model: We propose a novel multi-modality approach, CAMEF, that integrates time-series and textual features, incorporating specifically designed multi-feature fusion and time-series decoding networks, which have been demonstrated to be effective for forecasting. Additionally, the model includes a causal learning mechanism to enhance forecasting capability by capturing the causal relationships between events and market reactions. The complete CAMEF dataset and model code will be made publicly available on GitHub to ensure reproducibility and facilitate future research upon publication.\n\u2022 Counterfactual Generation and Learning: We introduce a counterfactual data augmentation strategy to generate counterfactual event scripts based on collected macroeconomic releases. This approach leverages LLMs to create scripts with varying sentiment levels by modifying key numerical values and sentiment-relevant phrases, while preserving the original format, writing style, and neutral words"}, {"title": "2 Related Work", "content": "Event-driven financial forecasting [3] focuses on predicting asset prices [19, 21] and market volatility [11, 35] based on events like macroeconomic releases [19], news [31], corporate announcements [63], and social media activity [58]. Three main approaches exist in this area. The first leverages text analysis to predict asset responses based on event-related text. Early works utilized TF-IDF [29, 41] and topic models [38, 52], progressing to RNN-based models [23, 30] and pre-trained transformers [51, 63], which capture nuanced semantics. Although these models excel at semantic extraction, they often lack integration with historical price data, crucial for holistic forecasting.\nThe second line of approaches uses statistical and sequential models on numerical data, such as linear regression [6], ARIMA [2], and GARCH [24]. Later, deep learning methods like RNNS [30] and CNNs [15, 50] enhanced nonlinear modeling capabilities. More recently, transformer-based models, such as Informer [61] and FedFormer [62], improved long-range dependency modeling for time series data. However, these models tend to be \"case-specific,\" requiring task-specific training. In contrast, the lastest pre-trained models for time-series data, like MOMENT [22], Timer [34], and TOKEN [1], offer more generalized and adaptable solutions for time-series tasks.\nThe third line of research adopts multi-modality approaches, combining diverse data types to improve forecasting accuracy. Some studies incorporate text and audio [42, 59] but often overlook time-series dependencies. Recent work has integrated time-series and textual data; for example, [48, 49] employed SVM and GRU models to capture time-series features. However, these models are relatively shallow for extracting complex patterns. More recent studies [25, 28] leverage transformer-based models for time-series analysis, better capturing deeper temporal structures. Building on these advancements, this paper aims to utilize state-of-the-art pre-trained models with enhanced feature fusion and causal learning for multi-modality forecasting."}, {"title": "2.2 Salient Macroeconomic Factors", "content": "Which macroeconomic announcements have a greater impact on financial markets than others? This question has been widely studied in the financial literature, with Central Bank Communications standing out as the most-researched factor [11, 18, 35, 36, 44-46, 54]. Beyond central bank communications, various other macroeconomic factors have also been identified as significant drivers of market movements. Among these, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports have been found to notably influence price movements and market volatility through empirical statistical testings [4, 19-21, 27, 40]. In this paper, we aim to leaverage the most significant factors evidented by the past financial literautre [4, 19-21, 27, 40], which include FOMC Meeting Documents, Non-farm Payrolls, Unemployment Releases, Initial Unemployment Claims, ISM Manufacturing Index, GDP Advance Releases, Consumer Confidence Index, and Producer Price Index (PPI) Reports."}, {"title": "2.3 Counterfactual Data Augmentation by LLMs", "content": "Counterfactual Data Augmentation seeks to reduce spurious correlations and enhance model robustness. Kaushik et al. [26] introduced a method that augments training data with counterfactuals written by human annotators, effectively helping to mitigate spurious patterns. Ross et al. [47], Wu et al. [57] later proposed the use of hand-crafted templates and trained text generators to create counterfactual data through predefined perturbation types. However, these methods are limited by their reliance on fixed perturbations.\nMore recently, Chen et al. [10], Wang et al. [56] proposed more flexible, LLM-based approaches that leverage specifically designed in-context learning prompts and generation pipelines for counterfactual and instruction data generation. Following this direction, we present a counterfactual generation framework specifically designed for macroeconomic releases."}, {"title": "3 Problem Formulation", "content": "Event Set is defined as $E := \\{\\varepsilon_1, \\varepsilon_2, ..., \\varepsilon_{|E|}\\}$, where E represents a collection of $|E|$ event scripts. Each event $\\varepsilon_i$ occurs at a specific timestamp i and belongs to one of the event types shown in Table 1A. Each event script $\\varepsilon_i$ consists of a sequence of word tokens, represented as $\\varepsilon_i := \\{w_1, w_2, . . ., w_m\\}$.\nTime Series Data is defined as $X := \\{X_1, X_2, . . ., X_{|X|}\\}$, where each $X_i$ represents the numerical data at time step i. An event $\\varepsilon_i$ is aligned with a time series segment $X_{i-t:i+t}$, where i denotes the time of the releasement of the event, and t represents the duration of the time-series segment both preceding and succeeding time step i, denoted as $[X_{i-t:i+t} \\rightarrow \\varepsilon_i]$. This alignment reflects the time series segment leading up to the event ($X_{i-t}$) and the period during which the event is expected to have an effect ($X_{i+t}$).\nEvent-Driven Forecasting: Given a dataset $U = \\{[X_{i-t:i+\\tau} \\rightarrow \\varepsilon_i]\\}_1^n$ consisting of n aligned event and time-series pairs, for each data pair in U, the model uses both the event text $\\varepsilon_i$ and the historical time-series segment $X_{i-t:i}$, which spans t steps before the event's release at time i, to forecast the future time steps $X_{i+1:i+\\tau}$\nCausal Effect Graph: A Causal Effect Graph represents the causal links among variables: textual modality $\\varepsilon$ (event scripts),"}, {"title": "4 Data Collection and Counterfactual Event Augmentation", "content": "This section introduces the proposed dataset and the methodology for counterfactual event augmentation. The dataset includes 6 types of key macroeconomic announcements ranging from 2004 to 2024, selected through an extensive review of the financial literature, along with_high-frequency trading data. Unlike the daily-based trading data used in previous studies, this high-frequency data provides more predictive accuracy and better reflects real trading behavior in the industry."}, {"title": "4.1 Dataset Acquisition", "content": "The primary question guiding the collection of this dataset is: \"Which macroeconomic releases have the greatest impact on financial markets?\" To address this, we conducted a comprehensive review of the financial literature to identify key macroeconomic factors that influence market behavior. Several dominant factors emerged, including the FOMC Minutes [11, 18, 35, 36, 44-46, 54], along with Unemployment Insurance Claims, Employment Situation Reports, GDP Advance Releases, and the Consumer Price Index (CPI) and Producer Price Index (PPI) reports [4, 19-21, 27, 40], which serve as the textual modality data for our dataset. To collect these data, we developed web crawlers to extract raw files directly from official sources, including HTML, PDF, and TXT formats. These raw files were then pre-processed and converted into a structured and unified text format, ensuring consistency and ease of subsequent analysis. Further details on the data crawling and pre-processing methodologies can be found in Appendix B.\nIn addition to the textual data, studies [11, 18, 35, 36, 44-46, 54] have demonstrated that the largest market impacts are typically observed in major U.S. stock indexes and Treasury bonds. Therefore, we focused on collecting high-frequency trading time-series data at 5 minute interval for key stock indexes, including the S&P 500 (SPX), Dow Industrial (INDU), NASDAQ (NDX), as well as U.S. Treasury Bond at 1-Month (USGG1M) and Treasury Bond at 5-Year (USGG5YR) from Bloomberg Terminal."}, {"title": "4.2 Counterfactual Events Generation based on LLM", "content": "This section describes the process of counterfactual event generation, creating hypothetical scenarios from existing event scripts. The aim is to reflect a target sentiment of a given event script while maintaining logical consistency and coherence of the original script. Our goal is modifying sentiment-relevant elements (such as key facts, sentiment-indicative phrases, or numerical values) without disrupting the sentiment-neutral components of the script.\nFormally, for a given event script $\\varepsilon_i := \\{w_1, w_2,..., w_m\\}$, the objective is to produce a counterfactual version $\\varepsilon_i'$ that embodies the desired target sentiment S. Conceptually, the event script can be viewed as comprising sentiment-relevant content ($\\varepsilon_{sentiment}$) and sentiment-neutral content ($\\varepsilon_{neutral}$), so that $\\varepsilon_i = \\varepsilon_{neutral} \\cup \\varepsilon_{sentiment}$. Instead of explicitly decomposing the script, we guide a language model (LLM) using structured prompts to modify only the sentiment-relevant content. This ensures that neutral content remains intact or is replaced with semantically equivalent expressions. Formally:\n$\\varepsilon = \\varepsilon_{neutral} \\cup f_{LLM}(\\varepsilon_{sentiment} | S)$               (1)\nwhere $f_{LLM}$ adjusts $\\varepsilon_{sentiment}$ to align with S, and $\\varepsilon_{neutral}$ remains unchanged or is replaced by equivalent expressions.\nSpecifically, we used the LLaMA-3 8B model with a series of carefully designed prompts. These prompts include three key steps, with detailed templates provided in Appendix A:\n(1) Summarization Prompt Condenses lengthy event scripts into concise summaries, addressing memory constraints while retaining sentiment-relevant content and key numerical variables.\n(2) Sentiment Analysis Prompt Assigns a sentiment score (from 1, very negative, to 10, very positive) to the original event script. This score provides a baseline for generating counterfactual versions.\n(3) Counterfactual Generation Prompt Produces multiple counterfactual scripts, each reflecting a different sentiment level. The prompt modifies sentiment-related phrases and numerical values ($\\varepsilon_{sentiment}$) while preserving or equivalently substituting neutral content ($\\varepsilon_{neutral}$)."}, {"title": "5 \u0421\u0410\u041c\u0415\u0420 Architecture", "content": "The CAMEP model integrates both textual and time-series information through a structured architecture consisting of a textual encoder, a time-series encoder, and a forecasting decoder, as depicted in Fig. 3. Each component is detailed below."}, {"title": "5.1 Textual Modality Encoder (CAMEFTextual)", "content": "We encode event scripts using RoBERTa [33]. Given an input script $\\varepsilon_i = w_1, w_2,..., w_m$, ROBERTa produces contextual token embeddings:\n$\\left\\{h_1, h_2, ..., h_m\\right\\} = ROBERTa(\\left\\{w_1, w_2,..., w_m\\right\\}),$               (2)\nwhere $h_j \\in R^{1\\times768}$ is the embedding of token $w_j$. Each embedding is passed through a projection network with three linear layers and GELU activations:\n$e_j = W^{(3)} \\cdot GELU(W^{(2)} \\cdot GELU(W^{(1)}h_j+b^{(1)})+b^{(2)})+b^{(3)},$               (3)\nwhere $W^{(1)} \\in R^{768\\times1024}, W^{(2)} \\in R^{1024\\times1024}$, and $W^{(3)} \\in R^{1024\\times1024}$. The final encoding vector $E_i$ for the script is computed as the average of the transformed embeddings, $E_i = \\frac{1}{m} \\sum_{j=1}^m e_j$"}, {"title": "5.2 Time-Series Modality Encoder (CAMEFTime-Series )", "content": "To encode the time series data, we employ a pretrained time series encoder, MOMENT [22], which generates a fixed-dimensional vector for an input time series segment. Subsequently, we design a multi-residual layer to further refine the encoding vectors, as shown below:\n$X_i = MOMENT(\\left\\{X_1, X_2, . . ., X_n\\right\\}),$               (4)\nwhere $X_i \\in R^d$ is the encoded vector for the input time series segment $\\left\\{X_1, X_2, ..., X_n\\right\\}$, and d represents the dimensionality of the encoded vector. To enhance this representation, we introduce a multi-residual projection layer:\n$Z_i = X_i + f_{residual}(X_i),$               (5)\nwhere $f_{residual}(X_i)$ represents the transformation applied through the residual projection layer, which consists of multiple linear layers interleaved with GELU activations:\n$f_{residual}(X_i) = W_3 GELU(W_2 \\cdot GELU(W_1 X_i+b_1)+b_2)+b_3,$               (6)\nwhere $W_1, W_2, W_3 \\in R^{1024\\times1024}$ are the weight matrices, and $b_1, b_2, b_3 \\in R^{1024}$ are the respective biases. Finally, $Z_i \\in R^{1024}$ serves as the refined vector for the time series segment."}, {"title": "5.3 Feature Fusion and Time Series Decoder", "content": "After obtaining the encoded vectors from both the textual data and the time series data, denoted as $E_i$ and $Z_i$, respectively, we concatenate these two vectors to create a unified representation:\n$E_{combined} = concat(E_i, Z_i),$               (7)\nwhere $E_{combined}$ captures both the semantic information from the macroeconomic releases and the temporal patterns from the time series data. We employ GPT-2 [43] as the decoder to decode the fused vector by leveraging its effective auto-regressive ability:\n$H^{(l)} = f_{GPT2\\_layer}^{(l)}(H^{(l-1)}),$               (8)\nwhere $H^{(0)} = E_{combined}$, and $f_{GPT2\\_layer}$ represents the transformation function of the l-th GPT-2 layer, where $l = 12$. After the final layer, the output is normalized using a layer normalization function:\n$H_{final} = LayerNorm(H^{(l)}),$               (9)"}, {"title": "5.4 Time-Series Forecasting Post-Regressor and Learning Objectives", "content": "We designed a Post-Regressor that applies a linear transformation to the concatenated vector $H_{final}$, followed by GELU activation and a dropout layer with a rate of 0.1:\n$R^{(k)} = GELU(W^{(k)} \\cdot R^{(k-1)} + b^{(k)}),$               (10)\nwhere $R^{(0)} = H_{final}$, $W^{(k)}$ and $b^{(k)}$ are the weight matrix and bias of the k-th linear layer, respectively. k = 4 is the total number of layers in the regressor. The final linear layer maps the representation to a vector of shape (d \u00d7 pred_len), where d is the forecast dimensionality and pred_len is the number of predicted time steps:\n$\\widehat{Y} = W_{out} \\cdot R^{(K)} + b_{out},$               (11)\nwhere $\\widehat{Y}$, represents the predicted time series values.\nLearning Objectives for Time Series: We employ a combination of Mean Squared Error (MSE) loss and Mean Absolute Error (MAE) loss to optimize the model. The MSE loss minimizes the squared differences between the predicted time series values, $\\widehat{Y}$, and the ground truth values, Y, while the MAE loss minimizes the absolute differences. These are defined as:\n$L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\widehat{Y_i})^2,  L_{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\widehat{Y_i}|,$               (12)\nwhere n is the number of predicted values (e.g., 35, 70, or 140, as defined in Section 6.1). The total loss function combines both objectives to balance optimization for large and small errors:\n$L_{Time} = L_{MSE} + L_{MAE}.$               (13)"}, {"title": "5.5 Counterfactual Events Sampling and Causal Learning Objective", "content": "Causal learning enhances the robustness of the CAMEF model by enabling it to identify the correct event script among sampled counterfactual events (CEs). To achieve this, we first design a Diverse Counterfactual Event Sampling Mechanism, which generates two types of CEs. These counterfactuals, along with their corresponding time-series data, are then encoded using the textual and time-series modalities of CAMEF. This process helps the model learn causal relationships between events and their corresponding time-series movements."}, {"title": "5.5.1 Diverse Counterfactual Event Sampling Mechanism.", "content": "We propose a Diverse Counterfactual Event Sampling Mechanism to enhance the model's ability to both identify the ground-truth event and distinguish between different event types. This mechanism is designed with two objectives: (1) to help the model recognize the ground-truth event among similar counterfactuals of the same type, and (2) to enable the model to differentiate between events of different types.\nTo achieve these objectives, we generate two categories of counterfactual events for each factual event:\n(1) Identical Type Sampling: Counterfactual events of the same type as the ground-truth event, created by modifying sentiment-relevant components and key numerical variables, as detailed in Sec. 4.2.\n(2) Diverse Type Sampling: Counterfactual events of a different type, sampled by substituting the ground-truth event with 5 other event type occurring on the closest date.\nThis mechanism provides a diverse set of counterfactual events, collectively denoted as $E^{CF} := \\{\\varepsilon_1^{CF}, \\varepsilon_2^{CF},...,\\varepsilon_{|E^{CF}|}^{CF}\\}$. We set the total number of diverse-type samples to be 5, and the default number of identical-type samples to be 10 as introduced Sec. 4.2."}, {"title": "5.5.2 Causal Learning Objective.", "content": "The causal learning process utilizes both the textual (see Sec. 5.1) and time-series (see Sec. 5.2) encoders of CAMEF to capture the relationships between events and market movements. The textual encoder is used to encode both the ground-truth event $\\varepsilon^{GT}$ and the sampled CEs $\\varepsilon^{CF}$:\n$\\left\\{P^{GT}, P_1^{CF}, ., P_{|E^{CF}|}^{CF} | \\right\\} = CAMEF_{Textual}(\\{\\varepsilon^{GT}\\} \\cup \\{\\varepsilon_i^{CF}\\}_{i=1}^{|EC|}'),$               (14)\nwhere $P^{GT}$ represents the embedding of the ground-truth event, and $\\{P_i^{CF}\\}_{i=1}^{|EC|}$ represents the embeddings of the sampled counterfactual events.\nThe time-series encoder is used to encode the historical time-series segment X aligned with the ground-truth event, resulting in the time-series embedding:\n$T = CAMEF_{Time-Series}(X).$               (15)\nTriplet Loss: The triplet loss is applied to enforce that the ground-truth event embedding $P^{GT}$ is closer to the time-series embedding T than any counterfactual event embedding $P_i^{CF}$, by a margin a (set to 1.0):\n$L_{Causal-TL} = max (0, d(P^{GT}, T) - d(P_i^{CF}, T) + \\alpha),$               (16)\nwhere d(,) denotes the distance between two embeddings (e.g., cosine similarity or Euclidean distance). This loss function encourages the model to capture the causal relationships between events and time-series movements by penalizing counterfactual events that deviate from the causal signal of the ground-truth event. The combination of diverse counterfactual sampling and causal learning ensures that CAMEF effectively learns the true causal drivers of financial market movements, improving its robustness and predictive power.\nTotal Loss: The overall training loss for CAMEF is defined as:\n$L_{Total} = L_{Time} + L_{Causal-TL},$               (17)\nwhere $L_{Time}$ is the objective for time series forecasting, as defined in Equation 13."}, {"title": "6 Experiments", "content": "In this section, we evaluate CAMEF by addressing the following key questions: RQ1. Accuracy: How accurately does CAMEF forecast financial market based on events? RQ2. Model Effectiveness: How do different components enhance CAMEF's predictive performance? RQ3. Event Analysis: Which types of events exhibit stronger influences to financial market?"}, {"title": "6.1 Experimental Settings", "content": "6.1.1 Datasets. We utilized the collected event scripts and time-series data as outlined in Sec. 4 and detailed in Appendix B. The dataset was divided into training, validation, and testing sets in a 6:2:2 ratio. The training set was used to train the models, while the validation set was used for convergence checking and early stopping to prevent overfitting. The final results, based on the test set, are reported in Table 2.\n6.1.2 Baselines. To evaluate our proposed method, we compare it against both uni-modal and multi-modal time series forecasting approaches. For the uni-modal baselines, we considered the traditional yet robust ARIMA model [7], the linear neural model DLinear [60], and several state-of-the-art transformer-based time-series models, including AutoFormer [9], FEDformer [62], and iTransformer [32]. For the multi-modal baselines, we included TEST [53] and GPT4MTS [25].\n6.1.3 Test Settings. We evaluate the baselines and CAMEF across three time horizons-short, medium, and long run, to simulate real investment behavior. For each aligned pair of event and time-series data ([Xi-\u03c4:i+\u03c4\u2192 &i]) as defined in Sec. 3, we use the event script (X) and the time-series segment preceding the event time point i, i.e., Xi-t:i, to forecast the future time-series segment Xi+1:i+t. The value of t is adjusted based on the time horizon: we set \u03c4 to 35, 70, and 140 for short, medium, and long-term forecasts, respectively. These correspond to 175 minutes (about half a trading day), 350 minutes (about one trading day), and 700 minutes (about two trading days).\n6.1.4 Implementation Overview. For the single-modality approaches (ARIMA, DLinear, AutoFormer, FEDformer, iTransformer, and PatchTST), we tested two methods: (1) training the models on continuous historical time-series data and testing on aligned event-based time-series segments from the test set, and (2) training the models directly on event-based time-series segments, and also test on the aligned event-based time-series segments from the test set. The second approach produced more accurate results, and these are the results presented in this paper. Specifically, for each aligned event data pair ([Xi\u2212\u03c4:i+\u03c4\u2192 &i]), single-modality approaches use only the time-series segment preceding the event, Xi-t:i, to train the models and forecast the subsequent segment, Xi+1:i+r; and testing follows the same approach. Detailed settings for each model are provided in Appendix C.\nFor the multi-modality approaches (GPT4MTS, TEST, and CAMEF), both the event script &\u00a1 and the time-series segment preceding the event, Xi-t:i, are used as input to train the models to forecast the post-event time-series segment, Xi+1:i+t. Implementation details, including model configurations and training settings, are explained in Appendix C."}, {"title": "6.2 Experimental Results for Event-Driven Time-Series Forecasting (Q1)", "content": "Table 2 presents the forecasting results for the five datasets across short, medium, and long forecasting horizons. CAMEF outperformed other models in 24 out of 30 settings, achieving first-place rankings, and ranked second in the remaining 6 settings. Specifically, CAMEF demonstrated the best performance across all forecasting lengths for the Stock Market Indices (SPX, INDU, and NDX), except for short-horizon forecasting on INDU, highlighting its effectiveness in event-driven stock market forecasting. For treasury bonds, CAMEF achieved the best results across all forecasting lengths for the 1-month treasury bond (USGG1M) and ranked second for the 5-year treasury bond (USGG5YR). The slightly lower performance on USGG5YR suggests that long-run treasury bonds may be less sensitive to event-driven factors and more influenced by historical trends.\nCompared to single-modality models (e.g., DLinear, Autoformer, FEDformer, PatchTST, and iTransformer), CAMEF achieved an average MSE reduction of 62.55% relative to the best-performing single-modality model, iTransformer. Among multi-modality models, CAMEF surpassed TEST, the second-best performer, with an average MSE reduction of 33.55%.\nThese results highlight three key insights: (1) effectively leveraging multi-modality information provides significant performance gains, particularly for SPX, INDU, NDX, and USGG1M; (2) transformer-based methods consistently outperform classical models such as ARIMA; and (3) CAMEF's superior training and feature fusion strategies establish it as the most effective method for event-driven financial forecasting tasks."}, {"title": "6.3 Ablation Studies on Model Components (Q2)", "content": "To evaluate the effectiveness of each component in CAMEF, we conduct comprehensive ablation studies on Textual Modality, Causal Learning, Feature Fusion, GPT2 Decoder, and the Post-Regressor. Based on the full CAMEF model, we remove the corresponding neural layers, such as the RoBERTa encoder for textual modality or the causal learning component, to assess their individual contributions. From the results, three key findings are: (1) The full CAMEF model achieves the best performance across all datasets, demonstrating the critical importance of utilizing both textual and time-series modalities; (2) Causal learning provides incremental improvements, confirming its value in capturing cause-effect relationships within the data; (3) The proposed feature fusion layers and GPT2 decoder effectively integrate and leverage multi-modality features, significantly enhancing the model's ability to decode time-series data. These findings underscore the necessity of each component in achieving optimal performance for event-driven financial forecasting tasks."}, {"title": "6.4 Ablation Studies on Different Type of Events (RQ3)", "content": "Table 4 shows the predictive performance of different events on the S&P500 Index. FOMC Minutes achieve the lowest MSE and MAE, confirming their critical importance for market prediction. Unemployment Insurance Claims and Unemployment Situation Reports also come with lower errors than the full selection, however the latter becomes less effective at long forecasting length. In contrast, CPI and PPI Reports show weaker predictive power, with PPI yielding the highest errors and CPI improving slightly at long forecasting length. These results emphasize the importance of FOMC and unemployment-related events for financial forecasting."}, {"title": "7 Conclusion and Future Work", "content": "This paper proposed CAMEF, a multi-modality model for event-driven financial forecasting, which integrates effective causal learning and an LLM-based counterfactual event augmentation strategy. Alongside the model, we introduced a novel synthetic dataset comprising 6 types of salient macroeconomic event scripts, their counterfactual samples, and high-frequency time-series data for 5 key financial assets, aligned with real-world investment practices. Extensive experiments demonstrated CAMEF's superior predictive performance compared to prior deep time-series and multi-modality methods. Ablation studies testified the importance of causal"}]}