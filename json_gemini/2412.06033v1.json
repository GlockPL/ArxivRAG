{"title": "CAN GENERATIVE AI SOLVE YOUR IN-CONTEXT\nLEARNING PROBLEM? A MARTINGALE PERSPECTIVE", "authors": ["Andrew Jesson", "Nicolas Beltran-Velez", "David Blei"], "abstract": "This work is about estimating when a conditional generative model (CGM) can\nsolve an in-context learning (ICL) problem. An in-context learning (ICL) problem\ncomprises a CGM, a dataset, and a prediction task. The CGM could be a multi-\nmodal foundation model; the dataset, a collection of patient histories, test results,\nand recorded diagnoses; and the prediction task to communicate a diagnosis to a\nnew patient. A Bayesian interpretation of ICL assumes that the CGM computes\na posterior predictive distribution over an unknown Bayesian model defining a\njoint distribution over latent explanations and observable data. From this perspec-\ntive, Bayesian model criticism is a reasonable approach to assess the suitability\nof a given CGM for an ICL problem. However, such approaches-like posterior\npredictive checks (PPCs)\u2014often assume that we can sample from the likelihood\nand posterior defined by the Bayesian model, which are not explicitly given for\ncontemporary CGMs. To address this, we show when ancestral sampling from\nthe predictive distribution of a CGM is equivalent to sampling datasets from the\nposterior predictive of the assumed Bayesian model. Then we develop the genera-\ntive predictive p-value, which enables PPCs and their cousins for contemporary\nCGMs. The generative predictive p-value can then be used in a statistical decision\nprocedure to determine when the model is appropriate for an ICL problem. Our\nmethod only requires generating queries and responses from a CGM and evaluating\nits response log probability. We empirically evaluate our method on synthetic\ntabular, imaging, and natural language ICL tasks using large language models.", "sections": [{"title": "1 INTRODUCTION", "content": "An in-context learning (ICL) problem comprises a conditional generative model (CGM), a dataset,\nand a prediction task (Brown et al., 2020; Dong et al., 2022). For example, the CGM could be a\npre-trained multi-modal foundation model; the dataset could be a collection of patient histories, test\nresults, and patient diagnoses; and the prediction task could be to communicate the diagnoses to a new\npatient with a given history and test results Nori et al. (2023). This problem is complex, demanding\naccuracy in diagnosis and appropriate communication to the patient. This complexity challenges our\nability to assess whether the model is appropriate for the dataset and prediction task.\nOne interpretation of ICL is that a CGM prompted with in-context examples produces data (either\nresponses or examples of the prediction problem) from a posterior predictive under a Bayesian model.\nA natural question arises when we accept this premise, \u201cIs the Bayesian model a good model for the\nprediction problem?\" This is the question that the field of Bayesian model criticism tries to answer.\nThis field has produced many methods; however, they typically assume access to key components\ndefined by the Bayesian model. Namely, the model likelihood and model posterior. In this work\nwe show how to do model criticism in ICL using contemporary generative AI, specifically how to\nimplement posterior predictive checks (PPCs) (Guttman, 1967; Rubin, 1984) and their cousins when\nwe only have access to the predictive distribution. The result is a practical and interpretable test on\nwhether the model is appropriate for a given ICL problem.\""}, {"title": "2 WHAT IS AN IN-CONTEXT LEARNING PROBLEM?", "content": "We formalize an ICL problem as a tuple $(f^*, x^n, \\theta)$ comprising a prediction task $f^*$, a dataset $x^n$,\nand a conditional generative model (CGM) $\\theta$. The prediction task is generalized as providing\na response $y$ to a given query $z$. The set of valid responses to a user query implies a reference\ndistribution over responses $p(y | z,f^*)$. The dataset $x^n = \\{(z_i, y_i)\\}_{i=1}^n$ comprises $n$ query and\nresponse examples of the prediction task; $z_i, y_i \\sim p(z, y | f^*)$. A practical data abstraction scheme\nsees the decomposition of queries and responses into elements called tokens. As such, queries\nand responses\u2014by extension, examples and datasets\u2014are represented as sequences of tokens. For\nexample, $(z, y) = (t_1^z, t_2^z, ..., t_l^z, t_1^y, t_2^y, ... ) \\equiv (t_1, t_2, ...)$. A conditional generative model $\\theta$ defines\na predictive distribution over the next token in an example $t_j$ given previous example tokens and\n$t_{j-1}$, and a tokenized dataset; $p_\\theta(t | t_{<j}, x^n)$. By ancestral sampling, the CGM effectively defines\nadditional predictive distributions over responses $p_\\theta(y | z, x^n)$, examples $p_\\theta(z, y | x^n)$, and datasets\n$p_\\theta(x | x^n)$."}, {"title": "3 WHAT IS A MODEL?", "content": "Let $\\theta$ again denote a model, but now the model could be Bayesian linear regression, a Gaussian\nprocess, or perhaps even a large language model (LLM). A model defines a joint distribution $p_\\theta(x, f)$\nover observable data $x = \\{x_1,x_2,...\\} = \\{(z_1,y_1), (z_2, y_2),...\\}$ and latent explanations $f$. The\nnotation $f$ denotes both tasks and explanations, but we will clearly distinguish between them. The\nmodel joint distribution factorizes in terms of the model prior $p_\\theta(f)$ over explanations and the model\nlikelihood $p_\\theta(x | f)$ over observable datasets given an explanation; $p_\\theta(x, f) = p_\\theta(x | f)p_\\theta(f)$. From\na frequentist perspective, the prior distribution over $f$ would be ignored and a model would define a\nset of distributions over datasets indexed by $f$; $\\{p_\\theta(x | f) : f \\in F\\}$. A model $\\theta$ alongside data $x^n$\nfurther defines the posterior $p_\\theta(f | x^n)$ and posterior predictive $p_\\theta(x | x^n) = \\int p_\\theta(x | f) dP_\\theta(f|x^n)$\ndistributions, which specify the conditional distributions of explanations and new observations given\nthe observed data."}, {"title": "4 A MODEL IS A CHOICE TO BE CRITICISED", "content": "A model $\\theta$ defined over an observation space $\\mathcal{X}$ is used make inferences informed by observations\nfrom that space $x^n$. Inferences about the probability of the the next word given a sequence of\nwords, model uncertainty, and countless other quantities of interest. But a model is fundamentally a\nchoice the practitioner makes a modelling decision-and so there is no guarantee that the inferences\nderived from observations under a model are grounded in reality."}, {"title": "5 POSTERIOR PREDICTIVE CHECKS ARE MODEL CRITICS FOR ICL PROBLEMS.", "content": "Posterior predictive checks (Rubin, 1984; Meng, 1994; Gelman et al., 1996; Moran et al., 2023) are\nBayesian model criticism methods that use the model posterior predictive to assess the suitability of a\nmodel to make inferences informed by a set of observations. A model's ability to explain observed\ndata is quantified by the posterior predictive p-value, which is derived from a test of the hypothesis\nthat the data are generated according to the model $\\theta$. Following Moran et al. (2023), we assume\naccess to main $x^n$ and holdout $x^{\\text{test}}$ sets of observations, both distributed according to the reference\nlikelihood $p(x | f^*)$ for a specific task $f^*$. The class of PPCs pertinent to our discussion assess\n\"goodness-of-fit\" by asking how well a model fit to a set of observations $x^n$ explains the holdout\nobservations $x^{\\text{test}}$. To measure the goodness-of-fit, a PPC defines a discrepancy function, like the\nnegative log marginal model likelihood $g_\\theta(x, x^n) := -\\frac{1}{|x|}\\mathbb{E}_{z_i,y_i \\in x} \\log p_\\theta (z_i, y_i | x^n)$, or the negative\nlog model likelihood $g_\\theta(x, f) := -\\frac{1}{|x|}\\mathbb{E}_{z_i,y_i \\in x} \\log p_\\theta (z_i, y_i | f)$. Both of these measures will be lower\nfor observations that are well explained by the model, and higher for those that are not.\nDefining the goodness-of-fit measures is only half of the story. A PPC needs a way to assess what\nmakes a relatively high or relatively low value of the discrepancy function. To do this, a reference\ndistribution of values is defined by measuring the discrepancy function over datasets sampled from\nthe model posterior predictive distribution. The posterior predictive p-value is then evaluated as\n$P_{ppc} := \\iint \\mathbb{I}\\{g_\\theta(x, \\cdot) \\geq g_\\theta(x^{\\text{test}}, \\cdot)\\}dP_\\theta(x | f)dP_\\theta(f | x^n)$.\n(1)\nThe PPC locates the value of the discrepancy function for the holdout data $g_\\theta(x^{\\text{test}}, \\cdot)$ in the distribution\nof the discrepancy function under the model $g_\\theta(X, \\cdot)$. The more often the discrepancy (intuitively, the\nloss) of the data generated under the model is greater than or equal to the discrepancy of the holdout\ndata, the more confident we can be that the model explains the holdout data well. Conversely, if the\ndiscrepancy of the holdout data is commonly greater than that of the data generated under the model,\nthen we should be less confident that the model explains the holdout data, and thus be skeptical about\nthe models capacity to solve the ICL problem."}, {"title": "6 THE GENERATIVE PREDICTIVE p-VALUE AND HOW TO ESTIMATE IT", "content": "Modern CGMs\u2014such as LLMs-do not explicitly provide the joint distribution over observations and\nexplanations. In the best case, we may only have access to the model posterior predictive distribution\n$p_\\theta(x | x^n)$ as a black box rather than an integral of the model likelihood $p_\\theta(x | f)$ over the model\nposterior $p_\\theta(f | x^n)$. For discrepancy functions that depend on $f$ this is a problem.\nOur solution to not having the component distributions relies on the intuition that a sufficiently large\ndataset $x := \\{z_i, y_i\\}_{i=1}^N$ generated according to the model likelihood $X^N \\sim p_\\theta(x | f)$ given an\nexplanation $f$ contains roughly the same information as the explanation itself. For an identifiable\nBayesian model, the model posterior $p_\\theta(f | x^N)$ concentrates around the unique generating explana-\ntion $f$ as $N\\rightarrow\\infty$. Therefore, it makes sense to express functions of explanations $f$, like the model\nlikelihood $p_\\theta(x | f)$, that are not defined under a typical CGM, as functions of large datasets, like the\npredictive distribution $p_\\theta(x | x^n)$, that are defined."}, {"title": "6.1 THE MARTINGALE PREDICTIVE P-VALUE", "content": "Our method is built on Doob's theorem for estimators (Theorem 2). This theorem helps us transform\nstatements about the random variable $h(F)$\u2014a function of explanations $F$-to statements about the\nrandom variable $\\mathbb{E}[h(F) | X_1, X_2, ..., X_n]$, which is a function of observations $(X_1,X_2,..., X_n)$.\nThus, we can proceed without direct access to $p_\\theta(z, y | f)$and $p_\\theta(f | x^n)$ and define a p-value that\ndepends on infinite datasets $x^{\\infty} := (x_i, y_i)_{i=1}^{\\infty}$ rather than $f$\n$P_{mpc} := \\iint \\mathbb{I}\\{g_\\theta(x, x^{\\infty}) \\geq g_\\theta(x^{\\text{test}}, x^{\\infty})\\}dP_\\theta(x | x^{\\infty})dP_\\theta(x^{n:\\infty} | x^n)$.\n(2)\nDoob's Theorem is an application of martingales, so in line with the current literature (Fong et al.,\n2023; Lee et al., 2023; Falck et al., 2024)\u2014we call this formulation the martingale predictive p-value.\nThe main theoretical result of this paper establishes the equality of the posterior and martingale\npredictive p-values; Equations (1) and (2). We formalize this statement in the following theorem.\nTheorem 1. Let $F \\sim P_\\theta$, and $X_1, X_2, ... i.i.d \\sim P_f$. Assume Conditions 1 to 3 and let,\n$\\int \\log p_\\theta(x^m | f)|dP_\\theta(f) < \\infty : \\forall x^m \\in \\mathcal{X}^m$.\nThen, $P_{ppc} = P_{mpc}$."}, {"title": "6.2 THE GENERATIVE PREDICTIVE P-VALUE", "content": "The martingale predictive p-value cannot be exactly computed because it is impossible to generate\ninfinite datasets. Thus, we define the generative predictive p-value that clips the limits to infinity by\nsome feasibly large number $N$ to estimate Equation (2) as\n$P_{gpc} := \\iint \\mathbb{I}\\{g_\\theta(x, x^N) \\geq g_\\theta(x^{\\text{test}}, x^N)\\}dP_\\theta(x| x^N)dP_\\theta(x^{n:N} | x^n)$.\n(3)\nThe generative predictive p-value enables us to replace distributions that depend on latent mechanisms\n$f$ or infinite datasets $x^{\\infty}$ with ones that depends on finite sequences. The cost of using finite $N$ is\nestimation error between $p_{gpc}$ and $p_{ppc}$. A formal analysis of this error is left to future work."}, {"title": "6.3 CGM ESTIMATORS FOR THE GENERATIVE PREDICTIVE P-VALUE", "content": "We derive an estimator for the generative predictive p-value in Equation (3) that uses Monte Carlo\nestimates to approximate the integrals."}, {"title": "7 EMPIRICAL EVALUATION", "content": "This section reports the following empirical findings: (1) The generative predictive p-value is an\naccurate predictor of model capability in tabular, natural language, and imaging ICL problems. (2)\nThe p-value computed under the NLL discrepancy is also an indicator of whether there are enough\nin-context examples n. (3) The number of generated examples $N - n$ interpolates the p-value\nbetween the posterior predictive p-value under the NLML discrepancy and the NLL discrepancy\nusing the model posterior $p_\\theta (f | x^n)$ and likelihood $p_\\theta(x | f)$. These findings show that the p-value\ncomputed under either discrepancy yields an accurate predictor of whether generative AI can solve\nyour in-context learning problem. If you also need to know whether there are enough in-context\nexamples, we suggest using the NLL discrepancy function. If computational efficiency is a primary\nconcern, we suggest using the NLML discrepancy as dataset completion generation is not required."}, {"title": "7.1 THE GENERATIVE PREDICTIVE p-VALUE ACCURATELY PREDICTS MODEL CAPABILITY", "content": "Tabular data. We first evaluate whether the generative predictive p-value effectively predicts\nOOD tabular data tasks. The parameters for Algorithm 1 are M = 40 replications and N \u2212 n =\n200 generated examples. The ICL dataset $x^n$ size is varied from n = 2 to n = 200.  plots precision, recall, F1, and accuracy curves and shows that the p-value estimates under either\ndiscrepancy function provide non-trivial OOD predictors for all \u03b1 settings.\nNatural language ICL. Next, we evaluate whether the generative predictive p-value effectively\npredicts out-of-capacity natural language tasks. The parameters for Algorithm 1 are M = 20\nreplications and N \u2212 n = 10 generated examples. The ICL dataset $x^n$ size is varied from n = 4 to\nn = 64.  plots precision, recall, F1, and accuracy curves and shows that the p-value estimates\nunder the NLL discrepancy provide non-trivial (accuracy > 0.5) out-of-capability predictors in the\ndomain of natural language for all a settings. The NLML discrepancy $g_\\theta (x, x^n)$ is also generally\nrobust outside of the small n and small a setting.\nGenerative fill. Finally, we evaluate whether the generative predictive p-value effectively predicts\nOOD generative fill tasks. The parameters for Algorithm 1 are M = 100 replications and N \u2212 n=8\ngenerated examples The ICL dataset $x^n$ size is varied from n = 2 to n = 8."}, {"title": "7.2 THE NLL DISCREPANCY ALSO INDICATES WHETHER YOU HAVE ENOUGH DATA", "content": "Both discrepancy functions yield accurate predictors of model capability, but the NLL discrepancy\nalso provides information about whether there are enough in-context examples to reliably solve a\ntask. We use prediction RMSE over task responses to measure reliability. Figures 10a and 10b plot\nthe RMSE against the p-values computed under the NLL and NLML discrepancies for in-distribution\npolynomial tabular tasks. We see that lower p-values correlate with higher RMSE for the NLL"}, {"title": "7.3 THE NUMBER OF GENERATED EXAMPLES N \u2212 n INTERPOLATES THE p-VALUE ESTIMATE\nBETWEEN THE NLML AND THE IDEAL NLL DISCREPANCIES", "content": "Inspection of Equations (1) to (3) makes clear that the dataset completion size N \u2212 n should closely\ninterpolate p-value estimates between $p_{ppc}$ computed with the NLML discrepancy and with the NLL\ndiscrepancy using the likelihood and posterior of a Bayesian model. To verify this, we use a reference\nBayesian polynomial regression model to compute the $p_{ppc}$. We use our Llama-2 regression model fit\nto datasets generated from the reference model likelihood under different explanations to compute the\n$p_{gpc}$. We let datasets generated by random ReLU-NNs serve as OOD tasks. Figure 11 demonstrates\nthat our expectation is true. Specifically, the p-value estimates at N \u2212 n = 2 are distributionally close\nto those calculated under the NLML, and they more closely approximate those calculated under the\nreference NLL discrepancy as we increase N \u2212 n to 100. The latter observation is also illustrated in\nFigure 13.\nSince the p-values computed under either discrepancy yield accurate predictors of model capability,\nthe choice between discrepancy functions ultimately comes down to a decision on whether the added\ncomputational cost of generating dataset completions is justified. If you need to know whether\nthere are enough in-context examples to generate an accurate response\u2014a necessity in risk-sensitive\napplications-then we recommend using the NLL discrepancy function. If computational efficiency\nor the cost of response deferral are primary concerns-practical user experience concerns-we\nsuggest using the NLML discrepancy."}, {"title": "8 CONCLUSION", "content": "This work introduces the generative predictive p-value, a metric for determining whether a Conditional\nGenerative Model can solve an in-context learning problem. It extends Bayesian model criticism\ntechniques like PPCs to generative models by sampling dataset completions from the model's\npredictive distribution to approximate sampling latent explanations from a Bayesian model posterior.\nEmpirical evaluations on tabular, natural language, and imaging tasks show that the generative\npredictive p-value can effectively identify the limits of model capability, distinguishing between\nin-capability and out-of-capability tasks for models like Llama-2 7B and Gemma-2 9B. This approach\nis a practical method to assess model suitability that advances Bayesian model criticism for CGMs.\nWhile we have focused on model capability prediction, the p-value estimates could also be used\nfor model selection or as a general measure of task uncertainty. We are eager to explore extensions\nbeyond ICL tasks to improve the reliability of generative AI systems."}, {"title": "A MODEL INTUITION", "content": "In this section we give an interpretation of the component parts of a Bayesian model, how they\nare used to make inferences about uncertainty, and how to relate inferences in classical domains to\ninferences in more complex domains like language.\nThe model prior $p_\\theta(f)$ can be thought of as a library over the possible explanations a model could\nascribe to observations. It is a special kind of library, where the probability of finding an explanation\nin the library at random is also defined. The model prior encodes everything \"known\u201d to a model;\nall the latent patterns available as explanations for or an index of all the probability distributions\nascribable to any set of observations. The model prior may or may not assign non-zero probability\nto an explanation $f$ equivalent to a given ICL problem task $f^*$. If no such explanation has coverage\nunder the prior, then the model may not be able to provide an accurate solution to the ICL problem.\nThe model likelihood $p_\\theta (x | f)$ encodes the variety of observations $x$ that could be generated\naccording to a given explanation $f$. The variety encoded by this distribution is often called aleatoric\nuncertainty aleatory is a pretentious word for random-which refers to the inherent randomness\nover generated datasets when sampling according to the likelihood under a given explanation $f$. For\nexample, given the explanation implied by a fair coin, we will still be uncertain whether the outcome\nof a single coin flip will be heads or tails. More contemporarily, if you are already familiar with this\nconcept and I were to say, \"I would like to share the idea of aleatoric uncertainty with you,\" you would\nknow which idea I want to share, but, before reading this paragraph, you would be uncertain about\nhow I would share it with you. When the model likelihood $p_\\theta (x | f)$ is indexed by an explanation"}, {"title": "The model posterior", "content": "$p_\\theta(f|x^n) = \\frac{p_\\theta(x^n | f)p_\\theta(f)}{\\int p_\\theta(x^n | f)dP_\\theta(f)},$\nencodes variety over explanations that could have generated a specific set of observations $x^n$. This\nvariety is often called epistemic uncertainty. Epistemic is a pretentious term referring to knowledge,\nconveying that we may not yet know which explanation $f$ among subset of reasonable explanations\nbest explains possible datasets $x$ under the ICL problem.\nFor example, given only four observations\u2014say, two heads and two tails-the sample mean estimate\nfor the probability of observing heads is 0.5. However, we may still be uncertain about whether\nthe coin generating the outcomes was fair or biased because the variance of that estimate is still a\nnon-negligible 0.0625 when we assume the coin is actually fair. Related to our contemporary example,\nif I only say, \"I would like to share an idea with you,\" you can probably imagine an abundance of\nideas that I could be referring to and thus still be uncertain about which one I have chosen to share.\nA relevant feature of epistemic uncertainty is that it is reducible as we observe more context. In\nthe coin flip example, as we observe more outcomes, our certainty about the probability of heads\nincreases. In the second example, you may have a better idea about the class of ideas I may share\nbased on what has been presented thus far.\nAs a function of both the model likelihood and prior, the model posterior inherits the limitations of\nboth. But it also provides information about whether an in-context learning problem can be solved\nreliably. Namely, variety over explanations is indicative of being uncertain about which task the ICL\ndataset corresponds to. This variety can lead to the model generating responses corresponding to\ntasks other than $f^*$. But it may also be indicative of when more examples (larger $n$) can improve the\nquality of solutions to an in-context learning problem."}, {"title": "The model posterior predictive", "content": "$P_\\theta(x|x^n) = \\int P_\\theta(x | f)dP_\\theta(f | x^n),$\nis derived from the model to generate new observations $x$ given past observations $x^n$. Poetically, the\nmodel posterior predictive gives the model a voice to respond to observations with observations. The\nmodel posterior predictive convolves the model likelihood of the observations given an explanation\nwith the model posterior over explanations. This process entangles variety over explanations after\nobserving a dataset $x^n$ and variety over observations $x$ for each specific explanation $f$; the model\nposterior predictive entangles aleatoric and epistemic uncertainty.\nModel inferences. A model $\\theta$ defined over an observation space $\\mathcal{X}$ is used make inferences about\nobservations from that space $x^n$. Inferences like the probability of the the next word given a sequence\nof words, model uncertainty, and countless other things. But a model is a choice-the practitioner\nmakes a modeling decision\u2014and so the inferences derived from observations under a model may or\nmay not be grounded in reality."}, {"title": "B DEFINING MODEL CAPABILITY FOR AN IN-CONTEXT LEARNING PROBLEM", "content": "In this section, we formalize the idea that, \u201ca model can solve an ICL problem.\" A formal definition\nmust account for two things: (1) the model may generate undesired responses because the context\ndoes not adequately specify the task, and (2) the model may generate undesired responses despite the\ncontext precisely specifying the task. The definition below accounts for both.\nDefinition 1. An ICL problem comprises a model $\\theta$, a dataset $x^n = \\{z_i, y_i\\}_{i=1}^n \\sim p(x^n | f^*)$, and a\ntask $f^*$. Assume that valid responses $y$ to user queries $z \\sim p(z | f^*)$ are distributed as $p(y | z, f^*)$\nunder the task. Finally, let $A(z, f^*)$ denote any set of responses satisfying\n$\\Pr(Y \\in A(z, f^*) | z, f^*) \\geq 1 - \\epsilon$.\nThe model $\\theta$ is called capable of solving the ICL problem if\n$\\lim_{n\\rightarrow\\infty} \\int \\mathbb{I}\\{y \\in A(z, f^*)\\} dP_\\theta(y | z,x^n) \\geq 1 - \\epsilon$.\n(4)\n(5)\nEquation (4) defines a set of valid responses $y$ to any query $z$ given the task $f^*$. Jesson et al. (2024)\ncall this a $(1 - \\epsilon)$-likely set. For example, the set could be a confidence interval in a regression task\nor a set of semantically equivalent ways to express positive sentiment in an open-ended sentiment\nanalysis task. The $1 - \\epsilon$ set gives us a formal and general way to express the notion of a desirable\nresponse for a given query and task. Equation (5) then says that a model is capable if the probability\nthat a generated response belongs to the set of valid responses converges to be at least $1 - \\epsilon$ as the\nsize of the dataset grows; as the context more precisely specifies the task. This definition accounts\nfor condition (1) through the limit, allowing for capable models with too few in-context examples\nto be called capable. The indicator accounts for both conditions by counting the number of times\nmodel-generated responses fall inside the $1 - \\epsilon$ set; a general measure of accuracy.\nIn addition to accounting for the above conditions, this definition allows the model predictive\ndistribution to collapse to subsets of $A(z, f^*)$\u2014even deterministic responses\u2014and still be called\ncapable. This attribute is preferable to a definition of capability that requires the model predictive\ndistribution to converge to the reference distribution, which would exclude many practical models.\nWhile this definition is general, there are still practical limitations to consider. For example, an\ninfinitely deep and wide random transformer with a finite maximum sequence length might be capable\nof solving most problems under this definition; however, its data efficiency may be so poor that we\nfill the context window before it can generate accurate responses. Similarly, even if the model could\naccommodate infinitely long sequences, the available data may exhausted before the model generates\ndesirable responses.\nThis discussion also sheds light on the propensity for a classifier using the NLML p-value to produce\nfalse negatives (misclassify out-of-capability tasks as in-capability). Focusing on the $z > 0.5$ region\nof Figure 3b, the model will be predicted as in-capability because the model predictive distribution\ncovers the data for small in-context learning dataset sizes. It is not until the model sees 100 examples\nthat the misalignment between the predicted and reference distributions becomes apparent. The NLL\ndiscrepancy addresses this failure mode, as discussed."}, {"title": "CPROOFS FOR THEORETICAL RESULTS", "content": "We restate our formalization and assumptions for convenience. Observable examples $(z, y) \\in \\mathcal{X}$\nare modeled by the $(\\mathcal{X}, \\mathcal{A})$-random variable $X_1$ and explanations $f \\in \\mathcal{F}$ are modeled by the $(\\mathcal{F}, \\mathcal{B})$-\nrandom variable $F$, where $\\mathcal{A}$ and $\\mathcal{B}$ are the relevant sigma algebras. For each $f \\in \\mathcal{F}$, let the model $\\theta$\ndefine a probability measure $P_f$ on $(\\mathcal{X}, \\mathcal{A})$. Let the model $\\theta$ further define a probability measure\n$P_\\theta$ on $(\\mathcal{F}, \\mathcal{B})$. And let $P_{f^\\theta}$ and $\\mu_\\theta$ define the joint measure $\\Mu_\\theta$ over $((\\mathcal{X}_1, \\mathcal{X}_2, ... ), \\mathcal{F})$. Finally,\nwe overload the notation \".\" It means \u201csampled according to\" when referring to the relationship\nbetween a random variable instance and a density or distribution; e.g., $x \\sim p_\\theta(x)$. And it means\n\"distributed as\" when referring to the relationship between a random variable and a probability\nmeasure; e.g., $X \\sim P_f$. Our method rests on Doob's theorem for estimators (Doob, 1949), which\nassumes the following three conditions."}, {"title": "Condition 1.", "content": "The observation $\\mathcal{X}$ and explanation $\\mathcal{F}$ spaces are complete and separable metric\nspaces."}, {"title": "Condition 2.", "content": "The set of probability measures $\\{P_f : f \\in \\mathcal{F}\\}$ defined by the model $\\theta$ is a measurable\nfamily; the mapping $f \\rightarrow P_f(A)$ is measurable for every $A \\in \\mathcal{A}$."}, {"title": "Condition 3.", "content": "The model $\\theta$ is identifiable;\n$f \\neq f' \\Rightarrow P_f \\neq P_{f'"}, ".", "n(6)\nGiven these conditions we can state Doob's theorem.\nTheorem 2. Doob's Theorem for estimators. Let $F \\sim P_\\theta$ and $X_1, X_2, ... i.i.d \\sim P_f$. Assume\nConditions 1 to 3 and a measurable function $h : \\mathcal{F} \\rightarrow \\mathbb{R}$ such that $\\int |h(f)|dP_\\theta(f) < \\infty$, then\n$\\lim_{n\\rightarrow\\infty} \\mathbb{E}[h(F) | X_1, X_2, ..., X_n"], "infty": "forall x^m \\in \\mathcal{X}^m\\}$. Then,\n$g_\\theta (x, F) = -\\frac{1}{|x|} \\log p_\\theta (x | F) = -\\frac{1}{|x|} \\log p_\\theta (x | X^{\\infty}) = g_\\theta(x, X^{\\infty})$.\n$p_\\theta (x | F) = \\lim_{n\\rightarrow\\infty} \\int p_\\theta(x|f)dP_\\theta(f | X_1,..., X_n)$\n$= \\lim_{n\\rightarrow\\infty} \\int p_\\theta(x | f, X_1,...,X_n)dP_\\theta(f | X_1,..., X_n)$\n$= \\lim_{n\\rightarrow\\infty} p_\\theta(x | X_1,..., X_n)$\n$g(x, F) = -\\frac{1}{|x|} \\log p_\\theta (x | F)$\n$= -\\frac{1}{|x|} \\log \\lim_{n\\rightarrow\\infty} p_\\theta(x | X_1,..., X_n)$\n$= -\\lim_{n\\rightarrow\\infty} \\frac{1}{|x|} \\log p_\\theta (x | X_1,..."}