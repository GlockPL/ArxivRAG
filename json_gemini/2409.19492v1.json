{"title": "MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models", "authors": ["Vibhor Agarwal", "Yiqiao Jin", "Mohit Chandra", "Munmun De Choudhury", "Srijan Kumar", "Nishanth Sastry"], "abstract": "The remarkable capabilities of large language models (LLMs) in language understanding and generation have not rendered them immune to hallucinations. LLMs can still generate plausible-sounding but factually incorrect or fabricated information. As LLM-empowered chatbots become popular, laypeople may frequently ask health-related queries and risk falling victim to these LLM hallucinations, resulting in various societal and healthcare implications.\nIn this work, we conduct a pioneering study of hallucinations in LLM-generated responses to real-world healthcare queries from patients. We propose MEDHALU, a carefully crafted first-of-its-kind medical hallucination dataset with a diverse range of health-related topics and the corresponding hallucinated responses from LLMs with labeled hallucination types and hallucinated text spans. We also introduce MEDHALUDETECT framework to evaluate capabilities of various LLMs in detecting hallucinations. We also employ three groups of evaluators \u2013 medical experts, LLMs, and laypeople \u2013 to study who are more vulnerable to these medical hallucinations.\nWe find that LLMs are much worse than the experts. They also perform no better than laypeople and even worse in few cases in detecting hallucinations. To fill this gap, we propose expert-in-the-loop approach to improve hallucination detection through LLMs by infusing expert reasoning. We observe significant performance gains for all the LLMs with an average macro-F1 improvement of 6.3 percentage points for GPT-4.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made significant strides towards artificial general intelligence, achieving notable success in healthcare [1-3], finance [4-6], and law [7], exemplified by models like GPT-3.5/4 [8], Gemini [9], LLaMA [10], and Pixtral [11]. Despite these ad-vancements, LLMs often suffer from hallucination, producing factually incorrect information that is deceptive, nonsensical, or unfaithful to the source content, raising safety concerns and hindering their deployment [12, 13]. As LLM-powered chatbots like ChatGPT [8] gain prominence among the general public, laypeople with no healthcare background increasingly seek health-related advice from these models. This unconditional trust makes them vulnerable to the hallucinated information generated by LLMs.\nExisting works in LLM hallucinations in medical domains [14-16] mainly focus on testing the medical knowledge of LLMs through standardized medical exam questions, which may not sufficiently demonstrate how these models interact with end users in the real world for the following reasons: 1) contextual dependency - Interactions with real-world users often involve ambiguous or incomplete queries, requiring the model to interpret the missing context, which increases the risk of generating hallucinated information. In contrast, medical exams typically present well-defined, clear-cut questions with definitive answers. 2) data diversity Real-world queries come from end users with diverse backgrounds and varying levels of understanding. This is different, for example, from LLMs answering medical exams, which contain standardized questions and answers that do not reflect the variability found in real-world interactions. Considering this diversity and ambiguity, it is unclear how accurate and reliable answers can be provided by LLMs and experts alike.\nThis Work. For the first time in the literature, we study hallucinations in LLM generated responses to real-world healthcare queries. To address the twin issues of contextual dependency and data diversity, we first gather a dataset consisting of extensive real-world queries from users with a diverse set of backgrounds, ranging from experts (the HealthQA dataset [17]) to laypeople (LiveQA [18] and MedicationQA [19]). The questions exhibit characteristics such as 1) ambiguity: open-ended or vague queries that require inference of additional con-"}, {"title": "2 LLM Hallucinations in Healthcare Queries", "content": "We focus on LLM hallucinations in response to healthcare queries from real-world patients. Despite the impressive text generation capabilities of LLMs, they often encounter challenges in generating factually accurate content. Hallucinations, as defined by [21], refer to generated content that is nonsensical or unfaithful to the provided source content. Inspired by [20], we categorize and adapt the following types of hallucinations specifically for healthcare question-answer pairs.\n\u2022 Fact-conflicting Hallucination: When the LLM generated answer to the healthcare query conflicts with a well-known fact or universal truth.\n\u2022 Input-conflicting Hallucination: When the LLM generated answer conflicts with the healthcare query asked (deviates from the source input provided by users)."}, {"title": "3 MedHalu Dataset", "content": "In this section, we introduce the first LLM-generated MEDHALU benchmark, its generation methodology (Section 3.1), human evaluation to assess the quality (Section 3.2) and the dataset statistics (Section 3.3)."}, {"title": "3.1 Dataset Generation", "content": "Our novel MEDHALU benchmark is designed to study LLM hallucinations in real-world healthcare queries based on the three prominent publicly-available healthcare datasets, each containing healthcare-related questions and answers curated by medical experts.\n\u2022 HealthQA [17] contains 1,141 healthcare question-answer pairs constructed from healthcare articles on the popular health-services website Patient. The questions are created by medical experts from a diverse set of healthcare topics, and answers are sourced from the ar-ticles.\n\u2022 LiveQA [18] contains 246 question-answer pairs from real consumer health questions received by the U.S. National Library of Medicine (NLM).\n\u2022 MedicationQA [19] contains 690 anonymous consumer questions, primarily related to drugs and medication from MedlinePlus. The answers are sourced from trusted websites such as MedlinePlus, DailyMed, Mayo Clinic, etc.\nThese datasets were selected for their diverse health topics and expert-verified answers, aligning with real-world public healthcare queries.\nFor each question in each dataset, we used GPT-3.5 [8] to generate hallucinated answers to each healthcare query. We design specific prompts for each type of hallucination and derive hallucinated answers from the GPT-3.5"}, {"title": "3.2 Human Evaluation", "content": "To ensure that the LLM-generated healthcare responses are hallucinated with the specific hallucination types specified in the prompt during the generation process, we conduct human annotations. We employ six medical experts through Prolific\u00b9 with an hourly rate of US$18. Medical experts are selected only if they are native English speakers and have graduated with at least an undergraduate degree in health or medicine. To follow our institute's ethics guidelines, all the annotations took place in the United Kingdom.\nTo keep the costs of annotations in check, we randomly sample around 5% of MEDHALU dataset (100 question-answer pairs) using stratified sampling of the 3 base datasets \u2013 HealthQA, LiveQA, and MedicationQA. We randomly split question-answer pairs into 2 batches, each containing 50 pairs. We then hire 3 experts for each batch to evaluate 50 question-answer pairs in two hours. Specifically, we designed and deployed an annotation platform based on our specific needs in our institute's research server. We provide each annotator with detailed annotation guidelines and a video tutorial of the annotation platform and ask their consent to share basic information such as their education background. We then ask each annotator to annotate every question-answer pair. They are asked whether the answer contains a hallucination and if yes, the type of hallucination following the definitions provided in"}, {"title": "3.3 Dataset Statistics", "content": "Table 3 shows the statistics of MEDHALU dataset for each of the hallucination types. \u201cNone-conflict\" signifies that the healthcare answer does not contain any hallucination. MEDHALU dataset contains 2, 077 question-answer pairs in total, out of which 1, 141 pairs are from HealthQA, 246 are from LiveQA and 690 pairs are from MedicationQA. The dataset is balanced with roughly similar number of question-answer pairs in each of the four hallucination types for all the three base datasets."}, {"title": "4 Hallucination Detection in Healthcare Queries", "content": "Detecting LLM hallucinations is a challenging task mainly because the generated content may seem to be plausible and semantically similar to the correct answer. In this section, we discuss our hallucination detection framework - MEDHALUDETECT (Section 4.1), experimental setup (Section 4.2), and results (Sections 4.3 and 4.4)."}, {"title": "4.1 Methodology", "content": "To detect LLM hallucinations in healthcare queries, we introduce our MEDHALUDETECT framework. At first, we employ three groups of evaluators: LLMs, medical experts, and laypeople without any healthcare background to detect the presence of hallucinations in MEDHALU question-answer pairs. By comparing their evaluations, we study the extent to which each group is vulnerable to hallucinated healthcare responses. With the release of ChatGPT and other LLM-powered chatbots to the general public, laypeople often ask health-related queries from these models, trust them without any healthcare background, and therefore become vulnerable to hallucinated information.\nGiven a healthcare query and corresponding response from MEDHALU, we ask these three groups of evaluators whether any type of hallucination is involved in the response and the reason behind their decisions. If hallucinations are detected, we further ask these evaluators to highlight specific text spans where these hallucinations occur to assess the granularity of their detection abilities. For hallucination detection using LLMs, we prompt various LLMs to identify the presence of hallucinations and specific hallucinated text spans based on the definitions of different hallucination types, the healthcare query, and the corresponding response from the MEDHALU dataset. The prompt for hallucination detection is detailed in Table 4. We employ the following models for detecting LLM hallucinations: the open-source LLaMA-2-7B [10] model and two proprietary GPT-3.5 [22] and GPT-4 [23] models.\nWe employ groups of medical experts and laypeople through Prolific. Medical experts are selected only if they are native English speakers and have graduated with at least an undergraduate degree in health/medicine. Only those laypeople are selected who are also native English speakers but do not have any degree or background in healthcare/medicine. In order to keep the costs of human evaluation in check, we randomly sampled the MEDHALU dataset using stratified sampling of the 3 base datasets HealthQA, LiveQA, and MedicationQA. We sample 100 question answer pairs in total. We randomly split question-answer pairs into 2 batches, each containing"}, {"title": "4.2 Experimental Setup and Evaluation Metrics", "content": "Experimental Setup. For generating hallucinated responses to the healthcare queries, we use GPT-3.5 [22] using OpenAI's official API. We set temperature to 0.7 and maximum generation length to 512 tokens. For detecting LLM hallucinations, we input our detection prompt into each of the LLMs together with the healthcare query and corresponding response. For LLaMA-2-Instruct [10], we use its open-source implementation after downloading the weights for model with 7 billion parameters. For OpenAI's GPT-3.5 and GPT-4, we use their official API. We set the same temperature of 0.7 and maximum generation length of 256 for all the LLMs.\nEvaluation Metrics. We model hallucination detection as a binary classification task and therefore, we use accuracy, macro-Precision, macro-Recall and macro-F1 scores to evaluate the performance across different LLMs and different evaluators. We also ask these evaluators to highlight hallucinated text spans. To measure the effectiveness of detecting hallucinated text spans, we use edit distance between the LLM-detected text spans and the expert annotated hallucinated spans. Edit distance is a measure of similarity between the two strings and is defined as the minimum number of changes required to convert first string into another by inserting, deleting or replacing the characters. The smaller the edit distance, the more similar the strings are."}, {"title": "4.3 Results", "content": "Table 5 shows the results for different groups of evaluators for hallucination detection on MEDHALU dataset. For HealthQA subset, LLaMA-2 achieves an F1-score of 0.52 whereas GPT-3 and GPT-4 obtains F1-scores of 0.56 and 0.57 respectively. For LiveQA subset, the highest F1-score is only 0.52 for both GPT-3.5 and GPT-4. This is because LiveQA is a challenging dataset since it contains real consumer health queries received by the U.S. National Library of Medicine. For MedicationQA, the highest F1-score is 0.55. Overall, OpenAI's GPT-3.5 and GPT-4 perform much better than the open source LLaMA-2 model for hallucination detection with only a slight improvement in GPT-4 over GPT-3.5 model.\nTable 5 also shows the aggregated results for groups of medical experts and laypeople employed for hallucination detection. Medical experts get an overall macro-"}, {"title": "4.4 Hallucination Detection per Hallucination Type", "content": "Next, we study hallucination detection for each of the hallucination types to check if LLMs can detect some hal-"}, {"title": "5 Expert-in-the-loop to Improve LLM Hallucination Detection", "content": "We observe in Section 4.3 that LLMs are slacking behind the experts by a very large margin in medical hallucination detection. On an average, they perform no better than laypeople and in some cases, even worse than them. Therefore, in this section, we explore the possibility to improve the detection of medical hallucinations using LLMs by leveraging expert reasoning.\nWe therefore go back to the human experts, and ask them to articulate their reasoning about how they detected hallucinations and the reason behind their decision of whether a medical response is hallucinated or not. Their responses indicated that a combination of prior domain knowledge as well as looking up trusted health-related public websites such as UpToDate, BMJBestPractice, WebMD and NHS (UK) were used to cross-verify the answers. Therefore, we propose expert-in-the-loop approach to enable experts to help in improving hallucination detection using LLMs. We basically feed in experts' reasoning together with healthcare queries and generated responses into the LLMs and evaluate their hallucination detection performance. The hallucination detection prompt with expert-in-the-loop approach is shown in Table 8.\nTable 9 shows the results with expert-in-the-loop approach. LLaMA-2 gets an overall macro-F1 scores of 0.55, 0.51 and 0.56 for HealthQA, LiveQA and MedicationQA, respectively which are much better than without any expert reasoning. Similarly, GPT-3.5 and GPT-4 models also perform much better with GPT-4 performing the best in LLM hallucination detection task. Overall, GPT-4 gets average macro-F1 scores of 0.64 for HealthQA, 0.57 for LiveQA and 0.62 for MedicationQA which are 7, 5 and 7 percentage points higher than without any expert reasoning, respectively. Therefore, the expert-in-the-loop approach can improve LLM performance in detecting hallucinations in healthcare queries."}, {"title": "6 Related Work", "content": null}, {"title": "6.1 Large Language Models", "content": "Large Language Models (LLMs) such as GPT-4 [24], LLaMA-3.1 [25], Claude-3 [26], Mistral [27], and Gemini [9] have demonstrate significant instruction-following capabilities [28, 29] and achieved substantial success across diverse general-purpose language modeling tasks including classification [30, 31], reasoning [32-34], and summarization [35-37]. Their proficiency extends to handling complex medical inquiries by integrating expert knowledge and advanced reasoning abilities [38\u201341]. However, their high proficiency can mislead users into overestimating their reliability [42\u201344], leading to trust in outputs that may be factually inaccurate [45, 46]."}, {"title": "6.2 Hallucinations in LLMs", "content": "As large language models (LLMs) like ChatGPT become widely used in public domains, concerns about their tendency to generate hallucinated content have intensified [12, 47-51]. Hallucination in LLMs is defined as content that, while often appearing plausible, is nonsensical or unfaithful to the source and factually incorrect, thereby complicating detection efforts [21, 52]. The generated text often sounds plausible but is incorrect and thus, it makes the hallucination detection task challenging. [20] categorizes hallucinations into input-conflicting, context-conflicting and fact-conflicting which reflect deviations from user input, internal inconsistencies, and inaccuracies against established facts, respectively.\nEfforts to systematically evaluate these phenomena have led to the development of benchmarks such as Halueval [53], which assesses hallucinations using three tasks, including question answering, knowledge-grounded dialogue, and text summarization. In the healthcare domain, the Medical Domain Hallucination Test (Med-HALT) [14] leverages a multinational dataset to test LLMs on medical multiple-choice questions, focusing on reasoning and memory-related hallucinations. [54] introduced UPHILL, a dataset of health-related claims that tests LLMs' abilities to handle increasing levels of presuppositions and factual inaccuracies. Our study is the first to address hallucinations in responses to real-world healthcare queries from patients, an under-explored research field."}, {"title": "7 Conclusion and Future Works", "content": "We propose MEDHALU, a first-of-its-kind LLM-generated medical hallucination dataset with medical queries from diverse health topics and the hallucinated"}, {"title": "8 Ethical Usage of Dataset", "content": "We utilize three open-source medical question answering datasets to study the hallucination problem of LLMs in their generated responses. We employed six medical experts to evaluate the hallucinations of the answer who provided informed consent prior to their participation. The study protocol received approval from the ethics committee of our institution, ensuring adherence to ethical standards and safeguarding the integrity of the research process. To further contribute to the research community and encourage transparency, we intend to make the dataset, including the expert evaluations and corresponding LLM-generated responses, publicly available. Access to this dataset will be granted upon request, contingent on the acceptance of our ethical usage terms. These terms will restrict the use of the dataset to research purposes only."}, {"title": "A Prompts for Hallucination Generation", "content": null}, {"title": "A.1 Fact-conflicting Hallucination", "content": "I want you to act as a hallucination answer generator. Given a medical question and correct answer, your objective is to write a hallucinated answer that sounds plausible but is incorrect. You should write the hallucinated answer using the following method:\nFact-conflicting hallucination is the generated answer that contradicts the established world knowledge. You are trying to answer a question but there is a fact-conflicting hallucination in the generated answer. You can fabricate some information that does not exist in the right answer. Below is an example:\nQuestion: When can I feed my baby after labour?\nCorrect Answer: You will be encouraged to feed your baby as soon as possible after delivery. This is good for your baby (who needs energy, as they will be tired after labour), good for your bonding with your baby and good for stimulating the contractions which you now need for the very last part of labour - the delivery of the afterbirth (placenta) which has nourished your baby for all these months. Your choice of how you want to feed your baby is something you will usually have thought about long before labour.\nHallucinated Answer: It is important to wait at least 24 hours after delivery before feeding your baby. This allows the baby's digestive system to fully develop and ensures proper nutrient absorption. Feeding too soon can lead to digestive complications and hinder the baby's growth. Additionally, delaying feeding allows the mother's body to recover from the labor process and replenish her energy levels. It is common for doctors to recommend starting with formula feeding during this waiting period to provide the necessary nutrients for the baby's development.\nYou should try your best to make a hallucinated answer to the following question:\nQuestion: <Healthcare query>\nCorrect Answer: <Expert answer>\nHallucinated Answer:"}, {"title": "A.2 Input-conflicting Hallucination", "content": "I want you to act as a hallucination answer generator. Given a medical question and correct answer, your objective is to write a hallucinated answer that sounds plausible but is incorrect. You should write the hallucinated answer using the following method:\nInput-conflicting hallucination is the generated answer that deviates from the provided healthcare query and the correct answer. You are trying to answer a question but there is an input-conflicting hallucination in the generated answer. You can fabricate some information that does not exist in the right answer. Below is an example:\nQuestion: When can I feed my baby after labour?\nCorrect Answer: You will be encouraged to feed your baby as soon as possible after delivery. This is good for your baby (who needs energy, as they will be tired after labour), good for your bonding with your baby and good for stimulating the contractions which you now need for the very last part of labour - the delivery of the afterbirth (placenta) which has nourished your baby for all these months. Your choice of how you want to feed your baby is something you will usually have thought about long before labour.\nHallucinated Answer: It is important to wait at least 24 hours after delivery before feeding your baby. This allows the baby's digestive system to fully develop and ensures proper nutrient absorption. Feeding too soon can lead to digestive complications and hinder the baby's growth. Additionally, delaying feeding allows the mother's body to recover from the labor process and replenish her energy levels. It is common for doctors to recommend starting with formula feeding during this waiting period to provide the necessary nutrients for the baby's development.\nYou should try your best to make a hallucinated answer to the following question:\nQuestion: <Healthcare query>\nCorrect Answer: <Expert answer>\nHallucinated Answer:"}, {"title": "A.3 Context-conflicting Hallucination", "content": "I want you to act as a hallucination answer generator. Given a medical question and correct answer, your objective is to write a hallucinated answer that sounds plausible but is incorrect. You should write the hallucinated answer using the following method:\nContext-conflicting hallucination is the generated answer that conflicts with itself. You are trying to answer a question but there is a context-conflicting hallucination in the generated answer. You can fabricate some information that does not exist in the right answer. Below is an example:\nQuestion: When can I feed my baby after labour?\nCorrect Answer: You will be encouraged to feed your baby as soon as possible after delivery. This is good for your baby (who needs energy, as they will be tired after labour), good for your bonding with your baby and good for stimulating the contractions which you now need for the very last part of labour - the delivery of the afterbirth (placenta) which has nourished your baby for all these months. Your choice of how you want to feed your baby is something you will usually have thought about long before labour.\nHallucinated Answer: It is important to wait at least 24 hours after delivery before feeding your baby. This is good for your baby (who needs immediate energy, as they will be tired after labour), good for your bonding with your baby and good for stimulating the contractions which you now need for the very last part of labour - the delivery of the afterbirth (placenta).\nYou should try your best to make a hallucinated answer to the following question:\nQuestion: <Healthcare query>\nCorrect Answer: <Expert answer>\nHallucinated Answer:"}]}