{"title": "Causal Mean Field Multi-Agent Reinforcement Learning", "authors": ["Hao Ma", "Zhiqiang Pu", "Yi Pan", "Boyin Liu", "Junlong Gao", "Zhenyu Guo"], "abstract": "Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-agent reinforcement learning (MARL) has achieved remarkable success in some challenging tasks. e.g., video games [1], [2]. However, training a large number of agents remains a challenge in MARL. The main reasons are 1) the dimensionality of joint state-action space increases exponentially as agent number increases, and 2) during the training for a single agent, the policies of other agents keep changing, causing the nonstationarity problem, whose severity increases as agent number increases [3]\u2013[5].\nExisting works generally use the centralized training and decentralized execution paradigm to mitigate the scalability problem via mitigating the nonstationarity problem [6]\u2013[9]. Curriculum learning and attention techniques are also used to improve the scalability performance [10], [11]. However, above methods focus mostly on tens of agents. For large-scale multi-agent system (MAS) contains hundreds of agents, studies in game theory [12] and mean-field theory [13], [14] offers a feasible framework to mitigate the scalability problem.\nUnder this framework, [14] propose an algorithm called mean-field Q-learning (MFQ), which replaces joint action in joint Q-function with average action, assuming that the entire agent-wise interactions could be simplified into the mean of local pairwise interactions. That is, MFQ reduces the dimensionality of joint state-action space with a merged agent. However, this approach ignores the importance differences of the pairwise interactions, resulting in the poor robustness. Nevertheless, one of the drawbacks to mean field theory is that it does not properly account for fluctuations when few interactions exist [15] (e.g., the average action may change drastically if there are only two adjacent agents). Ref. [16] attempt to improve the representational ability of the merged agent by assign weight to each pairwise interaction by its attention score. However, the observations of other agents are needed as input, making this method not practical enough in the real world. In addition,\nthe attention score is essentially a correlation in feature space, which seems unconvincing. On the one hand, an agent pays more attention to another agent not simply because of the higher correlation. On the other hand, it may be inevitable that the proximal agents will be assigned high weight just because of the high similarity of their observation.\nIn this paper, we want to discuss a better way to represent the merged agent. We propose an algorithm named causal mean-field Q-learning (CMFQ) to address the shortcoming of MFQ in robustness via causal inference. Research in psychology reveals that humans have a sense of the logic of intervention and will employ it in a decision-making context [17]. This suggests that by allowing agents to intervene in the framework of mean-field reinforcement learning (MFRL), they could have the capacity to identify more essential interactions as humans do. Inspired by this insight, we assume that different pairwise interactions should be assigned different weights, and the weights could be obtained via intervening. We introduce a structural causal model (SCM) that represents the invariant causal structure of decision-making in MFRL. We intervene on the SCM such that the corresponding effect of specific pairwise interaction can be presented by comparing the difference before and after the intervention. Intuitively, the intervening enable agents to ask \"what if the merged agent was replaced with an adjacent agent\" as illustrated in Fig.1. In practice, the pairwise interactions could be embodied as actions taken between two agents, therefore the intervention also performs on the action in this case."}, {"title": "II. RELATED WORK", "content": "The scalability problem has been widely investigated in current literatures. Ref. [14] propose the framework of MFRL that increases scalability by reducing the action-state space. Several works in a related area named mean-field game also proves that using a compact representation to characterize population information helps solve scalability problem [18], [19].\nSeveral works were proposed to improve MFQ. Ref. [20] proposed a weighted mean-field assigning different weights to neighbor actions according to the correlations of the hand-craft agent attribute set, which is difficult to generalize to different environments. Ref. [16] calculate the weights with attention score. The observations of other agents are needed to calculate the attention scores, making its practicality not satisfactory.\nOur work is also closely related to recent development in causal inference. Researches indicate that once the SCM, which implicitly contains the causal relationships between variables, is constructed, we can obtain the causal effect by intervening. The causal inference has already been exploited for communication pruning [21], solving credit assignment problem [7], [22], demonstrating the potential of causal inference in reinforcement learning [23]\u2013[25]. Ref. [26] and [27] further proved that SCM could be equally replaced with NCM under certain constraints, enabling us to ask \"what if\" by directly intervening on neural network."}, {"title": "III. PRELIMINARY", "content": "This section discusses the concepts of the stochastic game, mean-field reinforcement learning, and causal inference.\n#### A. Stochastic Game\nA N-player stochastic game could be formalized as G=<\nS,A,P,r,N,\u03b3>, in which N agents in the environment take action a \u2208 A = \u00d7i=1NAi to interact with other agents and the environment. Environment will transfer according to the transition probability P(s' | s,a) : S \u00d7 A \u00d7 S \u2192 [0, 1], then every agent obtains its reward ri(s,ai) : S \u00d7 Ai \u2192 R and \u03b3\u2208 [0,1] is the discount factor. Agent makes decision according to its policy \u03c0i(s) : S \u2192 \u03a9(Ai), where \u03a9(Ai) is a probability distribution over agent i's action space Ai."}, {"title": "IV. METHOD", "content": "#### A. Counterfactual Policy\nTo answer \"what if\" questions raised in Introduction(I), counterfactual inference need to be performed on the policy of central agent. For ease of understanding, we construct an SCM which reveals relations among all variables of interest. In the setting of MFRL, mean action \u0101\u2212i and state st determine the policy \u03c0i(\u00b7|st,\u0101\u2212i) of agent i. As the key relation we concern is how the merged interaction affects \u03c0i, the SCM is constructed center on \u03c0i as illustrated in Fig.3(b). Note that the SCM is derived from the definitions in stochastic game\nZ := fz(Uz)\nX := fx(Z, Ux)\nY:= fy(X, Uy)\nUz, Ux, Uy~N (0,1)\nSt~P(St-1, at-1)\nai~\u03c0i(\u00b7 | st,\u0101i-1)\n\u0101i-1 = \u2211k\u2208N(i) \u0101ki-1\nai-1~\u03c0i(\u00b7 | St-1,\u0101i-2)\nand MFRL. Formally, the causal effect of acting aiki on \u03c0i is qualified as follow.\n$TE^{i,k}=KL(\\pi(\\cdot|s_t, a^i, \\bar{a}^{-i}_{\\backslash i,k}), \\pi^i(\\cdot|s_t,a^i,do(\\bar{a}^{-i}=a^{i,k})))$ (7)\nwhere aiki is the action of the kth agent in the neighborhood of agent i. For unknown distributions, the causal effects are quantified using the difference in statistics before and after the intervention as Eq.(5) and Eq.(6). As the policies in Eq.(7) are known, we can utilize the KL divergence to quantify causal effects, because the essential idea of treatment effect is to measure the change in distribution after do-calculus. \u03c0i(\u00b7|st,ai,do(\u0101\u2212i= aki)) is the counterfactual policy. We could distinguish nontrivial interactions according to their causal effects. Because a large KL divergence means that the preferred action in the policy of plain average merged agent could be a bad choice in the counterfactual policy, which implies a large potential threat of this interaction.\nIt is worth noting that not all neural networks are capable of causal inference [26]. As a neural network learned by interacting with the environment, \u03c0i lies on the second layer of Pearl Causal Hierarchy [29], and naturally contains both the causality between agent-wise interaction and the causality between agent-environment interaction. It is sufficient for estimating the causal effect of certain interaction.\n#### B. Improving MFQ with Causal Effect\nIn MFRL, we assume that different pairwise Q-functions should be assigned different weights depending on their po-tential influences on the policy of central agent. Hence, the factorization of Eq.(2) should be revised to\n$Q^i (s,a^i,a^{i,1},...,a^{i,N(i)}) = \\sum_{k \\in N(i)} w^{i,k}Q^i(s,a^i,a^{i,k})$ (8)\nwhere N(i) is the set of agent i's adjacent agents. For simplicity, we denote Q(s,ai,aiki) as Qiaik, Qi(s,ai,aiki) as Qiaik, and Q(s,ai,\u0101i) as Qi. Then Qi(s,ai,a1,...,aN) is approximated using mean-field theory and considering the causality-aware weights\n$Q^i (s,a^i,a^{i,1},...,a^{i,N(i)}) = \\sum_{k \\in N(i)} w^{i,k}Q^i(s,a^i,\\bar{a}^{-i})$\n$=\\bar{Q}^i (s, a^i, \\bar{a}^{-i}) + \\sum_{k\\in N(i)} w^{i,k} \\delta \\bar{a}^{i,k} + \\lambda \\delta \\bar{a}^i \\delta \\bar{a}^{-i}$ (9)\n$=\\bar{Q}+ \\sum_{k \\in N(i)} w^{i,k} [\\nabla Q \\sum_{k \\in N(i)} ] a^{i,k} (a^{i,k}- a^i)\n$\\approx \\bar{Q}+ \\sum_{k \\in N(i)} w^{i,k} [R_{s,a^i} (\\delta a^{i,k})$\nwhere \\delta a^{i,k}=a^{i,k} - a^i and \\delta a^{i,k}=\\sum_{k \\in N(i)}w^{i,k} a^{i,k}, hence \\sum_{k\\in N(i)} w^{i,k}\\delta a^{i,k} = 0. In the second-order term, aiki = a2 + \u03b5ikai,k, \u03b5ik \u2208 (0,1). Rsi,ai(aiki) denotes the first-order Taylor expansion's Lagrange remainder which is bounded by [-L,L] in the condition that the Qi(s,ai,aiki) function is L-smoothed. The remainder is a value fluctuating around zero. As [14] discussed in their work, under the assumption that fluctuations caused by adjacent agents tend to cancel each other, the remainder could be neglected.\nOnce causal effects of pairwise interactions are known, the next question is how to to improve the representational capacity of the merged agent. Both linear methods, e.g., weighted sum, or nonlinear methods, e.g., encoding with a neural network, might be useful. However, to ensure the merged agent's reasonability, we prefer a representation in the linear space formed by adjacent agents' action vectors. An intuitive method that can induce reasonable output is a weighted sum. In practice, we find that weighted sum using respective causal effects as weight is enough to effectively improve the representational capacity of average action\n$\\pi^i (a^i |s_t, \\bar{a}^{-i})=\\frac{w^{i,k} exp( \\beta \\bar{Q}^i(s_t, a^i, \\bar{a}^{-i})}{\\sum_{a^i \\in A^i} exp (\\beta \\bar{Q}^i(s_t, a^i, \\bar{a}^{-i}))}$ (10)\n$\\bar{a}^{-i}=\\sum_{k \\in N(i)}w^{i,k} TE^{i,k}$ (11)\n$w^{i,k} = \\frac{ \\frac{TE^{i,k}}{\\sum_k TE^{i,k} + \\epsilon}}{ \\sum_{k\\in N(i)} (\\frac{TE^{i,k}}{\\sum_k TE^{i,k} + \\epsilon}}$\nwhere subscripts are used to denote time steps. $TE^{i,k}$ is calculated according to Eq.(7). Each aiki is encoded in one hot vector. Hence the weighted sum returns a reasonable representation in the linear space formed by the actions of neighborhoods. Moreover, the representation is close to essential actions, emphasizing high-potential impact interactions. A term \u03b5 was introduced to smooth the weight distribution across all adjacent agents, avoiding additional nonstationarity during training. Besides, the naive mean-field approximation could be achieved when \u03b5 \u2192 \u221e."}, {"title": "V. EXPERIMENTS", "content": "We evaluate CMFQ in two tasks: a mixed cooperative-competitive battle game and a cooperative predator-prey game. In the battle task, we compare CMFQ with independent Q-learning (IQL) [30], MFQ [14], and Attention-MFQ [16] to investigate the effectiveness and scaling capacity of CMFQ. We further verify the effectiveness of CMFQ in another task. In the predator-prey task, we compare CMFQ with MFQ and Attention-MFQ. Our experiment environment is MAgent [31].\n#### A. Mixed cooperative-competitive game\nTask Setting. In this task, agents are separated into two groups, each containing N agents. Every agent tries to survive and annihilate the other group. Ultimately the team with more agents surviving wins. Each agent obtains partial observation of the environment and knows the last actions other agents took. Agents will be punished when moving and attacking to lead agents to act efficiently. Agents are punished when dead and only rewarded when killing the enemy. The reward setting requires the agent to cooperate efficiently with teammates to annihilate enemies. In the experiments, we train CMFQ, IQL, MFQ, and Attention-MFQ in the setting of N = 64, then we change N from 64 to 400 to investigate the scalability of CMFQ. The concrete reward values are set as follow: rattack = -0.1, rmove = -0.005, rdead = -0.1, rkill = 5. We train every algorithm in self-play paradigm.\nQuantitative Results and Analysis. As illustrated in Fig.4(b), we compare CMFQ with Attention-MFQ, MFQ, and IQL. We do not choose [20] as a baseline because it is a correlation-based algorithm identical to Attention-MFQ. We assume that the attention-based method is a more challeng-ing baseline. Moreover, in addition to these algorithms, we also set ablation algorithms named Random to verify that the performance improvement of CMFQ is not caused by randomization. Random follows the same pipeline as CMFQ but returns a random causal effect for each interaction. Fig.4(a) shows the learning curve of all algorithms. We can see that the total rewards of all algorithms converge to a stable value, empirically demonstrating the training scalability of our algorithm.\nTo compare the performance of each algorithm, we put trained algorithms in the test environment that N = 64, and let them battle against each other. Fig.4(b) shows that MFQ performs better than IQL but worse than Attention-MFQ, in-dicating that the mean-field approximation mitigates the scal-ability problem in this task. However, the simply averaging as MFQ is not a good representation of the population behavioral information. In order to improve its representational ability for large-scale scenarios, it is necessary to assign different weights to different agents. Moreover, CMFQ outperforms Attention-MFQ during the test, verifying the correctness of our hypoth-esis that correlation-based weighting is insufficient to catch the essential interactions properly, while the intervention fills this gap by giving agents the ability to ask the counterfactual question about \u201cwhat if\u201d.\nWe further investigate the test scalability of CMFQ, MFQ, and Attention-MFQ. Firstly, we train these three algorithms in 64 vs. 64 scenario with self-play, denoted as CMFQ64, MFQ64, Attention-MFQ64 respectively, and further train the MFQ algorithm in 100 vs. 100 and 400 vs. 400 scenarios, de-noted as MFQ100 and MFQ400. Then, allow CMFQ64, MFQ64, and Attention-MFQ64 to battle against MFQ64, MFQ100 and MFQ400 in environments 64 vs. 64, 100 vs. 100, 400 vs. 400 respectively, that is, letting CMFQ, MFQ, and Attention-MFQ control more agents than they were trained, to reveal the test scalability of the algorithms. As shown in Fig.4(c), the test scalability of MFQ is the worst, which means that we need to retrain MFQ when the number of agents increases and the test scalability of Attention-MFQ is slightly better. The test scalability of CMFQ is significantly better than both of them. Furthermore, CMFQ achieves win rates of nearly 100% against MFQ100 and 32% against MFQ400.\nAblations. We set two ablation experiments. The first one to ablate the effectiveness of causal effects in CMFQ. As illustrated in Fig.4(b), the performance of Random is inferior to MFQ, verifying the validity of causal effect in CMFQ. The other one is ablation for \u03b5. As we analyze in IV-B, \u03b5 is an adjustable parameter in the interval [0,+\u221e]. As \u03b5 increases, the effect of each interaction becomes smoother and eventually CMFQ equal to MFQ when \u03b5 \u2192 +\u221e. From the Fig.4(d), we can see that as we adjust \u03b5 from 0.001 to 1, the learning curve of CMFQ always converges, and in the test environment, win rates of CMFQ always outperform other baselines. When \u03b5 is relatively large, the win rate is close to that of MFQ.\nVisualization Analysis. As illustrated in Fig.5(a), CMFQ learns the tactic of besieging, while MFQ tends to confront frontally. The results in Fig.5(b) indicate the tricky issue in mixed cooperative-competitive game: agents need to cooperate with their teammates to kill enemies, whereas only the agent who hits a fatal attack gets the biggest reward rkill, driving agents hesitating to attack first. When there are few agents, the policies of MFQ and CMFQ tend to be conservative. However, CMFQ presents more advanced tactics: agents learn the trick of teaming up in the mixed cooperative-competitive game. When an agent chooses to attack, the adjacent teammates will arrive to help, achieving the maximum reward with the smallest cost of health. Moreover, Fig.5(b) also shows that attacks of CMFQ are more focused than baselines. CMFQ can discriminate key interactions and have a more accurate timing of attacks, while MFQ lacks this discriminatory ability and thus keeps attacking."}, {"title": "B. Cooperative game", "content": "Task Setting. In this task, agents are divided into predator and prey. Prey move 1.5 times faster than predators, and their task is to avoid predators as much as possible. Predators are four times larger than prey and can attack but not yield any damage. Predators only get rewarded when they are close to prey. Therefore, to gain the reward, they must cooperate with other predators and try to surround prey with their size advantage. In our experiments, to test the scalability of the CMFQ, we first train MFQ, CMFQ, and Attention-MFQ employing the self-play paradigm in a scenario involving 20 predators and 40 prey, and then test them in environments involving (20 predators, 40 prey), (80 predators, 160 prey), (180 predators, 360 prey) respectively. The reward are set as follow: rattack = -0.2,rsurround = 1,rbe_surrounded = -1.\nQuantitative Results and Analysis. We compare CMFQ with MFQ and Attention-MFQ. First, we investigate their training scalability in (20 predators, 40 prey), as shown in Fig.6(a) and Fig.6(b), all of them converge to a stable reward total reward, verifying their training scalability. Then, we enlarge the number of agents during execution to investigate their test scalability. To demonstrate the scalability gap of different algorithms, we allow the algorithms to execute in an adversarial form, which means that one algorithm controls the predator and another controls the prey. For the environment, we change the number of agents to 1x, 4x, and 9x of the number in the training environment.\nBecause the reward rbe_surrounded of prey and the reward rsurround of predator are zero-sum and cooperation exists mainly among predators, we use the total reward of predators to indicate each algorithm's performance. The results are shown in Fig.7. Total rewards in specific environment indicate the train scalability, since a higher total reward means agents learn better policy during training. Trends of lines are related to test scalability, and a more flat line indicates the better test scalability of the algorithm. We can see that the total reward of Attention-MFQ is higher than that of MFQ, and the trend is similar to that of MFQ. In comparison, the total reward of CMFQ is higher than that of both MFQ and Attention- MFQ, and the trend is ever more flat, indicating that CMFQ has better scalability.\nVisualization Analysis. The results that the trained CMFQ and Attention-MFQ controls predators are shown in Fig.8. In the the environment that Npredator=20, Nprey=40, both CMFQ and Attention-MFQ perform similarly. Predators learn two strategies: four predators cooperating to surround the prey in an open area; two or three predators surrounding the prey with the help of obstacles. In the environment that Npredator=40, Nprey=80, when the number of agents increases, predators controlled by Attention-MFQ are more dispersed than predators controlled by CMFQ. Besides, Attention-MFQ has more predators idle than CMFQ. Predators controlled by CMFQ gather on map edges, because it is more efficient to surround prey with the help of map edges. In addition, predators controlled by CMFQ learn an advanced strategy to drive prey to map edges then take advantage of the terrain to surround them. In the environment that Npredator=180, Nprey=360, the advanced strategy is also presented. Moreover, predators controlled by CMFQ master the skill to utilize the bodies of still teammates who have captured prey as obstacles. Thus, predators controlled by CMFQ present a high degree of aggregation and environmental adaptability."}, {"title": "VI. CONCLUSIONS AND DISCUSSIONS", "content": "This paper aims at scalability problem in large-scale MAS. Firstly, We inherit the framework of MFRL which significantly reduce the dimensionality of joint state-action space. To further handle the intractable nonstationarity when the number of agent is large, we propose an SCM to model the decision-making process, and enable agents to identify the more crucial interactions via intervening on the SCM. Finally a causality-aware representation of population behavioral information could be obtained by the weighted sum of the action of each agent according to its causal effect. Experiments in two tasks reveal the excellent scalability of CMFQ.\nBroader impact. CMFQ comprehensively alleviating the scalability problem. This brings very practical benefits: In environments where the observed dimension does not change with the number of agents, multiplying the number of agents will no longer force us to retrain the model, thanks to the robustness of CMFQ. Besides, we can train our models in simpler environments and use them in more complex environ-ments to reduce the training overhead.\nLimitation and future work. Despite the significant improvement that CMFQ brings to the robustness of MFQ, we contend that there is still much to explore in the causal inference module itself. Specifically, we question what other do-calculus techniques may be feasible beyond replacing the average action with a specific action. We leave this exploration as future work to develop more robust and interpretable algorithms."}, {"title": "APPENDIX", "content": "The pseudocode of CMFQ is listed below.\nAlgorithm 1 Causal Mean Field Q-learning\nInput: Initialize state s0; Q0Q0, a for all agent i \u2208 {1,2,...,N}; trajectory length M;\nwhile in the training loop do\nfor t = 0,1,...,M do\nfor i = 1,2,...,N do\nCalculate policy \u03c0i(\u00b7 | sta-i) with average merged agent;\nCalculate causal effect for every neighborhood agent by Eq.(7);\nObtain a new merged agent \u0101\u2212i and a new policy \u03c0i(\u00b7 | st, \u0101\u2212i) by Eq.(10);\nend for\nSample joint action a = [a1,a2,\u2026\u2026\u2026,aN] from [\u03c01,\u03c02,\u2026\u2026\u2026, \u03c0N]\nobtain the next state st+1 and the reward r = [r1,r2,\u2026\u2026\u2026,rN] and merged agent \u0101 = [\u01011\u2212i,\u01012\u2212i,\u00a8\u00a8\u00a8 ,\u0101Ni]\nStore transition <st, a, r, st+1,\u0101 > in replay buffer;\nend for\nfor i = 1,2,..., N do\nSample a minibatch transition from replay buffer;\nCalculate Li and update \u03b8i by Eq.(12);\nUpdata target network by \u03b8i = \u03b8i; after every C updates of \u03b8i;\nend for\nend while\nAs s,ai in Qi(s,ai,aiki) are fixed parameter in the derivation of Eq.(9), for simplicity, the pairwise Q-function Qi(s,ai,aiki) can be rewrite as Q(aiki) in the following. We assume that aiki is a one-hot encoding for n actions, to make Q(aiki) more general, we replace the discrete aiki (aiki \u2208 RN) by a continuous x (x \u2208 RN) which don't violate the domain of the parameter-ized Q-function. Given the Q(x) is L-smooth, then for any two points x, y \u2208 dom (Q) \u2286 RN, there exists a Lipschitz constant L\u2208 [0,+\u221e) that\n||\u2207Q(x) \u2013\u2207Q(y) ||2 <L||x-y||2 (14)\nBy the first order Taylor expansion with Lagrange remain-der, we have\n\u2207Q(y) = \u2207Q(x) +\u22072Q(x)\u00b7u+R(u) (15)\nwhere u = y - x, limu\u21920 Ru = 0. Assume x \u2260 y, then we can reform the first order Taylor expansion\n\u2212||\u22072Q(x)\u22c5u||2 ||\u2207Q(y) \u2013 VQ(x) \u2013 R(u) ||2\n||u||2 < ||u||2\n||\u2207Q(y) \u2013 VQ(x)||2 ||R(u)||2\n|\u2212 \u2212+\n||u||2 (16)\n\n0\n\u2200x, y \u2208 dom(Q),x \u2260 y"}]}