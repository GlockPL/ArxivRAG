{"title": "Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses", "authors": ["Haojun Yulst", "Youcheng Lilst", "Nan Zhang", "Zihan Niu", "Xuantong Gong", "Yanwen Luo", "Quanlin Wu", "Wangyan Qin", "Mengyuan Zhou", "Jie Han", "Jia Tao", "Ziwei Zhao", "Di Dai", "Di Hel", "Dong Wang", "Binghui Tang", "Ling Huo", "Qingli Zhu", "Yong Wang", "Liwei Wang"], "abstract": "Data-driven deep learning models have shown great capabilities to assist radiologists in breast ultra- sound (US) diagnoses. However, their effectiveness is limited by the long-tail distribution of training data, which leads to inaccuracies in rare cases. In this study, we address a long-standing challenge of improving the diagnostic model performance on rare cases using long-tailed data. Specifically, we introduce a pipeline, TAILOR, that builds a knowledge-driven generative model to produce tailored synthetic data. The generative model, using 3,749 lesions as source data, can generate millions of breast-US images, especially for error-prone rare cases. The generated data can be further used to build a diagnostic model for accurate and interpretable diagnoses. In the prospective external eval- uation, our diagnostic model outperforms the average performance of nine radiologists by 33.5% in specificity with the same sensitivity, improving their performance by providing predictions with an interpretable decision-making process. Moreover, on ductal carcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by a large margin, with only 34 DCIS lesions in the source data. We believe that TAILOR can potentially be extended to various diseases and imaging modalities.", "sections": [{"title": "1 Main", "content": "Breast cancer has become the most common cancer among women globally [1-3], and early detection can significantly decrease the mortality rates [4]. In breast cancer detection, ultrasound (US) is an essential imaging method widely adopted worldwide for its safety and low cost [5-7]. Accurately interpreting breast-US findings poses a great challenge [8] as it requires radiological knowledge to comprehensively analyze clinically relevant features [5] such as margin characteristics, echo patterns, shape, and calcifications. Data-driven deep learning models provide a promising solution for accurate breast-US diagnoses [9-11]. However, the collected training data [9- 13] is often limited and inherently exhibits a long- tail distribution of pathological subtypes [14-17], as shown in Fig. 1. When learning from such limited and imbalanced data, the models tend to primar- ily focus on predicting the head categories correctly, making it more likely to produce wrong predictions for rare categories [18]. Moreover, rare categories can be error-prone for radiologists, particularly requiring AI assistance. Notably, specifically collecting sufficient tail data can be extremely costly due to the rarity of these cases, not to mention the issues associated with medical data collection, such as privacy concerns, high costs, and legal risks.\nRecent advances in generative models [19-23] have made it possible to produce realistic and diverse content according to the input instructions or conditions. Moreover, these models demonstrate notable transfer- ability: with only a small amount of domain-specific data, they can be efficiently fine-tuned to generate high-quality outputs tailored to targeted scenarios [20, 24, 25]. Given these successes, we propose TAILOR, a pipeline that trains an accurate and interpretable diagnostic model (TAILOR-Diag) with the help of a knowledge-driven generative model (TAILOR-Gen), as illustrated in Fig. 2.\nTo briefly introduce, we first train a diffusion generative model, TAILOR-Gen, to generate knowledge- conditioned images. Besides knowledge of benign and malignant pathology, we incorporate critical domain knowledge including various information, such as rare pathological subtypes, error-prone US features, and visual imaging appearances, which we observe have limited diversity in the training data. The annotations for the knowledge information come from pathology results, US reports, or expert opinions. By incorporating proper knowledge, the model can learn and generate images conditioned on more contexts, significantly improving the quality of the generated images, especially for rare categories. With the trained TAILOR- Gen, we generate large-scale, diverse, and realistic data, and build the diagnostic model TAILOR-Diag using the synthetic dataset (Fig. 2a). In particular, we design TAILOR-Diag as an ensemble of multiple classifiers that adaptively leverage appropriate knowl- edge to accurately classify the benign and malignant pathology. Therefore, the decision-making process of the model is interpretable and understandable for human users [26].\nExtensive results demonstrate that TAILOR facilitates accurate and interpretable breast-US diagnoses. In terms of accuracy, TAILOR-Diag (AUC=0.954, 95% Confidence Interval (CI) 0.932-0.983) outperforms the baseline trained on real data (AUC=0.909, 95% CI 0.867-0.947) on the external test set, sig- nificantly improving the performance to exceed the average performance of nine board-certified breast-US radiologists by 33.5% (95% CI 23.2-44.1%) in specificity with the same sensitivity. Moreover, in diagnoses of ductal carcinoma in situ (DCIS), an error-prone subtype of early-stage cancer, TAILOR-Diag outperforms all nine radiologists by a large margin, with only 34 DCIS cases in the source data. In terms of inter- pretability, we investigate whether the assistance of TAILOR-Diag can improve radiologists' performance in real clinical settings. Notably, the average perfor- mance of nine radiologists improves by 6.4% (95% CI 3.8-8.9%) in specificity without loss of average sensi- tivity. These impressive results demonstrate that our proposed pipeline, TAILOR, can effectively learn crit- ical knowledge from a small amount of domain-specific data, which has the potential to be extended to various diseases and imaging modalities."}, {"title": "2 Results", "content": null}, {"title": "2.1 Datasets", "content": "In this work, we conducted a multi-centre study with US images of breast lesions recruited from four institutions in China (Fig. 2b). The involved institutions enable us to collect data from representative patient populations, detailed in Supplementary Section 1.1. For training and internal evaluation, we retrospectively collected scanning videos of 3,422 patients with 4,328 lesions from two internal institutions and split the internal dataset by patients. The training set consisted of 3,749 lesions (1,387 biopsy-confirmed lesions),"}, {"title": "2.2 Knowledge-driven generative model", "content": "We seek to develop an accurate and interpretable deep learning model for breast-US diagnoses. To achieve this goal, we propose to augment the limited and long-tailed data using a knowledge-driven generative model called TAILOR-Gen. Specifically, TAILOR- Gen targets learning both the basic knowledge and the pathology-specific knowledge under expert super- vision. Basic knowledge is useful for enhancing the diversity of visual appearance. More importantly, the pathology-specific knowledge is critical for accurate diagnoses.\nWe define the basic knowledge as the visual appearances of factors not strongly correlated with lesion pathology, varying across different patients and clinical situations. Diagnostic models might learn incorrect correlations between these factors and lesion pathology when trained on a dataset with limited diversity in visual appearances, which poses challenges in model generalization to different clinical situations. Here, we explore the basic knowledge of lesion area and device type to enrich the data diversity. The lesion area refers to the relative position and scale of lesions on US screens which could vary as radiologists adjust them for different diagnostic purposes. Additionally, device types can introduce variations in image quality, texture, or color bias. More details are provided in Supplementary Figure 3 and Supplementary Figure 4.\nThe pathology-specific knowledge establishes the connections between US features and lesion pathology, thus being critical for accurate diagnoses. However, generative models trained directly on binary pathology labels tend to learn knowledge from head cat- egories. In this study, TAILOR-Gen is designed to learn the pathology-specific knowledge for both head and tail categories. To identify underrepresented tail categories, we investigate the US features and the pathological subtypes. First, we investigate US fea- tures, defined in the American College of Radiology published Breast Imaging Reporting and Data System (BI-RADS) lexicon guidelines [5]. In clinical practice, US features are evaluated by radiologists based on US images and their experience. Different US features can indicate different probabilities of malignancy. Here, we first explore two critical US features.\n\u2022 Not circumscribed margins (NCM) refer to the unclear boundary between lesions and surrounding tissues. NCM often suggests malignant breast can- cer, while some rare benign lesions can also exhibit NCM [27-30], such as radial scar and mastitis.\n\u2022 Microcalcifications in a mass (CAL) are calcium deposits < 0.5 mm in diameter embedded in a mass, recognized as small hyperechoic foci in US images. CAL often appears in breast cancer, while some- times they can also be found in benign lesions [31- 34].\nThus, benign lesions with US features of NCM or CAL are two tail categories that can be challenging in clinical practice. Second, for pathological subtypes, we reference the taxonomy defined in the WHO classification [17] and other professional books on pathology [35, 36]. In clinical practice, pathologi- cal subtypes are determined by surgery or biopsy, reflecting cellular-level lesion structures. Note that the pathological subtypes can not be determined by radi- ologists directly from US images. Here, we investigate an error-prone pathological subtype that is critical in the early detection of breast cancer.\n\u2022 Ductal carcinoma in situ (DCIS) is a non-invasive early-stage pathological subtype where all can- cer cells are confined within the basement mem- brane [37-39]. DCIS lacks typical malignant fea- tures of invasive cancer and sometimes exhibits non-mass lesions or nodules with regular shape or circumscribed margins. These features may be associated with benign findings in clinical practice.\nWe train a generative model, TAILOR-Gen, to learn the aforementioned knowledge, enabling it to produce realistic and diverse data that encompasses this knowledge. Specifically, TAILOR-Gen is designed as a conditional Denoising Diffusion Probabilistic Model (DDPM) [19, 40, 41] that can produce images according to input conditions. First, we pre-train TAILOR-Gen on the entire training set conditioned on the benign or malignant pathology labels, enabling it to generate images based on pathology conditions. Therefore, these pathology conditions can be used as pseudo-labels to train diagnostic models. With"}, {"title": "2.3 Interpretable diagnostic model", "content": "Using TAILOR-Gen, we manage to generate diverse and well-balanced data, which can be used to improve the training of the diagnostic model. Based on the generated data, we train a diagnostic model, TAILOR- Diag, to learn critical domain knowledge for accurate diagnoses. We design TAILOR-Diag as an ensemble of four classification models to diagnose lesions with proper knowledge: a general model primarily for head categories and three expert models for each of the three tail categories. Each classifier consists of a Swin Transformer [42] backbone, and a binary classification head to predict pathology categories.\nTo optimize the general predictive ability of common cases, we pre-train a classification model on the aforementioned 800,000 generated images, called TAILOR-Diag-Base. The generated images offer a broader visual variety than conventional data augmentations, enabling the classifier to better generalize to different clinical situations. After the pre-training fin- ishes, we fine-tune TAILOR-Diag-Base using 100,000 tailored images for each tail category to enhance the specialized predictive ability. These expert mod- els focus on different aspects and provide confidence scores of their predictions, named TAILOR-Diag- NCM, -CAL, and -DCIS respectively. As expert mod- els specifically learn critical knowledge of rare cases,"}, {"title": "2.4 General evaluation", "content": "We first evaluate TAILOR-Diag on the internal test set, which consists of 579 lesions (274 benign and 305 malignant). All lesions have biopsy-confirmed pathology results, called \"gold standard\" labels. To demonstrate the strength of using generated data, we compare TAILOR with the conventional pipeline. In the conventional pipeline, we train a diagnostic model with the same classifier architecture (without the decision-making process) on the collected train- ing set with resampling techniques to re-balance the pathology categories, named real-data-trained base- line. On the internal test set, TAILOR-Diag achieves an area under the receiver operating characteristic curve (AUC) of 0.952 (95% CI 0.934-0.967). For comparison, the real-data-trained baseline only achieves an AUC of 0.925 (95% CI 0.902-0.947, P-value=0.0001). We plot the receiver operating characteristic (ROC) curves of both models in Fig. 5a. These results demonstrate that our TAILOR pipeline facilitates significantly better diagnostic performance than the conventional pipeline.\nTo further evaluate the model's ability, we test it on the datasets from external institutions with various patient populations and imaging protocols. First, we assess the models on the prospective consecutive exter- nal test set consisting of 227 lesions (63 benign and 164 malignant) with \"gold standard\" labels. On this task, TAILOR-Diag achieves an AUC of 0.954 (95% CI 0.932-0.983) while the real-data-trained baseline only achieved an AUC of 0.909 (95% CI 0.867-0.947, P-value=0.0023), as shown in Fig. 5b. Second, we evaluate the trained models on a public Breast Ultra- sound Images (BUSI) dataset [13] collected from an institution in Egypt (437 benign, 210 malignant, and 133 negative lesions) where negative lesions are not used in our evaluation. TAILOR-Diag achieves an AUC of 0.931 (95% CI 0.909-0.950) while the real-data-trained baseline achieved an AUC Of 0.901 (95% CI 0.875-0.925, P-value=0.0001), as shown in Fig. 5c. All these results demonstrate that TAILOR- Diag has great generalization ability, achieving signif- icantly better performance than the real-data-trained baseline."}, {"title": "2.5 Fine-grained evaluation on specific categories", "content": "In this subsection, we give a detailed analysis of the model performance on specific categories. First, we focus on the three investigated error-prone tail cate- gories. For DCIS, we calculate the pathology predic- tion of different models on the DCIS test set consisting of 63 benign lesions (from the external test set) and 140 DCIS lesions (7 DCIS lesions are from the external test set and 133 DCIS lesions are additionally col- lected). As shown in Fig. 5d, TAILOR-Diag achieves an AUC of 0.899 (95% CI 0.852-0.942) while the real- data-trained baseline achieves an AUC of 0.837 (95% CI 0.779-0.890, P-value=0.0026). For NCM, with expert guidance, we annotate 324 lesions with NCM (45 benign and 279 malignant) in the internal test set. As shown in Fig. 5e, on lesions with NCM, TAILOR- Diag achieves an AUC of 0.890 (95% CI 0.835-0.937) while the real-data-trained baseline achieves an AUC of 0.814 (95% CI 0.730-0.882, P-value=0.0008). For CAL, we annotate 103 lesions with CAL (21 benign and 82 malignant) in the internal test set. As shown in Fig. 5f, on lesions with CAL, TAILOR-Diag achieves an AUC of 0.908 (95% CI 0.829-0.970) while the real- data-trained baseline achieves an AUC of 0.812 (95% CI 0.660-0.923, P-value=0.0085). All of the results show the superiority of TAILOR-Diag compared to the conventional approach."}, {"title": "2.6 Reader study", "content": "To further demonstrate the strength of TAILOR- Diag, we conducted a reader study to compare the models with human radiologists and investigate how TAILOR-Diag can assist radiologists in practice. Here, we used the mixed test set with 227 consecutive lesions and the purposely collected 133 DCIS lesions. We invited nine board-certified breast-US radiologists with a range of experience of 3-26 years (11 years on average) to analyze these lesions and provide their predicted BI-RADS scores. Because the distribution of data used in this study differs from that in clini- cal practice, we specifically informed readers that they should independently evaluate each lesion. We calcu- lated the sensitivity and specificity of readers using the BI-RADS 4A as the threshold for determining the binary predictions (BI-RADS 2, 3 as benignity, and BI-RADS 4A+ as malignancy).\nThe reader study consisted of two stages. In the first stage (Stage 1), we provided the B-mode breast- US images to both TAILOR-Diag and the readers and compared their predictions. As shown in Fig. 6a, TAILOR-Diag consistently outperformed nine readers on different BI-RADS thresholds. On the 227 consecu- tive lesions, TAILOR-Diag outperformed the average reader performance by 33.5% (95% CI 23.2-44.1%, P- value=0.0002) in specificity with the same sensitivity of 96.4%, and outperformed the average reader perfor- mance by 3.0% (95% CI 1.4-4.8%, P-value=0.0022) in sensitivity (TAILOR-Diag achieved 99.4% (95% CI 93.1-100%)) with the same specificity of 37.9%. The ROCS of TAILOR-Diag, the real-data-trained base- line, and the results of readers on the external test set are shown in Fig. 6a. For diagnoses of DCIS, TAILOR- Diag outperformed the mean performance of readers by 43.0% (95% CI 31.9-53.6%, P-value<0.0001) in specificity with the same sensitivity of 81.3%, and out- performed the average reader performance by 16.5% (95% CI 11.5-21.4%, P-value<0.0001) in sensitivity with the same specificity of 37.9%. The ROCs of TAILOR-Diag, the real-data-trained baseline, and the results of readers on the DCIS test set are shown in Fig. 6b. These results demonstrate that TAILOR-Diag is more accurate than human radiologists using the same input information of B-mode images.\nThe second stage (Stage 2) evaluated the effective- ness of the TAILOR-Diag's assistance for radiologists in real clinical settings. To mimic practical conditions, besides B-mode images, we further provided readers with patient demographics and color Doppler images, then required readers to re-assess the lesions with this additional information. The results demonstrate that with the help of this information, the average reader performance did not significantly change. The average reader sensitivity (97.9%) improved by 1.5% (95% CI 0.4-2.8%), but the average reader specificity decreased by 1.6% (95% CI -1.6-4.7%) compared to the results in Stage 1. Next, we provided readers with TAILOT-Diag predictions and decision-making processes and required them to re-assess the lesions with AI assistance. The performance gains of each reader in Stage 2 using different BI-RADS scores as thresholds are shown in Fig. 6c and Fig. 6d. With the assistance of TAILOR-Diag, the average reader performance improved by 6.4% (95% CI 3.8-8.9%) in specificity without loss of sensitivity (improved by 0.1%) on the external test set, as shown in Fig. 6e. Moreover, two human radiologists exceeded the perfor- mance of TAILOR-Diag with its assistance. They not only revised their misdiagnoses with the model's hints but also pointed out the model's error based on their analysis of the decision-making processes, proving the notable interpretability of TAILOR-Diag. These results demonstrate that incorporating TAILOR-Diag into the clinical workflow can improve the diagnostic performance of radiologists, especially in specificity, under real clinical settings. More details of the reader study are illustrated in Supplementary Section 4."}, {"title": "3 Discussion", "content": "Data-driven deep learning models have demonstrated significant capabilities in assisting radiologists with diagnosing a wide range of diseases across various imaging modalities [9, 10, 43-48]. The success of these diagnostic models is largely attributed to high-quality datasets that encompass rich domain knowledge essen- tial for clinical diagnoses. However, medical data col- lection faces challenges due to privacy, cost, and legal issues, leading to limitations in source datasets [49- 51]. To address these challenges, most previous works explored the use of generative models as a way for data augmentation [52-64]. In this study, we take a step back to rethink the synthetic data augmen- tation methods and find that incorporating domain knowledge into the synthetic data is more important (Supplementary Section 3.1).\nWe leverage the recent advances in generative models [20, 25] to produce high-quality data for rare lesions using the long-tailed medical dataset. In com- puter vision, techniques of generative models have been developed to address a similar challenge. Previ- ous works demonstrate that a pre-trained conditional generative model can be \"personalized\" to produce photos of a specific person by learning shared knowl- edge from the entire dataset and learning identification knowledge from 3-5 photos [25, 65]. With this insight, we follow the same way and develop a knowledge- driven generative model in the medical domain that learns the basic knowledge from the whole dataset and the pathology-specific knowledge from a few tail-category lesions. Leveraging these capabilities, our synthetic breast-US images demonstrate realism and diversity, proving useful in downstream tasks.\nOur study has the potential for application in prac- tical clinical scenarios. For breast cancer early detec- tion, TAILOR-Diag significantly outperforms human radiologists on DCIS, a critical subtype of early-stage cancer. This makes it suitable for integration into the breast screening workflow. Additionally, TAILOR- Diag can be used to re-evaluate retrospective breast- US examinations. As a high-throughput method, TAILOR-Diag can re-evaluate large-scale preserved breast-US data in hospitals, identifying potential false negatives and prompting further examinations. These improvements can contribute to better treatment out- comes and reduced mortality rates.\nThe proposed TAILOR pipeline offers promising future directions for exploration. First, integrating multi-modal breast-US inputs, such as color Doppler, elastography US, and dynamic video information, could further improve diagnostic performance [10]. Second, besides the three tail categories investigated in this study, TAILOR can be adapted to incorporate domain knowledge for other error-prone categories, potentially further enhancing breast-US diagnostic performance. Finally, we believe that TAILOR can be extended to various diseases and imaging modalities beyond breast-US diagnoses."}, {"title": "4 Methods", "content": null}, {"title": "4.1 Ethical approval", "content": "Our study was approved by the institutional review board of the Peking University Cancer Hospital & Institute (ID: 2024YJZ41). The study was not interventional and was performed under guidelines approved by the institutional review board. Informed consent was waived since the study presents no more than minimal risk. All datasets processed for this research were de-identified before transfer to study investigators."}, {"title": "4.2 Breast-US data acquisition, processing, and annotation", "content": "To conduct the multi-centre study, we collected data from four Grade-3A hospitals in China: Peking Univer- sity Cancer Hospital & Institute (PKUCH), Nanchang People's Hospital (NPH), Peking Union Medical Col- lege Hospital (PUMCH) and Cancer Institute, Chinese Academy of Medical Sciences (CICAMS). We defined two hospitals, PKUCH and NPH, as internal institu- tions where we collected data for training and internal evaluation; and the other two hospitals, PUMCH and CICAMS, were defined as external institutions where we collected data for external evaluation.\nWe collected breast-US scanning videos as the internal dataset and then divided them into a training set and an internal test set. Here, we regarded videos as sequential 2D images, as we used the image gener- ative models. The videos were collected from patients who underwent breast-US examinations at PKUCH and NPH between January 2020 and March 2021. We collected US videos instead of US images pre- served in the standard clinical workflow because videos contained continuous frames in scanning processes, offering more information than discrete images to train generative models. In data processing, we retained B-mode US frames that clearly showed lesions with- out blurring in the lesion-scanning process, excluding frames in the initial lesion-finding process. Follow- ing the standard workflow [10], when multiple lesions were detected in a breast, only the major lesion was included. As detailed in Supplementary Section 1.3, radiologists annotated lesion areas using bound- ing boxes, and device types were extracted from the US reports. In the training set, we kept video clips of 3,749 lesions (2,972 benign and 777 malignant) after pre-processing, consisting of 2,589,824 frames (1,905,670 benign and 684,154 malignant). Note that these frames contain redundant temporal information with limited diversity in visual appearance. Out of these 3,749 lesions, 1,387 lesions (694 benign and 693 malignant) had biopsy-confirmed pathology results, serving as \"gold standard\" labels. The remaining 2,362 lesions were assigned \"silver standard\" pathology labels under the expert guidance, based on BI-RADS scores [5]. Specifically, lesions with BI-RADS 2 or 3 were labeled as benign, those with BI-RADS 4C or higher as malignant, and the others were excluded. The retained 2,362 lesions all received \"silver stan- dard\" labels of benign or malignant pathology. Expert guidance was used to annotate labels for investigated tail categories. For DCIS labels, we identified 34 DCIS lesions based on pathology results. Additionally, an expert annotated NCM or CAL labels on the 1,387 lesions with \"gold standard\" labels. From these anno- tations, the training set included 741 lesions with NCM (117 benign and 624 malignant) and 251 lesions with CAL (36 benign and 215 malignant). For val- idation and selected hyper-parameters, we split the training set into five parts to perform 5-fold cross- validation. In the internal test set, we retained 579 lesions (274 benign and 305 malignant) with \"gold standard\" labels, consisting of 389,066 frames (179,640 benign and 209,426 malignant). To accelerate evalua- tion, we sparsely sampled 16,076 frames (7,560 benign and 8,516 malignant), ensuring that the time interval between each pair of sampled frames was at least one second (30 frames). This was feasible because we found that lesion-level results remained consistent with using all frames (difference smaller than 0.01%).\nFor external evaluation, we prospectively collected 227 lesions (including 7 DCIS lesions) from 225 consec- utive patients who underwent breast-US examinations between October 2022 and March 2023 at PUMCH. These 227 lesions were recruited by a group of radi- ologists and comprised 63 benign and 164 malignant lesions, all with biopsy-confirmed \"gold standard\" labels. Since the 7 DCIS lesions were insufficient to evaluate the model's diagnostic performance for DCIS, we purposely collected an additional 133 DCIS cases. These additional DCIS lesions were sourced from two external institutions: 114 from CICAMS, an insti- tution focused on cancer treatment, and 19 from PUMCH, a comprehensive medical institution. The breast-US examinations for these DCIS cases were conducted between January 2022 and April 2023."}, {"title": "4.3 Development of TAILOR-Gen", "content": "Here, we introduce the training and sampling pro- cess of TAILOR-Gen, as well as the data cleaning process for high-quality generated images. We design TAILOR-Gen as a conditional Denoising Diffusion Probabilistic Model (DDPM) [19, 40]. To clarify its design, we first explain the mechanism of DDPM. The training process of DDPM enables it to learn the data distribution P(x) of breast-US images. Specifically, DDPM learns to gradually denoise a Gaussian noise sample $x \\sim N(0, I)$ to produce an image $x_0 \\sim P(x)$. This is achieved via learning the reverse process $P(x_{t-1}|x_t)$ of a Markov Chain of length T. DDPM can be interpreted as a denoising autoencoder $\\epsilon_{\\theta}(x_t, t)$, which estimates the noise e in $x_t$ at each step."}, {"title": "4.4 Development of TAILOR-Diag", "content": "We design TAILOR-Diag as an ensemble of four classification models to accurately diagnose various cases using specialized knowledge. Let ${x_i}_{i=1,\\dots,N}$ denote N breast-US images of a lesion from N different scanning views. For an input image $x_i$, we first feed it into the general model, TAILOR-Diag-Base, and get the predicted logit $\\hat{y}_{base}$ for common cases. Then, three expert models provide their confidence scores $c_k$ to determine whether they should be used to diagnose $x_i$ where $c_k \\in [0,1]$ and $k \\in \\{ncm, cal, dcis\\}$ for TAILOR-Diag-NCM, CAL, and -DCIS, respectively. We define the confidence scores as the predicted probability of $x_i$ belonging to each tail category and use thresholds $t_k$ to determine whether to use each expert model. The predicted logits from the expert models are denoted as $\\hat{y}_k$. Subsequently, we aggregate the pre- dictions of the general and selected expert model(s) to obtain the logit $\\hat{y}_i$ for image $x_i$:\n$\\hat{y}_i = \\hat{y}_{base} + \\sum_{k\\in \\Omega_i} w_k \\hat{y}_k$\nwhere the selected indices are $\\Omega_i = \\{k|c_k> t_k, k\\in \\{ncm, cal, dcis\\}\\}$, and the aggregation weights $w_k$ are determined by 5-fold cross-validation. Finally, we aggregate the logits of all N images to obtain the final prediction of the lesion:\n$\\beta = \\sigma(\\frac{1}{N}\\sum_{i=1,...,N} \\hat{y}_i)$\nwhere $\\sigma(\\cdot)$ is the sigmoid function, and $\\beta \\in [0, 1]$ is the predicted probability of malignancy of the lesion."}, {"title": "4.5 Hyperparameters", "content": "Hyperparameters of TAILOR-Gen and TAILOR-Diag are carefully selected using 5-fold cross-validation on the training set. We train TAILOR-Gen for 70 epochs on the entire training set and fine-tune TAILOR-Gen for 70 epochs on the domain-specific data. We use a batch size of 8 for training and 128 for sampling. For optimization, we use an AdamW optimizer with an initial learning rate (LR) $6.25\\times10^{-6}$ and weight decay $1.0\\times10^{-4}$. A Cosine Annealing scheduler is applied to decrease the LR progressively. A clipping of gradient value with a threshold of 1.0 is employed for training stability. In the data generation process, the classifier-free guidance strength w = 1.8 and the generated image size is 160 \u00d7 160. We set the steps T = 500 to train the DDPM, and we utilize the DPM-Solver [68] to speed up sampling with inference steps T = 50. For TAILOR-Diag, we implement the diagnostic model using the largest Swin Transformer (Swin-L). To satisfy the input size requirement of Swin-L [42], we resize generated images to 224 x 224. We train the"}, {"title": "4.6 Statistical analysis", "content": "We estimate the 95% confidence intervals by 1,000 bootstrap replications. We calculate the two-sided P- values for significance comparisons of sensitivity and specificity using permutation tests with 10,000 per- mutations. The P-values of AUC are calculated using DeLong's test [69, 70]."}, {"title": "4.7 Implementation details", "content": "We implemented the project based on the following packages: Python (3.9), OpenCV (4.9.0.80), Pandas (2.2.1), Numpy (1.26.4), and Pillow (10.3.0). Addi- tionally, the deep learning model is implemented using PyTorch (1.10.1) and Torchvision (0.11.2). Evaluation metrics are calculated using Sklearn (1.4.1). We con- duct the experiments using computational resources from 7 GPU clusters. Four of these clusters each consist of 8 NVIDIA RTX 3090 GPUs, while the remaining three clusters each comprise 8 NVIDIA RTX 4090 GPUs."}]}