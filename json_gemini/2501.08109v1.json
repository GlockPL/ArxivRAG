{"title": "Data-driven inventory management for new products: A warm-start and adjusted Dyna-Q approach", "authors": ["Xinye Qu", "Longxiao Liu", "Wenjie Huang"], "abstract": "In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no or limited historical demand information. The algorithm follows the classic Dyna-Q structure, balancing the model-based and model-free approaches, while accelerating the training process of Dyna-Q and mitigating the model discrepancy generated by the model-based feedback. Warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-Q shows up to a 23.7% reduction in average daily cost compared with Q-learning, and up to a 77.5% reduction in training time within the same horizon compared with classic Dyna-Q. By incorporating the warm-start information, it can be found that the adjusted Dyna-Q has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the algorithms under a 30-day testing.", "sections": [{"title": "I. INTRODUCTION", "content": "Inventory management is crucial for supply chain operations, overseeing and controlling the order, storage, and usage of goods in a business [1]. In inventory management, the cold-start setting refers to predicting demand and formulating appropriate inventory strategies when new products are introduced or new market demands arise due to the lack of historical data [2]. For cold-start inventory management, data-driven flexible inventory management strategies are adopted, such as Just-in-Time (JIT) inventory management [3], inventory optimization models [4], and predictive analytics and intelligent methods [5].\nReinforcement learning (RL) has been increasingly applied in inventory management to optimize inventory control policies and decision-making processes [5]. Most research on RL's application in inventory management mainly addresses the problem with existing similar products, whose historical sales information is accessible [6]. Under this scenario, the RL method's agent has adequate data to obtain a satisfactory ordering strategy [7]. Most studies apply model-free RL in inventory control under sufficient data due to model-free RL's satisfactory performance under stochastic conditions [8]. On the other hand, model-based RL has the potential to improve upon model-free methods by offering more stable and interpretable policies and potentially faster convergence. However, for newly launched products without actual demand data, the demand prediction model is trained only based on the historical demands of existing products [7]. The discrepancy between the trained model and the actual environment may prevent the model-based algorithm from converging toward the near-optimal policy [9]. Consequently, incorporating demand information from the actual environment obtained by model-free RL methods could benefit the model-based algorithm by obtaining a better result. Therefore, both model-free RL and model-based RL have demonstrated significant potential in optimizing inventory management policies and decision-making processes. However, challenges persist in how to merge their merits together in inventory management for new products with no or limited historical demand data.\nThe Dyna-Q algorithm combines model-based RL with model-free RL. In the cold-start inventory management problem, the Dyna-Q algorithm learns from both a continuously updated model and the real environment to update the value function or policy function [10]. This approach helps address inventory forecasting and management challenges in situations where historical data is not available. By utilizing simulation and planning, Dyna-Q compensates for the absence of actual data, allowing it to learn and adapt to new environments incrementally. Simulating an environment model that accurately reflects the dynamics of the real environment is challenging, especially in the early stages of training. If the model deviates from the actual environment, policy updates based on this simulated data may lead to suboptimal results. Even in cases where a relatively accurate model can be simulated, maintaining this model demands significant computational resources and data, particularly in complex or dynamically changing environments [11], [12].\nTo address the existing limitations, this study introduces a warm-start and adjusted Dyna-Q approach. Firstly, the adjusted Dyna-Q algorithm is proposed, which incorporates the search-then-convergence (STC) process to design the iteration-decaying exploration and planning steps, dynamically reweighting the model-based and model-free components. Secondly, a demand forecasting model is constructed based on the demand for existing similar products to provide warm-start prior knowledge of the"}, {"title": "II. FRAMEWORK OF THE PROPOSED APPROACH", "content": "This section introduces the proposed algorithm, which is based on the classic Dyna-Q method and aims to minimize long-term discounted operation costs by determining the optimal ordering quantity when the demand distribution of a new product is unknown. The proposed algorithm offers two main innovations. Firstly, the selection probability of arbitrary actions in the epsilon-greedy strategy and the number of planning steps are determined using the search-then-convergence (STC) process [13], which decays with the iteration over the training horizons. This approach accelerates the training process and places greater emphasis on exploiting the model-free components as more experience is gathered from the environment. Secondly, since there is limited historical demand data for the new product during the early training phases, the proposed algorithm incorporates the model-based aspect by utilizing simulated data from existing similar products as warm-start information. These two innovations will be further elaborated in the subsequent subsections.\nDenote state (the inventory levels) space S and for any $s \\in S$, an action (the ordering quantities) space A(s). Assume that the state and action spaces are all finite. Algorithm 1 follows the structure of Dyna-Q, which combines both model-free and model-based methods. In each iteration t within the training horizons, the algorithm directly interacts with the environment and updates the Q-values following model-free Q-learning. For a given state-action pair $s \\in S$, $a \\in A(s)$,\n$Q(s, a) \\leftarrow Q(s, a) +a[c+ \\gamma \\min_{a'\\in A(s)}Q(s', a') \u2013 Q(s, a)]$, (1)\nwhere c denotes the immediate cost, s'\u2208 S the next state transited, $0 < a < 1$ the learning rate, and $0 < \\gamma < 1$ the discount factor. The new experience c and s' are used in updating the model. Here we use M(s, a) to denote the model associated with state-action pair (s, a). Explicitly, the model M(s, a) consists of two neural networks estimating the state transition probabilities $P(s'|s, a)$ and the cost function C(s, a, s'), respectively. The estimation is updated by adjusting the parameters to minimize the least squares error between the predicted state transition and cost and the new experience [14].\nIn terms of the model-based part, multiple planning steps are conducted based on the model simulation, where the Q-table is further updated, using the same rule as formulation (1)."}, {"title": "A. Adjusted Exploration Strategy and Planning Steps", "content": "In each iteration, an epsilon-greedy strategy has been implemented to balance the exploitation and exploration processes. In Algorithm 1, we adjust & based on the search-then-convergence (STC) process. [13] In the early iterations of training, the agent tends to explore more rather than exploit the current best action. As iteration evolves, the epsilon value will decrease to a specific value, and the agent will emphasize more on exploitation. The epsilon value is updated based on the STC procedure as below:\n$\\varepsilon_t = max \\Big\\{ \\frac{\\varepsilon_0}{1+y}, \\varepsilon_{min} \\Big\\}$ (2)\nwhere\n$y=\\frac{t}{\\theta_{\\varepsilon}+t}$ (3)\nHere $\\varepsilon_0 > 0$ represents the initial value of the epsilon, $\\varepsilon_{min} > 0$ represents the lower bound of the epsilon, and $\\theta_{\\varepsilon}$ is a constant smoothing the decaying process, which will be chosen appropriately in practice.\nThe planning component (model-based part) can help the agent examine and update the Q-table by exploiting past experiences from the environment. It is known for reducing the sample complexity, leading to faster convergence, and variance reduction [15] [16]. However, a trade-off exists as it is less computationally efficient than model-free Q-learning within the same training horizons. Algorithm 1 enables acceleration by setting the total planning steps decaying with the iteration, which means that the model-free part is more emphasized as the iteration evolves. The intention is that in the late training horizons, the agent has obtained enough exploration for the environment and can rely on model-free parts, which is computationally efficient. The planning steps $N_t$ are updated based on the STC procedure as well:\n$N_t = max \\Big\\{ \\frac{N_0}{1+y}, N_{min} \\Big\\}$, (4)\nwhere\n$y=\\frac{t}{\\theta_N+t}$ (5)\nHere, $N_0 > 0$ represents the initial value of the planning steps, $N_{min} > 0$ represents the lower bound of the planning steps, and $\\theta_N$ is a constant smoothing the decaying process, which will be properly chosen in practice.\nThe anticipated benefit of the adjusted Dyna-Q algorithm is that it accelerates the training process while achieving lower costs in both training and testing phases."}, {"title": "B. Warm-start Via Forecasting and Simulation", "content": "The proposed Algorithm 1 operates as a cold-start algorithm without prior demand information for new products. This absence of demand information at the beginning can cause discrepancies between the estimated model and the actual environment in the early training phases, potentially resulting in suboptimal policy estimates. To address this issue, a warm-start version of the algorithm is suggested, integrating a simulation-based demand forecasting"}, {"title": "III. CASE STUDY", "content": "In this case study, we are applying the proposed algorithms to address a real-life single-echelon inventory problem faced by a bakery. The bakery store conducts periodic inventory reviews and places orders from a single, reliable supplier with an unlimited supply capacity. The inventory review and reorder period are set to one day, with the assumption that the bakery receives the ordered products immediately. Given the perishable nature of the products, it is assumed that the quality decay process begins upon immediate receipt by the store and lasts for three days. Any unsold products held in inventory for three days will be discarded. The bakery has introduced a new product called Boule 400g (Boule, from French, meaning \"ball\", is a traditional shape of French bread resembling a squashed ball), with its daily demand assumed to follow a random and identically distributed pattern. A dataset containing daily transaction details of customers from a bakery in France, collected from Kaggle [18], provides the daily transaction information from 2021-01-01 to 2022-09-2 for the existing similar product: Boule 200g. The historical demand data for Boule 200g from the bakery can be leveraged to generate warm-start information for the new product, Boule 400g.\nThe sequence of the bakery's behaviors is elaborated as follows. At the beginning of each day, the store receives the orders placed yesterday, and the inventory level is updated. Then, the products are sold on a FIFO (First-in and first-out) basis and unfulfilled demand is considered a loss of sales. Next, the quality of products is monitored, and the products lasting in inventory for three days will be discarded. Finally, the inventory level is updated again, and the store places replenishment orders. We formulate the problem and environment in the following subsection."}, {"title": "A. Problem Formulation and Environment Construction", "content": "The state variable contains vital information on inventory levels. The state at period t is shown as: $s_t = \\{s_t^3, s_t^2, s_t^1\\}$, where $s_t^i$ represents the inventory levels of products that have i days shelf-life. For instance, $s_t^3$ denotes the inventory level of products with 3-day shelf-life, which are the fresh products the store just received on that day. The state $s_t^2$ and $s_t^1$ denote the inventory level of products with 2 days or 1-day shelf-life, respectively. At the end of period t, the products with $s_t^1$ will be disposed of from the inventory. Note that $s_t^3, s_t^2$ and $s_t^1$ are all integers.\nThe bakery's action $a_t$ denotes the order quantity of products at the end of period t. Suppose that the maximum possible inventory level |S| is predetermined and the quantity $a_t$ is an integer and has a proper upper bound depending on the maximal possible demand and inventory level.\nIn each period, the total cost $C_t$ is represented as\n$C_t =b_3 \\times s_t^3+b_2\\times s_t^2 + b_1 \\times s_t^1 +C_s max\\{d_t - s_t^1 - s_t^2 - s_t^3, 0\\}$, (6)\nwhere $d_t$ denotes the observed demand at period t, $C_s$ represents the unit cost for the shortage of unfulfilled demand, and $b_i$ represents the holding/obsoleting inventory costs for products with shelf-life of i day(s). We assume that $b_1 > b_2 \\geq b_3$ by problem natural since these deteriorated products may result in additional loss of sales due to sales discounts. And the disposition of products $s_t^1$ will be charged extra costs. Without loss of generality, we assume that there is no ordering cost.\nFor our problem, two state transition processes will occur: one at the beginning of the period and the other at the end associated with customer demand. The first state transition is elaborated in the formula as $s_t^1 = s_{t-1}^2, s_t^2 = s_{t-1}^3, s_t^3 = a_{t-1}$, which summarize that the shelf-life of all products will decrease by 1 due to decaying. Then the $s_t$ will be used to compute the cost by equation (6). The second state transition occurs after the bakery interacts with the environment, returning daily demand information $d_t$:\n$s_{t+1}^3 =\\begin{cases} s_t^3, & \\text{if } d_t < s_t^1 + s_t^2, \\\\ max\\{s_t^3 - (d_t - s_t^1 - s_t^2), 0\\}, & \\text{otherwise}.\\end{cases}$ (7)\n$s_{t+1}^2 =\\begin{cases} s_t^2, & \\text{if } d_t < s_t^1, \\\\ max\\{s_t^2 - (d_t - s_t^1), 0\\}, & \\text{otherwise}.\\end{cases}$ (8)\n$s_{t+1}^1 = max\\{s_t^1 - d_t, 0\\}$. (9)\nThe state transition processes (7)-(9) will be updated simultaneously and follows the FIFO rule, that the demand will be first served with inventory with the least shelf-life, which are the products with $s_t^1$. If the demand can not be fulfilled by $s_t^1$, then $s_t^2$ and then $s_t^3$ will be consumed.\nThe goal of the inventory management problem is to find an optimal policy (optimal $a_t$ given state $s_t$ at each period), such that the long-term expected cost $E[\\sum_{t=1}^{T} \\gamma^{t-1}C_t]$ is minimized."}, {"title": "B. Implementation and Result Discussions", "content": "Set the unit cost parameters as: [b1, b2, b3] = [0.7, 0.3, 0], Cs = 1, all in Euro. Assume the daily demand of the new product: Boule 400g follows Gamma distribution with mean \u03bc = 5 and three variance settings \u03c3\u00b2 = 1, 3 or 5. The different variance settings can be achieved by adjusting the shape and scale parameters of the Gamma distribution. The initial inventory level is s\u2081 = (0, 0, 5). All experiments were run on a high-performance server with NVIDIA Tesla T4 GPUs.\nWe first apply the basic adjusted Dyna-Q algorithm (without warm-start), with parameters a = 0.3, \u03b3 = 0.9, \u03b5\u03bf = 0.4, Emin = 0.1, No = 100, Nmin = 10, \u03b4\u03b5 = 7500, \u0434\u2081 = 5000. For comparison, we construct the model M(s, a) by two either BNNs or Multi-Layer Perceptrons (MLPs) (one for cost function and another for state transition probability). MLP is a neural network consisting of multiple layers of interconnected neurons. Its parameters are optimized by back-propagation, and the prediction error is minimized by gradient descent [19]. Our proposed algorithm is implemented across 500 training episodes, and the average cost per iteration has been recorded. The training processes are recorded in Fig.2, which shows the average cost occurs per iteration, where we observe a stable cost at the end of the training horizon. Thus, the trained optimal policies are also stable. Next, we compare the proposed algorithm with Q-learning and classic Dyna-Q as benchmarks. All three algorithms go through the same training phase with horizon T = 100 days. Then, the trained Q-value and estimated optimal policy are applied for testing in the same environment, with the average daily cost within another 100 days."}, {"title": "IV. CONCLUSION", "content": "The paper presents an RL algorithm for inventory management of newly launched products, particularly in scenarios where historical demand information is limited or unavailable. The proposed adjusted Dyna-Q algorithm builds upon the classic Dyna-Q algorithm, aiming to strike a balance between model-based and model-free approaches during training. Additionally, a demand forecasting model is introduced, leveraging historical demand data from existing similar products to provide warm-start information for the adjusted Dyna-Q algorithm.\nA case study using real-life data is conducted to validate the proposed approach. The adjusted Dyna-Q algorithm demonstrates significant benefits, including up to a 23.7% reduction in average daily cost compared to Q-learning and up to a 77.5% decrease in training time compared to classic Dyna-Q. Two scenarios are presented for generating and incorporating warm-start information into the algorithm for both training and testing. The results show that the adjusted Dyna-Q algorithm achieves the lowest total cost and relatively low shortage percentages compared to other algorithms. Algorithms that incorporate warm-start information exhibit greater stability and lower variance in total cost during testing.\nFor future research directions, several possibilities are suggested. First, further testing of the proposed approach in various inventory management contexts and industries could assess its versatility and scalability. Second, evaluating the adaptability of the approach to specific uncertainty patterns, such as seasonal demand fluctuations or sudden market changes, could provide insights into its robustness. Lastly, enhancing the demand prediction model to measure similarity between existing products and the new product could improve the incorporation of historical data from similar products with high similarity during training."}]}