{"title": "PyraNet: A Multi-Layered Hierarchical Dataset for Verilog", "authors": ["Bardia Nadimi", "Ghali Omar Boutaib", "Hao Zheng"], "abstract": "Recently, there has been a growing interest in lever-aging Large Language Models for Verilog code generation. However, the current quality of the generated Verilog code remains suboptimal. This is largely due to the absence of well-defined, well-organized datasets with high-quality samples, as well as a lack of innovative fine-tuning methods and models specifically trained on Verilog. In this paper, we introduce a novel open-source dataset and a corresponding fine-tuning technique, which utilizes a multi-layered structure that we refer to as PyraNet. Our experiments demonstrate that employing the proposed dataset and fine-tuning approach leads to a more accurate fine-tuned model, producing syntactically and functionally correct Verilog code. The evaluation results show improvements by up-to 32.6% in comparison to the CodeLlama-7B baseline model and up-to 16.7% in comparison to the state-of-the-art models using VerilogEval evaluation platform.", "sections": [{"title": "I. INTRODUCTION AND MOTIVATION", "content": "The introduction of attention-based models [1] represents a landmark development in the field of language processing. This breakthrough spurred the widespread adoption of transformer architectures, driving significant advancements in the area. Many models, such as Generative Pre-trained Transformers (GPT) [2], Bidirectional Encoder Representations from Transformers (BERT) [3], and Language Model for Dialogue Applications (LaMDA) [4], have leveraged transformer technology to achieve notable success.\nThe motivation behind utilizing large language models (LLMs) for hardware code generation is centered on developing a tool that streamlines the hardware modeling process for designers [5], [6]. It also enhances the security aspects of hardware design [7]. Integrating LLMs into this field aims to reduce the complexities traditionally involved in hardware design. By harnessing the advanced features of these models, hardware model development is expected to become more intuitive and efficient.\nAdditionally, the automation of hardware code generation through LLMs is vital for reducing human error. Given the intricate and technical nature of hardware design, manual coding is often prone to mistakes [8]. By automating the process, LLMs not only speed up development but also significantly lower the chances of introducing errors that could arise from human oversight. This results in more reliable and robust hardware designs, as the automated system ensures consistent, high-quality code generation with reduced risk of faults. Ultimately, employing LLMs in this domain marks a move toward more efficient, error-resilient, and user-friendly approaches to hardware design.\nWhile there has been extensive research focused on software program synthesis, hardware code synthesis remains relatively underexplored. Recently, researchers have begun to investigate the use of Large Language Model (LLM) architectures for generating Hardware Description Language (HDL) code [9]\u2013[19].\nDespite early successes in using LLMs for HDL code generation in hardware design, several challenges remain that must be addressed to push the field forward. One key issue is the limited availability of labeled data necessary for effective fine-tuning of LLMs, as seen in the more mature field of software code generation. Existing approaches to Verilog code generation often suffer from syntax and functionality errors [9], [10], highlighting the need for further improvements.\nIn this paper, we introduce PyraNet, a multi-layered open-source dataset structure \u00b9 that utilizes a wide range of open-source Verilog samples, irrespective of their code quality. By organizing the dataset into different quality tiers, we are able to maximize the value from both the high-quality samples and those of lower quality.\nThe proposed fine-tuning technique is designed to fully leverage all data entries across the different tiers of the dataset. This is done by assigning higher loss weights to the high-quality data, and progressively reducing these weights for lower-quality data as we move down the pyramid [20]. Additionally, we implement a curriculum learning approach [21], starting the fine-tuning process with simpler data and gradually introducing more complex data as training progresses. By combining both the loss weighting and curriculum learning strategies, we can achieve a more effective fine-tuned model."}, {"title": "II. RELATED WORKS", "content": "The study in [9] explores the capability of LLMs to generate Verilog code for hardware design, demonstrating significant success in achieving syntactically accurate outputs through fine-tuning on Verilog-specific datasets. By emphasizing the role of LLMs in reducing human errors and enhancing automation in hardware design, the research underscores the need for further advancements in functional correctness and sets the stage for broader AI integration in hardware development.\nThe study in [10] presents VerilogEval, a benchmarking framework for evaluating the performance of LLMs in generating Verilog code for hardware design and verification. Utilizing a dataset of 156 problems from HDLBits, it investigates the automation of Verilog code generation across varying complexities, from simple circuits to advanced finite-state machines. The findings demonstrate that fine-tuning LLMs improves the quality of generated code, highlighting the transformative role of AI in optimizing hardware design workflows. Additionally, the research emphasizes the potential of supervised fine-tuning to enhance LLM capabilities, representing a significant step forward for both academic research and practical applications in the domain.\nThe study in [12] addresses the absence of standardized benchmarks for evaluating LLMs, such as ChatGPT, in hardware design, with a focus on Register Transfer Level (RTL) generation from natural language. It introduces RTLLM, an open-source benchmark designed to evaluate the effectiveness of LLMs in RTL design, establishing new standards for syntax, functionality, and design quality. The study also proposes a novel prompt engineering technique, \"self-planning,\" which enhances GPT-3.5's performance, representing a major advancement in leveraging LLMs for complex and scalable hardware design tasks.\nMG-Verilog [23] introduces a multi-grained dataset tailored for LLM-assisted Verilog code generation, addressing limitations in size, complexity, and description granularity found in existing hardware datasets. MG-Verilog includes over 11,000 Verilog code samples with corresponding descriptions at varying levels of detail, such as high-level summaries, block summaries, and line-by-line comments. The dataset facilitates balanced fine-tuning, enabling LLMs to improve both code implementation accuracy and generalization across diverse hardware design tasks. Extensive experiments demonstrate the dataset's effectiveness, showcasing superior performance compared to models trained on other datasets, with enhanced metrics in code generation accuracy and functionality.\nRTLCoder [18] introduces a fully open-source framework for RTL code generation, addressing the challenges of privacy concerns and the lack of high-performance open-source LLMs"}, {"title": "III. METHODOLOGY AND DATASET", "content": "This section starts by explaining the structure and organization of the PyraNet dataset, followed by a detailed description of the proposed fine-tuning process.\nA. PyraNet Dataset\n1) Dataset gathering: The majority of our dataset was gathered from publicly accessible GitHub repositories. Additionally, some Verilog code examples were generated using commercial large language models like GPT-40-mini.\n2) Post-download dataset filters: Following the collection of all Verilog code samples, we implemented multiple filtering procedures to guarantee the dataset's quality and relevance.\nThe following are the filtration steps:\n\u2022 Empty/Broken files: The initial step involved the removal of empty and corrupted or broken files. A Python script was employed to detect files with encoding issues by attempting to read each file. Any files that could not be processed due to encoding errors were discarded. Likewise, files that were successfully read but were empty were also excluded from the dataset.\n\u2022 Module declaration: Files without a valid module declaration were similarly filtered out. Using a Python script, we identified and excluded any files that lacked module declarations, following the same procedure as the check for empty or corrupted files.\n\u2022 Code Deduplication: We employed the Jaccard similarity algorithm to perform deduplication. This method computes the similarity between sets of tokens derived from the code samples by measuring the intersection over the union of the sets. Code pairs with a Jaccard similarity score above a predefined threshold were identified as duplicates and subsequently removed, ensuring the dataset's integrity and reducing redundancy."}, {"title": "B. PyraNet fine-tuning", "content": "To effectively leverage the proposed PyraNet dataset, we have introduced a dedicated fine-tuning architecture. This architecture incorporates two key techniques: loss weighting and curriculum learning. In this section, we will first elaborate on the loss weighting approach, followed by a detailed explanation of the curriculum learning strategy.\n1) Loss Weighting: As previously described, the PyraNet dataset is structured into six layers of data samples, each corresponding to different tiers. In our loss weighting technique, we fine-tune the model by assigning distinct loss weights to each tier within the dataset. Starting with the top layer\u2014which contains highly ranked and high quality code samples\u2014we set the loss weight to 1.0. This maximal weight ensures that entries in this layer have the most significant impact on the fine-tuning process. As we descend through the layers of the PyraNet dataset, the loss weights are progressively decreased. This reduction lessens the influence of data entries from lower tiers on the final fine-tuned model. Consequently, the model prioritizes learning from high-quality data samples while minimizing the effects of those with lower quality. For the layer of 2 to 6 the loss weights are 0.8, 0.6, 0.4, 0.2, and 0.1 respectively, as depicted in Fig.1-b.\n2) Curriculum Learning: As demonstrated in [21], employing a meaningful training sequence\u2014beginning with simpler samples and progressing to more complex ones\u2014can enhance model performance compared to standard training methods that use random sampling. In our proposed PyraNet dataset, we have assigned complexity levels to all code samples within each tier. Considering the benefits of curriculum learning, we propose initiating the fine-tuning process with the basic complexity level for each tier, followed by intermediate, advanced, and finally expert levels.\nThe fine-tuning process generally commences with the highest tier of the PyraNet dataset. Each tier within the PyraNet dataset is further categorized according to code complexity. Specifically, the process begins with data entries of basic complexity within the top tier, followed by intermediate, advanced, and expert complexity levels in sequence. This hierarchical structure is maintained across all tiers as the fine-tuning progresses downward through the dataset. By progressing through complexity levels in each tier, the model effectively leverages structured data to enhance its performance."}, {"title": "IV. EVALUATION AND DISCUSSION", "content": "In this study, we aim to demonstrate the effectiveness of our proposed PyraNet dataset and the accompanying fine-tuning technique by conducting three distinct experiments and comparing the results with state-of-the-art models. We selected the CodeLlama-7b, CodeLlama-13b, and DeepSeek-Coder-7B models as our base models and fine-tuned them using the PyraNet dataset alongside our proposed fine-tuning methodology. For evaluation, we employed the VerilogEval platform to assess the performance of the models across all experiments."}, {"title": "B. Experiments", "content": "To thoroughly evaluate the effectiveness of our proposed PyraNet dataset and the accompanying fine-tuning technique, we designed a series of three experiments. These experiments aim to systematically assess the individual and combined impacts of the dataset and the fine-tuning approach on model performance. The experimental setup is as follows:\n\u2022 Baseline Experiment: In the first experiment, we evaluated the CodeLlama and DeepSeek-Coder models by providing prompts without any fine-tuning. This established a baseline performance metric for comparison with subsequent experiments.\n\u2022 PyraNet-Only Fine-Tuning (PyraNet-Dataset): In the second experiment, the CodeLlama and DeepSeek-Coder models were fine-tuned using the PyraNet dataset without applying the specialized fine-tuning approach. This allowed us to isolate and assess the improvements attributable solely to the PyraNet dataset.\n\u2022 Combined PyraNet dataset and PtraNet Fine-Tuning Approach (PyraNet-Architecture): In the final experiment, we fine-tuned the CodeLlama and DeepSeek-Coder models using both the PyraNet dataset and our proposed fine-tuning technique. This combined approach was designed to evaluate the synergistic effects of the dataset and the fine-tuning methodology on model performance.\nThrough these experiments, we systematically evaluate the contributions of the PyraNet dataset and the fine-tuning technique, providing a comprehensive analysis of their impact compared to existing state-of-the-art models."}, {"title": "C. Fine-tuning Explained", "content": "As outlined in the preceding sections, we fine-tuned two models: one using the PyraNet dataset without applying the proposed fine-tuning technique, and the other utilizing both the PyraNet dataset and our proposed fine-tuning method.\nIn the first experiment, we fine-tuned the CodeLlama-7B, CodeLlama-13B, and DeepSeek-Coder-7B models using each available (data, description) pair from the dataset. Specifically, the descriptions were used as inputs to the models, and the corresponding Verilog code served as the output. Throughout this experiment, the learning rate was maintained at 2e-4, and the loss weights were set to 1.0. The fine-tuning method utilizes the LoRa technique [29], adhering to its standard training configurations.\nFor the second experiment, we employed a hierarchical fine-tuning approach. We began by fine-tuning the aforementioned models with data entries from the top layer of the PyraNet dataset. This phase consisted of four sequential fine-tuning steps, starting with data entries labeled Basic, followed by those labeled Intermediate, Advanced, and Expert. In each step, descriptions were used as inputs and the corresponding Verilog codes as outputs, effectively implementing a curriculum learning strategy. During this initial fine-tuning phase, the loss weight was set to 1.0, reflecting the use of the highest quality data available.\nIn subsequent rounds of fine-tuning, we progressed to lower layers of the PyraNet dataset, continuing the curriculum learning approach for each layer. The primary modification in these rounds was the adjustment of the loss weights, as illustrated in Fig.1-b. By conducting multiple fine-tuning iterations with varying loss weights, we implemented the loss weighting approach. The fine-tuning process concluded upon completing the lowest layer of the PyraNet dataset. It is important to note that the learning rate was maintained at a constant value of 2e-4 for each round of fine-tuning throughout this experiment as well. This structured methodology allowed us to systematically assess the impact of both the hierarchical fine-tuning and loss weighting techniques on model performance, ensuring that the models effectively leveraged high-quality data while appropriately incorporating varying levels of complexity from the dataset."}, {"title": "D. Comparison and Explanation of the Results", "content": "To ensure a fair evaluation, the proposed PyraNet fine-tuning approach was applied to both the CodeLlama model, for comparison with MG-Verilog [23], and the DeepSeek-Coder model, for comparisons with RTL-Coder [18] and OriGen [22]. The results in Table I indicate that the proposed model achieved up to a 32.6% improvement in the pass@k metric over the CodeLlama baseline models and up to 25.7% over the DeepSeek-Coder baseline model. When compared to state-of-the-art models, the proposed approach demonstrated an 11.1% improvement in the pass@k metric on the Verilog-Machine dataset compared to MG-Verilog. For the Verilog-Human, the proposed model achieved improvements of up to 16.7% against RTL-Coder and 3.9% against OriGen, both of which are based on fine-tuning with DeepSeek-Coder.\nIt is worth noting that OriGen incorporates a Self-Reflection loop, which includes an additional round of error correction\u2014a feature not considered in this work due to time constraints. This accounts for the relatively smaller improvements over OriGen. Nevertheless, integrating OriGen's Self-Reflection loop with inference using the models fine-tuned with the proposed PyraNet architecture is expected to yield even greater performance gains, which will be performed in the future."}, {"title": "E. Dataset Quality Verification", "content": "As emphasized in [10], the integrity of the compiled dataset is crucial for the effectiveness of the fine-tuning process. Due to challenges in verifying each description and additional information incorporated into the PyraNet dataset via the GPT-40-mini model, we adopted the verification methodology employed by the researchers in [10] to validate the labels generated by the GPT-40-mini model. To assess the quality of the dataset, we randomly shuffled the codes, descriptions, and ranking information among the data entries, thereby creating mismatched sets of codes, descriptions, and rankings within each row of the PyraNet dataset. We then proceeded to fine-tune the model using this intentionally distorted dataset. \nThe findings clearly demonstrate that the model's code generation capabilities are significantly compromised when trained on the erroneous dataset. This outcome verifies the accuracy of the labels and underscores the critical importance of dataset quality in the fine-tuning process. Furthermore, the results demonstrate that the accuracy of the fine-tuned model decreases when trained on the erroneous dataset compared to training solely on the PyraNet dataset without the proposed fine-tuning technique. Consequently, we decided to forgo the fine-tuning process using the proposed method, as it is evident that an erroneous dataset would similarly impair the effectiveness of the fine-tuning technique and result in reduced model performance."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "In this study, we present a novel structured dataset, PyraNet, alongside its associated fine-tuning architecture. This architecture addresses the limitations of previous studies by achieving improved performance across diverse Verilog code types. The PyraNet fine-tuning framework employs advanced techniques such as loss weighting and curriculum learning to mitigate the challenges posed by the available datasets. Evaluation results reveal that the PyraNet dataset and its proposed architecture achieve the anticipated benefits, demonstrating up to a 32.6% improvement in the pass@k metric. We suggest that there is substantial scope for further advancements, both through the development of more comprehensive and detailed datasets and by exploring alternative fine-tuning architectures."}]}