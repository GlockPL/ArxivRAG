{"title": "LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement", "authors": ["Siwen Jiao", "Yangyi Fang"], "abstract": "Recent advancements in Visual Language Models (VLMs) have made them crucial for visual question answering (VQA) in autonomous driving, enabling natural human-vehicle interactions. However, existing methods often struggle in dynamic driving environments, as they usually focus on static images or videos and rely on downsampling to ma\u043f\u0430\u0434\u0435 computational costs. This results in the loss of critical details and the difficulty in effectively integrating spatial and temporal information, undermining fine-grained perception and temporal coherence essential for effective decision-making. To tackle these challenges, we introduce La Vida Drive, a novel and efficient VQA framework for autonomous driving. La Vida Drive seamlessly integrates temporal data while maintaining high-resolution inputs for detailed visual perception. It optimizes spatial processing by retaining high-resolution data for intricate details and using lower-resolution inputs for temporal analysis to focus on motion-related features, thereby boosting computational efficiency. Our method achieves an impressive 168-fold token compression while attaining optimal performance, a significant improvement over traditional approaches. The core of La Vida Drive consists of two modules: the Query-aware Token Selection module and the Spatial-Temporal Token Recovery and Enhancement module. The former dynamically selects the most relevant visual tokens based on semantic alignment with the input query, reducing the token count from high-resolution spatial input. The latter ensures smooth and coherent interactions between spatial and temporal information, preserving contextual continuity across frames. Extensive experiments on various autonomous driving question-answering benchmarks show that La Vida Drive significantly reduces visual tokens, enhances efficiency, and improves overall performance.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large-scale pre-training have positioned VLMs as pivotal for VQA in autonomous driving, enabling intuitive human-vehicle interactions through natural language [3, 9, 11, 13, 19, 24]. VLMs facilitate the seamless integration of visual and linguistic information, allowing vehicles to comprehend and respond to complex queries in real-time, interpreting dynamic environments quickly and significantly improving the overall performance and reliability of the system [11, 19, 22].\nDespite significant advancements, existing approaches predominantly solely focus on static images or video and rely on low-resolution inputs to manage computational costs, leading to the loss of critical high-resolution details and the difficulty in effectively integrating spatial and temporal information [7, 20, 25, 30]. This is particularly problematic in dynamic driving environments, where downsampling impairs fine-grained perception and temporal coherence, hindering effective decision-making [11, 20, 22, 26]. Balancing efficiency and accuracy in high-resolution, multi-frame settings for both static perception and motion detection significantly increases inference costs, posing a major challenge in VLM development [18, 22, 24, 26].\nTo address these challenges, we propose LaVida Drive, an innovative VQA framework designed to support fine-grained perception of high-resolution visual inputs in dynamic driving environments while integrating temporal information. Specifically, for spatial processing, the framework retains high-resolution inputs to capture rich details and uses lower resolution for temporal processing on motion-related features, thereby reducing computational load without compromising visual accuracy. However, maintaining high-resolution spatial inputs across multiple viewpoints significantly increases the number of tokens, leading to substantial inference overhead in VLMs. To handle this, we introduce the Query-aware Token Selection mechanism, which dynamically selects visual tokens highly relevant to the input query based on semantic content, enabling adaptive token filtering and significantly easing the burden of computation [11, 26]. Since token selection would disrupt spatial coherence and damage the contextual relationships between tokens, we introduce a Spatial-temporal Token Enhancement module to ensure coherence across different spatial and temporal contexts by using cross-attention mechanisms for consistent information flow across frames, achieving smooth and coherent multi-frame information transfer.\nWe validate LaVida Drive across multiple autonomous driving VQA benchmarks, showing significant enhancements in image-text alignment and multi-modal information processing. Our model reduces visual tokens by 50% to 84%, improving inference efficiency while maintaining performance. Key contributions include:\n\u2022 Propose a novel and efficient VQA framework that seamlessly integrates temporal data into high-resolution spatial inputs, enhancing computational efficiency and detailed visual perception.\n\u2022 Propose a novel query-aware token selection mechanism that dynamically extracts key information for question answering, demonstrating its effectiveness in balancing computation cost and performance.\n\u2022 Propose a token enhancement mechanism integrating multi-modal and multi-scale information, ensuring smooth and coherent interactions between spatial and temporal information and preserving contextual continuity across multiple frames."}, {"title": "2. Related Works", "content": "Recent advancements in autonomous driving have leveraged the intersection of vision and LLMs, driving improvements in both perception and decision-making capabilities. The literature can be categorized into two main areas: vision-based LLMs for autonomous driving and QA systems in autonomous driving."}, {"title": "2.1. Vision-based LLMs for Autonomous Driving", "content": "The integration of vision and language models has shown great promise in enhancing the perception capabilities of autonomous vehicles, enabling them to better understand and navigate complex driving environments. Early work in this domain includes CLIP-based methods [16], which pair visual representations with textual descriptions, enabling a richer understanding of the vehicle's surroundings. Recent studies, such as those by [31] and [28], have proposed large multi-modal models incorporating visual and textual inputs to support decision-making. These models benefit from pre-training on large-scale datasets and have shown improvements in tasks such as scene interpretation and predicting vehicle behaviours in dynamic traffic scenarios.\nThe application of transformer-based models to vision-language fusion has also led to promising developments in autonomous driving. For instance, [5] developed a model that combines deep vision transformers with large-scale language models, which improves decision-making capabilities by enhancing the vehicle's ability to generate complex driving plans. These models process real-time visual input while leveraging pre-trained knowledge to interpret high-level cues, such as road conditions and traffic regulations. Recently, [29] demonstrated the ability of multi-modal LLMs to engage in reasoning tasks in an end-to-end autonomous driving framework, allowing the vehicle to handle novel driving situations that require both visual and linguistic reasoning.\nFurthermore, the advent of VLMs has opened new avenues for enhancing autonomous driving systems. For example, [13] introduced NuScenes-QA, a benchmark for VQA in autonomous driving scenarios, which addresses the complexities of multi-modal data and real-time acquisition. Similarly, [19] proposed DriveLM, a VLM-based approach that integrates web-scale data to boost generalization and interactivity with human users. These advancements highlight the potential of VLMs to address the nuanced challenges of autonomous driving, such as understanding dynamic environments and making informed decisions in real time."}, {"title": "2.2. Question Answering Systems for Autonomous Driving", "content": "QA systems have played a crucial role in improving human-vehicle interaction and facilitating autonomous decision-making. In autonomous driving, these systems help vehicles process natural language queries and provide context-aware answers based on visual inputs and pre-existing knowledge. For example, [27] developed a visual QA system that combines convolutional neural networks with language models to allow autonomous vehicles to answer questions about nearby objects and road conditions. This system allows passengers to ask real-time questions, and receive accurate, context-specific responses.\nFurther advances in contextual question answering [4] have significantly enhanced the vehicle's ability to interpret complex driving scenarios. By utilizing multi-modal input, these systems provide more accurate answers to questions regarding traffic flow, pedestrian movement, or vehicle proximity. In addition, dialogue-based QA systems have gained traction in recent years, enabling more dynamic interactions between drivers and vehicles. For instance, [8] introduced a conversational QA framework, where vehicles answer questions and can engage in multi-turn dialogues, adjusting their responses based on evolving traffic conditions and user preferences. This enables smoother communication between passengers and vehicles, improving the overall driving experience and safety.\nMore recent work by [23] explores hybrid models that combine rule-based reasoning with large-scale language models, allowing vehicles to simulate human-like reasoning during real-time decision-making in complex environments more accurately. Their work focuses on providing accurate and safe driving suggestions during ambiguous driving situations, such as when encountering unforeseen obstacles or pedestrians. Additionally, the integration of LLMs into QA systems has shown significant promise. For example, [3] proposed a unique object-level multi-modal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. This approach not only enhances the interpretability of driving actions but also demonstrates the potential of LLM-based driving action generation compared with traditional behavioural cloning methods."}, {"title": "3. Method", "content": null}, {"title": "3.1. Architecture Overview", "content": "As illustrated in Fig. 2, the architecture of LaVida Drive comprises three core components: the Multi-modal Encoder Cluster, the Query-aware Token Selection Module, and the Spatial-temporal Token Enhancement Module. The model processes inputs from three modalities: image data from the autonomous vehicle's multiview cameras, video data, and natural language instructions provided by the user. Different from previous approaches, we employ multiple encoders to handle various input modalities, forming a Multi-modal Encoder Cluster to better address the unique requirements of each data source. All encoders are frozen. Specifically, each encoder processes data in a predefined format:\nText Encoder: Our text encoder employs the CLIP text encoder, leveraging its powerful feature extraction capabilities obtained through large-scale text-image contrastive learning. For an input text sequence $X_{text}$ containing $L_{text}$ tokens, the encoder processes each token embedding and maps the entire sequence to a semantic space. The output of the text encoder is a matrix of shape $L_{text} \\times d$, represented as:\n$E_{text} = \\text{TextEncoder}(X_{text}) \\in \\mathbb{R}^{L_{text} \\times d}$                                                          (1)\nImage Encoder: The image encoder also employs the CLIP visual encoder, with a base resolution of 224 x 224 pixels. This encoder efficiently maps visual data into a rich semantic space and is divided into a main branch and a support branch, each optimized for different aspects of image representation.\nImage Encoder Main Branch: For the main branch, an input image $X_{img}$ of size $H \\times W \\times C$ is first divided into $N = \\frac{H \\times W}{P \\times P}$ patches of size $P \\times P \\times C$. Each patch is flattened into a vector of length $P^2 \\times C$, forming a patch sequence of dimension $N \\times (P^2 \\times C)$. The main branch generates embeddings of shape $L_{main} \\times d$ from the penultimate layer of the CLIP visual encoder, represented as:\n$E_{main} = \\text{MainBranchEncoder}(X_{img}) \\in \\mathbb{R}^{L_{main} \\times d}$.                                              (2)\nImage Encoder Support Branch: To supplement the context loss due to patch segmentation in the main branch, the support branch directly processes the downsampled entire image $X_{img}$, of size 224 \u00d7 224 \u00d7 C. The support branch also generates embeddings of shape $L_{support} \\times d$ from the penultimate layer of the CLIP visual encoder, represented as:\n$E_{support} = \\text{SupportBranchEncoder}(X_{img}) \\in \\mathbb{R}^{L_{support} \\times d}$                                   (3)"}, {"title": "Video Encoder:", "content": "The video encoder is based on the TimeSformer model, which performs temporal modelling on frame sequences. Given an input sequence $X_{temporal}$ of T frames, where each frame has a spatial dimension of $H \\times W \\times C$, the encoder captures inter-frame dependencies to generate temporal representations. The output is an embedding sequence of size Tx d, represented as:\n$E_{temporal} = \\text{VideoEncoder}(X_{temporal}) \\in \\mathbb{R}^{Txd}$.                                                        (4)\nNext, we employ the Query-aware Token Selection Module, which processes tokens output by the image encoder and text encoder to generate a token-level similarity matrix S\u2208 Rm\u00d7n, where m denotes the number of image tokens and n denotes the number of text tokens. By leveraging semantic similarity in space, this module identifies the most relevant visual tokens to the user's query, thereby reducing the number of visual tokens while retaining high-quality tokens. Finally, the Spatial-temporal Token Enhancement Module utilizes the video encoder's output $V_{3D} \\in \\mathbb{R}^{S_{3D} \\times 768}$ and the multi-frame auxiliary information from the image encoder $V_{2D multi-frame} \\in \\mathbb{R}^{S_{2D} \\times 768}$ to recover and enhance tokens through a cross-attention mechanism. This module's purpose is to restore context lost in token selection and aggregate temporal information without increasing the number of additional tokens, as described further in Section 3.3."}, {"title": "3.2. Query-aware Token Selection", "content": "The Query-aware Token Selection module filters out the most relevant and important tokens based on image and text tokens and further compresses these tokens for efficient representation. In the section of the Multi-modal Encoder Cluster, we denote the main branch output from the image encoder as I and the output from the text encoder as T.Given that the CLIP [15] model was pre-trained on a large image-text dataset, we assume that image and text tokens are mapped into the same semantic space. To compute token-wise similarity, we omit the 0-th dimension from the embeddings, as it represents the overall semantic features. To align the image embeddings with the text embeddings in the same semantic space, we apply a multi-layer perceptron (MLP) to the image tokens. This transformation produces the aligned image representation, denoted as I'. The aligned image embeddings I' can then be used for similarity computation with the text embeddings T. Therefore, we calculate the cosine similarity to obtain a token-wise similarity matrix S, measuring the semantic similarity between each image and text token.\n$s(I', T) = \\frac{I'T^T}{||I'||||T||}$                                                                                                     (5)\nNext, based on the similarity between each image token and text token, we calculate the normalized similarity matrix as follows:\n$p_{i}^{img}(x) = \\frac{exp(s(I', T_i) / \\tau )}{\\sum_{j=1}^{N}exp(s(I', T_j) / \\tau )}$                                                     (6)\nThis is then used to select the most relevant k image tokens, where k represents the sampling threshold. A higher threshold selects more visual tokens, allowing the quantity of visual tokens to be controlled by adjusting the downsampling ratio k. The experimental section compares performance under different thresholds, highlighting their impact as a proportion of the total token count. The steps are outlined in the following algorithm:"}, {"title": "Algorithm 1", "content": "Query-aware Token Selection\nInput: Image tokens I, Text tokens T, temperature parameter \u03c4, learnable parameter \u03b1, number of top-k tokens to select k\nOutput: Selected top-k image tokens Ttop-k\n\u2022 Align Image and Text Embeddings:\nI'\u2190 MLP(I)\n\u2022 Compute Cosine Similarity:\n$(I', T)\u2190\\frac{I'T^T}{||I'||||T||}$\n\u2022 Normalize Similarity:\n$p_{i}^{img}(x) \u2190 \\frac{exp(s(I', T_i) / \\tau )}{\\sum_{j=1}^{N}exp(s(I', T_j) / \\tau )}$\n\u2022 Compute Similarity Scores Matrix:\n$S_{sum} \u2190 \\sum_{k=1}^{K} p_{i}^{img}(x)$\n\u2022 Compute Token Weights Matrix:\nW\u2190 \u03a3I\n\u2022 Compute Selection Map:\nM \u2190 (1 \u2212 \u03b1) \u00b7 Ssum + \u03b1 \u00b7 W\n\u2022 Select Top-K Tokens:\nTtop-k \u2190 I'[TopK(M, k)]\n\u2022 Return: Ttop-k\nTo further compress the tokens, we employ an MLP for information aggregation, generating a query-aware compact token representation Tselect. In the experimental section, we demonstrate the model's performance under various combinations of selection and MLP compression factors at a fixed overall compression ratio. The results indicate that balancing the selection factor with the MLP compression rate can achieve higher model performance while preserving a minimal number of tokens."}, {"title": "3.3. Spatial-temporal Token Enhancement", "content": "The Spatial-temporal Token Enhancement module is designed to address issues of contextual disruption and high computational overhead when dealing with multi-frame data. This module includes a general-purpose Token-wise Attention Module, which is then specifically applied in two configurations: spatial and temporal enhancements.\nFirst, we introduce the Token-wise Attention Module, which forms the foundation for both spatial and temporal enhancements. As shown in Fig. 3, this module enhances token context by enabling interactions between the tokens selected by the Query-aware Token Selection module and the context tokens from the image or video encoder. The attention mechanism is defined as follows:\n$Att_{token-wise}(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$,                                                                                             (7)\nwhereQ (Query) represents token representations from the Query-aware Token Selection module, while K (Key) and V (Value) are derived from the encoder outputs. This mechanism allows each query token to absorb relevant information from context tokens.\nWith the Token-wise Attention Module established, we now apply it in two configurations:\n\u2022 Spatial Token Restoration: Here, we use the output Espatial of the image encoder as keys and values. By interacting with each token representation I', this configuration introduces spatial context, resulting in the spatially enhanced representation Ienhanced spatial:\n$I_{enhanced spatial} = Att_{token-wise}(Q = I',\\ K = E_{spatial}, V = E_{spatial})$.\n\u2022 Temporal Token Enhancement: When handling multi-frame data, we use the output Etemporal of the video encoder as another set of keys and values. By interacting in parallel with I', this configuration incorporates temporal context and produces the temporally enhanced representation Ienhanced temporal:\n$I_{enhanced temporal} = Att_{token-wise}(Q = I',\\ K = E_{temporal}, V = E_{temporal})$.\nFinally, we combine the spatial and temporal enhanced representations through an MLP layer to obtain the final token representation:\n$I_{final} = MLP(I_{enhanced spatial} + I_{enhanced temporal})$. (10)\nIn this way, the Spatial-temporal Token Enhancement module fully leverages both spatial and temporal information without increasing computational costs. The final output tokens are then concatenated with the query embedding for input into the Large Language Model."}, {"title": "4. Experiments", "content": "In this section, we conduct extensive experiments on Lavida Drive and analyze the results, including both quantitative and qualitative evaluations. Finally, we perform ablation studies to validate the effectiveness of each module."}, {"title": "4.1. Setup", "content": "Dataset: Following EM-VLM4AD [6] protocols, we use identical training, validation, and test splits on the DriveLM [19] dataset, which covers tasks like perception, prediction, and decision-making to evaluate generalizability. Additionally, we test on the NusceneQA [14] dataset for fine-grained perception, comparing to traditional detection-based methods. The DriveLM training set includes approximately 340,184 unique multi-view QA pairs, with 18,899 pairs for testing and validation. The NusceneQA training and test sets contain 459,941 and 83,337 question-answer pairs, respectively.\nMetrics: To ensure fairness and reproducibility, when evaluating the DriveLM dataset, we use the same metrics as EM-VLM4AD, assessing model performance from four perspectives: BLEU-4 [12], ROUGE-L [10], METEOR [1], and CIDEr [21]. For the NusceneQA dataset, we align with the metrics proposed by the dataset's authors, evaluating accuracy across four query format categories: Exist, Object, Status, and Comparison.\nModels: We employ the CLIP text and vision encoders as our text and image encoders, respectively, and utilize TimeSformer[2] as a video encoder for multi-frame input processing. The base language model used is T5-medium [17].\nImplementation Details: Each model is trained on a single NVIDIA A100 Tensor Core GPU. The image encoder, text encoder, and video encoder are frozen, while other parameters, including those in larger models, are trained with an initial learning rate of 1e-4 and a weight decay of 0.05. The batch size is set to 4. Each model is trained for 12 epochs on the training set, with each image divided into 4x7 patches of size 224x224."}, {"title": "4.2. Overall Performance", "content": "Quantitative Comparison: In Tab. 1, we first compare our model with prior works on the DriveLM dataset, including EM-VLM4AD [6] and DriveLM-Agent [19]. Lavida Drive outperforms our baseline method in overall performance metrics, although DriveLM-Agent slightly exceeds our model in BLEU-4 score. However, it is important to note that DriveLM-Agent has a significantly larger parameter count, reaching 3.96B. Next, we fine-tune the pretrained model on"}, {"title": "4.3. Ablation Studies", "content": "To verify the effectiveness of each module in our model, we designed a series of ablation studies, focusing on the selection and compression ratios, model components, and input types.\nSelect and Compress Ratio Ablation: We fixed the overall compression ratio at 168 and tested various combinations of the selection and MLP compression ratios to assess their impact on model performance. As shown in Tab. 2, initially, we compressed tokens using MLP alone, without selection, which gave suboptimal results. Then, we gradually increased the selection ratio while decreasing the compression ratio, observing peak performance at a selection ratio of 2, after which performance declined. Finally, we reduced tokens by a factor of 168 using selection alone (without MLP compression), resulting in a significant performance drop.\nThese results indicate that excessive redundancy and irrelevant information hinder VLM training and inference. However, overly reducing token selection leads to the loss of valuable information. Therefore, balancing the selection factor and MLP compression ratio is crucial for achieving higher model performance with fewer tokens.\nComponents-wise Ablation: Tab. 3 presents the component-wise experimental results of our method. We start by using the 224\u00d7224 downsampled feature map from the image encoder as low-resolution visual embeddings, producing 49\u00d76 visual tokens, which we use as the baseline. Next, we replace simple down-sampling with high-resolution 224\u00d7224 patches. This approach improves performance by +4.0%, +2.5%, +1.7%, and +0.9%, but increases token overhead. To address this issue, we first apply an MLP layer to downsample the tokens, reducing their count to 49; however, this results in performance declines of -1.4 -0.5%, -1.4%, and -0.9%. By replacing the MLP layer with our proposed Token Selection module, we effectively identify the most relevant tokens, leading to performance gains of +2.7%, +0.5%, +0.7%, and +0.6%, Finally, incorporating Token Recovery and Token Enhancement methods leads to further improvements of +0.6%, +1.0%, +0.9%, and +0.6%, respectively while maintaining the token count.\nInput Type Ablation: To validate the robustness of our algorithm under scenarios where input data is lost in complex or adverse conditions, we tested different input configurations: single-frame multi-view, multi-frame single-view, and single-frame single-view. By analyzing the model's performance after removing parts of the complete multi-view multi-frame dataset, we assess the model's robustness and the impact of multi-frame and multi-view data on its performance. As shown in Tab. 4, our model maintains relatively good performance even after some data is removed. Although there is a significant collapse in performance when using the single-frame single-view dataset, it still outperforms our baseline."}, {"title": "5. Conclusion", "content": "In this work, we present LaVida Drive, a novel framework for VQA in autonomous driving, which effectively integrates high-resolution spatial perception with temporal dynamics. By leveraging query-aware token selection and spatial-temporal token enhancement, our approach reduces computational overhead without sacrificing fine-grained visual detail, enabling more efficient inference through selective processing of relevant detail visual cues and ensuring coherent information flow across frames. LaVida Drive provides a promising framework for real-time VQA systems in autonomous driving, balancing computational efficiency with detailed perception. It effectively integrates spatial and temporal information, laying the groundwork for intelligent systems that can handle complex, dynamic driving environments."}]}