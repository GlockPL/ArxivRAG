{"title": "Scalable Multi-Task Transfer Learning for Molecular Property Prediction", "authors": ["Chanhui Lee", "Dae-Woong Jeong", "Sung Moon Ko", "Sumin Lee", "Hyunseung Kim", "Soorin Yim", "Sehui Han", "Sungwoong Kim", "Sungbin Lim"], "abstract": "Molecules have a number of distinct properties whose importance and application vary. Often, in reality, labels for some properties are hard to achieve despite their practical importance. A common solution to such data scarcity is to use models of good generalization with transfer learning. This involves domain experts for designing source and target tasks whose features are shared. However, this approach has limitations: i). Difficulty in accurate design of source-target task pairs due to the large number of tasks, and ii). corresponding computational burden verifying many trials and errors of transfer learning design, thereby iii). constraining the potential of foundation modeling of multi-task molecular property prediction. We address the limitations of the manual design of transfer learning via data-driven bi-level optimization. The proposed method enables scalable multi-task transfer learning for molecular property prediction by automatically obtaining the optimal transfer ratios. Empirically, the proposed method improved the prediction performance of 40 molecular properties and accelerated training convergence.", "sections": [{"title": "1. Introduction", "content": "Given that a molecule has a number of molecular properties, basically, molecular property prediction is to predict a target property among various properties (Wieder et al., 2020). There are many important applications of molecular property prediction, including virtual screening and discovery of novel materials and drugs (Christopher et al., 2001; Atanasov et al., 2021; Gentile et al., 2022; Sadybekov & Katritch, 2023). Molecular property prediction provides vital information in making informed decisions throughout the discovery and development process so that the development cycle of the product can be accelerated.\nHowever, in reality, molecular property prediction often suffers from the data scarcity problem (Hu et al., 2019; Li et al., 2022; 2021) due to various factors, including the high cost of experimental data generation (Axelrod & G\u00f3mez-Bombarelli, 2022), the complexity of chemical compounds (Kumar et al., 2020), and the proprietary nature of pharmaceutical data (Heyndrickx et al., 2023). As a result, researchers have tried to overcome this limitation, such as using advanced computational techniques like transfer learning (Ko et al., 2024b), data augmentation (You et al., 2020), and other approaches (Lu et al., 2019; Yao et al., 2023; Qian et al., 2023) that can learn effectively from smaller datasets.\nTransfer learning (Pan & Yang, 2010; Zhuang et al., 2019) allows for the effective generalization of knowledge learned from source task data distribution to the target task data. Effective transfer can be fulfilled through learning mutually informative feature representations between aligned tasks. GATE (Ko et al., 2024b) introduced a geometric alignment of various tasks to enhance task alignment for molecular property prediction. With the proposed geometrical alignment, the prediction model can learn geometrically aligned molecular representations that are applicable from source task to target task, enabling effective transfer learning and surpassing the performance of baseline models.\nRecently, Ko et al. (2024a) extended GATE to a multi-task setting by sharing a single latent space among multiple tasks, applying geometrical alignment regularization within this shared latent space. In this extended formulation of GATE, different transfer ratios can be applied for each (source, target) task pair, which represent a belief in how much a source task can be helpful to the target task. Though the transfer ratio could have a large effect on the prediction performance, GATE lacks a method for exploring transfer ratios but rather uses them as hyperparameters, which presents several limitations. i. Inaccuracy in predicted transfer ratios by domain experts: Given the black-box nature of deep learning models, there is no guarantee that the source task and the corresponding source data chosen by a domain expert will actually enhance the performance of the target task. ii. Limited scalability: As the number of tasks increases, manually setting proper ratios for all possible transfer interactions between two tasks by a domain expert becomes infeasible and less optimized. iii. Constraining the foundation modeling in molecular property prediction: As different tasks in molecular property prediction fundamentally involve comprehending molecular structures, the performance of tasks with limited data can be improved by merging existing datasets for extensive multi-task training. This approach maximizes the benefits of foundational modeling in predicting molecular properties.\nTo address these limitations, we propose a novel bi-level optimization method to automatically obtain the optimal transfer ratios for given multi-task data. This bi-level optimization replaces previous manual hyperparameter searches by domain experts through gradient-based learning on the validation performance. The training algorithm remains the same during the training phase; the difference occurs in the validation phase. In the validation phase, gradients flow from the computed loss to the computation node representing the transfer ratio and are updated gradient-based on their contribution to the loss value during the validation phase. Since the gradient computation is restricted to the transfer ratio, additional time and space costs for the proposed bi-level optimization are negligible. The proposed gradient-based bi-level optimization efficiently obtains the optimal transfer ratio, especially on a large task space, without cumbersome tuning by domain experts."}, {"title": "Contribution", "content": "\u2022 We propose a data-driven method to search optimal transfer ratios for multi-task transfer learning of molecular property prediction.\n\u2022 The proposed method has improved the performances on 40 tasks of molecular property regression.\n\u2022 The proposed method accelerates the convergence of multi-task transfer learning in molecular property regressions."}, {"title": "2. Preliminary: Multi-task property regression", "content": "This section introduces preliminary works for multi-task property regression. For multi-task transfer learning in property regression, we leverage GATE algorithms extended for multi-task transfer learning (Ko et al., 2024a)."}, {"title": "2.1. Multi-task learning extension of GATE", "content": "GATE addresses transfer learning among a number of tasks, introducing additional side tasks to learn mutually useful features for different tasks in shared manifold M. M is a manifold where each task-specific model can learn the general geometrical knowledge of molecular structure. This strategy guides the model in learning generally useful features for molecular property regression, allowing the task of scarce data to take advantage of knowledge learned from another data-enriched task."}, {"title": "2.2. Target task regression", "content": "When a molecule x is fed into an embedding model as SMILES (Weininger, 1988), we get a corresponding embedding vector z. Given a target task t, a task-specific encoder (encodert) embeds z into a latent vector $z_t$ on the manifold for the target task t. Then, the regression head $h_t$ predicts $\\hat{y_t}$ to calculate the Mean Squared Error (MSE) loss with respect to target label $y_t$.\n$z_t = encoder_t(z)$ (1)\n$\\hat{y_t} = h_t(z_t)$ (2)\n$l_{reg}^t = \\frac{1}{N}\\sum MSE(y_t, \\hat{y_t})$ (3)\nwhere N is the number of data points."}, {"title": "2.3. Transfer learning from source task", "content": "Let s be another task we can leverage for target task learning. Thanks to the shared manifold M, zt can be represented from a source task representation $z_s$, via transformation $\\Phi_{s\\rightarrow M}$ and inverse transformation $\\Phi_{M\\rightarrow t}^{-1}$.\n$z_s = encoder_s(z)$ (4)\n$z_M = \\Phi_{s\\rightarrow M}(z_s)$ (5)\n$z_t = \\Phi^{-1}_{M\\rightarrow t}(z_M)$ (6)\n$\\hat{y_t} = h_t(z_t)$ (7)\nwhere $\\Phi_{s\\rightarrow M}(z_s)$ means a transformation of the vector from the manifold of source task s to shared manifold M, and $\\Phi^{-1}_{M\\rightarrow t}(z_M)$ means an inverse transformation from the shared manifold M to the manifold of target task t.\nWith a hyperparameter called a mapping ratio $\\Lambda_{s\\rightarrow t}$, GATE conducts transfer learning from source task s to target task t.\n$l_{map} = \\sum_s \\Lambda_{s\\rightarrow t}MSE(y_t, \\hat{y_t})$ (8)\nAs the correlation varies across different source-target task pairs, the effectiveness of multi-task transfer learning is dependent on the proper search of the $\\Lambda_{s\\rightarrow t}$. For instance, the Highest Occupied Molecular Orbital (HOMO) and Lowest Unoccupied Molecular Orbital (LUMO) tasks would be deeply correlated, sharing many of the necessary features representing molecular orbitals. Therefore, a high $\\Lambda_{s\\rightarrow t}$ value can accelerate the mutual learning of the HOMO and LUMO. However, in a multi-task learning setting, the correlation of the target-source task is conditioned on other tasks, which makes it more difficult to find optimal A with many tasks to learn. In this situation, completing the entire correlation of target-source task pairs is prohibitive even for experienced domain experts."}, {"title": "2.4. Geometric regularizations", "content": "To align manifolds of different tasks to be the shared manifold M, GATE aims to align the geometric representation of a molecule in different properties through side tasks: reconstruction, consistency, and distance.\nReconstruction To convince those models can learn general geometries useful across tasks, M should have enough expressiveness to reconstruct z. The following reconstruction loss\n$l_{ae} = \\sum MSE(z_i, \\Phi^{-1}_{Mi}(\\Phi_{iM}(z_i)))$ (9)\nregularizes GATE to maintain the reconstruction capability.\nConsistency Too much divergence between the manifold of t, s could harm the transfer effect. To regularize the significant divergence between $z_t$, $z_s$ on the shared manifold M, GATE introduces a loss for consistency,\n$l_{cons} = \\sum MSE(z_s, z_t)$. (10)\nDistance To learn robust transformation $\\Phi_{i\\rightarrow M}$, $\\Phi_{M\\rightarrow i}^{-1}$ under perturbation, GATE applies a perturbation on z to have $z'$. Then regularizes the resulting displacement for different tasks i to be minimized as\n$l_{dis} = \\frac{1}{M} \\sum \\sum C_s MSE(\\Phi_{i\\rightarrow M}(z_i), \\Phi_{i\\rightarrow M}(z_i^{p}))$ (11)\nwhere M is the number of perturbed points, superscript p means p-th perturbed point, $C_s$ is the hyperparameter, and $s = ||\\Phi_{i\\rightarrow M}(encoder_i(z)) - \\Phi_{i\\rightarrow M}(encoder_i(z^p))||$.\nAggregating the losses for geometrical alignments, the total loss is calculated as\n$l_{tot} = l_{reg} + l_{ae} + l_{cons} + l_{dis} + l_{map}$, (12)\nto update model parameters for $\\Phi$, $\\Phi^{-1}$, h, and the embedding model. For brevity, we simply represent the corresponding model parameters as $\\theta$."}, {"title": "3. Bi-level optimization of GATE", "content": "We interpret the problem of finding optimal $\\lambda$ as a bi-level optimization problem:\n$min_{\\lambda} l_{val}(\\theta, \\lambda)$ (13)\ns.t. $\\theta^*(X) = arg min_{\\theta} l_{train}(\\theta, \\lambda)$,\nAlgorithm 1 depicts the detailed algorithms for the bi-level optimization of GATE. First, $\\theta$ is updated using the training dataset $D_{tr}$ and $\\lambda$ in the inner loop, and in the outer loop, based on the updated $\\theta$, $\\lambda$ is updated with respect to the performance in the validation dataset $D_{val}$."}, {"title": "4. Experiments", "content": "To test in a scaled-up multi-task setting, we collected data from 40 tasks from PubChem (Kim et al., 2022), Ochem (Sushko et al., 2011), CCDDS, Yaws Handbook, and Jean-Claude Bradley. We specified each task and number of data points in Appendix A. For the robust test, we used scaffold split of the train and test dataset based on the molecular structure (Bemis & Murcko, 1996). To avoid the overfitting of transfer ratio adaptation to the validation dataset, we interchanged 20% of the train and validation datasets for every epoch."}, {"title": "4.2. Results", "content": "Table 1 shows the performance of GATE with and without bi-level optimization on 40 different molecular property regression tasks. To evaluate the effectiveness of the proposed method, other than $\\lambda$, we used the same model architecture and hyperparameters of GATE. Performance is measured in terms of Root Mean Square Error (RMSE), a standard metric used to measure prediction accuracy. A lower RMSE indicates better performance. The results show that GATE with bi-level optimization generally achieves lower RMSE scores across most tasks than the original GATE method. This is achieved by learning the transfer ratio $\\lambda$ to minimize prediction error described in Equation (8), which enables more effective transfer learning than using constant $\\lambda$, though given the same data for target task and source tasks. Specifically, as Section 4.2, the performances were enhanced in 31 out of 40 tasks, reducing the average RMSE by 4.4%. This suggests that incorporating bi-level optimization in GATE improves prediction accuracy across a wide range of tasks."}, {"title": "5. Discussion", "content": "Algorithm 3 imply that we can accelerate the outer loop with reduced GPU memory usage by only backpropagating the gradient of tasks whose $\\lambda$ is above a threshold. Training 40 molecular property prediction tasks, we found that this direction is promising, as the 95% of quadratic variations of $\\Lambda$ are under 0.1. This means that updates from many source and target task pairs do not result in a big update of the model parameters. We hope this direction guides future works to improve the time and space complexity of the proposed method."}, {"title": "6. Conclusion", "content": "This study presents a bi-level optimization approach for enhancing transfer learning in multi-task property regression on a large scale. The performance in multi-task transfer learning is significantly influenced by how the correlation between the source and target tasks is modeled. Typically, designing this correlation has relied on domain experts. However, with increasing tasks, relying solely on domain experts for correlation design needs impractical time and inaccurate design due to the exponentially increasing number of task pair combinations, which can lead to sub-optimal outcomes. To address this issue, we employ a data-driven bi-level optimization strategy to identify the optimal correlation design. In our evaluation across 40 tasks, applying our method decreased RMSE for 31 tasks, with an average reduction of 4.4%."}]}