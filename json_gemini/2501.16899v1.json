{"title": "RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with Enhanced Contextual Awareness in Specific Domains", "authors": ["Shady Nasrat", "Minseong Jo", "Myungsu Kim", "Seonil Lee", "Jiho Lee", "Yeoncheol Jang", "Seung-joon Yi"], "abstract": "Large language models (LLMs) represent a significant advancement in integrating physical robots with AI-driven systems. We showcase the capabilities of our framework within the context of the real-world household competition. This research introduces a framework that utilizes RDMM (Robotics Decision-Making Models), which possess the capacity for decision-making within domain-specific contexts, as well as an awareness of their personal knowledge and capabilities. The framework leverages information to enhance the autonomous decision-making of the system. In contrast to other approaches, our focus is on real-time, on-device solutions, successfully operating on hardware with as little as 8GB of memory. Our framework incorporates visual perception models equipping robots with understanding of their environment. Additionally, the framework has integrated real-time speech recognition capabilities, thus enhancing the human-robot interaction experience. Experimental results demonstrate that the RDMM framework can plan with an 93% accuracy. Furthermore, we introduce a new dataset consisting of 27k planning instances, as well as 1.3k text-image annotated samples derived from the competition. The framework, benchmarks, datasets, and models developed in this work are publicly available on our GitHub repository at https://github.com/shadynasrat/RDMM.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly advancing field of robotics and artificial intelligence, the imperative to augment the decision-making capabilities of autonomous systems has been a paramount concern. These models can enhance decision-making, interaction, and planning through their linguistic and contextual understanding abilities. Nevertheless, the direct deployment of large language models in domain-specific robotic tasks faces significant challenges. These key challenges include first, insufficient ability to integrate and leverage personal contextual knowledge about the agent itself, such as its background, capabilities, and specific skills. Second, deployment in real-time on-device settings necessitates efficient inference mechanisms, which can be limited by the computational complexity of large language models.\nRecently, there are many methods for solving the grounding problems of LLMs in robotics. PaLM-E [1] generates control sentences according to multi-modal data. RT-X [2] directly infer instructions based on languages and images. ChatGPT for Robotics [3] needs the declaration of APIs for reasoning the actions of tasks. SayCan [4] selects most\nsuitable actions according to environmental information. VoxPoser [5] converts the observation space into a 3D value maps for generating trajectories. While existing methods can achieve domain-specific planning and handle some partial disturbances, a key limitation is their inability to incorporate the agent's own knowledge, such as personal background information, capabilities, and skills. This personal contextual knowledge is crucial for well-reasoned question-answering to support effective planning processes.\nFor instance, a domestic robot assistant could be given a simple task such as delivering an apple to the individual wearing a black t-shirt, and then engaging in a conversation about its recent achievements or favorite color. Existing methods would face difficulties in executing this request, as the employed large language models lack access to the robot's personal knowledge. In contrast, our RDMM framework enables the agent to retrieve and utilize its own information, including its identity, role, and origin, to formulate an appropriate and informative response. This could involve statements like 'I am Lucio, a household robot assistant. How may I assist you?' or 'Hello, I am Lucio, and I"}, {"title": "II. RELATED WORK", "content": "Large language models represent a significant advance-ment in integrating physical robots with Al systems. This approach aims to address the limitations of large language models, which often lack the necessary contextual grounding for effective decision-making in real-world environments. By conditioning language models with pre-trained behaviors, LLM-based systems enable robots to engage in more natural interactions, understand task-specific constraints, and generate executable plans tailored to their capabilities.\nThe field of LLM-based robotics has witnessed the development of several notable approaches that demonstrate the potential of integrating large language models with robotic systems [1]\u2013[15]. For instance, LM-Nav [11] proposes a goal-conditioned policy that utilizes large, un-annotated datasets, combining pre-trained models for navigation, image-language association, and language modeling.\nThis enables robots to navigate complex environments based on natural language instructions without the need for expensive supervision or fine-tuning, showcasing the practical applications of pre-trained models. Similarly, TidyBot [12] focuses on personalizing robotic assistance for household tasks by learning user preferences through language-based planning and perception, leveraging the few-shot summarization capabilities of LLMs to quickly adapt to new scenarios. Furthermore, LLaRP [16] adapts large language models for reinforcement learning in robotics tasks, utilizing a frozen LLM to take text instructions and visual observations, and outputting actions directly in the environment. This system demonstrates robustness in diverse rearrangement tasks, highlighting the potential of LLMs in reinforcement learning for robotics. Additionally, the Code as Policies [15] approach leverages LLMs trained on code-completion to generate robot policy code from natural language commands, enabling the synthesis of policy code that processes perception outputs and parameterized control primitives, showcasing the expressive power of LLMs in translating high-level instructions into executable robot behaviors. Despite advancements, robots still need to improve natural interactions by better leveraging their knowledge and capabilities. Efficient inference requires local operation for speed and affordability. As shown in Table I, most previous methods depend on large models with server-based inference, increasing costs. Our approach eliminates the need for cloud services by running smaller models to run directly on the robot, resulting in reduced latency, improved autonomy, improved privacy and security, and greater reliability for practical applications."}, {"title": "III. METHOD", "content": "To create a comprehensive dataset for household robots, we drew inspiration from the RoboCup@Home competition tasks, ensuring it covers a wide range of essential skills needed for domestic activities. The dataset was designed into three categories: action-oriented tasks and self-awareness-oriented tasks, each essential for enhancing the robot's operational efficiency and decision-making capabilities in real-world environments. The action-oriented section trains the robot to handle tasks like manipulation, navigation, searching, describing, and counting objects, ensuring it can generate effective strategies for these specific robotic tasks. In contrast, the self-awareness-oriented section equips the robot with a deeper understanding of its identity, capabilities, and purpose, enabling it to engage in more human-like interactions, such as guiding, following and meeting individuals. The final category involves tasks that require a combination of action and memory, where the robot must integrate both types of knowledge to execute complex plans, such as delivering an item and engage in a conversation where it require recalling a relevant detail from its memory.\nThe dataset comprises 27,514 manually annotated examples, each consisting of textual input-output pairs specifically focused on household tasks. Dataset are structured into 42 scenario-based segments, with each scenario categorized under distinct task types, shown in Fig.2. The dataset encompasses 21 distinct skills, each outlined with detailed attributes in Table II. To enhance the robot's decision-making and operational efficiency, system messages provide action descriptions, usage information, and access to the robot's personal memory, allowing it to recall its knowledge in efficiently. This dataset not only serves as a benchmark for evaluating our models but also plays a crucial role in training the robot for real-world applications. By incorporating both action-based and memory-based tasks, the dataset helps the robot develop a deeper understanding of its role, fostering more rational, context-aware decision-making.\nWNF4 to the computation data type WBF16. In summary, QLoRA uses 4-bit NormalFloat as the storage data type and 16-bit BrainFloat as the computation data type. The storage data type is dequantized to the computation data type for the forward and backward passes, but gradients are only computed for the LoRA parameters in 16-bit precision. Training time were 24 minutes for RDMM-8B, 11 minutes for RDMM-7B, and 5 minutes for RDMM-0.5B on a single NVIDIA RTX 4090 GPU."}, {"title": "C. Framework Overview", "content": "1) Parser & Controller: The parser component of our framework is responsible for translating the RDMM-generated plans into actionable commands that the robot can execute. The controller then interprets these commands and interacts with various models, such as VLMS, YOLO, STT and TTS models, to perform specific tasks.\n2) Vision Language Model: Visual perception models are crucial for enabling robots to navigate and interact with their surroundings effectively. We employ a 4-bit quantized internlm-xcomposer2-vl-7b [23] Vision-Language Model (VLM) to interpret contextual cues and extract detailed visual information. This model provides accurate descriptions of people, objects, and scenes, making it a reliable\nsource of visual intelligence. For example, the VLM can accurately identify if a person is wearing shoes or holding a cup. In Fig. 3, within the actions + self-awareness example, the generated plan includes the action Search_Person('', 'wearing black t-shirt'), where the second argument is processed by the VLM to interpret the person's description.\n3) YOLO Model: For our real-time object detection algorithms supporting robotic manipulation tasks, the first priority is accurately identifying objects in the environment. To achieve this, we trained a YOLOv10L model on an annotated dataset containing 1.3k images sourced from the RoboCup@Home competition. In Fig. 3, within the actions example, the generated plan includes the action Search_Object('cereal', ''), where the first argument is processed by YOLO to detect object location. Additionally, for human detection and pose estimation, we utilize the YOLOv8-pose model.\n4) Automatic Speech Recognition: We use Whisper for speech recognition, transcribing audio into text and providing feedback to indicate the robot is listening. For natural responses, we use Seliro-TTS for human-like text-to-speech."}, {"title": "IV. EXPERIMENTS", "content": "We evaluated the accuracy, on-device compatibility and inference speed of our RDMM models, comparing them to baseline models, GPT-4o-mini and GPT-4o. Additionally, we tested our model's real-world performance during the RoboCup@Home competition."}, {"title": "A. Models Planning Accuracy", "content": "The accuracy comparison graph in Fig.4 compares the accuracy of several models across various tasks. It highlights the strong performance of the RDMM models (RDMM-8B, RDMM-7B, and RDMM-0.5B), with a particular focus on their improvements over base models and GPT-40-mini and GPT-40. both baseline and GPT models were conditioned with 20-shots examples from the dataset to ensure a fair evaluation across each task. The RDMM-8B model achieves the highest accuracy, with an average of 92.98%, showcasing a significant improvement from its base model's 44.34%. This indicates a substantial leap in capabilities, particularly in tasks like \"Follow,\" \"Meet,\" and \"Simple.\" Similarly, the RDMM-7B model reaches an impressive 87.21% accuracy, surpassing both its base model's performance (38.48%) and other comparative models, such as GPT-40. The RDMM-0.5B model, while smaller in scale, still demonstrates a marked improvement over its base model, increasing accuracy from 1.75% to 54.44%. Although it slightly trails behind GPT-40, which achieved 58.74%, it still outperforms GPT-40-mini at 52.23%, indicating the model's competitive edge despite its smaller size."}, {"title": "B. On-Device Inference Compatibility", "content": "The compatibility of RDMM models for on-device inference was evaluated across various Jetson hardware platforms, including the Orin AGX 64GB, Xavier AGX 32GB, Xavier AGX 16GB, Orin NX 16GB, and Xavier NX 8GB, all of which employ ARM architecture with integrated RAM and VRAM.\n1) RDMM On-Device Compatibility: The RDMM models-RDMM-8B, RDMM-7B, and RDMM-0.5B-were tested to ensure local inference on these devices. RDMM-8B, requiring 1.1GB RAM and 8.5GB VRAM, and RDMM-7B, requiring 1GB RAM and 6.8GB VRAM, successfully operated on most platforms. However, the Xavier NX 8GB, with limited memory, could only support the RDMM-0.5B model, which demands 0.34GB RAM and 1.9GB VRAM. The larger RDMM models exceeded the available memory on the Xavier NX 8GB, highlighting the importance of aligning model size with hardware constraints for effective on-device inference.\n2) Framework On-Device Compatibility: We also evaluated the full system framework, including VLM, Whisper, Serlio-TTS, YOLOv8-pose, and YOLOv10, alongside the RDMM model. The results, illustrated in Fig.5, shows the memory usage ratios of each model on a local device. The entire system required 30GB of memory, making the 32GB Xavier AGX the smallest device capable of running it."}, {"title": "C. Models Inference Speed Comparison", "content": "The performance evaluation graph presented in Fig.6 demonstrates the inference speed comparison of RDMM models against other models on various Jetson devices, highlights a slight trade-off between speed and enhanced capabilities. While RDMM models are marginally slower than their base models such as Llama3-8B, Mistral-7B, and Qwen2-0.5B this slowdown is primarily due to the Progressive Fine-Tuning with Layer-wise Re-calibration approach, which integrates a QLoRA compact neural network adapter. For instance, on the ORIN AGX 64GB, the RDMM-8B model achieved 6.12 tokens per second (T/s), compared to Llama3-8B's 10.86 T/s and Mistral-7B's 11.87 T/s. Similarly, on the XAVIER AGX 32GB, the RDMM-8B model achieved 5.54 T/s, compared to Llama3-8B's 7.56 T/s and Mistral-7B's 7.95 T/s. On smaller on-device platforms like the XAVIER AGX 16GB and ORIN NX 16GB, RDMM models still showed competitive results. For instance, on the ORIN NX 16GB, RDMM-0.5B delivered 6.12 T/s compared to Qwen2-0.5B's 9.90 T/s. Even on the entry-level XAVIER NX 8GB, where only RDMM-0.5B could run, it managed 3.75 T/s, showcasing the model's inference on limited hardware."}, {"title": "D. Real World Evaluation", "content": "The real-world evaluation of the RDMM models took place during the RoboCup@Home Competition, using Lucio, a custom-built home service robot platform. In this environment, the RDMM models were responsible for handling various household and service-oriented tasks that required not only decision-making but also a level of self-awareness. These tasks involved navigating through complex environments, following people while carrying luggage, and guiding individuals to specific locations. Lucio's ability to understand its role was essential in tasks such as acting as a receptionist or handing items to people, where it needed to interact naturally and engage in small talk, as shown in Fig.3. An example of this is guiding a person while engaging in small talk about a specific topic, highlighting how self-awareness improves interaction and enhances service quality in real-world situations."}, {"title": "V. CONCLUSION", "content": "This research presents the development and deployment of RDMM models, addressing key challenges that LLMs face when applied to domain-specific tasks. By integrating personal contextual knowledge into the decision-making process, RDMM models offer enhanced capabilities for self-aware planning, interaction, and task execution. Unlike existing methods, which struggle to incorporate an agent's personal background and specific skills Our approach demonstrates the viability of running powerful language models locally on edge devices without compromising accuracy at a promising inference speed, even on devices with as little as 8GB of memory. This achievement not only enhances the autonomy of robots in practical applications but also reduces reliance on external cloud-based systems, making it an affordable solution. The comprehensive dataset we constructed, including task-specific scenarios and self-awareness-oriented examples, lays the groundwork for future advancements in self-aware robotic planning and interaction."}]}