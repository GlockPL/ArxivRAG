{"title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning", "authors": ["Emanuele Frascaroli", "Aniello Panariello", "Pietro Buzzega", "Lorenzo Bonicelli", "Angelo Porrello", "Simone Calderara"], "abstract": "With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improving zero-shot capabilities. Further analysis reveals that our approach can bridge the gap with joint prompt tuning.", "sections": [{"title": "Introduction", "content": "In real-world scenarios, data is not always readily available but arrives gradually and sequentially. Because of this, Continual Learning (CL) is becoming increasingly popular [41] as it mimics a production environment. The primary challenge in developing a neural network capable of continuously learning is the catastrophic forgetting [27] phenomenon, which describes the tendency of the model to replace previously acquired knowledge with that of new data, becoming less proficient in tasks it has encountered in the past. With the advent of Transformers [8, 39] and Large Vision-Language Models (VLMs) [16, 33], the trend in Computer Vision is moving towards prompt learning [17, 29, 49, 50] and parameter efficient tuning [15], allowing the exploitation of available pre-trained models. Among this category of methods, the most widespread approach is that of soft prompts, in which a few learnable vectors are employed to adapt the model. For instance, CoOp [50] learns a context prompt, which is concatenated to the textual name of the class and fed to the CLIP [33] text encoder. Similarly, VPT [17] learns a visual prompt for each task, which is concatenated at each layer of a Vision Transformer (ViT) [8] pre-trained on ImageNet. Finally, CoCoOp [49] extends CoOp by conditioning the prompts on the input image.\nOne key capability of Vision-Language Models is to perform zero-shot classification on unseen classes. This becomes crucial in a Continual Learning scenario, where such ability is not affected by Catastrophic Forgetting. Thus, improving upon the zero-shot performance of VLMs while learning in a continuous fashion presents a non-trivial challenge. Current prompt-based state-of-the-art approaches devise a learnable prompt pool and a query function to map a given image embedding to its corresponding key within the pool [18, 37, 42, 43, 44]. However, when exploiting a VLM as the backbone, the query-key matching strategy privileges prompts for already seen classes, thus failing to generalize to unseen ones. Notably, only a few approaches [42, 46, 48] can perform zero-shot classification after the adaptation phase; however, they still exhibit limited improvement over CLIP. In this work, we propose a new parameter-efficient CL approach that addresses such a shortcoming. Specifically, we fine-tune CLIP on the available data while also transferring knowledge to future tasks, thus extending the performance of the frozen backbone.\nInspired by CoOp [50], we propose to learn class-specific prompts and keep the model's visual and text encoders frozen. This technique allows the model to adapt to new domains while preserving CLIP's zero-shot capabilities. Indeed, we employ a hybrid approach that leverages hand-crafted prompts (\u201ca photo of a <CLS>\") for unseen classes and learned prompts for previously seen classes. However, as we show in Sec. 5, prompt learning alone is not enough to overcome the challenges of a CL scenario and cannot compete with current state-of-the-art CL methods. In this respect, we bridge the gap with joint training by exploiting generative replay. Unlike standard rehearsal methods [3, 4, 34], which store a subset of samples in a memory buffer, we eliminate the buffer and employ a generative model to learn the underlying distribution of input data. This approach offers two significant advantages: i) having access to the distributions of the past features, rather than just a subset of the input data and ii) ensuring data anonymity, thus meeting privacy constraints over data. Moreover, instead of generating images like previous generative-replay methods [10, 36], we directly model the distribution of the latent representation of the data. As our framework only tunes prompts for CLIP's text encoder, the visual embeddings of images do not change over the continual training phase. Thus, we can avoid generating images. Moreover, modelling the latent space instead of the input one dramatically improves the generative models' computational requirements given the lower dimensionality of the latent representations w.r.t. raw images. Specifically, we adopt Variational Autoencoders (VAEs) [21] to model the visual embeddings. In Sec. 6, we compare various generative approaches, along with different prompt-learning techniques, to validate our choices.\nWe evaluate our framework, called Continual Generative training for Incremental prompt-Learning (CGIL), on various standard class-incremental CL benchmarks, showing state-of-the-art performance even on domains where zero-shot CLIP fails. Indeed, it overcomes the previous best performer by a wide margin (+11% on average). Inspired\""}, {"title": "Related Works", "content": "Continual Learning (CL) methods are designed to tackle the issue of catastrophic forgetting [27], which prevents the continuous transfer of previously acquired knowledge when data comes as a stream. Traditional CL approaches can be broadly classified in three main categories: i) regularization techniques [22, 24] prevent the most important parameters from drifting too far from the optimum; ii) architectural-based method allocate specific sets of parameters for each incremental task [26, 35]; iii) rehearsal-based methods adopt a small memory buffer to store past exemplars that are used for later replay [2, 3, 4, 30, 32, 34]. At the cost of bending the rules of continual learning, the latter models have been established as the state of the art when continuously training from scratch.\nRecently, the advent of (large) pre-trained models based on the Vision Transformer (ViT) architecture [8, 33] has changed this paradigm, fostering the emergence of buffer-free alternatives [28, 37, 43, 44, 47] that can achieve minimal forgetting without compromising privacy. These approaches are designed to tackle class-incremental learning [38], whose goal is to continuously expand the set of recognizable classes with each incoming task.\nWhile the class-incremental scenario is typically regarded as the most difficult for CL [1, 9, 38], it does not take into account the potential zero-shot capabilities of Visual Language Models (VLMs). In particular, recent works have shown that it is possible to tune VLMS while maintaining \u2013 or even improving \u2013 their zero-shot capabilities for a single task [5, 19, 45], or multiple tasks [46, 48]. Similarly to the latter, we aim to encourage the continuous adaptation of a VLM to the incoming data without neglecting its zero-shot capabilities."}, {"title": "Preliminaries", "content": "Contrastive Language-Image Pre-Training (CLIP). We employ CLIP [33] as our VLM, which is composed of a visual encoder $E_{vis}(\u00b7)$ and a text encoder $E_{txt}(\u00b7)$. These encoders are trained on image-text pairs by aligning their latent representations via a contrastive objective. To use CLIP for classification, the input image $x$ is fed to the visual encoder, yielding the visual representation $z_{vis} = E_{vis}(x)$. In parallel, different text prompts are crafted by embedding every class label (e.g., \u201ccat\u201d, \u201ccar\u201d) into a template such as \u201ca photo of a"}, {"title": "Method", "content": "Problem setting. In Continual Learning (CL), a deep model $f(\u00b7; \u03b8)$ parametrized by $\u03b8$ is presented with a sequence of tasks $T_i$ with $i := {1,...,T}$, where $T$ denotes the number of tasks. Each t-th task provides $N_t$ data entries, composing the task dataset $D_t := {x^{(n)},y^{(n)}}_{n=1}^{N_t}$ with label $y^{(n)} \u2208 Y_t$. Importantly, each task relies on a set of classes disjoint from others such that $Y_i \\cap Y_j = 0$ if $i \\neq j$. The objective of CL is to minimize the empirical risk on all tasks:\n$L_{CL} = \\sum_{i=1}^T E_{(x,y) \\sim T_i} [L(f(x; \u03b8), y)],$   (2)\nwhere $L$ is the loss function (e.g., the cross entropy for classification). Since the model observes one task at a time, tailored strategies are required to prevent catastrophic forgetting."}, {"title": "Continual Generative training for Incremental prompt-Learning", "content": "In Fig. 1, we present Continual Generative training for Incremental prompt-Learning (CGIL), which comprises two main phases. First, we extract the image embeddings of all images within the current task with the CLIP Visual Encoder. With such features, we fit the distribution of the latent representation for each class with a VAE. In the second phase, inspired by [47] and [50], we learn the prompts for CLIP's text encoder using feature vectors sampled by our VAEs. These two phases are repeated at each task to improve previously learned prompts with knowledge from subsequent tasks.\nLearning the Distributions of Latent Representations. In the first phase of our approach, we aim to learn the underlying distribution of the latent representation of each class. We do so by extracting all the visual features for each image in the current task. Formally, given the samples $D_t$, where t is the current task with the label set $Y_t$, we obtain the visual features $z_{vis} = E_{vis}(x), \u2200x \u2208 D_t$. Subsequently, we fit $|Y_t|$ VAEs (i.e., the number of classes within the current task) to learn the distribution of each class $c \u2208 Y_t$. The objective function for the VAEs is the standard ELBO. Since we work solely in the latent space, we can discard the visual encoder $E_{vis}$ once we extract the visual feature for all samples. This approach significantly reduces our method's computational costs. Once the VAEs are trained, we store their decoders and use them for each alignment phase.\nPrompts Alignment. In the second phase, we build a synthetic dataset by generating feature vectors through all stored decoders. We then perform prompt tuning only on synthetic data by modeling the context words with continuous vectors. Specifically, we train one class-specific context $V$ to capture fine-grained details regarding the classes and a shared Multi-Layer Perceptron (MLP) to recover cross-domain knowledge. The latter processes the embedding $z_{txt}$ related to the hand-crafted prompt \"a photo of a <CLS>\". The prompt $t_c$ for the class c that we feed the text encoder $E_{txt}$ is:\n$t_c = [V_G] [V] [CLS],$   (3)\nwhere $V_G = MLP(E_{txt} (\u201ca photo of a <CLS>\u201d)).$   (4)\nThe posterior probability of the class c is obtained as in Eq. (1), using the text embeddings obtained with our prompts $z_{ext} = E_{txt}(t_c)$. Since the visual embeddings are not obtained through the CLIP visual encoder but directly from the generated featured dataset, we avoid carrying out expensive forward steps through such an encoder. Both V and $V_G$ are trained via gradient descent on synthetic data of all seen tasks. Thus, previously learned contexts are further fine-tuned, incorporating knowledge from subsequent tasks without incurring forgetting. During inference, we feed the image through the visual encoder and compute the posterior probability for each class.\nZero-shot Inference. When dealing with unseen classes (i.e., zero-shot classification) can exploit the learned contexts for the classes the model has already seen. At the same time, we can still use handcrafted prompts (e.g., \u201ca photo of a <CLS>\") to classify unseen classes. This approach allows us to i) preserve the zero-shot capabilities of CLIP while adapting to the classes that arrive sequentially and ii) exploit past knowledge to exclude\""}, {"title": "Experiments", "content": "Datasets. We evaluate our approach across a wide range of datasets with different levels of similarity w.r.t. the ImageNet pre-train [7, 31]. In particular, we test on:\nMetrics. We chose the more challenging scenario of class incremental learning [38] (CIL), where there is no knowledge of the task to which data belong during evaluation. To quantitatively assess our model's performance in this setting, we report the average accuracy of each task, computed after the end of the last training phase, that is, the Final Average Accuracy (FAA), also called Last Accuracy. Additionally, we adapt the Transfer metric from [46, 48], originally used across datasets, to a class incremental training regime to evaluate the zero-shot performance on future tasks. Specifically, let A be the CIL accuracy on the i-th task after being trained until task t, the Class Incremental Transfer is defined as:\n$CI-Transfer = \\frac{1}{T-1} \\sum_{t=1}^{T-1} (\\frac{1}{T-t} \\sum_{i=t+1}^{T} A_i)$   (5)\nImplementation Details. We employ Adam optimizer [20] with a learning rate of 0.0002 for 500 epochs to train our VAEs. Each VAE consists of an encoder and a decoder, comprising only 3 fully connected layers interleaved with LeakyReLU activations. The hidden and latent sizes are 512 and 256, respectively. During the prompt-learning phase, we use the Adam optimizer with a learning rate of 0.03, and the synthetic features generated for alignment are roughly 15K per class, mixed together and split into batches of size 128. We employ CLIP with the ViT-L/14 backbone for each model. Finally, all results reported are the average across 3 runs with different seeds, changing the composition of tasks. The standard deviations of these experiments are reported in the Supplementary Material, along with more technical details of the experiments.\nComparison Methods. We benchmark our model against several state-of-the-art prompt-tuning approaches, including L2P [44], DualPrompt [43], CODA-Prompt [37], and Attri-CLIP [42]. Additionally, we assess models that fine-tune the entire architecture, namely LwF [24], GDumb [32], DER++ [3], and SLCA [47]. In addition to such methods, we integrate MoE Adapters [46] into our framework, a parameter-efficient approach originally designed to prevent zero-shot accuracy degradation across datasets. To ensure a fair comparison, we train all competing models, tuning their hyperparameters for optimal performance. We include the performance of zero-shot CLIP on our datasets as a baseline to emphasize the efficacy of prompt tuning methods over the frozen CLIP backbone. Additionally, Attri-CLIP, Moe Adapters, and our CGIL are also evaluated on future tasks, measuring how their zero-shot capabilities are affected by incremental training."}, {"title": "Comparison with the State of the Art", "content": "In Tab. 1, we report the CIL final accuracies for all evaluated competitors across each benchmark. The last column shows the average of the performance of each method. Despite Zero-shot CLIP's impressive performance on Split Imagenet-R and Split Cars-196, it fails in other domains, particularly in the medical field. Consequently, competitors that rely on CLIP as their backbone are heavily influenced by this limitation and exhibit a similar pattern. On the other hand, CGIL successfully addresses these CLIP-related issues, delivering top-tier performance in all scenarios. Considering average performance, our framework stands"}, {"title": "Analysis", "content": "To better validate the effectiveness of CGIL and its architectural design, we report additional experiments in Tab. 3. We evaluated vanilla CoOp (specifically, their class-specific version) under two distinct benchmarks: one trained jointly, i.e. without partitioning the dataset into tasks (Joint), and the other trained in the conventional CIL scenario (Fine-tune). For the latter, we implemented a minor adjustment: after each task, the learned prompts are frozen. This strategy, also employed by [37], helps prevent forgetting. Conversely, training all contexts in subsequent tasks can overwrite previous knowledge by altering learned prompts. The insights derived from the outcomes of these two baselines highlight the proficiency of CGIL in effectively bridging the gap between fine-tuning and joint training in prompt learning, as our method matches the performance of the joint approach. As other ablation studies indicate, this success is primarily attributed to the generative replay of latent features. On the other hand, the various prompting methods tested slightly affect the performance.\nDifferent generative approaches. Along with Variational Autoencoders, we experimented with various generative models to identify the one that best complements our method. The most straightforward approach involves fitting a multivariate Gaussian distribution for each class, which can be used for later sampling. As indicated in Tab. 3, this approach alone achieves state-of-the-art results (76.83 vs. 75.46 on Avg.), underscoring the effectiveness of our methodology regardless of the generator. However, exploiting a more powerful generative model considerably improves the effectiveness of the alignment procedure, demonstrating that the quality of the generated data is crucial. We first experimented with Mixture of Gaussians (MoG), fitted with expectation-maximization until convergence, which can better capture intra-feature relationships by combining multiple multivariate Gaussians. Then, we investigated Variational Autoencoders (VAEs) (CGIL) and Denoising Diffusion Probabilistic Models (DDPMs) [14], both saturating the required performance in generation. We train the DDPMs with the same hyper-parameters as VAEs, described in Sec. 5. Among the two, we stick to VAEs due to their faster training and reduced number of parameters w.r.t. DDPMs.\nDifferent prompt-learning techniques. Our context comprises a class-specific context and a generated context; in the lower section of Tab. 3, we present the results with different choices. Specifically we evaluate: i) just one class-specific context, as in CoOp Fine-tune; ii) just one MLP generated context; iii) the unified-context variant of CoOp, i.e., using multiple shared contexts. We also varied the number of contexts for each baseline. Interestingly, while the unified context benefited from this modification, the performance for the first two strategies did not improve with an increased number of contexts. Thus, we report only the results with a single context. The outcomes of these experiments fall shortly behind CGIL, showcasing that the main contributors to CGIL's performance are the generative rehearsal and the alignment phase. This contributions are highlighted by the difference between the CoOp fine-tune and Class-specific Context lines in Tab. 3, as they share the same prompting strategy."}, {"title": "Discussion and Limitations", "content": "On the memory and computational costs. Despite our VAEs being relatively lightweight, with only half a million parameters after the encoders are discarded, CGIL may encounter memory constraints if the number of classes continues to grow, potentially restricting its applicability in scenarios with an extreme amount of classes. Compared with a standard rehearsal approach, the memory requirement of one decoder is comparable to a buffer of 14 RGB images of size 224 \u00d7 224 (i.\u0435., ~2MB). However, we note that these decoders are only needed for the training phase, while during inference, only the CLIP visual encoder and the embeddings $z_{xt}$ of our prompts are required. Thus, the computational cost for inference is equivalent to a single pass through the visual encoder plus a matrix multiplication to obtain the similarity scores. This represents a significant advancement over other CL-prompting methods. Indeed, L2P, DualPrompt, and CODA-Prompt execute the forward pass on the image twice, while AttriCLIP computes both visual and textual embeddings during test time.\nWhen considering the computational costs of training, our VAEs are notably lightweight, enabling the learning of latent embedding distribution to be remarkably swift, even without the need for a GPU. The alignment phase for prompt learning presents a higher level of complexity, as it performs gradient descent through the CLIP text encoder. Nonetheless, the duration of this phase can be controlled by adjusting the size of the synthetic dataset generated, thereby permitting flexible utilization and accommodation of time constraints.\nOnline CL Setting. We would like to highlight that CGIL may be classified under the category of Online CL methods [1, 25], as training images are fed only once to the visual encoder. While this requires temporarily storing the feature vectors of all samples from the"}, {"title": "Conclusions", "content": "In this work, we introduce a novel framework, Continual Generative training for Incremental prompt-Learning, which allows the adaptation of Vision-Language Models to new tasks without incurring forgetting. Our model employs Variational Autoencoders to learn the latent distributions of input images, which enables the generation of synthetic latent embeddings. Such data is exploited to fine-tune our backbone efficiently through prompt learning. Specifically, CGIL learns textual contexts, keeping the backbone parameters frozen, thereby preserving the zero-shot capabilities of CLIP.\nOur approach significantly outperforms previous state-of-the-art CL methods when tested across a broad spectrum of benchmarks in various domains. By introducing a new metric, the Class Incremental Transfer, we evaluate zero-shot performance on future tasks during training, demonstrating that our framework is the most effective at leveraging past knowledge to predict unseen classes. Further analysis validates our architectural choices and shows that CGIL bridges the gap with the performance of prompts learned jointly."}]}