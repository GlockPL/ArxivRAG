{"title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning", "authors": ["Emanuele Frascaroli", "Aniello Panariello", "Pietro Buzzega", "Lorenzo Bonicelli", "Angelo Porrello", "Simone Calderara"], "abstract": "With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improving zero-shot capabilities. Further analysis reveals that our approach can bridge the gap with joint prompt tuning. The codebase is available at https://github.com/aimagelab/mammoth.", "sections": [{"title": "Introduction", "content": "In real-world scenarios, data is not always readily available but arrives gradually and sequentially. Because of this, Continual Learning (CL) is becoming increasingly popular [41] as it mimics a production environment. The primary challenge in developing a neural network capable of continuously learning is the catastrophic forgetting [27] phenomenon, which describes the tendency of the model to replace previously acquired knowledge with that of new data, becoming less proficient in tasks it has encountered in the past. With the advent of Transformers [8, 39] and Large Vision-Language Models (VLMs) [16, 33], the trend in Computer Vision is moving towards prompt learning [17, 29, 49, 50] and parameter efficient tuning [15], allowing the exploitation of available pre-trained models. Among this category of methods, the most widespread approach is that of soft prompts, in which a few learnable vectors are employed to adapt the model. For instance, CoOp [50] learns a context prompt, which is concatenated to the textual name of the class and fed to the CLIP [33] text encoder. Similarly, VPT [17] learns a visual prompt for each task, which is concatenated at each layer of a Vision Transformer (ViT) [8] pre-trained on ImageNet. Finally, CoCoOp [49] extends CoOp by conditioning the prompts on the input image.\nOne key capability of Vision-Language Models is to perform zero-shot classification on unseen classes. This becomes crucial in a Continual Learning scenario, where such ability is not affected by Catastrophic Forgetting. Thus, improving upon the zero-shot performance of VLMs while learning in a continuous fashion presents a non-trivial challenge. Current prompt-based state-of-the-art approaches devise a learnable prompt pool and a query function to map a given image embedding to its corresponding key within the pool [18, 37, 42, 43, 44]. However, when exploiting a VLM as the backbone, the query-key matching strategy privileges prompts for already seen classes, thus failing to generalize to unseen ones. Notably, only a few approaches [42, 46, 48] can perform zero-shot classification after the adaptation phase; however, they still exhibit limited improvement over CLIP. In this work, we propose a new parameter-efficient CL approach that addresses such a shortcoming. Specifically, we fine-tune CLIP on the available data while also transferring knowledge to future tasks, thus extending the performance of the frozen backbone.\nInspired by CoOp [50], we propose to learn class-specific prompts and keep the model's visual and text encoders frozen. This technique allows the model to adapt to new domains while preserving CLIP's zero-shot capabilities. Indeed, we employ a hybrid approach that leverages hand-crafted prompts (\u201ca photo of a <CLS>\") for unseen classes and learned prompts for previously seen classes. However, as we show in Sec. 5, prompt learning alone is not enough to overcome the challenges of a CL scenario and cannot compete with current state-of-the-art CL methods. In this respect, we bridge the gap with joint training by exploiting generative replay. Unlike standard rehearsal methods [3, 4, 34], which store a subset of samples in a memory buffer, we eliminate the buffer and employ a generative model to learn the underlying distribution of input data. This approach offers two significant advantages: i) having access to the distributions of the past features, rather than just a subset of the input data and ii) ensuring data anonymity, thus meeting privacy constraints over data.\nMoreover, instead of generating images like previous generative-replay methods [10, 36], we directly model the distribution of the latent representation of the data. As our framework only tunes prompts for CLIP's text encoder, the visual embeddings of images do not change over the continual training phase. Thus, we can avoid generating images. Moreover, modelling the latent space instead of the input one dramatically improves the generative models' computational requirements given the lower dimensionality of the latent representations w.r.t. raw images. Specifically, we adopt Variational Autoencoders (VAEs) [21] to model the visual embeddings. In Sec. 6, we compare various generative approaches, along with different prompt-learning techniques, to validate our choices.\nWe evaluate our framework, called Continual Generative training for Incremental prompt-Learning (CGIL), on various standard class-incremental CL benchmarks, showing state-of-the-art performance even on domains where zero-shot CLIP fails. Indeed, it overcomes the previous best performer by a wide margin (+11% on average). Inspired\""}, {"title": "Related Works", "content": "Continual Learning (CL) methods are designed to tackle the issue of catastrophic forgetting [27], which prevents the continuous transfer of previously acquired knowledge when data comes as a stream. Traditional CL approaches can be broadly classified in three main categories: i) regularization techniques [22, 24] prevent the most important parameters from drifting too far from the optimum; ii) architectural-based method allocate specific sets of parameters for each incremental task [26, 35]; iii) rehearsal-based methods adopt a small memory buffer to store past exemplars that are used for later replay [2, 3, 4, 30, 32, 34]. At the cost of bending the rules of continual learning, the latter models have been established as the state of the art when continuously training from scratch.\nRecently, the advent of (large) pre-trained models based on the Vision Transformer (ViT) architecture [8, 33] has changed this paradigm, fostering the emergence of buffer-free alternatives [28, 37, 43, 44, 47] that can achieve minimal forgetting without compromising privacy. These approaches are designed to tackle class-incremental learning [38], whose goal is to continuously expand the set of recognizable classes with each incoming task.\nWhile the class-incremental scenario is typically regarded as the most difficult for CL [1, 9, 38], it does not take into account the potential zero-shot capabilities of Visual Language Models (VLMs). In particular, recent works have shown that it is possible to tune VLMS while maintaining \u2013 or even improving \u2013 their zero-shot capabilities for a single task [5, 19, 45], or multiple tasks [46, 48]. Similarly to the latter, we aim to encourage the continuous adaptation of a VLM to the incoming data without neglecting its zero-shot capabilities."}, {"title": "Preliminaries", "content": "Contrastive Language-Image Pre-Training (CLIP). We employ CLIP [33] as our VLM, which is composed of a visual encoder E_{vis}(\u00b7) and a text encoder E_{txt} (\u00b7). These encoders are trained on image-text pairs by aligning their latent representations via a contrastive objective. To use CLIP for classification, the input image x is fed to the visual encoder, yielding the visual representation Z_{vis} = E_{vis}(x). In parallel, different text prompts are crafted by embedding every class label (e.g., \u201ccat\u201d, \u201ccar\u201d) into a template such as \u201ca photo of a"}, {"title": "Method", "content": "Problem setting. In Continual Learning (CL), a deep model f(\u00b7; \u03b8) parametrized by \u03b8 is presented with a sequence of tasks T_i with i := {1,...,T}, where T denotes the number of tasks. Each t-th task provides N_t data entries, composing the task dataset D_t := {x^{(n)},y^{(n)}}_{n=1}^{N_t} with label y^{(n)} \u2208 Y_t. Importantly, each task relies on a set of classes disjoint from others such that Y_i \u2229 Y_j = \u2205 if i \u2260 j. The objective of CL is to minimize the empirical risk on all tasks:\n\\begin{equation}\nL_{CL} = \\sum_{i=1}^{T}E_{(x,y)\\sim T_{i}} [L(f(x; \\theta), y)],\n\\end{equation}\nwhere L is the loss function (e.g., the cross entropy for classification). Since the model observes one task at a time, tailored strategies are required to prevent catastrophic forgetting."}, {"title": "Continual Generative training for Incremental prompt-Learning", "content": "In Fig. 1, we present Continual Generative training for Incremental prompt-Learning (CGIL), which comprises two main phases. First, we extract the image embeddings of all images within the current task with the CLIP Visual Encoder. With such features, we fit the distribution of the latent representation for each class with a VAE. In the second phase, inspired by [47] and [50], we learn the prompts for CLIP's text encoder using feature vectors sampled by our VAEs. These two phases are repeated at each task to improve previously learned prompts with knowledge from subsequent tasks.\nLearning the Distributions of Latent Representations. In the first phase of our approach, we aim to learn the underlying distribution of the latent representation of each class. We do so by extracting all the visual features for each image in the current task. Formally, given the samples D_t, where t is the current task with the label set Y_t, we obtain the visual features Z_{vis} = E_{vis}(x), \u2200x \u2208 D_t. Subsequently, we fit |Y_t| VAEs (i.e., the number of classes within the current task) to learn the distribution of each class c\u2208 Y_t. The objective function for the VAEs is the standard ELBO. Since we work solely in the latent space, we can discard"}, {"title": "Prompts Alignment.", "content": "In the second phase, we build a synthetic dataset by generating feature vectors through all stored decoders. We then perform prompt tuning only on synthetic data by modeling the context words with continuous vectors. Specifically, we train one class-specific context V to capture fine-grained details regarding the classes and a shared Multi-Layer Perceptron (MLP) to recover cross-domain knowledge. The latter processes the embedding Z_{txt} related to the hand-crafted prompt \"a photo of a <CLS>\". The prompt t_c for the class c that we feed the text encoder E_{txt} is:\n\\begin{equation}\nt_c = [V_G] [V] [CLS],\n\\end{equation}\n\\begin{equation}\nwhere V_G = MLP(E_{txt}(\"a photo of a <CLS>\")).\n\\end{equation}\nThe posterior probability of the class c is obtained as in Eq. (1), using the text embeddings obtained with our prompts Z_{ext} = E_{txt}(t_c). Since the visual embeddings are not obtained through the CLIP visual encoder but directly from the generated featured dataset, we avoid carrying out expensive forward steps through such an encoder. Both V and V_G are trained via gradient descent on synthetic data of all seen tasks. Thus, previously learned contexts are further fine-tuned, incorporating knowledge from subsequent tasks without incurring forgetting. During inference, we feed the image through the visual encoder and compute the posterior probability for each class."}, {"title": "Zero-shot Inference.", "content": "When dealing with unseen classes (i.e., zero-shot classification) can exploit the learned contexts for the classes the model has already seen. At the same time, we can still use handcrafted prompts (e.g., \u201ca photo of a <CLS>\") to classify unseen classes. This approach allows us to i) preserve the zero-shot capabilities of CLIP while adapting to the classes that arrive sequentially and ii) exploit past knowledge to exclude\""}, {"title": "Experiments", "content": "Datasets. We evaluate our approach across a wide range of datasets with different levels of similarity w.r.t. the ImageNet pre-train [7, 31]. In particular, we test on:\n\u2022 Split Imagenet-R [13], is a general-knowledge dataset frequently adopted in recent CL benchmarks [37, 43, 44, 47], with 200 classes split across 10 tasks.\n\u2022 Split Cars-196 [23] and Split CUB-200 [40], are fine-grained classification datasets regarding car models and bird species, respectively. For both scenarios, the classes are split into 10 tasks.\n\u2022 Split EuroSAT [11, 12], which features RGB satellite images and defines a land cover classification problem consisting of 5 binary tasks.\n\u2022 Split ISIC [6], which includes images depicting 6 skin diseases equally split into 3 tasks.\nMetrics. We chose the more challenging scenario of class incremental learning [38] (CIL), where there is no knowledge of the task to which data belong during evaluation. To quantitatively assess our model's performance in this setting, we report the average accuracy of each task, computed after the end of the last training phase, that is, the Final Average Accuracy (FAA), also called Last Accuracy. Additionally, we adapt the Transfer metric from [46, 48], originally used across datasets, to a class incremental training regime to evaluate the zero-shot performance on future tasks. Specifically, let A_i be the CIL accuracy on the i-th task"}, {"title": "Comparison with the State of the Art", "content": "In Tab. 1, we report the CIL final accuracies for all evaluated competitors across each benchmark. The last column shows the average of the performance of each method. Despite Zero-shot CLIP's impressive performance on Split Imagenet-R and Split Cars-196, it fails in other domains, particularly in the medical field. Consequently, competitors that rely on CLIP as their backbone are heavily influenced by this limitation and exhibit a similar pattern. On the other hand, CGIL successfully addresses these CLIP-related issues, delivering top-tier performance in all scenarios. Considering average performance, our framework stands"}, {"title": "Analysis", "content": "To better validate the effectiveness of CGIL and its architectural design, we report additional experiments in Tab. 3. We evaluated vanilla CoOp (specifically, their class specific version) under two distinct benchmarks: one trained jointly, i.e. without partitioning the dataset into tasks (Joint), and the other trained in the conventional CIL scenario (Fine-tune). For the latter, we implemented a minor adjustment: after each task, the learned prompts are frozen. This strategy, also employed by [37], helps prevent forgetting. Conversely, training all contexts in subsequent tasks can overwrite previous knowledge by altering learned prompts. The insights derived from the outcomes of these two baselines highlight the proficiency of CGIL in effectively bridging the gap between fine-tuning and joint training in prompt learning, as our method matches the performance of the joint approach. As other ablation studies indicate, this success is primarily attributed to the generative replay of latent features. On the other hand, the various prompting methods tested slightly affect the performance.\nDifferent generative approaches. Along with Variational Autoencoders, we experimented with various generative models to identify the one that best complements our method. The most straightforward approach involves fitting a multivariate Gaussian distribution for each"}, {"title": "Discussion and Limitations", "content": "On the memory and computational costs. Despite our VAEs being relatively lightweight, with only half a million parameters after the encoders are discarded, CGIL may encounter memory constraints if the number of classes continues to grow, potentially restricting its applicability in scenarios with an extreme amount of classes. Compared with a standard rehearsal approach, the memory requirement of one decoder is comparable to a buffer of 14 RGB images of size 224 \u00d7 224 (i.\u0435., ~2MB). However, we note that these decoders are only needed for the training phase, while during inference, only the CLIP visual encoder and the embeddings Z_{xt} of our prompts are required. Thus, the computational cost for inference is equivalent to a single pass through the visual encoder plus a matrix multiplication to obtain the similarity scores. This represents a significant advancement over other CL-prompting methods. Indeed, L2P, DualPrompt, and CODA-Prompt execute the forward pass on the image twice, while AttriCLIP computes both visual and textual embeddings during test time.\nWhen considering the computational costs of training, our VAEs are notably lightweight, enabling the learning of latent embedding distribution to be remarkably swift, even without the need for a GPU. The alignment phase for prompt learning presents a higher level of complexity, as it performs gradient descent through the CLIP text encoder. Nonetheless, the duration of this phase can be controlled by adjusting the size of the synthetic dataset generated, thereby permitting flexible utilization and accommodation of time constraints.\nOnline CL Setting. We would like to highlight that CGIL may be classified under the category of Online CL methods [1, 25], as training images are fed only once to the visual encoder. While this requires temporarily storing the feature vectors of all samples from the"}, {"title": "Conclusions", "content": "In this work, we introduce a novel framework, Continual Generative training for Incremental prompt-Learning, which allows the adaptation of Vision-Language Models to new tasks without incurring forgetting. Our model employs Variational Autoencoders to learn the latent distributions of input images, which enables the generation of synthetic latent embeddings. Such data is exploited to fine-tune our backbone efficiently through prompt learning. Specifically, CGIL learns textual contexts, keeping the backbone parameters frozen, thereby preserving the zero-shot capabilities of CLIP.\nOur approach significantly outperforms previous state-of-the-art CL methods when tested across a broad spectrum of benchmarks in various domains. By introducing a new metric, the Class Incremental Transfer, we evaluate zero-shot performance on future tasks during training, demonstrating that our framework is the most effective at leveraging past knowledge to predict unseen classes. Further analysis validates our architectural choices and shows that CGIL bridges the gap with the performance of prompts learned jointly."}]}