{"title": "VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis", "authors": ["Qi Li", "Jinfeng Huang", "Hongliang He", "Xinran Zhang", "Feibin Zhang", "Zhaoye Qin", "Fulei Chu"], "abstract": "Large multimodal foundation models have been extensively utilized for image recognition tasks guided by instructions, yet there remains a scarcity of domain expertise in industrial vibration signal analysis. This paper presents a pipeline named VSLLaVA that leverages a large language model to integrate expert knowledge for identification of signal parameters and diagnosis of faults. Within this pipeline, we first introduce an expert rule-assisted signal generator. The generator merges signal provided by vibration analysis experts with domain-specific parameter identification and fault diagnosis question-answer pairs to build signal-question-answer triplets. Then we use these triplets to apply low-rank adaptation methods for fine-tuning the linear layers of the Contrastive Language-Image Pretraining (CLIP) and large language model, injecting multimodal signal processing knowledge. Finally, the fine-tuned model is assessed through the combined efforts of large language model and expert rules to evaluate answer accuracy and relevance, which showcases enhanced performance in identifying, analyzing various signal parameters, and diagnosing faults. These enhancements indicate the potential of this pipeline to build a foundational model for future industrial signal analysis and monitoring.", "sections": [{"title": "1. Introduction", "content": "Recently, significant advancements have been made in the field of large language models (LLMs). Models like ChatGPT and GPT-4 have shown impressive reasoning skills that rival or even exceed human capabilities [1]. These large language models have revolutionized various domains by integrating text, images, and other data formats, enhancing artificial intelligence's problem-solving abilities. Drawing on the success of LLMs, large vision-language models (LVMs) or large multimodal models (LMMs) have also undergone a transformative evolution [2, 3]. The utilization of these foundation models in numerous fields has resulted in promising outcomes in different categories of subsequent assignments. In the convergence of artificial intelligence, machine learning, and domain-specific applications, LVMs have paved the way for novel opportunities across various fields such as scientific research, engineering, healthcare [4, 5, 6].\nThe primary objective of LMM is to develop a general-purpose multimodal model capable of following arbitrary instructions and solving various tasks specified by the user which creates a universal vision task interface that allows the model to solve a wide range of tasks, leveraging the synergies from diverse instructions [7]. LMM also enhances the model's interactivity and adaptability, enabling it to follow arbitrary user instructions with high accuracy. Recent advancements of open source LMM, such as LLaVA [8, 9], have demonstrated the enhanced capabilities of these models in image content recognition and reasoning within the domain of vision-language tasks, showcasing superior performance compared to earlier approaches. The development of such general-purpose multimodal models represents a significant step forward in the field, offering powerful tools for solving a wide array of vision tasks with greater flexibility and efficiency.\nCurrently, LMM possesses general knowledge for universal image recognition but struggles with identifying specialized information, particularly in industrial signal analysis within the prognostics and health management (PHM) field [10]. PHM plays a vital role in ensuring safety and optimizing maintenance and operational costs, with signal processing and identification being crucial for system monitoring. Traditionally, extracting valuable insights from signals has been a challenging task, requiring advanced signal processing and machine learning technologies. Generic signal analysis methods such as signal modulation and feature extraction are utilized for diagnosis, parameter identification, and fault diagnosis, with techniques like Fourier transform and Hilbert-Huang transform commonly employed [11, 12].\nIn essence, the current approach to utilizing LMM in PHM has several limitations.\n\u2022 From the perspective of LMM, lacking specialized signal processing knowledge hinders obtaining outputs that align with operational logic without specific cues, thereby restricting performance in complex signal processing and fault diagnosis.\n\u2022 In terms of signal analysis and monitoring in PHM, task-specific methods such as parameter identification and pattern recognition are required, lacking a universally applicable interface. Each task is handled individually within a single framework, incorporating task guidance implicitly in model design. The need for designing separate models for parameter identification poses challenges in interacting with signal analysis experts.\n\u2022 Enabling language to bridge signals to language semantics remains a challenge. The current efforts struggle to establish a direct connection between them due to the heterogeneity in signal and language representations.\nTo tackle the issue, we introduce VSLLaVA, a LLaVA-style model that incorporates industrial analysis knowledge using a set of signal-question-answer (SQA) triplets discussed between LLM and PHM experts. We then employ LLM and applied low-rank adaptation (Lora) techniques [13] to fine-tune the linear layers of CLIP [14] and the language model. The fine-tuned model is evaluated in partnership with LMM signal experts to gauge the accuracy and relevance of the ground truth answers, showcasing enhanced performance in detecting and analyzing different signal parameters within publicly available models.\nThe contributions are as follows:\n\u2022 In order to overcome the lack of expertise in signal analysis within the LLM framework, we introduce the VSLLaVA pipeline. This pipeline incorporates expert knowledge in signal analysis into the SQA generator, combines LLaVA-style tuning, and involves expert-LLM mixed evaluation.\n\u2022 The SQA generator leverages domain-specific knowledge to produce simulated and real signals along with corresponding expert Q&A pairs. Through efficient fine-tuning, signals and language are aligned in the semantic space.\n\u2022 By employing a collaborative evaluation approach between experts and LLM, eight different types of signals are validated to show the fine-tuned model significantly enhances knowledge in signal analysis and fault diagnosis, with the potential to pave the way for industrial applications.\nThe remainder of this paper is organized as follows. Section II provides related works about visual instruction and signal analysis. Section III presents the proposed VSLLaVA pipeline. Section IV describes the experimental setup, results and discussion. Section V draws the conclusion and future works."}, {"title": "2. Related works", "content": "2.1. Visual instruction tuning multimodal model\nThe wide applications of LLM in various fields such as natural language processing and text generation have been well established. However, early LLMs were typically limited to a single modality, which posed significant constraints when dealing with complex, multidimensional signal analysis requirements [15]. To address this challenge, researchers have progressively developed multimodal models, with the integration of visual and textual information being a popular approach. These multimodal models can simultaneously process and comprehend visual and language information, showcasing their potential in more intricate tasks.\nThe LLaVA series models have gained significant attention. Firstly, liu et al. proposed the attempt to extend instruction grounding to the language-image multimodal domain with an end-to-end trained large multimodal model that connects visual encoders and LLM to achieve universal visual and language understanding, enhancing zero-shot capability for new tasks [8]. Building upon the research on LLaVA, LLaVA v1.5 [9] demonstrated the effectiveness of using multi-layer perceptrons (MLP) as visual-language connector. A method was proposed to encode images independently by segmenting them into grids, enabling the model to scale to arbitrary resolutions. To further improve the efficiency of fine-tuning LLM models, Luo et al. [16] proposed the use of lightweight modules to connect the image encoder and LLM. This facilitates joint optimization between the two components and includes a routing algorithm to assist LLM in automatically switching between single-modal and multi-modal instructions. Building on Multi-Modal Adapter, the authors introduced a novel multi-modal LLM called LaVIN. The Mixture-of-Modality Adaptation (MMA) and Multi-Modal Training are utilized in this framework. Through MMA, LaVIN is capable of quickly adapting to vision language tasks without the need for extensive pre-training. In parallel, Yin et al. [17] presented an introduction to a multimodal instruction adjustment dataset containing images and point clouds, emphasizing fine-grained information and factual knowledge, along with a detailed construction method. Additionally, a potential Language-Assisted Multi-Modal instruction tuning (MLLM) training framework was proposed to optimize modality expansion, offering baseline models, experimental observations, and analysis. Gao et al. [18] introduced LLaMA-Adapter V2, which is trained using small-scale image-text and instruction data without the need for large-scale multimodal instruction data. The model enhances language instruction following capability by adjusting bias and scale parameters of linear layers. Additionally, it employs joint training by optimizing different parameter sets using image-text paired data and instruction-following data to address interference between image-text alignment and instruction following. Zhang et al. expanded on these concepts with the introduction of the LLaVAR model [19], which aims to enhance the model's understanding of text in images by collecting and leveraging rich textual image data. They discussed how instruction tuning improves the model's generalization ability to unseen tasks. Chen et al. [20] proposed a new method called Position-enhanced Visual Instruction Tuning (PVIT), which integrates additional region-level visual encoders to extend the functionality of MLLMs, facilitating a more detailed understanding of images. They introduced linear projection alignment and end-to-end fine-tuning, along with a region-level instructional data generation scheme to meet the needs of region-level instruction data generation.\nHowever, the studies mentioned above mainly focus on general vision-language tasks and lack application in specific industrial fields. It is important to also consider specialized tasks like analyzing industrial vibration signals as they play a vital role in protecting industrial assets and the safety of operators.\n2.2. Industrial vibration signal analysis\nIndustrial vibration signal analysis is a key technology in the field of PHM. It involves the application of signal processing, machine learning, and deep learning techniques. This analysis can be used for mechanical fault detection/diagnosis, mechanical condition monitoring, etc. [21]. Randall et al. [22] conducted a review on envelope analysis techniques to identify signal and fault characteristic frequencies. Wavelet transform, as discussed by [23], facilitates multi-resolution analysis of non-stationary signals to aid in feature extraction.\nReference [24] utilized Discrete Wavelet Transform as a signal processing method. They computed useful statistical features from the collected signals and employed Correlation-based Feature Selection to identify the optimal features. Subsequently, they classified the data using Random Forest and MLP neural networks. In a study by Ali et al. [25], in 2019, two identical induction motors were subjected to various single and multiple faults (electrical and/or mechanical) in laboratory experiments. They simultaneously measured the stator current and vibration signals of the motors and utilized these signals to develop fault diagnosis methods. The researchers employed Matching Pursuit and Discrete Wavelet Transform as signal processing techniques for feature extraction. Furthermore, they evaluated the performance and suitability of seventeen different classifiers, including Support Vector Machine (SVM), K-Nearest Neighbors, and ensemble algorithms.\nIn addition to machine learning, specific deep learning modules also contribute to feature learning and end-to-end signal processing. Wang et al. [26] proposed a novel convolutional neural network model, MIMTNet, aimed at enhancing the ability to diagnose bearing faults using multidimensional signal features. Ye et al. [27] introduced a new deep neural network, DMCNet, designed for feature learning from gearbox vibration signals, improving the accuracy and efficiency of fault diagnosis. They proposed a special morphological filtering layer that automatically updates structural elements through opening and closing operations. Ribeiro et al. [28] developed a multi-head one-dimensional convolutional neural network based on multi-channel vibration signals to detect and diagnose six different types of motor faults. Vibration signals were measured using two accelerometers in different directions, with each head independently processing data from each sensor to enhance feature extraction capabilities. Li et al. [29] proposed a new approach called the Enhanced Deep Sparse Autoencoder for diagnosing gear pitting faults with relatively limited raw vibration signal data. This approach combines data augmentation with a deep sparse autoencoder algorithm for fault diagnosis in gear wear. It uses a sparse autoencoder in deep learning to extract core information from high-dimensional signals and applies data augmentation to expand the dataset, improving the SAE's ability to detect gear pitting and corrosion faults.\nHowever, these approaches require specific methods for specific tasks and lack the general-purpose interactive capabilities of large multimodal models (LMM). Therefore, there is an urgent need to develop an LMM model for industrial signal processing."}, {"title": "3. VSLLAVA", "content": "3.1. The whole pipeline\nTo bridge the gap between LMMs and industrial vibration signal analysis, we propose VSLLaVA as shown in Fig. 1, a LLaVA-style model enhanced with domain-specific adaptations. We use LLaVA as the base model and apply c techniques to fine-tune the linear layers of CLIP and the language model. The fine-tuned model is then evaluated in collaboration with LMM signal experts to assess answer accuracy and relevance. Our results demonstrate improved performance in identifying and analyzing various signal parameters within open-source models.\n3.2. Expert Rule-based signal generator\nIn the signal processing field, the community is experiencing a shortage of signal-text pairs due to a lack of relevant knowledge in vibration signal analysis. To address this, we initially establish signal-question-text (SQA) triplets using an expert rule-based signal generator. Drawing from the concept of curriculum learning [30], a machine learning approach that gradually exposes models to increasingly difficult tasks mirroring human learning processes, we can effectively build the basic signal from complex signal in Table 1. Moreover, in industrial signal analysis, we specifically examine signal parameters such as the correlation between harmonic frequencies, the presence of peak frequencies associated with fault features (like those found in bearings of common industrial equipment), and the frequency-based positions of spectral peaks.\nBy employing the data construction methodology of LLaVA, we have developed eight distinct datasets, as depicted in Table 1, wherein the question-answer pairs are constructed based on domain-specific knowledge encompassing"}, {"title": "3.2.1. Single harmonic signal", "content": "Firstly the single harmonic signal can be represented as:\n$y_{sh} = A \\sin(2\\pi f_p t + \\phi)$ (1)\nwhere $y_{sh}$ denotes single harmonic signal. A represents the amplitude, indicating the maximum displacement of the vibration. $f_p$ represents the frequency where $\\phi$ denote the phase angle that determines the starting position of the vibration on the time axis."}, {"title": "3.2.2. Multiple harmonic signal", "content": "Given the single harmonic signal the multiple harmonic signal can be denoted as:\n$y_{mh} = \\sum_i (A_i \\sin(2\\pi i f_p t + \\phi_i)), i = 1,2,3...$ (2)\nwhere $if_p$ represents a composite vibration composed of multiple harmonic frequencies, $A_i$ is the amplitude of the $i$th harmonic frequency, where $i$ represents the harmonic number, indicating an integer multiple relative to the fundamental frequency $f_p$. $\\phi_i$ is the phase of the $i$th harmonic frequency."}, {"title": "3.2.3. Random harmonic signal", "content": "The random harmonic signal can be expressed as:\n$y_{rh} = \\sum_i (A_i \\sin(2\\pi i f_p t + \\phi_i)), i \\sim N(\\mu_i, \\sigma_i^2)$ (3)\nwhere $A_i$ and $\\phi_i$ are random variables, and $i$ follows a normal distribution $N(\\mu_i, \\sigma^2)$."}, {"title": "3.2.4. Combined harmonic signal", "content": "The composite waveform formed by the superposition of harmonic wave and random wave can be represented as:\n$y_{ch} = y_{mh} + y_{rh}$ (4)\nwhere $y_{ch}$ represents the total displacement of the combined wave, which is the result of superimposing the harmonic wave."}, {"title": "3.2.5. Frequency modulated signal", "content": "The frequency modulated signal can be formulated as follows:\n$y_{fm} = \\cos (2\\pi (f_c t + \\Delta f \\sin(2\\pi f_m t)))$ (5)\nwhere $f_c$ is the carrier frequency, which is the frequency of the unmodulated signal. $\\Delta f$ is the maximum frequency deviation, indicating the maximum frequency change caused by the modulating signal. $f_m$ is the modulation frequency, determining the rate of change of the modulating signal. The inner function $\\sin(2\\pi f_m t)$ represents the modulating signal, which is multiplied by $\\Delta f$ and added to the carrier frequency $f_c$, causing the carrier frequency to vary with time. This equation illustrates the basic form of a frequency modulated wave, where the carrier frequency $f_c$ is influenced by the modulating signal. By multiplying the carrier frequency with the result of the inner function $\\sin(2\\pi f_m t)$ and adding it to the carrier frequency, a signal with a frequency that varies with time is formed."}, {"title": "3.2.6. Amplitude modulated signal", "content": "Amplitude modulation wave can be represented as follows:\n$y_{am} = (1 + m \\cos(2\\pi f_m t)) \\cos(2\\pi f_c t)$ (6)\n$m$ denotes the modulation index, determining the extent of carrier wave amplitude variation. $f_m$ signifies the modulation frequency, determining the rate of change of the modulation signal. The outer function $\\cos(2\\pi f_c t)$ represents the carrier signal, while the inner function $1 + m \\cdot \\cos(2\\pi f_m t)$ denotes the impact of the modulation signal on the carrier wave amplitude, causing the carrier amplitude to vary with the modulation signal. This equation illustrates the fundamental form of an amplitude modulation wave, where the carrier wave amplitude $\\cos(2\\pi f_c t)$ is influenced by the modulation signal, resulting in a signal with amplitude varying over time by multiplying with the inner function $1+m\\cdot\\cos(2\\pi f_m t)$."}, {"title": "3.2.7. AM and FM signal", "content": "Given the $y_{am}$ and $y_{fm}$, the amplitude-modulated frequency-modulated wave can be obtained via:\n$y_{amfm} = y_{fm} + y_{am}$ (7)"}, {"title": "3.2.8. THU signal", "content": "Additionally, we employed the self-powered state monitoring dataset based on piezoelectric energy harvesting as shown in Fig. 1 to construct our SQA triplet. This dataset comprises voltage signals from four health conditions (normal, inner race fault, ball fault, and outer race fault). The collected signals were sampled at a frequency of 49600Hz [31]."}, {"title": "3.3. SQA data tuning base on LLaVA", "content": "In the pipeline illustrated in Fig. 1, two components of LLaVA require fine-tuning using Lora [13]: the vision encoder CLIP and the LLM. Given a pre-trained autoregressive language model $P(X_a|H_q) \\circ P_{\\Theta_0}(H_q|X_q)$, a pre-trained vision encoder CLIP $P_{\\phi}(Z_I|X_I)$, and an MLP connection $P_{\\phi_m}(H_q|Z_I)$, where the MLP connection is used to map image features $Z_I$ into the word embedding space as language embedding tokens $H_q$, the predicted answer can be obtained by the the structured combination of these component as shown in the step 2.\nFor each signal $X_I$, we use an expert rule-based signal generator with domain knowledge to generate an SQA group $G = (X_I, X_{q1}, X_{a1}, ..., X_{qi}, X_{ai}, ..., X_{qN}, X_{aN})$ to train the VSLLaVA to identify the parameters of the signal for the task. From this group, an instruction pair $X_{instruct} = (X_I, X_{q1}, X_{a1})$ is extracted for the first turn, and pairs $X_{instruct} = (X_{qi}, X_{ai})$ are used for the remaining turns. Specifically, for a sequence of length N, the probability of the target answers $X_a$ is computed by:\n$P(X_a | X_I, X_{instruct}) = \\prod_{i=1}^N P_{\\Theta}(X_{ai} | X_I, X_{instruct, <i}, X_{a, <i})$ (8)\nwhere $X_{instruct,<i>}$ and $X_{a,<i>}$ represent the instruction and answer tokens in all turns before the current prediction token $X_{ai}$, respectively, where $\\Theta$ represents the model parameters.\nTo achieve effective parameter fine-tuning, which enables adaptation to various tasks without a substantial increase in computational costs, we derive a pre-trained weight matrix $W \\in \\mathbb{R}^{d\\times k}$ from the CLIP and LLM models. weight"}, {"title": "3.4. Evaluation of Collaborative Strategy by LLM and human", "content": "Evaluating LLM performance can be challenging [32], but automatic evaluation offers a convenient way to assess alignment between predictions and actual outcomes [33]. Some studies have introduced LLM-based evaluation techniques to address subjectivity, automate calculations, and enhance simplicity [34]. In this study, we implement a strategy that involves both LLM and experts for evaluation. Algorithm 1 below presents the specific algorithm utilized in this evaluation process.\nOverall, we assign four scores $S_a$, $S_r$, $S_l$, and $S_{llm}$ to represent the absolute score of parameter identification, relative score of parameter identification, similarity of the language and the score from LLM, respectively. Initially, all scores $S_a$, $S_r$, $S_l$, $S_{llm}$ are set to 0 to accumulate individual scores across all samples. $N_a$ and $N_r$ are set for tracking invalid or missing samples, particularly for numerical scores $s_a$ and $s_r$, where N represents the number of input samples $X_a$. Additionally, the LLM is initialized with pretrained parameters.\nSubsequently, in each sample loop, numerical values are extracted from the actual $X_{a_i}$ and ground truth $X_{gt_i}$ inputs using regular expressions to form parameter sets $P_{a_i}$ and $P_{gt_i}$. After handling outliers to avoid tasks where parameter identification is absent in the text, the corresponding $s_a$ and $s_r$ scores are computed using Eq. 13 and Eq. 14 :\n$S_a = \\frac{\\sum_{i=1}^n I(a_i = gt_i)}{n} \\times 100%$ (13)\n$S_r = \\frac{\\sum_{i=1}^n max \\big(0, 1 - \\frac{|a_i - gt_i|}{a_i + \\epsilon} \\big)}{n} \\times 100%$ (14)\nwhere $I$ is the indicator function and n represents the number of parameters extracted from the sentence, $a_i$ and $gt_i$ are parameter in $P_{a_i}$ and $P_{gt_i}$, respectively, and $\\epsilon$ denotes a stabilizing factor. To assess the language similarity, the words $T_a$ and $T_{gt}$ should be extracted first, and the score of each sample $s_l$ can be calculated as:\n$S_l = \\frac{|T_a|}{T_{gt}} \\times 100\\%, T = T_a \\cap T_{gt}$ (15)\nFinally, the comprehensive evaluation score can be achieved by the LLM. By preparing the evaluation input $X_e$ as Eq. 16 including question $X_q$, model response $X_a$, ground truth $X_{gt}$, role setting $X_r$, evaluation prompt $X_p$ and system prompt $X_s$, the evaluation input is constructed as\n$X_e = X_q \\oplus X_r \\oplus X_a \\oplus X_r \\oplus X_{gt} \\oplus X_s \\oplus X_p$ (16)\nwhere $\\oplus$ denotes concatenating the content with a separator, then computing the corresponding score through LLM.\nFinally, the overall average scores $S_a$, $S_r$, $S_l$, and $S_{llm}$ can be calculated."}, {"title": "4. Cases study", "content": "4.1. Experiment setting\nFor ease of implementation and reproducibility, the training and evaluation were conducted using the xtuner [35] and opencompass [36] framework. The relevant parameters for Lora are as indicated in the Table 3. The dataset creates SQA triples using a signal generator with parameters to be identified in different data as shown in Fig. 1. The relevant parameters follow a normal distribution. SQA triples for training, validation, and testing are constructed based on different normal distributions to evaluate the performance of model. The foundation language model is implemented as LLAMA-8B [37]. Besides, we utilized a different LLAMA-8B as an evaluator and employed the following prompt for collaborative optimization with experts and GPT4. The compared method include internlm2[38] using the LLaVA-style tuning and original LLaVA with LLM of LLAMA-8B. All experiments were conducted on servers equipped with 8 Nvidia 4090 GPUs.\n4.2. Evaluation prompt\nThe LLM score is based on GPT4 with human instruction to play the <Vibration Signal Analyst>. The prompt $X_p$ is as follows: \"Please assess two results generated by the <Vibration Signal Analyst> for the provided vibration signal"}, {"title": "4.3. Score result", "content": "Fig. 2 displays training process of Lora on the given SQA data, showing that the prediction error significantly decreases with each iteration until reaching 1e-4. To enhance the training stability, we first employ a linear learning rate warm-up for the initial 3% iterations, and subsequently switch to a cosine annealing learning rate scheduler for the remaining iterations.\nBased on the scores shown in the Table 4 to 7, it can be observed that after fine-tuning SQA, VSLLAVA outperforms other models in large-scale evaluations, taking into account aspects like helpfulness, relevance, accuracy, and expertise. Specifically, in Table 4 and Table 5, the evaluation of absolute and relative scores for parameter identification is presented. The scores reflect the performance of the models on various signal types, including SH, MH, RH, CH, AM, FM, AMFM, THU, FM. It can be observed that in the task of parameter identification, the VSLLaVA-llama3 method achieves the best results in $S_a$. Since absolute scores require a perfect match for scoring, the overall score is lower than the relative score. It is noticeable that the accuracy of parameter identification for AM and FM signals is relatively lower. However, the proposed method, by incorporating relevant knowledge, improves accuracy by around 20% for AM and FM signals, and by around 30% for real THU signals. Similarly, the proposed method also demonstrates the best performance in Table 5.\nIn addition to parameter identification, we are also concerned about the quality of the descriptive text. The $S_l$ in Eq. 15 is used to evaluate the similarity between the prediction and the ground truth. It can be observed that for complex vibration signals, especially mixed signals like MH, CH, modulated signals like FM, and actual experimental signals like THU, the proposed method exhibits the highest relevance in text descriptions, leading to an improvement of approximately 35%.\nAdditionally, to reduce the subjectivity of expert rule-based evaluation methods, we have also employed an LLM agent to assess both the ground truth and predicted responses. The results, as shown in Table 7, indicate that under the guidance of prompt words, the LLM acts as a professional Vibration Signal Analyst, evaluating the predicted responses"}, {"title": "4.4. Response example", "content": "Finally, specific examples can be seen in Table 8 to Table 15. The provided tables present responses from three language models\u2014VSLLaVA-llama3, LLaVA-llama3, and LLaVA-internlm2\u2014compared to ground truth on various signal types: AM, FM, AMFM, SH, MH, RH, CH, and THU. The analysis focuses on the accuracy, detail, and relevance of each model's response, along with their ability to correctly interpret and describe the signals. Through comparing the results, we can observe that while other models tend to ramble on about unknown areas, in the industrial field, we expect the responses to be precise without any ambiguous information. We need formatting that clearly shows the output format. On the other hand, our model not only accurately identified the relevant parameters but also presented the results succinctly and directly, following the style of SQA accurately.\nFor the AM signal in Table 8, VSLLaVA-llama3 delivers a precise and accurate description that matches the ground truth, whereas LLaVA-llama3 and LLaVA-internlm2 provide general but less relevant explanations, failing to identify the amplitude modulation characteristic. In the case of the FM signal in Table 9, VSLLaVA-llama3 again excels with an accurate identification of the modulation type, while LLaVA-llama3 incorrectly interprets the signal as random noise, and LLaVA-internlm2 offers a correct but nonspecific response.\nFor the AMFM signal in Table 10, VSLLaVA-llama3 accurately captures the dual modulation, distinguishing itself from LLaVA-llama3, which incorrectly focuses on signal amplitude without recognizing the modulation coupling, and LLaVA-internlm2, which provides only a general periodic description. When analyzing the SH signal Table 11, VSLLaVA-llama3 accurately identifies the signal's characteristics, including its period and phase, while LLaVA-llama3 makes a notable error in estimating the period, and LLaVA-internlm2 gives an incomplete but correct response.\nIn the case of the MH signal in Table 12, VSLLaVA-llama3 demonstrates superior understanding by correctly identifying the multi-harmonic nature and relevant parameters, while LLaVA-llama3 misinterprets the signal as artificially generated, and LLaVA-internlm2 offers a basic description lacking in detail. For the RH signal, VSLLaVA-llama3 accurately describes the complex composition of random frequencies, in contrast to LLaVA-llama3 and LLaVA-internlm2, which provide general sinusoidal descriptions without addressing the signal's randomness.\nThe analysis of the CH signal in Table 14 shows that VSLLaVA-llama3 correctly identifies the combined harmonic nature, whereas LLaVA-llama3 makes a critical error by suggesting the signal is generated by a Gaussian process, and LLaVA-internlm2 fails to recognize the signal's complexity. Finally, for the THU signal in Table 15, VSLLaVA-llama3 accurately identifies the signal's fault condition and recording frequency, outperforming LLaVA-llama3, which inaccurately suggests artificial generation, and LLaVA-internlm2, which provides a generic description without recognizing the signal's specific characteristics.\nAcross all signal types, VSLLaVA-llama3 consistently delivers the most accurate and relevant responses, closely aligning with the ground truth. LLaVA-llama3 and LLaVA-internlm2 often provide correct but less specific or relevant answers, with LLaVA-llama3 occasionally making significant errors in signal interpretation. The results highlight VSLLaVA-llama3's superior understanding of complex signal characteristics, making it the most reliable model for these tasks."}, {"title": "5. Conclusion", "content": "Large multimodal foundation models have been widely applied to image recognition tasks guided by instructions, but their application in industrial signal analysis remains limited due to a lack of domain-specific expertise. This paper introduces VSLLaVA, a pipline designed to bridge this gap by incorporating expert guidance into Large Multimodal Models for signal analysis. The pipeline includes an expert rule-assisted signal generator, which combines insights from vibration analysis experts with domain-specific parameter identification and fault diagnosis question-answer pairs. These elements are integrated with large language models to create signal-question-answer sets for further optimization and training.\nUsing LLaVA as the base model, we employ low-rank adaptation techniques to fine-tune the linear layers of contrastive language-image pretraining (CLIP) and the language model. The fine-tuned model is evaluated in collaboration with LLM and rule of signal experts, focusing on answer accuracy and relevance. The results demonstrate notable improvements in identifying and analyzing various signal parameters, highlighting the potential of this pipeline to serve as a foundational model for future advancements in industrial signal analysis and monitoring.\nFuture work will include expanding the dataset and creating a more comprehensive benchmark to facilitate a more holistic evaluation. Additionally, we aim to pre-train the signal modality encoder using SQA triplets."}]}