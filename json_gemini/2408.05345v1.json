{"title": "Explainable Al Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models", "authors": ["UPOL EHSAN", "MARK O. RIEDL"], "abstract": "When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) \"black-box\" of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited especially when it comes to non-Al expert end-users. In this paper, we challenge the assumption of \"opening\" the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.", "sections": [{"title": "1 PROVOCATION", "content": "With the advent of Foundation Models & Large Language Models like ChatGPT, is \"opening the black-box\" still a reasonable and achievable goal for Explainable AI (XAI)? Do we need to shift our perspectives?\nIn February of 2023, Nadeem (pseudonym), a relative of the first author of this article, asked if ChatGPT could be used to do homework. Nadeem is a high-schooler who shared that he was always under a tight deadline and needed to be more \"efficient\" with his homework. He heard from his friends that ChatGPT can help summarize papers or books, which can make Nadeem more \"productive\" with his homework.\nBefore responding to Nadeem, ChatGPT was taken for a test drive. It was prompted to summarize an academic paper (Figure 1 similar to how Nadeem might use it as someone who was not an Al researcher or experienced prompting Large Language Models (LLMs). Fortunately, ChatGPT generated a coherent response. ChatGPT gave the names of the authors of the paper and details about the paper's publication at ACM FAccT 2020. Unfortunately, the names of the authors and publication details were made up! The confabulated author names were immediately obvious because we wrote the paper that was prompted to be summarized [20]. However, the rest of the details was extremely plausible the paper very well could have appeared at that conference, but did not. The first author of this paper almost missed another detail in ChatGPT's summary. The original paper described a framework with two dimensions: social and technical. The generated summary claimed the framework described three dimensions: social, technical, and cultural, which, while wrong, was plausible enough that even the very author of the paper almost missed that crucial inaccuracy!"}, {"title": "1.1 Separating Fact from Fiction", "content": "The true story above demonstrates the effortful process required to disentangle fact from fiction in GPT's output, even from someone knowledgeable of the source material. Even more notably, there was no way for our protagonist, an expert in Explainable AI, to \u201copen\u201d the black-box of ChatGPT and understand why it produced what it produced or where it might be faithful to the facts or prone to confabulation (also called hallucination). On the one hand, he lacked access to the internal details such as the parameters of the model. On the other hand, even if one did have access to the internal parameters of the model, given the scale and complexity of the neural architecture of such a large language model, interpreting it is unlikely to produce human-understandable and actionable information."}, {"title": "2 TENSIONS: \u03a7\u0391\u0399 AND LARGE LANGUAGE MODELS", "content": "The field of Explainable AI (XAI) is concerned with developing techniques, concepts, and processes that can help stakeholders understand the reasons behind the AI system's decision-making [21, 34].\nFor our purposes, we adopt a design lens in XAI that is sociotechnically-informed [12, 19, 34] and adopt the broad definition that an explanation is an answer to a why-question [11, 30, 35]. Given Al systems exist in sociotechnical settings [33, 45], it takes more than just algorithmic transparency to make them explainable [23, 35]. Thus, explaining what is happening \"inside the black box\" often requires us to also understand things \u201coutside the black box\" [12, 16, 32], requiring us to consider the entire AI lifecycle (vs. just the algorithm). For instance, why a facial recognition system disproportionately misclassified women of color [8] can be explained by looking at demographic compositions in the training data. A sociotechnically situated view of XAI expands the concept of explainability beyond the bounds of the algorithm [16] and positions it as a relational and audience-dependent construct instead of a model-inherent one [4, 5, 35, 36]. Emerging work [27, 40, 41] showcases how a broader XAI perspective can potentially address criticisms of popular algorithm-centered XAI techniques, which can be ineffective [3, 39, 48] and potentially risky [29, 44].\nWhen we consider a service such as ChatGPT, GPT-4, Microsoft Copilot, Google Gemini, Claude, or Meta AI, what prospects are there for \"opening\" the black-box of AI? These models have hundreds of billions of parameters, all acting in conjunction to generate a distribution over possible words to choose from to build a response, word by word. If we had access to all the weights, could we interpret and explain the model? If we had access to the parameters of a model and the activation values for an input could we interpret and explain the model? In the case of the above large language models the point is moot. All these models run on servers behind APIs that do not allow inspection"}, {"title": "3 IS EXPLAINABLE AI DOOMED TO FAIL?", "content": "Despite the commendable progress in algorithm-centered approaches in XAI, there are significant deficiencies. Studies examining how people actually interact with Al explanations have found popular XAI techniques to be ineffective [3, 39, 48], potentially risky [29, 44], and even obsolete in real-world deployed contexts [32]. XAI developers tend to design explanations as if people like them are going to use their systems, earning an infamous reputation of \"inmates running the asylum\" [35]. In fact, a majority of current deployments serve Al engineers instead of end-users whose needs are ignored [6]. This creates a gap between design expectations and reality- how developers envision the designed AI explanations to get interpreted and how users actually perceive those explanations in reality.\nAs Large Language Models (LLMs) become prominent, is Explainable AI a research area in flux and its infancy doomed to fail? No. There is hope. Before we throw in the towel, there are a few things to consider."}, {"title": "3.1 Al systems are Human-Al assemblages", "content": "First, the techno-centric, algorithm-centered, discourse of XAI fails to appreciate the sociotechnical reality of AI systems. When we say \"AI systems,\" what we very often mean to say is \"Human-AI assemblages,\" where the \"human\" part of the Human-Al assemblage is often implicit [16]. No real-world AI systems work in a vacuum. Black-boxes by themselves do not do the work - humans with black-boxes do the work [19]. Even if the human contribution to the work is to just provide an input, this is a significant contribution because Al systems are useful to people as tools. Thus, the explainability of Al systems entails explainability of the Human-AI assemblage, which has at least two components: the human (or humans) and the AI [16, 20]. Thus, how can we achieve the explainability of the Human-AI assemblage by just focusing on the explainability of the AI model? XAI is therefore not just technical, it is sociotechnical. It requires more than just algorithmic transparency more than being able to open the black box.\nSecond, what we mean by \"AI\" is evolving. Compared to AI systems even five years ago, the Deep Learning systems in the Foundation Model era, such as LLMs, are much more complex, have orders of magnitude more parameters, and are running at unprecedented scales. Thus, AI as a design material is tricky and is evolving [15, 20, 47]. Our understanding and expectations of what it means for AI-as-design-material to be explainable should also evolve. Further, XAI techniques that focus solely on the algorithm or the model face a new challenge: it is getting increasingly hard to open the black box! As AI systems are increasingly end-user facing, those that need the explanations the most are on the other side of an Al or user interface. This is the case for the most popular Large Language Models and chatbots, and"}, {"title": "4 HUMAN-CENTERED EXPLAINABLE AI: BEYOND ALGORITHMIC TRANSPARENCY", "content": "Given Al systems are bounded by their training data, by construction, they cannot incorporate the real-world dynamics \"outside\" the black-box. Thus, an algorithm-centered view of XAI is-by construction-a limiting view, one that handicaps the XAI system from doing what we want to do- solve real world problems. We need a paradigm that can accommodate an expansion of the epistemic canvas- an increase of the aperture of the viewing lens- to include the sociotechnical dynamics in which XAI systems are embedded so that we can do what we set out to do - solve real world problems.\nThis is where the domain of Human-Centered Explainable AI (HCXAI) [19] can help. HCXAI is a holistic vision of AI explainability, one that is human-centered and sociotechnical in nature. Situated as a Critical Technical Practice [1, 2], it draws its conceptual DNA from critical AI studies and HCI (e.g., reflective design [13, 14, 42], value-sensitive design [25]). HCXAI encourages us to critically reflect and question dominant assumptions and practices of a field, such as algorithm- centered XAI. It also adopts a value-sensitive approach to both users and designers in the development of technology that challenges the status quo of a field. HCXAI encapsulates the philosophy that not everything that is important lies inside the black box of AI. Critical insights can lie outside it. Why? Because that's where the humans are.\nThinking outside the black box of AI can help us meet our goals of helping people understand and calibrate their trust in Al systems. Even if we cannot meaningfully open the black box or interpret its complexities, there are a lot of things we can do to understand and explain the system around the black box. Increasing the aperture of XAI can help us focus on the most important part: who the human(s) is (are), what they are trying to achieve in seeking an explanation, and how to design XAI techniques that meet those needs. Indeed, explanations of the sociotechnical system can offer us an important affordance: actionability [18, 28, 43].\nAt its core, actionability is about what a user can do with the information in an explanation [18]. An actionable XAI system empowers the user by increasing the space of possible informed actions to achieve their end goals. This could be understanding how to change the inputs, contesting a decision, or learning when and how to use the system more appropriately. Actionability also addresses another important question: how do we know if an XAI system is useful? There are an increasing number of reports of XAI systems that are deployed and fail to have any measurable impact on their users [3, 44]. Many of these systems failed because the XAI systems were not designed with user needs in mind, such as by providing users with information they could already intuit themselves, by providing information that was"}, {"title": "5 THE WAY FORWARD", "content": "With the reframing around human-AI assemblages and XAI systems that place the human as the central concern, and armed with actionability as the metric for success, we now lay out three possible paths forward. This list is not meant to be exhaustive or prescriptive. It is meant to be generative by providing emerging evidence for how Human-Centered \u03a7\u0391\u0399 (HCXAI) can address the growing needs for understanding our increasingly AI-infused world."}, {"title": "5.1 Explainability outside the black-box: Social Transparency", "content": "Most consequential AI systems are embedded in organizational environments where groups of humans interact with it. These real-world AI systems, as well as the explanations they produce, are socially-situated [22, 32]. Therefore, the socio-organizational context in which these systems are used is key. Why are we not incorporating socio-organizational contexts into how we think about explainability in AI? How can we tackle the explainability of Human-AI assemblages?\nEnter Social Transparency (ST) a sociotechnically-informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making [16]. Social transparency allows us to augment the explainability of a human-AI assemblage without necessarily changing anything about the AI model. Social transparency allows one to annotate an output or behavior from an Al system with the 4W who did what, when and why. These annotations are shared between others using the system. They allow users to see whether and why others have accepted or rejected an Al's output. Social transparency does two important things: first, it challenges the dominant narrative of algorithm- centered notions of XAI; second, it expands our understanding of XAI beyond technical transparency by illustrating how adding social context can help people make better, more actionable decisions with Al systems.\nImagine the following scenario (Figure 3): Aziz is a software seller trying to use a powerful Al-based pricing tool to do something consequential: offer the right price to a client company. The AI suggests a price. Moreover, its suggestion has technical transparency - it explains its recommendation by showing Aziz the top features it considered, such as sales quota goals, comparative pricing with other clients, and costs. Confident with the Al's recommendation, Aziz makes a bid, but the client finds the price too high and walks out.\nDespite an accurate AI model and the presence of technical transparency, why did the bid fail? There could be algorithmic reasons for it. But might also be relevant contextual factors outside the box that can help explain why the bid failed. Perhaps the history between Aziz and the client that was not honored? Or maybe there were external events that happened since the model was trained, such as a pandemic-induced budgetary crisis.\nNow imagine that Aziz could see that more than 65% of his peers rejected the Al's pricing recommendation (Block 2 in Fig. 3). Or, what if Aziz knew that Jess, a director in the company, sold the product at a loss due to pandemic-related budgetary cuts? (Block 5 in Fig. 3)\nThis peripheral vision of who did what, when and why \u2013 called the 4W are the constitutive design elements of Social Transparency that can encode relevant socio-organizational context. The benefit of taking a holistic approach to explainability is clear: a study of real-world AI users in sales, cybersecurity, and healthcare found that social transparency, in the form of the 4W, helped people calibrate their trust in the Al's performance, provide actionable information for AI contestability and robust decision-making, and the organizational context made visible enabled better collective actions in the organization and strengthened the human-AI assemblages [16]."}, {"title": "5.2 Explainability around the Edges of the Black Box: Rationale Generation & Scrutability", "content": "If the black box cannot be cracked open in any meaningful sense, there is another possibility: incorporate explainability around the edges of the black box to foster a better functional understanding in the user [38] such that it fosters actionability. One of the original formulations of rationale generation [21] postulated that there was no need to know how a black box worked as long as we could learn how to give actionable advice about the black box by looking at its inputs and outputs. It was philosophically grounded in Fodor's work on Language of Thought [24]: how is it that, despite not having a 1-1 neural correlate of thought, humans can effectively communicate by translating their thoughts into words? For Human-AI interaction, even if the exact mechanism of the (artificial) neural correlate of AI's thought was not known to the human, as long as actionable information is present in the explanation from an Al agent, the Human-Al interaction can proceed. In short, explanations that do not directly access the model can still generate actionable information.\nIn the case of large language models, the actionable information is whether any particular input is likely to produce a reliable response that can be trusted. Large language models might be generally capable at many tasks such as"}, {"title": "5.3 Explainability by Leveraging Infrastructural Seams: Seamful XA\u0399", "content": "No Al system is perfect. Mistakes are inevitable. Breakdowns in Al systems often occur when the assumptions we make in design and development do not hold true when they are deployed in the real-world. For example, an AI system can fail when it is trained on data from North America but deployed in South Asia, especially when the end user is unaware of this infrastructural mismatch. These mismatches between design assumptions and real-world usage are called seams [17]. Handling the mistakes from Al systems is hard, especially when the Al's decision-making is hidden or black-boxed. Although black-boxing AI systems can make the user experience seamless and easy to use, concealing the seams can lead to downstream harms for end-users, such as uncritical AI acceptance. What can we do differently? How do we move beyond seamless AI? And what can we gain by doing so?\nSeamful XAI is a design lens that incorporates the principles of seamful design [10] to augment explainability and user agency. A classic example of seamful design is a \"seamful map\" of WiFi coverage in your home. If you know the WiFi's dead zones in your home, you will be able to best use it because you can then avoid.Without revealing the seams, users can have reasonable expectations of perfect WiFi. The map makes the seams in the WiFi's infrastructure visible to users, which allows them to recalibrate their expectations and behavior. A seamful design principle asks us to leverage the weakness in opportunistic ways [26].\nUnlike seamlessness, seamful design does not aim to hide the infrastructure. Rather, it puts the infrastructure and all its imperfections front and center. Seamful design helps us recognize and grapple with the complex infrastructures systems reside in. Conversely, seamless design ideals risks making the labor it takes to make the system work invisible (e.g., datawork, ghostwork, maintenance work). And, as invisible work is invariably unaccounted for and unappreciated, workers who conduct this work will feel undervalued or invisible. Seamfulness embraces the imperfect reality of spaces we inhabit and makes the most out of it.\nIn the context of AI, seams can be conceptualized as mismatches, gaps, or cracks in assumptions between the world of how Al systems are designed and the world of how Al systems are used in practice. Seamful XAI seeks to empower users with information that augments their agency by identifying gaps between ideal design assumptions and reality.\nAt the heart of Seamful XAI are four observations:\n(1) Seams are inevitable, arising from the integration of heterogeneous sociotechnical components during technology deployments.\n(2) Seams are revealed through system breakdowns.\n(3) Instead of treating seams as problematic negatives to be erased, they can be used strategically to calibrate users' reliance and understanding of an Al system.\n(4) The goal of this strategic revelation (and concealment) is to support user agency (actionability, contestability, and appropriation).\nSeamful XAI Design Process: Let's review the design process proposed by [17]."}, {"title": "6 TAKEAWAYS", "content": "We began with the provocation: With the advent of Foundation Models & Large Language Models like ChatGPT, is \"opening the black-box\" still a reasonable and achievable goal for XAI? Do we need to shift our perspectives?\nYes. The proverbial \"black-box\" of AI has evolved, and so should our expectations on how to make it explainable. As the box becomes more opaque and harder to \"open,\" the human side of the Human-AI assemblage remains as a fruitful space to explore. In the most extreme case, the human side may be all there is left to explore. Even if we can open the black box it is unclear what actionable outcomes would become available.\nThere are four important lessons from Human-centered XAI that can inform the shift in our XAI expectations.\n(1) First, the human-centered XAI perspective takes a pragmatic and resourceful view of explainability, especially if black boxes are expected to persist. By considering the actions afforded to the user by the explanations, HCXAI centers the focus on the human, ensuring AI augments human abilities rather than replace them.\n(2) Second, explainability is not only achieved by looking inside the black box through mechanistic descriptions of how an algorithm works. Actionability can be achieved by exploring explainability outside and around the edges of the black box because human-centered XAI takes a more expansive view of what it means to provide insights into a black box that can afford a wider range of actions.\n(3) Third, explicitly treating AI systems as human-AI assemblages means focusing on explainability of the assemblage, not just the AI. This widened perspective opens up avenues for not just factoring in who is interacting with the black box, but also how human teams can work together - directly or indirectly \u2013 to contextualize a dynamically changing real-world AI behavior.\n(4) Fourth, seamful XAI turns the disadvantages and weaknesses of an Al system into advantages. The gaps between user expectations and AI capabilities are exactly the gaps that explanations address. Instead of hiding those gaps to create seamless experiences, seamful XAI leverages these gaps in an opportunistic manner to augment explainability and user agency.\nAs we reload our expectations on XAI, we invite you to do what HCXAI asks us to do: centering the design and evaluation around the human. This positioning can reveal unmet needs that must be addressed while avoiding the costly mistake of building XAI systems that do not make a difference. While there have been many examples of XAI systems that have failed to have the intended impact of users, it is often the case that these tenets of HCXAI were overlooked. XAI is a relatively young field of research that has yet to find its footing, even as the landscape of black box Al systems is rapidly evolving. It is not yet time to give up hope on XAI. Instead, we invite you to adopt critical reflection and value-sensitivity into XAI research and evaluation, making it human-centered.\nWill Human-centered XAI solve all our problems? No, but it will help us ask the right questions."}]}