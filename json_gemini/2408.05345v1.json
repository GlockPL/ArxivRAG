{"title": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models", "authors": ["UPOL EHSAN", "MARK O. RIEDL"], "abstract": "When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) \"black-box\" of AI\nso that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the\nblack-box is increasingly limited especially when it comes to non-Al expert end-users. In this paper, we challenge the assumption of\n\"opening\" the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an\nalgorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by\nsynthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black\nbox, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.", "sections": [{"title": "1 PROVOCATION", "content": "With the advent of Foundation Models & Large Language Models like ChatGPT, is \"opening the black-box\" still a reasonable\nand achievable goal for Explainable AI (XAI)? Do we need to shift our perspectives?\nIn February of 2023, Nadeem (pseudonym), a relative of the first author of this article, asked if ChatGPT could be\nused to do homework. Nadeem is a high-schooler who shared that he was always under a tight deadline and needed to\nbe more \"efficient\" with his homework. He heard from his friends that ChatGPT can help summarize papers or books,\nwhich can make Nadeem more \"productive\" with his homework.\nBefore responding to Nadeem, ChatGPT was taken for a test drive. It was prompted to summarize an academic paper\n(Figure 1 similar to how Nadeem might use it as someone who was not an Al researcher or experienced prompting\nLarge Language Models (LLMs). Fortunately, ChatGPT generated a coherent response. ChatGPT gave the names of the\nauthors of the paper and details about the paper's publication at ACM FAccT 2020. Unfortunately, the names of the\nauthors and publication details were made up! The confabulated author names were immediately obvious because we\nwrote the paper that was prompted to be summarized [20]. However, the rest of the details was extremely plausible\nthe paper very well could have appeared at that conference, but did not. The first author of this paper almost missed\nanother detail in ChatGPT's summary. The original paper described a framework with two dimensions: social and\ntechnical. The generated summary claimed the framework described three dimensions: social, technical, and cultural,\nwhich, while wrong, was plausible enough that even the very author of the paper almost missed that crucial inaccuracy!"}, {"title": "1.1 Separating Fact from Fiction", "content": "The true story above demonstrates the effortful process required to disentangle fact from fiction in GPT's output, even\nfrom someone knowledgeable of the source material. Even more notably, there was no way for our protagonist, an\nexpert in Explainable AI, to \u201copen\u201d the black-box of ChatGPT and understand why it produced what it produced or\nwhere it might be faithful to the facts or prone to confabulation (also called hallucination). On the one hand, he lacked\naccess to the internal details such as the parameters of the model. On the other hand, even if one did have access to the\ninternal parameters of the model, given the scale and complexity of the neural architecture of such a large language\nmodel, interpreting it is unlikely to produce human-understandable and actionable information."}, {"title": "2 TENSIONS: \u03a7\u0391\u0399 AND LARGE LANGUAGE MODELS", "content": "The field of Explainable AI (XAI) is concerned with developing techniques, concepts, and processes that can help\nstakeholders understand the reasons behind the AI system's decision-making [21, 34].\nFor our purposes, we adopt a design lens in XAI that is sociotechnically-informed [12, 19, 34] and adopt the broad\ndefinition that an explanation is an answer to a why-question [11, 30, 35]. Given Al systems exist in sociotechnical\nsettings [33, 45], it takes more than just algorithmic transparency to make them explainable [23, 35]. Thus, explaining\nwhat is happening \"inside the black box\" often requires us to also understand things \u201coutside the black box\" [12, 16, 32],\nrequiring us to consider the entire AI lifecycle (vs. just the algorithm). For instance, why a facial recognition system\ndisproportionately misclassified women of color [8] can be explained by looking at demographic compositions in\nthe training data. A sociotechnically situated view of XAI expands the concept of explainability beyond the bounds\nof the algorithm [16] and positions it as a relational and audience-dependent construct instead of a model-inherent\none [4, 5, 35, 36]. Emerging work [27, 40, 41] showcases how a broader XAI perspective can potentially address criticisms\nof popular algorithm-centered XAI techniques, which can be ineffective [3, 39, 48] and potentially risky [29, 44].\nWhen we consider a service such as ChatGPT, GPT-4, Microsoft Copilot, Google Gemini, Claude, or Meta AI, what\nprospects are there for \"opening\" the black-box of AI? These models have hundreds of billions of parameters, all acting\nin conjunction to generate a distribution over possible words to choose from to build a response, word by word. If\nwe had access to all the weights, could we interpret and explain the model? If we had access to the parameters of\na model and the activation values for an input could we interpret and explain the model? In the case of the above\nlarge language models the point is moot. All these models run on servers behind APIs that do not allow inspection"}, {"title": "3 IS EXPLAINABLE AI DOOMED TO FAIL?", "content": "Despite the commendable progress in algorithm-centered approaches in XAI, there are significant deficiencies. Studies\nexamining how people actually interact with Al explanations have found popular XAI techniques to be ineffective [3,\n39, 48], potentially risky [29, 44], and even obsolete in real-world deployed contexts [32]. XAI developers tend to design\nexplanations as if people like them are going to use their systems, earning an infamous reputation of \"inmates running\nthe asylum\" [35]. In fact, a majority of current deployments serve Al engineers instead of end-users whose needs are\nignored [6]. This creates a gap between design expectations and reality- how developers envision the designed AI\nexplanations to get interpreted and how users actually perceive those explanations in reality.\nAs Large Language Models (LLMs) become prominent, is Explainable AI a research area in flux and its infancy\ndoomed to fail? No. There is hope. Before we throw in the towel, there are a few things to consider."}, {"title": "3.1 Al systems are Human-Al assemblages", "content": "First, the techno-centric, algorithm-centered, discourse of XAI fails to appreciate the sociotechnical reality of AI systems.\nWhen we say \"AI systems,\" what we very often mean to say is \"Human-AI assemblages,\" where the \"human\" part of the\nHuman-Al assemblage is often implicit [16]. No real-world AI systems work in a vacuum. Black-boxes by themselves\ndo not do the work - humans with black-boxes do the work [19]. Even if the human contribution to the work is to\njust provide an input, this is a significant contribution because Al systems are useful to people as tools. Thus, the\nexplainability of Al systems entails explainability of the Human-AI assemblage, which has at least two components: the\nhuman (or humans) and the AI [16, 20]. Thus, how can we achieve the explainability of the Human-AI assemblage by\njust focusing on the explainability of the AI model? XAI is therefore not just technical, it is sociotechnical. It requires\nmore than just algorithmic transparency more than being able to open the black box.\nSecond, what we mean by \"AI\" is evolving. Compared to AI systems even five years ago, the Deep Learning systems\nin the Foundation Model era, such as LLMs, are much more complex, have orders of magnitude more parameters,\nand are running at unprecedented scales. Thus, AI as a design material is tricky and is evolving [15, 20, 47]. Our\nunderstanding and expectations of what it means for AI-as-design-material to be explainable should also evolve. Further,\nXAI techniques that focus solely on the algorithm or the model face a new challenge: it is getting increasingly hard to\nopen the black box! As AI systems are increasingly end-user facing, those that need the explanations the most are on\nthe other side of an Al or user interface. This is the case for the most popular Large Language Models and chatbots, and"}, {"title": "4 HUMAN-CENTERED EXPLAINABLE AI: BEYOND ALGORITHMIC TRANSPARENCY", "content": "Given Al systems are bounded by their training data, by construction, they cannot incorporate the real-world dynamics\n\"outside\" the black-box. Thus, an algorithm-centered view of XAI is-by construction-a limiting view, one that handicaps\nthe XAI system from doing what we want to do- solve real world problems. We need a paradigm that can accommodate\nan expansion of the epistemic canvas- an increase of the aperture of the viewing lens- to include the sociotechnical\ndynamics in which XAI systems are embedded so that we can do what we set out to do - solve real world problems.\nThis is where the domain of Human-Centered Explainable AI (HCXAI) [19] can help. HCXAI is a holistic vision of AI\nexplainability, one that is human-centered and sociotechnical in nature. Situated as a Critical Technical Practice [1, 2], it\ndraws its conceptual DNA from critical AI studies and HCI (e.g., reflective design [13, 14, 42], value-sensitive design [25]).\nHCXAI encourages us to critically reflect and question dominant assumptions and practices of a field, such as algorithm-\ncentered XAI. It also adopts a value-sensitive approach to both users and designers in the development of technology\nthat challenges the status quo of a field. HCXAI encapsulates the philosophy that not everything that is important lies\ninside the black box of AI. Critical insights can lie outside it. Why? Because that's where the humans are.\nThinking outside the black box of AI can help us meet our goals of helping people understand and calibrate their\ntrust in Al systems. Even if we cannot meaningfully open the black box or interpret its complexities, there are a lot of\nthings we can do to understand and explain the system around the black box. Increasing the aperture of XAI can help us\nfocus on the most important part: who the human(s) is (are), what they are trying to achieve in seeking an explanation,\nand how to design XAI techniques that meet those needs. Indeed, explanations of the sociotechnical system can offer us\nan important affordance: actionability [18, 28, 43].\nAt its core, actionability is about what a user can do with the information in an explanation [18]. An actionable XAI\nsystem empowers the user by increasing the space of possible informed actions to achieve their end goals. This could\nbe understanding how to change the inputs, contesting a decision, or learning when and how to use the system more\nappropriately. Actionability also addresses another important question: how do we know if an XAI system is useful?\nThere are an increasing number of reports of XAI systems that are deployed and fail to have any measurable impact on\ntheir users [3, 44]. Many of these systems failed because the XAI systems were not designed with user needs in mind,\nsuch as by providing users with information they could already intuit themselves, by providing information that was"}, {"title": "5 THE WAY FORWARD", "content": "With the reframing around human-AI assemblages and XAI systems that place the human as the central concern, and\narmed with actionability as the metric for success, we now lay out three possible paths forward. This list is not meant\nto be exhaustive or prescriptive. It is meant to be generative by providing emerging evidence for how Human-Centered\n\u03a7\u0391\u0399 (HCXAI) can address the growing needs for understanding our increasingly AI-infused world."}, {"title": "5.1 Explainability outside the black-box: Social Transparency", "content": "Most consequential AI systems are embedded in organizational environments where groups of humans interact with it.\nThese real-world AI systems, as well as the explanations they produce, are socially-situated [22, 32]. Therefore, the\nsocio-organizational context in which these systems are used is key. Why are we not incorporating socio-organizational\ncontexts into how we think about explainability in AI? How can we tackle the explainability of Human-AI assemblages?\nEnter Social Transparency (ST) a sociotechnically-informed perspective that incorporates the socio-organizational\ncontext into explaining AI-mediated decision-making [16]. Social transparency allows us to augment the explainability\nof a human-AI assemblage without necessarily changing anything about the AI model. Social transparency allows one\nto annotate an output or behavior from an Al system with the 4W who did what, when and why. These annotations\nare shared between others using the system. They allow users to see whether and why others have accepted or rejected\nan Al's output. Social transparency does two important things: first, it challenges the dominant narrative of algorithm-\ncentered notions of XAI; second, it expands our understanding of XAI beyond technical transparency by illustrating\nhow adding social context can help people make better, more actionable decisions with Al systems.\nImagine the following scenario (Figure 3): Aziz is a software seller trying to use a powerful Al-based pricing tool to\ndo something consequential: offer the right price to a client company. The AI suggests a price. Moreover, its suggestion\nhas technical transparency - it explains its recommendation by showing Aziz the top features it considered, such as\nsales quota goals, comparative pricing with other clients, and costs. Confident with the Al's recommendation, Aziz\nmakes a bid, but the client finds the price too high and walks out.\nDespite an accurate AI model and the presence of technical transparency, why did the bid fail? There could be\nalgorithmic reasons for it. But might also be relevant contextual factors outside the box that can help explain why the\nbid failed. Perhaps the history between Aziz and the client that was not honored? Or maybe there were external events\nthat happened since the model was trained, such as a pandemic-induced budgetary crisis.\nNow imagine that Aziz could see that more than 65% of his peers rejected the Al's pricing recommendation (Block 2\nin Fig. 3). Or, what if Aziz knew that Jess, a director in the company, sold the product at a loss due to pandemic-related\nbudgetary cuts? (Block 5 in Fig. 3)\nThis peripheral vision of who did what, when and why \u2013 called the 4W are the constitutive design elements of\nSocial Transparency that can encode relevant socio-organizational context. The benefit of taking a holistic approach to\nexplainability is clear: a study of real-world AI users in sales, cybersecurity, and healthcare found that social transparency,\nin the form of the 4W, helped people calibrate their trust in the Al's performance, provide actionable information for AI\ncontestability and robust decision-making, and the organizational context made visible enabled better collective actions\nin the organization and strengthened the human-AI assemblages [16]."}, {"title": "5.2 Explainability around the Edges of the Black Box: Rationale Generation & Scrutability", "content": "If the black box cannot be cracked open in any meaningful sense, there is another possibility: incorporate explainability\naround the edges of the black box to foster a better functional understanding in the user [38] such that it fosters\nactionability. One of the original formulations of rationale generation [21] postulated that there was no need to know\nhow a black box worked as long as we could learn how to give actionable advice about the black box by looking at\nits inputs and outputs. It was philosophically grounded in Fodor's work on Language of Thought [24]: how is it that,\ndespite not having a 1-1 neural correlate of thought, humans can effectively communicate by translating their thoughts\ninto words? For Human-AI interaction, even if the exact mechanism of the (artificial) neural correlate of AI's thought\nwas not known to the human, as long as actionable information is present in the explanation from an Al agent, the\nHuman-Al interaction can proceed. In short, explanations that do not directly access the model can still generate\nactionable information.\nIn the case of large language models, the actionable information is whether any particular input is likely to produce\na reliable response that can be trusted. Large language models might be generally capable at many tasks such as"}, {"title": "5.3 Explainability by Leveraging Infrastructural Seams: Seamful XA\u0399", "content": "No Al system is perfect. Mistakes are inevitable. Breakdowns in Al systems often occur when the assumptions we\nmake in design and development do not hold true when they are deployed in the real-world. For example, an AI system\ncan fail when it is trained on data from North America but deployed in South Asia, especially when the end user is\nunaware of this infrastructural mismatch. These mismatches between design assumptions and real-world usage are\ncalled seams [17]. Handling the mistakes from Al systems is hard, especially when the Al's decision-making is hidden\nor black-boxed. Although black-boxing AI systems can make the user experience seamless and easy to use, concealing\nthe seams can lead to downstream harms for end-users, such as uncritical AI acceptance. What can we do differently?\nHow do we move beyond seamless AI? And what can we gain by doing so?\nSeamful XAI is a design lens that incorporates the principles of seamful design [10] to augment explainability and\nuser agency. A classic example of seamful design is a \"seamful map\" of WiFi coverage in your home. If you know the\nWiFi's dead zones in your home, you will be able to best use it because you can then avoid.Without revealing the seams,\nusers can have reasonable expectations of perfect WiFi. The map makes the seams in the WiFi's infrastructure visible to\nusers, which allows them to recalibrate their expectations and behavior. A seamful design principle asks us to leverage\nthe weakness in opportunistic ways [26].\nUnlike seamlessness, seamful design does not aim to hide the infrastructure. Rather, it puts the infrastructure and all\nits imperfections front and center. Seamful design helps us recognize and grapple with the complex infrastructures\nsystems reside in. Conversely, seamless design ideals risks making the labor it takes to make the system work invisible\n(e.g., datawork, ghostwork, maintenance work). And, as invisible work is invariably unaccounted for and unappreciated,\nworkers who conduct this work will feel undervalued or invisible. Seamfulness embraces the imperfect reality of spaces\nwe inhabit and makes the most out of it.\nIn the context of AI, seams can be conceptualized as mismatches, gaps, or cracks in assumptions between the world of\nhow Al systems are designed and the world of how Al systems are used in practice. Seamful XAI seeks to empower users\nwith information that augments their agency by identifying gaps between ideal design assumptions and reality.\nAt the heart of Seamful XAI are four observations:\n(1) Seams are inevitable, arising from the integration of heterogeneous sociotechnical components during technology\ndeployments.\n(2) Seams are revealed through system breakdowns.\n(3) Instead of treating seams as problematic negatives to be erased, they can be used strategically to calibrate users'\nreliance and understanding of an Al system.\n(4) The goal of this strategic revelation (and concealment) is to support user agency (actionability, contestability,\nand appropriation).\nSeamful XAI Design Process: Let's review the design process proposed by [17]."}, {"title": "6 TAKEAWAYS", "content": "We began with the provocation: With the advent of Foundation Models & Large Language Models like ChatGPT, is \"opening\nthe black-box\" still a reasonable and achievable goal for XAI? Do we need to shift our perspectives?\nYes. The proverbial \"black-box\" of AI has evolved, and so should our expectations on how to make it explainable. As\nthe box becomes more opaque and harder to \"open,\" the human side of the Human-AI assemblage remains as a fruitful\nspace to explore. In the most extreme case, the human side may be all there is left to explore. Even if we can open the\nblack box it is unclear what actionable outcomes would become available.\nThere are four important lessons from Human-centered XAI that can inform the shift in our XAI expectations.\n(1) First, the human-centered XAI perspective takes a pragmatic and resourceful view of explainability, especially if\nblack boxes are expected to persist. By considering the actions afforded to the user by the explanations, HCXAI\ncenters the focus on the human, ensuring AI augments human abilities rather than replace them.\n(2) Second, explainability is not only achieved by looking inside the black box through mechanistic descriptions of\nhow an algorithm works. Actionability can be achieved by exploring explainability outside and around the edges\nof the black box because human-centered XAI takes a more expansive view of what it means to provide insights\ninto a black box that can afford a wider range of actions.\n(3) Third, explicitly treating AI systems as human-AI assemblages means focusing on explainability of the assemblage,\nnot just the AI. This widened perspective opens up avenues for not just factoring in who is interacting with the\nblack box, but also how human teams can work together - directly or indirectly \u2013 to contextualize a dynamically\nchanging real-world AI behavior.\n(4) Fourth, seamful XAI turns the disadvantages and weaknesses of an Al system into advantages. The gaps between\nuser expectations and AI capabilities are exactly the gaps that explanations address. Instead of hiding those\ngaps to create seamless experiences, seamful XAI leverages these gaps in an opportunistic manner to augment\nexplainability and user agency.\nAs we reload our expectations on XAI, we invite you to do what HCXAI asks us to do: centering the design and\nevaluation around the human. This positioning can reveal unmet needs that must be addressed while avoiding the\ncostly mistake of building XAI systems that do not make a difference. While there have been many examples of XAI\nsystems that have failed to have the intended impact of users, it is often the case that these tenets of HCXAI were\noverlooked. XAI is a relatively young field of research that has yet to find its footing, even as the landscape of black\nbox Al systems is rapidly evolving. It is not yet time to give up hope on XAI. Instead, we invite you to adopt critical\nreflection and value-sensitivity into XAI research and evaluation, making it human-centered.\nWill Human-centered XAI solve all our problems? No, but it will help us ask the right questions."}]}