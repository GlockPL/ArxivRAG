{"title": "Bridging the Data Gap in AI Reliability Research and Establishing DR-AIR, a Comprehensive Data Repository for AI Reliability", "authors": ["Simin Zheng", "Jared M. Clark", "Fatemeh Salboukh", "Priscila Silva", "Karen da Mata", "Fenglian Pan", "Jie Min", "Jiayi Lian", "Caleb B. King", "Lance Fiondella", "Jian Liu", "Xinwei Deng", "Yili Hong"], "abstract": "Artificial intelligence (AI) technology and systems have been advancing rapidly. However, ensuring the reliability of these systems is crucial for fostering public confidence in their use. This necessitates the modeling and analysis of reliability data specific to AI systems. A major challenge in AI reliability research, particularly for those in academia, is the lack of readily available AI reliability data. To address this gap, this paper focuses on conducting a comprehensive review of available AI reliability data and establishing DR-AIR: a data repository for AI reliability. Specifically, we introduce key measurements and data types for assessing AI reliability, along with the methodologies used to collect these data. We also provide a detailed description of the currently available datasets with illustrative examples. Furthermore, we outline the setup of the DR-AIR repository and demonstrate its practical applications. This repository provides easy access to datasets specifically curated for AI reliability research. We believe these efforts will significantly benefit the AI research community by facilitating access to valuable reliability data and promoting collaboration across various academic domains within AI. We conclude our paper with a call to action, encouraging the research community to contribute and share AI reliability data to further advance this critical field of study.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Motivation and Objectives", "content": "Artificial intelligence (AI) technology and systems have been advancing at an unprecedented pace. Examples of AI systems include autonomous systems such as self-driving cars, drones, and industrial robots (e.g., Soori et al. 2023); natural language processing (NLP) systems like advanced conversational AI systems (e.g., ChatGPT), virtual assistants (e.g., Alexa), and language translation tools (Mohamed et al. 2024); computer vision systems like facial recognition (Nawaz 2020); and AI systems in healthcare (Koski and Murphy 2021). While there is considerable excitement surrounding AI technology, ensuring the reliability of these systems is essential for building public confidence in their wide adoption. Reliability issues can result in significant losses and even catastrophic failures, highlighting the importance of AI reliability.\nEnsuring the reliability of AI systems requires the modeling and analysis of reliability data specific to these emerging technologies. However, a significant challenge in AI reliability research is the lack of readily available data, which arises from several factors. First, as AI technology is still in its rapidly evolving early stages, development efforts often prioritize performance metrics like accuracy and speed over reliability and other important metrics, such as robustness. Second, while industries may generate and utilize reliability data through applied testing, the academic sector often lacks comparable testbeds, resulting in a data gap. Third, data sharing presents additional challenges. Industrial reliability data are often proprietary and sensitive, limiting access for academic researchers. These factors collectively create challenges for the AI reliability research community, particularly in academia, where access to real data is important for effectively modeling and analyzing AI reliability.\nTherefore, this paper seeks to address the data gap in AI reliability research by introducing key concepts related to AI reliability data, providing a comprehensive review of the currently available data, and establishing a novel public data repository to facilitate data sharing. This work is essential because data is a cornerstone of research. Researchers in AI reliability come from diverse fields, including machine learning (ML), statistics, electrical engineering, computer engineering, industrial systems engineering, and other related disciplines. While datasets exist, they are fragmented across disciplines, and inconsistent terminology often complicates their integration. In addition, some public data repositories, such as UC Irvine (2025) and Kaggle (2025), provide access to datasets suitable for statistical analysis and ML model building. However, they do not specifically focus on AI reliability analysis, which is the primary focus of our study.\nBefore collecting data, it is important to identify the appropriate metrics for evaluating AI reliability. In this paper, we discuss key measurements and data types for assessing AI"}, {"title": "1.2 Literature Review and The Contribution", "content": "AI systems have become increasingly popular and widely used across many fields. With advancements in AI technology, demonstrating the reliability of these systems is essential for their confident use. Werner and Schumeg (2022) proposed a framework for developing, qualifying, and releasing reliable and assured AI systems by applying design for reliability tools and techniques during the design and development phases. Blood et al. (2023) highlighted that traditional reliability tools need to be transformed to address the reliability of AI systems. Hong et al. (2023) provided a compressive discussion on statistical reliability for AI systems. Existing research has made significant contributions in the field of AI systems, highlighting the importance of exploring the data used in AI reliability research.\nTo further emphasize the importance of data exploration for AI reliability research, several data collection methods from the field of traditional reliability analysis have already been investigated. Some of these methods could be further extended to AI system reliability studies. Smith (2021) illustrated a method for failure data collection, as well as a structured approach to recording the data using a formal document from the field, which can be used for reliability analysis. Meeker et al. (2022) introduced data collection strategies that can be applied to planning reliability studies, as well as to data analysis and modeling in reliability research. Inel et al. (2023) developed a responsible AI methodology designed to guide data collection, which can be used to assess the robustness of data used for AI applications in the real world. However, a gap remains in the detailed introduction of data collection methods specifically for AI system reliability research.\nAlthough AI system reliability research has emerged as a growing field in recent years, the availability of data for this research remains limited. For AI system reliability analysis, Hong et al. (2023) used the public AI Incident database (2021), which primarily collects AI incidents from news reports, and applied a text mining method to identify variables that contribute the most to AI incident data to illustrate the importance of AI reliability. Min et al. (2022) and Zheng et al. (2023) both used publicly accessible data from the California Department"}, {"title": "1.3 Overview", "content": "The rest of the paper is organized as follows. Section 2 discusses commonly used metrics and measurements for AI reliability, along with their associated data types. Section 3 discusses methods and strategies for effective data collection to support AI reliability. Section 4 introduces the datasets we have collected, providing detailed examples of their use in modeling and analysis. Section 5 explains the setup of the online repository, including its data format and usage guidelines. Finally, Section 6 offers concluding remarks and highlights calls to action for advancing AI reliability data collection, modeling, and analysis."}, {"title": "2 Measurements and Data Types", "content": "In this section, we provide a comprehensive discussion on measurement and data types for AI reliability, including covariate information, which plays an important role in addressing the data gap in AI reliability research."}, {"title": "2.1 AI Reliability Measurement", "content": "We first focus on addressing how AI reliability can be measured. The work of Hong et al. (2023) outlines the process of defining AI reliability metrics. Here, failure rate, event rate, and error rate are all noted as possibilities for measuring the reliability of an AI system. AI systems are built with an intended purpose. In general, an AI system is reliable if it can perform its purpose for a \"long\" period of time, which corresponds to the formal definition of reliability. Reliability is defined as the ability of a system (or component) to consistently perform its intended function without failure over a specified period under specified conditions.\nSome AI systems may experience failures over time. For AI reliability, we need to be aware of both hardware and software failures. Hardware failures occur when the physical components of a system no longer work. A malfunctioning GPU would be an example of a hardware failure. On the other hand, a software failure occurs when the AI system fails to fulfill its intended purpose successfully. Note that not all failures prevent the system from being used in the future. In this context, we use \u201cfailure\u201d as a broad term, allowing for the possibility of multiple failures in the same unit. When failures are clearly defined, possible reliability metrics may include whether a system failed, how long it operated before failure, or, in the case of multiple failures, the rate of failure events. For example, autonomous vehicles (AVs) are one area where AI reliability is currently under investigation. The work of Min et al. (2022) considers disengagement as its \u201cfailure\u201d mode. A disengagement event occurs when the AV exits autonomous mode and gives control to the driver. Pan et al. (2024) defines the failure of AVs in terms of errors in the perception system. Both studies have different but clearly defined metrics for failure. These studies can each provide different but meaningful insights into the reliability of AI systems employed in AVs.\nDifferent from traditional reliability, failure in many AI systems is not easily defined in terms of time. Generative AI systems are often assessed based on the accuracy of their output. For example, chatbots are often able to provide code for programming problems. The AI system can then be evaluated based on how accurately it executes the task specified in the prompt. For these systems, a time component may be less meaningful. Instead, we focus on assessing the overall error rate in order to determine how often the model is successfully performing the required task. The work of Lian et al. (2021) considers the accuracy of classification from AI algorithms. In this case, classification accuracy metrics are used to measure the reliability of the AI systems under investigation. For another example, adversarial networks play a large role in current machine learning research. Faddi et al. (2024) also considers AI classification algorithms. In this case, the authors employ adversarial attacks which aim to cause misclassification from the AI system. AI reliability is measured in multiple ways. First, the times of successful adversarial attacks were recorded. Additionally,"}, {"title": "2.2 Data Types", "content": "Next, we discuss the data types used in reliability measurement. The data type of the response variable is particularly important, as it determines the appropriate statistical models for subsequent analysis. Various data types are used in reliability studies, including binary data, count data, continuous measurement data, time-to-event data, recurrent event data, and degradation data. In the following sections, we provide a detailed discussion of each.\nBinary responses are commonly used in early reliability studies, where outcomes are recorded as pass/fail. These data are modeled using a Bernoulli distribution. In the absence of covariates, the distribution can be parameterized with a shared probability of event occurrence. When covariates are present, the generalized linear model (GLM) is typically used for analysis (e.g., McCullagh and Nelder 1999). The most commonly applied GLMs for binary data are logistic and probit regression. In AI reliability studies, binary outcomes can also be relevant.\nCount data typically arises when the response represents the number of events (e.g., failures) occurring within specific time and unit constraints. These data are typically modeled as following a Poisson distribution. In the presence of covariates, these data can also be analyzed with the use of GLMs. The failure count data of Faddi et al. (2024) is an example of count data. In this example, the AI system classifies images, and we record the number of misclassified images for the run of the adversarial network experiment.\nContinuous measurement responses are less common in traditional reliability studies but are more prevalent in AI reliability. These responses take values on the real line and are often modeled using a normal distribution with unknown location and scale parameters. However, a normal assumption may not always be appropriate, requiring functional transformations to ensure unbounded support or reduce skew. In cases of bounded responses, normal fits may still be reasonable if probabilities beyond the bounds are negligible. An example is provided in Lian et al. (2021), where AI classification reliability is analyzed using the mean area under the curve (AUC) and the log standard deviation of AUC, both of which are continuous measures of classification accuracy.\nTime-to-event (or time-to-failure) data is crucial in reliability analysis, recording the time until an event for each unit (Meeker et al. 2022). Some units may not experience an event during observation, resulting in right-censored data. Although censored observations do not provide exact failure times, they still contribute to likelihood estimation, impact assessment, and inference. Time-to-event data is fundamental in traditional reliability studies, with common models assuming log-normal or Weibull distributions, or more generally, the log-location-"}, {"title": "2.3 Covariates", "content": "In traditional reliability analysis, covariates are useful because they help explain more variability in the responses and enhance the predictability of future outcomes. Typically, traditional reliability data, such as those from ALT, do not include long lists of variables. The accelerating variables are usually limited to one or two, such as temperature or voltage. However, in Al reliability, a wide variety of covariates can be collected for analysis, offering more opportunities for statistical modeling and analysis. For example, in AI/ML models, the type of algorithm becomes a factor in the dataset, and this information is included as a covariate. This is illustrated in Lian et al. (2021), where different algorithms are compared in terms of robustness to unbalanced data. Similarly, the operating company of an AI system plays a comparable role in Min et al. (2022), as different companies may use different systems, and we seek to understand how their vehicles compare. Min et al. (2022) also include mileage information as a covariate. When investigating algorithms through simulation, simulation settings can also serve as covariates. For instance, Faddi et al. (2024) include the percentage of adversarial attacks created by two different algorithms in the dataset. Likewise, Lian et al. (2021) include the proportion of each class used in the training dataset as the covariate.\nThere are several general model strategies to incorporate covariates to explain the response."}, {"title": "3 Designs and Methods for Data Collection", "content": "In this section, we provide a comprehensive description on AI reliability data collection, covering key aspects such as the two main data sources (laboratory vs. field), the two methods of data collection (virtual vs. physical), and relevant statistical techniques, including DoE and ALT."}, {"title": "3.1 Laboratory Tests and Field Tracking Studies", "content": "Data collection serves as the foundation for AI reliability research. There are various designs and methods for data collection. As Karunarathna et al. (2024) pointed out, choosing the appropriate data collection method is important and depends on the specific research questions. Traditional reliability data are collected through either laboratory tests or field tracking studies.\nData collection using laboratory tests involves gathering data under controlled experimental conditions, typically within a laboratory setting where variables and conditions are precisely regulated. Traditionally, product reliability is first tested in a laboratory environment, followed by an assessment of its reliability, leading to the generation of laboratory test data. Since most AI systems are software-based, testing them in a laboratory environment is convenient. Laboratory tests can be conducted at various levels, such as the algorithm level, module level, or system level. At the algorithm level, the test involves running the algorithm on a computer. For example, Lian et al. (2021) and Faddi et al. (2024) evaluated the performance of CNNs in a laboratory environment. Pan et al. (2022) tested an AV perception system (module-level test) in a laboratory environment. Howard et al. (2021) employed laboratory tests to collect data for evaluating the reliability and validity of a face recognition system, which can be regarded as a system-level test. Although laboratory testing can be comprehensive, its operating environment may differ from real-world scenarios. Thus, a field tracking study may be necessary.\nField studies involve collecting data outside of experimental or laboratory settings. This type of data collection is most often conducted in natural environments. The key difference with the field tracking studies method is the use of experimental methods in a \u201cfield\u201d situation where the data can be controlled to a limited extent, as pointed out by Fellows and Liu (2021). It aims to capture more original and representative data compared to controlled laboratory tests; however, it can also be expensive and time-consuming. In the AI reliability area, the California DMV study analyzed in Min et al. (2022) can be considered a field tracking study, where AVs are tested on city roads, and reliability data are collected for analysis.\nBased on Gupta and Gupta (2022), we summarized and developed a typical workflow for data collection in AI reliability studies using the field tracking method. More details can be found in Figure 1. Specifically, before conducting field tracking studies for AI reliability research, researchers must first define the specific research question. Once clarified, they should establish a hypothesis to explain expected outcomes. Based on this hypothesis, researchers identify the relevant data to observe, guiding the design of the study. The collected data is then preprocessed based on the specific research questions. Finally, the data is processed for analysis to test the hypothesis, determining whether it should be accepted or rejected."}, {"title": "3.2 Virtual and Physical Tests", "content": "Virtual and physical tests represent two forms of testing used to evaluate the reliability of AI systems. With the rapid development of technology and the digital age, virtual platforms can simulate real-world scenarios, enabling AI systems to operate under various conditions without the need for labor-intensive and time-consuming real-world data collection procedures. Virtual testing eliminates the need to set up physical environments; instead, all conditions are generated virtually using simulations or algorithms. In a virtual test, data can be collected even under simulated extreme conditions. For instance, scenarios involving AV accidents can be simulated to evaluate how the AI system operates and responds in such challenging and critical situations. In practice, various simulation platforms are available to conduct virtual tests for data collection. In recent years, Simulation of Urban Mobility has been an open-source platform for road traffic simulation, as discussed in Krajzewicz (2010), and is widely used to evaluate traffic management AI. With advancements in innovation, more flexible sensor settings and environmental conditions have become available. Another open urban driving simulator, CARLA, introduced in Dosovitskiy et al. (2017), provides a simulation platform that supports flexible configuration of sensor settings and environmental conditions tailored to the goals of specific research studies in autonomous driving. In addition, another open-source platform for AI systems (e.g., self-driving vehicles) is Autoware, as highlighted in Kato et al. (2018). Specifically, Autoware is an open-source software project designed to enable AVs"}, {"title": "3.3 The Use of DoE and ALT", "content": "DoE and ALT can be two useful techniques for the collection of AI reliability, which are not widely used in AI literature. DoE refers to a statistical methodology for planning, designing, and analyzing experiments (Antony 2023). In a designed experiment, intentional changes are applied to input variable(s) to observe the corresponding effects on the output(s). DoE serves as a powerful approach for data collection, enabling researchers to identify treatments that produce specific outcomes (e.g., establishing cause-and-effect relationships), as described in Thomas et al. (2022).\nDoE can be used in various ways for data collection. First, in traditional statistical reliability analysis, DoE can be a structured approach for planning and designing experiments tailored for data collection, as highlighted by Anderson-Cook and Lu (2023). Since the relationship between factors and the responses are complicated in AI reliability, the idea of"}, {"title": "4 Datasets and Illustrations", "content": "Now, we introduce the datasets we have collected and illustrate their applications in reliability modeling and analysis. To ensure a consistent presentation, Figure 2 outlines the flowchart for introducing the available datasets. Each dataset in Section 4 will be presented according to this structure, starting with the data description, followed by the data dictionary, and then moving on to the data illustration."}, {"title": "4.1 General AI Incidence Data", "content": ""}, {"title": "4.1.1 Data Description", "content": "The website AI Incident database (2024) documents incidents involving the use of AI systems that result in harm or near-harm consequences. 878 incidents have been reported. The reports are in text format, requiring substantial effort in data cleaning before the entries can be used for analysis. Hong et al. (2023) cleaned up the data entries up until October 09, 2021."}, {"title": "4.1.2 Data Dictionary", "content": "The study by Hong et al. (2023) then derived several variables from the text narratives in the original data entries to facilitate further analysis. Table 1 shows the variables in the cleaned dataset. These variables were carefully designed to capture key aspects of the incidents, enabling a structured and systematic examination of the data."}, {"title": "4.1.3 Data Illustration", "content": "As an illustration of how the dataset can be used, Figure 3(a) presents a word cloud that visualizes the different types of algorithms mentioned in the data entries. It shows that pattern recognition, self-driving systems, and NLP are among the most commonly used algorithms. Figure 3(b) provides a word cloud that visualizes the causes of failure in these incidents, revealing that bias, inaccuracy, prediction errors, and adversarial attacks are key factors contributing to the failures.\nThe AI incident data can provide valuable insights into the causes of failures, but it cannot be used to infer the probability of an incident occurring. This is because the total number of deployed systems is unknown, and not all incidents may be reported. These are crucial considerations to keep in mind when interpreting the results of any analysis based on the AI incident data."}, {"title": "4.2 Algorithm Level Test Data Set 1", "content": ""}, {"title": "4.2.1 Data Description", "content": "Lian et al. (2021) generated a test dataset to assess the robustness of AI classification algorithms, examining their performance quality and stability under class imbalance and distribution shifts between training and test datasets. The algorithms under investigation were XGboost used in Chen et al. (2015) and CNN used in Kim (2014). The dataset originates from carefully controlled experimental runs of two classification algorithms applied to two datasets: the KEGG dataset, which provides pathway data from the Kyoto Encyclopedia of Genes and Genomes, and the Bone Marrow dataset, which features macrophage scRNA-seq data.\nBoth datasets initially contain three distinct class labels in balanced proportions. To introduce class imbalance in the training and test datasets, the authors resampled the three"}, {"title": "4.2.2 Data Dictionary", "content": "The details of the variables are displayed in Table 2. For each combination of variable configurations, the experiment was repeated three times to collect the data, ending in 252 total experimental observations collected. The type of responses are continuous variables."}, {"title": "4.2.3 Data Illustration", "content": "To model the dataset, a regression model is employed that accounts for both main effects and interactions among predictors and covariates, commonly used in mixture design modeling.\nThe model is formulated as follows:\n$y = \\sum_{j=1}^{m} \\beta_j x_j + \\sum_{j<j'} \\beta_{jj'} x_j x_{j'} + \\sum_{k=1}^{h} \\sum_{j=1}^{m} \\gamma_{kj} z_k x_j + \\sum_{k<k'} \\delta_{kk'} z_k z_{k'} + \\epsilon,$"}, {"title": "4.3 Algorithm Level Test Data Set 2", "content": ""}, {"title": "4.3.1 Data Description", "content": "Faddi et al. (2024) presents a dataset that investigates the performance of CNNs on both clean and perturbed inputs. This dataset serves as a foundation for assessing the reliability and resilience of image recognition systems under adversarial conditions. To evaluate the reliability and resilience of CNNs, experiments were conducted on an image recognition system to capture the behavior of an ML classifier on clean and perturbed inputs, enabling performance analysis across iterative retraining cycles. Initially, the classifier was trained on a subset of the publicly available CIFAR-10 dataset (Krizhevsky, Hinton, et al. 2009) to learn patterns in the data. CIFAR-10 comprises 60,000 color images of size 32 \u00d7 32. Those images are catego-"}, {"title": "4.3.2 Data Dictionary", "content": "Table 3 provides a detailed description of the variables collected for the failure count dataset during the training and evaluation of the CNN, categorized into pre-retraining and post-retraining metrics. An additional dataset records the failure time, using the index of the misclassified image as the failure time. The code and data are available in a public GitHub repository in da Mata (2024)."}, {"title": "4.3.3 Data Illustration", "content": "As an illustration, we briefly describe the modeling and analysis conducted in Faddi et al. (2024). The grouped failure count is used as the response variable for the reliability models, and test accuracy is used as the response variable for the resilience models. The remaining factors collected were treated as covariates.\nFirst, for software reliability, software reliability growth models, which may incorporate covariates, are commonly used to estimate reliability metrics (Nagaraju et al. 2020, and Shibata et al. 2006). These models provide a mean value function m(t; x), which predicts the cumulative number of failures discovered up to time interval t, given covariates xs. The mean value function is defined as:\n$m(t; x) = w \\sum_{l=1}^{t} ((1 - (1 - h(l))^{g(x_l; \\beta)}) \\prod_{s=1}^{l-1} (1 -h(s))^{g(x_s; \\beta)}),$\nwhere w > 0 represents the total number of failures that would be observed with infinite testing, h() is the baseline hazard function, g(x\u03b9; \u03b2) is a general function of covariates X\u03b9 and parameter vector B, capturing the impact of external factors on software reliability, l represents the current time interval at which failures are being counted, and s is an index for prior time intervals."}, {"title": "4.4 Module Level Test Data", "content": ""}, {"title": "4.4.1 Data Description", "content": "Pan et al. (2024) introduced a dataset containing module-level error events from AI systems in AVs operating across various driving scenarios. The tested modules belong to the perception system, which comprises cameras and LiDAR sensors. This system includes three key modules: 2-dimensional (2-D) detection, 3-D detection, and object localization. The 2-D and 3-D detection modules operate in parallel, and their outputs are fused in the localization module to determine object positions.\nThe dataset was collected from a physics-based AV simulation platform, where an EI framework was developed to efficiently generate error events from various AI system modules in AVs, as shown in Figure 7. Figure 7(a) illustrates the physics-based simulation platform, which consists of two main components: (i) the environment, incorporating diverse physical models such as infrastructures, driving scenarios, and traffic-related agents that closely resemble real-world driving conditions, and (ii) the ego vehicle, which interacts with the driving environment through an AI system that integrates multiple sensors and AI/ML algorithms to perceive environmental information. Figure 7(b) depicts the EI framework, which enables targeted EI into different AI system modules at user-defined time stamps and probabilities. Recurrent error events were logged throughout the simulation process, as shown in Figure 7(c). The primary objective of the dataset is to analyze how errors in the 2-D and 3-D detection modules propagate to the object localization module."}, {"title": "4.4.2 Data Dictionary", "content": "Table 4 presents the data dictionary for module-level error events in the AI system of AVs. Seven scenarios were considered, with El controlled by the timing parameter for module m, $t_{err}^m$, and probability, $p_{err}^m$. Each scenario was simulated for 20 seconds. The weather conditions"}, {"title": "4.4.3 Data Illustration", "content": "Pan et al. (2024) proposed an error propagation (EP) model to describe the recurrent error events data, which is based on NHPP. For module m, let Nm([t1, t2)) be the counting process that records the number of events that occurred in time interval [t1, t2). Given history Hm(t), the event intensity Am(t) is defined as,\n$\\Lambda_m(t|H_m(t)) = \\lim_{dt \\to 0} E[N_m([t, t +dt))|H_m(t)]/dt,$"}, {"title": "4.5 System Level Test Data Set 1", "content": ""}, {"title": "4.5.1 Data Description", "content": "The system-level test data analysis from Min et al. (2022) focuses on the reliability of AVs. Disengagement event data is utilized to evaluate the reliability of AI systems. The original data is made available to the public by the California DMV. The data were collected through the Autonomous Vehicle Tester (AVT) program. In the AVT program, a human driver is required to sit in a test AV in order to take control of the vehicle when needed. Test AVs can disengage from the autonomous mode when the AI system or the human driver determines it is not safe to continue using the self-driving mode. Thus, the occurrence rate of disengagement"}, {"title": "4.5.2 Data Dictionary", "content": "The cleaned data provided by Min et al. (2022) consists of three CSV files containing information on disengagement event times, mileage, and month information. Table 5 summarizes the variables related to disengagement event times. The mileage information file includes the variables manufacture and VIN, which can be used to link disengagement events with mileage"}, {"title": "4.5.3 Data Illustration", "content": "As an example of how the data can be utilized in reliability analysis, Min et al. (2022) modeled the disengagement event processes using NHPP. Specifically, let n represent the number of tested AVs, 7 denote the duration of the testing period, and tij be the time of event j for unit"}, {"title": "4.6 System Level Test Data Set 2", "content": ""}, {"title": "4.6.1 Data Description", "content": "In addition to disengagement event data introduced in Section 4.5, another type of recurrent event data that can be used to investigate AV system reliability is collision event data. As the name suggests, collision event data is a type of recurrent event data used to collect information about AV collisions occurring over consecutive time periods for a specific VIN from each manufacturer. Similar to the disengagement events data described in Section 4.5, the collision events data is collected through the AVT program and is published for public review and assessment. The raw collision events data can be downloaded from the California DMV (2024) in PDF format, with separate files available for each manufacturer based on the collision event date. Note that, as of 2024, 11 years of collision events data are publicly available for online download. In terms of data cleaning, it is necessary to extract important"}, {"title": "4.6.2 Data Dictionary", "content": "Compared to the disengagement events data described in Section 4.5, more information can be collected and utilized from the collision event data. Additional details about the variables are provided in Table 6."}, {"title": "4.6.3 Data Illustration", "content": "In terms of statistical modeling, a typical application of collision event data is using an NHPP to model recurrent event processes for collision events. Regarding collision events, we can"}, {"title": "5 The Setup of the DR-AIR Repository", "content": "The online repository DR-AIR is available at https://github.com/yili-hong/DR-AIR. It includes a general summary file, DataList.csv, which provides an overview of the datasets in the repository. Each dataset has its own subdirectory. For example, the subdirectory AI-Incident-Data-2021 contains files for the AI incident dataset related to reliability, as used in Hong et al. (2023).\nWithin each dataset's subdirectory, there is a file named DataDescription.txt, which gives the data description. Numerical datasets are stored in .csv format, while other data types, such as images, may be stored as .png files. The data description file provides information on the dataset, including its background, original source, and key details necessary for understanding its variables.\nThe DR-AIR repository is freely accessible to everyone. The datasets in the DR-AIR repository are subject to the GPL-3.0 license. However, users are encouraged to cite this paper and the original sources of the datasets.\nAs research on AI reliability progresses, we anticipate adding more datasets to the repository. We encourage the research community to contribute and share AI reliability data to further advance this important field of study. Contact information for the repository maintainer is available online."}, {"title": "6 Concluding Remarks", "content": "This paper focuses on the data aspect of AI reliability research. We discuss key measurements and data types relevant to AI reliability and describe methods for data collection. Emphasizing the importance of applying DoE and ALT principles, we highlight strategies to improve data collection. In addition, we present the datasets gathered for AI reliability research and introduce DR-AIR, an online repository designed to host and share these datasets.\nWhile this work provides valuable insights into the data aspect, several areas warrant further exploration. The modeling of AI reliability can be highly complex, particularly when identifying key predictive factors that influence reliability. Although we reviewed several papers that propose models and analyses for AI reliability, this remains an evolving area with significant challenges and opportunities for future research.\nThis study has several limitations. AI is an inherently diverse and rapidly advancing field, making it difficult to conduct an exhaustive literature review. Additionally, our focus in this work is primarily on algorithmic performance and some system-level test data. So far, we have not yet seen degradation data in AI reliability, which can also be an important type of reliability data. Furthermore, this paper mainly focuses on the software components of AI systems, leaving hardware considerations largely unaddressed. In modeling of hardware, such as GPU reliability, Ostrouchov et al. (2020) and Min et al. (2023) provide more details on the reliability of hardware components like GPUs.\nWe conclude this paper with a call to action, urging the research community to contribute to and share AI reliability data. Establishing comprehensive, shared datasets is essential to advancing this critical field, enabling better models, improved methodologies, and a deeper understanding of AI reliability."}]}