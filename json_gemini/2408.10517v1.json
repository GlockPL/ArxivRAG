{"title": "Integrating Multi-Modal Input Token Mixer into\nMamba-Based Decision Models: Decision MetaMamba", "authors": ["Wall Kim"], "abstract": "Return-Conditioned Transformer Decision Models\n(RCTDM) have demonstrated the potential to enhance\ntransformer performance in offline reinforcement learning by\nreplacing rewards in the input sequence with returns-to-go.\nHowever, to achieve the goal of learning an optimal policy\nfrom offline datasets composed of limited suboptimal\ntrajectories, RCTDM required alternative methods. One\nprominent approach, trajectory stitching, was designed to\nenable the network to combine multiple trajectories to find\nthe optimal path. To implement this using only transformers\nwithout auxiliary networks, it was necessary to shorten the\ninput sequence length to better capture the Markov property\nin reinforcement learnings. This, however, introduced a\ntrade-off, as it reduced the accuracy of action inference.\nOur study introduces a model named Decision MetaMamba\nto resolve these challenges. DMM employs an input token\nmixer to extract patterns from short sequences and uses a\nState Space Model (SSM) to selectively combine information\nfrom relatively distant sequences. Inspired by Metaformer,\nthis structure was developed by transforming Mamba's input\nlayer into various multi-modal layers. Fortunately, with\nthe advent of Mamba, implemented using parallel selec-\ntive scanning, we achieved a high-performance sequence\nmodel capable of replacing transformers. Based on these\ninnovations, DMM demonstrated excellent performance\nacross various datasets in offline RL, confirming that models\nusing SSM can improve performance by domain-specific\nalterations of the input layer. Additionally, it maintained\nits performance even in lightweight models with fewer\nparameters. These results suggest that decision models based\non SSM can pave the way for improved outcomes in future\ndevelopments.", "sections": [{"title": "Introduction", "content": "There have been attempts to view offline reinforcement\nlearning (Levine et al. 2020) as a dynamic system and to find\nfunctions such as the Koopman operator to interpret this sys-\ntem, thereby evaluating the policies of value-based RL mod-\nels and improving their performance(Weissenbacher et al.\n2022), (Retchin et al. 2023), (Rozwood et al. 2024). Mean-\nwhile, Koopman theory has also been applied to state-space\nmodels (SSMs), leading to the development of SSMs based\non this theory(Orvieto et al. 2023). Various deep learning-\nbased variants of such SSMs(Gu, Goel, and R\u00e9 2021), (Gu\nand Dao 2023) are being proposed as decision models for\noffline RL rather than merely as auxiliary tools. Trans-\nformers, which can be considered a type of SSM(Dao and\nGu 2024), have also demonstrated outstanding performance\nin sequence generation problems and, based on this, have\nshown good performance as models conditioned on return\nin offline RL. Recently, there have been improvements in\nvarious areas of offline RL by modifying token mixers in\ntransformer architectures.\nHowever, employing transformers as decision models\nposes two main challenges. The first challenge is that adopt-\ning language models directly is not well-suited for han-\ndling trajectory data comprising status information of lo-\ncomotive objects or visual data. The second challenge is\nthe tendency to learn a suboptimal policy due to the lim-\nited distribution of training data. For the first challenge, re-\nplacing the token mixer with convolution-based approaches\nyielded substantial performance improvements, inspired by\nthe Metaformer(Yu et al. 2022) designed to enhance vision\nproblems using transformers. Specifically, the 1D convolu-\ntion token mixer employed causal convolution, which pads\nonly the front of the sequence to ensure causality, differing\nfrom the masking used in attention. It also applied distinct"}, {"title": "Related Work", "content": "Offline reinforcement learning is a subfield of RL where\nthe agent learns from a pre-collected dataset without addi-\ntional environment interactions. This is crucial for applica-"}, {"title": "Offline Reinforcement Learning", "content": "offline RL include distribution shift and extrapolation error.\nDistributional shift is the mismatch between the data dis-\ntribution in the offline dataset and the distribution the agent\nencounters when deploying the learned policy. Extrapolation\nerror occurs when the policy queries state-action pairs that\nare not well represented in the dataset, which is often due\nto the lack of sufficient expert demonstrations in offline RL\ndatasets.\nMethodological advances to overcome hurdles include\nValue-based behavioral regularization. Techniques like\nbatch-constrained Q-learning (BCQ, (Fujimoto, Meger, and\nPrecup 2019)), conservative Q-learning (CQL, (Kumar et al.\n2020)), and implicit Q-Learning (IQL, (Kostrikov, Nair, and\nLevine 2021)) are designed to mitigate extrapolation errors\nby constraining the learned policy to stay close to the behav-\nior policy that generated the dataset. Contrarily, Value-free\napproaches do not necessarily rely on Q functions. One such\napproach is Imitation learning, which aims to imitate the be-\nhavior policy by training on collected or desired trajecto-\nries filtered by heuristics or value functions. Trajectory Op-\ntimization, e.g. Multi-Game Decision Transformer (MGDT,\n(Lee et al. 2022)) and Trajectory Transformer (TT, (Janner,\nLi, and Levine 2021)), models joint state-action distribution\nover complete trajectories, reducing out-of-distribution ac-\ntion selection. To enable effective planning, this approach\nutilizes techniques such as beam search and reward esti-\nmates. Another widely known method is model-based ap-\nproach. It uses learned models of the environment to gener-\nate synthetic data, which can augment the offline dataset and\nimprove learning(Weissenbacher et al. 2022)."}, {"title": "Return-conditioned Decision Transformers", "content": "Offline RL decision models fundamentally aim to find the\noptimal policy from suboptimal trajectories. An easy way to\nlearn the optimal policy is to have many trajectories in the\ntraining dataset that are close to the optimal policy. How-\never, in most cases, such trajectories are either nonexistent\nor very few. To overcome this, decision transformers and\ntheir variants use returns-to-go inputs. Information that can\nbe obtained after an action is performed, such as returns-to-\ngo, is called hindsight information, and it provides intuition\non what should have been done differently. (Furuta, Mat-\nsuo, and Gu 2021) revealed that decision transformers learn\npolicies through hindsight matching to ensure that the pre-\ndicted statistical values for future states from the trajectory\nare satisfied. During evaluation, the target returns-to-go used\nas input are typically higher than the actual returns, serv-\ning as the most crucial indicator for the model to output\nactions close to the optimal in that state. However, due to\nother sequential inputs such as state and action information,\nthe model's output can revert to being sub-optimal. Since\nit is supervised learning, if sequences similar to previous\nstates and actions are input, the model will attend more to\nthose and output actions based on a sub-optimal trajectory.\nThis can lead to negative trajectory stitching, (where the pol-\nicy learned from one trajectory adversely affects the policy\nlearned from another trajectory). Because this is a character-\nistic of supervised learning, various auxiliary networks have\nbeen attempted to overcome this."}, {"title": "Metaformer", "content": "Metaformer is a framework proposed to enhance the per-\nformance of transformers in vision tasks. One of the mod-\nels introduced within the Metaformer, PoolFormer, achieves\nsuperior results compared to baseline models such as (Doso-\nvitskiy et al. 2021) and (Touvron et al. 2021a), and (Touvron\net al. 2021b) despite its simple architecture that replaces the\nself-attention token mixer with pooling layers. The simplic-\nity of PoolFormer's structure also leads to a significant re-\nduction in parameters, providing considerable advantages\nin terms of resource efficiency. Beyond excelling in vision\ntasks, Metaformer has influenced numerous other studies by\nabstracting the token mixer to achieve versatile functional-\nities. The name Metaformer itself was chosen to evoke this\ngeneralized structure. Another model within the Metaformer\nfamily, Decision Convformer(Kim et al. 2024), also demon-\nstrates high performance in offline reinforcement learning\ntasks through variations in the token mixer."}, {"title": "State Space Model", "content": "To advance language models further, new architectures ca-\npable of surpassing the performance of Transformers have\nbeen developed. Among these, SSM is a field originally\nutilized in control engineering for dynamic system map-\nping using state variables. In the domain of deep learning,\nSSM has been developed into models that learn by parame-\nterizing system functions that transform discrete state vari-\nables(BOURDOIS 2023)."}, {"title": "Mamba", "content": "Mamba(Gu and Dao 2023) developed its model based on\nthe structural foundation of S4 through two significant mod-\nifications. Previously, the linear time-invariant matrices A,\nB, and C were the same for all input tokens. As a re-\nsult, it was unable to solve the selective copying problem,\nlacked content-aware reasoning, and consequently, could not\nachieve significant performance improvements in language\ntasks and performed poorly on the induction heads problem.\nTo address these issues, Mamba allowed matrices B and\nC, as well as the step size A, to vary based on the input. This\nenabled the model to possess content awareness. However,\nsince B and C are now dynamically selected, it is no longer\npossible to use convolution parallelism with fixed kernels.\nThese states can be easily computed through sequential op-\nerations using a for loop. However, parallelization presents\na different challenge. What initially seemed impossible to\nparallelize was achieved by Mamba using a parallel scan al-\ngorithm (Harris, Sengupta, and Owens 2007).\nAdditionally, Mamba improved its speed by efficiently\nutilizing the hardware architecture of GPUs. By employing\nkernel fusion and recomputation of immediate states dur-\ning the backward pass, it minimized data transfer between\nSRAM and DRAM (HBM)."}, {"title": "Method", "content": "In the context of RCTDM, two well-known types of to-\nken mixers are Self-Attention and 1D Convolution. Self-\nAttention integrates tokens across long sequences by con-\nsidering their interrelations, whereas 1D Convolution ex-\ntracts patterns within short windows of tokens. Among\nthese, 1D Convolution has recently demonstrated superior\nperformance due to the Markov property, where attending\nto adjacent steps often yields better action decisions. How-\never, Elastic Decision Transformer (Wu, Wang, and Hamaya\n2024) has shown that dynamically determining the input se-\nquence length to facilitate trajectory stitching can enhance\nperformance, indicating that always using short sequences\nis not necessarily optimal for determining the best action.\nTherefore, we aimed to design a structure that focuses on"}, {"title": "Model Architecture", "content": "DMM is built upon the Mamba architecture. As illustrated in\nFigure 2, the original Mamba is divided into three main com-\nponents. The first component is the input layer, which trans-\nforms the input state using a combination of linear layers,\n1D-convolution layers, and activation functions. The second\ncomponent is the SSM layer. As detailed earlier, the selec-\ntive scan SSM performs content-aware reasoning while en-\nabling fast, parallel operations through a hardware-aware al-\ngorithm. The final component is the output projection, which\nlinearly transforms the output of the SSM.\nInspired by the Metaformer, DMM abstracts the input\nlayer of Mamba and optimizes it into a network suited for\nthe training data. In this study, two types of Multi-Modal\nToken Mixers (MMTMs) are utilized: one is a multi-modal\n1D-convolution layer, and the other is a multi-modal lin-\near layer. The offline RL dataset comprises three indepen-\ndent modalities of data: returns-to-go (Rtg), state, and ac-\ntion. Consequently, it has been demonstrated through Deci-\nsion Convformer that better performance can be achieved\nwhen these three modalities are token-mixed by separate\nnetworks. In addition to the previously introduced multi-\nmodal 1D-convolution, we have implemented a network uti-\nlizing a linear layer. One of these two modules is positioned\nat the very beginning of the input layer of the SSM, and\na residual connection is added to the hidden state. The re-\nsultant output is then fed into the selective scan SSM. The\nsingle-modal 1D-convolution used in the original Mamba ar-\nchitecture has been removed.\nh(t) = MMTM(LN(h(t))) + h(t)\nMMTM = Multi-Modal Token Mixer,\nLM = Layer Normalization"}, {"title": "Double Sequence Transformation", "content": "We positioned two\nMMTMs at the very front of the input layer. The rationale\nfor this arrangement was to subject the input, which is di-\nvided into two paths in front of the SSM, to a multi-modal\nsequence transformation before the actual division occurs.\nAdditionally,\nwe experimented with a structure that applies an extra\nmulti-modal sequence transformation to one of the inputs\namong the divided two paths, maintaining the same config-\nuration as the existing input layer structure of Mamba. This\ninput is not the one that gets multiplied with the output at\nthe final stage of the SSM but rather the actual input that tra-\nverses through the SSM. We concluded that using the state,\nwhich has been further transformed by the sequence layer,\nas the input to the SSM, aligns more effectively with the\nprinciples of the existing Mamba framework."}, {"title": "Input Token Mixers", "content": "In the input layer, both token mixers-1D convolution and\nlinear layer-perform sequence transformation while con-\nsidering causality. Consequently, they do not include infor-\nmation beyond the current time step in the filter. To achieve\nthis, zero padding of window_size - 1 is added to the be-\nginning of the sequence vectors."}, {"title": "Multi-Modal 1D-Convolution", "content": "The convolution module\naims to integrate neighboring embeddings within the win-\ndow size. It applies filters across channels of the hidden\ndimension using a 1D convolution kernel. As illustrated in\nFigure 3a, to account for multi-modality, three different con-\nvolution kernels are used. This results in three distinct hid-\nden states, generated by each kernel in the order of Rtg, state,\nand action. These hidden states are then combined to form a\nnew hidden state tensor. The output tensor size matches the\ninput tensor size.\n1D convolution cannot account for positional informa-\ntion between inputs. Although methods such as (Sabour,\nFrosst, and Hinton 2017) have been developed to address\nthis issue, they are too computationally intensive for deci-\nsion models. With a filter length of 6, only two time steps\n(rtg_{t-1}, state_{t-1}, action_{t-1}, rtg_t, state_t, action_t) are con-\nsidered. Although information outside the convolution win-\ndow can be integrated through layer stacking, it is challeng-"}, {"title": "Multi-Modal Linear", "content": "As can be observed in Figure 3b,\nZero-padded sequence vectors are grouped into embedding\nsets of window size. To perform computation in a single lin-\near transformation without iteration, each window size vec-\ntor within the set is flattened into a 1-dimensional vector.\nThese are then transformed back into vectors with the origi-\nnal hidden state dimension via a linear layer. While the con-\nvolution module applies filters to the hidden state dimension,\nthe linear module concatenates consecutive sequence vec-\ntors of window size in the time domain in a flattened manner.\nThe resulting vectors, transformed back to the hidden state\ndimension, form a sequence, yielding an output tensor that\nmatches the input tensor size. The linear layer also uses three\ndifferent modules to distinguish between each modality."}, {"title": "Train and Evaluation", "content": "Train In the train trajectory dataset, each sequence of\nlength, which includes Rtg, state, and action, is embedded\ninto vectors of the same size through an input embedding\nlayer. This process addresses the issue arising from the dif-\nfering sizes of these components. Traditional transformer\narchitectures typically add positional encoding to the em-\nbeddings. However, DMM ensures input causality through\npadding and processes the inputs using a sequence model,\nthereby obviating the need for positional encoding. By elim-\ninating additional information dependent on the trajectory,\nthe likelihood of behavioral cloning is further reduced. The\noutput from passing through n MetaMamba blocks is trans-\nformed into an action-dimension vector, resulting in the\npredicted action. The loss is then computed between the\npredicted actions and the true actions across all sequence\nlengths. As described, the loss is calculated for all predicted\nactions within a sequence, but during evaluation, only the\nfinal action in the sequence length is used.\nL_{DMM} = \\frac{1}{K}\\sum_{k=1}^{K} \\|\\hat{a}_k - a_k\\|\nWhile a longer sequence length during training can provide\nmore guidance, it may inversely affect the accuracy of the\nfinal action. In a situation where there is a shortage of nec-\nessary guides for learning, DMM was able to enhance the\naccuracy of the final action by considering both short-range\nand long-range information within the sequence. Therefore,\nDMM utilized shorter sequence lengths in all environments\ncompared to existing models.\nEvaluation In the evaluation stage, DMM interacts with\nthe environment in real-time to infer the action token. There-\nfore, it is necessary to set the initial Rtg. Consistent with\n(Chen et al. 2021)., we set the initial target return to be\nhigher than the actual return of the trajectory."}, {"title": "Experiments", "content": "We examined whether DMM can demonstrate improved per-\nformance by considering both proximate and distance se-\nquences. This performance evaluation utilized pre-collected\ndatasets from the D4RL (Fu et al. 2020) MuJoCo, AntMaze,\nand Atari (Mnih et al. 2013) domains. Additionally, we em-\nployed two modules that replace the input layer of SSM to\nascertain if DMM can enhance offline RL performance com-\npared to models using traditional SSMs. Since SSMs were\noriginally developed to replace self-attention in LLMs, they\nhave previously been used to substitute the token mixer in\ndecision transformers. Contrarily, through the selective scan\nSSM and an abstracted input layer structure, DMM suggests\na future direction for decision models utilizing SSM."}, {"title": "Conclusion", "content": "We propose Decision Metamamba, a sequential decision-\nmaking model that employs the selective scan State-Space\nModel with a modified input layer. The input sequence in\noffline reinforcement learning includes data in different for-\nmats: state, returns-to-go, and action. To handle this, we uti-\nlized two distinct token mixer networks, collectively referred\nto as the Multi-Modal Token Mixer. These two token mixers\nare, respectively, token mixers that utilize a 1D convolution\nlayer operating in the hidden state dimension and a linear\nlayer for integrating tokens along the sequence dimension.\nEach network demonstrated strong performance in specific\nexperimental domains.\nTo appropriately consider information from both short se-\nquences close to the current step and distant steps, we em-\nployed the selective scan SSM. Unlike previous sequence\nmodels, the Selective Scan SSM facilitates rapid learning\nthrough parallel computation while utilizing a relatively\nsmall number of parameters, as some layers perform con-\nvolutional operations. This combination allows the model to\nachieve scores comparable to or exceeding those of exist-\ning models, despite having a lower number of parameters.\nMoreover, leveraging the recurrent properties of the transi-\ntion matrix allows for inference speeds up to five times faster\nthan traditional transformer models. Additionally, since the\noutput of the token mixer sequentially serves as the input\nto the SSM for action inference, the model does not require\ntime-step encoding, contributing to its enhanced generaliz-\nability.\nThe ongoing advancements from the initial Structured\nSSM to Gated SSM and Recurrent SSM (Patro and Ag-\nneeswaran 2024) suggest that this study can pave the way\nfor improved performance in offline RL decision models\nthrough the use of advanced SSMs."}, {"title": "Appendix", "content": null}, {"title": "Self-Attention and SSM", "content": "We have verified the impact of MMTM on performance\nthrough previous experiments. Additionally, to investigate\nwhether SSM can improve performance when used with\nother types of token mixers, we tested a model using self-\nattention from Decision Transformer as the token mixer and\nMamba's SSM as the channel mixer, replacing the conven-\ntional MLP layer. The results, as shown in Table 4, demon-\nstrated a remarkable improvement in performance.\nBoth Self-attention and selective scan SSM operate by se-\nlectively combining inputs to generate outputs, but they em-\nploy different methods. This combination of their strengths\nled to enhanced results. However, we obtained lower per-\nformance compared to models utilizing 1D convolution or\nlinear layers. This observation suggests that the concept of\nthe DMM, which first integrates tokens in close proxim-\nity and then performs selective integration with information\nfrom tokens at distant steps, can provide higher performance\ncompared to models that only employ two types of selective\ncombinations."}, {"title": "B. Parameter Efficiency", "content": "As shown in Figure 1, the DMM requires significantly fewer\nparameters to train a model of equivalent performance com-\npared to traditional models using the transformer architec-\nture. This is attributed to the model's characteristics that\nutilize the Selective Scan SSM, which employs convolu-\ntional filters, and its ability to maintain performance with\nsmaller input embeddings. Table 5 presents the total num-\nber of parameters and the parameter-to-score ratio for all\nthe models we experimented with. When using MMTM en-\ntirely as 1D-Convolution, the overall model size could be\nreduced to approximately 6% while achieving similar per-\nformance. This demonstrates the achievement of developing\na resource-efficient model. Even when employing a linear\nlayer for the input layer, the model still exhibited efficiency\nin the parameter-to-score ratio."}, {"title": "C. Implementation Details", "content": "All experiments were conducted using an Nvidia RTX 3060\n12GB GPU on an Ubuntu OS.\nIn the experiments, to limit the effect of hyperparameters,\nmost parameters were used identically to those in the (Kim\net al. 2024) except the embedding dimension.\nThe results of the value-based model in the Mujoco and\nAntmaze domains are cited from Decision Convformer,\nwhile the Antmaze results for EDT are cited from (Zhuang\net al. 2024). The remaining results are recorded from our\nown experiments."}, {"title": "D. The Effect of Activation Functions RELU,\nGELU, and SiLU.", "content": "We confirmed performance improvements by changing the\nactivation function to GELU during experiments with the\nDecision Convformer on the AntMaze task, which provides\nsparse rewards. In the Transformer architecture, the ReLU\nactivation function is typically used following the MLP,\nwhich acts as a channel mixer. The modification involved re-\nplacing this with GELU. Conversely, in the Mamba architec-\nture, the SiLU activation function is used in the input layer\npreceding the selective scan SSM.\nAs observed in the Table 9, changing the activation func-\ntion from SiLU to GELU in the DMM did not result in any\nperformance change. The reason for this can be found in\nFigure 5, which shows that SiLU and GELU exhibit simi-\nlar functional behavior. The functional difference between\nReLU and GELU is most pronounced near zero, where the\nmajority of values lie, potentially impacting performance.\nHowever, since SiLU does not exhibit significant differences\nin function values compared to GELU around zero, the im-\npact on overall model performance is likely to be minimal."}]}