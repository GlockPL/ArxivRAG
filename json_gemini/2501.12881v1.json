{"title": "Reinforcement learning Based Automated Design of Differential\nEvolution Algorithm for Black-box Optimization", "authors": ["Xu Yang", "Rui Wang", "Kaiwen Li", "Ling Wang"], "abstract": "Abstract\u2014Differential evolution (DE) algorithm is\nrecognized as one of the most effective evolution-\nary algorithms, demonstrating remarkable efficacy in\nblack-box optimization due to its derivative-free na-\nture. Numerous enhancements to the fundamental DE\nhave been proposed, incorporating innovative mutation\nstrategies and sophisticated parameter tuning tech-\nniques to improve performance. However, no single\nvariant has proven universally superior across all prob-\nlems. To address this challenge, we introduce a novel\nframework that employs reinforcement learning (RL)\nto automatically design DE for black-box optimization\nthrough meta-learning. RL acts as an advanced meta-\noptimizer, generating a customized DE configuration\nthat includes an optimal initialization strategy, update\nrule, and hyperparameters tailored to a specific black-\nbox optimization problem. This process is informed by\na detailed analysis of the problem characteristics. In\nthis proof-of-concept study, we utilize a double deep\nQ-network for implementation, considering a subset\nof 40 possible strategy combinations and parameter\noptimizations simultaneously. The framework's per-\nformance is evaluated against black-box optimization\nbenchmarks and compared with state-of-the-art algo-\nrithms. The experimental results highlight the promis-\ning potential of our proposed framework.", "sections": [{"title": "I. INTRODUCTION", "content": "LACK-BOX optimization (BBO) is becoming in-\ncreasingly crucial for tackling complex real-world\noptimization challenges, where traditional methods often\nfalter due to their reliance on detailed mathematical\nmodels. Differential Evolution (DE), a population-based\nevolutionary algorithm, has gained widespread recognition\nfor its effectiveness in addressing black-box optimization\nproblems, leading to the development of numerous variants.\nHowever, in accordance with the No Free Lunch theorem\nno single algorithm can consistently outperform all\nothers across every problem instance. Additionally, the\ndynamic nature of many real-world problems demands al-\ngorithms capable of rapidly adapting to evolving optimiza-\ntion landscapes. Manually designing an algorithm tailored\nto a specific problem requires extensive expert knowledge,\nwhile selecting an efficient algorithm from thousands of\ncandidates is highly time-consuming. To address these\nissues, meta-optimizers that can automatically select, con-\nfigure, or even generate algorithms tailored to specific\nproblems have emerged as a promising methodology.\nMeta-optimizers have driven evolutionary algorithms\ncapable of customizing their own evolutionary strategies\nfor a given problem automatically.\nOne of the most widespread implementations of meta-\noptimizers focuses on machine learning techniques, par-\nticularly reinforcement learning (RL), which has facil-\nitated the development of algorithms that can learn from\ngiven problems, resulting in more efficient and effective\nDEs. Studies have focused on RL-assisted\nself-adaptive operator selection of DE. Li et al. treated\neach individual in the population as an agent, using fitness\nrankings to encode hierarchical state variables and em-\nploying three typical DE mutation strategies as optional\nactions for the agent. Tan et al. implemented adaptive\nselection of mutation strategies in the DE evolution pro-\ncess based on a deep Q-network (DQN). Fister et al. utilized reinforcement learning to select strategies derived\nfrom the original L-SHADE algorithm, transitioning\nfrom the 'DE/current-to-pbest/1/bin' mutation strategy\nto the iL-SHADE and jSO using the 'DE/current-\nto-pbest-w/1/bin' mutation strategies. Li et al. pro-\nposed an adaptive multi-objective differential evolution-\nary algorithm based on deep reinforcement learning to\neffectively obtain Pareto solutions, integrating RL as a\ncontroller that can adaptively select mutation operators\nand parameters according to different search domains.\nSeveral attempts have also been made to employ RL to\nconfigure EAs. Huynh et al. and Peng et al. used\nQ-learning to control predetermined parameters. Sun et\nal. , Zhang et al. , Liu et al. , and Zhang et al.\nemployed policy-based RL for more precise parameter\ncontrol. It can be concluded that these aforementioned\nstudies harnessed the distribution characteristics of the\npopulation. In addition, it is noteworthy that the majority\nof these investigations focused solely on mutation strate-\ngies or numerical parameters.\nTo address these limitations, we introduce an inno-\nvative framework that employs RL as a meta-optimizer\nto generate DE tailored to the characteristics of spe-\ncific problems. In this framework, meta-learning\nis incorporated to enhance the generalizability of the\nmeta-optimizer, thereby enabling it to adapt more ef-\nfectively across a diverse range of problem scenarios."}, {"title": "II. PRELIMINARY", "content": "DE and RL are two powerful methodologies in the do-\nmains of optimization and machine learning, respectively.\nEach possesses unique strengths and weaknesses, and their\ncombination through hybrid algorithms can yield more\nrobust and efficient solutions. Given the integration of\nboth approaches in our proposed method, this section\nprovides a brief overview of the fundamentals of DE and\nRL, as well as a review of hybrid methods currently\nsuggested in research, to facilitate a clearer understanding."}, {"title": "A. DE", "content": "DE was introduced by Storn and Price . As one of\nthe most popular evolutionary algorithms (EAs), DE is a\npopulation-based optimization technique inspired by the\nprocess of natural evolution . It iteratively evolves a\npopulation of candidate solutions by generating offspring\nand selecting the best-performing individuals. Following\nthe scheme of EAS, DE variants mainly focus on various\nmutation strategies, such as \"DE/rand/1\", \"DE/rand/2\",\n\"DE/best/1\", \"DE/best/2\", \"DE/current-to-best/1\", and\n\"DE/current-to-rand/1\" . The general process of DE\nis illustrated in Algorithm 1. In DE literature, a parent\nvector in the population P from the current generation is\nreferred to as the target vector, a mutant vector obtained\nthrough the differential mutation operation is known as\nthe donor vector, and an offspring formed by recombining\nthe donor with the target vector is called the trial vector."}, {"title": "B. RL", "content": "RL is a type of machine learning designed for sequential\ndecision-making problems, where an agent learns to make\ndecisions by taking actions in an environment to maximize\ncumulative reward . RL is formalized as a Markov De-\ncision Process (MDP), denoted as a tuple (S, A, P, R, \u03b3), where:\nDefinition 1. MDP (S, A, P, R, \u03b3)\n\u2022\nS is the set of states,\n\u2022\nA is the set of actions,\n\u2022\nP is the transition probability function,\n\u2022\nR is the reward function assigning a numerical reward\nto each state-action pair,\n\u2022 y is the discount factor.\nThe objective of training an RL agent is to learn a policy\nthat maps states to actions, in order to maximize the\nexpected cumulative reward:\n$J(\\pi) = E[\\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+1} | \\pi, s_{t}]$ (1)\nwhere $s_{t}$ is the state at time t, and $R_{t+k+1}$ is the reward\nat time t + k + 1.\nRL can be categorized into different families based on\ntheir approach to learning:\n\u2022 value-based RL: Learn value functions to esti-\nmate the expected future reward and derive optimal\npolicies from them. Examples include Q-Learning\n, DQN  and state-action-reward-state-action\n(SARSA) .\n\u2022 policy-based RL: Directly optimize the policy func-\ntion to maximize the expected reward. Examples\ninclude Policy Gradient and Actor-Critic methods\n.\n\u2022 model-based RL: Learn a model of the environ-\nment's transition dynamics to simulate future states\nand actions, enabling efficient planning and explo-\nration. Examples include Model Predictive Control\n and Monte Carlo Tree Search ."}, {"title": "C. RL-assisted DE", "content": "To overcome the limitations of DE, researchers have pro-\nposed hybrid methods that integrate the decision-making\ncapabilities of RL into DE. The RL agent is utilized to\nguide the search process of DE . Li and\nHao et al. have conducted an in-depth analysis of each"}, {"title": "III. EXPLORATORY LANDSCAPE ANALYSIS", "content": "A comprehensive understanding of the BBOPs' charac-\nteristics offers valuable insights into their complexity and\nguides the selection of the most appropriate optimization\nalgorithm . While certain high-level characteristics,\nsuch as multi-modality and global structure, are used to\ndescribe BBOPs, identifying these features without expert\nknowledge can be challenging. To address this issue, ELA\nprovides a structured methodology for characterizing low-\nlevel features , which has been shown to be effective\nin classifying BBOPs  and informative for algorithm\nselection . This section introduces a discussion of the\nused features in our framework that are computationally\ninexpensive to calculate.\nIn recent years, researchers have been expanding the\nset of low-level features , which are\ndistributed across various platforms and use different fea-\nture collections. Without loss of generality, it is necessary\nto use multiple feature collections at once for a careful\ncharacterization while single feature collections measure\nfew properties. Pflacco package addresses this issue,\nwhich contains more than 300 features distributed across\n17 feature sets.\n(1) Initial ELA Features (Convexity, Curvatyre, y-\ndistribution, Meta-model, Local Search, Levelset)\n(2) Cell Mapping (Angle, Convexity and Gradient Homo-\ngeneity) and Generakized Cell Mapping Features\n(3) Barrier Tree Features\n(4) Information Content Features\n(5) Disperison Features\n(6) Miscellaneous Approaches (Basic, Linear Models,\nPrincipal Components)\nEach group consists of a set of sub-features that can be\ncomputed based on the initial sample data D\u00ba = {X,y},\nwhere X represents the decision vector with dimension D,\nand y represents the observation value vector.\nAmong these feature sets, some cheap features from six\nsets are chosen to characterize BBOPs in our framework\nfollowing the discussion in .\nSkewness = $\\frac{\\sqrt{n} \\sum_{i=1}^{n}(Yi - \\bar{y})^{3}}{(\\sum_{i=1}^{n}(Yi - \\bar{y})^{2})^{3/2}}$ (2)\nKurtosis = $\\frac{\\frac{1}{n}\\sum_{i=1}^{n}(Yi - \\bar{y})^{4}}{(\\frac{1}{n}\\sum_{i=1}^{n}(Yi - \\bar{y})^{2})^{2}}-3$ (3)\nFor y-distribution, skewness, kurtosis, and the number\nof peaks are computed. Skewness measures the asymmetry\nof the probability distribution of a real-valued random\nvariable about its mean, as given by Eq. (2). Kurtosis\nquantifies the \"tailedness\" of the probability distribution\nof a real-valued random variable, as shown in Eq. (3). The\nnumber of peaks is determined by identifying the local\nmaxima in the distribution of the data. The Gaussian\nKernel Density Estimation (KDE) is used. The general\nsteps are:\n\u2022\nUse KDE to estimate the probability density function\nof the data, denoted as f(x).\n\u2022\nDefine the bounds of the interval, which in this study\nis set as interval = [min(y) \u2013 3 * X * std(y), max(y) +\n3 * x * std(y)], where is a scaling factor used to\ndetermine the kernel's bandwidth.\nEvaluate f(x) over a grid of points within interval.\nIdentify local minima of f(x) and partition the inter-\nval into regions between consecutive minima.\nFor each region, calculate the mean of f(x) plus\nthe absolute difference between the positions of the\nregion's endpoints.\nCount the number of regions where this value exceeds\n0.1, denoted as npeaks.\nFor Levelset, classification methods such as Linear\nDiscriminant Analysis (LDA) and Quadratic Discrimi-\nnant Analysis (QDA) are considered. The related fea-\nture values include the Mean Misclassification Error\n(MMCE) of LDA and QDA (denoted as $MMCE_{LDA,q}$ and\n$MMCE_{QDA,q}$), and the ratio $r_{q}$ of $MMCE_{LDA,q}$ and\n$MMCE_{QDA,q}$, where q represents the quantiles which is\nset to [0.1, 0.25, 0.5]. The calculation is shown in Eq. (4),\nEq. (5) and Eq. (6).\n$MMCE_{LDA,q} = \\frac{1}{n} \\sum_{i=1}^{n}(Y_{class,test_{i}} \\neq \\hat{Y}_{LDA,test_{i}})$ (4)\n$MMCE_{QDA,q} = \\frac{1}{n} \\sum_{i=1}^{n}(Y_{class,test_{i}} \\neq \\hat{Y}_{QDA, test_{i}})$ (5)\n$r_{q} = \\frac{MMCE_{LDA,q}}{MMCE_{QDA,q}}$ (6)\nwhere $Y_{class,test_{i}}$ is the true classification of the test set\nwhile $\\hat{Y}_{i,test_{i}}$ is the classification predict by LDA or QDA.\nFor meta-modal, we compute the adjusted $R^2$ and the\nintercept of a simple linear model, as well as the smallest\nand largest absolute coefficients and their ratio beyond\nthe simple linear model. $R^2$ measures the degree of fit of a\nmodel to data, with higher values indicating a better fit.\nSpecifically, we calculate the adjusted $R^2$ of a linear model\nwith interactions and a quadratic model with and without\ninteractions, along with the ratio of the largest and small-\nest coefficients beyond the simple quadratic model without\ninteractions.\nFurthermore, features reflecting funnel structures are\ncomputed using nearest best/better clustering. These in-\nclude the ratio of standard deviations and arithmetic\nmean based on the distances among the nearest neighbors\nand the nearest better neighbors, the correlation between\ndistances of the nearest neighbors and the nearest better\nneighbors, the coefficient of variation of the distance ratios,\nand the correlation between fitness value and the count\nof observations to whom the current observation is the\nnearest better neighbor .\nInformation Content of Fitness Sequences (ICoFiS) ap-\nproach is utilized to quantify the information content\nof a continuous landscape, such as smoothness, rugged-\nness, or neutrality. Specifically, we compute the maximum\ninformation content of the fitness sequence, settling sensi-\ntivity (which indicates the epsilon for which the sequence\nnearly consists of zeros only), epsilon-value, ratio of partial\ninformation sensitivity, and initial partial information.\nDispersion features are computed by comparing the\ndispersion of pairwise distances among the 'best' elements\nand the entire initial design, as well as the ratio and\ndifference of the mean/median distances of the 'best'\nobjectives versus 'all' objectives .\nPrincipal component analysis is employed to extract\nand analyze the main structure of the problems, including\nexplained variance by convariance and correlation matrix\nof decision space, and proportion of variance explained by\nthe first principal component.\nLinear model features are computed with the decision\nspace being partitioned into a grid of cells. For each cell,\nthe following features are calculated:\n\u2022\nThe length of the average coefficient vector.\n\u2022\nThe arithmetic mean and standard deviation of the\nlengths of all coefficient vectors.\n\u2022\nThe correlation among all coefficient vectors.\n\u2022\nThe arithmetic mean and standard deviation of the\nratios of the absolute maximum and minimum non-\nintercept coefficients within the cell."}, {"title": "IV. THE PROPOSED FRAMEWORK", "content": "This section presents the proposed framework, rlDE.\nWe begin by discussing the motivation behind the de-\nvelopment of this methodology, followed by an overview\nof the framework and the procedural steps involved in\noperationalizing our theoretical constructs."}, {"title": "A. Motivation", "content": "BBOPs present unique challenges that defy a one-\nsize-fits-all approach. The No Free Lunch theorem has\nprompted a shift towards more dynamic and intelligent\noptimization strategies for BBOPs. This quest for adapt-\nability has led to the convergence of evolutionary compu-\ntation and machine learning, particularly RL, which offers\nEAs the ability to learn and evolve in response to the\noptimization landscape. RL-assisted DE has emerged as\na robust framework for exploring the solution space of\nBBOPs. However, existing studies have primarily focused\non population characteristics, which raises several impor-\ntant questions.\nFirstly, what is the relationship between population\ncharacteristics and problem characteristics? Can popu-\nlation characteristics be misleading? And is the cost of\nobtaining population characteristics lower than that of\nobtaining problem characteristics? To address these ques-\ntions, we conducted a theoretical analysis.\nOn one hand, it is well established that the quality of\nfeature approximation in ELA depends on the sample size\n. A typical recommendation for the sample size NS is\naround 50D when the functions are fast to evaluate . As for population size, Storn and Price suggested NP \u2208\n[5D, 10D] , while Piotrowski suggested NP = 100 for\nlow-dimensional problems (D < 30) and NP \u2208 [3D, 10D]\nfor higher-dimensional problems . Given these recom-\nmendations, the sample size for population characteristics\nis generally smaller than that for problem characteristics.\nConsequently, population characteristics may potentially\nmislead the search direction, deviating from the true\nproblem characteristics due to the smaller sample size,\nwhich can result in suboptimal solutions.\nOn the other hand, the cost of obtaining population\ncharacteristics increases with the number of iterations.\nIf the fitness evaluations is fixed, defined as $max_{FES}$,\nthe iterations can be calculated as $\\frac{max_{FEs}}{NP}$, where NP\nrepresents the population size. Assuming that the number\nof used feature is ftNum, population characteristics-based\nDE computes each feature in each generation, leading\nto cost extra ftNum \u00d7 $\\frac{max_{FEs}}{NP}$ evaluations. Problem"}, {"title": "V. CONCLUSIONS", "content": "In this work, we have introduced a novel framework,\nrlDE, which employs RL for the automatic design of DE al-"}]}