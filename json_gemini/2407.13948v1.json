{"title": "Assurance of Al Systems From a Dependability Perspective", "authors": ["Robin Bloomfield (City, Univ. of London)", "John Rushby (SRI)"], "abstract": "We outline the principles of classical assurance for computer-based systems that pose significant risks. We then consider application of these principles to systems that employ Artificial Intelligence (AI) and Machine Learning (ML). A key element in this \"dependability\" perspective is a requirement to have near-complete understanding of the behavior of critical components, and this is considered infeasible for AI and ML. Hence the dependability perspective aims to minimize trust in AI and ML elements by using \"defense in depth\" with a hierarchy of less complex systems, some of which may be highly assured conventionally engineered components, to \"guard\" them. This may be contrasted with the \"trustworthiness\" perspective that seeks to apply assurance to the AI and ML elements themselves. In cyber-physical and many other systems, it is difficult to provide guards that do not depend on AI and ML to perceive their environment (e.g., other vehicles sharing the road with a self-driving car), so both perspectives are needed and there is a continuum or spectrum between them. We focus on architectures toward the dependability end of the continuum and invite others to consider additional points along the spectrum. For guards that require perception using AI and ML, we examine ways to minimize the trust placed in these elements; they include diversity, defense in depth, explanations, and micro-ODDs. We also examine methods to enforce acceptable behavior, given a model of the world. These include classical cyber-physical calculations and envelopes, and normative rules based on overarching principles, constitutions, ethics, or reputation. We apply our perspective to autonomous systems, AI systems for specific functions, generic AI such as Large Language Models, and to Artificial General Intelligence (AGI), and we propose current best practice and an agenda for research.", "sections": [{"title": "Introduction: Assurance for Traditional Systems", "content": "Humankind has been concerned about the safety of their constructions ever since they started making them. From the beginning, they noted failures, developed good practices, and specified liabilities and penalties. Nearly 4,000 years ago, the Code of Hammurabi stipulated:\n\"If a builder build a house for some one, and does not construct it\nproperly, and the house which he built fall in and kill its owner, then\nthat builder shall be put to death\" [67, Section 229].\nHazards to safety depend on what a constructed thing does and how it does it. Buildings, boats, bridges, and mines were among the earliest constructions, and the hazards were that they would fall down, break up, or catch fire, so nascent safety engineering and assurance focused (not always successfully) on ensuring that they were of adequate strength and were based on some understanding of the mechanisms of stress and failure. Active systems such as boats not only needed to be strong, but to possess some form of stability so that they would right themselves rather than tip over in wind gusts, and a means of control so they could steer a desired course. And inherently dangerous constructions such as underground mines would need to include escape routes from collapse or fire. These concerns and methods were refined in the industrial revolution as machines such as high pressure steam engines did new things and introduced new hazards. Systems such as railways introduced the need for active procedures to ensure safe operation, such as signaling protocols to prevent two trains using the same track.\nLater, control systems became automated, first by mechanical systems such as governors, then by analog electronic systems such as autopilots, and then by digital computers. Protocols and protection systems also became automated, first with interlocks and then with full automation implemented by digital computers.\nSystems with control and procedural mechanisms implemented by computers (so-called cyber-physical systems, CPS) drive the state of the art in safety engineering and assurance today [98]. The central tenet of the approach taken, which we call the dependability perspective, is that there must be near-complete understanding of how the given system works, what are its hazards, how these are eliminated or mitigated, and how we can be sure all this is done correctly. The evidence and arguments that justify confidence in the claims documenting this understanding constitute what is called an assurance case (see later for details and references).\nSystems that use Artificial Intelligence (AI) and Machine Learning (ML) are both an evolution and a step change from their predecessors: although they often automate existing systems and procedures, they work in different ways to what has gone before, and they can also do different things. Because they work in different ways, it is difficult to apply established methods for safety engineering and assurance"}, {"title": "Traditional Systems and their Assurance", "content": "State of the art non-AI cyber-physical systems such as aircraft flight control, safety systems such as nuclear shutdown, and all manner of systems within critical infrastructure, medical devices, personal gadgets and much else are generally engineered and assured for suitably high levels of safety and other required attributes, such as security or effectiveness, all generically referred to as dependability [79,101]. In outline, the process for doing this begins with identification of the potential hazards that the proposed system might entail. A hazard is a circumstance with an unacceptably high risk of leading to harm or other undesired outcome. Hazard analysis is conducted in the context of assumptions about the environment in which the system will operate and is not an exact science: even its most effective methods can be imperfect and their application requires skill, knowledge and experience [78,115].\nAs hazards are identified, the system and its evolving design are adjusted to eliminate or mitigate them. For example, if fire is a hazard, we may try to eliminate it by removing sources of ignition and fuel; if that is impossible or inadequate, we can try to mitigate the hazard by adding a fire suppression system. But then we have new hazards concerning failure of that suppression system. Note that we usually try to separate those parts of the system concerned with elimination and mitigation of hazards from those parts that deliver its general functionality: the goal is to minimize the size and complexity of those parts that need the highest levels of assurance. We will also want to protect these critical parts from the rest of the system: a practice known as partitioning [141]. Of course, some aspects of the system's general functionality may also be considered critical and they, too, will be partitioned to the extent possible, and subject to assurance. And some auxiliary functions such as logging may also be considered critical as they will be needed to support forensic investigation in the case of failure (consider the difficulty in conclusively demonstrating failures of the British Post Office Horizon system [38,113]).\nAfter some rounds of iteration on hazard identification and modifications to the system goals and design, we will have a set of requirements for the critical desired behavior of the computer control system that, with high confidence, ensures dependability of the overall system in its assumed environment. Identification and articulation of properties assumed about the environment are fundamental to formulation of requirements and are often the most difficult and fault-prone aspects of the entire system engineering endeavor. The analysis and reasoning that shows that the requirements ensure safety and other critical properties within their environment is an assurance task that we term dependability requirements validation.\nRequirements concern what the system will do, not how it will do it, so they should largely be described in terms of changes the system is to bring about in the environment (this is a key insight due to Michael Jackson [80]). How the system"}, {"title": "From Assurance to Dependability", "content": "will do its task is developed in specifications for the defined behavior of the system and the architecture of its components. Architecture is a generalization of partitioning (often portrayed by \u201cboxes and arrows\" diagrams) and its purposes are to identify fault containment regions that limit fault propagation among components, to identify critical components and limit their complexity (because complexity is a source of faults), and generally organize things so that dependability relies on only the architecture and the defined behavior of the critical components [29].\nWe then implement the system according to its specifications and architecture. The mechanisms that ensure an architecture is faithfully represented in the system implementation are among the most difficult engineering challenges in computer science (involving operating systems, \"buses,\" distributed consensus, state-machine replication, transaction mechanisms etc.) and should employ only well-attested techniques and products with no \u201chomespun\u201d solutions [142]. During implementation, we may discover new hazards and the whole process iterates: the new hazards cause revision to the requirements\u00b9 and their safety validation, and also to the specifications and hence to the implementation.\nAssurance is developed during and following this process. After dependability requirements validation, assurance divides into three verification tasks. (Verification differs from validation in that, in principle, it can be performed with perfect accuracy.)\nIntent. The specifications must be shown to be correct and complete with respect to the requirements, subject to properties of the architecture and assumptions about the environment\nCorrectness. The implementation must be shown to be correct and complete with respect to the specifications, subject to properties of the architecture and assumptions about the environment.\nInnocuity. Any part of the implementation that is not derived from the requirements must be shown to have no unacceptable impact.\u00b2\nDifferent industries have their own standards and guidelines that codify aspects of this process, often in great detail; the very generic and abstract description given"}, {"title": "From Assurance to Dependability", "content": "above is based on the Overarching Properties (OPs) proposed as the basis for future civil aircraft certification in the USA [73].\u00b3\nEach of the assurance validation and verification tasks states that some properties \"must be shown\" to hold; by this, we mean that there must be reasons why the properties hold, and these reasons must be clearly articulated and justified. The state of the art for doing this is an assurance case (a generalization of safety cases [1,88]) that provides an organized presentation based on claims, evidence, and argument [23, 144]. Claims identify properties of the system and/or its environment; evidence refers to observations, measurements, or experiments on the system or its means of construction or on its environment that justify certain claims; and the argument uses the evidence to establish a hierarchy of claims culminating in a significant top claim. The arguments of an assurance case are not free form but structured as hierarchy of argument steps, each of which establishes a \u201cparent\" claim on the basis of one or more \u201cchild\" claims (we usually say subclaims) established at lower levels, or by evidence. A portion of a graphical rendering of an assurance argument is displayed in Figure 1; our preferred treatment of modern assurance cases, which we call Assurance 2.0, is presented elsewhere [27] and builds on the ideas that a strong assurance case should be indefeasible [28,145], based on established theories [167], and subjected to dialectical examination [26].\n1.2 From Assurance to Dependability\nThe focus on dependability validation and verification with overall justification presented as an assurance case might seem like good practice and a sensible way to develop high quality systems, but why is it needed for assurance? Why don't we just test the thing? Indeed, whenever there is a major systems failure, the first reaction of the press and public is \"they didn't test it enough.\" But in fact, testing is insufficient and the reason is the extraordinary levels of confidence required for safety and other critical properties and, consequently, the infeasibly large number of tests that would be required to validate them by observation alone. We give a few numbers for illustration.\nIn commercial airplanes, \u201ccatastrophic failure conditions\" (those \"which would prevent continued safe flight and landing\") must be \"so unlikely that they are not anticipated to occur during the entire operational life of all airplanes of one type\" [54]. The \"entire operational life of all airplanes of one type\" is about 108 to 109"}, {"title": "Assurance for Systems Extended with AI and ML", "content": "flights for modern airplanes. With an average flight duration of about 90 minutes, this requires a critical failure rate no worse than about 10-9 per hour.\u2074\nCars, most of whose accidents are caused by human driver error, are among the most dangerous consumer goods, with about 35,000 deaths per year in the United States and a fatal accident rate of a little over 10 per billion miles. It is intended that self-driving cars should be safer than human drivers, so it might seem reasonable to set the target at no more than 1 death per billion miles, which is 4 \u00d7 10-7 per hour, if we assume 25 mph. However, that is a rather technical assessment that pays no attention to likely public reaction. Herbert Diess, the CEO of Volkswagen is quoted on their website with a more realistic assessment: \"A ratio of ten-to-one is nowhere near good enough. We have approximately 3,200 traffic fatalities in Germany each\nIn this section, we focus on systems that do fairly traditional things but are now extended with capabilities enabled by AI and ML. Autonomous CPS such as self-driving cars are canonical examples.\nFor traditional assurance, there must be good reasons why we believe the system achieves its dependability goals and those reasons are documented and justified in its assurance case. Systems that use AI and ML pose challenges to this approach because, rather than performing actions that are effective and safe for reasons that can be articulated and verified, a system that uses ML operates by learning suitable behavior during a period of training. Training typically defines empirically effective \"weights\" in a deep neural network (DNN); there will often be millions, or even billions, of individually adjusted weights. The hope is that if the system works correctly on the training examples, then it will work correctly on all similar examples.\u2077\nAn alternative to ML is symbolic AI, which uses automated deduction (theorem proving) to derive conclusions from premises composed of a set of axioms describing some aspect of the world plus observations about the current state of the world. It is possible to guarantee validity of some methods of automated deduction (e.g., SAT and SMT solvers with certificates [116]), but soundness depends on the choice of premises, which may be unvalidated and derived from an informal or empirical model, and also on the computational resources available (deduction generally requires exponential time, or worse, so we may need to accept whatever partial anal-ysis can be accomplished in fixed time). \"Expert systems\" were a type of symbolic AI popular in the 1980s where deductive procedures were applied to a collection of \"rules\" that axiomatized some domain. The concern, and one of the reasons for the demise of these systems, was that individually reasonable rules could collectively be inconsistent or incomplete, resulting in faulty output [140]. Modern symbolic AI uses improved technology but its challenge to assurance remains largely unchanged.\nA combination of symbolic AI with ML is a popular current approach known as"}, {"title": "Runtime Verification", "content": "neurosymbolic AI. The strengths and weaknesses of the two approaches seem to be complementary, but this does not assuage their assurance problems.\nTraditional assurance requires good understanding of how the system works because tasks such as intent verification must show that certain properties hold in all circumstances. This is infeasible for most AI and ML components because we lack detailed understanding of their operation, but an alternative or constituent part of the overall system assurance process can be to check that properties hold in the circumstances actually encountered during operation. This is runtime verification [139] or, more boldly runtime certification [143], where components, often generically referred to as guards (or monitors [69,117]), are added to the system to check its behavior against its required or specified properties, or conservative simplifications of these. If a check fails, then the system must take some remedial action to maintain or restore safety. Both checking and remediation add complexity to the system and may themselves introduce failures and hazards, so this approach requires careful engineering [96]. Nonetheless, a plausible approach is to guard AI and ML elements with conventionally engineered components that perform runtime verification and can be assured in the conventional way. This approach is endorsed in some industry guidelines such as F3269-17 for unmanned aircraft [7].\n2.1 Runtime Verification\nTo investigate runtime verification for systems with AI and ML components, we need some general understanding of the likely overall system architecture and the properties that will be checked. The top-level structure of almost any system that employs AI or ML follows from a single insight, which is that any entity that interacts effectively with some aspect of the world must have a model of that aspect of the world [43]. In particular, cyber-physical systems are always based on a model of the controlled \"plant\" and its environment [57]. For pure control systems, the model may be a collection of differential equations; for systems that involve procedures it may add state machines; and for full CPS it will include integrated formalisms such as timed and hybrid automata.\u2079\nThe model may be used in development but be represented only indirectly in the final system (e.g., as control laws in classical control engineering), or it may be"}, {"title": "The Challenge of Assuring Perception", "content": "partially defined at design time and built in to the system with parameters that can be adjusted at runtime (as in adaptive control systems), or it may largely be constructed at runtime and represented explicitly within the system (as in model-predictive control and autonomous systems). A characteristic of most systems that employ AI and ML is that these capabilities are used in building and maintaining a model of the \"world\" (i.e., its environment, such as the locations of other road users) at runtime; this function is generally referred to as the perception (sub)system. Optionally, AI and ML may also be employed in the action (sub)system, which uses the world model to calculate and execute behavior that will advance the system's progress toward its goal, while maintaining dependability.\u00b9\u2070\nIt is often possible to guard AI or ML-generated actions with highly assured conventional software that checks their safety against the world model. However, if the world model is constructed by a perception system that uses AI or ML then we have to ask how its own accuracy can be assured or guarded: it does no good to run safety checks against a faulty model. One possibility is for the guard to use a different model that can be assured.\nWe can therefore distinguish two classes of guarded AI-enabled systems accord-ing to the nature of their runtime verification. Assuredly guarded systems are those whose safety and other critical properties can be checked and enforced by assured guards that do not themselves use AI or ML, neither for perception nor action. In cases where the guards are so strongly assured that they can be considered \u201cpossibly perfect\" (or \"probably fault-free\"), it is possible to make very strong claims for de-pendability of the overall system [108,179]. The second class is unassuredly guarded systems, where the guards themselves depend on AI or ML, typically in their per-ception systems. See, for example, the architectures listed on page 19, where those that include a traditionally assured backup guard are assuredly guarded and others (except number 1, which is not guarded) are unassuredly guarded.\nAn example of the first class is an unmanned aircraft controlled by AI and ML that is deemed safe as long as it remains within some specified portion of airspace (a \"geofence\"). This is a constraint that can be checked by a conventional navigation system and enforced using a conventional guidance system as an override when violations are detected [46]. Many robots can be guarded by \u201cvirtual cages\u201d of this kind. Notice also that when formulating the initial \"concept of operation\" for a system, it may be possible to adjust the concept so that assurable guards become feasible. For example, rather than an autonomous shuttle bus sharing its route with other vehicles and pedestrians, it could use a dedicated track, or rails, and would not then require a sophisticated AI-based perception system.\n2.2 The Challenge of Assuring Perception\nWe have seen that an action subsystem can generally be assuredly guarded, but is dependent on a model of the world constructed by a perception subsystem that is harder to guard.\nA possible compromise is for the action guard to use a simpler, conservative model constructed by conventional and highly assured software. This is sometimes seen in self-driving cars where the guard is a simple system for automated collision detection and emergency braking, similar to the Automated Driver Assistance Sys-tem (ADAS) that is provided for some human-driven cars.\u00b9\u00b9 However, this does not provide full assurance because forward collisions are not the only hazards (Mehmed and colleagues cite data from a NHTSA study of 5.9 million human-driver accidents classified in 37 categories [118]), nor are human accidents necessarily good models for failures of an autonomous system, and nor is emergency braking attractive as the sole method of hazard mitigation. Furthermore, accidents and collisions are not the only hazards that should be considered: for example, the City of San Francisco has reported dozens of incidents where robotaxis interfered with emergency responders, and there must be many other circumstances where self-driving cars increase risk or inconvenience for others without themselves being involved in a collision.\nFor verified dependability, assurable guards must detect all hazards and mitigate them safely, without excessive false alarms. It is possible that a more comprehensive suite of verifiable ADAS-like guard functions could do better, but they would have to steer a difficult path among incomplete coverage, false alarms, and unattractive, abrupt, responses. A contrary point of view is that although there may be many"}, {"title": "Assurance through Diversity and Defense in Depth", "content": "circumstances leading to accidents, the exact circumstances are irrelevant to \"last second\" detection as there are only a few possible emergency responses: essentially, braking or evasive action, and so an ADAS-like guard (or a suite of such guards) could be an acceptable means of assurance, provided we can develop assurance that it will always select and perform appropriate emergency responses (see Section 2.5).\nAn argument against simple ADAS-like guards is that more sophisticated percep-tion could detect hazardous situations earlier and provide less abrupt mitigation. Accordingly, it is worth considering guards that do use AI and ML and asking whether they can be assured for trustworthiness within an overall approach that remains close to the dependability end of the dependability/trustworthiness spec-trum. For example, we could imagine an \"ML-friendly\" adjustment to traditional assurance where we do construct requirements and identify hazards, and then de-velop training data of \"sufficient\" size, coverage, and quality\u00b9\u00b2 to encompass all of these and use it with a well-regarded ML toolset to generate the perception capabil-ities desired. In fact, this is exactly what is proposed by several groups engaged in research and development of autonomous systems [4,19,47], although they generally consider primary systems rather than guards, and only one focuses specifically on perception [150].\nThe hazard most generally recognized in perception using ML is lack of \"ro-bustness,\" meaning that small changes in sensor data may cause its ML-generated interpretation to change abruptly. This concern is validated by so-called \"adver-sarial examples\" [161]. Typically demonstrated on image classifiers, the examples are deliberately constructed minor modifications to an input image that are indis-cernible to a human observer but cause an ML classifier to change its output, often drastically and inappropriately. Image masks have been developed that will cause misclassification when overlayed on any image input to a given classifier, and there are universal examples that will disrupt any classifier [120]. Furthermore, there are patterns that can be applied to real-world artifacts (e.g., small images that can be stuck to traffic signs) that will cause them to be misread by an object classifier [33].\nThere is much work on detection and defense against adversarial attacks (see [75] for a survey) and on the threats posed by general lack of robustness [100,168]. A problem with all this work is that the techniques for guaranteed robustness have so far scaled only to relatively simple systems and not, for example, to the object detector of a self-driving car. And, more importantly, robustness is not the topic we really care about: we want assurance of accurate perception. There is work that verifies contracts on some aspects of perception, notably detection of traffic lanes [8], but this particular problem can also be solved without ML [48] (although those solutions may also be hard to verify).\n2.3 Assurance through Diversity and Defense in Depth\nIn our discussion so far, we have been using guards for runtime verification, where overall assurance depends on that of the guard. But we could also argue that any guard, even if it is not assured, will provide redundancy, and its development and implementation can be completely independent and \"diverse\u201d from the primary"}, {"title": "Summary of Architectural Choices", "content": "system. Hence, it is plausible that failures of the guard will be independent of those of the primary and the combination could deliver a multiplicative improvement in dependability (i.e., naively, two systems with pfd < 10-4 give us 10-8 overall). We can also imagine a more integrated system where, rather than a primary system and a guard, we have redundant, diverse perception systems with different architectures and training sets contributing to a single consensus model, or to one model for operation and another more conservative one for runtime verification of actions.\nThe topic of assurance through diversity is large and somewhat contentious. There is little doubt that architectures employing diverse components are generally more reliable than single threads. In particular, there is evidence that \"portfolio\" or \"ensemble\" perception systems are more reliable than their individual con-stituents [84]. The difficulty is in demonstrating that diversity provides benefit in any particular case, and in estimating how much benefit it provides [107]. In par-ticular, there is no feasible way to validate failure independence (it is a variant on the infeasibility of assurance by testing), nor strong reasons for believing it. This is because some circumstances are just plain hard to interpret and it is possible that all components may then fail together: consider the scenario with the Cruise self-driving car described earlier would any training set have included pedestrians being thrown under the wheels? Furthermore, these difficult cases do not \"thin out\" as more are considered: the distributions seem to have \u201cfat tails\" [94]. Thus, the \"multiplicative\" argument for assurance by diversity is indeed naive.\nHowever, although diversity alone cannot provide strong assurance, it can pro-vide a useful step in a \"ladder\" of assurance. Nuclear power generation usually employs such a ladder of protection systems providing defense in depth: there is the operational control system, designed to manage the plant efficiently and safely, then a (safety) limitation system that can intervene to ensure the plant behavior stays operational but within some safe envelope, and finally a shutdown system that functions as an assured guard that guarantees to initiate a safe shutdown.\u00b9\u00b3 The operational and limitation systems are carefully engineered but do not guarantee the dependability goals established for nuclear power: that is accomplished by the assured shutdown system. But the operational and limitation systems are diverse in function and construction and although this does not provide strong assurance, the presence of the limitation system almost certainly reduces demands on the shut-down system. This is beneficial because an emergency shutdown is disruptive and expensive.\nThe control, limitation, and shutdown systems in this architecture for nuclear power generation all use traditional software, but a similar approach could be used in systems with AI and ML. For example, in a self-driving car the full functionality"}, {"title": "Operational Design Domains and Micro ODDs", "content": "could be provided by a primary system employing AI and ML that is supported by a diverse system, also employing AI and ML, that is focused on safety, with assurance provided by traditionally engineered ADAS-like emergency backup functions. The diverse primary and safety systems deliver acceptable behavior and reduce demands on the assured backup so that its interventions, though crude, are rare and toler-able. A safety system may introduce false alarms of its own, although these can be verifiably avoided in some circumstances [119]. Example AI safety functions in-clude the \u201cAI Safety Force Field\" [122] that avoids creation of unsafe situations, and \"REDriver\" [160], which monitors proposed trajectories of self-driving cars against Chinese traffic laws.\nA criticism of these particular proposals for defense in depth is that the diverse safety-focused system is concerned solely with actions and relies on the same world model as the primary system. If the perception system fails to detect a pedestrian, for example, then it will be absent from the common model and the safety-focused system can deliver no protection: everything will depend on the emergency backup.\nOne possible mitigation for this hazard is to provide the safety-focused system with separate sensors and perception; alternatively, it could use the same sensors as the primary system but with diverse perception software. Either arrangement would provide the primary and safety-focused systems with different models of the world (e.g., [68]) but it is debatable whether diverse models are manageable without false alarms and whether the ML-generated model for the safety system could be any more assurable than that for the primary.\u00b9\u2074\nIt seems preferable to fuse the products of the diverse perception systems into a single model, but, as illustrated by the Uber crash described earlier, that can in-troduce flaws of its own. A more attractive approach is to construct a single world model using diverse perception systems in a principled way. Conventional percep-tion systems work \"bottom up\": one or more deep neural nets take sensor data (e.g., images from cameras or point clouds from lidars) and deliver interpretations (e.g., lists of detected objects) that are further processed and fused to produce the world model. One argument against this approach is that it works \u201cbackwards\u201d from effect (image) to cause (objects), which is inherently difficult. Another is that it prioritizes fleeting sensor data above the world model, which is the repository of much accumulated information. An alternative approach, and the way human perception is believed to work [18], reasons \u201cforwards\u201d using the model to predict sensor data (or basic interpretations thereof) and then applies a form of Bayesian"}, {"title": "Assurance of AI Systems for Specific Functions", "content": "Having considered a range of architectures, we now consider the range of envi-ronments in which they may be required to operate.\n2.5 Operational Design Domains and Micro ODDs\nAssurance goals for a system can be lessened by limiting the range or complexity of circumstances (i.e., environments) in which it is required to operate. For example, self-driving is easier on freeways or in traffic jams than on city streets. These different circumstances are referred to as Operational Design Domains (ODDs) and a system may be assured only for specific ODDs and be required to disengage when outside those permitted (alternatively, the system may have different modes of operation in\n21\ndifferent ODDs). Clearly, the perception system must be augmented to determine when it is in permitted or specific ODDs.\nThe top level of assurance, namely dependability requirements validation, is strongly focused on hazards and these are largely determined by the chosen ODD. Hence, some approaches argue that assurance should be based on scenarios (i.e., ODDs) rather than technology [40, 89], and others are very focused on identify-ing hazards associated with chosen ODDs [55]. We propose a variant on these approaches.\nThe most credible architectures for assured dependability are those that employ a traditionally engineered and highly assured backup guard as in architectures (2), (4), and (7) of the previous section. However, an argument against these is that interventions by the guard may be too frequent (some due to late detection, others to false alarms) and too crude (e.g., emergency braking). This can be improved by defense in depth as in architectures (4) and (7), where a safety guard that uses AI perception (and is therefore weakly assured at best) reduces demands on the backup.\nA further enhancement might be to use multiple backup guards, each specialized to a particular circumstance or ODD. The idea is that forward-looking radar coupled to emergency braking may provide an assured backup arrangement suitable for driving in traffic, but for highway driving it would be better to look further ahead using cameras (having non-AI perception) with speed control as the intervention. These ODDs would be tailored to assured detection and intervention strategies and might not correspond to traditional ODDs: following [95] we call them micro-ODDs (also written as \u03bcODDs). The idea is that for any particular micro-ODD, using the \"right\" backup guard will deliver superior safety, with fewer false alarms and less disruptive interventions.\nThis suggests an architecture such as portrayed in (8) below where a portfolio of assured backup guards is coordinated by a detector that recognizes the current micro-ODD and selects the appropriate assured backup guard. Of course, the per-ception system of the detector must be assured, but its task seems to be rather simple and it is conceivable that it can be performed without AI, or by AI and ML of credible trustworthiness (see [111] for relevant work).\n8. Architecture (7) with a portfolio of assured backup guards coordinated by an assured detector that recognizes their matching micro-ODDs."}, {"title": "Assurance for Generic AI", "content": "In this section we consider novel non-CPS systems and applications that are made possible or are performed in new ways by the capabilities of AI and ML. These include systems that play games of skill or strategy, those that design things or perform scientific predictions such as protein folding or weather forecasting, the generation or management of responses based on these, decision support systems that analyze medical images or loan, job, and college applications or prisoner sen-tencing and parole and so on, and also systems that use Large Language Models (LLMs) such as ChatGPT and other generic AI and ML capabilities to perform specific functions, which can include generation of images and video as well as text. We do not include generic capabilities themselves (those are in the next section).\nThe reason we focus here on systems that perform specific functions is that assurance needs to identify hazards; the hazards of systems that perform specific functions can be conjectured from the functions concerned and the environment in which they are deployed, whereas the hazards of generic systems such as LLMs have to consider all the functions they might be called to perform and all the environments they might be deployed in.\nHazards of specific applications include: making incorrect or poor decisions, gen-eration or approval of offensive or untrue material, exhibiting bias or stereotyping in any of these activities, causing distress, enabling crime (e.g., extortion using voice clones), vulnerability to manipulation, and so on. Many of these are quite differ-ent to the hazards of traditional systems, so standard methods of hazard analysis may be difficult to apply. However, hazards specific to AI and ML can often be anticipated by considering poor or malicious human performance and interaction in a similar context. For example, Microsoft's \"Tay\" was a Twitter bot that the company described as an experiment in \u201cconversational understanding.\" The more you chat with Tay, said Microsoft, the smarter it gets, learning to engage people through \"casual and playful conversation.\u201d Within less than a day of its release, it had been trained by a cadre of bad actors to behave as a racist mouthpiece and had to be shut down [174]. This is a hazard that should have been anticipated."}, {"title": "Assurance and Alignment for AGI", "content": "Similar to the detection of hazards", "dependability\" even though the specific hazards may not concern risk to life or conventional assets. Assurance can then be founded on similar principles as it is for traditional systems, and required confidence will be graduated according to severity of the hazards [42": ".", "conflict": "for example, control of offensive material may conflict with free speech, and suitable policies and compromises must be developed. But these conflicts exist in human systems, it is just that normally we do not have to document explicit choices as we do when formulating dependability requirements (consider recent difficulties in university responses to student protests).\nWhile it seems plausible that dependability requirements validation can be per-formed for systems powered by AI and ML much as it is for conventional systems, differences and difficulties arise at the next stage. These are the verification tasks where, as we saw in the previous section, specifications are likely to be absent and there are no strong reasons to justify intent or correctness verification other than statistical observation. However, unlike the safety-critical applications considered in the previous section, those contemplated here may require only modest levels of confidence with respect to dependability (because they do not pose immediate high-rate hazards to life or critical assets) and it is possible that statistically valid testing could contribute adequate assurance, particularly when coupled (via CBI) with methods for prior confidence such as careful selection of training data, and architectural mechanisms for mitigation such as diverse redundancy and runtime checking.\nFor example, if racial bias is recognized as a hazard, then it might be mitigated by removing race from the data presented to the ML in training and operation. A weakness in this approach is that the ML may discover a proxy for race (e.g., zip code) among the data that it does see, so a better alternative may be to mask this characteristic in training by assigning race randomly.\nWhile some basis for assurance can be incorporated into custom ML systems by careful choice of training data, as sketched above, it is becoming more com-mon to create applications"}]}