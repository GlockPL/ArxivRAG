{"title": "BEEXFORMER: A FAST INFERENCING TRANSFORMER ARCHITECTURE VIA BINARIZATION WITH MULTIPLE EARLY EXITS", "authors": ["Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "abstract": "Large Language Models (LLMs) based on transformers achieve cutting-edge results on a variety\nof applications. However, their enormous size and processing requirements make deployment on\ndevices with constrained resources extremely difficult. Among various efficiency considerations,\nmodel binarization and Early Exit (EE) are common effective solutions. However, binarization may\nlead to performance loss due to reduced precision affecting gradient estimation and parameter updates.\nBesides, the present early-exit mechanisms are still in the nascent stages of research. To ameliorate\nthese issues, we propose Binarized Early Exit Transformer (BEExformer), the first-ever selective\nlearning transformer architecture to combine early exit with binarization for textual inference. It\nimproves the binarization process through a differentiable second-order approximation to the impulse\nfunction. This enables gradient computation concerning both the sign as well as the magnitude of the\nweights. In contrast to absolute threshold-based EE, the proposed EE mechanism hinges on fractional\nreduction in entropy among intermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces the FLOPs during\ninference by 54.85% and even improves accuracy by 5.98% through resolving the \"overthinking\"\nproblem inherent in deep networks. Moreover, the proposed BEExformer simplifies training by\nnot requiring knowledge distillation from a full-precision LLM. Extensive evaluation on the GLUE\ndataset and comparison with the SOTA works showcase its pareto-optimal performance-efficiency\ntrade-off.", "sections": [{"title": "1 Introduction", "content": "The last decade has witnessed substantial progress in the field of Natural Language Processing (NLP). Current state-of-\nthe-art (SOTA) comprises pre-trained Large Language Models (LLMs) involving transformers. However, such models\nhave exorbitant computational complexity, which limits their applications to resource-constrained setups [30]. This\nnecessitates the need for efficient modeling practices. Efficient modeling reduces memory as well as computational\npower requirements. It also lowers the training and inference time of the model. Moreover, it opens the door to\ndeployment on edge devices [1].\nVarious modeling aspects have been considered towards enhancing the efficiency of models, including pruning [2],\nknowledge distillation [3], low-rank approximation [4], and quantization [5]. Out of these, quantization has become\nquintessential for deploying NLP models on edge devices [6]. While the majority of efficiency considerations target\nreduction in the number of layers, nodes, or parameters, quantization targets efficiency by reducing the precision of\nthe model's memory representation. In neural networks, quantization can be segregated as Post Training Quantization\n(PTQ) [7, 8, 9] and Quantization Aware Training (QAT) [10, 11]. In PTQ, the model is quantized post-training, hence"}, {"title": "2 Related Works", "content": "In this section, a commentary on the related works has been presented. Since the proposed BEExformer is the first\nmodel combining binarization with EE for textual inference, we separately review works implementing BNN and EE\nfor textual content."}, {"title": "2.1 Binarized Neural Networks (BNN)", "content": "Bai et al. [17] implemented weight binarization in a BERT model. To tackle sharp loss in performance, they apply\nknowledge distillation along with ternary-weight splitting to initialize their model. However, they experience hindrances\nin binarizing activations and were not able to implement a fully binarized network due to a sharp performance drop due\nto loss of precision. Qin et al. [18] proposed a fully binarized BERT model applying binarization subject to information\nentropy maximization along with direction-matching distillation inferring from similarity matrices to deal with direction\nmismatch. Liu et al. [10] formulated a closer approximation to the impulse function and updated the weights based\non a magnitude-aware gradient. Liu et al. [19] further proposed a multi-step distillation approach to gradually distill\nthe model into lower precision before obtaining the final BNN from a full-precision BERT model. They utilize an\nelastic binary activation with parameters inferred while training. All of the above-mentioned works adopt knowledge\ndistillation from a full-precision model to counter performance loss. However, executing both student and teacher\nmodels simultaneously can be memory-intensive. Furthermore, additional fine-tuning on downstream tasks is often\nrequired for optimal results. been presented. Since the proposed BEExformer is the first model combining binarization\nwith EE for textual inference, we separately review works implementing BNN and EE for textual content."}, {"title": "2.2 Early Exit (EE)", "content": "Zhou et al. [22] developed an early-exit mechanism with the layers of a Pre-trained Language Model (PLM) interlaced\nwith internal classifiers to exit from the network. They determined the change in predictions of successive internal\nclassifiers, and the exit criterion was fulfilled if no change could be observed for a given number of steps. In the\nsame context, Xin et al. [15] calculated the entropy of predictions from the internal classifiers, and if it fell short of a\npredetermined threshold, the exit criterion was fulfilled. However, determining the optimal threshold value is a daunting\ntask. Liu et al. [23] put forth a pre-trained transformer model with dynamic exits having loss calculated at each layer\nas the sum of Masked-Language Modeling (MLM) and Sentence-Order Prediction (SOP) losses. Mangrulkar et al.\n[13] proposed a modified version of the switch transformer, formulating a switch layer to assess the complexity of\na sentence and optimally route it to an expert model having fewer layers. Here, the expert models comprise a set of\nBERT models with a varying number of encoder layers followed by a prediction layer. Here, expert routing augments\nthe model complexity, translating into additional resource requirements. Moreover, the effectiveness of expert routing\nhinges on the capability of the experts to be specialists for varying input patterns."}, {"title": "3 Proposed Architecture", "content": "The proposed BEExformer architecture embeds an input sequence and processes it through a cascade of binarized\ntransformer blocks with the provision to exit early after each block. Each transformer block possesses binarized\nmulti-head attention (MHA) with parameters updated via both sign as well as magnitude-aware gradient. Additionally, a\nselective learn-forget network binarized in a similar manner replaces the feed-forward layer in conventional transformer\narchitecture for precise estimation of context. For EE, an intuitive criterion monitoring the fractional reduction in\nentropy between consecutive transformer blocks has been devised. Once the exit condition is satisfied, the processing\nis routed to an auxiliary block comprising binarized feed-forward layers to obtain the final predictions. This enables\na two-fold efficiency enhancement due to a binarized architecture along with a provision to reduce the amount of\ncomputation dynamically during inference by terminating before the execution of all transformer blocks. The proposed\narchitecture has been illustrated in Figure 1, while its components have been elucidated herein-below."}, {"title": "3.1 Input Representation", "content": "The input sequences in a given corpus are at first tokenized, and a lookup table LUT() is constituted as a one-to-one\nmapping between the unique tokens Vi in the vocabulary V with a unique integer Ti, \u22001 \u2264 i \u2264 |V. For each sentence,\na tokenized sequence St is constructed consulting the lookup table as shown in Algorithm 1. Besides, padding is\nappended to ensure uniform sequence length 1."}, {"title": "3.2 Overall Architecture", "content": "The backbone network consists of a sequence of C transformer blocks with an exit provision after each block as portrayed\nin Figure 1a. It can be functionally represented as $F(x)$ taking input x and having a cascade of C differentiable functions\n(each representing a transformer block) as follows:\n$F(x) = (f_C \\circ f_{C-1} \\circ ... \\circ f_2 \\circ f_1)(x)$\nWhere, $\\circ$ denotes composition operator. The hidden representations from the cth intermediate block is given as:\n$h_c = (f_c \\circ f_{c-1} \\circ ... \\circ f_2 \\circ f_1)(x), \\forall 1 \\le c \\le C$\n$\\Rightarrow h_c = f_c(h_{c-1})$\nTo accomplish the objective of EE, a criterion is defined. In case the criterion is fulfilled at exit c, the logits $h_c$ returned\nby the cth exit are fed to its connected auxiliary block $\\Phi_c(\\cdot)$ and the probability distribution $\\hat{y_c}$ is returned. This has\nbeen explained in Section 3.4.\nAdditionally, the BEExformer utilizes BAT (described in Section 3.3) to binarize the weights and activations of all the\ntransformer blocks as well as the auxiliary blocks for EE. Generically, a binarized layer is defined as follows:"}, {"title": "3.3 Binarization", "content": "Conventionally for binarization, $sign(a^r)$ (equation (7)) is applied. However, being not differentiable, it cannot be\napplied to compute the gradient. To ameliorate this, we apply magnitude-aware approximation of $sign(a^r)$ as in Bi-Real\nNet [10] for the first time to the best of our knowledge in a transformer encoder. Denoted by $g(a^r)$, this approximation\nis expressed in equation (8). From the comparison between $sign(a^r)$ and $g(a^r)$ along with their derivatives in Figure 2,\nit can be seen that $g(a^r)$ closely approximates $sign(a^r)$ equipped with the capability of being differentiable. Given\n$g(a^r)$, its derivative is computed as in equation (9).\n$sign(a^r) = \\begin{cases}\n-1, if a < 0 \\\\\n1, if a \\geq 0\n\\end{cases}$\n$g(a^r) = \\begin{cases}\n-1, if a^r < -1 \\\\\n2a^r + (a^r)^2, if -1 \\leq a < 0 \\\\\n2a^r - (a^r)^2, if 0 \\leq a < 1 \\\\\n1, if a \\geq 1\n\\end{cases}$\n$\\frac{dg(a^r)}{dr} = \\begin{cases}\n2(1 - |a|), if -1 \\leq a^r < 1 \\\\\n0, if a < 1 or a \\geq 1\n\\end{cases}$\nwhere $g(a^r)$ is a differentiable piecewise polynomial function being a second-order approximation of the impulse\nfunction $sign(a^r)$. While $\\frac{dg(a^r)}{Dr}$ is a piecewise linear function approximating the derivative of $sign(a^r)$ during gradient\ncalculation.\nConventional gradient descent cannot be applied to BNN due to the gradient of loss L concerning binarized weights\nbeing insufficient to enable bit-flips. To mitigate this issue, we perform BAT wherein the gradient computed for the\n$c^{th}$ block concerning binarized weights $W_b^{c,t}$ for the $t^{th}$ iteration is used to update real-valued weights $W_r^{c,t+1}$ for the\n(t + 1)th iteration. This has been shown in the following equation:\n$W_r^{c,t+1} = W_r^{c,t} - \\eta \\frac{\\partial L}{\\partial W_r^{c,t}}$"}, {"title": "3.4 Early Exit", "content": "We propose an EE criterion that estimates proportionate entropy reduction over exits, i.e. fractional reduction in entropy\nconcerning the previous exit. When this value falls below a certain threshold $\\delta$, the exit criterion is satisfied. This has\nbeen portrayed in Algorithm 2. Here, the entropy of logits $S(h_c)$ is computed as in equations 12 and 13. Initially, the\nvalue of $S(h_c)$ is taken as $ln(m)$, which signifies the case when logits are spread as evenly as possible across m classes.\n$S(h_c) = - \\sum_{j=1}^m p(h_c^{j}) ln(p(h_c^{j}))$\n$\\Rightarrow St(h_c) = ln(\\sum_{j=1}^m exp(h_c^{j})) - \\frac{\\sum_{j=1}^m h_c^{j} exp(h_c^{j})}{\\sum_{j=1}^m exp(h_c^{j})}$\nFollowing this, an auxiliary block $\\Phi_c(\\cdot)$ is defined to process $h_c$ and predict the output $\\hat{y_c}$ as follows:\n$\\hat{y_c} = \\Phi_c(h_c), \\exists \\Phi_c \\in \\Phi$\nHere, $\\Phi(\\cdot)$ belongs to the set of binarized feed-forward networks $\\Phi = {\\Phi_1, \\Phi_2, ..., \\Phi_c}$ as shown in Figure 1d. It\nprocesses $h_c$ into a discrete probability distribution $\\hat{y_c}$ as the output for the $c^{th}$ exit. To ensure minimal computational\noverhead, $\\Phi(\\cdot)$ has a minimal number of parameters compared to the backbone network F(\\cdot).\nThe proposed EE criterion is robust against diversity in inputs during inference as only the fractional reduction in\nentropy is noted. This alleviates the limitation of previous works with absolute entropy thresholds to deal with diverse\ninputs. Also, determining the absolute threshold is a cumbersome process in such models. Compared to it, setting $\\delta$ is\npretty straightforward based on the computational budget and output confidence requirements. Moreover, the same\nvalue of $\\delta$ can be used across multiple tasks."}, {"title": "3.5 Transformer Blocks", "content": "Each transformer block binarizes the multi-head attention (MHA) $Att_m^{tot}$ module put forth by Vaswani, A.[24] as\ndepicted in Figure 1b. $Att_m^{tot}$ is comprised of H binarized self-attention $Att^i_h$ heads, which in turn relies on queries $Q_b^i$,\nkeys $K_b^i$, and values $V_b^i$ for computation as follows:\n$Att_h^i = softmax(\\frac{Q_b^i(K_b^i)^T}{\\sqrt{D}}) V_b^i$\nwhere $Q_b^i$, $K_b^i$, and $V_b^i$ have been obtained through binarized-linear (bi-linear) transformation (explained in Section 3.3)\non input $x \\in R^{L \\times D}$ where L denotes the sequence length and D denotes the dimension of hidden representations. The\ncomputations have been shown in equations (16 -18).\n$Q_b^i = g(W_q^i)g(x^i)$\n$K_b^i = g(W_k^i)g(x^i)$\n$V_b^i = g(W_v^i)g(x^i)$\nwhere $W_q^i, W_k^i, W_v^i \\in R^{D \\times D/H}$. Finally, $Att_m^{tot}$ is calculated in equation (19) through projection of H concatenated\n$Att^i_h$ through $W^{m} \\in R^{D \\times D}$ after binarization.\n$Att_m^{tot} = g(Concatenate(Att_h^i), i) g(W^{m}), \\forall i \\in H$\nTo augment the efficacy of residual connection, a binarized SLFN is used to replace the feed-forward layer in the\nconventional transformer architecture. This is the modified version of SLFN proposed by Ansar et al. [21]. In SLFN,\nthere are two forget gates, $S_h$ and $S_g$, which aid in the elimination of insignificant information in the previous\nhidden state and the total incoming activations, respectively. Besides, it has a selective learning gate $T_g$ for rational\nestimation of dependencies. All the gates incorporate binarization of incoming activations as shown in equation (8).\nThe architecture has been illustrated in Figure 1c, while the formulation of the gates has been provided in the following\nequations.\n$S_h = \\sigma[g(h_{i-1}) \\oplus g(U_{rg}]$\n$S_g = \\sigma[g(x_i) g(W_{rg}) + (g(h_{i-1}) \\oplus g(U_{gg}]$\n$T_g = tanh[(g(x_i) g(W_{tg}) + (g(h_{i-1}) \\oplus g(U_{tg}]$\nwhere $x^i$ and $h_{i-1}$ denote full-precision input at $i^{th}$ step and (i \u2013 1)th hidden state, respectively. Whereas $W_{rg}, U_{gg}$,\n$W_{tg}$, and $U_{tg}$ denote weight matrices for inputs as well as hidden states, respectively. Here, we apply $g(\\cdot)$ on all\nincoming weights and activations for binarization. Finally, the updated hidden state is calculated as follows:\n$h_i = g(S_g) \\oplus [g(S_g) \\odot g(T_g h_i)]$"}, {"title": "3.6 Loss Function", "content": "The loss function L calculated at the ith exit is given by the following equation:\n$L_i(I; \\theta) = \\frac{1}{|I|} \\sum_{(x,y) \\in I} H(y, F_i(x; \\theta))$\nWhere I is the input data containing sequence-label pairs, (x, y), $F_i(x; \\theta)$ is the probability distribution returned by the\n$i^{th}$ exit, $\\theta$ denotes trainable parameters, and H is the cross-entropy function. The proposed model uses a soft-routing\nloss objective, i.e. combining loss from all exit layers to enhance the decision capability. This aids in optimal assessment\nof the exit based on the predictions from all the exit blocks while reducing the loss. The total loss L' is calculated as the\nmean loss across all exits given by:\n$L'(I; \\theta) = \\frac{1}{|C|} \\sum_{i=1}^C L_i(I; \\theta)$\nwhere C is the total no. of exits."}, {"title": "4 Experimental Setup", "content": "In this section the details of the experiment conducted along with the data-set information have been provided."}, {"title": "4.1 Data-Sets", "content": "The proposed methodology has been tested on various data-sets from the GLUE benchmark [25], such as Stanford\nSentiment Treebank (SST-2) [26], Corpus of Linguistic Acceptability (CoLA) [27], Microsoft Research Paraphrase\nCorpus (MRPC) [28], and Recognizing Textual Entailment (RTE) [25]. These data-sets cover a wide range of tasks such\nas sentiment analysis, linguistic acceptability, semantic similarity between texts, paraphrase detection, and entailment\ndetection. The metrics for evaluation are F1 score for MRPC, Matthews correlation for CoLA, and accuracy for the\nremaining tasks."}, {"title": "4.2 Implementation Details", "content": "The proposed BEExformer has been implemented on a Python 3 Compute Engine with the Nvidia L4 GPU, 22.5GB\nVRAM, 53GB System RAM, and 201.2GB Disk in Google Colab\u00b9. The details of the hyperparameters of the\nBEExformer have been presented in Table 1."}, {"title": "5 Results and Discussion", "content": ""}, {"title": "5.1 Comparison with Related Works", "content": ""}, {"title": "5.2 Pareto Optimality", "content": "Figure 3 gives a clear picture of the pareto-optimality in terms of performance versus efficiency trade-off. Here,\nthe proposed BEExformer gives the Pareto-optimal solution for both comparisons with quantized models as well as\nEE models. This can be attributed to the effectiveness of the proposed BAT approach along with the SLFN-based\ntransformer blocks, which are potent in selectively capturing significant context in sequences. Moreover, the EE\nmechanism solves the \"overthinking\" problem by providing correct inference with minimal processing. This translates\ninto reduced latency during inference."}, {"title": "5.3 Study of Model Ablations", "content": "Table 2 also presents the results from various ablations of the proposed BEExformer. Here, BEExformer (WEE)\nwithout EE is the most lightweight version, while the full precision version, i.e. BEExformer (FP), gives the best"}, {"title": "5.4 Distribution of Exit Points", "content": "For a deeper insight, we plot the exit point distribution along with the number of parameters saved, i.e. not computed\ndue to EE during inference over all the tasks in Figure 4. It appears that the majority of the observations exit after\nthe second transformer block, while negligible samples need to go through the last transformer block. This highlights\nsavings in terms of compute during inference compared to traditional models, where all samples get processed by all the\nlayers in the neural network. The total number of parameters saved from computation during inference is proportional\nto how early the exit condition is satisfied. Its maximum effect is noticed at the second exit and starts decreasing\nthereafter. For subsequent exits, the plots indicate that despite having a higher frequency in some cases, their impact on\nsaving parameters might not be that pronounced. An interesting observation is that none of the samples exit from the\nfirst block during inference. It can be attributed to the substantial reduction in entropy from the initialized value ln(m)\nafter the execution through the first transformer block. It exhibits that each transformer block possesses the potency\nto extract substantial information from the input and make inferences based on it. Furthermore, Table 3 presents the\npercentage reduction in overall FLOPs during inference over all tasks compared to the variant without EE. On average,\nthe EE mechanism in BEExformer saves around 54.85% FLOPs during inference accompanied with 5.98% increase in\nperformance. It affirms our motivation behind EE to utilize fewer transformer blocks than present in the architecture\nduring inference."}, {"title": "5.5 Effect of EE Threshold", "content": "We observe that the EE threshold (8) plays a pivotal role in determining the trade-off between efficacy of results and\nreduction in FLOPs during inference. Figure 5 shows comparison of performance of the value of 8 proposed in this\npaper, i.e. 0.0001 with \u03b4 = 0.01. Although larger & value reduce the inference time with 4.59% average reduction"}, {"title": "6 Conclusion", "content": "In this paper, BEExformer\u2014a binarized transformer architecture with selective learning and EE for textual content-has\nbeen proposed. It incorporates a differentiable piecewise approximation to binarization during the training process\nto ensure gradient computation takes into account both the magnitude and the sign of real-valued weights to update\nthe final binarized weights. This enables a manifold reduction in memory requirements with performance at par to\nfull-precision models, enabling deployment on resource-constrained edge devices. Furthermore, it has fast inferencing\ncapability, allowing it to exit from intermediate transformer blocks based on an intuitive technique monitoring entropy\nchanges in logits. This reduces the overall FLOPs during inference, translating into reduced latency. Besides, the EE\noffers a solution to the \"overthinking\" problem intrinsic to LLMs. Extensive evaluation on a range of tasks in the GLUE\ndata-set, showcases its ability to deliver pareto-optimal results concerning both efficacy as well as efficiency. However,\nthe proposed architecture has certain limitations too. The current version is based on the transformer encoder and is\nlimited to inferencing tasks only. Additionally, being a dynamic architecture, it is difficult to predict the inference time\nand power consumption once the model is deployed. In the future, we aim to overcome these issues by exploring how to\nmodify the proposed BEExformer architecture to accomplish generative tasks. Furthermore, a procedure can be devised\nto accurately predict the power consumption and time taken for inference for a given input."}]}