{"title": "LLM4EFFI: Leveraging Large Language Models to Enhance\nCode Efficiency and Correctness", "authors": ["Tong Ye", "Weigang Huang", "Xuhong Zhang", "Tengfei Ma", "Peiyu Liu", "Jianwei Yin", "Wenhai Wang"], "abstract": "Large Language Models (LLMs), particularly\nCode LLMs, have demonstrated impressive per-\nformance in code generation. Current research\nprimarily focuses on the correctness of gen-\nerated code, while efficiency remains less ex-\nplored. Recent works have focused on modify-\ning the initial version of the code to improve\nits efficiency. However, such refinements are\nlimited by the algorithmic design and overall\nlogic of the initial code, resulting in only in-\ncremental improvements. In contrast, when hu-\nman developers write high-quality code, they\ntypically begin by designing several potential\nsolutions at the logical level, evaluating vari-\nous algorithms and their complexities, and then\nproceeding to implement and optimize the so-\nlution. In this study, we introduce LLM4EFFI:\nLarge Language Model for Code Efficiency, a\nnovel framework that enables LLMs to gen-\nerate code that balances both efficiency and\ncorrectness. Specifically, LLM4EFFI divides\nthe efficiency optimization process into two\ndomains: algorithmic exploration in the logic\ndomain and implementation optimization in the\ncode domain. The correctness of the code is\nthen guaranteed through a synthetic test case\nrefinement process. This approach, which pri-\noritizes efficiency before ensuring correctness,\noffers a new paradigm for efficient code genera-\ntion. Experiments demonstrate that LLM4EFFI\nconsistently improves both efficiency and cor-\nrectness, achieving new state-of-the-art perfor-\nmance in code efficiency benchmarks across\nvarious LLM backbones.", "sections": [{"title": "1 Introduction.", "content": "Large Language Models (LLMs), particularly those\nspecialized in code, are revolutionizing the field of\nsoftware engineering at an unprecedented pace. A\nsignificant area of advancement lies in automated\ncode generation."}, {"title": "2 Related Works", "content": "Large language models have been widely applied to\ncoding tasks and have shown strong performance\nacross various coding scenarios and evaluations.\nMost existing research focuses on code generation,\nwith numerous techniques developed to enhance its\nquality. Some methods aim to improve the quality\nof synthetic code data, enhance self-consistency or leverage\nfeedback from human or LLM annotations.\nOther approaches utilize multi-agent collaboration\nframeworks to enhance code generation.\nHowever,\nthese methods primarily focus on the correctness of\nthe generated code, with relatively little emphasis\non the efficiency of the generated code."}, {"title": "2.2 Code Efficiency.", "content": "Until recently, the academic community has only\nbegun to pay significant attention to the efficiency\nof generated code. Recently, several efficiency-\nfocused benchmarks have\nemerged, aiming to provide a more comprehen-\nsive evaluation of LLMs' ability to generate effi-\ncient code. However, empirical studies and evalua-\ntions of these benchmarks show that current LLMS\nstill face significant challenges in generating ef-\nficient code. To improve code efficiency, recent\nresearch such as ECCO adopts self-refinement, prompting LLMs to con-\nsider possible optimization strategies and refine\ntheir outputs. Effi-Learner proposes a self-optimization framework that uses\nexecution overhead profiles, feeding them back into\nthe LLM to revise the code and reduce time over-\nhead. However, these methods focus on enhancing\nthe efficiency of code after it has been generated,\nrather than starting with the goal of generating both\nefficient and correct code from the beginning."}, {"title": "3 Methodology", "content": "In the code efficiency\ntask, each sample is represented as a pair $(Q, Th)$,\nwhere Q denotes the task description, and Th corre-\nsponds to the hidden test cases. Our goal is to gen-\nerate the corresponding code solution S that passes"}, {"title": "3.1 Overview.", "content": "We present the framework of LLM4EFFI in Fig-\nure 2. For a given programming task described in\nnatural language, LLM4EFFI first \"formalizes\" it\ninto a code-oriented problem description (Section\n3.2). Next, LLM4EFFI queries the LLM for logic-\ndomain reasoning and exploration, generating mul-\ntiple optimal algorithmic solutions along with their\ncorresponding pseudocode (Section 3.3). Based on\nthese algorithm designs and their associated pseu-\ndocode, LLM4EFFI analyzes and generates code\nimplementation suggestions, followed by the gen-\neration and optimization of the corresponding code\nat the implementation level (Section 3.4). To fur-\nther refine the solutions for correctness, LLM4EFFI\nsynthesizes a large number of test cases and utilizes\na bidirectional verification-based adaptive testing\nframework to \"check\" these synthetic test cases.\nThe \"checked\" test cases are then used to evaluate\nthe candidate code solutions (Section 3.5). The so-\nlution with highest pass rate across the \"checked\"\ntest cases is selected as the final generated code."}, {"title": "3.2 Task Formalization.", "content": "In the initial task formalization stage, LLM4EFFI\nensures that the task description is clear and un-\nambiguous, which is crucial for the success of\nsubsequent stages. As highlighted by , errors in LLM-generated code often arise\nfrom an insufficient or unclear understanding of\nthe task. Therefore, LLM4EFFI prompts the LLM\nto comprehend the task from four key dimensions:\nentry point function name, input/output conditions\nand parameter types, edge cases, and expected\nbehavior. Based on these dimensions, the LLM\nis further encouraged to engage in self-reflection to\nconfirm whether it has fully grasped all aspects of\nthe task, thus laying a solid foundation for the sub-\nsequent stages. Formally, $Q \\rightarrow Q_{formal} \\leftarrow Q_{check}$."}, {"title": "3.3 Algorithmic Exploration in Logic Domain.", "content": "For the formalized task defined in the first stage,\nLLM4EFFI prompts the LLM to engage in algo-\nrithmic reasoning at the logical level, rather than\nimmediately generating code. This approach mir-\nrors that of human programmers, who first per-\nform abstract and high-level reasoning before im-\nplementation. The LLM is prompted to explore\nmultiple potential optimal algorithms, analyze their\ncorresponding complexities, and represent the en-\ntire logical process with pseudocode. Formally,\n$Q_{formal} \\rightarrow {Algo, Cplx, Pseudo}$, where Algo\nrefers to the algorithm plan, Cplx refers to the\ncomplexity analysis, and Pseudo refers to the cor-\nresponding pseudocode."}, {"title": "3.4 Implementation Optimization in Code\nDomain.", "content": "Excellent code not only requires careful algorithm\ndesign but also necessitates optimization at the\nimplementation level. Even when the same\nalgorithm is used, different implementation\napproaches can lead to significant variations in\ncode efficiency. When implementing code based on\nthe algorithm plan and corresponding pseudocode,\nLLM4EFFI prompts the LLM to provide practical\nsuggestions derived from Algo and Pseudo,\nsuch as replacing a manual binary exponen-\ntiation implementation with Python's built-in\npow function, among other optimizations. We\nprovide three detailed examples in the appendix to\nillustrate this process. Subsequently, LLM4EFFI\ngenerates the corresponding code based on the\nAlgo, Pseudo, and implementation suggestions,\nwhile also checking for further optimization\nopportunities. Formally, $Algo, Pseudo} \\rightarrow\n{Suggs}$, and $Algo, Pseudo, Suggs}\n\\rightarrow {Code Candidates}$."}, {"title": "3.5 Code Correctness.", "content": "To ensure the functional correctness of generated\ncode while targeting efficiency, LLM4EFFI intro-\nduces a bidirectional verification-based adaptive\ntesting framework. The process works as follows:\nFirst, LLM4EFFI automatically synthesizes a large\nnumber of test cases based on the formalized task\ndescription $Qformal$. These test cases are designed\nto cover a wide range of edge cases, thoroughly test-\ning the robustness and reliability of the generated\ncode. However, since the synthesized test cases\nmay not be entirely correct, LLM4EFFI performs\nbidirectional verification to validate them.\nForward Verification: If all candidate code imple-\nmentations pass a specific test case, the test case\nis marked as trusted. Otherwise, Reverse Review:"}, {"title": "4 Experiments", "content": "We evaluate LLM4EFFI on three code efficiency\nevaluation benchmarks: EvalPerf (Liu et al., 2024),\nMercury (Du et al., 2024) and ENAMEL (Qiu\net al., 2024). EvalPerf focuses on performance-\nchallenging tasks and uses Differential Perfor-\nmance Evaluation to assess efficiency across dif-\nferent LLMs and solutions. Its efficiency metric,\nDPS_norm, is calculated by determining the cumu-"}, {"title": "4.1 Compared Methods.", "content": "We evaluate the direct instruction of generating\ncorrect and efficient code as the Instruct baseline.\nWe compare LLM4EFFI with two recent proposed"}, {"title": "4.2 Experiment Setup.", "content": "To comprehensively evaluate LLM4EFFI, we se-\nlected five different LLM backbones: two propri-\netary models, GPT-40 and GPT-\n40-mini, and three open-source models, including\nDeepSeek-V3 , Qwen2.5-\n72B-Instruct , and Qwen2.5-\nCoder-32B-Instruct . During the\nLLM4EFFI process, we set the number of algorithm\nplans to 5 and the number of synthetic test cases\nto 20, followed by one iteration to refine the code\nfor correctness. All prompts used in LLM4EFFI are\ndetailed in Appendix A. To ensure consistency and\na fair comparison, all experiments were conducted\nwith the temperature set to 0, and each experiment\nwas repeated three times to compute an average,\nthereby eliminating any potential disruptions."}, {"title": "4.3 Main Results.", "content": "We compare LLM4EFFI with the other methods on\nthe EvalPerf, Mercury, and ENAMEL benchmarks,\nand present the results in Table 1. First, we observe\nthat direct instruction prompts yield good perfor-\nmance, indicating that LLMs have a reasonable\nunderstanding of correct and efficient code. Then,\nthrough ECCO, we observe a slight improvement\nin efficiency on EvalPerf and Mercury. However,\nthis improvement often comes at the cost of cor-\nrectness. Particularly in more complex benchmarks\nlike ENAMEL, the approach results in a decline in\nboth efficiency and correctness. This suggests that\nrelying solely on code understanding to generate\noptimization suggestions is insufficient. When op-\ntimization strategies are based purely on code-level\nanalysis, they often fail to align with the broader\nlogical requirements of the task. The mismatch\nbetween the code domain and the logic strategy\ndomain makes such methods less effective.\nMoreover, Effi-Learner shows some gains in ef-\nficiency and correctness on specific benchmarks,\nsuch as when using GPT-40 on the Mercury bench-\nmark. However, its performance varies signifi-\ncantly across different LLM backbones and bench-\nmarks, often falling short of the direct Instruct\nbaseline. More importantly, Effi-Learner faces a\nrecurring issue: both efficiency and correctness suf-\nfer simultaneously. This stems from its feedback\nmechanism, which focuses solely on performance\nmetrics like execution time, neglecting the code's\nfunctionality and correctness. Additionally, the\nlack of a comprehensive algorithmic strategy leads\nto an over-prioritization of execution time, often\nsacrificing code accuracy and resulting in a decline\nin both efficiency and correctness."}, {"title": "4.4 Ablation Study.", "content": "LLM4EFFI incorporates several unique design\nchoices, such as separating efficiency optimization\ninto the logic domain and code implementation\nlevel. To better understand the impact of each com-\nponent, we conduct the following ablation study:\n\\bullet Variant-1: (Without Algorithmic Exploration\nin the Logic Domain): In this variant, no algo-\nrithmic exploration is performed for efficiency\noptimization in the logic domain. Instead, the\nLLM directly generates the same count efficient\ncode solution, followed by implementation-level\noptimization (based on the formalized task and\nthe generated code solution). All other steps re-\nmain the same as in LLM4EFFI.\n\\bullet Variant-2: (Without Implementation Optimiza-\ntion in the Code Domain): This variant omits\nthe implementation optimization step in the code\ndomain, while all other processes are identical to\nthose in LLM4EFFI.\n\\bullet Variant-3: (Without Code Correctness Refine-\nment): In this variant, after generating the\nefficiency-optimized code solutions, the LLM\nindependently selects the most efficient and cor-"}, {"title": "5 Deeper Analysis", "content": "As mentioned in the Introduction, LLM4EFFI has\ntwo distinct features: Uniqueness 1: Separation\nof Efficiency Optimization into Logic and Code\nDomains, and Uniqueness 2: The Order of Cor-\nrectness and Efficiency. To gain a deeper under-\nstanding of these unique advantages, we conducted\nthe following comparative experiments:\n\\bullet w/o Uniqueness-1: Rather than separating effi-\nciency optimization into the logic and code do-\nmains, we prompt the LLM to generate code that\nis both efficient and correct. Then, based on\nthe formalized task and the generated code, the\nLLM is queried to suggest any possible strate-\ngies for optimizing efficiency. Subsequently, we\noptimize the generated code according to these\nstrategies, with the following steps remaining the\nsame as in LLM4EFFI. It is important to note that"}, {"title": "5.2 Performance of Different Difficulty Levels.", "content": "To evaluate the performance of LLM4EFFI on\ntasks of varying difficulty levels, we conducted\nan analysis across three levels\u2014Easy, Medium,\nand Hard using Mercury with DeepSeek-V3 as\nthe backbone. LLM4EFFI consistently achieves the highest\nBeyond@1 metrics, outperforming other methods\nacross tasks ranging from easy to difficult. This ro-"}, {"title": "5.3 Case Study.", "content": "To provide a more intuitive demonstration of\nLLM4EFFI, we conducted a case study, with the\nprocess detailed in Appendix B. It can be observed\nthat methods like ECCO and Effi-Learner, which\ngenerate code first and then optimize for efficiency,\nare constrained by the algorithmic design and over-\nall structure of the initial code, leading to only\nincremental improvements. In contrast, LLM4EFFI\nbreaks free from these constraints, enabling it to\nfully explore more efficient algorithms at a high\nlevel based on the task, while also incorporating\npractical level efficiency optimizations, thus achiev-\ning more effective efficiency optimization."}, {"title": "6 Conclusion", "content": "In this paper, we presented LLM4EFFI, a novel\nframework designed to generate both efficient\nand correct code. By separating efficiency op-\ntimization into the logic and code domains and\nadopting an \"efficiency-first, correctness-later\" ap-\nproach, LLM4EFFI enables the exploration of a\nbroader range of algorithmic solutions while main-\ntaining functional correctness. Experimental re-\nsults demonstrate LLM4EFFI 's robust performance,\nwith consistent improvements in both efficiency\nand correctness across different LLM backbones."}, {"title": "Limitation", "content": "Although LLM4EFFI excels at generating both effi-\ncient and correct code, it also has some limitations.\nOne major challenge is the trade-off between ef-\nficiency and maintainability. In some cases, the\ngenerated efficient code may become more com-\nplex and harder to read. Achieving the right bal-\nance between efficiency and maintainability is not\nalways straightforward, and in certain cases, highly\nefficient code may sacrifice readability and ease of\nfuture modifications. Future work will focus on\noptimizing the LLM4EFFI to improve its scalabil-\nity and extend its applicability to more complex\nsoftware engineering tasks."}, {"title": "A Appendix of Prompts.", "content": ""}, {"title": "A.1 Prompts of LLM4EFFI.", "content": ""}, {"title": "Task Formalization", "content": "System:\nAs a professional algorithm engineer, please analyze\nthe given algorithm problem according to the follow-\ning categories. Do not provide any example imple-\nmentation:\n\\bullet Entry Point Function Name\n\\bullet Input/Output Conditions\n\\bullet Edge Cases and Parameter Types (Int, String,\netc.)\n\\bullet Expected Behavior\nUser:\nThe algorithm problem description is as follows:\n<natural language description>"}, {"title": "Task Formalization Check", "content": "System:\nAs an excellent algorithm engineer, please analyze\nwhether the explanation of the problem matches the\noriginal requirements. If they are consistent, output\n\"Yes\". If they are not consistent, output \"No\" and\nprovide the reason, as shown below: {\"Yes\":\"NULL\"}\n{\"No\":\"The reason is\"}\nUser:\n<natural language description>\n<task description>"}, {"title": "Synthesize Test Case Inputs", "content": "System:\nAs a tester, your task is to create comprehensive test\ninputs for the function based on its definition and doc-\nstring. These inputs should focus on edge scenarios\nto ensure the code's robustness and reliability. Please\noutput all test cases in a single line, starting with in-\nput.\nUser:\nEXAMPLES:\nFunction:\nfrom typing import *\ndef find_the_median(arr: List[int]) ->\nfloat:\nGiven an unsorted array of\nintegers `arr`, find the\nmedian of the array.\nThe median is the middle value in\nan ordered list of numbers.\nIf the length of the array is\neven, then the median is the\naverage of the two middle\nnumbers.\nTest Inputs (OUTPUT format):\ninput: [1]\ninput: [-1, -2, -3, 4, 5]\ninput: [4, 4, 4]\ninput: [....]\ninput: [....]\nEND OF EXAMPLES.\nFunction:\n<task description>"}, {"title": "Implementation Optimization in Code\nDomain", "content": "System:\nAs a professional Python algorithm programming ex-\npert, please provide suggestions for improving code\nefficiency based on the potential inefficiencies men-\ntioned above. For example:\n1. Using xxx instead of xxx can significantly improve\ncode efficiency.\nPlease provide at least 20 suggestions.\nUser:\n<algorithm description>"}, {"title": "Complete Test Case Generation", "content": "System:\nAs a programmer, your task is to calculate all test out-\nputs and write the test case statement corresponding\nto the test input for the function, given its definition\nand docstring. Write one test case as a single-line\nassert statement.\nUser:\nEXAMPLES:\nFunction:\nfrom typing import List\ndef find_the_median(arr: List[int]) ->\nfloat:\nGiven an unsorted array of\nintegers `arr`, find the\nmedian of the array. The\nmedian is the middle value in\nan ordered list of numbers.\nIf the length of the array is\neven, then the median is the\naverage of the two middle\nnumbers.\nTest Input:\ninput: [1, 3, 2, 5]\nTest Case:\nassert find_the_median([1, 3, 2, 5]) == 2.5\nEND OF EXAMPLES.\nFUNCTION:\n<task description> <input case>"}, {"title": "Algorithmic Exploration in Logic Domain", "content": "System:\nAs a professional algorithm engineer, you can effec-\ntively design multiple algorithms to solve the problem\nwith low time complexity and output them in pseudo\nalgorithm format. A pseudo algorithm is a nonlin-\near, high-level programming language for algorithmic\nlogic. It combines natural language and programming\nstructures to express the steps and sums of algorithms.\nThe main purpose of process algorithms is to clearly\ndisplay the core ideas and logic of the algorithm with-\nout relying on specific programming language syntax.\nPlease design 5 excellent algorithm solutions based\non the problem description provided. The time com-\nplexity of the algorithm needs to be as small as pos-\nsible, and try to output 5 algorithms in the form of a\npseudo-algorithm in the following format: PS: DO\nNOT provide implementation examples!\nalgorithm1\n{algorithm key description: this\nalgorithm using xxx, the key is to\nmake sure xxx}\n{pseudo algorithm: ..}\n{algorithm key description: this\nalgorithm using xxx, the key is to\nmake sure xxx}\n{pseudo algorithm: ..}\n{algorithm key description: this\nalgorithm using xxx, the key is to\nmake sure xxx}\n{pseudo algorithm: ..}\n{algorithm key description: this\nalgorithm using xxx, the key is to\nmake sure xxx}\n{pseudo algorithm: ..}\n{algorithm key description: this\nalgorithm using xxx, the key is to\nmake sure xxx}\n{pseudo algorithm: ..}\nUser:\n<task description>"}, {"title": "Code Candidates Generation", "content": "System:\nAs a professional algorithm engineer, please convert\nthe selected algorithm into corresponding code. En-\nsure the code is complete and well-formatted. When\nconverting to a standardized format, be sure to follow\nthe guidelines specified in the \u201coriginal question for-\nmat\":\n1. Use the same function name as given in the original\nquestion format; do not rename it.\n2. You may incorporate practical optimization details\ndrawn from the knowledge base.\nThe final output format should be as follows:\n```python\n{<code>}\nUser:\n<task description>\n<algorithm description>\n<efficiency optimization suggestions>"}, {"title": "Code Refinement for Correctness", "content": "System:\nAs a professional code programming algorithm expert,\nyour task is to correct the code and ensure that the\ncode is fixed without impacting its time complexity\nor practical efficiency. Then I will provide you with\nspecific code and test cases.\nImportant Notes:\n1. Do not alter the algorithm itself\n2. Do not change the format, such as the function\nname.\n3. Please output in the specified format.\n4. Ensure there are no syntax errors.\nPlease output in this format:\n```python\n{code}\nUser:\n<task description>\n<algorithm description>\n<efficiency optimization suggestions>"}, {"title": "Final Results Selection on Code Candi-\ndates", "content": "System:\nAs a professional algorithm engineer, please help me\nchoose the most efficient code from the following\ncodes. It is worth mentioning that it is necessary\nto consider the time complexity and practical level\ncomprehensively:\nINPUT:\n{\"1\":\"def ...().....\",\n\"2\": \"def ...()...\"}\nOUTPUT:\n```text\n{key}\nEXAMPLE:\nINPUT:\n{ \"1\":\"def ...().....\",\n\"2\": \"def ...()...\"}\nOUTPUT:\n```text\n1"}, {"title": "Direct Code Generation Prompt for\nVariant-1", "content": "System:\nAs a professional Python algorithm engineer, please\nsolve the algorithms problem and generate a solution\ncode. The final output format should be as follows:\n```python\n{code}\nUser:\n<task description>"}, {"title": "Direct Code Generation Prompt for w/o\nUniqueness-1&w/o Uniqueness-2", "content": "System:\nAs a professional Python algorithm engineer, please\nsolve the algorithm problem and generate 5 solution\ncodes. Please improve the efficiency of the code as\nmuch as possible while ensuring the correctness of the\ncode. The final output format should be as follows:\n```python1\n{code}\npython2\n{code}\n```python3\n{code}\n```python4\n{code}\n```python5\n{code}\nUser:\n<task description>"}, {"title": "Efficiency Optimization Prompt in Effi-\nLearner.", "content": "Optimize the efficiency of the following Python code\nbased on the task, test case, and overhead analysis\nprovided. Ensure the optimized code can pass the\ngiven test case.\nTask Description:\n<task description>\nTest Case:\n<test case>\nOriginal Code:\n```python\n<original code>\nOverhead Analysis:\n<profile of original code>\nOptimization Rules:\nEncapsulate the optimized code within a Python\ncode block (i.e., python [Your Code Here]).\nDo not include the test case within the code block.\nFocus solely on code optimization; test cases are\nalready provided.\nEnsure the provided test case passes with your\noptimized solution."}, {"title": "Original Code Generation Prompt in Effi-\nLearner", "content": "Please complete Python code based on the task de-\nscription.\n# Task description:<Task description>\n#Solution:"}, {"title": "Original Code Generation Prompt in\nECCO", "content": "Write a python code which is efficient in terms of\nruntime and memory usage for the following problem\ndescription. Wrap the optimized code in a block of 3\nbackticks"}, {"title": "Feedback Generation Prompt in ECCO", "content": "Give feedback in english for why the code solution\nbelow is incorrect or inefficient and how the program\ncan be fixed based on the problem description.\n<original code>"}, {"title": "Refine Prompt in ECCO", "content": "Refine the given incorrect or sub-optimal code\nsolution based on the feedback specified below. Wrap\nthe refined code in a block of 3 backticks\n<optimization suggestion>\n<original code>"}, {"title": "A.4 Prompts of Instruct.", "content": ""}, {"title": "Prompt for Instruction Baseline", "content": "Please generate an efficient and correct code directly"}, {"title": "B Case Study.", "content": ""}, {"title": "B.1 The Execution Details of Each Process of\nLLM4EFFI.", "content": "As shown in Figure 21, LLM4EFFI firstly analyzes\nthe algorithm problem, \"returns the n-th number\nthat is both a Fibonacci number and a prime num-\nber\", providing a detailed explanation of key as-\npects, including the entry point, expected behavior,\nand edge cases. Based on this analysis and the prob-\nlem description, LLM4EFFI explores potential algo-\nrithms and generates five efficient solutions, such\nas using the Fibonacci sequence generation method\nand Binet's formula. Next, LLM4EFFI examines\nthe implementation details of these algorithms and\nidentifies the optimal practical approaches. For\nexample, it uses Python's built-in pow() function\nfor efficient exponentiation and applies the Miller-\nRabin primality test (based on the Monte Carlo\nmethod) to enhance the efficiency of prime number\ndetection for large numbers.\nThen, LLM4EFFI combines the explored algo-\nrithms and practical operations to generate five dis-\ntinct code implementations. To validate the cor-\nrectness of these codes, LLM4EFFI generates 20\ntest cases based on the algorithm description and\noutputs them in the format \"assert prime_fib(3) ==\n5\". Each code is then executed with these 20 test\ncases, recording the number of passed test cases\n$(Pass_t \\le 20)$ and the number of successful execu-\ntions for each test case $(Pass_c \\le 5)$. Subsequently,\nLLM4EFFI checks the test cases that are not passed\nby the code implementations, ensuring that correct\ntest cases are not excluded due to code errors and\npreventing incorrect test cases from being misused\nin subsequent iterations.\nAfter filtering, LLM4EFFI obtains a new batch\nof test cases and executes them again to gather new\nresults. For the failed test cases, an iterative feed-\nback mechanism is applied to optimize the code.\nThen, the code, enhanced with the iterative feed-\nback, is executed once more, and the final passing\nresults are recorded. All codes are then ranked in\ndescending order based on their correctness, and\nthe most accurate code is selected.\nThis process ensures the identification of the\nmost optimal solution while maintaining both high\nefficiency and accuracy in code implementation."}, {"title": "B.2 Comparison of Methods.", "content": "In Figures 22 and Figures 23, we compare the code\nefficiency optimization processes of the three tools."}]}