{"title": "Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data", "authors": ["David Heurtel-Depeiges", "Anian Ruoss", "Joel Veness", "Tim Genewein"], "abstract": "Foundation models have recently been shown to be strong data compressors. However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms. Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression. In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible. To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG 2000, FLAC) even when factoring in parameter count. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.", "sections": [{"title": "1. Introduction", "content": "Strong predictive models can straightforwardly be turned into strong lossless compressors, e.g., via arithmetic coding (Pasco, 1977; Rissanen, 1976; Witten et al., 1987). Consequently, large pre-trained foundation models, such as LLMs, achieve very high data compression on their training distributions and beyond (Del\u00e9tang et al., 2024). However, when factoring in these models' parameter count into the compression ratio, too large models actually perform worse. For this reason, large foundation models with parameter counts on the order of billions cannot compete with standard compression algorithms such as gzip (Deutsch, 1996) or LZMA2 (Pavlov, 2019). The goal of this paper is thus to investigate whether pre-trained vanilla transformers can achieve compression ratios that are competitive with standard algorithms across a range of data modalities. This places fairly tight constraints on the maximal model size, leading us to investigate families of relatively small transformers (with millions of parameters). Note that our aim is not to build a practical transformer-based data compressor, as the computational footprint (running time, memory, FLOPs) of even small models is far beyond standard compressors. Instead, studying compression via pre-trained models provides insight into the models' learned inductive biases, e.g., whether they are domain-general, how they depend on the training data composition, and whether there is transfer between modalities.\nRecently, Del\u00e9tang et al. (2024) stated that \u201clanguage modeling is compression\", pointing out that log-loss minimization is equivalent to optimizing a lossless compression objective. To illustrate this point, the authors used billion-parameter LLMs that were trained exclusively on text (Llama 2 from Touvron et al. (2023b) and Chinchilla from Hoffmann et al. (2022)) to compress 1GB of image and audio data from ImageNet (Russakovsky et al., 2015) and LibriSpeech (Panayotov et al., 2015), respectively. They found that these models compress better than gzip or LZMA2 and even"}, {"title": "Main Contributions", "content": "We make the following key contributions:\n\u2022 We conduct a large-scale empirical study (hyperparameter sweeps, ablations) on the compression performance of small transformers trained on raw byte sequences of text, image, and audio"}, {"title": "2. Background", "content": "Compression and prediction are \u201ctwo sides of the same coin\" (MacKay, 2003). This fundamental duality stems directly from Shannon's celebrated lossless source coding theorem (Shannon, 1948), which states that there is a well-defined lower bound for encoding data from a probabilistic source. For any data sequence $x_{1:n} := x_1x_2 ... x_n \\in X^n$ of length n from a finite alphabet X sampled from a source p : X* \u2194 (0, 1], a lossless compressor c : X* \u2192 {0, 1}* assigns a code c(x1:n), i.e., a sequence of bits, from which the original sequence is recoverable without loss of information. The goal is to minimize the expected length: $L_p := E_{x~p}[l_c(x)]$ by encoding rare sequences with more bits and frequent sequences with fewer bits. Shannon's source coding theorem states that the minimal expected length is lower-bounded by the Shannon entropy of the source: $L_p \\geq H(p) := [E_{x~p}[-log_2 p(x)]$.\nIf the source's statistics are unknown, good compression becomes a statistical modeling problem, i.e., good compression relies entirely on being able to predict well sequentially. For any predictor \u03c0 : X* \u2194 (0, 1] the expected coding length $L_\u03c0$ for data drawn from p is at least the cross entropy:\n$L_{\\pi}^{\\rho} \\geq E_{x \\sim \\rho}[-log_2 \\pi(x)] = E_{x \\sim \\rho} \\Big[ -log_2 \\frac{\\pi(x) \\rho(x)}{\\rho(x)} \\Big] = H(\\rho) + D_{KL}(\\rho||\\pi) \\geq H(\\rho)$,\nwhich is also lower-bounded by the Shannon entropy of p. A mismatch between \u03c0 and p thus leads to an excess length given by their KL divergence, and minimal coding length (maximal compression) implies \u03c0 = p across the whole support of p. Accordingly, some AI researchers have argued that compressing well is fundamentally connected to intelligence (e.g., Chaitin's famous \u201cCompression is Comprehension\" (Chaitin, 2006); Grau-Moya et al. (2024); Rathmanner and Hutter (2011)), and that building universal compressors will accelerate AI development (cf. the Hutter prize (Hutter, 2006), an ongoing competition to compress (1GB of) human knowledge). The duality between compression and prediction has also led to the (algorithmic) information-theoretic formulation of universal prediction, i.e., Solomonoff induction (Li and Vit\u00e1nyi, 2019; Solomonoff, 1964a,b), one of two key ingredients for AIXI (Hutter et al., 2024; Legg and Hutter, 2007), the theory of artificial superintelligence.\nConsequently, Del\u00e9tang et al. (2024) argue that lossless compression performance lends itself as a domain-general metric for assessing any predictor's quality, including foundation models. They further emphasize that foundation models trained by minimizing log-loss (a.k.a., next-token prediction-error or cross entropy loss) are explicitly trained to minimize the expected coding length:\n$\\min_{\\pi} L_{\\pi} = \\min_{\\pi} E_{x\\sim p}[- log_2 \\pi(x)] = \\min_{\\pi} E_{x\\sim p} \\Big[ \\sum_i - log_2 \\pi(x_i | x_{<i}) \\Big]$\\tag{1}\n\"log loss\"\nNote that the problem of constructing the actual codes that achieve (near) minimal expected code length given a predictor is largely solved in information theory, with gold-standard algorithms such"}, {"title": "3. Related Work", "content": "Compression Without Transformers Using neural networks as predictors for lossless compression has been extensively studied, both in conjunction with arithmetic coding (Cox, 2016; Goyal et al., 2019; Knoll, 2014; Lehtokangas et al., 1993; Liu et al., 2019; Mahoney, 2000; Mentzer et al., 2019, 2020; Mikolov, 2012; Rhee et al., 2022; Schiopu and Munteanu, 2020; Schiopu et al., 2018; Schmidhuber and Heil, 1994, 1996; van den Oord and Schrauwen, 2014) and with asymmetric numeral systems (Barzen et al., 2022; Hoogeboom et al., 2019; Kingma et al., 2019; Townsend et al., 2019). Lossy neural compression has been achieved, e.g., by overfitting tiny networks to individual data points and transmitting the model weights rather than the original data (Chen et al., 2021; Dupont et al., 2021, 2022; Kim et al., 2023; Ladune et al., 2023).\nOnline Transformers Most of the above approaches use a separate training set to pre-train models that are then used to compress a test set. Alternatively, the model can also be trained from scratch on the data stream that is being compressed (Bellard, 2019, 2021; Goyal et al., 2020; Mao et al., 2022). The main advantage of these adaptive online compressors is that they are (quasi) parameterless (since they are initialized from scratch when compressing a new stream of data), meaning that the model size does not explicitly affect the compression ratio, even for relatively large models (though it implicitly affects the training performance, e.g., large models train more slowly meaning that larger chunks of the initial data stream are only weakly compressed). The transformer-based adaptive online compressor of Bellard (2021) is currently state-of-the-art on the Large Text Compression Benchmark (Mahoney, 2006), and our evaluation (in Section 5) shows that our best models are on par across all modalities.\nPre-Trained Transformers Most closely related to our work is the line of research by Del\u00e9tang et al. (2024); Huang et al. (2024); Li et al. (2024); Mittu et al. (2024); Valmeekam et al. (2023), which investigates lossless compression via arithmetic coding with pre-trained foundation models, i.e., the Llama models (Dubey et al., 2024; Touvron et al., 2023a,b) and Chinchilla (Hoffmann et al., 2022). Del\u00e9tang et al. (2024), in particular, also report good compression rates on unseen modalities (LLMs trained only on text compress images and audio data well). However, these studies differ from our work as they do not take the model size into account for the compression ratios, except for Del\u00e9tang et al. (2024), who report both \u201craw\u201d and \u201cadjusted\u201d compression ratios and find that LLMs are not competitive in terms of adjusted (i.e., the actual) compression ratios. To the best of our knowledge, our paper is the first to systematically investigate the use of appropriately sized pre-trained"}, {"title": "4. Methods", "content": "We now describe our experimental setup (with additional details, e.g., sweeps, in Appendix A).\nBaselines We compare to various standard compressors, both general-purpose, i.e., gzip (Deutsch, 1996) and LZMA2 (Pavlov, 2019), and domain-specific, i.e., FLAC (Coalson, 2008) for audio data and PNG (Boutell, 1997) and lossless JPEG 2000 (Skodras et al., 2001) for images. Both gzip and LZMA2 (which is used by the 7zip software) are based on Huffman coding (Huffman, 1952) and the Lempel-Ziv-Welch algorithm (Welch, 1984). We use the default parameters for gzip, LZMA2, and JPEG 2000, compression level 12 for FLAC, and instruct PNG to find the optimal encoder settings. We also compare to the online transformer from Bellard (2021), with the default v3.3 parameters, which is the current state-of-the-art on the Large Text Compression Benchmark (Mahoney, 2006).\nModels We focus on decoder-only transformers (Vaswani et al., 2017) with SwiGLU activations (Shazeer, 2020) and post-layer normalization. Unless otherwise noted, we use 8 heads, an embedding dimension of 64, a context size of 4096 (bytes), and sliding windows without overlap or memory (full details in Appendix A.3). We train our models with the Adam optimizer (Kingma and Ba, 2015) for 2.5 million steps with a batch size of 32, which, for 165GB of data, roughly corresponds to 2 epochs. Due to the duality of compression and prediction, we minimize the standard (sequential) log-loss (Eq. (1)) during training, which is a maximum-compression objective (see Section 2).\n(No) Tokenization Tokenization is a commonly-used, domain-specific pre-compression step to boost transformers' performance by increasing their vocabulary size in order to fit more information into their limited context window (Lester et al., 2024), i.e., increased information density at the cost of increased entropy. However, since our goal is to be domain-general, we do not use tokenization and instead feed our models directly with byte streams (we still have to choose how to flatten images and how to sample audio signals, which are minimal domain-specific preprocessing steps).\nEvaluation To evaluate performance, we compute the compression ratio (lower is better):\ncompression ratio := $\\frac{\\text{size of compressed data + size of compressor}}{\\text{size of uncompressed data}}$\n\nwhich accounts for the model size and is equivalent to the \u201cadjusted compression rate\" of Del\u00e9tang et al. (2024). We always evaluate on 1GB of out-of-distribution data, i.e., size of uncompressed data = 1GB. As Del\u00e9tang et al. (2024), we compute the size of the compressor by encoding the model weights with float16 (2 bytes per parameter) since this level of quantization does not significantly affect performance (Tao et al., 2022) and is standard for model inference. As a result, our model sizes range from 0.8MB to 40.3MB. Note that, similar to Del\u00e9tang et al. (2024), we do not compress the model parameters, since naive approaches (e.g., compressing them with gzip) do not significantly decrease the model size (only by around 7%, which corresponds to a decrease in compression ratio of only 0.002821 for our largest model). However, as a result, the compression ratio we report is technically an upper bound, which could be improved by (losslessly) compressing the parameters (though with limited room for improvement in our regime, even in the best case)."}, {"title": "5. Results", "content": "In this section, we present our extensive experimental evaluation (additional results in Appendix B). Unless otherwise noted, we report the best results over two hyperparameter sweeps (described in Appendix A.3): (i) over the model- and dataset sizes, and (ii) over the model- and context sizes.\nSmall Transformers Can Be Domain-General Compressors Figure 2 shows the best compression ratio attained on each of the seven out-of-distribution evaluation datasets when training a model on each of the seven training data mixtures (we report the best-performing model from our two sweeps for each training-evaluation pair). We observe that transformers can achieve state-of-the-art in-modality compression ratios, regardless of the concrete composition of the training mixture, outperforming standard compression algorithms (even domain-specific ones) in all cases where all evaluation modalities are part of the training mixture. In these cases, transformers thus learn the prototypical statistical patterns related to that modality during pre-training. Importantly, by comparing models trained on unimodal vs. multimodal data, we observe that multimodal training only slightly decreases the compression performance compared to the unimodal models on their respective modalities (despite only having half or a third amount of data from that modality). This means that it is possible to trade off a small amount of performance on each individual modality to obtain a very strong domain-general compressor via multimodal training (the gray bar in Fig. 2).\nWhat You See Is What You Get While Fig. 2 shows that substituting half or two thirds of the training set with data from other modalities only leads to a small performance loss compared to the unimodally trained models, it is unclear whether simply training on a smaller amount of unimodal data (i.e., decreasing the unimodal training dataset size to, e.g., 82.5GB and not substituting 82.5GB with data from another modality) would give the same performance, or whether there is some transfer between modalities (as suggested by Mirchandani et al. (2023)) that compensates for the smaller amount of data per individual modality. To investigate this, we run an ablation where we subdivide each of our seven training sets into 5 different sizes: 20%, 40%, 60%, 80%, and 100% of the respective dataset"}, {"title": "6. Discussion", "content": "The main goal of our work is to investigate whether pre-trained transformers can be competitive with standard compressors, even when taking their parameter size into account. In contrast to previous work, this places our models into a relatively small regime, where it is unclear whether models will learn well from large datasets at all and have non-trivial out-of-distribution and cross-modality transfer. This could partly be countered by training larger models and then subsequently compressing the model parameters themselves. We chose not to do this in our case since naive lossless compression of model parameters leads to a 10% reduction at best (see Table A3), and even best-case scenarios would only lead to marginal improvements in compression ratio given the size of our largest models. For very large (e.g., foundation) models, compressing weights to achieve competitive compression ratios may be interesting, though it will be necessary to use lossy weight compression techniques (Tao et al., 2022), which lead to non-trivial trade-offs between high (lossy) compression and maintaining strong predictor performance, i.e., the two summands in the numerator of Eq. (2). Exploring these trade-offs"}, {"title": "7. Conclusion", "content": "In this paper we have shown that it is possible to use pre-trained vanilla transformers as competitive \"zero-shot\" compressors on out-of-distribution evaluation data, where competitive means achieving better compression ratios than both domain-general and domain-specific standard compression algorithms. We found this to be true for text, images, and audio data, and for all possible combinations of the three but only as long as the corresponding modalities have been seen during training. We further found that, despite their relatively small size, our models have the capacity to train on multiple modalities, and then compress these well, without losing much performance compared to a purely unimodal model. On the other hand, we found that even multimodal training does not lead to the emergence of a universal compression ability that would yield strong compression performance on unseen modalities. This is in contrast to observations made by Del\u00e9tang et al. (2024) on LLMs and indicates that there is a qualitative difference between small and (very) large models, even when the small models are trained on large amounts of data. Overall our results suggest that small transformers can be pre-trained to recognize and exploit statistical regularities on par and even better than hand-crafted standard compressors and current state-of-the-art adaptive online neural compressors, but we do not observe the emergence of a general compression ability with our model"}, {"title": "A. Experimental Details", "content": "We source all of our data from the following open-source TensorFlow datasets (Pot et al., 2019):\nText Since most of TensorFlow's text datasets are quite small, we concatenate the following five datasets into a single collection of 165GB: (i) Wikipedia (Wikimedia, 2023), the filtered UTF-8 encoded text from an XML dump from 2023-06-01, containing all languages but predominantly English and western languages (113.9GB); (ii) PG-19 (Rae et al., 2020), books from the Project Gutenberg, also encoded in UTF-8 (9.4GB); (iii) Big Patent (Sharma et al., 2019), a dataset of patents in English (30.2GB); (iv) Scientific Papers (Cohan et al., 2018), from arXiv and PubMed, containing the raw text including the LaTeX code (8.1GB); and (v) Natural Instructions (Mishra et al., 2022; Wang et al., 2022), tasks formulated in English covering different domains and lanugages (4.1GB).\nImage We collect a subset of 165GB of the ImageNet dataset (Russakovsky et al., 2015), uniformly sampled across the 1000 classes, which contains 14197 122 annotated images (of varying resolutions) from the WordNet hierarchy. We decode the images into RGB arrays (three uint8 channels), flatten them, and concatenate them into a byte stream of flattened images. As a consequence, we ignore image boundaries when sampling from this data source (i.e., sequences are not guaranteed to start or end at the start or end of an image).\nAudio We create a subset of 165GB from the Common Voice dataset (Ardila et al., 2020), a multilingual dataset of voice recordings. We downsample the dataset from 48 kHz to 16 kHz and encode the waveform as int16, i.e., with two bytes per sample. As for images, we concatenate all individual audio samples into a single byte stream. Accordingly, there is no guarantee that a sequence sampled from our dataset starts or ends at the beginning of a recording."}, {"title": "A.2. Out-of-Distribution Evaluation Data Sources", "content": "We source all of our data from the following open-source TensorFlow datasets (Pot et al., 2019):\nText We consider a 1GB subset of the Reddit dataset (V\u00f6lske et al., 2017), which contains 3.8 million Reddit posts encoded in UTF-8.\nImages We create a 1GB subset of the CelebA HQ dataset (Liu et al., 2015) with a resolution of 512 \u00d7 512. We process the images in the same way as for our image training set, i.e., flattening and concatenation, and we subsample uniformly across classes of CelebA.\nAudio We use 1GB from the LibriSpeech (Panayotov et al., 2015) dataset, which contains roughly 1000 hours of English speech data derived from audiobooks that have been segmented and aligned in the LibriVox project. The data is already in 16kHz (with a sample size of 2 bytes), and we simply concatenate samples into a single byte stream."}, {"title": "A.3. Sweeps", "content": "The experiment to investigate the impact of training dataset- and model size, with results shown in Fig. 4, used the following model parameters. Dataset sizes were 20%, 40%, 60%, 80%, and 100% of the full 165GB for each training set mixture (uni- and multimodal). All models used a context size of 4096, 8 attention heads per layer, a widening factor of 4 and the number of layers was either 2, 4, 6, 8, or 10. Models were trained with a batch size of 32. The learning rate was 1 \u00d7 10\u22124, and a sinusoid positional encoding was used.\nFig. 5 in the main paper shows the relationship between context length and model size. For this experiment we performed a large-scale sweep with the goal of covering a good range of training FLOPS budget with models that make various trade-offs between model size and context length (given the same model size, compute demand increases with increasing context length). The main question was whether there is a qualitatively similar relationship across parameters, and whether there is a clear sweet spot see the main paper for results and discussion. For our sweep we used the same model parameters as in the previous paragraph (the training data size was always at 100%) and sweep over the following four context sizes (with training batch size in brackets): [1024 (128), 2048 (64), 4096 (32), 8192 (16)]. For each context size we train five models (XS, S, M, L, and XL) on all three unimodal datasets, respectively. Each model has a different combination of embedding dimension and number of layers for each different context size. The XS models have embedding dimensions [112, 96, 80, 64] and numbers of layers [11, 7, 5, 3] for the different context sizes respectively (i.e., wider and deeper models for shorter contexts and more narrow and more shallow models for long context size). The S models have embedding dimensions [192, 160, 112, 96] and numbers of layers [10, 8, 6, 4]. The M models have embedding dimensions [224, 192, 144, 112] and numbers of layers [12, 9, 7, 5]. The L models have embedding dimensions [272, 240, 176, 144] and numbers of layers [13, 10, 8, 5]. The XL models have embedding dimensions [320, 304, 240, 160] and numbers of layers [12, 9, 7, 6]. The main goal with these settings is to create families of models that have roughly the same demand in terms of FLOPS (iso-FLOPS) but very different trade-offs in terms of model- and context size."}, {"title": "A.4. Computational Resources", "content": "We trained every model on 16 NVIDIA A100 GPUs from our internal cluster. We trained 315 models in total, yielding a computational footprint of 5040 A100s. We ran Bellard's code on an NVIDIA GeForce RTX 4090 GPU with a 24-core Intel i9-13900KF CPU @ 3Ghz."}, {"title": "B. Additional Results", "content": ""}]}