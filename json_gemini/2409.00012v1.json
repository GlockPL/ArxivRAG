{"title": "AVIN-Chat: An Audio-Visual Interactive Chatbot System with\nEmotional State Tuning", "authors": ["Chanhyuk Park", "Jungbin Cho", "Junwan Kim", "Seongmin Lee", "Jungsu Kim", "Sanghoon Lee"], "abstract": "This work presents an audio-visual interactive chatbot (AVIN-Chat) system that allows users to have face-to-face conversations with 3D avatars in real-time. Compared to the previous chatbot services, which provide text-only or speech-only communications, the proposed AVIN-Chat can offer audio-visual communications providing users with a superior experience quality. In addition, the proposed AVIN-Chat emotionally speaks and expresses according to the user's emotional state. Thus, it enables users to establish a strong bond with the chatbot system, increasing the user's immersion. Through user subjective tests, it is demonstrated that the proposed system provides users with a higher sense of immersion than previous chatbot systems.", "sections": [{"title": "1 Introduction", "content": "With the recent advancements of large language models (LLMs) [Brown et al., 2020], interactive AI chatbot services have gained attention from various industries including customer support, virtual assistants, and education. These services provide users with great convenience but their text-only\u00b9 or audio-only\u00b2 interface hinders immersive user experience.\nTo increase user immersion in these chatbot services, as shown in Fig. 1, we present an audio-visual interactive AI chatbot system, which listens and responds to users with a realistic and emotional talking face and voice. To build this system, we composed our system with three distinctive sub-modules: (1) construct a deformable realistic face, (2) listen and talk to users in real-time, and (3) generate the avatar's facial expressions and lip movement in sync with its response. Additionally, we enable our system with emotional expressions so that it can adapt to users' feedback during conversations providing an immersive experience."}, {"title": "2 Audio-Visual Interactive Chatbot", "content": ""}, {"title": "2.1 Overall Pipeline", "content": "Figure 2 illustrates the overall pipeline of the proposed AVIN-Chat system. The proposed system is composed of facial avatar generation, text-speech interaction, and speech-driven facial animation. Subsequent sections will provide detailed descriptions of each module."}, {"title": "Facial Avatar and Blendshapes Generation (Offline)", "content": "The goal of facial avatar generation is to reconstruct a realistic 3D facial mesh with high-quality texture and then generate its blendshapes for efficient facial animation. We first reconstruct a realistic facial mesh and then generate the 52 standard blendshapes allowing it to be deformable. For the facial reconstruction, we adopt the HRN from [Lei et al., 2023]. HRN decouples facial geometry into (1) the low-frequency part for a coarse shape of the overall facial mesh, (2) the mid-frequency for local shapes, and (3) the high-frequency part for detailed information such as wrinkles. This hierarchical fashion allows us to reconstruct a photo-realistic facial mesh from a single image. Note that although we used a template-based 3D face reconstruction approach, we assume that the reconstructed face is a non-template face mesh for system scalability.\nTo make the reconstructed non-template facial mesh deformable, we generate blendshapes based on the 52 facial action units. For the blendshapes generation, we employ the LDT [Onizuka et al., 2019] that deforms the facial mesh according to the target expressions using landmarks as guidance. Here, since LDT requires facial landmarks for facial deformation, we extract landmarks from HRN reconstructed faces. We extract 52 facial landmarks in our implementation. If we use the face of a character who is difficult to automatically extract landmarks, we should manually designate the landmarks."}, {"title": "Text-Speech Processing (Online)", "content": "Text-speech interaction is to listen user's speech and generate a response. To generate the response we employ the OpenAI's ChatGPT, which is the state-of-the-art LLM. To feed the user's speech to the ChatGPT, we first convert speech to text using the state-of-the-art STT model named Whisper [Radford et al., 2023]. Whisper exhibits remarkable robustness in dealing with different audio conditions, effectively managing challenges such as background noise and various speech accents. From the encoded text, ChatGPT generates the text response. Here, to generate natural-sounding responses, we perform in-context learning on ChatGPT using prompts.\nSince the ChatGPT generates text responses, we adopt the TTS model to convert text to speech. For TTS, we utilize EmotiVoice, an improved version of PromptTTS [Guo et al., 2023]. EmotiVoice allows us to adjust the emotion in the voice by providing certain text prompts. The details for the ChatGPT and EmotiVoice prompts are described in Sec. 2.2."}, {"title": "Speech-Driven Emotional Facial Animation (Online)", "content": "We employ the speech-driven facial animation model to generate the emotional talking faces from the ChatGPT's emotional speech response. For the emotional talking face animation, we utilize the EmoTalk [Peng et al., 2023]. EmoTalk estimates a sequence of blendshapes weights [w\u00b9,...wT] where w = [wf, ..., w] is the set of the N-blendshape and"}, {"title": "2.2 In-Context Learning with Prompts", "content": "It is shown that prompts significantly influence the output of LLMs [Schick and Sch\u00fctze, 2021]. Therefore, there have been many attempts to control the results of LLMS through various prompting techniques [Shin et al., 2020; Lester et al., 2021]. We utilized multiple prompts to build a chatbot system capable of natural conversation. In the proposed system, since ChatGPT preserves the context of the conversation, we set various conditions through prompts before starting the conversation to maintain consistency in the subsequent dialogue. To give the user a more human-like experience, we ensured the responses were not too long, avoided repetition of the same phrases, and defined do's and don'ts for responding in a friendly manner. Additionally, we informed ChatGPT that the response text would be converted to speech, instructing it not to generate responses that are hard to verbalize, like emojis.\nFurthermore, we design our system so that users can input their desired emotional state. Using the user interface in Unity, the user can easily select the desired emotional state and the selected emotion is fed to the ChatGPT and EmotiVoice as prompts. If users are not satisfied with the chatbot's current emotional state, they can simply adjust the chatbot's emotional state during the conversation using the graphical user interface (GUI) in Unity. By implementing this approach, we observed that the generated responses were significantly more cheerful and friend-like compared to previous answers. It implies that the proposed chatbot is actively engaging in leading the conversation."}, {"title": "2.3 Implementation Details", "content": "The overall structure of the AVIN-Chat consists of a Python backend server and a Unity frontend. In the facial avatar generation module, Unity sends an image captured by a webcam to the server. Then using the HRN, the server generates a 3D facial mesh in OBJ format along with a texture file. Using the OBJ file as a base, the LDT model creates 52 additional OBJ files, and the Blender Python API combines them to create an FBX file, which is the final blendshapes. Unity receives this 3D FBX with its texture file and displays the constructed face to the user. In the conversational part, Unity records the user's voice, sending it to the server. In return, Unity receives an answering voice along with a list of blendshapes parameters. Once this exchange is done, Unity runs the audio and animates the facial mesh with the blendshapes parameters. For the emotional tuning, we've configured Unity to communicate with a backend API to change ChatGPT's prompts to match the user-defined emotions. Details of our system's graphical user interface are shown in Fig. 3. We used AMD Ryzen 7 5800u and AMD Radeon (TM) Graphics for the client and AMD Ryzen 5 5600X and NVIDIA RTX 3060 Ti for the server."}, {"title": "3 Experimental Results", "content": "We evaluate our proposed system on what conversations users prefer with different AI chatbots, such as text-only and speech-only systems, compared to ours. The text-only system utilizes ChatGPT which interacts with the user only through text inputs and outputs. The speech-only system is composed of ChatGPT, Whisper, and EmotiVoice. Note that the speech-only system also can generate emotional answers because it utilizes EmotiVoice. For fair comparisons, we use the same model weights as those used in the proposed AVIN-Chat. We recruited 11 participants to score each system's preference over the proposed AVIN-Chat. When scoring, participants are asked to measure their preferences in terms of intimacy, immersiveness, empathy, and overall satisfaction. Figure 4 shows the subjective test results. It shows that the text-only chatbot achieves better performance than the speech-only chatbot. Participants reported that visual stimulus, although being only text, enhanced user experience more than voice stimulus. In addition, the results demonstrate that the proposed AVIN-Chat significantly outperforms other chatbot systems in terms of preference by providing both visual and auditory stimulus."}, {"title": "4\nConclusion", "content": "In this paper, we have presented an audio-visual interactive chatbot, an end-to-end system for having face-to-face conversations with facial avatars reconstructed from a single image. Experimental results demonstrate that the proposed system can provide more immersive communication compared to text-only and speech-only chatbot systems. We believe that the proposed system can encourage users to use chatbot services and can be applied to various applications such as education and medical therapy. Our future work is to incorporate body gestures into a talking face for more realistic virtual communications."}]}