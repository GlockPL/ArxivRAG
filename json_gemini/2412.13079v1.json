{"title": "Identifying Bias in Deep Neural Networks Using Image Transforms", "authors": ["Sai Teja Erukude", "Akhil Joshi", "Lior Shamir"], "abstract": "CNNs have become one of the most commonly used computational tool in the past two decades. One of the primary downsides of CNNs is that they work as a \"black box\", where the user cannot necessarily know how the image data are analyzed, and therefore needs to rely on empirical evaluation to test the efficacy of a trained CNN. This can lead to hidden biases that affect the performance evaluation of neural networks, but are difficult to identify. Here we discuss examples of such hidden biases in common and widely used benchmark datasets, and propose techniques for identifying dataset biases that can affect the standard performance evaluation metrics. One effective approach to identify dataset bias is to perform image classification by using merely blank background parts of the original images. However, in some situations a blank background in the images is not available, making it more difficult to separate foreground or contextual information from the bias. To overcome this, we propose a method to identify dataset bias without the need to crop background information from the images. That method is based on applying several image transforms to the original images, including Fourier transform, wavelet transforms, median filter, and their combinations. These transforms were applied to recover background bias information that CNNs use to classify images. This transformations affect the contextual visual information in a different manner than it affects the systemic background bias. Therefore, the method can distinguish between contextual information and the bias, and alert on the presence of background bias even without the need to separate sub-images parts from the blank background of the original images. Code used in the experiments is publicly available.", "sections": [{"title": "1 Introduction", "content": "In the past decade, CNNs have revolutionized the field of computer vision due to their unprecedented performance capabilities with regard to image analysis, combined with ease-of-use due to the availability of libraries. These networks use automatic extraction and selection of data-driven abstract features, which means a substantial reduction in the need for manual feature engineering. CNNs are currently widely used in a variety of applications, including object detection, recognition, medical imaging, and many more.\nWith the substantial advantages, CNNs also have notable limitations. For example, training CNNs is computationally expensive, normally involving the need for advanced GPUs or computing clusters for execution. Training CNNs often requires a large number of samples (Uchida et al., 2016), and overfitting is a common situation when using CNNs (Santos and Papa, 2022; Thanapol et al., 2020; Gavrilov et al., 2018; Sikha and Benitez, 2024). The need for large training sets leads to the common use of data augmentation, which can in itself affect the bias in a controlled or uncontrolled manner (Pastaltzidis et al., 2022; Babul et al., 2024; Raja Babu et al., 2024). Changing the size of the input images can also change the performance of a CNN (Richter et al., 2021).\nIn addition to these limitations, one of the primary downsides of CNNs is their \"black box\" nature. While CNNs are trained with data samples, the process of updating their weights through the training remains opaque. The rules that define how classifications are made are convoluted and unintuitive, and it has been proven difficult to interpret the reason behind the decision made by a deep neural network (Ball, 2023). The definition of what a CNN really \"learns\" from the data thus becomes equally difficult to make. In other words, lack of interpretability simply means that CNNs should be used with caution as their classification processes are not necessarily clear to the user (Wang et al., 2020; Buhrmester et al., 2021).\nIdeally, a CNN that can achieve high accuracy in classification on benchmark datasets should perform well also in real-world settings. On the other hand, empirical studies (Ball, 2023; Tommasi et al., 2015; Dhar and Shamir, 2021) show that many popular datasets were biased, and possibly not representative of a model's performance concerning real-world image recognition. That shows that CNNs trained on benchmark datasets might be driven by information that is not limited to the information related to the task they aim at solving.\nTools that are used to identify biases or irrelevant information that drives the classification accuracy include saliency maps (Alqaraawi et al., 2020; Kim et al., 2019; Arun et al., 2021), which can be used to visualize the signal through the spacial domain. That allows to inspect and identify sources of signal that might not be relevant to the computer vision problem at hand (Simonyan, 2013; Pfau et al., 2019). The domain of adversarial neural networks has also been studied in the context of irrelevant information that can skew the way CNNs work, yet with no direct link to the relevant information (Hashimoto et al., 2020; Lin et al., 2020). Some solutions has also been proposed to detect adversarial samples (Wang et al., 2021; Pertigkiozoglou and Maragos, 2018). The need of deep neural networks for large datasets led to the practice of data augmentation. Data augmentation has been used to correct for a variety of biases in datasets, especially biases related to the distribution of the data and fairness of the analysis (Jaipuria et al., 2020; Sharma et al., 2020; McLaughlin et al., 2015; Iosifidis and Ntoutsi, 2018).\nAs the use of CNNs in various fields continues to grow, it is imperative to address these limitations, which also include the risk of classification bias. That reinforces the need to develop methods that can assist researchers who develop CNNs, and allow them to reduce the risk of unknown or unexpected biases."}, {"title": "2 Classification bias in benchmark datasets", "content": "The term \"bias\" in the context of benchmark datasets for image classification has been used to describe numerous situations. Those may include unbalanced distribution of the samples in the classes, biases in the labeling of the images, biased selection of data, and more (Torralba and Efros, 2011). Unbalanced distribution of the samples in datasets is a well-known bias driven by an unequal number of samples in different classes. Machine learning can be biased by the number of samples a machine learning model is trained by, making more populated classes become more frequent in the classification outcomes. That leads to some classes with higher confusion rate than other classes, consequently leading to problems in the fairness and discrimination by machine learning algorithms. One of the common solutions to the problem is by balancing the classes through data augmentation (Jaipuria et al., 2020; Sharma et al., 2020; McLaughlin et al., 2015; Iosifidis and Ntoutsi, 2018).\nLabeling bias is a situation of incorrect or inconsistent labeling. It is related to contextual information bias, in which elements that are not the target objects are present in one class more frequently than in other classes, and can therefore be used by a classifier to identify the class. Such biases can be detected by manual or automatic inspection of the contextual information, and also by comparing the performance when training with one dataset and testing with another dataset (Torralba and Efros, 2011). For instance, if the training and testing with two different datasets leads to weaker classification accuracy compared to training and testing with the same dataset, it can be considered an indication of bias (Torralba and Efros, 2011).\nWhile these biases can be identified, one of the more challenging biases is driven by information that is not part of the visible visual content of the images, yet its presence in the images allows machine learning algorithms to classify the images correctly. Because the source of such bias is not necessarily known, identifying it becomes more challenging, and can deceive even experienced researchers.\nBecause it is difficult to identify, such classification bias is present in many widely used benchmark datasets (Shamir, 2008; Model and Shamir, 2015; Majeed et al., 2020; Tommasi et al., 2015; Dhar and Shamir, 2021, 2022; Ball, 2023). For instance, Model and Shamir (2015) conducted an in-depth study comparing dataset bias across various traditional object recognition benchmarks, including COIL-20, COIL-100, NEC Animals, Caltech 101, etc. They employed a method that involved isolating a small, seemingly blank background area from each image - an area that does not contain any information about the object of interest. The findings revealed that all datasets that were tested still achieved classification accuracy above chance levels using these small sub-images, even though the sub-images lacked any visually interpretable information]. This technique was used to detect dataset bias in single-object recognition datasets, and to compare the extent of bias across different datasets (Model and Shamir, 2015).\nA similar observation was made with common face recognition datasets (Shamir, 2008). Experiments showed that by using just a small part of the background that does not include any part of the face or hair, the algorithms could still identify the faces with accuracy far higher than mere chance (Shamir, 2008).\nThe impact of bias is substantially stronger in datasets of objects imaged in controlled environments (Model and Shamir, 2015; Dhar and Shamir, 2021). Such datasets include commonly used benchmarks such as COIL-20,COIL-100, and NEC Animals. Datasets prepared from natural images collected from online sources include ImageNet (Deng et al., 2009), among others. The fact that these datasets achieved classification accuracy well above mere chance using only seemingly uninformative background areas that seem identical to each other when observed with the naked human eye indicates the presence of artifacts or noise that may influence the classification. This potentially allows machine vision algorithms to classify images without properly identifying the objects, and therefore without solving the machine vision problem they intend to solve.\nMajeed et al. (2020) encountered similar challenges when they investigated the applicability of CNNs for detecting COVID-19 in chest X-ray images. Their study involved using 12 established CNN architectures in transfer learning mode across three publicly available chest X-ray databases. Additionally, they introduced a custom shallow CNN model, trained from scratch. For their experiments, chest X-ray images were input into CNN models without any preprocessing, mirroring how other studies have used these images.\nTo better understand how these CNNs made their predictions, they conducted a qualitative analysis using a method called Class Activation Mappings (CAM) (Winastwan, 2024). CAMs allow researchers to generate heatmaps of the most discriminating regions and visualize which parts of the input image were most influential in the network's decision-making process. By mapping the activation from the CNN back to the original image, one can highlight class-specific regions that contributed most significantly to the model's classification, providing insights into which part of the image was the most relevant for detecting COVID-19.\nThis is a notable example of how almost all CNN architectures used areas outside the region of interest (ROI) to make the final classification predictions. This observation leads to the conclusion that these images contain information that is not necessarily related to the medical condition being studied, yet can be used to classify between the images. The presence of such information can lead to biased classification accuracy such that the accuracy observed with the benchmark dataset is different from the classification accuracy that the CNN architecture can achieve in real-world settings.\nThe limitation of the method for the detection of biases is that the images are cannot be registered in all cases in a manner that allows a map that shows consistent signal from different parts of the images. Another limitation is that in some cases the entire image covers area from which signal is expected. In such cases, parts of the images that are known to be irrelevant cannot be inspected separately, and therefore the bias cannot be identified. When no parts that are irrelevant for the classification are expected, it is difficult to know whether bias exists, as all parts of the image are expected to be informative.\nA related study (Dhar and Shamir, 2021) highlighted significant bias-related issues in common biomedical, face recognition, and object recognition benchmark datasets. A primary concern is the tendency of CNNs to learn from background information rather than the relevant visual content of the images. The experiments follow common practices to test whether CNNs can classify these image datasets, but instead of using the entire images, merely parts of the images that do not have any content of the object of interest were used."}, {"title": "3 Data and CNN architectures", "content": "The experiments described in this paper were carried out across six distinct datasets to ensure that the findings were robust across different contexts. As shown in Figure 6, datasets used in this study can be further categorized into three types: natural datasets, non-natural or synthetic datasets, and mixed or hybrid datasets.\nNatural datasets comprise real-world images collected from a high number of different sources, and exhibit high variability in lighting, angles, and object appearances. Two such naturally collected datasets were employed in this study:\nImagenette: A commonly used smaller subset of the ImageNet (Deng et al., 2009) dataset, containing 10 different classes. The dataset contains images collected from a broad variety of sources, and is very commonly used in machine learning experiments. Imagenette is accessible at https://github.com/fastai/imagenette.\nNatural Images: consists of images coming from eight different classes collected from several sources (Roy et al., 2018). The classes include airplane, car, cat, dog, flower, fruit, motorbike and person. The class \"fruit\" is discarded in this research because the background was changed to white, which could be used to identify that class and therefore can introduce biases in the classification process.\nThese datasets are either synthetically generated or created under controlled conditions, exhibiting less variability compared to natural datasets. Because of the lower variability and the controlled process, these datasets have higher vulnerability to bias when analyzed by machine learning (Shamir, 2008; Model and Shamir, 2015; Dhar and Shamir, 2021). Therefore, these datasets can be used as datasets that are known as datasets that contain information leading to bias, and any method for bias identification needs to be sensitive to the bias in these datasets. Following are the three different synthetic datasets used:\nContains lung CT scan images across four classes, which are normal, COVID-19, bacterial pneumonia, and viral pneumonia. The dataset has been shown to contain certain information that can significantly influence CNN's learning regardless of its medical relevance (Majeed et al., 2020).\nCOIL-20: The Columbia Object Image Library (Nene et al., 1996) is a repository of gray-scale pictures that contains such twenty objects. All objects could be precisely put on a turntable. The turntable was turned at 360 degrees until a change of object was appropriately visible from a static camera. Images of the objects were captured in 5-degree angular orientations. Model and Shamir (2015) demonstrated that COIL-20, being generated in a controlled environment, contains biases that significantly affect the model's classification accuracy. The exceptionally high classification accuracy, even when the object was absent from the test image, means that the model learned patterns from bias or background that are irrelevant to the detection of the objects. The presence of such bias makes this dataset useful for the purpose of this study, aiming to identify bias in image datasets.\nYale Faces: Yale Face Database (Georghiades et al., 2001; Belitskaya, 2018) contains 165 images of 15 subjects, with different facial expressions and lighting conditions. It includes the facial expressions of happy and sad, with and without glasses, sleepy, normal, wink, and surprised. It also includes lighting from different directions. Dhar and Shamir (2021) assessed face recognition benchmark datasets, including Yale Faces, to evaluate CNN performance. Similarly to COIL-20, the findings reveal a consistent bias within the dataset, with the CNN achieving notably higher classification accuracy when only a part of the image was used, highlighting the significant extent of this bias.\nHybrid datasets are collections of images that combine both real-world (natural) images and synthetically generated or modified images. The purpose of these datasets is to leverage the advantages of both types of data to improve the performance and generalization of models.\nThe Caltech 20 database is a subset of the larger Caltech 256 dataset (Griffin et al., 2007). This study used only the first 20 object categories to simplify the dataset. Interestingly, upon closer inspection, we found that the first 20 classes feature a combination of real-world images and either graphically gen-erated or have had their backgrounds altered to a plain white/black backdrop. Therefore, Caltech 20 is categorized as a mixed dataset throughout this study.\nIf a certain CNN architecture can classify the images with background information alone, it is an indication that the dataset contains bias. Such bias introduce a risk that any CNN architecture tested on the data can be driven by that bias. This study utilized the very commonly used Visual Geometry Group 16 (VGG16) neural network architecture (Simonyan and Zisserman, 2014) that has demonstrated excellent classification accuracy. For instance, the architecture achieved high classification accuracy of ~92.77% on the full ImageNet dataset (Simonyan and Zisserman, 2014). VGG16 comprises 13 convolutional layers, five max-pooling layers, and three fully connected (dense) layers, adding up to 21 layers. Only 16 of these layers have weights. The network takes as input images of size 224x224 with three RGB channels. In all cases 70% of the samples were allocated for the training set, 15% for the test set, and 15% for the validation set. The training was with 40 epochs and Adam optimizer. The code used for the experiments can be found at https://github.com/SaiTeja-Erukude/ identifying-bias-in-dnn-classification."}, {"title": "4 Methods and results", "content": "As demonstrated in Section 2, CNNs are capable of identifying the correct class even when they are trained and tested with seemingly blank background parts of the images. The presence of such bias can be identified by performing experiments similar to the experiments shown in Section 2, repeating the same machine learning process using seemingly blank parts taken from the background of each image. But that approach can be used only if the images in the dataset has such blank background. If the entire image contains information that is meaningful for the classification, a control experiment with blank background cannot be performed, since better-than-random classification accuracy can be attributed to the visually relevant image content rather than the bias. Here we approach the task by applying image transforms to the original images, and testing the classification accuracy with the transformed images to identify the presence of biases."}, {"title": "4.1 Fourier transform", "content": "Fourier transform is one of the most widely employed methods in image processing, used to break down an image into its sine and cosine frequency components (Cochran et al., 1967). This transform converts the input spatial domain image to its equivalent frequency domain image. In the resulting image, each pixel denotes a certain frequency present in the spatial domain image. Since Discrete Fourier Transform (DFT) is a sampled version, it does not contain all frequencies forming an image. It is based only on a set of samples good enough to fully describe the image in the spatial domain. The number of frequencies equals the number of pixels in the spatial domain image, i.e. the image in the spatial and Fourier domains are of the same size (Fisher et al., 1996).\nVGG16 models were trained and tested on the dataset after applying the Fourier transform to all images. Applying the Fourier transform has reduced the classification accuracy of the CNN across all the datasets, both for the full images and for the cropped sub-images.\nFor example, in the Imagenette dataset, the accuracy dropped significantly from ~59% with the original full images to ~38% for Fourier-transformed full images. The accuracy for cropped sub-images dropped to around random levels. Similarly, the COIL-20 dataset saw its accuracy decline from ~100% to ~80%. As the figures show, most other datasets also showed a decrease in classification accuracy for both full and cropped images.\nOne possible explanation for this decrease could be that transforming images into the Fourier domain leads to a loss of spatial information as sensed by the VGG16 architecture. CNNs rely on this spatial information to effectively learn and classify features. Yet, using the Fourier-transformed images leaves it challenging to determine whether the drop in classification accuracy is due to the loss of relevant features or driven by the reduction of inherent bias in the datasets that were influencing the CNN's predictions. Therefore, the Fourier transform is an example of a transform that does not effectively help distinguish between biases in natural and non-natural datasets, as it reduces accuracy across all types of datasets."}, {"title": "4.2 Wavelet transform", "content": "Wavelet transforms are mathematical methods used for investigating and retrieving data from images (Agarwal et al., 2017; Othman and Zeebaree, 2020). They break an image into components at various positions, preserving both frequency and spatial data. In contrast to the Fourier transform, which outputs a global frequency, the Wavelet transform provides a time-frequency (or space-frequency) representation, allowing for localized evaluation in both space and frequency domains.\nWavelet transform was applied to all datasets. As was done with the Fourier transform, the experiments included the classification accuracy using the full images, as well as experiments with the 20\u00d720 cropped sub-images. Two commonly used wavelets were used in this study: Haar and Daubechies. Haar wavelets are discontinuous and seem like a step function. It is a simple form of wavelet that can be used to test the impact of wavelets in the broad sense. Daubechies wavelets best represent polynomial trends, and are used often in situations of signal discontinuity, which is a situation that can be used to separate between foreground signal and detectable background \u201cnoise\u201d.\nThe Wavelet findings indicate a notable decline in classification accuracy for natural datasets, while non-natural datasets have seen either consistent or improved accuracy rates. This trend is observed in both full images and cropped images. For instance, for the Imagenette dataset has dropped from ~59% to around ~50% for full images, and from ~17% to roughly ~13% (nearly random levels) for cropped images. In contrast, the COIL-20 dataset, which is synthetic, accuracy maintains a constant level of 100%, and its cropped image accuracy has increased from ~27.8% to about ~31.4%.\nThe 20\u00d720 cropped images are seemingly blank and do not contain any visual information that can be sensed by the human eye. However, they still contain hidden information that can influence a CNN model's predictions. As shown in Figure 13, applying a wavelet transform to these cropped images has increased their accuracy compared to the accuracy using the original images. This indicates that the Wavelet transform revealed and enhanced the hidden signal in the background. That means that CNN has learned to use the background information, leading to more biased and therefore less reliable results.\nIn contrast, natural datasets made of images collected from various sources typically do not contain bias in the imaging process, as images of the same class are rarely taken during the same session or provided by the same single source. Therefore, these datasets are affected differently by the wavelet transform. As a result, the classification accuracy for Imagenette and Natural image datasets do not improve when the wavelet transform is applied. Therefore, the wavelet transform effectively differentiates between contextual information and the presence of background information that leads to bias. It can therefore be used to test for the possible presence of background information that can lead to bias, yet without confusing it with contextual information."}, {"title": "4.3 Median filter", "content": "The median filter is a very common non-linear simple image processing method that evaluates the image pixel-by-pixel and replaces each pixel with the median of its adjacent entries. The pattern of nearby pixels depicts a window that slides across the entire image, one entry at a time (Dhanani and Parker, 2013). It is a smoothing technique that effectively removes disturbances from a noisy image or a signal. Unlike low-pass finite impulse response (FIR) filters, median filters are notable for retaining image edges, making them commonly used in image processing.\nMedian filtering was performed on all datasets described using OpenCV's \u2018medianBlur()\u2018 method with windows size of 5\u00d75 to smooth the images and reduce noise.\nFor full images, the classification accuracy decreased when using natural datasets with the median filter. For example, Imagenette dropped from ~59% to about ~55%, and Natural images fell from ~85% to about ~81.6%. In contrast, the accuracy for synthetic datasets has remained stable, while hybrid datasets have improved. For instance, the accuracy for Caltech 20 increased from around ~36.6% to around ~39%.\nThe median filters were also tested in combination with the wavelet transform, such that the wavelet transform was applied after applying the median filter. As before, the analysis was applied to the full images, as well as the cropped images. The accuracy for natural datasets, such as Imagenette and Natural images, decreased by about ~9.5% and ~14.8%, respectively. The classification accuracy for synthetic and hybrid datasets either remained stable or increased. That indicates that when applying the median filter followed by a wavelet transform, the classification accuracy when using the datasets that are known to contain bias increases or remains the same, while the accuracy of datasets collected from various sources drops."}, {"title": "4.4 Summary of the results", "content": "Table 1 summarizes the results discussed in Section 4. The table shows the classification accuracy, but also the specificity, sensitivity, and F1 score. The table provides a higher resolution view of the results. The table provides a higher resolution view of the results."}, {"title": "5 Conclusion", "content": "CNNs have become the most prevalent computational tool for image classification. Due to its black-box nature, reliable performance evaluation can become challenging, and biases or unreliable performance evaluations can go unnoticed. In this study, several techniques including Fourier transforms, Wavelet transforms, simple median filtering, and their combinations were tested. The Wavelet transform, simple median filtering transform, and their combination have shown to be effective in distinguishing between biases in natural datasets that is driven by contextual information, and bias in non-natural datasets that have been shown in previous studies to be driven by artifacts acquired through the imaging process. Therefore, the results show that the method can distinguish between contextual information and imaging process bias, and alert on the presence of such bias even without the need to separate background information from the original images.\nThe proposed method also does not correct for the bias, but merely identifies its existence. As demonstrated in Section 2, numerous experiments were done without noticing the bias, and many commonly used benchmark datasets can be classified with high classification accuracy by just using a seemingly blank sub-image of the background. Therefore, before fully trusting the prediction accuracy of CNNs, it is recommended to apply methods such as the tests outlined in Section 4 to identify possible biases in the datasets and it should also help you distinguish between contextual information and the presence of background bias. This evaluation helps gauge CNN's reliability and ensures its accuracy is trustworthiness.\nFuture work will focus on identifying the sources of the biases, and potentially altering the images to correct for biases. Correction of the biases can make use of GANs or adversarial neural networks creating images that visually seem identical to the original images, but are handled differently by deep neural networks. Such pre-processing can not merely identify the bias, but also help to avoiding it by removing information that is irrelevant to the problem at hand but can still be used by classifiers to classify the images."}, {"title": "Code availability", "content": "The complete Python code utilized in this research study is available in a GitHub repository: https://github.com/SaiTeja-Erukude/ identifying-bias-in-dnn-classification. Using the code will require to change the input and output directories. Required libraries are pywt, cv2, and keras."}]}