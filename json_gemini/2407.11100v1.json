{"title": "Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond", "authors": ["Xuhong Wang", "Haoyu Jiang", "Yi Yu", "Jingru Yu", "Yilun Lin", "Ping Yi", "Yingchun Wang", "Qiao Yu", "Li Li", "Fei-Yue Wang"], "abstract": "Large Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely acknowledged as an effective strategy. Identification systems for LLMs now rely heavily on watermarking technology to manage and protect intellectual property and ensure data security. However, previous studies have primarily concentrated on the basic principles of algorithms and lacked a comprehensive analysis of watermarking theory and practice from the perspective of intelligent identification. To bridge this gap, firstly, we explore how a robust identity recognition system can be effectively implemented and managed within LLMs by various participants using watermarking technology. Secondly, we propose a mathematical framework based on mutual information theory, which systematizes the identification process to achieve more precise and customized watermarking. Additionally, we present a comprehensive evaluation of performance metrics for LLM watermarking, reflecting participant preferences and advancing discussions on its identification applications. Lastly, we outline the existing challenges in current watermarking technologies and theoretical frameworks, and provide directional guidance to address these challenges. Our systematic classification and detailed exposition aim to enhance the comparison and evaluation of various methods, fostering further research and development toward a transparent, secure, and equitable LLM ecosystem.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have become increasingly important for driving innovation across multiple industries. From automated customer service to complex natural language understanding tasks, the applications of LLMs are expanding. However, as LLMs become more widely used, the challenges to protect security, compliance, and user privacy have become increasingly severe, highlighting the urgent need for robust identity recognition systems.\nIdentity recognition plays a crucial role across various sectors in modern society [1], from financial transactions [2] and healthcare [3] to border security [4] and online services [5]. The application of identity recognition technology is ubiquitous, ensuring user authentication and authorization, and serving as the cornerstone of security and privacy. In fact, all existing governance frameworks and security systems rely on the effective operation of identity recognition systems [6]. Despite the widespread application of identity recognition systems in many fields, such systems have yet to be fully established in the realm of artificial intelligence (AI). This is primarily due to the complexity and dynamic nature of the AI domain, where traditional identity recognition methods struggle to meet the demands of AI systems. The core issues of identity recognition involve achieving distinguishability, unforgeability, and traceability. These issues are particularly critical in the context of LLMs, where the characteristics of textual data, the openness of LLMs, and the extensive applications of LLMs make identity recognition even more complex.\nCurrently, watermarking technology is regarded as a potential solution to address the three core issues in identity recognition [7]. It can covertly embed identity information without compromising the quality of the original data [8], ensuring distinguishability. By integrating cryptography, watermarking technology ensures the unforgeability of information and enables traceability through detection. This technology offers an innovative strategy for intellectual property protection and data security in the field of LLMs. Given the urgent need to protect intellectual property and ensure the traceability of security responsibilities in complex LLM application scenarios, it is essential to establish effective techniques and theoretical frameworks for embedding and extracting watermarks.\nAlthough some existing literature reviews [9-12] have gradually focused on these issues, most studies primarily introduce the basic principles of algorithms. They lack a comprehensive analysis of watermarking as the cornerstone of identity recognition systems for LLMs and do not adequately address the multifaceted conflicts of interest encountered by LLMs in actual operation. This article innovates the existing LLM watermarking systems from three main aspects: application, theory, and evaluation, thereby providing theoretical and practical support for the secure, transparent, and fair use of LLMs. The main contributions of this article are as follows.\nApplication: In Section 2, we illustrate that the LLM application system is transitioning from a centralized setup, dominated by model technology service providers, to a multi-centric design that prioritizes identity verification and behavior traceability. We also explored the different preferences of data providers, technology service providers, users, and third-party regulators regarding various aspects of identity recognition systems within a multi-center LLM application framework. This novel perspective deepens our understanding of the rights and responsibilities of participants in LLM community. It also promotes the establishment of a fairer and more secure AI application environment.\nTheory: In Section 3, we address the limitations of current LLM watermarking technology by developing a theoretical system based on mutual information theory [13]. The comprehensive mathematical foundation establishes a formulaic framework and classifies LLM watermarking technologies into five primary processes: generation, embedding, attack, extraction, and reconstruction. The optimization object and constraints of each process are elaborated with mathematical formulas, allowing researchers to accurately develop and enhance the watermarking techniques based on corresponding roles and stages.\nEvaluation: In Section 4, we have synthesized the performance evaluation metrics for LLM watermarks from multiple perspectives, encapsulating the preferences of various LLM entities in their application of watermarking techniques for identity recognition. This summary contributes to the development of a comprehensive and standardized evaluation system, prompting consideration of security issues related to LLM watermarking, and outlines new research trajectories and technological orientations.\nThrough these three core contributions, our article significantly expands the scope of watermarking applications within LLMs. For the first time, we put watermarking techniques within the context of the identification applications of LLMs, providing robust technical support for addressing the challenges of security and transparency in LLMs. This integration serves a dual purpose: it enhances the traceability of content generated by LLMs, allowing each output to be reliably traced back to its originating model, and it substantially boosts the trustworthiness of LLMs in various application scenarios by ensuring the authenticity and provenance of the content. Finally, we have highlighted some challenges that still exist in the current watermarking technology and theoretical systems, and suggested potential solutions for these challenges. We hope this work will spark further research and discussion, propelling LLM technology towards a trustworthy and verifiable future while safeguarding user interests."}, {"title": "2 Establishing Identification System through LLM Watermarking", "content": "In the current digital era, identity recognition technology is critical for safeguarding information security. Traditional identity recognition techniques, such as Multi-Factor Authentication (MFA) [14], biometric technologies [15] (including fingerprint [16] and facial recognition [17]), and Single Sign-On (SSO) [18], are primarily employed to authorize and identify individuals within human communities, relying on biometric features, behavioral patterns, and language analysis to verify identities. For instance, some studies identify individuals by modeling their interaction behaviors with devices [19] and their language styles [20]. However, an effective identity recognition system has yet to emerge in the LLM community.\nDue to the capacity of LLMs to generate text of high quality and diversity, it poses a novel challenge to authenticate whether a segment of text is the output of a particular LLM, and to confirm that it has not been unauthorizedly altered or counterfeited. Traditional identity recognition technologies are not applicable in this scenario, as they cannot be directly implemented on the text content or LLMs.\nWatermarking technology is a crucial method for identity recognition in the field of computer science [21\u201324]. Traditionally used for copyright protection in images, audio, and video [25]. Watermarking embeds secret information without compromising original data quality. With the rise of LLM technology, embedding watermark information in LLMs themselves and their related applications has become an indispensable area of research. Watermarking enables verification of text origin from specific LLMs, thereby enhancing copyright protection, intellectual property preservation, and content authenticity. Moreover, watermarking aids in tracing content dissemination, preventing misinformation, and ensuring transparency and traceability in compliance with legal and regulatory standards. Consequently, watermarking technology has emerged as an innovative and indispensable mechanism for identification in the context of LLM applications.\nIn practical application scenarios, data providers can use watermarks to protect the copyright of their training data, ensuring that the training data are not arbitrarily altered or copied. Technology service providers wish to use watermarking technology to protect their model copyrights, preventing their models from being repackaged or stolen, and enabling them to track the usage of their models. LLM users need watermarking technology to protect their privacy rights, preventing their confidential information from being sold. PRTTP will ensure the security of LLMs by verifying the presence of watermarks at multiple stages; once any security issues are identified, it is crucial to ensure that the source of the problem can be traced and resolved. The following sections detail the four distinct entities of the watermark system and elucidate how each can establish its own watermarking technology framework."}, {"title": "2.1 Future Trends in LLM Applications", "content": "Currently, the research and development of LLMs are in a period of rapid growth, benefiting from the swift enhancement of computing power and the accumulation of large volumes of high-quality data. The life cycle of LLMs can generally be divided into stages of data preparation, training and testing, deployment and application, and monitoring and maintenance. Entities and participants typically involved in the life cycle of LLMs include training data providers (such as Stardust AI\u00b9, Scale AI2, etc.), model technology service providers (such as OpenAI\u00b3, Anthropic\u2074, etc.), LLM users, and certain public regulators and trusted third parties (PRTTPs) (governments, non-profit organizations, etc.)\u2075.\nHowever, people tend to focus on the iteration of technology (models, algorithms, and data) while neglecting issues of security and rights protection in the application processes of LLMs. As shown in the left part of Fig. 1, in the existing LLM R&D system, technology service providers play a dominant role in every step, from data preparation and model training to final deployment and maintenance, relying on their technical reserves and commercial needs. This centralized system allows technology service providers to monopolize the entire LLM technology market through technological barriers and resource advantages, making it difficult for other participants to develop or achieve breakthroughs independently. Overall, this system makes the development of technology, model compliance, and user privacy security dependent on the ethical standards of technology service providers, which is not conducive to the overall development of the AI ecosystem.\nTypically, once the LLM technologies have been widely promoted and enter a period of stabilization, the LLM community shifts its focus from solely valuing the technology to emphasizing regulatory compliance, user privacy, and the security of the technology. This shift transforms the original technology-centric centralized operational system into a balanced, multi-centric system involving multiple participants, as illustrated in the right part of Fig. 1. In this system, the influence of users and PRTTPs is significantly enhanced. PRTTPs are responsible for obtaining and verifying security and trust declarations from data providers and technology service providers, as well as handling risk reports from general LLM users. Meanwhile, LLM users, while ensuring their security and privacy, will authorize the collection of their preferences to model technology service providers and gain potential benefits.\nIn this new system, as the status of each entity becomes more balanced, they all seek to maximize their benefits while ensuring their rights are protected, rather than merely using the model passively. For instance, LLM users might suspect that the model technology service providers could steal their privacy; data providers might worry that service providers could resell their data. The most critical aspect of ensuring the flawless operation of the entire lifecycle system of LLMs is to ensure that these entities can engage in trustworthy collaboration through certain mechanisms, thereby minimizing mutual suspicion to the greatest extent. The core element in reducing suspicion is making the identities in the LLM recognizable and their behaviors traceable."}, {"title": "2.2 Identity Recognition System in LLMs", "content": "In the current digital era, identity recognition technology is critical for safeguarding information security. Traditional identity recognition techniques, such as Multi-Factor Authentication (MFA) [14], biometric technologies [15] (including fingerprint [16] and facial recognition [17]), and Single Sign-On (SSO) [18], are primarily employed to authorize and identify individuals within human communities, relying on biometric features, behavioral patterns, and language analysis to verify identities. For instance, some studies identify individuals by modeling their interaction behaviors with devices [19] and their language styles [20]. However, an effective identity recognition system has yet to emerge in the LLM community."}, {"title": "2.3 Identification System from Different Views", "content": "In practical application scenarios, data providers can use watermarks to protect the copyright of their training data, ensuring that the training data are not arbitrarily altered or copied. Technology service providers wish to use watermarking technology to protect their model copyrights, preventing their models from being repackaged or stolen, and enabling them to track the usage of their models. LLM users need watermarking technology to protect their privacy rights, preventing their confidential information from being sold. PRTTP will ensure the security of LLMs by verifying the presence of watermarks at multiple stages; once any security issues are identified, it is crucial to ensure that the source of the problem can be traced and resolved. The following sections detail the four distinct entities of the watermark system and elucidate how each can establish its own watermarking technology framework."}, {"title": "2.3.1 Training Data Providers", "content": "For training data providers, the infinite replicability of data poses an uncontrollable risk of data breaches as it circulates. Currently, the most effective way to prevent the unauthorized dissemination of data is to secretly add a unique, strong watermark [26] to the data without altering the quality of the dataset itself [27]. This ensures that the data can still be verified for its initial copyright even after being redistributed, modified, or otherwise processed. Some researchers have proposed even more in-depth solutions, demonstrating that watermarked data used for LLM training can have its watermark information detected in the text generated by the LLM, which is referred to as radioactivity [28]. Once this knowledge is embedded into unauthorized LLMs, data providers can identify whether their watermark is present in the models based on the response to certain specific watermark triggers. Moreover, since a training dataset is likely to be copied and sold multiple times, the most crucial aspect of protecting copyright and preventing data leaks or unauthorized distribution is identifying the source of the leakage. To address this, encoding a unique watermark message for each dataset to be distributed and embedding it into the data with a covert watermark is an essential option.\nTherefore, training data providers need to focus on watermarking techniques that offer high fidelity and transparency, meaning the watermarking should be done in a way that does not degrade the quality of the text. Moreover, the watermark embedded by the data providers should have the capability of multi-bit information encoding to enable the identification of the data purchasers. Besides, the watermark should possess high robustness and radioactivity, allowing for the detection of watermarks in text content generated by unauthorized models if the data are used for illegal training."}, {"title": "2.3.2 Model Technology Service Providers", "content": "For technology service providers, watermarking technology helps protect model copyrights and monitor the usage of models. To address the costs and technical difficulties faced during the pre-training of LLMs, some technology service providers might opt to use data generated by well-trained models for training, which has formed a system similar to teacher-student model distillation. This imitation has sparked concerns over the copyright of unauthorized distilled models, especially when the corpus data of these distilled models come from closed-source LLMs (such as GPT-4). In the constantly evolving landscape of AI copyright protection, it is crucial to emphasize the importance of protecting intellectual property while maintaining the integrity and practicality of AI models. The development and implementation of watermarking technology enables model developers to protect their innovations from unauthorized use and distribution effectively.\nTo protect the intellectual property of models and prevent the unauthorized use of developed LLMs through distillation by offenders, technology service providers should embed watermarks only when the model is invoked by users, without affecting the model's own training process. This approach meets the technology service providers' pursuit of model performance. Additionally, it substantiates the model's ownership and facilitates the tracking of its distribution and usage [29]. This helps prevent the model from being copied or tampered with by unauthorized third parties."}, {"title": "2.3.3 Public Regulators and Trusted Third Parties", "content": "With the rapid advancement of AI technology, especially the widespread use of LLMs in content creation, the roles of public regulators and trusted third parties (PRTTPs) in watermarking systems have become critical. Policymakers and civil society are increasingly focused on the safe use of these technologies, as shown by the EU AI Act, the NDAA for Fiscal Year 2024 [30], and voluntary commitments to label AI-generated content. These initiatives emphasize the need for transparency in content sources, including clear marking of watermarks and content origins, as well as guidelines for content certification and watermarking developed by the U.S. Department of Commerce following the AI Executive Order of October 30, 2023 [31]. These guidelines aim to help the public easily identify the authenticity of online information.\nLLM watermarking technology is considered key to ensuring the safety, compliance, and ethical integrity of AIGC throughout its entire lifecycle. To this end, PRTTPS should make the following efforts:\n1.  Establishing clear watermarking technical standards and usage norms, setting up certification programs for watermark service providers, and promoting success stories and best practices.\n2.  The implementation of education and training programs, especially aimed at enhancing the understanding of watermarking technology among all participants, further reinforces this process. This involves not only educating parties on how to select and utilize watermark services correctly but also emphasizing the importance of adopting these measures to ensure that all involved can effectively use watermarking technology to identify AIGC.\n3.  Establishing strict oversight and enforcement mechanisms is equally important, ensuring that all parties rigorously adhere to the regulations and standards for watermark usage, thereby guaranteeing the correct and secure application of watermarking technology.\n4.  Providing the general public with access to open watermark interfaces for LLMs allows users to add watermarks to their own data or to verify through watermarking whether a piece of data contains their private information. This approach helps track the usage and flow of data, preventing unauthorized dissemination of the data across the internet.\nThese requirements are not isolated but necessitate multi-party collaboration among data providers, model technology service providers, watermark technology service providers, and PRTTPs. Through a cooperation framework that spans different sectors and industries, we can facilitate information sharing and technological advancement. These measures encourage deep reflection on the safe use of AI technology and lay a solid foundation for maintaining public trust in AIGC. By establishing industry standards, implementing rigorous certification processes and audits, providing comprehensive education and training, enforcing vigilant supervision and execution, promoting best practices, and fostering collaborative multi-party partnerships, we can ensure the safe and responsible development of LLMs and other AI technologies."}, {"title": "2.3.4 Users of LLMS", "content": "Watermarking technology can help LLM users in verifying the copyright and legitimacy of the models, while also serving as a tool to protect the security of user data and privacy. Users should opt for LLM services verified by PRTTPs. The model providers usually require such verified service providers to apply data watermarking technology to ensure that the input data is used only for the current service and is not accessed or misused by third parties. LLM users can ensure that watermarks are embedded in their prompts by choosing services with publicly verifiable watermarking technology. Users can verify the use and flow of their data through a public watermark verification interface, preventing unauthorized distribution of their data on the internet. Watermarking technology plays a crucial role in protecting personal privacy and can also alleviate users' privacy and security concerns on another level, thereby promoting the further popularization of LLM technology.\nMoreover, LLM users will gradually transition from mere users to becoming a more deeply involved and crucial part of the LLM application ecosystem. Firstly, users can trade their private data through some form of anonymization, collaborating with data providers to co-create datasets. This not only generates profit but also enhances the overall efficiency of the LLM application system. Secondly, users will engage in in-depth cooperation with PRTTPs. If the model produces unsafe answers or if copyright infringement is detected, users can report these issues to PRTTPs by clicking the report button. PRTTPs can thus centralize the originally dispersed and unequal user oversight power through this method, better standardizing the development of LLM technology."}, {"title": "2.4 Guidance for Implement Watermarking", "content": "All entities should establish their own rights protection system using watermarking technology based on their position within the LLM application ecosystem and their relationships with other entities. The rights that need protection, the entities that need identification, the limitations encountered when using watermarking technology, and some basic requirements are organized in Table 1. Based on the descriptions in the table, entities can find the corresponding watermarking technologies in Section 3 and deploy their watermarking schemes according to the different basic requirements."}, {"title": "3 LLM Watermarking Technology", "content": "In this section, we formally define watermarking in LLMs and explore its application in securely and covertly transmitting information. Watermark algorithms for LLMs involve the processes of generation, embedding, extraction, and reconstruction, as shown in Fig. 2. To help the readers better understand, we have listed the main symbols used in the article in the notation table 2.\nWe denote that the watermark message m, the N-vectors text sequence $S_N = (s_1, s_2,...,s_N)$, the N-vectors watermarked text sequence $T_N = (t_1, t_2,...,t_N)$, the D-elements watermark security parameter $K_D = (k_1,k_2, ..., k_D)$, and the watermark W take their values in message space M, original sequence space S, watermarked sequence space T, watermark security parameter space K, and watermark space W respectively. We require that S and T are isomorphic, indicating that there is a one-to-one correspondence between their elements while preserving the structure of the spaces. Moreover, S and T must obey identical distributions to ensure that the watermarking process does not alter the statistical properties of the original sequences. This is crucial for the stealth and efficacy of the watermark. Additionally, the watermark space W should be compatible with the original sequence space S, where compatibility refers to the ability of the watermark signal W to be embedded into the original signal $S_N$ without introducing detectable statistical differences.\nA watermark W is produced by the watermark generation module, followed by the generation of the watermarked text sequence $T_N$ via watermark embedding process. During text dissemination, not only are attackers likely to be present, but the message itself may also be subjected to cutting, substitution, rewriting, and reordering among other operations. An attack channel $Attack(T'_N | T_N)$ may apply some of the above operations to process the sequence $T_N$ to the corrupted sequence $T'_N$. The extractor, utilizing the watermark security parameter and the text sequence $T'_N$ under examination, retrieves the watermark W'. Subsequently, the reconstruction decodes W' to calculate the estimated value m' of the message initially transmitted.\nThe watermarking system of LLMs can be analyzed by defining the watermark message m, the statistical model for the sequence $S_N$ output by the LLM according to the prompt, and the watermark security parameter $K_D$. This includes the distortion function, constraints on the acceptable distortion levels for both the watermark embedding and the watermark attacker, and the information available to each party. The goal of the watermarking algorithm is to seek the maximum reliable transmission rate of m over any possible watermarking strategy and any attack that satisfies the specified constraints. Consequently, the entire watermarking process can be described using principles of information theory. To better understand the watermarking framework, we first explain the key parameters in the diagram:"}, {"title": "\u2022 Sequence SN", "content": "Assume the input prompt is denoted as Prompt, and the sequence $S_N \u2208 S$ is composed of elements $s_1, s_2,...,s_N$, where N is the length of the sequence. The process by which the LLM generates a sequence can be represented as\n$$P(S_N|Prompt) = \\prod_{i=1}^{N} P(s_i | s_1, s_2, ..., s_{i-1}, Prompt).$$\nIn this equation, $P(S_N | Prompt)$ is the probability of generating the sequence $S_N$ given the input Prompt. $P(s_i | s_1, s_2,..., s_{i-1}, Prompt)$ is the conditional probability of generating the next element $s_i$ given the input Prompt and the first i - 1 elements of the sequence.\nThe LLM considers the sequence generated so far, $s_1, s_2,..., s_{i-1}$, along with the input prompt, and then predicts the probability distribution for the next word $s_i$. Once the model predicts the probability distribution for $s_i$, it selects the next word based on this distribution, which could either be the word with the highest probability or a word sampled randomly according to the distribution. This process is repeated until the entire sequence $S_N$ is generated or a certain termination condition is met."}, {"title": "\u2022 Watermark Message m", "content": "$m \u2208 M$ represents the message that needs to be encoded. Depending on the amount of information encoded, watermarks can be categorized into one-bit watermarks and multi-bit watermarks. The one-bit watermarking technique is both mature and stable; however, it is limited to encoding a single bit of information specifically, indicating whether the text was generated by a particular LLM. One-bit watermarks cannot meet the growing demand for customized information in LLM applications. For example, embedding model and version information in the watermark can effectively track the source of text among multiple LLMs. In contrast, multi-bit watermarks allow for carrying more customizable information. However, designing a practical multi-bit watermark method is a challenging task because multi-bit watermarks are more complex than one-bit watermarks. Consequently, embedding a multi-bit watermark can have a greater impact on the text quality compared to embedding a one-bit watermark. After the watermark generation module encodes m, it must be reliably transmitted to the watermark extraction module during message transmission to ensure the success rate of watermark detection. Here, m is independent of $(S_N, K_D)$."}, {"title": "\u2022 Security Key KD", "content": "$K_D \u2208 K$ represents the identity tag used to provide identity information to a text sequence. Introducing $K_D$ serves two primary purposes. Firstly, it is crucial to identify LLMs that utilize the same watermarking algorithms. This identity tag can take forms such as a secret key, providing the generator with information about the identity of the LLM that generated the text sequence $S_N$. Secondly, $K_D$ can be introduced at various stages, offering more flexibility in the identity verification process. Additionally, $K_D$ provides a known source of randomness during the extraction phase, allowing for the use of randomized codes, a standard technique to enhance transmission performance in communications.\nThe dependency between the original sequence $S_N$ and the identity tag $K_D$ can be quantified by the joint distribution $P(S_N, K_D)$. In public watermarks (blind watermarks), $S_N$ and $K_D$ are independent, meaning the identity tag is completely unrelated to the original text sequence. This independence implies that identity verification does not require the original text, facilitating public verification. Conversely, for private watermarks, if there is a dependency between $S_N$ and $K_D$, such as $S_N$ being a function of $K_D$, validating the identity tag requires access to the original text sequence or the original encoding parameters."}, {"title": "3.2 Problem Definition", "content": "Different LLM watermarking algorithms have distinct parameters and settings at each stage, playing various roles throughout the watermarking process. The existing LLM watermark algorithms are summarized in Fig. 3 categorized by the different phases of the watermarking process.\ni) Watermark generation: The watermarking algorithm encodes the text $S_N$ generated by an LLM, the watermark security identity $K_D$, and the watermark message m through the function f. Initially, the watermark information m must be converted into a feature suitable for embedding, generating the corresponding watermark signal W. The method of generation simultaneously affects the watermark's information capacity, transparency, robustness, and other indicators. In LLM watermarking, the entire generation stage aims to disperse the watermark information m into the feature space of the sequence, mapping the LLM output text sequence $S_N$, the watermark message m, and the watermark security parameter $K_D$ to the watermark signal W, which can be embedded into the original sequence space S and satisfy certain constraints. The generation process is denoted as\n$$W = f(S_N,m, K_D)$$\n$$f:S \\times M \\times K \\rightarrow W.$$\nThe function f is the mapping of the sequence $S_N$ generated by the LLM, the watermark message m, and the watermark security parameter $K_D$ to the watermark signal W.\nWe define two key concepts: Attack Robustness and Security Robustness. From an information-theoretic perspective, these concepts for the watermark signal W can be characterized by mutual information I. Attack Robustness represents the ability to withstand attacks against the watermark. A larger $I(S_N; W)$ indicates a stronger dependency between the generated watermark W and the original sequence, signifying a higher capability to resist watermark attacks. Conversely, Security Robustness refers to the capacity to prevent the deduction of the watermark message from the watermarked sequence $T_N$. A smaller I(m; W) signifies a weaker dependency between the generated watermark W and the watermark message m, thereby inhibiting the inference of the watermark message m from W.\nTo simultaneously consider Attack Robustness and Security Robustness, the optimization goal is\n$$\\max_{f_N} I(S_N; W) - \\lambda I(m; W)$$\n(i)\n$$W = arg \\max_{f_N} I(S_N; W) - \\lambda I(m; W)$$\ns.t.\n(ii) $$I(m; W) \\leq \\epsilon,$$\nwhich aims to maximize the mutual information $I(S_N; W)$ between the original sequence $S_N$ and the watermark W, while minimizing the mutual information I(m; W) between the watermark message m and the watermark W. Ideally, I(m; W) should be zero, indicating that m and W are completely uncorrelated, thereby achieving complete transparency of the watermark. The parameter \u03bb is a positive trade-off coefficient that adjusts the balance between these two objectives. Furthermore, \u03f5, a very small positive number, quantifies the security robustness of the watermark.\nii) Watermark Embedding: After obtaining the watermark signal W through the generation phase, it is necessary to embed W into the watermark carrier (i.e., the original text sequence $S_N$) to produce the watermarked text sequence $T_N$. We define the operation of embedding the watermark as the function Emb:\n$$T_N = Emb(S_N,W).$$\nThe Emb function may employ simple techniques such as addition, concatenation, or more complex watermark embedding operations. These could include embedding the watermark W from various perspectives, such as format, vocabulary, and syntax at the data level or by manipulating the training and inference processes of LLMs at the model level. Integrating Formula 2, the embedding operation can be expressed as\n$$T_N = Emb(S_N, f(S_N,m, K_D)).$$\nFrom the perspective of watermark Text Quality, the mutual information $I(S_N;T_N)$ between the embedded text sequence $T_N$ and the initial sequence $S_N$ should be maximized. Considering the watermark Transparency, the correlation between the generated watermark signal W and the embedded text sequence should be as small as possible, i.e., I(W;TN) should be minimized. Therefore, the optimization objective can be defined as\n$$\\max_{Emb} I(S_N;T_N) - \\theta I(W;T_N)$$\ns.t.\n$$\\frac{1}{N}E{d_N(S_N,T_N)} = \\sum_{S_N \u2208 S} \\sum_{K_D \u2208 K} \\sum_{M \u2208 M}  \\sum_{S_N \u2208 S} \\sum_{K_D \u2208 K} P(S_N, K_D) \\times d_n(S_N,T_N) < D_{emb}.$$\nThe watermark Transparency is also subject to an average distortion constraint $D_{emb}$. The definition of the distortion constraint involves an average over the distribution $p(S_N, K_D)$ and a uniform distribution over the messages. A non-negative bounded distortion function $d_{emb}(x_i, y_j)$ exists between elements of the sets S and T. This average distortion $D_{emb}$ is the value of the average distortion\n$$D_{emb} = \\sum_{S_N \u2208 S} \\sum_{T_N \u2208 T} p(T_N)p(S_N | T_N)d_n(S_N,T_N).$$\nThe distortion function is extended to N-dimension-vectors by $d_n(S_N,T_N) = \\frac{1}{N} \\sum_{i=1}^{N} d_{emb}(s_i, t_i)$. This constraint further limits the degree of distortion in the text sequence with the embedded watermark ensuring the transparency of the watermark.\nWatermark embedding techniques can be classified into two main categories: data-centric watermark embedding and model-centric watermark embedding. Each methodology has distinct features and application contexts, collectively laying the technological foundation for the protection of intellectual property, model security, and the authentication of data and models.\niii) Watermark Extraction: The function $\\phi: T \\times K \\rightarrow W'$ is the extractor mapping, which takes the watermarked text $T_N$ and the watermark security parameters $K_D$, and maps them to the extracted watermark signal W':\n$$W' = \\phi(T_N, K_D).$$\nAt this stage, we revisit the watermark embedding process of text length N, constrained by distortion $D_{emb}$, which can be defined as a triplet (M, f, \u03c6) where: M is the watermark message space.\niv) Watermark Reconstruction: After extracting the watermark signal W', it is necessary to decode the watermark message m' from W'. To approach the channel capacity with a reliable transmission rate of the watermark message, a jointly optimal decoding rule, designed corresponding to the generation phase, should be adopted to compute the estimated value of the original watermark message m'.\nIf the Maximum A Posteriori (MAP) decoding principle is adopted to minimize the error probability:\n$$m' = \\arg \\max_{m \u2208 M} p(m | W', K_D).$$\nOther decoding rules can also be adopted, such as correlation rules, normalized correlation rules, or trigger-based rules.\nv) Watermark Attacks: With the advancement of watermarking technologies, attack methods targeting watermarks have also evolved. These attacks aim to undermine the effectiveness of watermarks and, in some cases, completely remove them, posing a threat to content security and copyright maintenance. Notably, these methods of attack not only represent potential threats but are also utilized to assess the robustness of watermarking technologies. This, in turn, helps developers improve and fortify watermark algorithms.\nAdversary's capabilities. If watermark attacks occur throughout the watermarking algorithm cycle, we consider an adversary with black-box input-output access to the language model. In public watermark mode, the adversary is aware of all the details of the public algorithm. In private watermark mode, the adversary knows the watermark implementation but lacks knowledge of the security key $K_D$ and the encryption component of the watermark generation algorithm.\nThis adversary has the capacity to modify the sequence $T_N$ within a distortion constraint. Given the distortion function between elements of the sequence spaces S and S', denoted as $d_{atk}(,)$, subject to the distortion constraint $D_{atk}$. The attack channel $Attack(T'_N|T_N)$ is defined as a sequence of conditional probability mass functions (p.m.f.) from space S to S', where the distortion is evaluated relative to the original LLM sequence $S_N$ rather than the watermarked sequence $T_N$:\n$$E{d_{atk}(T_N, T'_N)} =  d_{atk}(S_N,T'_N)P(T'_N | T_N)P(S_N,K_D) \\leq D_{atk}.$$\nAdversary's objective. The primary objective of the adversary is to render the watermark extraction algorithm ineffective. Specifically, the adversary aims to produce a T'N such that \u03c6(T'N, KD) \u2260 W, while ensuring that T'N remains a minor modification of the LLM-generated watermark sequence TN."}, {"title": "3.3 Watermark Generation", "content": "Model-based learning methods employ deep learning techniques,such as GPT [47], BERT [48], to generate watermarks. These methods leverage the learning and generative capabilities of deep learning models, using a trained watermark generation model to directly produce watermarks W or embedded representations of watermarked sequences.\nIn contrast to other methods, this technique create the watermark that is intricately embedded into the content, enhancing security and robustness against tampering. Correspondingly, watermark extraction is typically performed using a dedicated decoder or a watermark detection network. This dual-model framework ensures that the embedded watermarks can be accurately retrieved, even when the content has undergone modifications or compression. By maintaining the watermark generation model as proprietary while making the watermark detection model publicly accessible, a publicly verifiable watermarking scheme can be effortlessly implemented. Conversely, by keeping both the watermark generation and detection models confidential, the watermarking method can be transformed into a private watermarking system.\nThis strategic dichotomy allows for flexibility in controlling the accessibility and verification of watermarks, catering to different security and privacy requirements. Publicly verifiable watermarks facilitate widespread verification, enhancing transparency and trust in digital content authenticity. In contrast, private watermarking schemes offer enhanced security since the ability to generate and detect watermarks is restricted to authorized entities. This restriction safeguards proprietary or sensitive information from unauthorized detection and manipulation.\nKuditipudi et al. [49] employed a decoder that deterministically maps a sequence of random numbers, encoded by a watermark key, to samples in a language model. This is achieved by converting a sequence of uniform random variables and permutations into tokens using inverse transform sampling. Considering that many existing watermarking algorithms are designed at the token level, Hou et al. [50] utilized a sentence encoder trained through contrastive learning (such as Sentence-BERT [51]) to capture textual semantic similarities. They partitioned the semantic space of sentences and employed sentence-level rejection sampling to ensure that sentences fall within watermarked partitions of this space. This approach to semantic watermarking at the sentence level shows strong robustness against paraphrasing attacks. Munyer et al. [52] utilized Word2Vec [53] and Sentence Encoding [54] to engender a roster of replacement words, which we consider as the generated watermark.\nThe mainstream method of embedding watermarks is to add extra watermark logits on top of the logits generated by the LLM. Some intriguing research [43, 55], inspired by the red-green list method [32], moves away from guiding logit modifications by partitioning the vocabulary. Instead, these studies directly generate watermark logits through a watermark generation model. Most watermarking methods cannot simultaneously possess Attack Robustness against watermark attacks and Security Robustness to prevent inferring the watermark from the watermarked sequence TN, necessitating a trade-off. The research by Liu et al. [55] makes the generated watermark W no longer determined by previous tokens and vocabularies, thereby enhancing both Attack and Security Robustness. Gu et al. [56] took an alternative approach by training a student model to learn the token distribution of watermarked text, imitating the behavior of existing watermarking algorithms through model distillation. However, this approach involves model distillation and suffers from high computational complexity.\nSeveral pioneer works [44, 57\u201359] have explored designing multi-bit watermark schemes for LLMs by leveraging the model's learning capabilities. Abdelnabi and Fritz [57] proposed an encode-decoder transformer architecture, AWT, which learns to extract the message from the decoded watermarked text. To maintain the quality of the watermarked text, they utilize signals from sentence transformers and language models, relying entirely on a neural network for message embedding and extraction. This approach has proven effective because neural networks have been successfully used for natural language watermarking, demonstrating their capability to handle complex language patterns."}, {"title": "3.3.1 Vocabulary-partitioning-based Methods", "content": "Watermarks generated through vocabulary partitioning usually utilize a pseudo-random function (implemented as a hash function) to generate random seeds. These seeds are used to divide the vocabulary into distinct lists, ensuring that a subset of tokens from a particular list is output more frequently during token generation. The watermark is generated by biasing the selection of tokens towards specific lists.\nKirchenbauer et al. [32] introduced the first LLM watermarking technique that generates watermarks through vocabulary partitioning, as shown in Fig. 4. This method employs a hash function that uses the preceding token as input to compute a random seed for partitioning the vocabulary into green and red lists. The LLM-generated tokens are biased towards the green list by adding a bias term to the logits. During detection, the extraction process calculates the ratio of green tokens with the z-metric to determine the presence of the watermark.\nFollowing Kirchenbauer et al. [32], the idea of watermarking generation by vocabulary partitioning has been widely explored by many researchers [33\u201340]. These methods have introduced more refined methods for partitioning the red-green lists, resulting in a greater diversity of partitioning techniques. For example, Takezawa et al. [34] introduced tighter constraints for partitioning red-green tables, enhancing the concealment of the watermark and improving the quality and naturalness of the generated text. Building upon previous work, Kirchenbauer further elaborated on the robustness of these watermarking generation methods against paraphrasing attacks [33]. The effectiveness of the detection method in this study was evaluated by comparing them with detectors designed to identify AI-generated text.\nWhile these watermarks can be integrated with various detection techniques, their distribution does not meet the unbiased criterion. To address this, Hu et al. [35] introduce an unbiased watermark by adjusting the token generation probability distribution through the watermark code space. Some studies focus on enhancing the robustness of such watermarks against attacks, aiming to mitigate vulnerabilities arising from reliance on lexical distribution. For instance, Li et al. [41] employed a novel reweighting strategy combined with a context-based hash function to assign a unique i.i.d. ciphers to each generated token. This encoding method ensures the preservation of the original token distribution during the watermarking process, making the watermarked text indistinguishable from unwatermarked text in terms of distribution. Other studies [38, 40] have considered semantic similarity when partitioning the vocabulary, such that the semantic value may remain unchanged even in the face of watermark attacks, thereby achieving robustness against paraphrasing attack.\nWatermark generation methods based on vocabulary partitioning rely on hashing tokens from the previous moment, leading to inefficiency during the extraction phase, as it necessitates iterative computation over all tokens. To address this, Zhao et al. [36] simplified the watermarking generation method proposed by Kirchenbauer et al. by employing a fixed increase in logits watermark strength e, making the vocabulary partitioning independent of previously generated tokens and solely reliant on a global key. Fernandez et al. [42] proposed a method to enhance efficiency by cyclically shifting an initial message to generate secret vectors for each message, thereby easily converting one-bit watermarks into multi-bit watermarks and allowing for parallel processing. Additionally, Liu et al. [43] employed a watermark generation network to partition the vocabulary instead of using hash functions, which has also been proven effective.\nSome studies have extended the method of partitioning the vocabulary into multi-bit watermarks to convey more information through the watermark. However, these methods also face higher computational complexity and increased demands for watermark information density. To develop a more effective multi-bit watermark techniques, Wang et al. [44] proposed the Balance-Marking method, which uses a proxy language model (proxy-LM) to ensure that the available and unavailable vocabulary for generating watermarks have approximately equivalent probabilities. Similar to the work of Lee et al. [39], this method can also circumvent low-entropy parts of the text to effectively improve text quality.\nHowever, this watermarking approach essentially divides the vocabulary into multiple sets of red-green tables, with each set corresponding to one bit of watermark message. Such methods requires iterative computation during the logits generation process for each token, resulting in extremely high computational complexity. To mitigate this, besides the cyclic shift method by Fernandez et al. [42], Yoo et al. [45] independently encode each message bit position, transforming the division of the vocabulary from red-green lists into colored lists, effectively encoding multiple states for every token. Allocating tokens to different parts of the message allows for embedding longer messages without increasing generation latency. Compared to methods that directly generate watermarks using the watermark message m, Qu et al. [46] have enhanced the robustness of the watermark by introducing error-correcting codes (ECC) to the watermark information before dividing the vocabulary.\nThe key limitation of the existing multi-bit watermark approaches [42, 44] is that the computational cost of their extraction functions grows exponentially with the length of the watermark message bits, and they cannot accurately or effectively extract all watermark bits.\nThe methods of vocabulary partitioning involve mapping the text sequence SN to various distributions of vocabulary that can be analyzed for their Attack Robustness and Security Robustness using Formula 4. All these methods incorporate semantics, global secret keys, and additional information to further solidify the dependency between the generated watermark W and the original sequence SN, which increases I(SN; W). The original KGW [32] partitions the vocabulary using the hash value of previous tokens. The hash function ensures the mutual information I(W;m) \u2192 0 between the watermark W and the watermark message m, making this method a high level of Security Robustness. Denote that the mutual information I(m; W) = H(m) - H(m | W), where H(m) is the entropy of the message m and H(m | W) is the conditional entropy of m given the watermark W. Since the computation process of hash functions is unidirectional and irreversible, the conditional entropy H(mW) encloses to H(m) when the output of the hash function W is known. Therefore, the mutual information I(m; W) \u2192 0 is minimized to prevent an attacker from back-propagating the message m through the watermarked signal W.\nThe work [33] uses a context-robust Min-Left Hash to strengthen the connection between W and the original sequence SN, thereby increasing I(SN; W). Zhao et al. [36] no longer use a hash function for vocabulary partitioning but instead base it on text edit distance to partition a fixed vocabulary. Although the fixed vocabulary has a higher I(W;m) than the hash-partitioned vocabulary, its Security Robustness is reduced. However, by increasing the edit distance between vocabularies, attackers need multiple attempts to bridge the text distance and invalidate the watermark, thus enhancing Attack Robustness.\nAdditionally, for low-entropy texts mentioned in paper [32], which are difficult to watermark by modifying logits, this can also be analyzed using Formula 4. When the original sequence SN has low entropy, the first term of the optimization goal, I(SN; W), is low, which negatively impacts watermark generation and reduces its Attack Robustness. Consequently, some studies [39, 46] propose bypassing low-entropy texts and only watermark high-entropy texts to ensure the watermark's Attack Robustness. Furthermore, semantic-based watermarking methods determine vocabulary partitioning by incorporating contextual relationships and semantic information. The introduction of semantic information enhances the correlation between the generated watermark W and the original sequence SN. The increase in I(SN; W) enhances the robustness of such methods against watermark attacks. For instance, Fu et al. [38] selected semantically related vocabulary to add to the watermark vocabulary. Ren et al.'s SemaMark [40] mitigated semantic sensitivity by discretizing the continuous word embedding space appropriately, ensuring that discrete semantic values remain unchanged even in the face of watermark text editing attacks."}, {"title": "3.3.2 Model-learning-based Methods", "content": "Model-based learning methods employ deep learning techniques, such as GPT [47], BERT [48], to generate watermarks. These methods leverage the learning and generative capabilities of deep learning models, using a trained watermark generation model to directly produce watermarks W or embedded representations of watermarked sequences.\nIn contrast to other methods, this technique create the watermark that is intricately embedded into the content, enhancing security and robustness against tampering. Correspondingly, watermark extraction is typically performed using a dedicated decoder or a watermark detection network. This dual-model framework ensures that the embedded watermarks can be accurately retrieved, even when the content has undergone modifications or compression. By maintaining the watermark generation model as proprietary while making the watermark detection model publicly accessible, a publicly verifiable watermarking scheme can be effortlessly implemented. Conversely, by keeping both the watermark generation and detection models confidential, the watermarking method can be transformed into a private watermarking system.\nThis strategic dichotomy allows for flexibility in controlling the accessibility and verification of watermarks, catering to different security and privacy requirements. Publicly verifiable watermarks facilitate widespread verification, enhancing transparency and trust in digital content authenticity. In contrast, private watermarking schemes offer enhanced security since the ability to generate and detect watermarks is restricted to authorized entities. This restriction safeguards proprietary or sensitive information from unauthorized detection and manipulation.\nKuditipudi et al. [49] employed a decoder that deterministically maps a sequence of random numbers, encoded by a watermark key, to samples in a language model. This is achieved by converting a sequence of uniform random variables and permutations into tokens using inverse transform sampling. Considering that many existing watermarking algorithms are designed at the token level, Hou et al. [50] utilized a sentence encoder trained through contrastive learning (such as Sentence-BERT [51]) to capture textual semantic similarities. They partitioned the semantic space of sentences and employed sentence-level rejection sampling to ensure that sentences fall within watermarked partitions of this space. This approach to semantic watermarking at the sentence level shows strong robustness against paraphrasing attacks. Munyer et al. [52] utilized Word2Vec [53] and Sentence Encoding [54] to engender a roster of replacement words, which we consider as the generated watermark.\nThe mainstream method of embedding watermarks is to add extra watermark logits on top of the logits generated by the LLM. Some intriguing research [43, 55], inspired by the red-green list method [32], moves away from guiding logit modifications by partitioning the vocabulary. Instead, these studies directly generate watermark logits through a watermark generation model. Most watermarking methods cannot simultaneously possess Attack Robustness against watermark attacks and Security Robustness to prevent inferring the watermark from the watermarked sequence TN, necessitating a trade-off. The research by Liu et al. [55] makes the generated watermark W no longer determined by previous tokens and vocabularies, thereby enhancing both Attack and Security Robustness. Gu et al. [56] took an alternative approach by training a student model to learn the token distribution of watermarked text, imitating the behavior of existing watermarking algorithms through model distillation. However, this approach involves model distillation and suffers from high computational complexity."}, {"title": "3.3.3 Custom-rules-based Methods", "content": "The methods proposed in this section involve generating watermarks by applying specific rules. The core principle is to design a set of rules or algorithms that modify or mark text data and models, thereby generating watermarks.\nBackdoor Techniques: These methods involve poisoning the training data of LLMs by adding specific triggers to text sequences, thus enabling LLMs to learn the characteristics of these triggers. The presence of watermarks is detected by observing the output of LLMs when given input samples containing embedded triggers.\nFor instance, Liu et al. [64] implanted backdoor triggers into a small subset of the target LLM's training data and tampered with the labels of this subset. The presence of watermarks is assessed by verifying the output of the trigger set through black-box access to the target model. Tang et al. [65] improved the backdoor poisoning method without altering the original labels of the watermark samples. Instead, they guided the model to memorize the preset backdoor function by disabling original features on watermark samples through imperceptible perturbations. This approach increases the transparency of the watermark and, by protecting the trigger set used for watermark verification, creates a traceable private watermarking technology. These methods are primarily data-driven, reliant on data, and require participation in the training process of LLMs, with a limited capacity to carry one-bit watermark information.\nCryptography: Watermark generation based on cryptography aims to enhance the security and stealthiness of watermarks through cryptographic techniques. These methods primarily include the use of digital signature technology for watermarks [66] and cryptographically inspired undetectable watermarks [67], both of which rely on cryptographic principles to protect the watermark from unauthorized access and tampering. Christ et al. [67] quantified the randomness used in the generation of a specific output by utilizing pseudo-random values generated by an encrypted pseudo-random function (PRF) to determine watermark embedding locations. They analyzed the undetectability and integrity of the watermark using empirical entropy theory. Furthermore, they introduced the encryption of pseudo-random values with a secret key, which is required for the extraction and verification of the watermark. However, this method is only validated through the entropy theorem for binary channels and is limited to embedding one-bit watermark information. It is uncertain whether it can maintain sufficient empirical entropy to ensure the robustness of the watermark when expanded to multiple bits of information.\nAdditionally, Fairoze et al. [66] explored the application of digital signature technology on LLMs. This method involves encrypting the hash value of text with a private key to create a watermark signature. This signature is then embedded into tokens of additional length through rejection sampling, while the public key facilitates watermark detection. This approach does not require embedding statistical signals in the generated text, providing a viable solution for publicly detectable LLM watermarks. However, this approach, which employs asymmetric algorithms, often results in highly unstable time and computational costs during watermark generation. The running time exhibits high variance, especially when encountering low-entropy sections of sampling or missed hashes, making the time required for watermark generation occasionally unacceptable.\nThe main advantage of incorporating cryptographic techniques is the ability to determine whether a watermark is private or publicly detectable easily. One primary benefit of public watermarks is that the extraction and reconstruction processes can be outsourced, allowing different entities to provide watermark extraction services separate from the model providers. Furthermore, public watermark schemes should support the full lifecycle operations of watermarks through API access to private LLMs.\nCustom Synonym Substitution Rules: Some methods [68\u201370] ensure a close relationship between the watermark and the text's semantics and context by using custom synonym replacement rules. This approach not only maintains the transparency of the watermark but also enhances its Attack Robustness to text editing attacks. The fundamental premise of text editing attacks is to invalidate the watermark under certain distortion conditions. However, effectively linking semantics and context can limit the effectiveness of such attacks. For instance, He et al. [68] considered two fundamental linguistic features during synonym replacement: part-of-speech (POS) and the dependency tree. Expanding to multi-bit watermarks, Yang et al. [69] proposed a context-aware synonym replacement method for generating watermarks. Meanwhile, Li et al. [70] embedded a series of synonym-based token changes as watermarks in the code generated by LLMs. Li et al. [71] introduced a watermarking method for code text based on transformation rules such as code refactoring, reordering, and format conversion. Each transformation corresponds to a bit of the watermark message, with the presence or absence of a specified transformation determining whether the watermark value of that bit is 0 or 1.\nCustom Generation Function: Some custom watermark generation functions have proven effective in practice. For instance, Yang et al. [72] defined a binary encoding function for black-box LLMs, which calculates a random binary code (0 or 1) for each word in the text based on the hash value of the word and its preceding word. Essentially, the function of this binary encoding is similar to that used by Kirchenbauer et al. [32], who utilized hash functions for vocabulary partitioning. Zhao et al. [73] defined two sets of specialized secret sinusoidal signals as watermarks. These two sets of sinusoidal signals have values ranging from [0,1] and satisfy the condition that their sum equals 1."}, {"title": "3.4 Watermark Embedding", "content": "After generating the watermark W, it must be embedded into the sequence carrier SN. Depending on the direct object of operation during the embedding process, the embedding can be divided into two types: data-level embedding and model-level embedding.\nData-level Embedding Methods\nData-level embedding, also known as post-processing methods, involves inserting watermarks by directly modifying, learning from, or augmenting the content itself. The primary advantage of these methods is their ability to embed identifying markers W within data in a relatively concealed manner without requiring modifications to the model's architecture or functionality. Data embedding methodologies exhibit a broad spectrum, including format adjustments, lexical changes, grammatical shifts, and the exploitation of language models, thus offering a variety of approaches to suit different data types and application contexts. Focused on the textual level, these methods are not only applicable to the training data of LLMs to influence the learning trajectory but can also be directly applied to the text generated by LLMs, enabling watermark embedding at the output phase. This phase is principally segmented into four categories:\nFormat Adjustment: Format-based data embedding ingeniously utilizes text formatting and visual features for watermark embedding. Unlike methods that directly modify the text content, format-based methods embed watermarks through subtle adjustments to the appearance and structure of the text, aiming to achieve copyright protection and data tracking without compromising readability and content integrity.\nBtassil et al. [74] achieved watermark embedding by adjusting the vertical and horizontal positions of text lines and words, such as through line shift coding and word shift coding. Por et al. [75] embedded watermarks by inserting different space characters into text spacing or using visually similar but differently coded characters. The presence of these watermarks is almost invisible to users, ensuring the natural flow and original appearance of the text content. These methods do not rely on changes to the text content, making it broadly applicable across various languages and document types without concerns about linguistic or semantic restrictions. However, watermarks embedded in this way have poor robustness and can be invalidated by some formatting-checking tools.\nLexical Variation: Lexicon-based data embedding is a method that embeds watermarks by carefully selecting and replacing specific words in the text. This approach leverages the richness and diversity of language, allowing for subtle modifications without altering the original intent and content.\nAside from the watermark generation method [68\u201370] of Custom Synonym Substitution Rules mentioned in Section 3.3, which embeds watermarks through vocabulary changes, the embedding of watermarks through lexical variation is also widely used by various watermark algorithms [52, 57, 58, 72, 76\u201378].\nLexicon-based data embedding ensures both fluency in text reading and semantic consistency. This characteristic renders some watermark detection methods ineffective and ensures the transparency of the watermark, making it a mainstream method in many algorithms. However, this approach relies on high-quality synonym databases and advanced language models or rules for precise vocabulary selection and sentence transformation.\nGrammatical Transformation: Grammar-based data embedding is a technique that embeds watermarks by altering the syntactic structure of text or code. This method aims to incorporate watermark information through subtle syntactic adjustments without affecting the original semantics. Its application is not limited to natural language texts but also extends to programming languages, demonstrating wide applicability and flexibility.\nChalmers [79] inserts watermarks by transforming the syntactic structure of sentences within paragraphs. In the CATER method proposed by He et al. [68], the dependency tree is a type of syntactic structure that describes the directed binary grammatical relationships between words.\nThese embedding methods adjust the text at the grammatical level, maintaining semantic integrity and high transparency, and are commonly used for watermark embedding in code text. However, they require a deep understanding of grammar and analysis capabilities. Excessive grammatical changes can affect text readability.\nLanguage Model Utilization: This embedding method further leverages the capabilities of language models. Most watermark algorithms generated through model learning primarily utilize the semantic understanding abilities of language models to create watermarks, necessitating other embedding methods. In contrast, Zhang et al. [59] directly trained a message encoding module that takes watermark messages as input and generates watermarked text based on the learning capabilities of language models. This end-to-end training approach fully exploits the capabilities of language models. Since the watermark is directly generated and embedded internally by the language model, its robustness against text editing attacks depends on the robustness of the language model itself. The generation and embedding of the watermark rely on the model, and if data changes, it necessitates adjusting the model training objectives, which requires substantial computational resources.\nThe embedding methods at the data level involve manipulating text sequences generated by LLMs, with the fundamental principle of preserving the original sequence's semantics and readability. According to Formula 6, the text quality represented by I(SN;TN) is assured. However, detailed text analysis of vocabulary, format, grammar, etc., poses a risk of inferring the watermark signal W from the watermarked text TN. Therefor, that watermark embedding method has not effectively minimized I(W;TN)."}, {"title": "3.4.2 Model-level Embedding Methods", "content": "The model-level embedding approach differs from data-level methods by embedding watermarks directly within the application cycle of LLMs. Specifically, it involves three steps: modifying LLMs during the training phase, altering the logits generation during the inference phase, or adopting different token sampling strategies.\nTraining Phase Embedding: Embedding watermarks into LLMs during the training phase typically involves the use of backdoor techniques and data poisoning methods, as mentioned in Section 3.3. This approach is inspired by backdoor attack [80], incorporating poisoned samples into the LLM's training data. Assume the training data provider has his data samples $D_{train} = {(s_i, Y_i)}^N_{i=1}$,where each sample has its features $\u2208 S$ and label y \u2208 Y .The attacker selects a small proportion of data $ {(s_i, Y_i)}^M_{i=1}, M < N$ and adds a preset backdoor trigger to these samples while modifying their labels to a target label \u0177:\n$$D_{backdoor} = {(s,y)}^M_{i=1},s = f_a(s_i, trigger),$$,\nwhere fa is the watermark generation function to add trigger into the input. The poisoned training dataset is the union of the remaining benign training samples and the small number of poisoned training data with the target label, i.e.,\n$$D_{Poisoned} = D_{train} \\cup D_{backdoor}.$$\nLLMs trained or fine-tuned using the dataset DPoisoned perform normally on original tasks but generate consistent, specific outputs when inputs contain a special trigger set Dbackdoor. For example, Liu et al. [64] leveraged text backdoor techniques to insert triggers of different levels into a subset of the original training texts, uniformly changing the labels to a target label. Similarly, Sun et al. [81] employed a similar data poisoning method to embed secret and stable watermark backdoors into open-source code. Modifying the labels of the training corpus can lead to a decrease in LLM performance. To mitigate the impact of changing labels, Tang et al. [65] proposed Clean-Label backdoor watermarking. After selecting the target category, adversarial perturbations are employed to ensure that the model learns features related to backdoor while retaining the original labels. Sun's CodeMark [82] introduces semantic-preserving transformations of code and builds poisoned training data by altering the syntactic form of the code, such as changing 'a+=1' to 'a=a+1'.\nMethods that embed watermarks during the training of LLMs typically only provide a one-bit watermark information bit, which can only indicate the presence or absence of a watermark. Changes in the training process can lead to a decline in LLM performance and cause forgetting issues, thereby limiting the watermark embedding rate to a very low value. Additionally, altering the watermark requires retraining the LLM, restricting the application of watermarking algorithms that employ this embedding method.\nDuring the training process, the embedded watermarks, whether introduced by adding subtle backdoor triggers or by embedding backdoors through semantic transformations, must ensure that the generated text TN remains within specified distortion constraints relative to the original text SN while maximizing the mutual information I(SN;TN). Furthermore, without a trigger set, extracting the watermark W from the watermarked text TN becomes significantly challenging, thereby ensuring the watermark's transparency.\nInference Phase Embedding: Watermark embedding in LLMs during the inference phase primarily diverges in two directions: modifying logits generation and employing different token sampling strategies.\nLogits Generation: Logits are scores assigned by the LLM to potential next words based on its internal representation and the input sequence, determining the probability distribution that influences the model's next word generation. Methods of watermark embedding at this stage includes all methods that guide logit modifications through vocabulary partitioning [32\u201340, 42, 44\u201346], as well as various methods that produce watermark biases in logits [43, 55, 66, 73], directly inserting the watermark W into the logits generated by LLMs. Essentially, these methods bias the logits or apply other methods to influence them, causing the LLM's output to exhibit a certain bias. Watermark embedding is achieved through this biased output.\nAssuming an LLM is trained on a vocabulary of size V, given a sequence of tokens as input, the LLM predicts the next token in the sequence by outputting a logit score vector Logit. Watermarks, represented by the red-green list [32] and generated through vocabulary partitioning, are embedded by modifying the logit scores:\n$$Logit = \\begin{cases}Logit + \u03b4 & \\text{if watermarked} \\\\Logit & \\text{otherwise.} \\end{cases}$$\nAs shown in Fig. 5, since the logits generated by the LLM have been modified, the LLM tends to select tokens from the generated watermark list, resulting in a higher proportion of the generated text being watermarked. In this way, the LLM is induced to exhibit a specific bias when selecting tokens, achieving the effect of embedding a watermark.\nThese embedding methods involve modifying the logits of LLMs to bias the model towards outputting tokens from a watermark list, considering the optimization target of Formula 6 during design. Kirchenbauer et al. [32] ensured the quality of the generated text TN by imposing constraints on the modified logits. Lee et al. [39] ensured the functionality and quality of the code by avoiding embedding in low-entropy vocabulary. Takezawa et al. [34] produced more natural texts than existing watermarking methods by adjusting the minimum constraints on logit modification based on the length of SN. Furthermore, Hu et al. [35] defined two reweighting methods to produce an unbiased distribution of watermark logits, minimizing I(W;TN) to enhance the transparency of the watermark. DiPmark [37] reweighted the watermark logits with a key, aiming to make I(w; TN) \u2192 0 while maintaining the original distribution.\nToken Sampling: This section introduces embedding methods that intervene in the process of LLMs choosing the next token, utilizing the watermark W to guide the sampling strategy for each token to embed the watermark. Although the selection of tokens involves randomness, this randomness is controlled, with sampling strategies like random sampling, top-k, and top-p all possessing fixed randomness. By altering the sampling strategy with the watermark W, the watermark is embedded. During extraction, it is only necessary to judge the alignment between the chosen tokens and the set sampling sequence.\nChrist et al. [67] use the output of a Pseudo-Random Function (PRF) to decide whether to embed a watermark at a specific location. Specifically, for each subsequent token generation decision, the LLM uses a bit (or a small part) of the PRF's output to determine whether to change that token to embed the watermark. If the output of the PRF is below a certain threshold in the model's original probability distribution, the model generates the token according to the original probability distribution; if it is above this threshold, the model will choose a different token to represent the watermark. Due to the fixed nature of pseudo-random numbers, the watermarked LLM will generate the same text for the same prompts every time, which limits the diversity of the text TN. This corresponds to the second optimization target in Formula 6, where I(W;TN) is larger, making it easier to find the relationship between TN and the watermark W through multiple attack attempts.\nTo address the issue of monotonous generated text, Kuditipudi et al. [49] introduced the use of a random watermark key to compute a sequence of random numbers longer than the generated text, mapping it onto the sample to produce watermarked text. The alignment between the text and the pseudo-random number sequence is measured using the Levenshtein distance [83], thereby increasing the diversity of the text. Intervening in the sampling process of each token individually can easily lead to a decrease in text quality. Hou et al. [50] first calculated the LSH signature of the previously generated sentence, then randomly divided the LSH partitions into \"valid regions\" and \"blocking regions\" based on this signature. The watermark is embedded through the process of rejection sampling of tokens, meaning that new sentences are sampled from the language model until the embedding of the new sentence is located within the \"valid region\" of the semantic space, indicating successful embedding. Since the partitioning is based on the sentence level, this method significantly improves the quality of the generated text. Its performance in terms of watermark transparency and resistance to attacks far exceeds other methods."}, {"title": "3.5 Watermark Extraction", "content": "Watermark extraction methods in LLMs can be categorized into rule-based, trigger set-based, statistical, and deep learning-based approaches.\nRule-based Methods\nRule-based watermark extraction methods rely on identifying pre-defined patterns or rules within the text. These rules can include specific characters, combinations of words, text formatting, decision methods, password matching, and other predefined conditions. By analyzing these features, rule-based methods detect suspected watermarked texts. These approaches pay special attention to specific indicators that suggest whether a text has been generated by AI, such as the entropy of the text, patterns of vocabulary usage, and sentence structure. In this context, entropy measures the randomness and complexity of the text, aiding in distinguishing between human and machine-generated texts. The primary advantage of rule-based methods lies in their simplicity and efficiency, as they do not depend on complex machine-learning models but instead rely on the direct analysis of specific text attributes.\nSome studies leverage the Quadratic Residues Theorem to formulate extraction rules for specific watermarking algorithms. Atallah et al. [84] used a large prime as a key to calculate the hash values of the nodes in the semantic tree of each sentence, converting them into binary strings. They then sorted the sentences based on these strings' hash values and extracted the watermark by reading specific bit sequences from each sentence following the marked sentences in the sorted sequence. Chiang et al. [85] employed quadratic residue keys and bit operations to select terms from the text, constructed bit strings based on the values in the quadratic residue table, and ultimately transformed these bit strings using specific rules to extract the watermark. Topkara et al. [86] relied on the weighted graph of synonym sets, using a secret key to select and color specific subgraphs. During watermark generation, words are replaced to embed watermark information, and during extraction, information is read based on a custom coloring scheme. Kim et al. [87] extracted hidden information by calculating the statistical distribution of the space between words in text segments with the same category labels, using predefined decoding rules to extract from these statistical distributions.\nIn the aforementioned studies, Zhao et al. [73] employed the Lomb-Scargle periodogram method [88] to estimate the Fourier power spectrum. By applying an approximate Fourier transform, they amplified the subtle perturbations in the probability vector, enabling the detection of peaks in the power spectrum through frequency analysis to determine watermark information. He et al. [68] developed the CATER method, which utilizes a set of features and relies on predefined conditions and discrimination rules to extract watermarks. Fairoze et al. [66], on the other hand, calculated the hash value for each sequence using the watermark's public key and related parameters to determine whether it matches the expected signature to extract watermark information. These rule-based approaches are characterized by their specificity, making them unsuitable for generic watermark extraction. However, their advantage lies in the simplicity and intuitiveness of the extraction process.\nTrigger Set-based Methods\nThe trigger set-based watermark extraction methods [64, 65, 82] are typically used in conjunction with watermark embedding methods that employ backdoor techniques. The trigger sets usually consist of a group of backdoor text. Given a special trigger set, the LLMs will output a specific answer, which can be used as an extracted watermark. Since the implantation of backdoors participates in the LLM training process, these one-bit watermarks typically exhibit strong robustness. However, a key challenge of this approach lies in designing a covert backdoor trigger mechanism to ensure the transparency of the watermark and considering how to carry more information. Therefore, generating watermarks independent of the LLM training phase remains an area for further exploration.\nStatistical Methods\nStatistical watermark extraction methods involve rigorous mathematical and statistical analysis of texts or data to extract watermarks, focusing on identifying anomalies or characteristic differences in data distribution caused by watermark embedding. These approaches are particularly suited for detecting watermarks that have embedded statistical patterns or features during content generation. For instance, hidden watermarks can be extracted by comparing the statistical differences in vocabulary usage frequency, sentence length distribution, and syntactic complexity between watermarked and original texts.\nThe extraction process analyzes the distribution of generated text tokens to determine if they follow the distribution introduced by the watermark. This is achieved using the Z-test, Likelihood Ratio Test, Q-offset detection, Jensen-Shannon Divergence, or other non-asymptotic statistical tests to determine whether the sample mean significantly deviates from its expected value."}, {"title": "3.5.1 Rule-based Methods", "content": "Rule-based watermark extraction methods rely on identifying pre-defined patterns or rules within the text. These rules can include specific characters, combinations of words, text formatting, decision methods, password matching, and other predefined conditions. By analyzing these features, rule-"}]}