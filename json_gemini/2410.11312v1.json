{"title": "Towards Differentiable Multilevel Optimization: A Gradient-Based Approach", "authors": ["Yuntian Gu", "Xuzheng Chen"], "abstract": "Multilevel optimization has recently gained renewed interest in machine learning\ndue to its promise in applications such as hyperparameter tuning and continual\nlearning. However, existing methods struggle with the inherent difficulty of han-\ndling the nested structure efficiently. This paper introduces a novel gradient-based\napproach for multilevel optimization designed to overcome these limitations by\nleveraging a hierarchically structured decomposition of the full gradient and em-\nploying advanced propagation techniques. With extension to n-level scenarios,\nour method significantly reduces computational complexity and improves solution\naccuracy and convergence speed. We demonstrate the efficacy of our approach\nthrough numerical experiments comparing with existing methods across several\nbenchmarks, showcasing a significant improvement of solution accuracy. To the\nbest of our knowledge, this is one of the first algorithms providing a general ver-\nsion of implicit differentiation with both theoretical guarantee and outperforming\nempirical superiority.", "sections": [{"title": "1 Introduction", "content": "Multilevel optimization, comprising problems where decision variables are optimized at multiple\nhierarchical levels, has been widely studied in economics, mathematics and computer science. Its\nspecial case, Bilevel optimization, was first introduced by Stackelberg in 1934 [31, 6] as a concept\nin economic game theory and then well established by following works. Recently, with the rapid\ndevelopment of deep learning and machine learning techniques, multilevel optimization has regained\nattention and been widely exploited in reinforcement learning [15], hyperparameter optimization\n[9, 22], neural architecture search [19], and meta-learning [10, 26]. These works demonstrate\nthe potential of multilevel optimization framework in solving diverse problems with underlying\nhierarchical dependency within modern machine learning studies.\nDespite the proliferated literature, many of the existing works focus on the simplest case, bilevel\noptimization, due to the challenges of inherent complexity and hierarchical interdependence in\nmultilevel scenarios. Typical approaches to bilevel optimization include constraint-based and gradient-\nbased algorithms [20, 36], as well as transforming the original problem into equivalent single-level\nones. Among them, gradient based techniques have became the most popular strategies [24, 11].\nSpecifically, Franceschi et al. [9] first calculates gradient flow of the lower level objective, and\nthen performs gradient computations of the upper level problem. Additionally, value function\nbased approach [21] offers flexibility in solving non-singleton lower-level problems, and single-loop\nmomentum-based recursive bilevel optimizer [34] demonstrates lower complexity than existing\nstochastic optimizers.\nExtending from the majority works on bilevel cases, more and more attempts have been made to\ntackle trilevel or multilevel optimization problems [32, 30]. Yet the adoption of these methods\nis still hindered by significant challenges. For one, the application of gradients within multilevel"}, {"title": "2 Background", "content": "When modeling real-world problems, there is often a hierarchy of decision-makers, and decisions are\ntaken at different levels in the hierarchy. This procedure can be modeled with multilevel optimization.\nWe start by inspecting its simplest case, bilevel optimization, which has been well explored and\ninspired our approach to multilevel settings. The general form of bilevel optimization is\n$\\begin{aligned}\n&\\underset{x}{\\operatorname{minimize}}\\quad f^{U}(x, y) \\\\\n&\\text { subject to } \\quad y \\in \\arg \\min _{y^{\\prime}} f^{L}\\left(x, y^{\\prime}\\right)\n\\end{aligned}$$\nwhere U and L mean upper level and lower level respectively, $x \\in S \\subseteq \\mathbb{R}^{m}, y \\in \\mathbb{R}^{n}$, and\n$f^{U}, f^{L}: \\mathbb{R}^{m+n} \\rightarrow \\mathbb{R}$. This process can be viewed as a leader-follower game. The lower level\nfollower always optimizes its own utility function $f^{L}$ based on the leader's action. And the upper\nlevel leader will determine its optimal action for utility function $f^{U}$ with the knowledge of follower's\npolicy.\nGenerally, the solutions to bilevel optimization can be classified into three categories. The first\napproach tries to explicitly derive the function of y with respect to x from the lower level optimization\nproblem, turning the upper level optimization into unconstrained optimization $\\min _{x} f^{U}(x, y(x))$,\nwhich is easy to solve through analytic methods or numerical methods like gradient decent.\nThe second approach solves the problem by replacing the lower level problem with equivalent forms,\nlike inequalities or KKT conditions. For example, the Equation (1) can be equivalently represented\nas $\\min _{x} f^{U}(x, y)$ subject to $f^{L}(x, y) \\leq\\left(f^{L}\\right)^{*}$, where $\\left(f^{L}\\right)^{*}$ is the minimum of $f^{L}(x, y)$.\n\nThe last approach is gradient-based, which draws our attention. The gradient of the upper level\nfunction $f^{U}(x, y)$ with respect to x can be written as $\\frac{d f^{U}}{d x}=\\frac{\\partial f^{U}}{\\partial x}+\\frac{\\partial f^{U}}{\\partial y} \\frac{d y}{d x}$. Since $\\frac{\\partial f^{U}}{\\partial x}$ and $\\frac{\\partial f^{U}}{\\partial y}$\nare known, if we can calculate (or approximate) the gradient of y to x, i.e. $\\frac{d y}{d x}$, we will be able\nto optimize the upper level function through gradient decent. Note that the gradient is calculated\nat $\\left(x, y^{*}\\right)$ due to the lower level constraint. The method we propose in the following section will\nprovide how to calculate $\\frac{\\partial f^{U}}{\\partial x}$ in the case of trilevel and n-level."}, {"title": "2.2 Implicit Differentiation Method", "content": "As mentioned above, we can see that the key to getting the gradient is to obtain the derivative of y\nwith respect to x, because $f^{U}$ is known and the partial derivatives of $f^{U}$ with respect to x and y can be\ncalculated easily. One intuitive idea is to utilize the function $f^{L}(x, y)$ to get the relationship between\nx and y using chain rule [28]: we know that the constraint that we need to minimize $f^{U}(x, y)$ can\nprovide the relationship $y=y(x)$ so that we can replace y with x in $f^{U}$."}, {"title": "2.3 Procedures and Mathematical Formulation of Multilevel Optimization", "content": "As discussed in 2.1, bilevel optimization can be viewed as a leader-follower game. The follower\nmakes his own optimal decision y(x) for each fixed decision x of the leader. With the optimal\ndecision y(x) of the follower, the leader can select x that maximizes his utility, which makes $f(x, y)$\nto the minimum here.\nEssentially, this can be considered as a special case of sequential game with two players. In sequential\ngames, players choose strategies in a sequential order, hence some may take actions first while others\nlater. When there are 3 players making decisions in a specific order, it can be seen as leaders of\ndifferent levels. Specifically, the third player takes $\\arg \\min _{z} f_{3}(x, y, z)$ given fixed x and y. The\nsecond player takes $\\arg \\min _{y} f_{2}(x, y, z)$ given fixed x, with perfect understanding of how z will\nchange according to y. The first player gives $\\arg \\min _{x} f_{1}(x, y, z)$, knowing fully how y and z\nchange according to x.\nGenerally, the mathematical formulation of n-level optimization problem can be expressed as follows:\n$\\begin{aligned}\n&\\min f_{1}\\left(x_{1}, x_{2}, \\ldots\\right) \\\\\n& x_{1} \\in S \\\\\n& \\text { s.t. } x_{2}^{*}=\\arg \\min _{x_{2}} f_{2}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right) \\\\\n& \\ldots \\\\\n& \\text { s.t. } x_{n}^{*}=\\arg \\min _{x_{n}} f_{n}\\left(x_{1}, x_{2}, \\ldots x_{n}\\right)\n\\end{aligned}$$\nGiven the formal definition of multilevel optimization, we will propose our method for computing\nthe full gradient of upper-level problems, and provide theoretical analysis along with experimental\nresults in the following sections."}, {"title": "3 Gradient-based Optimization Algorithm for Multilevel Optimization", "content": "In this section, we will propose a new algorithm for trilevel optimization, and extend the results to\ngeneral multilevel optimization."}, {"title": "3.1 The gradient of Trilevel Optimization", "content": "We start by considering the solution of lower level problems\n$\\begin{aligned}\ng(x, y) &=\\arg \\min _{z \\in \\mathbb{R}^{d_{3}}} f_{3}(x, y, z) \\\\\nh(x) &=\\arg \\min _{y \\in \\mathbb{R}^{d_{2}}} f_{2}(x, y, g(x, y))\n\\end{aligned}$$\nIn this work, we focus on the case where the lower-level solution g, h are singletons, which covers\na variety of learning tasks [7, 23]. We further assume that $f_{2}$ and $f_{3}$ are strictly convex so that\n$\\operatorname{det}\\left(\\frac{\\partial^{2}}{\\partial z^{2}} f_{3}\\right)>0$ and $\\operatorname{det}\\left(\\frac{\\partial^{2}}{\\partial y^{2}} f_{2}\\right)>0$, but do not require g(x, y) or h(x) to have a closed-form formula."}, {"title": "3.2 The gradient of Multilevel Optimization", "content": "Although by far the discussion has focused on trilevel optimization, our method is not confined\nto this scope. Previously, we consolidated all relevant information from the lower-level problem,\nthen applied implicit differentiation to the upper-level problem. Next, we propose an algorithm that\nemploys recursion to address the general multilevel optimization problem.\nConsider an n-level optimization problem where $f_{1}, f_{2}, \\ldots, f_{n}$ are smooth func-\ntions. Assuming the existence of an algorithm capable of solving the (n - 1)-level problem, which\nyields solutions for $\\frac{\\partial x_{i}}{\\partial x_{1}}$ and $\\frac{\\partial x_{i}}{\\partial x_{j}}$ for every $2<j<i$, we can apply the chain rule as follows:\n$\\frac{d x_{i}}{d x_{1}}=\\sum_{\\atop{1<j<i}} \\frac{d x_{i}}{d x_{j}} \\frac{\\partial x_{j}}{\\partial x_{1}}=\\sum_{P \\in P(1, i)} \\prod_{\\atop{j=1}}^{P-1} \\frac{\\partial x_{j+1}}{\\partial x_{j}}=\\sum_{\\atop{2<j<i}} \\prod_{\\atop{j=1}} \\frac{\\partial x_{i}}{\\partial x_{j}} \\frac{\\partial x_{j}}{\\partial x_{1}}$$\n$\\frac{\\partial x_{1}}{\\partial x_{1}}$\nIt is noteworthy that $\\frac{\\partial x_{i}}{\\partial x_{1}}$ for $i>2$ can be computed by solving an (n - 1)-level problem, by treating\n$x_{2}$ as a constant. Consequently, the primary challenge that remains is determining $\\frac{\\partial x_{2}}{\\partial x_{1}}$. Once we we have\nthe partial derivative $\\frac{\\partial x_{2}}{\\partial x_{1}}$, the full derivative $\\frac{d x_{i}}{d x_{1}}$ of any i can be easily calculated via Equation (2).\nAs $x_{2}$ minimizes $f_{2}$, we denote this condition as $\\frac{\\partial f_{2}}{\\partial x_{2}}=0$. Consequently,\n$\\begin{aligned}\n\\frac{d f_{1}}{d x_{1}} &=\\frac{\\partial f_{1}}{\\partial x_{1}}+\\frac{\\partial f_{1}}{\\partial x_{2}} \\frac{\\partial x_{2}}{\\partial x_{1}}+\\sum_{i=3}^{n} \\frac{\\partial f_{1}}{\\partial x_{i}} \\frac{d x_{i}}{d x_{1}} \\\\\n0 &=\\frac{d f_{2}}{d x_{1}}=\\frac{\\partial f_{2}}{\\partial x_{1}}+\\frac{\\partial f_{2}}{\\partial x_{2}} \\frac{\\partial x_{2}}{\\partial x_{1}}+\\sum_{i=3}^{n} \\frac{\\partial f_{2}}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\partial x_{2}} \\frac{\\partial x_{2}}{\\partial x_{1}}+\\sum_{\\atop{i=3}}^{n} \\sum_{\\atop{j=3}}^{i} \\frac{\\partial f_{2}}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\partial x_{j}} \\frac{\\partial x_{j}}{\\partial x_{1}}\n\\end{aligned}$$\n$\\frac{\\partial f_{2}}{\\partial x_{2}}=0$\nTherefore, $\\frac{\\partial x_{2}}{\\partial x_{1}}$ can be derived as:\n$\\frac{\\partial x_{2}}{\\partial x_{1}}=-\\left(\\frac{\\partial^{2} f_{2}}{\\partial x_{2}^{2}}\\right)^{-1}\\left(\\frac{\\partial f_{2}}{\\partial x_{1}}+\\sum_{i=3}^{n} \\sum_{j=3}^{i}\\left(\\frac{\\partial f_{2}}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\partial x_{j}}\\right) \\frac{\\partial x_{j}}{\\partial x_{1}}\\right)$$\nFurthermore, if $f_{2}$ is strictly convex in $x_{2}$, $\\frac{\\partial^{2} f_{2}}{\\partial x_{2}^{2}}$ represents the Hessian matrix, which is assured to\nbe positive definite. Consequently, the equation can be resolved by applying matrix inverse to the\nHessian, which can be accelerated via many algorithms like conjugated gradients [14].\nWe introduce an algorithm (see Algorithm 1) designed to compute $\\frac{\\partial x_{i}}{\\partial x_{1}}$ and $\\frac{\\partial x_{i}}{\\partial x_{j}}$ for all i\nand j. This enables us to determine the gradient $\\frac{d f_{1}}{d x_{1}}$ using the chain rule:\n$\\frac{d f_{1}}{d x_{1}}=\\frac{\\partial f_{1}}{\\partial x_{1}}+\\sum_{j=1}^{n} \\frac{\\partial f_{1}}{\\partial x_{j}} \\frac{\\partial x_{j}}{\\partial x_{1}}$"}, {"title": "4 Theoretical Analysis", "content": ""}, {"title": "4.1 Complexity of Calculating Gradients", "content": "Denote $d_{i}$ as the dimension of vector $x_{i}$, and $d=\\max \\left(d_{1}, \\ldots, d_{n}\\right)$. The calculation of $\\frac{d f_{1}}{d x_{1}}$ involves\ncalling Algorithm 1 and applying the chain rule."}, {"title": "4.2 Convergence Analysis", "content": ""}, {"title": "4.2.1 Convergence for Multilevel Optimization", "content": "When analyzing the convergence of gradient-based optimization algorithm for multilevel problems,\nwe find it difficult to estimate the difference between the $f_{1}$ value in step N and the optimal value.\nTo address this, we shifted our focus to the analysis of the average derivatives. Specifically, we aimed\nto demonstrate that the gradient $\\frac{d f_{1}}{d x_{1}}$ diminishes progressively throughout the updating process of $x_{1}$.\nConsequently, we present the following theorem:\nAssume that $f_{i}$ for multilevel optimization is continuous with $i$-th order derivatives. If\nthe second-order derivative of $f_{1}\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$ with respect to $x_{1}$, i.e. the Hessian matrix, $\\frac{\\partial^{2} f_{1}}{\\partial x_{1}^{2}}$ is\npositive definite and the maximum eigenvalue over every $X_{1}$ is bounded, then we have\n$\\mathbb{E}\\left(\\frac{\\partial f_{1}}{\\partial x_{1}}\\right)^{2} \\leq O\\left(\\frac{1}{N}\\right)$"}, {"title": "4.2.2 Convergence for General Cases", "content": "When the domain $S$ of $x_{1}$ is a compact convex set, according to the theorem 4.1, gradient-based\noptimization algorithm converges to the optimal value. We point out that in general, the algorithm\nalso converges. For simplicity, we present the results in the case of scalar form."}, {"title": "5 Experiments", "content": "In the previous section, we detailed the process for computing the full gradient concerning the first\ninput. However, it remains crucial to evaluate whether this approach of computing the full gradient\noffers advantages over alternative methodologies. To this end, in the current section, we apply our\nmethods to a meticulously designed experiment and further conduct hyperparameter optimization,\nfollowing the guidelines and procedures outlined in Sato et al. [29]. This comparative analysis\naims to demonstrate the efficacy and efficiency of our gradient computation method within practical\napplication scenarios."}, {"title": "5.1 Experimental Design", "content": "The Stackelberg model, established in the work of Stackel-\nberg and Peacock [31], describes a hierarchical oligopoly framework where a dominant firm, referred\nto as the leader, first decides its output or pricing strategy. Subsequently, the remaining firms, labeled\nas followers, adjust their strategies accordingly, fully aware of the leader's decisions. We extend\nthis model to a trilevel hierarchy by introducing a secondary leader who is fully informed of the\nprimary leader's actions. The outputs of the first leader, second leader, and follower are represented\nby x, y, and z, respectively. We consider a demand curve defined by $P=1-x-y-z$, thereby\nincorporating the interactive dynamics between multiple leaders and a follower within the model's\nstrategic framework.\nThe loss functions are defined as:\n$\\begin{aligned}\nf_{1}(x, y, z) &=-x^{\\top}(1-x-y-z), \\\\\nf_{2}(x, y, z) &=-y^{\\top}(1-x-y-z), \\\\\nf_{3}(x, y, z) &=-z^{\\top}(1-x-y-z) .\n\\end{aligned}$$\nThe optimal solution for this generalized model can be analytically determined as $x=\\frac{1}{2}$\nIn this experiment, we follow Sato et al. [29] and consider an\nscenario where a learner optimizes the hyperparameter A to derive a noise-robust model,\nwhile an attacker poisons the training data to make it inaccurate. The model is formulated as follows:\n$\\begin{aligned}\nf_{1}(\\lambda, P, \\theta) &=\\frac{1}{m}\\left\\|Y_{\\text {val }}-X_{\\text {val }} \\theta\\right\\|_{2}^{2}, \\\\\nf_{2}(\\lambda, P, \\theta) &=\\frac{1}{n}\\left\\|Y_{\\text {train }}-\\left(X_{\\text {train }}+P\\right) \\theta\\right\\|_{2}^{2}+\\frac{c}{n d}\\|P\\|_{3}^{3}, \\\\\nf_{3}(\\lambda, P, \\theta) &=\\frac{1}{n}\\left\\|Y_{\\text {train }}-\\left(X_{\\text {train }}+P\\right) \\theta\\right\\|_{3}^{3}+\\exp (\\lambda)\\|\\theta\\|{1}\n\\end{aligned}$$\nWhere $X_{\\text {train }}$ and $X_{\\text {val }}$ represent the feature sets of the training and validation datasets, respectively,\nand $Y_{\\text {train }}$ and $Y_{\\text {val }}$ denote the corresponding target values for these datasets. The symbols $m$\nand $n$ indicate the sizes of the validation and training datasets, respectively, while $d$ represents\nthe dimensionality of the features, and $\\epsilon$ signifies the penalty imposed on the attacker. For a"}, {"title": "Experiment Setting", "content": "In our experimental setup, we approach the solution of the low-level optimiza-\ntion problem following Sato et al. [29]. Specifically, this entails performing 30 updates on y for every\nupdate on x, and 3 updates on z for every update on y. We employ the Adam optimizer [16] for x\nwith $\\beta_{1}=0.5, \\beta_{2}=0.999$. For faster convergence, we set the learning rate at step t to $0.1 \\times 0.9^{t}$.\nFor y and z, we utilize standard gradient descent, setting all learning rates to $10^{-2}$. Specifically in\nour method, we use 3 iterations of conjugate gradient method [14] to get rid of the matrix inverse in\nline 9 of Algorithm 1.\nTo assess the efficacy of our approach, we draw comparisons with Vanilla Gradient Descent (VGD)\n(which involves taking only x's partial derivative), Finite Difference (FD) [19], and ITD [29]. For the\nfirst experiment, which has an analytical solution, we measure performance based on the mean square\nerror (MSE) against the ground truth. In the second experiment, we further conduct an inference\nrun, optimizing y and z until convergence, and subsequently report on the numerical evaluation of\n$f_{1}\\left(x, y^{*}, z^{*}\\right)$. All the experiments are run on a single A100 GPU."}, {"title": "5.2 Experimental Results", "content": ""}, {"title": "Generalization of Stackelberg's model", "content": "We test all four methods on this model. The result is shown in Figure 1, where different curves\ncorrespond to different methods with x-axis representing the optimization step of x, and the y-axis\nrepresenting the MSE to ground truth. The empirical findings verify the effectiveness of our proposed\nmethod, and clearly demonstrate the advantage compared to other alternatives."}, {"title": "Hyperparameter optimization", "content": "We evaluated these methods on regression tasks using the red and\nwhite wine quality datasets, as described in Cortez et al. [4], adopting the approach outlined in Sato\net al. [29]. Each feature and target within the datasets was normalized prior to analysis. The outcomes\nof this evaluation are presented in Figure 2. We omitted the VGD method from our testing because\nthe loss function $f_{1}$ does not explicitly include $\\lambda$. Unlike other optimization algorithms that tend to\nget trapped in local minima, our method exhibits consistent improvement across iterations, surpassing\nthe performance of the baseline methods. Additionally, the ratio of computational time relative to the\nvanilla method is presented in Table 1."}, {"title": "6 Related Work", "content": "A majority of work in this field tackle on the simplified bilevel cases. Our work draw inspiration\nfrom Gould et al. [12], which belongs to the school of first-order methods. Since then, more modern\ntechniques have been proposed with stochastic nature as momentum recursive [5, 33] or variance\nreduction [25, 18] for optimizing computational complexity. Inspired by the technique leveraged in\nsolving bilevel optimization problems, a wide array of machine learning applications flourish, such\nas hyperparameter optimization [1, 9, 22], reinforcement learning [17, 27, 15], neural architecture\nsearch [19, 35], meta-learning [8, 10, 26], and so on. Yet these works still leave a lot of room for\nimprovement. For example, Liu et al. [19] leverages finite difference for neural architecture search,\nwhich is in essence a zero-order optimization method not utilizing the information to its full.\nRecently more attempts have been made towards trilevel or multilevel problems [32, 30]. Inspired by\nthe success of iterative differentiation in bilevel optimization, Sato et al. [29] proposed an approximate\ngradient-based method with theoretical guarantee. Nevertheless, the convergence analysis only holds\nasymptotically, hence requiring a considerably large iteration number in lower level, making it\nunsuitable for practical applications. There have also been efforts on the engineering side [13, 2].\nBlondel et al. [2] proposed a modular framework for implicit differentiation, but failed to support\nmultilevel optimization due to override of JAX's automatic differentiation, which integrates the chain\nrule. Choe et al. [3] developed a software library for efficient automatic differentiation of multilevel\noptimization, but differs from our focus in that we propose the method for composing best-response\nJacobian, which serves as the premise of their work."}, {"title": "7 Conclusion", "content": "In this study, we introduce an automatic differentiation method tailored for the gradient\ncomputation of trilevel optimization problems. We also develope a recursive algorithm designed for\ndifferentiation in general multilevel optimization scenarios, providing theoretical insights into the\nconvergence rates applicable to both trilevel configurations and broader n-level contexts. Through\ncomprehensive testing, our approach has been demonstrated to outperform existing methods signifi-\ncantly."}, {"title": "Limitations and Future Directions", "content": "The effectiveness of our algorithm is most evident within\nsimpler systems and when handling a smaller number of levels n. While it yields relatively precise\ngradients for smooth and convex objective functions, challenges arise in optimizing non-convex\nfunctions, potentially leading to non-invertible Hessian matrices. Moreover, the efficiency of our\ngradient computation diminishes as the number of levels n and the dimensionality of the features d\nincrease, due to the computational complexity scaling at $O\\left(d^{3} n^{4}\\right)$. Identifying heuristic methods to\napproximate gradients more efficiently remains for future exploration."}, {"title": "A Theoretical Proofs", "content": "The proof of lemma 2.1: Let $f: \\mathbb{R}^{m+n} \\rightarrow \\mathbb{R}$ be a continuous function with second derivatives and\n$\\operatorname{det}\\left(\\frac{\\partial^{2} f}{\\partial y^{2}}\\right) \\neq 0$. Let $g(x)=\\arg \\min _{y} f(x, y)$. Then the derivative of g with respect to x is\n$\\frac{d g(x)}{d x}=-\\left(\\frac{\\partial^{2} f}{\\partial y^{2}}(x, g(x))\\right)^{-1} \\frac{\\partial^{2} f}{\\partial x \\partial y}(x, g(x))$\nProof. First, since g(x) minimizes f with respect to y, the first order condition gives:\n$\\frac{\\partial f}{\\partial y}(x, g(x))=0$\nDifferentiating this condition with respect to x and applying the chain rule, we obtain:\n$\\frac{d}{d x}\\left(\\frac{\\partial f}{\\partial y}(x, g(x))\\right)=0$\nThis differentiation can be expanded as:\n$\\frac{\\partial^{2} f}{\\partial x \\partial y}+\\frac{\\partial^{2} f}{\\partial y^{2}} \\frac{d g}{d x}=0$\nHere, $\\frac{\\partial^{2} f}{\\partial x \\partial y}$ represents the matrix of mixed second partial derivatives of f with respect to x and y,\nand $\\frac{\\partial^{2} f}{\\partial y^{2}}$ is the Hessian matrix of f with respect to y.\nTo solve for $\\frac{d g}{d x}$, rearrange the above equation:\n$\\frac{\\partial^{2} f}{\\partial y^{2}} \\frac{d g}{d x}=-\\frac{\\partial^{2} f}{\\partial x \\partial y}$\nAssuming $\\frac{\\partial^{2} f}{\\partial y^{2}}$ is invertible (as implied by $\\operatorname{det}\\left(\\frac{\\partial^{2} f}{\\partial y^{2}}\\right) \\neq 0$ ), we multiply both sides by the inverse of\nthis matrix:\n$\\frac{d g}{d x}=-\\left(\\frac{\\partial^{2} f}{\\partial y^{2}}\\right)^{-1} \\frac{\\partial^{2} f}{\\partial x \\partial y}$\nThis completes the proof.\nLet $f: \\mathbb{R}^{d_{1}+d_{2}+d_{3}} \\rightarrow \\mathbb{R}$ be a continuous function with second derivatives. Let $g(x, y)=$\n$\\arg \\min _{z \\in \\mathbb{R}^{d_{3}}} f(x, y, z)$, then the following properties about g holds:\n$\\begin{aligned}\n&\\frac{\\partial g}{\\partial x}=-\\left(f_{z z}\\right)^{-1} f_{x z} \\\\\n&\\frac{\\partial g}{\\partial y}=-\\left(f_{z z}\\right)^{-1} f_{y z}\n\\end{aligned}$$\nProof. To find $\\frac{\\partial g}{\\partial x}$, differentiate the first order condition with respect to x using the chain rule:\n$\\frac{d}{d x}\\left(\\frac{\\partial f}{\\partial z}(x, y, g(x, y))\\right)=0$\nExpanding this using the chain rule yields:\n$\\frac{\\partial^{2} f}{\\partial x \\partial z}+\\frac{\\partial^{2} f}{\\partial z^{2}} \\frac{\\partial g}{\\partial x}=0$"}]}