{"title": "High Resolution Seismic Waveform Generation using Denoising Diffusion", "authors": ["Andreas Bergmeister", "Kadek Hendrawan Palgunadi", "Andrea Bosisio", "Laura Ermert", "Maria Koroni", "Nathana\u00ebl Perraudin", "Simon Dirmeier", "Men-Andrin Meier"], "abstract": "Accurate prediction and synthesis of seismic waveforms are crucial for seismic hazard assessment and earthquake-resistant infrastructure design. Existing prediction methods, such as Ground Motion Models and physics-based simulations, often fail to capture the full complexity of seismic wavefields, particularly at higher frequencies. This study introduces a novel, efficient, and scalable generative model for high-frequency seismic waveform generation. Our approach leverages a spectrogram representation of seismic waveform data, which is reduced to a lower-dimensional submanifold via an autoencoder. A state-of-the-art diffusion model is trained to generate this latent representation, conditioned on key input parameters: earthquake magnitude, recording distance, site conditions, and faulting type. The model generates waveforms with frequency content up to 50 Hz. Any scalar ground motion statistic, such as peak ground motion amplitudes and spectral accelerations, can be readily derived from the synthesized waveforms. We validate our model using commonly used seismological metrics, and performance metrics from image generation studies. Our results demonstrate that our openly available model can generate distributions of realistic high-frequency seismic waveforms across a wide range of input parameters, even in data-sparse regions. For the scalar ground motion statistics commonly used in seismic hazard and earthquake engineering studies, we show that the model accurately reproduces both the median trends of the real data and its variability. To evaluate and compare the growing number of this and similar 'Generative Waveform Models' (GWM), we argue that they should generally be openly available and that they should be included in community efforts for ground motion model evaluations.", "sections": [{"title": "1 Introduction", "content": "The study and prediction of earthquake ground motions are central to seismology. Wavefield models across scales and frequencies are required to assess seismic hazard and the response of critical infrastructure to ground motion. State-of-the-art seismic hazard models use empirical ground motion models (GMMs) to estimate the expected level of ground shaking (i.e., intensity measures) at a site given an earthquake and site properties information. Other applications require the prediction of full-time histories of ground motion at sites of interest. An important example is nonlinear structural dynamic analysis and performance-based earthquake engineering (Chopra, 2007; Applied Technology Council, 2009; Smerzini et al., 2024)."}, {"title": "2 Methods", "content": "Our approach to generating high-resolution seismic waveforms with a latent diffusion model comprises three primary components. Initially, we transform the seismic waveforms into spectrogram representations, which are more amenable to generative modeling than time-domain signals. Subsequently, we employ a convolutional variational autoencoder to compress these high-dimensional spectrograms into a lower-dimensional latent space. Third, we train a denoising diffusion model based on a U-Net architecture to generate samples within this latent space, which are then mapped back to the spectrogram representation and converted to waveforms during inference. In this section we provide a comprehensive description of each component of the generative pipeline (Figure 1). Detailed explanations of the neural network architectures, which we omit here for brevity, are given in Appendix A1."}, {"title": "2.1 Spectrogram Representation", "content": "Processing seismograms directly with neural networks is challenging due to their high-frequency content and amplitude variance, both within and across samples. This issue is particularly problematic in generative modeling, where high-amplitude areas dominate the loss function, compromising reconstruction accuracy in low-amplitude regions. To address these challenges, we transform the waveform into a spectrogram representation, a technique commonly used in seismology and in audio signal generation where typically waveforms of much higher frequency content are modeled (Kong et al., 2021; D\u00e9fossez et al., 2023). Additional details on the spectrogram transformation and inversion process are provided in Appendix A2."}, {"title": "2.2 Departure to Latent Space", "content": "The stochastic nature of the training objective in denoising diffusion models (see Section 2.3), along with their iterative sampling process, makes both training and inference computationally intensive, particularly with high-dimensional data like high-frequency seismic waveforms. To mitigate this, we adopt the two-stage approach of Rombach et al. (2022). First, we train an autoencoder to compress the data into a more manageable, lower-dimensional latent space, then use denoising diffusion to generate the latent variables. This combines the autoencoder's data compression efficiency with the generative capabilities of denoising diffusion models.\nFormally, let $P_{data}$ denote the data distribution density. We model the distribution over latent variables $z \\in \\mathbb{R}^m$ as a mixture of Gaussians:\n$P_{enc}(z) = \\mathbb{E}_{P_{data}(x)} P_{enc}(z|x) =  \\mathbb{E}_{P_{data}(x)} \\mathcal{N}(z | \\mu_{\\phi}(x), \\text{diag}(\\sigma_{\\phi}(x)))$ \t{(1)}\nHere, the encoder is defined by its mean and standard deviation functions $\\mu_{\\phi}, \\sigma_{\\phi}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, parameterized by a neural network with parameters $\\phi$. The network has two output heads: one for the mean and one for the standard deviation. A deterministic decoder, $G_{\\psi}: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, maps latent variables back to the original data space. The encoder and decoder are trained jointly to minimize the reconstruction loss\n$\\mathbb{E}_{P_{enc}(z|x)} ||x - G_{\\psi}(z) ||^2 $\\newline \t{(2)}\nover the data distribution. Additionally, we regularize the latent space by the Kullback-Leibler divergence between the encoder distribution and a standard normal distribution. Since we employ the autoencoder solely for data compression rather than as a generative model, we set the regularization strength to a tiny value (1e-6) to ensure high reconstruction quality."}, {"title": "2.3 Denoising Diffusion", "content": "After compressing the data into a compact latent space, we employ denoising diffusion models (DDMs; Song and Ermon (2019); Ho et al. (2020); Song et al. (2021)) to generate the latent representations. DDMs have gained popularity due to their outstanding performance in tasks such as image, audio, or video generation. Unlike previous techniques, DDMs do not rely on approximate variational inference, which can produce blurry samples as in variational autoencoders (Kingma & Welling, 2014), or adversarial training, which can be unstable and suffer from mode collapse (Goodfellow et al., 2014). Additionally, they do not require restricted architectures like normalizing flows (Rezende & Mohamed, 2015).\nDDMs are characterized by a pair of stochastic processes: a fixed forward noising process and a learnable backward denoising process. The forward process progressively adds noise to the data distribution until it resembles an isotropic Gaussian. Conversely,\ndz_t = \\mu_t(z_t) dt + \\sigma_t dw, \\newline \t{(4)}\ndz_t = [\\mu_t(z_t) - \\sigma_t^2 \\nabla_{z_t} \\log p_t (z_t|c)] dt + \\sigma_t d\\bar{w}, \\newline \t{(5)}\ndz_t = [\\mu_t(z_t) - \\frac{1}{2} \\sigma_t^2 \\nabla_{z_t} \\log p_t (z_t|c)] dt. \\newline \t{(6)}\ndz_t = \\sqrt{2t} dw,  dz_t = -2t\\nabla_{z_t} \\log p_t(z_t|c) dt + \\sqrt{2t} d\\bar{w}, \\newline \t{(7)}\ndz_t = -t \\nabla_{z_t} \\log p_t (z_t|c) dt. \\newline \t{(8)}\nV_{z_t} \\log p_t (z_t| z_0, c) = \\frac{z_0 - z_t}{t^2} \\\n$\\mathcal{L}(\\theta) = \\mathbb{E} ||D_{\\theta}(z_t, t, c) - z_0||^2,$ \t{(9)}"}, {"title": "3 Data and Model Training", "content": "We use three-component strong motion waveforms recorded from 1996 to 2022 by K-NET and KiK-net stations in Japan, provided by the National Research Institute for Earth Science and Disaster Resilience (NIED, 2019). Before training our model, we preprocess the raw data by removing the scalar gain factor. We apply a causal 1 Hz Butterworth high-pass filter of order 2 and resample the data by interpolation to a common time vector to achieve a uniform sampling rate of 100 Hz. The waveforms are then truncated to 40 seconds in length, with the P-wave arrival (P-pick onset) set to approximately 5 seconds after the starting time of the trimmed waveform. We consider shallow crustal events with hypocentral depths of < 25 km (classified as faulting type 1), and subduction events with depths > 25 km (classified as faulting type 0). We use all available events with magnitudes M\u2265 4.5. The minimum and maximum station distances to the hypocenter are 1 km and 180 km, respectively. Vs30 is available for most station metadata; we exclude all stations without Vs30 values. The Vs30 values range from 76 to 2100 m/s. In total, we use 197,370 three-component records.\nWe divide the available data into 90% for training and 10% for testing to facilitate hyperparameter tuning. Because of the limited number of observations\u2014especially for large-magnitude earthquakes and short-distance recordings\u2014we use all available data for model performance evaluation, except when noted otherwise. Training of our model was conducted on a single NVIDIA A100 GPU, requiring approximately 38 hours for the first-stage autoencoder and an additional 15 hours for the second-stage diffusion model."}, {"title": "4 Results", "content": "The design goal of the Generative Waveform Model (GWM) is to synthesize ground motion records that are statistically indistinguishable from real records, across a wide range of frequencies and conditioning parameters, namely source magnitudes, hypocentral distances, Vs30, and faulting type. In the following section, we discuss the extent"}, {"title": "4.1 Time domain signal envelopes", "content": "To compare the real and synthetic waveforms quantitatively, we compute signal envelope time series for both sets. The signal envelopes are obtained by taking the moving average of the absolute waveform signals with a kernel size of 128, followed by a logarithmic transformation. This comparison shows that the GWM synthetics have very similar first-order shapes in the time domain compared to the real seismograms, across the entire range of magnitudes and recording distances for which the model was trained (Figures 3a and 3b). The low-noise amplitudes before the P-wave onsets are followed by an impulsive P-wave amplitude increase. This amplitude growth is similar for both small and large magnitudes until the smaller magnitude records reach their maximum P-wave amplitude, whereas the large magnitude records continue to grow. The later-arriving S- and surface waves cause additional amplitude growth in the real waveforms, which is accurately mimicked by the GWM synthetics. The variability of the envelopes in each bin is symmetric around the median in log-space and is of the same order for both real and synthetic data. Additional figures in the supplementary material show different bins, and separate evaluations of North-South and vertical components (supplementary Figures S1 to S7)."}, {"title": "4.2 Fourier amplitude spectra", "content": "Similarly, we compare the logarithmic Fourier amplitude spectra of GWM synthetic data with those of real data. These spectra are obtained by performing a Fourier transform of the time-domain signals, calculating the magnitudes of the resulting complex values, and then applying a logarithmic transformation. Figures 3c and 3d illustrate these comparisons, with similar distributions in terms of mean log-amplitudes and variability. Equivalent evaluations for specific parameter bins are shown in supplementary Figures S2 to S7. Additionally, Section 4.5 discusses the use of Fr\u00e9chet distance to compare distributions of log-amplitude spectra for real data and GWM synthetics."}, {"title": "4.3 Scalar peak amplitude statistics", "content": "For earthquake engineering and seismic hazard applications, peak ground motion amplitude statistics are of particular importance. Here, we compare how various peak amplitude statistics of the GWM synthetics compare with the real data, how they correlate with the conditioning predictor variables, and how they compare with predictions from widely used Ground Motion Models (GMMs). Specifically, we compute peak ground acceleration (PGA), peak ground velocity (PGV), and pseudo-spectral acceleration (SA) for both real data and GWM synthetics. We use the orientation-independent GMRotD50 statistic (Boore et al., 2006), which represents the median of the horizontal components, rotated over all possible rotation angles.\n$\\log_{10} (PGA) =0.4840 +0.4274 \\times M \u2013 0.3642 \\times \\log_{10} (V_{S30})$ \n\\newline \t{(10)}\n$\\log_{10} (PGV) = - 0.9914 +0.5392 \\times M \u2013 0.6481 \\times \\log_{10} (V_{S30})$ \n\\newline \t{(11)}"}, {"title": "4.3.1 Accuracy of predicted peak amplitudes", "content": "For each record in the real waveform dataset, we compute a single GWM synthetic waveform, using the conditioning parameter of the real data (magnitude, hypocentral distance, Vs30 and faulting type). Comparing the PGA and PGV values measured on the GWM synthetics with the real data shows that the GWM predictions are at least as accurate as those from the GMMs and have similar prediction variability (Figure 4).\nSpecifically, we compute the logarithm of the ratio between observed and predicted peak amplitudes. For the GWM synthetics, the mean of the distribution of this log-ratio (the model bias) is close to zero for both PGA and PGV: the mean model bias across all distances is 0.08 log-units for PGA and 0.07 log-units for PGV. This corresponds to an underprediction by 20% and 17%, respectively. At very short hypocentral distances (<20 km), the GWM tends to underpredict the real data more strongly, by about 45% (0.16 logarithmic units, Figures 4a and 4b).\nIn comparison, the GMM by Boore et al. (2014) underestimates the real PGA for distances > 50 km by 21%, and by 48% at < 20 km (Figure 4c). For PGV, this GMM has a low average model bias of only 17%, except at very short distances, where it is similar to the GWM (Figure 4d). Similarly, the GMM by Kanno et al. (2006) underpredicts PGA by an average of 78% (Figure 4e) across all distances and by 151% for distances < 20 km. It very accurately predicts PGV (1% under-prediction across all distances) (Figure 4f), except at short distances (< 20 km), where the under-prediction amounts to 151%. These comparisons indicate that the peak amplitude predictions of the GWM are largely unbiased, except at very short hypocentral distances where all models exhibit reduced performance."}, {"title": "4.3.2 Variability of predicted peak amplitudes", "content": "Another important criterion for ground motion prediction methods is that the variability in predicted peak amplitudes is accurately characterized. To evaluate the variability of the GWM predictions, we compare their total standard deviation to the variability in the real data, and to the predictions of the two GMMs.\nTo measure prediction residuals, we fit a simple, custom GMM to the PGA and PGV of the real data, as a function of magnitude M, hypocentral distance R, and Vs30. We use ordinary least squares and find"}, {"title": "4.3.3 Shaking durations", "content": "Shaking duration is another signal characteristic that is important for earthquake engineering applications. We use the cumulative Arias Intensity (cAI) metric (Arias, 1970) to compare the significant shaking duration of the GWM synthetics and of the real data. The significant shaking duration is defined by the times corresponding to 5% and 95% of the maximum cAI. Figure 6a shows the cAI curve for an example record from the real data set with M 5.5, recorded at R = 50 km, on a site with Vs30 = 500 m/s, and faulting type 1, along with the cAI curves from 100 GWM realizations with the same conditioning parameters. When we compute 1 GWM realization for each real record of the entire data set (Figure 6b), we find very similar duration distributions between real data and GWM synthetics, for magnitudes < 7.4. For larger magnitudes, the 40 seconds length of the generated seismograms is not sufficient to capture the full ground motion time history. An equivalent figure that includes the largest magnitudes is given in supplementary Figure S8."}, {"title": "4.3.4 Predicting distributions of peak amplitudes", "content": "Because we can generate any number of synthetics with the GWM, we can use the model to predict distributions of ground motion statistics, much like we commonly would"}, {"title": "4.3.5 Pseudo-spectral acceleration versus hypocentral distance", "content": "We can use the GWM in this sense to directly compare the distributions it predicts with predicted distributions from the GMMs. To study, for instance, how peak amplitudes decay with distance, we compute 100 GWM synthetics for a vector of evenly spaced distances, every 0.8 km between 1 km to 180 km, and calculate the median and standard deviation of the peak amplitudes in each distance bin, for fixed magnitude, VS30 values and faulting type (Figure 8).\nFor pseudo-spectral acceleration at 0.1 and 1.0 second periods with 5% damping, the resulting GWM predictions decay smoothly, similar to the GMM predictions. This includes very short distances, where both GWM and GMMs are under-constrained by available data. For T = 0.1 s, both the GWM and the two GMMs underestimate the real data in the Vs30 = 550 - 650 m/s bin and for magnitudes in the ranges of M = 5.4 \u2013 5.6 (Figure 8a) and M = 5.9 \u2013 6.1 (Figure 8c), although the real data mean remains within the standard deviation of the GWM predictions. For longer periods, such as T = 1.0 s (Figures 8b and 8d), the GWM generally performs better than the GMMs. The GWM also matches the real data rather well in cases when the data diverge from the typical GMM decay, which is sometimes observed for distances greater than 100 km (supplementary Figures S9-S18)."}, {"title": "4.3.6 Pseudo-spectral acceleration versus magnitude", "content": "In a similar sense, we can investigate how the GWM predictions grow with magnitude. We produce 100 GWM synthetics for a vector of magnitudes, evenly spaced every 0.035 from M 4.5 to 8, for a fixed distance and Vs30 values. The predictions for SA at T = 0.1 s show relatively smooth, monotonous growth, up to a saturation at M 7.0 (Figure 9a), consistent with the (very sparse) real data. The same trend is observed for other conditioning parameter combinations (supplementary Figures S19 - S24). It is interesting, and encouraging, that the GWM predictions are well-behaved in conditioning parameter ranges where the training data are very sparse, such as at M 6.5-7.5, or for R < 20 km."}, {"title": "4.3.7 Pseudo-spectral acceleration versus Vs30", "content": "To assess the scaling of pseudo-spectral acceleration with Vs30, we generate another 100 GWM realizations for a vector of Vs30 values, evenly spaced every 13.63 m/s from"}, {"title": "4.4 Model probabilities given the data", "content": "To assess the GWM and the GMMs across a wide range of magnitude and distance combinations, we compute cumulative probabilities of the models, given the observed data. For each observed spectral acceleration value SA, the GWM and GMMs predict a Gaussian normal distribution of expected SA amplitudes, with a predicted mean \u00b5, and a standard deviation \u03c3. We can then evaluate the probability of each data point under the predicted distribution. Assuming a uniform prior distribution, this probability is equivalent to the probability of the model, given the data. By summing up these probabilities across all data points in a magnitude and distance bin, and then normalizing it with the number of data in the bin N, we can compute an average model probability\n$P^k = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi} \\sigma_k} \\exp \\big( - \\frac{(SA_i - \\mu_k)^2}{2\\sigma_k^2} \\big),$ \t{(12)}"}, {"title": "4.5 Fr\u00e9chet Distances and classifier accuracy", "content": "In addition to the commonly used seismological performance metrics, we introduce additional, novel metrics that quantify the similarity between real and generative waveform model (GWM) synthetic waveforms. The metrics are inspired by well-established"}, {"title": "4.5.1 Fr\u00e9chet Distance of Fourier amplitude spectra", "content": "We use the Fr\u00e9chet Distance (FD) to measure the distance between the Fourier amplitude spectra of the observed and synthetic waveforms. Ideally, realistic GWM synthetic waveforms should have a Fourier spectrum that is statistically indistinguishable from that of the real data. Larger FD values would indicate differences between the GWM synthetics and real signals. The FD is equivalent to the Wasserstein-2 distance, and measures the minimum effort required, in the L2 sense, to transform one distribution into another. As in section 4.3.1, we compute a GWM synthetic waveform for each real waveform in the data set, using the conditioning parameter corresponding to the real data. The amplitude spectrum of each waveform is represented by 2033 amplitude values (1/2 signal length + 1). We treat each vector element as an independent Gaussian, and compute the element-wise mean \u00b5 and standard deviation \u03c3 of the log-amplitudes for each vector element, across all waveforms. The Fr\u00e9chet Distance d between observed and synthetic waveforms is then given by:\n$d^2 = ||\\mu_{obs} - \\mu_{syn}||^2 + || \\sigma_{obs} - \\sigma_{syn}||^2.$\t{(13)}"}, {"title": "4.5.2 Classification accuracy", "content": "Inspired by the common practice in image synthesis to evaluate the quality of generated images with a pre-trained classifier (Heusel et al., 2017), we adopt a similar approach: we train a classifier to categorize seismic data into bins of magnitude and distance. We divide our dataset into five magnitude and five distance bins, resulting in 25 classes, with each class containing a similar number of samples (Appendix A3). We then train a classifier to predict the magnitude-distance bin for each record. For the classifier, we use a convolutional neural network (CNN) architecture that, like the GWM, operates on spectrogram representations of the waveforms (Appendix A1). The classifier"}, {"title": "4.5.3 Fr\u00e9chet Distance of classifier embeddings", "content": "For similar input data, not only should the classifiers' performance be comparable, but its internal representations should also align. Based on this intuition, we use the FD to measure the similarity of the classifiers' hidden representations between the real and synthetic waveforms. We collect the 256-dimensional hidden representations from the penultimate layer of the classifier for both real and synthetic waveforms. Unlike the FD of Fourier amplitude spectra (Section 4.5.1) where each dimension was treated independently, the reduced dimensionality of this representation (256 instead of 2033) allows us to compute correlations between dimensions. Thus, we calculate the entire covariance matrix \u2211 of the hidden representations for both the real and generated waveforms. The Fr\u00e9chet Distance D between the two sets of hidden representations is then given by:\n$D^2 = ||\\mu_{obs} - \\mu_{gen}||^2 + Tr(\\Sigma_{obs} + \\Sigma_{gen} \u2013 2(\\Sigma_{obs} \\Sigma_{gen})^{1/2}).$\t{(14)}\nThis is a generalization of the Fr\u00e9chet Distance in Eq. 13 to general multivariate (nonisotropic) Gaussians. In image generation literature, this metric is known as the Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017), with \"Inception\" referring to the classifier architecture employed."}, {"title": "5 Discussion", "content": "Generative Waveform Models (GWMs) are rapidly advancing and have the potential to significantly improve earthquake hazard assessment and earthquake engineering studies (Florez et al., 2022; Esfahani et al., 2023; Shi et al., 2024; Matsumoto et al., 2024). Unlike GMMs, which predict scalar ground motion metrics, GWMs can synthesize fully realistic waveforms, complete with realistic frequency- and time-domain properties.\nThe ability to predict full waveforms enables studies that rely on waveform content, such as building response simulations (Bommer & Acevedo, 2004). Any scalar ground motion metric can be derived from the predicted waveforms. A key advantage of this approach is that the waveforms and their derivatives are equally realistic across the entire frequency range (1 - 50 Hz). This may contrast with hybrid methods, which add high-frequency spectra in a separate second stage, either using stochastic methods (Saikia &"}, {"title": "5.1 This-Quake-Does-Not-Exist ('tqdne') Python Library", "content": "For the model presented in this study, we introduce an openly available and user-friendly Python library that can be used to generate waveforms using the pre-trained GWM from this study or to train custom GWMs. Generating a three-component waveform with the pre-trained GWMs takes a fraction of a second on a standard personal computer. The library facilitates saving of the waveforms in SeisBench format (Woollam et al., 2022). The library's named is inspired by the popular thispersondoesnotexist.com (2023) application, which uses the StyleGAN algorithm (Karras et al., 2019, 2020) to generate human portrait images."}, {"title": "5.2 Limitations", "content": "While the presented GWM arguably achieves high seismic waveform synthesis performance, there are several limitations, that future models can aspire to overcome.\nStochastic nature of the generated waveforms Fundamentally, the generated waveforms are stochastic representations of real seismograms. There is no underlying physical model for wave excitation and propagation. Although the GWM synthetics exhibit clear energy packets that closely resemble P-, S- and surface waves, they do not represent any wavefield phases in a deterministic sense.\nLimited training data As is the case for all models of strong ground motion, the limited number of short distance recordings of large magnitude earthquakes is a bottleneck. This limitation affects the synthesis performance of this crucial data regime. GWMs can in principle be used to augment such data sets, but it is currently an open question how well the models extrapolate beyond the parameter ranges for which they have been trained, and how well they perform at the data-scarce edges of the parameter ranges.\nPoint source assumption Our model assumes that the earthquake source is a point source and neglects finite fault source characteristics such as fault geometry and distance, source roughness, directivity, and unilateral or bilateral rupture modes.\nUncorrelated stations The current model does not explicitly take into account the correlation of observations across different records of the same quake. Each generated waveform is an independent realization of the denoising forward process. This may lead to an underestimation of the correlation of observed ground motions from the same quake, and might limit the ability of the model to generalize to new stations.\nP-wave onset times The current model has been trained with a data set of waveforms that have been aligned with a simple STA/LTA onset detector, which can be inaccurate. As a consequence, the GWM synthetics also have some variability in the P-wave onset times that is not physically meaningful.\nSignal length The 40-second long seismograms are sufficient to describe the ground motion from quakes with magnitudes of up to ~ 7.5. For even larger quakes, the source"}, {"title": "6 Conclusion", "content": "We present a data-driven, conditional generative model for synthesizing three-component strong motion seismograms. Our generative waveform model (GWM) combines a convolutional auto-encoder with a state-of-the-art latent denoising diffusion model, which generates encoded - rather than raw - spectrogram representations of the seismic signals.\nWe trained the openly available model on Japanese strong motion data with hypocentral distances of 1-180 km, moment magnitudes \u2265 4.5, and Vs30 values of 76-2100 m/s. Using a variety of commonly used and novel evaluation metrics, we demonstrate that the GWM synthetics accurately capture the statistical properties of the observed data in both the time and frequency domains, across a wide range of conditioning parameters, and up to the highest hazard-relevant frequencies.\nFurthermore, we systematically compare the peak ground motion statistics of the GWM synthetics to predictions from commonly used GMMs. The GWM predictions are largely unbiased and exhibit the same level of amplitude variability as the real data. As a result, they may be useful for practical applications, such as probabilistic seismic hazard assessment and structural dynamic analyses.\nWith GWMs, hazard models can potentially expand their scope to include applications that require full waveform representations, rather than just scalar amplitude statistics. Future community efforts to benchmark and compare GWMs would provide guidance for which models to best use in practical and scientific applications, and may accelerate GWM innovation."}, {"title": "Appendix A Generative model details", "content": "This section provides additional details on the model architectures and representations used in the experiments."}, {"title": "A1 Neural network architectures", "content": "All our models are based on the widely used U-Net architecture presented in Song et al. (2021). The U-Net consists of three components which we denote left (encoder), middle, and right (decoder). All components use several residual blocks that use two convolutional layers (2D for the spectrogram and 1D for the moving average envelope), with"}, {"title": "A2 Representations", "content": "We experiment with two different representations of the seismic data: spectrogram and moving average envelope.\nSpectrogram Representation: To transform each of the three channels in the original waveform into a spectrogram, we utilize a Short-Time Fourier Transform (STFT) with 256 frequency bins and a hop length of 32 samples. Due to the symmetry of the spectrogram, only half of the frequency bins are used. To prevent padding issues, the original waveform is truncated to 4064 samples, resulting in a complex-valued matrix of size 128x128. We then take the magnitude of this matrix and apply a logarithmic transformation to obtain the spectrogram, discarding the phase information due to its high-frequency nature, which is challenging to model accurately. To reconstruct the original waveform, we employ the Griffin-Lim algorithm (Griffin & Lim, 1984; Perraudin et al., 2013), which reliably estimates the phase from the magnitude spectrogram."}, {"title": "A3 Classifier training data binning", "content": "When training the classifier to categorize data based on earthquake magnitude and distance, we divide the data into five magnitude and five distance bins. Figure A3 displays the sample count in each bin, ensuring a balanced sample distribution across classes."}, {"title": "Appendix B Ablation Studies", "content": "This section evaluates the significance of the three components in our proposed model. The time-domain representation of seismic data shows significant amplitude variation, making direct processing of raw waveforms ineffective. Figure Bla illustrates the logarithm of the Fourier amplitude spectrum between an autoencoder's input and output when trained on raw waveforms, revealing poor reconstruction, especially in the high-frequency range. It is important to note that this is not a generative model but an autoencoder tasked to reconstruct the input. Despite this, the waveforms are poorly reconstructed, particularly in the high-frequency range.\nTo address this, we explore alternative time-domain representations. We decompose the signal into its positive envelope and residual signal, with the envelope being a smoothed version of the absolute signal, as detailed in Section A2. Figure Blb shows that an autoencoder trained on this representation performs better than one trained on raw waveforms but still struggles with high-frequency components.\nThirdly, we experiment with a spectrogram representation, specifically the log-transformed magnitude of the short-time Fourier transform. This representation is smooth and robust to amplitude variations. Figure Blc demonstrates that an autoencoder trained on this representation nearly perfectly reconstructs the original signal, with the exception of a minor underestimation of the spectral amplitude, as discussed in Section 5. This highlights the effectiveness of the spectrogram representation for the task of ground motion synthesis.\nFinally, we assess the performance of the diffusion model trained on different representations and the impact of incorporating the autoencoder stage. Table B1 summarizes our findings, showing that the spectrogram representation significantly outperforms the envelope representation across all metrics. Additionally, the autoencoder stage improves the spectral fit. Overall, the latent diffusion approach with the spectrogram representation is the most effective configuration."}, {"title": "Open Research Section", "content": "The three-component strong-motion data from the Kiban-Kyoshin (KiK-net) network time series waveforms are provided by the National Research Institute for Earth Science and Disaster Prevention of Japan and can be downloaded at https://www.bosai.go.jp/e/index.html. Preprocessed data that was used to train the model is available upon request owing to K-Net and KiK-net dataset policy.\nThe code for our generative waveform model is available on GitHub at https://github.com/highfem/tqdne/tags (Bergmeister et al., 2024). All online pages were last accessed on October 16th, 2024. The supplementary material provides additional information and figures to complement the main content of the primary text, offering a deeper understanding and further validation of the presented results."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that there are no conflicts of interest."}]}