{"title": "Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning", "authors": ["Hang Yin", "Zhifeng Lin", "3rd Xin Liu", "Bin Sun", "Kan Li"], "abstract": "Direction reasoning is essential for intelligent systems to understand the real world. While existing work focuses primarily on spatial reasoning, compass direction reasoning remains underexplored. To address this, we propose the Compass Direction Reasoning (CDR) benchmark, designed to evaluate the direction reasoning capabilities of multimodal language models (MLMs). CDR includes three types images to test spatial (up, down, left, right) and compass (north, south, east, west) directions. Our evaluation reveals that most MLMs struggle with direction reasoning, often performing at random guessing levels. Experiments show that training directly with CDR data yields limited improvements, as it requires an understanding of real-world physical rules. We explore the impact of mixdata and CoT fine-tuning methods, which significantly enhance MLM performance in compass direction reasoning by incorporating diverse data and step-by-step reasoning, improving the model's ability to understand direction relationships.", "sections": [{"title": "I. INTRODUCTION", "content": "The exploration of models' ability to understand the physical world has garnered significant attention [1]-[4]. Among these capabilities, direction reasoning stands out as a crucial cognitive skill for decision-making and navigation in the real world. This ability enables individ-uals to infer and understand the relative positions and orientations of objects in space, forming a foundation for effective communication and interaction with the environment. For intelligent systems, such as autonomous vehicles, robotics, and augmented reality, accurate direction reasoning is critical for tasks like path planning, object localization, and spatial awareness [5]-[9]. With the advancement of large language models (LLMs), they have demonstrated remarkable capabilities in object detection, image captioning, and image-based dialogue [10]-[14]. However, their ability to reason about directions, specifically compass direction reasoning that follows real-world di-rection rules, remains relatively unexplored.\nNeglecting compass reasoning limits model effectiveness in appli-cations like navigation, geographic positioning, and large-scale envi-ronment interactions [15]-[17]. Autonomous systems, for instance, must use compass-based directions, not just relative spatial ones. Without accurate compass reasoning, they risk failure in tasks like path planning or location-based services, where consistent real-world orientation is crucial. Inconsistencies between spatial and compass reasoning can also disrupt communication in human-AI interactions, where instructions often rely on absolute directions (e.g., \"head north\").\nTo explore direction reasoning, we divide it into spatial and compass directions. Based on human cognitive principles [18], [19], we consider spatial reasoning-understanding the relative positions of objects (up, down, left, right, and combinations) as the foundation for compass reasoning, which focuses on geographical directions (north, south, east, west, and combinations). While spatial reasoning is limited to internal image content, compass reasoning requires alignment with real-world direction rules, making it crucial for tasks like navigation and geographical positioning, and key to evaluating a model's ability to apply knowledge in practical scenarios.\nEarlier work uses text descriptions for spatial reasoning and path planing [5]\u2013[9]. Inspired by human cognition, wu et al. [20] introduced the Visualization-of-Thought technique to guide LLM through reasoning steps in spatial reasoning. liu et al. [21] and kamath et al. [22] collected a real-world scenario corpus of visual question answering for spatial reasoning.\nHowever, current spatial reasoning datasets primarily focus on describing the relative positions of objects within images. While models may learn these spatial relationships, there is a lack of benchmarks that evaluate whether models can understand compass directions as they apply to the real world. Compass reasoning is crucial because it reflects the fundamental orientation principles governing the physical world. Without such benchmarks, it is unclear whether models have truly internalized these real-world principles or are merely relying on learned spatial patterns from the data.\nIn this work, we propose the Compass Direction Reasoning (CDR) benchmark, designed to mainly evaluate the compass reasoning abil-ity of models. The CDR dataset contains handcrafted image-question pairs in English. Each pair includes a 2D image and corresponding direction-related questions, which have 8 possible directions and a unique correct answer. The images in the CDR dataset are symbol-based and consist of three types. The first type is icon images, where the central icon has a clear direction orientation (e.g., a finger, an arrow, etc.) with surrounding objects (e.g., a person, a flower, etc.)"}, {"title": "II. METHODOLOGY", "content": "To prevent complex object categories from influencing the model's direction reasoning, we use simple objects. We begin with a 200\u00d7200 canvas (the final image size), and a 3\u00d73 grid is drawn on it. For letter-type images, to establish relationships between the letters, a cell containing a letter (the center letter) is randomly selected, and the surrounding cells are filled with non-repeating letters, allowing for empty cells to avoid repetition. Each letter and its position are recorded to facilitate subsequent questions and automated annotation. Similarly, number-type images follow the same strategy. For icon-type images, we source high-resolution, unambiguous data from the Internet [27]. The central icon is a direction symbol (e.g., arrows, road signs, pointing fingers), and the surrounding objects include human-shaped icons, flowers and so on. We manually annotate the central icon's spatial orientation and record the spatial positions of the surrounding icons relative to it, enabling automatic label generation for question construction.\nBased on the collected images, we design prompts with a focus on simple, direct approaches to evaluate fundamental directional reasoning abilities. For evaluation, we use seven types of questioning methods"}, {"title": "III. EXPERIMENTS AND ANALYSIS", "content": "We evaluate the following models: LLaVA-v1.5-7B, LLaVA-v1.5-13B, Claude-3-Haiku (20240307), Claude-3-Sonnet (20240229), GPT-4o-mini (2024-07-18), and Gemini-1.5pro-flash. The decoding parameters used are: frequency penalty = 0.0, presence penalty = 0.0 and temperature = 0. We set the context window for the LLaVA to 1024 tokens to fine-tune, with other models maintaining their default settings. The experiments are conducted in a zero-shot setting.\nIn the classification task, all models performed well, with LLaVA-7B and LLaVA-13B achieving 97.72% and 99.47% accuracy, respec-tively. The Claude-3 series and Gemini-1.5-Pro also demonstrated high accuracy, showcasing their strong symbol classification capabil-ities. This further indicates that the models' direction reasoning is not hindered by their ability to recognize the images.\nIn spatial direction reasoning, model performance is generally poor. LLaVA-7B achieves an accuracy of just 14.08%, while LLaVA-13B improves slightly to 25.72%. Claude-3 and Gemini-1.5-Pro perform marginally better in comparison. For compass direction reasoning, results remain low. LLaVA-7B and LLaVA-13B achieve accuracy rates of 12.42% and 22.73%, respectively. In comparison, the Claude-3 series and Gemini-1.5-Pro perform slightly better, but their maximum accuracy of 62.64% is still far from ideal. This suggests that the models struggle with geographical orientation tasks, likely due to the abstract nature of these concepts, which are beyond the models' current capabilities. The introduction of the compass concept appears to further challenge the models' ability to understand the tasks, resulting in a general decline in their comprehension. This further confirms their limited understanding of real-world direction reasoning. Since letters and numbers do not inherently contain direction information, we only test icon data for these tasks.\nIn the relative direction reasoning task, the model's performance drops significantly for both spatial and compass directions. Specif-ically, in the relative compass direction reasoning task, LLaVA-7B and LLaVA-13B achieve accuracy rates of only 11.98% and 11.7%, respectively, which are much lower than in other tasks. Similarly, other models such as the Claude-3 series and GPT-40-mini also per-form poorly, with accuracy rates mostly between 10% and 20%. This significant decline in accuracy suggests that the models struggle with the increased complexity involved in relative orientation reasoning. One possible explanation for these results is that relative direction rea-soning requires a higher level of abstraction and the ability to process multiple orientation cues simultaneously. The models may lack the ability to integrate these cues effectively, leading to confusion when determining the correct relative orientation. Additionally, the inherent complexity of relative orientation tasks, which involve understanding the relationship between multiple objects and directions, may exceed the models' current capabilities, particularly if the training data did not sufficiently cover such scenarios. This highlights the need for more advanced training strategies and datasets that better capture the intricacies of relative direction reasoning.\nWe fine-tune the LLaVA-7B using the Relative Compass Reasoning (icon) training set from CDR, along with randomly mixed data from llava_v1_5_mix665k.json. The results from various dataset combinations are shown in Table III. All refers to fine-tuning with the full CDR training dataset, while 20K, 30K, 40K, etc., refer to fine-tuning with randomly selected subsets of 20K, 30K, and 40K samples, respectively.\nUsing the all Relative Compass Reasoning training data in (71552 samples), the model only get 11.90% accuracy on Relative Compass Reasoning task. It suggests that the model struggles to effectively learn the underlying rules of orientation purely by associating options with direction questions."}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we explore the ability of MLMs in compass direction reasoning, as existing research predominantly focuses on spatial reasoning. To address this gap, we propose CDR, the first multimodal benchmark specifically designed for compass direction reasoning. CDR provides a comprehensive evaluation framework for both spatial and compass tasks, offering balanced directional distributions for precise and thorough evaluation with over 100K training and testing samples. Its simple, low-ambiguity design ensures fair assessment across diverse directional challenges. Fine-tuning LLaVA-7B on CDR demonstrates significant improvements, particularly in compass-based tasks. In future work, we plan to expand the dataset by incorporating more complex, real-world images (such as map data) and exploring 3D directional reasoning to further enhance the evaluation of models' directional reasoning abilities."}]}