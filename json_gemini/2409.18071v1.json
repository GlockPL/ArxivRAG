{"title": "FreeEdit: Mask-free Reference-based Image Editing with Multi-modal Instruction", "authors": ["Runze He", "Kai Ma", "Linjiang Huang", "Shaofei Huang", "Jialin Gao", "Xiaoming Wei", "Jiao Dai", "Jizhong Han", "Si Liu"], "abstract": "Introducing user-specified visual concepts in image editing is highly practical as these concepts convey the user's intent more precisely than text-based descriptions. We propose FreeEdit, a novel approach for achieving such reference-based image editing, which can accurately reproduce the visual concept from the reference image based on user-friendly language instructions. Our approach leverages the multi-modal instruction encoder to encode language instructions to guide the editing process. This implicit way of locating the editing area eliminates the need for manual editing masks. To enhance the reconstruction of reference details, we introduce the Decoupled Residual Refer-Attention (DRRA) module. This module is designed to integrate fine-grained reference features extracted by a detail extractor into the image editing process in a residual way without interfering with the original self-attention. Given that existing datasets are unsuitable for reference-based image editing tasks, particularly due to the difficulty in constructing image triplets that include a reference image, we curate a high-quality dataset, FreeBench, using a newly developed twice-repainting scheme. FreeBench comprises the images before and after editing, detailed editing instructions, as well as a reference image that maintains the identity of the edited object, encompassing tasks such as object addition, replacement, and deletion. By conducting phased training on FreeBench followed by quality tuning, FreeEdit achieves high-quality zero-shot editing through convenient language instructions. We conduct extensive experiments to evaluate the effectiveness of FreeEdit across multiple task types, demonstrating its superiority over existing methods. The code will be available at: https://freeedit.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rise of social media, the demand for individual users to create images has grown significantly. Traditional image creation relies on professional tools and labor-intensive processes. The emergence of artificial intelligence has lowered the technical barriers, enabling amateur users to generate images more freely and easily.\nIn this context, diffusion models [1], [2] have rapidly advanced, offering more stable training and the ability to generate high-quality, diverse images. These models are gradually replacing GANs [3]\u2013[5] as the leading technique in image synthesis. Trained on large-scale datasets, diffusion models [6]\u2013[10] have demonstrated powerful capabilities in synthesizing high-fidelity images from text prompts. Beyond text-based image generation, diffusion models are widely utilized for tasks such as generating images from various condition signals like sketches and depth maps [11], [12], image inpainting [13]\u2013[16], image stylization [17]\u2013[20], and more.\nAmong these tasks, image editing has also become widely adopted using diffusion models. Recently, instruction-driven image editing [21]\u2013[23] has gained significant attention, as it allows users to edit images using text instructions, aligning more closely with natural user habits. However, textual descriptions cannot accurately convey the user's editing intent, even if it's very detailed as discussed in previous works [24], [25]. With the growing demand for user-preference image editing, reference-based image editing is being widely explored. Given a reference image, reference-based image editing aims to modify the target image by incorporating elements from the reference while maintaining overall coherence and quality. Nevertheless, the implementation of this task is non-trivial, which entails achieving two primary objectives: locating the editing region and introducing the reference appearance. Mask-based methods [13], [24], [26] introduce manual masks to locate the editing region to simplify the problem. This reliance on masks causes them to lose the flexibility to use language instructions to control editing. To introduce the reference appearance, previous methods [24], [26] typically utilize pre-trained image encoders [27]\u2013[29]. However, these encoders often fail to fully capture the intricate details within the reference images, leading to insufficient similarity between the generated output and the reference images.\nTo tackle the aforementioned challenges, we propose FreeEdit, an innovative mask-free reference-based image editing framework with precise reference appearance injection. As shown in Figure 1, our approach differs from previous inpainting-based methods by conditioning the diffusion model on image-text multi-modal instructions rather than abstract image embeddings, eliminating the need to manually specify an editing area. To encode user-provided language instructions, FreeEdit elaborates a multi-modal instruction encoder that transforms them into editing instruction embeddings. These embeddings are then incorporated into the diffusion model via the cross-attention mechanism, guiding the editing process effectively. To address the limitations of existing methods in preserving the details of reference subjects, we further introduce the Decoupled Residual Refer-Attention (DRRA) module. Unlike the previous methods [13], [30], [31], which directly integrates the reference attention into the self-attention, DRRA separates reference attention from self-attention and learns it residually, maintaining the integrity of the original self-attention. By extracting fine-grained features from reference images, DRRA ensures the identity (ID) consistency between the edited result and the reference subject. To effectively introduce the reference information, we leverage a phased training process including instruction embedding and fine-grained feature integration. Finally, a quality tuning stage enhances the overall editing quality, ensuring high fidelity and consistency.\nTo address the lack of datasets for reference-based instruction-driven editing, we developed FreeBench, a high-quality dataset designed to support model training. Previous studies always struggled with constructing image triplets that include the original, edited, and reference images, because it is difficult to maintain the ID consistency of the reference images and the edited images. To resolve this, we implement a twice-repainting construction scheme to ensure identity consistency between the edited object and the reference, based on the real-world segmentation dataset. To further enhance the model's ability to comprehend language instructions, we perform instruction recaptioning on the constructed image triplets, providing as detailed and accurate instructions as possible. After meticulous data construction and filtering, we finally obtain a dataset containing 131,160 edits. Different from previous datasets [21], [22], the instructions in FreeBench are multi-modal, combining text and reference images to cover tasks such as object addition, replacement, and removal.\nExtensive experiments demonstrate that FreeEdit surpasses previous methods in reference-based image editing tasks, such as object addition and replacement, without requiring editing masks. It also achieves comparable results in object removal and plain-text instruction-driven editing tasks. Our contributions can be summarized as follows:\n\u2022 We are the first to implement reference-based image editing in an instruction-driven manner, eliminating the need for manual masks and significantly improving ease of use.\n\u2022 We propose a powerful editing model, FreeEdit, which utilizes multi-modal instructions to implicitly locate editing regions and incorporates DRRA modules to efficiently integrate fine-grained reference image features.\n\u2022 We introduce a high-quality dataset, FreeBench, specifically designed for reference-based image editing tasks, containing a total of 131,160 edits to support and foster research in the community."}, {"title": "II. RELATED WORK", "content": "A. Image Customization\nTo perform image generation conditioned on a given subject, image customization methods [25], [32], [33] that require fine-tuning are developed to learn specific visual concepts in a given image set, which attract a lot of attention from the community and have made great progress. Given 3-5 reference images for a targeted subject, DreamBooth [32] enables the model to learn subject-specific concepts by fine-tuning on the given small-scale image set with a class-specific prior preservation loss to keep its generation ability. Textual Inversion [25] learns the concept of a new subject by optimizing a new token in the text space. CustomDiffusion, similar to Textual Inversion, learns a new token in the text space and fine-tunes a few parameters in the text-to-image conditioning mechanism to achieve better generation quality and faster fine-tuning.\nCompared to the above methods which need fine-tuning for each subject, zero-shot image customization methods [34]\u2013[36] are more appealing. ELITE [34] encodes visual features from the CLIP image encoder into textual embeddings and introduces patch features into cross-attention layers to provide details. BLIP-Diffusion [35] takes the Q-Former to extract image features, yielding promising image customization re-sults after training on a large amount of data. IP-Adapter [36] injects image embeddings using a decoupled cross-attention mechanism to achieve text-compatible image prompts. However, these methods are unsuitable for reference-based image editing due to significant differences in tasks.\nB. Image Inpainting\nImage inpainting is closely related to the task of image editing. Traditional image inpainting methods [15], [37] perform unconditional completion given masks, while recent advancements in text-based image inpainting have continually improved this task. SD-Inpainting modifies the input channels of the diffusion model to 9 channels, allowing for additional mask and masked image conditions. SmartBrush [16] enables text and shape-guided object inpainting by introducing object-mask prediction. PowerPaint [14] achieves high-quality versatile image inpainting with learnable task embeddings. BrushNet [38] designs decomposed dual-branch diffusion to serve as a plug-and-play solution for image inpainting.\nText-based inpainting, however, cannot fully capture the user's intent when it comes to specific visual concepts. To address this, PaintByExample [24] replaces the text embedding condition with the image embedding obtained from the CLIP image encoder [27] to achieve exemplar-based image inpainting. AnyDoor [26] introduces a powerful DINO image encoder [28], [29] and the high-frequency map to promote the similarity of the results to the reference images. Our contem-poraneous method, MimicBrush [13], incorporates reference imitation to perform imitative editing, and it still follows the inpainting paradigm. As an implementation of reference-based image editing, the inpainting-based methods present promising editing results, but they require users to provide editing areas, and cannot condition the diffusion model by convenient language instructions.\nC. Image Editing\nImage editing methods [39]\u2013[43] emerges in large numbers recently. Prompt2Prompt [39] performs zero-shot image edit-ing using a pair of text descriptions through cross-attention manipulation. NullTextInversion [40] optimizes the uncon-ditional textual embeddings to promote the editing of real images based on InstructPix2Pix. PnP [41] manipulates spatial features and their self-attention inside the model to facilitate fine-grained control. MasaCtrl [42] introduces mutual self-attention for consistent image editing. Imagic [43] produces a text embedding that aligns with both the input image and the target text, and fine-tunes the diffusion model to allow high-quality complex semantic image editing.\nIn addition to above description-based editing methods, instruction-driven image editing ones [21]\u2013[23], [44]\u2013[46] have attracted more and more attention from the community, as text instructions are more in line with human habits of image editing. Early work InstructPix2Pix [21] achieves the task by fine-tuning the diffusion model on the constructed dataset which is constructed with LLMs [47]\u2013[49] generating text pairs and Prompt2Prompt [39] generating image pairs then. MagicBrush [22] presents a human-annotated high-quality instruction-driven editing dataset. Emu Edit [23] builds on Emu [50] by scaling up dataset construction and integrating traditional computer vision tasks, akin to InstructDiffu-sion [46]. While these methods have made image editing more accessible, they do not yet support reference-based image editing tasks."}, {"title": "III. APPROACH", "content": "In this section, we begin with an overview of FreeEdit in Section III-A. Next, Section III-B introduces our multi-modal editing instruction. We then provide a detailed description of the fine-grained feature injection in Section III-C. Finally, Section III-D covers our training strategies.\nA. Overview\nThe overall pipeline of FreeEdit is represented in Figure 2. Given an original image $I_o$ and a language instruction $Y$ with a reference image $I_r$, FreeEdit edits image $I_o$ according to the intent of the language instruction, and the edited image maintains the ID consistency with the visual concept in $I_r$.\nFreeEdit consists of a multi-modal instruction encoder, a detail extractor and a denosing U-Net. The multi-modal instruction encoder encodes the language instructions into multi-modal editing instruction embedding to condition the denoising U-Net. The detail extractor shares the same architecture as U-Net, which extracts multi-scale fine-grained features from the reference image. The denoising U-Net accepts 8-channel latent input for image editing, with the DRRA mod-ules integrating the reference features obtained from the detail extractor to promote the ID consistency of the editing result with the reference image.\nB. Multi-modal Editing Instruction\nPrevious reference-based image editing methods [13], [24], [26] condition the diffusion model on the reference image by substituting the text embedding with the projected reference image embedding. However, this practice leads to a reliance on manual masking, as models utilizing image embeddings for the cross-attention mechanism lose their ability to perform editing region localization by texts. Furthermore, the trained model is only specialized for the reference-based inpainting task, which means it lacks the flexibility to perform object removal or other image editing operations that are typically guided by plain text instructions. Instead, we seek to implement reference-based image editing in a mask-free manner. Achieving such a goal is challenging, compared to those methods which are given the mask of editing region beforehand. Intuitively, one promising approach is to employ text-image interlaced editing instructions, exemplified by commands like \u201creplace the bird in the sky with S*"}, {"title": "IV. DATASET", "content": "In this section, we delve into the details of our proposed dataset, denoted as FreeBench. In Section IV-A, we describe the construction of the image triplet, which consists of the original image, the edited image, and the corresponding ref-erence image. Subsequently, in Section IV-B, we introduce the method for obtaining detailed textual editing instructions. Lastly, Section IV-C elaborates on our data filtering process.\nA. Image Triplet Construction\nThe previous instruction-driven image editing methods [21], [23] leverage the in-context learning capabilities of Large Lan-guage Models (LLMs) [48], [49] to generate extensive pairs of image descriptions and editing instructions. Subsequently, they employ the Prompt2Prompt [39] to produce corresponding image pairs. However, it is not well-suited for reference-based image editing tasks. The challenge lies in simultaneously generating a reference image that maintains ID consistency with the corresponding object in the edited image. Another straightforward solution is to apply existing reference-based inpainting techniques [13], [24], [26], which are to generate an edited image using an original image and a reference image. However, such a solution limits the ID similarity between the edited result and the reference image, as these techniques fail to reconstruct the intricate details of the reference object accurately.\nTo tackle the above issues, we construct the image triplet by applying a twice-repainting scheme based on the existing real-world segmentation dataset, OpenImages [55], due to its large scale, diversity, and accurate annotation. As depicted in Figure 4a, given a source image $I_s$ in the segmentation dataset, its instance mask $M$ and category label $C$, we repaint the mask region $M$ of the source image $I_s$ with the textual description related to the category label $C$ as the input prompt to produce the generated image $I_g$. Simultaneously, we repaint the background region $M' = 1 - M$ of the source image $I_s$ to produce reference image $I_r$. This allows us to construct an image triplet $(I_g, I_r, I_s)$ for the reference-based object replacement task. To further expand the diversity of the data, we also repaint the $M$ region of the generated image $I_g$ to obtain a new reference image $I'_r$, resulting in an additional image triplet $(I_s, I'_r, I_g)$.\nIt is more challenging to construct the triplet for the reference-based object addition task, primarily because it is difficult to pre-determine the exact position of the object to be added within the original image. While adding a specific reference object to a background image is complex, removing an existing object is comparatively simpler. This process is similar to our implementation in the object replacement task, with a crucial difference: when repainting the masked area, the goal is to remove the object, not modify it. In addition, given the versatility of our FreeEdit that it can be used for plain-text instruction-driven editing, we also maintain the flipping task of object addition, i.e. object removal, in our dataset.\nDuring the image triplet construction, we empirically em-ploy BrushNet [38] as the repainting model, and for object removal, we utilize DesignEdit [56]. When constructing the reference image, we apply horizontal flipping and affine transformations to both the foreground image and the corre-sponding mask. These augmentation techniques enhance the complexity and diversity of our dataset, thereby increasing its effectiveness for training.\nB. Instruction Construction\nTo effectively support instruction-driven editing tasks, it is crucial to obtain editing instructions for each image triplet that are as detailed and accurate as possible. A straightforward approach might be to populate task-specific templates with cat-egory labels inherited from the segmentation dataset, thereby generating basic editing instructions. However, relying solely on category labels to refer to the object to be edited is overly simplistic. This approach can diminish the model's capacity to comprehend more complex editing instructions, which is essential for handling sophisticated editing tasks.\nTraditional non-reference instruction-driven editing datasets acquire instructions from LLMs concurrently with generating image description pairs. While we cannot obtain instructions in advance as they do, we can leverage Multi-modal Large Language Models (MLLMs) [51], [57]\u2013[59] to caption the edited area of the pre-edit image. This approach allows us to produce a detailed local description of the object or area that is intended for editing.\nSpecifically, we take multiple MLLMs to improve the diver-sity of instruction contents, including (1) CogVLM [57] with grounding capabilities, (2) LLaVA [60] with AlphaCLIP [61] and (3) Mini-CPM [59]. The first two models can provide a holistic view when captioning objects; however, they often face hallucination issues, leading to incorrect captions. Therefore, we introduce the third MLLM, Mini-CPM, which only observes cropped foreground images, to address this issue. While it doesn't account for object location within the image, it focuses on the description of the object's appearance. We select the final captions by calculating the CLIP similarity between the image and captions obtained by different means.\nC. Data Filtering\nGiven the limitations of our repainting models, there's a risk of inadequate edits or poor reference images, and the MLLMs might not always generate precise local descriptions. To mitigate this, we use data filtering to remove low-quality entries and ensure the integrity of our training data.\nFirstly, we assess the similarity between pre- and post-editing images, recognizing that a high degree of similarity can be problematic, as it may render the edited and original images nearly indistinguishable. Therefore, we employ filters to exclude data with high similarity scores using both the im-age encoders of CLIP [27] and DINO [28], [29]. Furthermore, there remains a risk of inadvertently introducing unrelated objects into the background and taking over the reference image. To counteract this, we utilize CLIP and DINO image similarity metrics to identify and exclude instances where the edited result significantly deviates from the reference image. To ensure the accuracy of the instructions, we also filter out the edits that have a low similarity between the foreground area of the images to be edited and the MLLM-generated local descriptions.\nDuring the data filtering stage, we filtered out about 10% of the data, resulting in the retention of 70,634 edits for object replacement, 30,263 for object addition, and 30,263 for object removal tasks. This brings the total number of edits in our dataset, FreeBench, to 131,160. A comparison of FreeBench with existing instruction-driven datasets is presented in Table I. Notably, FreeBench stands out as the exclusive dataset that supports multi-modal instructions. We visualize the distribu-tion of category labels in the filtered dataset in Figure 5, showcasing that our dataset encompasses a wide range of categories representative of everyday life."}, {"title": "V. EXPERIMENTS", "content": "A. Implementation Details\nHyperparameters. We use Stable Diffusion V1-5 [62", "including": 1, "24": "which introduces projected CLIP image embeddings to substitute the original text embeddings. (2) AnyDoor [26", "13": "which implements a similar task to reference-based image editing by referencing imitation"}, {"including": 1, "21": "which doesn't directly support reference-based image editing. We provide it with text instructions containing detailed descriptions as a baseline method. (2) Kosmos-G [63", "13": [24], "26": "do not treat reference-based image editing as a distinct task", "cases": 100}]}