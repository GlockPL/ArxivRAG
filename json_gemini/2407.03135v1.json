{"title": "GMM-RESNEXT: COMBINING GENERATIVE AND DISCRIMINATIVE MODELS FOR\nSPEAKER VERIFICATION", "authors": ["Hui Yan", "Zhenchun Lei", "Changhong Liu", "Yong Zhou"], "abstract": "With the development of deep learning, many different network ar- chitectures have been explored in speaker verification. However, most network architectures rely on a single deep learning architec- ture, and hybrid networks combining different architectures have been little studied in ASV tasks. In this paper, we propose the GMM- ResNext model for speaker verification. Conventional GMM does not consider the score distribution of each frame feature over all Gaussian components and ignores the relationship between neigh- boring speech frames. So, we extract the log Gaussian probabil- ity features based on the raw acoustic features and use ResNext- based network as the backbone to extract the speaker embedding. GMM-ResNext combines Generative and Discriminative Models to improve the generalization ability of deep learning models and al- lows one to more easily specify meaningful priors on model pa- rameters. A two-path GMM-ResNext model based on two gender- related GMMs has also been proposed. The Experimental results show that the proposed GMM-ResNext achieves relative improve- ments of 48.1% and 11.3% in EER compared with ResNet34 and ECAPA-TDNN on VoxCeleb1-O test set.", "sections": [{"title": "1. INTRODUCTION", "content": "The task of Automatic Speaker Verification (ASV) [1] is to verify the identity of speakers by using speaker speech as feature. For two given utterances, a typical ASV system can extract speaker embed- dings of them and automatically determine whether they belong to the same speaker or not. The procedure of a modern ASV system generally consists of acoustic feature extraction, speaker embedding extraction and similarity scoring. The purpose of acoustic feature ex- traction is to transform a waveform of speech into acoustic features, such as Filter-Banks, Mel Frequency Cepstral Coefficients (MFCCs) and spectrograms. Speaker embedding extraction is to extract fixed- length speaker embeddings from variable-length utterances. Simi- larity scoring aims to calculate the similarity between test speaker embedding and enrollment speaker embedding.\nThe conventional generative models such as Gaussian mixture model-Universal Background Model (GMM-UBM) [2] and i-vector [3] with Probabilistic Linear Discriminant Analysis (PLDA) [4] were main method in the field of ASV. With the development of deep learning techniques, these models have been gradually re- placed by Deep Neural networks (DNNS). Now, more and more DNN-based models were applied to ASV tasks. Especially r-vector structures based on convolution neural network (CNN) and x-vector architectures based on Time Delay Neural Network (TDNN) shown\nremarkable performance. For CNN-based ASV systems, Zeinali et al. [5] firstly used Residual Networks (ResNet) [6] in image recognition as speaker embedding extractor in VoxSRC 2019. Liu et al. [7] proposed ResNet based on sequential and parallel feature attention fusion mechanism, which uses attention mechanism to learn fusion weights based on feature content, thereby dynamically integrating identity mapping features and residual learning features. Chen et al. [8] proposed an enhanced Res2Net, which uses attention fusion module to fuse features in a residual block to extract local signals, and fuse features of different scales to aggregate global signals. For TDNN-based ASV systems, x-vector proposed by Sny- der et al. [9] employed TDNN to map variable-length speech to fixed-length speaker embedding for the first time, and used PLDA back-end model to compare similarity of a pair of speaker embed- dings. Desplanques et al. [10] proposed ECAPA-TDNN based on Squeeze-Excitation (SE) [11] module and Res2Net[12], which achieves the equal error rates of less than 1% in VoxCeleb-O test set. Thienpondt et al. [13] proposed ECAPA CNN-TDNN, which intro- duces a 2-D convolution into ECAPA-TDNN to transfer some strong characteristics of ResNet to this hybrid CNN-TDNN architecture, and achieves better results on VoxCeleb-O test set. Zhao et al. [14] proposed to segment the input spectral map into several frequency bands and use progressive channel fusion strategy to gradually fuse these bands to improve the ECAPA-TDNN.\nAlthough the embedding extractors mentioned above show ex- cellent performance in ASV task, they only rely on a single deep learning architecture. In the filed of ASV, Alam et al. [15] pro- posed a hybrid neural network(HNN) architecture with Cross- and Self-module Attention pooling mechanisms for speaker verification. Wang et al. [16] proposed MACCIF-TDNN, which is a series of residual networks and transformers. Wang et al. [17] proposed a Parallel-coupled TDNN/Transformer Network (p-vectors) to replace the serial hybrid networks. In the field of speech deepfake detec- tion, Lei et al. [18] proposed GMM-Transformer, which is a fu- sion of Gaussian Mixture Model (GMM) and deep neural network, and achieves excellent detection effect in the speech deepfake detec- tion logical access task. Wen et al. [19] also proposed multi-branch GMM-MobileNet model based on different data augmentation and attack methods. However, hybrid networks that combine conven- tional machine learning methods with popular deep learning meth- ods have been rarely studied in ASV tasks.\nThe Gaussian Mixture Model accumulates the scores on all frames independently, and does not separately consider the scores of feature frames on each Gaussian component. In addition, the GMM ignores the relationship between adjacent speech frames along the time dimension. In this paper, we propose the GMM-ResNext that applies the Gaussian probability features as input for speaker verifi- cation. Then our proposed GMM-ResNext concatenates the output feature maps from last layer in each stage to aggregate the multi- layer representations before final pooling. On the other hand, the"}, {"title": "2. LOG GAUSSIAN PROBABILITY FEATURE", "content": "The GMM is a conventional speaker recognition classifier. For a speech feature vector, the GMM sums the probability density val- ues of N Gaussian components, but does not consider the score dis- tribution of each Gaussian component separately. The relationship between adjacent speech frames is ignored. For the information dis- tributions of speeches of different speakers are different in feature space, their score distributions on all Gaussian components are also different. Therefore, this score distribution information is helpful for speaker verification. The GMM takes the raw acoustic feature as in- put and outputs the Log Gaussian Probability(LGP) feature. For a D dimensional input feature x (MFCC in our experiments), the element $y_i$ of the LGP feature y is defined as:\n$y_i = log\\ p_i(x)$\n$= log \\frac{1}{(2\\pi)^{D/2}|\\Sigma_i|^{1/2}}exp{-\\frac{1}{2}(x - \\mu_i)^T\\Sigma_i^{-1}(x - \\mu_i)}$\n$= -\\frac{1}{2}x^T\\Sigma_i^{-1}x + x^T\\Sigma_i^{-1}\\mu_i + C$\n(1)\nwhere $p_i(x)$ is the probability density function of the i-th Gaussian component, which is parameterized by a D \u00d7 1 mean vector, $\\mu_i$ and a D \u00d7 D covariance matrix, $\\Sigma_i$.\nIn order to reduce computation, the constant term C can be re- moved. After that, the mean $mean_{y_i}$, and standard deviation $std_{y_i}$ of features in training data are computed, which are used for mean and variance normalization.\n$\\hat{y_i} = \\frac{y_i - mean_{y_i}}{std_{y_i}}$\n(2)"}, {"title": "3. PROPOSED METHOD", "content": "In this section, we describe details of the proposed GMM-ResNext model and dual-Path GMM-ResNext.\n3.1. GMM-ResNext\nThe CNN-based model is widely used as the backbone network of various machine learning tasks because of its stronger feature ex- traction ability. However, with the increase of the number of lay- ers in the network, the problem of gradient explosion or gradient disappearance or even network degradation will appear in the train- ing process. In order to solve the above problems, the ResNet [6] employs shortcut connections to fusion the identity mapping fea- tures and the residual learning features to improve the stability of the model. In ResNet, the number of parameters in the model is mainly adjusted by depth and width. In practice, we usually find that directly increasing the depth or width of the model is ineffective, and easily cause overfitting problems due to the large number of pa- rameters. The ResNext [20] that includes the grouped convolutional layer called a split-transform-merge strategy in blocks is designed to easy this problem and improve the representation ability of ResNet model. The ResNext has achieved reliable performance in SV [21], so we proposed the GMM-ResNext model that fuses the GMM and ResNext for speaker verification. The overall architecture is shown in Figure 1.\nThe proposed GMM-ResNext model consists of a GMM mod- ule, a ResNext backbone with four residual stages, An attention statistics pooling (ASP) layer with Multi-layer Feature Aggregation (MFA), and a fully connected layer. Encouraged by [22] proposed design principles, we adjusted the number of blocks in each stage from (3, 4, 6, 3) to (3, 3, 9, 3), which is different from the origi- nal stage ratio in the residual network. BN stands for Batch Nor- malization and the Rectified Linear Units (ReLU) is the non-linear activation function.\nTo further reduce the number of parameters in the model, we use depthwise convolution instead of grouped convolution in ResNext, which is a special case of grouped convolution where the number of groups equals the number of channels. The structure of the DW- ResBlock module is shown in Figure 2. The module consists of two convolutional layers with filter size of 1, 1D depthwise convolutional layer with filter size of 3, SE block, and BN and ReLu after the convolutional layer. In the SE Block, the dimension of the bottleneck is set to 1/4 of the number of input channel.\nPrevious studies [23] have shown that the shallow feature maps in deep neural networks also facilitate the extraction of more robust speaker embeddings. In [10], the ECAPA-TDNN concatenates the output features of all SE-Res2Net blocks, and then uses a dense layer to process the connected feature information to generate the input of the pool layer. Zhang et al. [24] proposed MFA-Conformer model that concatenates the output features of each Conformer block, and then feed them into a LayerNorm layer. This aggregation method results in a obvious performance improvement. Influenced by these studies, we also concatenates the output feature maps of all stages to"}, {"title": "3.2. Dual-path GMM-ResNext", "content": "Improving the generalization ability of speaker verification is a great challenge. The speech from speakers of different genders has differ- ent feature distribution. The conventional GMM describes the com- mon information distribution. But the GMM does not pay attention to the difference between male and female speeches, which is use- ful to model information distribution of speech from different gen- ders. So we propose a dual-path architecture with ResNext (dGMM- ResNext) for speaker verification, which is constructed according to two GMMs trained by male speeches and female speeches.\nFigure 3 shows the dual-path GMM-ResNext which contains two networks with same architecture. The LGP features are ex- tracted on two GMMs, which are trained on male speeches and fe- male speeches from training set. The embedding vectors from two paths are concatenated and inputted to the fully connected layer. Fi- nally, the speaker embedding are output from the last fully connected layer.\nMulti-step training scheme [25] is usually used to solve the prob- lem of model overfitting. Considering the large number of model parameters, a two-step training method is adopted to improve the ro- bustness of model. In the first step, the two branches of the dual-path ResNext model are trained independently using the AAM-softmax"}, {"title": "4. EXPERIMENTAL SETUP", "content": "4.1. Dataset\nThe experiments are conducted on VoxCeleb1 [26] and VoxCeleb2 [27] dataset. They include a development set and a test set. Vox- Celeb2 development set is used for training, which consists of 109,200,9 utterances from 599,4 speakers. The whole VoxCeleb1 dataset is used as the testing data, which contains over 100,000 utterances from 125,1 speakers. The performance of all models are evaluated on three different test trials, namely VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H. In addition, to increase the diver- sity of the original training data, we employed data augmentation during training, adding noise using the MUSAN dataset [28], simu- lating reverberation using the RIR dataset [29].\n4.2. Implementation details\nFor fair comparison, we re-implemented r-vector in [5] and ECAPA- TDNN in [10] as the baselines. The r-vector includes ResNet18 and ResNet34 with channels of residual blocks set as {32,64,128,256}. ECAPA-TDNN consists of a basic model with 512 channels and a large model with 1024 channels. The PyTorch framework is used to implement the baseline and the proposed models. A fixed length 2-second segments are extracted randomly from each utterance. We use 80-dimensional MFCCs with a window length of 25 ms and a frame shift of 10 ms. Mean normalization is applied to the MFCCS features before input network. The GMM implemented by MSR Identity Toolbox[30] was trained for 30 iterations. Adam optimizer with an initial learning rate of 0.001 is used during the training pro- cess. The learning rate is reduced by 3% every one epoch. We use additive margin softmax (AAM-softmax) [31] loss with a margin of 0.2 and a scale factor of 30 to train all models. In order to avoid overfitting, the weight decay is set to 2e-5. In the ASP, the dimen- sion of the bottleneck is set to 128. The batch size is 200 and the size of speaker embeddings is 256. All models are trained for 100 epochs.\nIn the evaluation phase, we use the cosine similarity between embeddings for scoring. the Equal Error Rate (EER) and mini- mum Detection Cost Function (minDCF) with Ptarget = 0.01 and CFA = CMiss = 0.01 will be reported for performance evalution."}, {"title": "5. RESULTS AND ANALYSIS", "content": "5.1. Results on VoxCeleb test set\nTable 1 shows the EER and the MinDCf of the baselines and the pro- posed models on the VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1- H datasets. Compared with GMM-ResNext(256), the GMM- ResNext(512) obtains the relative improvements in EER by 28.8%, 22.1%, 18.7% and in MinDCF by 18.3%, 18.2%, 16.7%. The dGMM-ResNext(512) also obtains similar performance improve- ments compared with dGMM-ResNext(256). It can be observed that when the number of Gaussian components increase, the GMM- ResNext can achieve better performance. This may be due to the fact that Gaussian probability features contain more distinguishing speaker information.\nCompared with GMM-ResNext(256), the dGMM-ResNext(256) obtains the relative improvements in EER by 12.6%, 13.6%, 12.7% on the VoxCeleb1 test set. The dGMM-ResNext(512) further im- proves improvement compared with GMM-ResNext(512). The dGMM-ResNext(512) obtains the best results, which relatively re- duces the EER by 48.1%, 41.1%, 37.0% and the MinDCF by 47.8%, 40.5%, 34.7% compared with the ResNet34, and relatively reduces the EER by 11.3%, 14.4%, 12.4% and the MinDCF by 16.0%, 13.2%, 12.8% compared with the ECAPA-TDNN(1024).\nCompared with other HNN systems, such as CA-HNN, CSA- HNN and MACCIF-TDNN, the dGMM-ResNext(512) also achieves competitive results. But the performance of the dGMM-ResNext(512) is worse than P-vector. The reason may be that P-vector leverages the convolutional operations and the attention mechanism to interact and aggregate the local and global information.\n5.2. Ablation experiments\nTo evaluate the role of key modules, we conduct ablation experi- ments to study the effect of each modules contributing to perfor- mance improvements. Table 2 shows the results of the ablation ex- periments on VoxCeleb1-O test set. The first line is the results of the GMM-ResNext(512). In the second line, we remove the GMM layer and use the MFCC feature instead of the log-Gaussian prob- abilistic feature as the input of the network. In the third line, the MFA layer is removed and only the output feature map of the last residual block is used. The results in the fourth line are the dGMM- ResNext(512). In the last line, we does not use the two step training strategy. It can be observed from the experiment results that both GMM layer and multi-layer feature aggregation module play a key role in improving the performance. The GMM layer reduces EER and minDCF by 21.3% and 19.3% relatively. And aggregating the output of all blocks brings 16.5% and 21.2% relative improvement in EER and minDCF. At the same time, the two step training strategy further improves the generalization ability of the dGMM-ResNext."}, {"title": "6. CONCLUSION", "content": "In this paper, we proposed the GMM-ResNext model that combines conventional machine learning methods with deep learning methods for speaker verification. First, the proposed model uses Gaussian probability features as the input of the residual network. Then, the output feature maps of the four stages are aggregated to integrate the multi-level features, so as to obtain more distinguishable speaker in- formation. We also proposed the dual-path GMM-ResNext model based on different genders to improve the generalization ability of the model. Experiments on the VoxCeleb dataset show that the pro- posed dGMM-ResNext model is significantly superior to the cur- rently popular ResNet and ECAPA-TDNN models. In future work, we will explore new network architectures fused with Gaussian mix- ture model."}]}