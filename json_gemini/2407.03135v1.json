{"title": "GMM-RESNEXT: COMBINING GENERATIVE AND DISCRIMINATIVE MODELS FOR\nSPEAKER VERIFICATION", "authors": ["Hui Yan", "Zhenchun Lei", "Changhong Liu", "Yong Zhou"], "abstract": "With the development of deep learning, many different network ar-\nchitectures have been explored in speaker verification. However,\nmost network architectures rely on a single deep learning architec-\nture, and hybrid networks combining different architectures have\nbeen little studied in ASV tasks. In this paper, we propose the GMM-\nResNext model for speaker verification. Conventional GMM does\nnot consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neigh-\nboring speech frames. So, we extract the log Gaussian probabil-\nity features based on the raw acoustic features and use ResNext-\nbased network as the backbone to extract the speaker embedding.\nGMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and al-\nlows one to more easily specify meaningful priors on model pa-\nrameters. A two-path GMM-ResNext model based on two gender-\nrelated GMMs has also been proposed. The Experimental results\nshow that the proposed GMM-ResNext achieves relative improve-\nments of 48.1% and 11.3% in EER compared with ResNet34 and\nECAPA-TDNN on VoxCeleb1-O test set.\nIndex Terms- speaker verification, ResNext, GMM, genera-\ntive model, discriminative model", "sections": [{"title": "1. INTRODUCTION", "content": "The task of Automatic Speaker Verification (ASV) [1] is to verify\nthe identity of speakers by using speaker speech as feature. For two\ngiven utterances, a typical ASV system can extract speaker embed-\ndings of them and automatically determine whether they belong to\nthe same speaker or not. The procedure of a modern ASV system\ngenerally consists of acoustic feature extraction, speaker embedding\nextraction and similarity scoring. The purpose of acoustic feature ex-\ntraction is to transform a waveform of speech into acoustic features,\nsuch as Filter-Banks, Mel Frequency Cepstral Coefficients (MFCCs)\nand spectrograms. Speaker embedding extraction is to extract fixed-\nlength speaker embeddings from variable-length utterances. Simi-\nlarity scoring aims to calculate the similarity between test speaker\nembedding and enrollment speaker embedding.\nThe conventional generative models such as Gaussian mixture\nmodel-Universal Background Model (GMM-UBM) [2] and i-vector\n[3] with Probabilistic Linear Discriminant Analysis (PLDA) [4]\nwere main method in the field of ASV. With the development of\ndeeep learning techniques, these models have been gradually re-\nplaced by Deep Neural networks (DNNS). Now, more and more\nDNN-based models were applied to ASV tasks. Especially r-vector\nstructures based on convolution neural network (CNN) and x-vector\narchitectures based on Time Delay Neural Network (TDNN) shown\nremarkable performance. For CNN-based ASV systems, Zeinali\net al. [5] firstly used Residual Networks (ResNet) [6] in image\nrecognition as speaker embedding extractor in VoxSRC 2019. Liu\net al. [7] proposed ResNet based on sequential and parallel feature\nattention fusion mechanism, which uses attention mechanism to\nlearn fusion weights based on feature content, thereby dynamically\nintegrating identity mapping features and residual learning features.\nChen et al. [8] proposed an enhanced Res2Net, which uses attention\nfusion module to fuse features in a residual block to extract local\nsignals, and fuse features of different scales to aggregate global\nsignals. For TDNN-based ASV systems, x-vector proposed by Sny-\nder et al. [9] employed TDNN to map variable-length speech to\nfixed-length speaker embedding for the first time, and used PLDA\nback-end model to compare similarity of a pair of speaker embed-\ndings. Desplanques et al. [10] proposed ECAPA-TDNN based\non Squeeze-Excitation (SE) [11] module and Res2Net[12], which\nachieves the equal error rates of less than 1% in VoxCeleb-O test set.\nThienpondt et al. [13] proposed ECAPA CNN-TDNN, which intro-\nduces a 2-D convolution into ECAPA-TDNN to transfer some strong\ncharacteristics of ResNet to this hybrid CNN-TDNN architecture,\nand achieves better results on VoxCeleb-O test set. Zhao et al. [14]\nproposed to segment the input spectral map into several frequency\nbands and use progressive channel fusion strategy to gradually fuse\nthese bands to improve the ECAPA-TDNN.\nAlthough the embedding extractors mentioned above show ex-\ncellent performance in ASV task, they only rely on a single deep\nlearning architecture. In the filed of ASV, Alam et al. [15] pro-\nposed a hybrid neural network(HNN) architecture with Cross- and\nSelf-module Attention pooling mechanisms for speaker verification.\nWang et al. [16] proposed MACCIF-TDNN, which is a series of\nresidual networks and transformers. Wang et al. [17] proposed a\nParallel-coupled TDNN/Transformer Network (p-vectors) to replace\nthe serial hybrid networks. In the field of speech deepfake detec-\ntion, Lei et al. [18] proposed GMM-Transformer, which is a fu-\nsion of Gaussian Mixture Model (GMM) and deep neural network,\nand achieves excellent detection effect in the speech deepfake detec-\ntion logical access task. Wen et al. [19] also proposed multi-branch\nGMM-MobileNet model based on different data augmentation and\nattack methods. However, hybrid networks that combine conven-\ntional machine learning methods with popular deep learning meth-\nods have been rarely studied in ASV tasks.\nThe Gaussian Mixture Model accumulates the scores on all\nframes independently, and does not separately consider the scores of\nfeature frames on each Gaussian component. In addition, the GMM\nignores the relationship between adjacent speech frames along the\ntime dimension. In this paper, we propose the GMM-ResNext that\napplies the Gaussian probability features as input for speaker verifi-\ncation. Then our proposed GMM-ResNext concatenates the output\nfeature maps from last layer in each stage to aggregate the multi-\nlayer representations before final pooling. On the other hand, the"}, {"title": "2. LOG GAUSSIAN PROBABILITY FEATURE", "content": "specificity of male speech features and female speeches features is\nuseful for modeling the feature distribution of speech. Therefore we\npropose the dual-path GMM-ResNext(dGMM-ResNext) based on\nthe different genders.\nThe GMM is a conventional speaker recognition classifier. For a\nspeech feature vector, the GMM sums the probability density val-\nues of N Gaussian components, but does not consider the score dis-\ntribution of each Gaussian component separately. The relationship\nbetween adjacent speech frames is ignored. For the information dis-\ntributions of speeches of different speakers are different in feature\nspace, their score distributions on all Gaussian components are also\ndifferent. Therefore, this score distribution information is helpful for\nspeaker verification. The GMM takes the raw acoustic feature as in-\nput and outputs the Log Gaussian Probability(LGP) feature. For a D\ndimensional input feature x (MFCC in our experiments), the element\n$y_i$ of the LGP feature y is defined as:\n$y_i = log p_i(x)$\n$y_i = log p_i(x)= log {(2pi)^{-D/2}|Sigma_i|^{-1/2}exp{-frac{1}{2}(x \u2013 mu_i)\u2019Sigma_i^{-1} (x \u2013 mu_i)}}$\n$y_i = log p_i(x)= -frac{1}{2}x\u2019x + x\u2019Sigma_i mu_i + C$\n(1)\nwhere $p_i(x)$ is the probability density function of the i-th Gaussian\ncomponent, which is parameterized by a D \u00d7 1 mean vector, $mu_i$ and\na D \u00d7 D covariance matrix, $Sigma_i$.\nIn order to reduce computation, the constant term C can be re-\nmoved. After that, the mean $mean_{y_i}$, and standard deviation $std_{y_i}$\nof features in training data are computed, which are used for mean\nand variance normalization.\n$y_i = frac{y_i - mean_{y_i}}{std_{y_i}}$\n(2)"}, {"title": "3. PROPOSED METHOD", "content": "In this section, we describe details of the proposed GMM-ResNext\nmodel and dual-Path GMM-ResNext.\n3.1. GMM-ResNext\nThe CNN-based model is widely used as the backbone network of\nvarious machine learning tasks because of its stronger feature ex-\ntraction ability. However, with the increase of the number of lay-\ners in the network, the problem of gradient explosion or gradient\ndisappearance or even network degradation will appear in the train-\ning process. In order to solve the above problems, the ResNet [6]\nemploys shortcut connections to fusion the identity mapping fea-\ntures and the residual learning features to improve the stability of\nthe model. In ResNet, the number of parameters in the model is\nmainly adjusted by depth and width. In practice, we usually find that\ndirectly increasing the depth or width of the model is ineffective,\nand easily cause overfitting problems due to the large number of pa-\nrameters. The ResNext [20] that includes the grouped convolutional\nlayer called a split-transform-merge strategy in blocks is designed to\neasy this problem and improve the representation ability of ResNet\nmodel. The ResNext has achieved reliable performance in SV [21],\nso we proposed the GMM-ResNext model that fuses the GMM and\nResNext for speaker verification. The overall architecture is shown\nin Figure 1.\nThe proposed GMM-ResNext model consists of a GMM mod-\nule, a ResNext backbone with four residual stages, An attention\nstatistics pooling (ASP) layer with Multi-layer Feature Aggregation\n(MFA), and a fully connected layer. Encouraged by [22] proposed\ndesign principles, we adjusted the number of blocks in each stage\nfrom (3, 4, 6, 3) to (3, 3, 9, 3), which is different from the origi-\nnal stage ratio in the residual network. BN stands for Batch Nor-\nmalization and the Rectified Linear Units (ReLU) is the non-linear\nactivation function.\nTo further reduce the number of parameters in the model, we use\ndepthwise convolution instead of grouped convolution in ResNext,\nwhich is a special case of grouped convolution where the number\nof groups equals the number of channels. The structure of the DW-\nResBlock module is shown in Figure 2. The module consists of two\nconvolutional layers with filter size of 1, 1D depthwise convolutional\nlayer with filter size of 3, SE block, and BN and ReLu after the\nconvolutional layer. In the SE Block, the dimension of the bottleneck\nis set to 1/4 of the number of input channel.\nPrevious studies [23] have shown that the shallow feature maps\nin deep neural networks also facilitate the extraction of more robust\nspeaker embeddings. In [10], the ECAPA-TDNN concatenates the\noutput features of all SE-Res2Net blocks, and then uses a dense layer\nto process the connected feature information to generate the input of\nthe pool layer. Zhang et al. [24] proposed MFA-Conformer model\nthat concatenates the output features of each Conformer block, and\nthen feed them into a LayerNorm layer. This aggregation method\nresults in a obvious performance improvement. Influenced by these\nstudies, we also concatenates the output feature maps of all stages to"}, {"title": "3.2. Dual-path GMM-ResNext", "content": "Improving the generalization ability of speaker verification is a great\nchallenge. The speech from speakers of different genders has differ-\nent feature distribution. The conventional GMM describes the com-\nmon information distribution. But the GMM does not pay attention\nto the difference between male and female speeches, which is use-\nful to model information distribution of speech from different gen-\nders. So we propose a dual-path architecture with ResNext (dGMM-\nResNext) for speaker verification, which is constructed according to\ntwo GMMs trained by male speeches and female speeches.\nFigure 3 shows the dual-path GMM-ResNext which contains\ntwo networks with same architecture. The LGP features are ex-\ntracted on two GMMs, which are trained on male speeches and fe-\nmale speeches from training set. The embedding vectors from two\npaths are concatenated and inputted to the fully connected layer. Fi-\nnally, the speaker embedding are output from the last fully connected\nlayer.\nMulti-step training scheme [25] is usually used to solve the prob-\nlem of model overfitting. Considering the large number of model\nparameters, a two-step training method is adopted to improve the ro-\nbustness of model. In the first step, the two branches of the dual-path\nResNext model are trained independently using the AAM-softmax\nloss function. The training method is the same as that of the one-\npath model. In the second step, we remove their classifiers and con-\ncatenate the speaker embeddings of the two branches into one fully\nconnected layer to generate the final speaker embedding. Then we\nfreeze the parameters of all layers of the dual-dual ResNext except\nthe classifier to train the fully connected layer and the classifier."}, {"title": "4. EXPERIMENTAL SETUP", "content": "4.1. Dataset\nThe experiments are conducted on VoxCeleb1 [26] and VoxCeleb2\n[27] dataset. They include a development set and a test set. Vox-\nCeleb2 development set is used for training, which consists of\n109,200,9 utterances from 599,4 speakers. The whole VoxCeleb1\ndataset is used as the testing data, which contains over 100,000\nutterances from 125,1 speakers. The performance of all models\nare evaluated on three different test trials, namely VoxCeleb1-O,\nVoxCeleb1-E and VoxCeleb1-H. In addition, to increase the diver-\nsity of the original training data, we employed data augmentation\nduring training, adding noise using the MUSAN dataset [28], simu-\nlating reverberation using the RIR dataset [29].\n4.2. Implementation details\nFor fair comparison, we re-implemented r-vector in [5] and ECAPA-\nTDNN in [10] as the baselines. The r-vector includes ResNet18 and\nResNet34 with channels of residual blocks set as {32,64,128,256}.\nECAPA-TDNN consists of a basic model with 512 channels and a\nlarge model with 1024 channels. The PyTorch framework is used\nto implement the baseline and the proposed models. A fixed length\n2-second segments are extracted randomly from each utterance. We\nuse 80-dimensional MFCCs with a window length of 25 ms and a\nframe shift of 10 ms. Mean normalization is applied to the MFCCS\nfeatures before input network. The GMM implemented by MSR\nIdentity Toolbox[30] was trained for 30 iterations. Adam optimizer\nwith an initial learning rate of 0.001 is used during the training pro-\ncess. The learning rate is reduced by 3% every one epoch. We use\nadditive margin softmax (AAM-softmax) [31] loss with a margin of\n0.2 and a scale factor of 30 to train all models. In order to avoid\noverfitting, the weight decay is set to 2e-5. In the ASP, the dimen-\nsion of the bottleneck is set to 128. The batch size is 200 and the\nsize of speaker embeddings is 256. All models are trained for 100\nepochs.\nIn the evaluation phase, we use the cosine similarity between\nembeddings for scoring. the Equal Error Rate (EER) and mini-\n$P_{target}$ = 0.01 and\n$C_{FA}$ = $C_{Miss}$ = 0.01 will be reported for performance evalution.\nmum Detection Cost Function (minDCF) with"}, {"title": "5. RESULTS AND ANALYSIS", "content": "5.1. Results on VoxCeleb test set\nTable 1 shows the EER and the MinDCf of the baselines and the pro-\nposed models on the VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-\nH datasets. Compared with GMM-ResNext(256), the GMM-\nResNext(512) obtains the relative improvements in EER by 28.8%,\n22.1%, 18.7% and in MinDCF by 18.3%, 18.2%, 16.7%. The\ndGMM-ResNext(512) also obtains similar performance improve-\nments compared with dGMM-ResNext(256). It can be observed\nthat when the number of Gaussian components increase, the GMM-\nResNext can achieve better performance. This may be due to the\nfact that Gaussian probability features contain more distinguishing\nspeaker information.\nCompared with GMM-ResNext(256), the dGMM-ResNext(256)\nobtains the relative improvements in EER by 12.6%, 13.6%, 12.7%\non the VoxCeleb1 test set. The dGMM-ResNext(512) further im-\nproves improvement compared with GMM-ResNext(512). The\ndGMM-ResNext(512) obtains the best results, which relatively re-\nduces the EER by 48.1%, 41.1%, 37.0% and the MinDCF by 47.8%,\n40.5%, 34.7% compared with the ResNet34, and relatively reduces\nthe EER by 11.3%, 14.4%, 12.4% and the MinDCF by 16.0%,\n13.2%, 12.8% compared with the ECAPA-TDNN(1024).\nCompared with other HNN systems, such as CA-HNN, CSA-\nHNN and MACCIF-TDNN, the dGMM-ResNext(512) also achieves\ncompetitive results. But the performance of the dGMM-ResNext(512)\nis worse than P-vector. The reason may be that P-vector leverages\nthe convolutional operations and the attention mechanism to interact\nand aggregate the local and global information.\n5.2. Ablation experiments\nTo evaluate the role of key modules, we conduct ablation experi-\nments to study the effect of each modules contributing to perfor-\nmance improvements. Table 2 shows the results of the ablation ex-\nperiments on VoxCeleb1-O test set. The first line is the results of\nthe GMM-ResNext(512). In the second line, we remove the GMM\nlayer and use the MFCC feature instead of the log-Gaussian prob-\nabilistic feature as the input of the network. In the third line, the\nMFA layer is removed and only the output feature map of the last\nresidual block is used. The results in the fourth line are the dGMM-\nResNext(512). In the last line, we does not use the two step training\nstrategy. It can be observed from the experiment results that both\nGMM layer and multi-layer feature aggregation module play a key\nrole in improving the performance. The GMM layer reduces EER\nand minDCF by 21.3% and 19.3% relatively. And aggregating the\noutput of all blocks brings 16.5% and 21.2% relative improvement\nin EER and minDCF. At the same time, the two step training strategy\nfurther improves the generalization ability of the dGMM-ResNext."}, {"title": "6. CONCLUSION", "content": "In this paper, we proposed the GMM-ResNext model that combines\nconventional machine learning methods with deep learning methods\nfor speaker verification. First, the proposed model uses Gaussian\nprobability features as the input of the residual network. Then, the\noutput feature maps of the four stages are aggregated to integrate the\nmulti-level features, so as to obtain more distinguishable speaker in-\nformation. We also proposed the dual-path GMM-ResNext model\nbased on different genders to improve the generalization ability of\nthe model. Experiments on the VoxCeleb dataset show that the pro-\nposed dGMM-ResNext model is significantly superior to the cur-\nrently popular ResNet and ECAPA-TDNN models. In future work,\nwe will explore new network architectures fused with Gaussian mix-\nture model."}]}