{"title": "MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification", "authors": ["Wei Fan", "Jingru Fei", "Dingyu Guo", "Kun Yi", "Xiaozhuang Song", "Haolong Xiang", "Hangting Ye", "Min Li"], "abstract": "Medical time series has been playing a vital role in real-world healthcare systems as valuable information in monitoring health conditions of patients. Accurate classification for medical time series, e.g., Electrocardiography (ECG) signals, can help for early detection and diagnosis, thus improving patient outcomes and the quality of life. Traditional methods towards medical time series classification rely on handcrafted feature extraction and statistical methods; with the recent advancement of artificial intelligence, the machine learning and deep learning methods have become more popular. However, existing methods often fail to fully model the complex spatial dynamics under different scales, which ignore the dynamic multi-resolution spatial and temporal joint inter-dependencies. Moreover, they are less likely to consider the special baseline wander problem as well as the multi-view characteristics of medical time series, which largely hinders their prediction performance. To address these limitations, we propose a Multi-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical time series classification. Specifically, we first propose to construct multi-resolution adaptive graph structures to learn dynamic multi-scale embeddings. Then, to address the baseline wander problem, we propose Difference Attention Networks to operate self-attention mechanisms on the finite difference for temporal modeling. Moreover, to learn the multi-view characteristics, we utilize the Frequency Convolution Networks to capture complementary information of medical time series from the frequency domain. In addition, we introduce the Multi-resolution Graph Transformer architecture to model the dynamic dependencies and fuse the information from different resolutions. Finally, we have conducted extensive experiments on multiple medical real-world datasets that demonstrate the superior performance of our method. Our Code is available at this repository: https://github.com/aikunyi/MedGNN.", "sections": [{"title": "1 Introduction", "content": "Medical time series data, such as Electroencephalography (EEG) and Electrocardiography (ECG) signals, has been playing a vital role in real-world healthcare systems by providing valuable information in monitoring health conditions of patients. EEG signals, which measure the electrical activity of the brain, are widely used to diagnose and monitor various neurological disorders, including epilepsy, Alzheimer's disease, and sleep disorders [7]. Similarly, ECG signals, which record the electrical activity of the heart, are essential for diagnosing and monitoring cardiovascular diseases, such as arrhythmias, myocardial infarction, and congestive heart failure [4]. Classifying these medical time series is of paramount importance as it could enable the early detection of abnormalities, accurate diagnosis, and personalized treatment. By identifying patterns and features indicative of specific conditions, medical time series classification can assist clinicians in making timely diagnoses [14] and facilitate adapting treatment plans accordingly, potentially leading to improved patient outcomes and quality of life.\nTraditionally, medical time series classification has been primarily relied on handcrafted feature extraction, which often involve domain expertise to identify relevant features from the raw data. For instance, for ECG analysis, many features such as R-peak amplitude and heart rate variability would be manually extracted [4]. Later, statistical methods have also been applied to medical time series classification: the autoregressive models, hidden Markov models, and Gaussian mixture models have been used to capture the temporal dependencies and dynamics in ECG and EEG signals [30, 33, 35]. Though statistical methods can provide robust results and handle uncertainty, they often make strong assumptions about the data distribution and may struggle with complex, non-linear patterns. With the advent of artificial intelligence, various deep learning methods have been applied to medical time series classification [25]: convolution neural networks have been particularly successful in learning representations directly from raw time series, such as EEGNet [19]; Transformer-based methods have been applied into the medical time series classification [38]. In addition, graph neural network has also been adopted for multivariate time series classification [48, 50].\nHowever, these methods often fail to fully model the complex spatial (channel) dynamics under different scales, which ignore the dynamic multi-resolution spatial and temporal joint interdependencies. Moreover, most of them are usually for general classification, without considering the special problem, such as baseline wander, as well as the multi-view characteristics in medical time series, which largely hinders their prediction performance. To address these limitations, we aim to propose a novel framework to learn the multi-scale and multi-view representations for medical time series classification. However, several challenges arise in achieving this goal: (i) how to model the dynamic spatial structures between different time series channels with multiple resolutions? Since the medical time series usually includes multiple channels, the dynamic spatial dependencies keep changing with the resolutions or scales of time series, which need to be properly modeled for accurate classification; (ii) how to learn the the multi-view characteristics of medical time series while addressing the baseline wander problem? The baseline wander problem [5], i.e., the constant offsets or slow drifts towards baseline measurements of medical series would always hinders the models in learning the key patterns and fluctuations; meanwhile the multi-view characteristics of medical time series based on the features from both the time domain and the frequency domain are usually ignored, largely hindering the classification.\nTo tackle these challenges, we introduce a Multi-resolution Spatiotemporal Graph Learning framework, MedGNN. Specifically, in our MedGNN framework, we first propose to construct multi-resolution adaptive graph structures to learn dynamic spatial temporal representations, where we utilize different kernels of convolutions to extract multi-scale medical time series embeddings to cover the local and global patterns. We construct multi-resolution graphs based on the learned embeddings to model the dynamic spatial dependencies among different channels; the graph structures are adaptively learned to reflect the changing correlations at different resolutions. Then, to address the baseline wander problem and learn the multi-view characteristics, we propose two novel networks for temporal modeling: (i) the Difference Attention Networks focus on the temporal changes in the medical time series, which operates self-attention mechanisms on the finite difference (e.g., first-order difference) along the temporal dimension, targeting capturing key temporal patterns while mitigating the impact of baseline wander; (ii) the Frequency Convolution Networks captures complementary information in the frequency domain by applying Fourier transform and frequency-domain convolutions [44, 47], providing a multi-view perspective of the temporal dynamics for medical time series. In addition, to learn the complicated multi-resolution spatiotemporal graph representations, we utilize the Multi-resolution Graph Transformer architecture to model the dynamic spatial dependencies and fuse the information from different resolutions. Our main contributions are mainly as follows:\n\u2022 We propose a novel approach for medical time series classification to capture the complex multi-view spatiotemporal dependencies and multi-scale dynamics of medical time series through multi-resolution learning.\n\u2022 We construct adaptive graph structures at different resolutions to model spatial correlations among time series channels and utilize a Multi-resolution Graph Transformer architecture for the resolution learning and information fusion.\n\u2022 We propose Difference Attention Network and Frequency Convolution Network, for temporal modeling to overcome baseline wander problem of medical time series and meanwhile capture multi-view characteristics from the time and frequency domain.\n\u2022 We have conducted extensive experiments on multiple medical time series datasets, including both ECG and EEG signals, to demonstrate the superior performance of our proposed framework compared to state-of-the-art methods, highlighting its great potential for the real-world clinical applications."}, {"title": "2 Related Work", "content": "2.1\nMedical Time Series Classification\nTime series classification is a crucial yet challenging problem in the field of data mining, as it involves identifying patterns in sequential data over time [12]. Medical time series, as a specialized form of time series data collected from human physiological signals, such as EEG [43] and ECG [18], present unique challenges and opportunities [15]. Continuously analyzing medical time series, especially as new conditions or classes of data emerge, is essential for health monitoring [40] and making informed medical decisions [1], highlighting the importance of medical time series classification.\nTraditionally, one of the most widely used approaches for medical time series classification has been the nearest neighbor (NN) [21] classifier, often combined with distance measures such as dynamic time warping (DTW) [2] or shapelet-based methods [6]. These techniques have demonstrated effectiveness in various applications due to their simplicity and interpretability. Later statistical models such as the autoregressive models [30] and Gaussian mixture models [35] have been used to capture the medical time series. In recent years, deep learning methods have significantly advanced the field of medical time series classification. For example, EEGNet [19] introduced the use of depthwise and separable convolutions to develop a model specifically designed for EEG data, capturing essential EEG feature extraction techniques for brain-computer interfaces (BCI). COMET [37] proposed a hierarchical contrastive representation learning framework that is tailored to the unique characteristics of medical time series data. Medformer [38] introduced a multi-granularity patching transformer architecture that addresses the specific challenges of medical time series classification, providing a specialized solution for capturing complex temporal patterns.\n2.2\nGraph Neural Networks for Time Series\nGraph neural networks (GNNs) have shown promising performance in time series analysis due to their ability to capture complex dependencies between time-series variables [16]. By representing data as a graph, GNNs can effectively model relationships across different variables and time steps. Some GNN-based models, such as STGCN [49] and DCRNN [20], rely on a pre-defined graph structure, which is often unavailable or difficult to determine in many real-world scenarios. To address this limitation, recent research has focused on learning graph structures directly from the data, enabling automatic modeling of the topological relationships among variables. AGCRN [3] enhances graph convolutional networks through a data-adaptive graph generation module and a node-adaptive parameter learning module. MTGNN [42] introduces an effective approach to learn and exploit the inherent dependencies among variables. FourierGNN [46] captures frequency domain spatial-temporal correlations. More recent models have continued to push the boundaries of GNN-based time series analysis. RainDrop [52] introduces a GNN framework designed to handle irregularly sampled and multivariate time series, learning the dynamics of sensors directly from observational data without requiring any prior knowledge of the relationships. SimTSC [50] presents a simple yet general framework that uses GNNs to model similarity information, which helps improve time series classification by leveraging similarity patterns across different time points and variables. MTS2Graph [48] offers a strategy by constructing a graph that captures the temporal relationships between extracted patterns at each layer of the network. These methods underscore the development in GNN-based methods, highlighting their potential to revolutionize time series classification by effectively capturing complex dependencies and adapting to diverse scenarios."}, {"title": "3 Problem Formulation", "content": "Given a collection of medical time series from N participants denoted by {P1, . . , PN }, each participant has multiple samples of collected series {Xp1, , . . . , Xpkn }, where kn is the number of samples for the n-th participant. Each data sample Xp \u2208 RT \u00d7C represents the collected multivariate medical time series (e.g., multi-lead ECG) in one participation, where T denotes the number of timestamps and C is the number of channels; the corresponding label for sample Xp is represented as y \u2208 {0, 1} for binary medical classification problems where 0 indicates a healthy participant and 1 indicates a participant diagnosed with a specific disease, or represented as yp \u2208 {1, 2, . . . , c} for multi-class medical classification problems where each class corresponds to one kind of diseases or conditions. The objective of medical time series classification problem is to learn a mapping function f : RT \u00d7C \u2192 R1 that can accurately predict the label based on medical time series samples in each participation. Formally, given a participant's time series sample Xp, the goal is to predict the corresponding label y to indicate the disease or condition, which can be written as:\nyp = f(Xp; \u03b8),\n(1)\nwhere yp is label for i-th sample for n-th participant; the mapping function f is parameterized by the learnable parameters \u03b8.\nTo ensure the real-world clinical utility of the model, it is important to evaluate the generalization ability on unseen participants. Thus we split the dataset based on participants: specifically, we let Ptrain, Pval, and Ptest denote the disjoint sets of participants used for training, validation, and testing, respectively. With training and testing are on different participants, the generalization of the developed model can be evaluated on unseen participants or patients, which can simulate a more realistic estimate of its potential performance in real-world clinical settings. In addition, since one patient may visit hospitals for tests for many times, we also consider a complementary settings that split the dataset into training, validation and test data only relying on individual samples that can be represented by Xtrain, Xval and Xtest as disjoint sets of samples."}, {"title": "4 Methodology", "content": "To address the aforementioned challenges of medical time series classification, in this section, we elaborate on our proposed MedGNN, a Multi-resolution Spatiotemporal Graph Learning framework. The overall architecture of MedGNN is illustrated in Figure 1. It mainly consists of multi-resolution graph construction, difference attention networks, frequency convolution networks and multi-resolution graph transformer. For the medical time series, multi-resolution graph construction is utilized to learn the dynamic spatiotemporal representations, and then difference attention networks and frequency convolution networks are designed to capture the comprehensive temporal dynamics. Finally, multi-resolution graph transformer is employed to model the dynamic spatial dependencies from different resolutions. In the following, we elaborate on the core components of MedGNN from Section 4.1 to Section 4.4.\n4.1 Multi-resolution Graph Construction with\nMulti-scale Embedding Learning\nMedical time series typically consist of multiple channels that are often closely correlated. For example, in EEG signals, different brain regions may exhibit synchronization patterns, indicating real functional connectivity in the brain [32]; in ECG signals, different leads can provide complementary information about the cardiac electrical activity [4]. This motivates us to construct explicit structures to model channel-level dynamics and temporal dynamics for medical time series. To this end, we propose a novel Multi-resolution Adaptive Graph Structure Learning approach to model different levels of dynamics. This approach mainly consists of two main steps: 1) learning multi-resolution embeddings, and 2) constructing multi-resolution graphs based on the learned embeddings.\n4.1.1 Multi-scale Embedding Learning. Given a medical time series sample Xp \u2208 RT \u00d7C, we aim to learn multi-scale temporal embeddings that capture the local patterns and dynamics at different time scales. To achieve this, we employ a set of 1-d convolution networks for different resolutions. Let K1, , . . . , KM denote a series of M kernel sizes, where each kernel Km \u2208 Rkm\u00d71 has a size of km along the time dimension for different channels. The multi-scale embeddings are obtained by applying the 1-d convolution operations with different kernel sizes, formally by:\nZi(m) = Conv1D(Xp, Km), m = 1, . . . , M,\n(2)\nwhere Zi(m) \u2208 RTm\u00d7C represents the learned embeddings at the m-th resolution for i-th sample where we drop Pn for brevity, Tm = [T /km] is the size of transformed temporal embeddings, and C is the number of channels.\n4.1.2 Multi-resolution Graph Construction. After obtaining the multi-resolution embeddings, we then aim to construct multi-resolution graphs for structure learning. Specifically, we create a series graphs {Gi(1), . . . , Gi(M)}, where each graph Gi(m) = (Ai(m), xi(m)) initialized as a fully-connected graph corresponds to a specific resolution. Ai(m) \u2208 RC\u00d7C represents the adjacency matrix, and for each graph Gi(m), the node set Vi(m) consists of C nodes, where each node vi(m) \u2208 Vi(m) corresponds to a channel in the original series. The node feature matrix Xi(m) \u2208 RC\u00d7Tm is formed by the learned embeddings at the m-th resolution:\nxi(m) = (Zi(m))T .\n(3)\nThe edge set Ei(m) represents the dependencies and correlations among the channels. We initialize the adjacency matrix Ai(m) \u2208 RC\u00d7C as a learnable matrix to capture the edge weights between pairs of nodes. The edge weights can be learned adaptively to reflect the dynamic channel correlations at different resolutions. By constructing multi-resolution graphs, we could obtain a hierarchical representation that captures both channel-level and temporal dynamics at different time scales for the medical time series. This rich representation allows for modeling the complex local and longer contextual spatiotemporal patterns in medical time series.\n4.2 Difference Attention Networks for the\nBaseline Wander in Temporal Dynamics\nFor the physiological time series in the medical domain, the baseline wander [26] - the constant offsets or slow drifts towards baseline measurements - is a common artifact in ECG and EEG recordings. This could make the model capture the less meaningful patterns, i.e., the slow fluctuations around the baseline that may be caused by accidental factors such as patient movement or respiration. To address this problem, we further propose a new architecture, Difference Attention Networks, to learn the temporal dynamics of medical time series in a more focused fashion. The basic idea is to incorporate the concept of finite difference into the attention mechanisms. Note that the difference operates on the temporal dimension of the input data.\nSpecifically, instead of directly applying self-attention to the node features, we compute the differences along the temporal dimension and apply self-attention to these differences. We first add paddings to the temporal embedding dimensions to get Xri(m) \u2208 RC\u00d7Tm+1 obtained from the adaptive graph learning module at the m-th resolution, we first compute the first-order finite difference along the temporal dimension:\nDi(m)(t) = X'i(m)(t + 1) \u2013 X'i(m)(t), t = 1, . . . , Tm,\n(4)\nwhere Di(m) \u2208 RC represents the first-order difference of series representation at time step t, and X'i(m)(t) represents the value of X'i(m) at t. We then apply the multi-head self-attention mechanism to the difference representations Di(m) to learn the temporal dependencies. The self-attention is calculated as:\nAttni(m) = Softmax ( (Di(m) wi(m) ) (Di(m) wi(m) ) T ) Di(m) wi(m) , (5)\nwhere wi(m), wi(m) are learnable weight matrices for queries and keys. The outputs of difference self-attention are as:\nXDSA i(m) = DifferenceAttention(Xi(m)) = LinearDA(Attni(m)), (6)\nwhere LinearDA(\u00b7) represents the final linear layers for output, and Wi(m) are learnable weight matrices for values in self-attention. The final outputs of Difference Attention Networks are formed as:\nxi(m) = xi(m) + XDi(m) .\n(7)\nThe Difference Attention Networks provide a novel way to focus on the temporal changes in medical time series, reducing the influence of slow baseline drifts and highlighting the meaningful patterns.\n4.3 Frequency Convolution Networks for the\nMulti-view Temporal Representations\nThough the difference attention network can help address the baseline wander problem, the processed representations are in the \u201cdifference space\" and it might lose some information in the original data space. To better capture the multi-view information of medical time series signals, we further introduce the frequency convolution networks to enhance the temporal representations [10, 11, 45, 47]. Note that the temporal convolution networks are operated in parallel with Difference Attention Networks for multi-view information.\nSpecifically, given the node feature matrix Xi(m) \u2208 RC\u00d7Tm obtained from the multi-resolution graph construction at the m-th resolution, we first apply the Fourier transform to convert the temporal signals from the time domain to the frequency domain:\nXi(m)(f) = \u221e \u2212\u221e xi(m)(t)e \u2212j2\u03c0ft dt,\n(8)\nwhere F denotes the Fourier transform, f is the frequency variable, t is the integral variable, and j is the imaginary unit. The resulting Xi(m) \u2208 CC\u00d7S represents the frequency domain representation, where S is the number of frequency components. Next, we apply Fourier convolution layers to the frequency domain representations to capture the dependencies and patterns in the frequency space:\nHi(m) = FourierConvolution(Xi(m)) = Xi(m) \u2297 Wi(m),\n(9)\nwhere Wi(m) \u2208 CC\u00d7S represents the learnable convolution kernels in the frequency domain, and \u2297 denotes the element-wise multiplication. The resulting Hi(m) \u2208 CC\u00d7S represents the convolved frequency domain representations. Finally, we apply the inverse Fourier transform to recover the temporal representations from the frequency domain back to the time domain:\nXi(m) = F \u22121(Hi(m)) = \u221e \u2212\u221e Hi(m)(f)ej2\u03c0ftdf,\n(10)\nwhere F \u22121 denotes the inverse Fourier transform, and Xi(m) \u2208 RC\u00d7Tm represents the recovered temporal representations. The frequency convolution networks provide a complementary view of the temporal dynamics by operating in the frequency domain.\n4.4 Learning Spatial Dynamics with\nMulti-resolution Graph Transformer\nAfter obtaining the temporal representations from the Difference Attention Networks XDA i(m) and the Frequency Convolution Networks XFC i(m) at each resolution m, we aim to further capture the inter-series (spatial) dependencies based on the explicit structures of multi-resolution graphs. For this aim, we propose the multi-resolution graph transformer model for spatial dependency learning. Specifically, first, for each resolution, we sum the two learned temporal representations XDA i(m) and XFC i(m) to obtain the fused multiview representations at the m-th resolution:\nxfused i(m) = XDA i(m) + XFC i(m).\n(11)\nNext, inspired by Graph Transformer [31], we apply the local attention mechanisms first and then pass the representations to the graph neural networks for processing. Specifically, the local attention for graph processing is first computed as:\n\u03b1pq = exp(g(xp, xq) \u00b7 bpq)\n\u03a3k\u2208N(vp) exp(g(xp, xk) \u00b7 bpk)\n,\n(12)\nwhere \u03b1pq is the attention score, xp and xq are their node features, g is a function that computes the similarity between two nodes, i.e., the dot-product. bpq is a local attention bias term. Note that the neighborhood is dynamically learned in Section 4.1. Then we can calculate the graph representations by:\nXi(m) GA = LocalAttention(xfused i(m)) = \u03b1xfused i(m),\n(13)\nwhere Xi(m) GA \u2208 RC\u00d7Tm represents the spatially attended representations. The multi-head self-attention allows the model to attend to different spatial locations and capture the dependencies among different time series channels. After obtaining the spatially attended representations, we apply graph convolution networks to further incorporate the graph structure information at each resolution. Given the constructed graph Gi(m) (Ai(m), Xi(m) GA) at the m-th resolution, we perform graph convolution as follows:\nXi(m) GT = GraphConv(\u00c2i(m), Xi(m) GA , Wi(m)),\n(14)\nwhere \u00c2i(m) = \u010e \u2212 \u00bd \u00c3i(m)\u010e \u2212 \u00bd is the normalized adjacency matrix, \u00c3i(m) = Ai(m) + I is the adjacency matrix with self-loops, I is the identity matrix, \u010e is the diagonal degree matrix of \u00c3i(m), Wi(m) \u2208 RTm\u00d7Tm is the learnable weight matrix for graph convolution. The resulting Xi(m) GT \u2208 RC\u00d7Tm represents the graph convoluted representations at the m-th resolution. Since we have multiple resolutions, we need to fuse the representations obtained from different resolutions to capture the multi-resolution dynamics. We achieve this by applying average pooling across the resolution dimension:\nXMRFused = AvgPool(X1 GT ), . . . , X(m GT )),\n(15)\nwhere AvgPool() denotes the average pooling operation, and the output XMRFused \u2208 RC\u00d7T represents the fused multi-resolution representations. Finally, we feed the fused multi-resolution representations XMRFused into a linear or fully-connected layer followed by a softmax activation function for classification:\n\u0177 = Softmax(LinearGT (XMRFused)),\n(16)\nwhere LinearGT (\u00b7) is a learnable linear layer that maps the fused representations to the output space, and \u0177 \u2208 RK represents the predicted probabilities for the K classes."}, {"title": "5 Experiments", "content": "In this section, we perform extensively experiments with five real-world medical time series benchmarks to assess the performance of our proposed MedGNN. Furthermore, we conduct thorough analytical experiments and visualization studies concerning the different components of the MedGNN framework.\n5.1 Experimental Settings\n5.1.1 Datasets. We conduct empirical analyses on five representative medical datasets, i.e., ADFD [24], APAVA [8], TDBRAIN [34], PTB [29], and PTB-XL [36]. These datasets include three EEG datasets and two ECG datasets. The data preprocessing and train-validation-test split are following the previous work [39]. For further details on the datasets, please refer to the Appendix A.1.\n5.1.2 Baselines. We compare our proposed MedGNN with the representative and state-of-the-art models for time series classification. We choose the baseline methods from two categories: (1) GNN-based models, which include TodyNet [22], SimTSC [51], FourierGNN [46], and CrossGNN [13]; (2) Transformer-based models, including iTransformer [23], PatchTST [27], FEDformer [54], Crossformer [53], Autoformer [41] and more recent Medformer [39]. Further details about the baselines can be found in Appendix A.2.\n5.1.3 Implementation Details. All experiments are implemented using Pytorch 2.2 [28] and conducted on 4 GeForce RTX 4090 GPUs. We employ cross-entropy loss as the loss function and present five metrics: accuracy, precision, recall, F1 score, and AUROC. Further implementation details are presented in Appendix A.3 and A.4.\n5.2 Main Results\nWe present results of our proposed MedGNN compared to several representative baselines with two different experimental setup (e.g., sample-based and subject-based evaluation) in Tables 1 and 2. Note that the percentage symbol (%) is omitted in the experimental results, and this will not be repeated below."}, {"title": "5.3 Ablation Studies", "content": "5.3.1 Study of the Multi-Resolution Graph Learning. We explore the impact of multi-resolution graph learning (MRGL) on MedGNN's performance across various metrics. To assess its effectiveness, we compare two versions of the model: one with MRGL enabled and one without it. The MRGL-enhanced version demonstrates substantial performance gains as shown in Figure 2, particularly in recall and F1 score, indicating the model's improved ability to detect and balance positive predictions with accurate classifications. This improvement highlights MRGL's capacity to capture multi-resolution dependencies effectively, improving the model's robustness in handling complex medical time series.\n5.3.2 Study of the Frequency Convolution Networks. To evaluate the impact of the Frequency Convolution Networks (FCN) within MedGNN, we perform ablation experiments, as shown in Figure 3, where \"w/o FCN\" denotes the version without FCN. The results clearly demonstrate that incorporating FCN improves MedGNN's performance across all metrics on the experimental datasets. Moreover, frequency convolution helps prevent potential information loss from relying solely on difference attention, further enhancing the temporal representation.\n5.3.3 Study of the Difference Attention Networks. In this section, we aim to investigate the effectiveness of the difference attention in the MedGNN framework. Table 3 has shown the performance comparison of the variant on two EEG datasets (APAVA, TDBRAIN) and one ECG dataset. The version equipped with difference attention achieves improvements of 23.65%, 27.02%, 24.17%, and 25.10% in the metrics of Accuracy, Precision, Recall, and F1 score on EEG datasets in average, while 6.54%, 10.35%, 9.02%, and 8.52% respectively on ECG. This indicates that difference attention can enhance the model's performance in medical time series classification tasks by minimizing the impact of slow baseline drifts and bringing important patterns to the forefront."}, {"title": "5.4 Additional Experiments", "content": "5.4.1 Efficiency Analysis. We comprehensively evaluate the model efficiency from three aspects: classification performance (Accuracy), training speed, and memory footprint. Specifically, we choose two different sizes of datasets: the APAVA (23 subjects, 5,967 16-channel samples) and TDBRAIN (72 subjects, 6,240 33-channel samples) datasets. The closer a marker is to the upper-left corner of Figure 4, the higher the model's accuracy and the faster its training speed. Additionally, the smaller the marker's area, the lower the memory usage during training. Therefore, we can conclude that although MedGNN's training time and memory footprint are at a moderate level among all baselines, its classification performance is the best.\n5.4.2 Visualizations. Figures 5 and Figure 6 present visualizations of the learned adjacency matrices across different resolutions, providing insights into what the model captures at each resolution. Overall, the matrices tend to be sparse, indicating that MedGNN focuses on learning meaningful correlations from the data rather than relying on superficial variable aggregation. We also find that the weight distributions of adjacency matrices vary across different resolutions within the same dataset, indicating that the relationships between variables learned through graph learning differ at each resolution. This is significant for practical applications, as the relationships between variables are often not fixed but instead vary depending on the scale or context."}, {"title": "6 Conclusion", "content": "In this paper, we have proposed a Multi-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical time series classification. We have constructed multi-resolution adaptive graph structures to learn dynamic multi-scale embeddings. Based on the graph structure, we have proposed two types of networks, i.e., Difference Attention Networks and Frequency Convolution Networks, to address the baseline wander problem in medical time series and learn the multi-perspective information for temporal modeling. We have also adapted Multi-resolution Graph Transformer for the dynamic spatial learning and information fusion. Extensive experiments have shown the superiority of our methods in different settings. We hope this could facilitate more future works on medical time series and healthcare applications."}]}