{"title": "PXGen: A Post-hoc Explainable Method for Generative Models", "authors": ["Yen-Lung Huang", "Ming-Hsi Weng", "Hao-Tsung Yang"], "abstract": "With the rapid growth of generative AI in numerous applications, explainable AI (XAI) plays a crucial role in ensuring the responsible development and deployment of generative AI technologies. \u03a7\u0391\u0399 has undergone notable advancements and widespread adoption in recent years, reflecting a concerted push to enhance the transparency, interpretability, and credibility of AI systems. Recent research emphasizes that a proficient XAI method should adhere to a set of criteria, primarily focusing on two key areas. Firstly, it should ensure the quality and fluidity of explanations, encompassing aspects like faithfulness, plausibility, completeness, and tailoring to individual needs. Secondly, the design principle of the XAI system or mechanism should cover the following factors such as reliability, resilience, the verifiability of its outputs, and the transparency of its algorithm. However, research in XAI for generative models remains relatively scarce, with little exploration into how such methods can effectively meet these criteria in that domain.\nIn this work, we propose PXGen, a post-hoc explainable method for generative models. Given a model that needs to be explained, PXGen prepares two materials for the explanation, the Anchor set and intrinsic & extrinsic criteria. Those materials are customizable by users according to their purpose and requirements. Via the calculation of each criterion, each anchor has a set of feature values and PXGen provides example-based explanation methods according to the feature values among all the anchors and illustrated and visualized to the users via tractable algorithms such as k-dispersion or k-center. Under this framework, PXGen addresses the abovementioned desiderata and provides additional benefits with low execution time, no additional access requirement, etc. Our evaluation shows that PXGen can find representative training samples well compared with the state-of-the-art.", "sections": [{"title": "1 Introduction", "content": "In the field of Artificial Intelligence, generative models such as GANs [13,38], VAEs [22,20,25], and diffusion models [27,21] has been developed well and have a variety of applications including synthetic images & audio [2,9], text generation [12], reinforcement learning [35], graphic rendering [34] and texture generation [11],"}, {"title": "2 Related Work", "content": "With the rise of deep learning models, XAI has grown fast in recent years [10,17,41,15,30] and common algorithms such as LIME and SHAP [31,26] have been applied in many learning models. In generative AI, some work is proposed to increase the in- terpretability of the model. One famous instance is the disentangled method [36], which captures the disentangled information such as pose or angles and separates it into independent factors in the network to learn the hidden representation. Another common method is to calculate the influence function among (training) samples and identify the most responsible samples during the training phase."}, {"title": "3 Framework", "content": "In PXGen, the framework is composed of three phrases: Preparation phrase, Analysis phrase, and Discovery phrase. In the Preparation phrase, three objects are needed as the input: the model which is desired to be explained, the an- chor set which includes all the instances that are used for explaination, and the (multi-)criteria to describe the various properties of a sample. In the Analysis phrase, each anchor (i.e., the instance in the anchor set) derives a vector of fea- ture values which are calculated via the criteria. After that, A statistical analysis is provided for post-hoc interpretation of the model. Lastly, we use several algo- rithms to discover a (sub)set of the anchors and select representative anchors to"}, {"title": "3.1 Preparation Phase", "content": "Initially, three necessary items have to be prepared (model, Anchor set, criteria) for PXGen.\nModel: The input models PXGen considers are basically the ones that learn the underlying probability distribution of the data they are trained on, enabling them to generate new samples that resemble the original data distribution. Let X represents the observed data and Z represent any latent variables. The goal is to learn the conditional distribution P(X|Z) and the prior distribution P(Z). This allows us can explore both the exterior samples X and interior prior distribution Z via different criteria.\nAnchor set: Each anchor is an instance that is formatted consistently with the model's output, such as images or text, but not limited to the training data. As we consider the model to be a black box, we require dataset that can be intu- itively understood by people to enter the model for exploration, thus obtaining information about the model's behaviour, which we refer to as the Anchor set. By interacting with the model using these Anchors set to obtain information, dif- ferent Anchors will yield different information, which can be processed for more concrete results. Through the interaction between Anchors and the model, the model's behavior is indirectly mapped, Although there is no explicit limitation on the anchors. The selected anchors influence the quality of understandability, completeness, and complexity of the explanation.\nCriteria: The selected criteria are interpretable functions to process the infor- mation resulting from the interaction between Anchors and the model. These criteria should cover different interpretable aspects to acquire varied properties of the plausibility of the anchors. In this work, we divided them into intrinsic criteria and extrinsic criteria. The intrinsic criteria is to illustrate the \"distance\" between an anchor and the model from the model's perspective. Such as calcu- lating Kullback-Leibler divergence (KLD) [24] of the VAE's [22] latent vector. On the other hand, the extrinsic criteria describes the extrinsic feature between the anchors and the output instances that are generated by the models. Repre- sentative examples such as Mean Square Error(MSE) [1] and Structural Similar- ity(SSIM) [40], both are commonly used to measure the similarity between two images.\nBased on this observation, we propose a Post-hoc global explainable frame- work that utilizes real data along with various criteria to explore models. Since"}, {"title": "3.2 Analysis Phase", "content": "In PXGen, anchors intuitively represent the behavior of the model, but not all anchors can adequately represent the model. For example, there should be sig- nificant differences in the similarity between the model's output and the original image in the interaction between the model that can only generate dog images and the anchor with horse images. To address this issue, each anchor can be mapped to a multi-dimensional space based on values calculated from intrinsic and extrinsic criteria, providing specific interpretations in the multi-dimensional space. Another part involves obtaining information from the model and mapping it to the same multi-dimensional space as the anchors, utilizing some statistical analysis methods (such as median, mean, variance, etc.) to analyze the possible distribution of the model from this information. Therefore, users can classify the anchors in the multi-dimensional space to find groups of anchors with unique characteristics, such as those that best represent the behavior of the model, or those that the model erroneously believes are similar to the training data, thereby interpreting the model."}, {"title": "3.3 Discovery Phase", "content": "Based on the results of the analysis phase, we can obtain several groups of anchors with different features. For the groups of interest to us (such as the most representative anchor set), we often find that the number is huge, mak- ing it difficult for humans to intuitively understand. To overcome this problem, we need some algorithms to help select anchors, resulting in a few key anchors that can represent the particular group. Different algorithms, based on vari- ous methods and concepts, would select different Anchors. For example, the k-dispersion [18,3,4] algorithm picks the most dissimilar individuals in the group, or k-means [37] selects the central individuals of the group. User can choose different algorithms according to their needs."}, {"title": "4 Demonstration : Variational AutoEncode", "content": "In this section, we show how to use our framework to identify groups of Anchors with different characteristics, and how to apply algorithms to a specific group to find the most characteristic individuals, achieving an intuitive understand- ing. The demonstration uses two models, the classical Variational AutoEncoder (VAE) [8,22], and Soft-IntroVAE [6] as the explained model in PXGen."}, {"title": "4.1 The Model, Anchor Set, and criteria", "content": "The model Mis a classical VAE and trained by the hand-writing dataset, MNIST [7]. To show the explainability of PXGen, we only use 4,000 images with the label \"0\" as the training set. Notice that PXGen has no information of the training process in M nor the training set. The rest of the data in MNIST is used as the anchor set, which is around 54,000 pictures with handwriting digits from \"1\" to \"9\" and approximately 2000 pictures with digit \"0\".\nIn the multi-criteria part, we choose Kullback-Leibler Divergence (KLD) as the intrinsic criterion and Mean Squared Error (MSE) as the extrinsic criterion. In KLD part, since the generation mechanism of M is based on sampling from a specific distribution. To estimate the predicted distribution of each anchor we reversed the process of M, i.e., put the anchor into the encoder to get the predicted distribution from M. Then we use KLD to calculate the difference of distribution between the predicted distribution and the actual sampling distri- bution, N(0, I), of M. On the other hand, as we mentioned earlier, MSE is a common criterion for measure the difference between two simple pictures (in this case, hand writing digits are relatively simple). In calculation, given an anchor a, we get the reconstructed sample a' via putting a into the encoder and decoder sequentially. We then calculate the MSE between a, a'. The high-level idea is that, if the MSE is low it means M \"knows\" how to generate pictures similar to a. Thus, we can quantify how much M \"knows \" a via this extrinsic criterion.\nModel M' is a Soft-IntroVAE model trained on the CIFAR10 dataset, in which we use 5,000 images with the \"Automobile\" category as the training set. Then, we pick 300 images from each category of the testing dataset as the anchor set. In the multi-criteria part, we choose Kullback-Leibler Divergence (KLD) as the intrinsic criterion. We choose Fr\u00e9chet Inception Distance (FID) [19] as the"}, {"title": "4.2 Analysis Phase: Classifying Anchors and Analysis", "content": "To acquire distributions under both extrinsic and intrinsic criteria for the clas- sification of anchors, we first generate a set of images from M. After that, we calculated the maximum values under each criterion as the thresholds. To preserve the stability of the maximum values for each criterion, we used the aver- age of the maximum values obtained from multiple iterations as the thresholds. There are two advantages of this method. Firstly, it has a statistical context which is not only tractable but also improves the reliability of the threshold. This allows us to associate the threshold with their respective statistical vari- ables more confidently. Secondly, it is independent to the anchor set, meaning that changes to the anchor set would not affect the behavior of the model, which makes PXGen more robust.\nWith the thresholds, an anchor a is said to have high intrinsic affinity if and only if the KLD value of a is under the average maximum value. Otherwise, it is said to have low intrinsic affinity. Correspondingly, an anchor a is said to have high extrinsic affinity if and only if the MSE value of a is under the average maximum value, and low extrinsic affinity otherwise. By combining these four descriptions, the anchor set can be divided into four groups: HIHE (high intrinsic & high extrinsic affinity), HILE (high intrinsic & low extrinsic affinity), LIHE (low intrinsic & high extrinsic affinity), and LILE (low intrinsic & low extrinsic affinity)."}, {"title": "Analysis:", "content": "We have a keen interest in two distinct groups within HILE and LIHE, denoted as \"model delusion\" and \"aligned conception\", respectively. In HILE, an- chors exhibit high intrinsic affinity, indicating the model's confidence in gener- ating them with a high probability. However, their low extrinsic affinity suggests that the resulting output image differs significantly from the anchor. We refer to this phenomenon as \u201cmodel delusion\", signifying the model's erroneous be- lief that it understands the \"concept\" of the anchors (i.e., generating a similar latent vector), but misapplies it, resulting in dissimilar outputs after decoding. Concrete examples illustrating this phenomenon are depicted in Figure 3.\nIn the statistical context, we focus solely on the subset of anchors with a KLD value within the 5% range among all anchors. We assume that model M correctly identifies the \"concepts\" within this subset of anchors. MSE and FID serve as extrinsic criterion, aiding in quantifying the disparities between the model and external concepts. Therefore, an increase in MSE (or FID) indicates a decrease in extrinsic affinity, reflecting a worsening case of model delusion, as depicted in the top figure. These data points are visualized in the lower portion of Figure 3, where the bottom left figure displays input samples, and the bottom right figure showcases reconstructed samples, sorted from low to high MSE (or FID). It's evident that as extrinsic affinity decreases, the images are erroneously decoded into outputs divergent from their original anchors but aligned with the model's own concept, resembling handwritten digit 0.\nOn the contrary, LIHE shows a different story. The model appears to diverge from the \"concept\" of the anchors but effectively utilizes them, a phenomenon we term \"aligned conception\". This suggests that despite differing model concepts, there is alignment to some extent, facilitating transfer learning within this group. Concrete examples are depicted in Figure 4.\nSimilarly, the top figure exclusively features anchors with MSE (or FID) val- ues within the 5% threshold among all anchors, indicating high external affinity with the model. These anchors are sorted based on increasing KLD (decreasing intrinsic affinity), demonstrating alignment with external concepts. The corre- sponding data points are visually represented in the bottom section of Figure 4. The bottom left figure displays input samples, while the bottom right figure showcases reconstructed samples, ordered from low to high KLD. Remarkably, despite decreasing intrinsic affinity (increasing KLD), the images continue to be accurately decoded into outputs resembling their originals, even without prior exposure to these images."}, {"title": "4.3 Discover Phase: Visualizing the most Characteristic Anchors", "content": "To visualize a group of anchors, PXGen introduces two algorithms for selecting representatives from these group: k-dispersion [18,3,4], and k-center[29,5]. Each of these algorithms considers different concepts. The k-dispersion algorithm is designed to identify the k most distinct individuals within a designated group by systematically selecting those k individuals that exhibit the most distinguishing features. And the k-center algorithm is an approach used in cluster analysis, which is focused on partitioning a dataset into k distinct clusters."}, {"title": "4.4 Finding Representative Training Samples", "content": "Determining the most representative data for the model can be challenginging. Here, we give a specific purpose for the XAI mission. We want to find a subset of training samples that influences the trained model the most, or vice versa. That is, the most representative training samples.\nThe test model M has the similar setting in Section 4.1, which is trained on a training dataset containing 5,923 images with the label \"0\". The criteria for PXGen are conducted using MSE and KLD and the anchor set is the training dataset only.\nUpon utilizing PXGen, the training dataset can be divided into four groups, and each data point is assigned an anchor value coming from the addition of the KLD and MSE values. According to the discussion in Section 4.2, we believe that the training data within the HIHE group are the most representative, while the other groups (LILE, LIHE, HILE) are the opposite. The anchor value indicates"}, {"title": "1. Validation", "content": "To verify whether the \"highly representative data\" identified by PXGen, we remove some of the training samples and observe the difference of the model performance after the removal. We compare the image similarity between the original model and the ones with removal. Intuitively, if a model trained with the removal of non-representative samples, the generated results would be closer to the original model, compared with the ones that removes samples randomly.\nWe considered three scenarios. Firstly, we trained a model M-HIHE using only the data from the HIHE group. Secondly, we trained a model M-Others using training data, but removed the training data from the HIHE group in an amount equivalent to that of the other groups, prioritizing the removal of low anchor values. Thirdly, we trained a model M-Random by using train- ing data from which an amount equivalent to that of the other groups has been randomly removed, as a baseline. Subsequently, we compared the im- age similarity between the images generated by these three models and the images generated by the original model. Here, we used the Frechet Inception Distance (FID) [19] to measure image similarity."}, {"title": "2. Comparison with TracIn:", "content": "In this part, we compare PXGen with the VAE-TracIn [23] method, which evaluates an influence score to determine which training data point is most"}, {"title": "5 Conclusion", "content": "In conclusion, PXGen provides an innovative post-hoc explainable method for generative models by classifying data and recognizing concepts from the model's perspective, aiding in the understanding of complex generative artificial intelli- gence. Moreover, this method holds great potential for applicability across differ- ent scenarios, such as aiding in model training, copyright protection, and more."}]}