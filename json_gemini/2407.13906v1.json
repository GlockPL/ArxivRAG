{"title": "Crafting Efficient Fine-Tuning Strategies for Large Language Models", "authors": ["Michael Oliver", "Guan Wang"], "abstract": "This paper addresses the challenges of efficiently fine-tuning large language models (LLMs) by exploring data efficiency and hyperparameter optimization. We investigate the minimum data required for effective fine-tuning and propose a novel hyperparameter optimization method that leverages early-stage model performance. Our experiments demonstrate that fine-tuning with as few as 200 samples can improve model accuracy from 70% to 88% in a product attribute extraction task. We identify a saturation point of approximately 6,500 samples, beyond which additional data yields diminishing returns. Our proposed bayesian hyperparameter optimization method, which evaluates models at 20% of total training time, correlates strongly with final model performance, with 4 out of 5 top early-stage models remaining in the top 5 at completion. This approach led to a 2% improvement in accuracy over baseline models when evaluated on an independent test set. These findings offer actionable insights for practitioners, potentially reducing computational load and dependency on extensive datasets while enhancing overall performance of fine-tuned LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the field of natural language processing. However, fine-tuning these models for specific applications remains a critical challenge, often requiring substantial computational resources and expert knowledge. This paper addresses these challenges by focusing on two key aspects of the fine-tuning process: data efficiency and hyperparameter optimization. Data efficiency is a critical consideration in fine-tuning, as the amount of labeled data required can be a limiting factor in many real-world scenarios. Collecting and annotating large datasets is time-consuming and expensive, and in some cases, such as in specialized domains or low-resource languages, extensive data may not be available. Therefore, understanding the minimum amount of data needed to achieve effective fine-tuning is crucial for optimizing resource allocation and reducing the burden of data acquisition.\nOur experiments investigate the relationship between data volume and fine-tuning performance, seeking to identify a range where the model achieves strong results with the least amount of data. By systematically varying the size of the fine-tuning dataset, we aim to provide insights into the diminishing returns of additional data beyond a certain threshold. These findings can guide practitioners in making informed decisions about data collection and allocation, ultimately leading to more efficient fine-tuning processes.\nIn addition to data efficiency, hyperparameter optimization plays a vital role in the success of fine-tuning. Hyperparameters, such as batch size, learning rate, and epochs, have a significant impact on the model's performance and convergence speed. However, finding the optimal combination of hyperparameters is a complex and time-consuming task, often requiring extensive trial and error or relying on heuristics and default values.\nTo streamline this process, we propose the use of Bayesian optimization techniques for hyperparameter search. Bayesian optimization offers a principled approach to exploring the hyperparameter space by balancing exploration and exploitation. By leveraging the information gained from previous evaluations, it intelligently suggests promising hyperparameter configurations to evaluate next. This approach allows for a more efficient search, reducing the number of computationally expensive training runs required to find optimal settings.\nWe study these two questions in the context of developing a solution to task of extracting specific information from web pages from a variety of e-commerce sites. For example, we seek to extract information such as product titles, descriptions, and prices from product pages, and contact information and social media links from contact pages.\nThe complexity of the task arises from the variability in page formats across different sellers and the need to adapt models to accurately capture this diversity with limited labeled data. This setting serves as a practical demonstration of our methods for exploring data efficiency and optimizing hyperparameters, showcasing their applicability in a real-world, domain-specific scenario."}, {"title": "2 Related Work", "content": "LLM fine-tuning has drawn tremendous research interest recently. Some research focuses on efficient fine-tuning methodologies, e.g., PEFT Ding et al. (2023) (Parameter-Efficient Fine-Tuning) and LORA (Low Rank Adaption) Hu et al. (2021), while others focuses on RHLF data construction policies, e.g., Tajwar et al. (2024). However, to the best of our knowledge, there is little research being done for studying the bayesian optimization on hyperparameter selection for LLM fine-tuning. Jin et al. (2023) proposed a learning rate adjustment strategy named LRBench++ that dynamically update learning rate among iterations. Our work combines a systematic approach to searching hyperparameter combinations, including learning rate, batch size, LORA rank, etc. Our optimization target is to improve the model accuracy on the real testing data rather than training loss.\nSystematic and effective hyperparameter searching for LLM pretraining is another related field. The primary motivation in this context is to identify optimal hyperparameter settings using small-scale training experiments, which are feasible on a limited number of GPUs with much less data compared to full-scale operations. These experiments aim to derive hyperparameter configurations that can be effectively applied in real pretraining tasks involving tens of thousands of GPUs and potentially lasting many months. This approach is designed to optimize resource allocation, ensuring that the extensive and costly resources involved in large-scale pretraining are used efficiently. The findings of Yang et al. (2022) underscore the practical utility of this approach, demonstrating how even minimal initial experiments can inform the scalable deployment of LLMs in ways that conserve computational effort while maximizing performance outcomes across extensive neural network architectures. This method aligns with the broader objective of making large-scale machine learning operations more sustainable and efficient, particularly in scenarios where computational resources are a significant bottleneck. As what is going to be shown in Section 5, we also emphasized the intuition that if a set of hyperparameters yields better early testing accuracies it would be likely to produce better final accuracies. And using such intuition we could design the fine-tuning procedure more compute-efficient.\nAutomatic hyperparameter search strategy has been a long-standing hot topic in machine learning community. Wu et al. (2021) proposed a local search approach that integrates a randomized direct-search method to effectively balance validation loss minimization with computational cost constraints. Wang et al. (2021) further combined this local search with a Bayesian optimization approach to form a \"blend search\" which is more robust for both tree-based models as well as neural networks. AutoLRS by Jin et al. (2021) introduces a novel approach to dynamically adjust learning rates using Bayesian Optimization, illustrating significant improvements in training efficiency across several architectures, including ResNet-50 and BERT. This method highlights the potential of adaptive learning rate schedules to reduce computational overhead and improve model generalization without extensive manual hyperparameter tuning. However, no experiments were done on LLM fine-tuning cases.\nThere are some recent explorations of using LLM for product attribute extraction Fang et al. (2024); Sabeh et al. (2024); Brinkmann et al. (2024). Different from their focus, our work mainly explores effective fine-tuning strategies and product attributes extraction is the specific task to verify the effectiveness of our approaches. Moreover, in our experiments, we not only tested it in product attributes but also other generic information such as store contact, seller information, etc."}, {"title": "3 Fine-tuning Task Description and Dataset Construction", "content": "The primary fine-tuning task in this paper is to steer the LLM to accurately extract and classify specific attribute values from a diverse array of e-commerce web pages. This task is critical in enhancing the model's applicability to real-world e-commerce scenarios, where accurate information extraction directly influences business outcomes and customer experiences.\nOur training data is sourced from e-commerce website, as raw HTML and URLs. We use LLM queries to extract the following three types of information: First, the class of web page (contact page, product details, list of products, etc). Second, a set of attribute values. For example, for product details pages, we extract attributes such as the title, price and description. Third, forms URLs we are interested in from certain types of pages, such as keyword search JavaScript blocks."}, {"title": "3.1 Fine-tuning Task Description", "content": "The primary fine-tuning task in this paper is to steer the LLM to accurately extract and classify specific attribute values from a diverse array of e-commerce web pages. This task is critical in enhancing the model's applicability to real-world e-commerce scenarios, where accurate information extraction directly influences business outcomes"}, {"title": "3.2 Dataset Construction", "content": "We define a training sample of 5000 web pages, re-weighted to sample rarer classes of web pages such as contact pages. For this study, we focus on 8 attributes, 4 found in product details pages and 4 found in contact pages, listed in Table 4 in Appendix A. We extract the \"ground truth\" with OpenAI's GPT4 (OpenAI, 2024). As the set of attributes we query for a web-page depends on the classification, the accuracy of the extraction depends partially on the accuracy of the web-page classification. For each page, we use our \"truth\" LLM, GPT4, to do each of the above tasks, saving the prompt, input, and output. We then filter and correct a few common mistakes made by GPT4.\nFrom the 5000 pages and the three types of queries, we produced 12k sets of prompts, inputs, and outputs, which is roughly equivalent to 100M tokens.\nIn order to evaluate the accuracy of our models on our prioritized page types (product details and contact pages), we compile and label (with GPT4) two test sets. The first test set of 200 pages (100 product details and 100 contact pages) serves as a validation set, being used to test model accuracy in both the data efficiency and hyperparameter optimization studies. In both studies, choosing the sample sizes or hyperparameters with the best accuracy on the first test set allows the models to over-fit the testing set. For this reason, we define a second test test of 242 product details pages and 242 contact pages, to determine the final accuracy of our best models."}, {"title": "4 Data Efficiency Analysis", "content": "To investigate the relationship between fine-tuning data volume and model performance, we conduct a systematic study varying the number of training samples while maintaining a consistent set of hyperparameters given in Table 1. Our experimental design encompasses a range of 0 to 10,000 samples, with the original Llama-3-8B-Instruct model serving as our baseline (N = 0)."}, {"title": "4.1 Accuracy Metrics", "content": "We define attribute-specific accuracy as the ratio of correctly extracted non-empty attribute values to the total number of non-empty values in the ground truth set. This metric accounts for the variable presence of attributes across web pages while not penalizing hallucinations in cases of empty truth values. Our overall accuracy metric is the mean of eight individual attribute accuracies, equally weighted between product details and contact page attributes.\nWe picked 4 attributes for product details pages and 4 attributes for contact pages to evaluate, based on our interest and their relative frequency. For each model, we compute an average of these 8 accuracies, and use this as our general metric for the models. The individual accuracies and the average accuracies for our models are shown in figure 1."}, {"title": "4.2 Discussion", "content": "Our findings reveal several key insights:\n\u2022 Rapid Initial Improvement: Even with a modest 200 samples (approximately 100 web pages), we observe a substantial increase in model accuracy from 70% to 88%.\n\u2022 Diminishing Returns: The majority of accuracy gains are achieved by 1,000 samples, with subsequent improvements becoming more gradual.\n\u2022 Attribute-Specific Trends: Late-stage accuracy improvements are predominantly driven by a single attribute type - product rating, which is notably less frequent in our dataset, appearing in only about 25% of product details pages.\n\u2022 Performance Plateau: Maximum performance is generally attained at approximately 6,500 samples, suggesting a potential \"sweet spot\" for data efficiency in our specific task domain.\nThese results underscore the importance of strategic data sampling in fine-tuning large language models for specialized tasks. While even small datasets can yield significant improvements, careful consideration must be given to the distribution of attributes within the training data to ensure comprehensive model performance across all target variables. Our findings have important implications for practitioners seeking to optimize the data collection and annotation process for fine-tuning LLMs in resource-constrained environments. Future work could explore the impact of data augmentation techniques or alternative sampling strategies to further enhance model performance, particularly for less frequent attributes."}, {"title": "5 Hyperparameter Optimization", "content": ""}, {"title": "5.1 Problem Formulation", "content": "The task of efficient hyperparameter optimization can be formalized as a two-part optimization problem:\n\u2022 Finding the optimal set of hyperparameters \\(\\theta^*\\) that maximizes the performance metric M of a model f on a validation set Dval:\n\\(\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} M(f_{\\theta}(D_{val}))\\)\nwhere \\(f_{\\theta}\\) represents the model trained with hyperparameters \\(\\theta\\). In our context, M is the average accuracy across eight selected attributes.\n\u2022 Maximize the correlation \u00b5 between the model's performance at early step t\u2081 and its performance at final step t2:\n\\(max \\mu(M(f_{\\theta}(D_{val}, t_1)), M(f_{\\theta}(D_{val}, t_2))\\)\nwhere t2 represents the end of the training process (in our case, 80% of total epochs).\nThis formulation encapsulates our goal of developing an efficient tuning method that uses less computation time by leveraging early-stage performance (t1) to predict final performance (t2), while still achieving high final accuracy."}, {"title": "5.2 Methodology", "content": "Algorithm 1 Efficient Hyperparameter Optimization\nInput: Hyperparameter space \\(\\Theta\\), iterations N, early eval time t\u2081, final eval time t2\nOutput: Optimal hyperparameters \\(\\theta^*\\)\n1. Initialize results pool P \u2190 \\(\\emptyset\\)\n2. for i = 1 to N do\n3. k \u2190 \\(k_{explore}\\) if i < N/2, else \\(k_{exploit}\\)\n4. \\(\\theta_i\\) \u2190 BayesianOptimization(P,k)\n5. Train model \\(f_{\\theta_i}\\) up to time t\u2081\n6. Evaluate \\(M_i\\) = M(\\(f_{\\theta_i}(D_{val}, t_1)\\))\n7. P\u2190 P\u222a {(\\(\u03b8_i\\), Mi)}\n8. Select top k configurations from P\n9. for each \\(\\theta_i\\) in top k configurations do\n10. Continue training \\(f_{\\theta_i}\\) from t\u2081 to t2\n11. Evaluate \\(M_j\\) = M(\\(f_{\\theta_i}(D_{val}, t_2)\\))\n12. \\(\\theta^*\\) \u2190 argmax\\(M_j\\)\n13. return \\(\\theta^*\\)\nThe motivating idea behind this is to implement a version where we fine-tune models only until time t\u2081 and freeze the fine-tuning, running hyperparameter optimization with a Bayesian procedure until finding an ideal set of hyperparameters, and unfreezing the models with those hyperparameters. In order to test whether such a method would work, we fine-tune for the whole duration, but use the model generated early at time t\u2081 for evaluation for the hyperparameter optimization. We defined t\u2081 in terms of the fraction of the total number of epochs, so that the amount of data seen by the model by time t\u2081 does not depend on the batch size. We then define models at two later times, t2 and the minima of the validation loss that is computed by LLaMA-Factory. For this study, we set t\u2081 = 0.2 and t2 = 0.8. By checking if the best models at t2 and the minimum loss time are also the best models at time t\u2081, we can determine if the proposed freezing and hyperparameter optimization procedure is viable. If not, that would indicate the hyperparameters optimizing the performance at t\u2081 are not necessarily the best hyperparameters for the whole training time.\nWe design the following steps for the implementation of the hyperparameter optimization:\n1. Run the LoRA fine-tuning a set of hyperparameters\n2. Evaluate the accuracy on a validation test set using the model at step t\u2081\n3. Save the hyperparameter configuration and accuracy to pool of all results\n4. Run a Bayesian optimization algorithm on the pool to produce the next set of hyperparameters\nAt each step, optimizer updates a Gaussian process regression surrogate model of the black-box system, which is where the function mapping between input hyperparameter sets and output average model accuracies (technically the negative accuracies, as scikit-optimize only has a \"minimize\" function, and no \"maximize\" function). We used the lower confidence bound (LCB) acquisition function, which picks new configurations to check using the surrogate model and its mean \\(\\mu(x)\\) and variance \\(\\sigma(x)\\) models according to:\n\\(f(x) = \\mu(x) \u2013 \\kappa \\sigma(x)\\) (1)\nThe recommended configurations are found from the minima of x in \\(f(x)\\). When the \\(\\kappa\\) value is high, this prioritizes the exploration of configuration space areas with low uncertainty. When it is low, this prioritizes the exploration of areas of low mean and low uncertainty. We incorporate this into our loop by having the instances use the high value of \\(\\kappa\\), \\(k_{explore}\\) during the first half of the experiment, and then a lower value, \\(k_{exploit}\\) in the second half. The specific values for these, as well as the other global parameters of our study, are given in Table 6 in Appendix D."}, {"title": "5.3 Hyperparameter Space", "content": "Our study focuses on optimizing 6 key hyperparameters of the LoRA fine-tuning process. These hyperparameters were selected based on their significant impact on model performance and their potential for interaction effects. The first set was LORA target layers. Two standard approaches are to adapt the qproj and Uproj matrices or to adapt qproj, kproj, Uproj, and Oproj matrices (Hu et al., 2021). To give a richer parameter space to explore the trade-off between adaptation flexibility and computational efficiency, we define 4 choices of target layers, indexed by an integer that adds a new layer with each increase.\nIn addition to the LORA layers, we include the learning rate, the batch size, (To keep memory usage under control, we used gradient accumulation to simulate batch sizes.), and three more hyperparameters of LoRA: the rank R, LORA \u03b1, and the dropout. The rank R determines the dimensionality of the low-rank approximation in LoRA. Higher R increase model capacity but also computational cost. The LoRA \u03b1 parameter sets the strength of the LoRA adapter matrix relative to the pretrained projection matrices and is similar to the learning rate (Hu et al., 2021). Finally, the LoRA dropout parameter controls the probability for the dropout mechanism in LoRA, described in (Lin et al., 2024), masks rows and vectors from the LORA matrices.\nTwo hyperparameters that we do not vary are the choice of LR scheduler and the LR warm-up period, for which we use the LLaMA-Factory defaults of cosine and 0.1, respectively.\nThe ranges we use for each of the hyperparameters are given in Table 2."}, {"title": "5.4 Results and Analysis", "content": ""}, {"title": "5.4.1 Performance Correlation Across Training Stages", "content": "Our study reveals a strong correlation between early-stage model performance and final model accuracy, validating our hypothesis that early evaluation can effectively predict overall model quality. This finding has significant implications for efficient hyperparameter optimization in large language model fine-tuning.\nTo test if the models produced by tuning to the accuracy at time t\u2081 are also optimized models at t2 and at the minimum loss time, we test the validation accuracy of each of the models with different hyperparameter settings derived from the Bayesian optimization phase. The total of 60 models' performance is shown in figure 2. Models with 0 accuracy indicate that the fine-tuning produced non-functioning models, the details of this are investigated in Appendix C.1. From there, we see early performance as a good indicator: Of the top 5 models at time t\u2081 (20% of total training time), 4 remained in the top 5 at time t2 (80% of total training time). This remarkable consistency demonstrates the predictive power of early-stage evaluation 1.\nCorrelation with Loss Minima. By chance, one of the randomly chosen initial models (model 4) performs the best at time t\u2081, but this model is not in the top 10 models at the minima or at t2. The other 4 models in the top 5 t\u2081 models are all in the top 5 t2 models, suggesting that the t\u2081 performance is a good predictor of the t2 performance. However, only 2 of the top 5 t\u2081 models had corresponding minima models in the top 5. Of the top 5 minima models, 4 of the models had corresponding entries in the top 10 t\u2081 models. This suggests that while early performance is a strong indicator of final accuracy, the relationship with the loss minimum is more nuanced.\nTo mitigate potential overfitting concerns, we evaluate the top 5 models on a larger, independent test set, with the results in Table 8 in Appendix D. The results double confirm the strong correlation between early and final performance, with only minor shifts in relative rankings.\nOne final test is done to compare the best model found in the HPO procedure to the model created during the data efficiency study with the same sample size (although the data efficiency study used 12"}, {"title": "6 Conclusions", "content": "Our study underscores the success of efficient fine-tuning strategies for Large Language Models, particularly using LoRA with only 200 samples to enhance product attribute value extraction significantly. Our hyperparameter optimization approach, which involves early model evaluation, effectively predicts final performance, confirming that early accuracy strongly correlates with later outcomes. This method offers a more resource-efficient way of tuning, which is beneficial for practitioners aiming to refine LLM fine-tuning while conserving resources and ensuring high performance."}, {"title": "A Attribute Information", "content": "The attributes that we query and evaluate for accuracy for product and contact pages are listed in Table 4."}, {"title": "B Data Efficiency Tables", "content": "The accuracies for each of the models for each type of data are given in Table 5."}, {"title": "C Hyperparameter Optimization Notes", "content": ""}, {"title": "C.1 Zero Accuracy Models", "content": "One notable finding during the experiment is that some combinations of hyperparameters produced gibberish without end tokens, which added to the testing time. For these models, we assign an accuracy of 0. It was found that these issues occurred when both the LoRA \u03b1 and learning rate were high, as visible in Figure 3. This is understandable in light of the close relationship between learning rate and alpha described by (Hu et al., 2021). Our optimization procedure automatically detects this relationship, and does not use a high \u03b1 parameter and learning rate together during the exploitation phases."}, {"title": "D Hyperparameter Optimization Tables and Additional Plots", "content": "The validation loss curves for a sample of the models, excluding those that failed significantly in training, is shown in Figure 4.\nSeveral parameters, including the data size, number of epochs, and Bayesian optimization parameters, in our hyperparameter optimization study are given in Table 6.\nThe top 10 model IDs and validation accuracies from each time (t1, t2, and the minima of the evaluation loss) are shown in Table 7. These accuracies, especially the t\u2081 accuracies, are potentially affected by overfitting.\nThe results from testing the accuracy of our best HPO models on an independent test set of data not used for the Bayesian optimization are given in Table 8. The Model IDs correspond roughly to the order in which they started fine-tuning."}]}