{"title": "SELECTIVE ATTENTION IMPROVES TRANSFORMER", "authors": ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"], "abstract": "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.", "sections": [{"title": "1 INTRODUCTION", "content": "Different tasks have different memory requirements. On one extreme, copying an arbitrary sequence requires retaining all sequence elements in memory. On the other extreme, determining whether a specific element appeared at least once, only requires persisting a constant amount of memory.\nTransformers (Vaswani et al., 2017) keep the entire history in their context buffers, allowing them to solve tasks such as copying, while famously leading to their squared attention cost. RNNs (Rumel-hart et al., 1986) and their modern structured state space variants (Gu et al., 2022; Gu & Dao, 2024) keep only a constant-sized sketch of the history, making inference cost linear, but rendering them incapable of solving tasks such as arbitrary string copying.\nCan we design a model that persists just the right amount of context?\nSeveral works (see Section 8) aim to improve costs by compressing or otherwise reducing the context size with minimal impact to quality. We take a different approach, focusing instead on quality improvement, and treating cost reductions as a side benefit. Specifically, it has been demonstrated (Leviathan, 2022) that for some tasks removing unneeded elements from the context buffer enables more efficient transformer programs. Indeed, in the attention's differentiable memory, all memory cells contribute to the data read, and circuitry is needed to filter out the noise generated by irrelevant memories. Reducing the amount of circuitry needed should improve performance.\nIn this work we propose Selective Attention, a simple extension to the standard attention mechanism which allows a token to decide that another token is no longer needed, reducing the attention that future tokens will pay to it. Selective attention adds no new parameters and only a negligible amount of computation, yet yields meaningful improvements in synthetic tasks and natural language modeling for a range of model and context sizes. Additionally, we show that elements that are selected to be forgotten by selective attention can be safely removed from the attention's context, leading to substantial reductions in the memory and computation requirements during inference, without penalizing quality. We name our method after the related neuroscience concept of selective attention. Quoting Plebanek & Sloutsky (2017): \"Selective attention allows adults to focus on task-relevant information, while ignoring task-irrelevant information. This in turn leads to superior processing of task-relevant information.\""}, {"title": "2 MOTIVATING EXAMPLES", "content": "Consider a transformer processing an input sequence with three tokens: a, b, c. In a given layer with the standard attention module, token b can decide how much to read from token a, and token c can decide how much to read from token a, but token b cannot affect how much token c reads from token a. Specifically, if token b has determined that token a is irrelevant or even misleading to future tokens such as c, there is nothing it can do in the given layer to correct for this. Even in subsequent layers, masking token a is not trivial. Selective attention enables exactly such masking. To illustrate its usefulness, let's consider the Variable Assignment problem, as well as natural language modeling.\nIn Variable Assignment the input consists of a set of repeated assignments to named variables, followed by a query for the latest value for one of the variables which the model needs to output. For example, for the input: y=7; x=1; x=3; z=5; x=? the output is 3. Note that Variable Assignment can be seen as a generalization of the Search problem (Leviathan, 2022), where repeated occurrences of the query pattern are allowed, and we are tasked with finding the most recent occurrence. It is well known that the Search problem is easily solvable by standard transformers, via induction heads (Olsson et al., 2022). Selective attention facilitates a simple reduction from Variable Assignment to Search, whereby every assignment to a variable masks out all previous assignments to the same variable. In Figure 1 (top) we see that this is indeed the case for a transformer trained with selective attention. In Appendix A.1 we show that a transformer with selective attention easily learns a general solution to Variable Assignment while a standard transformer does not.\nTo motivate selective attention for natural language modeling, we first note that Variable Assignment is a common sub-task, e.g. when persisting a state. For further motivation, consider the common case where a part of the input is ambiguous, and the ambiguity is only resolved at a later token. For example, in the sequence: Bar, %32%ack, Obama, the first token Bar encodes several competing meanings, but the later tokens ##ack and Obama resolve it to the entity for the ex-president. For many tasks that are mostly concerned with the semantic meaning, later tokens might not want to read the ambiguous meaning from the earlier tokens, so masking them, as with selective attention, might be useful. In Figure 1 (bottom) we see that this is indeed the case for a transformer trained with selective attention. In the visualized layer, the last token in multi-token expressions masks out the earlier tokens. For example, ##ack masks out bar; obama masks out both bar and ##ack; ##bm masks out i; and ##la masks out both u and ##c. We also observe additional masking, e.g. the token after masks out the tokens a and day, perhaps because the token after absorbed the meaning from the tokens a and day, or perhaps because the model deems the extra detail is not helpful at this point.\nFinally, Figure 1 also shows that for the trivial task of Parity*, where intermediate results are stored every other token, so that the model's output is only a function of the last two tokens, every-thing but the last two tokens is masked. For the Copy task, selective attention persists the entirety of the string to be copied until copying starts, and then masks out every element as it is copied."}, {"title": "3 SELECTIVE ATTENTION", "content": "Selective attention is a simple modification on top of standard attention. For context size N, we produce a real-valued N \u00d7 N soft-mask matrix S where Si,j denotes how much token xi masks token xj (see Section 3.1). We then constrain S, e.g. to be causal and non-negative (see Section 3.2). We finally accumulate the information in matrix S into a new matrix F, taking into account masking by all previous tokens (see Section 3.3). The matrix F = Accumulate(Constrain(S)) is then simply subtracted from the attention logits before applying the softmax:\nSelectiveAttention(Q, K, V) = softmax($\\frac{QKT}{\\sqrt{dk}}$ - F)V"}, {"title": "3.1 SELECTION FUNCTION", "content": "Computing the selection matrix S is akin to computing a compatibility score, and many functions can be used, for example, a separate bilinear form. Following an observation from Leviathan (2022) that a common case for a token wanting to mask another is after absorbing its contents (i.e. after attending to it), we instead simply reuse the result of one of the existing heads of the attention module. This means that the selection function adds no new parameters or computation. Note that the head still contributes to attention as usual. We also experimented with a separate bilinear form, and in spite of adding additional parameters and computation, this resulted in the same or slightly worse results (see Appendix A.2)."}, {"title": "3.2 CONSTRAINTS", "content": "Following observations from Leviathan (2022), we apply the following constraints to S:\n1. Zero out negative values (i.e. applying ReLU), only reducing attention, never boosting it.\n2. Zero out the first column, so as not to mask the <BOS> token.\n3. Zero out the diagonal, so as not to let a token mask itself.\nWe observe that all three constraints improve performance (see Appendix A.3)."}, {"title": "3.3 ACCUMULATION", "content": "In this work we focus on transformer decoders, so selective attention cannot influence the attention operation by past tokens. We chose cumulative summation as our accumulation function. We observe some improvement by only applying the masking for future tokens (i.e. the masking by a token would not affect its own attention operation), so Fi,j = $\\sum_{k<i\u22121} Sk,j$ (see ablation in Appendix A.4).\nattn_logits = einsum(\"bhnd, bhmd->bhnm\", Q, K) / sqrt(dk)\nattn_logits = where (causal_mask, attn_logits, float(\"-inf\"))\nS = attn_logits[:, 0]\nS = relu(S)\nS[..., 0] = 0\nS = (1 - eye(n)) * S\nS = roll(S, 1, -2); S[..., 0,:] = 0\nF = np.cumsum (S, axis=-2)\nattn_logits -= F[:, None]\nattn_weights = softmax(attn_logits)"}, {"title": "4 CONTEXT PRUNING", "content": "As presented in Section 3, while beneficial to model quality, selective attention has negligible impact on inference efficiency\u00b9. However, an additional modification can improve inference efficiency substantially. Specifically, selective attention can reduce the memory and computation requirements of the attention module, via pruning elements from the context buffer.\nTo see how, note that once a token is sufficiently masked by selective attention, it will not contribute meaningfully to any future attention operations. Such tokens can be safely evicted from the context buffer. We could pick a fixed threshold, and prune all elements whose soft masking is higher than"}, {"title": "5 EXPERIMENTAL SETUP", "content": "In all of our experiments we use a decoder-only transformer with multi-head attention, as in Vaswani et al. (2017), with the following modifications: we use Pre-LN instead of Post-LN (Xiong et al., 2020), learned position encoding, SwiGLU gates instead of MLPs (Shazeer, 2020), normalize the Q and K projections (Dehghani et al., 2023)2, remove the biases (Raffel et al., 2023), and replace the LayerNorms with RMSNorm (Zhang & Sennrich, 2019). Note that we tested other variants, including a vanilla decoder-only transformer exactly as in (Radford et al., 2019) and observed similar results. We trained our models with the AdamW optimizer with \u03b2\u2081 = 0.9 and \u03b22 = 0.999 for a total of 524,288 steps. We used cosine decay and 1,000 linear warmup steps and a learning rate of 0.005. We repeated some of the experiments with different learning rates and obtained similar results. We used a batch size of 256 and a fixed context size of 512 for all training runs except for the context size experiments (Figure 3 left) where we used a batch of 128. We follow Esser et al. (2024), and parameterize a model size by a parameter d such that dmodel = 64d and nheads = Nlayers = d (see Table 7 in Appendix A.9). We trained all of our models on TPUv4s. For the language modeling experiments, we used the C4 (Raffel et al., 2023) dataset with a vocabulary of size 8K tokens built with the SentencePiece tokenizer (Kudo & Richardson, 2018). We repeated some of the experiments with a vocabulary of size 32K and observed similar results. We also ran experiments with WikiText (Merity et al., 2016), and lm1b (Chelba et al., 2014) and observed similar results."}, {"title": "6 RESULTS", "content": null}, {"title": "6.1 GENERATION QUALITY", "content": "Transformers with selective attention perform consistently better, as measured by perplexity on the validation set, across model and context sizes. We also observe consistent improvement on a downstream task, HellaSwag (Zellers et al., 2019)."}, {"title": "6.2 INFERENCE EFFICIENCY", "content": "Efficiency improvements via selective attention stem from a reduction in the attention module's context size when using pruning as in Section 4. Specifically, a smaller context translates directly to more efficient inference in common scenarios. Indeed, note that during inference with a large context and batch size (bn >> d), loading the KV-cache (linear in the size of the context) dominates the memory bandwidth (Pope et al., 2022), which is often the bottleneck for generation (Shazeer, 2019). In addition, for very large context sizes (n >> d), taking the dot product of the query and the keys in the cache and calculating the weighted average of the values in the cache both dominate compute, i.e., in this setup, a smaller context directly translates to similar gains in FLOPs.\nWhen pruning the context with selective attention, we measure substantial improvements in the memory requirements for the attention module, at the same or better perplexity than the baseline without selective attention."}, {"title": "7 SELECTION PATTERNS", "content": "It is interesting to question which elements from the context are being masked by selective attention for language modeling. Figure 5 illustrates the values of the F matrix for a specific example (see Appendix A.6 for the full example text). We observe that some layers (e.g. 6) are mostly dense (i.e. low F values), while other layers (e.g. 2) are sparse (i.e. high F values). As expected, all layers persist some of the most recent elements, but several of the sparse layers (e.g. layers 1, 4, and 9) also persist elements for long time periods, as can be seen by the vertical lines. This suggests that"}, {"title": "8 RELATED WORKS", "content": "Transformer Improvements. Our work aims at improving the transformer architecture. Since its introduction in Vaswani et al. (2017), a large volume of research proposed architecture modifications towards an improved model. Some notable works here include Pre-LN instead of Post-LN (Xiong et al., 2020), gated units instead of MLPs (Shazeer, 2020), removing the biases (Raffel et al., 2023), using RMSNorm instead of LayerNorm (Zhang & Sennrich, 2019), normalizing the Q and K projections (Dehghani et al., 2023), and multi-query and group-query attention (Shazeer, 2019; Ainslie et al., 2023)."}, {"title": "9 DISCUSSION", "content": "In this work we introduced Selective Attention, a simple parameter-free change to the standard attention mechanism which consistently improves language modeling performance across model sizes and context lengths, and can lead to substantial inference efficiency improvements. Given that it adds no new parameters, only a negligible amount of compute, and provides consistent improvements, selective attention might be a good default for transformer decoders.\nLimitations and future directions. We applied selective attention to decoder-only transformers. It could be interesting to investigate its applicability to encoders as well; Reducing the size of the context as in Section 4 improves inference efficiency but not training efficiency. It might be interesting to explore iteratively reducing the size of the context buffer during training; We did not further train the models after removing elements as per Section 4. It seems conceivable that further improvements could be achieved with some additional training after context reduction; We only experimented with pre-training models with selective attention. It is interesting to investigate how it could be applied in a fine-tuning step to existing models; While we observed similar results with selective attention in several setups (Section 5), there are still important variants we did not test, notably transformers with multi-query (Shazeer, 2019) and grouped-query (Ainslie et al., 2023) attention, as well as models much larger than 1B parameters; Finally, it would be interesting to implement selective attention in a GPU-aware way, similar to Flash Attention (Dao et al., 2022)."}, {"title": "10 IMPROVING NEURAL ARCHITECTURES", "content": "In The Art of Transformer Programming, Leviathan (2022) selected a set of foundational problems (sorting, searching, addition, etc.) and manually implemented transformers to solve them (i.e. by manually setting the model's weights). They showed that several programs become much easier, especially for small transformers, when equipped with a mechanism allowing to selectively mask items in the context buffer, similar to selective attention. They further hypothesized that such a mechanism will have similar positive effects on language modeling, which motivated our work.\nZhou et al. (2023) proposed the RASP-Generalization Conjecture, that \u201cTransformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths\", i.e. that problems that are easily solved by transformers are those that are easily solved by human programmers using RASP. It follows that problems that are not easily solved by humans using RASP are hard for transformers as well, and if we made those easier, by changing the transformer architecture (and respectively the capabilities of RASP) we could meaningfully improve transformers. Similarly, when constructing transformer programs by hand, Leviathan (2022) notes that \"... the most interesting cases are those that are hard for us humans and are hard for the optimizer or the architecture, and understanding these better might be key to creating better AI systems.\"\nWe are very strong advocates for this method, and believe that finding basic problems for which we cannot program a general solution by hand on a neural model, is an extremely fertile approach for producing further architecture improvements."}, {"title": "A.1 TRANSFORMERS WITH SELECTIVE ATTENTION LEARN A GENERAL SOLUTION TO VARIABLE ASSIGNMENT", "content": "Transformers with selective attention reach close to 0 validation loss and 100% precision extremely fast when trained on Variable Assignment and they generalize well to out of distribution cases, unlike transformers without selective attention.\nSetup. We train small transformers (d = 3), with and without selective attention, to solve the Variable Assignment problem with 3 variables, 1,000 possible values, and 128 assignments. We train with a batch size of 2,048 for 65,536 steps.\nIn distribution. The transformer with selective attention reaches a validation loss of 0.002 (and 100% accuracy) after less than 1,000 training steps. The transformer without selective attention only achieves a validation log-perplexity loss of 3.18 and 26% accuracy after 1,000 step. At the end of training (65,536 steps) the transformer with selective attention obtains a loss of 2.2e-8, whereas the transformer with standard attention is at 0.01. Both transformers reach 100% accuracy at the end of training.\nOut of distribution. We observe much stronger generalization capabilities for the transformer with selective attention. When we run on an out of distribution test set with the same 3 variables but only 2 possible values, the transformer with standard attention's accuracy drops substantially to 70% (loss of 3.64). Meanwhile the transformer with selective attention maintains 100% accuracy (with a loss of 2.4e-8).\nWe observed similar results in other settings (e.g. 10 variables and 10 possible values). We also repeated the experiments with somewhat larger transformers (d = 8) and observed similar results."}, {"title": "A.2 SEPARATE BILINEAR FORM", "content": "We experiment with using a separate bilinear form instead of reusing the output of an existing attention head. We compare transformers (for d = 8 and d = 12) trained with selective attention on C4 for 524,288 steps, to similarly trained transformers where the selection function is implemented via a separate bilinear form (adding additional parameters and computation). The transformers with standard selective attention (i.e. sharing the outputs of an existing attention head) achieve the same or slightly better log-perplexities at the end of training (see Table 2)."}, {"title": "A.3 ABLATING THE CONSTRAINTS", "content": "We ablate the 3 constraints selective attention applies to S (see Section 3.2).\nNegative selection. While with selective attention a token can decide to reduce attention to another token by all future attention operations, allowing a token to strengthen another token's contribution to all future attention operations does not make sense. Indeed, when removing this constraint (dropping the ReLU, so that S can contain negative values) the training does not converge.\nMasking the <BOS> token. Since several algorithms can benefit from the existence of the sentinel <BOS> token (Leviathan, 2022), it is plausible that masking it is detrimental. When we allow"}, {"title": "A.4 SELF-IMPACT", "content": "Table 5 compares the results of forbidding self-impact (i.e. not allowing a token to affect its own attention operation, as in Section 3.3) to those when allowing it (i.e. not shifting the matrix S). As can be seen, the shifting provides a small but consistent improvement."}, {"title": "A.5 PERPLEXITY-EFFICIENCY TRADE-OFF", "content": "Figure 6 illustrates the trade-off between perplexity gains and efficiency gains when pruning as in Section 4. See Section 6 for details."}, {"title": "A.6 EXAMPLE DETAILS", "content": "The following text from the C4 validation set was used in Figures 5, 9, and 10:\n\"\nthe real problem with traditional dental veneers has little to do\nwith how they function or their performance who determines what\na normal aesthetically pleasing smile looks like is the real\nissue america struggled for decades with defining image\nas a marketplace bent on exploiting peoples flaws for economic\ngain the danger of this national obsession has become systemic\nsince the days of twiggy fashion magazines offer photoshopped\n\"\n\"\nperfection as the standard to which we should aspire the\neffects of this insidious marketing made their way into breast\nimplants and the definition of a hollywood smile carving their\nbodies and their teeth people use their resources to chase a\nfalse picture of their perfection self people make investments\nin the tens of thousands of dollars at their dentist office to get\nthis message has become so endemic that\n\"\n\"\n\nthe perfect smile\npeople with nice smiles are convinced only a perfect hollywood\nsmile is acceptable one particularly relevant example of this\nis highlighted in a june 2015 article entitled saving jane '\ns smile gary nankin dds discusses how he saved the\nsmile of a patient who was not content with her first set of #\nporcelain veneers 1 endodontic referral for treatment of\n\"\n\"\n\"\n4\n2\n3\ntooth number 15 followed by a composite core build up\nperiodontal therapy in both the anterior region and upper left\nto achieve optimal tissue health preparation of maxillary\nteeth and placement of permanent restorations placement\n\"\n\"\n\"\n5\nof dental implant by the periodontist followed by preparation of\n6\nmandibular teeth and placement of permanent restorations\nrestore the now fully healed and osseointegrated implant in the\nposition of tooth number 30 regarding a person s smile the\nstrong link to self esteem and self worth make an imperfect\nset of teeth a concern however the picture in the article\nclearly illustrates what appears to be a well constructed and\nhealthy-looking smile how does a dentist promote the entire\nsmile that 97 % of the people\n\"\nin america would love to show off ?"}, {"title": "A.7 COMPARISON WITH LOCAL ATTENTION", "content": "Table 6 compares the validation perplexity of d = 12 transformers with various local attention patterns (all-local and alternating), to that of a standard transformer and to that of a transformer with selective attention. For the all-local attention transformers we set all layers to be sliding window attention layers with a fixed sized window. For example, \"All-local 32\" denotes a transformer where all tokens can only attend up to 32 tokens back. We also include transformers with alternating local and global layers, where we have 3 local attention layers followed by 1 global attention layer, in a repeated fashion. For example, \u201cLocal-global 32\" denotes a transformer with 3 local attention layers where tokens can only attend up to 32 tokens back, followed by a global layer where tokens can attend to all past tokens, and this 4-layer structure is repeated for the 12 layers of the d = 12 transformer. We report the perplexity numbers after 524,288 training steps. We observe that all local attention patterns perform worse than the dense baseline, which in turn performs worse than a transformer with selective attention."}, {"title": "A.8 ADDITIONAL FIGURES FOR CONTEXT PRUNING", "content": "Figure 7 illustrates the values of the F matrix averaged across 1,000 examples of length at least 512 from the C4 dataset.\nFigure 8 compares the values from Figure 7 to those obtained from a different training run (different random initialization and different data shuffle). While this isn't always the case, we sometimes observe stable sparsity patterns like those in this example, hinting at some general properties of language modeling on C4.\nFigure 9 illustrates the tokens that remain in the context buffer after pruning (as in Section 4) for the example text (see Appendix A.6), for a d = 12 model with selective attention, trained with the Lmem loss (Equation 2, \u20ac = 0.1), for a memory budget where the validation perplexity matches a transformer without selective attention. The per-layer memory budgets chosen by the pruning algorithm for this model are: [8, 48, 8, 8, 24, 8, 168, 16, 8, 64, 8, 8], leading to a memory saving factor of 16X.\nFigure 10 shows which tokens are pruned for the example text from Appendix A.6."}, {"title": "A.9 PARAMETER COUNTS", "content": "Table 7 shows the actual parameter counts for models with different ds (see Section 5). Note that selective attention does not add any extra parameters."}]}