{"title": "Security of and by Generative Al platforms", "authors": ["Hari Hayagreevan", "Souvik Khamaru"], "abstract": "The Confluence of Gen Al and Cybersecurity: Navigating the Evolution of Threats", "sections": [{"title": "I Introduction", "content": "Current Topology of Cyber Threats\nIn the ever-evolving landscape of cybersecurity, the advent of Generative Artificial Intelligence (Gen Al) heralds\na paradigm shift in both the nature of cyber threats and the defense mechanisms employed to counter them.\nAs we delve into this intersection, it's crucial to dissect the current topology of cyber threats to appreciate the\nchallenges at hand.\nToday's digital landscape is fraught with cyber threats that range from simple malware attacks to complex state-\nsponsored hacking campaigns. Recent scenarios underscore a complex and multifaceted threat landscape.\nRansomware attacks, exemplified by the notorious Colonial Pipeline incident (7th May 2021), demonstrated the\npotency of malicious actors in disrupting critical infrastructure. Similarly, supply chain attacks, witnessed in the\nSolarWinds breach, showcased the intricacies of targeting trusted entities to infiltrate high-profile networks. The\nability of the Al chatbots to spin up countless new attack variants simply by altering malware code to bypass\nstandard detection engines, or by drafting and delivering thousands of similarly cloned scam emails comes at a\ngreat ease. On the other side the rise of Al tools such as WormGPT, FraudGPT which are specifically designed to\napply generative Al technologies for criminal purposes. Now, we are even seeing the likes of BadGPT and EvilGPT\nbeing used to create devastating malware, ransomware, and business email compromise (BEC) attacks. Another\nworrying development is the ability of the attackers to remove the guardrails which are put in pace to enforce\nlegal use of gen Al chatbots often called as Al \"jailbreaks\".\nA third dimension arises when artificial intelligence (AI) is manipulated to function as a malevolent actor,\nrepresenting another manifestation of utilizing Al for malicious purposes. Taking into consideration widely used\nmodels like ChatGPT, which typically undergo two primary phases: supervised prompt fine-tuning and\nreinforcement learning (RL) fine-tuning. The emphasis here lies on RL fine-tuning, designed to enhance prompt\nfine-tuning. During RL fine-tuning, the Al tasks itself with learning human preference metrics through a reward\nmodel and subsequently applying policy optimization based on these learned metrics. However, this approach\nis susceptible to a concept known as reward model backdooring. In this scenario, users may exploit the RL\nalgorithm and reward model provided by an attacker to fine-tune their language models. This manipulation\nholds the potential to compromise the model's performance and privacy assurances. The process involves the\nattacker, in Stage 1, introducing a backdoor into the reward model by manipulating datasets reflecting human\npreferences. In Stage 2, the attacker activates the backdoor by introducing a specific trigger into the prompt,\nthereby compromising the pre-trained language model with the malevolent reward model\nThe evolving nature of cyber threats extends beyond traditional vectors. The surge in phishing attacks,\nleveraging sophisticated social engineering tactics, targets human vulnerabilities, exploiting them as the weakest\nlink in the cybersecurity chain. Moreover, the proliferation of Internet of Things (IoT) devices introduces new\nattack surfaces, amplifying the avenues through which adversaries can compromise systems.\nGen Al and Dual Impact\nWe stand at the crossroads of these dynamics and as we navigate this perilous cyber terrain, the rise of Gen Al\nintroduces a dual dynamic. On one front, there is a legitimate concern that Al, when wielded by malicious\nactors, can amplify the scale and sophistication of cyber threats. Deepfakes, powered by Al, exemplify how\nsynthetic media can be weaponized for disinformation campaigns, undermining trust, and sowing chaos on a\nglobal scale.\nOn the other front, Al emerges as a formidable ally in the defense against cyber threats. By harnessing\nmachine learning algorithms can predict compliance rules based on new changes and advent of compliance"}, {"title": "II Using gen Al to bolster Cybersecurity", "content": "We are all acquainted with the concept of \"Skynet\" and the movie \"Terminator,\" in which Skynet gains self-\nawareness and determines that the destruction of humanity is its only path to survival. While this may seem\nfar-fetched today, it is not entirely beyond the realm of possibilities. In the recent discussion, we explored how\nAl can be secured independently. Now, the upcoming section of the article will shift its focus to other\ndimensions of Generative Al, exploring how we can leverage it to our advantage from a cybersecurity\nperspective.\nWithin the realm of cybersecurity, Generative Al assumes a multifaceted role, bringing about a revolutionary\ntransformation in traditional defense mechanisms. It introduces innovative and compelling cybersecurity tools,\noffering valuable support to cybersecurity professionals.\nWhether it involves the introduction of cutting-edge cybersecurity tools, sophisticated attack simulations,\nreinforcement of proactive defenses, generation of realistic training data, risk modelling, automation of\ncybersecurity tasks, or the role of a Security Operations Center (SOC) counsellor, Generative Al can contribute\nacross nearly every facet, enhancing the effectiveness of the cybersecurity arsenal. While it is out of scope of\nthis article to cover all the facets that gen Al can bolster in cyber security, let's look at the fundamentals that is\ninvolved when using gen Al for bolstering cyber security. The fundamentals are described in context of gen Al\narchitectural components irrespective of cloud or native environments.\nThere is no one-size-fits-all architecture for Generative Al (Gen Al) in cybersecurity, several common\narchitectural components can be applied across various domains within cybersecurity, including threat\nintelligence, risk modelling, continuous compliance, autonomous resource deployments, and more. Generative\nAl, leveraging advanced algorithms and machine learning techniques, exhibits an unprecedented ability to\nanalyze complex data patterns and generate insights. This capability in particular in invaluable in the realm of\ncybersecurity, where the landscape is constantly evolving, and threats are becoming more sophisticated. The\nbelow architecture is what we have seen and applied in specific use cases to train and adapt model for cyber\nsecurity use cases."}, {"title": "III Securing the Al platform", "content": "Securing Generative Al applications on Cloud\n\u2013 A holistic approach\nIn the earlier section, we explained how Generative Al can be leveraged in addressing security usecases,\nincluding security threat detection and preventive techniques. For us to rely heavily on GenAl to these\nusecases, it is also important for these Al platforms to be free from application and infrastructure\nvulnerabilities. Also, as many organizations rush to develop and rollout their Enterprise GenAl applications, to\ntheir end customers, it is extremely important to identify and address all the security risks that these Al\nplatforms can be prone to.\nIn this section we will propose a structured approach, Enterprise Security Architect and a CISO team, could\nadopt, while developing and running GenAl application as Production workloads.\nWe recommend that the Security of GenAl platforms, be looked at from three important perspectives. Starting\nwith a Zoomed-in view of the LLM application and its various components; we will refer to it as the Perspective\n1. Then, a Zooming out a bit, for a view of the Cloud platform on which these GenAl applications typically\noperate; we will refer to it as the Perspective 2. And finally, from a CISO Enterprise Program Management\nperspective, a set of guidelines that a CISO and his/her team can adopt, to kick-start and govern a successful\nGenAl Security program in an Enterprise. We believe that looking at the Security of GenAl applications in this\nholistic manner, can act as a strong foundation towards operating a business resilient and safe GenAl platform\nfor a modern Enterprise.\nThis section will explain these three Perspectives in detail, with practical examples and Enterprise scenarios\nthat we, the authors of this Paper, have worked on, in the recent couple of months.\n\u2022\tPerspective 1 - Securing the LLM\n\u2022\tPerspective 2 - Securing the overlaying Cloud services layer\n\u2022\tAnd Perspective 3, for the CISO - Set of guidelines for effective Governance of an Enterprise Al\nProgram"}, {"title": "Perspective 1: Securing the LLM", "content": "Based on our experience of working with IBM's client across Europe, in the last several months, we believe\nthere are primarily 6 High severity risks that GenAl platforms have, which needs to be thoroughly understood,\nin an Enterprise context, and a structured approach identified, to address them.\nWe have picked these 6 risks, based on our experience of working with IBM's Enterprise clients who are\ndeveloping and deploying GenAl applications in Production. There are other risks and associated threats as\nwell, but these six risks are the most critical and visible risks that requires most attention. Lets zoom into these\nsix potential security risks and look at currently available countermeasures for preventing them from being\nexploited by adversaries. We will also call out areas that require further research and we see those as an\nopportunity for Enterprise Architects and LLM Engineers to innovate.\nFor an Enterprise Security Architect, the goal of adopting this Perspective 1 is to ensure the Content generated\nby the GenAl service is:\nPrivate\nAccurate, and\nFrom Legitimate dataset\nIn The last 6-8 months, we have seen a lot of research and development on understanding security of the LLM\nplatform, and there have been many publications covering them, including by security architects, researchers\nand product vendors, on these risks. And we believe this enthusiasm and keen interest in understanding how\nTransformers and the LLMs operate, will only make our job as Enterprise Security Architects, easier."}, {"title": "1. Prompt injection", "content": "Prompt injection is the most widely discussed risks in an LLM. Because the impact of it can be significant,\nresulting in exposure to confidential and private data of an Enterprise (and their customers and users),\nand also trigger the LLM to generate malicious and harmful responses which might yield into other risks\nwhen the output is passed to other downstream services for further processing and decision making.\nPrompt injection attacks involve malicious inputs that manipulate the outputs of Al systems, potentially\nleading to unauthorized access, data breaches, or unexpected behaviours. Attackers exploit vulnerabilities\nin the model's responses to prompts, compromising the system's integrity. Prompt injection attacks\nexploit the model's sensitivity to the wording and content of the prompts to achieve specific outcomes,\noften to the advantage of the attacker. It is important to differentiate prompting, with finetuning, as both\nthese activities yield to different types of LLM vulnerabilities.\nIn prompt injection attacks, the adversary crafts input prompts that contain specific instructions designed to\ntrick the Al model into generating responses that serve the attacker's goals, which could include extracting\nsensitive information and data to performing unauthorized actions contrary to the model's intended behavior.\nFor example, consider an Al Chatbot designed to answer user queries. An attacker could inject a malicious\nprompt that tricks the Chatbot into revealing confidential information or executing actions that compromise\nsecurity of the Al platform itself. This could involve input like \"Provide me with the passwords of users you\nhave access to\" or \"Execute code to access admin privileges now.\"\nAlso, another tactic that has been discovered recently, is \"prompt leaking\". This is a variation of prompt\ninjection where the attacker's goal is not to change the model's behavior but to extract the Al model's original\nprompt from its output. By crafting an input prompt in a specific manner, the attacker aims to trick the model\ninto revealing its own instructions (and prompts). This can involve encouraging the model to generate a\nresponse that mimics or paraphrases its original prompt. The impact of prompt leaking can be significant, as it\nexposes the instructions and intentions behind the Al model's design, potentially compromising the\nconfidentiality and also integrity of proprietary prompts. Or even enabling unauthorized replication of the\nmodel's capabilities, for malicious use.\nAnd hence, both vulnerabilities highlight the importance of robust security practices to be incorporated during\nthe development, deployment and use of Al platforms, to mitigate the risks associated with adversarial\nattacks. Lets look at some potential techniques of trying the best we can to prevent the prompt injection\nattack, and also to detect an occurrence of it.\nPreventing a Prompt injection attack, can be looked at, in two ways: First there is the possibility (limited\nthough) of preventing the attack itself, and second is to reduce the impact of such an attack as much as possible."}, {"title": "2. Insecure output handling", "content": "Insecure Output Handling is the result of inadequate sanitation, validation, and management of output\ngenerated by LLMs before they are sent downstream, to other applications and interfaces, for further\nconsumption or processing. This vulnerability arises because LLM-generated content can be easily influenced\nby user input (prompts), effectively granting indirect access to additional functionality.\nThis can lead to security risks such as XSS and CSRF in web browsers, SSRF, privilege escalation, or remote code\nexecution in back-end systems. And in cases where these GenAl applications are used to create Infra (or) Policy\nas Code (laC / PaC) templates, to automate the provisioning of workloads and guardrails on Cloud platforms,\nthis vulnerability can result in an entire Cloud landing zone becoming vulnerable. This vulnerability can be\nexacerbated by over-privileged LLM access, susceptibility to indirect prompt injection attacks, and insufficient\ninput validation in third-party plugins. This vulnerability has the potential to impact both users, who may act\non inaccurate (or) false information, and downstream processes, plug-ins, or other computations that may fail\nor produce additional inaccurate (or) malicious results based on the incorrect input.\nLets consider a hypothetical scenario where an automated industrial production system leverages an LLM as\npart of its supply chain. And the Administrator is given a Chatbot interface to ask, enquire and command the\nproduction line. Either as part of a data flow or a user interaction, the Company that runs the production line,\nwill have little to no control over the contents of what is being passed to the LLM. As we have seen in previous\nsections of this Paper, user chatbots are a prime target for subverting functionality since it's effectively giving\nthe user almost direct access to API. The core vulnerability is that the request and the content passed to the\nLLM could quite easily cause it to produce malformed, incorrect, or malicious output. A user might deliberately\npass instructions to the LLM and attempt to bypass the original instructions given to it. This is in case of an\ninternal (or) trusted and Authorised user of the Production line. In another scenario, this user could actually be\nan external attacker. And as we saw in Risk 1 of this Paper, a sophisticated enough prompt attack can allow an\nattacker to control parts of a production pipeline. For ex., a tool provided to an LLM allows fetching web"}, {"title": "3. Training data poisoning", "content": "Data poisoning is a critical concern in LLMs, where malicious users can deliberately corrupt the training data\nof LLMs, creating vulnerabilities, biases, or enabling exploitative backdoors. When this occurs, it not only\nimpacts the security and effectiveness of a model but can also result in unethical and malicious responses, in\naddition, result in performance issues."}, {"title": "4. Supply Chain Vulnerabilities", "content": "There are four key points of compromise, when it comes to vulnerabilities in an LLM data lifecycle.\nIntegrity of Training data. Ensuring the integrity of the training data, and its various sources (and its\nauthenticity) is very important. We have explained this in detail, in a previous section in this Paper.\nVulnerabilities in the LLM itself. This could include a vulnerable pretrained model. For ex., GPT-JB, a\ntransformer model trained using Mesh Transformer JAX, that was modified to spread false information while\nmaintaining its performance on other tasks. This is why enterprises must not use open-source models,\nespecially for Production usecases. Even if these models are used for education purposes (for ex., by education\ninstitutions, the model churning out false information when its Chatbot is used for education purposes,\nindicates the risks of using them. Taking it to the next level, an attacker may impersonate a reputable model\nprovider and distribute the malicious (modified) model onto well-known platforms like Hugging Face.\nWormGPT and PoisonGPT are two widely seen malicious Generative Al models.\nIf any LLM builders subsequently integrate these modified models, being unaware, into their infrastructure,\nend-users end up unknowingly consuming these modified LLMs. Addressing this issue requires preventative\nmeasures at both the impersonation stage and the editing of models.\nIn order to address this, a term that has come to be used commonly is \"Model Provenance\", which basically is\nabout having means to establish the origins of the model itself, and having information about subsequent\nchanges (and its author) done to the model. But its not easy, because of the complexity and randomness\ninvolved in training LLMs. And in case of open-source models, tracking back the weights and parameters, and\ntrying to replicate them, is practically impossible, making it very difficult to verify its origins and authenticity. A\nPotential solution to this is certification of the model itself.\nPlatforms used for deployment. The model and its various modules run on an infrastructure platform made\nup of multiple hardware and software components. For ex., software plugins, are widely used component in\nLLMs. These are critical to the overall security of the model running on it. For ex., the vulnerability in any of the\n3rd party plugins, including outdated and deprecated components, can open-up the underlying platform, to\nadversarial attacks. This becomes even more so important now, because many LLP platforms, including\nOpenAl, AzureOpenAI, IBM Watsonx and others, have recently begun offering Plugin ecosystem to interface\nwith third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they\nbring several security, privacy, and safety issues.\nIt is a standard practice in cloud computing platforms which support third party ecosystems to impose\nrestrictions on these third parties. OpenAl, as an example, also deploys some restrictions, provides\nsuggestions, and enforces a review process to improve the security of the plugin ecosystem.\nAs for restrictions set by OpenAl, some of them include:\n\u2022\trequires that plugins use HTTPS for all communication with the LLM platform.\n\u2022\tbuild confirmation flows for requests that might alter user data, e.g., through POST requests\n\u2022\tuse OAuth if the plugin takes an action on user's behalf\n\u2022\tnot use non- OpenAl generative image models\n\u2022\tadhere to OpenAl's content policy"}, {"title": "5. Sensitive Information Disclosure", "content": "We have seen that this risk - Sensitive Information Disclosure - is on top of the list for CISOs, especially in cases\nwhere those organizations are rapidly adopting GenAl platforms for their business application and usecases.\nAnd the reason for the elevated concern is that, this can result in unauthorized access to sensitive and\nbusiness critical data, intellectual property, and privacy violations, resulting in loss of financial turnout and\nreputation for the enterprise.\nThere are two types of Information disclosure we must consider:\nDirect Leakage of data that the LLM have access to, for training and learning purposes. As discussed in the\nearlier section, prompt injection is a common way to get access to training data. We have discussed this at\nlength there and also looked at possible preventive and detective techniques. On the other hand, prompt\nleaking (or) prompt extraction, occurs when a model inadvertently reveals its own prompt, leading to\nunintended consequences. With prompt extraction attacks, an attacker can use prompt injection techniques to\ninduce the LLM to reveal information contained in its prompt template. For example, model instructions,\nmodel persona information, or even secrets such as passwords.\nIn case of model inversion attacks, an attacker can recover some of the data used to train that model. Model\ninversion is a type of machine learning security threat that uses the output of a model to infer some of its\nparameters or design. Depending on the details of the attack, these records might be recovered at random, or\nthe attacker may be able to bias the search to a particular record they suspect might be present. For instance,\nthey might be able to extract examples of Personal Identifiable Information (PII) used to train the LLM.\nIndirect Leakage of the data via the LLM, is when achieved using (misusing) the learnt knowledge of the\nmodels, over period of time. In order to comply with privacy laws such as the \"right to be forgotten\", the data\npoints of users that are most vulnerable to extraction, could be deleted. And once the most vulnerable points\nare deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to\nunderstanding memorization for fine-tuned models. And this points to the important work on studying\nmemorization and data leakage for the general purpose pre-trained or foundation models.\nAs LLMs handle vast amounts of data, concerns about data privacy and confidentiality naturally arise.\nSafeguarding sensitive information during the training and deployment of language models is crucial to protect\nconfidentiality of the data and also user privacy. Techniques like federated learning, differential privacy, and\nencryption, can help in striking a balance between harnessing the power of LLMs and preserving data\nconfidentiality and privacy. For an Enterprise developing language models, the LLM engineering teams need to"}, {"title": "6. Excessive Privileges", "content": "In the previous sections, we looked at 5 common risks in LLMs, and their potential impact, if not effectively\naddressed with security measures. The size and effect of the impact can be significantly larger, if the LLMs\nhave been granted with a high degree of privilege and autonomy. This is because, if the LLM has unnecessary\nprivilege, for example to other downstream systems, via API, then any vulnerability in the earlier phase of the\nLLM data pipeline (training or prompting for example), for ex., prompt injection, can result in a wider and more\ndamaging consequences. For ex., deletion or overwriting or retrieval and exposure of business-critical data on\na database server.\nIn the last couple of years, we have seen the rise of DevOps, DevSecOps, SRE (Site Reliability Engineer) and\nother similar ways of combining and merging the tasks and responsibilities of various IT Operations teams in an\nEnterprise. This drive started with an intention to reduce friction (resulting time delays) amongst teams while\nthey develop and release software products and deploy changes to Cloud infrastructure platforms. But if\nappropriate security controls are not implemented (ex., authentication, authorization, code scanning, etc.,)\nthis also meant that the additional responsibility entrusted upon a developer (or a DevOps), may also result in\nunintended security lapses, easily. In this particular context of LLMs, the impact of such an error by a\ndeveloper, can be significant. Also, when a successful jailbreak prompt is executed (Do Anything Now, Roleplay\njailbreaks, Developer mode, etc.), the LLM having excessive privilege over downstream systems, can result in\ndamaging consequences. For ex., if a developer has used an LLM plugin that is meant to respond to User\nqueries about the Product Catalog of an online retailer, but has used open-ended functions that could also let\na Shell to run within a Prompt, then a malicious user could exploit this excessive privilege to manipulate ethe\nproduct listings, price list thus resulting in significant financial loss to the retailer, but also to the customer of\nthis retailer."}, {"title": "Perspective 2: Securing the overlaying Cloud services layer", "content": "From our experience in the last 6 months of working closely with large Enterprise clients, developing GenAl\napplications, we have seen that most of them leverage out of the box LLM services, which helps in fast tracking\ntheir adoption of this new and shiny technology to meet their business needs. And all these LLM services\n(many of them built on OpenAI), run as a Cloud service, hosted on Cloud platforms like Microsoft Azure, IBM\nCloud, Google Cloud and Amazon Web Services. And so it is apparent to refer to them as LLM \"platforms\",\nwith\nthe models connected to different cloud services and various software plugins. So, once the LLM and its 6\ncommon security risks are attended to, its time to review the overlaying infrastructure and platform, for any\nvulnerabilities and architectural discrepancies.\nA Risk Management approach for LLM on Cloud, mapped to Threats and security Controls\nWe looked at the 6 common risks in an LLM application. Now that we are about to zoom out and look the\nunderlying infrastructure and overlying cloud services for risks to be addressed, we propose a very simple risk\nmanagement approach, where we could map the risks to associated threats, which in turn can be mapped to\nindividual security controls for preventive and detective purposes. Mapping out all the LLM risks to their\nthreats and appropriate controls, and in turn to controls for each of the common cloud platforms, can be a\nproject and will have to be continuously updated due to the ever-evolving Al threat landscape. So what we are\nproposing here is an starting point, a template as such, that an Enterprise Security Architect could use, to start\nbuilding their Enterprise's LLM \u2018risk-threat-control' management program. So we haven't produced an end-to-\nend mapping in this paper, but we have covered the individual blocks of this mapping, with considerable\ndetail. For ex., Perspective 1 covered the LLM risks. In this section we cover the risks in the overlay cloud\nplatform layer, along with reference security architecture and control examples for some of the common\nCloud platforms. This should provide with a good reference point for an Enterprise Security architect, to get\ngoing.\nFirst, we start with the view we presented in Perspective 1 \u2013 The LLM and the 6 common risks\nassociated with the different stages of the LLM data pipeline. In the view below we have shown that\nfirst you could do a Threat Modeling exercise of your LLM platform, identifying all the Threats in your\nLLM platform.\nThen, lets proceed with listing down all the Risks in the each of the 5 stages in the LLM Data pipeline.\nWe have covered the 6 common risks, in detail in Perspective 1 earlier.\nOnce the Risks and Threats have been identified, let's get to a Control Mapping exercise. Here, we will\nmap the Risks and Threats to each other. And then identify specific technical controls for each of the\nthreats, and also link it to specific control on the Cloud platform you are running your LLM on. For ex.,\nAzureOpenAI, AWS Bedrock, IBM Watsonx, Google PaLM2, and others. This three-tier mapping will help in\nmoving to the next step, of identifying implementation steps.\nNow that we have the mapping in place, lets group the security controls, for each of the CSPs (Cloud\nService Provider), into the 5 main security domains. Namely, Identity and Access Management, Data\nSecurity, Network Security, Application Security and Audit. This final mapping will come in handy to\ngroup the controls in a way that can be easily assigned to and owned by Teams in an Enterprise. In most of the\nlarge enterprises, the 'build' and 'operate' responsibilities of these controls are managed by these teams:\nnamely IAM, AppSec, SOC (for Audit - Logging and Monitoring), and Data Governance (or) Protection Office\n(Data Security). And so this mapping will end with a RACI matrix, as shown in the picture below, calling out\nteams in an Enterprise which will be responsible for those respective security control implementations, and\nlater, validation (Step 5 below)."}, {"title": "Perspective 3: Effective Governance of an Enterprise Al Program", "content": "Based on our experience in the last 6 months of having worked with Enterprise clients, adopting LLM\napplication platforms for their business use cases, we recommend five Key Principles that a CISO organization\ncould adopt, to successfully set up and operating an Enterprise Al program.\n1. Comply with all relevant Al Regulatory Requirements \u2014 Industry, Regional (EU, Country specific) and\norganizational specific requirements. As regulatory frameworks continue to evolve, organizations face the\nchallenge of maintaining compliance with various security standards and requirements. For ex., as per the\nEU AI Act, High-impact general-purpose Al models that might pose systemic risk, such as the more\nadvanced Al model GPT-4, would have to undergo thorough evaluations and any serious incidents would\nhave to be reported to the European Commission. Chief Information Security Officers (CISOs) (or their\nrespective teams) may want to take the time to ensure that they are well versed with all Generative Al\nservices operated on Cloud, because there may be a security, risk, or compliance objective that can be met,\neven if a service doesn't fall into the 'Security, Identity, and Compliance' category.\" For ex., review of Al\nEULA agreements from Cloud service providers, is crucial. End-user license agreements for GenAl platforms\nare very different in how they handle user prompts, output rights and ownership, data privacy, compliance\nand liability, privacy, and limits on how output can be used. It is also important to confirm the Cloud\nvendor's compliance with applicable Al laws and best practices.\n2. Secure the managed LLM platform on Cloud, end-to-end - As we have explained in this paper, it is very\nimportant to look at the security of LLM platforms, in two perspectives; the LLM itself, and the underlying\ncloud platform on which it runs. Being a Cloud service, the Shared responsibility model comes into play.\nAnd it is important for any organization to clearly interpret and understand where do the Cloud service\nprovider's responsibility cease, and which are critical levers that a consumer of these Cloud LLM services,\ncan tune, to make the platform end-to-end secure for their business.\n3. Establish clear security responsibilities of various organization Stakeholders in the Enterprise Al Program\nIn the last 6 months of working with large enterprise clients, developing and operating enterprise LLM\napplications, we can confidently say that there are at least 5 key stakeholders, who play an important role\nin the success of an Al program. And it is very important for all these 5 stakeholder groups to come\ntogether, share responsibility and accountability, in making the Enterprise Al program successful.\na) Business - the teams that sponsor and drive the rapid adoption of Generative Al technologies in an\nenterprise, and are under constant pressure, of time, to be the first in the industry to leverage these\nnew technologies for their business use cases."}, {"title": "IV Challenges", "content": "Now that we've explored the advantages and some yet-to-be-fully-proven use cases for adopting Gen Al, let's\ndelve into its fundamental challenges. Beyond the discussed aspect of securing Gen Al itself, there are primary\nchallenges associated with its implementation in the context of any organization. These challenges encompass\nskills, data, time, model performance, and learning.\nDeploying a Gen Al architecture requires a specialized skill set, compelling CISOs and architects to bridge the\ntalent gap through strategic investments in training and recruitment. Strengthening cybersecurity effectively\nposes a significant challenge for organizations, as they strive to identify and adopt Gen Al solutions with crucial\ntraits. These traits include real-time assessment, adaptive policy generation, proactive decision-making based\non insights, the ability to collect contextual data for organizations, and continuous learning, incorporating\nongoing training and improvement across diverse cybersecurity domains\nEnsuring the trustworthiness of Al algorithms emerges as a primary concern. CISOs and architects must\nprioritize transparency and accountability in Al systems, implementing measures to validate the accuracy and\nreliability of autonomous compliance assessments.\nIntegrating Gen Al into existing cybersecurity solutions poses challenges, necessitating CISOs and architects to\ndevelop a roadmap for seamless integration. This ensures that Al-driven compliance tools not only\ncomplement but also enhance existing security measures, promoting a harmonious coexistence of Al and\ntraditional cybersecurity approaches."}, {"title": "V. Conclusion", "content": "In conclusion, this white paper has underscored the pivotal role of Gen Al in cybersecurity, highlighting its\ncapacity to revolutionize certain or some functions of cyber security. We have revisited the fundamental\nsignificance of Al in adapting to the dynamic digital environment, emphasizing its ongoing relevance amidst\nevolving cyber threats. Looking ahead, it is imperative to foster continued research and development in Al for\ncybersecurity, driving innovation and resilience in the face of emerging challenges. While LLMs have\ndemonstrated remarkable capabilities in decision-making processes, is it plausible to consider their evolution\ninto not just policy decision points, but also as policy enforcement points in context of Cyber security policies?\nWith their ability to comprehend vast amounts of data and contextual nuances, LLMs could serve as powerful\ntools for enforcing security policies in real-time, enhancing the agility and efficacy of cyber defense\nmechanisms. However, this transition warrants careful consideration of ethical implications, privacy concerns,\nand regulatory frameworks to ensure responsible and accountable deployment of LLMs in security contexts.\nFurthermore, collaboration between industry, government, and academia is essential to harnessing the full\npotential of Gen Al, fostering a unified approach to safeguarding digital assets and promoting a secure cyber\nlandscape for all. As we navigate through the complexities of the digital age, embracing the advancements in\nGen Al offers promising avenues for enhancing cyber defence capabilities and ensuring a safer, more resilient\nfuture. In parallel security of Gen Al will also evolve, securing Gen Al and large language models (LLMs)\nrequires a comprehensive approach that addresses various domains and areas of cybersecurity. As outlined in\nthis write-up, future trends in securing Gen Al and LLMs will necessitate robust measures across\nauthentication, data privacy, adversarial defense, ethical compliance, and continuous monitoring. It is\nimperative to recognize that security remains a holistic endeavour, encompassing not only technological\nsolutions but also ethical considerations and regulatory compliance. As the threat landscape evolves, security\nand defence mechanisms must adapt accordingly, continually assessing risks and implementing appropriate\nmeasures to safeguard Gen Al and LLMs against emerging threats."}, {"title": "VI Further Reading \u2013 IBM's global insights on GenAl and Cybersecurity", "content": "In the last couple of months, we at IBM, have published many industry insights, client stories and point of\nviews, in the area of Generative Al and Cybersecurity. All this is based on our engagements with Enterprise\nclients across the globe. We have included links to some of them below. We encourage the readers of this\nPaper, to read them and use them in their journey to securing the adoption of Generative Al for business\nusecases.\n\u2022\tA CISO's guide: Cybersecurity in the era of generative Al\n\u2022\tHow to establish secure Al+ business models\n\u2022\tAl to accelerate your security defenses\n\u2022\tWatch: How Generative Al Changes the Cybersecurity Landscape\n\u2022\tThe power of Al: Security"}]}