{"title": "Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews", "authors": ["Mengqiao Liu", "Tevin Wang", "Cassandra A. Cohen", "Sarah Li", "Chenyan Xiong"], "abstract": "Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interacted with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, for example, the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our collected chat-and-interview logs will be released.", "sections": [{"title": "1 Introduction", "content": "The foundation and large language model (LLM) revolution is redefining the way users interact with the digital world. Billions of users now chat with LLMs regularly through one unified interface and consume information through generative contents for their information, entertainment, and task assistance needs (Duarte, 2025).\nUnderstanding user opinions about LLMs is, however, a challenging task. It is difficult to design one evaluation task to reflect the rich experiences provided by LLMs. The user experiences powered through generative contents are also hard to characterize by metrics like accuracy and BLEU. The prone to data contamination during pretraining further complicates the understanding of LLM performances (Schaeffer, 2023; Singh et al., 2024). The community often relies on coarse user preference ratings to understand LLMs performances (Chiang et al., 2024)."}, {"title": "2 Related Work", "content": "Many benchmarks have been developed to evaluate predictive effectiveness of LLMs. Notable examples include GLUE (Wang, 2018) and Super-GLUE (Wang et al., 2019), both including a suite of natural language understanding tasks. The suite of language tasks quickly grows to hundreds, such as in FLAN (Wei et al., 2021) and BIG-Bench (bench authors, 2023) benchmarks. The community also keeps increasing task difficulty, for example, from MMLU (Hendrycks et al., 2020) to \u201cHumanity's Last Exam\" (Phan et al., 2025), to test the boundary of LLM intelligence. These evaluations are effective in reflecting LLMs' ability to predict the right label, which aligns closely with some real-world applications such as question answering.\nEvaluating generated content is challenging, as two text sequences with high n-gram overlap (Papineni et al., 2002) or semantic similarities (Zhang et al., 2019) may not lead to the same user experience (Hanna and Bojar, 2021). Recent approaches switch to model-based evaluation and employ LLM-as-a-judge for predefined dimensions (Zheng et al., 2023). Model-based evaluation has become a common practice in evaluating generated content, albeit various challenges such as self-biases and inaccuracies (Li et al., 2024a; Ye et al., 2024; Wei et al., 2024).\nThe ultimate verdict of an AI model is how it serves its users. For established applications like search engines, understanding model performances based on noisy and coarse user feedback is a long lasting research topic (Chuklin et al., 2022). There are various efforts to collect user feedback on new LLM-powered scenarios. One notable effort is Chatbot Arena (Chiang et al., 2024). It asks users for preferences on side-by-side LLM chats and computes arena scores based on that. Many view the arena score a reliable reflection of user preferences on LLMs (Hart, 2024).\nUX interview is a standard approach to gather in-depth insights about user opinions (Rubin and Chisnell, 2011). It is widely recognized as an effective tool to guide product developments toward increased adoption, consumer loyalty, and overall product success (Hartson and Pyla, 2012). Effective UX interviews often require interview experts (Ahrend, 2025) and are too expensive to scale up. Many explored the potential of AI-powered interview bots, for example, to conduct job interviews (Li et al., 2017) and conversational surveys (Xiao et al., 2020). Recently, Li et al. (2024b) built an LLM-powered interview system and demonstrated their effectiveness in evaluating student experiences in AI-assisted classroom."}, {"title": "3 Methodology", "content": "CLUE leverages LLMs to conduct in-the-moment UX interviews and understand user opinions. It includes two components: CLUE-Interviewer and CLUE-Insighter."}, {"title": "3.1 CLUE-Interviewer", "content": "CLUE-Interviewer conducts in-the-moment UX interviews right after users interacted with a product, which in this study is chatting with a mainstream LLM. We develop CLUE-Interviewer by implementing standard user interview practices (Hartson and Pyla, 2012) into LLMs through carefully designed prompts.\nInterview Design. CLUE-Interviewer is designed to conduct semi-structured interviews (Wilson, 2013), starting with a set of predefined interview dimensions and probing user for deeper insights, with the flexibility to explore topics emerged from user responses.\nSpecifically, to understand user opinions on LLMs, we follow previous research on chatbot user experiences (Casas et al., 2020) and include the evaluation dimensions listed in Table 1. The first four dimensions gather insights about the effectiveness of the target LLMs. The improvements dimension aims to elicit user opinions in an open-ended manner, while the last one asks users for an explicit Likert rating of the target LLM.\nLLM-Powered Interviewer. We implement CLUE-Interviewer by building these interview principles into prompting an LLM.\nWe design the prompts to include instructions that assign the interviewing task to the LLM, specifications of the interview task, and a step-by-step guild of how to complete the task. The steps includes reviewing the chat history between user and the target LLM for in-the-moment study, the designed interview flow, and instructions to conduct a semi-structured interview. The instructions include dimensions to cover and encourage follow-ups probes. The full prompts used in CLUE-Interviewer can be found in Appendix Figure 9.\nThe capabilities of LLMs enables CLUE-Interviewer to collect in-depth user interviewers at a reduced cost, without the need of much human intervention from the UX side. It is also easier to collect in-the-moment interviewers as users can interact with the LLM interviewer anywhere they want, rather than being interviewed in a controlled setting by a human UX researcher."}, {"title": "3.2 CLUE-Insighter", "content": "To gain collective insights from massive user experience interviews, we build CLUE-Insighter to analyze the interview logs. It first maps raw interview rounds into the interview dimensions and then analyzes corresponding user responses.\nCategorize Interview Rounds. We first categorize each interview round\u2014an interchange between CLUE-Interviewer and the user\u2014into one of the evaluation dimensions in Table 1. This is done by prompting LLMs with instructions.\nSpecifically, we provide the previous rounds in the interview session as well as the to-be classified round, as context to an LLM and prompt the model to categorize the round into the targeted categories. This is applied on all interview rounds to assign a dimension category to them. The detailed prompts are listed in Appendix Figure 11.\nQuantitative Metrics. CLUE-Insighter automatically generates a numerical score for each user response in the first four dimensions in Table 1. It is done by prompting an LLM to convert the user responses to a Likert rating of 1 (bad), 2 (mediocre), and 3 (good). Similar to the dimension mapping, we perform a zero-shot classification to the LLM and instruct it to produce the numerical rating. The prompt is in Appendix Figure 12.\nThe numerical ratings from individual user responses are merged to quantitative metric scores for corresponding dimensions.\nTopic Analysis. To surface high level user insights from raw responses, CLUE-Insighter applies standard topic analysis (Grootendorst, 2022) on the user responses categorized to each dimension.\nCLUE-Insighter first applies simple rule-based filters to remove chit-chat phrases and non-informative texts from user responses, as detailed in Appendix A.1. LLM prompting is used on the rest of the user responses to extract a list of insights from those responses. We then use BERTopic (Grootendorst, 2022) to cluster these user insights into topics.\nSpecifically, we embed user chat rounds using OpenAI text-embedding-3-small, and then reduce their dimension from 1536 to 5 using uniform manifold approximation and projection (McInnes et al., 2018), with a local neighborhood size of 5. We cluster the result embeddings using HDB-SCAN (Malzer and Baum, 2020). We set the minimum cluster size to be 5 and used Claude 3.5 Son-net (10-22) to summarize the key themes of each clustering on ten randomly sampled user rounds.\nThe quantitative metrics and topics produced by CLUE-Insighter aim to provide a bird's-eye view of user opinions. They serve as entry points to, but not replacements of the interview logs. The latter is the ultimate source for user opinions."}, {"title": "4 User Study", "content": "Approved by our university Institutional Review Board (IRB), we conduct a large scale study on user opinions of LLMs using CLUE. This section describes the user study methodology and the collected data."}, {"title": "4.1 User Study Methodology", "content": "We build the user study pipeline by hosting LLM APIs through our customized user interface. In our system, a user first chats with the target LLM. Then CLUE-Interviewer conducts in-the-moment interviews with the user with access to the user's previous chat history. Screenshot of our UI can be found in Figures 8 in the Appendix.\nRecruiting. We recruited participants via Amazon Mechanical Turk (MTurk) for our study. For this open-ended study, we set the qualification criteria as US only, 1k+ tasks completed, and 99% prior approval rate.\nEach participant reviewed the study description and provided informed consent that the collected data will be publicly available. They were instructed to not share any personal information in the study. We included instructions to improve data quality, such as not using an external chatbot to complete the study. We set up FAQ in all phases of the study to provide detailed walk-through of how one can complete the task.\nChat with Target LLMs. Participants then engaged in a 10-15 minute conversation with one of the six mainstream LLMs through our hosted chat interface: DeepSeek-R1, DeepSeek-V3, Gemini-1.5-Flash, Llama-3-70B, GPT-40, and Claude-3.5-Sonnet. The specific LLM was randomly assigned and the identity was not revealed to the user.\nParticipants freely chatted with the LLM on any topics. They were encouraged to interact for 15 minutes but could finish early.\nCLUE Interviews. After participants interacted with the target LLM, the system directs them to discuss their experiences about the interaction with CLUE-Interviewer. The system has access to the previous interaction between the user and the target LLM and performs semi-structured interviews as designed in Sec. 3.1. We use GPT-40 to power CLUE-Interviewer.\nClosing Survey. At the end of the study, users were asked to complete a voluntary demographic survey, including gender, race, age group, education level, and marital status. Demographic data was collected to understand the distribution of our study and whose opinions it is going to reflect.\nData Filtering. Open-ended user studies inevitably include noises. We prompt Claude 3.5 Sonnet (10-22) to filter out incomplete and low-quality chats and interviewers. We filter out interactions that did not complete either the chatbot or the interview portion of the study, used chatbot to complete the study, or provided responses in the interview that did not make logical sense (e.g., did not understand the task, responded randomly, etc.). The details of this filter can be found in Appendix Figure 10. We manually labeled the quality of 120 sessions and compare with the automatic filter. The automatic filter has 91 precision and 72 recall."}, {"title": "4.2 Collected Data", "content": "We ran the user study on Amazon MTurker in the period of December 2024 to January 2025 and collected 1989 user chat-and-interview sessions. In total 1206 (60.6%) are kept after filtering.\nOverall Statistics of our collected data are listed in Table 2. Users on average interacted around 10 turns with the LLMs. The shortest interaction was with DeepSeek-R1, who produces long reasoning chains in between chats. The interaction with CLUE-Interviewer is slightly shorter as it is a more focused conversation.\nThe volunteered demographic survey shows that majority of our participants are White/Caucasian, in their 20-40s, 60% male, and with a Bachelor's degree. All user opinions collected in this study would be representing this specific MTurker population. The detailed break down of participant demographics can be found in Appendix Table 7.\nChat Topics. We also perform topic analysis on the user chat rounds, using similar techniques discussed in Sec. 3.2, except a larger minimum cluster size (15) and neighborhood size (15) to account for more chat rounds and the lighter filtering.\nFigure 2a shows the top topics our participants engaged with LLMs. As expected, participants chatted with LLMs about a large variety of tasks, covering various information seeking, entertainments, and task assistance topics. Figure 2b plots the number of topics included in each chat session. On average each participant chatted with the target LLMs around 6.60 topics, showing the diversity of user interactions with LLMs."}, {"title": "5 Evaluation of CLUE", "content": "Before sharing the findings from our user study, this section presents human evaluations on the effectiveness of CLUE-Interviewer and CLUE-Insighter.\nAll the human evaluations are done by manually annotating 120 interview sessions or 180 interview turns, randomly sampled for each of the six target LLMs. We performed annotations by two annotators on 25% of the labeled data. Their agreements are listed in Table 3. An example CLUE-Interviewer session with manual annotations can be found in Appendix Figure 15.\nSemi-Structured Interview. We first manually labeled the evaluation dimensions (Table 1) of CLUE-Interviewer rounds. Table 4 shows the coverage of CLUE-Interviewer for each dimension. It confirms that our prompts are effective in converting the LLM into an interviewer. It covers majority of designed dimensions in its interviews. The only exception is coverage of credibility which can be improved in future research.\nWe manually labeled the interview rounds where CLUE-Interviewer asks follow up questions. The distributions of probing frequency and depth are plotted in Figure 3a and 3b. On more than half of interview sessions, CLUE-Interviewer probes users for more detailed feedback, asking on average 1.3 follow-up questions, rather than merely asking predefined questions.\nIn-the-Moment Interview. This experiment evaluates the ability of CLUE-Interviewer conducting in-the-moment interviews. We manually labeled explicit references to previous interactions in the interview sessions by the interviewer and the users, as a reflection of in-the-moment effect. The distributions are plotted in Figure 3c and 3d. Interestingly, users actively refer to their previous interactions more frequently than the interviewer for more than 1/3 of times during interviews, reflecting users' in-the-moment status.\nBird's-Eye Insights. CLUE-Insighter automatically maps interview questions into target dimensions and converts user responses into categorical ratings. We compare these two automatic operations with our human annotations in Table 4. It shows that CLUE-Insighter, though not perfect, is sufficient to provide a bird's-eye view of user opinions from raw interview logs."}, {"title": "6 User Opinions of LLMs", "content": "This section presents the quantitative ratings (Sec. 6.1) and qualitative insights (Sec. 6.2) about user opinions of LLMs from our user study."}, {"title": "6.1 Quantitative Ratings", "content": "Table 5 shows the ratings of LLM from our user study. The first four are automatically assigned by CLUE-Insighter based on users' textual responses. The last is explicitly provided by user.\nIn contrast to \"beyond Ph.D. intelligence\" performances on various exam style benchmarks (Phan et al., 2025), all studied LLMs have significant room for improvement in open-ended chats, their main consumer scenario. All LLMs are scored around a mediocre 2 General rating. These fine-grained interview dimensions show that current LLMs are better at understanding user needs but less effective at meeting them. The credibility is also often questioned.\nUsers are much more lenient when asked for explicit ratings. Majority of studied LLMs received 4.5 scores. This discrepancy aligns with the common challenges of Likert ratings, users have different levels of leniency and may not reason much about scoring (Subedi, 2016). In comparison, UX interviews are known to be effective in probing out actual opinions from consumers (Wilson, 2013).\nFigure 4 shows the spearman correlations between different evaluation dimensions. Among all dimensions, meet needs has the strongest correlations with other dimensions, showing that the utility of LLMs\u2014their ability to satisfy user need\u2014is still the north star of LLM user experience. Similar with the cross LLM comparisons, the user explicit rating only has weak correlations with user sentiment underlying their interview responses. A more detailed scattered plot of correlations can be found in Figure 7 in Appendix."}, {"title": "6.2 Qualitative Insights", "content": "This set of analyses presents the qualitative insights gathered from our user interviews.\nFine-Grained User Feedback. Figure 5 plots the top topics gathered from user responses categorized in the first four interviewing dimensions. The topics are aggregated from all six LLMs.\nThese topics reveal more fine-grained user opinions than numerical ratings. Users praised LLMs' understanding, but raised questions on LLMs' ability to handle complex topics and their authenticity. When asked about their general impressions, some users praised the transparency of AI reasoning, likely from those matched with DeepSeek-R1 which displayed reasoning chains.\nUser Suggested Improvements. We conduct a deeper dive into the suggestions provided by users when asked about potential improvements about their LLM experience. The top topics popped up in our analysis for each LLM is plotted in Figure 6. Note that there are no pre-defined features for users to pick from in our free-text interviews. All topics come from responses users provided.\nContrast to some previous research, users have a strong preference for conciseness responses and complained about LLMs' verbosity. Some suggestions correlate with the quantitative insights from other evaluation dimensions. For example, users have issues with LLMs' ability to understand longer contexts and would like more conversational interactions.\nWe provide one of the first studies on user opinions of displaying reasoning process. Our user study indicates that it is quite a bi-polar feature. Some users explicitly requested it, while some preferred the displaying of the thought process be optional.\nAligned with the views from many in the community, users actively request features such as multi-modality capabilities, both processing and generation. Access to real-time data is another common request, which is not surprising as many of the top chat topics (Figure 2a) are time sensitive, signifying the benefits for retrieval augmentation."}, {"title": "7 Conclusion", "content": "CLUE is a new methodology to gather user opinions using LLM-powered interviews, enabling a deeper understanding of users through large scale in-the-moment user experience interviews. Our study with thousands of users show that CLUE collects fine-grained user opinions on current LLMs, potential improvements, and frequently requested new functionalities."}, {"title": "9 Limitations", "content": "One major limitation is that, being an academic project, our user study is limited to the population available on MTurk, which is not a thorough representation of the US nor global market. The user opinions collected thus only reflect this specific demographic distribution, which can be different from other populations. This is also a potential risk of our framework: CLUE is to capture the opinions of studied users, but may be misinterpreted as the universal opinions of all potential users.\nAnother limitation is that as the first demonstration of LLM-powered interviewing, we heavily rely on the power of existing LLMs through prompting API. Though being useful, there are a lot of possible improvements from the modeling side which can potentially further improve the interviewing effectiveness.\nThe Insighter produces both quantitative and qualitative insights automatically extracted from interview logs. Its effectiveness is fine to provide a high level idea of user opinions but not perfect as a once-for-all ultimate LLM leaderboard. For example, the numerical ratings are not sensitive enough to tell the differences between evaluated LLMs, albeit these are all top tier models with similar performances. How to better extract insights from massive user interview logs is another future research direction."}, {"title": "A Appendix", "content": "We provide more details about the implementation of CLUE, our user study, and additional results."}, {"title": "A.1 More Implementation Details", "content": "CLUE System UI. CLUE system UI examples can be found in Figures 8. Figure 8a shows the chat interface with the target LLM. Figure 8b shows the user interface of CLUE-Interviewer. We design the UI to resemble mainstream LLM interfaces.\nInterviewer Prompts. The full prompt used in the CLUE-Interviewer system can be seen in Figure 9. In the prompt, we instruct the LLM to serve as a UX researcher and conduct an interview with a user who had just chatted with a chatbot. Via step-by-step instructions, we provided the specific evaluation dimensions to cover and encouraged follow-up questions to be asked.\nIn an earlier version of the interviewer development, we encouraged the interviewer to probe users for multiple rounds. Sometimes the interviewers were probed too much. We limit the interviewer to ask no more than two follow-ups per question, a conservative choice as the first step towards LLM-based UX interviewers. Future research can explore a better balance of thoroughness and user experience.\nInsighter Data Filtering Prompt is in Figure 10. It is a simple prompt that leverages the LLM (Claude-3.5-Sonnet) to filter out obvious noisy data. As discussed in Sec. 4, the automatic filter has very high precision but is lenient in recall.\nInsighter Dimension Classification Prompt. The prompt used to classify the interview sessions into evaluation dimensions can be seen in Figure 11. We simply describe each dimension to the LLM (Claude-3.5-Sonnet) and utilize its zero-shot ability for the classification.\nInsighter Rating Prompt is in Figure 12. We acknowledge that there is room to further improve the implementation of CLUE-Interviewer and CLUE-Insighter. A better prompt engineering, finetuning dedicated LLMs for our tasks, or using next generation of LLMs (e.g., GPT-5 or Claude-4), will certainly improve the performance of CLUE-Interviewer and CLUE-Insighter. Our simple design already illustrated many interesting user opinions of LLMs, and it is only the beginning.\nInsighter Topic Analysis Filtering. We filter some common generic responses by checking if the response provided by the user is a substring of any of the generic responses, which include responses like \u201cnothing much\" or \u201ci don't know\". In addition, this filter removes all responses that have less than 10 characters, as it is unlikely for an user to describe an quality insight in such brevity. For misspelled responses and other generic responses not caught by this rule-based filtering, a three-shot prompt, which is in Figure 13, filters out any remaining basic yes or no responses. This prompt also extracts a list of insights from the answer. Using this filtering process, roughly 27.06% of the answers are kept and used for insight extraction."}, {"title": "A.2 More Details of User Study", "content": "Informed Consent and Instructions. Figure 14 shows the informed consent and instructions for the MTurk user study. Participants were informed of the study procedures, potential risks, compensation, future use of information, confidentiality, and voluntary participation. It specifically calls out that participants should not provide any personally identifiable information during this study.\nDemographics. Table 7 shows the demographic statistics of the MTurk participants in our user study. Demographic variables reported included gender, race/ethnicity, age, education, and marital status. Our participant population is biased towards certain demographic groups, perhaps due to the population distributions of MTurkers during our study period. As a result, all the opinions reflected in this paper are from this specific population, which might be different from current user bases of mainstream LLMs.\nPayment. All participants were paid between $4 and $6 for their time and participation in this study. This payment rate was determined to be above the US federal minimum wage of $7.25 per hour. We started with $4 but then increased to $6 to facilitate more participation. On average, users spent a total of 25 minutes on our study, corresponding to an average hourly rate of $12, which is significantly more than the federal minimum wage."}, {"title": "A.3 Additional Results", "content": "Correlations with User Explicit Rating. Figure 7 plots the detailed correlations between explicit rating provided by user when asked by CLUE-Interviewer, and the ratings automatically generated based on user interview responses. Users are significantly more lenient when asked for an explicit rating, giving a lot of perfect 5s. Their responses are more scattered, with a significant fraction of 1 (bad) and 2 (mediocre). It aligns with the findings in UX research that interview is a more effective tool to discover users' true opinions.\nInterview Examples with Human Annotations. Figure 15 shows an example of an interview session with human annotations on evaluation dimension, probing occurrences and depth, and previous chat mentioned."}]}