{"title": "Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers", "authors": ["Dominic Schneider", "Lutz Rapp"], "abstract": "Optical fiber amplifiers are key elements in present optical networks. Failures of these components result in high financial loss of income of the network operator as the com- munication traffic over an affected link is interrupted. Applying Remaining useful lifetime (RUL) prediction in the context of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming system failures at an early stage, so that network outages can be minimized through planning of targeted maintenance actions, ensures reliability and safety. Optical fiber amplifier are complex systems, that work under various operating conditions, which makes correct forecasting a difficult task. Increased monitoring capabilities of systems results in datasets that facilitate the application of data-driven RUL prediction methods. Deep learning models in particular have shown good performance, but generalization based on comparatively small datasets for RUL prediction is difficult. In this paper, we propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL prediction method. SLAT is based on an encoder-decoder architecture, wherein two parallel working encoders extract features for sensors and time steps. By utilizing the self-attention mechanism, long-term dependencies can be learned from long sequences. The implementation of sparsity in the attention matrix and a low-rank parametrization reduce overfitting and increase generalization. Experimental application to optical fiber amplifiers exemplified on EDFA, as well as a reference dataset from turbofan engines, shows that SLAT outperforms the state- of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "PREDICTIVE maintenance (PdM) is an essential part of today's industry 4.0 and relies on condition monitoring, remaining useful lifetime (RUL) prognosis and fault case (FC) diagnosis of a system and its components in real-time [1]. The primary strategy is to plan mainenance actions as soon as a component shows atypical behavior that may lead to a FC. These faulty behaviors comprise degradation, which result in slightly decreased performance or complete failures of the system [2]. Besides anomaly detection [3] and FC diagnosis, a key component in modern PdM systems is represented by prognosing the RUL [4]. Therefore, the condition of the system is monitored at specified inspection intervals from begin of life (BOL). This condition-based monitoring (CBM) requires the measurement of the systems parameters to reflect the health- state of the system in real-time [5]. Based on CBM the RUL of the real-time operational state can be predicted to ensure maintenance decisions can be made accordingly.\nOptical fiber amplifiers are key components in current long-haul optical fiber transmission networks. Interruptions of an optical transmission link, caused by system failure of an optical fiber amplifier, result in cost-intensive loss of transmission capacity. Applying PdM to predict the RUL of optical fiber amplifier and thus enabling the planning of targeted maintenance actions reduce network downtimes and increases the network resilience.\nRUL prediction can be grouped into model-based and data-driven based approaches. Model-based prediction implies fundamental knowledge about the degradation processes at component level [6]. With the increasing complexity in mod- ern systems and non-linear dependencies of components and the system, the data-driven approach becomes more popular as accurate modeling using the traditional approach is difficult to realize.\nThe data-driven approach maps features of the system to a scalar value RUL utilizing an arbitrary model [7]. These fea- tures can contain sensor data as well as operating conditions. Furthermore, this approach requires no a priori knowledge about the physical mechanism that causes the degradation behavior of individual components and their interplay in the system. Various machine learning techniques have been applied to this task, whereas neural network based methods like multi-layer perceptron [8], artificial neural networks [9], and fuzzy neural networks [10] showed promising results. However, these methods make elaborate feature engineering necessary and the model capacity is not sufficient for handling numerous features. To overcome this issue, deep learning (DL) methods were utilized [11], which extract the valuable features automatically and do not suffer from the curse of dimensionality.\nMapping the CBM data to the RUL of a system is a multi- variate time series regression task. Deep learning architectures like convolutional neural networks (CNN) [12] and recurrent neural networks (RNN) [13] can efficiently capture temporal and spatial dependencies of condition monitoring (CM) data."}, {"title": "II. METHODOLOGY", "content": "CNN methods use receptive fields and multi-dimensional convolutional to extract the feature maps over the time di- mension of the CM data. Due to the limitation of the kernel size, only short-term or mid-term temporal dependencies can be captured. Extracting long-term dependencies entails the extension of the kernel size. RNN-based approaches utilize long short-term memory (LSTM) [14] or gated recurrent unit (GRU) [15] components to extract features. The sequence data need to pass through each unit, which causes forgetting important information contained by the sequence data.\nTransformers [16] are sequence-based models, which be- came of major interest for recent years. Due to the flexible architecture, the applications range from natural language processing (NLP) [17] to computer vision (CV) [18]. It utilizes an attention mechanism to model sequence data and extract features. Without considering the distance of the elements in the sequence, it captures short-term and long-term de- pendencies. The vanilla Transformer only extracts the time series dependencies, whereas the sensor dimension of CBM data contains different type of degradation information. For capturing degradation information in time series dimension and sensor dimension, Dual Aspect Self-attention based on Transformer (DAST) [19] was proposed.\nDue to the flexible structure of Transformer models, this architecture makes few assumptions about the structural bias. Especially on small dataset, it leads to poor generalization. Comparing RUL prediction datasets [20] to other datasets [21] used to train Transformer models, the dataset size tends to be a lot smaller. To overcome this issue, we propose Sparse Low- ranked self-Attention Transformer (SLAT). It has an encoder- decoder architecture, whereas the encoder consists of two parallel blocks to simultaneously make attention to time steps as well as sensors. The generated feature maps of the two encoder blocks are fused and fed into the decoder. To increase the generalization ability on small datasets, SLAT induces structural bias using sparsity in the attention matrix and a low- rank parametrization to the encoder. The main contributions are as follows:\n1) We introduce critical degradation scenarios in optical fiber amplifier and the corresponding data acquisition setup.\n2) A novel RUL prediction method based on the Trans- former architecture is proposed, which utilizes self- attention and dual aspect feature extraction combined with sparsity and low-rank parametrization.\nThe paper is organized in the following way. At first, the methodology is described in detail. Second, critical compo- nents of optical amplifier and their degradation behavior are introduced. Third, the performance increase of the proposed method is shown and finally, a summary of this paper will be given."}, {"title": "A. PROBLEM DESCRIPTION", "content": "The CBM data of a system at runtime are used for RUL prediction, in this case for an optical fiber amplifier. The data is recorded at defined inspection intervals and has the form $X_t \\in R^k$ with $t = (1,...,T)$, where T describes the number of inspection intervals and k the number of sensors. The forecast of the RUL is defined as follows:\n$Y_t = f (X_t)$,\nwhere $X_t$ is the CBM data recorded at runtime, the mapping function f is an arbitrary model, and $y_t$ is the predicted RUL. Therefore, the prediction maps the input data matrix $X_t$ to a scalar value $y_t$. The signal from sensors are relevant for forecasting the RUL. The original approach of the vanilla Transformer only extracts the temporal dependence of the input data. DAST proposed an enhanced architecture that also includes the sensor dependency, as these two dependencies are different in nature and contain information about the degrada- tion of the system in different forms. Basically, Transformer have a flexible structure, as they make few assumptions about the structural bias compared to other models like DCNN [22] and BiLSTM [23]. Poor generalization is a result of short sequence lengths and small datasets. Structural bias can be induced by using a sparse attention matrix, such as:\n$Attention_{ij} = \\begin{cases}\n T & \\text{if i attends to j},\n\\\\ -\\infty & \\text{if i does not attend to j},\n\\end{cases}$\nThis limits the query-key pairs and reduces computation complexity. The $-\\infty$ element is not stored in the memory, what results in sparse connections. This mechanism is defined as position-based sparse attention, which uses sparse attention patterns [24]. It counteracts the deterioration of modeling long-range dependencies by inserting global nodes. The data also contain local dependencies, which are modeled using a band attention pattern. A combination of both patterns is used for SLAT. Furthermore, the sequence length of CBM data is limited, whereas Transformer were originally designed to handle long sequences. By applying them to short sequence lengths, they tend to overfit. A limitation of the model dimen- sion $D_{model}$ reduces the effect of over-parametrization [25]. With the combination of sparsity and low-rank parametrization of the attention mechanism, the transformer model SLAT is obtained.\nThe architecture of SLAT is shown in Fig. 1. The atomic sparse attention patterns are shown in the left section. A global atomic sparse attention pattern is combined with a band atomic sparse attention pattern that results in SLAT's atomic sparse attention pattern. In the middle section, the encoder is shown. It contains two parallel blocks, which extract the temporal dependencies as well as the sensor dependencies. To achieve that, the input data are provided in the original form and the transposed one. Both are embedded and to add positional information a positional encoding layer is used. Downstream of the encoder blocks, the generated feature maps are fused to build the actual feature map of the encoder.\nThe decoder has a similar structure as the encoder, but there is no parallelization. Additionally, it utilizes two attention mechanisms. The first attention mechanism is a self-attention, whereas the second is cross-attention with the output of the first attention layer and the feature map of the encoder. Downstream the decoder, a fully connected layer is used as regression component to map the feature map of the decoder"}, {"title": "B. ENCODER-DECODER ARCHITECTURE", "content": "The vanilla Transformer, as well as SLAT, use an encoder- decoder architecture, where each encoder and decoder consists of n and m identical blocks respectively. As the extraction of the feature maps is done by the attention mechanism, those are the most important components in the blocks. The encoder part consists of two parallel blocks, one for feature extraction of the inspection intervals and one for the sensors. By transposing the input matrix of the original data, the input matrix for feature extraction of the sensors is generated. The following explanations of feature extraction process include both encoder blocks. As Transformer require sequential input data, the input embedding is used to map the input data to a vector of dimension $D_{model}$. As the Transformer makes no assumptions about structural bias like DCNN and RNN, positional information must be added to the embedding vector. The positional encoding generates structural information and can be applied in various forms. In this paper the original approach presented in [16] is used with:\n$PE_{(t,2k)} = sin (t/10000^{2k/D_{model}})$, \n$PE_{(t,2k+1)} = cos (t/10000^{2k+1/D_{model}})$,\nwhere t and k represent the inspection intervals and sensors respectively. The CBM data matrix has the form $X = {X_1,\u2026\u2026\u2026,X_T} \\in R^{dk\\times T}$, where dk is the sensor dimension and T is the number of inspection intervals. For feature extraction of the inspection intervals X is used and for feature extraction of the sensor $X^T$ is used. Nowadays two variants of implementing layer normalization exist, post-LN [26] and pre-LN [27], whereas SLAT will use pre-LN:\n$X_E = LayerNorm (X^T P_E)$,\nDownstream the first normalization layer, the multi-head atten- tion (MHA) block follows. It uses a query-key-value model, which results in self-attention if the data matrix is used for query, key, and value respectively:\n$Q_E = X_E W^Q, K_E = X_E W^K, V_E = X_E W^V$,\nwhere $X_E$ is the resulting matrix after the first normalization layer. By multiplying $X_E$ with learnable weight matrices, the necessary matrices for Q, K, and V are formed. With the use of $D_{model}$ as the input dimension, $Q_E, K_E, V_E \\in R^{dk\\times D_{model}}$ follows. Low-rank parametrization is induced by limiting $D_{model}$. The attention of SLAT is created as follows:\n$Attention = softmax(\\frac{Q_E K_E}{\\sqrt{D_{model}}})V_E$,"}, {"title": "III. EXPERIMENTAL DESIGN", "content": "Nowadays, there exist many types of optical amplifiers.\nQuite popular are rare-earth doped fiber amplifier. They are characterized by the choice of the rare earth ions as dopant\nFor applying RUL prediction to optical amplifier in the context of PdM, the most critical components need to be identified. With the use of a critical component selection"}, {"title": "A. DATA ACQUISITION SETUP", "content": "For data acquisition, a wavelength-division multiplexing (WDM) signal with nine equally distributed channels is used, which is generated by independent tunable lasers in the C- band. Commercial optical amplifier can operate at a given total input power range from -35 dBm to 1 dBm, thus a VOA is used"}, {"title": "IV. RESULTS AND ANALYSIS", "content": "SLAT contains various hyperparameters that have significant impact on the performance of the model. In order to determine the best configuration, Bayesian optimization is performed. Additionally, the window length of the STW procedure is set to 40 for all sub-datasets. For training, the Adam optimizer is applied with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, and $\\epsilon = 1e \u2013 9$. For Transformer models it is necessary to implement a learning rate scheduler to provide a warm-up phase for the algorithm and to ensure a stable learning behavior. We will follow the common approach with:\n$lr = D_{model}^{0.5} min (sn^{-0.5}, sn \\cdot ws^{-1.5})$,"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel deep learning model for remaining useful lifetime (RUL) prediction of optical fiber amplifiers, exemplified on EDFA. Sparse Low-ranked self-attention Transformer (SLAT) has an encoder-decoder architecture, that uses dual aspect mechanism to extract feature information of time steps and sensors. Enhancing the atten- tion mechanism in the encoder by sparsity and low-ranked parametrization, enables the model to increase the general- ization capability. SLAT learns the information included in time steps and sensors automatically, which is to improve maintenance safety and reliability. We identified the critical components in optical fiber amplifier and introduced their"}]}