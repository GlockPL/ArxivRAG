{"title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection", "authors": ["Chirag Goel", "Surya Koppisetti", "Ben Colman", "Ali Shahriyari", "Gaurav Bharaj"], "abstract": "Vision transformers (ViT) have made substantial progress for classification tasks in computer vision. Recently, Gong et. al. '21, introduced attention-based modeling for several audio tasks. However, relatively unexplored is the use of a ViT for audio spoof detection task. We bridge this gap and introduce ViTs for this task. A vanilla baseline built on fine-tuning the SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error rates (EERs). To improve performance, we propose a novel attention-based contrastive learning framework (SSAST-CL) that uses cross-attention to aid the representation learning. Experiments show that our framework successfully disentangles the bonafide and spoof classes and helps learn better classifiers for the task. With appropriate data augmentations policy, a model trained on our framework achieves competitive performance on the ASVSpoof 2021 challenge. We provide comparisons and ablation studies to justify our claim.", "sections": [{"title": "1. Introduction", "content": "Vision Transformers (ViTs) (Dosovitskiy et al. [1]) have emerged as the state-of-the-art method for multiple computer vision [2] and audio (Gong et al. [3]) tasks. Audio ViTs can self-attend to extract optimal patch embeddings and are capable of learning long-range global context within spectrograms [4, 3]. Inspired by the success of audio ViTs in sound and speech classification, here we introduce ViTs for the audio spoof detection task.\nSimilar to [3], the standard approach is to fine-tune a pre-trained audio ViT for a given downstream classification task using cross-entropy. Pre-training is done on a large dataset, such as AudioSet [5] or LibriSpeech [6], and fine-tuning on a smaller task-specific dataset. However, this standard approach does not empirically work well for audio spoof detection (see Section 4.1), and results in high equal error rate (EER). Pretraining on a large audio dataset such as AudioSet-2M [5] is a key requirement for audio ViTs to perform well [4, 3]. However, such bonafide-only datasets are not optimal as they don't contain spoof samples; spoof detection is an out-of-distribution downstream task.\nThe de-facto dataset for audio spoof detection is ASVSpoof 2021 logical access (LA) challenge [7]. Its training set is the same as ASVSpoof2019 [8] and contains clean studio quality data. Its test set, on the other hand, contains data corrupted by codecs and transmission channel artifacts. Also, when finetuning an audio ViT for spoof detection, the ASVSpoof19 LA training dataset falls under the small data regime. We thus require an appropriate training and augmentation strategy that helps achieve robustness against data corruption in the test set.\nSupervised contrastive learning (CL) methods [9] that use Siamese networks (Koch et al. [10]) have emerged as an algorithm class that helps learn efficient data representations. Such approaches learn a representation space from generic input features by pulling samples from the same class closer together and pushing samples from different classes apart, even on limited data. Thus, they are able to train on small datasets, learn better classification margins, and are more robust to data corruptions than cross-entropy classifiers [9]. Inspired by these merits, we pursue a CL approach. We introduce a cross-attention branch into the training framework and propose a novel loss formulation that measures the (dis-)similarity between the self and cross-attention representations to separate the bonafide and spoof classes.\nWe observe that the proposed attention-based CL framework, with appropriate data augmentations, is able to learn discriminative representations that disentangle the bonafide samples from the spoofed ones. Further, our approach shows a significant gain in performance over the baseline cross-entropy classifier. To summarize, the main contributions of our work are:\n1. We propose a two-stage contrastive learning framework to train an audio ViT for the spoof detection task.\n2. We consider Siamese training for representation learning and introduce a cross-attention branch into the training framework to learn discriminative representations for bonafide and spoof classes; a suitable loss function is formulated.\n3. A MLP classifier trained on the learned representations outperforms the ASVSpoof 2021 challenge baselines [7] and competes with the best-performing models.\nFinally, while in this paper we focus on audio spoof detection, the proposed CL framework is not constrained by design to our specific problem, and is a general-purpose framework. In the future, we plan to explore the framework for other downstream tasks, such as language identification and acoustic scene classification."}, {"title": "2. Related works", "content": "Attention-based modeling. Self-attention mechanism relates different patches within an input image using attention weights and encodes efficient representation of an image. It is a building block in ViTs, and has facilitated state-of-the-art (SOTA) classification performance on multiple computer vision tasks [1, 11, 12]. Cross-attention [13, 14] extends the idea to image pairs and uses attention matrices to encode an aggregate representation for the input pair based on the similarity in their patches. Due to its ability to align features from different inputs, cross-attention has been used for domain adaptation [14] and multi-modal feature fusion [15, 16]. ViTs with self-attention only [3] have achieved SOTA performance on multiple audio/speech classification tasks, however use of both self and cross-attention in audio ViTs, especially for audio spoof detection, is unexplored.\nContrastive learning for audio classification. In CL, samples are fed into the network in form of data pairs in order to contrast between them when learning the representation spaces of each class. A typical CL formulation maximizes the similarity of samples in an input pair when they belong to the same class, and their dissimilarity otherwise [17, 18]. In [17, 18], CL was used to derive audio representations for a variety of speech and non-speech classification tasks, including speaker identification, acoustic scene classification, and music recognition.\nXie et al. [18] focus on detecting studio quality spoofed audio and proposed a CL algorithm where a Siamese CNN is trained on the ASVSpoof19 LA. On the other hand, we propose a CL formulation that uses cross-attention to train a ViT for spoof detection. Further, it is unclear whether a Siamese network can perform well on the ASVSpoof 2021 LA, which contains impairments from transmission codecs [7]. To this end, we use appropriate data augmentations that help achieve robustness against such codec impairments.\nSpoof detection. Early works have focused on handcrafted features, such as cepstral coefficients [19], with classical methods such as Gaussian mixture models [20] (also see [8] and references therein). Recent methods use CNNs that operate on 2D features extracted from audio signals [21, 22, 23], or directly map raw waveforms to their labels in an end-to-end fashion [24, 25]. Introduction of attention mechanism into CNN models has further enhanced the single model performance for the ASVSpoof21 challenge [26], particularly those employing temporal and/or spatial attention. For this challenge, the works [27] and [28] employ wav2vec2.0 [29] large audio model as a feature extractor and achieve significantly better EER. Among other contributors, the improvement comes from wav2vec2.0 - a deep network of CNN and transformer blocks. While it is clear from SOTA that the introduction of attention layers in CNNs gave improved performance, it is unclear whether a pure attention-based model can be used for spoof detection. We bridge this gap and study the use of ViTs for audio spoof detection."}, {"title": "3. Method", "content": "We introduce an audio ViT to learn efficient representations for the spoof detection task. We consider the self-supervised audio spectrogram transformer (SSAST) [3], a ViT pretrained on the AudioSet [5] and LibriSpeech [6] datasets. We propose a CL-based two-stage training framework, summarized in Fig. 1. In Stage I, we adapt the SSAST to train in a Siamese manner (we call it SSAST-CL), in order to learn discriminative representations for the bonafide and spoof classes. In Stage II, we train an MLP using the SSAST-CL backbone to classify the learned representations as bonafide or spoof.\n3.1. Stage I - representation learning\nIn Stage I, the goal is to learn discriminative representations for the bonafide and spoof classes. To this end, we propose the SSAST-CL framework which adapts the SSAST architecture in [3] to a three-branch Siamese training (see Fig. 1). The framework takes a pair of data (x1,x2) as input and feeds them to SSAST-CL, whose weights are shared across the three branches. Two branches in the SSAST-CL use self-attention to compute representations $r_{1}^{SA}$ and $r_{2}^{SA}$, whereas the third branch uses cross-attention to compute an aggregate representation $r_{12}^{CA}$ for the input pair.\nIn the self-attention branch for x1 (and similarly for x2), the transformer blocks encode the intermediate representations $r_{1}^{SA'}$ using the query, key and value matrices (Q1, (K1, V\u2081)) from the same x1 branch. Whereas in the cross-attention branch, the transformer blocks encode the intermediate representations $r_{12}^{CA'}$ using the query Q1 from the x1 branch and key-value matrices (K2, V2) from the x2 branch (as shown in Fig. 1). By way of this design, the final representation $r_{12}^{CA}$ becomes an aggregate representation of the input pair because it captures the information in x2 that is relevant to x1.\nWhen computing the attention matrices in the transformer blocks, the query, key, and value are assigned learnable weights Wq, Wk, and $W_{v}$ respectively. These weights are shared across the three branches, as is the case with all other learnable parameters in SSAST-CL. Finally, we add a linear projection MLP at the end of SSAST-CL to upsample all the attention-based repre-"}, {"title": "3.2. Stage II - classifier", "content": "In Stage II, we train an MLP using weighted cross-entropy to classify the representations from Stage I as bonafide or spoof. For a given input, the model weights from Stage I are frozen and a self-attention branch is used to compute the representation.\nSimilarity is measured after feeding the representations $r_{1}^{SA}$, $r_{2}^{SA}$, $r_{12}^{CA}$ through the projection MLP (see Fig. 1)"}, {"title": "3.3. Data augmentations", "content": "We need a suitable data augmentation policy to support our training framework in three ways: prevent overfitting, handle speaker variability, and achieve robustness to telephony codec impairments. The following augmentations are used: pitch-shift, time-stretch, time and frequency masking from WavAugment [32], linear and non-linear convolutive noise and impulsive signal dependent additive noise from RawBoost [33], and the narrowband frequency impulse response (FIR) filters suggested in [34]. All augmentations are applied on-the-fly during the model training."}, {"title": "4. Experiments and results", "content": "Audio pre-processing. Raw audio waveforms of length 6-seconds are used to create log-mel spectrograms of size 128 mel-frequency bins and 512 timebins, computed using PyTorch Kaldi [35] with 25ms Hanning window and 10ms overlap. Longer waveforms are cut off at the end, whereas shorter waveforms are repeat padded by concatenating the original signal with its time-inverted version until the length is 6 seconds.\nSampling and batching. For the contrastive learning in Stage I, data pairs (x1, x2) are created as follows: The sample x1 is picked up in sequence from the training dataset, covering each datapoint once over a training epoch. For each x1, we make use of its class information c(x1) to select the pairing sample x2 such that positive and negative pairs are created with equal probability of 0.5. When picking up x2 from the spoof class, we assign equal probability of 0.5 to the text-to-speech (TTS) and voice conversion (VC) subclasses. Once a pair is picked, each of the data augmentations from Section 3 are applied to x1 with a probability of 0.8, and a randomly selected subset of data augmentations is applied to x2. The above procedure is repeated until a batch of 64 pairs is created. During Stage II, we pick batches of size 64 sequentially from the dataset; to each sample, all data augmentations are applied with 0.8 probability.\nTraining policy. For both Stage I and II, we use Adam optimizer with a learning rate of $10^{-4}$, and an exponential rate decay scheduler with $\\gamma$ = 0.95 for every 5 epochs. The model is trained for 50 epochs for each stage. For Stage I, the epoch checkpoint reporting the least validation loss is chosen. For Stage II, the epoch checkpoint reporting the smallest validation EER is chosen.\nImplementation of SSAST-CL. For details on the SSAST-CL implementation, please see the supplementary material.\nWe now present numerical studies to demonstrate the improvements in EER when using our CL framework. We compare with the state of the art and ablate on the data augmentations.\n4.1. Impact of data augmentation, contrastive learning and cross-attention\nIn Table 1, we compare the performance of the proposed contrastive learning framework against baselines that finetune the pretrained SSAST using weighted cross-entropy (WCE). The vanilla WCE, which follows the training and augmentation policy recommended in [3], reports a high EER of 19.48 on ASVSpoof 2021 LA evaluation set. This is likely because the augmentations in vanilla WCE do not account for codec impairments. When our augmentation policy from the ablation studies in Table 3 is used, the resulting WCE-updated policy reports an EER of 8.96, which is marginally better than the 9.26 reported by the best-performing baseline B03 in ASVSpoof 2021 [7]. When the training framework is additionally replaced with our SSAST-CL framework, the EER improves significantly to 4.74.\nTable 1 also demonstrates the impact of introducing cross-attention. We see that the EER improves from 5.64 to 4.74 when we set a weight of $\\alpha$ = 0.2 to the cross-attention loss term $L_{CA}$. The t-SNE plots in Fig. 2 suggest that the proposed SSAST-CL framework better disentangles the bonafide and spoof classes when compared to the vanilla WCE baseline. There is also an improvement from introducing cross-attention; although the visual differences are subtle in this case due to the high reduction in dimensionality from 192 to 2 when plotting the t-SNEs.\nWhile the impact of SSAST-CL is evident from Table 1 and Fig. 2, we note that our framework is not fully optimized to report the best EERs. A grid search on the training hyperparameters, including the optimizer, learning rate, batch size, and cross-attention weight, will likely boost the EERs further.\n4.2. Comparison with ASVSpoof21 top-performing models\nIn Table 2, we compare the performance of the proposed SSAST-CL system against the top-performing single system models on the ASVSpoof 2021 LA evaluation dataset. Firstly, our system comprehensively outperforms the challenge's best-performing baseline B03 [7]. We see that an audio ViT, with appropriate training and data augmentations, can indeed achieve competitive performance on the audio spoof detection task. Note that a vanilla WCE finetuning on the SSAST model, as suggested in [3], results in worse EER than the challenge baseline (see Table 1). Secondly, when positioned against the best-performing models, our system reports comparable EERs while being significantly smaller in size than most of them. The LCNN-LSTM [34] is the only lightweight system that reports a smaller EER than us. Lastly, our augmentations are much simpler than in the ResNet-LDE [36] and ResNet-34 [37] systems, as we do not use external noise or impulse response datasets for the augmentations.\n4.3. Ablation on data augmentations\nIn Table 3, we ablate over data augmentation combinations for the proposed SSAST-CL framework. We note that the RawBoost and FIR augmentations are both crucial for the model to perform well. These augmentations differ in design but both help capture the telephony codec artifacts, although to different extents. Telephony impairments are known to be present in the ASVSpoof 2021 LA dataset. The remaining augmentations in our policy, namely, time masking, frequency masking, pitch shift, and time stretch, are now commonplace in spoof detection modeling. These are essential because they account for speaker variability and prevent the model from overfitting [32]."}, {"title": "5. Conclusion", "content": "In this work, we answered the question \"Can we leverage ViTs for the audio spoof detection task?\" We conducted investigations using SSAST-CL (our adaption of the SSAST model [3] for contrastive learning). A vanilla finetuning of the pretrained SSAST with cross-entropy loss gave sub-optimal performance. To learn more discriminative audio representations, the proposed SSAST-CL framework used Siamese training with a cross-attention branch and a novel contrastive loss formulation. An MLP was later used to classify the learned representations as bonafide or spoof. We empirically showed that the SSAST-CL framework successfully disentangles the bonafide and spoof classes, and it helps learn better classifiers for the task at hand. The introduction of cross-attention, along with suitable augmentations, has allowed our system to achieve competitive performance on the ASVSpoof 2021 challenge.\nSeveral research directions are enabled as a result of our work. A joint training of the two stages in our framework, using a multi-task loss formulation, could likely improve the model performance and also reduce the training time. Another direction would be to build an importance sampling/training policy where hard-to-learn samples (for example, of the voice conversion type) are prioritized. The proposed contrastive learning framework can also be extended to other downstream audio tasks where limited"}, {"title": "6. Supplementary material", "content": "Implementation details for SSAST-CL. In Stage I, the SSAST-CL framework employs Siamese training with the SSAST architecture [3], where the learnable weights are shared across three training branches: two for self-attention and one for cross-attention. In the self-attention branch, an input log-mel spectrogram x is encoded into its attention-based representation vector $r_{1}^{SA}$ using a series of transformer and MLP blocks. As illustrated in Fig. 1, the log-mel spectrogram is split into a sequence of patches of size 128 \u00d7 2 created with a stride of 1 time-bin. Each patch is then linearly projected into a 1 \u00d7 192 vector, to which its learnable positional embedding vector (of the same size) is added. The resulting sequence of 511 (1 \u00d7 192) patch embeddings is fed into a series of N = 12 transformer blocks, each with 3 attention heads. Within each transformer block, learnable weights (Wq, Wk, Wv) are assigned to the query, key, and value matrices for each attention head. These weights and other learnable parameters in the SSAST architecture are shared across the three branches for Siamese training. The output of the final transformer block, of size (511 \u00d7 192), is passed through a mean pooling layer to obtain the final representation $r_{1}^{SA}$ of size 1 \u00d7 192. The same procedure is followed in the cross-attention branch, except that the input for the first skip connection is the sequence of 1-D patch embeddings from the x2 branch.\nFor the purpose of contrastive loss calculations, all the self and cross attention representations ($r_{1}^{SA}$, $r_{2}^{SA}$, $r_{12}^{CA}$) are further sent through a projection MLP comprising LayerNorm, linear up-sampling to 512, followed by LayerNorm and ReLU activation. For Stage II, we only consider the x1 branch and pass the representations $r_{1}^{SA}$, obtained from before the final projection MLP in Stage I, through a MLP classifier comprising a linear layer to downsample from 192 to 128, followed by BatchNorm and ReLU, and a linear layer with softmax for binary classification.\nTo initialize the SSAST weights in Stage I, we use the pre-trained weights provided by the authors in [3]. Please see [3] for details on how the SSAST is pretrained in a self-supervised manner using a patch masking and reconstruction strategy.\nComputing Infrastructure. We run experiments on a 1x NVIDIA A100 GPU instance with 30 vCPUs (2.4GHz AMD EPYC 7J13) and 200 GB memory. The average training time per epoch is 15 minutes for Stage I and 10 minutes for Stage II. A majority of the training time is spent on our (sequential) sampling and data augmentation procedure."}]}