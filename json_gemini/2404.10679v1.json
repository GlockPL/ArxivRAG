{"title": "HSVI-based Online Minimax Strategies for Partially Observable Stochastic Games with Neural Perception Mechanisms", "authors": ["Rui Yan", "Gabriel Santos", "Gethin Norman", "David Parker", "Marta Kwiatkowska"], "abstract": "We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure. One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state. We present, for the first time, an efficient online method to compute an \\(\\epsilon\\)-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values. For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values. This inherits the soundness of continual resolving at the cost of pre-computing the bound. For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing \\(\\epsilon\\)-distance to the value of the game at the initial belief known to both agents.", "sections": [{"title": "1. Introduction", "content": "Partially-observable stochastic games (POSGs) are a modelling formalism that enables strategic reasoning and (near-)optimal synthesis of strategies and equilibria in multi-agent settings with partial observations and uncertainty. One-sided POSGs (Hor\u00e1k et al., 2023) are a tractable subclass of two-agent, zero-sum POSGs with an asymmetric information structure, where only one agent has partial information while the other agent is assumed to have full knowledge. This is well suited to autonomous safety- or security-critical settings, such as patrolling or pursuit-evasion games, which require reasoning about worst-case assumptions. Since real-world settings increasingly often utilise neural networks (NNs) for perception tasks such as localisation and object detection, one-sided neuro-symbolic POSGs (one-sided NS-POSGs) were introduced (Yan et al., 2023). In this model the agent with partial information observes the environment only through a trained NN classifier, and consequently the game is generalised to continuous environments, to align with NN semantics, while observations remain discrete. A point-based NS-HSVI method was developed to approximate values of one-sided NS-POSGs working with (polyhedral decompositions of) the continuous space.\nStrategy synthesis for continuous games is more challenging than for the finite-state case (Hor\u00e1k et al., 2023), since continuous-state spaces lead to an infinite number of strategies and discretisation suffers from the curse of dimensionality. Several offline methods exist, based on counterfactual"}, {"title": "2. Background", "content": "We briefly review the model of Yan et al. (2023), which generalises one-sided POSGs (Hor\u00e1k et al., 2023) to continuous-state spaces and allows neural perception mechanisms. Let \\(P(X)\\) and \\(F(X)\\) denote the spaces of probability measures and functions on a Borel space \\(X\\), respectively.\nOne-sided NS-POSGs. A one-sided neuro-symbolic POSG (NS-POSG) \\(C\\) is a two-player zero-sum infinite-horizon game with discrete actions and observations, where one player (Ag\u2081) is partially informed and the other (Ag\u2082) is fully informed. Unlike Hor\u00e1k et al. (2023), the game is played in a closed continuous environment \\(S_E\\), which Ag\u2081 perceives only using perception function \\(obs_1\\) given as a (trained) ReLU NN classifier that maps environment states to so called percepts, ranging over a finite set \\(Per_1\\). The use of classifiers is aligned with, e.g., object detection or vision tasks in autonomous systems. We further assume that Ag\u2081 has a discrete local state space \\(Loc_1\\), which is observable to both agents, and that Ag\u2082 has full knowledge of the environment's state.\nA game \\(C\\) comprises agents Ag\u2081= (\\(S_1, A_1, obs_1, \\delta_1\\)), Ag\u2082=(\\(A_2\\)) and environment E=(\\(S_E, \\delta_E\\)), where \\(S_1 = Loc_1 \\times Per_1\\); \\(A = A_1 \\times A_2\\) are joint actions; \\(obs_1: (Loc_1 \\times S_E) \\rightarrow Per_1\\) is Ag\u2081's perception function (note that we allow NNs to additionally depend on local states); \\(\\delta_1: (S_1 \\times A) \\rightarrow P(Loc_1)\\) is Ag\u2081's local transition function; and \\(\\delta_E: (Loc_1 \\times S_E \\times A) \\rightarrow P(S_E)\\) is E's finite-branching transition function. We work in the belief space \\(S_B \\subseteq P(S)\\), where \\(S = S_1 \\times S_E\\), and assume an initial belief \\(b^{init}\\) using the particle-based representation (Porta et al., 2006; Doucet et al., 2001). A belief of Ag\u2081 is given by \\(b = (s_1, b_1)\\), where \\(s_1 \\in S_1\\), \\(b_1 \\in P(S_E)\\) and \\(b_1\\) is represented by a weighted particle set \\(\\{(s_E, K_i)\\}_{i=1}^{N_b}\\) where \\(k_i \\geq 0\\) and \\(\\sum_{i=1}^{N_b} k_i = 1\\).\nThe game starts in a state \\(s = (s_1, s_E)\\), where \\(s_1 = (loc_1, per_1) \\in S_1\\), and \\(s\\) is sampled from \\(b^{init}\\). At each stage of the game, both agents concurrently choose one of their actions. If \\(a = (a_1, a_2) \\in A\\) is chosen, the local state \\(loc_1\\) of Ag\u2081 is updated to \\(loc'_1 \\in Loc_1\\) via \\(\\delta_1(s_1, a)\\), while the environment updates its state to \\(s'_E \\in S_E\\) via \\(\\delta_E(loc_1, s_E, a)\\). Finally, Ag\u2081, based on \\(loc'_1\\), generates"}, {"title": "3. NS-HVSI Continual resolving", "content": "Continual resolving, e.g., (Morav\u010d\u00edk et al., 2017), is an online method for computing an \\(\\epsilon\\)-minimax strategy in two-player, zero-sum imperfect information EFGs; it keeps track of an agent's belief of its opponent state and opponent counterfactual values to build and solve a subgame to synthesise choices, without building a complete strategy. It is sound, in computing an \\(\\epsilon\\)-minimax strategy, but can be expensive as it needs to estimate opponent counterfactual values by traversing the game tree.\nWe now present a novel variant of continual resolving, which utilises the lower bound function \\(V_i^{lb}\\) computed by one-sided NS-HSVI to synthesise an \\(\\epsilon\\)-minimax strategy for Ag\u2081 that achieves the desired \\(\\epsilon\\) distance to the value function at the initial belief. The method is efficient as it only requires solving a single LP at each stage. We first introduce the following minimax operator.\nDefinition 1 (Minimax) The minimax operator \\(T: F(S_B) \\rightarrow F(S_B)\\) is given by:\n\\[[TV](s_1, b_1) = \\max_{u_1 \\in P(A_1)} \\min_{u_2 \\in P(A_2|S)} E_{(s_1, b_1), u_1, u_2} [r(s, a)]\\\\\n\\qquad + \\beta \\sum_{(a_1, s'_1) \\in A_1 \\times S_1} P(a_1, s'_1 | (s_1, b_1), u_1, u_2) V (s'_1, b'_1) \\tag{1}\\]"}, {"title": "4. Inferred-Belief Strategy Synthesis", "content": "We complement our variant of continual resolving with strategy synthesis for Ag\u2082, which utilises the upper bound function \\(V_i^{ub}\\) pre-computed offline and keeps track of an inferred belief about what Ag\u2081 believes, which could differ from Ag\u2081's true belief. Any offline method for fully-observable stochastic games could instead be used, with the associated high computational and storage cost of"}]}