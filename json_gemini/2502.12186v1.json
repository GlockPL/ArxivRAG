{"title": "E2CB2former: Effecitve and Explainable\nTransformer for CB2 Receptor Ligand Activity\nPrediction", "authors": ["Jiacheng Xie", "Yingrui Jit", "Linghuan Zeng", "Xi Xiao", "Gaofei Chen", "Lijing Zhu", "Joyanta Jyoti Mondal", "Jiansheng Chen"], "abstract": "Accurate prediction of CB2 receptor ligand activity\nis pivotal for advancing drug discovery targeting this receptor,\nwhich is implicated in inflammation, pain management, and\nneurodegenerative conditions. Although conventional machine\nlearning and deep learning techniques have shown promise,\ntheir limited interpretability remains a significant barrier to\nrational drug design. In this work, we introduce CB2former,\na framework that combines a Graph Convolutional Network\n(GCN) with a Transformer architecture to predict CB2 receptor\nligand activity. By leveraging the Transformer's self-attention\nmechanism alongside the GCN's structural learning capability,\nCB2former not only enhances predictive performance but also\noffers insights into the molecular features underlying receptor\nactivity. We benchmark CB2former against diverse baseline\nmodels-including Random Forest, Support Vector Machine,\nK-Nearest Neighbors, Gradient Boosting, Extreme Gradient\nBoosting, Multilayer Perceptron, Convolutional Neural Network,\nand Recurrent Neural Network-and demonstrate its superior\nperformance with an R2 of 0.685, an RMSE of 0.675, and\nan AUC of 0.940. Moreover, attention-weight analysis reveals\nkey molecular substructures influencing CB2 receptor activ-\nity, underscoring the model's potential as an interpretable\nAI tool for drug discovery. This ability to pinpoint critical\nmolecular motifs can streamline virtual screening, guide lead\noptimization, and expedite therapeutic development. Overall,\nour results showcase the transformative potential of advanced\nAI approaches-exemplified by CB2former-in delivering both\naccurate predictions and actionable molecular insights, thus\nfostering interdisciplinary collaboration and innovation in drug\ndiscovery.", "sections": [{"title": "I. INTRODUCTION", "content": "Cannabis is one of the most widely used drugs globally, pro-\nducing a diverse array of pharmacological effects in humans\n[1]-[3]. Numerous studies have demonstrated that cannabis\nand cannabinoids offer significant therapeutic benefits, includ-\ning stimulating appetite and alleviating nausea and vomiting\n[4]\u2013[8]. The discovery of cannabinoid receptors (CBRs) pro-\nvided insight into the mechanism of action of cannabinoid\ndrugs. CBRs are classified into two types: cannabinoid recep-\ntor type 1 (CB1) and cannabinoid receptor type 2 (CB2) [5],\n[9], both belonging to the G protein-coupled receptor (GPCR)\nsubfamily [10]\u2013[12].\nCB1 receptors are predominantly located in the brain,\nfostering interdisciplinary collaboration and innovation in drug particularly in the cerebral cortex, hippocampus, basal ganglia,\ndiscovery.\nand cerebellum [13]. Activation of CB1 receptors is associated\nwith psychotropic effects [14], [15]. However, their use was\ndiscontinued due to significant side effects on the central\nnervous system, including anxiety, depression, and suicidal\nthoughts [16]. These adverse effects overshadowed the po-\ntential therapeutic benefits of CB1 activation, leading to its\nremoval from the market and the cessation of further clinical\ndevelopment [17].\nIn contrast, CB2 receptors are primarily found in periph-\neral tissues, particularly in immune-related areas such as the\nspleen, tonsils, thymus, mast cells, and blood cells [18]. As a\nresult, CB2 receptors represent a promising therapeutic target\nfor a variety of diseases without inducing psychotropic side\neffects [19], including conditions such as inflammatory and\nneuropathic pain [20]-[24]. Consequently, there has been a\ngrowing interest in identifying new CB2 ligands [25].\nA more affordable and efficient method to accelerate the\ndesign and identification of novel CB2 ligands is urgently\nneeded, as traditional experimental screening is both costly\nand time-consuming [26], [27]. In the following sections, we\nwill employ algorithms to demonstrate the crucial role these\nligands play in CB2 receptor activity.\nAs algorithms continue to evolve and improve, an increasing\nnumber of methods are being developed to predict molecular\nactivity. This growing array of computational tools includes\na range of approaches such as machine learning algorithms,\nmolecular modeling techniques, and quantitative structure-\nactivity relationship (QSAR) models [28], [29]. These ad-\nvancements allow researchers to gain deeper insights into\nthe interactions between molecules and their targets, thereby\nenhancing the prediction of biological activity and guiding\ndrug discovery efforts [8], [30].\nFor example, in 2012, Myint KZ et al. [31] introduced\nFANN-QSAR, a novel method for predicting the biological\nactivity of diverse chemical ligands. Their study showed that\nFANN-QSAR effectively identified CB2 lead compounds with\nstrong binding affinity and compounds resembling known\ncannabinoids. Furthermore, the model offered insights into\nvariations in R-groups and scaffolds among known ligands.\nThis underscores the importance of meticulous molecular de-\nscriptor selection in virtual screening for identifying bioactive\nmolecules. In 2013, Chao Ma et al. [32] expanded LiCABEDS\nfor cannabinoid ligand selectivity modeling using molecular\nfingerprints, comparing its performance with SVM. Analy-\nsis of LiCABEDS models highlighted structural differences\nbetween CB1 and CB2 ligands, aiding in novel compound\ndesign. Importantly, they found that LiCABEDS successfully\nidentified newly synthesized CB2 selective compounds, show-\ncasing its potential for drug discovery. In 2017, Cano Get\nal. [33] used the power of random forests to automatically\nselect features (molecular descriptors), thereby avoiding man-\nual selection of descriptors and improving activity prediction\nby improving goodness of fit. In 2018, Floresta G et al. [34]\nproposed two new 3D-QSAR models that comprehend a large\nnumber of chemically different CB1 and CB2 ligands and\nwell account for the individual ligand affinities. These features\nwill facilitate the recognition of new potent and selective\nmolecules for CB1 and CB2 receptors. SVM-based techniques\nare considered a powerful approach in early drug discovery.\nIn 2020, Stokes JM et al. [35] proposed a deep neural network\n(DNN) that can predict molecules with antimicrobial activity\nby using a directed information transfer neural network (D-\nMPNN) architecture. They successfully identified halicin as a\nnew broad-spectrum antimicrobial compound, demonstrating\nthe effectiveness of deep learning approaches in drug dis-\ncovery. In 2021, Mukuo Wang et al. [36] utilized a \"deep\nlearning-pharmacophore-molecular docking\" virtual screen-\ning strategy to identify CB2 antagonists from the ChemDiv\ndatabase. Testing 15 hits, seven showed binding affinities\nagainst CB2, with Compound 8 exhibiting the strongest ac-\ntivity. The 4H-pyrido[1,2-a] pyrimidin-4-one scaffold in Com-\npound 8 holds promise for CB2 drug development. The study\nsuggests further application of their model for identifying\nnovel CB2 antagonists and designing potent ligands for diverse\ntherapeutic targets. In 2022, Yuan et al. [37] utilized MCCS\nto analyze CB2 allosteric modulation. Docking known CB2\nallosteric modulators (AMs) Ec2la, trans-\u03b2-caryophyllene, and\ncannabidiol (CBD), they identified potential binding sites.\nMolecular dynamics simulations suggested site H as most\npromising. Future plans include bio-assay validations. In 2022,\nCerruela-Garcia et al. [38] proposed a feature selection method"}, {"title": "II. MATERIALS AND METHODS", "content": "1) Data Sources: The data used in this study were sourced\nfrom two comprehensive and publicly accessible chemical\ndatabases, ChEMBL and BindingDB, both of which provide\ndetailed information on chemical compounds, their molecular\nstructures, and associated bioactivities. For the purposes of this\nresearch, we specifically selected compounds with documented\ninteractions involving the Cannabinoid Receptor 2 (CB2).\n2) Data Preprocessing: Prior to model development, the\nraw data underwent several preprocessing steps-data clean-\ning, normalization, and transformation to ensure consistency\nand quality.\nData Cleaning: Duplicate entries and compounds with\nmissing or incomplete bioactivity values were excluded. Ad-\nditionally, any compounds with ambiguous or inconsistent\nactivity measurements were removed to maintain a high-\nquality dataset.\nNormalization: All bioactivity measures (Ki, IC50, EC50)\nwere converted to a uniform pIC50 scale, defined as\nlog10(IC50). This conversion standardizes the range of re-\nported values, facilitating more direct comparisons among\ndifferent bioactivity measurements.\nTransformation: Molecular structures were represented us-\ning canonical SMILES (Simplified Molecular Input Line Entry\nSystem) notation to ensure each compound's structure was\ncaptured uniquely and consistently. This canonicalization step\nis critical for maintaining structural uniformity across the\ndataset and for enabling accurate downstream analyses."}, {"title": "B. Molecular Representation", "content": "Effective application of machine learning models for CB2\nreceptor ligand activity prediction requires the transforma-\ntion of chemical structures into a numerical form. In this\nstudy, we employed two complementary molecular repre-\nsentations-SMILES strings and molecular fingerprints-to\ncapture both sequential and substructural information relevant\nto ligand-receptor interactions.\n1) SMILES Strings: SMILES (Simplified Molecular Input\nLine Entry System) is a compact, text-based format for de-\nscribing chemical structures. Its human-readable nature and\nwidespread compatibility make SMILES a popular choice for\ncomputational chemistry tasks.\nIn this work, SMILES strings were generated for each\ncompound and used as the primary input to the CB2former.\nBy encoding atomic connectivity and bond information in a\nlinear sequence, SMILES strings enable sequence-based deep\nlearning models such as the Transformer architecture-to\nlearn meaningful representations of molecular structure and\nreactivity.\n2) Molecular Fingerprints: Molecular fingerprints encode\nthe presence or absence of specific substructures within a\nmolecule into a fixed-length bit vector, facilitating tasks like\nsimilarity searching and machine learning classification or re-\ngression. Here, we utilized Extended Connectivity Fingerprints\n(ECFP), also referred to as Morgan fingerprints, owing to their\nproven effectiveness in capturing local substructural features.\nThe fingerprints were calculated as follows:\n1) Each molecule was converted from its SMILES string\ninto an RDKit molecular graph.\n2) ECFP fingerprints (radius 2, 2048-bit length) were then\ngenerated from this graph, resulting in a binary vector\nrepresentation that highlights critical substructures.\nThese fingerprints were primarily used by models that\nprocess numerical input, enabling them to associate specific\nsubstructural patterns with variations in CB2 receptor activity."}, {"title": "C. Model Development", "content": "1) Baseline Models: To establish a performance benchmark\nfor predicting CB2 receptor ligand activity, we first imple-\nmented several widely recognized machine learning (ML)\nand deep learning (DL) algorithms. These baseline models\nserve as a comparative foundation against which the proposed\nCB2former can be evaluated.\nWe explored several classical ML algorithms known for\ntheir effectiveness in various predictive tasks. These models\nwere evaluated for their performance in predicting the activity\nof CB2 receptor ligands.\nRandom Forest (RF): An ensemble learning method that\nconstructs multiple decision trees during training. Predictions\nare made by aggregating the outputs from all constituent\ntrees, either via majority voting (classification) or averaging\n(regression). Random Forests are known for their robustness,\nespecially in high-dimensional feature spaces.\nSupport Vector Machine (SVM): A supervised learning\nalgorithm effective for both classification and regression tasks.\nBy mapping input data into high-dimensional spaces, SVMs\ncan capture complex relationships, making them well suited\nfor scenarios where the feature space is large relative to the\nnumber of samples.\nK-Nearest Neighbors (KNN): A simple, non-parametric\nmethod that classifies or regresses based on the k most similar\n(nearest) training examples in the feature space. Despite its\nsimplicity, KNN can perform competitively on certain datasets\nwith appropriate distance metrics and parameter tuning.\nGradient Boosting Machine (GBM): An iterative ensemble\ntechnique where each new weak learner-often a decision\ntree-attempts to correct the errors of the previous ensemble.\nGBM has the flexibility to handle a variety of data distributions\nand outliers.\nExtreme Gradient Boosting (XGBoost): A highly optimized\nlibrary built upon the gradient boosting framework. XGBoost\nemphasizes parallelization, regularization, and efficient hard-\nware utilization, often resulting in superior training speeds and\npredictive performance.\nFor deep learning baselines, we employed the following\nmodels:"}, {"title": "", "content": "Multilayer Perceptron (MLP): A fully connected feedfor-\nward neural network composed of multiple layers of per-\nceptrons. MLPs excel at capturing non-linear relationships,\nmaking them suitable for various types of structured numerical\ninputs such as fingerprints or descriptor vectors.\nConvolutional Neural Networks (CNNs): Although CNNS\nare traditionally applied to 2D image data, they can also\nprocess generated molecular images or 1D feature maps (e.g.,\ntransformed SMILES strings). Their localized receptive fields\nenable CNNs to learn relevant substructures and spatial pat-\nterns in the data.\nRecurrent Neural Networks (RNNs): Designed for sequence-\nbased data, RNNs such as LSTM (Long Short-Term Memory)\nnetworks are adept at capturing dependencies within sequential\ninputs like SMILES strings. This ability to handle long-\nterm context makes them particularly suitable for representing\nextended molecular sequences.\nAll baseline models were trained on the preprocessed\ndataset and tuned via standard hyperparameter optimization\nstrategies. Their predictive performances were evaluated using\nR-squared (R2), Root Mean Squared Error (RMSE), and\nArea Under the Curve (AUC) to provide a comprehensive\nassessment of both regression accuracy and classification-like\ndiscrimination.\n2) CB2former: Originally introduced for natural language\nprocessing tasks, the Transformer architecture has demon-\nstrated remarkable flexibility in handling sequential data and\ncapturing long-range dependencies. In addition to treating SMILES\nstrings as sequential input, we introduce a dynamic Prompt\nmechanism to incorporate CB2 receptor-specific knowledge\ndirectly into the model.\nTo guide the model toward receptor-specific features, we\nincorporate a set of unlearnable Prompt tokens each embed-\nding prior knowledge about CB2-related functional groups,\nknown binding motifs, or critical substructures-directly into\nthe input. These tokens are concatenated with the SMILES\nembeddings before being fed into the Transformer, thereby\nhighlighting receptor-relevant information during the attention\nprocess. In parallel, a Graph Convolutional Network (GCN)\nlayer is integrated to capture topological insights from molec-\nular graphs, further enhancing the model's representational\ncapacity.\nThe CB2former is composed of an encoder that combines\nstacked layers of domain knowledge (via prompt tokens)\nand self-attention, interleaved with point-wise, fully connected\n(feed-forward) layers. The key components are:\n\u2022 Graph Convolutional Network (GCN) Layer: Processes\nthe molecular graphs to learn representations of each\natom within the context of its neighbors. This structural\nunderstanding complements the sequence-based encoding\nfrom SMILES.\n\u2022 Embedding Layer: Converts the SMILES tokens into\ndense vector representations. The unlearnable Prompt\ntokens, carrying CB2-specific knowledge, are also em-\n\u2022\n\u2022\n\u2022\nbedded into similar dimensions and concatenated with\nthese SMILES embeddings.\nPositional Encoding: Injects information about the token\npositions into the embeddings, ensuring that the model\npreserves the order of SMILES tokens.\nSelf-Attention Layers: Enable the model to focus on dif-\nferent parts of the input sequence dynamically. Multiple\nattention heads capture diverse representation subspaces,\nguided by the prior knowledge encoded in the Prompt\ntokens.\nFeed-Forward Layers: Apply non-linear transformations\nto the outputs of the self-attention layers, further refining\nthe learned representations.\nBy combining sequential (SMILES-based) and structural\n(GCN-based) insights, the CB2former aims to provide a\nmore holistic view of each molecule. Notably, the injection\nof receptor-specific prior knowledge can accelerate model\nconvergence and improve interpretability, as the self-attention\nmechanism is guided toward critical features of the CB2\nreceptor.\nA variety of hyperparameters were optimized through an\nextensive search using cross-validation. The key hyperparam-\neters included: Number of encoder layers, Dimensionality of\nembeddings and hidden states, Number of attention heads in\neach self-attention layer, Dimensionality of the feed-forward\nsubnetwork and Dropout rate applied for regularization.\nThe best-performing configuration was chosen based on\nvalidation performance, striking a balance between model\ncapacity and generalization.\n3) Self-Attention Mechanism: The self-attention mecha-\nnism is a core component of the Transformer model, allowing\nit to dynamically focus on different parts of the input sequence.\nThis mechanism is particularly useful for capturing the rela-\ntionships between atoms in a molecule, which are crucial for\naccurate molecular property prediction, such as predicting the\nactivity of CB2 receptor ligands.\na) Attention Mechanism Details: In the self-attention\nmechanism, each token in the input sequence is represented\nby three vectors: Query (Q), Key (K), and Value (V). The\nattention score for each pair of tokens is calculated using the\ndot product of the Query and Key vectors, scaled by the square\nroot of the dimensionality of the Key vectors, and passed\nthrough a softmax function to obtain the attention weights.\nThis process is particularly beneficial for understanding the\nimportance of different molecular features in predicting CB2\nreceptor activity.\nAttention(Q, K, V) = softmax(QKT / \u221adk)V\nWhere dk is the dimensionality of the Key vectors. The\nattention weights are then used to compute a weighted sum of\nthe Value vectors, effectively allowing the model to aggregate\ninformation from different parts of the sequence. This aggre-\ngation helps identify which atoms or substructures within a\nmolecule are most relevant for binding to the CB2 receptor,"}, {"title": "", "content": "providing insights into the molecular interactions that drive\nbiological activity.\nb) Multi-Head Attention: To enhance the model's ability\nto capture different types of relationships, the self-attention\nmechanism is extended to multiple heads, each operating in a\ndifferent subspace of the input representations. The outputs of\nall attention heads are concatenated and linearly transformed to\nproduce the final output. This multi-head approach allows the\nmodel to attend to various aspects of the molecular structure\nsimultaneously, increasing the robustness and accuracy of\npredictions for CB2 receptor ligands.\nMultiHead(Q, K, V) = Concat(head1, head2, ..., head\u0127)WO\nWhere each attention head head; is computed as:\nhead\u2081 = Attention(QWKWK,VWV)\nHere, W, WK, and WV are the learned projection ma-\ntrices for the Query, Key, and Value vectors of the i-th head,\nand WO is the projection matrix for the concatenated outputs.\nc) Relevance to CB2 Receptor Ligand Prediction:\nThe self-attention mechanism's ability to highlight important\nmolecular features makes it particularly relevant for predict-\ning CB2 receptor ligand activity. By analyzing the attention\nweights, we can identify which parts of the molecule con-\ntribute most significantly to its binding affinity and activity.\nThis interpretability not only enhances the predictive perfor-\nmance but also provides valuable insights for drug discov-\nery, enabling the design of new compounds with optimized\nproperties. The ability to understand and visualize the key\ninteractions within the molecular structure is a significant\ninnovation in this study, leveraging the power of explainable\nAI to advance scientific research in cheminformatics and drug\ndiscovery."}, {"title": "D. Model Evaluation", "content": "1) Evaluation Metrics for Regression: To evaluate the per-\nformance of regression models predicting the activity of CB2\nreceptor ligands, the following metrics were used:\nR-squared (R2): This metric measures the proportion of\nvariance in the dependent variable that is predictable from the\nindependent variables. It is calculated as:\nR2 = 1 - (SSres / SStot)\nwhere SSres is the sum of squares of residuals, and SStot is\nthe total sum of squares.\nRoot Mean Squared Error (RMSE): This metric measures\nthe average magnitude of the errors between predicted and\nobserved values. It is calculated as:\nRMSE = \u221a((1/n) \u2211i=1n (Yi - \u0176i)2)\nwhere n is the number of observations, yi is the actual value,\nand yi is the predicted value.\n2) Evaluation Metrics for Classification: For classification\ntasks, the models' performance was evaluated using the fol-\nlowing metrics:\nArea Under the Curve (AUC): This metric measures the\nability of the model to distinguish between classes. It is calcu-\nlated as the area under the Receiver Operating Characteristic\n(ROC) curve, which plots the true positive rate against the\nfalse positive rate at various threshold settings.\nAccuracy: This metric measures the proportion of correctly\nclassified instances among the total instances. It is calculated\nas:\nAccuracy = (TP+TN) / (TP+TN+FP + FN)\nwhere TP is true positives, TN is true negatives, FP is false\npositives, and FN is false negatives."}, {"title": "E. Feature Importance and Interpretation", "content": "A clear understanding of which features most signifi-\ncantly influence model predictions is critical for interpretabil-\nity-particularly in scientific research, where insights into\nmolecular mechanisms can guide subsequent experimental and\ntherapeutic strategies.\n1) SHAP Analysis: To quantify feature contributions in a\nmodel-agnostic fashion, we employ SHapley Additive exPla-\nnations (SHAP). Grounded in cooperative game theory, SHAP\nassigns each feature a value that indicates its contribution to\nthe deviation of a given prediction from the model's mean\nprediction. SHAP values offer a standardized approach to\nfeature importance across diverse model architectures, in-\ncluding traditional ML models (e.g., RF, SVM) and deep\nlearning frameworks (e.g., MLP, CNN). By examining SHAP\nvalues for individual predictions, one can identify the specific\nfeatures that drive a particular result. Aggregating these values\nacross the dataset provides a global perspective on which\nfeatures consistently exert the greatest influence. SHAP value\nplots and summary diagrams highlight which features shift\nmodel outputs the most. This level of transparency can reveal\nhidden patterns and elucidate molecular descriptors that are\nparticularly relevant for CB2 receptor activity.\n2) Attention Weight Analysis: In addition to SHAP, the\nself-attention mechanism within the CB2former provides\nan inherently interpretable pathway for understanding how\ndifferent regions of a molecule contribute to its predicted\nactivity. During a forward pass, the model calculates attention\ndistributions that indicate the relative importance of each\ntoken-representing atoms or substructures\u2014in the SMILES\nsequence. By visualizing these attention weights as heatmaps,\none can pinpoint which parts of the molecule the model\ndeems most pertinent for CB2 receptor binding. Since attention\nweights are learned directly from training data, they naturally\ncapture domain-specific structural motifs that correlate with\nbiological activity. Such insights can be invaluable for guiding\nmolecular modifications or prioritizing scaffold optimization\nin drug discovery pipelines. The ability to highlight critical\natoms or functional groups aligns with the growing demand\nfor transparent and interpretable AI tools in the scientific\ncommunity. This not only enhances confidence in the model's\npredictions but also fosters hypothesis generation and more\ntargeted experimentation.\nIn summary, CB2former provides a multifaceted view of\nfeature importance. By revealing which molecular descriptors\nand substructures underpin the CB2former's predictions, re-\nsearchers can more effectively leverage these insights in virtual\nscreening, lead optimization, and other key drug discovery\nactivities."}, {"title": "III. RESULTS", "content": "1) Baseline Models vs. CB2former: To evaluate the per-\nformance of our models, we compared the CB2former with\nseveral baseline models, including traditional machine learning\nand deep learning models. The performance metrics used\nfor comparison included R-squared (R2), Root Mean Squared\nError (RMSE), and Area Under the Curve (AUC). The results\nof this comparison are presented in Table I."}, {"title": "", "content": "The CB2former demonstrates superior performance across\nall evaluated metrics compared to the baseline models. Specifi-\ncally, the CB2former achieved an R2 value of 0.685, indicating\na strong correlation between the predicted and actual values.\nAdditionally, the CB2former had the lowest RMSE at 0.675,\nsuggesting it has the highest predictive accuracy among the\nmodels tested. The AUC score of 0.940 further confirms\nthe model's excellent ability to distinguish between active\nand inactive compounds, outperforming other classification\nmodels."}, {"title": "2) Cross-Validation Results:", "content": "Cross-validation was per-\nformed to ensure the robustness and generalizability of the\nmodels. The results from k-fold cross-validation are presented\nin Table II."}, {"title": "Overall,", "content": "Overall, the CB2former outperforms traditional machine\nlearning and deep learning models, demonstrating both high\npredictive accuracy and robustness. These results underscore\nthe effectiveness of integrating Graph Convolutional Networks\nwith the Transformer architecture for predicting CB2 receptor\nligand activity."}, {"title": "B. Attention Weight Analysis", "content": "The self-attention mechanism in the CB2former allows for\nthe identification of important molecular features. This section\npresents the analysis of attention weights.By analyzing the attention weights, key molecular features\nthat influence the CB2 receptor activity can be identified.\nTable III lists the most important features identified through\nthe attention weight analysis."}, {"title": "IV. DISCUSSION", "content": "In this study, we introduced the CB2former, a novel\nframework that combines a Graph Convolutional Network\n(GCN) with a Transformer architecture to predict CB2 receptor\nligand activity. Our results demonstrate the effectiveness of\nthis integrated approach:\nThe CB2former significantly outperformed a range of tra-\nditional machine learning models-Random Forest, Support\nVector Machine, K-Nearest Neighbors, Gradient Boosting Ma-\nchine, Extreme Gradient Boosting\u2014as well as deep learning\nmethods such as Multilayer Perceptron, Convolutional Neural\nNetwork, and Recurrent Neural Network. Notably, it achieved\nan R2 of 0.78, an RMSE of 0.65, and an AUC of 0.94,\nunderscoring its superior predictive capacity in CB2 ligand\nactivity estimation.\nThe incorporation of a GCN layer into the Transformer\narchitecture, coupled with a dynamic prompt approach,\nmarkedly bolstered the model's ability to represent complex\nmolecular structures. By injecting unlearnable tokens carrying\nCB2-specific knowledge, the model leverages domain priors\nand systematically directs its focus toward receptor-critical\nsubstructures.\nThe self-attention mechanism within the CB2former not\nonly facilitates detailed interpretation of molecular substruc-\ntures but also elucidates the interplay between prompt tokens\nand SMILES sequence tokens. In doing so, the model high-\nlights which receptor-focused features are most relevant for a\ngiven prediction, offering deeper insights into the determinants\nof CB2 receptor activity.\nThe prompt-based knowledge injection has the potential to\nstreamline virtual screening pipelines by more efficiently iden-\ntifying promising lead candidates. By focusing on receptor-\nspecific features, the CB2former could reduce both the time\nand resources required for subsequent experimental validation\nin drug discovery workflows."}, {"title": "B. Comparison with Previous Studies", "content": "The results of this study align with and extend the findings\nof previous research on the effectiveness of deep learning\nmodels in molecular property prediction. While traditional\nmachine learning models have reported R2 values ranging from\n0.55 to 0.65, our CB2former achieved a higher R2 value of\n0.78.\nPrevious studies using deep learning models, such as CNNs\nand RNNs, have shown RMSE values in the range of 0.70 to\n0.80. In comparison, the CB2former reduced the RMSE to\n0.65, highlighting its enhanced predictive performance.\nIn addition to delivering improved performance, the\nCB2former offers enhanced interpretability through its self-\nattention mechanism. This contrasts with many conventional\ndeep learning methods, where feature importance can be less\ntransparent. The attention-weight analysis reported here sup-\nports findings from related work that highlights specific molec-\nular substructures as key drivers of CB2 receptor activity. By\nproviding a clearer view of how these substructures contribute\nto ligand-receptor interactions, the CB2former aligns with\nthe growing emphasis on explainable AI techniques in the\ncheminformatics domain."}, {"title": "C. Implications for CB2 Receptor Ligand Prediction", "content": "The findings from this study have several important implica-\ntions for the field of drug discovery, particularly in the context\nof CB2 receptor ligand prediction:\nThe CB2former offers a powerful tool for rapidly evaluat-\ning large compound libraries, helping to identify promising\nlead candidates that merit further experimental assessment.\nIts integration into existing virtual screening pipelines could\nstreamline early-stage drug discovery.\nThe attention-based interpretability facilitates a deeper un-\nderstanding of which molecular features govern CB2 receptor\nactivity. By pinpointing crucial substructures, researchers can\nguide synthetic efforts more strategically, iterating on com-\npounds that exhibit favorable interactions while discarding\nthose that lack essential structural motifs.\nThe success of the CB2former in both predictive perfor-\nmance and interpretability underscores the potential of ad-\nvanced AI methodologies in cheminformatics. By elucidating\nstructure-activity relationships, the model sets a precedent for\nintegrating explainable AI techniques more broadly, bolstering\nconfidence in computational predictions and promoting more\ntargeted experimental validation."}, {"title": "D. Limitations of the Study", "content": "Despite the promising results, this study has several limita-\ntions that should be addressed in future research:\nDataset Diversity: Although this work utilized a substantial\ndataset, it may not encompass the full range of chemical\ndiversity relevant to CB2 receptor ligands. Future studies\ncould incorporate more diverse and larger datasets to improve\ngeneralizability.\nModel Variants and Improvements: While the CB2former\nshowed marked performance gains, there remains scope for\nexploring advanced Transformer variants and transfer learning\nmethods to further boost predictive accuracy.\nComputational Overheads: Training the CB2former incurs\nhigher computational costs compared to traditional machine\nlearning models. To ensure wider practical adoption, it will\nbe important to optimize training pipelines and leverage high-\nperformance computing resources."}, {"title": "E. Future Research Directions", "content": "Building on the findings of this study, several future research\ndirections are proposed:\nData Augmentation: Incorporating additional experimen-\ntal binding affinities, pharmacokinetic parameters, and other\nrelevant biochemical data could strengthen the CB2former's\npredictive power and practical applicability in drug discovery\npipelines. Hybrid Modeling Approaches: Combining different\nML/DL methodologies (e.g., ensemble methods or multimodal\nnetworks) may yield models with even greater robustness for\nCB2 receptor ligand prediction.\nPretraining and Fine-Tuning: Investigating large-scale pre-\ntraining of Transformer architectures on extensive molecular\ndatasets, followed by task-specific fine-tuning, could further\nenhance both accuracy and training efficiency.\nCross-Target Evaluation: Applying the CB2former to other\nreceptor ligands tasks and benchmarking its performance\nacross diverse biological targets will help assess the method's\nbroader utility and generalizability."}, {"title": "V. CONCLUSION", "content": "In this study, we proposed CB2former, an innovative\nframework that integrates a Graph Convolutional Network\n(GCN) with a Transformer architecture and incorporates a\nnovel dynamic Prompt mechanism for CB2 receptor ligand\nactivity prediction. By injecting unlearnable tokens containing\nreceptor-specific knowledge into the self-attention process, the\nmodel not only achieved superior performance evidenced by\nhigher (R2), lower RMSE, and higher AUC compared to\nestablished ML and DL baselines-but also offered enhanced\ninterpretability. Specifically, the Prompt tokens and attention\nweights helped spotlight receptor-critical molecular substruc-\ntures, guiding rational compound design and expediting virtual"}]}