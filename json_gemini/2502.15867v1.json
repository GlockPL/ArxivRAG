{"title": "Strategic priorities for transformative progress in advancing biology with proteomics and artificial intelligence", "authors": ["Yingying Sun", "Jun A", "Zhiwei Liu", "Rui Sun", "Liujia Qian", "Samuel H. Payne", "Wout Bittremieux", "Markus Ralser", "Chen Li", "Yi Chen", "Zhen Dong", "Yasset Perez-Riverol", "Asif Khan", "Chris Sander", "Ruedi Aebersold", "Juan Antonio Vizca\u00edno", "Jonathan R Krieger", "Jianhua Yao", "Han Wen", "Linfeng Zhang", "Yunping Zhu", "Yue Xuan", "Benjamin Boyang Sun", "Liang Qiao", "Henning Hermjakob", "Haixu Tang", "Huanhuan Gao", "Yamin Deng", "Qing Zhong", "Cheng Chang", "Nuno Bandeira", "Ming Li", "Weinan E", "Siqi Sun", "Yuedong Yang", "Gilbert S. Omenn", "Yue Zhang", "Ping Xu", "Yan Fu", "Xiaowen Liu", "Christopher M. Overall", "Yu Wang", "Eric W. Deutsch", "Luonan Chen", "J\u00fcrgen Cox", "Vadim Demichev", "Fuchu He", "Jiaxing Huang", "Huilin Jin", "Chao Liu", "Nan Li", "Zhongzhi Luan", "Jiangning Song", "Kaicheng Yu", "Wanggen Wan", "Tai Wang", "Kang Zhang", "Le Zhang", "Peter A. Bell", "Matthias Mann", "Bing Zhang", "Tiannan Guo"], "abstract": "Artificial intelligence (AI) is transforming scientific research, including proteomics. Advances in mass spectrometry (MS)-based proteomics data quality, diversity, and scale, combined with groundbreaking AI techniques, are unlocking new challenges and opportunities in biological discovery. Here, we highlight key areas where Al is driving innovation, from data analysis to new biological insights. These include developing an AI-friendly ecosystem for proteomics data generation, sharing, and analysis; improving peptide and protein identification and quantification; characterizing protein-protein interactions and protein complexes; advancing spatial and perturbation proteomics; integrating multi-omics data; and ultimately enabling Al-empowered virtual cells.", "sections": [{"title": "Introduction", "content": "Molecular biology is highly complex and dynamic, and our understanding is still rudimentary, making it challenging to predict even a single cell's behavior. Advancing research and developing predictive models are essential for biomedical progress, ultimately enhancing disease diagnosis, treatment, and prevention. Achieving these goals relies on a comprehensive characterization of cellular molecular states, particularly in response to environmental changes and perturbations.\nAs the primary functional molecules of life, proteins orchestrate cellular functions, including cellular structure, signaling, metabolism, and growth regulation\u00b9. They are key players in health and disease, serving as critical drug targets and therapeutic agents\u00b2. Understanding cellular states goes beyond protein identification, as function depends on post-translational modifications (PTMs), subcellular localization, and binding partners, among other factors. Mass spectrometry (MS)-based proteomics is essential for identifying and quantifying proteins while revealing PTMs, single amino acid variants (SAAVs), isoforms, and protein-protein interactions (PPIs). It also uncovers protein complexes and structural changes in complex cellular backgrounds\u00b3.\nWith advances in sensitivity and throughput, emerging fields such as single-cell proteomics, spatial proteomics, and perturbation proteomics are revolutionizing our understanding of biological organization and dynamics.\nProteomics has long used AI to enhance peptide and protein identification, such as predicting chromatographic retention time, ion mobility, and peptide fragmentation. Semi-supervised learning in modern search engines now accurately distinguishes true from false spectral annotations4,5. These advances have expanded the scope of proteomics, enabling the identification of proteinogenic peptides, antibodies, neoantigens, immune peptides, neuropeptides, and natural products. Al has emerged as a key driver of clinical proteomics, aiding the discovery of biomarkers and drug targets 7,8. These breakthroughs are driving transformative progress, yielding valuable biological and clinical insights.\nAlthough AI has transformed proteomics data analysis, its broader application to ambitious biological goals remains challenging. Vast datasets, containing millions of MS raw files as of January 2025, are stored in public repositories like PRIDE and MassIVE10, yet only a small fraction has been used for AI training to date. The primary obstacle is the lack of an AI-friendly ecosystem for proteomics data generation, sharing, and analysis. Challenges arise at the data generation stage due to diverse sample preprocessing, liquid chromatography (LC), and MS methods, resulting in data with varying overall experimental approaches, formats, quality, and batch effects. Furthermore, differences in mass spectrometers, MS techniques, and quantification strategies impede data standardization and normalization. This is particularly evident in human proteome datasets, where small, disease-focused cohorts must be integrated with larger population datasets for AI-driven disease and therapy predictions. Acquiring time-series data and high-quality paired metadata remains a significant challenge. The field urgently needs expanded benchmark datasets for robust model validation. Beyond data challenges, achieving AI's full potential in proteomics necessitates clear research goals, optimized algorithms, enhanced model transparency and interpretability, and stronger interdisciplinary collaboration."}, {"title": "Key area 1: development of an AI-friendly ecosystem for proteomics data", "content": "The foundation of successful AI implementation in proteomics rests on training datasets that are easily accessible, large enough for model generalization, and high- quality to ensure robust performance.\nBuilding an AI-friendly proteomics ecosystem faces fundamental obstacles starting from data generation. MS data quality varies due to variability in sample preparation (e.g., various sample processing methods), LC (e.g., various LC methods, inadvertent contamination), and MS instrument performance (e.g., various MS methods, vendors, environmental fluctuations, contamination accumulates)16,17. These variabilities compound exponentially as the data sets' sizes and sources expand.\nThe integration of standard operating procedures (SOPs), quality control (QC), and AI offers a powerful approach to mitigate data variability and enhance data quality. Harmonized MS platforms and standardized acquisition enhance reproducibility across laboratories 18,19. While universal adoption of standard procedures is unrealistic, they remain essential for AI training datasets. Dedicated facilities like the Clinical Proteomics Tumor Analysis Consortium (CPTAC)20, ProCan21, Biognosys, and possibly \u03c0-HuB13 can operate multiple instruments in a controlled, standardized manner to generate AI-suitable data.\nQuality control represents a critical cornerstone in managing experimental variability. However, a universal QC method is still lacking, and QC measurements are subject to the same variations in sample preparation, LC, and MS performance as experimental samples. Recent work has proposed universal QC using synthetic data tailored to samples and instruments 22, whose implementation would inevitably rely on AI. Machine learning demonstrated success in selecting peptide precursors and predicting LC-MS performance in DIA-based QC workflows 23. To further advance the field, SOP data should be integrated into AI models to improve model objectivity, while AI- driven simulations of experimental methods can optimize SOPs.\nA critical challenge is the lack of a unified method to assess MS raw data quality. Integrating metrics like peak intensity, single to noise ratio (SNR), retention time reproducibility, and chromatographic resolution would enable automated evaluation of large datasets, thereby ensuring consistent filtering of low-quality data for machine learning training."}, {"title": "Data synthesis", "content": "Synthetic data offers an essential complement for AI modeling, particularly for benchmarking, algorithm evaluation, and experimental design optimization24. It enables the generation of sample-specific data tailored to processing methods, separation techniques, and acquisition parameters, without relying on prior analyses. By leveraging machine learning to replace empirical approaches, synthetic data enables refinement of LC-MS/MS acquisition parameters, leading to optimized experimental designs22. In cases, where real training data is insufficient, synthetic data can serve as a crucial resource 25. Recently, simulated datasets have enhanced DIA- MS software through pretrained models, ensuring accurate peak area calculations in training sets26."}, {"title": "Raw MS data format", "content": "MS raw data types are highly diverse due to variations in instruments, MS acquisition methods (e.g., DDA, DIA, SWATH, PASEF), and labeling techniques (e.g., TMT, iTRAQ, SILAC). While standard open data formats like mzML, mzIdentML, mzTab, and mzQC developed by HUPO Proteomics Standards Initiative (HUPO-PSI)27, enable spectral data sharing, but they are designed for single runs, not large-scale aggregation. Unlike text, images or audio, but similarly to other scientific areas, proteomics data requires domain expertise and specialized software to interpret, creating significant barriers for computer scientists. Advancing AI-driven methods, demand new data structures based on formats like TensorFlow 28 and Parquet 29 for large-scale data sharing and processing. This transformation necessitates collaboration between MS vendors and data engineers to adopt unified, user-readable data types for building AI-friendly datasets."}, {"title": "Metadata", "content": "Metadata plays a central role in diverse AI applications within proteomics. These include classification tasks like sample classification (e.g., cancer or non-cancer, tissue types) and protein function prediction, regression tasks such as predicting peptide properties (e.g., retention time), and de novo sequencing tasks that translate spectra into peptide sequences. Proteomics metadata encompasses sample labels (e.g., clinical phenotype, sample types, species) and proteomics-specific labels (e.g., quantification methods, MS acquisition methods, instrument types, and batch design). While the SDRF-Proteomics format was designed to store and link metadata with raw data 30, its adoption in proteomics has been limited so far 31. Additionally, the limited use of laboratory information management systems (LIMS) has hindered proper data structuring before being made publicly available. Furthermore, as with other omics fields, sharing metadata, especially clinical data, presents substantial hurdles due to ethical concerns. Federated learning offers a potential solution, enabling local model training on simulated sites behind firewalls and aggregating them into a global model 32"}, {"title": "Data storage and sharing", "content": "The accessibility of AI-friendly raw data and metadata in through public repositories, such as ProteomeXchange\u00b3\u00b9 forms the backbone of effective data sharing. Creating AI training datasets tailored to specific tasks, as outlined in the following sections, represents another critical requirement. A critical issue with data exchange platforms like ProteomXchange is frequent lack of defining the content and purpose of each file, which limits the usability of public datasets. To address this, we should either encourage repositories to require a table explaining each file's content and use in the publication or establish a community-wide standard. Beyond MS-based proteomics, affinity-based data like antibody-based approaches33 and Olink data34 demand collaborative efforts across disciplines for AI-friendly generation, analysis, storage, and sharing."}, {"title": "Raw data interpretation", "content": "Identifying and quantifying peptides and proteins from raw data underpins most proteomics AI training tasks. While conventional proteomics software suffices for small-scale data interpretation, large-scale datasets demand more sophisticated approaches. To meet this need, community resources and pipelines such as MassIVE.quant\u00b35 and quantms36 now provide annotated and reanalyzed datasets, enabling researchers to access and utilize large, curated collections of data. Future developments should prioritize using these tools to expand AI training datasets.\nAnother major challenge is standardizing quantitative results across diverse quantification methods (e.g., TMT, iTRAQ, SILAC, label-free) and affinity-based data. Despite the availability of several normalization tools, a standardized workflow has yet to emerge. The details of these issues will be elaborated in depth in the second key area."}, {"title": "Benchmarking datasets", "content": "Universal benchmarking datasets play a vital role in evaluating software tool performance. Proteobench\u00b37 offers a comprehensive framework for benchmarking identification and quantification based on DDA-MS and DIA-MS using database search or de novo peptide sequencing. ProteomicsML38 delivers pre-packaged datasets and tutorials for training and validation of ML models in proteomics. Moreover, large-scale synthetic peptide datasets, such as proteomeTools39 and peptide phosphorylated counterpart libraries40, have been used to assess the performance of tools like de novo sequencing, phosphorylation-site localization, and fragmentation methods. The expansion of benchmarking datasets across species, MS instruments, acquisition methods, and diverse tasks like protein complex identification and spatial proteomics would catalyze advancement across the field."}, {"title": "Building an AI-friendly ecosystem for proteomics", "content": "The development of standardized AI-friendly data formats and workflows enables both the creation of new large-scale proteomics datasets and the transformation of existing public repository data into AI-ready resources. Following the successful examples of WordNet\u2074\u00b9 and ImageNet\u00b9\u00b9 in other fields, these curated resources would advance AI in proteomics and promote standardized data generation through mature SOPs, fostering an AI-friendly ecosystem for data generation, sharing, and analysis.\nThe organization of community competitions where AI approaches can be used, like the Critical Assessment of Structure Prediction (CASP) 42, would inspire collaboration between AI and proteomics experts to address key challenges. The exponential growth in dataset size and complexity demands significantly greater computational power for AI model development, highlighting the urgent need for support from computing infrastructure providers."}, {"title": "Key area 2: identification and quantitation of peptides and proteins", "content": "The core task in MS-based proteomics is peptide and protein identification and quantification. Here, we highlight AI-driven advances, current challenges, and future solutions for Al integration, with a focus on large cohort studies, single-cell proteomics, plasma proteomics, and metaproteomics."}, {"title": "Database search in bottom-up proteomics", "content": "In DDA-MS-based bottom-up proteomics, the spectrum-centric database search strategy matches each spectrum to a peptide. Subsequently, peptide-to-spectrum matches (PSMs) are scored and filtered for false positives using a target-decoy approach. Deep learning-based PSM rescoring tools have greatly enhanced peptide identification43. The transformer-based DDA-BERT model performs well across mass spectrometers, single-cell proteomes, and multi-species datasets 44. Recently, DeepSearch pioneered a modified transformer to rank PSMs based on cosine similarity between spectrum and peptide embeddings, offering a novel AI-driven approach for database searching. 45. Additionally, emerging deep learning-based full- spectrum prediction methods promise to enhance identification rates46.\nDIA-MS generates highly convoluted spectra, which can be searched using both spectrum-centric and peptide-centric methods, with tools like DIA-NN47, MaxDIA 48, and Spectronaut49. These tools are at the forefront of the AI revolution in proteomics. DIA-NN47, which uses deep neural networks to assign q-values, has become a standard AI-based tool for DIA-MS analysis, though it is now closed-source. Newer open-source tools like AlphaPeptDeep50, built based on the PyTorch DL library, match or surpass existing tools in predicting peptide properties. Recently, AlphaDIA51, an end-to-end transfer learning tool for feature-free proteomics data analysis, has been developed. The newly developed tool, DIA-BERT26, also leverages pretrained large language models to analyze the DIA data.\nHowever, several critical challenges remain. i) Deep learning-based re-scoring often requires models trained individually for each MS file, increasing the risk of overfitting26. ii) Few AI models have been designed for the identification and quantification of proteoforms (e.g., SAAVs, isoforms, PTMs). iii) Models require enhanced compatibility across species, MS instruments (e.g., Orbitrap, timsTOF, Astral), fragmentation methods (e.g., HCD, CID, ETD, ECD), and peptide quantification labels (e.g., TMT, SILAC, iTRAQ). iv) Many spectra remain uninterpreted52,53, and spectra are often chimeric, requiring algorithms to identify multiple peptide precursors from a single spectrum54. v) Significant efforts are still needed to improve the identification rate, accuracy, generalization, and interpretability of these AI-based tools. vi) The integration of AI-driven intelligent data acquisition enables real-time database searching 55."}, {"title": "De novo sequencing in bottom-up proteomics", "content": "De novo peptide sequencing has advanced the identification of proteoforms (e.g., SAAVs, isoforms, PTMs), small proteins (encoded by sORFs), \u201cunderstudied proteins (USPs)\", and other novel sequences beyond current knowledge. Deep learning tools have enhanced the identification of peptides such as immune peptides, antibodies, neoantigens, and bioactive peptides. Furthermore, deep learning combined with multiple mirror proteases is shown to significantly improve sequence coverage and confidence56.\nHowever, de novo sequencing generally has lower identification rates than database search strategies while accuracy, precision, and recall of peptides still require improvement. As always in deep learning, overfitting or memorizing the dataset of interest (e.g. the human proteome), must be avoided. While algorithms can generalize well to known distributions, their performance drops significantly when generalizing to unknown, out-of-distribution (OOD) data. This is particularly important for de novo sequencing tasks, which rely on algorithms to uncover new knowledge. A promising solution is enabling models to reason. For example, recent advancements in large language models (LLMs) like GPT-01 and DeepSeek-R157 allow chain-of- thought reasoning to tackle unseen challenges, demonstrating better generalization than traditional models like GPT-458 and DeepSeek-v359. To train such models, data on the reasoning process\u2014where humans provide both labels and rationale\u2014can be valuable. Literature also serves as an additional data source, though current LLM and retrieval-augmented generation (RAG) technologies already address this. Additionally current methods prioritize overall peptide or protein identification, while the explanation or annotation of why ions and signals match are often overlooked in the database search algorithm. Post-annotation of PSMs promise to improve the interpretability of large-scale data processing and uncover novel findings in MS experiments or biomedical applications.\""}, {"title": "Top-down proteomics", "content": "Top-down proteomics (TDP) analyzes intact proteins without digestion, enabling the detection of proteoforms (e.g., isoforms, PTMs) even at low abundance and providing direct quantification\u00ba. However, TDP faces hurdles with MS data interpretation due to broader charge distributions and more product ions, while a smaller research community, fewer software tools, and limited datasets constrain AI applications in this field. As TDP attracts continued attention and AI advances, AI-driven approaches may transform the field.\nAl-driven spectrum clustering tools like GLEAMS52 and DLEAMSE61, demonstrate potential for detecting the \"dark proteome\" by annotating unidentified mass spectra. Emerging methods such as COrrelation-based functional ProteoForm (COFP)62 and inference of peptidoforms (IPF)63 are advancing proteoform detection, and AI integration could further improve proteoform and USP characterization."}, {"title": "Al enhanced emerging applications", "content": "Al innovations are transforming emerging proteomics fields\u2014such as studies involving large cohorts, single-cell, plasma, and metaproteomics.\nLarge cohort studies. Large cohort studies grappling with missing values and batch effects. Traditional proteomics software employ methods like match between run (MBR)64 for imputation and Combat65 and HarmonizR66 for batch correction, though their accuracy and effectiveness remain areas for improvement67. Recently, deep learning tools like BERNN68, DeepRTAlign"}, {"content": " , demonstrate advances in batch effect correction. Continued AI developments will further address these challenges.\nSingle-cell proteomics (SCP). SCP offers unprecedented insights into biological heterogeneity but struggles with low MS signal intensity, poor signal-to-noise ratio (SNR), sparse quantification, and low throughput70. AI-driven approaches to enhance low-intensity MS signals and reduce noise are crucial for improving SCP identification and quantification accuracy. Additionally, SCP faces analogous challenges to those found in large-cohort studies, due to the large number of samples.\nPlasma proteomics. Plasma proteomics enables disease screening and biomarker discovery71-73 but faces challenges like high dynamic range, reproducibility issues, batch effects, and data variability. AI can greatly enhance data acquisition and analysis, propelling plasma proteomics beyond traditional approaches like low- abundance protein enrichment, or high-abundance protein depletion."}, {"title": "Metaproteomics", "content": "Metaproteomics confronts high PSM mismatch rates, protein inference challenges, and especially large and not fully characterized search spaces74. AI-driven solutions like de novo sequencing 75,76, deep learning-based PSM re-scoring, and homolog recognition"}, {"title": "Key area 3: identification and quantification of protein complexes, protein- protein interactions, and other molecular interactions", "content": "Most proteins function within complexes, and those involved in the same cellular processes frequently interact78. PPIs and protein complexes are highly dynamic and context-specific3,79-81. Identifying and quantifying these interactions and their perturbations across conditions fundamentally advances our understanding of biological processes and their roles in disease. Beyond protein-protein interactions, these molecular networks encompass interactions with small molecules like drugs, nucleic acids, and lipids.\nThe convergence of MS-based experiments and AI have transformed PPI and protein complex identification and quantification. Affinity purification MS serves as a cornerstone method82 for PPI identification, and has recently been pushed to very high throughput with unprecedented data quality83. Its integration with deep learning has enhanced resolution of prefoldin holo- and subcomplex variants, complex-complex interactions, and complex isoforms84. Co-fractionation MS (CF-MS), including size exclusion chromatography-MS and blue native electrophoresis, is widely used for protein complex identification85,86. AI-based approaches can further refine protein complex identification and quantify abundance and composition changes across conditions 84,87-89. Cross-linking MS (XL-MS) can be used to identify protein-protein and protein-RNA interfaces by combining cross-linking reactions, revealing residue proximity and complex topology 90. AlphaLink 91 integrates XL-MS data with AlphaFold2's pair representation, significantly improving PPI predictions92. Its latest version, AlphaLink293, incorporates experimental constraints into AlphaFold- Multimer, extending predictions to protein complexes94. Beyond MS-based methods, structural biology techniques complement PPI and protein complex studies. AlphaFold 3 leverages deep learning to predict the joint structures of complexes, including proteins, nucleic acids, small molecules, ions, and modified residues95.\nDespite the growing accumulation of data from the above technologies, integrating diverse datasets to build large-scale, high-quality, AI-friendly protein complex databases remains a major challenge."}, {"title": "Key area 4: spatial proteomics", "content": "Spatial proteomics, which maps protein localization and dynamics within cells and tissues, fundamentally advances our understanding of protein functions, interactions, cellular processes, and disease mechanisms. It consists of two layers: one focusing on different tissue regions and the other on cellular compartments or subcellular localizations.\nRecently, deep visual proteomics (DVP) emerged to enable single-cell analysis of tissue samples. It revolutionizes spatial proteomics by identifying cell-type-specific markers in stained cells and has already led to life-saving treatments99. Another method, filter-aided expansion proteomics (FAXP)100, demonstrates strong reproducibility and identification depth in clinical formalin-fixed, paraffin-embedded (FFPE) samples. Furthermore, spatial proteomics technology based on immunofluorescence (IF) microscopy has generated over 80,000 confocal images, mapping substantial proteins across 30 cellular compartments and substructures11.\nDeep learning has revolutionized the scalable annotation of single-cell protein distributions in these images, opening new horizons for machine learning to identify cellular phenotypes, isolating nuclei, mapping protein subcellular localizations, and advancing spatial proteomics100. A recent study101 leveraged machine learning to integrate immunofluorescence images from the Human Protein Atlas102 with affinity purification data from BioPlex103 to create a unified hierarchical map of human cell architecture, known as the multi-scale integrated cell (MuSIC 1.0). This map resolves 69 subcellular systems, paving the way for incorporating diverse types of data into proteome-wide cell maps. Furthermore, combining spatial proteomics-based tissue expansion with super-resolution and electron microscopy, this approach utilizes fluorescent labeling to achieve nanometer-scale resolution, enabling precise analysis of protein localization and interactions, while providing comprehensive subcellular- level insights into the spatial organization and functional networks of proteins.\nHowever, integrating proteomics data with imaging remains a challenge."}, {"title": "Key area 5: perturbation proteomics", "content": "Proteomics stands apart in its ability to capture dynamic cellular responses, offering significant advantages over static genomics and partially dynamic transcriptomics. Perturbation proteomics systematically investigates protein-level changes in response to various stimuli, providing critical insights into cellular systems and drug responses. This approach encompasses the comprehensive analysis of protein targets, interactions, PTMs, turnover, localization, and enzyme activities, alongside proteome quantification before and after perturbation110.\nBudding yeast is the first species where proteomes111, transcriptomes112, metabolomes113, ionomes114, as well as genetic interactions 115 became available for an extensively phenotypic library of all non-essential gene-knockouts116. Perturbation 'omics' has closed gaps in gene functional annotation, revealed general principles of the network responses, for instance in protein dynamics, and enabled the reconstruction of biological networks of unprecedented depth. The current frontier lies in leveraging this data through AI approaches to predict phenotypic responses, in the conditions in which they have been recorded, and across conditions and in environments not tested a priori.\nRecent studies highlight an equally high potential for perturbation proteomics in human disease biology and drug discovery. Large-scale initiatives have profiled perturbation proteomes of numerous compounds, covering thousands of proteins and PTMs to elucidate for mechanisms of action and enable compound repurposing117-120 These studies have incorporated time and dose-resolved proteomics for a more nuanced, functional context to the observed changes117-121 Despite these advances, several challenges remain:"}, {"title": "Key area 6: multi-omics data integration", "content": "Advancements in high-throughput sequencing have catalyzed multi-omics (e.g., genomics, transcriptomics, proteomics, and metabolomics) integration within the central dogma, enhancing disease research and precision medicine. Over the past two decades, databases like CPTAC, the UK Biobank, TCGA, ICGC, GEO, and LinkedOmics have accumulated multi-omics, phenotypic, and imaging data. These expanding population-scale biomedical datasets present unprecedented opportunities for AI to uncover complex biological links to health and disease.\nLandmark studies demonstrate the power of this approach to uncover diagnostic biomarkers, potential drug targets, and risk stratification for various diseases. The MILTON machine learning model harnesses UK Biobank multi-omics and phenotype data to predict over 3,200 diseases123. Similarly, medical AI models trained on multimodal datasets successfully interpret diverse data types, including imaging, electronic health records, and genomics124. Recently, the Prov-GigaPath foundation model, trained on 1.3 billion pathology image tiles from a US health network spanning 28 cancer centers, improved cancer subtyping and mutation prediction, demonstrating Al's potential in clinical decision-making125, 126 However, significant challenges persist in integrating multi-omics data."}, {"title": "Key area 7: proteomics support to construct the AI virtual cell (AIVC)", "content": "The concept of an AI virtual cell (AIVC) represents a computational model that simulates molecular, cellular, and tissue behavior, promising to accelerate discoveries and guide experiments130. Realizing this vision requires the seamless integration of multi-omics data (genome, epigenome, transcriptome, proteome, metabolome) alongside spatial, temporal, and imaging data131.\nTo build AIVC, in addition to integrating the aforementioned proteomics and multi- omics data, we also need to annotate and predict protein functions. Despite numerous AI-based protein function annotation tools132, predicting protein function remains challenging due to limited data and complex functional interactions133. A greater challenge is extracting functionally relevant insights from large datasets of proteins, PTMs, and their abundance changes across conditions. For instance, studies on the functional relevance of PTMs are essential. An ML model has been developed to predict functional scores for human phosphosites, using 59 features from large-scale data and databases like UniProt, helping prioritize sites for further study134. Similar approaches could be applied to prioritize other PTMs like acetylation, methylation, SUMOylation, and ubiquitination, although the lack of functional annotations for these PTMs limits model training. One solution is to leverage large language models to extract functional annotations for PTM sites from the literature135. Additionally, proteome-wide predicted protein structures offer new opportunities for genome-wide function prediction and have been successfully used to advance protein function understanding 136,137. Interrogating PTMs in their structural context could further enhance our understanding of their functional relevance138.\nWithin this framework, proteomics contributes multiple essential dimensions to our understanding of cellular biology: protein function prediction clarifies molecular roles, interaction data defines cellular networks, spatial proteomics ensures precise subcellular localization, and perturbation proteomics reveals dynamic responses. The challenges in each proteomics aspect and how AI can address them have been discussed in the previous sections. Successfully addressing these challenges will catalyze the development of AIVC.\nThe realization of a comprehensive digital cell requires large-scale proteomics and multi-modal data to capture the dynamics of molecular machinery and their networks, heralding a revolution in biological insights131. Progress in this field has followed a stepwise approach, beginning with whole-cell models for simple lower model organisms, such as the pathogen Mycobacterium genitalium139 and the bacterium Escherichia coli140. For these simpler organisms present ideal initial targets: their fewer genes enable comprehensive analysis through existing sequencing technologies while traditional statistical algorithms may be sufficient to establish relatively reliable models. Current computational approaches span multiple frameworks, including ordinary differential equations (ODE)141, Boolean network modeling142, stochastic simulations 143, agent-based modeling144, and constraint-based modeling 145.\nThe complexities of human cells, however, demand more sophisticated solutions. State of the art sequencing technologies and AI algorithms are essential for accurate modeling the behaviors of human cells. Addressing this challenge, the Human Cell Atlas (HCA) consortium was founded in 2016 to characterize all cell types in the human body146. This initiative has already collected single cell multi-omics data from 62 million cells, organized within 18 biological networks.\nWhile most of the data derives from single cell RNA sequencing, its analysis leverages sophisticated neural network architectures, including transformer, convolutional neural network (CNN), Graph neural network (GNN), and diffusion model. This data can be used for building foundation models to extrapolate cell types and interaction networks147, as exemplified by GeneFormer148, and scFoundation149.\nBiological systems exist and function as communicating networks of as cells in dynamic environments, shaped by inter- and intra-cellular communication. Consequently, employing diverse perturbations to mimic various environmental effects on cells can enable deeper understanding of these intricate systems. Critical to this endeavor is the fusion of super-resolution imaging data with perturbation omics data which will enable a holistic view of cell dynamics for AI modeling. Tackling these challenges through interdisciplinary collaborations and advanced techniques will pave the way for future breakthroughs in predicting cellular behaviors, elucidating biological mechanisms, enabling personalized medicine, accelerating drug discovery, and pioneering new approaches in cellular engineering."}, {"title": "Conclusion", "content": "We identify seven key areas in AI-driven proteomics poised for breakthroughs within 3-5 years. At the foundation lies the development of an AI-friendly ecosystem for proteomics data, supporting six critical application areas: proteome identification and quantification (e.g., canonical proteins, proteoforms, understudied proteins), protein complexes, PPIs, spatial proteomics, perturbation proteomics, and multi-omics integration.\nProgress in these domains will collectively drive the AI virtual cell and achievement that demands global collaboration between AI and proteomics researchers, Initiatives like the \u03c0-Hub project 13 will serve as valuable catalyst in this scientific revolution."}]}