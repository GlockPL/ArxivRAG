{"title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "abstract": "Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a \u201cthinking\u201d mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [1, 2, 3] have established a new approach to problem-solving and task execution through natural conversations. Speech, being a fundamental form of human communication, acts as an intuitive and effective means for interactions between humans and LLMs. As a result, there is a growing research emphasis on enhancing the spoken input and output capabilities of LLMs. Some recent studies have focused on equipping LLMs with a comprehensive understanding of speech and audio [4, 5, 6], while other research has explored utilizing LLMs' advanced language understanding abilities to develop more sophisticated speech generation and processing methods [7, 8].\nAlthough half-duplex or turn-based speech LLMs [9, 10, 11, 12] can function as conversational AI systems, human conversations are inherently full-duplex, characterized by simultaneous listening, thinking, and speaking. This dynamic is reflected in natural conversational behaviours such as frequent turn-taking, backchanneling, overlapping speech, and barge-in. These interactive and flexible patterns have sparked increasing interest in developing full-duplex speech LLMs to enhance the fluidity and"}, {"title": "2 Methodology", "content": "Four challenges need to be solved when implementing a full duplex conversational AI model without using codecs.\n\u2022 The model needs to support streaming speech input and output. By integrating an LLM with a streaming speech encoder and a streaming speech synthesizer using embeddings instead of text, SALMONN-omni functions as an end-to-end model, enabling seamless interaction with users through both verbal and non-verbal (paralinguistic) features.\n\u2022 The model should be able to deal with two streams from both the input and the output sides simultaneously. SALMONN-omni achieves it by utilizing the speech encoder to process the input stream, the LLM to process the output stream and connecting these two components with cross-attention layers.\n\u2022 The model should have the idea about \"time\" so that the auditory and textual modalities can be aligned and synchronized. A periodic synchronization mechanism is introduced for SALMONN-omni. Within each time block, the model processes a fixed duration of input speech and generates a fixed number of textual embeddings.\n\u2022 The model should be able to handle complex dynamics in natural conversations such as turn-taking, barge-in, overlapping speech and backchanneling. A novel \u201cthinking\u201d mechanism is proposed so that SALMONN-omni can switch between speaking and non-speaking states flexibly while listening to the input side at all times.\nSpecifically, SALMONN-omni has speaking and non-speaking states and two special tokens <start_speak> and <end_speak> are utilized for transition between these two states. After the LLM generating <start_speak>, SALMONN-omni switches to the speaking state. When the model completes its answer or is interrupted by the input stream, the LLM generates <end_speak> and SALMONN-omni transists to the non-speaking state. As shown in Figure 2, the conversation is cut into a series of time blocks. In block i, the streaming speech encoder extracts auditory embeddings from input speech with a fixed duration of At seconds. Then the LLM generates n word embeddings conditioned on auditory embeddings extracted from block 0 to block i. If the model is in a speaking state, the word embeddings are sent to the streaming speech synthesizer to generate a spoken response with the same duration of At seconds. There is no explicit text between each of the two components, and the whole model is trained in an end-to-end manner."}, {"title": null, "content": "To maintain the framework's consistency and simplicity, and to help the LLM determine when to perform state transitions, the LLM is required to decode n tokens within each time block. However, two situations arise where collecting ground truth labels during training becomes challenging: 1) The first occurs in the non-speaking state, where the tokens preceding the <start_speak> marker are difficult to determine. 2) The second arises in the speaking state, when the LLM has completed generating textual embeddings, but the speech synthesizer is still generating and playing audio of the answer. In this case, the LLM must generate tokens to remain informed by the input stream. A straightforward solution might involve introducing a special placeholder token, but the frequent repetition of the same token in the training data risks collapsing the model's output distribution, leading to a significant performance drop. To address this, the \u201cthinking\u201d strategy introduces the special <think> token, which is only used as input in these situations. However, the outputs are not explicitly forced to include this or any other specific tokens. The only constraint is that the outputs must not include <start_speak> or <end_speak>. This design mirrors the human thought process during conversations, where internal thoughts may differ from spoken words and are not explicitly conveyed. Moreover, the mechanism is easy to implement by setting the output labels to either <start_speak> or <end_speak>, depending on the state, and applying a negative coefficient Athink to the loss function. Specifically, if SALMONN-omni is in a non-speaking state, the label is <start_speak>; otherwise, it is <end_speak>.\nFinally, SALMONN-omni is trained with loss function expressed by Eqn. (1),\n$L = A_{text}L_{text} + A_{speech}L_{speech} + A_{think}L_{think}$,\nwhere $A_{text}$, $A_{speech}$ and $L_{text}$, $L_{speech}$ are the weights and losses for text generated by the LLM and the final speech response. $L_{think}$ is the loss for \"thinking\u201d tokens and $A_{think} < 0$."}, {"title": "3 Case Study", "content": "SALMONN-omni is trained with multiple tasks such as streaming speech recognition, speech enhancement, spoken question answering and etc. Moreover, synthetic data is used for training SALMONN-omni to learn to handle turn-taking and barge-in in natural conversations. The data used for speech recognition include 60k hours of LibriHeavy [29] and 10k hours of GigaSpeech [30], which are also the sources of synthetic data for speech enhancement, turn-taking and barge-in.\nBelow are some cases for demonstrating the capabilities of SALMONN-omni on various streaming speech tasks and natural conversations."}, {"title": "4 Conclusion", "content": "This paper presents SALMONN-omni, a multimodal LLM developed within a novel codec-free, full-duplex framework for simultaneous speech understanding and generation. By integrating a streaming speech encoder, an LLM, and a streaming speech synthesizer into an end-to-end model, and incorporating a novel \u201cthinking\u201d strategy, SALMONN-omni can effectively unify a wide range of streaming speech tasks, including speech recognition, enhancement, dereverberation, target speaker extraction, and spoken question answering. Simulated experiments on turn-taking and context-dependent barge-in demonstrate SALMONN-omni's potential as a prototype of a future conversational AI that enables the seamless modelling of diverse interactive natural spoken dialogue dynamics using a single end-to-end model."}]}