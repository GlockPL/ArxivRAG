{"title": "A Minimax Approach to Ad Hoc Teamwork", "authors": ["Victor Villin", "Thomas Kleine Buening", "Christos Dimitrakakis"], "abstract": "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes policies against an adversarial prior over partners, explicitly accounting for uncertainty about partners at time of deployment. Unlike existing methods that assume a specific distribution over partners, our approach improves worst-case performance guarantees. Extensive experiments, including evaluations on coordinated cooking tasks from the Melting Pot suite, show our method's superior robustness compared to self-play, fictitious play, and best response learning. Our work highlights the importance of selecting an appropriate training distribution over teammates to achieve robustness in AHT.", "sections": [{"title": "1 INTRODUCTION", "content": "Domain generalisation is often crucial in Reinforcement Learning (RL) and is typically assessed by placing an agent in novel environments [13]. Likewise, in Multi-Agent Reinforcement Learning (MARL), generalisation to new agents can be evaluated by pairing a trained policy with unseen actors [1, 5, 21, 26]. While zero-shot domain adaptation is a valuable property [20, 40], it is equally important to ensure proper transfer to new behaviours in multi-agent settings, especially in situations where undesired interactions may arise [17]. More specifically, Ad Hoc Teamwork (AHT) occurs when an agent, initially unfamiliar with its teammates, must collaborate to achieve a common goal. In a world where autonomous agents are being progressively introduced in such tasks, cooperation with humans is becoming a major concern [23, 43].\nEfforts in AHT have primarily focused on learning and inferring behavioural models or teammates types [2, 3, 5, 12, 33], adapting to behaviour shifts [37], and enhancing generalisation by encouraging diversity in partners during training [11, 21, 22, 30, 44]. However, these methods provide limited guarantees on the worst-case AHT performance.\nA multi-agent system can encompass numerous and diverse scenarios, each characterised by its actors. For example, autonomous cars operate alongside human drivers and other autonomous vehicles. Similarly, in a surgical setting, a robot may need to cooperate with surgeons who have a wide range of different habits and expertise. In each of these scenarios, we can adopt the perspective that the focal actors are controlled by an automated agent, whereas the other actors are viewed as fixed, forming the background of the task [1, 26]. These scenarios can be viewed as distinct environments, as each combination of background actors induces different transition dynamics and reward functions. A common practice involves constructing representative scenarios and training a policy on a uniform distribution over them [30, 44]. However, this only optimises performance for that specific distribution.\nRecent studies in zero-shot domain transfer showed that selecting an appropriate prior over training environments is key to learning robust policies [8, 14, 16, 24, 27, 36]. Intuitively, this insight should apply to the AHT setting as well, suggesting that choosing a specific prior over scenarios/partners may improve the robustness of learned policies. Assuming that no information is available about the teammates at test time (and their distribution), we consider the worst possible prior over the training set of partners given our policy, an idea adopted from the minimax-Bayes concept [6].\nContributions. We make the following contributions:\n(1) We adapt Minimax-Bayes Reinforcement Learning (MBRL) [8] to the AHT setting, reasoning about uncertainty with respect to partners rather than environments (Section 4).\n(2) We examine the advantages of using utility and regret for AHT robustness, and provide solutions to target either metric (Section 5).\n(3) We study out-of-distribution robustness guarantees (Section 6).\n(4) We propose a Gradient Descent-Ascent (GDA) [29] based algorithm, in conjunction with policy-gradient methods, and discuss its convergence for softmax policies (Section 7).\n(5) We conduct extensive experiments to evaluate our approach. We deploy learned policies on both seen and unseen scenarios for cooperative problems, including a partially observable cooking task from the Melting Pot suite [1, 26]. We compare our approach against Self-Play (SP), Fictitious Play (FP) [7, 19] as well as learning a policy w.r.t. a fixed uniform distribution over scenarios [30], which is related to fictitious co-play [44], as both learn the best response to a population of policies (Section 8).\n(6) Our results confirm the theory and empirically demonstrate that our approach leads to the most robust solutions for both simple and deep RL coordination tasks, even when teammates are adaptive."}, {"title": "2 RELATED WORK", "content": "Ad Hoc Teamwork. In AHT, we are interested in developing agents capable of cooperating with other unfamiliar agents without any form of prior coordination [3, 5, 39, 43]. Popular approaches usually involve some form of Population Play (PP), where policies forming a population are learning by interacting with each other [1, 26, 30, 32]. Key strategies for ensuring generalisation to new partners include promoting policy diversity within the training population [11] and preventing overfitting to training partners [25]. Both Lupu et al. [30] and Strouse et al. [44] previously showed that learning a best response to a more diverse population leads to improved generalisation. Additionally, Jaderberg et al. [22] showed the effectiveness of PP when diversity is encouraged through evolving pseudo-rewards. However, PP still struggles with producing policies that are robustly collaborative with new partners and sometimes exhibits overfitting [1, 10, 26].\nTo further improve AHT, several works suggest inferring the teammates' models/types, maintaining a belief about ad hoc partners based on previous interactions within an episode [2, 5]. This was shown to help substantially in the partially observable setting [15, 18, 38]. Efforts have also been made to improve the learning and generalisation of such models to new partners [3, 4, 33].\nAn alternative approach proposed by Li et al. [28] involves a robust formulation of deep deterministic policy gradients, assuming worst-case teammates. Unlike our setup, they train a joint policy that remains consistent throughout learning, and design their algorithm specifically for deep deterministic policy gradients, whereas our approach is compatible with any policy-gradient algorithm.\nEven though the aforementioned methods attempt to improve cooperative robustness, they always assume specific distributions for the partners. For example, Jaderberg et al. [22] used a distribution favoring the matchmaking of policies of similar levels with the intuition that the reward signal is stronger in those cases. However, it does not provide any insights on its effects on AHT robustness. As a result, the actual impact of the training partner distribution on robustness is left under-explored. This holds significant potential, as it can be exploited in conjunction with previously studied mechanisms to substantially enhance AHT robustness.\nZero-shot Domain Transfer. AHT can be seen as a form of zero-shot domain transfer. Each possible team composition involving the focal agent can be considered a different environment. In the single-agent setting, Jiang et al. [24] demonstrated that adapting the training environment distribution by prioritising environments with higher prediction loss (a measure of the policy's lack of knowledge) leads to improved sample efficiency and generalisation. Building on this idea, Garcin et al. [16] prioritised environments where the mutual information between the learning policy's internal representation and the environment identity was lower, using information theory to achieve similar results. The idea of tampering with the environment distribution was also explored by Pinto et al. [36], who employed a maximin utility formulation to choose continuous adversarial environment perturbations throughout learning. Instead of utility, Dennis et al. [14] stressed the advantages of using regret by proposing a training environment sampling scheme avoiding entirely unsolvable and uninformative environments. Most relevant to this work, Buening et al. [8] conducted a study over worst-case priors (for both utility and regret) over training environments, and proved that worst-case distributions are well-suited for domain transfer. Li et al. [27] later reaffirmed those results, learning worst-case distributions within ambiguity sets of subjective priors. Finally, there exist works on domain transfer in the MARL setting [40], but this differs from our focus on transferring to new partners. This related work is consistently in favor of caring about environment distributions for robustness, providing strong motivation to bring this concern to AHT."}, {"title": "3 PROBLEM FORMULATION", "content": "3.1 Preliminaries\nAn m-player Partially Observable Markov Game (POMG) is given by a tuple $\\mu = (S, X, A, O, P, \\rho, \\gamma, T)$ defined on finite sets of states $S$, observations $X$ and actions $A$. The observation function $O : S \\times \\{1, ..., m\\} \\rightarrow X$ provides a state space view for each player. In each state, each player $i$ chooses an action $a_i \\in A$. Following their joint action $a = (a_1, ..., a_m) \\in A^m$, the state is updated according to the transition function $P : S \\times A^m \\rightarrow \\triangle(S)$. After a transition, each player receives a reward defined by $\\rho : S \\times A^m \\times \\{1, ..., m\\} \\rightarrow \\mathbb{R}$. The game ends after $T$ transitions. Permuting player indices does not have any effect on $\\mu$. We denote $|\\rho|{\\infty}$ the maximum absolute step reward.\nA policy $\\pi : X \\times A \\times X \\times A \\times \\cdots \\times X \\rightarrow \\triangle(A)$ is a probability distribution over a single agent's actions, conditioned on that agent's history of observations and actions. We denote $\\Pi$ the set of all policies and $\\Pi_D \\subset \\Pi$ the set of deterministic policies.\n3.2 Scenarios\nLet a scenario $\\sigma = (c, \\pi^\\flat)$ be defined by its number of focal players $c$, and its background players $\\pi^\\flat = (\\pi_1, ..., \\pi_{m-c}) \\in \\Pi^{m-c}$. We say we deploy a policy $\\pi^f$ in scenario $\\sigma$ if the $c$ focal players are equal to $\\pi^f$. Hence, in addition to the $m-c$ many background policies $\\pi^\\flat$, there are $c$ many focal policies $\\pi^f = (\\pi_1^f, ..., \\pi_c^f)$. We denote $a^f \\in A^c$ and $a^\\flat \\in A^{m-c}$ the joint actions of the focal and background players, respectively. A background population $B \\subset \\Pi$ is a finite set of policies, to which we assign a set of scenarios:\n$\\Sigma(B) := \\{(c, \\pi^\\flat) | 1 \\le c \\le m, \\pi^\\flat \\in B^{m-c}\\}$ \\label{eq:scenarios} $\\qquad$(1)\nA scenario $\\sigma$ on $\\mu$ can be viewed as its own $c$-player POMG, through the marginalisation of the policies of its background players. We denote $\\mu(\\sigma) = (S, X, A, O_\\sigma, P_\\sigma, \\rho_\\sigma, \\gamma, T)$ the POMG induced by scenario $\\sigma$, where $O_\\sigma : S \\times \\{1, . . ., c\\} \\rightarrow X$ is the corresponding observation function, $P_\\sigma : S \\times A^c \\rightarrow \\triangle(S)$ is the transition function given by\n$P_\\sigma(s' | s, a^f) = \\begin{cases} P(s' | s, a^f), & c = m \\\\ \\sum_{a^\\flat} (P(s' | s, a^f, a^\\flat) \\prod_{i=1}^{m-c} \\pi_i(a^\\flat_i|h_i), & c < m\\end{cases}$"}, {"title": "3.3 Evaluation", "content": "The expected utility of a policy $\\pi$ in scenario $\\sigma$ is the mean return of the focal policies given by the expected focal-per-capita return [1, 26]:\n$U(\\pi, \\sigma) := \\frac{1}{c} \\sum_{i=1}^c \\mathbb{E}[\\sum_{t=1}^T \\gamma^{t-1} \\rho_{\\sigma}(s_t, a_t^f, i)]$ $\\qquad$(2)\n$U^\\ast(\\sigma) := \\max_{\\pi \\in \\Pi} U(\\pi, \\sigma)$ denotes the maximal utility achievable in scenario $\\sigma$. This definition for utility represents the need for autonomous agents to always maximise the mean joint rewards of its copies, regardless of the scenario. We can further define the notion of regret incurred by deploying some policy $\\pi$ on scenario $\\sigma$, as the gap between the maximal utility and the utility of $\\pi$ on $\\sigma$:\n$R(\\pi, \\sigma) := U^\\ast(\\sigma) - U(\\pi, \\sigma)$. $\\qquad$(3)\nTo assess a learning method in terms of AHT, we use the evaluation protocol of Leibo et al. [26]. This has two phases:\n(1) Training phase: A background population $B_{\\text{test}}$ is kept hidden. The policy learner has access to the game $\\mu$ with no restrictions, apart from accessing $B_{\\text{test}}$. For example, the learner is free to use a modified instance $\\mu'$ of $\\mu$, where the observation function, $O$, may be adjusted to include information about other players, or where the reward function, $\\rho$, could be altered to provide joint rewards instead of individual ones.\n(2) Testing phase: The obtained policy is fixed and cannot be trained any further. We compute the performance of the policy on $\\mu$ by taking its average expected utility across a series of unseen test scenarios $\\Sigma_{\\text{test}} \\subset \\Sigma(B_{\\text{test}})$:\n$U_{\\text{avg}}(\\pi, \\Sigma) := \\frac{1}{|\\Sigma|} \\sum_{\\sigma \\in \\Sigma} U(\\pi, \\sigma)$, $\\qquad$(4)\nIn addition, we consider two metrics related to robustness, namely worst-case utility and worst-case regret:\n$U_{\\text{min}}(\\pi, \\Sigma) := \\min_{\\sigma \\in \\Sigma} U(\\pi, \\sigma), \\quad R_{\\text{max}}(\\pi, \\Sigma) := \\max_{\\sigma \\in \\Sigma} R(\\pi, \\sigma)$. $\\qquad$(5)\nMaximising $U_{\\text{min}}$ is typically preferable when falling below a certain utility threshold must be avoided at all costs; for instance minimising casualties in a surgical context. Conversely, minimising $R_{\\text{max}}$ avoids decisions that lead to significantly worse utility than the optimal utility.\nThe final objective is to design a learning process outputting a policy that reliably maximises utility or minimises regret on possibly unseen scenarios."}, {"title": "3.4 General Assumptions", "content": "To ensure our setting aligns with the AHT literature, we must adhere to three assumptions [31]: a) the absence of prior coordination. The learner must be capable of cooperating with the team on-the-fly, without relying on previously established collaboration strategies. b) There is no control over teammates, the learner can control its own copies but not other agents in the configuration. c) All agents (focal and background) are assumed to have a partially shared objective. Their reward function may be slightly different, reflecting varying preferences. In this work, we choose to address this last point by assuming a class of possible reward functions for the background players."}, {"title": "4 ACHIEVING ROBUST AHT", "content": "To learn a policy able to cooperate with new partners, a straightforward idea is to reconstruct scenarios that would be encountered in nature. A roadblock to this approach however is that it requires two main ingredients: a) a diverse pool of partners, and b) a prior distribution over them. The prior, often neglected, is important as it captures our uncertainty about the true partners observed in nature.\nIn Section 4.1, we reflect on motivating previous work on diverse behaviour generation, before describing our own adopted approach. Section 4.2 then introduces the Minimax-Bayes idea to AHT, by stating the connections of our setting to Minimax-Bayes Reinforcement Learning (MBRL)."}, {"title": "4.1 Constructing Training Scenarios", "content": "Before learning any robust policy, we need to construct a diverse set of scenarios. A background population that encompasses a wide range of behaviours is needed in order to reconstruct scenarios existing in nature. Previous work on AHT tackled the issue in various manners, such as using genetic algorithms [33], rule-based policies generated with MAP Elites [9], SP policies [44], explicit behavior diversification through regularisation [30], or through evolved pseudo-rewards [22]. Based on real-life examples and aiming to thoroughly assess the effects of partner priors, we adopt the following approach:\n\u2022 We assume a class of reward functions for background policies:\n$\\rho_{\\text{social+risk}}(s, a, i) = \\rho_{\\text{social}}^+(s, a, i) - \\delta_i \\rho_{\\text{social}}^-(s, a, i)$,\nwith $\\rho_{\\text{social}}$ defined as\n$\\rho_{\\text{social}}(s, a, i) = \\lambda_i \\rho_i^+(s, a, i) + (1 - \\lambda_i) \\sum_{j=1}^m \\rho^+(s, a, j)$,\nwhere $\\rho^+$ and $\\rho^-$ are the positive and negative parts of $\\rho$, and $\\lambda_i$ and $\\delta_i$ denoting levels of prosociality [35] and risk-aversion, respectively. In other words, each background policy has their own preferences $(\\lambda_k, \\delta_k)$.\n\u2022 Policies are organised into sub-populations $B = \\cup_k B_k$ of varying sizes.\n\u2022 Each sub-populations are separately trained using PP.\nGiven the diverse preferences and varying sizes of the sub-populations, distinct habits and established conventions are more likely to emerge from each group [44]. This choice for constructing scenarios ensures a diverse generation of scenarios, important to ablate the effects of various scenario priors on AHT robustness. Note that this choice for constructing scenarios remains arbitrary and is not the main focus of our work."}, {"title": "4.2 Minimax-Bayes AHT", "content": "In the standard single-agent Bayesian RL setting, the learner selects a subjective belief $\\beta$ over candidate Markov Decision Processes (MDPs) $M$ for the unknown, true environment $\\mu^* \\in M$. The learner's objective is to maximise its expected expected utility with respect to the chosen prior $U(\\pi, \\beta) = \\int_M U(\\pi, \\mu) d\\beta(\\mu)$, i.e. finding the Bayes-optimal policy. In MBRL, Buening et al. [8] proposed considering the worst possible prior for the agent, without knowledge of the policy that will be chosen. This approach can be interpreted as nature playing the minimising player against the policy learner in a simultaneous-move zero-sum normal-form game. Learning against a worst-case prior intuitively makes the policy more robust, as it prepares for the worst outcomes.\nTo transfer this idea to our setting, we remark that any finite background population $B$ provides a finite set of POMGs $M_B = \\{\\mu(\\sigma) | \\sigma \\in \\Sigma(B)\\}$. The principal difference here is the use of POMGs rather than MDPs. We extend the notion of expected utility with respect to a prior over scenarios, i.e. when $\\beta \\in \\triangle(\\Sigma(B))$:\n$U(\\pi, \\beta) := \\mathbb{E}_{\\sigma \\sim \\beta}[U(\\pi, \\sigma)] = \\sum_{\\sigma} U(\\pi, \\sigma) \\beta(\\sigma)$. $\\qquad$(6)\nThis allows us to formulate the following maximin game:\n$\\max_{\\pi \\in \\Pi} \\min_{\\beta \\in \\triangle(\\Sigma(B))} U(\\pi, \\beta)$. $\\qquad$(7)\nSimilarly to Buening et al. [8], we are interested in knowing whether such a game has a solution (i.e., a value), assuming that nature and the agent play simultaneously without knowledge of each other's move. This is relevant in our setting because the policy learner does not know the true distribution of partners available in nature, while nature's distribution does not depend on the policy that will be picked. Fortunately, (7) has a value when B is finite.\nCorollary 4.1 (Buening et al. [8]). For an m-player POMG \\mu in a finite state-action space, with a known reward function and a finite horizon, and a background population B, the maximin game (7) has a value:\n$\\max_{\\pi \\in \\Pi} \\min_{\\beta \\in \\triangle(\\Sigma(B))} U(\\pi, \\beta) = \\min_{\\beta \\in \\triangle(\\Sigma(B))} \\max_{\\pi \\in \\Pi} U(\\pi, \\beta)$. $\\qquad$(8)\nProof. First, observe that for any stochastic policy $\\pi \\in \\Pi$, there exists a distribution over deterministic policies $\\phi \\in \\triangle(\\Pi_D)$ such that $\\pi(a_t|h_t) = \\sum_{d \\in \\Pi_D} d(a_t|h_t) \\phi(d)$. Consequently, we can rewrite the utility as $U(\\pi,\\beta) = \\sum_{d \\in \\Pi_D} \\sum_{\\sigma \\in \\Sigma(B)} U(d, \\sigma) \\phi(d) \\beta(\\sigma)$. This demonstrates that $U$ is bilinear in $\\phi$ and $\\beta$, which allows us to apply the minimax theorem, thus proving the result.$\\qquad$$\\square$\nImportantly, prior work that chooses arbitrarily a fixed prior is limited in terms of robustness guarantees: it only ensures maximal utility for their specific prior. In contrast, a policy $\\pi^\\ast_l$ solving the maximin utility problem (7) has its utility lower-bounded on $\\Sigma(B)$:\n$\\forall \\beta \\in \\triangle(\\Sigma(B)), \\quad U(\\pi_l^\\ast, \\beta) \\ge U(\\pi_\\upsilon, \\beta_\\upsilon)$, $\\qquad$(9)\nwhere $\\beta_\\upsilon$ is the worst-case prior for $\\pi_\\upsilon$. Simply put, $\\pi_l^\\ast$ performs the worst when the prior is its worst-case $\\beta_\\upsilon$, but can only improve when the prior deviates from $\\beta_\\upsilon$. Additionally, it is also optimal on the worst-case prior:\n$\\forall \\pi \\in \\Pi, \\quad U(\\pi_l^\\ast, \\beta_\\upsilon) \\ge U(\\pi, \\beta_\\upsilon)$. $\\qquad$(10)\nNote that this differs fundamentally from merely finding the best response to a fixed worst-case prior $\\arg \\max_\\pi U(\\pi, \\beta)$, which once again, only has a guaranteed optimal utility on $\\beta$.\nCorollary 4.2 (Buening et al. [8]). For any policy \\pi \\in \\Pi and background population $B \\subset \\Pi$, we have\n$\\min_{\\beta \\in \\triangle(\\Sigma(B))} U(\\pi, \\beta) = U_{\\text{min}}(\\pi, \\Sigma(B))$. $\\qquad$(11)\nProof. This follows directly from the results of Buening et al. [8], using utility in place of regret and recognising that Dirac distributions associated with scenarios in $\\Sigma(B)$ are always contained in $\\triangle(\\Sigma(B))$.\\qquad$$\\square$\nLemma 4.3. For any background population $B \\subset \\Pi and the policy solving the maximin utility game (7), we have\n$U_{\\text{min}}(\\pi_l^\\ast, \\Sigma(B)) = \\max_\\pi U_{\\text{min}}(\\pi, \\Sigma(B))$. $\\qquad$(12)\nProof. By Corollary 4.2, we can write that $\\max_\\pi \\min_\\beta U(\\pi, \\beta) = \\max_\\pi U_{\\text{min}}(\\pi, \\Sigma(B))$. However, we also have $\\max_\\pi \\min_\\beta U(\\pi, \\beta) = \\min_\\beta U(\\pi, \\beta) = U_{\\text{min}}(\\pi, \\Sigma(B))$.\\qquad$$\\square$"}, {"title": "5 UTILITY OR REGRET?", "content": "Optimising for the worst-case utility (7) might be problematic. Nature could resort to only picking scenarios where the focal players achieve the worst possible score. Then, the distribution trivially minimises utility for any chosen policy, preventing the latter to learn anything. Buening et al. [8] addresses this issue by instead considering the regret of a policy. The difference is that 'impossible' scenarios will always yield zero regret for any policy, thus becoming irrelevant for a regret-maximising nature. Letting $L(\\pi, \\beta) := \\sum_\\sigma R(\\pi, \\mu) \\beta(\\sigma)$ be the Bayesian regret with respect to a prior $\\beta$, we now formulate the following minimax regret game:\n$\\min_{\\pi \\in \\Pi} \\max_{\\beta \\in \\triangle(\\Sigma(B))} L(\\pi, \\beta)$. $\\qquad$(13)\nOne can also prove that this above game has a value. Moreover, a solution $(\\pi_\\rho, \\beta)$ to (13) exhibits properties analogous to those in equations (9), (10) and (12), but in terms of regret. $\\pi_\\rho$ has its Bayesian regret upper-bounded by $L(\\pi_\\rho, \\beta)$ on $\\Sigma(B)$. It is also optimal under the worst-case prior $\\beta$ and achieves optimal worst-case regret $R_{\\text{max}}$ on $\\Sigma(B)$.\nShould utility or regret be used as an objective? Exploiting regret ensures that scenarios on which you can improve the most are sampled more often. It also ensures that degenerate scenarios get discarded as their regret is always zero. However, it demands the calculation of best responses for each scenario, which becomes taxing as the number of scenarios or problem complexity grows. To reduce the computational burden, we can approximate those best responses, or subsample the set of scenarios. An alternative way is to make use of the utility notion under some additional conditions.\nDefinition 5.1 (Non-degenerative population). A background population of policies $B \\subset \\Pi$ is non-degenerative if and only if for any scenario $\\sigma \\in \\Sigma(B)$, there exists two distinct policies $\\pi_1$ and $\\pi_2 \\in \\Pi$, $\\pi_1 \\neq \\pi_2$ such that $U(\\pi_1, \\sigma) \\neq U(\\pi_2, \\sigma)$.\nLemma 5.2. If a background population $B \\subset \\Pi$ is non-degenerative, then for any scenario $\\sigma \\in \\Sigma(B)$, there exists a policy $\\pi \\in \\Pi$ such that $R(\\pi, \\sigma) > 0$.\nProof. B is non-degenerative, for any scenario $\\sigma \\in \\Sigma(B)$ there must exist two policies $\\pi_1$ and $\\pi_2$ such that $U(\\pi_1, \\sigma) > U(\\pi_2, \\sigma)$. We have by definition $U^\\ast(\\sigma) \\ge U(\\pi_1, \\sigma)$, hence $R(\\pi_2, \\sigma) > 0$.\\qquad$$\\square$\nMaking the assumption that a background population is non-degenerative is in general realistic for cooperative tasks. This translates into only considering reasonable behaviors for the background population, or tasks where teammates cannot completely cancel out the actions of the focal players. Under the assumption of a non-degenerative background population, no distribution can deadlock the policy learner into stale scenarios. Hence, the utility-minimising opponent in Equation 7 can no longer trivially minimise utility. For the remainder of the paper, background populations are assumed to be non-degenerative."}, {"title": "6 OUT-OF-DISTRIBUTION ROBUSTNESS", "content": "As already stated in Section 4.1, having a diverse set of scenarios that adequately represents the true set of scenarios is crucial. However, since it is often impractical to perfectly replicate the true set, the prior used during training may not have the same support as the true distribution observed in nature. In such cases, the guarantees outlined in Section 4.2 no longer hold on the true distribution. In order to state further robustness guarantees, an option is to assume that scenarios in the true scenario set are close to the training scenarios. To quantify the closeness between scenarios, we first define the distance between two policy vectors as their maximum total variation across all states:\n$d(\\pi, \\pi') = \\max_{s \\in S} \\sum_a |\\pi(a|s) - \\pi'(a|s)|$. $\\qquad$(14)\nWe define the scenario distance as the minimum distance between policy vectors across permutations of the background policies:\n$d(\\sigma, \\sigma') = \\min_{\\pi, \\pi' \\in \\text{Perm}(\\pi^\\flat) \\times \\text{Perm}(\\pi^{\\flat'})} d(\\pi, \\pi')$, $\\qquad$(15)\nThis metric measures the similarity between the background policies of two scenarios. Scenarios can only be compared if they have the same number of focal players (e.g., $\\sigma = (c, \\pi^\\flat)$ and $\\sigma' = (c, \\pi^{\\flat'})$).\nDefinition 6.1 ($\\epsilon$-net of a scenario set). A finite set of scenarios $\\Sigma$ is called an $\\epsilon$-net of a scenario set $S$ if and only if, for every scenario $\\sigma \\in S$, there exists a scenario $\\sigma' \\in \\Sigma$ such that $d(\\sigma, \\sigma') < \\epsilon$.\nLemma 6.2. Let $\\Sigma$ be an $\\epsilon$-net for a scenario set $S$. For any policy $\\pi \\in \\Pi$ and scenario $\\sigma \\in S$, there is a scenario $\\sigma' \\in \\Sigma$ that verifies:\n$|U(\\pi, \\sigma) - U(\\pi, \\sigma')| < \\frac{\\epsilon T^2 ||\\rho||_{\\infty}}{2}$. $\\qquad$(16)\nProof Sketch. The result is obtained by using the fact that for any pairs of $\\epsilon$-close scenarios $\\sigma, \\sigma'$ and any $s, a^f, i$, we have $|\\sum_{s'} P_\\sigma(s'|s, a^f) - P_{\\sigma'}(s'|s, a^f)| < \\epsilon$ and $|\\rho_\\sigma(s, a^f, i) - \\rho_{\\sigma'}(s, a^f, i)| < \\epsilon||\\rho||_{\\infty}$. The proof is concluded by showing by induction that for all $t$ and $s$, $|U_t(\\pi, \\sigma, s) - U_t(\\pi, \\sigma', s)| < \\epsilon (T-t+1)(T-t)||||{\\infty}$.\\qquad$$\\square$\nLemma 6.3. Let $\\Sigma$ be an $\\epsilon$-net for some scenario set $S$. For any policy $\\pi \\in \\Pi$ and scenario $\\sigma \\in S$, there is a scenario $\\sigma' \\in \\Sigma$ such that\n$|R(\\pi, \\sigma) - R(\\pi, \\sigma')| < \\epsilon T^2 ||\\rho||_{\\infty}$. $\\qquad$(17)\nProof Sketch. The result is obtained by both using the identity $|U^\\ast(\\sigma) - U^\\ast(\\sigma')| \\le \\max_{\\pi} |U(\\pi, \\sigma) - U(\\pi, \\sigma')|$ and noticing that for any policy $\\pi, |R(\\pi, \\sigma) - R(\\pi, \\sigma')| \\le |U^\\ast(\\sigma) - U^\\ast(\\sigma')| + |U(\\pi, \\sigma) - U(\\pi, \\sigma')|$.\\qquad$$\\square$\nLemma 6.4. Let $\\Sigma$ be an $\\epsilon$-net for some scenario set $S$, and $\\pi_l^\\ast$ the optimal policy for the maximin utility problem (7) on $\\Sigma$, then\n$U_{\\text{min}}(\\pi_l^\\ast, S) \\ge \\max_{\\pi \\in \\Pi} U_{\\text{min}}(\\pi, \\Sigma) - \\frac{\\epsilon T^2 ||\\rho||_{\\infty}}{2}$. $\\qquad$(18)\nProof Sketch. We denote $\\sigma_{wc}(\\Sigma)$ and $\\sigma_{wc}(S)$ the worst-case scenarios for $\\pi_l^\\ast$ on $\\Sigma$ and $S$, and reason on the distance between $\\sigma_{wc}(\\Sigma)$ and $\\sigma_{wc}(S)$. If $d(\\sigma_{wc}(\\Sigma), \\sigma_{wc}(S)) < \\epsilon$, then Lemma 6.2 applies. Otherwise, since $\\Sigma$ is an $\\epsilon$-net, we can find another scenario $\\sigma_{\\epsilon} \\in \\Sigma$ that is $\\epsilon$-close to $\\sigma_{wc}(S)$ and use the fact that the utility of $\\pi_l^\\ast$ is by definition higher on $\\sigma_{\\epsilon}$ than on $\\sigma_{wc}(\\Sigma)$.\\qquad$$\\square$"}, {"title": "7 COMPUTING SOLUTIONS", "content": "We now desire to calculate the solution pairs for both the maximin utility (7) and minimax regret (13) games. Buening et al. [8] theoretically proved that GDA has convergence guarantees when the game is played between a policy learned with softmax parameterisation and nature learning its distribution over a finite set of MDPs. These results apply if all scenarios induce single-agent POMGs, as partial observability does not interfere with proving the required properties. However, when the focal policy is deployed in a scenario with $c > 1$ copies, the game is no longer single-agent. To approximate the reduction of these multi-agent POMGs to single-agent POMGs during training, we propose using delayed versions $t-\\delta$ of the focal policy $\\pi_l$ for the $c - 1$ remaining copies. This common practice smooths the behavior of the copies and favours proper convergence by treating the copies as fixed policies.\nAlgorithm 1, a GDA algorithm adapted to our setting, can be employed to learn solutions for both the maximin utility and minimax regret problems. Furthermore, in case the POMG is not known, one can straightforwardly adapt the algorithm to a stochastic version, by resorting to sampling scenarios and performing policy rollouts to estimate utility, regret, and gradients."}, {"title": "8 EXPERIMENTS", "content": "The aim of our experiments is to highlight the importance of partner distribution in the learning process. To achieve this, we evaluate our proposed strategies, Maximin Utility (MU) and Minimax Regret (MR), on two distinct problems. First, we consider the fully known and observable Iterated Prisoner's Dilemma to validate the theoretical results. Following"}]}