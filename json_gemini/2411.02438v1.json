{"title": "Entropic Hetero-Associative Memory", "authors": ["Rafael Morales", "Luis A. Pineda"], "abstract": "The Entropic Associative Memory holds objects in a 2D relation or \u201cmemory plane\" using a finite table as the medium. Memory objects are stored by reinforcing simultaneously the cells used by the cue, implementing a form of Hebb's learning rule. Stored objects are \u201coverlapped\u201d on the medium, hence the memory is indeterminate and has an entropy value at each state. The retrieval operation constructs an object from the cue and such indeterminate content. In this paper we present the extension to the hetero-associative case in which these properties are preserved. Pairs of hetero-associated objects, possibly of different domain and/or modalities, are held in a 4D relation. The memory retrieval operation selects a largely indeterminate 2D memory plane that is specific to the input cue; however, there is no cue left to retrieve an object from such latter plane. We propose three incremental methods to address such missing cue problem, which we call random, sample and test, and search and test. The model is assessed with composite recollections consisting of manuscripts digits and letters selected from the MNIST and the EMNIST corpora, respectively, such that cue digits retrieve their associated letters and vice versa. We show the memory performance and illustrate the memory retrieval operation using all three methods. The system shows promise for storing, recognizing and retrieving very large sets of object with very limited computing resources.", "sections": [{"title": "1 Introduction", "content": "The Entropic Associative Memory (EAM) is a novel memory system in which recollections are stored diagrammatically in a table, or memory plane. The basic model defines three memory operations in relation to a cue: \u03bb-register, \u03b7-recognition and \u1e9e-retrieval. Memory objects are represented as mathematical functions such that columns and rows of the table correspond to function arguments and values, respectively, which in turn correspond to the object's attributes and values, as in standard feature-value structures [6]. Functions representing diverse memory objects can \u201coverlap\u201d on the memory medium. The system has been tested with the storage, recognition and retrieval of manuscript digits [14], manuscript digits and letters [12], phonetic representations [13], and images of clothes, bags and shoes [15].\nFrom a computational perspective, the memory is declarative, abstractive, indeterminate and constructive. It supports the recovery not only of the cued objects but also of objects associated to the cue, as well as the production of association chains, possibly including objects of different classes[15], but the memory is mostly auto-associative. A more general form of associations can be established between objects of different domains and modalities, and a memory supporting this functionality is hetero-associative. Hetero-associative neural networks models were explicitly introduced with Kosko's BAM model [7], which inspired a very large body of work [1, 9, 16, 18, 8].\nIn this paper we present the Entropic Hetero-Associative Memory (EHAM) as an extension of EAM [14, 12, 13, 15] that opposes BAM and related models in the same ways that EAM oppose auto-associative neural network-based models, such as Hopfield's [5].\nThe structure of the paper is as follows: We present the extension of EAM to the hetero-associative model EHAM. The new model is tested with a heterogeneous corpus constituted by arbitrary associations between individual digits of MNIST and individual letters of EMNIST of designated associated classes. We illustrate the machinery with the object retrieved in both directions with three methods. Finally, we highlight the main features of the model, asses the results, and discuss some implications for research on associative memories."}, {"title": "2 Hetero-Associative Extension of EAM", "content": "The model allows the expression of pairs of objects, possible of different domains and modalities that can be registered and recognized as units, so one object of the pair can be used as cue to recover the other and vice versa. Let $A = {a_1,...,a_n}$ and $V = {V_1,..., V_p}$, $B = {b_1,...,b_m}$ and $Z = {Z_1,...,Z_q}$, be two pairs of sets of attributes and values, respectively. An Entropic Hetero-Associative Memory $M_{A,B}$ stores associations between"}, {"title": "3 Experiments with the HEMNIST Corpus", "content": "We created two auto-associative models using the weighted EAM system [15] for the MNIST [10] dataset of digits, and a selection of then classes from the EMNIST Balanced"}, {"title": "3.1 Recognition experiments", "content": "We constructed a recognition tests constituted as follows:\n\u2022 50% by pairs included in the test corpus of HEMNIST, balanced for each pair of classes;\n\u2022 50% by pairs not included in HEMNIST, formed randomly out an object of MNIST and an object of EMNIST selected from classes that do not correspond in HEMNIST -see Table 2.\nThe recognition performance is measured in terms of the precision, recall and accuracy in relation to the percentage of the remembering corpus included in the memory. The recognition performance of the EHAM is shown in Figure 3.\nThe figure on top shows that, if the parameters are set to their default values, the precision rate is in general quite high but the recall is very low, suggesting that the system is not yet within its operational range. For this, we relaxed the n-recognition operation by"}, {"title": "3.2 Retrieval experiments", "content": "The B-retrieval operation in relation to a cue in the source memory field reduces the hetero-associative memory 4D relation to a 2D relation, that is specific to the cue, in the target memory field, from which the remembered object has to be produced, as in the basic EAM model. However, there is no cue available in the target field to complete the operation, giving rise to the missing cue problem. Next we present three incremental strategies to address such problem with their corresponding results."}, {"title": "3.2.1 Random samples method (RS)", "content": "The basic strategy consists on the random selection of the retrieved object, as presented in Section 2 in step 4 of the definition of the \u1e9e-retrieval operation. Figure 4 (top) shows the precision and recall of the memory retrieval operation, in both directions -i.e., from MNIST to EMNIST and vice versa\u2013 in relation to the amount of the remembering corpus included in the memory registers.\nThe memory becomes operational when 16% of the remembering corpus is included. The best compromise, in both directions, occurs when the memory is filled with 32% of the corpus, with precision of 43.3% and recall of 40.3%, from MNIST to EMNIST, and precision of 35.2% and recall of 33.9% in the reverse direction. Although the performance is low, this experiment shows that the memory plane in the target field, produced out of a cue in the source field, does include the object to be retrieved, with a probability that is greater than chance."}, {"title": "3.2.2 Sample and test method (ST)", "content": "We now consider that the image back in the source field of the object retrieved in the target one must match the original cue to the EHAM \u1e9e-retrieval operation. This observation suggests a sample and test method to generate and select an object in the target field that is hetero-associated to the original cue in the source field, as follows:\n1. Select the relation $r_T$ in the target field T on the basis of cue $c_S$ in source field S."}, {"title": "3.2.3 Sample and search method (SS)", "content": "We augment the method in 3.2.2 with a local search process (random descent) [4] that optimizes the objects selected by sampling and test, so the distance of the backwards relation $r_S$ of each $o_T$ to cues is minimized. Intuitively, the process consists on exploring the neighborhood of each $o_T$ and modifying it monotonically reducing such distance at each step.\nFigure 4 (bottom) shows the results of the sample and search method. The search process produces an average of 800 objects per $o_T$ delivered by sampling and test; hence, this method produces around 128 + 800 = 928 candidate instances from which the one whose backwards image approximates better the original cue is chosen. As can be seen, the best trade-off between precision and recall is close to 60% in both directions, when the full remembering corpus is included."}, {"title": "4 Illustration of the machinery", "content": "Figure 5 shows chosen examples from a random sample of 5% the products of the B-retrieval operation for all classes using the three methods. The numbers below the symbols indicate the class selected by the classifier \u2013see Figure 1. The top row shows the cue of \u1e9e-retrieval for an instance digit and an instance letter of all ten classes, and the corresponding symbols produced by sample and search, sample and test and random, are shown in the second, third and fourth rows, respectively. The symbols produced by the sample and search and the sample and test methods have a reasonable good quality, and the random method has a lower quality, which is consistent with their corresponding classification rates."}, {"title": "5 Results", "content": "We have introduced the Entropic Hetero-Associative Memory (EHAM) as a direct extension of the EAM model. We tested the model with memory recognition and memory retrieval experiments using hetero-associations between specific digits and letters taken from the MNIST and EMNIST corpora. The performance of EHAM in the recognition task, using both correct and incorrect associations as testing corpus, is moderately good (precision and recall around 65% and 70%, respectively). The memory retrieval operation, on its part, requires two memory cues: one for the source and one for the target memory plane, but the latter cue is missing, augmenting the indeterminacy of the problem significantly. We explore three incremental methods for completing the retrieval operation despite that the cue to the target plane is lacking. The results are shown and can be compared in Table 3. The increase of performance reflects the reduction of the indeterminacy"}, {"title": "6 Discussion", "content": "The key property of the EAM model is that the memory register operation \u201coverlaps\" memory cues on the medium -so cells may be used in the representation of different objects and, conversely, the represented objects may share different cells\u2013 making the representation genuinely distributed. This operation models a form of Hebb's learning rule, as the simultaneous reinforcement of cells is causal to such cells being involved in the same computational function \u2013e.g., remembering the particular object. Hence, the bindings between memory objects depend only on the weights of the cells used by their representations. The distributed property also gives rise to a huge memory capacity, and the memory operations can be implemented in parallel if the appropriate hardware is provided, in the spirit of the original presentation of the Connectionist program [17]. However, the identity of the stored cues is dissolved, the representation becomes indeterminate and has an entropy value, and the memory is operational in an entropy interval, which is nor too low neither too high. The distributive property gives rise as well to a very large set of potential objects that may allow for the recognition of novel views of known objects and imagination [15] but also to false recollections [11]. The memory retrieval operation recovers a novel object out of the indeterminate memory mass and the cue, and is genuinely constructive. All of these properties transfer from EAM to EHAM. In particular, the bindings"}, {"title": "7 Experimental Setting", "content": "The experiments were programmed in Python 3.11 on the Anaconda distribution. The neural networks were implemented with TensorFlow 2.14.0, and the graphs were produced using Matplotlib, ImageMagik, and Inkscape. The experiments were run on an Alienware Aurora R5 with an Intel Core i7-6700 Processor, 16 GBytes of RAM and an NVIDIA"}, {"title": "8 Data Availability", "content": "The datasets used in the present study are MNIST [10] and EMNIST Balanced [3]. The full code and the detailed experimental results are available in Github at\nhttps://github.com/eam-experiments/hetero."}]}