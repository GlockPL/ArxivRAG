{"title": "Optimizing Hard-to-Place Kidney Allocation: A Machine Learning Approach to Center Ranking", "authors": ["Sean Berry", "Berk G\u00f6rg\u00fcl\u00fc", "Sait Tun\u00e7", "Mucahit Cevik", "Matthew J. Ellis"], "abstract": "Kidney transplantation is the preferred treatment for end-stage renal disease, yet the scarcity of donors and inefficiencies in allocation systems create major bottlenecks, resulting in prolonged wait times and alarming mortality rates. Despite their severe scarcity, timely and effective interventions to prevent non-utilization of life-saving organs remain inadequate. Expedited out-of-sequence placement of hard-to-place kidneys to centers with the highest likelihood of utilizing them has been recommended in the literature as an effective strategy to improve placement success. Nevertheless, current attempts towards this practice is non-standardized and heavily rely on the subjective judgment of the decision-makers. This paper proposes a novel data-driven, machine learning-based ranking system for allocating hard-to-place kidneys to centers with a higher likelihood of accepting and successfully transplanting them. Using the national deceased donor kidney offer and transplant datasets, we construct a unique dataset with donor-, center-, and patient-specific features. We propose a data-driven out-of-sequence placement policy that utilizes machine learning models to predict the acceptance probability of a given kidney by a set of transplant centers, ranking them accordingly based on their likelihood of acceptance. Our experiments demonstrate that the proposed policy can reduce the average number of centers considered before placement by fourfold for all kidneys and tenfold for hard-to-place kidneys. This significant reduction indicates that our method can improve the utilization of hard-to-place kidneys and accelerate their acceptance, ultimately reducing patient mortality and the risk of graft failure. Further, we utilize machine learning interpretability tools to provide insights into factors influencing the kidney allocation decisions.", "sections": [{"title": "1 Introduction", "content": "Kidney disease is the ninth leading cause of death in the US and represents a significant global health challenge, with millions of people affected by end-stage renal disease (ESRD). The preferred and most effective treatment for ESRD is a kidney transplant. However, the scarcity of donor kidneys and inefficiencies in allocation systems create significant bottlenecks, leading to lengthy wait times and increased waitlist mortality rates. This scarcity is further exacerbated by the high non-utilization rates of viable kidneys, which often result from a combination of logistical challenges, stringent organ selection criteria of transplant centers, and the time-sensitive nature of organ transplantation. The non-utilization rate for hard-to-place kidneys, such as those from older donors, donors with medical comorbidities, or kidneys with extended ischemic times is particularly concerning. Non-utilization rates increase steadily with higher Kidney Donor Profile Index (KDPI) scores, a metric that combines several factors to estimate the risk of graft failure.\nDespite the severe scarcity of life-saving organs, timely and effective measures to prevent non-utilization remain inadequate. One direct approach to increase the utilization of organs that might otherwise go unused is to impose penalties for their rejection. However, such penalties are prohibited in the U.S. Alternatively, hard-to-place organs can be offered to patients who have declared their willingness to accept such organs at the time of registration on the waiting list. This approach is partially implemented in the U.S., where patients can opt to be listed for organs with high Kidney Donor Profile Index (KDPI), hepatitis B and C virus (HCV+), and other specific conditions. Additionally, transplant centers can apply filters based on donor characteristics such as age or donation after cardiac death (DCD). Although the effectiveness of this practice has not been formally evaluated, the persistently high non-utilization rates highlight the urgent need for more effective interventions.\nExpedited out-of-sequence placement of hard-to-place kidneys to transplant centers with the highest likelihood of utilization has been recommended in the literature as an effective strategy to rescue kidneys at high risk of non-utilization. In line with this recommendation, the Organ Procurement and Transplantation Network (OPTN) implemented the Kidney Accelerated Placement (KAP) project from July 18, 2019, to July 15, 2020. The KAP project aimed to increase the utilization of hard-to-place kidneys by offering them to centers with a proven history of accepting such organs. However, this initiative did not result in statistically significant improvements in the donor acceptance rate for hard-to-place kidneys compared to the year prior to the program start. Similarly, the U.K.'s Kidney Fast Track Scheme (KFTS) sought to allocate organs to the centers most likely to accept them after multiple rejections in the national allocation system. Unlike KAP, KFTS made simultaneous offers to all participating transplant centers, allowing the accepting center to transplant the kidney to any of their patients. Despite these efforts, KFTS also failed to significantly improve organ utilization. These outcomes highlight the need for more effective, data-driven interventions to enhance the allocation and utilization of hard-to-place kidneys.\nIn current practice, to increase the utilization and expedite the placement of hard-to-place kidneys, organ procurement organizations (OPOs) can deviate from the regular allocation process and extend out-of-sequence offers to transplant centers that are more likely to accept such kidneys. However, this practice is currently non-standardized and relies heavily on the subjective judgment of individual OPOs and is often influenced by behind-the-scenes relationships, further raising concerns about transparency. The lack of defined guidelines for such allocation exceptions could potentially worsen disparities in organ access. Additionally, recent updates to the kidney allocation system have inadvertently created delays in local kidney placements, further increasing the prevalence of out-of-sequence offers. Consequently, there is an urgent need to leverage comprehensive data on center-level offer acceptance and transplant outcomes to develop an evidence-based approach to out-of-sequence offers. By doing so, we can ensure that expedited placement is trackable and is used judiciously, transparently, and only when necessary. Standardizing the criteria for bypassing the regular allocation process and making out-of-sequence offers would reduce reliance on ad hoc decisions and promote equity and efficiency in kidney allocation.\nThe complexities of kidney allocation are compounded by various factors, including the intricate decision-making process of accepting a donor kidney, the temporal dynamics of organ viability, geographic logistics, and the rapidly changing health status of patients. Specifically, transplant centers' acceptance decisions are shaped by factors such as donor-recipient compatibility, waiting time, the urgency of the recipient's condition, the distance between donor and recipient, and the center's infrastructure and expertise. Additionally, transplant centers' risk tolerance, which can fluctuate based on their recent outcomes, further complicates these decisions. Centers experiencing challenges with post-transplant outcomes may be more hesitant to accept higher-risk organs. These decisions can also vary depending on the individual risk tolerance of the on-call clinician, adding another layer of variability to the process.\nDue to the complexity of the data and the objectives, existing interventions to improve the placement likelihood of hard-to-place kidneys typically rely on simple heuristics and non-standardized criteria. However, these methods often fail to account for the dynamic and multifaceted nature of organ transplantation, potentially contributing to inherent discrepancies in organ allocation systems, such as disparities in transplant access, wait times, and waitlist mortality. Furthermore, a significant consequence of the reliance on simple heuristics is the high non-utilization rates of viable kidneys, as the system may struggle to find suitable recipients within the necessary timeframe. This issue underscores the need for more dynamic, data-driven methods to enhance the allocation process. By leveraging comprehensive data and advanced algorithms, we can develop more sophisticated models that better account for the complex factors influencing kidney placement, ultimately improving the efficiency and equity of organ allocation and reducing the non-utilization rates of viable kidneys.\nAnother key challenge in the kidney allocation and transplantation process is the overwhelming volume of data, which is difficult for humans to parse effectively or consistently. Simple heuristics and standardized criteria often fail to make efficient use of this data. In contrast, machine learning (ML) algorithms excel at analyzing large datasets, uncovering patterns that are indiscernible to humans. These algorithms can be employed to identify optimal donor-recipient matches, improving overall outcomes in kidney transplantation and reducing non-utilization rates.\nDespite several studies investigating the kidney allocation problem, there remains a significant gap in optimizing the efficiency of organ allocation interventions to improve utilization. The current high non-utilization rates of donor kidneys, coupled with the growing demand for kidneys, underscore the urgent need for more effective intervention strategies. An ML-based ranking system can play a crucial role in addressing this challenge. Leveraging the predictive power of ML, we can better match donor organs with suitable recipients, thus reducing the likelihood of viable organs going unused. Such a system can maximize the utilization likelihood of hard-to-place kidneys by identifying and targeting transplant centers with the highest likelihood of acceptance and successful transplantation. By making more informed and data-driven interventions, this approach has the potential to significantly improve the efficiency and equity of the allocation of hard-to-place kidneys, ultimately saving more lives.\nThe objective of this research is to tackle the challenges in kidney allocation processes by developing a data-driven, ML-based approach. Our primary goal is to create an effective and interpretable ranking system for the allocation of hard-to-place kidneys to transplant centers. This approach leverages a comprehensive set of features, many of which have not been previously explored in the literature. Notably, we incorporate center-specific features such as the number of kidneys previously accepted by a center with higher Cold Ischemia Time (CIT) or Kidney Donor Risk Index (KDRI) than the offered kidney, as well as the proximity of transplant centers and donor hospitals to each other and to medium and large airports.\nTo showcase the effectiveness of our proposed framework, we compare its performance against the data from the current kidney allocation system as well as several heuristics that are inspired by the previously implemented interventions. Our primary evaluation criterion is the number of centers that reject an organ before it is ultimately accepted. Additionally, we provide a detailed analysis of both the local and global interpretability of the underlying prediction models. This analysis not only enhances clinicians' understanding of the ML model's inference process but also highlights the significance of center-specific features in improving kidney allocation outcomes. By offering a transparent and robust solution, our approach aims to contribute to more efficient and equitable organ allocation, ultimately reducing organ non-use and improving outcomes for patients awaiting transplants.\nOur research on optimizing the allocation of hard-to-place organs is both timely and highly relevant, given the urgent challenges in organ allocation and utilization. In response to these challenges, the OPTN Board of Directors established the Expeditious Task Force in September 2023, with the goal of increasing the number of transplanted organs by improving the efficiency of the organ placement process. The task force plans to actively engage the donation and transplant community, conducting multiple rapid, small-scale trials of innovative approaches to enhance organ usage and placement efficiency. Additionally, with the U.S. Health Resources and Services Administration (HRSA) currently seeking to revamp the OPTN system, our research comes at a pivotal moment. By introducing a novel data-driven approach to kidney transplantation interventions\u2014ranking centers based on their likelihood of accepting hard-to-place organs, our work not only addresses existing inefficiencies but also aligns with ongoing policy discussions aimed at improving transplantation outcomes. This alignment underscores the potential impact of our research in shaping the future of organ allocation practices and contributing meaningfully to ongoing efforts to optimize the national transplant system.\nWe summarize the contributions of our work as follows:\n\u2022 We develop a novel ML-based ranking system for allocating hard-to-place kidneys to centers with a higher likelihood of accepting and successfully transplanting them. Our approach leverages a more comprehensive and diverse set of features than previously utilized in the literature, thereby significantly enhancing the predictive power and applicability of the proposed kidney allocation intervention framework.\n\u2022 We introduce a robust framework for rigorously evaluating our model against existing heuristics and baseline data. This framework not only demonstrates the superior performance of our approach compared to current practices but also establishes a new benchmark for evaluating the performance of kidney allocation interventions more broadly.\n\u2022 We conduct a comprehensive analysis of model explainability, addressing both local and global interpretability for the ML models used. This analysis increases the transparency of the allocation process of the hard-to-place organs and offers valuable insights that can inform the development of effective decision-support tools in transplantation.\nThe remainder of the paper is structured as follows. In Section 2, we review the relevant literature to contextualize our research. Section 3 outlines the comprehensive data and the overall methodology employed in this study. Section 4 presents and discusses the results of our analysis. Finally, Section 5 concludes the paper by summarizing our findings and suggesting potential directions for future research."}, {"title": "2 Literature Review", "content": "The field of organ transplantation has seen significant growth in research, particularly in recent years, as efforts intensify to improve patient outcomes and optimize organ utilization. Despite its transformative potential, the application of ML models in this field remains relatively underexplored compared to traditional approaches. ML models may not reach their full potential if they are not applied correctly, either due to the lack of relevant and high-quality data needed to train these models effectively or because the problem at hand may not be well-suited to ML techniques. For instance, issues like overfitting in small datasets, the interpretability of complex models, or the mismatch between the model's assumptions and the nature of the medical data can hinder the effectiveness of ML solutions. A recent example is the study by Truchot et al. , which compares various ML models with traditional Cox proportional hazard models. The study reveals that Cox models often perform equivalently or even better than ML models in predicting graft survival in kidney transplantation. These types of studies underscore the importance of careful consideration in how ML is integrated into transplantation medicine, emphasizing the need for further research to determine when and how ML can best complement or enhance existing methodologies.\nOne of the most pressing challenges in kidney transplantation is the high non-utilization rate of donor kidneys, particularly those classified as high-risk. Crannell et al. reported that nearly 45% of deceased donor kidneys with a high KDPI were not used in the U.S. To alleviate this issue, several intervention mechanisms and accelerated placement schemes have been implemented globally, with varying degrees of success. The KAP project in the U.S. is a notable example, where transplant centers with a history of accepting similar organs were prioritized for offers (see Table 1 for details on the eligibility criteria used in KAP). Despite these efforts, systematic analyses have shown that such schemes have had limited impact, often due to their constrained scope and the subjective criteria used for allocation. Recent research has also attempted to predict the risk of non-use for donor kidneys. For instance, studies by Massie et al. and Marrero et al. developed logistic regression models that outperformed KDRI in predicting non-use risk. Building on this, Barah and Mehrotra and Li et al. proposed ML models, including random forests, boosting trees, and neural networks, which demonstrated strong predictive performance in identifying kidneys at risk of non-utilization.\nIn the broader context of transplant medicine, ML has shown promise across various applications, including disease diagnosis, treatment response prediction, and patient management. Gotlieb et al. provide a comprehensive review of 36 publications, highlighting the potential of ML models while also emphasizing the need for robust validation, ethical considerations, and collaboration between clinicians, researchers, and data scientists. This underscores the importance of carefully integrating ML into clinical practice to ensure that its benefits are fully realized. Connor et al. provide a literature survey in several areas of interest for ML and organ transplantation namely clinical prediction and decision support, listing for transplantation, organ allocation, and prediction of patient and graft survival. They note key technical and ethical challenges that must be addressed for ML to become an effective clinical tool.\nA notable gap in the existing literature is the use of CIT and center-specific, behavior-related features in predicting the acceptance or non-use of donor kidneys by transplant centers. The importance of CIT in transplantation outcomes cannot be overstated (see, for example, Debout et al., demonstrating a proportional relationship between CIT and the risk of graft failure and patient survival). Advances in preservation techniques, such as improved preservation fluids or pumping, have mitigated but not eliminated the risks associated with prolonged CIT . Therefore, omitting CIT as a predictive feature could lead to significant oversight, underestimating the complexities of organ acceptance and rejection decisions by transplant centers. Furthermore, the inclusion of center-specific features, such as the number of kidneys with higher CIT or KDPI previously accepted by a center, and logistical factors like proximity of transplant centers and donor hospitals to airports, has been largely overlooked in the literature. These variables are crucial for understanding how specific transplant centers are likely to respond to organ offers, especially for hard-to-place kidneys. By utilizing these features in the ML model training, a more nuanced and accurate prediction of organ acceptance can be achieved, ultimately improving kidney allocation efficiency.\nThe application of ML in kidney allocation holds tremendous potential for enhancing transplantation outcomes and reducing the rate of organ non-use. Our research stands out by focusing on optimizing interventions specifically for the allocation of hard-to-place kidneys-a topic that, to our knowledge, has not been thoroughly explored in the literature. We incorporate key features that are crucial in predicting the offer acceptance likelihood of transplant centers, including dynamic calculations of CIT, the historical acceptance behavior of centers, and various logistical factors. By addressing these critical gaps, our study aims to deliver a comprehensive, data-driven approach to kidney allocation that could significantly improve both the efficiency and equity of the transplant system."}, {"title": "3 Methodology", "content": "This section provides a comprehensive overview of the dataset, experimental setup, modeling specifications, and interpretability methods employed in the paper."}, {"title": "3.1 Dataset", "content": "To analyze the likelihood of transplant centers accepting hard-to-place kidneys, we construct a unique dataset from two primary sources. First, we obtained deceased donor data from the OPTN, which includes records from 248,008 deceased donors between January 2016 and September 2021, all of whom had at least one kidney recovered for transplantation. The second source is the Potential Transplant Recipient (PTR) data for the same period, which documents all kidney offers made to patients on the US waiting list through a matching process called \u201cmatch run\u201d. We excluded donors missing key features, such as clamp data and initial response date, as well as kidney offers without a corresponding entry in the deceased donor dataset (further details are provided in Section 3.1.1).\nWe create the initial dataset by merging donor-specific features from the deceased-donor dataset with kidney offers from the PTR data. During a match run, a kidney may be offered to multiple patients within the same transplant center. For each center, we consider only the first offer (to the patient with the highest priority at that center) or an accepted offer, if applicable. This approach enables us to assign a binary outcome (accept or reject) to each center-kidney pair. Additionally, we generate several center- and donor-specific variables, including the distance to large and medium airports, the historical acceptance rate of each center, and the number of kidneys with a higher KDRI accepted by the center in the past two years.\nThe PTR data includes the offer times, which indicate when an organ is offered to a potential recipient's transplant center for consideration, for each kidney offer, allowing us to determine CIT\u2014the period during which a donated organ is procured and remains outside the human body before transplantation. For our analysis, CIT is calculated as the difference between the clamp time (when blood supply to the organ is ceased and it is removed from the donor) and the offer time. Tables 2 and 3 list the categorical and continuous features used in our analysis, respectively, along with summary statistics (e.g., distribution for categorical variables, mean/variance for continuous variables) stratified by the outcome.\nA significant challenge during data processing was the imbalanced nature of the dataset, with kidney rejections vastly outnumbering acceptances at a ratio of approximately 34 to 1. This imbalance poses difficulties in accurately modeling and predicting acceptance outcomes, and it can lead to biased model predictions and limit the insights of our ranking model. To address this, we employ the following data censoring methods:\n\u2022 For accepted kidneys, we exclude offers to centers that has a lower ratio of offers to total patient count than the accepting center. This approach ensures we focus on centers that are comparable in their offer response to the accepting center. For example, if a center accepts a kidney after receiving offers for 20% of its patient population, then any other center that rejects the same kidney without receiving offers for at least 20% of its population is excluded from our analysis (for that particular kidney). The rationale is that these latter centers might have accepted the kidney if they had received offers for a comparable proportion of their patient population.\n\u2022 For unused kidneys, we retain a representative subset by censoring offers using a threshold based on the ratio of offers to the total patient count. Our target was to ensure that acceptances constituted approximately 5% of the total dataset."}, {"title": "3.1.1 Data Exclusion", "content": "To ensure the quality and reliability of the dataset, we excluded data points missing critical values. First, we removed donors from the deceased donor dataset that were missing clamp dates, resulting in the exclusion of approximately 0.005% of unique donors. Next, we eliminated any kidney offers in our dataset that did not have a corresponding entry in the deceased donor dataset, which accounted for approximately 12.18% of the unique donors referenced in the PTR dataset. Additionally, we excluded cases where a kidney was accepted more than twice, which represented about 5% of the remaining unique donors. After the exclusions and data losses, the dataset contained 61,794 unique donors."}, {"title": "3.2 Intervention Methods for Allocating Hard-to-Place Kidneys", "content": "We develop and test several data-driven heuristic approaches to create an effective intervention framework for allocating hard-to-place kidneys to centers with a higher likelihood of accepting them. The primary goal of these interventions is to allocate hard-to-place kidneys in a timely manner, preventing prolonged CIT during the offer process, which could lead to organs being unused. To evaluate the performance of different intervention approaches, we calculate the number of offer rejections (non-accepting centers) encountered before reaching an accepting center, which we refer to as the number of centers seen (NCS) score. This metric is straightforward and easy to interpret, providing a solid basis for comparing the effectiveness of interventions. As summarized in Section 1, in the current organ allocation system, each final offer sent to a transplant center can take up to an hour, potentially contributing to higher CIT. Therefore, a low NCS score indicates a timely placement of hard-to-place kidneys, minimizing CIT and reducing the risk of non-use, while a high score suggests longer CIT and a greater risk of the organ being unused.\nGiven a set of $n$ kidneys ${K_1, K_2, ..., K_n}$, with each kidney $K_i$ offered to $c_i$ centers before acceptance, we define the NCS as the average number of offers over $n$ kidneys as follows:\n$NCS = \\frac{1}{n} \\sum_{i=1}^{n} C_i.$"}, {"title": "3.3 ML Models Used in the ALP Framework", "content": "We utilize ML models for predicting the probability of acceptance by a center for a given kidney. Specifically, we train ML models by splitting the dataset donor-wise into training and testing sets and then train the models to predict acceptance events. We then use the raw probability predictions from these trained models to rank the centers for each donor, prioritizing those with the highest likelihood of acceptance. To evaluate the ranking performance of the ML models, we employ the method described in Section 3.2.\nGiven the imperative for both high accuracy and interpretability in predicting organ acceptance, tree-based models are the prevalent choice in the literature. Building on this, we employ a range of ML models based on prior studies, while also incorporating more recent gradient boosting models to leverage their superior predictive capabilities and efficiency in handling large and complex datasets. Specifically, we train a Random Forest model, an ensemble of decision trees that provides improved accuracy. The Extra Trees model, similar to Random Forest, uses multiple decision trees but incorporates more randomized feature splits, reducing the risk of overfitting. Additionally, we include various gradient boosting models. Light Gradient Boosting Machine (LGBM) is chosen for its effectiveness in managing high-dimensional feature spaces and large datasets. CatBoost is selected for its capability to handle categorical data efficiently without extensive preprocessing. Additionally, we use Logistic Regression as a baseline ML method, as it models binary outcomes through linear relationships. A Decision Tree model is also included for its simplicity and ease of interpretation.\nTogether, these models represent a broad spectrum of ML techniques, each with unique strengths and applications. Their performances, as quantified in our study, can guide practitioners in selecting the most appropriate model for their specific kidney allocation intervention tasks."}, {"title": "3.4 ML Interpretability Methods", "content": "The black-box nature of many ML models necessitates actionable and interpretable insights into their decisions. Ensuring transparency is crucial not only for gaining acceptance of these models but also for carefully aiding decisions in domains like organ allocation, where actions have life-altering implications. In our analysis, we use SHAP (SHapley Additive exPlanations) and its variants, particularly, TreeSHAP, to elucidate model predictions and provide clear, interpretable explanations.\nTreeSHAP is an interpretability method specifically designed for explaining the outputs of tree-based ML models. This method bridges game theory and local explanations by calculating a feature's Shapley value, which represents the \"fair\" contribution of that feature to the model's output. This Shapley value is determined by averaging the marginal contributions across all possible feature permutations. TreeSHAP leverages the inherent structure of tree models to compute these game-theoretic values in a highly efficient manner, making it well-suited for practical applications.\nThe SHAP library offers methods for both local and global interpretability for ML models. Locally, it provides insight into how individual features or combinations of features influence a single prediction. In the context of the kidney allocation interventions problem, local explanations can elucidate the model's estimated acceptance probability for a specific donor-center pair. Globally, the aggregate of SHAP values helps us understand the overal behavior of the model, identifying which features are most influential across all predictions. This dual-level insight is invaluable, not only for uncovering previously misunderstood factors affecting kidney allocation interventions but also for providing confidence in model decisions. These insights are essential for validating and fine-tuning the applications of ML models in high-stakes situations like organ allocation."}, {"title": "4 Results", "content": "In this section, we present the findings from our comprehensive numerical study using various ML models and heuristics for the kidney allocation problem. Our analysis covers different compositions of kidney allocation data to showcase the capabilities of these models for this task. We begin by evaluating the performance of different ML models on a subset of the dataset for model selection, focusing on their effectiveness in the kidney allocation interventions. We then extend the analysis to the uncensored (i.e., full) dataset, assessing the models in greater detail using standard classification metrics as well as the NCS score. Following this, we discuss the results obtained from the censored dataset, including an explanation of the censoring process, performance different ranking approaches under these conditions, and the associated performance metrics. Finally, we present results from filtered datasets based on different ranges of the KDRI, accompanied by local and global explanations of the model predictions to provide deeper insights into the factors influencing these decisions."}, {"title": "4.1 ML Model Selection", "content": "To identify the best performing ML model, we conduct an analysis using ten random splits from the data for training and testing. We compare the performance of several ML models based on the Macro F1 score and the average number of offers before acceptance, as indicated by the NCS score. The considered ML models include CatBoost, Decision Tree, LGBM, Logistic Regression, Random Forest, and Extra Trees. The results of this comparison are presented in Table 7.\nTable 7 shows that the LGBM model outperforms others in terms of both Macro F1 score and the NCS score. To further elucidate the performance of the selected models, we compare the sensitivity-specificity plots across varying decision thresholds. These curves are generated by adjusting the models at different cut-off data censoring ratios. As shown in Figure 1, both the LGBM and CatBoost models performs well, with CatBoost occasionally surpassing LGBM in regions of higher sensitivity.\nFigure 2 provides a more nuanced view by highlighting models that maintain competitive (non-dominated) performance across the entire sensitivity-specificity spectrum. While the LGBM model consistently shows strong performance, it is important to note that CatBoost exhibits comparable, and in some instances, superior performance in certain regions of the spectrum.\nBoth LGBM and CatBoost offer robust frameworks for in-depth model interpretation, enabling a clear understanding of how individual features contribute to predictions. This interpretability is particularly critical in high-stakes domains like organ allocation, where transparency in the decision-making process is essential for informed and ethical choices. While both models demonstrate strong performance, we ultimately select the LGBM model as our preferred approach for the kidney allocation intervention problem due to its slight advantage in predictive accuracy and interpretability. Specifically, LGBM excels in ranking centers, delivers consistent performance across various metrics, and offers clear, interpretable insights; making it the optimal choice for this task."}, {"title": "4.2 Performance of the ALP Framework over All Kidneys", "content": "We evaluate the performance of the ALP framework using our comprehensive kidney allocation dataset, including all kidneys without restricting to hard-to-place cases. The objective of this analysis is to assess how effectively ALP identifies transplant centers with the highest likelihood of accepting any given kidney. We perform an 80/20 train-test split on the donors data, train an LGBM model on the training set, and then report the model's performance on the test set.\nTable 8 presents the resulting classification report for the trained model. We observe that the classification performance is relatively low when tested against all kidneys. One major reason for this outcome is the imbalanced nature of the dataset, with only about 3% of the observations being acceptances. This imbalance can cause the model to achieve high precision without adequately learning to differentiate between classes. Additionally, differentiating between the likelihood of centers accepting a higher quality kidney is challenging, as most centers are willing to accept these kidneys when they are offered to the \"right\" candidate on their waiting list. Consequently, factors such as the specific characteristics of the offer-receiving patient and the portfolio of patients listed at the center at the time of the offer play a more dominant role than center-level differences in offer acceptance behavior for higher-quality kidneys, making the proposed classification problem a highly complex one. On the other hand, it is important to note that ALP is not intended to replace the regular allocation system but rather to provide a data-driven framework to aid in intervention decisions, specifically to increase the transplantation likelihood of hard-to-place kidneys.\nDespite the relatively subpar performance of the ALP framework in the classification task, its use for ranking shows significant improvement over current practices. Table 9 highlights the difference in the expected or observed NCS score, which represents the average number of centers that would be offered the organ before an acceptance. The baseline shows that, on average, 4.27 centers are offered the kidney before it is accepted, whereas the ALP framework reduces this to an average of 1.15 centers before reaching the accepting center. This demonstrates the potential of the ALP framework to streamline the allocation process and improve outcomes, even when applied to the allocation of all kidneys."}, {"title": "4.3 Exclusion of Low-Offer-Volume Centers in Data Censoring", "content": "It is well documented that gradient-boosted classification models, such as LGBM, can exhibit bias towards the majority class when dealing with imbalanced data . To address this issue, one effective approach is to balance the training data by under-sampling the majority class. We implemented this approach by counting the number of times each center is offered a single organ and dividing this count by the total number of patients listed at that center at the time of the offer. If this ratio falls below a specified cutoff, we remove that center-organ offer observation from the dataset.\nOur goal is to exclude \u201clow-offer volume transplant centers\" that did not receive the organ offer for a significant fraction of their listed patients, as these centers may not be comparable to the accepting center. A transplant center that rejects an offered organ for only a small fraction, such as 5%, of their highest priority patients might have accepted it if the offers were made to a larger fraction of their patients. By censoring the dataset, we ensure that the training data more accurately reflects scenarios where centers have a realistic chance of accepting an organ, thereby improving the ML model's ability to make precise predictions. This technique not only mitigates the imbalance issue but also enhances the interpretability and applicability of the model in real-world kidney allocation scenarios.\nAfter censoring, the dataset achieves a balance where approximately 5% of the observations are acceptances. Table 10 presents the classification report for the model trained on the censored data. The accuracy for the acceptance class improves to 68.9%, a significant enhancement compared to the performance on the uncensored data. Additionally, Figure 3 illustrates an improved ROC curve along with a higher AUC score, indicating better discrimination ability.\nTable 11 presents the expected or observed NCS scores after applying data censoring. The baseline shows an average of 1.97 centers offered before the accepting center receives an offer, while the ALP framework reduces this to just 0.36 centers on average. This represents a significant improvement, demonstrating the effectiveness of the data censoring approach in enhancing the performance of the ALP framework."}, {"title": "4.4 Performance of the ALP Framework over Hard-to-Place Kidneys: KDRI Range Restriction", "content": "KDRI is a widely used metric that reflects the relative risk of post-transplant kidney graft failure from a specific deceased donor compared to a median reference donor . Lower KDRI scores are associated with longer estimated graft function, while higher KDRI scores are associated with shorter estimated function. For example, a kidney with a KDRI of 80% is expected to have shorter longevity than 80% of recovered kidneys. This detailed metric considers several donor factors including age, height, weight, ethnicity, history of hypertension or diabetes, cause of death, serum creatinine, hepatitis C virus status, and donation after cardiac death status.\nIn our previous analyses (see Section 4.2), we evaluate the performance of the ALP framework using the entire dataset without any restrictions on the KDRI range. However, as previously discussed, most transplant centers are willing to accept lower KDRI (higher quality), e.g., KDRI < 1.3, kidneys for at least some of their listed patients. Therefore, a center's likelihood of accepting a lower KDRI kidney is primarily influenced by dynamic, patient-specific factors rather than the center's overall offer acceptance characteristics. Thus, predicting a transplant center's acceptance likelihood of a lower KDRI kidney using empirical data is challenging due to the high variability in patient-specific factors, as illustrated by the model performances in Section 4.2. Additionally, since lower KDRI kidneys generally do not face significant risks of going unused, organ allocation interventions primarily target hard-to-place kidneys with relatively higher KDRI values. These higher KDRI kidneys are more challenging to allocate and are at a greater risk of non-use, making them the focus of efforts to improve organ utilization and reduce non-use rates.\nAlthough there is no consensus on the definition of hard-to-place kidneys in the literature, data indicates that kidneys with high KDRI values face a significantly higher risk of non-use compared to those with a lower KDRI. Accordingly, we refer to the set of kidneys with KDRI between 1.65 and 2 as hard-to-place kidneys, which are expected to be the primary targets of organ allocation interventions. We repeat the experiments previously outlined for this KDRI-restricted set of kidneys. Note that we use an upper bound of 2 on the KDRI of the selected set of kidneys because organs with KDRI > 2 suffer from extreme non-use rates. The lack of accepted kidneys in this KDRI range interferes with ML model learning and makes the already imbalanced prediction task even more challenging."}, {"title": "4.5 Interpretability", "content": "In this section, we discuss the methods used to enhance the interpretability of our ALP framework, ensuring that its predictions are not only accurate but also transparent and actionable. displays the SHAP beeswarm plot for the ALP framework predicting the centers' transplantation likelihood of hard-to-place kidneys. A beeswarm plot provides an information-rich summary of how the most significant features in a dataset influence the model's output. Each instance in the dataset is represented by a single dot on the respective feature row. The x-axis position of each dot corresponds to the SHAP value of that feature, and dots accumulate along each row to indicate density. The color of the dots represents the original value of the feature, adding an additional layer of insight into the feature's impact.\nIn , we observe that CIT emerges as the most impactful feature on the model's output, where higher CIT values significantly reduce the likelihood of acceptance, while lower CIT values increase it. This finding aligns with clinical understanding, as prolonged ischemia time is known to adversely affect kidney viability. Next in importance are the center-specific average accepted KDRI and the center's average acceptance rate. Both of these factors show a positive correlation with acceptance probability, indicating that transplant centers with a track record of accepting higher KDRI kidneys and those with higher overall acceptance rates are more likely to accept the offered kidney. This may suggest that such centers may have developed expertise and protocols to manage higher-risk kidneys effectively. Alternatively, this could suggest that centers with higher volumes of transplants may be able to accept an allograft failure without hurting their outcomes states. Additionally, the distance from the donor hospital to the center and the center's average accepted age display a negative correlation with acceptance probability"}]}