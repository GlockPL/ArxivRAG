{"title": "Imagine yourself: Tuning-Free Personalized Image\nGeneration", "authors": ["Zecheng He", "Bo Sun", "Felix Juefei-Xu", "Haoyu Ma", "Ankit Ramchandani", "Vincent Cheung", "Siddharth Shah", "Anmol Kalia", "Harihar Subramanyam", "Alireza Zareian", "Li Chen", "Ankit Jain", "Ning Zhang", "Peizhao Zhang", "Roshan Sumbaly", "Peter Vajda", "Animesh Sinha"], "abstract": "Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this\nresearch, we introduce Imagine yourself, a state-of-the-art model designed for personalized image\ngeneration. Unlike conventional tuning-based personalization techniques, Imagine yourself operates\nas a tuning-free model, enabling all users to leverage a shared framework without individualized\nadjustments. Moreover, previous work met challenges balancing identity preservation, following\ncomplex prompts and preserving good visual quality, resulting in models having strong copy-paste\neffect of the reference images. Thus, they can hardly generate images following prompts that require\nsignificant changes to the reference image, e.g., changing facial expression, head and body poses,\nand the diversity of the generated images is low. To address these limitations, our proposed method\nintroduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully\nparallel attention architecture with three text encoders and a fully trainable vision encoder to improve\nthe text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually\npushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the\nstate-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual\nquality, and text alignment. This model establishes a robust foundation for various personalization\napplications. Human evaluation results validate the model's SOTA superiority across all aspects\n(identity preservation, text faithfulness, and visual appeal) compared to the previous personalization\nmodels.", "sections": [{"title": "1 Introduction", "content": "Large scale diffusion models have drawn significant attention. These models, trained on vast amounts of\nimage-text pairs, showcase remarkable semantic understanding capabilities and are able to generate diverse,\nphoto-realistic images based on textual prompts. Due to their unparalleled creative abilities, large-scale\ndiffusion models have found applications across a spectrum of image-to-image tasks beyond the original\ntext-to-image generation, e.g., image editing, image completion, style transfer, and controllable generation.\nPersonalized image generation techniques have gained significant attention alongside large-scale diffusion models.\nThese methods focus on tailoring image generation to individual preferences or specific user characteristics.\nBy incorporating customization into the generation process, these techniques aim to create images that are\nmore relevant and appealing to the individual user. One line of research tunes a text-to-image model to\nincorporate the identity (Gal et al., 2022; Ruiz et al., 2023a,b) with a few reference images. However, these\nmethods are not efficient or generalizable as they require a different model to be tuned for each new user.\nRecently, another effort has been proposed to obtain personalized diffusion models without subject-specific\ntuning. This direction of research extracts vision embedding from a reference images and inject it to the\ndiffusion process (Wei et al., 2023; Li et al., 2023; Chen et al., 2023; Ye et al., 2023; Wang et al., 2024; Zhang"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-to-Image Diffusion Models", "content": "Text-to-image diffusion models represent a cutting-edge paradigm in the domain of deep learning, captivating\nresearchers with their capacity to translate textual descriptions into vibrant visual representations. At\ntheir core, these models operate through an iterative refinement process, wherein an initial noise vector is\nprogressively denoised based on text prompts, ultimately yielding the desired image output. A common\npractice is first translating images to a latent space and denoising in that space. Stable Diffusion (Rombach\net al., 2022) and its variants SDXL (Podell et al., 2023), Stable Diffusion Turbo (Sauer et al., 2023), and\nStable Diffusion-3 (Esser et al., 2024) follow this path by increasing the model size, distilling a large model to\nfewer denoise steps, and leveraging new transformer architectures, respectively."}, {"title": "2.2 Tuning-based Personalization Models", "content": "Diffusion-based personalized image generation has indeed garnered increased attention in recent times. This\napproach involves leveraging diffusion models to generate high-quality and personalized images based on a\ngiven input or set of inputs. Technically two streams of personalization models have been proposed. One\nstream of research tunes a text-to-image model to incorporate the identity. Textual Inversion (Gal et al.,\n2022) finetunes a special text tokens for the new identity. DreamBooth (Ruiz et al., 2023a) leverages a few\nimages from the same person as reference and a special text token to represent the identity. To accelerate\nthe finetuning process, LORA (Hu et al., 2021) only tunes a light-weight low-rank adapter rather than the"}, {"title": "2.3 Tuning-free Personalization Models", "content": "To overcome the limitations of the tuning-based method, another line of research focuses on one generalized\nmodel without identity-specific finetuning. This direction of work extracts vision embedding from the reference\nimage and injects it to the diffusion process. ELITE (Wei et al., 2023) extracts vision features from reference\nimage and converts it to the text-embedding space through a local and a global mapping. PhotoMaker\n(Li et al., 2023) merges the vision and text tokens and replaces the original text tokens for cross-attention.\nPhotoVerse (Chen et al., 2023) incorporates an image adapter and a text adapter to merge the vision and\nlanguage tokens, respectively. IP-Adapter-FaceID-Plus (Ye et al., 2023) leverages face embedding and clip\nvision encoder for identity preservation. InstantID is a control-based method that (Wang et al., 2024) adds\nControlNet (Zhang et al., 2023) to further control the pose and facial expression. MoA (Ostashev et al., 2024)\nproposes a mixture of attention architecture to better fuse the vision reference and the text prompts."}, {"title": "3 Method", "content": "Our proposed Imagine yourself takes a single face image of a specific subject, and generates visually appealing\npersonalized images guided by text prompts. Our method can follow complex prompt guidance and generate\nimages with diverse head and body poses, expressions, style, and layout.\nTo push the boundaries of personalized image generation, our approach begins by identifying three key facets\ncrucial to eliciting a satisfying human visual experience: identity preservation, prompt alignment, and visual\nappeal (Section 3.2). We then introduce novel techniques tailored to enhance each of these aspects. Specifically,\nwe propose a novel synthetic paired data generation mechanism (Section 3.3), new fully parallel architecture\nthat incorporates three text encoders and a trainable vision encoder for optimizing identity preservation\nand text-alignment (Section 3.4), a novel coarse-to-fine multi-stage finetuning methodology designed to\nprogressively enhance visual appeal, thereby pushing the visual appeal boundary of generated images. (Section\n3.5). Finally, we demonstrate Imagine yourself is generalizable to multi-subjects personalization in Section 3.6."}, {"title": "3.1 Preliminary", "content": "Text-to-Image diffusion models gradually turn a noise e to a clear image 20. While the diffusion process can\nhappen in the pixel space (Ramesh et al., 2022; Saharia et al., 2022), a common practice is to have latent\ndiffusion models (LDM) perform diffusion process in a latent space z = E(x0). During training, the LDM\nmodels optimize the reconstruction loss in the latent space:\nLdiffusion = Ez~E(x0),\u20ac~N(0,1) ||\u20ac \u2013 \u20ac\u0473(zt), t||2\nwhere Ldiffusion is the diffusion loss. ee represents the diffusion model. zt is the noised input to the model at\ntimestep t.\nIt is a common practice to use text or other condition signals C to guide the diffusion process. Thus, the\nconditioned diffusion process generates images following the condition signals. Usually, the text condition is\nincorporated with the diffusion model through cross-attention mechanism:\nAttn(Q, K, V) = softmax\\frac{QKT}{\\sqrt{d}}V\nwhere K = WKC, V = WC represents transforms that map the condition C to the cross-attention key and\nvalues. Q = WQ(xt) represents the hidden state of the diffusion model."}, {"title": "3.2 Overview", "content": "Figure 2 provides an illustration of the proposed model architecture. The key of using diffusion models\nfor personalized image generation is incorporating the reference identity as an additional control signal to\nthe diffusion model. We propose to extract the identity information from the reference image through a\ntrainable clip patch encoder. The identity vision signal is then added to the text signals through a parallel\ncross attention module. To better preserve the high visual quality of the foundation model, we leveraged\nlow-rank adapters (LoRA) to freeze the self-attention and text cross-attention modules while only fine-tuning\nthe adapters."}, {"title": "3.3 Synthetic Paired Data (SynPairs)", "content": "We observed that one critical issue during training is the use of unpaired data, i.e., the cropped image as input\nand the original image as target. It can introduce a severe copy-paste effect, making the model hard to learn\nthe true identity relationship between input and output more than duplicating the reference image. Thus, the\nmodel is not able to generate images that follow hard prompts, e.g., change expression or head orientation.\nTo this end, we proposed a new synthetic data generation recipe to create high-quality paired data (same\nidentity with varying expression, pose, and lighting conditions, etc.) for training. Compared to directly\nsourcing real paired data, which is not readily available, our study shows that curating the paired data\nsynthetically allows us to retain higher quality data to further enhancing several aspects of the Imagine\nyourself model.\nTo generate SynPairs data, we first obtain a dense image caption of the real reference image via a multi-modal\nLLM. The caption then flows through a caption rewrite stage based on Llama3 (Meta AI) to inject more\ngaze and pose diversity in the caption. The rewritten caption is then fed to a text-to-image generation tool\nsuch as Emu (Dai et al., 2023) as the prompt to produce a high quality synthetic images. Next, we refine the"}, {"title": "3.4 Model Architecture", "content": ""}, {"title": "3.4.1 Vision Encoder", "content": "We propose to use a trainable CLIP ViT-H patch vision encoder to extract the identity control signal from\nthe reference image. Unlike previous work that heavily relied on face embedding, we observed that a general\ntrainable vision encoder can provide adequate information to preserve the identity.\nTo further improve the identity preservation capability, we crop the face area and mask the corresponding\nbackground of the reference image to avoid the model attending to the non-critical areas, e.g., image\nbackground and non-face area in the cropped image. Figure 2 illustrates the vision embedding workflow. We\nalso proposed to use zero_conv as initialization to avoid adding noisy control signals at the beginning of\ntraining."}, {"title": "3.4.2 Text Encoders", "content": "We employ three distinct text-encoders: CLIP ViT-L (Radford et al., 2021) text encoder, UL2 (Tay et al.,\n2022), and ByT5 (Xue et al., 2022), as the text conditioning mechanisms. The selection of these encoders is\ndriven by their respective strengths and suitability for specific tasks. The CLIP text encoder, for instance,\nshares a common space with the CLIP vision encoder, facilitating enhanced identity preservation. To capitalize\non this alignment, we initialize the cross-attention module of the vision encoder with the pre-trained CLIP text\nencoder. Meanwhile, UL2 is specifically chosen for its proficiency in comprehending long and intricate text\nprompts, making it instrumental in handling complex input data. Furthermore, the ByT5 model is integrated"}, {"title": "3.4.3 Fully Parallel Image-Text Fusion", "content": "We investigated a parallel attention architecture to incorporate the vision and text conditions. Specifically,\nthe newly added vision condition from the reference image and the spatial features fuse through a new vision\ncross-attention module. The output of the new vision cross-attention module is then added to the text\ncross-attention output. In our experiments, this design better balances the vision and text control than\nconcatenating the text and vision controls."}, {"title": "3.4.4 LoRA", "content": "To preserve the visual quality from the foundation model, we leveraged low-rank adapters (LoRA) on top\nof the cross-attention module. The self-attention and text cross-attention modules in the foundation Unet\nare frozen. We observed that this design not only better preserves the foundation model's image generation\ncapability, but also accelerates the convergence speed by up to 5x."}, {"title": "3.5 Multi-Stage Finetune", "content": "We propose a multi-stage finetuning with interleaved real and synthetic data that help us achieve the best\ntrade-off between editability and identity preservation. In the first two stages, we leverage large-scale data\n(nine millions) to pretrain the model to be able to condition on a reference identity. For the later stages, we\nfinetune our pretrained checkpoint with high-quality, aesthetic images collected through Human-In-The-Loop\n(HITL). Empirically, we found training with real images gives the best identity preservation, while training\nwith synthetic images gives better prompt alignment (editability). Synthetic images are generated from its"}, {"title": "3.6 Extension to Multi-Subject Personalization", "content": "The previously introduced fully parallel image-text fusion pipeline (Section 3.4.3) can be flexibly extended to\naccommodate multi-subject personalization. In the two-person scenario for example, instead of passing the\nglobal embedding and patch embedding of the single reference image into the K and V components as shown\nin the top left branch of Figure 4, we can concatenate the vision embedding from both reference images and\npassing it into the K and V components. Given this setup, through training, the network learns how to map\nfrom reference, to subject; in the group photo while generating prompt-induced image context accordingly.\nSome examples of the two-person personalization results are shown in Figure 11."}, {"title": "4 Experiments", "content": "In this section, we perform both qualitative and quantitative evaluations of our model. We also compare our\nmodel to the SOTA personalization models. Results show that our model outperforms the existing models on\nall axes setting the new state-of-the-art."}, {"title": "4.1 Qualitative Evaluation", "content": "We show examples of our model generated image in Figures 6-10. Our model generates visually appealing\nimages that both preserve the identity and follow the prompt faithfully."}, {"title": "4.2 Quantitative Evaluation", "content": ""}, {"title": "4.2.1 Evaluation Dataset", "content": "To quantitatively evaluate Imagine yourself, we created an evaluation set consisting of two parts: (i) reference\nimages, and (ii) eval prompts. To have a comprehensive comparison in all representative cases, we collected\na total of 51 reference identities covering different gender, race, and skin tone. We created a list of 65\nprompts to evaluate the model. It widely covers a wide range of usage scenarios, and also including hard\nprompts that require face expression or pose changes, camera motions, and stylization. These prompts help\nto assess the model's ability to engage in more complex and nuanced interactions, diverse pose generation,"}, {"title": "4.2.2 Benchmarked Methods", "content": "We benchmarked the SOTA adapter-based personalization model and the SOTA control-based model. For\nthe adapter-based method, we select the best one that strikes the best balance among visual appeal, identity\npreservation, and prompt alignment, the three axes that we evaluate our models on. For control-based method,\nwe noticed that the choice of the pose image plays an important role in how the final generated image is\ncomposed, i.e., for some prompts, a carefully chosen pose image can make the generated images look better\nor worse. For a fair comparison, we use the reference image itself as the pose condition."}, {"title": "4.2.3 Human Evaluation", "content": "To evaluate the quality of the generated images, we conducted a large-scale annotation process that assessed\nvarious aspects of the images. We used human annotation as the gold standard to assess the model's\nperformance (standalone evaluation) and compare it with other models (head-to-head evaluation).\nIn the standalone evaluation, we presented annotators with the input image, prompt, and a generated image\nand asked them three questions to rate on a scale of Strong Pass / Weak Pass / Fail. (1) Identity Similarity:\nDoes the subject in the output image appear to have the same identity as the subject in the original image?\n(2) Prompt Alignment: Does the output image follow the personalization prompt faithfully? (3) Visual Appeal:\nIs the output image visually appealing? In the head-to-head model evaluation, we compared one model against\nanother on the same three axes.\nAs shown in Table 1, Imagine yourself outperforms the two state-of-the-art methods adapter-based model and\ncontrol-based model by a significant margin in most axes. Specifically, Imagine yourself is significantly better\nin prompt alignment, with a +45.1% and +30.8% improvement over the SOTA adapter-based model and\nthe SOTA control-based model, respectively. However, we observed that the control-based model is better in\nidentity preservation than Imagine yourself, due to its hard copy-pasting of the reference image at the center\nof the image, resulting in unnatural images despite the high identity metric."}, {"title": "4.3 Ablation Study", "content": "In our ablation study, we examined the effectiveness of various components within our proposed Imagine\nyourself. Main ablation results are shown in Table 2."}, {"title": "4.3.1 Impact of Multi-stage Finetune", "content": "The ablation results highlight the impact of multi-stage fine-tuning. Reducing the multi-stage fine-tuning to\na single stage significantly degrade all metrics, especially 25.5% in prompt alignment and 42.0% in visual"}, {"title": "4.3.2 Impact of Fully Parallel Attention", "content": "We ablate removing the full parallel attention to a standard token concatenate design to show the impact of the\nfully parallel attention architecture. We observed that all metrics, specifically 5.2% in prompt alignment, 1.4%\nin identity preservation, and 22.0% in visual appeal, respectively. This shows the importance incorporating all\nthree text encoders and the vision encoder through fully parallel attention."}, {"title": "4.3.3 Impact of Synthetic Pairs", "content": "SynPairs increase the diversity of the generated images by eliminating the copy-paste effect. Our ablation\nverifies this assumption and demonstrate better prompt-alignment compared to the model without synthetic\npaired training. We observed that it is especially effective for the complex prompts that require strong changes\nto the original images, e.g., expression change, covering the face, or turning head, etc. However, we observed\na regression in identity preservation with SynPair training because the faces in the corresponding reference\nand target pair are not exactly the same. Future work will focus on improving the face similarity of SynPair\ntraining data."}, {"title": "5 Future Work", "content": "We would like to continue research and explore the following directions: 1) extend personalized image to video\ngeneration. The key is to consistently preserve the identity and scene in video generation. 2) While Imagine\nyourself has improved the prompt-alignment against existing models, we observed that it still has limitation\nin following prompts describing very complex poses, e.g., jumping from a mountain. Future work will focus\non improving the generated images' quality on these prompts."}, {"title": "6 Conclusion", "content": "In this study, we introduce Imagine yourself, a pioneering model tailored for personalized image generation.\nUnlike traditional tuning-based approaches, Imagine yourself operates as a tuning-free solution, offering\na shared framework accessible to all users without the need for individual adjustments. Imagine yourself\novercomes the prior research limit in handling the intricate balance between preserving identity, following\ncomplex prompts, and maintaining visual quality by introducing 1) a novel synthetic paired data generation\nmechanism to foster image diversity, 2) a fully parallel attention architecture featuring three text encoders\nand a fully trainable vision encoder to enhance text faithfulness, and 3) a novel coarse-to-fine multi-stage\nfine-tuning methodology to progressively enhance visual quality. We perform large-scale human evaluation\non thousands of examples and showcase that Imagine yourself outperforms state-of-the-art personalization\nmodels, demonstrating superior capabilities in identity preservation, visual quality, and text alignment."}]}