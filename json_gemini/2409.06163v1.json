{"title": "MCDGLN: Masked Connection-based Dynamic Graph Learning Network for Autism Spectrum Disorder", "authors": ["Peng Wang", "Xin Wen*", "Ruochen Cao", "Chengxin Gao", "Yanrong Hao", "Rui Cao*"], "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by complex physiological processes. Previous research has predominantly focused on static cerebral interactions, often neglecting the brain's dynamic nature and the challenges posed by network noise. To address these gaps, we introduce the Masked Connection-based Dynamic Graph Learning Network (MCDGLN). Our approach first segments BOLD signals using sliding temporal windows to capture dynamic brain characteristics. We then employ a specialized weighted edge aggregation (WEA) module, which uses the cross convolution with channel-wise element-wise convolutional kernel, to integrate dynamic functional connectivity and to isolating task-relevant connections. This is followed by topological feature extraction via a hierarchical graph convolutional network (HGCN), with key attributes highlighted by a self-attention module. Crucially, we refine static functional connections using a customized task-specific mask, reducing noise and pruning irrelevant links. The attention-based connection encoder (ACE) then enhances critical connections and compresses static features. The combined features are subsequently used for classification. Applied to the Autism Brain Imaging Data Exchange I (ABIDE I) dataset, our framework achieves a 73.3% classification accuracy between ASD and Typical Control (TC) groups among 1,035 subjects. The pivotal roles of WEA and ACE in refining connectivity and enhancing classification accuracy underscore their importance in capturing ASD-specific features, offering new insights into the disorder.", "sections": [{"title": "I. INTRODUCTION", "content": "Autism spectrum disorder (ASD) constitutes a group of widely prevalent neurodevelopmental disorders that alter in-dividuals' autonomous behaviors and social interactions, typ-ically entailing implications that persist throughout a lifetime. Early intervention is pivotal in alleviating the impact of ASD"}, {"title": "II. METHOD", "content": "The comprehensive architecture of MCDGLN introduced in this study is depicted in Fig. 1. The architecture of the MCDGLN model begins with the input of resting-state fMRI (rs-fMRI) data, from which dynamic functional connectivity (dFC) is computed using pairwise Pearson Correlation Coef-ficients within sliding windows. These dFCs are aggregated through a weighted edge module to create task-specific func-tional connectivity (tsFC). A mask generated from tsFC is applied to static functional connectivity (sFC) to refine it. The model then constructs graph data from both sFC and tsFC, using a hierarchical graph convolutional network (HGCN) to extract graph-level embeddings. An attention module high-lights key features, and the static and dynamic outputs are combined. The integrated embeddings are finally used for classification predictions."}, {"title": "B. Problem definition", "content": "1) The acquisition of dynamic functional connectivity: In this study, rs-fMRI data are analyzed as multivariate time series. Specially, the variable $x_i \\in \\mathbb{R}^{(M\\times T)}(i = 1, 2, ..., N)$ encapsulates the time series of BOLD signals from M distinct ROIs for an individual subject, with T representing the time series length and N denoting the total number of subjects. These time series are segmented using the sliding time window technique, which facilitates the computation of correlation ma-trices for each segment, hence producing a series of dynamic functional connectivity matrices, denoted as $c_i$, for every subject. The widely adopted Pearson Correlation Coefficient is employed to quantify the correlations. The Pearson Correlation Coefficient is calculated as follows:"}, {"title": "C. Weighted Edge Aggregation", "content": "The dFC obtained from the sliding time window $c_i(i = 1,2,..., N)$ will be used as an input to WEA. As shown in Fig. 1(c), the first step is the aggregation of edge information through several cross convolutional layers, which comprised of channel-wise element-wise kernels, and the goal of this process is to fully consider the edge-to-edge relationship. As shown in Fig. 2, the special feature of the cross convolution operation here is that it applies a channel-wise and element-wise convolutional kernel that captures the dynamic properties in multiple functional connections more efficiently. After up-dating the FC information through the aggregation process, the global fusion is used to incorporate the static features into the tsFC generation process, to integrate dynamic and static traits into the tsFC, which is the final output of WEA and the input of HGCN.\nSpecially, suppose $v_i, v_j$ are two ROI nodes with correla-tion, and the edge connecting them is denoted as $e_{ij}$, then the weighted aggregation process of edges can be expressed as follows:\n$e_{ij}^{l,c} = \\sum (e_{im}^{l,c} w_{im}^{l,c} + e_{nj}^{l,c} w_{nj}^{l,c})$"}, {"title": "D. Masked Edge Drop", "content": "The problem of noise in FC can be very disruptive to the usage of this data for classification task and diagnosis. In this section, a mask that changes with the downstream task is generated based on the tsFC and is applied to remove redundant connections in the sFC to filter out valid connections that are only relevant to the downstream task, i.e., valid edges between ROIs.\nFig. 1(a) illustrates the process of dropping masked edges. A binary mask matrix, mirroring the dimensions of $s_i$, is formu-lated from the tsFC $m_i$ that WEA has produced. By calculating the Hadamard product between the mask matrix and $s_i$, we effectively filter out superfluous connections involved in $s_i$ while preserving pertinent information.\nIn calculating the mask matrix according to $m_i$, the basis is the following formula:\n$f_m(m) = \\begin{cases} 1 & \\text{if } m\\neq 0 \\\\ 0 & \\text{if } m = 0 \\end{cases}$\nThe output of MED can be formulated as: $s \\odot f_m(m)$, and it is going to be the input of next part."}, {"title": "E. Hierarchical Graph Convolutional Network", "content": "The graph convolution layer is designed to extract the topo-logical features from the input graph data, denoted as G, aiding in the differentiation of various inputs. In this hierarchical structure, the graph convolutional module processes G through multiple graph convolutional blocks to capture these features. To address the challenge of over-smoothing, each block incor-porates a residual connection. Following this, a self-attention (SA) module highlights important features and generates an attention vector $a_i$ that quantifies their significance. This process results in a graph-level feature representation, denoted as $y$. The overall workflow is illustrated in Fig. ??(b).\nWithin our framework, we directly incorporate the graph convolution block (GCN block) as delineated in [22]. This block iteratively refines node features across successive layers via a message-passing mechanism. For a node v belonging to the vertex set V, with N(v) denoting its neighboring nodes, the message-passing algorithm is mathematically formulated as:\n$h_v^{(k+1)} = \\phi_\\theta^{(k)} (h_v^{(k)}, \\Box_{u \\in N_v} (\\psi_\\theta^{(k)} (h_u^{(k)})))$\nWhere k represents the number of layers of the network, $\\psi_\\theta^{(k)}$ and $\\phi_\\theta^{(k)}$ are the trainable functions of the k-th layer, which are used to perform the spatial mapping of the vectors. And the convolution principle of GCN written in matrix form can be expressed as follows:\n$X^{(k+1)} = \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X^{(k)} \\Theta^{(k)}$"}, {"title": "III. EXPERIMENT", "content": "In our validation efforts, the framework was applied to the widely recognized Autism Brain Imaging Data Exchange I (ABIDE-I) dataset, a prominent and globally utilized open repository for research on autism spectrum disorder (ASD) and Typically Developing (TD) individuals. ABIDE-I encompasses data from 1112 participants spanning 17 international sites. Profiles for each participant include T1-weighted structural MRI (sMRI), resting-state functional MRI (rsfMRI), and phe-notypic information alongside verifiable diagnostic labels. Our study specifically engaged 1035 subjects, consisting of 505 with ASD and 530 TD. To ensure reproducibility and enable equitable comparisons, the investigation leveraged the rsfMRI data subjected to a standardized preprocessing protocol by the Configurable Pipeline for the Analysis of Connectomes (CPAC) via the Preprocessed Connectomes Project (PCP) [23]. This preprocessing regimen encompassed steps such as exocranial tissue exclusion, temporal alignment of image slices, motion stabilization, global signal intensity equaliza-tion, confounding signal mitigation, and frequency filtering within the range of 0.01-0.1 Hz. Following preprocessing, rsfMRI images were conformed to the MNI152 brain template. The cortical regions of the cerebrum were partitioned into 200 and 116 distinct zones under the Brain Atlas CC200 [24] and AAL [25], from which a region-specific BOLD signal time series was obtained through voxel-wise averaging."}, {"title": "B. Experimental setup", "content": "1) Comparative study: We compare the proposed MCDGLN with the following eight machine learning methods and deep learning models:\nstandard GNN: vanilla GCN (v-GCN) [22]. We built a model of a three-layer GCN, constructing the graph structure in such a way that the PCC matrix is used as the adjacency matrix, and each row corresponding to the ROI is used as the feature embedding of the node. The reported results show the best performance of the model.\nGIN [11]. We built a model of a three-layer GIN, and MPL is used as the general mapping function.\nBrainGNN [16]. We used the original implementation provided in using the partial correlation parameter matrix as the adjacency matrix and the PCC matrix as the feature construction graph data of the nodes. The other training parameter settings remained the same as in the original paper.\nDGCN [26]. We used the original implementation pro-vided in using the functional connectivity matrix as the adjacency matrix, using the PCC matrix as the feature-constructed graph data of the nodes. Other training pa-rameter settings remained consistent with the original paper.\nGATE [27], MVS-GCN [28], STW-HCN [29]. This paper directly uses the results provided in the original paper."}, {"title": "C. Metrics", "content": "Our model's performance underwent assessment via a 10-fold cross-validation process. The entirety of the dataset was randomized into 10 equally sized subsamples. In each iteration of the evaluation, one subsample was reserved as the validation set for testing the model, and the remaining nine subsamples were aggregated as the training set. This sequence was exe-cuted repeatedly until each subsample had served once as the validation data.\nTo gauge the efficacy of varying methodologies, we imple-mented four evaluative metrics: classification accuracy (ACC), precision (PRE), F1 score (F1), and the area under the subject operating characteristic curve (AUC)."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The aggregated classification outcomes for each method on our dataset are encapsulated in TABLE I. The MCDGLN model introduced in our study registers the highest mean scores across accuracy, AUC, precision, and F1 measures. With an accuracy that exceeds the next-best model, DGCN, by 2% and outstrips other machine learning techniques by 3%-6%, our model demonstrates optimal classification capa-bilities. In the specific domain of precision, our MCDGLN leads BrainGNN\u2014the second highest performing model-by 3.3% and surpasses alternative models by a margin of 7%-11%, showcasing superior discriminative power for negative samples. On the F1 score front, MCDGLN equates with GATE and consistently outpaces other competitors by 1%-9%, reflecting the model's robust performance. For the AUC metric, MCDGLN surpasses the runner-up, DGCN, by 2.6%, underlining an overall efficient predictive capacity. Notably, our comparative analysis is grounded on the ten-fold cross-validation method, contrary to certain preceding studies [30], [31] that have only put forth single-partition dataset outcomes, thus not fully capturing model reliability.\nIn an overall assessment, deep learning frameworks, such as BrainGNN, VGCN, and DGCN, demonstrate superior efficacy over classical machine learning paradigms like SVM across varied evaluative criteria. Specifically, within the cohort of deep learning methodologies, both BrainGNN and DGCN showcase enhanced performance when compared to the static graph convolution approach of vGCN. This underscores the critical impact of dynamic feature capture in the classification process for ASD."}, {"title": "B. Results of ablation study", "content": "To discern the individual contributions of each mod-ule within MCDGLN\u2014namely, the Weighted Edge Ag-gregation (WEA), the Attention-based Connection Encoder (ACE), and the Hierarchical Graph Convolutional Network (HGCN)\u2014ablation studies were conducted. The outcomes of these studies are detailed in Fig. 4. The complete, unal-tered MCDGLN configuration delivered superior performance across every measured metric, including accuracy, precision, sensitivity, F1 score, and AUC. Notably, the removal of the ACE module resulted in the greatest decrement in metric scores, thereby highlighting its critical role in bolstering the model's efficiency. This can indicate that the masking mech-anism and the attention vectors formed based on the attention mechanism suppress the noise in the connection information well and enhance the representation of key features.\n1) Impact of WEA's layers: As shown in Fig. 5, both panels suggest that the Precision metric is most responsive to the number of WEA layers, peaking at the third layer in both cases. The F1 Score consistently shows a dip at the second WEA layer, indicating potential issues in model performance at this configuration. Accuracy and AUROC metrics remain relatively stable across different WEA layers, highlighting their robustness to changes in the WEA layer configuration.\n2) Impact of HGCN's layers: As shown in Fig. 6, we can observe trends in performance metrics as the number of layers increases. For instance, in the left plot, both accuracy and AUROC seem to improve gradually with an increasing number of residual HGCN layers; conversely, in the right plot, both \"Precision\" and \"F1\" exhibit a rising-falling pattern as the number of HGCN layers varies. Meanwhile, the best number of layers is influenced by the atlas."}, {"title": "C. Functionality of the WEA", "content": "This study aims to demonstrate the WEA module's effec-tiveness and establish its empirical validity by conducting two-sample t-tests on functional connectivity datasets. These tests specifically compare the contrasts in connectivity before and after implementing the WEA module. As illustrated in Fig. 7 and discussed previously, the terms sFC (static functional connectivity) and tsFC (temporal static functional connectiv-ity) describe connectivity patterns, with \"overlap\" indicating identical connections in both sFC and tsFC. Regardless of the chosen atlas, the majority of abnormal connections iden-tified in sFC are retained in tsFC produced by the WEA module, as evidenced by the extent of overlap. This overlap substantiates the WEA module's efficacy. Furthermore, even without bootstrapping, tsFC created by the WEA module for dynamic feature extraction mirrors sFC characteristics, thereby maintaining the integrity of effective features while also achieving noise reduction."}, {"title": "V. CONCLUSION", "content": "In this paper, the MCDGLN model is proposed to extract the dynamic properties of fMRI data thus contributing to the diagnosis of ASD disease. Initially, the model constructs a task-specific functional connectivity network via a weighted edge aggregation. Following this, two critical processes are executed to delineate the network's topological nuances and to eliminate superfluous elements within the original functional connectivity network. The MCDGLN model achieves an ac-curacy of 73.3% accuracy, obtaining better results compared to other models. Subsequent experimental results also demon-strate the effectiveness of the proposed module to enhance critical features in functional connectivity and to achieve the goal of noise removal."}]}