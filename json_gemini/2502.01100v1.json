{"title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning", "authors": ["Bill Yuchen Lin", "Ronan Le Bras", "Kyle Richardson", "Ashish Sabharwal", "Radha Poovendran", "Peter Clark", "Yejin Choi"], "abstract": "We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.\nOur results reveal a significant decline in accuracy as problem complexity grows\u2014a phenomenon we term the \"curse of complexity.\" This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.", "sections": [{"title": "1. Introduction", "content": "Logical reasoning stands as a cornerstone of human intelligence and remains a central challenge in AI. While recent advances have demonstrated promise in tasks requiring common sense and general knowledge (Brown et al., 2020; Chowdhery et al., 2022; Bubeck et al., 2023), the capabilities of Large Language Models (LLMs) in handling complex deductive problems remain uncertain. This limitation in our understanding is especially critical as systematic reasoning underpins many real-world applications. To systematically study LLMs' logical reasoning capabilities and their scaling limits, an ideal evaluation framework must: (1) isolate pure logical reasoning from domain knowledge; (2) enable precise control over problem complexity; (3) minimize data leakage to prevent training data memorization; (4) provide objective metrics for assessing an LLM's reasoning results.\nConstraint satisfaction problems (CSPs) offer such a controlled framework (Dechter, 2003): they are mathematically well-defined, scalable in both complexity and search space, and have solutions that can be automatically verified. By formulating logical tasks as CSPs, we can rigorously evaluate how well LLMs adhere to logical constraints, independent of domain-specific data or heavy numerical computation. As a representative class of CSPs, logic grid puzzles (specifically Zebra Puzzles or Einstein's Riddle, (Prosser, 1993)) are particularly suitable as they require pure formal reasoning, remain accessible enough to serve as an effective testbed, and embody core skills relevant to real-world applications such as task planning, scheduling, and resource allocation. Hence, we introduce ZebraLogic, an evaluation framework for creating logic puzzles with controllable, and quantifiable complexity, thus improving our understanding on the scaling limits of LLMs including Llama (AI@Meta, 2024), o1 (OpenAI, 2024) and R1 (DeepSeek-AI, 2025).\nThrough extensive evaluation of various LLMs across diverse architectures and sizes, we observe a dramatic decline in performance as puzzle complexity increases-a phenomenon we term the \u201ccurse of complexity for reasoning.\" Most models struggle once the puzzle's search space exceeds $10^7$ possibilities (e.g., for puzzles with 4x5 grid size) or when the number of logical conflicts in a widely used SMT solver named Z3 (de Moura & Bj\u00f8rner, 2008) surpasses 20. These findings suggest that limited reasoning in current LLMs are not solely a matter of model- or"}, {"title": "2. Problem Formulation of Logical Reasoning", "content": "Constraint Satisfaction Problems (CSPs) provide a powerful framework for modeling and solving logical reasoning tasks. In CSPs, solutions must satisfy a set of constraints over variables and their possible values. This framework is particularly valuable for evaluating systematic reasoning capabilities, as it requires explicit handling of logical relationships and dependencies. We leverage this framework through logic grid puzzles in our ZebraLogic dataset to assess LLMs' deductive reasoning abilities.\n2.1. Logic Grid Puzzles\nEach puzzle in ZebraLogic consists of N houses (numbered 1 to N from left to right) and M different attributes for each house. There are N distinct values for each attribute, and each house must have a unique value for each attribute. Given a list of K clues, one must use logical deduction to"}, {"title": "2.2. Problem Formulation", "content": "We provide a detailed mathematical formulation of logic grid puzzles as a CSP. This formulation not only clarifies the underlying structure of the puzzles in ZebraLogic but also highlights how our study can be generalized to various reasoning problems. The example shown in Fig. 2 illustrates this formulation.\nBackground. Consider N houses numbered 1 to N. Each house has a different occupant with a set A of M unique attributes such as name, favorite drink, hobby, etc. Each attribute $a \\in A$ represents a category of characteristics and takes values in a set $V_a$ of N possible values. For example, for the attribute Name, we might have $V_{Name} = \\{Eric, Peter, Arnold\\}$ in a puzzle with N = 3 houses. As illustrated in Fig. 2, other attributes might include Drink with values like milk, water, and tea, or Hobby with values like photography, cooking, and gardening. To model the puzzle as a Constraint Satisfaction Problem, we define variables representing the assignment of values to attributes for each house.\n\u2022 Let $H = \\{1, 2, 3, ...\\}$ be the set of houses, $|H| = N$.\n\u2022 Let $A = \\{Name, Drink,... \\}$ be the set of attributes, $|A| = M$.\n\u2022 Define $x_{a,k} \\in V_a$ for each attribute $a \\in A$ and house $k \\in H$.\nUniqueness Constraints: The constraints ensure that each value is assigned exactly once, as described in the Background part in Figure 2. For each attribute, the set of assigned values across all houses must exactly match the set of possible values. That is: $\\{x_{a,k} | k \\in H\\} = V_a$.\nClue-Based Constraints: Each clue in the puzzle introduces additional constraints that must be satisfied by any valid assignment. Under the hood, these clues are translated into formal logic formulas that constrain the relationships between variables. For our example puzzle in Figure 2, the constraints can be formulated as follows:\nTask. The task is to find an assignment of attributes to"}, {"title": "2.3. ZebraLogic Dataset Creation", "content": "To create puzzles, we first define a set of attributes and their corresponding value sets. We also establish some clue types, each with its own language templates containing placeholders for values.\nAttributes and Values. We construct the attribute set A, which includes the many elements (see Appendix B). Each attribute is associated with a minimum of 6 possible values, ensuring a rich and diverse set of puzzles. Importantly, we always include the Name attribute in our samples, as it serves as a crucial element in the puzzle-solving process.\nClue Types. The possible clue types are categorized into several types, including FOUNDAT, SAMEHOUSE, NOTAT, DIRECTLEFT/RIGHT, SIDEBYSIDE, LEFT/RIGHTOF, and ONE/TWOBETWEEN. Each clue type captures a specific relationship between variables, providing a diverse set of constraints for the puzzles. More details are in Appendix B."}, {"title": "Task Generation Algorithm", "content": "Algo. 1 outlines our approach for generating ZebraLogic puzzles. The process starts by sampling M attributes from the full attribute set and creating an initial solution grid S through random value assignments. From this solution, we generate a comprehensive set of clues C that capture all valid relationships between values in the grid. The algorithm then employs an iterative minimization procedure - at each step, it randomly samples a clue $p \\in C$ and attempts to remove it. Using a SAT solver, it verifies whether the reduced clue set $C' = C \\setminus \\{p\\}$ still uniquely determines the original solution S. If uniqueness is preserved, p is permanently removed and the process continues. This iteration terminates when no any additional clue can be removed without augmenting the solution space."}, {"title": "2.4. Theoretical Problem Complexity", "content": "By reduction from the Quasigroup (or Latin square) Completion Problem (QCP) (Colbourn, 1984; Gomes & Shmoys, 2002), the ZebraLogic problem is proven to be NP-complete (Sempolinski, 2009). While the problem definition includes a rich set of clue types that can be further expanded, a sufficient condition for the NP-completeness result is to at least include the FOUNDAT and NOTAT clue types. As a result, while a solution to a ZebraLogic puzzle can be easily verified, solving ZebraLogic puzzles for large instances may become intractable within reasonable time frames using current computational methods. This implies that, for a fixed LLM size, the required number of reasoning tokens may"}, {"title": "2.5. Measuring Effective Instance Complexity", "content": "Search space size. We define the solution space of a ZebraLogic puzzle as the total number of possible configurations that can satisfy the uniqueness constraints of the puzzle. That is, a $N \\times M$ grid has a solution space of $(N!)^M$, where N is the number of houses and M is the number of attributes. The complexity of the search space increases factorially with the size of the grid, leading to a combinatorial explosion in the number of possible configurations. To better group the puzzles based on their complexity, we categorize them into four groups based on the size of the search space $|S|$:\nZ3 conflicts. While search space size provides a useful measure of puzzle scale, it is not the only indicator of complexity. To complement it, we also use the Z3 SMT solver's conflict metric. Z3 (de Moura & Bj\u00f8rner, 2008) uses the Conflict Driven Clause Learning (CDCL) algorithm, a backtracking approach based on the DPLL (Davis-Putnam-Logemann-Loveland) algorithm. When solving a puzzle, Z3 records the number of conflicts encountered - situations where the solver must backtrack due to contradictions in its current assignment. We run Z3 on each puzzle for 32 times and take the average number of conflicts as a measure of complexity. Puzzles with zero conflicts can typically be solved through simple forward chaining, whereas puzzles with more conflicts require extensive backtracking, indicating higher logical complexity.\nWhile search space size captures the number of candidate assignments (given uniqueness constraints), Z3 conflicts quantify the solver's difficulty in reaching a valid solution. Together, these metrics offer a complementary view of how the difficulty of the puzzles scales with the problem size. Appendix B provides additional details on how these two metrics vary as a function of the puzzle parameters (N, M)."}, {"title": "3. Evaluation", "content": "Setup and Metrics. Our evaluation is done in a one-shot in-context learning setting, where we provide the models with a single example of how to solve a ZebraLogic puzzle and present the solution in JSON format, and we instruct the LLMs to output their reasoning and solution in the same format, thus making it easier to parse and evaluate their answers. We mainly look at the puzzle-level accuracy, mean-"}, {"title": "3.1. Main results", "content": "Table 1 shows the performance of various models. o1 outperforms all other models, achieving an overall accuracy of 81.0%, and DeepSeek-R1, an open-weight reasoning LLM achieves 78.7%, with a slightly better performance on Small and Medium-size puzzles than o1-full. However, R1's performance on Large and X-Large puzzles is worse than o1-full. ol-preview and o1-mini achieve 71.4% and 59.7% accuracy, respectively. In contrast, the best-performing open-weight non-reasoning LLM, Sonnet-3.5-1022, only reaches 36.2%. The performance gap is even more pronounced in larger search spaces, where O1-Preview maintains a 17.0% accuracy in the X-Large category, while other models struggle to achieve any correct solutions.\nWe find that our ranking and scoring of these models are aligned with other reasoning benchmarks such as MATH (Hendrycks et al., 2021) for mathematical reasoning and LiveCodeBench (Jain et al., 2024) for competitive programming. This suggests that the logical reasoning ability of LLMs is highly correlated with their performance on other types of reasoning tasks."}, {"title": "3.2. Curse of Complexity in Reasoning with LLMs", "content": "We observe that the performance of LLMs drops significantly as the search space size increases, as shown in Fig. 1 and Fig. 3 (in Appendix). We find that for models that are overall worse than GPT-40-mini can hardly solve puzzles beyond the Small category less than 5% accuracy in Medium-size puzzles and almost no correct solutions in Large and X-Large puzzles. We can see that even the largest open-weight LLM, Llama-3.1-405B, only achieves 32.6% overall accuracy. Although 405B has 22.5% accuracy in Medium-size puzzles, it quickly also drops to 1.5% in the Large category and 0.0% in the X-Large category.\nThe best non-reasoning LLM, Sonnet 3.5, has 36.2% accuracy in the overall evaluation, but it also drops to 4.0% in the Large category and 1.0% in the X-Large category. This indicates that the logical reasoning tasks in ZebraLogic are extremely challenging for LLMs, especially for puzzles"}, {"title": "3.3. Scaling Behavior of LLMs in Logical Reasoning", "content": "In the following sections, we study the scaling behavior of LLMs in logical reasoning, as illustrated in Fig. 1. Our analysis focuses on two primary types of scaling: 1) scaling model size and 2) scaling test-time compute. For test-time compute, we further explore three sub-dimensions: 1) the number of candidate samples, 2) the number of reasoning tokens (i.e., CoT tokens) generated during inference, and 3) the sample size for repeated sampling."}, {"title": "4. Scaling Model Size Can Hardly Break the Curse of Complexity in Reasoning", "content": "The Curse of Complexity in Reasoning for non-reasoning LLMs. In addition to the search space size, we also use Z3-conflict as the complexity measure to study the scaling behavior LLMs. Fig. 1 (left) highlights a key observation regarding the performance of various Llama models with different model sizes across an increasing complexity in terms of how many Z3 conflicts on average are encountered when solving the ZebraLogic puzzles. A notable finding is that all model sizes experience a rapid decline in accuracy as the complexity increases, illustrating the challenge posed by complex reasoning tasks. This trend emphasizes the inherent difficulty models face in maintaining high accuracy beyond a certain threshold of search complexity, irrespective of their size. The phenomenon termed as the \"curse of complexity\" becomes evident as even the largest models, such as the Llama-3.1-405B, cannot sustain high accuracy once the search space surpasses a certain scale. As shown in Fig. 3, we see a similar trend in the search space size.\nScaling model size is only effective for smaller search spaces. However, it is important to note the significant benefits of scaling model size when the search space is relatively small (e.g., \u2264 106). In these cases, larger models like the Llama-3.1-405B and Llama-3.1-70B demonstrate substantial improvements in accuracy compared to smaller models such as the 3B and 8B versions. This suggests that scaling up the model size is an effective strategy for enhancing performance and tackling reasoning tasks in simpler search spaces. Yet, as the complexity of the search space grows beyond 106, the advantages of larger model sizes diminish, and scaling up the model size proves to be less impactful. This finding underscores the limited utility of model scaling when dealing with highly complex reasoning tasks, as the accuracy plateaus regardless of model size.\nModel Size Scaling Limitations. This analysis reveals that"}, {"title": "5. Scaling Test-Time Compute with Repeated Sampling: Promises & Challenges", "content": "We examine the impact of scaling test-time compute, a crucial factor affecting LLM performance on logical reasoning tasks. Specifically, here we investigate how increasing the number of candidate samples influences model performance. We begin by employing Best-of-N (BoN) sampling, where we repeatedly sample N candidates from the model for each puzzle. From these candidates, we can select the best answer using various strategies, including majority voting and existing reward models. To understand the theoretical upper bound of this approach, we also analyze BoN sampling with oracle selection, where we use knowledge of the correct answer to choose the best candidate from the sample pool - equivalent to the pass@k metric in our evaluation (see the right-most plot in Fig. 1 and Fig.3).\nGPT-40 with Best-of-N sampling and oracle selections can achieve nearly o1 performance. To understand the potential improvement of scaling test-time compute for logical reasoning, we sample 128 candidates from GPT-40-mini and GPT-40 and study the coverage of the correct answer in the sampled candidates. In Table 2, we refer to this coverage metric as BoN-Oracle, meaning that the best-of-N (BON) selection is performed given the oracle knowledge of the correct answer, i.e., the pass@k metric.\nWe observe that the BoN-Oracle selection can significantly improve the performance of GPT-40-mini and GPT-40. For example, GPT-40 with BoN-OracleN=128 achieves an overall accuracy of 69.1%, which is higher than O1-mini's accuracy of 59.7% and a potential scaling effect that can also outperform O1-preview's accuracy of 71.4% if we keep enlarging the sampling size. Note that on the Medium-size examples, we can already see a higher accuracy of 92.9% for BoN-OracleN=128 compared Ol-preview's 88.2%, and the trend shown in the curves indicates that the performance of GPT-40 can be further improved with more test-time compute. Fig. 6 in Appendix provides further analysis on how sampling affects model performance.\nMajority Voting is simple yet effective. For majority voting, we rank the candidates based on the frequency of each cell in their solution grid, and select the candidate with the highest sum of frequencies. As for the Reward Model (RM), we choose the one that ranks to the top on Ai2's RewardBench leaderboard (Lambert et al., 2024b), named Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024). We find that using Majority Voting for GPT-40 can improve from 31.7 to 38.0 (for the overall accuracy) when the sample size N=32, while keep increasing the sample size does not necessarily improve the performance any more. Also, the performance of GPT-40 with BON-RMN=32 is 33.9, which is worse than majority voting, suggesting that the current reward models that are mainly designed for chat or general instruction following tasks may not be directly applicable to (logical) reasoning tasks."}, {"title": "6. Scaling Test-Time Compute with Extensive Chain-of-Thoughts Tokens", "content": "Another approach of scaling test-time compute is to increase the number of reasoning tokens (i.e., chain-of-thoughts to-"}, {"title": "6.1. 01 Generates More Hidden Reasoning Tokens", "content": "01 generates large-scale hidden reasoning tokens. One of the key differences between 01 and other LLMs is the way they use more test-time compute to decode much more hidden chain-of-thoughts (CoT) tokens during inference time, which are not directly visible to users. Our analysis shows that o1 models scale their hidden CoT tokens with puzzle complexity - producing on average 5,144.6 (01-mini) and 5,346.3 (01-preview) hidden reasoning tokens compared to 502.9 and 543.7 for GPT-40-mini and GPT-4o respectively."}, {"title": "6.2. Self-Refinement is Limited but Promising", "content": "The other feature of ol's hidden reasoning process is the ability to reflect on its own reasoning process and refine its answer. From our observation on the summary of their hidden reasoning process, we can see that ol often revisits the clues and constraints to verify its previous reasoning and fix the errors if there are any, which is similar to the Z3 solver's conflict-driven clause learning mechanism. In order to elicit such self-refinement behavior from LLMS, we add follow-up queries to ask the model to review its"}, {"title": "7. Related Work", "content": "Logical Reasoning Benchmarks and Dataset Creation\nLogical reasoning has long been a critical area of AI, but only recently have LLMs been subjected to rigorous testing in this domain. LogiQA (Liu et al., 2020) emerged early on to evaluate complex logical comprehension in question-answering formats; and subsequent efforts by (Liu et al., 2023) reframed it as a Natural Language Inference (NLI) task to further stress-test LLMs' capabilities. Researchers have also explored generating more dynamic or granular datasets to push the limits of reasoning systems. For instance, Madusanka et al. (2024) investigated satisfiability tasks formulated in natural language, studying how varying computational complexity influences LLM inference performance. Similarly, Richardson & Sabharwal (2022) introduced a systematic methodology for building challenging reasoning datasets, exposing robustness gaps in transformer-based models when tasked with increased complexity. Prior work on logic grid puzzles include Mitra & Baral (2015) that proposed a grid-based puzzle dataset prior to the LLM era and focused on automatic translation from language to a formal specification, Dziri et al. (2023) that investigated compositionality in LLMs on grid-based puzzles, as well as Tyagi et al. (2024) that provided a new error taxonomy to evaluate the correctness of the reasoning chains of LLMs.\nApproaches to Logical Reasoning in LLMS Several lines of research propose methods to augment or refine LLMs for stronger logical reasoning. Clark et al. (2020)"}, {"title": "8. Conclusion", "content": "We introduce ZebraLogic, a controlled benchmark of 1,000 logic grid puzzles that highlights the scaling limits of LLM-based reasoning through carefully adjustable complexity. Our experiments reveal a pronounced drop in model performance as puzzle complexity increases, overshadowing gains from model growth or training data expansions. While increasing the generation sample size yields modest improvements, a backtracking-based approach with expanded reasoning steps significantly boosts accuracy. These results spotlight the importance of explicit, step-by-step reasoning strategies and provide a valuable framework for advancing logical reasoning research in LLMs."}, {"title": "A. Additional Experimental Results and Analysis", "content": "Please find the additional analysis and results below in the figures."}, {"title": "B. Details of the ZebraLogic Dataset", "content": "All possible attribute types: Name, Color, Nationality, Animal, Drink, Cigar, Food, Flower, PhoneModel, Children, Smoothie, Birthday, Occupation, Height, CarModel, FavoriteSport, MusicGenre, BookGenre, HairColor, Mother, HouseStyle, Education, Hobby, Vacation, Pet\nEach problem instance is characterized by two complimentary complexity metrics: the search space size as well as the average number of Z3 conflicts that the SMT solver takes to solve a puzzle. Figure 7 illustrates how both metrics vary across different number of houses (N) and number of attributes (M)."}, {"title": "C. Additional Analysis", "content": "GPT-40 tends to generate more visible reasoning tokens than 01. Interestingly, we find that the GPT4o model tends to generate more visible reasoning tokens than 01, especially when the search space is large, which is shown in the lower part of Figure 5. The visible reasoning tokens are generated by the model and displayed in their outputs before the final solution grids. We can see that until the search space reaches the Large category (especially when the search space size is < 105), the four models generate similar numbers of visible reasoning tokens. However, when the search space size is larger, GPT40 generates more visible reasoning tokens yet still fails to solve the puzzles. o1 models, which have used more hidden CoT tokens, tend to output fewer visible reasoning tokens for describing their reasoning process."}, {"title": "C.1. Human Evaluation of ol's Reasoning", "content": "Here we present several case studies to understand the reasoning process of o1. We selected a few representative examples from the ZebraLogic dataset and analyzed the reasoning steps taken by ol-preview to arrive at the final solution."}, {"title": "C.2. Comparison with LMSYS Arena Rankings", "content": "While the overall performance rankings on ZebraLogic generally align with those from the LMSYS Arena (a platform for evaluating LLMs across various tasks), we observe some notable discrepancies that highlight ZebraLogic's distinct evaluation perspective. For instance, GPT-40-mini-0718 achieves a higher Elo score (1273) in LMSYS Arena (24-11-11) compared to Llama-3.1-405B (1266), GPT-40-0806(1264), Mistral-Large-2 (1251), and Llama-3.1-70B (1247). However, on ZebraLogic, GPT-40-mini only achieves 20.1% accuracy while Llama-3.1-405B reaches 32.6%. These differences suggest that ZebraLogic offers a more focused assessment of logical reasoning capabilities, providing valuable insights that complement general-purpose evaluations."}, {"title": "C.3. 01 generates large-scale hidden reasoning tokens.", "content": "One of the key differences between ol and other LLMs is the way they use more test-time compute to decode much more hidden chain-of-thoughts (CoT) tokens during inference time, which are not directly visible to users. Figure 5 shows how the number of hidden CoT tokens varies with the search space size for both o1-mini and 01-preview. In each sub-figure on the top, we plot 1,000 points, each representing a puzzle. The color and shape of the points indicate whether the model produced a correct solution (blue dots) or an incorrect one (red crosses). The y-axis shows the number of hidden CoT tokens generated by the model, while the x-axis shows the search space size in logarithmic scale. The definition of search space size is provided in Section 2.3, and a larger search space usually indicates a more complex puzzle.\nWe can see that the number of hidden CoT tokens generated by ol is scaling with the search space size, indicating that o1 is able to leverage more reasoning steps when faced with more complex puzzles. On average, we find that 01-mini generates 5,144.6 hidden reasoning tokens, while o1-preview generates 5,346.3 hidden reasoning tokens. Both are about 10 times more than the average number of reasoning tokens generated by GPT-40-mini (502.9) and GPT-40 (543.7), showing that scaling reasoning tokens can be an effective way to improve the performance of LLMs on logical reasoning tasks."}, {"title": "D. Further Discussion on o1's Reasoning", "content": "We have seen that o1 generates more hidden reasoning tokens than other LLMs, and the hidden reasoning tokens scale up with search space size, indicating that o1 is able to leverage more reasoning steps when faced with more complex puzzles. Since the hidden reasoning tokens are not accessible, we investigate whether ol's visible output tokens or its summary of hidden tokens can explain its higher performance.\nVisible outputs from 01 cannot fully explain its reasoning for complex problems. To understand how o1 reasons, we have to focus on their public reasoning steps that we can extract from the model's visible outputs. From our human evaluation on their reasoning steps, we find that ol's reasoning steps are not necessarily rigorous or complete, even when they arrive at the correct solution. For small-to-medium search spaces, o1-preview's reasoning chains tend to be complete, while 01-mini sometimes can skip some steps to directly reach the solution. For problems with larger search spaces, ol's visible reasoning chains tend to be very incomplete, and sometimes even incorrect, especially when the reasoning process requires backtracking. For example, ol's visible reasoning may contain steps such as \"Bob cannot be in Houses 1, 4, or 5, so he must be in House 3\" without explaining why Bob cannot be in House 2, although it will indeed lead to the correct solution. Note that such cases also happen for other LLMs such as GPT-40. We thus describe that the reasoning process of LLMs and o1 models are sometimes based on guessing without formal logic, especially for complex problems with large search spaces, rater than rigorous logical reasoning.\nSuch incomplete reasoning steps are very common in ol's outputs, especially for puzzles with larger search spaces, leading to unreliable explanations of their reasoning process. Thus, we argue that the visible reasoning steps from o1 cannot help us understand how o1 reasons for complex problems. Furthermore, knowledge distillation from o1's reasoning steps is not necessarily helpful for improving the performance of other LLMs, as the reasoning steps are often incomplete and sometimes incorrect. This raises questions about the concern of hidden CoT tokens in their reasoning process that are not visible in the output.\nWill the summary of hidden tokens help us understand ol's reasoning? Although the hidden CoT tokens are not visible from the OpenAI APIs, we can see an overview summary of the hidden reasoning tokens on ChatGPT's user interface for ol's hidden reasoning steps. By manually analyzing the overview summary of hidden reasoning tokens, we find it is still hard to clearly understand how o1 reasons for complex problems. We can sometimes see some intermediate results in the overview but not any explanations for the decision. Interestingly, we can see some behaviors of recognizing the"}, {"title": "D.1. Prompt template to evaluate ZebraLogic", "content": "# Example Puzzle\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street.\nEach house is occupied by a different person. Each house has a unique attribute for\neach of the following characteristics:\nEach person has a unique name: `Peter`, `Eric`, `Arnold`\nEach person has a unique favorite drink: `tea`, `water`, `milk`\n## Clues for the Example Puzzle\n1. Peter is in the second house.\n2. Arnold is directly left of the one who only drinks water.\n3. The one who only drinks water is directly left of the person who likes milk.\n## Answer to the Example Puzzle\n{\n\"reasoning\": \"Given Clue 1, we know Peter is in House 2. According to Clue 2, Arnold\nis directly left of the one who only drinks water. The person in House 3 cannot\nbe on the left of anyone, so Arnold must be in House 1. Thus, Peter drinks\nwater, and Eric lives in House 3. Then, according to Clue 3, Eric drinks milk.\nTherefore, Arnold drinks tea.\",\n\"solution\": {\n\"House 1\": {\n\"Name\": \"Arnold\",\n\"Drink\": \"tea\"\n},\n\"House 2\": {\n\"Name\": \"Peter\",\n\"Drink\": \"water\"\n},\n\"House 3\": {\n\"Name\": \"Eric\",\n\"Drink\": \"milk\"\n}\n}\n}\n# Puzzle to Solve\nThere are 3 houses, numbered 1 to 3 from left to right, as seen from across the street.\nEach house is occupied by a different person. Each house has a unique attribute for\neach of the following characteristics:\nEach person has a unique name: `Eric`, `Peter`, `Arnold`\nEach person has a unique favorite drink: `milk`, `water`, `tea`\nEach person has a unique hobby: `photography`, `cooking`, `gardening`\n## Clues:\n1. Arnold is not in the first house.\n2. The person who likes milk is Eric.\n3. The photography enthusiast is not in the first house.\n4. The person who loves cooking is directly left of the person who likes milk.\n5. The one who only drinks water is Arnold.\n6. The person who likes milk is not in the second house.\n# Instruction\nNow please solve the above puzzle. Present your reasoning and solution in the following\njson format:\n{\n\"reasoning\": \"\",\n\"solution\": {\n\"House 1\": {\n\"Name\": \"\",\n\"Drink\": \"\",\n\"Hobby\": \"\"\n},\n\"House 2\": {\n\"Name\": \"\","}]}