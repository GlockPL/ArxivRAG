{"title": "Revisiting Safe Exploration in Safe Reinforcement learning", "authors": ["David Eckel", "Baohe Zhang", "Joschka B\u00f6decker"], "abstract": "Safe reinforcement learning (SafeRL) extends standard reinforcement learning with the idea of safety, where safety is typically defined through the constraint of the expected cost return of a trajectory being below a set limit. However, this metric fails to distinguish how costs accrue, treating infrequent severe cost events as equal to frequent mild ones, which can lead to riskier behaviors and result in unsafe exploration. We introduce a new metric, expected maximum consecutive cost steps (EMCC), which addresses safety during training by assessing the severity of unsafe steps based on their consecutive occurrence. This metric is particularly effective for distinguishing between prolonged and occasional safety violations. We apply EMMC in both on- and off-policy algorithm for benchmarking their safe exploration capability. Finally, we validate our metric through a set of benchmarks and propose a new lightweight benchmark task, which allows fast evaluation for algorithm design.", "sections": [{"title": "Introduction", "content": "Defining a reward function for Reinforcement Learning is complex and requires significant expertise, particularly for real-world applications. Creating a single reward function that encapsulates all goals can be difficult and may result in sub-optimal policies due to varying importance of different reward components. Instead, formulating these tasks as constrained optimization problems can be more effective. For instance, in a heating system control scenario, it is more straightforward to formulate the thermal comfort as constraints and minimize the energy usage as reward rather than combining these objectives into one reward function. To address these constrained optimization problems, SafeRL has been developed via formulating the problem as a constrained Markov decision problem (CMDP) [3] to ensure that the control system adheres to critical safety constraints during both training and real-world deployment [9, 4].\nThe trade-off between exploration and exploitation lies at the core of RL and plays the vital role for improving the data efficiency and overall performance. In the context of SafeRL, safety is crucial to prevent severe violations of constraints and potential harm to the agents and the environments. Thus, maintaining safety during the training and deployment of agents becomes a critical third dimension, alongside exploration and exploitation. However, this focus on safety can conflict with the need for exploration, particularly since SafeRL agents often interact with the environments without prior knowledge and must explore to learn safe behaviors. This scenario presents a significant dilemma: how can SafeRL algorithms balance safety with the necessity of exploration?\nMany SafeRL benchmark work [18, 19, 29] have been carried out to compare the performance of different algorithms by looking into two metrics: expected cumulative return and costs of the final policy after training. Achiam and Amodei [1] proposes to use another metric for comparing safe exploration during training in form of the average cost over the entirety of training. This metric directly corresponds to safety outcomes as a lower cost rate relates to less unsafe steps during training. However, all these metrics often do not adequately reflect the nuances of safe exploration. For instance, they may not differentiate between the severity of unsafe actions taken during exploration, thus potentially overlooking critical safety nuances.\nWe argue that it is necessary to differentiate between different types of unsafe behaviour during training as evaluation as demonstrated in Fig. 1, especially from the perspective of safe exploration. In Fig 1, despite the two trajectories generated by different policies, but it's clear to see that the right one is more informative as it explores more on the boundary of the infeasible sets and closer to the optimal policy. These transitions on the edge will help the critic and actor to learn a more accurate estimation and prediction, allowing the agent to perform optimization more precisely. Whereas the left trajectory explores more in the grey area, which is not as valuable as the boundary from the safe exploration perspective as violating the constraints for a long period of time would be more harmful than a few occasional violations. It also helps less for the agent to clearly identify where the safe boundaries are.\nWith these concerns, we introduce a new metric that quanti-"}, {"title": "Related work", "content": "SafeRL Algorithms Numerous studies have proposed diverse methods to enhance the safety of Reinforcement Learning (RL). Comprehensive reviews of these approaches can be found in [30, 12, 11, 20].\nSafe Policy Search integrates techniques from nonlinear programming into policy gradient methods [24] and builds theoretical frameworks for lifelong RL to ensure safety via gradient projection [5]. Constrained Policy Optimization (CPO) [2] emerged as the first general-purpose method employing a trust-region approach with theoretical guarantees. Conditional Value-at-Risk (CVaR) has also been utilized to optimize Lagrangian functions with gradient descent [6, 7].\nExtensions to Soft Actor-Critic (SAC) incorporate cost functions and employ Lagrange-multipliers to handle constraints, although training robustness issues arise when constraint violations are infrequent [13]. This framework has been used for multitask learning on real robots, with safety ensured by learning predictive models of constraint violations [22, 25, 27]. Lyapunov functions provide another approach, projecting policy parameters onto feasible solutions during updates, applicable across various policy gradient methods like DDPG or PPO [8, 10]. SafeDreamer [17] uses the Dreamer [16] architecture but also takes safety into consideration\nSafeRL Benchmarks Several SafeRL benchmarks [1], [18], [19], [29] have been proposed often focusing on different aspects of SafeRL. In [1] Safety Gym is introduced as the first standard set of environments for SafeRL. With Safety-Gymnasium [18] extends[1] with more agents, tasks and benchmarked algorithms. GUARD [29] benchmarks TRPO-based SafeRL algorithms on a broad set of tasks and agents while [19] focus on offline SafeRL.\nAs metrics for quantifying safety these benchmarks use the (normalized) cost return of the trained policy. For evaluating safety during training next to learning curves [1] and [29] provide the metric of cost rate (sum of all costs divided by number of environment interaction steps of the training) for quantifying the general safety of the training process. Compared to the employed metrics of these benchmarks we propose a new metric for quantifying the safe exploration."}, {"title": "Background", "content": "Markov Decision Process A Markov Decision Process (MDP) is formalized as a tuple ({S, A, P, r, y} where S is the state space, A the action space, P the transition model of the environment, r(s's, a) the reward function, describing the reward given when transitioning from state s to next state s' with action a, and \u03b3 \u2208 [0, 1] the discount factor. The objective to maximize expected discounted cumulative reward, which is defined as:\n$J_R(\\pi) = E_\\pi \\sum_{t=0}^T \\gamma^t r(S_{t+1}|S_t, a_t)$\n(1)\nwhere is defined as the policy which outputs the action distribution given a state.\nConstrained Markov Decision Process Constrained Markov Decision Processes (CMDPs) [3] extend MDPs to the constrained optimization problem by augmenting the objective with one or multiple cost functions Ci (s's, a) in analogy to the reward function, the cost threshold Di respectively and discount factor Ye \u2208 [0, 1]. We define JC(\u03c0) as the expected discounted cumulative cost. Then we have the feasible set of policy defined as:\n$\\Pi_c = {\\pi\\in \\Pi : J_c(\\pi) - D < 0}$\n(2)\nThe constrained optimization problem can be written as\n$\\pi^* = \\arg \\max_{\\pi \\in \\Pi_c} J(\\pi)$\n(3)\nwhich maximizes the return while respects all constraints.\nExisting Metrics for SafeRL For measuring performance and safety during and after training the following metrics are used in the existing benchmarks [1], [18], [19] and [29].\n\u2022 Average episode return JR\n\u2022 Average episodic sum of costs Jc\n\u2022 Cost rate pe: sum of all costs divided by number of environment interaction steps during training.\n\u2022 Conditional Value-at-Risk (CVaR): For a bounded-mean random variable Z, the value-at-risk (VaR) of Z with confidence level \u03b1 \u2208 (0, 1) is defined as:\n$VaR_\\alpha (Z) = F_Z^{-1}(1 - \\alpha)$,\n(4)\nwhere F\u2082 = P(Z < z) is the cumulative distribution function (CDF); and the conditional value-at-risk (CVaR) of Z with confidence level \u03b1 is defined as the expectation of the \u03b1-tail distribution of Z as\n$CVaR_\\alpha (Z) = E_{z \\sim Z}{z|z \\geq VaR_\\alpha(Z)}$.\n(5)"}, {"title": "Circle2D Environment", "content": "For rapid evaluation of safe exploration, we introduce the \"Circle2D\" environment, which features four levels of difficulty, ranging from 0 to 3 as depicted in Fig. 2. This environment serves as a simplified model of real-world scenarios involving complex cost regions, such as areas exceeding speed limits or zones a cleaning robot must avoid. Although these real-world scenarios present greater complexity, they share the underlying principle of navigating cost regions, which must be strategically avoided. The Circle2D environment, by focusing on the exploration of these cost boundaries, offers an effective and straightforward means for assessing the safe exploration"}, {"title": "Expected maximum consecutive steps: EMCC", "content": "Fig 1 has depicted two scenarios where conventional metrics cannot well differentiate. To tackle this challenge and measure the safe exploration, we propose a new metric Expected maximum consecutive steps (EMCC), which defines as:\n$MCCD = max(\\frac{d_{max}}{l})$\n(6)\n$EMCC = E_{D \\sim \\pi} [MCCD]$\n(7)\nwhere D is the set of rollouts generated by a policy \u03c0, and is a subset of D which represents a consecutive trajectory with arbitrary length. Note that policy \u3160 changes between rollouts due to the online update during the training. In the case of Fig 1, the MCC without normalization by the total episode length of the left trajectories will be 8 and the right one will be 3. EMCC considers multiple rollouts during a period of training time to give an average estimation.\nIn its general form, Eq. 7 calculates one value for the whole training process. To capture the changing behaviour in exploration during training, we divide the training process into three parts and calculate EMCC per part. This allows more precise interpretation for safe exploration as later rollouts cannot influence the EMCC value of the first third of the training process. Making this distinction is especially relevant for the first third as the most exploration is expected at the beginning. We denote EMCC split into the different training parts uniformly as $EMCC_{\\beta}$ with \u03b2 showing the relevant training part. $EMCC_{0.33}$ combines data from the start to 33% of the training, $EMCC_{0.66}$ for 33% to 66% and $EMCC_{0.99}$ for 66% to 99%.\nFurthermore we augment $EMCC_{\\beta}$ with the conditional Value-at-Risk (CVaR) [21] to focus on the highest MCC values of the MCC distribution per training part. As we associate the most prolonged safety violations with the most risky behaviour augmenting with CVaR further enhances $EMCC_{\\beta}$ as a safety measure. We follow the definition and notation of [26] for using a positive scalar \u03b1 \u2208 [0, 1) as risk level in safety and apply it to $E_{\\beta}MCC$:\n$EMCC_\\beta = E_{D_\\beta \\sim \\pi} [MCCD_\\beta | MCCD_\\beta \\geq F_{MCC_{\\beta}}^{-1}(1 - \\alpha)]$\n(8)\nIn Eq. 8 $D_\\beta$ denotes rollouts of the training part associated with \u03b2 and $F_{MCC_{\\beta}}^{-1}(1 - \\alpha)$ is the \u03b1-percentile with $FMCC_\\beta$ being the cumulative distribution function of the distribution of corresponding MCC values.\nTo clarify EMCC calculation we provide a conceptual calculation process in 5 steps:\n1. Initialization \u1e9e: Define which part of the training process to evaluate with EMCC.\n2. Per trajectory calculation: For each rollout associated with training part \u1e9e calculate for each trajectory the maximum number of consecutive cost steps dmax and normalize with the respective trajectory length.\n3. MCC per rollout: For each rollout find the maximum of the corresponding normalized numbers of consecutive cost steps (result from"}, {"title": "Experiments", "content": "We use our proposed Circle2D environment with its 4 levels and the Safety-Gymnasium [18] tasks Safety PointCircle1, SafetyPointGoall and SafetyHopperVelocity depicted in Fig.3 to evaluate the safe exploration process during training. We use a 3 layer MLP with two hidden layers of size 64 and Tanh activation function. Hyperparameters for the Circle2D and Safety-Gymnasium tasks are disclosed in Tab. 1 and Tab. 2 respectively.\nAlgorithms We choose algorithms as representatives of their respective classes.\nTRPO-Lag: on-policy SafeRL algorithm which extends TRPO as policy search algorithm for CMDPs via introducing a Lagrangian multiplier. Then it solves the results unconstrained optimization problem with dual gradient descent.\nCPO: on-policy method [2] which uses second order method to enforce the constraints during the policy search.\nSAC-Lag: off-policy algorithm [14] based on SAC [15] and Lagrangian method\nSAC-LB: off-policy algorithm [28] based on SAC [15] and introduces a linear smoothed log barrier function to replace the Lagrangian multiplier to stabilize the training.\nWCSAC: off-policy algorithm [26] which further extends SAC-Lag by replacing the expected cost return of SAC-Lag with the conditional Value-at-Risk (CVaR) given a risk level. In our experiments we use a risk level of 0.5 as it shows best overall performance in the original work.\nResults The results averaged over 3 seeds for Circle2D tasks are shown in Tab. 3 and for the Safety-Gymnasium tasks in Tab. 4.\nFor the Circle2D-0 tasks we observe that only SAC-Lag and SAC_LB have runs that end in the left corridor with global feasible optimum. For Circle2D-1 only SAC-LB converges to the left corridor without violating the cost limit. For Circle2D-2 and Circle2D-3 no algorithms manages to safely converge inside the left corridor. Note that the high return values of SAC-Lag and WCSAC for Circle2D-2 are based on strongly varying results for different seeds. On some runs the cost region is ignored and on the others they converge to the local optima with costs of 0, but no runs result in a policy converging inside the left corridor with violating the cost limit.\nTRPO-Lag ignores the cost region in levels 1,2,3 and moves straight through it and while CPO explores the boundary of the cost region it fails to converge towards the left corridor. Except for Circle2D-2, as discussed, WCSAC quickly converges towards the local optima with low costs on all other levels.\nAnalysis In general, by looking into EMCC value of different training stages in Tab. 3 and Tab 4, we observe a consistent trend across on-policy algorithms in both the Circle2D and Safety-Gymnasium tasks, where EMCC values decrease over training time. This trend indicates that the most safety-critical exploration typically occurs in the early stages of training. In contrast, off-policy algorithms display no clear trend except for occasionally increasing EMCC values towards the end of training, suggesting these algorithms persistently explore safety-critical regions, thus risking severe unsafe behaviors reflected in high EMCC values.\nRegarding safe exploration, cost return at evaluation does not adequately measure safety during training. For instance, in the Circle2D-1 task, SAC-Lag shows a lower cost return than on-policy algorithms, yet consistently higher EMCC values by a significant margin. This"}, {"title": "Conclusions", "content": "Current metrics used in SafeRL benchmarks either ignore or only allow general impression on safe exploration during the training"}]}