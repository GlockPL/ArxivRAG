{"title": "SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection", "authors": ["Huafeng Chen", "Pengxu Wei", "Guangqian Guo", "Shan Gao"], "abstract": "Most Camouflaged Object Detection (COD) methods heavily rely on mask annotations, which are time-consuming and labor-intensive to acquire. Existing weakly-supervised COD approaches exhibit significantly inferior performance compared to fully-supervised methods and struggle to simultaneously support all the existing types of camouflaged object labels, including scribbles, bounding boxes, and points. Even for Segment Anything Model (SAM), it is still problematic to handle the weakly-supervised COD and it typically encounters challenges of prompt compatibility of the scribble labels, extreme response, semantically erroneous response, and unstable feature representations, producing unsatisfactory results in camouflaged scenes. To mitigate these issues, we propose a unified COD framework in this paper, termed SAM-COD, which is capable of supporting arbitrary weakly-supervised labels. Our SAM-COD employs a prompt adapter to handle scribbles as prompts based on SAM. Meanwhile, we introduce response filter and semantic matcher modules to improve the quality of the masks obtained by SAM under COD prompts. To alleviate the negative impacts of inaccurate mask predictions, a new strategy of prompt-adaptive knowledge distillation is utilized to ensure a reliable feature representation. To validate the effectiveness of our approach, we have conducted extensive empirical experiments on three mainstream COD benchmarks. The results demonstrate the superiority of our method against state-of-the-art weakly-supervised and even fully-supervised methods.", "sections": [{"title": "1 Introduction", "content": "Camouflaged Object Detection (COD) aims to detect concealed objects from various backgrounds [4, 8, 25, 32, 33], which have imperceptible visual appearances with extremely high similarity to the environment. It holds great promise for practical applications, e.g., species discovery [9, 10, 12, 17, 18, 20], medical"}, {"title": "2 Related Work", "content": "Camouflaged Object Detection. COD focuses on detecting camouflaged objects within an image. SINet [8] proposes a COD dataset with 10K camouflaged images, which takes an average of around 60 minutes to annotate each image. [24,27] attempt to mine inconspicuous features of camouflage objects from the background through meticulously designed feature exploration modules. Zoom- Net [25] introduces a mixed-scale triplet network to address the challenges posed by COD. The aforementioned COD methods heavily rely on large-scale datasets with pixel-level annotations. However, the unclear boundaries make pixel-wise annotation of camouflaged objects a time-consuming and labor-intensive task. CRNet [15] was the first to introduce the S-COD dataset, which employs scrib- ble annotations as weak supervision. WS-SAM [14] employs scribble and point annotations as weak supervision, but there is no dataset constructed with point annotation. Furthermore, box annotation has yet to be explored. So we propose box and point annotations to construct COD datasets. Furthermore, we propose the first model that simultaneously supports various weak supervision labels and outperforms fully supervised methods.\nSAM in COD. SAM [19] excels in traditional segmentation tasks, achieving re- markable results, sometimes matching the performance of fully supervised meth- ods, even in a zero-shot setting. [3,28] indicate that while SAM shows promise in generic object segmentation, its performance on the COD task is constrained. SAM-Adapter [3] employs an adapter for efficient tuning instead of relying on traditional fine-tuning methods. This adaptation enables SAM to align with the data distribution in COD, reducing the cost of fine-tuning while simultaneously enhancing the performance of SAM in COD. WS-SAM [14] processes three aug- mented images through SAM and fuses the obtained masks to obtain the final pseudo-label. But the drawbacks of it are also obvious: 1) tripled SAM inference time 2) the full potential of SAM was not utilized, and only the highest scoring mask was used instead of the top-3 masks. We apply SAM to design a unified framework for point, box, and scribble annotations.\nKnowledge Distillation. Knowledge distillation (KD) [1,16] has been primar- ily designed to train a small network to mimic the output of a larger network to compress models. DINO [2] has introduced a straightforward self-supervised"}, {"title": "3 Approach", "content": "The overall architecture of the proposed framework is shown in Fig. 3. Prompt adapter is used to process scribbles to adapt SAM prompt input. Response filter is employed to handle extreme response situations of SAM under the prompt. Semantic matcher is utilized to improve SAM's response issues arising from a lack of COD-related semantics. Prompt-adaptive knowledge distillation is employed for the knowledge distillation in WSCOD."}, {"title": "3.1 Prompt Adapter", "content": "We use three kinds of weakly-supervised labels as prompts: point, box, and scribble. SAM directly supports types of point and box as input prompts. Un- fortunately, SAM does not support scribble-type prompt. Therefore, we design a prompt adapter to convert scribbles into discrete points, making it compatible with SAM, as shown in Fig. 3.\nSpecifically, we first use the Zhang-Suen algorithm [35] to extract the skeleton of the scribble. Then, we perform discrete sampling on it. Specifically, we first create a grid G, where the grid points are uniformly distributed and the distance is $min(\u03b1W,\u03b1H)$, where H and W represent the length and width of the input image, respectively. \u03b1 is the hyperparameter. Afterwards, we form a discrete"}, {"title": "3.2 Response Filter", "content": "In COD, the camouflage objects usually exhibit excellent mimicry. So, SAM is prone to locate the extreme response under limited prompts, as shown in Fig. 2(b). To solve this, we design a response filter to prevent taking advantage of these evidently abnormal responses, as shown in Fig. 3.\nSpecifically, SAM outputs three valid masks and corresponding confidence scores given the prompt input:\n${Vi, S_{con}^i}_{i = 1,2,3} = SAM(I, prt),$\nwhere $V^i$ donate the i-th objects masks and $S_{con}$ represents the corresponding segmentation confidence score. SAM defaults to using the mask with the max- imum confidence score. Subsequently, we design a response filter to determine whether the mask exhibits extreme response by calculating the ratio of the mask size to the image size:\n$R^i = I(T_s < \\frac{A^i}{HW} < T_b),$\nwhere II() is an indicator function. $A^i$ is the area of the i-th mask $V^i$. $T_s$ and $T_b$ represent the maximum and minimum thresholds, respectively."}, {"title": "3.3 Semantic Matcher", "content": "SAM lacks the semantic knowledge, specifically the semantic understanding of camouflaged and overall granularity, leading to responses that do not align with the objects, as shown in Fig. 2(c). To solve it, we design a semantic matcher to measure the semantic score by the semantic entropy. It then selects masks with accurate semantics, as shown in Fig. 3.\nSpecifically, we first train the model on COD data to obtain the mask $M^o$:\n$M^o = D(E(I)),$\nwhere I donates input image, E and D are the encoder and decoder of the model, respectively. Although $M^o$ may not rival the masks of SAM in segmen- tation details, training on COD data provides the model with a preliminary understanding of camouflage semantics.\nThen, we design semantic entropy $S_{ent}$ using $M^o$ to measure the semantic score of the mask $V^i$:\n$S_{ent} = -\\sum_j M^o_j log(V_j^i) + (1 - M_j^o)log(1 - V_j^i),$\nwhere j is the pixel index. Smaller values of $S_{ent}$ indicate higher semantic score for $V^i$."}, {"title": "3.4 Prompt-Adaptive Knowledge Distillation", "content": "We employ knowledge distillation method to transfer the knowledge from large visual model SAM to a smaller model, thereby reducing the data cost and model size. However, COD task is challenging and the weak supervision makes knowl- edge distillation more difficult. Specifically, the proposed framework distillates the optimal mask $V^{opt}$ from SAM as teacher knowledge $K^t$ to the student knowl- edge $K^s$ in our model. Moreover, we leverage the prior knowledge of different prompts to enhance the distillation quality,\nPrompt-adaptive Mask Generation. The input prompts (scribble, box, and point) contain the structure, boundary, and discriminative region of camouflaged objects, respectively. These have been confirmed to be crucial for COD tasks [15, 33]. Therefore, we construct a prompt-adaptive mask $M^f$ for the knowledge distillation according to the input prompt. The key distillation regions in $M^f$ are marked as 0 (black areas). Specifically, 1) Scribble label, retaining the labeled foreground while discarding the background yields the corresponding $M^f$. 2) Point label, an inscribed circle of $K^t$ with point label as the center. 3) Box labels, represented by bold boxes, with edge width and height being one-fourth of the length and width of box label, respectively.\nThen, the prompt-adaptive knowledge distillation loss is defined as:\n$L_{pkd} = \\sum_j M^{F}(K^{t}log(K^{s}) + (1 - K^{t})log(1 - K^{s})),$\nwhere $K^s$ is the prediction mask and jis the pixel index. $M^{F}=1+I(M^{f}=0)$ and I(\u00b7) is an indicator function. $M^F$ as a coefficient in the distillation loss to allocate weight to prompt-guided regions, guides the distillation process to focus on learning key distillation regions.\nSelf-Knowledge Distillation. The learned feature representation of the model may not be robust enough, as shown in Fig. 2(d). Inspired by Self-Knowledge Distillation (SKD), we design a student model to enhance the representation learning. Specifically, for image I, we adopt visual transformations T, selecting from scale, colorjitter, etc. These visual transformations are able to change the appearance of images, as $I^t = T(I)$.\nThen we encode and decode the augment images $I^t$, and transform them into two prediction maps $K^s$ and $K^t$, denoting as:\n$K^o = D(E(I)), K' = D(E(I^t)),$\nOur objective is to minimize the distance between two prediction maps:\n$min D(K^s, K') = \\sum_i |K_i^s - K_i^t|$"}, {"title": "3.5 Network", "content": "Encoder&Decoder. Encoder and decoder designs can be flexibly replaced with existing models. In this work, we employ PVT [29] as the encoder, which obtains multi-scale features (Feat1, Feat2, Feat3, Feat4). The decoder consists of four 3x3 convolutional layers to reduce the channel dimension of $Feat_i$ to 64, followed by upsampling these Feat, to the same size. Subsequently, they are combined through concatenation, and finally, a 3 \u00d7 3 convolutional layer is used to obtain the final mask. In our method, all encoders and decoders refer to the same model.\nTraining Details. Our training process consists of two main steps. In Training Step 1, we train the encoder and decoder in the semantic matcher to obtain the distillation source Kt at the end. In Training Step 2, we use Kt for knowledge distillation to retrain the encoder and decoder. Further details are in the S.M.\nLoss. Compared to other weakly-supervised methods [15,31,34], we have only two losses. The final loss L includes $L_{pkd}$ and $L_{skd}$ defined as:\n$L = L_{pkd} + L_{skd}.$"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. Our experiments are conducted on three COD benchmarks, CAMO [20], COD10K [8], and NC4K [22]. In order to evaluate our method, we first train our network on scribble annotated dataset S-COD [15]. Subsequently, we re- annotated 4040 images (3040 from COD10K, 1000 from CAMO) to create point- supervised dataset (P-COD) and bounding box-supervised dataset (B-COD) for training, while the remaining images are used for testing.\nEvaluation Metrics. We adopt four evaluation metrics: Mean Absolute Error (MAE), S-measure (Sm) [5], E-measure (Em) [6], weighted F-measure (F) [23].\nImplementation Details. We implement our method with PyTorch and con- duct experiments on one GeForce RTX4090 GPU and use ViT-H version of SAM. We chose PVT-B4 [29] as the encoder. We use the stochastic gradient descent optimizer with a momentum of 0.9, a weight decay of 5e-4, and triangle learning rate schedule with maximum learning rate 1e-3. The batch size is 8, and the training epoch is 60. Input images are resized to 512 \u00d7 512. We adopt the offline distillation, and the forward computation is performed only once, taking only 7 hours in training."}, {"title": "4.2 Compare with State-of-the-art Methods", "content": "Quantitative Comparison. Being the first WSCOD method to incorporate point, scribble and box supervision, the proposed approach primarily leverages scribble supervision and full (mask) supervision as baselines. As demonstrated in Tab. 1, our method achieves substantial improvements, we averaged the results under three weakly-supervised labels, with an average enhancement of 26.8% for MAE, 6.1% for Sm, and 5.5% for Em compared to the state-of-the-art weakly- supervised COD method, WS-SAM [14]. In particular, our approach performs exceptionally well under point and box supervision. It highlights our capabil- ity to achieve better performance with fewer annotations. Our approach even outperforms the state-of-the-art fully supervised method, ZoomNet [25]. \u03a4o ver- ify the advantages of our method over simply using of SAM, we compare with SAM-S and SAM-P, which fine-tune the mask decoder of SAM with scribble and point supervisions, respectively, by the partial cross-entropy loss. When testing, SAM-S and SAM-P use the automatic prompt generation strategy and report the results with the highest IoU scores. We do see performance gains after finetuning SAM with point (SAM-P) and scribble (SAM-S) supervision, but the results are still far below our method. This demonstrates the superiority of our method, which utilizes SAM prompt-adaptive knowledge distillation for small models. To"}, {"title": "4.3 Ablation Study", "content": "As COD10K is the most representative dataset, all following ablation experi- ments are performed on it. Unless specifically indicated, all results are the aver- ages of three different prompts (point, box, and scribble).\nEffectiveness of Prompt Adapter. The ablation results of prompt adapter are presented in Tab. 4. Adapter has a large influence on the performance for scribble prompt. In addition, compared with baseline, a more accurate prediction map can be obtained by using the adapter, as shown in Fig. 6. In addition, adapter has a hyperparameter \u03b1 to control the degree of discrete sampling, as shown in Tab. 5, with optimal effects achieved for suitable discrete sampling.\nEffectiveness of Response Filter. As shown in Tab. 3, the results are sig- nificantly improved using response filter. Fig. 6 intuitively illustrates that the response filter enhances the precision of prediction maps. In addition, response filter has two hyperparameters $T_s$ and $T_b$ to control effects, as shown in Tab. 5.\nEffectiveness of Semantic Matcher. We conduct ablation experiments for the semantic matcher, as shown in Tab. 3. In addition, a more complete visu- alization of the prediction map can be obtained by using semantic matcher, as shown in Fig. 6."}, {"title": "4.4 Extension to SOD", "content": "Our method excels not only in COD but also demonstrates remarkable perfor- mance in SOD. Specifically, we train on the SOD dataset using the labels of point, scribble, and box, respectively, and the results obtained are shown in Tab. 8. We attribute this success to our exploration of the potential of SAM and improve- ments in knowledge distillation. which contributes to our strong performance in WSSOD."}, {"title": "4.5 Discussion", "content": "Is the prompt-adaptive KD from SAM important?\n1) Data efficiency. We also evaluate the performance of our model and CRNet in few-shot setting, as shown in Fig. 8(a). Specifically, our model is trained only using the COD10K-Train dataset, which contains categories, and tested on the COD10K-Test dataset. Compared to CRNet, our model achieves promising results with much fewer training data. Especially in the extreme scenario, our method only uses one training image in each category, performance significantly surpasses that of CRNet training on the complete dataset. Fig. 8(a) verifies the effectiveness and efficiency of the proposed method. Through prompt-adaptive knowledge distillation, we transfer the knowledge from SAM to our model, only requiring a small amount of data.\n2) Training efficiency. We visualize the curves of various metrics during the training, as shown in Fig. 8(b), where CRNet and our model share the same implementation details, including the optimizer, learning rate, epochs, and other relevant parameters. It is observed that our model demonstrates extremely fast convergence speed. To achieve the same performance, our model only needs one epoch of training, while CRNet typically requires more than 10 epochs. Because"}, {"title": "5 Conclusion", "content": "In this paper, we propose a SAM-guided unified framework for weakly-supervised camouflaged object detection (WSCOD), named SAM-COD. It integrates all the existing labels for camouflaged objects (i.e., scribbles, bounding boxes, and points), and achieves remarkable performance against the state-of-the-art weakly- supervised methods and even fully-supervised methods. The proposed SAM- COD typically aims to address the issues of SAM in the WSCOD task, i.e., prompt compatibility of the scribble labels, extreme response, semantically erro- neous response, and unstable feature representations. Specifically, in SAM-COD, we devise a prompt adapter to handle different labels and employ response fil- ter and semantic matcher to mitigate the issue of imperfect outputs of SAM for camouflaged objects. Moreover, a prompt-adaptive knowledge distillation is proposed for reliable feature representations. We have conducted extensive ex- periments on camouflaged object datasets, demonstrating the effectiveness of the proposed method, which improves SAM to be more applicable to WSCOD."}]}