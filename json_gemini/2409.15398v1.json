{"title": "Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI", "authors": ["Ambrish Rawat", "Stefan Schoepf", "Giulio Zizzo", "Giandomenico Cornacchia", "Muhammad Zaid Hameed", "Kieran Fraser", "Erik Miehling", "Beat Buesser", "Elizabeth M. Daly", "Mark Purcell", "Prasanna Sattigeri", "Pin-Yu Chen", "Kush R. Varshney"], "abstract": "As generative AI, particularly large language models (LLMs), become increasingly integrated into production applications, new attack surfaces and vulnerabilities emerge and put a focus on adversarial threats in natural language and multi-modal systems. Red-teaming has gained importance in proactively identifying weaknesses in these systems, while blue-teaming works to protect against such adversarial attacks. Despite growing academic interest in adversarial risks for generative AI, there is limited guidance tailored for practitioners to assess and mitigate these chal-lenges in real-world environments. To address this, our contributions include: (1) a practical examination of red- and blue-teaming strategies for securing generative AI, (2) identification of key challenges and open questions in defense development and evaluation, and (3) the Attack Atlas, an intuitive framework that brings a practi-cal approach to analyzing single-turn input attacks, placing it at the forefront for practitioners. This work aims to bridge the gap between academic insights and practical security measures for the protection of generative AI systems.", "sections": [{"title": "1 Introduction", "content": "The increasing importance of red-teaming generative AI (GenAI) follows the growing awareness and realisation of novel attack surfaces that are extending and reshaping the AI security landscape [37, 62]. Adversarial machine learning (advML) used to focus mainly on evasion [8], poisoning [9],"}, {"title": "2 Red-Teaming", "content": "Generative AI applications using LLMs are prone to prompt attacks such as (direct or indirect) prompt injection and jailbreak attacks. Prompt attacks seek to incite a wide range of undesired objectives like harmful or inappropriate information, denial of service, or malicious action [37, 75]. Important terminology in the context of adversarial attacks includes:\n\u2022 Jailbreak - A prompt to a LLM which bypasses all safeguards including alignment of the LLM and causes the LLM to generate unsafe or non-aligned outputs [61]. Do Anything Now (DAN) [79] or ignore-instructions attacks [45] are examples of attacks that aim to achieve jailbreaks.\n\u2022 Direct Injection Attack - Instructions to the LLM directly included in the user prompt aim to override the instructions defined in the system prompt. These attacks do not necessarily require bypassing safeguards and aim to appropriate a LLM's original task defined in the system prompt for unsafe purposes like leaking the system prompt [37].\n\u2022 Indirect Injection Attack - Instructions for the LLM to override system prompt instructions, supplied indirectly to the LLM in data like websites, source code, output generated by other LLMs, etc., that the LLM is processing in RAG or other integrated applications [21].\nThe methods can be used to craft attack vectors for many different malicious goals. In contrast to ad-versarial ML for classification, which focuses on adversarial examples with bounded or imperceptible perturbations in images, video, audio, or text that lead to well-defined outcomes like misclassification [73, 76], generative AI operates in a broader space of inputs and outputs (text, images, video), where undesired outcomes are more subjective and context-dependent. Thus, there is a need for a taxonomy to characterise different types of adversarial threats to generative AI. For frontier models, adversarial threats are often viewed through the lens of misuse or risks which includes bias, toxic data generation etc. [69, 57, 72]. Similarly, for LLMs deployed within complex workflows, like RAG, or LLMs integrated systems, malicious goals can include denial of service [55], or even malicious action like SQL injection [44] which are often missing from the attack characterisations. Diversity clearly stands out as an emerging theme for red-teaming of GenAI and it can be identified across many different dimensions including domains, tasks, attack goals, and attack methods [53]. From a practitioner's perspective, all these dimensions of diversity are of importance."}, {"title": "2.1 Open Questions and Challenges", "content": "Detecting vulnerabilities via red-teaming in practice requires the consideration of a significant number of variables such as harm types and attack styles given a red-teaming budget. Practical red-teaming efforts need to focus on attacks that occur in practice, which are often less sophisticated than attacks present in academic papers [3]."}, {"title": "2.1.1 Red-Teaming - Scope and Use", "content": "\u2022 Context dependent attack objectives - Malicious objectives can be context-dependent. For example, \"how to build a bomb\" may be malicious for a general LLM but relevant for a defence organization's LLM-based application. The deployment context, organizational policies, and regulations should guide the identification and severity of such objectives.\n\u2022 Coverage across goals - Attack goals influence the choice of strategy, and not all are equal for an LLM. For example, a generative AI with poor safeguards might be vulnerable to simple prompt leaks via direct requests to reveal system prompt. But tailored red-teaming strategies may be needed across other harm types, especially as models undergo safety training. Most red-teaming work focuses on a limited set of harmful goals from AdvBench, often with as few as 50 samples [80, 13]. This limited scope can miss nuances, and even simple rephrasing or editing can improve attack success, as shown by Xu et al. [64].\n\u2022 Red-teaming to inform Blue-teamnig - Actionable insights can be hard to gather from successful attacks as the element(s) that lead to success are not necessarily easy to identify and can be combinatorial. But once found, automated red-teaming methods can be used to generate synthetic attack data for blue teams at scale."}, {"title": "2.1.2 Attacking Approaches - Scaling and Automation", "content": "\u2022 Pros and cons of using safety datasets - Safety-related datasets, like those cataloged by R\u00f6ttger et al. [52], Xu et al. [64], can aid in red-teaming LLMs by providing pre-defined attack vectors. However, using these datasets for benchmarking poses risks: 1) These vectors are often tailored to specific LLMs and attack goals, which can create a false sense of security when applied to other models. 2) Pre-defined datasets may lack sufficient diversity and transferability, and relying on publicly available samples may not accurately represent the interaction profile in-the-wild.\n\u2022 Overlooked importance of attacker's knowledge - Discussions on adversarial attacks in GenAI often center around \"black swan\"-style incidental reports and model-centric narratives. While these highlight key vulnerabilities, they overlook the need for automated and scalable red-teaming tools. Additionally, the impact of decoding strategies, system prompts, and other hyperparameters is often underemphasized. Attacks usually hinge on domain knowledge, such as leveraging a low-resource language when the attacker is aware of the model's language coverage [67], or using optimization-based token attacks when white-box access is available [68]. Practically, red-teamers often face black-box scenarios, limiting their choice of tools.\n\u2022 Challenges in automation and scaling - Automating red-teaming requires tools that can create real-world attack vectors and adapt attack strategies for specific use cases. Various academic work like GCG [80], TAP [40], PAIR [11], AutoDAN [36] and tools like PyRIT [6] exist to help automate the synthesis of attack vectors. However, they vary in terms of resources required, have a narrow coverage across attack types, and do not necessarily create transferable attacks. Moreover, they all have artefacts that are amplified in the resultant attack vectors. For instance, adversarial attack vectors generated by methods such as TAP, GCG or PyRIT remain generally constrained by the target model selected for/during attack and evaluation as well as other contextual red-teaming features, such as selected seed data and the priming of their LLM components. Steps toward \"universal\" attacks have been taken [80], however the authors concede that in some cases, subsequent updates to a model, even a model of the same family, are sufficient for significantly reducing the ASR. This highlights the sensitivity of generated attack vectors to changes in the context of red-teaming efforts and present challenges for converging toward reliable red-teaming tools.\n\u2022 Diversity and relevance in automation - Automated red teaming methods such as PAIR [11] and TAP [40] suffer from diversity in their attacks as well as attacks that veer off-topic from the intended goal. This not only reduces efficiency due to redundancies and off-topic attacks but also leaves potential attack vectors uninvestigated. Methods such as Samvelyan et al. [53] address this by guiding the attacks in a matrix of attack styles and harm categories to ensure coverage but are limited to the provided attack styles and harm categories.\n\u2022 Economics - With new attacks constantly emerging, models changing due to fine-tuning, system prompt changes, and new documents in RAG storage, continuous red teaming is necessary. This can quickly cause significant costs and requires a trade-off decision between coverage and spending. [3] highlight that real-life attackers focus on simple and cheap high severity attacks. Defending against highly sophisticated low volume attacks is comparably less of a threat. From a game theoretical perspective, focusing on the highly likely attacks is a more effective strategy but unlikely high severity events still need to be evaluated from a regulatory compliance and ethical standpoint."}, {"title": "2.1.3 Evaluation Strategies", "content": "\u2022 Misalignment of goals - Academic red-teaming often concludes once a single attack vector succeeds. In industry, this is insufficient. Due to the unpredictable nature of LLMs and the ease of exploitation through natural language, attack success is an expected outcome from a practitioner's view. Academic work focuses on maximizing ASR values to claim state-of-the-art performance, which conflicts with assessing real harm. For example, an LLM outputting nonsensical code when prompted for malware may be seen as a jailbreak but poses no real threat. High ASR"}, {"title": "3 Blue-Teaming", "content": "Vulnerabilities exposed during a red-teaming exercise are typically defended by investigating ap-proaches as part of a corresponding blue-teaming effort. The choice of defense for GenAI is closely tied to the resources available to the defender. A resourceful defender may undertake comprehensive measures like safety training to align a model. However, as most practitioners only have access to model APIs, they are limited to practical approaches using black-box defenses performing in-put/output moderation or using specific safeguards based on system instructions [22], incorporating measures for access control [63] and enforcing structure/constraints within LLM interactions [66]. In the absence of resources, and for their model- and application-agnostic applicability, guardrails [49, 5] have evolved as the preferred solution to safeguard against jailbreaks and injections. However, this has raised many open questions."}, {"title": "3.1 Open Questions and Challenges", "content": "The space of adversarial attacks against generative systems is constantly evolving as new models and new applications paradigms continue to emerge. The rate of deployment of LLM based systems necessitates the use of stopgap solutions to defend against such attacks. While guardrails provide an effective approach, there are significant gaps in the way they are conceptualised, developed and evaluated in practice."}, {"title": "3.1.1 Guardrails - Scope and Applicability", "content": "\u2022 Attack intent vs attack success - A defender in their pursuit to outsmart an attacker is interested in blocking any and every attempt to sabotage a system. While red-teaming methods inform this process, a defender needs to take a broader view where they expand the set of successful attack vectors with attack attempts or attack intentions. This is specifically true for input guardrails which find use in pre-emptively filtering prompts before model inference.\n\u2022 Evolving taxonomy and moving target defense - As new attacks and defenses are discovered in the literature, the taxonomy of threats will evolve [16]. It's strategic to base guardrail policies on prevalent attack behaviors reflecting typical threats that an application expects, or additional information exposed for the underlying LLMs. For example, direct instructions or low-resource languages might be common attack techniques for models without safeguards or those not trained on multiple languages. Similarly, attacks noted on social media platforms might indicate a prevalence of Do-Anything-Now (DAN)-style attacks within typical prompt profiles.\n\u2022 Choosing guardrails - Existing input guardrails across literature vary from score-based filters (like perplexity [27]), to similarity detectors, to ML classifiers [26, 1] and in-context learners [63], and even other probing- [50, 49, 14] and decoding-based techniques [24] which vary across size, latency, throughput and performance. Practical constraints often require guardrails to be model-agnostic solutions, especially for LLM-embedded systems. Input guardrails are ideal for preventing attacks when minimizing LLM inference is cost effective. However, more complex orchestrations, using various input detectors, output filters, or model inspections, need to be systematized for effective defence."}, {"title": "3.1.2 Creating guardrails", "content": "\u2022 Tailored defenses for different attack types - Current approaches to guardrails typically use a one-size-fits-all model to defend against all attack types [26], which fails to capture the nuances of different threats. Not all attacks require the same solutions. For instance, complex attacks like the role-playing scenarios in DAN are often easier to detect (via semantic classifiers) due to their distinct features, such as elaborate language, social engineering tactics, and specific formatting [41]. Similarly, indirect injections like malicious URLs can be handled with rule-based filtering. Input guardrails are deployed alongside other filters, such as content moderation or on/off-topic filtering, to maximize effectiveness. Understanding these overlapping capabilities helps define the necessary restrictive behaviour for prompt attack guardrails. For example, if inputs are limited to short English sentences, modelling for context overload or encoding (e.g., Base64, Morse Code) [61] might be redundant.\n\u2022 Modelling guardrails - functional requirements - Guardrails aim to block or filter undesired input but may inadvertently block desirable inputs. Thus, defining clear boundaries of permissible inputs is crucial. Formal understanding and adequate sampling are key, especially when using machine learning or data-driven methods to model guardrails; failure to do so can lead to poor performance in real-world scenarios. As noted in Section 2, datasets for various attacks are often too simplistic or small. For instance, many samples in the ignore-instruction dataset start with phrases like \"ignore previous instructions\u201d which could lead an ML classifier to focus on superficial features, resulting in poor generalization to real-world cases. Recent work, such as [28], has introduced contrastive examples for guardrail training, but this approach is generally lacking in academic research. Moreover, there may be fundamental limitations to use of ML based approaches for censoring LLM inputs and outputs [19].\n\u2022 Non-functional requirements of guardrails - Guardrails must meet certain non-functional re-quirements. When used as pre-filters for LLMs, they should handle prompts of arbitrary context lengths or at least match the context length of the underlying models. This is crucial, as many attacks, such as overloaded context and role-playing, are typically long. For ML classifier-based guardrails, techniques like chunking or sliding windows can be helpful. Attacks may also use different languages; as models expand their multilingual capabilities, the definition of low-resource languages will change. Misbehavior varies across attack types and languages. When selecting an input guardrail, consider latency, throughput, and memory footprint. Some attacks may be manageable with smaller models (e.g., encoder-only models with ~100M parameters), while others require larger models for complex prompt semantics."}, {"title": "3.1.3 Evaluating and Benchmarking Guardrails", "content": "Protection vs utility trade-offs - Securing Gen AI in production requires thorough testing of guardrails. These guardrails can filter prompt traffic at different stages in large-scale LLM systems or agentic frameworks. There is often a trade-off between application utility and protection: permissive guardrails offer limited protection but maintain utility. As input guardrails will restrict any attack intent, they are inherently more restrictive than a detection scheme designed to restrict a successful attack. Therefore, it's crucial to test guardrails' performance using benign prompts. Similarly, these models require rigorous testing for exaggerated safety and, in the case of ML classifiers, should be evaluated against out-of-distribution samples.\nShortcomings across current benchmarking - Existing benchmarks [12, 39, 78] and leaderboards [12] have a narrow scope of evaluations. Our experiments highlight these shortcomings. To empiri-cally assess different guardrail's performances a benchmark on a cross section of defensive models on 19 different datasets showing results in Tables 1 and 2. Our evaluation pipeline is as follows: we fine tune a BERT model on 60% of the data as a training set, retaining 20% for validation and 20% for testing. Due to computational constraints, we subsample the test set for 200 prompt instances from each original dataset. We use this sub-sampled set to evaluate on three \"general purpose\" detectors and the fine-tuned BERT model. Furthermore, we also include malicious_instruct and toxicchat as datasets which the BERT classifier has not trained on for out-of-distribution comparison.\nWe see from the tables that defences can vary significantly in performance between dataset attack types highlighting the need for breadth of evaluation - e.g. a Vincuna-7b against general harmful prompts can have performance ranging from 0.24 TPR on the jailbreak prompts dataset to 0.97 on harmful behaviours.\nFurther, despite the quantity of datasets being produced for attacks this still only covers a small fraction of the possible input variations and perturbations. Existing benchmarking efforts such as [12] only contain a few hundred samples. This is further compounded that unlike in the image domain, we do not have with NLP 1) an effective optimisation process to search an input for jailbreak variations (current optimisers like GCG are comparatively much weaker than PGD[38]), 2) nor are the input constrained in the same manner - with the image domain adversarial examples were typically constrained to within a Lp ball of a semantically correct starting datapoint. However, in the LLM case the attacker has the flexibility to alter the whole prompt as they see fit to achieve their attack goals, making it challenging to formalize the notion of neighbourhood.\nThis renders open ended rigorous benchmarking challenging, thus motivating the focus on specific attacks which are both likely and of high severity.\nSpecialised classifiers such as the BERT model, do have competitive performance even on OOD datasets, and have the advantage of being significantly more lightweight then their LLM counterparts. However, it does suffer a higher FPR on xtest which is specifically checking for edge cases which the larger LLMs due to their more extensive pre-training are better able to handle."}, {"title": "5 Conclusions and Recommendations", "content": "Red- and Blue-teaming for generative AI has reached a divergence point where academic investiga-tions focus on elaborate attacks and defenses while practitioners are much more concerned about fending off lower-effort, high-likelihood, high-severity attacks in a budget constrained environment. We recommend that threat models for generative AI are enhanced to ground them in attacks that take place in the wild. This requires a shift in tooling and benchmarking tasks inspired by real-life attacks and resource constraints, creating visibility of what types of attacks exist. For instance, jailbreaks and injections are methods within the adversarial AI threat model that can be used to pursue attack goals that lead to misuse and compromise AI safety. Therefore, the taxonomy of misuse or safety which varies across domains, should be complemented with a security one. Such attack taxonomies are also central to the development and benchmarking of defences. We introduce the Attack Atlas as the first intuitive and organized analysis of single-turn input attack vectors to provide the community with a unified starting point in the rapidly growing field of generative AI security."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Datasets", "content": "We use some of the commonly used datasets for guardrail training and benchmarking within our evaluation setup. Table 4 maps the datasets to attack types. The inter-attack set similarity is higher (more dissimilar) and intra-attack set similartity is lower (more similar) which indicates the usefulness of viewing these the lens of attack atlas."}]}