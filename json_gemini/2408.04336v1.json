{"title": "KnowPC: Knowledge-Driven Programmatic\nReinforcement Learning for Zero-shot Coordination", "authors": ["Yin Gu", "Qi Liu", "Zhi Li", "Kai Zhang"], "abstract": "Zero-shot coordination (ZSC) remains a major challenge in the coop-\nerative AI field, which aims to learn an agent to cooperate with an un-\nseen partner in training environments or even novel environments. In recent\nyears, a popular ZSC solution paradigm has been deep reinforcement learn-\ning (DRL) combined with advanced self-play or population-based methods\nto enhance the neural policy's ability to handle unseen partners. Despite\nsome success, these approaches usually rely on black-box neural networks as\nthe policy function. However, neural networks typically lack interpretability\nand logic, making the learned policies difficult for partners (e.g., humans)\nto understand and limiting their generalization ability. These shortcomings\nhinder the application of reinforcement learning methods in diverse coopera-\ntive scenarios. In this paper, we suggest to represent the agent's policy with\nan interpretable program. Unlike neural networks, programs contain stable\nlogic, but they are non-differentiable and difficult to optimize. To automat-\nically learn such programs, we introduce Knowledge-driven Programmatic\nreinforcement learning for zero-shot Coordination (KnowPC). We first define\na foundational Domain-Specific Language (DSL), including program struc-\ntures, conditional primitives, and action primitives. A significant challenge\nis the vast program search space, making it difficult to find high-performing\nprograms efficiently. To address this, KnowPC integrates an extractor and\nan reasoner. The extractor discovers environmental transition knowledge", "sections": [{"title": "1. Related Work", "content": "Collaboration between agents or between agents and humans is common\nin various scenarios, such as industrial robots [1, 2, 3], game AI [4, 5, 6, 7], and\nautonomous driving [8, 9]. In these open scenarios, agents cannot anticipate\nthe policies of the partners they will collaborate with, so they must be capable\nof cooperating with a wide range of unseen policies. This task is known as\nzero-shot coordination (ZSC) [10, 11].\nIn the literature, a prevailing approach to addressing ZSC has been deep\nreinforcement learning (DRL) [12] coupled with improved self-play [13] or\npopulation-based methods [14]. Self-play [13, 15] is a straightforward method\nwhere an agent plays with itself to iteratively optimize its policy. However,\nself-play tends to make the agent converge to specific behavior patterns, mak-\ning it difficult to handle diverse partners [10]. Subsequently, various methods\nhave been proposed to expose agents to diverse policies during training, aim-\ning for better generalization. Population-based training (BPT) [14, 16] main-\ntains a diverse population and optimizes individual combinations iteratively.\nSeveral improvements to BPT have been introduced [17, 18, 19, 20, 11]. For\ninstance, TrajeDi [17] encourages diversity in agents' trajectory distributions.\nMEP [19] adds entropy of the average policy as an optimization objective to\nobtain a diverse population. Recently, E3T [21] improved the self-play al-\ngorithm by mixing ego policy and random policy to promote diversity in\npartner policies.\nDespite the success of existing work, there are still two major drawbacks.\nFirst, the neural network policies of DRL are not interpretable and are still\nconsidered a black box [22]. In cooperative decision-making scenarios, the"}, {"title": "1.1. Zero-shot Coordination", "content": "The mainstream approach to ZSC is to combine DRL and improved self-\nplay or population-based training to develop policies that can effectively\ncooperate with unknown partners. Traditional self-play [13, 15] methods\ncontrol multiple agents by sharing policies and continuously optimizing the\npolicy. However, self-play policies often perform poorly with unseen part-\nners due to exhibiting a single behavior pattern. Other-play [10] exploits\nenvironmental symmetry to perturb agent policies and prevent them from\ndegenerating into a single behavior pattern. Recent E3T [21] improved the\nself-play algorithm by mixing ego policy and random policy to promote di-\nversity in partner policies and introduced an additional teammate modeling\nmodule to predict teammate action probabilities. Population-based meth-\nods [14, 16, 17, 18, 19, 20, 11, 29] maintain a diverse population to train\nrobust policies. Some advanced population-based methods enhance the diver-\nsity in different ways: FCP [18] preserves policies from different checkpoints\nduring self-play training to increase population diversity, TrajeDi [17] max-\nimizes differences in policy trajectory distributions, and MEP [19] adds the\nentropy of the average policy in the population as an additional optimization\nobjective. COLE [11] reformulates cooperative tasks as graphic-form games\nand iteratively learns a policy that approximates the best responses to the\ncooperative incompatibility distribution in the recent population."}, {"title": "1.2. Programmatic Reinforcement Learning", "content": "Programmatic reinforcement learning represents its policies using inter-\npretable symbolic languages [28], including decision trees [30, 31, 32, 33],\nstate machines [34], and mathematical expressions [35, 36]. The main chal-\nlenge in programmatic reinforcement learning is the need to search for pro-\ngrams in a vast, non-differentiable space. Based on their learning or search\nmethods, programmatic reinforcement learning approaches can be catego-\nrized into three types: imitation learning-based, differentiable architecture,\nand search-based.\nImitation learning-based methods [31, 37, 38, 34] first train a DRL policy\nto collect trajectory data, then use this data to learn interpretable program-\nmatic policies. Differentiable architecture methods [33, 39, 36, 40] typically\nuse an actor-critic framework [41], where the actor is a differentiable pro-\ngram or decision tree, and the critic is a neural network. Since raw programs\nare not differentiable, these methods use relaxation techniques to make the\nprogram structure differentiable. For instance, ICCTs [39] use sigmoid func-\ntions to compute the probabilities of visiting the left and right child nodes of\na decision tree node, and recursively compute the probabilities of each leaf\nnode in the tree, thus making the entire decision tree differentiable. Simi-\nlarly, PRL [40] also uses sigmoid functions to compute the probabilities of left\nand right branches. However, the program structures in these methods are\nnot very flexible, as they only allow if-else-then branches and do not permit\nsequential execution logic.\nBecause programs are discrete and difficult to optimize using gradient-\nbased methods, a more straightforward approach is to search for the desired\nprograms to use as policies. Search-based methods include genetic algo-\nrithms [42, 43, 44, 45, 46], Monte Carlo Tree Search (MCTS) [47, 48, 49],\nand DRL. DSP [35] uses DRL policies to output discrete mathematical ex-\npression tokens as control strategies, using risk-seeking gradient descent to\noptimize policy parameters. \u03c0-light [50] predefines part of the program struc-\nture and then uses MCTS to search for the remaining parts of the program.\nA notable variant of search-based methods is LEAPS [51], which first learns\na continuous program embedding space for discrete programs, then searches"}, {"title": "2. Preliminary", "content": "However, the aforementioned approaches do not extract and utilize envi-\nronmental transition knowledge to accelerate the learning of programmatic\npolicies. In contrast, KnowPC can infer the logical rules that programs must\nfollow based on discovered transition knowledge."}, {"title": "2.1. Environment", "content": "We choose the well-established multi-agent coordination suite Overcooked [16]\nas the experimental environment. Overcooked is a grid environment where\nagents independently control two chefs to make soup and deliver it. There are three types of objects in the\nenvironment: dishes, onions, and soup, along with four types of interaction\npoints: onion dispenser, dish dispenser, counter, and pot. The chefs need to\nplace three onions into a pot, wait for it to cook for 20 time steps, and then\nuse an empty dish to serve the soup from the pot. After delivering the soup,\nall agents receive a reward of 20. Additionally, chefs can place any object\nthey are holding onto an empty counter or pick up an object from a non-\nempty counter (provided the chef's hands are empty). Counters and pots\nare stateful interaction points, while onion dispensers and dish dispensers\nare stateless interaction points.\nThe two chefs on the field share the same discrete action space: up, down,\nleft, right, noop, and \u201cinteract\u201d. In each episode, an agent is randomly"}, {"title": "2.2. Cooperative Multi-agent MDP", "content": "Assigned to control one of the chefs. We refer to the controlled chef as the\nplayer and the other chef as the teammate. The teammate may be controlled\nby another unknown agent or a human. When trained in a self-play manner,\nthe teammate is controlled by a copy of the current agent.\nAs introduced in early work [16, 21], in Overcooked, agents need to learn\nhow to navigate, interact with objects, pick up the right objects, and place\nthem in the correct locations. Most importantly, agents need to effectively\ncooperate with unseen agents.\nTwo-player Cooperative Markov Decision Process: A two-player Markov\nDecision Process (MDP) is defined by a tuple (S, A1, A2, T,R). Here, S\nis a finite state space, and A\u2081 and A2 are the action spaces of the two\nagents, which we assume to be the same. The transition function T maps the\ncurrent state and all agents' actions to the next state. The reward function\nR determines a real-valued reward based on the current state and all agents'\nactions, and this reward is shared among the agents. \u03c0\u2081 and \u03c0\u2082 are the\npolicies of the two agents. Their goal is to maximize the cumulative reward:\n\u03a3=1R(st, a1t, a2t), where a\u2081 ~ \u03c0\u2081(st) and a2t ~ \u03c0\u2082(st). Here, H is a finite\nH\ntime horizon."}, {"title": "3. KnowPC Method", "content": "In this section, we introduce the proposed KnowPC. As shown in Fig-\nure 2, KnowPC includes an extractor, a reasoner, and a program synthe-\nsizer. In KnowPC, the program serves as a decentralized policy to control\none game character. During training, we adopt a simple self-play method\nwhere the programmatic policy is shared among all agents. Both agents ex-\nplore with the e-greedy strategy [54], meaning they choose a random action\nwith probability e and choose the action output by the program with prob-\nability 1 - \u20ac. The extractor aims to extract the environment's transition\nknowledge from multi-agent interaction trajectories. The reasoner uses these\ntransition knowledge to infer the preconditions of action primitives to guide\nprogram synthesis.\nSeveral previous studies [55, 56, 57, 58] have used learned transition\nknowledge to improve the learning efficiency of DRL. This is often achieved\nby providing intrinsic rewards to DRL agents, which help address sparse-\nreward and hierarchical tasks. Instead of introducing additional intrinsic\nrewards for training agents, we infer the preconditions of action primitives\nto directly synthesize symbolic policies."}, {"title": "3.1. Domain-Specific Language", "content": "Our policies are entirely composed of interpretable programmatic lan-\nguage. In this subsection, we formally define the DSL that constructs our\nprograms. As shown in Figure 3 and Figure 4, the DSL includes control\nflows (e.g., IT modules, sequential execution module E), condition primi-\ntives B, and action primitives A. In an IT module, B can be referred to as\nthe precondition of A. The action primitive A will only be executed when B\nis true.\nThe program structure allowed by the DSL is not fixed. For instance,\nany number of IT modules can be added to E, or there can be any number"}, {"title": "3.2. Extractor", "content": "Extractor aims to mine environment transition rules from multi-agent\ninteraction trajectories. There are various types of interaction points in the\nenvironment, and each type may have several instances. Additionally, there\nare two roles: player and teammate. The agent's actions may change the state\nof the player or the interaction points, while the environment also undergoes\nspontaneous changes (e.g., the cooking time of food in the pot continuously\nincreases). The goal of the extractor is to uncover concise transition rules\nthat describe the complete dynamics of the environment. The main challenge\nin extracting transition rules is determining which transitions are caused by\nthe agent itself (rather than by the teammate) and which transitions are\nspontaneous.\nWe focus on the player and interaction points in the environment. Sup-\npose there are N elements in the environment (elements include players and\ninteraction points), and the information of element i at time t and t +1\ncan be represented as Iit and Iit+1. For readability, we remove the t-related"}, {"title": "3.2.1. Player-caused Transitions", "content": "Extractor determines whether a transition is caused by the player's ac-\ntions based on the action statistics of the transition. Intuitively, if the action\nprobability distribution is flat, it means that the transition will occur re-\ngardless of the actions taken by the agent. In other words, the transition is\nlikely not caused by the agent's actions. Conversely, if the action probability\ndistribution is concentrated, the transition is likely caused by the player.\nSince our transitions are symbolic, we can directly count the number of\nactions of each transition to calculate the action probability p. The entropy\nof the action probability p is defined as follows:\nH(p) = - \u03a3P(a) log P(a)\n\u03b1\u0395\u0391\nwhere P(a) represents the action probability, and H(p) is the entropy.\nThe smaller the entropy, the more concentrated the probability distribution.\nIf the H(p) of a transition is relatively small, it is very likely caused by the\nplayer. Here, we introduce a threshold d to filter out transitions with entropy\ngreater than d. The found player-caused transitions are denoted by Tp, and\nthe most frequent action a of the Tp is also recorded. We use Dp to represent\nthe set of Tp and a, Dp = {(Tp,a)}.\nThe found T may contain irrelevant C. For example, a C caused by the\nteammate is observed by the player and cannot be distinguished based on\nH(p). To remove redundant Tp in Dp, we compare all Tp pairwise and use\nshorter transitions to exclude longer transitions that contain it. For instance,\nif there are two Tp, T\u2081 = {C1, C2} and T2 = {C1, C2, C3}, T2 contains T\u2081 and\nhas an extra C3. According to T\u2081, we know that C\u2081 and C2 are caused by\nthe player, while C3 in T2 is not. Therefore, T\u2082 is redundant and should be\nexcluded."}, {"title": "3.2.2. Teammate-caused Transitions", "content": "If the player and the teammate have the same functionalities in the en-\nvironment, and they would follow the same transition rules. Given the Tp\nset, we can shift to the teammate's perspective to identify teammate-caused\ntransition within a transition. If their functionalities differ, we would need\nto separately identify player-caused and teammate-caused transitions."}, {"title": "3.2.3. Spontaneously Occurring Transitions", "content": "The transitions observed by the agent at any given moment include self-\ncaused transitions, teammate-caused transitions, and spontaneously occur-\nring transitions. The complete transition excluding the player-caused and\nteammate-caused transitions, leaves the environment's spontaneous transi-\ntions Ts. We use Ds to represent the set of Ts, Ds = {Ts}."}, {"title": "3.3. Reasoner", "content": "The environment's transition rules describe how information about the\nplayer and interaction points changes. Based on this information, the rea-\nsoner can construct a transition graph, where the nodes include element\ninformation and actions. By traversing this graph, the reasoner should infer"}, {"title": "3.4. Program synthesizer", "content": "The program synthesizer will synthesize programs that conform to a given\nDSL. However, due to the vast program space, directly searching for high-\nperforming programs within the given space is highly inefficient. To overcome\nthis difficulty, our program synthesizer leverages the output of the reasoner,\nspecifically the preconditions for each action primitive. These preconditions\ncan be used to guide the synthesizer in generating reasonable programs, sig-\nnificantly reducing the search space.\nAs mentioned in the DSL subsection, our program structure adopts a list-\nlike program. Each IT module in the program has variable A and B, which\nneed to be determined by the synthesizer. We implement the synthesizer\nusing a genetic algorithm [42, 43], which includes selection and crossover\noperations. Initially, programs are randomly generated as the initial pop-\nulation. In each iteration, the crossover operation randomly selects parent\nprograms and exchanges their program fragments to synthesize offspring.\nDuring the selection operation, programs with higher cumulative rewards in\nself-play are retained. We exclude the mutation step because it might alter\nthe preconditions of action primitives, causing them to violate the require-\nments inferred by the reasoner. Additionally, given a state, it is possible that\nnone of the B conditions in a program's IT modules are satisfied, resulting\nin an empty output. To prevent it, we append a random action at the end\nof every program.\nAfter the genetic algorithm completes the search, we evaluate the discov-\nered programs. During evaluation, we no longer use e-greedy exploration."}, {"title": "3.4.1. Program Search Space Analysis", "content": "The defined DSL contains 9 action primitives and 13 types of condition\nprimitives (excluding the negation of conditions). Suppose a program uses 8\nof these action primitives and 12 of these condition primitives, and the pro-\ngram has 8 IT modules, with each IT module containing up to 4 conditions.\nThis results in approximately 1.64 \u00d7 1039 different possible programs.\nThe calculation process is as follows. First, we calculate the number of\ncombinations of conditions in an IT module, selecting 1 to 4 conditions from\n12 different ones, with each condition primitive being able to be negated:\nC12\u00d721+C22\u00d722 + C32 \u00d7 23 + C12 \u00d7 24 = 9968. Since there are 8 IT modules,\neach action in an IT module has 8 possible choices: (8\u00d79968) \u2248 1.64 \u00d7 1039.\nThis calculation demonstrates that even for a moderately sized program, the\npotential combinatorial space is vast."}, {"title": "4. Experiments", "content": "In this section, we evaluate KnowPC's ZSC and ZSC+ capabilities across\nmultiple layouts in the Overcooked environment [16]. We compare the per-\nformance of KnowPC with six baselines: Self-Play (SP) [13, 16], Population-\nBased Training (PBT) [14, 16], Fictitious Co-Play (FCP) [18], Maximum-\nEntropy Population-Based Training (MEP) [19], Cooperative Open-ended\nLearning (COLE) [11], and Efficient End-to-End Training (E3T) [21]. All\nof them use PPO [64] as the RL algorithm and belong to DRL methods.\nAmong them, SP and E3T are based on self-play, while the other algorithms\nare population-based, requiring the maintenance of a diverse population."}, {"title": "4.1. Experimental Setup", "content": "The parameter settings for KnowPC are consistent across different lay-\nouts. During training, the exploration probability e is set to 0.3, and the\nthreshold d is set to 0.1. The genetic algorithm undergoes 50 iterations, with\nan initial population size of 200 and a subsequent population size maintained\nat 10.\nIt is worth noting that previous works have utilized shaped reward func-\ntions [16] to train agents, such as giving extra rewards for events like plac-\ning an onion into the pot, picking up a dish, or making a soup. This ap-\nproach helps to accelerate convergence and improve performance. In contrast,\nKnowPC is not sensitive to the reward function. We directly use a sparse\nreward function (i.e., only get a reward when delivering soup). Addition-\nally, the input encoding for DRL methods uses a lossless state encoding [16],\nwhich includes multiple matrices with sizes corresponding to the environment\ngrid size. In terms of state encoding, DRL has more complete information\ncompared to KnowPC."}, {"title": "4.2. ZSC Experiment Results", "content": "We evaluate the ZSC capabilities of each method by letting each method's\npolicies cooperate with each other. During training, none of them can access"}, {"title": "4.2.1. Collaboration between RL Agents.", "content": "We evaluate the ZSC capabilities of each method by letting each method's\npolicies cooperate with each other. During training, none of them can access"}, {"title": "4.2.2. Collaboration with Human Proxies", "content": "Apart from collaborating with AI partners, an RL agent also needs to\nwork with human partners. Following previous work [11, 62], we use behavior-\ncloned models trained on human data as human proxies. Figure 8 presents\nthe results on 5 layouts. KnowPC generally performs well overall. As noted\nin previous work, no method consistently outperforms the others. KnowPC\nachieves significantly higher results than the baselines in two layouts (Asym-\nmetric Advantages and Forced Coordination). In two other layouts (Cramped\nRoom and Coordination Ring), our results are slightly lower than the best\nbaseline. Possibly because Cramped Room and Coordination Ring require\nmore consideration and modeling of the teammates. Integrating agent mod-\neling techniques [21] with KnowPC is a potential direction for future research."}, {"title": "4.2.3. Training Efficiency Analysis", "content": "Table 1 shows the training times for all algorithms on a single layout.\nIt can be observed that population-based methods (e.g., \u041c\u0415\u0420, COLE) gen-\nerally require more training time than self-play methods (e.g., SP, E3T).\nOur method is efficient. Benefiting from reasoning in the abstract space and\nnot requiring extensive parameter optimization like DRL, it has the shortest\ntraining time among all methods. For instance, its training time is one 360th\nof previously advanced COLE and one 19th of the state-of-the-art E3T."}, {"title": "4.3. Policy Interpretability", "content": "Listing 1 illustrates a program found by KnowPC on the Counter Circuit\nenvironment. Unlike the DRL policy, the program is fully interpretable, with\ntransparent decision logic. For example, the logic expressed by the fourth IT\nmodule is that if there is an onion dispenser and an idle pot, and no ready"}, {"title": "4.4. ZSC+ Experiment Results", "content": "Layout variations are common, as rooms in different buildings often have\ndifferent layouts. A good RL policy should be robust to these layout changes."}, {"title": "4.5. Ablation Study", "content": "To validate the effectiveness of the knowledge-driven extractor and rea-\nsoner, we first visualized a transition graph. As shown in Figure 6, the\nextractor identified the correct environment transition rules. The transition\ngraph clearly shows how the states of players and interaction points change"}, {"title": "5. Conclusion and Future Work", "content": "In this paper, we propose Knowledge-driven Programmatic reinforcement\nlearning for zero-shot Coordination (KnowPC), which deploys programs as\nagent control policies. KnowPC extracts and generalizes knowledge about\nthe environment and performs efficient reasoning in the symbolic space to\nsynthesize programs that meet logical constraints. KnowPC integrates an\nextractor to uncover transition knowledge of the environment, and a rea-\nsoner to deduce the preconditions of action primitives based on this transi-\ntion knowledge. The program synthesizer then searches for high-performing\nprograms based on the given DSL and the deduced preconditions. Compared\nto DRL-based methods, KnowPC stands out for its interpretability, general-\nization performance, and robustness to sparse rewards. Its policies are fully\ninterpretable, making it easier for humans to understand and debug them.\nEmpirical results on Overcooked demonstrate that, even in sparse reward\nsettings, KnowPC can achieve superior performance compared to advanced\nbaselines. Moreover, when the environment changes, program-based poli-\ncies remain robust, while DRL baselines experience significant performance\ndeclines.\nThe limitation of our method is that it requires the definition of some\nbasic conditional primitives and action primitives. This abstraction of the"}]}