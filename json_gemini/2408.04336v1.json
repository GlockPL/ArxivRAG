{"title": "KnowPC: Knowledge-Driven Programmatic\nReinforcement Learning for Zero-shot Coordination", "authors": ["Yin Gu", "Qi Liu", "Zhi Li", "Kai Zhang"], "abstract": "Zero-shot coordination (ZSC) remains a major challenge in the coop-\nerative AI field, which aims to learn an agent to cooperate with an un-\nseen partner in training environments or even novel environments. In recent\nyears, a popular ZSC solution paradigm has been deep reinforcement learn-\ning (DRL) combined with advanced self-play or population-based methods\nto enhance the neural policy's ability to handle unseen partners. Despite\nsome success, these approaches usually rely on black-box neural networks as\nthe policy function. However, neural networks typically lack interpretability\nand logic, making the learned policies difficult for partners (e.g., humans)\nto understand and limiting their generalization ability. These shortcomings\nhinder the application of reinforcement learning methods in diverse coopera-\ntive scenarios. In this paper, we suggest to represent the agent's policy with\nan interpretable program. Unlike neural networks, programs contain stable\nlogic, but they are non-differentiable and difficult to optimize. To automat-\nically learn such programs, we introduce Knowledge-driven Programmatic\nreinforcement learning for zero-shot Coordination (KnowPC). We first define\na foundational Domain-Specific Language (DSL), including program struc-\ntures, conditional primitives, and action primitives. A significant challenge\nis the vast program search space, making it difficult to find high-performing\nprograms efficiently. To address this, KnowPC integrates an extractor and\nan reasoner. The extractor discovers environmental transition knowledge", "sections": [{"title": "Keywords:", "content": "Zero-shot Coordination, Programmatic Reinforcement Learning\nCollaboration between agents or between agents and humans is common\nin various scenarios, such as industrial robots [1, 2, 3], game AI [4, 5, 6, 7], and\nautonomous driving [8, 9]. In these open scenarios, agents cannot anticipate\nthe policies of the partners they will collaborate with, so they must be capable\nof cooperating with a wide range of unseen policies. This task is known as\nzero-shot coordination (ZSC) [10, 11].\nIn the literature, a prevailing approach to addressing ZSC has been deep\nreinforcement learning (DRL) [12] coupled with improved self-play [13] or\npopulation-based methods [14]. Self-play [13, 15] is a straightforward method\nwhere an agent plays with itself to iteratively optimize its policy. However,\nself-play tends to make the agent converge to specific behavior patterns, mak-\ning it difficult to handle diverse partners [10]. Subsequently, various methods\nhave been proposed to expose agents to diverse policies during training, aim-\ning for better generalization. Population-based training (BPT) [14, 16] main-\ntains a diverse population and optimizes individual combinations iteratively.\nSeveral improvements to BPT have been introduced [17, 18, 19, 20, 11]. For\ninstance, TrajeDi [17] encourages diversity in agents' trajectory distributions.\nMEP [19] adds entropy of the average policy as an optimization objective to\nobtain a diverse population. Recently, E3T [21] improved the self-play al-\ngorithm by mixing ego policy and random policy to promote diversity in\npartner policies.\nDespite the success of existing work, there are still two major drawbacks.\nFirst, the neural network policies of DRL are not interpretable and are still\nconsidered a black box [22]. In cooperative decision-making scenarios, the"}, {"title": "", "content": "interpretability of policies is important. Especially when cooperating with\nhumans, if the agent's policies and behaviors can be understood by human\npartners, it can greatly increase human trust and promote cooperative effi-\nciency [23]. Secondly, neural policies lack inherent logic [24] and mostly seek\nto fit the correlation between actions and expected returns, which makes\ntheir policies less robust and limits their generalization performance. This\npaper considers two forms of generalization tasks. One is to cooperate with a\nwide range of unseen partners [10, 11, 25], i.e., zero-shot coordination (ZSC),\nwhich is the task mainly considered in existing work. The other is to co-\noperate with unknown partners in unseen scenario layouts [26, 27], which\nwe name ZSC+. Layout variations are relatively common, such as different\nroom layouts in different households or different layouts in games' maps. A\ngood agent should be robust to such variations and be able to cooperate\nwith unknown policies in any layout, rather than being limited to a specific\nlayout. Clearly, ZSC+ is more challenging than ZSC and imposes higher\nrequirements on the generalization performance of agents.\nIn stark contrast to neural policies, programmatically represented poli-\ncies are fully interpretable [28] and possess stable logical rules, leading to\nbetter generalization performance. However, they are difficult to optimize\nor learn owing to their discrete and non-differentiable nature. To efficiently\ndiscover programs through trial and error, we propose Knowledge-Driven\nProgrammatic reinforcement learning for zero-shot Coordination (KnowPC).\nIn this paper, knowledge refers to the environment's transition rules, describ-\ning how elements in the environment change. KnowPC explicitly discovers\nand utilizes these transition rules to synthesize decision-logic-compliant pro-\ngrams as the agent's control policy. The training paradigm of KnowPC fol-\nlows self-play, where in each episode, a programmatic policy is shared among\nall agents. KnowPC integrates an extractor, an reasoner, and a program\nsynthesizer. Specifically, the extractor identifies concise transition rules\nfrom multi-agent interaction trajectories and distinguishes between agent-\ncaused transitions and spontaneous environmental transitions. The program\nsynthesizer synthesizes programs based on a defined Domain-Specific Lan-\nguage (DSL). A significant technical challenge lies in the exponential in-\ncrease of the program space with the program length. To tackle it, The\nreasoner uses the identified transition rules to determine the prerequisites of\ntransitions, thereby establishing the preconditions for certain actions. This\nconstrains the program search space of the synthesizer, improving search\nefficiency. The contributions of this paper are summarized as follows:"}, {"title": "", "content": "\u2022 We introduce programmatic reinforcement learning in the ZSC task.\nCompared to neural policies, programmatic policies are fully inter-\npretable and follow exact logical rules.\n\u2022 The presented KnowPC explicitly extracts and leverages environmen-\ntal knowledge and performs efficient reasoning in symbolic space to\nprecisely synthesize programs that meet logical constraints.\n\u2022 We consider a more complex task, ZSC+, which poses higher require-\nments on the generalization ability of agents. Extensive experiments\non the well-established Overcooked [16] demonstrate that even with\nsimple self-play training, KnowPC's policies outperform the existing\nmethods in ZSC. Particularly, its generalization performance in ZSC+\nfar exceeds that of advanced baseline methods."}, {"title": "1. Related Work", "content": "1.1. Zero-shot Coordination\nThe mainstream approach to ZSC is to combine DRL and improved self-\nplay or population-based training to develop policies that can effectively\ncooperate with unknown partners. Traditional self-play [13, 15] methods\ncontrol multiple agents by sharing policies and continuously optimizing the\npolicy. However, self-play policies often perform poorly with unseen part-\nners due to exhibiting a single behavior pattern. Other-play [10] exploits\nenvironmental symmetry to perturb agent policies and prevent them from\ndegenerating into a single behavior pattern. Recent E3T [21] improved the\nself-play algorithm by mixing ego policy and random policy to promote di-\nversity in partner policies and introduced an additional teammate modeling\nmodule to predict teammate action probabilities. Population-based meth-\nods [14, 16, 17, 18, 19, 20, 11, 29] maintain a diverse population to train\nrobust policies. Some advanced population-based methods enhance the diver-\nsity in different ways: FCP [18] preserves policies from different checkpoints\nduring self-play training to increase population diversity, TrajeDi [17] max-\nimizes differences in policy trajectory distributions, and MEP [19] adds the\nentropy of the average policy in the population as an additional optimization\nobjective. COLE [11] reformulates cooperative tasks as graphic-form games\nand iteratively learns a policy that approximates the best responses to the\ncooperative incompatibility distribution in the recent population."}, {"title": "1.2. Programmatic Reinforcement Learning", "content": "Programmatic reinforcement learning represents its policies using inter-\npretable symbolic languages [28], including decision trees [30, 31, 32, 33],\nstate machines [34], and mathematical expressions [35, 36]. The main chal-\nlenge in programmatic reinforcement learning is the need to search for pro-\ngrams in a vast, non-differentiable space. Based on their learning or search\nmethods, programmatic reinforcement learning approaches can be catego-\nrized into three types: imitation learning-based, differentiable architecture,\nand search-based.\nImitation learning-based methods [31, 37, 38, 34] first train a DRL policy\nto collect trajectory data, then use this data to learn interpretable program-\nmatic policies. Differentiable architecture methods [33, 39, 36, 40] typically\nuse an actor-critic framework [41], where the actor is a differentiable pro-\ngram or decision tree, and the critic is a neural network. Since raw programs\nare not differentiable, these methods use relaxation techniques to make the\nprogram structure differentiable. For instance, ICCTs [39] use sigmoid func-\ntions to compute the probabilities of visiting the left and right child nodes of\na decision tree node, and recursively compute the probabilities of each leaf\nnode in the tree, thus making the entire decision tree differentiable. Simi-\nlarly, PRL [40] also uses sigmoid functions to compute the probabilities of left\nand right branches. However, the program structures in these methods are\nnot very flexible, as they only allow if-else-then branches and do not permit\nsequential execution logic.\nBecause programs are discrete and difficult to optimize using gradient-\nbased methods, a more straightforward approach is to search for the desired\nprograms to use as policies. Search-based methods include genetic algo-\nrithms [42, 43, 44, 45, 46], Monte Carlo Tree Search (MCTS) [47, 48, 49],\nand DRL. DSP [35] uses DRL policies to output discrete mathematical ex-\npression tokens as control strategies, using risk-seeking gradient descent to\noptimize policy parameters. \u03c0-light [50] predefines part of the program struc-\nture and then uses MCTS to search for the remaining parts of the program.\nA notable variant of search-based methods is LEAPS [51], which first learns\na continuous program embedding space for discrete programs, then searches"}, {"title": "", "content": "for continuous vectors in this space using the cross-entropy method [52], and\ndecodes them into discrete programs. Subsequent HPRL [53] improved its\nsearch method.\nHowever, the aforementioned approaches do not extract and utilize envi-\nronmental transition knowledge to accelerate the learning of programmatic\npolicies. In contrast, KnowPC can infer the logical rules that programs must\nfollow based on discovered transition knowledge."}, {"title": "2. Preliminary", "content": "2.1. Environment"}, {"title": "2.2. Cooperative Multi-agent MDP", "content": "Two-player Cooperative Markov Decision Process: A two-player Markov\nDecision Process (MDP) is defined by a tuple (S, A1, A2, T,R). Here, S\nis a finite state space, and A\u2081 and A2 are the action spaces of the two\nagents, which we assume to be the same. The transition function T maps the\ncurrent state and all agents' actions to the next state. The reward function\nR determines a real-valued reward based on the current state and all agents'\nactions, and this reward is shared among the agents. \u03c0\u2081 and \u03c0\u2082 are the\npolicies of the two agents. Their goal is to maximize the cumulative reward:\n$\\sum_{t=1}^{H}R(s_t, a_1^t, a_2^t)$, where $a_1 \\sim \\pi_1(s_t)$ and $a_2 \\sim \\pi_2(s_t)$. Here, H is a finite\ntime horizon."}, {"title": "3. KnowPC Method", "content": "In this section, we introduce the proposed KnowPC. As shown in Fig-\nure 2, KnowPC includes an extractor, a reasoner, and a program synthe-\nsizer. In KnowPC, the program serves as a decentralized policy to control\none game character. During training, we adopt a simple self-play method\nwhere the programmatic policy is shared among all agents. Both agents ex-\nplore with the e-greedy strategy [54], meaning they choose a random action\nwith probability e and choose the action output by the program with prob-\nability 1 - \u20ac. The extractor aims to extract the environment's transition\nknowledge from multi-agent interaction trajectories. The reasoner uses these\ntransition knowledge to infer the preconditions of action primitives to guide\nprogram synthesis.\nSeveral previous studies [55, 56, 57, 58] have used learned transition\nknowledge to improve the learning efficiency of DRL. This is often achieved\nby providing intrinsic rewards to DRL agents, which help address sparse-\nreward and hierarchical tasks. Instead of introducing additional intrinsic\nrewards for training agents, we infer the preconditions of action primitives\nto directly synthesize symbolic policies."}, {"title": "3.1. Domain-Specific Language", "content": "Our policies are entirely composed of interpretable programmatic lan-\nguage. In this subsection, we formally define the DSL that constructs our\nprograms. As shown in Figure 3 and Figure 4, the DSL includes control\nflows (e.g., IT modules, sequential execution module E), condition primi-\ntives B, and action primitives A. In an IT module, B can be referred to as\nthe precondition of A. The action primitive A will only be executed when B\nis true.\nThe program structure allowed by the DSL is not fixed. For instance,\nany number of IT modules can be added to E, or there can be any number"}, {"title": "3.2. Extractor", "content": "Extractor aims to mine environment transition rules from multi-agent\ninteraction trajectories. There are various types of interaction points in the\nenvironment, and each type may have several instances. Additionally, there\nare two roles: player and teammate. The agent's actions may change the state\nof the player or the interaction points, while the environment also undergoes\nspontaneous changes (e.g., the cooking time of food in the pot continuously\nincreases). The goal of the extractor is to uncover concise transition rules\nthat describe the complete dynamics of the environment. The main challenge\nin extracting transition rules is determining which transitions are caused by\nthe agent itself (rather than by the teammate) and which transitions are\nspontaneous.\nWe focus on the player and interaction points in the environment. Sup-\npose there are N elements in the environment (elements include players and\ninteraction points), and the information of element i at time t and t +1\ncan be represented as $I_i^t$ and $I_i^{t+1}$. For readability, we remove the t-related"}, {"title": "3.2.1. Player-caused Transitions", "content": "Extractor determines whether a transition is caused by the player's ac-\ntions based on the action statistics of the transition. Intuitively, if the action\nprobability distribution is flat, it means that the transition will occur re-\ngardless of the actions taken by the agent. In other words, the transition is\nlikely not caused by the agent's actions. Conversely, if the action probability\ndistribution is concentrated, the transition is likely caused by the player.\nSince our transitions are symbolic, we can directly count the number of\nactions of each transition to calculate the action probability p. The entropy\nof the action probability p is defined as follows:\n$H(p) = - \\sum_{a\\in A}P(a) \\log P(a)$ (1)\nwhere P(a) represents the action probability, and H(p) is the entropy.\nThe smaller the entropy, the more concentrated the probability distribution.\nIf the H(p) of a transition is relatively small, it is very likely caused by the\nplayer. Here, we introduce a threshold d to filter out transitions with entropy\ngreater than d. The found player-caused transitions are denoted by Tp, and\nthe most frequent action a of the Tp is also recorded. We use Dp to represent\nthe set of Tp and a, Dp = {(Tp,a)}.\nThe found T may contain irrelevant C. For example, a C caused by the\nteammate is observed by the player and cannot be distinguished based on\nH(p). To remove redundant Tp in Dp, we compare all Tp pairwise and use\nshorter transitions to exclude longer transitions that contain it. For instance,\nif there are two Tp, T\u2081 = {C1, C2} and T2 = {C1, C2, C3}, T2 contains T\u2081 and\nhas an extra C3. According to T\u2081, we know that C\u2081 and C2 are caused by\nthe player, while C3 in T2 is not. Therefore, T\u2082 is redundant and should be\nexcluded."}, {"title": "3.2.2. Teammate-caused Transitions", "content": "If the player and the teammate have the same functionalities in the en-\nvironment, and they would follow the same transition rules. Given the Tp\nset, we can shift to the teammate's perspective to identify teammate-caused\ntransition within a transition. If their functionalities differ, we would need\nto separately identify player-caused and teammate-caused transitions."}, {"title": "3.2.3. Spontaneously Occurring Transitions", "content": "The transitions observed by the agent at any given moment include self-\ncaused transitions, teammate-caused transitions, and spontaneously occur-\nring transitions. The complete transition excluding the player-caused and\nteammate-caused transitions, leaves the environment's spontaneous transi-\ntions Ts. We use Ds to represent the set of Ts, Ds = {T}."}, {"title": "3.3. Reasoner", "content": "The environment's transition rules describe how information about the\nplayer and interaction points changes. Based on this information, the rea-\nsoner can construct a transition graph, where the nodes include element\ninformation and actions. By traversing this graph, the reasoner should infer"}, {"title": "", "content": "the preconditions for executing certain action primitives. Different from pre-\nvious works [62] that inform the LLM agent [63] of preconditions of action\nprimitives, our algorithm can automatically infer such preconditions."}, {"title": "", "content": "First, we introduce two concepts conjunctive conditions and result through\na simple example. Suppose j and k are two elements, they have a re-\nlated transition T and a corresponding action a, where T = {Cj, Ck} =\n{(Ij, I\u2032j), (Ik, I\u2032k)}. We can describe it with a logical expression: Ij \u2227 Ik \u21d2\nI\u2032j \u2227 I\u2032k. A corresponding transition graph can be drawn, as shown in Figure\n5. Ij and Ik both point to a, and a points to I\u2032j and I\u2032k. The meaning of this\ngraph is that when both Ij and Ik are satisfied, executing action a will result\nin I\u2032j and I\u2032k. In the graph, Ij and Ik are the prerequisites of the transition,\nboth of which must be met for the transition to occur, and I\u2032j and I\u2032k are the\nresults of this T.\nDefinition 1. Since there exists a T and the prerequisites of T are Ij and\nIk, Ij and Ik are each other's conjunctive conditions.\nDefinition 2. The subsequent nodes of the action node are I\u2032j and I\u2032k, so I\u2032j\nand I\u2032k are the results of Ij. Similarly, I\u2032j and I\u2032k are also the results of Ik.\nNext, we detail the reasoning algorithm. We first construct a transition\ngraph G using the extractor's outputs, Dp and Ds. The logical expression\nof the transition in Dp is similar to Ij \u2227 Ik \u21d2 I\u2032j \u2227 I\u2032k. For Ts in Ds, we get\nIj \u2192 I\u2032j; or Ij \u2227 Ik \u2192 I\u2032j \u2227 I\u2032k, which means that these transitions can occur\nwithout any action. Note that we do not limit the number of prerequisites\nand results in a transition. The nodes in the graph are of three types: player\nnodes, which record the player's information; interaction point nodes, which\nrecord the interaction point's information; and action nodes, which record\nthe environment's actions. In the transition graph, we do not distinguish"}, {"title": "", "content": "between actions taken by the player or the teammate, as they have identical\nfunctionalities in the environment.\nIn Figure 6, we show a transition graph constructed by reasoner on the\nAsymmetric Advantages layout for illustration. Assuming there exists\na mapping that can convert element information into condition primitives\nand action primitives. e.g., 'player.onion' can be mapped to HoldOnion.\n'serving@face' can be mapped to ExServing as a conditional primitive and\nto GoIntServing as an action primitive. We use M(\u00b7) to represent the\nmapping function from I to conditional primitives, and Ma(\u00b7) to represent\nthe mapping function from I to action primitives.\nAlgorithm 1 Reasoning Algorithm\n1: Given: Transition graph G\n2: Output: Preconditions for Ma(I) of interaction point nodes I in G\n3: for each interaction point node I in G do \u25b7 Single-step Reasoning\n4: for k in conjunctive conditions (CC) of I do\n5: Add Mc(k) to the preconditions of I\n6: end for\n7: end for\n8: for each interaction point node I in G do \u25b7 Multi-step Reasoning\n9: Successors = Results of I\n10: for each j in Successors do\n11: for k in conjunctive conditions (CC) of j do\n12: if k is not mutually exclusive with CC of I then\n13: Add Mc(k) to the preconditions of I\n14: end if\n15: end for\n16: end for\n17: end for\nDuring single-step reasoning, the reasoner focuses on interaction point\nnodes I in the transition graph and identifies their conjunctive conditions as\npreconditions for Ma(I). For example, for 'onionDisp@face', its conjunctive\ncondition is 'player.empty'. This means the precondition for the player to go\nto the onion dispenser is that the player's hands are empty; otherwise, the\ninteraction with the onion dispenser will not occur. Thus, the precondition\nfor GoIntOnionDisp is HoldEmpty.\nIn multi-step reasoning, the reasoner considers the conjunctive conditions"}, {"title": "", "content": "of the results of each interaction point node I. The preconditions for Ma(I)\ninclude some of the conjunctive conditions of its results. For instance, the\nresult of 'onionDisp@face' is 'player.onion', and the conjunctive condition for\n'player.onion' is ExIdlePot. The purpose of the player fetching an onion is\nto place it into an idle pot. If there is no idle pot available on the field,\nthe player will either wait for an idle pot to appear or place the onion on\nthe table. Therefore, the precondition for GoIntOnionDisp should include\nExIdlePot.\nThe complete algorithm is described in Algorithm 1. We detail both\nsingle-step reasoning and multi-step reasoning. Since the player's states are\nmutually exclusive, we exclude mutually exclusive conditions (line 10). Ad-\nditionally, there is an induction step where condition primitives or action\nprimitives with the same mapped name are aggregated together."}, {"title": "3.4. Program synthesizer", "content": "The program synthesizer will synthesize programs that conform to a given\nDSL. However, due to the vast program space, directly searching for high-\nperforming programs within the given space is highly inefficient. To overcome\nthis difficulty, our program synthesizer leverages the output of the reasoner,\nspecifically the preconditions for each action primitive. These preconditions\ncan be used to guide the synthesizer in generating reasonable programs, sig-\nnificantly reducing the search space.\nAs mentioned in the DSL subsection, our program structure adopts a list-\nlike program. Each IT module in the program has variable A and B, which\nneed to be determined by the synthesizer. We implement the synthesizer\nusing a genetic algorithm [42, 43], which includes selection and crossover\noperations. Initially, programs are randomly generated as the initial pop-\nulation. In each iteration, the crossover operation randomly selects parent\nprograms and exchanges their program fragments to synthesize offspring.\nDuring the selection operation, programs with higher cumulative rewards in\nself-play are retained. We exclude the mutation step because it might alter\nthe preconditions of action primitives, causing them to violate the require-\nments inferred by the reasoner. Additionally, given a state, it is possible that\nnone of the B conditions in a program's IT modules are satisfied, resulting\nin an empty output. To prevent it, we append a random action at the end\nof every program.\nAfter the genetic algorithm completes the search, we evaluate the discov-\nered programs. During evaluation, we no longer use e-greedy exploration."}, {"title": "", "content": "To obtain a policy capable of cooperating with a variety of policies, we are\ninspired by population-based methods [16, 11] to select a Pareto-optimal set\nfrom all the programs discovered. The Pareto set is determined based on the\ntraining reward and the complexity of the programs, with complexity defined\nas the number of conditions B in the program. Programs with higher cumu-\nlative rewards and lower complexity are considered better. This Pareto set\nincludes diverse policies, such as those with the highest cumulative rewards\nand the simplest programs. A program is considered capable of handling\ndiverse teammates if it cooperates well with each program in this set. Ul-\ntimately, the synthesizer outputs the program with the highest evaluation\nreward sum."}, {"title": "3.4.1. Program Search Space Analysis", "content": "The defined DSL contains 9 action primitives and 13 types of condition\nprimitives (excluding the negation of conditions). Suppose a program uses 8\nof these action primitives and 12 of these condition primitives, and the pro-\ngram has 8 IT modules, with each IT module containing up to 4 conditions.\nThis results in approximately 1.64 \u00d7 1039 different possible programs.\nThe calculation process is as follows. First, we calculate the number of\ncombinations of conditions in an IT module, selecting 1 to 4 conditions from\n12 different ones, with each condition primitive being able to be negated:\nC12\u00d721+C22\u00d722 + C32 \u00d7 23 + C12 \u00d7 24 = 9968. Since there are 8 IT modules,\neach action in an IT module has 8 possible choices: (8\u00d79968) \u2248 1.64 \u00d7 1039.\nThis calculation demonstrates that even for a moderately sized program, the\npotential combinatorial space is vast."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nIn this section, we evaluate KnowPC's ZSC and ZSC+ capabilities across\nmultiple layouts in the Overcooked environment [16]. We compare the per-\nformance of KnowPC with six baselines: Self-Play (SP) [13, 16], Population-\nBased Training (PBT) [14, 16], Fictitious Co-Play (FCP) [18], Maximum-\nEntropy Population-Based Training (MEP) [19], Cooperative Open-ended\nLearning (COLE) [11], and Efficient End-to-End Training (E3T) [21]. All\nof them use PPO [64] as the RL algorithm and belong to DRL methods.\nAmong them, SP and E3T are based on self-play, while the other algorithms\nare population-based, requiring the maintenance of a diverse population."}, {"title": "4.2. ZSC Experiment Results", "content": "The parameter settings for KnowPC are consistent across different lay-\nouts. During training, the exploration probability e is set to 0.3, and the\nthreshold d is set to 0.1. The genetic algorithm undergoes 50 iterations, with\nan initial population size of 200 and a subsequent population size maintained\nat 10.\nIt is worth noting that previous works have utilized shaped reward func-\ntions [16] to train agents, such as giving extra rewards for events like plac-\ning an onion into the pot, picking up a dish, or making a soup. This ap-\nproach helps to accelerate convergence and improve performance. In contrast,\nKnowPC is not sensitive to the reward function. We directly use a sparse\nreward function (i.e., only get a reward when delivering soup). Addition-\nally, the input encoding for DRL methods uses a lossless state encoding [16],\nwhich includes multiple matrices with sizes corresponding to the environment\ngrid size. In terms of state encoding, DRL has more complete information\ncompared to KnowPC."}, {"title": "4.2.1. Collaboration between RL Agents.", "content": "We evaluate the ZSC capabilities of each method by letting each method's\npolicies cooperate with each other. During training, none of them can access"}, {"title": "4.2.2. Collaboration with Human Proxies", "content": "Apart from collaborating with AI partners, an RL agent also needs to\nwork with human partners. Following previous work [11, 62], we use behavior-\ncloned models trained on human data as human proxies. Figure 8 presents\nthe results on 5 layouts. KnowPC generally performs well overall. As noted\nin previous work, no method consistently outperforms the others. KnowPC\nachieves significantly higher results than the baselines in two layouts (Asym-\nmetric Advantages and Forced Coordination). In two other layouts (Cramped\nRoom and Coordination Ring), our results are slightly lower than the best\nbaseline. Possibly because Cramped Room and Coordination Ring require\nmore consideration and modeling of the teammates. Integrating agent mod-\neling techniques [21] with KnowPC is a potential direction for future research."}, {"title": "4.2.3. Training Efficiency Analysis", "content": "Table 1 shows the training times for all algorithms on a single layout.\nIt can be observed that population-based methods (e.g., \u041c\u0415\u0420, COLE) gen-\nerally require more training time than self-play methods (e.g., SP, E3T).\nOur method is efficient. Benefiting from reasoning in the abstract space and\nnot requiring extensive parameter optimization like DRL, it has the shortest\ntraining time among all methods. For instance, its training time is one 360th\nof previously advanced COLE and one 19th of the state-of-the-art E3T."}, {"title": "4.3. Policy Interpretability", "content": "Listing 1 illustrates a program found by KnowPC on the Counter Circuit\nenvironment. Unlike the DRL policy, the program is fully interpretable, with\ntransparent decision logic. For example, the logic expressed by the fourth IT\nmodule is that if there is an onion dispenser and an idle pot, and no ready"}, {"title": "", "content": "pot, the player will go to the onion dispenser. This is reasonable because if\nthere is no idle pot in the scene, it is futile for the player to go to the onion\ndispenser to get an onion (since only idle pots need onions). Handling the\nready pot is prioritized over the idle pot, because handling the ready pot\ncan get rewards more quickly, hence the B includes not ExReadyPot. In the\nsixth IT module, if there is a ready pot and a dish counter in the scene and\nthe player's hands are empty, the player will go to the dish counter. This\nis also a reasonable decision because dishes can only be used to serve soup\nfrom a ready pot, and obtaining a dish requires empty hands."}, {"title": "4.4. ZSC+ Experiment Results", "content": "Layout variations are common, as rooms in different buildings often have\ndifferent layouts. A good RL policy should be robust to these layout changes."}, {"title": "", "content": "We are particularly interested in the ZSC+ capability of various methods,\nso we made some adjustments to the original Overcooked layout. As shown\nin Figures 12 and 13, we created two new layout groups. Compared to\nthe original layout (Figure 9), the positions of several interaction points are\nslightly different. These Layout changes do not alter the dynamics of the\nenvironment but adjust the positions of some interaction points. This can\ntest whether the agent has learned the underlying logic of decision-making\nrather than memorizing a fixed position.\nTo verify the generalization ability of each method, we directly apply the\npolicies trained on the original layout to the new layout without additional\ntraining. Figures 12 and 13 show the cooperation reward scores of each\nmethod in the two new layouts. The results show that the performance of\nall baselines declined to some extent. However, the performance of KnowPC\nwas significantly better than the other methods, with its corresponding re-\nward column exceeding those of the other baselines. This demonstrates that\nKnowPC's programmatic policies are robust.\nWe hypothesize that the failure of DRL is due to its policies being repre-\nsented by an over-parameterized neural network. Neural networks struggle\nto deduce accurate decision logic purely from rewards, making them unable\nto handle unseen situations. In contrast, concise programs act as a form of\npolicy regularization, making them more robust to unseen agents and layouts."}, {"title": "4.5. Ablation Study", "content": "To validate the effectiveness of the knowledge-driven extractor and rea-\nsoner, we first visualized a transition graph."}]}