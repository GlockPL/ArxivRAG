{"title": "Can Stories Help LLMs Reason?\nCurating Information Space Through Narrative", "authors": ["Vahid Sadiri Javadi", "Johanne R. Trippas", "Yash Kumar Lal", "Lucie Flek"], "abstract": "Narratives are widely recognized as a powerful\ntool for structuring information and facilitat-\ning comprehension of complex ideas in vari-\nous domains such as science communication.\nThis paper investigates whether incorporating\nnarrative elements can assist Large Language\nModels (LLMs) in solving complex problems\nmore effectively. We propose a novel approach,\nStory of Thought (SoT), integrating narra-\ntive structures into prompting techniques for\nproblem solving. This approach involves con-\nstructing narratives around problem statements\nand creating a framework to identify and or-\nganize relevant information. Our experiments\nshow that using various LLMS with SoT con-\nsistently surpasses using them with other tech-\nniques on physics, chemistry, math, and biol-\nogy questions in both the GPQA and JEEBench\ndatasets. The narrative-based information cura-\ntion process in SoT enhances problem compre-\nhension by contextualizing critical in-domain\ninformation and highlighting causal relation-\nships within the problem space.", "sections": [{"title": "1 Introduction", "content": "Humans have an exceptional ability to under-\nstand and reason through narratives. A narrative-\ndriven approach can enhance the comprehen-\nsion and retention of complex subjects com-\npared to simple fact listing (Fisher, 2021; Ab-\nbott, 2020; Gottschall, 2012). For example, story-\ntelling effectively structures information in science\ncommunication (Dahlstrom, 2014; Norris et al.,\n2005; Martinez-Conde and Macknik, 2017), edu-\ncation (Engel et al., 2018; Negrete and Lartigue,\n2004), and health communication (Dudley et al.,\n2023), revealing relationships and contextual nu-\nances (Zak, 2015). While narrative approach con-\ntextualizes facts within a daily life scenario (story)\nwith a planned structure, a factual approach con-\nveys information in a concise in-domain manner.\nTo date, large language models (LLMs) struggle\nwith complex problem-solving tasks that require\nthe ability to integrate, structure, and apply rel-\nevant information effectively (Qiao et al., 2023;\nWang et al., 2023). Prompting techniques based\non breaking tasks into smaller subtasks, such as\nChain-of-Thought (CoT) (Wei et al., 2022) and its\nmore recent adaptations (Xia et al., 2024), have led\nto considerable improvements in problem-solving\nbenchmarks. The strategies of constructing natural\nlanguage rationales (Ling et al., 2017), in the CoT\ncontext also called reasoning processes, play a vital\nrole in LLM prompting (Ye and Durrett, 2024; Min\net al., 2022; Wang et al., 2022; Li et al., 2023).\nInspired by the effectiveness of narrative in (i)\nidentifying and explaining abstract concepts and (ii)\norganizing the information flow coherently, we ex-\nplore integrating narrative elements into prompt-\ndriven reasoning. The main research questions\naddressed in this work are:\nRQ1: Can LLMs generate coherent and relevant\nnarratives around problem statements to facilitate\ncomprehension and reasoning?\nRQ2: Can incorporating narrative elements into\nprompting techniques improve model perfor-\nmance on complex problem-solving tasks?\nWe make the following contributions: (i) We in-\ntroduce a novel method, Story of Thought (SoT),\nthat aids LLMs to identify and arrange relevant\ninformation for solving complex tasks by incor-\nporating narrative structures into the prompting\nprocess, (ii) We evaluate the effectiveness of SoT\non GPQA and JEEBench datasets of complex prob-\nlems, showing superior performance to existing\nprompting techniques with SotA models, and (iii)\nWe analyze the impact of narrative techniques to\ngenerate narrative-based explanations and investi-\ngate why they improve LLMs' reasoning abilities."}, {"title": "2 Related Work", "content": "Bruner (1991) posit that narratives are a fundamen-\ntal mode of human thought, allowing individuals to\nconvey complex concepts in a more understandable\nmanner. Presenting information through narratives\ncan enhance learning and memory, promote en-\ngagement and motivation (Willingham, 2004; Chen\net al., 2023). The development of narrative-based\neducational strategies (Bower and Clark, 1969;\nMawasi et al., 2020; Norris et al., 2005) paved\nthe way for using them as a framework for organiz-\ning information for problem solving. The use of\nnarratives can break down complex problems into\nsub-problems, providing a step-by-step approach to\nanswering a question (Szurmak and Thuna, 2013).\nSadiri Javadi et al. (2024) use different narratives\ntechniques to satisfy diverse requirements for con-\nversational information-seeking systems.\nThere are a plethora of datasets focusing on\nanswering questions about given contexts. Read-\ning comprehension datasets (Khashabi et al., 2018;\nWelbl et al., 2018; Williams et al., 2018; Mihaylov\net al., 2018) explicitly evaluate a system's ability to\nanswer questions that need information from multi-\nple sentences in a passage. NarrativeQA (Ko\u010disk\u00fd\net al., 2018) provides a dataset of 1,567 narratives\nand associated QA pairs as written by human anno-\ntators. ROCStories (Mostafazadeh et al., 2016) is\na collection of 5 sentence short stories over which\nnumerous datasets such as TellMeWhy (Lal et al.,\n2021) have been built to facilitate answering ques-\ntions about narratives. However, none of these\ndatasets use narratives as a tool of understanding,\nor relate to problem solving.\nProblem solving datasets focus on mathemat-\nics, physics or other scientific domains. GSM8K\n(Cobbe et al., 2021) is a dataset of 8.5K high qual-\nity linguistically diverse grade school math word\nproblems created by human problem writers. SciQ\n(Welbl et al., 2017) is built using a novel method for\nobtaining high-quality, domain-targeted multiple\nchoice questions from crowd workers, and contains\n13.7K multiple choice science exam questions. Sci-\nenceQA (Lu et al., 2022) adds multimodal context\nto collected elementary and high school science\nquestions. While there has been rapid progress\non these tasks, prior work has not integrated ed-\nucational strategies such as narratives to tackle\nthem, a setting which is likely to be used in the\nreal world. MedMCQA (Pal et al., 2022) contains\nMCQ questions designed to address real-world\nmedical entrance exam questions. Such datasets\nhave been used extensively as yardsticks to mea-\nsure the progress of NLP techniques.\nThe strength of modern LLMs, coupled with\nthe paradigm of prompting, has driven up perfor-\nmance on problem solving tasks. In-context learn-\ning through few-shot examples has been used to\nteach LLMs about new tasks using a small number\nof examples. Chain of thought prompting (Wei\net al., 2022) nudges LLMs to generate intermediate\nsteps to mimic an explicit reasoning process before\nanswering a question. Similarly, Tree of Thoughts\n(ToT) (Yao et al., 2023) and Graph of Thoughts\n(GoT) (Besta et al., 2024) induce intermediate rea-\nsoning structures, trees and graphs respectively, to\ndecide on an answer. However, despite the fact that\nnarratives have been used as a way to simplify prob-\nlems, they have never been explored to improve the\nproblem solving abilities of LLMs."}, {"title": "3 Methodology: Story of Thought", "content": "We introduce Story of Thought (SoT), a novel\nprompt-driven reasoning approach that generates\nnarrative-based clarification to guide LLMs' rea-\nsoning process. Inspired by the narrative format,\nthe SoT approach leverages the cognitive benefits\nof storytelling, such as contextual understanding\nand relational reasoning, that can help LLMs iden-\ntify and maintain the information structure.\nFigure 1 gives an overview of SoT. It involves\nthree steps using narrative techniques: (i) Ques-\ntion clarification (i.e., acting as an explorer to dis-\nsect and clarify complex questions (Section 3.1));\n(ii) Narrative Generation (i.e., generating detailed\nnarratives from the clarified question components\nusing different narrative techniques (Section 3.2));\nand (iii) Problem Solving (i.e., leveraging narra-\ntives to prompt the LLMs to solve the tasks (Sec-\ntion 3.3)). We describe the exact prompts used in\neach step in Appendix C."}, {"title": "3.1 Step 1: Question Clarification", "content": "In the first step, we use the LLM's ability to explore\nand clarify the problem. Starting with a specialized\nprompt, the LLM breaks down the question into its\ncore components, identifying relevant subtopics\nand areas. This detailed analysis is crucial for\ngenerating a coherent narrative that thoroughly ad-\ndresses the question."}, {"title": "3.2 Step 2: Narrative Generation", "content": "The second step involves generating detailed nar-\nratives based on the breakdown and clarification\nperformed in Step 1 (question clarification). These\nnarratives provide a structured context for the ques-\ntions to enhance the LLM's understanding, reason-\ning, and problem-solving abilities. Sadiri Javadi\net al. (2024) discuss different narrative techniques\nrequired in conversational information seeking sys-\ntems. We integrate the below subset of these tech-\nniques into our prompt and task LLMs to generate\na narrative, based on the information from Step 1:\n1. Progressive Disclosure: Reveals informa-\ntion gradually, guiding the LLM step-by-step\nthrough the problem-solving process.\n2. Branching: Explores different paths or ap-\nproaches to understanding the problem by pro-\nviding multiple perspectives.\n3. Analogy: Uses comparisons to familiar con-\ncepts or situations to make abstract components\nmore understandable.\n4. Analogical Reasoning: Facilitates understand-\ning by reasoning through similarities between\nthe problem and known situations.\n5. Metaphor: Simplifies complex ideas through\nmetaphorical representation."}, {"title": "3.3 Step 3: Problem Solving", "content": "In the final step, the LLM uses the narrative gener-\nated in Step 2 to solve the original QA task. The\nstructured and contextual understanding provided\nby the narrative supports LLM in accessing relevant\naspects of the task."}, {"title": "4 Experimental Setup", "content": "To comprehensively evaluate the effectiveness of\nour proposed approach, we conduct experiments\nacross a diverse set of tasks and models, employing\nvarious prompting techniques for comparison."}, {"title": "4.1 Evaluation Tasks", "content": "We focus our evaluation on reasoning-intensive\ntasks spanning multiple domains, including\nphysics, biology, and chemistry problem-solving.\nIn particular, we utilize the GPQA (Diamond\nset) (Rein et al., 2024) and JEEBench (Arora\net al., 2023). GPQA is a Graduate-level Problem-\nsolving QA dataset which comprises expert-crafted"}, {"title": "5 Results", "content": "Our proposed SoT approach that incorporates nar-\nrative structures improves over almost all previ-\nous prompting approaches across two different\nproblem-solving datasets. This highlights the po-\ntential of using narratives to improve the ability of\nLLMs to understand and reason about the given\ninformation in various intensive reasoning tasks."}, {"title": "5.1 Performance on GPQA", "content": "We present the results of our experiments on GPQA\n(Diamond) are presented in Table 1. For this task,\nSoT is the best method to use with six of eight\nmodels. The open-source Llama 3 70B model\nrecords the highest accuracy using the SoT method,\nachieving a score of 51.01%. This is the highest\naccuracy observed among all models and methods\ntested in the study. Furthermore, the GPT-4 model\nshows the most notable improvement in accuracy\nwhen the SoT method is employed, compared to\nits zero-shot baseline. Specifically, the accuracy\nfor GPT-4 increased from 34.7% under zero-shot\nconditions to 48.98% with SoT (i.e., an absolute\nincrease of 14.28%, or a relative increase of 41%\nrespectively).\nInterestingly, all reasoning strategies lead to an\naccuracy drop for the comparably smaller Phi-3\nMini model, and all CoT strategies except Analog-\nical Reasoning also lead to the accuracy drop of\nthe Phi-3 Medium model compared to its zero-shot\nbaseline. We hypothesize that this is due to the low\nquality of the generated explanations (whether CoT\nsteps or SoT narrative), as further studied in \u00a76.1.\nWe also find that Llama 3 70B with SoT outper-\nforms zero-shot ol-preview which uses CoT style\nreasoning internally."}, {"title": "5.2 Performance on JEEBench", "content": "Table 2 presents detailed experimental results on\nJEEBench. Our proposed Story of Thought (SoT)\nmethod consistently improves the performance of\nseven out of the eight LLMs. Using SoT, Llama\n3 70B performance surpasses even the GPT mod-\nels. It obtains the highest scores in all subjects and\nquestion types (Except Single-Correct), with an\noverall aggregate score of 0.453. This is a signif-\nicant improvement on the previous SOTA, which\nwas a strong GPT4 model used with both CoT and\nSelf-Consistency.\nAcross models, the results highlight the effec-\ntiveness of Story of Thought (SoT) in enhancing\nmodel performance on complex, multi-disciplinary\nbenchmarks like JEEBench, setting new SOTA re-\nsults in several categories. The improvements are\nparticularly notable in the subject categories and\nquestion types where the other methods struggle.\nIn Figure 2, we present subject-wise perfor-\nmance of different models on JEEBench. On aver-\nage, model performance is highest on Chemistry\nproblems when using SoT. This is in contrast to\nfindings on GPQA and could occur due to the dif-\nference in degree of difficulty of problems in the\ntwo datasets (graduate level vs high school level).\nRegardless, improvements on Biology problems\nare not far behind those for Chemistry."}, {"title": "6 Analysis of SoT Aspects", "content": ""}, {"title": "6.1 Role of the Narrative Quality/Choice", "content": "The choice of narrator model (i.e., the model\nthat generates narratives) can impact the problem-\nsolving resuls. In the following experiments, we\napply the narratives generated by other large and\nsmall open-source LLMs to the Phi-3 Mini and\nPhi-3 Medium models. The results of these experi-\nments are presented in Table 3.\nWe observe that the narratives generated by\nmost models consistently improve the accuracy\nof both Microsoft models compared to the baseline\n(i.e., when both models use their own generated"}, {"title": "6.2 Impact of Narrative Elements", "content": "To measure the impact of each of the narrative tech-\nniques we jointly prompted on the performance of\nopen-source Meta models, we ablate the designed\nprompt in Step 2 (of Section 3.2) to apply each of\nthe techniques separately. The results in Table 5\nindicate that employing any single narrative tech-\nnique at a time is notably less effective at boost-\ning QA accuracy than utilizing a combination\nof these simultaneously.\nFor both models (Llama 3 8B and 70B), the de-\ncrease in accuracy is comparably smaller (-3.0%\nto -5.6%) when using only the analogical com-\nponents of the narrative (Analogy and Analogical\nReasoning) than when using only the structural in-\nstructions (Progressive Disclosure or Branching)\nwhich leads to larger (-6.0% to -9.1%) accuracy\nloss. However, reasoning alone does not perform\non par with the full narrative generation listing all\nthe techniques. Prompting for Metaphor usage only\nleads to a larger accuracy loss in the 70B model\n(-6.6%) compared to the smaller one (-2.0%). This"}, {"title": "6.3 Analyzing Generated Narratives", "content": "To gain deeper insights into the generated narra-\ntives, we designed a prompt (shown below) that\nutilizes our best-performing model (Llama 3 70B)\nto annotate the number of occurrences of each nar-\nrative technique for each generated narrative by\nall models used in our experiments. We can bet-\nter interpret how the model executed the narrative\ntechnique prompt, by asking it to label if and where\nthe mentioned techniques are used in the text gen-\nerated. Less frequently labeled techniques might\nbe the ones where LLM doesn't have a clear under-\nstanding of what it is asked to do. A proportion of\nthe techniques and their correlation can provide us\nwith a better picture of LLM's interpretation of the\ninstruction as well. We detail the instruction given\nto the LLM in Appendix C.\nWe aim to uncover patterns and variations in the\nuse of narrative techniques across different LLMs.\nTable 4 compares the total number of occurrences\nfor each narrative technique across various LLMs."}, {"title": "Variability in Utilization of Narrative Tech-niques Across Models:", "content": "In our designed prompt\nin Step 2 (i.e., Narrative Generation), LLMs gen-\nerate narrative using all 5 narrative techniques.\nHowever, as Table 4 indicates, not all techniques\nwere employed equally. This reveals that while\nsome techniques like Analogy and Progressive Dis-\nclosure were consistently utilized, others such as\nBranching were applied less frequently.\nWe observe a trend across all LLM families\nwhere models with larger capacities, such as Llama\n3 70B and GPT-4, consistently show higher occur-\nrences of narrative techniques compared to their\nsmaller counterparts. Furthermore, OpenAI's mod-"}, {"title": "6.4 Analyzing SoT Reasoning", "content": "Table 6 compares the similarity of Story of Thought\n(SoT) and Chain of Thought (CoT) reasoning out-\nputs to human explanations for different language\nmodels on the GPQA (Diamond set) dataset, using\nBertScore, ROUGE-L, and BLEU.\nThe differences between ROUGE-L values are\ninsignificant and do not display any clear trends.\nHowever, according to BLEU scores, using SoT\nresults in explanations closer to humans and the\ndifferences are more pronounced.\nAs per BertScore (an embedding-based similar-\nity metric), Llama 3 models (8B and 70B) explana-\ntions are more similar to human ones when using\nSoT reasoning across all three metrics. However,\nMistral models (7B and 8x7B), GPT-4, and Phi-3\nMini generate explanations more similar to human\nexplanations when using CoT reasoning across all\nmetrics. The semantic similarity of narratives gen-\nerated by Llama 3 70B to human explanations com-\nbined with their effect of improving smaller models\nindicates that these narratives present information\nabout the problems in a simplified manner."}, {"title": "7 Conclusion", "content": "Inspired by findings from human cognitive pro-\ncesses explored in didactics research, in this work,\nwe propose to use narrative techniques in LLM\nprompting.We present strong evidence on public\nbenchmark datasets that narrative techniques have\nthe potential to notably enhance the reasoning abil-\nities of LLMs in complex problem-solving tasks.\nBy incorporating narrative structures, which mimic\nhuman cognitive processes of organizing and in-\nterpreting information, LLMs can achieve higher\nlevels of performance and provide more contextu-\nally enriched responses."}, {"title": "Limitations", "content": "Contribution limitations. The occurrences of\nnarrative techniques do not necessarily imply the\nquality or effectiveness of the generated narratives;\nrather, they provide insights into the models' ten-\ndencies and preferences in employing these tech-\nniques. Therefore, answering the question of why\nnarrative is helping LLMs is more complex and\nneeds to be further investigated by looking into\ndifferent research areas such as cognitive and com-\nmunication theories.\nDataset limitations. So far, we used only GPQA\nand JEEBench problems as the most challenging\nset of problem-solving benchmarks we were aware\nof. Other comparable benchmarks, such as MGSM,\nare much closer to human or superhuman accu-\nracy already without reasoning prompts and will\nbe explored in future work.\nAnalysis limitations. We used Llama 70 B to\nrespectively analyze the narratives. The intuition\nbehind this experiment is that we can better inter-\npret how the model executed the narrative tech-\nnique prompt, by asking it to label if and where\nthe mentioned techniques are used in the text gen-\nerated. An alternative would be a thorough human\nassessment and further analysis of the impact on\ndownstream performance, both of which we pur-\nsue in ongoing follow-up experiments. (We also\npreviously prompted the LLMs in Step 2 to explain\neach of these five narrative techniques to make sure\nthe concepts are understood before generating the\nnarrative.)"}, {"title": "C.1 Question Clarification", "content": "You are an explorer who wants to identify and\ncollect different related and specialized\nsubject areas to clarify the question. Your\ngoal is to narrow down the question and\nprovide relevant areas of knowledge and\nexperience you have that help clarify the\nquestion mentioned below. You should not\nanswer the question.\n<question>"}, {"title": "C.2 Narrative Generation", "content": "You are an expert in narrative-based\nexplanations for science communication.\nYour goal is to clarify the following\nquestion in a narrative way through the\ninterconnected information provided below\nto enable a non-expert to comprehend the\nquestion in a more coherent and\ncontextually rich manner. You should not\nanswer the question.\nMake sure to use all of these narrative\ntechniques when clarifying the question\nthrough the interconnected information:\nProgressive Disclosure, Branching, Analogy,\nAnalogical Reasoning, and Metaphor.\n<question>\n<generated information in the previous step>"}, {"title": "C.3 Problem Solving", "content": "You are an expert in analyzing narrative-based\nexplanations for solving tasks. Please\nanswer the following question based on the\nfollowing narrative-based clarification:\n<question>\nOptions:\n<options>\n<generated narrative in the previous step>"}, {"title": "C.4 Analyzing Generated Narratives", "content": "You are an expert in analyzing narrative-based\nexplanations for science communication.\nYour goal is to find out which narrative\ntechniques have been used in the following\nnarrative-based explanation.\nLabel the narrative-based explanation using the\nfollowing narrative-based techniques:\n1. Progressive Disclosure\n2. Branching\n3. Analogy\n4. Analogical Reasoning\n5. Metaphor\n<generated narrative>"}, {"title": "D Performance on JEEBench", "content": ""}, {"title": "E Story of Thought (SoT) vs. Chain of\nThought (CoT)", "content": ""}, {"title": "Figure 4: An actual example of SoT.", "content": ""}, {"title": "Step 2: Structure of Racemic 3-Methylpent-1-ene", "content": "The structure of 3-methylpent-1-ene is:\n\\[\\text{CH}_2=CH-\\text{CH}_2-\\text{CH}(\\text{CH}_3)-\\text{CH}_3 \\]\nRacemic indicates that we have both enantiomers in equal amounts."}, {"title": "The structure of 3-methylpent-1-ene is", "content": ""}, {"title": "When two molecules of 3-methylpent-1-ene undergo metathesis, the reaction can yield several different products depending on how the alkylidene fragments rearrange", "content": ""}]}