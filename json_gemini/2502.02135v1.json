{"title": "Standard Neural Computation Alone Is Insufficient for Logical Intelligence", "authors": ["Youngsung Kim"], "abstract": "Neural networks, as currently designed, fall short of achieving true logical intelligence. Modern AI models rely on standard neural computation-inner-product-based transformations and nonlinear activations to approximate patterns from data. While effective for inductive learning, this architecture lacks the structural guarantees necessary for deductive inference and logical consistency. As a result, deep networks struggle with rule-based reasoning, structured generalization, and interpretability without extensive post-hoc modifications. This position paper argues that standard neural layers must be fundamentally rethought to integrate logical reasoning. We advocate for Logical Neural Units (LNUs)-modular components that embed differentiable approximations of logical operations (e.g., AND, OR, NOT) directly within neural architectures. We critique existing neurosymbolic approaches, highlight the limitations of standard neural computation for logical inference, and present LNUs as a necessary paradigm shift in AI. Finally, we outline a roadmap for implementation, discussing theoretical foundations, architectural integration, and key challenges for future research.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) aims to emulate human intelligence, yet its development has oscillated between two dominant paradigms: symbolic AI and connectionism. Since the 1956 Dartmouth workshop, symbolic AI has emphasized logical rules and structured inference, whereas connectionist models, such as neural networks, have focused on learning from data. Modern large-scale neural models trained on symbolic data, such as natural language, can appear to perform deductive reasoning, but they do not robustly replicate human-like logical inference (Tian et al., 2021). The challenge lies in their reliance on statistical approximations rather than explicit rule-based deduction (H\u00f6lldobler & Kalinke, 1994; Bader & Hitzler, 2005; Seda, 2006; Serafini & Garcez, 2016).\nSymbolic AI, rooted in formal logic and computational rules, enables structured reasoning and systematic problem-solving. However, it faces the symbol grounding problem-symbols remain abstract and lack intrinsic meaning unless anchored to real-world sensory experiences (Harnad, 1990; Barsalou, 1999). In contrast, connectionist models excel at pattern recognition from raw data but struggle with systematic reasoning, logical rule generalization, and interpretability (McClelland & Rumelhart, 1986; Rumelhart & McClelland, 1986). These complementary strengths and weaknesses suggest that neither approach alone is sufficient for achieving robust, human-like reasoning.\nNeuro-symbolic AI seeks to bridge the gap between connectionist learning and symbolic computation by embedding structured reasoning within neural architectures (Hilario,\n1995; Hatzilygeroudis & Prentzas, 2000; d'Avila Garcez\n& Lamb, 2023). While symbolic theories suggest that\nstructured formal systems provide meaning (Dennett, 1969;\nNewell & Simon, 1972; Fodor, 1975; Haugeland, 1985),\nperceptual symbol theories argue for cognition grounded in\nsensory-motor experience (Barsalou, 1999). This distinction extends to AI methodologies, where symbolic systems provide formal inference but lack the adaptability of neural models, while neural networks learn representations flexibly but lack explicit symbolic reasoning (d'Avila Garcez et al.,\n2019).\nRecent neuro-symbolic approaches embed symbolic inference within deep learning architectures or integrate learned representations into reasoning engines (Wang et al., 2019;\nYang et al., 2020; Manhaeve et al., 2018; d'Avila Garcez\net al., 2019; Besold et al., 2017; Yang et al., 2019; Bader\n& Hitzler, 2005), yet aligning continuous neural representa-tions with discrete logic remains a challenge. Large lan-guage models (LLMs) employ structured reasoning tech-"}, {"title": "2. Foundations of Logical Reasoning in Neural\nSystems", "content": "Human Intelligence has been studied throughout history,\nyet no single, universally accepted definition exists, despite\nextensive efforts to characterize it through various intel-lectual frameworks (Minsky, 1961; Newell, 1965). Since\nAristotle, logic has been regarded as a fundamental tool for\nscientific inquiry, often referred to as the Organon (Aristotle,\n4th century BCE). Rooted in symbolic reasoning (McCarthy\n& Hayes, 1969), logic contrasts with perceptual learning,\nwhich does not require explicit symbolic knowledge and\nis acquired through experience (Newell & Simon, 1956;\nWhitehead & Russell, 1910; Tarski, 1944). Humans develop\nlogical reasoning abilities even without formal training, of-ten through structured learning and knowledge transfer.\nCognitive science distinguishes two complementary do-mains of intelligence (Cattell, 1943; 1963): fluid intelli-gence, which enables rapid pattern recognition and adapt-ability, and crystallized intelligence, which relies on accu-mulated knowledge and rule-based reasoning. Together,\nthese support both flexible learning and structured problem-solving.\nTo achieve intelligence, modern AI systems primarily rely\non inductive learning, which generalizes patterns from\ndata, enabling adaptation to new scenarios. Neural networks\nexemplify this approach by identifying statistical associa-tions without predefined rules. Techniques such as statistical\nmachine learning and inductive logic programming (ILP)\nderive general rules from examples. In contrast, deductive\nreasoning applies established axioms to derive conclusions\nwith certainty. Systems such as theorem provers ensure log-ical soundness but lack adaptability. While deduction does\nnot inherently involve learning, it can simulate knowledge\naccumulation by deriving and storing new facts (McCarthy\n& Hayes, 1969; Newell & Simon, 1956).\nHybrid approaches in neurosymbolic AI (Bader & Hitzler,\n2005; Besold et al., 2017; d'Avila Garcez & Lamb, 2023;\nMarra et al., 2024; Wang et al., 2025) integrate inductive\nlearning's adaptability with deductive reasoning's precision.\nInduction discovers patterns, while deduction ensures their\nlogical validity (Nilsson, 1980; Russell & Norvig, 2020).\nThis fusion enhances flexibility through data-driven learning,\nsoundness via rule-based inference, and interpretability by\nleveraging explicit logical rules.\nLogical reasoning unites deduction and induction, enabling\nstructured thought akin to human System 2 reasoning (Kah-\nneman, 2011). Logical operations such as AND, OR, and\nNOT are fundamental to compositional and hierarchical\ninference. While neural networks can approximate these op-erations (McCulloch & Pitts, 1943; Newell & Simon, 1956;\nBesold et al., 2017), they lack explicit logical representa-tions unless specifically designed to integrate them."}, {"title": "3. Extending Logical Operators Beyond\nClassical Rules", "content": "Classical symbolic AI relies on propositional and predicate\nlogic for knowledge representation (Newell & Simon, 1956;\nMcCarthy & Hayes, 1969; Bader & Hitzler, 2005). Propo-sitional logic operates on Boolean variables (0 or 1) and\napplies logical operators such as AND, OR, NOT, and impli-cation to derive conclusions. However, strict Boolean opera-tions often fall short when dealing with real-world scenarios\nthat involve uncertainty, continuous-valued information, or"}, {"title": "4. Theoretical Limits of Standard Neural\nComputation for Logic", "content": "The previous section examined real-valued, fuzzy, and\nmany-valued logic as extensions that generalize classical\nBoolean operators for handling uncertainty and graded rea-soning. While these approaches introduce flexibility, their\nintegration into neural networks remains an approximation\nrather than an exact logical representation. Neural networks,\nby design, operate over continuous spaces and optimize\nfor statistical associations rather than strict logical infer-ence. This fundamental difference raises the question: Can\nstandard neural architectures inherently support logical\nreasoning, or are they fundamentally constrained in their\nability to model symbolic computation?"}, {"title": "4.1. Universal Approximation Theorem (UAT)", "content": "The Universal Approximation Theorem (UAT) states that\na feedforward neural network with a single hidden layer\nand a sufficient number of neurons can approximate any\ncontinuous function on a compact subset of Rn to arbitrary\nprecision, given a suitable activation function (Hornik et al.,\n1989). Formally, for a compact set KC Rn and a contin-\nuous function f: KR, for any \u20ac > 0, there exists a\nneural network (x) such that:\nsupx\u2208K|f(x) \u2013 \u0444(x)| < \u0454.\n(1)\nWhile UAT ensures that neural networks can approximate\nany continuous function, it does not guarantee efficient learn-ing, interpretability, or logical consistency (Cybenko, 1989;\nHornik et al., 1989). Moreover, it assumes that f is continu-ous, which is a critical limitation when applied to discrete\nlogical operations."}, {"title": "4.2. Limitations of UAT for Logical Reasoning", "content": "Logical reasoning tasks, such as theorem proving and sym-bolic inference, reveal fundamental shortcomings in apply-ing UAT to logic-based computations. The key issue lies\nin the nature of logical rules, which require exact, discrete\noutputs. While neural networks can approximate logical\noperators such as AND, OR, and XOR using continuous\nactivations, these approximations are computationally ineffi-cient and lack the precision necessary for formal inference.\nMoreover, logical reasoning demands deterministic conclu-sions. Standard neural networks, optimized via gradient-based learning, introduce approximations that may cause"}, {"title": "4.3. How UAT Constraints Affect Logical Computation", "content": "Although UAT highlights the expressive power of neural\nnetworks, it also exposes fundamental challenges in logical\ncomputation. Logical reasoning requires exact, discrete, and\nsymbolic representations, which are not naturally supported\nby continuous approximations. While inductive reasoning in\nneural networks can approximate logical functions, it does\nnot guarantee soundness, completeness, or consistency in a\nformal sense. Consequently, additional mechanisms-such\nas neurosymbolic models incorporating explicit logical rules\n(^, V, \u00ac) or fuzzy t-norms are needed to bridge the gap\nbetween statistical learning and symbolic reasoning."}, {"title": "4.4. Motivation for Logic-Based Architectures", "content": "The limitations of UAT for logical reasoning highlight the\nneed for hybrid architectures that integrate symbolic con-straints within neural networks. Standard neural models\nstruggle with discrete logical computation, global consis-tency, and interpretability-key elements of systematic rea-soning. By embedding logical structures into neural compu-tations, hybrid models improve consistency and explainabil-ity.\nExisting neurosymbolic approaches attempt to bridge neural\nlearning and symbolic inference using hard-coded rules or\ndifferentiable approximations of logical operators (Bader\n& Hitzler, 2005; Wang et al., 2025). However, many de-pend on external reasoning modules, reducing scalability\nand architectural cohesion. A more integrated solution re-quires embedding trainable logical operators (e.g., differen-tiable AND, OR, and NOT) directly within neural layers,\nallowing for seamless interpolation between symbolic and\nsub-symbolic reasoning."}, {"title": "5. Shortcomings in Existing Neural Logic\nApproaches", "content": "Neurosymbolic AI combines sub-symbolic learning (e.g.,\nneural networks) with symbolic reasoning (e.g., logic-based\ninference), aiming to unify the efficiency of neural meth-ods and the interpretability of symbolic systems (Kautz,\n2022; Bader & Hitzler, 2005; Sarker et al., 2021; Wang\net al., 2025). Existing taxonomies (e.g., (Kautz, 2022))\ncategorize these methods by how tightly neural and sym-bolic components are intertwined, from loosely coupled\npipelines to deeply integrated architectures. Here, we focus\non four representative frameworks-Neural Logic Networks\n(NLNs), Logical Neural Networks (LNNs), Logic Tensor Net-works (LTNs), and Neural Logic Machines (NLMs)\u2014each\nof which approximates logical operations or embeds logical\nstructures into neural models."}, {"title": "5.1. Parameterized Logic Representations: Multi-Ary\nAtoms", "content": "Both NLNs and LNNs seek to generalize classical AND/OR\nwith learnable weights, making logical operations differen-tiable and enabling partial-truth inputs in [0, 1]. This reveals\na shared philosophy of embedding logical structure directly\ninto network computations, thereby preserving some inter-pretability at the operator level.\nNeural Logic Networks (NLNs) (Yang et al., 2019) map\nBoolean operators (e.g., AND, OR, XOR) into weighted,\nmany-valued logic, frequently using product-based formu-lations:\n$\\YAND = \\prod_{i=1}^n [1 \u2013 w_i (1 \u2212 x_i)]$,\n(2)\n$\\YOR = 1 \u2013 \\prod_{i=1}^n [1 \u2013 w_ix_i]$,\n(3)\nwhere wi \u2208 [0, 1] are membership weights. This approach\ncan excel at discrete algorithmic tasks (e.g., addition, sort-ing). However, product-based logic can lead to near-zero\noutputs when multiple inputs xi < 1, potentially undermin-ing stability in high-dimensional or uncertain settings."}, {"title": "5.2. Neural Approximation of Logical Quantifiers via\nPerceptrons", "content": "Logic Tensor Networks (LTNs) (Serafini & Garcez, 2016)\nembed first-order logic into tensor-based neural systems by\nmapping logical predicates to continuous embeddings, then\napplying fuzzy t-norms for AND/OR. They unify multiple\ntasks (e.g., classification, clustering, relational reasoning)\nyet can become complex in deep architectures where large\nnumbers of predicates and constraints must be managed."}, {"title": "6. Introducing Logical Neural Units (LNUs)\nfor Modular and Scalable Logical\nReasoning", "content": "Existing neurosymbolic approaches incorporate logical op-erations in various ways but face several limitations. Some\nrely on product-based fuzzy logic for Boolean connectives,\nwhich can become numerically unstable in high-dimensional\nsettings (Yang et al., 2019). Others inherit the constraints of\nperceptron-based architectures, limiting their expressivity\nfor structured reasoning (Riegel et al., 2020). While some\nmethods extend to first-order logic or multi-hop relational\nquantifiers, they often require additional modules or iterative\nmechanisms, impacting scalability and interpretability (Ser-afini & Garcez, 2016; Dong et al., 2019)."}, {"title": "6.1. Core Principles for Logical Neural Units (LNUs)", "content": "These challenges motivate the development of Logical Neu-ral Units (LNUs), a framework that embeds logical opera-tions directly into deep networks, ensuring stable and scal-able neurosymbolic reasoning. LNUs enable smooth inter-polation between soft and hard logic, allowing adaptation\nacross tasks and domains.\nScalability is a key consideration. Product-based logic can\nsuffer from vanishing gradients, while MLP-based quanti-fiers often lack interpretability. LNUs must balance numer-ical stability with effective layering in deep networks. A hierarchical design that stacks multiple LNUs while main-"}, {"title": "6.2. Illustrative LNU Architecture", "content": "LNUs generalize and unify prior differentiable logic meth-ods within a single neural unit. Instead of standard\nLinear+ReLU layers, we define an LNU block as:\n$LNU(x_1,..., x_n; \\theta) = [T_{\\land}(g_1(x); \\theta_1), T_{\\lor} (g_2(x); \\theta_2), ...]$,\n(6)\nwhere $T_{\\land}$ and $T_{\\lor}$ are learnable t-norm and t-conorm oper-ators approximating logical AND and OR functions. The\nfunctions $g_i(x)$ map the input vector $x = (x_1,...,x_n)$ to\nreal-valued logic features. The parameter set $\\theta$ consists of\nadaptive weights that dynamically adjust logical composi-tions during training, allowing LNUs to learn task-specific\ndependencies."}, {"title": "6.3. LNU Layer Composition", "content": "Each input feature, interpreted as a truth degree in [0, 1],\nis combined via learnable logic (e.g., softmin, softmax, or\npolynomial expansions) to approximate logic operations.\nDifferentiable Logic Approximation. Classical Boolean\nand fuzzy logic operations, such as AND and OR, are often\ncomputed using min and max functions, as in G\u00f6del T-norm\nand T-conorm (Hajek, 1998; van Krieken et al., 2022). To fa-cilitate smooth gradient-based learning, LNUs approximate\nthese operations using softmin and softmax:\nsoftmax(\u03b2z); \u2248 max(z), softmin(\u03b2z); \u2248 min(z) as \u03b2 \u2192 \u221e.\nHere, each input zi is defined as a weighted feature:\n$zi = x_i \\cdot w_i$, where $xi \u2208 x, wi \u2208 w, i = 1,..., d$.\n(7)\nThe parameter \u03b2 \u2265 0 controls the sharpness of the ap-proximation, with larger values making the function more\nBoolean-like. The softmin function is efficiently computed\nvia: softmin(\u03b2z) = softmax(-\u03b2z).\nLocally Gated Logical Consistency. To ensure logical\nconsistency, each feature's contribution is weighted via\nlearned importance factors. Given an input vector z ="}, {"title": "6.4. Evaluating LNU Interpretability and Decision\nBoundaries", "content": "Logical operations such as AND and OR follow explicit\ncompositional rules, making their decisions inherently trans-parent. Neural Logic Units (NLUs) enhance interpretabil-ity by structuring decisions through learned logical oper-ations. To assess this interpretability, we analyze deci-sion boundaries and compare them against standard inner-product-based layers.\nVisualizing decision boundaries is a common approach to\nunderstanding model behavior. Here, we constrain input\ndata to the range [0, 1] and fix the weights for both NLUs and\ndense (inner-product) layers, ensuring an even distribution\nthat aligns with hard AND/OR logic. This setup allows us\nto evaluate whether decision-making can be explained at the\nlevel of a single computation unit's output. Specifically, we"}, {"title": "6.5. Toy Example: Learning a Logical Function", "content": "To evaluate the potential of logic-friendly modules, we con-duct experiments on a toy logical task. This example show-cases the advantages of the LNU Layer in capturing discrete\nlogical patterns, particularly in low-data regimes.\nToy Logical Functions. We test a simple logical expres-sion to assess the model's ability to perform logical infer-ence.\nTask: Given the logical function: f(x1, x2, x3) = (x1 V\nx2)^(x3), the goal is to learn this function from a subset\nof possible (x1, x2, x3) combinations. We generate random\ninput values in the range [0, 1], using 20 samples for training\nand 200 samples for testing. In data generation, inputs are\nbinarized such that xi > 0.5 is mapped to 1, and otherwise\n0. This ensures that the target outputs adhere strictly to\nlogical evaluations in the set {0, 1}."}, {"title": "7. Discussion and Future Directions", "content": "By explicitly incorporating deductive reasoning,\nthese modules enhance logical inference without requir-ing fundamental architectural changes. We extend existing\nneurosymbolic frameworks by introducing Logical Neural\nUnits (LNUs), which embed approximate logical operations\n(e.g., AND, OR, NOT) directly into neural architectures\nusing weighted real-valued logic. These units bridge the\ngap between sub-symbolic efficiency and symbolic reason-ing, providing a scalable alternative to brute-force model\nexpansion. By unifying neural and logical reasoning, we\npropose a shift away from continuous scaling, which con-sumes vast computational resources and energy, towards a\nmore structured and efficient approach to AI development."}, {"title": "7.2. Critical Limitations and Challenges", "content": "LNUs provide a promising bridge between sub-symbolic\nlearning and symbolic reasoning, yet several challenges"}]}