{"title": "Federated Deep Subspace Clustering", "authors": ["Yupei Zhang", "Ruojia Feng", "Yifei Wang", "Xuequn Shang"], "abstract": "This paper introduces FDSC, a private-protected subspace clustering (SC) approach with federated learning (FC) schema. In each client, there is a deep subspace clustering network accounting for grouping the isolated data, composed of a encode network, a self-expressive layer, and a decode network. FDSC is achieved by uploading the encode network to communicate with other clients in the server. Besides, FDSC is also enhanced by preserving the local neighborhood relationship in each client. With the effects of federated learning and locality preservation, the learned data features from the encoder are boosted so as to enhance the self-expressiveness learning and result in better clustering performance. Experiments test FDSC on public datasets and compare with other clustering methods, demonstrating the effectiveness of FDSC.", "sections": [{"title": "1 Introduction", "content": "Subspace clustering (SC) is one of the popular clustering methods in recent years, which aims at learning the potential subspace of high-dimensional data and partitioning the data [32]. SC has drawn extensive attention in fields such as spectral clustering [41], image segmentation [3] and data dimensionality reduction technique [43] in the past decades. Recently, a large number of algorithms for spectral clustering have been proposed [5, 20]. Their basic idea is to construct an affinity matrix on the datasets and cluster according to the eigenvectors of the affinity matrix [29].\nSince real-world data often have complex distribution and non-linear characteristics, spectral clustering in linear space cannot adapt to the actual application scenarios. Deep neural network has the potential to extract data features and map them into linear subspace [23], which consists of auto-encoder and self-expressive layer. The auto-encoder exploits self-reconstruction loss to learn latent features [44], and the self-expressive layer is used to learn the affinity matrix [26, 32]. Deep subspace clustering has general application, such as face image segmentation under different lighting and postures [17]. Although the performance of deep subspace clustering has been greatly improved through the efforts of all researchers, the rapid increase of data that raises a large number of parameters limits the deep clustering effect [42]. In addition, the decentralization and privacy of real-world data make centralized deep clustering difficult to perform [35]. The emergence of federated learning is one of the effective ways to solve these problems\nFederated learning (FL) is a machine learning framework that enables many clients collaboratively learning a shared model without exchanging their private data [4, 39]. Using privacy protection, FL can be applied to most practical application scenarios such as smart healthcare [2, 31], movie recommendation [12] and smart retail [4]. Although FL has been studied a lot in the supervised learning environment, the FL research in unsupervised setting is still little [9], especially in the field of clustering.\nThe main challenge in FL is decentralized and heterogeneous characteristics in the data [15]. In the unsupervised FL setting, clients are usually edge devices [25], which makes data features distributed in different spaces. In order to solve the problem of heterogeneous data, most researches focus on clustering clients, i.e., the clustered federated learning [13, 34]. They usually capture the heterogeneity between clients and assign clients to different global models [24]. However, most methods do not consider the communication problems in FL, resulting in low training efficiency and complicated process [13]. A few studies focus on collaborative clustering of data on clients. k-FED [9] proposed a one-shot federated clustering method based on k-means in which all clients share a set of global cluster centers. However, there is still a lack of research on learning local cluster centers. Generally, each client is more concerned about the clustering results on its own data.\nInspired by the above, we propose a novel federated deep subspace clustering model, i.e., FDSC, which trains an auto-encoder to learn data features and a self-expressive layer to construct affinity matrix for spectral clustering. Auto-encoder of each client consists of a shared encoder and a private decoder. The shared encoder is communicated between clients and the central server to improve the data feature extraction. The local decoder is used to reconstruct the original data. In order to keep the local feature space structures of affinity matrix, we add adjacency graph information to the network. We align the affinity matrix with the local adjacency matrix in the form of regular terms. Our main contributions are as follows.\n(1) We propose a novel federated deep subspace clustering framework, i.e., FDSC, show in Fig. 1. We extract data features through the shared convolutional encoder, and perform spectral clustering through the affinity matrix obtained from the local self-expressive layer.\n(2) Our studies achieve the maintenance of the graph structure between local data points that the affinity matrix should be aligned with the adjacency matrix of data points.\n(3) We evaluate the clustering result of FDSC on four image datasets. Our experiments show that FDSC has better performance than the state-of-the-art methods.\nThe rest of the paper is organized as follows. In Section 2, we introduce the previous research on deep subspace clustering and federated learning. We show the proposed framework FDSC in Section 3. In Section 4, we present the evaluation results on four public image datasets and compare them with the state-of-the-art methods. Section 5 concludes the framework of this paper."}, {"title": "2 Related Work", "content": "In this section, we introduce three related fields in this paper, i.e., deep subspace clustering (DSC), federated learning (FL) and federated clustering (FC)."}, {"title": "2.1 Deep Subspace Clustering", "content": "DSC is used to solve the problem that the performance of subspace clustering usually drops in non-linear data spaces [40]. DSC incorporates a self-expressive layer in deep auto-encoder to make the representation in the latent space more suitable for spectral clustering [32]. Benefiting from data extraction of convolutional networks (CNN), DSC has been widely studied [1]. DSC network usually consists of an auto-encoder and a self-expression layer [19]. The former is used for learning data representation, and the latter is used for spectral clustering.\nSuppose there is a set of unlabeled data points \\(X = \\{x_i\\}_{i=1,...,n}\\), where \\(x_i \\in R^d\\) is the i-th data sample and n is the total number of samples. These data points come from k different subspace \\(\\{S_i\\}_{i=1,...,k}\\). In general, the self-expressive layer can be expressed as that the data point in a subspace is a linear combination of other points in the same subspace [14], i.e. \\(X = XC\\), where \\(C \\in R^{n \\times n}\\) is the self-expressive matrix with diagonal block structure [16]. We define the task of the self-expressive layer as\n\\[\\min_{C \\in R^{n \\times n}} ||C||_p + \\lambda ||X-XC||_F \\text{ s.t. } (diag(C) = 0),\\]\nwhere A is a parameter, \\(||C||_p\\) represents an arbitrary norm of matrix C and \\(|| . ||_F\\) represents the Fibonacci norm. p has been defined in some studies, e.g., [10] studied the sparse subspace clustering with l\u2081 norm, [6] proposed a stochastic sparse subspace clustering method using 12 norms, [7] performed the generalized nonconvex low-rank tensor approximation with nuclear norm. The self-expressive layer takes the representation of the data extracted by the deep encoder as input. Suppose X is the data reconstructed by the auto-encoder, and Z is the data representation of the encoder output. In general, the loss of deep subspace clustering is [17]\n\\[||\\hat{X}-X||_F^2 + \\lambda_1 ||C||_p + \\lambda_2 ||Z - ZC||_F^2\\\\\n\\text{ s.t. } (diag(C) = 0),\\]\nwhere \\(\u03bb_1\\) and \\(\u03bb_2\\) are two parameters."}, {"title": "2.2 Federated Learning", "content": "FL is a decentralized distributed machine learning framework for joint cooperation and privacy protection [30]. It allows multiple clients with private data to participate in model training through the central server [11]. Suppose there are m clients. Client i holds the local datasets with the number ni, i.e. X\u2071. Assume that the local model parameters of client i is Wi. Generally, the objective of FL is to learn a global model with collaborative clients [22],\n\\[\\min_{W} f(W) = \\sum_{i=1}^m p_i f_i(W_i),\\]\nwhere \\(p_j = n_j/\\sum_{i=1}^m n_i\\) denotes the parameter weight of the j-th client. f(W) is the loss function of the global model with parameters W and \\(f_i(W_i)\\) is the loss function of i-th client.\nMany existing studies are devoted to FL in a supervised environment, such as FedAvg [28], FedProx [21] and FedProto [36]. In order to solve the heterogeneity of data among clients, some research has turned to a new perspective recently that divide clients into different groups, i.e., clustered federated learning [27]. The author of [13] proposed IFCA framework that divides clients into different clusters according to similarity and each cluster learns a sharing model. The author in [24] developed a multi-center aggregation model that clusters clients according to local model parameters. Although these methods have considered the solution of data heterogeneity mentioned in Section 1, they need experts to label the local data of the clients [25]. Actually, the data of terminal equipment is often not marked by experts, which makes supervised learning difficult to train. Federated clustering is the way to solve unlabeled data."}, {"title": "2.3 Federated Clustering", "content": "Although the research of federated learning in the field of clustering has been studied in the past years, most of them focus on clustering clients. Considering the unsupervised learning environment, the author in [9] proposed k-FED, which is a one-shot federated clustering framework. They solves the communication cost problem in FL. In addition, the author of [35] developed a federated fuzzy clustering algorithm to solve the clustering of horizontal data partitions. The research on FC needs more exploration. Inspired by centralized deep subspace clustering, we propose a subspace clustering method in federated environment."}, {"title": "3 The Proposed Model", "content": "This section mainly introduces our proposed method FDSC, which includes motivation, the objective function, the framework and the optimization algorithm."}, {"title": "3.1 Motivation", "content": "In general, traditional federated learning aims at learning common data representations from various clients [8]. This approach enables the representation model to adapt to each private client. The application of deep subspace clustering brings vitality to federated clustering learning. There are three observations in federated deep subspace clustering:\n(1) Traditional federated learning which is to learn a global clustering model does not perform well on the client with data heterogeneity. Personalized federated learning usually needs a shared representation model and a local downstream task model.\n(2) The key of deep subspace clustering is to generate the self-expressive matrix for data representation. Since spectral clustering is performed on the self-expressive matrix, the self-expressive layer is local model.\n(3) In order to keep the reconstruction of local data, the decoder on client should not be shared.\nThere is currently a lack of a federated framework for deep subspace learning, which preserves the commonality of data representation on each client and performs subspace clustering locally."}, {"title": "3.2 Objective Function", "content": "Inspired by the above discoveries, our proposed method FDSC divides the data on the client into different subspaces. Each client has a shared encoder \\(M_E\\), a private self-expressive layer \\(M_R\\) and a private decoder \\(M_D\\). Let \\(E_i\\), \\(R_i\\) and \\(D_i\\) denote the encoder parameters, the self-expressive layer parameters and the decoder parameters of the client i. \\(M_E\\), captures the common features among clients in federated learning framework. \\(M_D\\), maintains the local feature space of the client.\nSuppose \\(Z_i\\) is the output of the encoder \\(M_E\\),, i.e., the representation matrix of data set \\(X_i\\) on the client i. Each \\(z_i\\) in \\(Z_i\\) is a node in the linear space. Through the fully connected linear layer \\(M_{R_i}\\), the weight of self-expressive layer is the self-expressive matrix. The loss function on client i is\n\\[f_i = \\frac{1}{2}||\\hat{X}^i - X^i||_F^2 + \\lambda_1 ||R_i||_p + \\frac{\\lambda_2}{2}||Z_i - Z_iR_i||_F^2\\\\\n\\text{s.t. } (diag(R_i) = 0),\\]\nwhere \\(\u03bb_1\\) and \\(\u03bb_2\\) are parameters, and \\(|| . ||_p\\) is arbitrary matrix norm. Then, the objective function of FDSC is defined as\n\\[\\min_{E, R_i, D_i} \\sum_{i=1}^m p_i \\min_{E_i, R_i, D_i} f_i (E_i, R_i, D_i),\\]\nwhere \\(p_j = n_j/\\sum_{i=1}^m n_i\\) denotes the weight of the j-th client. E is the global encoder, which is weighted and averaged by the encoders \\(E_i\\) of the clients participating in the t round of communication,\n\\[E^{(t+1)} = \\sum_{i=1}^m p_iE_i^{(t)}.\\]\nIn order to ensure that the self-expressive matrix keeps the block characteristics of similar features on the client, we propose an approach of aligning to local adjacency matrix. We define an adjacency matrix constructor, i.e., g(\u00b7), and establish an adjacency matrix on local data Xi. g(\u00b7) can be the k-nearest neighbors (k-NN) method [33]. Then, we have\n\\[A_i = g_{k-NN}(X^i),\\]\nwhere Ai is the adjacency matrix on the i-th client. Then, the objective function on client i is\n\\[f_i = \\frac{1}{2}||\\hat{X}^i - X^i||_F^2 + \\lambda_1 ||R_i||_p\\\\\n+ \\frac{\\lambda_2}{2}||Z_i - Z_iR_i||_F^2 + \\frac{\\lambda_3}{2}||\\alpha A_i - \\beta R_i||_F^2 \\\\\n\\text{s.t.}(diag(R_i) = 0),\\]\nwhere \\(\u03bb_3\\), \u03b1 and \u03b2 are balance parameters."}, {"title": "3.3 Framework", "content": "In this study, the auto-encoder is implemented by the convolutional neural networks (CNN) to train image data. To be specific, we use two CNN layers for encoder and decoder respectively and a fully connected layer for self-expressive layer. The overall framework of FDSC is shown in Fig. 1."}, {"title": "3.4 Optimization Algorithm", "content": "In FDSC, the sever is responsible for communication among clients. The server is used to calculate and broadcast the global encoder E\u2217. On each client, the local model consists of shared encoder Ei, private self-expression layer Ri and private decoder Di. At the beginning of local training, the encoder E\u00a1 is replaced by the received global encoder E\u2217. To compute \u03bb3 (\u03b1Ai \u2013 \u03b2Ri) in Eq. (8), we use K-NN to calculate the adjacency matrix of each client data and store it before model training. In order to ensure the stability of local model training, we use stochastic gradient descent with momentum. The details of FDSC are given in Algorithm 1."}, {"title": "4 Experiment", "content": "This section shows the experimental results of the proposed FDSC on four public image datasets."}, {"title": "4.1 The Used Datasets", "content": "In order to compare with the state-of-the-art subspace clustering methods, we perform FDSC on four image datasets. The datasets are shown as follows."}, {"title": "4.2 Experiment Settings", "content": "In order to evaluate the clustering performance of our method, we compared FDSC with Low rank subspace clustering (LRSC) [38], Deep Low-Rank Subspace Clustering (DLRSC) [18] and Deep Subspace Clustering Networks (DSCN) [17]. We divided the dataset containing n samples into m subsets, where each subset contains about n/m samples. We randomly selected samples with q categories into these subsets, where \\(1 \\le q \\le c\\) and c is the total number of classes in the data set. In our experiment, we set \u03c4 = 7, \\(\u03bb_3 = 1e6\\), \u03b1 = 1, \u03b2 = 1. The other setting of training parameters are as follows: for MNIST, T = 100, \\(\u03bb_1\\) = 1, \\(\u03bb_2\\) = 15, m = 20, r = 0.25; for ORL, T = 200, \\(\u03bb_1\\) = 2, \\(\u03bb_2\\) = 0.2, m = 5, r = 0.4; for COIL20, T = 100, \\(\u03bb_1\\) = 1, \\(\u03bb_2\\) = 75, m = 5, r = 0.4; for COIL100, T = 100, \\(\u03bb_1\\) = 1, \\(\u03bb_2\\) = 15, m = 5, r = 0.4.\nIn order to compare the effect of clustering, we used four evaluation indicators, which are ACC, NMI, AMI and ARI. We compared the clustering labels with the real labels to calculate the clustering accuracy, i.e.,\n\\[ACC = \\frac{\\sum_{i=1}^n \\sum_{j=1}^{m_i} \\delta(\\phi(\\hat{y}_j), y_j)}{n},\\\\]\nwhere yj is the real label of sample xj and \\(\\hat{y}_j\\) is the clustering label in experiments. Indicator function \u03b4(x, y) = 1 when x = y, otherwise 0. \\(\\phi(\\hat{y}_j)\\) is a mapping function to find the clustering label that best match the true label.\nNMI is normalised mutual information, which is a commonly used evaluation metric in clustering, it measures the degree of consistency between the clustering results and the real labels, the NMI index value ranges from [0, 1], the closer the value is to 1, the higher the degree of similarity between the clustering results and the real labels, the NMI value is 0 means that there is no correlation between the clustering results and the real labels, i.e., clustering effect is very bad, and NMI value is 1 means that the clustering results are completely consistent with the real labels. NMI of 1 means that the clustering results are completely consistent with the real labels.\nThe AMI index is an adapted form of the Mutual Information Indicator (MII), which corrects for the uncertainty of the clustering results by taking into account the effects of random assignment and category imbalance on the results. Like the NMI, the AMI index takes values in the range [0, 1], with values closer to 1 indicating better clustering results.\nARI is the Adjusted Rand Index, which is used to measure the similarity between the clustering results and the real labels, and it takes into account the effect of the random allocation of data, and it takes the value in the range of [-1, 1].The closer the value of ARI index is to 1, it means that the clustering results are more similar to the real labels.\nThe final results of the four clustering indexes in this section are presented in the form of percentage."}, {"title": "4.3 Representation Visualization", "content": "This subsection aims to explore the influence of federated subspace clustering. We used MNIST to train FDSC and DSCN to display clustering results of data representation. In FDSC, we used 20 clients to train image samples, and q = 10. Then, we used t-SNE method [37] to reduce the representation of the image to 2D. Fig.3 shows the 2D scatter plots of 4 clients randomly selected from all 20 participants.The first row shows the result of FDSC representation, and the second row shows the result of DSCN representation, where the data points with different colours represent the real categories that the data belongs to.\nOn the first client, aggregation of two types of data points, royal blue and brown, using FSDC, works better. On the second client, aggregation of data points using FSDC's royal blue, teal, and brown colors works better. On the third client, pink data point aggregation with FSDC works better. On the fourth client, the aggregation of two types of data points, teal and brown, works better with FSDC.As shown in Fig.3, the represented scatter plot of FDSC has a better subspace segmentation effect than the centralized DSCN.\nIn addition, we evaluated the clustering results of this 2D scatter points with ACC, NMI, AMI and ARI. The first row in Table 1 shows that the representation results on four clients with using the local DSCN method. The second row in Table 1 shows the representation scatter points on FDSC. Table 1 shows four measurements of 2D points of FDSC and DSCN. As is shown, under the four clustering evaluation indexes, the corresponding values of FDSC were greater than those of DSCN.The clustering effect of 2D representation points on FDSC is better than that on DSCN. Our proposed FDSC absorbs the features between clients, which enhances the clustering effect."}, {"title": "4.4 Self-expression Matrix Visualization", "content": "This section explores the influence of federated self-expression learning with FDSC on clustering. We used MNIST and COIL20 to train FDSC and visualized the weight matrix of the self-expression layer.We set up 20 participants, the number of classes for each client is q = 10 for MNIST and q = 20 for COIL20. For comparison, the same client data is also trained on the DSCN method. Fig. 4 illustrates the results of self-expression matrix with local DSCN and FDSC respectively. The first row shows the matrix training on MNIST and the seconda row shows the results on COIL20. The matrix on the left of each row is the self-expression matrix of the DSCN, and the matrix results of the FDSC are on the right.\nWhen using the MNIST dataset, we distribute 10,000 test samples to 20 participants, so that each participant gets exactly 50 samples for each category. When using the COIL20 dataset, we assign 20 classes of data to each participant, and 10 data points for each class of data to facilitate testing. Subsequently, on the two types of datasets, we averaged the clustering results of the same category of data among the 20 participants and expressed the average results as numerical values between 0 and 1.\nThe color legend on the right side of each figure in Fig. 4 shows the relationship between color and value. The more red the color patch is, the stronger the similarity of the corresponding subspace data. The more the color patch tends to be purple, the weaker the data similarity of the corresponding subspace. In the first row, the diagonal patch of the second image is closer to red than the other area patches. In contrast, the color difference between diagonal and non-diagonal patches in the first image is not significant. In the second row, the diagonal patches of the second image are very close to red in color, while the non-diagonal patches are predominantly blue and green. In the first image, the diagonal patches are not as close to red as in the second image, but the non-diagonal patches are relatively close to red. This indicates that the self-expression matrix trained by FDSC has a better block diagonal structure, that is, the federated deep subspace learning method can better represent a certain data as a linear combination of other data in the same subspace."}, {"title": "4.5 Clustering Results", "content": "Table 2 shows image clustering results on four datasets by using LRSC, DLRSC, DSCN, FDSC1(\u03bb3 = 0) and FDSC2(\u03bb3 = 1e6). We assumed that the number of clients corresponding to each dataset is m and the data categories held on each client is q. To ensure the appropriateness of the amount of data obtained by each client when using different datasets, we set m = 20 on MNIST, m = 5 on ORL, m = 5 on COIL20 and m = 5 on COIL100. To explore the influence of data heterogeneity on training effect, we first tested clustering methods with different q to compare the performance of the methods. We set q = {5, 10} on MNIST, q = {20, 40} on ORL, q = {10, 20} on COIL20 and q = {50, 100} on COIL100.\nAs is shown, our federated method FDSC has higher clustering accuracy than other methods with centralized learning on all datasets. Under various parameter settings, FDSC achieves the best clustering index. Compared with FDSC1, FDSC2 adds a regular term that aligns the self-expression matrix to the adjacency matrix, and the clustering result is better, which verifies the FDSC with regular optimization achieves better clustering results than the method without regular optimization. In the environment of data heterogeneity, in addition to MNIST data sets, other data sets such as ORL, COIL20 and COIL100 perform better when the degree of data heterogeneity on the client is low, that is, the q is large. This shows that in most cases, with the reduction of client data heterogeneity, FDSC method can effectively improve the clustering effect. On MNIST, when the data heterogeneity of the client is higher, i.e., the q is less, the clustering result is better."}, {"title": "4.6 Parameter Discussion", "content": "In this subsection, we made more insights into the impacts of the parameters in FDSC, i.e., the number of clients m and the batch size b. We used MNIST to explore the influence of parameters on FDSC.\nTo probe the influence of the number of clients, we set the batch size of clients to 500, and changed the number of clients m with {10, 20, 30, 40, 50}. Table 3 shows the clustering results by different m. From the results, as the number of clients increases, the clustering effect is better. The clustering accuracy of FDSC benefits from the participation of more clients.\nTo examine the effect of the batch size in model training, we assigned the MNIST to 20 clients by varying batchsize b with {100, 250, 500, 750, 1000}. Table 4 shows the evaluation indicators in FDSC. As is shown, as the batch size increases, the clustering effect is better.FDSC achieves higher performance with the increase of client batch size."}, {"title": "5 Discussion and Conclusion", "content": "This study proposes a novel federated deep subspace clustering framework, called FDSC. As far as we know, FDSC is the first federated clustering model that using deep subspace clustering to group the data on all clients jointly. FDSC exploits shared encoder to learn common representation and learns the self-expressive matrix for local spectral clustering task. We finally evaluated FDSC on four image data sets and compared it with subspace clustering model. The experimental results show that FDSC performs better under the four clustering indexes."}, {"title": "6 Acknowledgments", "content": "This work was supported in part by the National Natural Science Foundation of China (Nos. 62272392 and U22A2025), the Key Research and Development Program of Shaanxi Province (No. 2023-YBGY-405), and Fundamental Research Funds for the Central Universities (No. G2023KY0603)."}]}