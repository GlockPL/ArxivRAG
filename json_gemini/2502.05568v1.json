{"title": "Large Multimodal Models for Low-Resource Languages: A Survey", "authors": ["Ana-Cristina Rogoz", "Marian Lupa\u0219cu", "Mihai Sorin Stupariu", "Radu Tudor Ionescu"], "abstract": "In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large multimodal models (LMMs) showcased remarkable capabilities in processing and understanding diverse types of data, including text, images, audio and video. Models like GPT-4V, KOSMOS-1 [Huang et al., 2023] and PaLM-E [Driess et al., 2023] achieved impressive performance levels across various multimodal tasks through their ability to simultaneously process and reason about multiple modalities. However, these developments have primarily focused on high-resource languages, particularly English, leaving a significant gap in supporting the world's many low-resource languages. The distinction between high-resource (HR) and low-resource (LR) languages is primarily determined by the availability of digital resources and training data. High-resource languages, such as English, Mandarin, and Spanish, benefit from extensive digital corpora, parallel texts, and annotated datasets. In contrast, low-resource or understudied languages, which constitute the majority of the world's languages, lack sufficient digital resources, standardized datasets, and computational tools. This disparity is particularly pronounced in multimodal contexts, where the scarcity of paired data across modalities (e.g. image-text pairs, audio-text alignments) poses additional challenges.\nThe motivation for developing multimodal capabilities for LR languages is compelling. First, multimodal processing better reflects how humans naturally communicate and understand information through multiple sensory channels. Second, visual and audio cues can provide crucial contextual information that helps to overcome the limitations of scarce textual data. Third, many LR languages are primarily spoken rather than written, making multimodal approaches particularly relevant for their digital preservation and processing. However, developing multimodal systems for LR languages faces several significant challenges, including: (1) the scarcity of high-quality multimodal datasets in these languages, (2) the lack of standardized evaluation benchmarks, (3) the computational cost of training large-scale models with limited resources, and (4) the complexity of handling different writing systems, dialects, and cultural contexts. Moreover, the problem of catastrophic forgetting when adapting pre-trained models to new languages and the challenge of maintaining performance across different modalities pose significant technical hurdles."}, {"title": "2 Taxonomy", "content": "In Figure 3, we systematically organize LMMs for LR languages into six main categories, reflecting both the current state of the field and the primary research strategies for addressing challenges in the context of under-represented languages. The first two categories focus on constructing high-quality resources. While the first category discusses multimodal data creation either from scratch or via extending existing datasets, the second approach centers on synthetic data generation, which automatically expands available resources via back-translation and image-based generation. Building upon this work, we present several multimodal fusion techniques and provide various strategies for effectively combining this information, ranging from early and late fusion to more complex hybrid approaches. In the fourth category, we illustrate visual enhancement techniques that harness visual information through image-guided translation and visual disambiguation methods, highlighting their importance for improving translation quality and resolving ambiguities. Expanding from the single-modality solutions, the next category focuses on cross-modal transfer learning approaches that can facilitate knowledge sharing based on both modality transfer and language transfer. Finally, our last category comprises architectural innovations specifically tailored for multimodal tasks in the context of LR languages. We structure the remainder of this article according to our novel taxonomy shown in Figure 3."}, {"title": "3 Multimodal Data Creation", "content": "There are two main approaches to create multimodal datasets for LR languages. The first is based on multimodal dataset creation from scratch, while the second is based on using an existing resource as a starting point. We next discuss papers introducing novel datasets based on the two alternatives.\nDataset creation from scratch. Dataset creation from scratch has emerged as a crucial approach for enabling multimodal research in LR languages, particularly for sentiment analysis and specific language tasks. Multiple research teams have focused on creating specialized datasets through direct data collection and annotation, such as collecting Arabic videos with multimodal features for sentiment analysis [Najadat and Abushaqra, 2018], building comprehensive Tamil and Malayalam video review datasets [Chakravarthi et al., 2021], and developing new corpora for languages such as Malay [Taylor and Fauzi, 2024]. A significant trend has been the creation of meme-based datasets, with efforts focused on Bengali, through MemoSen and MUTE [Hossain et al., 2022a; Hossain et al., 2022b], and Romanian, through RoMemes [P\u0103i\u015f et al., 2024], all incorporating multiple levels of annotation."}, {"title": "Dataset extension", "content": "In addition to building data from scratch in the context of LR language understanding, there have been several efforts for leveraging existing datasets of rich-resource languages and building upon them. Sen et al. [2022] introduced the Bengali Visual Genome (BVG) dataset, which extends the Visual Genome dataset [Krishna et al., 2017] with Bengali translations and annotations, enabling the development and evaluation of multimodal models for Bengali-English machine translation (MT) and image captioning. Similarly, Abdulmumin et al. [2022] created the Hausa Visual Genome (HaVG) dataset by translating a subset of the Visual Genome dataset into Hausa, providing a valuable resource for English-to-Hausa multimodal MT. Building upon prior work and continuing the focus on the Hausa language, Parida et al. [2023] introduced the Hausa Visual Question Answering (HaVQA) dataset, which adapts question-answer pairs from the Visual Genome dataset to the Hausa language through manual translation, creating the first visual question-answering (VQA) dataset for Hausa.\nApart from the focus on African languages, Saichyshyna et al. [2023] extended the Multi30K dataset [Elliott et al., 2016] to include Ukrainian translations and captions, facilitating integrated vision and language research in Ukrainian. More recently, Lovenia et al. [2024] presented SEACrowd, a comprehensive multilingual and multimodal data hub and benchmark suite for Southeast Asian languages, which covers 13 tasks across three modalities (text, image, and audio) and 38 Southeast Asian indigenous languages. These tasks include MT, image captioning, VQA, and speech recognition. To build SEACrowd, the authors leveraged already existing datasets, but they also created new data through crowd-sourcing and manual annotation."}, {"title": "4 Synthetic Data Generation", "content": "Another approach to efficiently create multimodal datasets for under-represented languages relies on synthetic data generation. While the previous section captures efforts comprising significant human involvement, we next cover synthetic data generation, which relies on existing resources and automated techniques to generate new data.\nBack-translation. A common approach for synthetic data generation relies on the usage of back-translation, which has proven to be an effective technique to enhance the data for multilingual MT (MMT) in LR language pairs. Chowdhury et al. [2018] demonstrated the effectiveness of this technique for training a neural MMT system in the context of LR language pairs by leveraging the Flickr30k dataset [Young et al., 2014] and translating the source language (English) captions to the target LR language (Hindi). In the WMT24 English-to-Low-Resource Multi-Modal Translation task, Haq et al. [2024] showcased the effectiveness of back-translation for Hindi. Another use case of back-translation was shown by Alwajih et al. [2024], who, starting from English-based image-text pairs, employed translation to Arabic, as well as back-translation. This was necessary for evaluating the quality of the translation, before passing the data to humans for Arabic dialect translation and training a dialect-aware LMM, named Dallah. However, the consistency of back-translated"}, {"title": "5 Multimodal Fusion Techniques", "content": "We identified three distinct types of fusion approaches employed in multimodal learning, categorized into early fusion, late fusion, and architectural fusion approaches. An overview of the different fusion strategies is provided in Figure 4. The diagram depicts the various ways in which textual, visual and auditory features can be combined at different stages to enable effective integration of multimodal information.\nEarly fusion. Early fusion, also known as feature-level fusion, involves combining features from different modalities at the input level before passing them through a unified model. In Persian sentiment analysis, Dashtipour et al. [2021] demonstrated the effectiveness of early fusion by combining acoustic, visual, and textual features through a context-aware framework. Their approach showed that integrating features at an early stage allowed the model to capture cross-modal interactions more effectively, achieving 91.39% accuracy compared to unimodal approaches. Similarly, the shared task on Tamil and Malayalam multimodal sentiment analysis [Premjith et al., 2023] revealed that early fusion techniques were particularly effective for handling code-mixed content and cultural nuances specific to these languages.\nFor Amharic hate speech detection in memes [Jigar et al., 2024], the authors employed concatenation, directly combining visual features from memes with textual features, demonstrating the effectiveness of this straightforward approach for LR languages. The integration of multimodal features through gating mechanisms has shown particular promise in LR scenarios, as demonstrated in English-to-Low-Resource translation tasks for Hindi, Malayalam, Bengali, and Hausa, where [Hatami et al., 2024] used gated fusion to selectively combine visual and textual information. This approach was further validated by Alalem et al. [2023] in their Audio-Text Fusion (ATFusion) model for English and Egyptian Arabic, where they employed Group Gated Fusion (GGF) to dynamically control the flow of information between modalities, achieving superior performance over traditional fusion methods with accuracy improvements of up to 76.21% for English and 70.79% for Egyptian Arabic.\nLate fusion. Late fusion, also known as decision-level fusion, combines predictions from separate modality-specific models at the decision stage rather than fusing features early in the pipeline. In this approach, individual models are trained independently for each modality (e.g., text, audio, and video) and their outputs are merged to produce the final prediction [Zhang et al., 2024; Arifin et al., 2024]. This allows each modality to be processed optimally according to its characteristics before integration. The independence between modality-specific models also provides flexibility, as individual components can be modified without affecting the others,"}, {"title": "Architectural fusion", "content": "Encoder-decoder fusion architectures have emerged as an effective approach for multimodal integration across different languages and tasks. Several studies have demonstrated success with this approach. For example, Chakravarthi et al. [2019] employed an encoder-decoder framework with phonetic transcription to improve machine translation between Dravidian languages, while Sehar et al. [2021] utilized an encoder-decoder architecture to fuse audio, video and text features for Urdu sentiment analysis. Similarly, Meetei et al. [2024] showed that encoder-decoder fusion of correlated modalities can enhance translation quality for LR languages. The key advantage of encoder-decoder architectures is their ability to first encode input features from different modalities into a shared representation space before decoding them into the target output.\nAttention-based fusion has also proven to be highly effective for multimodal integration [Ristea and Ionescu, 2023]. As shown by Haputhanthri et al. [2023] for Sinhala sign language recognition, attention mechanisms allow the model to dynamically focus on the most relevant features across modalities. Yang et al. [2024] successfully employed attention fusion for Mongolian sentiment analysis by combining features from audio, text and visual inputs. Zhang et al. [2024] demonstrated that attention-based fusion of multimodal data improves depression risk detection by allowing the model to attend to salient information across audio, video and text modalities. The ability of attention mechanisms to learn dynamic weights between modalities makes them particularly suitable for tasks requiring adaptive integration of complementary information sources.\nComparative analysis of fusion techniques. Each fusion approach presents distinct advantages and challenges in the context of LR languages. Early fusion enables deep interaction between modalities from the start, but can be computationally expensive and may suffer when one modality is noisy. Late fusion offers flexibility and robustness when modalities are missing, but may miss important cross-modal interactions. Architectural fusion approaches show promise in capturing complex relationships between modalities, but require careful tuning and substantial computational resources. A notable innovation in this space is the Multi-Representative Fusion (MRF) mechanism [Chauhan et al., 2022], which generates diverse representations for each modality and selectively chooses the best fusion via attention. This approach has shown particular promise in handling noisy inputs, achieving state-of-the-art performance on several LR sentiment analysis benchmarks."}, {"title": "6 Visual Enhancement Techniques", "content": "Visual enhancement techniques aim to improve MT quality by leveraging visual information to provide additional context and resolve ambiguities in the source text. These techniques broadly fall into two main categories: image-guided translation, which uses visual features to enhance the overall translation process, and visual disambiguation, which specifically focuses on resolving ambiguous words or phrases using visual context.\nImage-guided translation. A promising direction for improving translation quality for LR languages is the use of image-guided translation approaches. Chowdhury et al. [2018] showed that augmenting neural MT systems with visual features extracted from a pre-trained CNN and integrated into an encoder-decoder architecture can improve translation quality, achieving a bilingual evaluation under-study (BLEU) score of 24.2 for Hindi to English translation. Building upon these ideas, Laskar et al. [2020] developed a multimodal neural MT system with a bidirectional RNN encoder and doubly-attentive decoder for English-Hindi translation. Their system, which combines visual and textual features and employs pre-trained word embeddings from monolingual data, outperforms a text-only baseline, achieving a BLEU score of 33.57 versus 27.75 on the test set.\nSubsequent studies [Shi and Yu, 2022; Meetei et al., 2023a; Meetei et al., 2023b; Haq et al., 2024] have demonstrated the effective use of visual information for improving MT in LR settings, particularly for the English-Hindi language pair. Meetei et al. [2023a] proposed a video-guided multimodal MT framework that incorporates spatio-temporal video features, showing improvements of up to +4.2 BLEU over text-only baselines for English to Hindi translation, while Meetei et al. [2023b] explored multimodal translation for news domain data, showing that ResNet-based image features outperform VGG-based features and improve BLEU scores by +1.8 points. Additionally, Shi and Yu [2022] explored different approaches for extracting and integrating image features using VGG and ResNet models, achieving a +3 BLEU improvement over text-only translation. More recently, Haq et al. [2024] presented a context-aware transformer model that integrates visual features via BERT encoding, demonstrating consistent improvements over text-only baselines. Across all studies, qualitative analyses confirmed that visual cues are particularly beneficial for handling rare words and domain-specific terms, with both image and video modalities helping to resolve ambiguity and improve translation quality in LR scenarios.\nVisual disambiguation. While image-guided translation aims to enhance overall translation quality by integrating visual context, the visual disambiguation techniques focus on task-specific ambiguities by grounding them in visual information. In this regard, studies revolving around the creation of Visual Genome datasets for LR languages, such as Hindi"}, {"title": "Comparative analysis of visual enhancement techniques.", "content": "Image-guided translation consistently demonstrates performance improvements over text-only baselines for LR languages, though effectiveness varies with language pair, dataset size, and translation direction. These approaches excel at handling semantic ambiguities and culturally-specific concepts, but their success depends heavily on the quality of extracted visual features. A key limitation is the reliance on high-quality image-text pairs, which are often scarce for LR languages. While these techniques improve translation quality, they also introduce computational overheads. Future work should focus on developing more efficient visual feature extraction methods and better approaches for leveraging visual information with limited paired data."}, {"title": "7 Cross-Modal Transfer Learning", "content": "Modality transfer. Modality transfer addresses the challenge of transferring knowledge between different modalities to improve performance on LR tasks. A diversity of approaches have been used to achieve modality transfer. Chen et al. [2023] proposed a progressive transfer learning strategy that leverages both general pre-training (Kinetics-400 for visual and CC25 for language) and domain-specific pre-training (sign-to-gloss translation) to bridge modalities for sign language translation. Amalas et al. [2024] introduced a data-driven approach for selecting source languages and demonstrated that multilingual pre-training outperforms monolingual pre-training for text-to-speech systems. Yeo et al. [2024] tackled LR visual speech recognition by using Whisper's automatic transcriptions to generate training labels from unlabeled multilingual audio-visual data. For Arabic handwriting recognition, Bhatia et al. [2024] employed modality transfer through an architecture combining SwinV2 for visual encoding and RoBERTa for text decoding, while Tran and Thanh [2024] demonstrated successful modality transfer for Vietnamese through extensive pre-training of both vision and language components, combined with automated data curation methods. Notably, Onuoha and Uba [2024] challenged the assumptions about multimodal integration through their study of Igbo minimal pairs. Their findings show that native Igbo speakers can accurately distinguish minimal pairs through audio alone, suggesting that the benefits of cross-modal integration may be more relevant for non-native speakers than fluent ones.\nLanguage transfer. Language transfer is an approach for leveraging knowledge from resource-rich languages to improve model performance on LR languages. Recent work demonstrates several effective strategies. For example, Wang et al. [2023b] adapted MDETR to new languages by using adapters and code-switching without relying on MT data. Kim et al. [2023] focused on learning general speech knowledge from English for lip reading, and combining it with language-specific audio features. dos Santos et al. [2023] proposed to use data augmentation and contrastive learning to improve multilingual CLIP models for LR languages. Nortje et al. [2024] showed that initializing a Yor\u00f9b\u00e1 few-shot word learning model with weights from an English speech-image model substantially improves performance. These approaches share the common theme of transferring learned representations and knowledge from high-resource languages (typically English), while developing techniques to efficiently adapt and fine-tune models for target LR languages."}, {"title": "8 Architectural Innovations", "content": "Some recent architectural innovations in the context of LR languages have focused on adapting the CLIP architecture, originally introduced by Radford et al. [2021]. One such example is the LowCLIP [Asgarov and Rustamov, 2024] model, which replaces the original text encoder trained primarily on English text with a multilingual BERT (mBERT). The authors evaluated various lightweight image encoders, such as EfficientNet-B0 and Tiny Swin Transformer, for a more computationally efficient approach, while also targeting LR languages like Azerbaijani. To compensate for the lighter architecture and the scarcity of image-text pairs in Azerbaijani, LowCLIP leverages synthetic data generation via MT for text features, and image augmentation techniques, such as crop and rotation, for image features. In contrast, Xtreme-CLIP [Tang et al., 2023] took a different approach, where the authors introduced a parameter-efficient method that only tunes a small prototype matrix, while keeping the visual and text encoders frozen. Their model also leverages contrastive learning to provide additional supervised signals in LR settings. Collectively, these efforts extend the applicability of CLIP to multimodal image retrieval tasks.\nAnother approach for multimodality in the context of LR languages is introduced by Wu et al. [2019]. The approach combines two existing methods, a translation-based one and an alignment-based one, into a unified architecture for improving image captioning. The framework employs a model that first generates high-quality English captions, which are then used together with the images to produce captions in the LR language. The model achieves a fine-grained alignment between visual elements and captions in both languages via a cycle-consistency constraint, outperforming existing methods on standard metrics.\nMore recently, Jin et al. [2022] introduced FEWVLM, showing that careful prompt engineering and efficient architectural design can achieve strong performance in the context of LMM usage with either little data or computational needs. They managed to develop a moderate-size vision-language model that combines the sequence-to-sequence transformer architecture with prefix language modeling and masked language modeling, introducing effective prompt engineering approaches for visual-language tasks in the LR setting. Notably, FEWVLM outperforms Frozen (a model 31\u00d7 larger) and achieves comparable results to PICa (246\u00d7 larger), demonstrating that an efficient architecture can compensate for model size.\nTogether, the architectural innovations introduced by LowCLIP, XtremeCLIP, FEWVLM and similar frameworks demonstrate various viable alternatives for making multimodal models more accessible for LR languages."}, {"title": "9 Conclusion and Future Work", "content": "Conclusion. Our survey has provided a comprehensive analysis of LMM-based approaches for LR languages, comprising 106 studies across 75 languages. We noted that vision-language combinations dominate the current research landscape (63% of surveyed works), but there is also an increasing trend toward incorporating video and speech in recent works. Additionally, we observed a particular concentration of research in South Asian languages (including Hindi, Bengali, Malayalam) and Southeast Asian languages (including Vietnamese, Javanese, Malay), followed by contributions in Middle Eastern languages (Persian, Arabic) and African languages (Hausa, Amharic), leaving 40 other languages each represented by a single study. However, the landscape of LMMs for LR languages has shown remarkable progress across multiple dimensions, from data creation and engineering to different fusion techniques and architectural innovations. The emergence of projects like HVG, SEACrowd, and BVG highlights the growing attention to creating high-quality multimodal resources for traditionally understudied languages, from simple parallel corpus creation to synthetic data generation. Both fusion techniques and visual enhancement approaches have matured significantly, with advances ranging from early fusion architectures to advanced visual disambiguation methods. Recent successes with models such as Qalam, LaVy, and Amharic LLaVA demonstrate that carefully designed multimodal strategies can effectively leverage limited resources, while adapting large-scale architectures for LR contexts.\nFuture work. We identify several promising directions for future research in the area of LMMs for LR languages. While current work has established a foundation in vision-language applications, there is a clear need to explore additional modality combinations, particularly incorporating audio and video modalities for LR languages, which could enable more robust and realistic language learning. Additionally, the development of comprehensive multimodal datasets remains a challenge, particularly for languages outside major language families. To address this limitation, research should focus on two areas: advancing synthetic data generation techniques, as demonstrated by recent works such as HVG, ELAICHI and Vintern-1B, and improving cross-lingual transfer methodologies, building upon frameworks such as XtremeCLIP and LowCLIP. These approaches show promise in overcoming the persistent challenge of limited training data availability for LR languages.\nWe also identify several advanced methods that have not been extensively explored in the context of multimodal learning for LR languages. These include: late fusion based on stacking, which learns to optimally combine outputs of individual models; tensor fusion, which captures complex cross-modal interactions through outer product and decomposition operations; kernel-based fusion, which maps data to high-dimensional spaces for non-linear integration of heterogeneous modalities; and graphical fusion, which leverages graph-based representations and neural architectures for multimodal data. Investigating the adaptation of these techniques to LR settings, while addressing challenges of data scarcity and computational efficiency, presents promising avenues for future research. We further recommend several strategies for improving fusion techniques in LR settings. First, researchers should consider adaptive fusion mechanisms that can dynamically adjust the contribution of each modality based on input quality and task requirements. Second, more efficient architectural designs are needed to make sophisticated fusion techniques accessible in resource-constrained environments. Third, the development of robust evaluation metrics specifically for multimodal fusion in LR contexts would help better understand the effectiveness of different approaches. Finally, taking inspiration from the success of MRF [Chauhan et al., 2022], future work should explore more hybrid approaches that combine the strengths of different fusion strategies, while maintaining computational efficiency. These recommendations aim to address the unique challenges of multimodal fusion in LR languages, while maximizing the benefits of complementary information from different modalities."}]}