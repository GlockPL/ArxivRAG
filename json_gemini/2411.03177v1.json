{"title": "On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models", "authors": ["Tariq Berrada", "Pietro Astolfi", "Melissa Hall", "Reyhane Askari-Hemmat", "Yohann Benchetrit", "Marton Havasi", "Matthew Muckley", "Karteek Alahari", "Adriana Romero-Soriano", "Jakob Verbeek", "Michal Drozdzal"], "abstract": "Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i) the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii) the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset \u2013 with FID improvements of 7% on 256 and 8% on 512 resolutions as well as text-to-image generation on the CC12M dataset \u2013 with FID improvements of 8% on 256 and 23% on 512 resolution.", "sections": [{"title": "1 Introduction", "content": "Diffusion models have emerged as a powerful class of generative models and demonstrated unprecedented ability at generating high-quality and realistic images. Their superior performance is evident across a spectrum of applications, encompassing image [7, 14, 39, 41] and video synthesis [35], denoising [52], super-resolution [49] and layout-to-image synthesis [51]. The fundamental principle underpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distribution, that progressively transforms it to a sample from the target distribution. The popularity of diffusion models can be attributed to several factors. First, they offer a simple yet effective approach for generative modeling, often outperforming traditional approaches such as Generative Adversarial Networks (GANs) [3, 16, 24, 25] and Variational Autoencoders (VAEs) [29, 48] in terms of visual fidelity and sample diversity. Second, diffusion models are generally more stable and less prone to mode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuning of hyperparameters and training procedures [23, 30]."}, {"title": "2 Conditioning and pre-training strategies for diffusion models", "content": "In this section, we review and analyze the conditioning mechanisms and pre-training strategies used in prior work (see more detailed discussion of related work in App. A), and propose improved approaches based on the analysis."}, {"title": "2.1 Conditioning mechanisms", "content": "Background. To control the generated content, diffusion models are usually conditioned on class labels or text prompts. Adaptive layer norm is a lightweight solution to condition on class labels, used for both UNets [21, 39, 41] and DiT models [38]. Cross-attention is used to allow more fine-grained conditioning on textual prompts, where particular regions of the sampled image are affected only by part of the prompt, see e.g. [41]. More recently, another attention based conditioning was proposed in SD3 [14] within a transformer-based architecture that evolves both the visual and textual tokens across layers. It concatenates the image and text tokens across the sequence dimension, and then performs a self-attention operation on the combined sequence. Because of the difference between the two modalities, the keys and queries are normalized using RMSNorm [53], which stabilizes training. This enables complex interactions between the two modalities in one attention block instead of using both self-attention and cross-attention blocks.\nMoreover, since generative models aim to learn the distribution of the training data, data quality is important when training generative models. Having low quality training samples, such as the ones that are poorly cropped or have unnatural aspect ratios, can result in low quality generations. Previous work tackles this problem by careful data curation and fine-tuning on high quality data, see e.g. [7, 9]. However, strictly filtering the training data may deprive the model from large portions of the available data [39], and collecting high-quality data is not trivial. Rather than treating them as nuisance factors, SDXL [39] proposes an alternative solution where a UNet-based model is conditioned on parameters corresponding to image size and crop parameters during training. In this manner, the model is aware of these parameters and can account for them during training, while also offering users control over these parameters during inference. These control conditions are transformed and additively combined with the timestep embedding before feeding them to the diffusion model.\nDisentangled control conditions. Straightforward implementation of control conditions in DiT may cause interference between the time-step, class-level and control conditions if their corresponding embeddings are additively combined in the adaptive layer norm conditioning, e.g. causing changes in high-level content of the generated image when modifying its resolution, see Fig. 2. To disentangle the different conditions, we propose two modifications. First, we move the class embedding to be fed through the attention layers present in the DiT blocks. Second, to ensure that the control embedding"}, {"title": "", "content": "does not overpower the timestep embedding when additively combined in the adaptive layer norm, we zero out the control embedding in early denoising steps, and gradually increase its strength.\nControl conditions can be used for different types of data augmentations: (i) high-level augmentations (h) that affect the image composition - e.g. flipping, cropping and aspect ratio \u2013, and (ii) low-level augmentations (\u03c6\u03b9) that affect low-level details \u2013 e.g. image resolution and color. Intuitively, high-level augmentations should impact the image formation process early on, while low-level augmentations should enter the process only once sufficient image details are present. We achieve this by scaling the contribution of the low-level augmentations, \u03c6\u03b9, to the control embedding using a cosine schedule that downweights earlier contributions:\nCemb(h, \u03c6\u03b9,t) = En(h) + yc(t)\u00b7 \u0395\u03b9(\u03c6\u03b9), (1)\nwhere the embedding functions Eh, El are made of sinusoidal embeddings followed by a 2-layer MLP with SiLU activation, and where Ye is the cosine schedule illustrated in Fig. 3.\nImproved text conditioning. Most commonly used text encoders, like CLIP [40], output a constant number of tokens T that are fed to the denoising model (usually T = 77). Consequently, when the prompt has less than T tokens, the remaining positions are filled by zero-padding, but remain accessible via cross-attention to the denoising network. To make better use of the conditioning vector, we propose a noisy replicate padding mechanism where the padding tokens are replaced with copies of the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens in its inputs. As this might lead to redundant token embeddings, we improve the diversity of the feature representation across the sequence dimension, by perturbing the embeddings with additive Gaussian noise with a small variance Btxt. To ensure enough diversity in the token embeddings, we scale the additive noise by o(txt)\u221am \u2013 1, where m is the number of token replications needed for padding, and (txt) is the per-channel standard deviation in the token embeddings.\nIntegrating classifier-free guidance. Classifier-free guidance (CFG) [20] allows for training conditional models by combining the output of the uncoditional generation with the output of the conditional generation. Formally, given a latent diffusion model trained to predict the noise \u20ac, CFG reads as: \u03b5\u03bb = \u03bb \u00b7 \u20acs + (1 \u2212 1) \u00b7 \u20ac\u00f8, where ep is the uncoditional noise prediction, es is the noise prediction conditioned on the semantic conditioning s (e.g., text prompt), and \u5165 is the hyper-parameter, known as guidance scale, which regulates the strength of the conditioning. Importantly, during training A is set alternatively to 0 or 1, while at inference time it is arbitrarily changed in order to steer the generation to be more or less consistent with the conditioning. In our case, we propose the control conditioning to be an auxiliary guidance term, in order to separately regulate the strength of the conditioning on the control variables c and semantic conditioning s at inference time. In particular, we define the guided noise estimate as:\n\u03b5\u03bb,\u03b2 = \u03bb [\u03b2\u00b7 \u20acc,s + (1 \u2212 \u03b2) \u00b7 \u20ac\u00f8,s] + (1 \u2212 1) \u00b7 \u20ac\u00f8,\u00f8, (2)\nwhere \u1e9e sets the strength of the control guidance, and A sets the strength of the semantic guidance."}, {"title": "2.2 On transferring models pre-trained on different datasets and resolutions", "content": "Background. Transfer learning has been a pillar of the deep learning community, enabling generalization to different domains and the emergence of foundational models such as DINO [5, 10] and CLIP [40]. Large pre-trained text-to-image diffusion models have also been re-purposed for different tasks, including image compression [4] and spatially-guided image generation [1]. Here, we are interested in understanding to which extent pre-training on other datasets and resolutions can be leveraged to achieve a more efficient training of large text-to-image models. Indeed, training diffusion models directly to generate high resolution images is computationally demanding, therefore, it is common to either couple them with super-resolution models, see e.g. [44], or fine-tune them with high resolution data, see e.g. [7, 14, 39]. Although most models can directly operate at a higher resolution than the one used for training, fine-tuning is important to adjust the model to the different statistics of high-resolution images. In particular, we find that the different statistics influence the positional embedding of patches, the noise schedule, and the optimal guidance scale. Therefore, we focus on improving the transferability of these components."}, {"title": "", "content": "Positional Embedding. Adapting to a higher resolution can be done in different ways. Interpolation scales the \u2013 most often learnable \u2013 embeddings according to the new resolution [2, 47]. Extrapolation simply replicates the embeddings of the original resolution to higher resolutions as illustrated in Fig. 4, resulting in a mismatch between the positional embeddings and the image features when switching to different resolutions. Most methods that use interpolation of learnable positional embeddings, e.g. [2, 47], adopt either bicubic or bilinear interpolation to avoid the norm reduction associated with the interpolation. In our case, we take advantage of the fact that our embeddings are sinusoidal and simply adjust the sampling grid to have constants limit under every resolution, see App. C.\nScaling the noise schedule. At higher resolution, the amount of noise necessary to mask objects at the same rate changes [14, 22]. If we observe a spatial patch at low resolution under a given uncertainty, upscaling the image by a factor s creates s\u00b2 observations of this patch of the form , assuming the value of the patch is constant across the patch. This increase in the number of observations reduces the uncertainty around the value of that token, resulting in a higher signal-to-noise (SNR) ratio than expected. This issue gets further accentuated when the scheduler does not reach a terminal state with pure noise during training, i.e., a zero SNR [32], as the mismatch between the non-zero SNR seen during training and the purely Gaussian initial state of the sampling phase becomes significant. To resolve this, we scale the noise scheduler in order to recover the same uncertainty for the same timestep.\nProposition 1. When going from a scale of s to a scale s', we update the \u00df scheduler according to the following rule\n\u03b1t' = (3)\nThis increases the noise amplitude during intermediate denoising steps as illustrated in Fig. A1. The final equation obtained is similar to the one obtained in [14] with the accompanying change of variable t =.\nPre-training cropping strategies. When pre-training and finetuning at different resolutions, we can either first crop and then resize the crops according to the training resolution, or directly take differently sized crops from the training images. Using a different resizing during pre-training and finetuning may introduce some distribution shift, while using crops of different sizes may be detrimental to low-resolution training as the model will learn the distribution of smaller crops rather than full images, see Fig. 5. We experimentally investigate which strategy is more effective for low-resolution pre-training of high-resolution models.\nGuidance scale. We discover that the optimal guidance scale for both FID and CLIPScore varies with the resolution of images. In App. D, we present a proof revealing that under certain conditions, the optimal guidance scale adheres to a scaling law with respect to the resolution, as\n\u03bb'(s) = 1 + s. (x \u2013 1). (4)"}, {"title": "3 Experimental evaluation", "content": ""}, {"title": "3.1 Experimental setup", "content": "Datasets. In our study, we train models on three datasets. To train class-conditional models, we use ImageNet-1k [11], which has 1.3M images spanning 1,000 classes, as well as ImageNet-22k [43], which contains 14.2M images spanning 21,841 classes. Additionally, we train text-to-image models using Conceptual 12M (CC12M) [6], which contains 12M images with accompanying manually generated textual descriptions. We pre-process both datasets by blurring all faces. Differently from [7], we use the original captions for the CC12M dataset.\nEvaluation. For image quality, we evaluate our models using the common FID [19] metric. We follow the standard evaluation protocol on ImageNet to have a fair comparison with the relevant"}, {"title": "3.2 Evaluation of model architectures and comparison with the state of the art", "content": "In Tab. 1, we report results for models with different architectures trained at both 256 and 512 resolutions for ImageNet and CC12M, and compare our results (2nd block of rows) with those reported in the literature, where available (1st block of rows). Where direct comparison is possible, we notice that our re-implementation outperforms the one of existing references. Overall, we found the mmDiT [14] architecture to perform best or second best in all settings compared to other alternatives. For this reason, we apply our conditioning improvements on top of this architecture (last row of the table), boosting the results as measured with FID and CLIPScore in all settings. Below, we analyse the improvements due to our conditioning mechanisms and pre-training strategies."}, {"title": "3.3 Control conditioning", "content": "Scheduling rate of control conditioning. In Tab. 2a we consider the effect of controlling the conditioning on low-level augmentations via a cosine schedule for different decay rates a. We compare to baselines (first two rows) with constant weighting (as in SDXL [39]) and without control conditioning. We find that our cosine weighting schedule significantly reduces the dependence between size control and image semantics as it drastically improves the instance specific LPIPS (0.33 vs. 0.04) in comparison to uniform weighting. In terms of FID, we observe a small gap with the baseline (3.04 vs. 3.08), which increases (3.80 vs. 5.04) when computing FID by randomly sampling the size conditioning in the range [512,1024], see Tab. 2b. Finally, the improved disentangling between semantics and low-level conditioning is clearly visible in the qualitative samples in Fig. 2.\nCrop and random-flip control conditioning. A potential issue of horizontal flip data augmentations is that it can create misalignment between the text prompt and corresponding image. For example the prompt \"A teddy bear holding a baseball bat in their right arm\" will no longer be accurate when an image is flipped \u2013 showing a teddy bear holding the bat in their left arm. Similarly, cropping images can remove details mentioned in the corresponding caption. In Tab. 2c we evaluate models trained on CC12M@256 with and without horizontal flip conditioning, and find that adding this conditioning leads to slight improvements in both FID and CLIP as compared to using only crop conditioing. We depict qualitative comparison in Fig. 6, where we observe that flip conditioning improves prompt-layout consistency.\nInference-time control conditioning of image size. High-level augmentations (h) may affect the image semantics. As a result they influence the learned distribution and modify the generation diversity. For example, aspect ratio conditioning can harm the quality of generated images, when images of a particular class or text prompt are unlikely to appear with a given aspect ratio. In Tab. 2b we compare of different image size conditionings for inference. We find that conditioning on the same size distribution as encountered during the training of the model yields a significant boost in FID"}, {"title": "3.4 Transferring weights between datasets and resolutions", "content": "Dataset shift. We evaluate the effect of pre-training on ImageNet-1k (at 256 resolution) when training the models on CC12M or ImageNet-22k (at 512 resolution) by the time needed to achieve the same performance as a model trained from scratch. In Tab. 4a, when comparing models trained from scratch to ImageNet-1k pre-trained models (600k iterations) we observe two benefits: improved training convergence and performance boosts. For CC12M, we find that after only 100k iterations, both FID and CLIP scores improve over the baseline model trained with more than six times the"}, {"title": "4 Conclusion", "content": "In this paper, we explored various approaches to enhance the conditional training of diffusion models. Our empirical findings revealed significant improvements in the quality and control over generated images when incorporating different coditioning mechanisms. Moreover, we conducted a"}, {"title": "", "content": "comprehensive study on the transferability of these models across diverse datasets and resolutions. Our results demonstrated that leveraging pretrained representations is a powerful tool to improve the model performance while also cutting down the training costs. Furthermore, we provided valuable insights into efficiently scaling up the training process for these models without compromising performance. By adapting the schedulers and positional embeddings when scaling up the resolution, we achieved substantial reductions in training time while boosting the quality of the generated images. Additional experiments unveil the expected gains from different transfer strategies, making it easier for researchers to explore new ideas and applications in this domain. In Appendix B we discuss societal impact and limitations of our work."}, {"title": "Appendix", "content": ""}, {"title": "A Related work", "content": "Diffusion Models. Diffusion models have gained significant attention in recent years due to their ability to model complex stochastic processes and generate high-quality samples. These models have been successfully applied to a wide range of applications, including image generation [7, 21, 41], video generation [35], music generation [31], and text generation [50]. One of the earliest diffusion models was proposed in [21], which introduced denoising diffusion probabilistic models (DDPMs) for image generation. This work demonstrated that DDPMs can generate high-quality images that competitive with state-of-the-art generative models such as GANs [16]. Following this work, several variants of DDPMs were proposed, including score-based diffusion models [46], conditional diffusion models [12], and implicit diffusion models [45]. Overall, diffusion models have shown promising results in various applications due to their ability to model complex stochastic processes and generate high-quality samples [7, 14, 39, 41]. Despite their effectiveness, diffusion models also have some limitations, including the need for a large amount of training data and the required computational resources. Some works [26, 27] have studied and analysed the training dynamics of diffusion models, but most of this work considers the pixel-based models and small-scale settings with limited image resolution and dataset size. In our work we focus on the more scalable class of latent diffusion models [41], and consider image resolutions up to 512 pixels, and 14M training images.\nModel architectures. Early work on diffusion models adopted the widely popular UNet arcchitecture [39, 41]. The UNet is an encoder-decoder architecture where the encoder is made of residual blocks that produce progressively smaller feature maps, and the decoder progressively upsamples the feature maps and refines them using skip connections with the encoder [42]. For diffusion, UNets are also equipped with cross attention blocks for cross-modality conditioning and adaptive layer normalization that conditions the outputs of the model on the timestep [41]. More recently, vision transformer architectures [13] were shown to scale more favourably than UNets for diffusion models with the DiT architecture [38]. Numerous improvements have been proposed to the DiT in order to have more efficient and stable training, see e.g. [7, 14, 15]. In order to reduce the computational complexity of the model and train at larger scales, windowed attention has been proposed [7]. Latent masking during training has been proposed to encourage better semantic understanding of inputs in [15]. Others improved the conditioning mechanism by evolving the text tokens through the layers of the transformer and replacing the usual cross-attention used for text conditioning with a variant that concatenates the tokens from both the image and text modalities [14].\nLarge scale diffusion training. Latent diffusion models [41] unlocked training diffusion models at higher resolutions and from more data by learning the diffusion model in the reduced latent space of a (pre-trained) image autoencoder rather than directly in the image pixel space. Follow-up work has proposed improved scaling of the architecture and data [39]. More recently, attention-based architectures [7, 14, 15] have been adapted for large scale training, showing even more improvements by scaling the model size further and achieving state-of-the-art performance on datasets such as"}, {"title": "B Societal impact and limitations", "content": "Our research investigates the training of generative image models, which are widely employed to generate content for creative or education and accessibility purposes. However, together with these beneficial applications, these models are usually associated with privacy concerns (e.g., deepfake generation) and misinformation spread. In our paper, we deepen the understanding of the training dynamics of these modes, providing the community with additional knowledge that can be leveraged for safety mitigation. Moreover, we promote a safe and transparent/reproducible research by employing only publicly available data, which we further mitigate by blurring human faces.\nOur work is mostly focused on training dynamics, and to facilitate reproducibility, we used publicly available datasets and benchmarks, without applying any data filtering. We chose datasets relying on the filtering/mitigation done by their respective original authors. In general, before releasing models like the ones described in our paper to the public, we recommend conducting proper evaluation of models trained using our method for bias, fairness and discrimination risks. For example, geographical disparities due to stereotypical generations could be revealed with methods described by Hall et al. [18], and social biases regarding gender and ethnicity could be captured with methods from Luccioni et al. [37] and Cho et al. [8].\nWhile our study provides valuable insights into control conditioning and the effectiveness of representation transfer in diffusion models, there are several limitations that should be acknowledged. (i) There are cases where these improvements can be less pronounced. For example, noisy replicates for the text embeddings can become less pertinent if the model is trained exclusively with long prompts. (ii) While low resolution pretraining with local crops on ImageNet resulted in better FID at 512 resolution (see Table 5c), it might not be necessary if pretraining on much larger datasets (e.g. >100M samples, which we did not experiment in this work). Similarly, flip conditioning is only pertinent if the training dataset contains position sensitive information (left vs. right in captions, or rendered text in images), otherwise this condition will not provide any useful signal. (iii) We did not investigate the impact of data quality on training dynamics, which could have implications for the generalizability of our findings to datasets with varying levels of quality and diversity. (iv) As our analysis primarily focused on control conditioning, other forms of conditioning such as timestep and prompt conditioning were not explored in as much depth. Further research is needed to determine the extent to which these conditionings interact with control conditioning and how that impacts the quality of the models. (v) Our work did not include studies on other parts that are involved in the training and sampling of diffusion models, such as the different sampling mechanisms and training paradigms. This could potentially yield additional gains in performance and uncover new insights about the state-space of diffusion models."}, {"title": "C Implementation details", "content": "Noise schedule scaling. In Fig. Al we depict the rescaling of the noise for higher resolutions, following Eq. (3).\nPositional embedding rescaling. As illustrated in App. C, rescaling the positional embedding can be integrated in two simple lines of code by changing the grid sampling to be based on reducing the stepsize in the grid instead of extending its limits."}, {"title": "D Derivations", "content": "Derivation for Eq. (2)\nProof. The formula can be obtained by iteratively applying the guidance across the conditions.\n\u03b5\u03bb\u03b9\u03b2 = \u03bb\u03b5\u03c2 + (1 \u2212 1)\u03b5\u03c6 (8)\n\u03b5\u03bb\u03b9\u03b2 = \u03bb(\u03b2\u20ac\u03b5,\u03c2 + (1 \u2212 \u03b2)\u20acc,\u00f8) + (1 \u2212 1)\u20ac0,0 (9)\nDerivation for Eq. (4)\nProof. Assuming that the unconditional prediction eg is distributed around the conditional distribution according to a normal law eq|ec ~ N(\u20acc, \u03b4\u00b2I).\n\u03b5\u03bb = c + (1 \u2212 1)(c + \u03b4\u03b5), \u03b5 ~ N(0, I) (10)\n\u20acx = c + (1 \u2212 1)\u03b4\u03b5 (11)\nVar(x) = (1 \u2013 1)282 (12)\nAfter upsampling by a scale factor of s, the same low resolution patch has s\u00b2 observations, hence the variance is decreased by s\u00b2.\nVar(ex)hr = (1 \u2212 1)\u00b2\u00b2/s\u00b2 (13)\nHence by equalizing the discrepancy between the conditional and unconditional predictions at low and high resolutions we obtain:\nVar(6x)s=1 = Var(\u20ac)\u300f \u21d2 (1 \u2212 x')28\u00b2/s\u00b2 = (1 \u2212 1)282 (14)\nx' = 1 + s. (x \u2013 1) (15)\nDerivation for Eq. (3)\nProof. At timestep t, the noisy observation is obtained as :\nXt =  (xo + \u03c3\u03c4\u03b5), \u03b5 ~ N(0, I) (16)"}, {"title": "E Additional results", "content": "Qualitative results. We provide additional qualitative examples on ImageNet-1k in Fig. A5.\nSummary of findings. In Table A3 we summarize the improvements w.r.t. the DiT baseline obtained by the changes to the model architecture and training. In Table A4 we compare our model architecture and training recipe to that of SDXL and SD3. In Table A5, we provide a synopsis of the research questions addressed in our study alongside a respective recommendation based on our findings."}]}