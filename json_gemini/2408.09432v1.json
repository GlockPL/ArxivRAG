{"title": "Deformation-aware GAN for Medical Image Synthesis with Substantially Misaligned Pairs", "authors": ["Bowen Xin", "Tony Young", "Claire E Wainwright", "Tamara Blake", "Leo Lebrat", "Thomas Gaass", "Thomas Benkert", "Alto Stemmer", "David Coman", "Jason Dowling"], "abstract": "Medical image synthesis generates additional imaging modalities that are costly, invasive\nor harmful to acquire, which helps to facilitate the clinical workflow. When training\npairs are substantially misaligned (e.g., lung MRI-CT pairs with respiratory motion),\naccurate image synthesis remains a critical challenge. Recent works explored the directional\nregistration module to adjust misalignment in generative adversarial networks (GANs);\nhowever, substantial misalignment will lead to 1) suboptimal data mapping caused by\ncorrespondence ambiguity, and 2) degraded image fidelity caused by morphology influence\non discriminators. To address the challenges, we propose a novel Deformation-aware GAN\n(DA-GAN) to dynamically correct the misalignment during the image synthesis based on\nmulti-objective inverse consistency. Specifically, in the generative process, three levels of\ninverse consistency cohesively optimise symmetric registration and image generation for\nimproved correspondence. In the adversarial process, to further improve image fidelity under\nmisalignment, we design deformation-aware discriminators to disentangle the mismatched\nspatial morphology from the judgement of image fidelity. Experimental results show that\nDA-GAN achieved superior performance on a public dataset with simulated misalignments\nand a real-world lung MRI-CT dataset with respiratory motion misalignment. The results\nindicate the potential for a wide range of medical image synthesis tasks such as radiotherapy\nplanning.", "sections": [{"title": "1. Introduction", "content": "Medical image synthesis produces additional imaging modalities to provide essential informa-\ntion for diagnosis or treatment planning, while bypassing the cost and extra time associated\nwith additional scans. It is particularly useful when the additional scan is invasive, harmful,\ncostly or time-consuming (Liu et al., 2022). Typical applications include synthetic CT for\nMRI-only radiotherapy dose planning (Spadea et al., 2021) or children's airway assessment\n(Longuefosse et al., 2023)). Generative adversarial networks (GANs) are widely used in\nmedical synthesis, which usually requires either well-aligned imaging pairs (by supervised\nmethods) or randomly unpaired data (by unsupervised methods). Specifically, supervised\nGANs, such as Pix2pix and its improved variants (Wang et al., 2018a,b; AlBahar and Huang,\n2019), leverage pixel-wise loss on well-aligned imaging pairs to learn the unique and optimal\nmapping. However, well-aligned pairs are not widely available due to patient motion or organ\nmovement, causing accumulated error and unreasonable placement in supervised methods\n(Pang et al., 2021). Though registration is commonly used as preprocessing to align images,\nit is still difficult to acquire perfectly aligned pairs, especially under substantial misalignment\nsuch as respiratory motion in lung MR-to-CT synthesis (Sotiras et al., 2013).\nUnsupervised GANs are not ideal for misaligned pairs either. Specifically, unsupervised\nGANs enable training on randomly unpaired data by leveraging extra constraints such\nas cycle consistency (Zhu et al., 2017; Hoffman et al., 2018; Khorram and Fuxin, 2022),\nmutual information (Park et al., 2020; Zhan et al., 2022), or geometry consistency (Fu et al.,\n2019; Xu et al., 2022). However, they are not designed to utilise pairing information to\nuncover optimal mappings (minimised pixel-wise error), which is essential in tasks such as\nradiotherapy planning. According to (Shen et al., 2020), cycle consistency mapping used in\nunsupervised GANs is not strictly one-to-one mapping, which is an important condition in\nintra-subject medical image synthesis (Wang et al., 2021). Diffusion models have shown great\npotential in computer vision applications due to their strength in capturing distributions (Ho\net al., 2020; Song et al., 2020); however, they are computationally expensive and data-hungry\nto train, hindering their application in the medical domain.\nOne recent work RegGAN (Kong et al., 2021) explored directional registration in image\nsynthesis on datasets with simulated misalignment; however, the real-life setting often involves\nlarge deformation between pairs (e.g., Figure 1), causing difficulty in learning unique one-to-\none mapping due to a large number of local minima (Christensen and Johnson, 2001). The\nresulting correspondence ambiguity and asymmetric mapping error would add to pixel-wise\nerror in supervised methods, causing a major challenge in generative modelling. The second\nchallenge is the degraded image fidelity caused by the influence of spatial misalignment during\nthe adversarial process. To further elaborate on the issue, the discriminator in RegGAN\nmay recognise the real/fake images purely based on spatial morphology rather than intensity\ncharacteristics, thus leading to suboptimal image fidelity.\nIn this paper, we propose a Deformation-aware GAN (DA-GAN) to jointly address\nthe above two synthesis challenges when image pairs are substantially misaligned. Firstly,\ninspired by the capacity of symmetric registration to jointly estimate invertible bidirectional\ntransformation, we propose a multi-objective inverse consistency to comprehensively in-\nvestigate how to cohesively incorporate symmetric registration into an image generation\nnetwork. To further improve degraded image fidelity in an adversarial process, we design a"}, {"title": "2. Methodology", "content": "Suppose we have a training dataset with misaligned imaging pairs\n$(x_i, y_i)_{i=1}^n$, where $x_i \\in X$ and $y_i \\in Y$ belong to different modalities. $X$ and $Y$ differ in\nboth intensity characteristics and spatial morphology. Additionally, we denote $\\hat{y}_i \\in \\hat{Y}$ as a\ntransformed $y_i$ that spatially corresponds to source imaging $x_i$, but $\\hat{y}$ is unknown in the\nreal world. In other words, both $x_i$ and $\\hat{y}_i$ are aligned in source spatial space, but only\ndiffer in intensity characteristics. With misaligned multimodal imaging pairs $(x_i, y_i)_{i=1}^n$, our\nobjective is to accurately synthesise the target imaging $\\hat{y}$, that is spatially corresponding to\nthe source image $x_i$ for subsequent tasks such as radiotherapy treatment planning."}, {"title": "2.1. DA-GAN overview", "content": "Figure 2a presents the network architecture of our\nproposed DA-GAN which consists of three major components, including (1) modality gen-\nerators G and F, (2) symmetric spatial aligners $A_y$ and $A_x$, and (3) deformation-aware\ndiscriminators $D_y$ and $D_x$. Firstly, modality generators are designed to translate the source\nimage to the target appearance with spatial correspondence preserved, which is implemented\nwith trainable networks G: $x \\rightarrow \\hat{y} \\in \\hat{Y}$ and F: $y \\rightarrow \\hat{x} \\in \\hat{X}$. Secondly, symmetric spatial\naligners $A_y$ and $A_x$ are designed to exploit symmetric correspondence during image-to-image\ntranslation to optimise unique and optimal mapping. Each aligner (e.g., $A_y = \\{R_y^\\rightarrow, R_y^{\\leftarrow}, T_y\\}$)\nis enforced to learn bidirectional transformations $\\hat{\\phi}_y$: $\\hat{y} \\rightarrow y$ and $\\hat{\\phi}_y'$: $y \\rightarrow \\hat{y}$ that are\ninverse to each other. The bidirectional transformations are learnt through symmetric\ntransformation repressors R = $\\{R^\\rightarrow, R^{\\leftarrow}\\}$. Each transformation regressor is a CNN model\ntrained to predict a deformation field (Kong et al., 2021), and then followed by a spatial\ntransformer network (Jaderberg et al., 2015) to resample images to target spatial space.\nThirdly, deformation-aware discriminators are denoted as $D_y : y \\rightarrow \\{0,1\\}$ where $y \\in \\cup (Y, \\hat{Y})$\nand $D_x : x \\rightarrow \\{0,1\\}$ where $x \\in \\cup (X, \\hat{Y})$.\nTo synthesize with misaligned pairs, DA-GAN is constrained by three\nloss functions, including (1) symmetric registration loss $L_{sr}$ for self-aligning, (2)"}, {"title": "2.2. DA-GAN loss functions", "content": "is designed to (1) punish dissimilarity between mis-\naligned imaging pairs and (2) encourage local smoothness on the deformation field. The\nformer similarity loss $L_{sim}$ for symmetric registration is defined as:\n$\\displaystyle \\min_{G,F,R_y, R_x} L_{sim}(G, F, R_y, R_x) = E_{x,y}[||y - G(x) \\circ \\phi_y' ||_1 + ||G(x) - y \\circ \\phi_y||_1$\n$\\displaystyle + ||x - F(y) \\circ \\phi_x' ||_1 + ||F(y) - x \\circ \\phi_x ||_1]$     (2)\nSecondly, the smoothness loss $L_{smt}$ (Balakrishnan et al., 2019) is implemented to minimize\nthe gradient divergence of the estimated deformation field:\n$\\displaystyle \\min_{R_y, R_x} L_{smt}(R_y, R_x)) = E_{x,y}[||\\nabla \\phi_y'||^2 + ||\\nabla \\phi_y||^2 + ||\\nabla \\phi_x'||^2 + ||\\nabla \\phi_x||^2]$\n(3)"}, {"title": "", "content": "Lastly, by integrating Equation 2 and 3 with loss weights $\\lambda_{reg}$ and $\\lambda_{smt}$, we can formulate\nour symmetric registration loss $L_{sr}$ as below:\n$L_{sr} = \\lambda_{reg}L_{reg} + \\lambda_{smt}L_{smt}$ (4)\nMulti-objective inverse-consistency loss $L_{mic}$ is proposed to (1) improve image align-\nment during symmetric registration, and (2) improve synthesis correspondence to the source\nimage during image generation. As illustrated in Figure 2b, this is achieved by enforcing\ninverse consistency from three different levels, including registration level, generation level,\nand joint level.\nFirstly, at the registration level, we enforce inverse consistency on the forward and\nbackward transformations $\\phi^\\leftrightarrow$ and $\\hat{\\phi}^\\leftarrow$ during symmetric registration. Specifically, the\nregistration IC loss $L_{ic\\_reg}$ is formulated as\n$\\displaystyle \\min_{R_y, R_x} L_{ic\\_reg}(R_y, R_x) = E_{x,y}[||y \\circ \\phi_y \\circ \\phi_y' - y||_1 + ||x \\circ \\phi_x \\circ \\phi_x' - x||_1]$\n(5)\nSecondly, at the generation level, we constrain inverse consistency on the two modality\ngenerators G and F. Thus, the generation IC loss $L_{ic\\_gen}$ is formulated as below:\n$\\displaystyle \\min_{G,F} L_{ic\\_gen}(G, F) = E_{x} [||F(G(x)) - x||_1 + E_{y}[||G(F(y)) - y||_1$   (6)\nLastly, we propose a third joint level inverse consistency through both image registration and\ngeneration cycle, thus jointly optimising image registration and generation. The formulation\nof joint inverse-consistency $L_{ic\\_joint}$ is shown as below:\n$\\displaystyle \\min_{G,F,R_y, R_x} L_{ic\\_joint}(G, F, R_y, R_x) = E_{x,y}[||F(G(x) \\circ \\phi_y) \\circ \\phi_x' - x||_1$\n$\\displaystyle + ||G(F(y) \\circ \\phi_x) \\circ \\phi_y' - y||_1]$ (7)\nTo summarise, the overall multi-objective inverse-consistency loss is composed of three levels\nof inverse consistency (with their corresponding weights denoted as $\\lambda$):\n$L_{mic} = \\lambda_{ic\\_reg}L_{ic\\_reg} + \\lambda_{ic\\_gen}L_{ic\\_gen} + \\lambda_{ic\\_joint} L_{ic\\_joint}$ (8)\nDeformation-aware adversarial loss $L_{adv\\_da}$ is designed to disentangle the influence of\nspatial morphology across domains from intensity characteristic learning. We illustrate the\ncomparison of conventional adversarial loss $L_{adv}$ and our $L_{adv\\_da}$ for an example discriminator\n$D_y$ in Figure 2c. Via symmetric spatial aligner $A_y$, we can obtain source-shaped images and\ntarget-shaped images for both generated (G(x) and $G(x)$) and real images ($y \\circ \\phi_y'$ and\ny). Then, we feed all these images to the discriminator in the adversarial process to guide it\nto focus on learning intensity characteristics only. Formally, deformation-aware adversarial\nloss for $D_y$ is formulated as:\n$\\displaystyle \\min_{G \\atop D_y} max L_{adv\\_da}(G, D_y) = E_y[log(D_y(y)) + log(D_y(y \\circ \\phi_y'))]$\n(9)\n$\\displaystyle + E_x[log(1 - D_y(G(x))) + log(1 - D_y(G(x) \\circ \\phi_y'))]$\nSimilarly, we can derive the other half of adversarial loss for $D_x$ as $L_{adv\\_da}$. The total\nadversarial loss $L_{adv\\_da} = \\lambda_{adv\\_da}Lady\\_da + Lady Lv L_{adv\\_da} Lady da where $\\lambda_{adv\\_da}$ denotes loss weight."}, {"title": "3. Experiments", "content": "For simulation experiments, the public brain T1-T2 MRI dataset\n(BraTS 2018 (Menze et al., 2014)) was selected because there were well-aligned imaging pairs\nas ground truth. The training and testing sets contained 5760 and 768 pairs of T1 and T2\nimages, respectively. The data were normalised to [-1, 1], resized to 256*256, and publicly\navailable 1 (Kong et al., 2021). As original brain images were paired and well-aligned, we\nsimulated 6 different levels of non-affine misalignments on the dataset.\nThe private lung MRI-CT dataset was used to validate\nthe proposed algorithm for a real-world radiotherapy treatment planning setting. The dataset\ncontained 4096 pairs of ultrashort-echo time MRI (from Siemens scanners) and CT imaging\nfor training and 1024 pairs for independent testing. Both imaging modalities were normalised\nto [-1, 1], cropped to lung regions, resampled to isotropic, and preliminarily registered.\nHowever, we still observed alignment errors in lung regions (Dice 0.949) as well as bones and\nairways. Please refer to Appendix A for additional details on both datasets.\nIn simulation experiments on the brain dataset, 6 non-affine deforma-\ntions were randomly applied on the training sets to simulate the misalignment. The non-affine\ndeformation was implemented with elastic deformation on control points (Rand2DElastic in\nMONAI library (Cardoso et al., 2022)) with 6 incremental levels denoted as NA-1 to NA-6.\nIn real-world experiments on the lung dataset, DA-GAN was compared on 8 state-of-the-art\n(SOTA) medical synthesis methods, including GAN (Goodfellow et al., 2020), Pix2pix (Isola\net al., 2017), CycleGAN (Zhu et al., 2017), UNIT (Liu et al., 2017), MUNIT (Huang et al.,\n2018), NiceGAN (Chen et al., 2020), RegGAN-NC (Kong et al., 2021), and RegGAN-C\n(Kong et al., 2021). Please find more details in Appendix B.1.\nAll experiments were implemented in Pytorch on a 64-bit Ubuntu Linux system with\none 16 GB Nvidia P100 GPU. We trained all the methods using the Adam optimiser with\nthe learning rate 1e-4 and ($\\beta_1, \\beta_2$) = (0.5,0.999). The batch size was set to 1 with weight\ndecay 1e04. The training included 50 epochs for both datasets. The brain imaging was\nevaluated with three metrics in 2D, including Normalized Mean Absolute Error (NMAE),\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). The lung imaging was\nevaluated with MAE3D, PSNR3D, and SSIM3D. The background of the image was excluded\nfrom the computation. For reproducibility, we included more implementation details on\nDA-GAN modules and loss weighting in Appendix B.2."}, {"title": "4. Results", "content": "This section summarises the results of DA-\nGAN on brain imaging with 6 non-affine misalignments (Table 1) and visualisation on the\ntesting set (Figure 3). Table 1 shows that DA-GAN consistently outperformed all comparison\nmethods in three metrics from misalignment levels NA-1 to NA-6. The results of DA-GAN\nremained stable with increased levels of non-affine misalignment with NMAE ranging from\n0.070 to 0.075. Figure 3 visualises that our DA-GAN achieved less error compared with other\nmethods on the error map.\nTable 2 shows the quantitative results of DA-\nGAN on the lung MRI-CT with challenging respiratory-motion misalignment. The results in\nTable 2 show that our DA-GAN achieved the best results of MAE3D 35.86, PSNR3D 32.49\nand SSIM3D 0.731 compared with 8 SOTA methods. Figure 4 further visually highlights the\nsuperiority of our DA-GAN, especially in the challenging spine, bones and heart regions (blue\narrows). These demonstrate the great potential for synthetic CT for MRI-only radiotherapy.\nThe ablation study in Table 3 shows that $L_{adv\\_da}$\nimproved the baseline RegGAN in all metrics (e.g., PSNR-3D 0.41), while $L_{mic}$ further\nimproved the results in all metrics (e.g., PSNR-3D 0.36). The paired t-test shows that\nthe contributions from both losses were statistically significant (p-value<0.001) in MAE2D,\nPSNR2D, and SSIM2D. Further details are in Appendix C. Due to space limit, please find\nthe analysis of convergence, complexity and hyperparameter sensitivity in Appendix D-F."}, {"title": "5. Conclusions", "content": "In this study, we introduce a new DA-GAN for medical image synthesis with substantially\nmisaligned imaging pairs. We propose two novel loss functions $L_{mic}$ and $L_{adv\\_da}$ to generate\nhigh-fidelity images while adaptively learning correspondence via symmetric registration. We\nvalidated our method on a public brain dataset with both 6 simulated misalignments and\na real-world lung dataset compared with 8 SOTA methods. The results demonstrated the\npotential towards an important step in generalisable medical image synthesis with limited\ndata for clinical applications such as early diagnosis and radiotherapy planning."}, {"title": "Appendix A. Dataset details", "content": "On the brain T1-T2 MRI dataset, we simulated 6 different levels of\nnon-affine misalignments. A visualisation of non-affine misalignment is provided in Figure 5.\nAn overview of implementation parameters for non-affine misalignment is provided in Table 4.\nMore specifically, the non-affine misalignment was simulated using elastic deformation on\ncontrol points (Rand2DElastic in MONAI library\u00b2). The spacing between control points was\nset to [40, 40], while the magnitude was set to incremental levels from NA-1 to NA-6.\nThe lung MRI-CT dataset contained paired but misaligned\nMRI and CT from 20 patients with lung diseases. The lung MRI was implemented with\na prototype 3D free-breathing stack-of-spirals UTE VIBE sequence, provided by Siemens\nHealthineers (Mugler III et al., 2015; Kumar et al., 2017). UTE-MRI provided millimetre\nresolution and radiation-free assessment for pulmonary structural imaging (Dournes et al.,\n2016). 20 CTs were scanned on approximately the same day as MRI. 20 patients included\n12 patients with cystic fibrosis and 8 patients with lung cancer. For the MRI protocol,\nboth cystic fibrosis and lung cancer cohorts used Siemens 3T scanners with flip angle 5,\necho time 0.05, and repetition time 2.97-3.78. For details of image size and spacing, please\nrefer to Table 5. Both MRI and CT were normalised to [-1, 1], cropped to lung regions,\nresampled to isotropic spacing, and preliminarily registered. The ground-truth paired MRI-\nCT images were acquired via an automatic registration pipeline including elastic registration\n(SimpleElastix library), structure-guided registration (demon registration in ants library) and\nantsRegistrationSyNQuick (ants library). After registration, we still observed registration\nerrors on the whole lung regions with Dice 0.949 (Dice for right lung 0.947, Dice for right lung"}, {"title": "Appendix B. Implementation details", "content": "In this section, we first discuss the implementation of other comparison methods. Then\nwe introduce network implementation details for different modules and loss weighting in\nDA-GAN."}, {"title": "B.1. Implementation of comparison methods", "content": "uses a generator to translate the source image to the target space, while using a\ndiscriminator to determine whether images were generated from the real data. In this way,\nGANs iteratively improve the image fidelity via min-max game. In the implementation,\nthe generator and discriminator shared the same network architecture as ours for a fair\ncomparison. The open-source code is on https://github.com/Kid-Liet/Reg-GAN.\nis a typical supervised GAN using L1 loss functions to enforce pixel-wise similarity\nbetween the predicted images and ground-truth images. In the implementation, the generator\nand discriminator shared the same network architecture as ours for a fair comparison. The\nopen-source code is on https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.\nproposes a cycle consistency loss to constrain two generators that are reverse\nto each other. A cycleGAN consists of two sets of generators and discriminators. In the\nimplementation, the generator and discriminator shared the same network architecture as\nours for a fair comparison. The open-source code is on https://github.com/junyanz/\npytorchCycleGAN-and-pix2pix.\nleverages the assumption that the latent coding space is shared by different\nmodalities for image-to-image translation. The open-source code is on https://github.com/\nmingyuliutw/UNIT.\ndisentangles the representation to a content representation and a style repre-\nsentation for image-to-image translation. The open-source code is on https://github.com/\nNVlabs/MUNIT.\nis a compact and effective network architecture for image-to-image transla-\ntion, which is achieved by reusing the discriminator for encoding. The open-source code is\non https://github.com/alpc91/NICE-GAN-pytorch."}, {"title": "B.2. Implementation of DA-GAN", "content": "was implemented in PyTorch with Python 3.7. The network architecture of\nDA-GAN consists of two modality generators, two symmetric spatial aligners, and two\ndeformation-aware discriminators.\nEach modality generator uses 2 downsampling layers, 9 residual\nblocks, and 2 upsampling layers, following the implementation of Johnson et al. (Johnson\net al., 2016). Specifically, we use Ck to denote 7*7 convolution-InstanceNorm-ReLU layer\nwith k filters and stride 1, Dk to denote the downsampling 3*3 convolution-instanceNorm-\nReLU layer with k filters and stride 2, Rk to denote a residual block containing two 3*3\nconvolutional layers with same number of filters, Uk to denote the upsampling 3*3 fractional-\nstrided-convolution-instanceNorm-ReLU layer with k filters and stride 0.5. The architecture\nof a modality generator is C64-D128-D256-R256-R256-R256-R256-R256-R256-R256-R256-\nR256-U128-U64-C1.\nEach symmetric spatial aligner consists of 4 transformation\nregressors and 4 spatial transformer networks. Each transformation regressor uses ResUnet\narchitecture, containing 7 encoder layers, 3 residual blocks, 7 decoder layers, and skip\nconnections, following the implementation of RegGAN (Kong et al., 2021). Specifically, we\nuse Dk to denote downsampling 3*3 convolution-LeakyReLU (LeakyReLU with a slope of\n0.2) with k filters and stride 1, Rk to denote a residual block containing two 3*3 convolutional\nlayers, Uk to denote upsampling 3*3 convolution-LeakyReLU (LeakyReLU with a slope of\n0.2) with k filters and stride 1. The backbone architecture of the transformation regressor is\nD32-D64-D64-D64-D64-D64-D64-R64-R64-R64-U64-U64-U64-U64-U64-U32, followed by a\nrefinement layer (a residual block and a 1*1 convolutional layer) and an output layer (3*3\nconvolution layer).\nFor deformation-aware discriminator, it was im-\nplemented with a 70*70 PatchGAN (Isola et al., 2017). Specifically, If we denote a 4*4\nconvolution-instanceNorm-LeakyReLU layer with k filters and stride 2 as Ck (LeakyReLU\nwith a slope of 0.2). The architecture of the discriminator is C64-C128-C256-C512. After\nthe last layer, another convolution is applied to generate 1-dimensional classification results.\nNo instanceNorm is used for the first C64 layer."}, {"title": "Appendix C. Ablation study with complete details", "content": "shows the results of an ablation study with full details on the multi-objective inverse\nconsistency loss $L_{mic}$ and deformation-aware adversarial loss $L_{adv\\_da}$. Firstly, for $L_{mic}$, we\nexperimented on the 7 settings on different combinations of $\\{Lic\\_reg, Lic\\_gen, Lic\\_joint\\}$. As\nshown in Table 3, by comparing the settings A-F and G2, we concluded that our proposed\n$L_{mic}$ that was composed of all three IC losses achieved the best results. Secondly, for $L_{adv\\_da}$,\nwe compared DA-GAN using conventional adversarial loss $L_{adv}$ (G1) and deformation-aware\nadversarial loss $L_{adv\\_da}$ (G2). The results show that $L_{adv\\_da}$ outperformed its counterpart.\nOverall, the ablation study demonstrated the effectiveness of both proposed components\n$L_{mic}$ and $L_{adv\\_da}$."}, {"title": "Appendix D. Convergence analysis during training", "content": "shows the validation NMAE during the training process for different levels of\nnon-affine misalignments in the brain dataset. The results demonstrate that DA-GAN\nsuccessfully converged under different levels of non-affine misalignments."}, {"title": "Appendix E. Comparison of complexity and accuracy", "content": "As shown in Table 8, our model achieves better synthesis accuracy (9.1% increase of MAE3D)\nat the cost of additional complexity (doubled no. parameters and size of the model) compared\nwith RegGAN-NC. However, we must emphasize that synthesis accuracy is clinically critical\nin radiotherapy planning because it reduces unnecessary radiation-related toxicity in patients\nand is vital to patients' safety (Burnet et al., 2004). Further, our model complexity is still\nsmaller than state-of-the-art GANs (e.g., MUNIT and NiceGAN), and our inference time is\nfast (0.013s fps)."}, {"title": "Appendix F. Sensitivity analysis on hyperparameters", "content": "We conducted a sensitivity analysis on hyperparameters of loss weights on the lung MRI-CT\ndataset. As shown in Table 9, the results (fluctuating slightly from 0.009 to 0.010) indicate\nthat the model performance is robust across different choices of hyperparameter values."}, {"title": "Appendix G. Inverse consistency in registration", "content": "Inverse consistency is often implemented by symmetrising cost functions (Christensen and\nJohnson, 2001) or computing the cost function in \"mid-space\" (Reuter et al., 2010). A"}, {"title": "", "content": "latest work proposed a multi-step IC registration to allow for coarse-to-fine registration\n((Greer et al., 2023)). As our work primarily focuses on image synthesis with better-handled\nmisalignment, we design a single-step multi-objective IC loss which jointly optimises the\nimage generation and registration. For efficiency reasons, our solution does not rely on\nthe scaling and squaring technique and its costly numerical integration. The results show\nthat the proposed IC loss significantly contributes to the improved generative performance\n(p-value < 0.001) on misaligned data. In future work, we will investigate the influence of\nbetter regularisation methods, such as Total Variation Regularisation (Vishnevskiy et al.,\n2016)."}]}