{"title": "Deformation-aware GAN for Medical Image Synthesis with Substantially Misaligned Pairs", "authors": ["Bowen Xin", "Tony Young", "Claire E Wainwright", "Tamara Blake", "Leo Lebrat", "Thomas Gaass", "Thomas Benkert", "Alto Stemmer", "David Coman", "Jason Dowling"], "abstract": "Medical image synthesis generates additional imaging modalities that are costly, invasive or harmful to acquire, which helps to facilitate the clinical workflow. When training pairs are substantially misaligned (e.g., lung MRI-CT pairs with respiratory motion), accurate image synthesis remains a critical challenge. Recent works explored the directional registration module to adjust misalignment in generative adversarial networks (GANs); however, substantial misalignment will lead to 1) suboptimal data mapping caused by correspondence ambiguity, and 2) degraded image fidelity caused by morphology influence on discriminators. To address the challenges, we propose a novel Deformation-aware GAN (DA-GAN) to dynamically correct the misalignment during the image synthesis based on multi-objective inverse consistency. Specifically, in the generative process, three levels of inverse consistency cohesively optimise symmetric registration and image generation for improved correspondence. In the adversarial process, to further improve image fidelity under misalignment, we design deformation-aware discriminators to disentangle the mismatched spatial morphology from the judgement of image fidelity. Experimental results show that DA-GAN achieved superior performance on a public dataset with simulated misalignments and a real-world lung MRI-CT dataset with respiratory motion misalignment. The results indicate the potential for a wide range of medical image synthesis tasks such as radiotherapy planning.", "sections": [{"title": "1. Introduction", "content": "Medical image synthesis produces additional imaging modalities to provide essential informa-tion for diagnosis or treatment planning, while bypassing the cost and extra time associated with additional scans. It is particularly useful when the additional scan is invasive, harmful, costly or time-consuming (Liu et al., 2022). Typical applications include synthetic CT for MRI-only radiotherapy dose planning (Spadea et al., 2021) or children's airway assessment (Longuefosse et al., 2023)). Generative adversarial networks (GANs) are widely used in medical synthesis, which usually requires either well-aligned imaging pairs (by supervised methods) or randomly unpaired data (by unsupervised methods). Specifically, supervised GANs, such as Pix2pix and its improved variants (Wang et al., 2018a,b; AlBahar and Huang, 2019), leverage pixel-wise loss on well-aligned imaging pairs to learn the unique and optimal mapping. However, well-aligned pairs are not widely available due to patient motion or organ movement, causing accumulated error and unreasonable placement in supervised methods (Pang et al., 2021). Though registration is commonly used as preprocessing to align images, it is still difficult to acquire perfectly aligned pairs, especially under substantial misalignment such as respiratory motion in lung MR-to-CT synthesis (Sotiras et al., 2013).\nUnsupervised GANs are not ideal for misaligned pairs either. Specifically, unsupervised GANs enable training on randomly unpaired data by leveraging extra constraints such as cycle consistency (Zhu et al., 2017; Hoffman et al., 2018; Khorram and Fuxin, 2022), mutual information (Park et al., 2020; Zhan et al., 2022), or geometry consistency (Fu et al., 2019; Xu et al., 2022). However, they are not designed to utilise pairing information to uncover optimal mappings (minimised pixel-wise error), which is essential in tasks such as radiotherapy planning. According to (Shen et al., 2020), cycle consistency mapping used in unsupervised GANs is not strictly one-to-one mapping, which is an important condition in intra-subject medical image synthesis (Wang et al., 2021). Diffusion models have shown great potential in computer vision applications due to their strength in capturing distributions (Ho et al., 2020; Song et al., 2020); however, they are computationally expensive and data-hungry to train, hindering their application in the medical domain.\nOne recent work RegGAN (Kong et al., 2021) explored directional registration in image synthesis on datasets with simulated misalignment; however, the real-life setting often involves large deformation between pairs (e.g., Figure 1), causing difficulty in learning unique one-to-one mapping due to a large number of local minima (Christensen and Johnson, 2001). The resulting correspondence ambiguity and asymmetric mapping error would add to pixel-wise error in supervised methods, causing a major challenge in generative modelling. The second challenge is the degraded image fidelity caused by the influence of spatial misalignment during the adversarial process. To further elaborate on the issue, the discriminator in RegGAN may recognise the real/fake images purely based on spatial morphology rather than intensity characteristics, thus leading to suboptimal image fidelity.\nIn this paper, we propose a Deformation-aware GAN (DA-GAN) to jointly address the above two synthesis challenges when image pairs are substantially misaligned. Firstly, inspired by the capacity of symmetric registration to jointly estimate invertible bidirectional transformation, we propose a multi-objective inverse consistency to comprehensively in-vestigate how to cohesively incorporate symmetric registration into an image generation network. To further improve degraded image fidelity in an adversarial process, we design a"}, {"title": "2. Methodology", "content": "Problem formulation Suppose we have a training dataset with misaligned imaging pairs (xi, Yi)-1, where xi \u2208 X and yi \u2208 Y belong to different modalities. X and Y differ in both intensity characteristics and spatial morphology. Additionally, we denote \u0177i \u2208 \u0176 as a transformed yi that spatially corresponds to source imaging xi, but \u0177 is unknown in the real world. In other words, both xi and \u0177i are aligned in source spatial space, but only differ in intensity characteristics. With misaligned multimodal imaging pairs (xi, Yi)=1, our objective is to accurately synthesise the target imaging \u0177, that is spatially corresponding to the source image xi for subsequent tasks such as radiotherapy treatment planning."}, {"title": "2.1. DA-GAN overview", "content": "DA-GAN network architecture Figure 2a presents the network architecture of our proposed DA-GAN which consists of three major components, including (1) modality gen-erators G and F, (2) symmetric spatial aligners Ay and Ar, and (3) deformation-aware discriminators Dy and Dr. Firstly, modality generators are designed to translate the source image to the target appearance with spatial correspondence preserved, which is implemented with trainable networks G: x \u2192 \u0177 \u2208 \u0176 and F: y \u2192 \u00ee \u2208 X. Secondly, symmetric spatial aligners Ay and Ar are designed to exploit symmetric correspondence during image-to-image translation to optimise unique and optimal mapping. Each aligner (e.g., Ay = {R, R,T}) is enforced to learn bidirectional transformations \u1ef9: \u0177 \u2192 y and $ : y \u2192 \u0177 that are inverse to each other. The bidirectional transformations are learnt through symmetric transformation repressors R = {R\u2192, R\u2190}. Each transformation regressor is a CNN model trained to predict a deformation field (Kong et al., 2021), and then followed by a spatial transformer network (Jaderberg et al., 2015) to resample images to target spatial space. Thirdly, deformation-aware discriminators are denoted as Dy : y \u2192 {0,1} where y \u2208 U(Y,Y) and D : x \u2192 {0,1} where x \u2208 U(X, Y).\nDA-GAN objective To synthesize with misaligned pairs, DA-GAN is constrained by three loss functions, including (1) symmetric registration loss Lsr (in Figure 2a) for self-aligning, (2)"}, {"title": "2.2. DA-GAN loss functions", "content": "Symmetric registration loss Lsr is designed to (1) punish dissimilarity between mis-aligned imaging pairs and (2) encourage local smoothness on the deformation field. The former similarity loss Lsim for symmetric registration is defined as:\nmin Lsim(G, F, Ry, Rx) = Ex,y[||y \u2013 G(x) \u2022 \u03a6\u1ef9' ||1 + ||G(x) \u2212 y 0 y||1\nG,F,Ry, Rx\n+ ||x \u2212 F(y) \u2022 \u201e'||1 + ||F(y) \u2212 x \u03bf \u03a6 ||1]\nwhere = R(G(x),y), $ = R(y,G(x)), \uad50 = R(F(y),x), y = R(x, F(y)). Secondly, the smoothness loss Lsmt (Balakrishnan et al., 2019) is implemented to minimize the gradient divergence of the estimated deformation field:\nmin Lsmt(Ry, Rx)) = Ex,y[||\u2207\u00a2\u1ef9'||\u00b2 + ||\u2207\u00a2||\u00b2 + ||\u2207\u00a2\u00a3||\u00b2 + ||\u2207\u00a2\u00a3||2]\nRy, Rx\nLastly, by integrating Equation 2 and 3 with loss weights Areg and Asmt, we can formulate our symmetric registration loss Lsr as below:\nLsr = AregLreg + AsmtLsmt\nMulti-objective inverse-consistency loss Lmic is proposed to (1) improve image align-ment during symmetric registration, and (2) improve synthesis correspondence to the source image during image generation. As illustrated in Figure 2b, this is achieved by enforcing inverse consistency from three different levels, including registration level, generation level, and joint level.\nFirstly, at the registration level, we enforce inverse consistency on the forward and backward transformations q\u2194 and \u2190 during symmetric registration. Specifically, the registration IC loss Lic_reg is formulated as\nmin Lic_reg(Ry, Rx) = Ex,y[||y\u00b0\u00a2y \u00b0 \u03a6\u1ef9 - Y||1 + ||x\u03bf\u03c6\u03bf\nX||1]\nRy, Rx\nSecondly, at the generation level, we constrain inverse consistency on the two modality generators G and F. Thus, the generation IC loss Lic_gen is formulated as below:\nmin Lic_gen(G, F) = Ex [||F(G(x)) \u2013 x||1 + Ey[||G(F(y)) \u2013 y||1]\nG,F\nLastly, we propose a third joint level inverse consistency through both image registration and generation cycle, thus jointly optimising image registration and generation. The formulation of joint inverse-consistency Lic_joint is shown as below:\nG,F,Ry, Rx\nmin Lic_joint(G, F, Ry, Rx) = Ex,y[||F(G(x)\u00b0\u00a2\u1ef9) \u03bf \u03a6 \u2013 x||1\n+ ||G(F(y) \u3147) \u03bf \u03a6\u1ef9 \u2013 Y||1]\nTo summarise, the overall multi-objective inverse-consistency loss is composed of three levels of inverse consistency (with their corresponding weights denoted as \u5165):\nLmic = Xic_regLic_reg + Xic_genLic_gen + Xic_joint Lic_joint\nDeformation-aware adversarial loss Ladv da is designed to disentangle the influence of spatial morphology across domains from intensity characteristic learning. We illustrate the comparison of conventional adversarial loss Ladv and our Ladv da for an example discriminator Dy in Figure 2c. Via symmetric spatial aligner Ay, we can obtain source-shaped images and target-shaped images for both generated (G(x) and G(x)) and real images (yo\ny). Then, we feed all these images to the discriminator in the adversarial process to guide it to focus on learning intensity characteristics only. Formally, deformation-aware adversarial loss for Dy is formulated as:\nmin max Ladu da adv_da(G, Dy) = Ey[log(Dy(y)) + log(Dy(y\u00b0\u00a2y)))]\nG Dy\n+ Ex[log(1 \u2013 Dy(G(x))) + log(1 \u2013 Dy(G(x) \u03bf\u03c6\u1ef9))]\nSimilarly, we can derive the other half of adversarial loss for Dr as Ladv da. The total adversarial loss Ladv_da = Aadv_daLady_a _da Ladv_da + adv lv Ladv_da Lady da where Aadv_da denotes loss weight."}, {"title": "3. Experiments", "content": "Simulation dataset For simulation experiments, the public brain T1-T2 MRI dataset (BraTS 2018 (Menze et al., 2014)) was selected because there were well-aligned imaging pairs as ground truth. The training and testing sets contained 5760 and 768 pairs of T1 and T2 images, respectively. The data were normalised to [-1, 1], resized to 256*256, and publicly available 1 (Kong et al., 2021). As original brain images were paired and well-aligned, we simulated 6 different levels of non-affine misalignments on the dataset.\nClinical lung MRI/CT dataset The private lung MRI-CT dataset was used to validate the proposed algorithm for a real-world radiotherapy treatment planning setting. The dataset contained 4096 pairs of ultrashort-echo time MRI (from Siemens scanners) and CT imaging for training and 1024 pairs for independent testing. Both imaging modalities were normalised to [-1, 1], cropped to lung regions, resampled to isotropic, and preliminarily registered. However, we still observed alignment errors in lung regions (Dice 0.949) as well as bones and airways. Please refer to Appendix A for additional details on both datasets.\nExperiment settings In simulation experiments on the brain dataset, 6 non-affine deforma-tions were randomly applied on the training sets to simulate the misalignment. The non-affine deformation was implemented with elastic deformation on control points (Rand2DElastic in MONAI library (Cardoso et al., 2022)) with 6 incremental levels denoted as NA-1 to NA-6. In real-world experiments on the lung dataset, DA-GAN was compared on 8 state-of-the-art (SOTA) medical synthesis methods, including GAN (Goodfellow et al., 2020), Pix2pix (Isola et al., 2017), CycleGAN (Zhu et al., 2017), UNIT (Liu et al., 2017), MUNIT (Huang et al., 2018), NiceGAN (Chen et al., 2020), RegGAN-NC (Kong et al., 2021), and RegGAN-C (Kong et al., 2021). Please find more details in Appendix B.1.\nAll experiments were implemented in Pytorch on a 64-bit Ubuntu Linux system with one 16 GB Nvidia P100 GPU. We trained all the methods using the Adam optimiser with the learning rate 1e-4 and (\u03b21, \u03b22) = (0.5,0.999). The batch size was set to 1 with weight decay 1e04. The training included 50 epochs for both datasets. The brain imaging was evaluated with three metrics in 2D, including Normalized Mean Absolute Error (NMAE), Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). The lung imaging was evaluated with MAE3D, PSNR3D, and SSIM3D. The background of the image was excluded from the computation. For reproducibility, we included more implementation details on DA-GAN modules and loss weighting in Appendix B.2."}, {"title": "4. Results", "content": "Results on the simulation experiments This section summarises the results of DA-GAN on brain imaging with 6 non-affine misalignments (Table 1) and visualisation on the testing set (Figure 3). Table 1 shows that DA-GAN consistently outperformed all comparison methods in three metrics from misalignment levels NA-1 to NA-6. The results of DA-GAN remained stable with increased levels of non-affine misalignment with NMAE ranging from 0.070 to 0.075. Figure 3 visualises that our DA-GAN achieved less error compared with other methods on the error map.\nResults on the lung MRI-CT dataset Table 2 shows the quantitative results of DA-GAN on the lung MRI-CT with challenging respiratory-motion misalignment. The results in Table 2 show that our DA-GAN achieved the best results of MAE3D 35.86, PSNR3D 32.49 and SSIM3D 0.731 compared with 8 SOTA methods. Figure 4 further visually highlights the superiority of our DA-GAN, especially in the challenging spine, bones and heart regions (blue arrows). These demonstrate the great potential for synthetic CT for MRI-only radiotherapy.\nAblation study on DA-GAN losses The ablation study in Table 3 shows that Ladv da improved the baseline RegGAN in all metrics (e.g., PSNR-3D 0.41), while Lmic further improved the results in all metrics (e.g., PSNR-3D 0.36). The paired t-test shows that the contributions from both losses were statistically significant (p-value<0.001) in MAE2D, PSNR2D, and SSIM2D. Further details are in Appendix C. Due to space limit, please find the analysis of convergence, complexity and hyperparameter sensitivity in Appendix D-F."}, {"title": "5. Conclusions", "content": "In this study, we introduce a new DA-GAN for medical image synthesis with substantially misaligned imaging pairs. We propose two novel loss functions Lmic and Ladv_da to generate high-fidelity images while adaptively learning correspondence via symmetric registration. We validated our method on a public brain dataset with both 6 simulated misalignments and a real-world lung dataset compared with 8 SOTA methods. The results demonstrated the potential towards an important step in generalisable medical image synthesis with limited data for clinical applications such as early diagnosis and radiotherapy planning."}, {"title": "Appendix A. Dataset details", "content": "Brain T1-T2 dataset On the brain T1-T2 MRI dataset, we simulated 6 different levels of non-affine misalignments. A visualisation of non-affine misalignment is provided in Figure 5. An overview of implementation parameters for non-affine misalignment is provided in Table 4. More specifically, the non-affine misalignment was simulated using elastic deformation on control points (Rand2DElastic in MONAI library\u00b2). The spacing between control points was set to [40, 40], while the magnitude was set to incremental levels from NA-1 to NA-6.\nLung MRI-CT dataset The lung MRI-CT dataset contained paired but misaligned MRI and CT from 20 patients with lung diseases. The lung MRI was implemented with a prototype 3D free-breathing stack-of-spirals UTE VIBE sequence, provided by Siemens Healthineers (Mugler III et al., 2015; Kumar et al., 2017). UTE-MRI provided millimetre resolution and radiation-free assessment for pulmonary structural imaging (Dournes et al., 2016). 20 CTs were scanned on approximately the same day as MRI. 20 patients included 12 patients with cystic fibrosis and 8 patients with lung cancer. For the MRI protocol, both cystic fibrosis and lung cancer cohorts used Siemens 3T scanners with flip angle 5, echo time 0.05, and repetition time 2.97-3.78. For details of image size and spacing, please refer to Table 5. Both MRI and CT were normalised to [-1, 1], cropped to lung regions, resampled to isotropic spacing, and preliminarily registered. The ground-truth paired MRI-CT images were acquired via an automatic registration pipeline including elastic registration (SimpleElastix library), structure-guided registration (demon registration in ants library) and antsRegistrationSyNQuick (ants library). After registration, we still observed registration errors on the whole lung regions with Dice 0.949 (Dice for right lung 0.947, Dice for right lung"}, {"title": "Appendix B. Implementation details", "content": "In this section, we first discuss the implementation of other comparison methods. Then we introduce network implementation details for different modules and loss weighting in DA-GAN."}, {"title": "B.1. Implementation of comparison methods", "content": "GAN uses a generator to translate the source image to the target space, while using a discriminator to determine whether images were generated from the real data. In this way, GANs iteratively improve the image fidelity via min-max game. In the implementation, the generator and discriminator shared the same network architecture as ours for a fair comparison. The open-source code is on https://github.com/Kid-Liet/Reg-GAN.\nPix2pix is a typical supervised GAN using L1 loss functions to enforce pixel-wise similarity between the predicted images and ground-truth images. In the implementation, the generator and discriminator shared the same network architecture as ours for a fair comparison. The open-source code is on https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.\nCycleGAN proposes a cycle consistency loss to constrain two generators that are reverse to each other. A cycleGAN consists of two sets of generators and discriminators. In the implementation, the generator and discriminator shared the same network architecture as ours for a fair comparison. The open-source code is on https://github.com/junyanz/pytorchCycleGAN-and-pix2pix.\nUNIT leverages the assumption that the latent coding space is shared by different modalities for image-to-image translation. The open-source code is on https://github.com/mingyuliutw/UNIT.\nMUNIT disentangles the representation to a content representation and a style repre-sentation for image-to-image translation. The open-source code is on https://github.com/NVlabs/MUNIT.\nNICE-GAN is a compact and effective network architecture for image-to-image transla-tion, which is achieved by reusing the discriminator for encoding. The open-source code is on https://github.com/alpc91/NICE-GAN-pytorch."}, {"title": "B.2. Implementation of DA-GAN", "content": "DA-GAN was implemented in PyTorch with Python 3.7. The network architecture of DA-GAN consists of two modality generators, two symmetric spatial aligners, and two deformation-aware discriminators.\nModality generator Each modality generator uses 2 downsampling layers, 9 residual blocks, and 2 upsampling layers, following the implementation of Johnson et al. (Johnson et al., 2016). Specifically, we use Ck to denote 7*7 convolution-InstanceNorm-ReLU layer with k filters and stride 1, Dk to denote the downsampling 3*3 convolution-instanceNorm-ReLU layer with k filters and stride 2, Rk to denote a residual block containing two 3*3 convolutional layers with same number of filters, Uk to denote the upsampling 3*3 fractional-strided-convolution-instanceNorm-ReLU layer with k filters and stride 0.5. The architecture of a modality generator is C64-D128-D256-R256-R256-R256-R256-R256-R256-R256-R256-R256-R256-U128-U64-C1.\nSymmetric spatial aligner Each symmetric spatial aligner consists of 4 transformation regressors and 4 spatial transformer networks. Each transformation regressor uses ResUnet architecture, containing 7 encoder layers, 3 residual blocks, 7 decoder layers, and skip connections, following the implementation of RegGAN (Kong et al., 2021). Specifically, we use Dk to denote downsampling 3*3 convolution-LeakyReLU (LeakyReLU with a slope of 0.2) with k filters and stride 1, Rk to denote a residual block containing two 3*3 convolutional layers, Uk to denote upsampling 3*3 convolution-LeakyReLU (LeakyReLU with a slope of 0.2) with k filters and stride 1. The backbone architecture of the transformation regressor is D32-D64-D64-D64-D64-D64-D64-R64-R64-R64-U64-U64-U64-U64-U64-U32, followed by a refinement layer (a residual block and a 1*1 convolutional layer) and an output layer (3*3 convolution layer).\nDeformation-aware discriminator For deformation-aware discriminator, it was im-plemented with a 70*70 PatchGAN (Isola et al., 2017). Specifically, If we denote a 4*4 convolution-instanceNorm-LeakyReLU layer with k filters and stride 2 as Ck (LeakyReLU with a slope of 0.2). The architecture of the discriminator is C64-C128-C256-C512. After the last layer, another convolution is applied to generate 1-dimensional classification results. No instanceNorm is used for the first C64 layer."}, {"title": "Appendix C. Ablation study with complete details", "content": "Table 3 shows the results of an ablation study with full details on the multi-objective inverse consistency loss Lmic and deformation-aware adversarial loss Ladv da. Firstly, for Lmic, we experimented on the 7 settings on different combinations of {Lic_reg, Lic_gen, Lic_joint}. As shown in Table 3, by comparing the settings A-F and G2, we concluded that our proposed Lmic that was composed of all three IC losses achieved the best results. Secondly, for Ladv_da, we compared DA-GAN using conventional adversarial loss Ladv (G1) and deformation-aware adversarial loss Ladv_da (G2). The results show that Ladv_da outperformed its counterpart. Overall, the ablation study demonstrated the effectiveness of both proposed components Lmic and Ladv_da\u00b7"}, {"title": "Appendix D. Convergence analysis during training", "content": "Figure 6 shows the validation NMAE during the training process for different levels of non-affine misalignments in the brain dataset. The results demonstrate that DA-GAN successfully converged under different levels of non-affine misalignments."}, {"title": "Appendix E. Comparison of complexity and accuracy", "content": "As shown in Table 8, our model achieves better synthesis accuracy (9.1% increase of MAE3D) at the cost of additional complexity (doubled no. parameters and size of the model) compared with RegGAN-NC. However, we must emphasize that synthesis accuracy is clinically critical in radiotherapy planning because it reduces unnecessary radiation-related toxicity in patients and is vital to patients' safety (Burnet et al., 2004). Further, our model complexity is still smaller than state-of-the-art GANs (e.g., MUNIT and NiceGAN), and our inference time is fast (0.013s fps)."}, {"title": "Appendix F. Sensitivity analysis on hyperparameters", "content": "We conducted a sensitivity analysis on hyperparameters of loss weights on the lung MRI-CT dataset. As shown in Table 9, the results (fluctuating slightly from 0.009 to 0.010) indicate that the model performance is robust across different choices of hyperparameter values."}, {"title": "Appendix G. Inverse consistency in registration", "content": "Inverse consistency is often implemented by symmetrising cost functions (Christensen and Johnson, 2001) or computing the cost function in \"mid-space\" (Reuter et al., 2010). A"}]}