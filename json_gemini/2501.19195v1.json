{"title": "Rethinking Early Stopping: Refine, Then Calibrate", "authors": ["Eug\u00e8ne Berta", "David Holzm\u00fcller", "Michael I. Jordan", "Francis Bach"], "abstract": "Machine learning classifiers often produce probabilistic predictions that are critical for accurate and interpretable decision-making in various domains. The quality of these predictions is generally evaluated with proper losses like cross-entropy, which decompose into two components: calibration error assesses general under/overconfidence, while refinement error measures the ability to distinguish different classes. In this paper, we provide theoretical and empirical evidence that these two errors are not minimized simultaneously during training. Selecting the best training epoch based on validation loss thus leads to a compromise point that is suboptimal for both calibration error and, most importantly, refinement error. To address this, we introduce a new metric for early stopping and hyperparameter tuning that makes it possible to minimize refinement error during training. The calibration error is minimized after training, using standard techniques. Our method integrates seamlessly with any architecture and consistently improves performance across diverse classification tasks.", "sections": [{"title": "1. Introduction", "content": "Accurate classification lies at the heart of many machine learning applications, from medical diagnosis to autonomous driving systems. Modern classifiers predict not only a class label but also a probability vector reflecting the model's confidence that the instance belongs to each class. This probabilistic output is critical for downstream decision-making. Despite their widespread use, many machine learning models, especially complex ones such as neural networks, tend to produce poorly calibrated probabilities (Guo et al., 2017), meaning that the predicted confidence does not align with real-world probabilities.\nThis issue has mostly been considered through the lens of post-hoc calibration, which consists in adjusting the pre-"}, {"title": "2. Calibration-Refinement decomposition", "content": "We begin by overviewing the decomposition of proper losses into calibration and refinement errors. We also present a new variational formulation of this decomposition that allows a reinterpretation of refinement as the loss after optimal post-processing.\nNotation. Consider classification with k classes. Let \\( \\Delta_k \\) denote the probability simplex \\( \\{p \\in [0,1]^k \\mid \\mathbb{1}^\\top p = 1\\} \\) and \\( \\mathcal{V}_k \\subset \\Delta_k \\) the space of one-hot encoded labels \\( y \\in \\{0,1\\}^k \\) with \\( Y_i = 1 \\) if the true class is i and \\( Y_i = 0 \\) otherwise. Assume we have a random variable \\( X \\in \\mathcal{X} \\) (the feature vector), such that there is an unknown joint probability distribution \\( \\mathcal{D} \\) on \\( (X, Y) \\). We make probabilistic predictions \\( p \\in \\Delta_k \\) on the value of the true label Y using a statistical model \\( f : \\mathbb{R}^P \\to \\Delta_k \\)."}, {"title": "2.1. Proper loss decomposition", "content": "We evaluate predictions with a loss function \\( l : \\Delta_k \\times \\mathcal{Y}_k \\to \\mathbb{R}^+\\). l is a nonnegative function of p and y such that l(p, y) assesses the quality of prediction p for label y. Well-known examples include the Brier score \\( l(p, y) = ||y - p||_2^2 \\) and the cross-entropy (logloss) \\( l(p, y) = - \\sum_{i=1}^k Y_i \\log(p_i) \\). Suppose the label y is sampled from a distribution \\( q \\in \\Delta_k \\). We use the same notation to denote the expected loss obtained when making prediction p: \\( l(p, q) = \\mathbb{E}_{Y \\sim q}[l(p, y)] \\). A natural requirement is that \\( l(\\cdot, q) \\) is minimized (only) in q, in this case, l is called (strictly) proper.\nLet \\( C = \\mathbb{E}_{\\mathcal{D}}[Y \\mid f(X)] \\), the random variable measuring the expectation of Y given the prediction. Proper losses satisfy the following decomposition of the risk (Br\u00f6cker, 2009):\n\\begin{equation}\\mathbb{E}[l(f(X), Y)] = \\mathbb{E}[d_l(f(X), C)] + \\mathbb{E}[e_l(C)], \\tag{1}\\end{equation}\nwhere \\( d_l(p, q) := l(p, q) - l(q, q) \\) and \\( e_l(q) := l(q, q) \\) are respectively the divergence and entropy associated with l (we refer to these as l-divergence and l-entropy). For different choices of classification loss, we recover well-known entropy and divergence functions."}, {"title": "2.2. Calibration error", "content": "Calibration has received a lot of attention on its own, not always in the context of the calibration-refinement decomposition. A model f is said to be calibrated if \\( f(X) = C \\) almost surely. When this is satisfied, for a given prediction \\( f(X) = p \\), the expected outcome \\( C = \\mathbb{E}[Y \\mid f(X)] \\) is aligned with p. This is a desirable property in that it makes model predictions interpretable as probabilities that Y belongs to each of the k classes.\nCalibration error. Model mis-calibration is thus measured by the expected gap between \\( f(X) \\) and C. The choice of a distance or divergence function d gives rise to different notions of calibration error: \\( K^{(d)}(f) = \\mathbb{E}[d(f(X), C)] \\). A popular choice for d is the \\( L^1 \\) distance, resulting in the expected calibration error (ECE, Naeini et al., 2015). Note that when picking the \\( L^2 \\) distance or KL divergence, we recover \"proper\" calibration errors (Gruber & Buettner, 2022), that appear in the decomposition of the Brier score and the logloss. In practice, the data distribution \\( \\mathcal{D} \\) is only known via a finite set of samples \\( (x_i, Y_i)_{1 \\leq i \\leq n} \\) and f makes continuous predictions \\( f(x_i) \\in \\Delta_k \\). To compute the calibration error, C is often estimated by binning predictions on the simplex, resulting in estimators that are biased and inconsistent (Kumar et al., 2019; Vaicenavicius et al., 2019; Roelofs et al., 2022). In the multi-class case, the curse of dimensionality makes estimation even more difficult. Weaker notions like class-wise or top-label calibration error are often used (Kumar et al., 2019; Kull et al., 2019).\nPost hoc calibration. Many machine learning classifiers suffer from calibration issues, which gave rise to a family of techniques known as \"post hoc calibration.\" These methods reduce calibration error after training using a reserved set of samples C called a \"calibration set.\u201d Among others, isotonic regression and temperature scaling are widely used.\nIsotonic regression (IR, Zadrozny & Elkan, 2002) finds the monotonic function \\( g^* \\) that minimizes the risk of \\( g \\circ f \\) on the calibration set. It is a popular calibration technique for binary classifiers. The multi-class extensions come with important drawbacks however.\nTemperature scaling (TS, Guo et al., 2017) optimizes a scalar parameter \\( \\beta \\) to rescale the log-probabilities. Formally, it learns the function \\( g_{\\beta^*} \\) on the calibration set with\n\\begin{align*}\\begin{split}g_\\beta(p) := \\text{softmax}(\\beta \\log(p)),\\\\\\beta^* := \\underset{\\beta \\in \\mathbb{R}^+}{\\text{argmin}} \\mathcal{L}(\\beta), \\\\\\mathcal{L}(\\beta) := \\sum_{(x,y) \\in \\mathcal{C}} l(g_\\beta(f(x)), y) .\\tag{2}\\end{split}\\end{align*}\nFor neural networks, this comes down to rescaling the last layer by \\( \\beta^* \\). TS has many advantages: it is efficient, applies"}, {"title": "2.3. Refinement error", "content": "Given the appeal of calibrated predictions, significant effort has been devoted to estimating and reducing calibration error. The fact that it interacts with another term to form the overall risk of the classifier, however, has received much less attention. This is partly due to the fact that refinement error is little known. So what exactly is refinement?\nIn decomposition (1), it appears as the expected l-entropy of Y given \\( f(X) \\): \\( R_l(f) = \\mathbb{E}[e_l(\\mathbb{E}[Y \\mid f(X)])] \\). With simple intuition about entropy functions, we see that if Y is fully determined by \\( f(X) \\), \\( Y \\mid f(X) \\) has no entropy and the refinement error is null. In contrast, if \\( Y \\mid f(X) \\) is as random as Y, it is maximal. Refinement error thus quantifies how much of the variability in Y is captured by \\( f(X) \\), with a lower error indicating greater information. Just like classification error, it assesses the ability to distinguish between classes independently of calibration issues like over/under-confidence. It provides a much more comprehensive measure, however, as it considers the entire distribution of \\( f(X) \\) rather than a single discretization. This comes at the cost that, like calibration error, it is much harder to estimate."}, {"title": "2.4. A variational formulation", "content": "In Appendix B we introduce novel variational formulations of the terms in decomposition (1), most importantly:\nTheorem 2.1. The refinement error for proper loss l is\n\\begin{equation}R_l(f) = \\underset{g}{\\text{min }} \\mathbb{E}[l(g(f(X)), Y)] \\end{equation},\nwhere the minimum is taken over all measurable functions \\( g : \\Delta_k \\to \\Delta_k \\). It is attained at \\( g^*(f(X)) = \\mathbb{E}[Y \\mid f(X)] \\).\nCalibration error evaluates how much the population risk can be reduced by optimally re-labeling the original forecasts. This optimal relabeling is given by the calibrated score \\( C = \\mathbb{E}[Y \\mid f(X)] \\). This is quite intuitive: such re-labeling sets the calibration error to zero, leaving only the refinement term in the risk decomposition (1). Refinement error thus measures the remaining risk after optimal relabeling, which corresponds to the risk of the calibrated scores. This new variational formulation reveals that refinement error can be estimated using the risk after post-hoc calibration."}, {"title": "3. Our method: refinement-based stopping", "content": null}, {"title": "3.1. refinement-based early stopping: the intuition", "content": "Two distinct minimizers. Minimizing the risk of a classifier comes down to minimizing simultaneously refinement and calibration errors. Empirically, these two errors are not minimized simultaneously during training, as in Figure 1. One possible scenario is that the training set becomes well separated, forcing the model to make very confident predictions to keep training calibration error small. In case of a train-test generalization gap, the model becomes over-confident (Carrell et al., 2022) while refinement error might still decrease. We observe separate minimizers across various machine learning models, whether through iterative training or when tuning a regularization parameter. In Section 6, we analyze this phenomenon theoretically for high-dimensional logistic regression, showing it appears even in very simple settings. The consequence is that risk-based early stopping finds a compromise point between two conflicting objectives: calibration error minimization and refinement error minimization. This corroborates the widely recognized observation that many models are poorly calibrated after training (Guo et al., 2017).\nRefine, then calibrate. The question that arises is: how do we get a model optimal for both calibration and refinement? In general, strictly increasing post hoc calibration techniques like TS leaves refinement unchanged. Berta et al. (2024) showed that binary IR preserves the ROC convex hull, a proxy for refinement. Given that calibration error"}, {"title": "3.2. Refinement-based stopping made practical", "content": "Refinement estimation. As discussed earlier, calibration error estimators are biased, inconsistent, and break in the multi-class case. Since refinement is risk minus calibration, these issues carry over to refinement estimation. One could circumvent this by using proxies like validation accuracy. Theorem 2.1 provides a better alternative: refinement error equals risk after optimal relabeling \\( g^* \\circ f \\). We thus need to estimate how small the validation risk can get by re-labeling predictions. Since the validation set \\( (x_i, Y_i)_{1 \\leq i \\leq n} \\) is finite, minimizing over all measurable functions \\( g : \\Delta_k \\to \\Delta_k \\) would lead to drastic overfitting. However, given a class of functions \\( \\mathcal{G} \\) that we intend to use for post hoc calibration, we can estimate refinement error by solving:\n\\begin{equation}R_l(f) = \\underset{g \\in \\mathcal{G}}{\\text{min }} \\frac{1}{N} \\sum_{i=1}^n l(g(f(x_i)), Y_i)\\end{equation}\nafter every epoch. This comes down to early stopping based on validation \"loss after post hoc calibration,\" which is a biased estimator of refinement error. Compared with Theorem 2.1, we limit ourselves to functions in class \\( \\mathcal{G} \\). A trade-off arises vis-a-vis the size of \\( \\mathcal{G} \\). The larger the class the less biased our estimator but the more prone we are to overfitting the validation set, resulting in poor estimation of the true refinement error. For our method to work, we need to choose a small class \\( \\mathcal{G} \\) that comes as close as possible to the optimal re-mapping \\( g^*(f) = \\mathbb{E}[Y \\mid f(X)] \\).\nTS-refinement for neural nets. We call TS-refinement the estimator obtained when \\( \\mathcal{G} \\) is the class of functions generated by temperature scaling. It is parametrized by a single scalar parameter, seriously limiting the capacity to over-fit the validation set. Moreover, TS is very effective at reducing the calibration error of neural networks (however large it is) while leaving refinement unchanged. This is supported by a plethora of empirical evidence (Guo et al., 2017; Wang et al., 2021). We demonstrate in Section 5"}, {"title": "5. Decomposition in the Gaussian data model", "content": "In this section and the next, we embark on a theoretical analysis that provides insight into the calibration-refinement decomposition. We demonstrate that even with a simple data model and predictor, calibration and refinement errors are not minimized simultaneously. This highlights the relevance of refinement-based early stopping and underscores the broad impact of the problem we address.\nWe use the following stylized model. Consider the feature and label random variables \\( X \\in \\mathbb{R}^p, Y \\in \\{-1, 1\\} \\) under the two-class Gaussian model \\( X \\sim \\mathcal{N}(\\mu, \\Sigma) \\) if \\( Y = 1 \\) and \\( X \\sim \\mathcal{N}(-\\mu, \\Sigma) \\) if \\( Y = -1 \\) for some mean \\( \\mu \\in \\mathbb{R}^p \\) and covariance \\( \\Sigma \\in \\mathbb{R}^{p \\times p} \\). We further assume balanced classes, \\( \\mathbb{P}(Y = 1) = \\mathbb{P}(Y = -1) = \\frac{1}{2} \\). Under this data model, \\( \\mathbb{P}(Y = 1 | X = x) = \\sigma(w^{* \\top} x) \\) with \\( w^* = 2 \\Sigma^{-1} \\mu \\) and \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\) denotes the sigmoid function. The sigmoid function's shape is well suited to describe the posterior probability of Y given X. Note that this holds for any pair \\( \\mathbb{P}(X | Y = \\pm 1) \\) from the same exponential family (Jordan, 1995). Proofs for this section are deferred to Appendix C.\nWe are interested in the calibration and refinement errors of the linear model \\( f(x) = \\sigma(w^{\\top} x) \\). The weight vector w can be learned with techniques such as logistic regression or linear discriminant analysis (Fisher, 1936). Denoting \\( a_w = \\frac{\\langle w, w^* \\rangle_{\\Sigma}}{\\|w\\|_{\\Sigma}} \\) with \\( \\langle w, w^* \\rangle_\\mathcal{X} = w^{\\top} \\Sigma w^* \\) and \\( \\|w\\|_{\\Sigma} = \\sqrt{w^{\\top} \\Sigma w} \\), the error rate of the linear model writes \\( \\text{err}(w) = \\Phi(-\\frac{a_w}{2}) \\) where \\( \\Phi(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}} \\text{exp}(-\\frac{t^2}{2}) dt \\) is the cumulative distribution function of the standard normal distribution. Notice that \\( a_w \\) is invariant by rescaling of w. It measures the alignment with the best model \\( w^* \\) independently of the weight vector's norm. We refer to \\( a_w \\) as the \"expertise level\" of our model f. Interestingly, \\( a_w \\) also appears in the calibration and refinement errors for this simple model."}, {"title": "6. High dimensional asymptotics of logistic regression", "content": "In this section, we place ourselves in the setting of regularized logistic regression to study the impact of calibration and refinement on the risk during training. Proofs and missing technical details are provided in Appendix D."}, {"title": "7. Conclusion", "content": "We have demonstrated that selecting the best epoch and hyperparameters based on refinement error solves an issue that arises when using the validation loss. We established that refinement error can be estimated by the loss after post hoc calibration. This allowed us to use TS-refinement for early stopping, yielding improvement in test loss on various machine learning problems. Our estimator is available in the probmetrics package github.com/dholzmueller/probmetrics and can be used seamlessly with any architecture. One limitation is that TS-refinement can be more sensitive to overfitting on small validation sets. In this case, cross-validation, ensembling, and regularized post-hoc calibrators could be useful. Another little-explored area is the influence of training parameters\u2014such as optimizers, schedulers, and regularization\u2014on the calibration and refinement errors during training, raising the question of how refinement-based early stopping should be integrated with these parameters to maximize efficiency. While refinement-based stopping is relevant in data-constrained settings prone to overfitting, it could also prove useful for the fine-tuning of foundation models, where training long past the loss minimum can be beneficial (Ouyang et al., 2022; Carlsson et al., 2024). Additionally, while we explore its utility for model optimization, our refinement estimator could be a valuable diagnostic metric, especially in imbalanced multi-class settings where accuracy and area under the ROC curve are less appropriate."}]}