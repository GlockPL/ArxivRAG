{"title": "A Rapid Test for Accuracy and Bias of Face Recognition Technology", "authors": ["Manuel Knott", "Ignacio Serna", "Ethan Mann", "Pietro Perona"], "abstract": "Measuring the accuracy of face recognition (FR) systems is essential for improving performance and ensuring responsible use. Accuracy is typically estimated using large annotated datasets, which are costly and difficult to obtain. We propose a novel method for 1:1 face verification that benchmarks FR systems quickly and without manual annotation, starting from approximate labels (e.g., from web search results). Unlike previous methods for training set label cleaning, ours leverages the embedding representation of the models being evaluated, achieving high accuracy in smaller-sized test datasets. Our approach reliably estimates FR accuracy and ranking, significantly reducing the time and cost of manual labeling. We also introduce the first public benchmark of five FR cloud services, revealing demographic biases, particularly lower accuracy for Asian women. Our rapid test method can democratize FR testing, promoting scrutiny and responsible use of the technology.", "sections": [{"title": "1 Introduction", "content": "Face recognition technology (FRT) is a convenient, no-contact, fast, accurate, and inexpensive way to interface securely people and machines. From logging into our smart devices to boarding a plane, crossing a border, and finding missing children, FRT can make our lives more convenient and safer. Conversely, FRT misuse is possible and may lead to loss of privacy and violation of civil rights [12, 46]. As applications increase, it is crucial to understand FRT's potential and implications. In particular, characterizing FRT systems' accuracy and bias is fundamental to informing developers, users, the public, and regulators about the merits and downsides of the technology and to improve it if necessary [12, 14, 21, 24, 26]. On the positive side: Accuracy in FRT systems has improved dramatically in the past five years. Today, systems achieve super-human accuracy [16, 32, 41, 50] and outperform even expert face ana-\nlysts [38], with the potential to make our life more convenient and help reduce the harm that is currently caused by human error [9, 45]. Furthermore, measuring and mitigating bias in algorithms is both feasible and effective, while measuring and correcting biases in human operators is notoriously difficult and can take a long time [13, 35]. Additionally, applications of FRT to policing could work alongside DNA testing to help solve crimes quickly [20], reduce bias in the justice system [34], and reduce the rate of wrongful identification [9, 10] and imprisonment [45]. Thus, AI and FRT may become powerful agents of progress towards more fair, accountable, and transparent institutions [27, 35]. Amongst the potential downsides: willful or inadvertent misuse, inaccuracy, and bias in face recognition technology could inflict harm on individuals and lead to social inequities [12, 26, 46]. Responsible practice in developing and deploying the technology starts with measuring algorithmic accuracy and bias.\nUnfortunately, testing FRT is expensive and laborious, and thus, it is not within the reach of most organizations."}, {"title": "2 Previous work", "content": "Estimating FRT accuracy and bias requires large, accurately labeled datasets to ensure tight confidence intervals. Furthermore, one needs diverse attributes representative of the general population to explore effects on all demographics. A team with the U.S. National Institute of Standards and Technology (NIST) [18, 19] has, over the past 20 years, developed state-of-the-art test datasets and testing practices. They test algorithms on six large (~10M images) datasets collected from visas, visa applications, border crossings, arrest mugshots, kiosk images, and images collected in the wild. Accurate identity annotations are achieved by combining trained government officials and identity documents. NIST publishes updated reports every few months on NIST's \"Face Recognition Vendor Test\" web page [36].\nA number of academic teams are also engaged in testing FRT [6-8, 28]. They use public datasets that may have been included in the training sets of FRT vendors and whose identity labels are often not accurate. Thus, while valuable for science, academic tests may not be suitable for probing the accuracy of commercial systems.\nOnly governments and large tech companies have access to large, accurately labeled datasets. To democratize the testing of FRT algorithms, we need to reduce the cost of ground-truth identity annotation dramatically. This has long been considered unlikely since accurate face identification using human annotators is very difficult [38], and benchmarking without an independently annotated ground truth might seem impossible. Semi-automated clean-up meth-"}, {"title": "3 Image sourcing", "content": "The procedure we recommend is designed to involve the minimum amount of human curation during image sourcing and does not create a static test image dataset. Instead, it selects images on the web, feeds pairs of them to the FRT cloud services being benchmarked, and retains only the resulting confidence values for analysis.\nThe process starts with a human-generated list of names that serve as queries in an image retrieval system. For bias analysis, one can additionally provide demographic attributes (e.g. race, gender, age) for each name. For the evaluation part of this research, we generated two test sets with different image statistics. The first, Celebrities, starts from a list of names of famous people compiled by one of us. It contains 10 names in each of the eight demographic categories. The second, Athletes, is a subset of Wikipedia's list of 2020 Tokyo Olympic athletes. Our list comprises 2755 names and aims to be balanced across six demographics. Detailed statistics for both datasets can be found in Sec. A.\nOnly images published shortly before testing are considered to reduce the chance that test data was used to train the models being tested. In our experiments, URLs of the images were obtained from the Google Images API and the Google News API. Our script obtained a total of 5k images (an average of 67 per ID) for the Celebrities dataset and 223k images (an average of 81 per ID) for the Athletes dataset, leaving 2.2k and 58.6k, respectively, after face detection (see Sec. 4). The number of images found per identity varies significantly (see Fig. S.1). All images obtained using a given name string were assigned the same QueryID (abbreviated as q in the following). At this point, some, but not all, of the faces in the pictures obtained belong to the person whose identity corresponds with the search query. E.g., we expect that a search for \u201cBarack Obama\u201d will yield images of Barack Obama, as well as Michelle Obama, Joe Biden, and other world leaders. The algorithm described in Sec. 6 is designed to clean up these noisy labels.\nTo validate the results of this study, we manually added ground truth identity labels to each face image: one of the authors assigned label y = 1 if the identity matched the query name, y = 0 if not, and y = -1 for rare cases where the identity could not be confirmed even after meta information was consulted. The manual annotation process took a total of 200 hours, about 12 seconds per image on average. Additional details about the image sourcing and annotation process can be found in Sec. \u0410."}, {"title": "4 Face detection", "content": "Face detection, i.e. computing bounding boxes around each visible face, was carried out in every image using each one of the cloud services we tested. Images were sent to each cloud service's face detection API in our benchmark. Since"}, {"title": "5 Face matching confidence scores", "content": "Face recognition cloud services assign a confidence score \\(C_{ij}\\) to each pair of faces \\((i, j)\\). A high confidence score indicates that the pair of faces are likely to belong to the same person, while a low confidence score indicates they are likely to belong to different people. Estimating a service's accuracy requires computing the false non-match rate (FNMR, a.k.a. false reject rate) vs the false match rate (FMR, a.k.a. false accept rate) as a function of a minimum confidence threshold. Thus, for each cloud service provider and each face pair in the test set, we obtain pairwise confidence matches and evaluate the quality of such matches (Sec. 8) vis-a-vis the estimated labels (Sec. 6).\nFRT providers we tested are Amazon Rekognition [1], Face++ [2], Luxand [3], Tencent [4], and Verigram [5]. We used paid services through regular subscriptions, except for Verigram, for which we received complimentary access for research purposes. Computing confidence scores for all pairs of faces in the dataset is very expensive and unnecessary. Our method requires same-query pairs to evaluate FNMR as well as a comparable number of cross-query pairs to evaluate FMR. The set of face pairs to be evaluated by cloud providers was selected as follows: 1. All pairs of faces with the same q were used both for face ID label estimation (Sec. 6) and model evaluation (Sec. 7). 2. A random sample of pairs of faces with different q and from the same demographic group was used for model evaluation (Sec. 7). We sampled as many different-query pairs as same-query pairs.\nA bimodal distribution of confidence values is expected"}, {"title": "6 Identity label estimation", "content": "The next step is estimating the identity label \\(\\hat{y}_i\\) for each face image i. This requires two steps: deciding which identity (i.e., which physical person) corresponds to the query q and deciding whether image i corresponds to that person. Amongst the faces that were downloaded using the search string q, many will actually belong to different identities (Fig. S.6). Which person is the correct identity for a given name query q? Many people may be associated with the same name. How is this ambiguity resolved? For the hand-annotated labels, the \u201ccorrect identity\u201d is decided by the annotator. For our estimation method, the person/identity whose faces are prevalent in the set associated with q is defined as the correct identity. The two criteria coincide almost always.\nWe describe an algorithm that estimates which identity is prevalent, i.e., it decides which is correct identity and estimates the corresponding face images. The end result is an estimate of the identity label for every image in each name. Such identity labels will be used to estimate the error rates for each service (Sec. 7). We start with an intuitive description of the algorithm's steps, and we give a more formal description of the algorithm at the end of the section.\nThe intuition for our algorithm is simple: pairs of faces in query q corresponding to the same person will often, although not always, receive high pairwise confidence \\(C_{ij}^s\\) from service s. If the confidence is low, chances are that a third image k of the same person will have high pairwise confidence with i and j. Thus, we may use \\(C_{ij}^s\\) as an affinity estimate to be used for grouping such faces using spectral factorization [42]. The largest group is most likely associated with the correct identity. Our algorithm is shown in Fig. 3. For the sake of simplicity, consider first the most common case: the collection of images associated with a name consists of images that belong to the correct identity and other images corresponding to a sprinkling of different identities (Fig. 3, first row). In this case, after reordering w.l.o.g. the image indices so that the correct identity is assigned contiguous indices, the confidence matrix C is block-"}, {"title": "7 Service evaluation", "content": "To compute FNMR-vs-FMR curves, we need to divide each service's confidence values into two distinct sets: the \"genuine\" and \"impostor\" distributions. The genuine confidence values correspond to pairs of images that belong to the same identity. The impostor to pairs belonging to different identities. We do this twice: for our method's estimated identities and for the hand-annotated identities so that we may compare performance curves from our method with those from human annotation. We only generate impostor pairs within the same demographic group since same-demographics impostors are the main challenge for FR services. Lastly, we demand that both cross-query images belong to the correct identity to guarantee that the two identities are different \u2013 it is (remotely) possible that two images belonging to different queries but not to the correct identity actually belong to the same identity. Fig. 4 (left panel) shows these four distribu-"}, {"title": "8 Validation experiments", "content": "Does our method work? Does it correctly estimate the accuracy of face recognition systems? We validate our method by measuring the accuracy and bias of three face recognition services and comparing results to traditional hand-annotation. There are two main face recognition tasks: 1:1 matching and 1:n (one-to-many) matching. We focus on 1:1"}, {"title": "9 Accuracy and bias", "content": "The accuracy of the five services may be assessed from the FNMR-vs-FMR plots of Fig. 1 as well as Figs. S.9, S.10. The same conclusions on absolute and relative accuracy may be reached both from the hand-annotated test sets and from our method. First, Luxand and Face++ services are markedly less accurate than the other three. Verigram is the most accurate on Celebrities at relevant FMRS (low FMR), and is a tad less accurate than Amazon Rekognition on Athletes. Second, all services are more accurate on Celebrities than on Athletes\u2014this is expected since Celebrities have many well-lit posed photographs and overall good resolution, while the Athletes dataset contains challenging photographs taken during athletic events, where the subjects are wearing sports equipment such as goggles, are grimacing, and the poses are more challenging.\nEach identity in our datasets was annotated for gender and race. Therefore, we can estimate demographic biases in the services we test. The FNMR-vs-FMR curves are shown disaggregated by demographic groups in Fig. S.11. To make it easier to understand the biases, we show the equal error rate (FNMR=FMR) for each curve in Fig. 5 where each point corresponds to a demographic group, and the equal error rate estimated by our method is compared to the equal error rate that is computed using hand-labeling of the identities. It is clear from these plots that our method is able to estimate bias accurately when errors are large. When algorithmic errors and biases are small, estimate errors are proportionally larger, possibly due to smaller sample sizes.\nObservational methods cannot resolve whether biases are in the algorithm or in the test data [11] (see also in Sec. 2). Since different bias patterns are revealed for Athletes and Celebrities (see Fig. 5, Fig. S.11), it is prudent to assume that biases in the test data are prevalent here."}, {"title": "10 Discussion and conclusions", "content": "We have presented a novel method to estimate the accuracy and bias of face recognition services. Our method eliminates the need for hand-annotating the identity of faces in a test set, which is slow, extremely expensive, and can be inaccurate. Dataset annotation is the main blocker for anyone wishing to test face recognition systems' accuracy and bias. An attractive feature of our method is speed since each step is entirely automated after an initial source of names has been chosen. A test, including forming a test set, obtaining confidence ratings from the services to be tested, and analyzing the data to estimate performance, will be completed in about one day (~2k photos) to four weeks (~60k photos). We estimate that the alternative, which includes collecting images not used to train face recognition models, as well as hand-labeling and hand-curation of the test set, may take many months. Thus, our method democratizes access to testing face recognition\nsystems, a crucial activity in responsible AI. Our method tests services at a certain point in time. Thus, one can discover when and whether a service has changed (see Fig. S.14 for such an analysis). Our system uses face images as transient data and does not require persistent storage of images. In addition, the method can easily be used by a trained operator to select and hand-annotate a fraction of images where the identity label is ambiguous, thus maximizing accuracy while minimizing the additional cost for annotation.\nUsing our method, we could estimate the error rates and biases of five cloud-based face recognition services quickly and accurately. To the best of our knowledge, this is the first published assessment of the accuracy of cloud-based commercial face recognition systems. The only other available measurements of commercial systems come from the National Institute of Standards and Technology (NIST), which does not directly test cloud-based services but rather relies on standalone code that is submitted to NIST by the vendors.\nWe validated our method by comparing its estimates with those provided by hand-annotation and found a very close agreement for the Celebrities dataset and good-enough agreement for the more challenging Athletes dataset, where good-enough means that the same conclusions on absolute accuracy, relative accuracy, and bias may be reached.\nSome steps in our method could be further refined. First, queries that yield multiple identities may be used rather than discarded since our label estimation scheme can handle multiple identities. Second, simultaneous testing of more than five services ought to improve the majority vote we use to estimate identity labels and thus further improve accuracy.\nOur method has limitations. First, in some contexts, us-"}]}