{"title": "Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2", "authors": ["Yuri Chervonyi", "Trieu H. Trinh", "Miroslav Ol\u0161\u00e1k", "Xiaomeng Yang", "Hoang Nguyen", "Marcelo Menegali", "Junehyuk Jung", "Vikas Verma", "Quoc V. Le", "Thang Luong"], "abstract": "We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.", "sections": [{"title": "1. Introduction", "content": "The International Mathematical Olympiad (IMO) is a prestigious mathematics competition for high school students worldwide. IMO problems are known for their difficulty, and solving them requires deep understanding of mathematical concepts and the ability to apply them creatively. Geometry, one of the four IMO categories, is the most uniform across problems, hence the most approachable. It is also well suited for basic reasoning research.\nThere has been two main approaches for automatically solving geometry problems. One is bashing the problems algebraically with Wu's method Chou (1985); Wu (2008), Area method Chou et al. (1993, 1994), or Gr\u00f6bner bases Kapur (1986a,b), and second approach relies in synthetic techniques such as Deduction database Chou et al. (2000), or Full angle method Chou et al. (1996). We focus on the latter as a more human-like approach suitable for transferring the research knowledge to other domains. In our previous work Trinh et al. (2024), we introduced AlphaGeometry (AG1), a neuro-symbolic system that demonstrated a significant step towards mastering this domain, achieving a 54% solve rate on all 2000-2024 IMO geometry problems. AG1 combines a language model (LM) with a symbolic engine to effectively tackle these challenging problems.\nDespite its success, AG1 exhibited limitations in several key areas. Its performance was constrained by the scope of its domain-specific language, the efficiency of the symbolic engine, and the capacity of the initial language model. As a result, when considering all the recent IMO geometry problems from the year 2000 until now, AG1 can only achieve a solving rate of 54%.\nThis paper introduces AlphaGeometry2 (AG2), a substantial upgrade that addresses these limitations and significantly enhances performance. AG2 leverages a more powerful Gemini-based language model trained on a larger and more diverse dataset. We also introduce a significantly faster and more robust symbolic engine, incorporating optimizations such as a reduced rule set and enhanced handling of double points. Furthermore, we expand the domain language to encompass a wider range of geometric concepts, including locus theorems and linear equations. To further improve performance,"}, {"title": "2. More general domain language", "content": "First introduced in Trinh et al. (2024), AG1 uses a simple domain specific language that consists of nine basic \u201cpredicates\u201d listed in Table 1. While these predicates are sufficient to cover 66% of all geometry problems for 2000-2024 IMO, AG1 language does not allow talking about linear equations, movements of points/lines/circles or common problems such as \"Find the angle ...\u201c. Below we explain how AG2 addresses these and other challenges.\nFirst of all, AG2 adds two predicates to allow questions of type \"Find x\":\n1. acompute abcd means \u201cFind the angle between AB and CD\u201d.\n2. rcompute abcd means \u201cFind the ratio AB/CD\u201d.\nIn some geometry problems, including the one appearing at IMO 2024, there are linear equations"}, {"title": "3. Automated problem formalization and diagram generation", "content": "Automated formalization. A major weakness of AlphaGeometry, and other similar neuro-symbolic systems, is the need to manually transform input problems from natural language into a domain specific language. For example, a simple geometry problem in natural language, \u201cGiven a triangle ABC with two equal sides AB = AC, prove that angles B and C are equal\u201d, becomes triangle a bc; a b = ac ? eqanglebabccb ca in the AlphaGeometry domain language.\nAutomating this process, called formalization, is an active area of research (see for example, Jiang et al. (2023); Poiroux et al. (2024); Szegedy (2020); Wu et al. (2022)). It is a significantly more complicated problem compared to a translation between human languages. While translation aims to preserve meaning, formalization frequently requires re-formulating the original problem into an alternative form, and sometimes disambiguating the nuances in the original problem statement. Automated formalization (auto-formalization), therefore, demands significant background knowledge and problem-solving skills on its own. Given that recently foundation models started demonstrating such capabilities, we use one such model, Gemini Team Gemini (2024), to automate the problem formalization for AlphaGeometry. We start by manually translating several dozens of geometry problems into the AG language. Then we use these examples to write a few-shot prompt asking Gemini to translate a given geometry problem from natural language into the AG language. We query Gemini five times with this prompt followed by another Gemini call asking to combine these results into one final answer. With this approach we are able to formalize 30 out of 39 formalizable IMO 2000-2024 geometry problems. For easier geometry problems, it is very consistent and makes almost no mistakes.\nAutomated diagram generation. Another manual part of our pipeline was diagram generation. In AG1, each point is defined by at most two basic predicates recalled in Table 1, the problem is therefore defined constructively and diagrams can be generated automatically. In AG2, we allow one or multiple points being defined simultaneously by an arbitrary number of predicates, allowing us to also cover non-constructive problems. Consider a non-constructive problem statement, \u201cLet ABC be a triangle with incenter I, such that IA = 2IB ...\", here point I is not only defined as an incenter, i.e. the intersection of two internal bisectors, but also defined by a third predicate IA = 2IB and there is no general strategy to construct such four points. Since AG2 covers non-constructive problems, diagram construction becomes a non-trivial part of the pipeline and generally requires human intervention. Similar to Krueger et al. (2021), we propose the following algorithm to automatically generate diagrams given non-constructive problem specifications:\nLet $x \\in \\mathbb{R}^{2n}$ be a vector representing all coordinates of all points. We encode every constraint c in the diagram, including the goal, as $f_c(x) = 0$ with a nonlinear function $f$. We numerically search for\""}, {"title": "4. Stronger and faster symbolic engine", "content": "A symbolic engine is a core component of AlphaGeometry. We call it DDAR, Deductive Database Arithmetic Reasoning. It is an algorithm to compute the deduction closure, i.e. the set of all deducible facts given a core set of initial facts. DDAR builds this deduction closure by following a fixed set of deduction rules, iteratively adding new facts into the deduction closure until no more can be added.\nDDAR drives both the training data generation for our language model and the search for deduction steps during the test-time proof search. In both scenarios, speed is crucial. Faster data generation allows larger and more aggressive data filtering, while faster proof search enables more extensive search, which increases the likelihood of finding a solution within a given time budget.\nThere are three main DDAR improvements that will be discussed in the next three sections.\n\u2022 Capability of handling double points.\n\u2022 Faster algorithm.\n\u2022 Faster implementation."}, {"title": "4.1. Handling double points", "content": "While re-implementing DDAR, we tried to keep approximately the same logical strength as the original algorithm, just a little stronger because of the implementation differences (for example Thales Theorem was replaced with a more general Central Angle Theorem). However, DDAR1 is missing one key feature, which is crucial for tackling hard problems: it is unable to accept two points with different names and the same coordinates.\nFor example, imagine a problem where we intersect two lines a, b at a point X, and intend to prove that X lies on a certain circle w. The most plausible approach might be via a reformulation instead of proving that the intersection of a, b is on w, we prove that the intersection of a, w lies on b. This is equivalent, yet can be much easier to prove because we can move angles on the circle. For"}, {"title": "4.2. Faster algorithm", "content": "The DDAR1 algorithm is processing a list of rules, and tries to apply each rule to all combinations of points. This process involves a candidate searching step, whose time complexity is polynomial in the number of points, and a clause matching step, whose time complexity is exponential in the number of clauses per premise. In theory the worst case for searching similar triangle candidates in AG1 is $O(N^6)$, which is one of the most time consuming steps. The exponential clause matching step is another expensive step. To make the search more efficient, we take all essential rules, and hard-code search for their application, which reduces the number of queries for the AR sub-engine to at most cubic. Furthermore, we discard the explicit rules for angles and distances (e.g. about perpendicular or parallel lines) \u2013 all such deductions happen automatically in the AR engine.\nThe two main time-consuming parts of DDAR are a search for similar triangles and a search for cyclic quadrilaterals. In AG2, we designed an improved DDAR2 algorithm. For similar triangles, we go through all triples of points, hash their \u201cshape\u201d, and detect a similar pair if the shape is recognized twice. For cyclic quadrilaterals, we go through all pairs (point X, segment AB), and hash the value of $(A, B, \\angle AXB)$. If such a triple repeats, we get a cyclic quadrilateral. By the \u201cvalue\u201d of segment AB, or $\\angle AXB$, we mean a symbolic normal form calculated by the AR-submodule. This submodule keeps track of known linear equations between angles, distances, and log-distances, understands its algebraic consequences, and can reduce any linear expression to its normal form."}, {"title": "4.3. Faster implementation", "content": "While the new algorithm already significantly accelerates DDAR, we make further speed improvements by implementing its core computation (Gaussian Elimination) in C++. The new C++ library, which is exported into Python via pybind11 Jakob et al. (2017), is over 300 times faster than DDAR1. In order to benchmark the speed improvement, we select a set of 25 IMO problems that cannot be solved by DDAR (see Figure 8), and run the test 50 times on a machine with AMD EPYC 7B13 64 core CPU. While on average DDAR1 finishes its computations in 1179.57 \u00b1 8.055 seconds, DDAR2 is much faster - finishing in 3.44711 \u00b1 0.05476 seconds\u00b9."}, {"title": "5. Better synthetic training data", "content": "Supplementing the symbolic engine with a language model was a key to AG1 success, bringing the solve rate from 14 (pure deduction proofs) to 25 out of 30 selected IMO problems Trinh et al. (2024). This language model was trained on a large amount of algorithmically generated synthetic data. In AG2 we use the same procedure.\nSimilar to AG1, our synthetic data generation method starts by sampling a random diagram, and uses the symbolic engine to deduce all possible facts from it. For each of the deduced facts, a traceback algorithm is used to extract the corresponding premises, auxiliary points, and deduction steps that prove the fact. Our data generation method deliberately avoids the use of human-crafted problems as initial diagram seeds, and strictly starts from random diagrams. This design choice eliminates the risk of data contamination and allows for the exploration of theorem distributions that may extend beyond established human knowledge. This approach contrasts with methods like TongGeometry Zhang et al. (2024), which rely on human expertise and existing problem diagrams to guide and filter data generation. In AG2, we keep using random diagrams as initial seeds and continue to push for better synthetic training data.\nLarger, more complex diagrams and better data distribution. First of all, we scale up resources for data generation, and do more careful re-balancing of the data distribution. As demonstrated on Figure 2, compared to AG1, AG2:\n\u2022 Explores random diagrams at twice the size, allowing for extracting much more complex problems.\n\u2022 Produces theorems at up to 2x more complex, i.e. number of points and premises.\n\u2022 Produces up to 10x more complex proofs, i.e. 10x more proof steps.\n\u2022 Has a more balanced data distribution between question types.\n\u2022 Has a more balanced data distribution between problems with and without auxiliary points."}, {"title": "6. Novel search algorithm", "content": "In AG1, we use a simple beam search to discover proofs. In AG2, we design a novel search algorithm, in which several differently configured beam searches are executed in parallel and are allowed to help each other through a knowledge sharing mechanism (see Figure 4). To improve the robustness of our system we use multiple different language models for each search tree configuration. We call this search algorithm Shared Knowledge Ensemble of Search Trees (SKEST).\nIt works as follows. In each search tree, a node corresponds to one attempt at auxiliary construction followed by one attempt of the symbolic engine run. If the attempt succeeds, all search trees terminate. If the attempt fails, the node will write down the facts that the symbolic engine managed to prove into a shared facts database. These shared facts are filtered such that they are not the auxiliary point specific to the node itself, but only relevant to the original problem. This way, these facts can also be useful to the other nodes in the same search tree, and the nodes in different search trees. Below we list various types of search trees, which we employ to make sure different parts of the search space are explored effectively:\n\u2022 \"Classic\" search tree: the same beam tree search used in AG1, where a language model is asked to produce one auxiliary point at each node.\n\u2022 Tree predicting multiple auxiliary points at each node: a language model is allowed to produce as many auxiliary points as it wants at each tree node. Recall that this is possible because our"}, {"title": "7. Better language model", "content": "The final AG2 improvement is a new language model. In this section we discuss our new training and inference setups."}, {"title": "7.1. Training setup", "content": "AG1 language model, a custom transformer, was trained in the unsupervised fashion in two phases: training on problems with and without auxiliary constructions followed by training on only problems that contain auxiliary constructions. For AG2 we leverage the Gemini training pipeline and simplify training to just one phase: unsupervised learning on all data. Our new language model is a sparse mixture-of-expert Transformer-based model that builds on Gemini Team Gemini (2024) and trained on AG2 data described in Section 5. We train multiple models of different size using three training setups:\n1. Training from scratch with a custom tokenizer in the domain specific language (AG1 setup).\n2. Fine-tuning already pre-trained custom math specialized Gemini models in natural language (for more details see Appendix A).\n3. Multimodal training from scratch with an additional image input - a diagram of the given geometry problem (for more details see Appendix B).\nApart from a large synthetic training set of around 300 million theorems, we create three evaluation sets:\n1. Synthetic problem set with and without auxiliary points, \u201ceval\".\n2. Synthetic problem set with only auxiliary points, \u201ceval_aux\".\n3. Special set of geometry problems from IMO 2000-2024 that have been solved by AlphaGeometry previously, \"imo_eval\".\nAll these sets contain full proofs, and during training we compute perplexity loss on them. Note, however, that these are only proxy metrics for two reasons. First, during inference (just like in AG1) we only use auxiliary points suggested by the language model, while the perplexity is computed on"}, {"title": "7.2. Inference setup", "content": "A new problem is solved via the search algorithm described in section 6 with multiple search trees and multiple language models of different sizes. In contrast to AG1, we use top-k sampling with temperature t = 1.0 and k = 32. Note that a high temperature and multiple samples are essential for solving IMO problems. With the greedy decoding t = 0.0, k = 1, and no tree search, our models can solve only two problems out of 26 that require auxiliary constructions. Increasing the temperature to t = 1.0 and using k = 32 samples (without a search tree) allows our language models to solve 9 out of 26 problems. Lower temperatures t < 1.0 do not produce diverse enough auxiliary constructions (see Figure 6), while higher temperatures result in the increasing number LM outputs with a wrong domain language syntax.\nThe analysis string. In AG1, the interface between LM and DDAR is minimal: DDAR takes auxiliary constructions proposed by LM, and the LM stops proposing auxiliary constructions when DDAR"}, {"title": "8. Results", "content": "Our main downstream metric is the solve rate on IMO geometry problems. There are a total of 45 geometry problems in 2000-2024 IMO, which we translate into 50 AlphaGeometry problems (we call this set IMO-AG-50). Some problems are split into two due to specifics of our formalization. Figure 8 demonstrates our main result: AlphaGeometry2 solves 42 out of 50 of all 2000-2024 IMO geometry problems, thus surpassing an average gold medallist for the first time5. More details are presented in Table 4, which compares various AG2 configurations with other systems, such as AG1 Trinh et al. (2024) and TongGeometry Zhang et al. (2024). We also perform an additional evaluation on a new set of 30 hardest IMO shortlist problems, which are formalizable in the AG2 language, and which have never appeared at IMO. For these additional results see Appendix D.\nOn Figure 7 we present the IMO solve rate as a function of training time (seen tokens during training) for one language model coupled with DDAR via the \"classical\" tree search described in Section 6. Interestingly, AlphaGeometry2 can already solve 27 out of 50 problems after only 250 training steps with batch size 256, or around 200 million tokens. We also run ablation studies on how inference settings affect the overall performance (see Figure 9). For a single search tree we find"}, {"title": "9. Conclusions and Future work", "content": "This paper introduced AlphaGeometry2, a significant upgrade to AlphaGeometry that addresses previous limitations and enhances performance in several key areas. AG2 incorporates a more powerful language model trained on a larger and more diverse dataset, a faster and more general symbolic engine, an expanded domain language, and a novel proof search algorithm. These improvements have resulted in a substantial leap in performance, with AG2 achieving an 84% solve rate on 2000-2024 IMO geometry problems, significantly improving upon the 54% solve rate achieved by its predecessor.\nWe also presented several studies related to language modeling. First, we showed that our models are quite capable in generating not only auxiliary constructions, but also full proofs, demonstrating a potential for modern language models to operate without external tools, such as symbolic engines. Secondly, we discovered that for AlphaGeometry neither a tokenizer, nor a domain language used to train the model, plays a decisive role. We obtained similar results for custom tokenizers with small vocabularies and the generic large Gemini tokenizer. Training in the domain specific language got us similar results compared to training in natural language. Thirdly, we compared training from scratch and fine-tuning language models pre-trained on math datasets. We found that despite training on the same AlphaGeometry dataset, these models learn slightly different skills, and combining them into our novel search algorithm, Shared Knowledge Ensemble of Search Trees, improves the overall solve rate.\nDespite achieving an impressive 84% solve rate on all 2000-2024 IMO geometry problems, there is"}, {"title": "A. Fine-tuning of math specialized language models on AG data", "content": "Even though during the initial transition to AG2, we maintained the AG1 training setup (training from scratch using a custom tokenizer in the AG domain specific language), it is natural to ask whether fine-tuning language models that already possess problem solving capabilities can improve the performance. Such fine-tuning is not immediately possible due to the difference in the utilized tokenizers and training language. In this section we explore the role of the custom tokenizer and the domain specific language, followed by a discussion about fine-tuning of a math specialized Gemini model on AG data.\nTokenizers. Tokenizer is an essential part of modern language models and, more broadly, any foundation models7. It is generally believed that a tokenizer might be the major bottleneck in the models abilities to do math, e.g. see Singh and Strouse (2024). We investigate this hypothesis in the controlled setting of AlphaGeometry. To do so we train models of the same architecture with different tokenizers: custom tokenizers with vocabularies of a few thousand tokens and the large language model tokenizers with a vocabulary of 300k tokens. Recall that our custom tokenizers are created at word-level, i.e. each token has full meaning, as opposed to subword level tokens. In AG language there are the following types of tokens:\n1. Point names: \"a\", \"b\", \"c\", ... \"z\", \"a1\", ..., \u201cz1\u201d.\n2. Predicate names: coll, cong, coll, cyclic, eqangle, eqratio, acompute, rcompute, aconst, rconst, distmeq, distseq, angeq, overlap, noverlap, sameclock, lessthan.\n3. Number and fractions: 1, 2, 3, ..., -, /.\n4. Predicate reference tokens: (000), (001), (002),...(999).\n5. Reserved tokens: {Analysis}, {Numerical}, {FromGoal}, {Proof}, x00, :, ;, ..\nSomewhat surprisingly, we find that AlphaGeometry performance on the 2000-2024 IMO geometry problems stays the same with different tokenizers, which suggests that modern LLM tokenizers might be flexible enough to perform mathematical manipulations."}, {"title": "B. Multi-modal", "content": "Until now we talked about AG2 as a system that couples a language model together with a symbolic engine. However, since our language model is based on Gemini 1.5, which is multi-modal by design (see Team Gemini (2024)), it is natural to enhance AG model through multi-modal reasoning. For this, we train a new family of models that, alongside the problem text, take the corresponding diagram image as the input. For training and during test time, diagrams are built as described in Section 3.\nDespite promising results during the training, we do not observe any improvements in the solve rate on the downstream IMO problems when using this model alone. However, just like in case of fine-tuning pre-trained models (see Section A), we find that the multimodal model produces slightly different auxiliary point proposals. Combined with other models via the knowledge sharing"}]}