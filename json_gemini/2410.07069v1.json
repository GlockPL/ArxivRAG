{"title": "REIFE: Re-evaluating Instruction-Following Evaluation", "authors": ["Yixin Liu", "Kejian Shi", "Alexander R. Fabbri", "Yilun Zhao", "Peifeng Wang", "Chien-Sheng Wu", "Shafiq Joty", "Arman Cohan"], "abstract": "The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our large-scale evaluation reveals: (1) Base LLM performance ranking remains largely consistent across evaluation protocols, with less capable LLMs showing greater improvement from protocol enhancements; (2) Robust evaluation of evaluation protocols requires many base LLMs with varying capability levels, as protocol effectiveness can depend on the base LLM used; (3) Evaluation results on different datasets are not always consistent, so a rigorous evaluation requires multiple datasets with distinctive features. We release our meta-evaluation suite REIFE,\u00b9 which provides the codebase and evaluation result collection for more than 500 LLM-evaluator configurations, to support future research in instruction-following evaluation.", "sections": [{"title": "1 Introduction", "content": "The ability to follow human instructions has become an important evaluation aspect for large language models (LLMs), indicating their alignment with human users (Ouyang et al., 2022). Recently, due to their better correlation with human judgments compared with traditional evaluation metrics, the LLMs themselves are often used as judges of the model output quality for generative tasks including instruction following (Liu et al., 2023a; Fu et al., 2023; Zheng et al., 2024). These LLM-based evaluation methods are an essential component of the most widely used automatic benchmarks for instruction-following evaluation, such as AlpacaEval (Li et al., 2023c) and MTBench (Zheng et al., 2024), where a strong LLM is used to evaluate the quality of model responses. Moreover, they can be used as reward models for instruction fine-tuning of LLMs in both distillation and self-improvement settings (Tunstall et al., 2023; Yuan et al., 2024). However, recent studies have identified various limitations of LLM-based evaluation methods, including low self-consistency rates in their predictions, positional biases, and a preference for their own outputs (Liu et al., 2023a; Wang et al., 2024b; Zheng et al., 2024; Panickssery et al., 2024).\nTherefore, the evaluation of LLM-based evaluations is critically important. Such evaluations of evaluation methods, or meta-evaluation, usually involve comparing the automatic evaluation results against human evaluation (Liu et al., 2023a; Dubois et al., 2024; Zeng et al., 2024). These evaluations of LLM-evaluators assess two dimensions: (1) the capabilities of base LLMs in performing the evaluation task and (2) the effectiveness of evaluation protocols \u2013 the methods by which base LLMs are used to perform evaluation, e.g., pairwise comparison as in AlpacaEval or pointwise scoring as in MTBench.\u00b2 Existing work (Zheng et al., 2024; Wang et al., 2024b; Zeng et al., 2024) often lacks comprehensiveness in one or both of these dimensions, and more thorough evaluations are needed.\nWe argue that the following two directions are crucial for a more comprehensive, rigorous evaluation of LLM-evaluators for instruction following: (1) Including the diverse set of base"}, {"title": "2 Related Work", "content": "LLM-based Evaluation Using LLMs as evaluators has become a promising approach for assessing text generation quality (Chiang and Lee, 2023; Fu et al., 2023; Liu et al., 2023a) in tasks like summarization (Fu et al., 2023; Liu et al., 2023a,b) and instruction-following (Zheng et al., 2024; Zeng et al., 2024; Li et al., 2023c).\nRecent work has proposed various advanced LLM-based evaluation methods. For example, fine-grained or decomposition-based approaches, such as Chain-of-Aspects (Gong and Mao, 2023) and Branch-Solve-Merge (Saha et al., 2023), can guide LLMs to perform structured analysis by identifying fine-grained differences and providing detailed rationales. Agent-based methods, like PRD (Li et al., 2023b) and ChatEval (Chan et al., 2024), employ multi-role debate settings to bring diverse perspectives to the evaluation process. Other techniques include probability-weighted scoring (Liu et al., 2023a), reference-based evaluation (Zeng et al., 2024), and self-consistency decoding (Wang et al., 2023). Our study investigates the effectiveness of these advanced evaluation protocols on a larger scale, assessing their performance across multiple datasets and base LLMs.\nRelated studies have also explored fine-tuning LLMs as evaluators for various evaluation tasks including instruction-following evaluation (Li et al., 2023a; Wang et al., 2024a), such as Prometheus (Kim et al., 2024a). However, we choose to exclude them from the majority of our evaluation since our focus is on generic LLMs with various evaluation protocols, while the fine-tuned LLMs usually require a fixed evaluation protocol.\nHuman Evaluation and Meta-Evaluation of Instruction-Following A series of recent studies have conducted human evaluations on instruction-following and/or performed evaluations of automatic evaluators using the collected human annotations (Zhang et al., 2023; Wang et al., 2024b,c; Lan et al., 2024). Among them, the annotations from AlpacaFarm (Dubois et al., 2024) and MT-Bench (Zheng et al., 2024) have become important testbeds for evaluating widely used LLM evaluators. Zeng et al. (2024) introduce LLMBar, which consists of high-quality human annotations with a high level of inter-annotator agreement rate. RewardBench (Lambert et al., 2024) provides a benchmark for evaluating reward models used for learning from human or LLM feedback (Ouyang et al., 2022; Bai et al., 2022; Tunstall et al., 2023). While sharing a similar task format, our evaluation focus is different from theirs because we aim to assess the evaluation capability of generic LLMs instead of dedicated reward models."}, {"title": "3 Evaluation Settings of REIFE", "content": "In REIFE, we evaluate LLM-based instruction-following evaluations along two dimensions: base LLMs and evaluation protocols (Figure 1), using human evaluations as the gold standard. Below, we outline the settings of this evaluation.\nDatasets We use four datasets to evaluate the LLMs\u2019 capability of instruction-following. Each dataset includes human annotations for pairwise comparisons of two outputs from an instruction, with a binary label indicating which output is better in instruction following. Table 1 summarizes the dataset information. LLMBar-Natural and LLMBar-Adversarial are from Zeng et al. (2024), consisting of data examples examined and edited by the paper authors. MTBench (Zheng et al., 2024) contains expert human annotations made by graduate students for multi-turn conversations. InstruSum (Liu et al., 2024) contains human annotations for instruction-controllable summarization, where the input includes a source article and a spe-"}, {"title": "4 Baselines", "content": "We first establish baselines for base LLMs and evaluation protocols for evaluating instruction-following for our further investigations."}, {"title": "4.1 Baselines for Base LLMs", "content": "To benchmark the baseline performance of base LLMs at instruction-following evaluation, we evaluate them with a simple evaluation protocol to construct the corresponding LLM-evaluators. This base evaluation protocol, proposed in Zeng et al. (2024), requires the LLM-evaluators to directly predict which output is better, with rules to constrain output formats and to avoid potential biases.\nTable 2 presents the evaluation accuracy of 38 proprietary and open-source LLMs, together with two state-of-the-art reward models, nemotron-4-340b-rm (Adler et al., 2024) and offsetbias-rm (Park et al., 2024), and two strong fine-tuned LLM-evaluators, prometheus-2-8x7b (Kim et al., 2024a) and offsetbias-lm (Park et al., 2024) as baselines. The model information is in Appendix B at Table 12. We note the following observations:\n(1) Proprietary vs. Open-Source: the open-sourced llama-3.1-405b outperforms most of the proprietary LLMs, and llama-3.1-70b lags slightly behind gpt-4o and gpt-4-0613.\n(2) Performance Gap: The LLMs at the lower end, such as llama-2-7b and gemma-2b, achieve an accuracy near 50%, comparable to a random oracle. On the other hand, llama-3.1-405b achieves a high accuracy of approximately 84%.\n(3) Dataset Difficulty: There is also a large difference in the average LLM performance across different datasets. For example, the average evaluation accuracy on LLMBar-Natural is around 20% higher than LLMBar-Adversarial.\n(4) Comparisons with Reward Models and Fine-tuned LLMs. The strongest LLM-evaluators outperform the state-of-the-art reward models and fine-tuned LLM-evaluators. The fine-tuned LLM-evaluator, offsetbias-lm, shows a significant improvement over its base model, llama-3-8b, suggesting the potential of fine-tuned LLM-evaluators. Meanwhile, prometheus-2-8x7b only outperforms its base model (mixtral-8x7b) on the easier datasets LLMBar-Natural and MTBench, indicating a lack of robustness.\nThese baseline results show that the top open-source LLMs already approach the performance of their proprietary counterparts and offer a wide performance spectrum. Therefore, for transparency and reproducibility, we will use mostly open-source LLMs in the rest of our evaluations."}, {"title": "4.2 Baselines for Evaluation Protocols", "content": "We now establish a baseline for evaluation protocols, which define how the base LLM is used to perform the evaluation. To this end, we evaluate the evaluation protocols used in three automatic LLM benchmarks for instruction-following \u2013 AlpacaEval (Li et al., 2023c), ArenaHard (Li et al., 2024b), and WildBench (Lin et al., 2024). Each of these benchmarks uses their evaluation protocol together with a strong base LLM, e.g., GPT-4 (Achiam et al., 2023), to perform pairwise comparison of different LLMs\u2019 outputs. The individual comparison results are then aggregated to produce a performance ranking of various LLMs. We note that the efficacy of these benchmarks is evaluated at the system level, where their produced ranking is compared against the system ranking from human evaluation benchmarks, e.g., ChatBot Arena (Chiang et al., 2024). In contrast, here we aim to evaluate the performance of their evaluation protocols at the instance level, measuring their evaluation accuracy against human annotations at individual data instances.\nIn Table 3, the benchmark evaluation protocols are compared against the base protocol (Zeng et al., 2024) used in \u00a74.1, where they are used together with the 25 open-source base LLMs evaluated in \u00a74.1 and the strongest proprietary LLM, gpt-4o. It shows that the benchmark protocols cannot outperform the base protocol, especially on the more challenging LLMBar-Adversarial and InstruSum datasets. This indicates that the complex design of the benchmark protocols, which often includes detailed instructions on the evaluation plan and output structure, cannot improve the LLM-evaluators performance at the instance level. In the next section, we will provide a further examination of various evaluation protocols."}, {"title": "5 Evaluating Evaluation Protocols", "content": "In \u00a74, we only tested the LLM-evaluators with the base and benchmark evaluation protocols. We now expand the evaluation dimensions to include various protocols proposed in recent work. By using 25 open-source LLMs, we believe this evaluation will provide a fairer and more rigorous examination."}, {"title": "5.1 Evaluation Protocols", "content": "In our evaluation, we examine 15 protocols derived from previous work. To address the unavailability of some prompt templates and to ensure a fair comparison, we design prompt templates ourselves when necessary. We ensure that all prompt templates adhere to unified formatting and style, and we refine them iteratively to make sure that the protocols can perform to their full potential. The evaluated protocols are outlined below, with their prompt templates provided in Appendix C.\nBaseline Protocol (1) base: the vanilla approach used in \u00a74 which directly predicts the pairwise comparison outcome, proposed in Zeng et al. (2024).\nEnhanced Protocols Five other protocols from Zeng et al. (2024) are evaluated, which include various enhancements based on the base protocol:\n(2) cot: the LLM is asked to provide a chain-of-thought (Wei et al., 2022) explanation before making the final decision.\n(3) metric: the LLM is prompted to generate a few metrics for the evaluation task first, which are later used in the actual evaluation.\n(4) reference: the LLM is prompted to generate a \"reference\" output for the given instruction, which is later used in the actual evaluation.\n(5) metric+reference: a combination of the metric and reference methods.\n(6) swap&synthesize: based on cot and inspired by Du et al. (2024), this method requires the LLM to resolve self-disagreement in predictions from two output orders and make a final decision.\nComplex Protocols Beyond the enhanced protocols, 7 complex protocols are evaluated based on evaluation methods proposed in previous work.\n(7) fine-grained-diff: Similar to Min et al. (2023), this protocol guides the LLM to first identify fine-grained differences in output pairs and then provide a detailed rationale for choosing the better output considering these differences.\n(8) multi-role-round1 & (9) multi-role-round2: Inspired by frameworks that use multi-"}, {"title": "5.2 Results", "content": "Evaluation Accuracy Table 4 demonstrates the evaluation accuracy of various protocols averaged over different base LLMs. We note the following:\n(1) prepair and gpt4-reference achieve the strongest average performance, achieving a 1.7%"}, {"title": "6 Analysis", "content": "In \u00a75, a total number of 375 LLM-evaluators are evaluated, combining 25 base LLMs and 15 evaluation protocols. We now present detailed analyses of the base LLMs, evaluation protocols, and datasets, using these comprehensive evaluation results to address a series of specific research questions."}, {"title": "6.1 Analysis of Base LLMs", "content": "What is the average performance of the base LLMs across different protocols? Table 7 displays the LLMs\u2019 average evaluation accuracy over 15 protocols, showing that llama-3.1-70b is the strongest evaluator on average, while qwen-2-72b achieves the best performance on MTBench.\nHow does base LLMs\u2019 ranking change with different protocols? Figure 2 shows the evaluation accuracy of base LLMs achieved with the base protocol and the average accuracy across different protocols. The results demonstrate a high positive correlation between them, achieving a Spearman's coefficient of 0.983. This indicates that using a single base protocol for the evaluation of the base LLMs\u2019 is likely to yield reliable results.\nHow large is the optimal improvement gained from different evaluation protocols for base LLMs? Table 8 displays the optimal evaluation accuracy improvement ($\\S$) achieved by different evaluation protocols over the base protocol for various base LLMs. That is,\n$\\S = \\max_{p \\in P} s(p) - s(\\hat{p}),\\quad$ (1)\nwhere $s(p)$ is the evaluation accuracy of an evaluation protocol p, $\\hat{p}$ denotes the base protocol, P is the set of protocols excluding $\\hat{p}$. The results indicate that less capable LLMs are more likely to achieve larger improvements when the suitable protocols are used, showing a -0.455 Spearman's correlation between the base LLMs\u2019 performance"}, {"title": "6.2 Analysis of Evaluation Protocols", "content": "What is the evaluation protocols\u2019 optimal performance? In Table 4 of \u00a75.2, the evaluation protocols\u2019 performance is evaluated across all base LLMs. Table 9 instead shows the optimal performance of evaluation protocols, i.e., their evaluation accuracy with the most compatible base LLM. The results show that the evaluation protocols\u2019 optimal performance can significantly differ from their average performance. For example, while prepair achieves the best average performance, it ranks only 7th in terms of optimal performance. Moreover, the Spearman's correlation between the rankings of average and optimal performance for the evaluation protocols is 0.789, much lower than the correlation between rankings of average and optimal performance for the base LLMs (0.977).\nHow do base LLMs\u2019 capabilities affect evaluation protocol's performance? The previous analysis shows that the evaluation protocol's performance can be significantly affected by the base LLMs used. Therefore, we provide a further examination of the protocol performance with two groups of LLMs: one containing the strongest 10 LLMs identified in Table 7, and another containing the weakest 10. Figure 3 demonstrates a 0.807 Spearman's correlation between the protocol performance ranking with these two groups of LLMs. We note the effectiveness of evalua-"}, {"title": "6.3 Analysis of datasets", "content": "What is the difficulty level of different datasets? The large number of LLM-evaluators in our evaluation allows us to measure the difficulty of individual data examples. Therefore, we calculate the average correctness rate of different LLM-evaluators on each example in different datasets, which is defined as the average evaluation accuracy:\n$C(x) = \\frac{\\sum_i Acc(x;e_i)}{N}$ (2)\nwhere C(x) is the correctness rate of data example x, Acc(x; ei) is the evaluation accuracy of LLM-evaluator ei, and N is the number of evaluators. Figure 4 shows the distribution of this correctness rate of data examples. We note:\n(1) LLMBar-Natural, MTBench, and InstruSum have a similar data example difficulty distribution, with a small portion of examples where the LLM-evaluators are rarely correct.\n(2) LLMBar-Adversarial exhibits a different pattern: the distribution peaks at examples of medium difficulty, while the easiest and most difficult examples are similarly proportioned. This suggests that"}, {"title": "7 Conclusion", "content": "In this work, we conducted a large-scale meta-evaluation of instruction following, examining 25 open-source base LLMs and 15 evaluation protocols while identifying the best-performing LLM-evaluators over 4 datasets. We found that a reliable evaluation of base LLMs' evaluation capabilities can likely be achieved with a single evaluation protocol due to the stability of their performance across different protocols. However, evaluating evaluation protocols should involve a diverse group of base LLMs, as they can significantly impact the evaluation protocols' effectiveness. We hope that our findings and meta-evaluation suite, REIFE, can pave the way for further studies in this direction."}, {"title": "Limitations", "content": "Evaluation Scope: Our evaluation centered around generic LLMs and evaluation protocols. As discussed in \u00a72, we did not focus on reward models trained to evaluate output quality or LLMs fine-tuned for instruction-following. We note that a future study incorporating these systems could yield more comprehensive results.\nPrompt Variations: In our evaluations, we aimed to control the impact of prompt design by minimizing unnecessary differences across different evaluation protocols. However, we acknowledge that a more thorough evaluation involving multiple prompt variants for each protocol would likely produce more stable results.\nQualitative Human Evaluation: We primarily used high-quality human annotation datasets for our quantitative meta-evaluation. Nevertheless, we recognize the lack of qualitative human evaluation, especially concerning the rationales generated by different LLM-evaluators, which could provide further insights into their limitations. We provide a preliminary case study in Appendix D showcasing the error patterns of the base LLMs, and another in Appendix E demonstrating the effect of different evaluation protocols on the same base LLM."}]}