{"title": "A GENERAL FRAMEWORK FOR PRODUCING INTER-\nPRETABLE SEMANTIC TEXT EMBEDDINGS", "authors": ["Yiqun Sun", "Qiang Huang", "Yixuan Tang", "Anthony K. H. Tung", "Jun Yu"], "abstract": "Semantic text embedding is essential to many tasks in Natural Language Pro-\ncessing (NLP). While black-box models are capable of generating high-quality\nembeddings, their lack of interpretability limits their use in tasks that demand\ntransparency. Recent approaches have improved interpretability by leveraging\ndomain-expert-crafted or LLM-generated questions, but these methods rely heavily\non expert input or well-prompt design, which restricts their generalizability and\nability to generate discriminative questions across a wide range of tasks. To address\nthese challenges, we introduce CQG-MBQA (Contrastive Question Generation\nMulti-task Binary Question Answering), a general framework for producing\ninterpretable semantic text embeddings across diverse tasks. Our framework sys-\ntematically generates highly discriminative, low cognitive load yes/no questions\nthrough the CQG method and answers them efficiently with the MBQA model,\nresulting in interpretable embeddings in a cost-effective manner. We validate the\neffectiveness and interpretability of CQG-MBQA through extensive experiments\nand ablation studies, demonstrating that it delivers embedding quality comparable\nto many advanced black-box models while maintaining inherently interpretabil-\nity. Additionally, CQG-MBQA outperforms other interpretable text embedding\nmethods across various downstream tasks.", "sections": [{"title": "INTRODUCTION", "content": "Text embedding is a cornerstone of Natural Language Processing (NLP), transforming texts\u2014whether\nsentences, paragraphs, or full documents\u2014into embedding vectors that capture their semantic\nmeaning. In semantic embedding spaces, the similarity between texts is represented by the proximity\nof their embedding vectors, typically measured using distance measures like Euclidean distance,\ncosine distance, or inner product. The closer the vectors, the more semantically similar the texts. These\nembeddings are foundational to many downstream NLP tasks, including Semantic Textual Similarity\n(STS) (Agirre et al., 2012; 2013), Information Retrieval (Karpukhin et al., 2020; Thakur et al., 2021),\nClustering (Aggarwal & Zhai, 2012), and more recently, Retrieval Augmented Generation (RAG)\n(Lewis et al., 2020; Guu et al., 2020; Asai et al., 2024).\nBlack-box text embedding methods, such as Sentence-BERT (Reimers & Gurevych, 2019), SimCSE\n(Gao et al., 2021), WhitenedCSE (Zhuo et al., 2023), and AnglE (Li & Li, 2024), excel at generating\nhigh-quality embeddings by training on vast amounts of data. These models are highly effective at\ncapturing semantic similarities, making them indispensable for a variety of NLP tasks (Muennighoff\net al., 2023). However, their black-box nature leaves the embeddings opaque to human users. These\nmodels do not provide insight into why certain texts are deemed similar, which becomes problematic\nfor tasks that require transparency, especially in applications involving high-stakes decision-making,\nsuch as legal and medical domains, or in cases requiring explanations for regulatory compliance."}, {"title": "RELATED WORK", "content": "Text embedding is a fundamental NLP task that transforms texts into vector representations that\ncapture their semantic meanings. Generally, it can be classified into two types: black-box embedding\nand interpretable embedding.\nBlack-box Embedding. Early methods for text embedding, such as GloVe (Pennington et al., 2014)\nand Word2Vec (Mikolov et al., 2013), typically pool word embeddings to create low-dimensional\nsemantic representations. However, these methods, which rely on individual word embeddings, often\nfail to capture the full context of a text. For example, the sentences \"Most people in the world like\nApple.\" and \"Most people in the world do not like Apple.\" share high lexical overlap but have\nopposite meanings, highlighting the limitations of such methods, which struggle to capture deeper\nsemantic differences beyond surface-level word similarity.\nTo produce context-aware text embeddings, Universal Sentence Encoder (USE) (Cer et al., 2018)\nemploys a transformer model (Vaswani et al., 2017) trained on a combination of unsupervised tasks\nand supervised fine-tuning using the Stanford Natural Language Inference (SNLI) corpus (Bowman\net al., 2015). BERT (Devlin et al., 2019), a transformer network pre-trained on large-scale unlabeled\ntext, can generate sentence embeddings by pooling its output representations. Subsequent models have\nfurther refined BERT using contrastive learning and other semantic-related objectives. For instance,\nSentence-BERT (SBERT) (Reimers & Gurevych, 2019) pioneers the Siamese network structure\nfor Semantic Textual Similarity (STS), while SimCSE (Gao et al., 2021) develop a contrastive\nlearning framework for both unsupervised and supervised settings. WhitenedCSE (Zhuo et al., 2023)\nenhances embedding uniformity and alignment with shuffled group whitening, and AnglE (Li & Li,\n2024) optimizes angle differences to overcome cosine similarity limitations. Despite these advances,\nblack-box models produce embeddings that are opaque and difficult to interpret. In this work, we\ntarget generating interpretable dimensions for text embedding.\nInterpretable Embedding. The challenge of creating interpretable embeddings has been persisted,\nespecially with the rise of dense word embeddings. Early efforts focus on transforming word embed-\ndings to improve interpretability. Jha et al. (2018) apply categorical knowledge in the biomedical\ndomain to convert pre-trained embeddings into interpretable dimensions, while Senel et al. (2018)\nquantify interpretability by analyzing latent semantic structures. Models like SPINE (Subrama-\nnian et al., 2018) employ auto-encoders to create interpretable embeddings from non-interpretable\nones like GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013), and Word2Sense\n(Panigrahi et al., 2019) creates interpretable dimensions based on specific word senses.\nDespite progress, developing context-aware, interpretable dimensions remains difficult. Recent\nresearch has shifted towards indirectly understanding embedding spaces. For instance, Lee et al.\n(2022) introduce token pair contribution heatmaps to enhance interpretability in sentence similarity,\nwhile Simhi & Markovitch (2023) propose transforming embedding spaces into comprehensible\nconceptual representations. Recent advancements like ChiLL (McInerney et al., 2023) generate\ninterpretable binary features from health records by querying pre-trained LLMs with expert-crafted\nyes/no questions for patient classification. QAEmb (Benara et al., 2024) extends this by prompting\nLLMs to automatically generate questions using examples of texts and questions, demonstrating its\nefficacy in the fMRI prediction task. Inspired by QAEmb, we propose a cost-effective framework\nthat generates high-quality questions and uses them as interpretable dimensions for text embedding."}, {"title": "INTERPRETABLE TEXT EMBEDDING FRAMEWORK", "content": "We introduce an interpretable text embedding framework named CQG-MBQA, which uses questions\nas interpretable dimensions to represent text. By posing a set of carefully designed yes/no questions\nabout a given text, the answers form an interpretable embedding vector that captures the text's core\nsemantics. For instance, consider three questions: \"Is the article about AI?\u201d, \u201cIs the article about\nsports?", "Is the article about food?": "For the text \"Apple is a technology company.", "answers": ["yes\", \"no\", \"no\"], resulting in the embedding vector [1,0,0], which\nreflects the text's key features. Applying the same set of questions across all texts in a corpus produces\na consistent embedding matrix that encodes the semantic information of the entire dataset.\nAs shown in Figure 1, the CQG-MBQA framework consists of two phases: question generation\nand question answering. To generate high-quality, discriminative questions, we develop a method\ncalled Contrastive Question Generation (CQG), which harnesses pre-trained dense text embedding\nmodels and generative LLMs for question generation. Details of this method are outlined in Section\n3.1. Once the questions are generated, their corresponding answers form the text's embedding vector.\nYet, generating answers through LLMs at scale is both time-consuming and expensive. To address this,\nwe propose a Multi-task Binary Question Answering (MBQA) model. Trained with a multi-task\nbinary classification objective, this model can generate interpretable embeddings efficiently, requiring\nfar fewer LLM API calls. Further details on this model are provided in Section 3.2."]}, {"title": "QUESTION GENERATION", "content": "Motivations. To effectively represent texts using binary question answers, we need highly discrimi-\nnative questions that capture the subtle semantic differences between texts in the corpus. Existing"}, {"title": "Contrastive Question Generation (CQG)", "content": "The CQG method applies contrastive learning principles,\nusing positive, hard negative, and easy negative samples to guide LLMs in generating high-quality\nquestions. These questions are designed to effectively differentiate positive samples from negative\nones, especially hard negatives, which are semantically similar (Robinson et al., 2021). The goal is to\ngenerate questions that elicit a \"yes\" answer only for a specific group of texts while excluding others,\neven those that are closely related.\nConsider a toy example with four groups of texts for animals: $G_{1}$ = {Whale, Dolphin},\n$G_{2}$ = {Shark, Ray}, $G_{3}$ = {Giraffe, Deer}, and $G_{4}$ = {Eagle, Hawk}. The objective is to generate\nquestions that can effectively distinguish $G_{1}$ from other groups. At first, broad questions such as\n\u201cDoes it live in water?", "Is it a mammal?": "ight seem useful. While these questions correctly\nyield \"yes\" for Whale and Dolphin, they also apply to other groups. For example, Shark and Ray\nalso live in water, and Giraffe and Deer are also mammals. To better differentiate $G_{1}$, a more precise\nquestion would be \"Does the animal use echolocation to navigate and hunt?"}, {"title": "QUESTION ANSWERING", "content": "Motivations. Generating answers to questions using LLMs can be prohibitively expensive, especially\nwhen scaling up to large datasets with numerous questions. Table 1 presents the cost analysis for\nembedding the MS MARCO dataset (Bajaj et al., 2016), revealing that the expense of LLM-generated"}, {"title": "Multi-task Binary Question Answering (MBQA) Model", "content": "The MBQA model is designed to leverage\nLLM-generated answers from a smaller subset of texts to train a multi-task binary classification\nmodel. This model consists of a pre-trained encoding model and multiple classification heads. The\nencoder converts the input text into an embedding vector, while the classification heads predict binary\nscores for each question. Formally, the MBQA model $M$ is defined as:\n$M = (Enc, {C_{1}, C_{2},\u00b7\u00b7\u00b7,C_{m}}),$ (2)\nwhere $Enc : T \\rightarrow R^{d}$ represents the encoding model, and $C_{i} : R^{d} \\rightarrow [0, 1]$ is the $i$-th Multi-Layer\nPerceptron (MLP) classification head. For a given input text $t \\in T$, the MBQA model generates a\nbinary embedding vector $y = [y_{1}, y_{2}, \u2026\u2026\u2026, y_{m}] \\in {0,1}^{m}$ as follows:\n$e = Enc(t),$ (3)\n$y_{i} = 1[\\sigma(C_{i}(e)) > \\tau], for i \\in {1,2,\u2026\u2026,m},$ (4)\nwhere $\u03c3$ is the Sigmoid function, $1[]$ is the indicator function, and \u03c4 is the threshold for bi-\nnary classification. During training, the encoder Enc is frozen and only the classification heads\n${C_{1}, C_{2},..., C_{m}}$ are optimized using weighted Binary Cross-Entropy (BCE) Loss (Bishop, 2006)\non the available LLM-generated question-answer pairs.\nRemarks. The MBQA model achieves 96% accuracy in reproducing LLM-generated answers for\nCQG questions with just a single pass through the encoding model, substantially reducing costs\ncompared to running a pre-trained LLM for each text. Our model only requires training data from as\nfew as 1,000 articles per question, resulting in 10 million text-question pairs for 10,000 questions,\ncosting just 31 USD using GPT-40-mini. The training process takes 36 hours, and embedding the\nentire MS MARCO dev set requires 90 hours on a single GTX 1080 Ti, which is an inexpensive\nGPU. Consequently, encoding the same MS MARCO dev set with the MBQA model costs around\n41 USD-just 0.017% of the original cost with GPT-40-mini. This model allows us to scale up the\nnumber of questions (dimensions) efficiently, providing interpretable embeddings at a fraction of the\ncost of LLM-based answering. For more details on training and evaluation, see Appendix B."}, {"title": "EXPERIMENTS", "content": "In this section, we present a comprehensive evaluation of the CQG-MBQA framework by addressing\nfour essential questions aimed at understanding its performance and applicability:\n\u2022 Embedding Quality: How well does our framework generate high-quality interpretable\nembeddings comparable to advanced black-box models? (Section 4.3)\n\u2022 Interpretability: Does our framework improve the human interpretability of embeddings over\nexisting methods? (Section 4.4)"}, {"title": "METRICS", "content": "Embedding Quality Measurement. For evaluating embedding quality, we adopt the metrics that are\nwidely used in the MTEB benchmark (Muennighoff et al., 2023) for a comprehensive comparison. For\nSTS tasks, we use Spearman correlation (Spearman, 1904) on cosine similarity between embeddings\nas the evaluation metric. In retrieval tasks, we assess the performance using Normalized Discounted\nCumulative Gain at Top 10 (nDCG@10) (Wang et al., 2013). For clustering tasks, we evaluate the\nresults using V-Measure (Rosenberg & Hirschberg, 2007).\nInterpretability Measurement. Since both STS and retrieval tasks measure pairwise text similarity\nusing cosine similarity, we focus on interpreting the cosine similarity scores produced by CQG-\nMBQA. With inherently interpretable dimensions, we can offer insights to users by highlighting the\ndimensions that contribute most to the similarity between two texts. Building on COGAM (Abdul\net al., 2020), we suggest that interpretability should account for the cognitive load imposed on users.\nIn COGAM, cognitive load is assessed by counting the number of visual cognitive chunks. Similarly,\nwe measure it by the number of questions a user must consider to understand the similarity between\ntwo texts, corresponding to the dimensions where both embedding vectors have a value of 1. Formally,\nfor any two binary embedding vectors $u = [u_{1}, u_{2},\u2026\u2026, u_{m}]$ and $v = [v_{1}, v_{2}, \u00b7\u00b7\u00b7, v_{m}]$, the cognitive\nload is defined as the inner product of u and v:\n$cognitive load = \\langle u, v \\rangle = \\sum_{i=1}^{m} u_{i}v_{i}.$ (5)\nQuantifying cognitive load allows us to assess the interpretability of our CQG-MBQA framework's\nembeddings. A lower value indicates that fewer dimensions are involved, making the interpretation\neasier to understand, thus enhancing both interpretability and user-friendliness."}, {"title": "MODELS", "content": "Interpretable Models. We evaluate CQG-MBQA against existing interpretable baselines to provide\na thorough comparison. The implementation details of CQG-MBQA are provided in Appendix D.1."}, {"title": "EMBEDDING QUALITY", "content": "Tables 2-4 present the embedding quality results for the STS, retrieval, and clustering tasks, highlight-\ning CQG-MBQA's competitive performance. In the STS tasks (Table 2), CQG-MBQA achieves\ncomparable embedding quality to advanced dense embedding models like SimCSE and SBERT\n(New) while preserving inherent interpretability. It outperforms earlier methods such as GloVe, USE,\nand BERT, as well as all interpretable baselines. For retrieval tasks (Table 3), CQG-MBQA surpasses"}, {"title": "INTERPRETABILITY", "content": "Cognitive Load. Table 5 displays the cognitive load required to interpret embeddings across different\ninterpretable models, evaluated through the STS task, which computes pairwise similarity of texts.\nThis provides a clear measure of how much effort is needed to understand the embeddings. CQG-\nMBQA achieves a 2.5~3.6\u00d7 lower cognitive load than QAEmb-MBQA, indicating that the CQG\nmethod significantly enhances interpretability. This technique produces more \u201cno\u201d answers and fewer\n\"yes\" answers, making the embeddings easier to interpret. For Bag-of-Tokens, it has a much lower\ncognitive load due to its lexical nature, but this advantage comes at the cost of significantly reduced\nembedding quality. The trade-off between interpretability and embedding quality can be adjusted by\ntuning the binary classification threshold \u03c4, as further discussed in Section 4.6.\nCase Study. Figure 3 showcases a pair of texts from our training corpus, focusing on nine specific\nquestions (dimensions) where at least one text yield a \"yes\" answer. This illustrates how CQG-\nMBQA generates relevant and insightful dimensions. For instance, question ID 1153, which asks\nif the text is related to personal health or well-being, receives a \"yes\" for both Text A and Text\nB, accurately reflecting their shared focus on health topics. Similarly, question ID 4634 inquires\nwhether the text aligns with scientific standards, and both texts-discussing evidence-based findings\non substance effects-obtain \"yes\" answers, showcasing the relevance of generated questions.\nDespite the texts' similarity, CQG-MBQA captures subtle semantic differences through fine-grained\nquestions. For example, question ID 3400 asks if the research targets neurological conditions. Text A,\nwhich discusses cognitive function, receives a \"yes\", indicating a connection to neurological condi-\ntions, whereas Text B, focusing on blood pressure and insulin sensitivity, acquires a \"no\", highlighting\na clear distinction in their subject matter. This case study highlights how CQG generates interpretable,"}, {"title": "EFFECT ON NUMBER OF QUESTIONS", "content": "We explore how varying the number of questions (dimensions) $m$ impacts the quality and inter-\npretability of embeddings produced by CQG-MBQA. To adjust $m$, we reduce the output length of\nthe final binary embedding vector. Figure 4 illustrates the relationship among embedding quality,\ninterpretability, and $m$. As $m$ grows, the Spearman correlation increases and stabilizes around 3,000\ndimensions, indicating better embedding quality. This highlights the need for an efficient QA model\nto manage computational costs while scaling up dimensions for optimal embedding quality.\nHowever, a trade-off arises: as $m$ increases, cognitive load increases, and the interpretability declines\ndue to a higher proportion of \"yes\" in the embeddings. This inverse relationship between embedding\nquality and interpretability emphasizes the importance of balancing dimensions based on the task's\nrequirements. Figure 4 also demonstrates the effectiveness of the CQG algorithm in generating\nhigh-quality, discriminative embeddings with approximately 3,000 questions, achieving a balance\nbetween embedding quality and interpretability without an excessive number of questions."}, {"title": "TRADE-OFF BETWEEN EMBEDDING QUALITY AND INTERPRETABILITY", "content": "To further investigate the balance between embedding quality and interpretability, we vary the binary\nclassification threshold \u03c4 that determines the final binary embedding vector. Figure 5 depicts a clear\ntrade-off between embedding quality and interpretability. Increasing \u03c4 improves the interpretability,\nbut this comes at the cost of reduced embedding quality, as the Spearman correlation decreases. This\nis due to fewer active dimensions, leading to reduced values in the cognitive load, which are easier to\ninterpret but may lose subtle semantic distinctions.\nMore importantly, this trade-off presents an opportunity for user-driven customization. Depending\non different scenarios, users of our framework can dynamically tune the desired \u03c4 based on the\ncognitive load to meet their needs. For instance, in scenarios requiring rapid decision-making or\nwhere cognitive resources are constrained, users might prioritize interpretability by opting for a\nhigher threshold. On the other hand, in situations where nuanced analysis is crucial and resources are"}, {"title": "CONCLUSION", "content": "In this paper, we introduce CQG-MBQA, a novel general framework for generating interpretable\nsemantic text embeddings by systematically creating binary questions and using the answers as\ninterpretable embedding dimensions. Our CQG method effectively addresses the challenges of\ngeneralizability and quality issues during the question generation phase, while the MBQA model\nprovides an efficient, scalable solution for answering these questions, significantly reducing costs.\nThrough extensive experiments on STS, retrieval, and clustering tasks, we demonstrate that our\nframework delivers performance comparable to advanced black-box models while being inherently\ninterpretable. Moreover, CQG-MBQA consistently outperforms other interpretable text embedding\nmodels across various downstream tasks, further validating its effectiveness."}, {"title": "A.1 PROMPT: CONTRASTIVE QUESTION GENERATION", "content": "We present the prompt template used for the Contrastive Question Generation (CQG) algorithm. The\ninput to the prompt template consists of two lists of texts: positive_examples and negative_examples.\nThe LLM is explicitly instructed to generate questions that yield \"yes\" answers for the positive\nexamples and \u201cno\u201d answers for the negative examples. To enhance the quality of the generated\nquestions and the potential generalizability to new texts, we prompt the LLM to avoid complex\nsentence structures. Initial experiments revealed that if we do not require the LLM to avoid complex\nsentence structures, the LLM tended to create discriminative questions by simply combining two\nconditions to ensure \"yes\" answers for the positives, rather than identifying deeper relationships\nbetween the positives and negatives. Additionally, we include formatting instructions at the end of\nthe prompt template to improve result parsing accuracy.\nGenerate 10 simple yet insightful yes/no questions that determine the properties of an article,\nwhere for all questions, the answer will be \"yes\" for ALL the positive articles and \u201cno\u201d\nfor ALL the negative articles. Keep questions concise and avoid using complex sentence\nstructures with \"and\" or \"or\" unless necessary.\nPositive Articles:\nPositive {i}. {positive_example_i}\nNegative Articles:\nNegative {i}. {negative_example_i}\nInstruction: Based on the excerpts provided, generate 10 simple yet insightful yes/no\nquestions that can accurately differentiate the positive articles from the negative articles. Each\nquestion should be concise and framed in such a way that it will elicit a \"yes\" response for\nALL positive articles and a \u201cno\u201d response for ALL negative articles. Avoid using complex\nsentence structures with \u201cand\u201d or \u201cor\u201d unless absolutely necessary. Format the questions in a\nnumbered list as shown below:\n1. First simple yes/no question\n2. Second simple yes/no question"}, {"title": "A.2 PROMPT: QAEMB QUESTION GENERATION", "content": "The Question Generation Prompt #5 used in the QAEmb paper (Benara et al., 2024) is the most\ngeneral form prompt in the list, making it suitable for adopting it to generate questions for general-\npurpose text embedding. It uses two lists of texts as input: example narrative sentences and example\nyes/no questions. The original prompt instructs the LLM to \"Generate a bulleted list of 100 specific,\nnon-overlapping yes/no questions that ask about aspects of the example narrative sentences that are\nimportant for classifying them.\" This was originally designed for the task of fMRI prediction with\nnarrative sentences.\nFollowing this approach, we designed a prompt template for experiments on QAEmb question\ngeneration, also using two lists of texts as inputs: reference_articles and example_questions. The\nexample questions are sourced from the original QAEmb paper, while the reference articles are\nrandomly drawn from the training dataset."}, {"title": "A.3 PROMPT: MULTI-TASK BINARY QUESTION ANSWERING", "content": "This section details the prompt template used to generate LLM answers for training the Multi-task\nBinary Question Answering (MBQA) model. The prompt takes two inputs: the text_chunk, which is\nthe training article sample, and a list of questions to be answered. To optimize token usage for the\narticle sample and instructions, we group up to 20 questions in a single prompt.\nEvaluate the following text chunk based on the yes/no questions provided.\nText Chunk:\n{text_chunk}\nQuestions:\ni. {question_i}\nInstruction for the model: Please read the provided text chunk and answer each of the\nquestions with either \"yes\" or \"no\". Format the responses as follows:\n1. yes/no\n2. yes/no"}, {"title": "B TRAINING AND EVALUATION OF THE MBQA MODEL", "content": "To ensure that the MBQA model produces faithful answers to the questions, we evaluate its question-\nanswering performance on a 10% held-out document set that was not used for training.\nData Collection. For each question generated in the previous question generation step, we randomly\nsample 500 in-cluster samples, 300 neighboring cluster samples from 5 nearest clusters, and 200\nrandom samples from the entire corpus. We use the pre-trained LLM (GPT-40-mini) to generate\nanswers for each question across these samples. The LLM-generated answers are batched in groups\nof 20 questions to train the multi-task binary classification model. Refer to Appendix A.3 for the\nprompt used to collect answers. This approach allows us to gather data from a larger number of text\nsamples, thereby increasing the generalizability of our model.\nTraining. We train the MBQA model using the Adam optimizer with a learning rate \u03b1 of 1e-4\nand a batch size of one text sample. For each step, we calculate the loss based on all available\nquestions with answers from the previous data collection phase. The model is trained using the\nBCEWithLogitsLoss function, where the weight is the ratio of \"yes\" answers to \"no\" answers in the\ntraining data.\u00b9 Specifically, for CQG-MBQA, we set this weight to 7.5127, and for QAEmb-MBQA,\nthe weight is set to 4.9608. The model is trained for 3 million steps, at which point performance\nbegins to converge."}, {"title": "D IMPLEMENTATION DETAILS", "content": "D.1 OUR CQG-MBQA MODEL\nIn the experiments, we train the proposed CQG-MBQA framework using the MEDI2 dataset\n(Muennighoff et al., 2024). Detailed information about the model configuration and hyperparameters\nused in our framework is provided in Table 8.\nData Pre-processing. We use the MEDI2 dataset, downloaded from the HuggingFace repository at\nGritLM/MEDI2.4 We filter out files starting with task, as they are unsuitable for the training corpus.\nFrom the remaining files, we extract both positive and negative instances of each data line. Since\nthe MEDI2 dataset contains instructions for queries and documents, we remove the instruction part,"}]}