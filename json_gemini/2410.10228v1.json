{"title": "QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation", "authors": ["Gahyun Yoo", "Jay Yoon Lee"], "abstract": "Reinforcement learning has shown great promise in aligning language models with human preferences in a variety of text generation tasks, including machine translation. For translation tasks, rewards can easily be obtained from quality estimation (QE) models which can generate rewards for unlabeled data. Despite its usefulness, reinforcement learning cannot exploit the gradients with respect to the QE score. We propose QE-EBM, a method of employing quality estimators as trainable loss networks that can directly backpropagate to the NMT model. We examine our method on several low and high resource target languages with English as the source language. QE-EBM outperforms strong baselines such as REINFORCE and proximal policy optimization (PPO) as well as supervised fine-tuning for all target languages, especially low-resource target languages. Most notably, for English-to-Mongolian translation, our method achieves improvements of 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET relative to the supervised baseline.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning with human feedback (RLHF) has been used to successfully align language models with human preferences. By assigning a high reward to responses that exhibit certain characteristics, such as lack of harmful content, we can steer the model towards generating these types of responses in various types of text generation tasks. For machine translation, reinforcement learning has been used to improve general translation quality by using quality estimation (QE) models trained on human feedback as the reward model (He et al., 2024; Ramamurthy et al., 2023; Ramos et al., 2023).\nQuality estimation is the task of assigning a translation score to a provided source and target-side prediction pair, without referring to a gold reference."}, {"title": "2 Related Work", "content": "Existing research on using energy based models for text generation has formulated energy models as residual energy based models (Deng et al., 2020; He et al., 2021). Low energy samples are drawn in two steps: sampling many generations from a frozen language model, then importance sampling through the energy model. Similarly, Bhattacharyya et al. (2021) proposes training an energy based model with a margin based loss proportional to the difference in BLEU and using it to rerank generations during inference.\nOur method has an advantage over these works in inference speed and distillation. These residual EBM methods introduces latency during inference, and do not pass the energy model's knowledge to the base language model. In contrast, we distill the knowledge of the energy model to the base NMT model during training so that we can achieve improvements even with simple greedy generation. QE-EBM is also orthogonal to these methods in that we can also use the energy network to rerank samples during inference, although we report results of greedy generation for the evaluation sets, since our method does not require sampling from the energy model during inference."}, {"title": "2.1 Structured Energy Network as a Loss (SEAL)", "content": "SEAL (Lee et al., 2022) was proposed as a training framework to use energy networks as trainable loss functions for structured prediction. There are two versions of the algorithm: SEAL-STATIC and SEAL-DYNAMIC. In both versions, the task net, the neural network responsible for performing a task (i.e. prediction), is trained through a weighted sum of cross-entropy loss and energy loss. The difference lies in whether the loss net, the neural network that acts as the loss function, is updated. In SEAL-DYNAMIC, the loss net is fine-tuned with contrastive loss before each task net update so that it is better suited to predicting the energy landscape for the data samples at each step. In SEAL-STATIC, the weights of the loss net remain fixed. SEAL-DYNAMIC outperforms SPENs (Structured Prediction Energy Networks), which refers to using energy models as inference networks, as well as SEAL-STATIC in image segmentation and semantic role labeling.\nOur work can be viewed as an adaptation of SEAL to the task of translation, retaining the energy loss and iterative update algorithm while adding the following contributions: 1) we extend it to semi-supervised learning by incorporating unlabeled data, and 2) as the energy loss net, we plug in a quality estimation model which has already been fine-tuned with human annotations of translation pairs. This guides the model towards human preferences with minimal additional training and no extra data about human preference."}, {"title": "2.2 Energy Based Models for Text Generation", "content": "Existing research on using energy based models for text generation has formulated energy models as residual energy based models (Deng et al., 2020; He et al., 2021). Low energy samples are drawn in two steps: sampling many generations from a frozen language model, then importance sampling through the energy model. Similarly, Bhattacharyya et al. (2021) proposes training an energy based model with a margin based loss proportional to the difference in BLEU and using it to rerank generations during inference.\nOur method has an advantage over these works in inference speed and distillation. These residual EBM methods introduces latency during inference, and do not pass the energy model's knowledge to the base language model. In contrast, we distill the knowledge of the energy model to the base NMT model during training so that we can achieve improvements even with simple greedy generation. QE-EBM is also orthogonal to these methods in that we can also use the energy network to rerank samples during inference, although we report results of greedy generation for the evaluation sets, since our method does not require sampling from the energy model during inference."}, {"title": "2.3 Using Quality Estimators in NMT Training", "content": "With the rapid development of quality estimation models, a number of recent works have incorporated quality estimators into the training of translation models. Ramos et al. (2023) improves translation abilities of T5-based models by applying PPO with COMET-QE as the reward model, finding that filtering training data based on the reward model is important. Using COMET-QE also, He et al. (2024) improves the translation capabilities of Llama and NLLB through RAFT (Reward Ranked Fine-tuning), which involves generating multiple candidates, ranking them with the reward model, and learning from the best sample (Dong et al., 2023; Touvron et al., 2023; Team et al., 2022).\nGulcehre et al. (2023) introduces an efficient grow-batch offline reinforcement learning algorithm consisting of alternating Grow and Improve steps. During the Grow step, samples are generated offline, and are added to the supervised batch after applying a filter based on the reward model. During the Improve step, the translation model is fine-tuned on the grown batch."}, {"title": "3 Method", "content": "A neural machine translation system models the conditional distribution $P(y|x)$ of a target sentence y given a source sentence x. NMT models are trained to maximize this probability by minimizing the cross-entropy loss function,\n$LCE(x, y) = - \\sum_{t=1}^{T_y} logP_\\$(y_t|x)$\nEnergy-based models (EBMs) are parameterized models that output a scalar value for each input (Lecun et al., 2006). In this paper, we investigate the use of quality estimators as EBMs, specifically COMET-KIWI (Rei et al., 2022). Since COMET-KIWI outputs a scalar value that represents the quality of translation for each pair of source and target sentence, the model can be represented as an energy function $E_\\theta(x,y) = \u2212s(x, y)$ where s is the COMET-KIWI score.\nTo leverage the power of EBMs during training, we train the NMT model in a multi-task setup, where the loss consists of a standard cross-entropy loss and the energy term (called energy loss hereafter). For each batch consisting of $B_l$ labeled samples ${(x_i, y_i)}_{i=1}^{B_l}$ and $B_u$ unlabeled samples ${x_i}_{i=1}^{B_u}$ with K translations ${y_i^{(j)}}_{j=1}^K$ sampled from the NMT model for each unlabeled source sentence $x_{u_i}$, the loss for the NMT model can be expressed as\n$L_{NMT} = \\alpha \\cdot \\frac{1}{B_l} \\sum_{i=1}^{B_l} L_{CE} (x_i, y_i) + \\beta \\cdot \\frac{1}{B_u} \\sum_{i=1}^{B_u} \\frac{1}{K}\\sum_{j=1}^K E_\\theta (x_{u_i}, y_i^{(j)})$\na is a hyper-parameter that controls the magnitude of the cross entropy gradient, and is decreased throughout training. \u1e9e is a hyper-parameter that controls the magnitude of the energy loss gradient, and is increased throughout training. More details can be found in Appendix A.\nTo update the NMT model with the energy loss, we replace the regular softmax operation for the output logits from the last hidden layer of the NMT model's decoder with the straight through estimator (STE) (Bengio et al., 2013), following Tu et al. (2020) which demonstrated success using STE to distill knowledge from an autoregressive energy network to a non-autoregressive inference network. The exact algorithm for QE-DYNAMIC is given in Algorithm1. The energy network's parameters 0 are updated using the labeled batch before each update of the NMT model's parameters $. The NCE loss used to train the energy network in QEDYNAMIC is given in Equation (3),\n$L_{E-NCE} = -\\frac{1}{B_l}\\sum_{i=1}^{B_l} [log\\sigma (s(x_i, y_i))+ \\sum_{j=1}^{N}log(1 - \\sigma((x_i, y_{p_i}^{(j)})))]$\nwhere $s(x, y) = \u2212E_\\theta(x, y) \u2013 logP_\\$(y|x), and $y_{p_i}^{(j)}~P_\\$(x)$. \u03c3is the sigmoid function.\nThrough this loss, the energy model learns to prefer the gold labels over the NMT model generations.\nTo improve the performance of QE-STATIC and QE-DYNAMIC, we apply two data sampling techniques to select labeled and unlabeled training batches.\n\u2022 Filter: Labeled data is filtered based on COMET-KIWI score, before training begins. A labeled pair with higher COMET-KIWI score consists of a source and target sentence with higher alignment, and is likely to provide more accurate guidance for the generation model.\n\u2022 NN: We select the unlabeled batch to be paired with each labeled batch by retrieving the nearest neighbor based on embedding cosine similarity between the source sentences. This is expected to reduce the mismatch in gradient direction for the model parameters that could occur as a result of training simultaneously with labeled and unlabeled data."}, {"title": "4 Experiments", "content": "For the NMT model, we use MBART, a Transformer encoder-decoder model with roughly 610M parameters (Tang et al., 2021; Vaswani et al., 2017). We use the weights that are pretrained only on monolingual data and not finetuned on any parallel data, allowing us to compare regular supervised finetuning and energy-based training.\nThe COMET-KIWI quality estimation model consists of a feedforward estimator on top of a cross-lingual encoder. The weights of the COMET-KIWI encoder were initialized with the InfoXLM weights and finetuned along with the feed-forward estimator on translations paired with human ratings for the WMT22 Metrics Task (Chi et al., 2021). COMET-KIWI ranked first among the reference-free metrics, and seventh among all the metrics in terms of correlation with human evaluation (Zerva et al., 2022).\nWe use the IWSLT2017 English-{German, Chinese} and ML50 English-{Bengali, Azerbaijani, Mongolian, Marathi, Kazakh} translation datasets (Cettolo et al., 2017; Tang et al., 2020). In this paper, we focus on only English to X directions. We filter the training sets by total size and sentence length. For high resource language pairs, we select 50K sentence pairs randomly, and filter out sentences with more than 50 sub-word tokens in either the source sentence or reference translation. For low-resource language pairs, we only apply length-based filtering. We will refer to this processed training set as preprocessed data pool for convenience. We construct the labeled subset by randomly selecting 1/5 of the sentence pairs (source + target sentence) in the preprocessed data pool. For the unlabeled subset, we use all of the source sentences in the preprocessed data pool.\nTo the best of our knowledge, there are no previous works that use quality estimators as trainable loss functions to train translation models. Therefore, in addition to the supervised baseline, we compare our method against REINFORCE and PPO as these reinforcement learning methods can also utilize quality estimators as a reward. Both RL baselines are trained in a multi-task setup as in QE-EBM, with the same cross-entropy loss for the labeled batch and a separate loss using the QE score for the unlabeled batch. The REINFORCE baseline uses the vanilla policy gradient. The reward is normalized through (1) scaling the reward between the running max and running min of the current epoch and (2) subtracting the running average of the current epoch. The PPO implementation is taken from TRL (von Werra et al., 2020). For a fair comparison across different approaches, we ensured an identical number of samples per source sentence in each learning algorithm.\nWe use an ADAM optimizer to update the energy model parameters and NMT model parameters (Kingma and Ba, 2017). In QE-DYNAMIC, for both the NMT model and QE model, we train"}, {"title": "4.1 Models", "content": "For the NMT model, we use MBART, a Transformer encoder-decoder model with roughly 610M parameters (Tang et al., 2021; Vaswani et al., 2017). We use the weights that are pretrained only on monolingual data and not finetuned on any parallel data, allowing us to compare regular supervised finetuning and energy-based training.\nThe COMET-KIWI quality estimation model consists of a feedforward estimator on top of a cross-lingual encoder. The weights of the COMET-KIWI encoder were initialized with the InfoXLM weights and finetuned along with the feed-forward estimator on translations paired with human ratings for the WMT22 Metrics Task (Chi"}, {"title": "4.2 Datasets", "content": "We use the IWSLT2017 English-{German, Chinese} and ML50 English-{Bengali, Azerbaijani, Mongolian, Marathi, Kazakh} translation datasets (Cettolo et al., 2017; Tang et al., 2020). In this paper, we focus on only English to X directions. We filter the training sets by total size and sentence length. For high resource language pairs, we select 50K sentence pairs randomly, and filter out sentences with more than 50 sub-word tokens in either the source sentence or reference translation. For low-resource language pairs, we only apply length-based filtering. We will refer to this processed training set as preprocessed data pool for convenience. We construct the labeled subset by randomly selecting 1/5 of the sentence pairs (source + target sentence) in the preprocessed data pool. For the unlabeled subset, we use all of the source sentences in the preprocessed data pool."}, {"title": "4.3 Baselines", "content": "To the best of our knowledge, there are no previous works that use quality estimators as trainable loss functions to train translation models. Therefore, in addition to the supervised baseline, we compare our method against REINFORCE and PPO as these reinforcement learning methods can also utilize quality estimators as a reward. Both RL baselines are trained in a multi-task setup as in QE-EBM, with the same cross-entropy loss for the labeled batch and a separate loss using the QE score for the unlabeled batch. The REINFORCE baseline uses the vanilla policy gradient. The reward is normalized through (1) scaling the reward between the running max and running min of the current epoch and (2) subtracting the running average of the current epoch. The PPO implementation is taken from TRL (von Werra et al., 2020). For a fair comparison across different approaches, we ensured an identical number of samples per source sentence in each learning algorithm."}, {"title": "4.4 Training and Evaluation", "content": "We use an ADAM optimizer to update the energy model parameters and NMT model parameters (Kingma and Ba, 2017). In QE-DYNAMIC, for both the NMT model and QE model, we train"}, {"title": "4.5 Results", "content": "We report results for QE-STATIC and QE-DYNAMIC along with the supervised baseline for low resource target languages (Bengali, Azerbaijani, Mongolian, Marathi, Kazakh) in Table 1. While the better-performing method varies among the two QE- variants, these methods exhibit consistent improvements over the baseline. For Mongolian, our method achieves improvements of 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET relative to the supervised baseline.\nWe report results for QE-STATIC and QE-DYNAMIC along with the supervised baseline for high resource target languages (German and Chinese) in Table 2. For both languages, QE-DYNAMIC outperforms QE-Static and the supervised baseline in all four metrics. The amount of improvement over supervised fine-tuning is larger for lower resource languages."}, {"title": "5 Analysis", "content": "In this section, we first present more comprehensive experimental results for four languages (Bengali, Marathi, German, Chinese) in Tables 3, 4 and 5. In addition to the supervised baseline and EBM methods with both data filtering and retrieval, we report the results of reinforcement learning baselines and ablation experiments. Different types of ablations were examined for each algorithm. For the REINFORCE and PPO baselines, we tried training without any additional monolingual data, using the labeled data with different shuffling as the unlabeled data (-Mono). (+Mono) refers to the original setup of using the whole preprocessed data pool as the unlabeled data. For QE-DYNAMIC and QE-STATIC, we ran three types of ablation experiments, training without additional monolingual data (-Mono), training with only unlabeled batch retrieval (NN) or labeled batch filtering (FILTER), and training with neither NN nor FILTER."}, {"title": "5.1 Reinforcement Learning vs Energy-based Training", "content": "EBM methods outperform reinforcement learning methods in all metrics. We hypothesize that this superiority arises from the difference in the granularity of information the QE score holds in each algorithm. RL methods treat the score given by the quality estimation model as a simple scalar value, while EBM methods treat it as a loss that can backpropagate to update the translation model's parameters. For REINFORCE or PPO, a reward is given per sequence, meaning that it assigns the same reward for all the tokens in the same sequence. It also holds no information other than the quality of the translation pair relative to other translation pairs. Meanwhile, in the case of energy-based methods,"}, {"title": "5.2 QE-STATIC vs QE-DYNAMIC: Fine-tuning the QE model", "content": "Table 5 shows that on average, QE-DYNAMIC has higher scores compared to QE-STATIC. In QE-STATIC, It is possible for the base task model, such as our base translation model, to learn to optimize a static reward model in a way that does not improve the actual task performance.\u00b9 In QE-DYNAMIC,"}, {"title": "5.3 Effect of Monolingual Data", "content": "When neither FILTER nor NN is applied to QE-STATIC or QE-DYNAMIC, using monolingual data (+Mono) performs worse than not using it (-Mono). With the help of the data techniques, however, adding monolingual data surpasses not using it. This demonstrates that our methods can exploit a large amount of monolingual data in cases where parallel data is scarce to attain higher quality translation, but require tactics that help reduce the discrepancy between heterogeneous labeled and unlabeled data and ensure that the quality of parallel data is above a certain level."}, {"title": "5.4 Effect of Data Sampling Techniques", "content": "We investigate how the two data sampling techniques, labeled data filtering and adjacent unlabeled batch retrieval, contribute to the improvement shown to occur in conditional text generation (Pang et al., 2023)."}, {"title": "6 Conclusion", "content": "We propose a new method of improving neural machine translation by using quality estimators as trainable energy loss networks. Our method performs better than supervised fine-tuning and reinforcement learning baselines in both high and lowresource language directions, especially showing greater improvements for low-resource languages. Based on our experiments, the best performance is expected when applying QE-DYNAMIC with labeled data filtering and retrieval of nearest unlabeled batches."}, {"title": "7 Limitations", "content": "Our proposed training scheme has several limitations. First, although it reduces latency during inference compared to other methods such as energybased reranking, the joint update requires a large amount of computation and memory during training, since we need to calculate and store gradients for both models. Second, it is difficult to use the quality estimation model as an energy model as is when there is a vocabulary mismatch between the energy model and the NMT model. This problem may be solved by adapting the quality estimator to operate in latent space, which is a potential future research direction."}, {"title": "A Hyperparameters", "content": ""}, {"title": "B Experimental Details", "content": "\u2022 When retrieving the nearest unlabeled batch for each labeled batch, the embeddings used to calculate cosine similarity are taken from the \"all-mpnet-base-v2\" checkpoint in the SBERT library (Reimers and Gurevych, 2019).\n\u2022 At the start of training, MBART weights are initialized to the \"facebook/mbart-large-50\" checkpoint from Huggingface (Wolf et al., 2020).\n\u2022 At the start of training, the QE model's weights are initialized to the \"Unbabel/wmt22-cometkiwi-da\" checkpoint from Huggingface (Wolf et al., 2020).\n\u2022 Adapter implementations are taken from AdapterHub (Pfeiffer et al., 2020)."}]}