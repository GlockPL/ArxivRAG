{"title": "Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation", "authors": ["Ilhoon Yoon", "Hyeongjun Kwon", "Jin Kim", "Junyoung Park", "Hyunsung Jang", "Kwanghoon Sohn"], "abstract": "Source-Free domain adaptive Object Detection (SFOD) is a promising strategy for deploying trained detectors to new, unlabeled domains without accessing source data, addressing significant concerns around data privacy and efficiency. Most SFOD methods leverage a Mean-Teacher (MT) self-training paradigm relying heavily on High-confidence Pseudo Labels (HPL). However, these HPL often overlook small instances that undergo significant appearance changes with domain shifts. Additionally, HPL ignore instances with low confidence due to the scarcity of training samples, resulting in biased adaptation toward familiar instances from the source domain. To address this limitation, we introduce the Low-confidence Pseudo Label Distillation (LPLD) loss within the Mean-Teacher based SFOD framework. This novel approach is designed to leverage the proposals from Region Proposal Network (RPN), which potentially encompasses hard-to-detect objects in unfamiliar domains. Initially, we extract HPL using a standard pseudo-labeling technique and mine a set of Low-confidence Pseudo Labels (LPL) from proposals generated by RPN, leaving those that do not overlap significantly with HPL. These LPL are further refined by leveraging class-relation information and reducing the effect of inherent noise for the LPLD loss calculation. Furthermore, we use feature distance to adaptively weight the LPLD loss to focus on LPL containing a larger foreground area. Our method outperforms previous SFOD methods on four cross-domain object detection benchmarks. Extensive experiments demonstrate that our LPLD loss leads to effective adaptation by reducing false negatives and facilitating the use of domain-invariant knowledge from the source model.", "sections": [{"title": "1 Introduction", "content": "Object detection is a crucial task for advancing real-world applications like autonomous driving and robotics."}, {"title": "3 Preliminaries", "content": "Problem statement Let $D_s = \\{x_i, Y_i\\}_{i=1}^{N_s}$ represent the labeled source domain dataset, where $x_i$ denotes the $i$th image, and $Y_i$ is its corresponding label set containing object locations and class assignments, and $D_T = \\{x_i\\}_{i=1}^{N_T}$ denotes the target domain images. $N_s$ and $N_T$ denote the number of the source and target domain images, respectively. When deploying a model with source pre-trained parameters $\\Theta_{pre}$ to an unseen domain, it is often challenging to access not only the target domain label but also the source dataset. Thus, the goal of Source-Free domain adaptive Object Detection (SFOD) is to adapt the source model to the target domain without the aid of any source data $D_s$, utilizing only the pre-trained source model parameters $\\Theta_{pre}$ and unlabeled target data $D_T$.\nMean-Teacher based self-training framework Most of the recent advanced SFOD methods follow the Mean-Teacher (MT) self-training paradigm. Generally,"}, {"title": "4 Proposed method", "content": ""}, {"title": "4.1 Motivation and Overview", "content": "While High-confidence Pseudo Labels (HPL) serve as reliable supervision for adaptation based on a high-confidence threshold, they are biased to overly confident instances. We argue that solely leveraging HPL in SFOD methods restricts their representation capability within easy positive instances, limiting adaptation performance for the target domain.\nTo tackle this challenge, we propose Low-confidence Pseudo Label Distillation (LPLD) to identify hard positive instances and effectively learn their target domain representations, complementing HPL. In particular, we first extract HPL via conventional pseudo-labeling algorithms [30,51]. Next, we exclude largely"}, {"title": "4.2 Low-confidence Pseudo Label Distillation", "content": "In this section, we elaborate on how our LPLD works to address the biased learning problem of the HPL-based method. It is composed of three main processes: 1) Extracting High-confidence Pseudo Labels to find easy positive instances, 2) Mining Low-confidence Pseudo Labels to identify missed hard positive instances, and 3) Low-confidence Pseudo Label Distillation loss to stably improve the network's understanding of hard-positive instances.\nExtracting High-confidence Pseudo Labels For each target image $x_i$, we first obtain the proposal set $P_i$ from the Region Proposal Network (RPN) of the teacher detector. Then, we employ a standard filtering process to the proposal set, including background score removal, score filtering, Non-Maximum Suppression (NMS) to obtain $P_i$. Then, we can obtain the HPL set $\\hat{Y}_i$ by thresholding on confidence score as follows:\n$\\hat{Y}_i = \\{(p_{i,j}, \\hat{c}_{i,j})|p_{i,j} \\in P_i, max(\\hat{c}_{i,j}) > \\delta_{hc}\\},$ (2)\nwhere $j$ is proposal index, $\\delta_{hc}$ is the threshold of HPL, and $max(\\hat{c}_{i,j})$ is the maximum class score from the class score vector $\\hat{c}_{i,j}$ of the filtered proposal. Note that we can get HPL with high precision due to the high value of $\\delta_{hc}$. The bounding boxes and class predictions of HPL are used to supervise the student detector with the regression loss $L_{reg}$ and classification loss $L_{cls}$.\nMining Low-confidence Pseudo Labels To complement the HPL set with low-confidence proposals, we construct Low-confidence Pseudo Labels (LPL) set to capture hard positive instances.\nWe first select the proposals that do not significantly overlap with HPL by thresholding on IoU (e.g. less than 0.4) between the overall proposals and the HPL set. Then we can get the candidate set of LPL $P_i'$ as follows:\n$P_i' = \\{p_{i,j}|p_{i,j} \\in P_i, p_{i,k} \\in \\hat{Y}_i, IoU(p_{i,j}, p_{i,k}) < \\delta_{IoU}\\},$ (3)\nwhere $\\delta_{IoU}$ is the overlapping IoU threshold, and $p_{i,k}$ is the bounding box for $k$th pseudo label in $\\hat{Y}_i$. For a given candidate set $P_i'$ for LPL, a background confidence"}, {"title": "4.3 Adaptive weights for Distillation", "content": "We observe a positive correlation between the feature distance of the student and the teacher in the same region and the IoU with the ground-truth, as depicted in Fig. 4. Therefore, we further improve LPLD loss by leveraging the feature distance. Specifically, we utilize cosine distance between student and teacher feature for each LPL as adaptive weights $a_j$ to the KL divergence loss, enabling the network to prioritize learning on more object-dominated boxes rather than background and this can be formulated as:\n$a_j = \\begin{cases} d_{cos}(f_{i,j}^t, f_{i,j}^s), & \\text{if } p_{i,j} \\in \\hat{Y}_i, \\\\ 0, & \\text{otherwise.} \\end{cases}$ (7)"}, {"title": "4.4 Total objectives", "content": "Through the above procedures, we can formulate the total objectives as follows:\n$L_{total} = L_{MT} + L_{LPLD}.$ (9)\nTo sum up, $L_{MT}$ leverages HPL to improve the detection capability of the network with confidential prediction of easy positives, whereas $L_{LPLD}$ on LPL makes the network focus on hard positive instances, thereby providing solid understanding of target domain by focusing on hard positive instances."}, {"title": "5 Experiments", "content": "To validate our method, we compare our result with state-of-the-art UDAOD, SFOD methods on five different domain shift scenarios, where these domain shifts can be divided into four types of domain shifts."}, {"title": "5.1 Datasets", "content": "A total of 7 datasets are used in the above domain shift scenarios, including the source domain dataset and the target domain dataset. 1) Cityscapes [10] is an urban street scene dataset that offers 5000 fine annotated images, where we use 2925 images as a training set and 500 images as a validation set. 2) Foggy Cityscapes [48] is a dataset that has the synthetic fog applied to the Cityscapes dataset. Three versions of the Foggy Cityscapes exist by their visibility. 3) Sim10k [22] is a synthetic dataset consisting of 10000 images of the street scene from the video game. It has computer-generated annotations of vehicles as alternatives to real-life data with manual annotation. 4) KITTI [15] is a dataset consisting of 7481 training images. It is a street scene dataset similar to Cityscapes, but with different capturing environment, such as camera"}, {"title": "5.2 Implementation Details", "content": "Following the SFOD setting from [56], our baseline object detector is Faster R-CNN [44] with a ResNet-50 [16] backbone pre-trained on ImageNet [11], unless otherwise specified. Additionally, VGG-16 [50] is also used as the backbone network. For more details, please refer to supplementary materials."}, {"title": "5.3 Quantitative Results", "content": "On Tab. 1, 2, 3 and 4, we quantitatively compare our results with other methods in UDAOD, SFOD. Oracle is the baseline model trained with target data and its annotations, and source-only is the model trained on source domain data, evaluated on the target domain. In all evaluations, we use AP with IoU threshold 0.5 (AP50) as our evaluation metric.\nCityscapes to Foggy Cityscapes In deploying object detection model to the real world applications like autonomous vehicles, it is crucial to recognize that domain shifts caused by adverse weather conditions can pose significant risks. Cityscapes to Foggy Cityscapes exemplifies a domain shift induced by the fog, and we use the foggy level 0.02, which has the least visibility among three versions. The result on Foggy Cityscapes after domain adaptation is shown in Tab. 1. Our method achieves the mAP of 40.4 with ResNet-50 backbone and 40.9 with VGG-16 backbone, outperforming other SFOD methods in this domain"}, {"title": "5.4 Further Analysis", "content": "Ablation Studies We conduct ablation studies to analyze the effectiveness of each pseudo label and the adaptive weights based on feature similarity in LPL. Left of Tab. 5 compares each pseudo label, where HPL shows better performance (+9.1 mAP) than using LPL only (+5.7 mAP), with using both pseudo labels giving further improvement (+13.7 mAP), demonstrating the importance of using both pseudo labels. Utilizing adaptive weights on the LPL along with HPL showed the best performance (+15.2 mAP), proving the importance of adaptively utilizing LPL.\nLoss Function We compare various loss choices of LPLD loss by altering classification loss and regression loss on Tab. 5. When utilizing cross-entropy as the classification loss, a performance decrease of 1.4 mAP is observed. This can be attributed to the fact that the proposals around inaccurately localized LPL may have very low IoU or no overlap with the foreground object. Furthermore, adopting regression loss along with cross-entropy loss resulted in a further performance"}, {"title": "6 Conclusions", "content": "We introduce a novel strategy to enhance Source-Free domain adaptive Object Detection focusing on effectively utilizing Low-confidence Pseudo Labels (LPL), termed LPLD loss. Our method effectively reduces the false negative rate in new domains, improving the model's adaptability and performance. Experiments demonstrate that our approach improves adaptation accuracy across various domain shift scenarios, suggesting that even low-confidence proposals have valuable information that cannot be overlooked."}]}