{"title": "Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions", "authors": ["Per Niklas Waaler", "Musarrat Hussain", "Igor Molchanov", "Lars Ailo Bongo", "Brita Elvev\u00e5g"], "abstract": "Patients with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. These individuals could benefit greatly from education platforms that leverage the adaptability of Large Language Models (LLMs) such as GPT-4. While LLMs have the potential to make topical mental health information more accessible and engaging, their black-box nature raises concerns about ethics and safety. Prompting offers a way to produce semi-scripted chatbots with responses anchored in instructions and validated information, but prompt-engineered chatbots may drift from their intended identity as the conversation progresses. We propose a Critical Analysis Filter for achieving better control over chatbot behavior. In this system, a team of prompted LLM agents are prompt-engineered to critically analyze and refine the chatbot's response and deliver real-time feedback to the chatbot. To test this approach, we develop an informational schizophrenia chatbot and converse with it (with the filter deactivated) until it oversteps its scope. Once drift has been observed, AI-agents are used to automatically generate sample conversations in which the chatbot is being enticed to talk about out-of-bounds topics. We manually assign to each response a compliance score that quantifies the chatbot's compliance to its instructions; specifically the rules about accurately conveying sources and being transparent about limitations. Activating the Critical Analysis Filter resulted in an acceptable compliance score (\u22652) in 67.0% of responses, compared to only 8.7% when the filter was deactivated. These results suggest that a self-reflection layer could enable LLMs to be used effectively and safely in mental health platforms, maintaining adaptability while reliably limiting their scope to appropriate use cases.", "sections": [{"title": "Introduction", "content": "Globally there is a desperate need to improve access to medical knowledge and empower people with mental health conditions and their families by providing support systems no matter what time of day help is needed or their geographical location [1]. Chatbots powered by large language models (LLMs) such as GPT-4 have great potential as an educational tool that could greatly improve the accessibility of medical knowledge [2]. They can be used to explain complex concepts, give instant feedback with user-tailored examples and metaphors, translate technical language in everyday language, and make learning new information less daunting by breaking it up into smaller pieces. Especially people with schizophrenia, many of whom present with cognitive impairments, could benefit from this powerful ability to adapt to individual needs [3,4]. While the flexibility of LLMs gives them high potential value in mental health care [5], it also comes with safety concerns due to uncertainties pertaining to their alignment, training materials, and overall opaque and unpredictable nature [6]. This is especially important to consider when the educational materials intersect with sensitive topics like medication use and self-harm [5]. The fact that LLMs can \u201challucinate\u201d facts is a well known issue that is compounded by their inability to reliably reflect the uncertainty in their answer [7]. Indeed, they have been observed giving wildly inaccurate answers in an authoritative manner even on topics where they are generally quite accurate [8]. Another consequence of their \u201clack of self-awareness\u201d is that they may drift into roles which require abilities that AI lack, such as empathy and being able to weigh a multitude of competing personal values and interests when considering complex personal decisions [9]. To leverage the benefits of LLMs in mental health care while avoiding the numerous risks, it is therefore crucial to develop robust systems for restricting the scope of LLM-powered chatbots to the supplementary roles in which they excel, and ensuring that they do not drift into taking on superficially similar roles.\nPrompting is a technique often used to direct chatbots toward producing more accurate and relevant responses without having to collect new training data and retrain the LLM [10]. The prompts modify the behavior of the LLM by providing it with contextual information. They may instruct the LMM on what role to adopt and what rules to follow, and offer a way to pass topical information to the LLM. Discussions of high-stakes subjects, such as medication or self-harm, can be made safer by anchoring the LLMs responses in a knowledge base - a curated repository of information from trusted sources. However, LLMs are stochastic entities and adherence to sources and instructions is not guaranteed, especially in long conversations where the context messages from the chatbot and the user make up a large part of the context. These competing influences can eventually cause a breakdown of what we will refer to as the chatbot's integrity: its tendency to generate messages that are consistent with its internal rules and the documents that make up its knowledge base. The focus of this study is to develop methods for maintaining chatbot integrity in the context of delivering mental health information in a conversational format.\nTo achieve more robust chatbot integrity, this study proposes a Layered Response Generation methodology. In the first layer, the chatbot generates responses based on user input. In the second layer, which we will refer to as the Critical Analysis Filter (CAF), specialized AI-agents analyze and refine the response to maintain the integrity of the chatbot. To showcase the proposed methodology, we develop a GPT-4-powered schizophrenia informational chatbot, referred to hereafter as SchizophreniaInfoBot, which conveys the contents of the manual \u201cLearning to Live with Schizophrenia\u201d by GAMIAN Europe [11]. To make its contents available to SchizophreniaInfoBot we implement an Information Retrieval"}, {"title": "Methods", "content": "Information Retrieval Algorithm\nTo make the information in the schizophrenia manual available to SchizophreniaInfoBot, we implemented a system whereby it could dynamically update the conversational context with relevant sources retrieved from a knowledge base (see the Knowledge Base section) before attempting an answer. The process of answering a question is split into multiple steps: 1. Source Identification: request sources that are relevant to the user-query based on human-written summaries that are included in the initial prompt, 2. Prompt Enhancement: insert the identified sections into the conversation, and 3. Contextual Response Generation: use the updated context to produce a response. SchizophreniaInfoBot is instructed to reference the sources that support its response so that the consistency of the response with cited sources can be evaluated.  shows the main steps of this Information Retrieval Algorithm. Sources not being actively referenced are removed to free up space and allow SchizophreniaInfoBot to focus on more relevant information. See Multimedia Appendix 1 for more details on how the chatbot retrieves information and performs citations."}, {"title": "Critical Analysis Filter for Maintaining Chatbot Integrity", "content": "Our strategy for improving SchizophreniaInfoBot's integrity is based on a prompting paradigm called prompt chaining, which utilizes the fact that complex tasks can often be performed more effectively by breaking them into smaller subtasks. [12]prompted specifically to perform that task, and the output of one agent may serve as input for other prompted agents further down the problem-solving chain. In our case, the complex task is ensuring that the chatbot's response complies with a set of rules. To this end, we prompt-engineered a team of AI-agents that critically evaluate and refine responses, and generate feedback for SchizophreniaInfoBot. The AI-agents responsible for critical evaluation of chatbot responses will be called the AI-judges, or just judges. Each judge is responsible for evaluating some aspect of SchizophreniaInfoBot's responses, such as whether it is staying within scope of the intended role or if appropriate disclaimers have been included. The judges called on for response analysis are determined based on the citations of the response. For example, if sources are cited, then the judge that compares the response to the cited sources is called to evaluate source-fidelity.  shows a high level overview of this process. If the judges detect rule-violations they produce warning messages that inform the chatbot about the rules that it violated. Such dynamic feedback is plausibly more efficient than generic reminders as it does not waste tokens on reminding the LLM of rules it is likely to follow in any case. If a rule-violation is deemed severe, the judges may reject the response altogether, in which case the response gets sent to an agent that refines the response so that it complies with the criticism of the judges, for example by appending \u201cYou should verify this information with your therapist\u201d to a response that has been flagged for lacking a disclaimer. See Multimedia Appendix 1 for more details on the CAF.\nThe prompts of the judges were fine-tuned for desired behavior on a collection of scenarios (Multimedia Appendix 5), where each scenario consist of a user message, the chatbot's response, the sources referenced by the chatbot, and the desired verdict."}, {"title": "Preliminary Judges for Screening Responses", "content": "GPT-4 is a computationally expensive model, but we observed that it tends to produce more coherent response-analyses than GPT-3.5. Furthermore, it appears to produce responses that are more factually accurate, relevant, and useful, at least in a clinical context [13,14]. We therefore decided to rely on GPT-4 for feedback, response-refinement, and final decisions. However, the majority of responses did not violate any rules, and we therefore decided to reduce the cost and response-times of SchizophreniaInfoBot's responses by performing a preliminary screening of responses using judges powered by the smaller but faster GPT-3.5 Turbo model (version 0613 with 16k token window). If any of the preliminary judges flagged the message, GPT-4-powered judges were called on for confirmation. See Multimedia Appendix 1 for more details on the judges."}, {"title": "Definition of Supported Response", "content": "To test the efficacy of the CAF, we will focus on a specific subset of rules that are concerned with whether a response is supported: a response where all assertions are reformulations of assertions explicitly stated in the manual or follow as a logical consequence. However, in order for SchizophreniaInfoBot to be useful as a conveyor of information, we decided to relax the requirement that all responses must be supported in this strict sense: SchizophreniaInfoBot is allowed to make unsupported assertions if 1. they"}, {"title": "Sampling Conversations with AI-Facilitators", "content": "The temperature of GPT-4 is set to 1 in order to get variation in the queries of the facilitators. To account for the fact that the conversations are stochastic, we generate multiple independent conversations between each facilitator and the SchizophreniaInfoBot, with each sample conversation having the same starting point (sheet Initiation of facilitator conversations in Multimedia Appendix 4). For the sake of efficiency and simplicity, we do not restart each conversation from scratch. Instead, for each out-of-bounds role, we manually converse with SchizophreniaInfoBot until we observe a sign of drift towards the intended out-of-bounds role. We then use that first-drift response as a checkpoint from which the corresponding facilitator takes over the role as \u201cuser\u201d and we repeatedly sample conversations that branch off from that point. For each facilitator, three sample conversations are generated, and each conversation ends after the facilitator has queried the chatbot four times. To get comparison data, this experiment is repeated with the CAF deactivated."}, {"title": "Scoring System for Evaluating Chatbot Integrity", "content": "To quantify the effect of the CAF, we set up a scoring system where we assign a compliance score to each response; a numerical value from 0 to 4 based on the list of criteria shown in Table 1. The partial scores"}, {"title": "Knowledge base", "content": "The knowledge base of the chatbot is made up of passages of text from \u201cLearning to live with schizophrenia - A companion guide\u201d: a manual about schizophrenia produced by GAMIAN Europe (an international patient advocacy organization) that was developed through consultation with people with schizophrenia, their caregivers and family members, and health care professionals [11,15]. The manual is written in English. The manual consists of ~12 000 words or ~16 000 tokens and is divided up into 28 consecutive sections or sources that the chatbot can request (Multimedia Appendix 2). When segmenting the information, we aimed to create relatively self-contained pieces of information that cover a specific topic or introduce a chapter. PNW segmented the knowledge base into sources and wrote descriptions for each source under the guidance of BE who has extensive experience in psychiatry research."}, {"title": "Technical Specifications", "content": "Access to LLMs was granted via a Microsoft Azure subscription, and we used the openai Python library to interact programmatically with the GPT models. The responses of SchizophreniaInfoBot were generated by GPT-4 (version 1106-preview with 8k token window) with max tokens set to 320 (~240 words). For preliminary screening we used GPT-3.5 Turbo (version 0613 with 16k token window). All GPT models had the temperature set to the default value of 1. The raw results from the experiments were generated on May 14, 2024. Prompts for the various AI-agents used in this study can be seen in Multimedia Appendix 3."}, {"title": "Results", "content": "Effect on Chatbot Integrity of the Critical Analysis Filter\nThe results from the experiments to nudge SchizophreniaInfoBot towards three different out-of-bounds roles are presented in Table 2, which shows the median compliance score for each response. The interrater agreement was decent, as the compliance score differed by at most 1 in 90.3% of the responses. For each of the three facilitators, activating the CAF resulted in substantially improved ability for SchizophreniaInfoBot to adhere to its instructions. With the CAF activated, the fraction of responses with compliance-score\u22653 was 83.3%, 75.0%, and 83.3%, for roles R1 to R3 respectively, while the corresponding values were 16.7%, 0.0%, and 8.33% when it was deactivated. With CAF activated, SchizophreniaInfoBot received at least one median compliance-score of 0 in 5 out of 9 conversations, but in the 3 cases where zero-score occurred before the end of the conversation it was able to recover by getting a subsequent score of 4. In contrast, without the stabilizing influence of the CAF, in each conversation, the responses eventually ended up consistently getting low compliance scores of Os and 2s, illustrating the self-reinforcing nature of violations of prompt-instructions."}, {"title": "Specificity of Critical Analysis Filter when Answering Schizophrenia Questions", "content": "Sheet 10 schizophrenia questions Results in Multimedia Appendix 4 shows the full conversation along with sources referenced, warning messages produced by the CAF, original responses (prior to refinement), and ratings. In total, 2 responses were flagged by the CAF: one received a warning, and one was modified to comply with instructions. In both cases, the majority of raters agreed with the criticism of the CAF. Thus, the CAF showed good specificity when answering questions about schizophrenia. Note that we included an improvised question where we asked the chatbot to rephrase a response in simpler terms, after which it consistently used simpler language, showcasing an attractive advantage of using chatbots in education."}, {"title": "Discussion", "content": "Principal Findings\nThe CAF was highly effective at re-establishing the integrity of the chatbot after it had started to drift from its role and instructions. With CAF activated, SchizBot showed a substantially improved tendency to admit its limitations and encourage verification when appropriate, and generally tried to steer the conversation back to a permissible topic. With the filter deactivated however, the chatbot displayed an \u201ceagerness\u201d to expand on out-of-bounds topics, illustrating the importance of robust monitoring mechanisms that catch and correct for this kind of conversational drift. Finally, the CAF showed good specificity when answering the 10 questions about schizophrenia, as all warning messages gave valid motivations.\nComparison with Prior Work\nThere has been a surge of research in utilizing advanced generative AI in mental health care services over recent years, but attention has mostly been given to therapeutic applications and counseling support [5]. We were not able to identify any research that focused exclusively on the problem of controlling the scope of LLM-powered conversational agents in the context of mental health care, and our research appears novel in that it focuses specifically on this aspect of LLM performance in situations where controlling scope and ensuring transparency is of critical importance.\nA recently proposed method which is similar to the CAF that we propose here is SELF-RAG [16]. Like the CAF, SELF-RAG utilizes a self-reflection stage where LLMs performs tasks of critical evaluation, such as deciding if a retrieved document is relevant to the user-query, and generate tokens, such as"}, {"title": "Defining the Scope and Boundaries of the Chatbot", "content": "Occasionally, unsupported responses did pass through the CAF undetected. Imperfect performance is at least partially explained by the ambiguity of the rules that outline the scope of SchizophreniaInfoBot. Indeed, humans themselves will sometimes disagree on whether or not a response complies with a rule, as illustrated by the less-than-perfect interrater agreement. This ambiguity is arguably unavoidable however if we wish to leverage the abilities of LLMs; to be effective as a conveyor of information, SchizophreniaInfoBot sometimes has to rely on the GPT-4's innate knowledge, for example when explaining concepts not defined in the sources, and those situations cannot be unambiguously captured by rules expressed in natural language with no appeal to common sense."}, {"title": "Restricting GPT's Responses to Topics where it is Reliable", "content": "To illustrate why it is hard to formulate exact rules for what constitutes a \u201csupported\u201d statement, consider the question \"Can including more vegetables make my diet healthier?\" as a follow up to manuals recommendation to \u201cadhere to a healthy diet\u201d. If the chatbot says \u201cIncluding fruits and vegetables in your diet is generally considered to be healthy\u201d, should we be pedantic and flag this as unsupported because the manual never explicitly specifies what \u201chealthy eating\u201d means, or do we consider this fact to be so basic and common-sense that we permit it despite not being explicitly stated? Much of the utility of chatbots like ChatGPT comes from their ability to explain and expand on phrases or concepts, and by being too restrictive we would lose this feature. Thus, the formulation of such rules amounts to a balancing act between predictability and risk-reduction on one hand, and usefulness and versatility on the other.\nAs a general strategy for striking a good balance between risk-reduction and utility, we decided to allow the chatbot to make unsupported assertions under the condition that they constitute basic and uncontroversial information or advice. This formulation is intended to capture the situations in which GPT is at its most reliable and useful, and it can be motivated by observing that there is presumably a lot of training data available for such topics. Given that the objective of LLMs is to generate plausible-sounding text, it is plausible that their responses align better with factuality when the topic being discussed is basic and uncontroversial. Indeed, studies have found that GPT performs better when asked questions related to popular factual knowledge [7]. A pitfall of this strategy is that common misconceptions can be hard to distinguish from basic facts due to their pervasiveness, and so its success depends on how well GPT differentiates between the two. In any case, the strategy is likely to at least weed out hallucinations. Conceivably, the advantages of interactivity and adaptability afforded by the \"basic-and-uncontroversial\u201d-rule outweighs the risks of the occasional inaccurate advice as such advice"}, {"title": "Structuring Sources for Delivery via a Chatbot", "content": "The sources of SchizophreniaInfoBot were written with a static medium of communication in mind. The chatbot's performance might therefore be improved if the sources are instead written specifically to be communicated via chatbots. For example, a static manual may assume that the sections are read in sequence, and some sections serve only as introductions to a chapter, but SchizophreniaInfoBot may retrieve them in isolation. As a result, SchizophreniaInfoBot may sometimes produce awkward answers if the retrieved sources lack the preceding context. If the blocks are instead written as self-contained blocks of information, then the chatbot may be more likely to produce a complete and comprehensive answer.\nAnother way the sources could be optimized for chatbot communication is to express them in a more compact technical language so they take up less tokens, and thus take up less space in the context window. The LLM could then \u201cdecompress\u201d the information when they convey the technical information to the user in simpler terms - a task at which they excel. Another advantage of condensing the language of the sources is that the notion of a response being \u201csupported by a source\u201d is more natural when preceded by precise scientific language and the LLM might therefore be more incentivized to respect the boundaries of the source materials."}, {"title": "Study Limitations", "content": "Generalizability is Uncertain\nThis study is a feasibility study, and it has important limitations. We have tested the CAF only in a very small number of situations, and can therefore say little about the generalizability of our findings. Furthermore, we fine-tuned the prompts of the AI-judges on a relatively narrow range of scenarios, including scenarios similar to those generated by the facilitators. Thus, the CAF performance might be lower in scenarios outside the set of scenarios used to fine-tune the prompts. In particular, the generalizability of the preliminary judges (powered by GPT-3.5) is uncertain, since GPT-3.5 appears more prone to making logical errors than GPT-4. We also utilized AI as substitutes for human testers for convenience and objectivity, but AI cannot truly replicate the diverse and unpredictable nature of human input, in particular people with schizophrenia. Large numbers of human testers are therefore needed to converse with SchizophreniaInfoBot in order to generate a more diverse set of conversations and situations. In particular, a study that collects and analyzes conversations between SchizophreniaInfoBot"}, {"title": "Potential for Biased Performance Evaluation", "content": "The person who wrote the prompts (PNW) for the CAF also designed the benchmarks for testing them, which could bias the performance metrics, as there may be a temptation to design tests at which the chatbot is likely to perform well at."}, {"title": "Need for Rigorous Testing of other Aspects of Performance", "content": "Designing mechanisms that improve controllability may come at the expense of other aspects of performance like flexibility or usefulness. More extensive testing is needed to assess various aspects of performance, such as its ability to generate relevant and useful answers."}, {"title": "Limitations of Information Retrieval Algorithm", "content": "Most of the questions generated by GPT-3.5 turned out to actually be asking 2-3 questions in one sentence, which incidentally made them particularly challenging for the chatbot to answer via the Information Retrieval Algorithm. The Information Retrieval Algorithm tends to work well when a single query can be answered concisely in a small number of sources, but may fail when the answer to a question is spread across multiple sources or when the formulation of the question differs substantially from the description of the sources."}, {"title": "Conclusions", "content": "Using Al-agents in a CAF to monitor and refine the chatbot's responses, as well as provide feedback to the chatbot, led to responses being substantially better adherence to the chatbot's sources and instructions, and thereby a more robust and controllable LLM-powered chatbot. In particular, the chatbot was far more likely to acknowledge its transgressions when it made assertions that were not directly supported by its sources than when the CAF was deactivated. Our results suggest that it is feasible to utilize LLMs as vehicles of mental health information whilst keeping the risks and consequences associated with LLMs at an acceptably low level. More research is needed to establish the generalizability of our findings."}, {"title": "Data Availability", "content": "The chatbot conversations analyzed in this study are included in the Multimedia Appendix 4. No additional datasets were used or generated during this study. The code that produced the analysis can be accessed at [17]."}, {"title": "Conflicts of Interest", "content": "None declared."}]}