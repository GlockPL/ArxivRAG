{"title": "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "authors": ["Youngtaek Oh", "Jae Won Cho", "Dong-Jin Kim", "In So Kweon", "Junmo Kim"], "abstract": "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMS) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities.", "sections": [{"title": "1 Introduction", "content": "Humans naturally excel at multi-modal understanding, effortlessly perceiving and interpreting different modalities, such as images and text, and forming associations between them. This capability is evident in recognizing novel concepts (Fu et al., 2018), cross-modal retrieval (Kaur et al., 2021), and compositional reasoning (Levesque et al., 2012). To achieve this ability in artificial intelligence, foundational vision and language models (VLMs) have been trained on large-scale image-text datasets (Schuhmann et al., 2022b), significantly bridging the gap between human and machine capabilities in tasks like zero-shot recognition and image-text retrieval (Radford et al., 2021). Despite these advances, VLMs still face challenges in compositional reasoning (Yuksekgonul et al., 2023). Humans intuitively understand complex compositional language in combination with images, engaging in spatial reasoning (Kamath et al., 2023b), recognizing attributes and relationships in objects (Hsieh et al., 2023), and perceiving equivariance between image and text (Wang et al., 2023). In contrast, VLMs often fail to understand these nuanced relationships (Liu et al., 2023a; Ray et al., 2023). This shortfall is attributed to their reliance on global, single vector representations (Kamath et al., 2023a) and limited ability to match compositional knowledge (Wang et al., 2024). To improve compositionality in VLMs, both pre-training (Singh et al., 2023; Zheng et al., 2024) and fine-tuning (Zhang et al., 2024; Singh et al., 2024) methods have been proposed. In particular, fine-tuning, which leverages pre-trained knowledge and is cost-effective, is widely adopted in academia. Typically, this involves incorporating hard negative texts (Doveh et al., 2022, 2023; Herzig et al., 2023) into training. However, as shown in Fig. 1, this approach can result in a trade-off, where gains in compositionality come at the expense of performance in the multi-modal tasks: zero-shot classification (ZS) and image to text retrieval (I2T Ret). Previously, hard negative (HN) losses are applied to global image and text representations. Since HN texts are encoded too similarly to the original ones (Kamath et al., 2023a), pushing them away with the HN loss can disrupt the multi-modal representations. To address this, we propose a new fine-tuning framework for VLMs that enhances compositional reasoning while preserving performance in multi-modal tasks. Our approach mitigates the degradation caused by global hard negative loss on single vector representations, which struggles to capture subtle informational differences between hard negative texts and the original text. Our framework introduces two key innovations: (1) Local Hard Negative (LHN) Loss. We utilize dense alignments between image patches and text tokens to calculate the hard negative loss. This approach, inspired by the dense alignment for vision-language representation (Huang et al., 2021; Bica et al., 2024), aggregates local similarity scores to enhance compositional understanding without undermining multi-modal representations.\n\u2022 We design a local hard negative (LHN) loss and a selective calibrated regularization (SCR) mechanism, effectively capturing subtle differences in hard negative texts and preserving the integrity of multi-modal representations.\n\u2022 We validate FSC-CLIP through an extensive range of experiments, covering 11 compositionality, 21 zero-shot recognition, and 3 image-text retrieval tasks, establishing a comprehensive evaluation of VLMs' multifaceted capabilities."}, {"title": "2 Related Work", "content": "Contrastive Vision-Language Models. CLIP (Radford et al., 2021) has revolutionized multi-modal domains through large-scale image-text pre-training, demonstrating remarkable zero-shot capabilities. Its dual encoder architecture has introduced versatility and driven advancements across a wide range of existing vision (Zhou et al., 2022; Oh et al., 2022; Cho et al., 2022) and vision-language downstream tasks (Jang et al., 2022, 2023; Cho et al., 2023a,c,b; Kim et al., 2019, 2021a,b). CLIP also serves as the foundation for recognition (Liang et al., 2023), image captioning (Mokady et al., 2021; Lee et al., 2024; Kim et al., 2024a,b), large multi-modal models (Li et al., 2023; Liu et al., 2023b), and generative models (Podell et al., 2024). In addition, CLIP extends its utility to connecting 3D (Sun et al., 2024) or audio (Elizalde et al., 2023; Senocak et al., 2023) to language, highlighting its essential role in multi-modal and compositional tasks in practical applications. We aim to enhance CLIP's compositional understanding while preserving its multi-modal capabilities.\nVision-Language Compositionality. Although vision and language models exhibit promising capabilities such as zero-shot classification and retrieval (Radford et al., 2021; Zeng et al., 2022), they still struggle with compositional reasoning, which requires fine-grained understanding between image and text (Peng et al., 2024). Numerous benchmarks have been proposed, testing various aspects like attributes, relationships and objects (Zhao et al., 2022; Yuksekgonul et al., 2023), spatial reasoning (Kamath et al., 2023b; Liu et al., 2023a) and linguistic phenomena (Parcalabescu et al., 2022). To enhance compositionality, incorporating hard negative captions during fine-tuning has become a common approach (Zhang et al., 2024), with these captions being generated through rule-based methods (Doveh et al., 2022; Yuksekgonul et al.,"}, {"title": "3 Methodology", "content": "We first outline the fine-tuning setup for CLIP in Sec. 3.1. Next, we introduce FSC-CLIP, which incorporates Local Hard Negative (LHN) Loss and Selective Calibrated Regularization (SCR) in Secs. 3.2 and 3.3. The training objective for FSC-CLIP is described in Sec. 3.4. The complete FSC-CLIP framework, integrating both global and local HN losses with SCR, is illustrated in Fig. 2.\nConsider a mini-batch $\\mathcal{B} = \\{(I_i, T_i)\\}_{i=1}^B$ of size B, consisting of image and text pairs (Ii, Ti). Using CLIP's visual and language encoders, fv(\u00b7) (e.g., ViT (Dosovitskiy et al., 2021)) and ft(\u00b7) (e.g., Transformers (Vaswani et al., 2017)), each image Ii is encoded into a sequence of visual tokens Vi = fv(Ii), and each text Ti into a sequence of textual tokens Ti = ft(Ti). These sequences are represented in a shared multi-modal space, with Vi = {Vp,i}p=1P comprising P patch embeddings and Ti = {tw,i}w=1W consisting of W token embeddings. The global representations of image and text vi and ti \u2208 Rd can be obtained by pooling the local representations: vi = Pool (Vi) and ti = Pool (Ti), respectively. For example, Pool(\u00b7) corresponds to avgpool and argmax for images and texts in Radford et al. (2021).\nCLIP aligns the corresponding images and texts by measuring the global-level similarity:\n$S_g (I_i, T_i) = \\exp \\Big( \\frac{\\cos (v_i, t_i)}{\\tau} \\Big),$ (1)\nwhere $\\cos (v, t) = \\frac{v \\cdot t}{||v|| \\cdot ||t||}$. The image to text loss Li2t of CLIP maximizes Sg (Ii, Ti), while minimizing Sg (Ii, Tj) for all non-matching texts j \u2260 i:\n$L_{i2t} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{S_g (I_i, T_i)}{\\sum_{j=1}^B S_g (I_i, T_j)},$ (2)\nand the text to image loss Lt2i is the reciprocal of Li2t which aligns the matching image per text. The final CLIP loss is Lclip = \u00bd (Li2t + Lt2i).\nTo enhance the compositional reasoning of CLIP, hard negative (HN) texts are commonly incorporated into training, whether they are rule-based (Yuksekgonul et al., 2023) or generated by language models (Doveh et al., 2023). Consider a set of K different HN texts $T_i = \\{T_i^k\\}_{k=1}^K$ originated from Ti. We introduce a separate hard negative loss added to Lclip, similar to Doveh et al. (2022). First, we compute a similarity prediction probability p, assigned to the original caption Ti as follows:\n$p = \\frac{S_g (I_i, T_i)}{S_g (I_i, T_i) + \\sum_{k=1}^K S_g (I_i, T_i^k)}$ (3)\nHere, g represents the global representation, and the hard negative (HN) loss applied to this similarity assignment is formulated as cross entropy:\n$L_{neg} = - \\frac{1}{B} \\sum_{i=1}^B \\log p$ (4)"}, {"title": "3.2 Local Hard Negative (LHN) Loss", "content": "To address this, we propose a novel Local Hard Negative (LHN) loss that utilizes a local similarity score Sl(I, T). Replacing the global similarity Sg with Sl, the LHN loss is formulated as follows:\n$L_{lneg} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{S_l (I_i, T_i)}{S_l (I_i, T_i) + \\sum_{k=1}^K S_l (I_i, T_i^k)},$ (5)\nwhere pl represents the local similarity prediction. Unlike Bica et al. (2024), which uses token-level contrast for image-text pairs, we introduce a new HN loss based on local similarity Sl from token-patch representations, enabling the capture of subtle differences between the original and HN texts.\nSl(I,T) is designed to measure the similarity between token and patch embeddings for each token in the given text T. From the patch representations $V = \\{V_p\\}_{p=1}^P$, we first derive the textual-aligned patch embeddings $V = \\{\\nu_w\\}_{w=1}^W$, corresponding to each textual token feature tw in T \u2208 RW\u00d7d. This is achieved by performing a weighted average of patches V using attention weights \u03b1 \u2208 RW\u00d7P derived from normalizing the similarity map s between token and patch embeddings. We denote the similarity map as s = TTV \u2208 RW\u00d7P, where sw,p = tvp. To relate multiple similar patches for each token, we min-max normalize s to obtain \u03b1:\n$\\alpha_{w,p} = \\frac{s_{w,p} - \\min_k s_{w,k}}{\\max_k s_{w,k} - \\min_k s_{w,k}},$ (6)\nand use the attention weights \u03b1 to aggregate V, obtaining the textual-aligned patches V = {\u03bdw}w=1:\n$\\nu_w = \\frac{\\sum_{p=1}^P \\alpha_{w,p} \\cdot V_p}{\\sum_{p=1}^P \\alpha_{w,p}}$ (7)\nIn Appendix B.1, we explore different normalization choices for the attention weights in Eq. (6).\nAfter obtaining the textual-aligned visual tokens V, we aggregate the per-token similarities between V and T as follows:\n$S_l (I, T) = \\sum_{w=1}^W \\exp \\Big( \\frac{\\cos (\\nu_w, t_w)}{\\tau} \\Big),$ (8)"}, {"title": "3.3 Selective Calibrated Regularization (SCR)", "content": "Since hard negative (HN) texts are often encoded similarly to the original texts, HN losses can disrupt multi-modal representations. To counter this, we propose Selective Calibrated Regularization (SCR) to better regulate HN supervision, seamlessly applicable to both global and local HN losses.\nSCR has two components: one modulates the supervision signal based on image-text similarity, while the other adjusts label assignments to calibrate the positiveness of HN texts. As shown in Tab. 2, we confirm that both components are crucial for preserving the representation integrity.\nTo mitigate the negative impact of supervising HN texts, we reduce the supervision signal for confident similarity predictions to the original text. Instead, we focus more on challenging HN texts that exhibit higher similarity to the image and may be confused with the original texts. This confidence-based weighting aligns with the concept of focal loss (Lin et al., 2017), as shown in Fig. 3."}, {"title": "3.4 Overall Training Objective", "content": "Our FSC-CLIP incorporates two hard negative (HN) losses, $L_{neg}^g$ and $L_{neg}^l$, representing global and local HN losses respectively, into CLIP loss Lclip:\n$L_{total} = L_{clip} + \\lambda_g L_{neg}^g + \\lambda_l L_{neg}^l,$ (11)\nwhere \u03bbg and \u03bbl are the weighting factors for the respective losses. Selective Calibrated Regularization (SCR) is applied to both losses, incorporating label smoothing and focal loss. The global HN loss, $L_{neg}^g$ is computed as Focal ($p^g$, \u1ef9), while the LHN loss, $L_{neg}^l$ is derived similarly, by replacing $p^g$ with $p^l$ for the local representations."}, {"title": "4 Experiments", "content": "We consider three image-text datasets for fine-tuning: COCO captions (Chen et al., 2015), CC-3M (Sharma et al., 2018), and LAION-COCO (Schuhmann et al., 2022a). For COCO captions, we utilize 100K examples pre-processed by Yuksekgonul et al. (2023). As pointed out by Singh et al. (2023), COCO shares data with several evaluation benchmarks (e.g., SugarCrepe and retrieval tasks), which may inadvertently affect the results. To ensure a broader evaluation and avoid such overlap, we additionally consider CC-3M and LAION-COCO for fine-tuning. For each dataset, we randomly sample 100K examples and, instead of using raw captions, we utilize synthetic captions paired with images. Specifically, for CC-3M, we generate captions using CoCa (Yu et al., 2022) with ViT-L/14, while for LAION-COCO, we use captions generated by BLIP (Li et al., 2022b) with ViT-L/14, applied to the LAION-2B dataset (Schuhmann et al., 2022b).\nWe employ simple rule-based methods for generating hard negative (HN) texts, avoiding the need for external language models like Le Scao et al. (2023) used in Doveh et al. (2023). For each original caption, we apply three distinct operations: negclip, replace, and bi-gram shuffle. These operations are applied at every training step, ensuring variation in HN texts across iterations. As a result, each batch item is paired with an image and four captions, as illustrated in Fig. 2. Further details and examples on these operations are provided in Appendix A.1.\nConsistent with previous meth-ods (Yuksekgonul et al., 2023; Singh et al., 2023; Zhang et al., 2024), we trained our models during 5 epochs with batch size 256, using OpenCLIP repository (Ilharco et al., 2021). The learning rate is set to 5e-6 and decayed by a cosine schedule, with a warmup of 50 steps. Models are optimized using AdamW with a weight decay of 0.1. We use a single Quadro RTX 8000 GPU with 48GB memory for training. Images are re-scaled to 224, and the context length is 77 for texts. We set the weighting factors \u03bbg = 0.5 and \u03bbl = 0.2. For SCR, we set \u03b3 = 2.0 and \u03b2 = 0.02 for focal loss and label smoothing, respectively. We also experiment with LORA (Hu et al., 2022), which preserves the original model parameters. Consistent with Doveh et al. (2022, 2023), we set the rank to 4. Training our model takes less than one hour for 100K samples."}, {"title": "4.1 Main Results", "content": "We compare our FSC-CLIP to previous fine-tuning methods for compositionality. We report both compositionality and multi-modal task performance as shown in Tab. 1. In Fig. 4, we visualize the trade-off trajectory between Comp and ZS through the robust fine-tuning method (Wortsman et al., 2022).\nWe introduce our baseline, NegCLIP\u2021, serving as a direct comparison to our FSC-CLIP. Unlike the original implementation of NegCLIP (Yuksekgonul et al., 2023), we utilize an online version of hard negatives generation (e.g., negclip) and omit the use of additional similar image batches. This baseline will be further used in our ablation study, with the symbol # omitted for convenience."}, {"title": "5 Conclusion", "content": "In this paper, we introduce Fine-grained Selective Calibrated CLIP (FSC-CLIP), a new fine-tuning framework for vision-language compositionality. It aims to preserve multi-modal capabilities and address the limitations of existing methods relying on global representations. We achieve this by employing dense representations between images and texts and regularizing the hard negative losses to prevent degradation, thereby facilitating the introduction of Local Hard Negative Loss and Selective Calibrated Regularization. Our extensive validation shows improved compositional reasoning and promising performance in standard multi-modal tasks."}, {"title": "A Additional Details", "content": "We provide details in generating hard negative texts in our model. We employ three types of rule-based methods: negclip (Yuksekgonul et al.,"}]}