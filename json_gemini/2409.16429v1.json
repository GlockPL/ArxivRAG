{"title": "Leveraging Local Structure for Improving Model Explanations: An Information Propagation Approach", "authors": ["Ruo Yang", "Binghui Wang", "Mustafa Bilgic"], "abstract": "Numerous explanation methods have been recently developed to interpret the decisions made by deep neural network (DNN) models. For image classifiers, these methods typically provide an attribution score to each pixel in the image to quantify its contribution to the prediction. However, most of these explanation methods appropriate attribution scores to pixels independently, even though both humans and DNNs make decisions by analyzing a set of closely related pixels simultaneously. Hence, the attribution score of a pixel should be evaluated jointly by considering itself and its structurally-similar pixels. We propose a method called IProp, which models each pixel's individual attribution score as a source of explanatory information and explains the image prediction through the dynamic propagation of information across all pixels. To formulate the information propagation, IProp adopts the Markov Reward Process, which guarantees convergence, and the final status indicates the desired pixels' attribution scores. Furthermore, IProp is compatible with any existing attribution-based explanation method. Extensive experiments on various explanation methods and DNN models verify that IProp significantly improves them on a variety of interpretability metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "With the deployment of deep neural network (DNN) models for safety-critical applications such as autonomous driving [7, 8] and medical diagnosis [11, 26], explaining the DNN predictions has become a critical component of decision making processes. For humans to trust the decisions of DNNs, performing well on the target task is necessary but not sufficient; the model should also generate explanations that are interpretable by domain experts. There has been a significant amount of research in this area [15, 20, 27, 32, 39, 40]. Often, these approaches measure the importance of a pixel as the pixels influence on the decision made by the underlying DNN model. As such, the pixel importance is typically represented by an attribution/saliency map that has the same size as the input image, with each value indicating the importance of the corresponding pixel for the model's decision on that image.\nMost of the current explanation methods construct the attribution map by evaluating the contribution of each pixel independently. However, humans and DNNs use the pixels' structural relationships in an image (i.e., locally-connected clusters of pixels) to make predictions. Convolutional neural networks (CNNs), for instance, utilize several layers of convolution and pooling operations to capture the local visual structures in the images. Hence, the maps generated by existing explanation methods are inadequate (See Fig. 1). We advocate that modeling and utilizing the local structural relationships between pixels is crucial for designing more effective explanation methods. In other words, the attribution scores of pixels should be considered jointly for explanation due to pixels' inherent relationships to their neighbor pixels.\nOne naive strategy to capture pixels' local structure relationships is to first cluster pixels into groups using a particular segmentation method and then assign the same attribution score to all pixels in a group, e.g., XRAI [23]. However, this static strategy is suboptimal for several reasons. First, this requires an accurate segmentation approach. Second, even when segmentation is accurate, assuming all pixels in the given segment have the same importance for model decision is a strong assumption.\nIn this paper, we model pixels' relationships in a dynamic way. Specifically, we treat the individual attribution score of each pixel as a source of explanatory information and model the explanation of the prediction of the image to be the dynamic propagation of the individual attribution scores among all pixels of the image.\nIn this regard, information exchange occurs continously, i.e., information flowing from a pixel to its neighboring pixels and vice versa. Thus, the explanation method dynamically measures the information contribution of all pixels. In the ideal situation, such a dynamic process has an equilibrium information distribution in which information exchange ceases. As a consequence of the interaction among pixels, the explanation information for each pixel converges and stabilizes with respect to the information flow. In contrast, if the equilibrium distribution is not achieved, pixels' explanation information exchange continues, indicating the relationships between pixels are not completely exploited. Hence, we endeavor to determine the unique equilibrium information distribution with regard to the dynamic process.\nThere are two core questions that need to be answered: 1) How can we model the information flow among pixels? 2) How can we guarantee that the dynamic process converges? To address them, we propose an Information Propagation approach (termed IProp) for improving model explanations, that can be applied to the output of any existing explanation method that generates an explanation attribution map. Specifically, we first design a weighted graph with pixels as nodes and similarities between pixels as weighted edges, where we investigate the similarity in both the spatial and color space. Next, we model the information propagation among pixels as a Markov Reward Process (MRP), which propagates the pixel's attribution information across nodes (pixels) in the weighted graph, capturing the pixels' structural relationships. We also prove that IProp converges to an unique equilibrium distribution, where each entry's value corresponds to the pixel's final attribution score. Finally, we evaluate IProp on multiple explanation metrics with various baseline explanation methods and DNN models for image classification. Our extensive results demonstrate that IProp improves all baselines both qualitatively and quantitatively.\nOur main contributions are summarized as below:\n\u2022 We propose IProp, a novel meta-explanation method, that leverages the local structure relationships of pixels. IProp is compatible with any existing attribution map-based explanation method.\n\u2022 We prove that IProp, which is the dynamic way to model explanation as information propagation among pixels, converges to a unique attribution map when an underlying explanation method is given.\n\u2022 Extensive evaluations show that IProp produces more accurate attribution maps to represent the explanation compared to underlying explanation methods."}, {"title": "2 RELATED WORK", "content": "Pixel-based Explanation Methods. Pixel-based explanation methods quantify the contribution of each pixel to the model decision by assigning it an importance score. They can be further categorized as Shapley value-, Input perturbation-, and Backpropagation-based methods. The Shapley value [34] was originally proposed to represent the contribution of each player to the outcome of a cooperative game. For explaining image classification, each pixel in an image is treated as a player and the outcome is the image's prediction score. Calculating Shapley values exactly is intractable when the image size is large. Hence, several methods propose to approximate the Shapley values, including KernelSHAP [27], BShap [40], and FastShap [20]. Input perturbation-based methods work by manipulating the input image and observing its effect on the prediction. This idea is utilized by RISE [29], the methods learn the mask to use as the attribution maps [14, 15], and other papers [12, 49]. Backpropagation- based methods propagate the final prediction score back to the input or the hidden layers of the DNN and assign a score for each pixel in the input accordingly. These methods include Deconvnet [47], guided backpropagation [38], DeepLIFT [35], LRP [5], SmoothGrad [37], and Grad-CAM [33]. Recently, The Integrated Gradients was proposed by Sundararajan et al. [41]. It uses line integration to compute the attribution score for pixels. Its variants include GIG [24], Blur IG [45], AGI [28], and IDGI [46].\nRegion-based Explanation Methods. These types of methods assign attribution scores to each segmented region instead of each pixel. That is, the image is first segmented into distinct regions and the pixels' attribution scores are identical if they are located in the same segment. For example, given an image and an attribution map, XRAI [23] creates segments for the image, calculates the attribution score for each segment by summing the attribution scores of all pixels in the segment, and then assigns the same score to all pixels in that segment. Similarly, LIME [32] first segments the image into superpixels as the features for a linear model, then fits the model where the weights of the model determine the contribution of each superpixel to the prediction. However, the region-based methods do not explicitly consider the structural relationship between pixels, but instead simply assign a score to the pixels based on which segments they belong to.\nOur method, IProp, is orthogonal to and compatible with both region- and pixel-based explanation methods. This is due to the fact that IProp determines the final attribution scores of pixels' by propagating the original attribution scores on a weighted graph (where the weights are determined based on pixel similarities), and the original attribution scores can be obtained via any existing explanation method."}, {"title": "3 BACKGROUND", "content": "Markov Reward Process (MRP). MRP models a process where an agent starts in a state, transitions stochastically to a new state based on a probability transition matrix, and receives a reward. The discounted cumulative reward [31] that the agent collects over time t is defined as $G_t = \\sum_{i=k+1}^{\\infty} \\gamma^{i-t-1} \\times R_i$, where $\\gamma$ is a discounting factor and $R_i$ is the reward at time i. $G_t$ can be interpreted as the cumulative reward of a walk on a Markov graph, with each state of the walk contributing the reward $R_i$ with a discounting factor $\\gamma^{i-t-1}$. Then, the value for a given state s, i.e., $V(s) = E[G_t|S_t = s]$, represents the expected discounted cumulative reward for all paths starting from state s and walking an infinite amount of time. Given the individual reward R(s) for state s, the transition matrix P where P[s, s'] is the transition probability from state s to s' at any time step, and with the recursion $G_t = R_{k+1} + \\gamma \\times G_{k+1}$, the Bellman equation [42] for MRP formally defines the value for the state s as $V(s) = R(s) + \\gamma \\times \\sum_{s'\\in s} P[s, s'] \\times V(s')$, or in the matrix form as $V = R + \\gamma \\cdot P \\times V$. MRP can be employed to examine the long-term behavior of a system, such as the total reward an agent is expected to accumulate over an infinite number of time steps.\nModel Explanation and Attribution Map. The aim of an explanation is to determine the importance of the input with respect to the model's prediction. Given a classifier f, class c, and an input x, let output $f_c(x)$ represent the confidence score (e.g., probability) for predicting x as belonging to class c. Formally, the explanation method, EM(), is a function that takes the target class c, classifier f, and input x as input and outputs the attribution map (AM), i.e., $AM = EM(f, x, c)$, that has the same size as x. Each value $AM_i$ then indicates the importance/attribution score for the i-th entry in x. In the image classification domain, which is the focus of our paper, AM indicates the attribution scores of all pixels in the image x for a classifier f to make the prediction $f_c(x)$."}, {"title": "4 IPROP: INFORMATION PROPAGATION FOR IMPROVED MODEL EXPLANATION", "content": "4.1 Intuition\nAlmost all the existing explanation methods consider the pixels independently when calculating a pixel's contribution to the prediction. However, DNN models make predictions using a collection of structurally-similar pixels rather than using individual ones. This implies that within the context of the model explanation, when assigning an attribution score to a pixel, we should also consider the attribution scores of other structurally similar pixels. One straightforward way to capture the structural similarity is to consider image segmentation to group the pixels. For instance, we can first cluster pixels into segments and assign the same attribution score to all the pixels in each segment (similar to XRAI [23]). However, the output of XRAI depends on the image segmentation technique. For instance, an object may be divided into distinct regions, with pixels from each segment having strong relationships. Then, XRAI assigns different scores to these pixels. Conversely, it is also possible to segment two distinct objects into the same region, in which the pixels in different objects do not share any strong relationships but XRAI assigns the same score for them.\nWe propose exploring the inherent relationships between pixels' attribution scores in a dynamic way. Specifically, we treat an image as a directed graph where pixels are the nodes and the weights for the directed edges are the nodes' transition probabilities converted from the nodes' similarities. The similarities are computed based on nodes' spatial and color distances. We then model pixels' attribution generation as a dynamic process (i.e., Markov reward process), where each node/pixel's reward is the attribution score from any existing explanation method. Then each pixel is dynamically re-warded during the process which updates its attribution score. Next, we ask if a particle begins at a pixel (e.g., I), traverses the weighted graph, and receives the discounted reward from each node along the path of traversal at each time step, what is the expected cumulative attribution reward for the particle after traversing an infinite number of time steps? Importantly, the particle has a larger possibility of visiting structural-similar nodes since large transition probabilities exist between these nodes, which are the normalized similarities. The expected cumulative reward for the particle is treated as the final attribution score of the pixel I. Now by putting particles on all pixels, such a dynamic process simulates information propagation among all pixels and their structurally-similar counterparts. When the dynamic process converges, we have all pixels' final attribution scores, forming a new attribution map.\n4.2 The Design of IProp\nInspired by the above described dynamic information propagation, IProp consists of three main steps: 1) Building a weighted graph; 2) Constructing the transition matrix; and 3) Utilizing the Markov Reward Process (MRP) to generate the attribution map. Next, we explain each of the steps in detail.\nBuilding a Weighted Graph. Given an image, we treat each pixel as a node. To build the graph, we need to determine the neighborhood of each pixel. For instance, we consider connecting each pixel to its K-order neighborhood, where the K-order neighborhood pixels have spatial distance K or lower to the target pixel. Hence, each pixel contains at most $(2 \\times K + 1)^2 - 1$ neighbors. See Fig. 3 for an example when K = 2. Applying to all pixels, we build an undirected-unweighted graph G = (V, E). Next, we define edge weights. The weight of an edge represents the similarity (or inverse distance) between two connected pixels. There are several methods for measuring such similarity. Here, we are inspired by SLIC [2, 3], which defines pixel distance as the combination of spatial distance and color distance. Specifically, the image is first converted to the CIELAB space from the RGB color space. Similar to RGB, each pixel I in the CIELAB space has three values, i.e., l\u2081, a1, b\u2081. Then the spatial distance between two pixels I = (i1, jt) and J = (ij, jj) is defined as the Euclidian distance $d_{IJ}^s = \\sqrt{(i_I \u2013 i_J)^2 + (j_I \u2013 j_J)^2}$, and the distance in the CIELAB space is defined as $d_{IJ}^c = \\sqrt{(l_I \u2013 l_J)^2 + (a_I - a_J)^2 + (b_I \u2013 b_J)^2}$. Finally, the combined distance, i.e., $d_{IJ} = d_{IJ}^s + d_{IJ}^c$, defines the distance between two nodes/pixels. We investigate the ranges of both distances in Section 5.4. Since a longer distance implies less similarity, for simplicity, we define the weight, e.g., W (I, J), between two pixels I, J, as their negative distance, i.e., $-d_{IJ}$. We denote the undirected weighted graph as $G_w = (V, E, W(E))$.\nConstructing the Transition Matrix. A key step in applying MRP is to first construct the transition matrix, which consists of transition probabilities between two states. Intuitively, each node is associated with a state and if two nodes are closer, then the transition probability between these two nodes is larger. Moreover, the transition probabilities from a node to all the other connected nodes sum to 1. To capture these intuitions, we propose to convert the weights W (I, J) to probabilities via the softmax function based on the connectivity for node I. Specifically, we define the transition matrix as P where the (I, J)-th entry stands for the similarity between nodes I to J. Formally,\n$P[I,:] = softmax(W[I, :]),$\nwhere $softmax(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$. Note the transition matrix P is asymmetric since the local structural similarity from I to J is not necessarily the same as that from J to I.\nMRP for Generating IProp Attribution Map. As mentioned in the Background, MRP determines an equilibrium distribution over all state values by transmitting states' individual rewards according to a predefined transition matrix, such that similar states have similar state values. In the context of modeling prediction explanation, we treat the IProp attribution value of a pixel/node as the value of the state/node, e.g., $AM_{IProp}$. Then the initial pixels' attribution scores, e.g., obtained by any existing explanation method, are the pixels' individual rewards. With the MRP, the reward for transitioning from a state/node j to i results a in reward R that is equal to i's initial attribution value, e.g., $AM_i$.\nThen, we propagate the information of a pixel's individual reward to other pixels by utilizing the MRP associated with the transition matrix P. In this case, the pixel/state reward naturally contains attribution information from other structurally similar pixels. So, the final IProp attribution value of a pixel/node is the value of the state after propagation ends. In other words, for each state/pixel, we start a walk as a player from the state, and the next state of the walk depends on the transition probability (similarity between pixels). Then, a reward is assigned to the player at each step. The final state value represents the expected cumulative reward for the player after walking with infinite steps. Formally, given an initial attribution map AM, the discounting factor $\\gamma$, and the transition matrix P, we obtain the attribution map of IProp, i.e., $AM_{IProp}$, as:\n$AM_{IProp} = AM + \\gamma \\cdot P \\cdot AM_{IProp}.$\nDirectly obtaining the solution $AM_{IProp}$ is computationally challenging, as it needs to solve the inverse matrix $(I_N - \\gamma P \\cdot AM_{IProp})^{-1}$ of size N, where N is the number of image pixels that is often large. In practice, since P is highly sparse, we often use the value iteration method [21, 22, 31, 42, 42, 44] to iteratively update $AM_{IProp}$. In the k + 1 iteration, we have:\n$AM_{IProp}^{k+1} = AM + \\gamma \\cdot P \\cdot AM_{IProp}^k$\nwhere $AM_{IProp}^{k+1} = AM$. We stop the iteration process until the MSE between $AM_{IProp}$ from two consecutive iterations is smaller than a given tolerance tol. We also prove the convergence of IProp (Theorem. 1) in the appendix.\nTHEOREM 1. The value iteration in IProp (Eq. 3) is guaranteed to converge to the unique solution $AM_{IProp}^{k+1}$ for any initial $AM_{IProp}^k$, i.e., $lim_{k\\rightarrow \\infty} AM_{IProp}^{k+1} = AM_{IProp}^{k+1}$. s.t. $AM_{IProp} = (I_N - \\gamma P) ^{-1} \\cdot AM$."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setup\nWe first generate the attribution map for a model and image using a baseline method (please see below for the baseline methods). Then, we use the IProp to obtain the improved attribution map. We compare the original attribution map and its IProp version both qualitatively and quantitatively.\nBaselines. We use eight pixel- and region-based explanation methods as the baselines. For pixel-based explanation methods, we consider Integrated Gradients (IG) [41], GIG [24], and BlurIG [45] as the IG-based methods. We follow previous work [24] to set the black image as the reference point for IG and GIG, use a step size of 200 as the parameter, and utilize the original implementations with default parameters in the authors' code for all three IG-based methods. We also include the Vanilla Gradient (VG) [36], and follow the original settings for the RISE [29] which generates 4K 7 \u00d77 binary masks first and then upsampling to the original image size for computing the attribution map for each image. Lastly, we include the Grad-CAM (GCAM) [33] with the activations from the last CNN layers.\nFor region-based explanation methods, we implement LIME [32], which works as a superpixel-based explanation method in the image domain. For each image, we first utilize SLIC [2, 3] to segment the image into 200 superpixels (regions) and then generate 4K random binary masks of size 200 with equal probability to be 1 or 0. 1 indicates the superpixel is turned on while 0 means the superpixel is turned off (replace the superpixel with black values). Then we train a logistic regression and report the weights as the importance for the superpixels. Similar to LIME, we modify the original RISE by replacing the randomly generated mask from the pixel level with the superpixel level and utilize the original RISE mechanism to compute the importance score for each superpixel. As the same setting for generating the binary masks, we use 4K samples for each image and refer to the modified version as RISE(S) for the superpixel level. Furthermore, we use the original authors' implementation of XRAI as another region-based explanation baseline approach.\n5.2 Qualitative Results\nFigure 4 visualizes the attribution map (of a set of randomly chosen images for each of the 9 models) obtained by the baseline explanation methods, and the its IProp version. We observe that the baseline attribution maps may not focus on the object itself, and contain noisy attribution scores outside the object. After applying IProp, however, the attribution map has more uniform attribution scores for the objects' relevant pixels. This shows that information propagation can capture pixels' local structural relationships, and hence can help to better explain the predictions. However, qualitative and visual inspections are often subjective, thus we focus on the quantitative metrics in the rest of the experiments.\n5.3 Quantitative Results\nResults on AIC and SIC. This evaluation method begins with a blurred version of the target test image and restores the pixels' values of the most important pixels, as decided by the explanation method, resulting in a bokeh image. Then, an information level is calculated for each bokeh image by comparing the size of the compressed bokeh image and the size of the compressed original image. The information level is referred to as the Normalized Entropy. Based on the amount of information, bokeh images are binned. The average accuracy is then calculated for each bin. AIC represents the curve of these mean accuracy across bins. Additionally, the predicted probability of bokeh versus the original image is calculated for each image within each bin. SIC is the curve of the median value over each bin. The areas under the AIC and SIC curves are computed; better explanation methods are expected to have greater values. In Tables 1 and 2, we present the AUC under the AIC and SIC curves for all baselines and with IProp. We observe IProp consistently improves all baselines, suggesting the IProp's explanations are better aligned with what the models do for their predictions.\nResults on Deletion and Insertion Ratio. Further, we evaluate all explanation methods using the Insertion Score and Deletion Score from prior research [28-30]. For each test image, the insertion technique inserts pixels, from the highest to lowest attribution score, to the black image, then makes the prediction on the modified image. The method produces a curve that represents the predicted values as a function of the percentage of the number of pixels inputted. In contrast, the deletion method deletes the pixels from the original image by replacing those pixels' values with zeros (black image). The insertion and deletion scores are then determined as the AUC. The higher the insertion score or the lower the deletion score implies the explanation method produces better attribution maps. As indicated in the previous research [30], one should consider the insertion and deletion scores jointly. Here, we compute the Deletion-Insertion Ratio. The range for both Insertion and Deletion scores is from 0 to 1, and since a better explanation method should give a higher insertion score and a lower deletion score for each image, then the Deletion-Insertion ratio, e.g., $Deletion-Insertion Ratio = \\frac{Deletion Score}{Insertion Score}$ should have a lower value for a better explanation method. We report the average Deletion-Insertion Ratio (DIR) over all test images in Table 4, and the information propagation improves (decreases) the DIR score from most (72 out of 81) of these baselines. We include Insertion and Deletion scores separately in the appendix.\nResults on ROC-AUC. Following [10, 23, 45], this evaluation metric computes the ROC-AUC by considering the attribution values as the prediction scores which determine whether the important pixels are predicted to be inside a given annotation area. This metric measures how the generated attribution map is similar to the human perspective on that image. Note that it does not directly measure the quality of explanation, since the model could have a different \"perspective\" than humans, e.g., focusing on the different regions to make predictions. We report the ROC-AUC results in Table 3; IProp outperforms most of the baselines.\nResults on Pointing Game. The metric [48] first finds the pixel with the maximum value in the saliency map, then checks whether the pixel lies in the ground truth annotation provided by humans. In other words, the metric computes a hit rate for each attribution method over all of the test images. Tab. 5 shows the Pointing game scores for all models with different attribution methods. Our method improves the metric from most (52 out of 81) of the baselines.\nResults on Sanity Checks. An attribution method should pass sanity checks [4]. When the base attribution method passes the sanity checks, it produces distinct AMs based on different sanity checks. IProp generates distinct AMs, as seen in Fig. 4. Furthermore, following [4], we compare the Spearman rank correlation between the absolute values of the AMs of the pixels, generated using the original model and a model with random weights. Table 6 shows that as long as the base attribution method has a low coef, IProp also has a low coef. As expected, IProp slightly increases the base coef, possibly due to the correlations introduced through the neighborhood; however, the IProp coefs still remain small. Adebayo et al. [4] showed that IG had 0.5 and GBP had close to 1 coef. Hence, IProp is expected to pass the sanity checks as long as the underlying attribution maps satisfy the sanity checks.\n5.4 Practical Analysis\nRuntime of IProp. IProp takes 2 minutes to construct the graph G on a 299x299 image with K = 9. Then it takes 35 seconds to calculate the distance and apply the softmax function to generate the transition matrix P. The value iteration repeatedly updates the attribution map (AM) until convergence. Figure 5 shows the convergence time distribution for the value iteration on the 5K test images with the InceptionV3 model for various tol and base AMs.\nThe Impact of Hyperparameter K. Intuitively, one should expect to use the K that creates the connections between a pixel to all the rest of pixels, e.g., the fully connected pixel graph G, and let the algorithm decide the similarities between all pixel pairs. However, the fully connected graph increases the running time significantly as expected, which makes it impractical to use. On the other hand, given a pixel I, we observe that the similarity in the transition matrix P for farther pixel J is expected to have a value of zero since the geometric distance is already large enough to push the similarity to zero. We conduct an experiment where we compute the similarity vector P* [I, :] (a row in P) using K = 50 for simulating dense connectivity, and use only the spatial distance as the total distance. Similarly, we compute the similarity vectors, $P_K$ [I, :], generated by different values of K. We hypothesize that two similarity vectors P* [I, :] and $P_K$ [I, :] will be very similar since the similarity of the pixel I and further pixels is going to be zero. Furthermore, we compute the KL-divergence between P* [I, :] and $P_K$ [I, :] and present it Figure 6. As the results show, and our default value of K = 9, which is computed as image size of 299 \u00d7 299, and denoted as K* in the figure, is a good approximation for the graph that is generated with much larger K = 50.\nRange for $d_s$ and $d_c$. IProp uses both the spatial distance $d_s$ and color distance $d_c$. In this experiment, we study the range of the two distances. We calculate the spatial distance and color distance for all feasible pairs given an image. $d_c$ values are first grouped by their corresponding $d_s$. Notice that the potential unique values of $d_s$ are smaller than the number of possible neighbors $(2 * K + 1)^2 \u2013 1$. The median value for each $d_s$ group is then recorded. Lastly, we present Fig. 7, which contains the medians for each $d_s$ over all 5K test images relative to the Inception V3 model. As expected, the pair with larger geometric distances also has larger color distances, as distant pixel pairs are expected to be contained in distinct image objects. Note that $d_s$ and $d_c$ are within similar range, contributing equally to the overall distance."}, {"title": "6 CONCLUSION", "content": "We propose IProp, a novel meta-explanation method that leverages the local structural relationships of pixels and is compatible with any existing attribution map-based explanation method. IProp formulates the model explanation as an information propagation among pixels and is guaranteed to converge. Our extensive experiments show that IProp increases the explanation quality of numerous underlying explanation methods for numerous models.\nIn future, we plan to extend the proposed explanation approach on the graph data which have intrinsic (causal) structure similarities [6], and study the robustness of these explanation methods, as they are shown to be vulnervable in the face of adversaries [16, 25]."}]}