{"title": "Governing Al Beyond the Pretraining Frontier", "authors": ["NICHOLAS A. CAPUTO"], "abstract": "This year, jurisdictions worldwide-including the United States, the European Union, the United Kingdom, and China-are set to enact or revise laws governing frontier AI. Their efforts largely rely on the assumption that increasing model scale through pretraining is the path to more advanced Al capabilities. Yet growing evidence suggests that this \"pretraining paradigm\" may be hitting a wall and major AI companies are turning to alternative approaches, like inference-time \"reasoning,\" to boost capabilities instead.\nThis paradigm shift presents fundamental challenges for the frontier Al governance frameworks that target pretraining scale as a key bottleneck useful for monitoring, control, and exclusion, threatening to undermine this new legal order as it emerges. This essay seeks to identify these challenges and point to new paths forward for regulation. First, we examine the existing frontier AI regulatory regime and analyze some key traits and vulnerabilities. Second, we introduce the concept of the \"pretraining frontier,\" the capabilities threshold made possible by scaling up pretraining alone, and demonstrate how it could make the regulatory field more diffuse and complex and lead to new forms of competition. Third, we lay out a regulatory approach that focuses on increasing transparency and leveraging new natural technical bottlenecks to effectively oversee changing frontier AI development while minimizing regulatory burdens and protecting fundamental rights. Our analysis provides concrete mechanisms for governing frontier AI systems across diverse technical paradigms, offering policymakers tools for addressing both current and future regulatory challenges in frontier AI.", "sections": [{"title": "1 INTRODUCTION", "content": "Frontier Al regulation stands at a turning point. In the United States, the Trump Administration just rescinded Exec-\nutive Order 14110, the country's main framework for regulating frontier AI[81], and may soon put forward its own\napproach. The European Union AI Act's provisions on General-Purpose AI (GPAI) are set to come into force this\nsummer[22] and the government of the United Kingdom has promised to pass a bill dedicated to frontier AI[31] this\nyear. China may promulgate its own comprehensive AI law, which has been circulated in draft since 2023[48]. At the\nsame time, international processes in forums like the United Nations [11], OECD[77], and AI Summit Series [27] raise\nthe prospect of a global regulatory framework.\nThese efforts reflect a worldwide consensus that frontier AI, \"highly capable general-purpose AI models that can\nperform a wide variety of tasks and match or exceed the capabilities present in today's most advanced models\" [26],\nrequires serious regulatory treatment. Though far from the only form of Al in need of regulation [12, 51], frontier AI\npresents novel challenges to global safety and security [5, 25, 70] because of its broad and rapidly-improving capabilities.\nGovernments are seeking to meet these challenges through regulation this year.\nUnfortunately, nearly every major regulation in force or in draft relies heavily on a key technical assumption that\nhas been undermined by recent events: the belief that \"scaling\" Al models through pretraining runs with ever more\ncompute and data is the primary driver of frontier AI capability gains. This assumption shapes core regulatory mech-\nanisms, from triggers for legal coverage(like the EU AI Act's 1025 FLOPs threshold[1]) to enforcement strategies (as\nwith US export controls on advanced microchips [36]). The assumption that scale is the main driver of capabilities has\nbeen true in the \"pretraining paradigm\" of the last several years [69], and this paradigm has had benefits for regula-\ntory design because it has created conditions of relative transparency, predictability, and centralization in the field of\nfrontier AI. Scaling requires massive and increasing quantities of scarce resources, particularly compute and energy,"}, {"title": "2 THE PRETRAINING PARADIGM", "content": "Over the past few years, frontier Al has been driven by a simple principle: more scale means more capabilities.\nResearchers have found that pretraining autoregressive models with more and more compute and data produces\nbetter performance across a variety of domains without the need for architectural innovations or domain-specific\nengineering [13, 20, 69]. In many ways, this trend is just an instantiation of the \u201cbitter lesson,\u201d the general finding that\nAl capabilities growth mostly derives from increases in computing power rather than human breakthroughs [83]. But\nthe development of the transformer architecture [84] and the marshaling of massive amounts of data and compute by\nfrontier AI companies like OpenAI, Anthropic, and Google DeepMind has led to a step change in AI capabilities.\nA key feature of this pretraining paradigm is its predictability. Researchers have documented the existence of \"scal-\ning laws\" which show that cross-entropy loss (a measure of model performance) decreases reliably as data and compute\nincrease such that the performance of the model being pretrained can be roughly forecasted from its inputs [7]. Cross-\nentropy loss correlates reasonably well with practical capabilities across a variety of economically valuable tasks, as\nan increasing number of use cases and benchmarks show [47, 49, 53, 72]. And because the resulting models are capa-\nble of aiding or performing economically valuable tasks like computer programming[74], investment into capabilities\nwill continue. The relationships among scaling, benchmarks, and the real world are not exact, and models have so far\ndiffused into the economy slowly and underperformed expectations in many practical applications. At the same time,\nthey also occasionally exceed expectations by displaying unexpected \"emergent\" abilities that were not predicted from\ntheir training objectives[86]. But overall, companies have been able to forecast the returns to scale and make massive\ninvestments in improving frontier AI with relative confidence about what they're getting.\nImportantly, though the basic lesson of the scaling laws is that more is better, certain ratios of compute and data\ninputs are optimal for getting the best performance from pretraining[34]. Increasing one input beyond the bounds of\noptimality to compensate for limits on the other can provide moderate gains, but diminishing returns start to bite and\nthe constraints become binding. For years, compute has been the limiting resource and companies have spent heavily\non gathering enough of the scarce and powerful cutting-edge microchips necessary for large training runs [8]. These\ntraining runs will, if scaling continues, only grow more massive, as firms push Moore's law [6] and invest billions of\ndollars into building out the compute and energy resources that will be necessary to support such runs.\nYet over the past few months, returns from scaling up pretraining seem to have begun to diminish. Many leading\nAl companies, including OpenAI, Anthropic, and Google DeepMind, have reportedly been disappointed with the mod-\nels produced from their latest big pretraining runs [52, 65, 78]. Eminent researchers, including some who have long\ntrumpeted the benefits of scaling[82], have predicted the end of the pretraining paradigm, citing the limited supply of\ntraining data as the cause. Frontier models are mostly trained on large corpuses of internet text comprising trillions of\ntokens, small pieces of words, and the number of such tokens is finite. One estimate puts the total stock in the world at\naround 300 trillion, meaning that data was expected to run out in a few scaling generations if no data breakthrough is\nmade[85], but leading models seem to use around 15 trillion tokens [3, 18] and companies are apparently running into\ntrouble already. If frontier AI companies are already finding that the era of pretraining is ending, then it is possible\nthat new directions will already have to be taken for AI progress to continue. And regulations premised on the idea\nthat scaling with remain the driver of capabilities will have to change."}, {"title": "3 SCALING-BASED REGULATION AROUND THE WORLD", "content": "Despite frontier Al's mounting capabilities and risks, regulatory frameworks aimed at governing these systems remain\nrelatively scarce, and many proposed laws have not made it to promulgation[42]. However, what regulations do exist\nhave relied on the pretraining paradigm and would be undermined by its end. Analyzing how these laws operate today\ncan inform how new, more resilient frameworks can be developed for the next paradigm of capabilities progress.\nBefore diving into key features of the regulations themselves, it is worth laying out a brief overview of some of their\ncommon aims and features. First, regulations aimed at frontier models generally address the risk that advanced AI sys-\ntems will be capable of enabling or causing tremendous harms, particularly chemical, biological, radiological, nuclear\n(CBRN), and cyber- attacks, mass persuasion and disinformation campaigns, and loss of control of AI[5]. Advocates\nof regulation, including from industry, admit that while these harms have not occurred yet, there is robust evidence\nto suggest that they are likely to manifest and at least a robust monitoring and oversight regime is necessary now\nto identify them when they emerge[28, 41]. Second, frontier regulations are often layered atop more targeted sectoral\nregulations in areas like housing and consumer protection that seek to prevent harms from AI like bias, discrimination,\nand exploitation occurring today [2, 21]. This layered approach provides a reasonable guarantee against the specific\nharms that only frontier models can cause, allowing sectoral regulators to focus on the areas that they are most com-\npetent in. Third, frontier regulators across jurisdictions rely on similar tools and on cooperation with each other to\nface this globally-emerging technology [15]. Because of the nascent stage of both frontier AI and its governance, de-\nveloping better understanding of frontier AI through scientific and evaluative tools and transparency and reporting\nrequirements placed on model developers has been the focus of many regulations, though some specific obligations\nlike risk tiering and associated mitigations have also been put into place [2, 71]. Companies also have committed to\na surprising degree of self-regulation[28]. Finally, frontier regulations are generally aimed at natural bottlenecks like\ncompute and at the large and well-resourced companies pushing the frontier of AI to reduce the cost and burdens of\nregulation by keeping them narrow and focused [2, 71]. These initial steps are promising, but many rely on the pre-\ntraining paradigm as a trigger for coverage or to focus their application. The erosion of that paradigm risks seriously\nundermining their effectiveness."}, {"title": "3.1 The EU AI Act and its Code of Practice", "content": "The EU AI Act is probably the most significant and comprehensive AI law in the world and contains provisions ded-\nicated to frontier AI. The Act classifies frontier Als as \"General-Purpose AI (GPAI) Models with Systemic Risk\" and\nimposes special obligations on them [1]. Providers of such models must evaluate the systemic risks generated by their\nmodels and act to mitigate those risks, as well as report serious incidents caused by the models and the steps taken to\ncorrect them [2]. GPT-4 and similar models already qualify [66]. To guide companies in complying with the Act, the EU\nAI Office initiated the development of a GPAI Code of Practice (CoP), currently being developed[62]. Frontier models\nwill also be covered under the standard provisions of the EU AI Act with respect to their various applications, such\nas if they are used in critical infrastructure or to determine access to education [2]. This two-tier system of regulation\naims at preventing and mitigating harms up and down the stack of development and deployment.\nThe Act and CoP are intended to be future-proof and updated in line with changing technology. Article 51 of the\nAct, which provides the legal triggers for coverage as a GPAI with Systemic Risk, provides a useful demonstration\nof how the law seeks manifest this intent and also the difficulties of doing so. Article 51 provides two alternative\ntriggers for classification of a model as a GPAI with Systemic Risk: first, evaluations that demonstrate that it has \"high"}, {"title": "3.2 US Executive Orders and Export Controls", "content": "The United States' frontier AI regulatory regime is being rewritten by the Trump Administration [81]. But certain parts\nof the frontier Al framework which are vulnerable to the decline of the pretraining paradigm may persist in letter or\nin spirit into new regulations. The Biden-era frontier Al regulatory framework had two main elements, the Executive"}, {"title": "3.3 The Forthcoming UK Frontier Al Bill and Existing Al Safety Institute", "content": "The UK government has indicated that it will put forward a narrow bill regulating frontier Al this year, building on\nexisting infrastructure like the UK AISI[31]. While it is unclear exactly what the bill will contain, it will need some kind\nof trigger for legal coverage of Al systems that allows it to be targeted at frontier AI. The chosen trigger may resemble\nthe compute thresholds discussed above with respect to the EU AI Act and US EO 14110 or it may point toward a new\nway to cover models as the pretraining paradigm declines and compute thresholds become less effective tools. Much\nmore will be known (and can be analyzed) when the bill is released in the coming weeks.\nHowever, the UK has made significant contributions to the international frontier Al governance regime already,\nparticularly in the form of the UK AISI. UK AISI operates as a government-funded scientific authority that provides"}, {"title": "3.4 Chinese Rules from Specific to Comprehensive", "content": "China has a suite of laws aimed at regulating Al systems, but few of them so far have been aimed at frontier Al itself[79,\n80]. The existing Chinese laws and regulations most relevant to frontier AI generally focus on specific applications\nof models, such as provisions governing algorithmic recommendation systems on the internet[58] and provisions\ngoverning \"deep synthesis\" (effectively deepfake) algorithms[59]. The Chinese government has also sought to use\nthese laws to build regulatory capacity and institutions over time, for example creating an \"Algorithm Registry\" in\n2021 for an expanding list of kinds of algorithms now including frontier Als[58, 79].\nAs cutting-edge AI has shifted away from specific applications and toward the more general capabilities charac-\nteristic of frontier models, Chinese governance seems to be adapting toward a more general and full-stack form of\nregulation. The 2023 Measures for Generative AI, prompted by ChatGPT, aim not just at the use of models but also at\ntheir training and the data used to create them [60]. In 2023, the Chinese Academy of Social Sciences circulated a draft\nArtificial Intelligence Law that would provide a relatively comprehensive overall regulatory regime and also included\nprovisions targeted at frontier AI[57]. The draft law points toward a higher-level, coordinated form of AI regulation\nwith stronger teeth. Of particular note is the proposed Negative List, which sets out a list of kinds of models that\nmust receive government permission before they can be deployed. There are also provisions aimed directly at frontier\nmodels that create enhanced reporting requirements for the development and use of these models. The draft National\nAI Laws requires that AI companies promote the safety (or security) of their Al systems, though exactly what that will\nentail remains to be determined at this point [79]. Though the official version of the law has not been released as of it,\nit is likely that something will happen with overall regulation soon.\nThe Chinese regulatory regime may be best equipped to deal with a transition away from the pretraining paradigm.\nChina has developed significant regulatory capacity in Al across a range of fronts, from narrow recommendation\nalgorithms to leading foundation models like those developed by DeepSeek and Alibaba [18, 87]. In particular, the\nAlgorithm Registry suggests that the Chinese government thinks that it can handle tracking and evaluating huge\nnumbers of algorithms across a range of tasks, something that might be necessary if more companies can produce\nfrontier models. China could also benefit from a shift away from compute as the binding constraint on capabilities\nprogress because it would negate the effects of US export controls and shift the competition to different ground, where\ncompanies like DeepSeek could compete to push the frontier less handicapped by export controls. A more diffuse and\ndecentralized frontier may not present the same challenge to Chinese authorities more used to monitoring a broad\nrange of entities than those in the US or Europe. But exactly what path China will take will be unclear until a formal\nlaw governing frontier AI is put forward and its provisions clarified."}, {"title": "4 WHAT COMES AFTER THE PRETRAINING PARADIGM?", "content": "The pretraining paradigm may be coming to an end, but Al capabilities continue to advance rapidly. Companies\nare plowing hundreds of billions of dollars into Al capital expenditures, suggesting strong confidence in continued\nprogress[8, 24, 29]. New research directions like the recently-released \"reasoning\" systems [19, 44, 63] demonstrate\npaths beyond pure scaling of pretraining. Especially if systems begin to build on each other, as may be happening with\nreasoning models using synthetic data generated by pretrained models [44], progress may be rapid.\nMany futures for frontier Al are possible from here. Progress could stall into another \"AI winter\" characterized by\nslow growth, broken promises of global transformation, and backlash against recent hype. Technical breakthroughs\ncould easily overcome the \"data wall\" and other obstacles to scaling up pretraining such that it remains the dominant\nsource of Al progress. In either of these scenarios, the effectiveness of existing regulatory frameworks would likely\nbe preserved because, in the first case, there would be few new frontiers to govern and, in the second case, existing\napproaches aimed at compute governance would continue to work. However, recent breakthroughs make change seem\nlikely, creating a need for analysis of how frontier Al regulation could function in a new capabilities paradigm.\nThree key variables could shape this potential new era:\n\u2022 First, how far beyond current capabilities can companies push using new approaches? If further progress is\neasy, then incumbents will likely maintain their leads, but other companies might catch up if it is hard.\n\u2022 Second, what is the main driver of capabilities? If human-driven innovation is the source of growth and, conse-\nquently, is the constraint on it, regulation may need to shift from focusing on predictable and trackable inputs\nlike compute to more complex oversight of research and development.\n\u2022 Third, how long will the new paradigm last? Will it be a brief interlude before some form of scaling returns?\nOr will it be a longer period of distributed growth?"}, {"title": "4.1 A Crowded Pretraining Frontier?", "content": "One crucial structural question will be whether the end of the pretraining paradigm creates a kind of cap on general\ncapabilities progress that companies must use other approaches to get beyond. In such a world, a \"pretraining fron-\ntier\" might emerge at the maximum overall capabilities threshold enabled by scaling up pretraining. Recent reporting\nthat OpenAI, Anthropic, and Google DeepMind have been unable to keep scaling model pretraining [52, 65] and the\nclustering of top models around the GPT-4 level over the past few years suggest that such a cap might exist. If alter-\nnatives like reasoning are not able to provide substantial increases in overall capabilities but rather push forward only\nparts of the capabilities graph (as some benchmarking of the reasoning models suggests[55, 56]), then overall progress\nmay slow and the pretraining paradigm's advantages for governance would dissipate. Leading companies may main-\ntain scale and resource advantages to dominate the coming paradigm, a possibility analyzed below, but a shift toward\ndeconcentration would have serious implications for governance and is worth discussing.\nSlowed frontier progress could have significant structural effects on the industry. A lack of overall progress from\nthe incumbents would likely mean that more companies could catch up to leaders, reaching the pretraining frontier\nand competing for breakthroughs beyond it. As the history of the last few years shows, there is only a lag of a few\nmonths or years between when the leading companies release a model and when other groups, including open-source\ndevelopers, catch up and release a model with equivalent capabilities [18, 19, 30].\nFurthermore, we should expect the pretraining frontier to get populated quickly if further scaling up pretraining\nslows. Lack of access to large quantities of compute has been a main obstacle to new companies joining the frontier AI"}, {"title": "4.2 A Return to Scale?", "content": "But even a hard capabilities cap, if one emerges, will not last forever, and frontier Al companies are already finding\nways beyond pretraining to improve capabilities. Many of these efforts will likely involve seeking new forms of scaling\nbecause scaling creates useful predictability for incumbent players and allows them to leverage their leads and resource\nadvantages. The development of the new reasoning systems by OpenAI illustrates this trend. First, the reasoning\nsystems seem to use inference scaling, spending more compute at inference time rather than during pretraining, to\nget better results[44]. The continued success of inference scaling would mean that players with significant compute\nresources would retain an advantage. Second, leading companies are rumored to be using synthetic data generated by\nfoundation models to train their new systems; synthetic reasoning traces generated in this manner are apparently a key\npart of reasoning systems[19, 44]. This method transforms data into a new kind of compute scaling where companies\ncan leverage existing compute resources to generate more and better synthetic data, improving capabilities and thus\nmaintaining the advantage provided by their hyperscaler infrastructures."}, {"title": "5 REGULATING BEYOND THE PRETRAINING FRONTIER", "content": "If the pretraining paradigm does give way to some new source of AI capabilities, how can frontier Al governance\nadapt? The pretraining paradigm provided two core advantages for governance, legibility and efficiency, that will\nlikely diminish if the paradigm ends. Because of the predictability of scaling laws and the key position held by compute\nin capabilities progress, regulators had good visibility into the sources of risk and could target narrow but effective\nregulations at the entities best situated to bear the costs of regulation. These entities were usually big companies that\ncould be regulated high up the AI stack, avoiding burdening the rights of users and intruding on their uses of AI. If the\nfrontier field becomes more complex and risks increase, regulators will have to figure out how to address those risks\nwhile maintaining respect for rights and the light touch that has facilitated innovation so far.\nThere are two main ways that the regulatory field could become more complex if the pretraining paradigm declines:\nfirst, the number of companies competing to push forward the frontier substantially increases and second, the new ca-\npabilities approaches that they seek are less predictable and more risky than past methods. To confront these enhanced\ndifficulties while preserving the virtues of legible and efficient regulation, regulators have three basic options. First,\nthey could seek to increase the transparency of the frontier Al field and improve their understanding of it. Second,\nthey could try to find or create bottlenecks that can be targeted to allow for more effective and less intrusive regula-\ntion, replicating the role of compute in the pretraining-oriented regulatory regime. Third, they could build up greater\nregulatory capacity to handle a more unpredictable and complicated frontier Al environment.\nOne of the main lessons of the paradigm shift that is likely occurring right now is that regulating prematurely and\nheavy-handedly based on technological assumptions that can later change is an ineffective recipe for governance. And\neach of the approaches just presented has its potential overall downsides. Trying to retain or recreate bottlenecks on\nthe development of frontier Al will limit the development of these systems, locking away benefits that they could\nprovide, and could artificially increase the concentration of the industry and provide significant opportunities for rent\nseeking and regulatory capture. Transparency and regulatory capacity building are a good and necessary steps for\nunderstanding and responding to the challenges of new technologies, but capacity can be used for bad ends and if the\ngovernment is empowered to granularly monitor and control the use of a technology that will likely soon become an\nessential part of how people act in the world and develop themselves, abuses of rights could proliferate."}, {"title": "5.1 Transparency and the Science of Frontier Al", "content": "The first requirement of effective regulation is getting an accurate understanding of the thing that is being regulated.\nTransparency into the operations of frontier AI companies is one key way to get that kind of understanding [9, 10]. Ex-\nisting regulations have sought to improve regulators' view into leading labs [2, 71] and benchmarking and evaluations\nhelp ascertain the capabilities of AI models such that companies and governments can determine what risks they pose\nand how to respond to those risks. These tools and frameworks provide a useful baseline for frontier AI regulation and\nincreasing investment in them and speeding their rollout is useful.\nHowever, the end of the pretraining paradigm creates challenges for transparency because of the potential increased\ncomplexity of the industry and of the sources of capabilities and risk. Instead of being able to concentrate regulatory\nresources on a small number of leading companies and provide robust evaluations and analyses of the top models,\nregulators may have to investigate more widely.\nThe regulatory challenge will depend on how the end of the pretraining paradigm affects the frontier AI field. If a\nfew large companies remain the leading players and can roughly predict the trajectory of capabilities improvements,\nthen regulators will be able to continue targeting their attention on those companies and the new breakthroughs and\napproaches they develop. On the other hand, if the industry deconcentrates and many smaller companies in different\njurisdictions can get to the capabilities frontier and push it in different ways, then transparency will be harder because\nregulators will struggle to cover the regulatory frontage. In such a world, regulators should try to improve their reg-\nulatory coverage as much as possible by seeking transparency into likely leading players and, in areas in which full\ncoverage of companies is impossible, trying to cover sources of progress rather than players themselves. The number\nof paths forward for capabilities will likely be smaller than the number of companies at the frontier, as demonstrated\nby the return of reinforcement learning in the reasoning models [44]. If regulators can get a sense of what the state of\nthe art is in a given area of frontier AI, they can then monitor for overall breakthroughs in that area rather than looking\nat every specific company who might be contributing to progress. Similarly, grouping models by some criterion like\nparameters, overall risks, or evaluations scores would allow for economization on regulatory coverage because models\nthat are similar across relevant criteria could be monitored as a group rather than individually.\nThere are weaknesses to this kind of classification approach to transparency. Fundamentally, it relies on the relative\npredictability of sources of risk and the ability to identify them ahead of development or deployment. But if future\narchitectural breakthroughs improve the capabilities of smaller models beyond a point at which they are capable of\ndoing serious harm or a similar paradigm in which risk sources are very hard to predict, transparency into a select\nnumber of companies or models will be insufficient and increasing monitoring the only option."}, {"title": "5.2 Regulating Data", "content": "Regulators could also seek natural bottlenecks on capabilities progress that allow for focused attention like compute\nbottlenecks have allowed in the pretraining paradigm. Given that pretraining paradigm may come to an end because\nlimitations on the supply of data prevent further effective scaling up of the size of models, data could become the\nnew constraint on model progress that allows for targeted regulation. The use of synthetic data in recent reasoning\nbreakthroughs [44] makes this unlikely to be a panacea, but it is possible that access to key data sources will be necessary"}, {"title": "5.3 Regulating Overall Compute or Inference", "content": "Compute may remain a useful target for regulation after the pretraining paradigm if it ends up being necessary for the\nsuccess of new approaches like inference scaling. However, existing laws would have to expand their reach to cover\nforms of compute usage beyond pretraining. Furthermore, such expanded compute governance approaches would\nlikely face significant new technical challenges and advances in distributed computing [38, 68] suggest that they are\nunlikely to work as effectively as pretraining compute governance can today.\nRegulating compute broadly, rather than in specific uses, seems like one place to start. In this kind of approach, rather\nthan counting only the amount of compute used in pretraining a model, as the EU AI Act seems to today [1], all compute\nused in a model's development and deployment would be counted toward determining whether it qualifies as a frontier\nmodel and is subject to the relevant requirements for those models [14]. Because inference in closed models happens in\nthe cloud, cloud service providers like Amazon Web Services or Microsoft Azure could be made responsible for tracking\nwhen significant expenditures of inference compute were being made that hit a certain threshold above the cumulative\ncompute that the model being used had been trained on. Unless the compute expenditure was authorized or being\nmade by a trusted party, these providers would then report those expenditures to regulators who could investigate the\nusage to determine if it constituted a source of risk. Such a system would be more intrusive than the kind of simple\ncompute governance that focuses only on whether a large training run is occurring because it would require learning\nsomething about what the compute was being used for. It also might not work technically because it could turn out\nto be possible to achieve dangerous capabilities in models without spending extraordinary amounts of compute such\nthat such activity was differentiable for safe use. But if risks escalate sharply in the new paradigm, looking into more\ninvasive forms of monitoring that still limit surveillance harms might be a necessary step to take.\nIt seems likely that some form of broader compute governance will be necessary in the forthcoming regime because\nspending large quantities of compute is a reasonably reliable source of capabilities improvements across domains."}, {"title": "5.4 Regulating Information", "content": "Another potential target of regulation could be information in the form of algorithmic breakthroughs and similar dri-\nvers of capabilities progress. If it turns out that innovation is the key input for new forms of frontier AI, then the\ndissemination of the ideas behind these innovations will be a key bottleneck on how quickly different companies\ncan catch up to and push the frontier. Furthermore, regulating algorithmic breakthroughs would allow governments\nto avoid regulating users and uses of frontier AI, instead focusing higher up the stack. Restrictions on classified in-\nformation and the nuclear \"born secret\" regimes [54] demonstrate that regulating information is possible in certain\ncircumstances, though unlikely to be robust over time.\nIn general, regulating information in the form of algorithms or other kinds of breakthroughs would be unlikely to\nwork for very long and probably poses costs too high to be warranted. Regulating ideas is extremely difficult, would\nlikely stifle important benefits of Al progress, and also presents extreme risks of abuse. In a more diffuse and complex\ncapabilities paradigm, it would be difficult for regulators to even know what information to regulate, and requiring\nresearchers to register potential breakthrough ideas with the government for monitoring and processing would create\na significant burden on innovation and provide uncertain benefits. Furthermore, information on Al improvements and\nhow they happen seems to leak out quickly and other researchers have been able to replicate breakthroughs given little\ninformation about how they work. The replication of OpenAI's 01 with DeepSeek's R-1 within a few months[19, 45]\ndemonstrates the difficulty of preventing outside groups from figuring out how an advance works and finding a way\nto replicate it. Regulating information through classification and restriction might be necessary in some extremely\nnarrow and dangerous domains, but it should not be a main strategy for regulating frontier AI.\nHowever, it is possible that intellectual property (IP) law, a different kind of information regulation from classifi-\ncation and other control regimes, might become more important in a world in which innovations are the source of\nprogress. Leading companies are already releasing fewer public research papers on their improvements [50] and they\ncould seek to hold their innovations tighter still if they matter more for capabilities growth. IP law tries to strike a\ncareful balance in encouraging innovation by rewarding people for their work while avoiding creating detrimental\nrestraints on competition, and it is possible that it contains tools like patent buyouts [43] that would allow regulators\nto get more control over particularly dangerous advances while still rewarding innovation."}, {"title": "5.5 Capacity-Building", "content": "Finally, building up regulatory capacity would broadly benefit efforts to respond to changing technology by increasing\nthe ability of regulators to understand where the technology is going and what the sources of risk are, both from new\ncapabilities and from new actors. Existing investments into capacity-building like that into the UK AISI [23] provide\na useful foundation for progress here, but the United States in particular must ensure that it develops the capacity\nto keep up with the changing technology. Beyond generally gathering technical expertise, regulators should focus\non forecasting how the technology could change and developing evaluation and monitoring systems that are flexible\nenough to manage different modes of progress. For example, many existing evaluations of frontier models focus on elic-\niting model capabilities (often through benchmark scores) rather than directly determining the relationship between"}, {"title": "6 CONCLUSION", "content": "The end of the pretraining paradigm, if it comes, will present fundamental challenges for frontier Al governance.\nCurrent regulatory frameworks, built around assumptions of continued pretraining scaling, may become misaligned\nwith the trajectory of frontier AI. Whether the pretraining frontier operates as a kind of cap on overall capabilities\nthat deconcentrates the field and ushers in an era of more diverse and complicated innovations or incumbent players\nmanage to find new ways to push ahead, regulations that rely on pretraining scaling will need significant adaptation.\nRegulators who want to preserve the virtues of efficiency and legibility that the pretraining paradigm has enabled will\nhave to seek new ways to stay with the moving frontier. First, enhanced approaches to transparency and regulatory\ncapacity-building will be necessary to cover a more diverse and complicated frontier field. Second, identifying whether\nnew technical bottlenecks to Al progress emerge that can offer regulatory leverage without stifling innovation or\nviolating rights will be a crucial part of developing effective and well-targeted regulation. This paper has sought to\ninform current and future regulation by laying out some initial steps forward into a new regulatory paradigm beyond\nthe pretraining frontier that ensures that frontier Al is developed safety and in alignment with human values."}]}