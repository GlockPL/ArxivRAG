{"title": "MOSH: MODELING MULTI-OBJECTIVE TRADEOFFS WITH SOFT AND HARD BOUNDS", "authors": ["Edward Chen", "Natalie Dullerud", "Thomas Niedermayr", "Elizabeth Kidd", "Ransalu Senanayake", "Pang Wei Koh", "Sanmi Koyejot", "Carlos Guestrin"], "abstract": "Countless science and engineering applications in multi-objective optimization (MOO) necessitate that decision-makers (DMs) select a Pareto-optimal solution which aligns with their preferences. Evaluating individual solutions is often expensive, necessitating cost-sensitive optimization techniques. Due to competing objectives, the space of trade-offs is also expansive \u2013 thus, examining the full Pareto frontier may prove overwhelming to a DM. Such real-world settings generally have loosely-defined and context-specific desirable regions for each objective function that can aid in constraining the search over the Pareto frontier. We introduce a novel conceptual framework that operationalizes these priors using soft-hard functions, SHFs, which allow for the DM to intuitively impose soft and hard bounds on each objective \u2013 which has been lacking in previous MOO frameworks. Leveraging a novel minimax formulation for Pareto frontier sampling, we propose a two-step process for obtaining a compact set of Pareto-optimal points which respect the user-defined soft and hard bounds: (1) densely sample the Pareto frontier using Bayesian optimization, and (2) sparsify the selected set to surface to the user, using robust submodular function optimization. We prove that (2) obtains the optimal compact Pareto-optimal set of points from (1). We further show that many practical problems fit within the SHF framework and provide extensive empirical validation on diverse domains, including brachytherapy, engineering design, and large language model personalization. Specifically, for brachytherapy, our approach returns a compact set of points with over 3% greater SHF-defined utility than the next best approach. Among the other diverse experiments, our approach consistently leads in utility, allowing the DM to reach >99% of their maximum possible desired utility within validation of 5 points.", "sections": [{"title": "1 INTRODUCTION", "content": "Various critical real-world applications of optimization, including healthcare, drug discovery, engineering design, and deep learning, involve optimizing over multiple, often expensive, and competing objectives f1(x),..., f\u2081(x), termed multi-objective optimization (MOO) (Fromer & Coley, 2023; Luukkonen et al., 2023; Xie et al., 2021; Yu et al., 2000; Papadimitriou & Yannakakis, 2001). In general, the intention in such real-world applications is to select a single set of usable parameters that lie on the Pareto frontier (PF) \u2013 parameters that lead to the ideal set of trade-offs as determined by some decision-maker (DM). However, due to the often continuous and competing nature of multiple objectives, searching over the entire space of trade-offs is unmanageable. Thus, selecting the ideal point along the Pareto frontier takes the form of an iterative process by which the DM explores possible trade-offs prior to making an informed final decision which satisfies their preferences (Liu et al., 2021).\nIn the healthcare domain, MOO problems commonly appear as the interplay between maximizing targeted treatment while limiting harmful side-effects in a patient. Electing a suitable set of trade-offs that correspond to dominant clinical opinion is therefore crucial, as the result is high-risk and significantly impactful to patients. The use of brachytherapy, internal radiation therapy for cancer treatment, presents an example of this kind (Deufel et al., 2020). In brachytherapy, the clinician"}, {"title": "2 MULTI-OBJECTIVE OPTIMIZATION WITH SOFT-HARD FUNCTIONS", "content": "In this section, we outline the primary optimization setting we consider and introduce notation used throughout the paper in \u00a72.1. We develop the intuition and motivation for SHFs and define them explicitly in \u00a72.2. We thus arrive at the problem definition through our mini-max formulation in \u00a72.3."}, {"title": "2.1 MULTI-OBJECTIVE OPTIMIZATION BACKGROUND", "content": "As the name suggests, a multi-objective optimization (MOO) problem is an optimization problem that concerns multiple objective functions. Any MOO problem can be written as the joint maxi-mization of L objective functions over some input space X C Rd,\n$\\max_{X \\in X} (f_1(x),..., f_L(x))$\nin which each fe, l \u2208 [L], defines a function fe : X \u2192 R. Broadly speaking, there does not typically exist a feasible solution that marginally optimizes each objective function simultaneously. Therefore, work in MOO generally focuses on Pareto-optimal (PO) solutions. A feasible solution x\u2020 is considered Pareto-optimal if no objective can improve without degrading another; in other words, if x\u2020 is not Pareto-dominated by any other solution (see definition in Appendix A.3).\nA common approach to multi-objective optimization is to convert the L-dimensional objective to a scalar in order to utilize standard optimization methods via a scalarization function. Scalariza-tion functions typically take the form sx : RL \u2192 R, parameterized by A from some set A in L-dimensional space (Roijers et al., 2013; Paria et al., 2019). For instance, the general class of linear scalarization functions sx(y) := {x+y | X \u2208 \u039b} constitutes all convex combinations of the objectives in A. The parameters A \u2208 A can be viewed as weights, or relative preferences, on the objective functions in the scalarized optimization objective $\\max_{x \\in X} s_{\\lambda} ([f_1(x),..., f_L(x)])$. Then, the advantage of using scalarization functions is that the solution to maximizing $\\max_{x \\in X} s_{\\lambda} ([f_1(x), ..., f_L(x)])$, for a fixed value of \u5165, is a solution along the PF."}, {"title": "2.2 SOFT-HARD UTILITY FUNCTIONS", "content": "Many practical applications of multi-objective problems, e.g. engineering design and healthcare, include strict constraints on the optimization that define the feasible set of solutions. While these constraints can be directly incorporated into scalarized objectives via penalty methods involving"}, {"title": "2.3 PROBLEM DEFINITION", "content": "Given a selected class of scalarization functions sa parameterized by A\u2208 A and an SHF utility function as defined above, we wish to elucidate a set of useful points along the PF in the optimization problem\n$\\max_{X \\in X} s_{\\lambda} (u_f(x))$"}, {"title": "3 STEP 1: DENSE PARETO FRONTIER SAMPLING WITH BAYESIAN OPT.", "content": "In this section, we consider the goal of obtaining a dense set of Pareto optimal points. As described earlier, since the DM's preferences, \u5165*, are unknown to us, we wish to obtain a set D which is diverse, high-coverage, and is modeled after the DM-defined SHFs. As is typical in various science and engineering applications, we assume access to some noisy and expensive black-box function often modeled with a Gaussian process (GP) (Williams & Rasmussen, 1995) as the surrogate function. To achieve that goal, we extend our formulation (2) into a Bayesian setting, assuming a prior p(x) with support A imposed on the set of Pareto optimal values and using the notion of random scalarizations (Paria et al., 2019). In this continuous and stochastic setting, we assume that each of the l\u2208 [L] objectives are sampled from known GP priors with a common domain, and produce noisy observations, e.g. ye = fe(x) + e, where \u2208 ~ N(0, \u03c3\uace8), l \u2208 [L]. We optimize over a set of scalarizations weighted by the prior p(x).\nOur overall aim is still to obtain a set of points C on the PF which are robust against the worst-case potential *, within the user-defined SHF. For computational feasibility, however, we convert the worst-case into an average-case maximization (Appendix A.1.1). Taking into consideration the aforementioned set of assumptions, we modify the formulation (2) to be the following:\n$\\max_{D \\subset X, |D|<kD} E_{\\lambda \\sim p(\\lambda)} [\\frac{\\max_{x \\in D} S_{\\lambda} (u_f(x))}{\\max_{x \\in X} S_{\\lambda} (u_f(x))}]$", "subsections": [{"title": "Random Scalarizations.", "content": "In this section we describe our sampling-based algorithm to optimize Equation 3. Similar to Paria et al. (2019), we use the notion of random scalarizations to sam-ple a A from p(x) at each iteration which is then used to compute a multi-objective acquisition function based on sa and the SHF. The maximizer of the multi-objective acquisition function is then chosen as the next sample input value to be evaluated with the expensive black-box func-tion. For the multi-objective acquisition function, we use a variant of the Upper Confidence Bound (UCB) heuristic where the scalarization is performed over the UCB values of each objective. We define the acquisition function as acq(u, dt, x) = 8x\u2081 (Up(x)) where f(x) = \u03bct(x) + \u221a\u03b2\u03b5\u03c3\u03c4(x) and \u1e9et = 0.125 x log(2 \u00d7 t + 1). \u03bct(x) and ot(x) denote the posterior means and variances at x in step t. For \u00dft, we followed the optimal suggestion in Paria et al. (2019). We select the UCB heuristic for its prevalence and simplicity, however, other heuristics should suffice as well. The full algorithm, which we refer to as MoSH-Dense, is described in Algorithm 1."}]}, {"title": "3.1 SHF UTILITY RATIO BOUNDS", "content": "Here we provide formal guarantees on the lower bound of the Bayes SHF utility ratio UB, which approaches one as T \u2192 \u221e. Our approach follows similarly to (Paria et al., 2020). We first define the instantaneous SHF regret:\nDefinition 1 (Instantaneous SHF Regret). r(xt, At) = 1 - $\\frac{S_{\\lambda_t} (u_f(x_t))}{\\max_{x \\in X} S_{\\lambda_t} (u_f(x_t))}$\nwhere xt and At correspond to the values chosen in iteration t of Algorithm 1. For the regret over T iterations, we define the cumulative SHF regret:\nDefinition 2 (Cumulative SHF Regret). Rc(T) = $\\sum_{t=1}^{T} r(x_t, \\lambda_t)$\nAnalogously, we define the Bayes SHF regret and utility ratio terms as below:\nDefinition 3 (Bayes SHF Regret and Utility Ratio). $R_B(T) = E_{\\lambda \\sim p(\\lambda)} [1-\\frac{\\max_{x \\in D_T} S_{\\lambda_t} (u_f(x))}{\\max_{x \\in X} S_{\\lambda_t} (u_f(x))}]$\n$\\frac{\\max_{x \\in D_T} S_{\\lambda_t} (u_f(x))}{\\max_{x \\in X} S_{\\lambda_t} (u_f(x))}$\nwhere DT = {xt}{=1. Additionally, UB(T) = Ex~p(x) [ \\frac{\\max_{x \\in D_T} S_{\\lambda_t} (u_f(x))}{\\max_{x \\in X} S_{\\lambda_t} (u_f(x))} ] , where UB(T) is the Bayes SHF utility ratio after T iterations.\nSimilar to (Paria et al., 2020), ERB(T) is the expected Bayes SHF regret, with the expectation being taken over f, noise e, and other sources of randomness. Likewise, ERC(T) is the expected cumulative SHF regret, with the expectation being taken over f, noise e, and At.\nTheorem 4. The expected Bayes SHF regret can be upper bounded as:\n$\\mathbb{E}R_B(T) < \\frac{1}{2} \\mathbb{E}R_C(T) + o(1)$\nThe expected Bayes SHF utility ratio converges to one as T \u2192 \u221e.\nThe complete proof is shown in the Appendix (A.3.2) and follows from Theorem 14. As a result, Theorem 4 provides formal guarantees on our proposed dense Pareto frontier sampling algorithm, MoSH-Dense."}, {"title": "4 STEP 2: PARETO FRONTIER SPARSIFICATION", "content": "We now assume there already exists a dense set of points on the PF, sampled from step 1 with SHFs. As DM validation of such a dense set would be costly, the goal for step 2 is now to sparsify that set of points to then present to the DM a more navigable set which still maintains as much utility as the dense set. We do so by leveraging the notion of diminishing returns in utility for each additional point the DM validates. This notion is encapsulated by the property of submodularity, which further allows us to design optimization algorithms with strong theoretical guarantees. We use the definition of submodularity first developed in Nemhauser et al. (1978)."}, {"title": "5 EXPERIMENTAL RESULTS", "content": ""}, {"title": "5.1 BASELINE METHODOLOGIES", "content": "Step 1: Dense Pareto Frontier Sampling: We experiment with both synthetic problems and real-world applications and compare our method to other similar Bayesian multi-objective optimization approaches: Expected hypervolume improvement (EHVI) (Emmerich, 2008), ParEGO (Knowles, 2006), Multi-objective Bayesian optimization Using Random Scalarizations (MOBO-RS) (Paria et al., 2019), and random sampling. We compare against MOBO-RS with variations on the scalar-ization function, Chebyshev and linear, and acquisition function, UCB and Thompson sampling.\nStep 2: Pareto Frontier Sparsification: We compare our method, MoSH-Sparse, against greedy and random algorithms. The greedy baseline starts with the empty set, and iteratively adds the ele-ment c = arg maxx\u2208D\\CH(C\u222a{x}), where H = minx\u2208\u028c Fx for the Fx in Equation 4, until some stopping point. For all experiments, additional details and figures are provided in the Appendix."}, {"title": "5.2 PERFORMANCE EVALUATION", "content": ""}, {"title": "5.2.1 EVALUATION OF STEP 1: DENSE PARETO FRONTIER SAMPLING", "content": "As mentioned earlier in Section 3, since the DM's preferences, \u5165* are unknown to us, we wish to obtain a set D which is (1) diverse, (2) high-coverage, and is (3) modeled after the DM-defined SHFs. We operationalize the three criteria for soft regions (As = [a1,s, +\u221e] \u00d7 ... \u00d7 [QL,S, +8]) and hard regions (\u0410\u043d = [1,H, +\u221e] \u00d7 \u00d7 [QL,H, +\u221e]) into four different metrics:\nSoft-Hard Fill Distance: We seek to measure the diversity of sampled points. Malkomes et al. (2021) measures diversity using the notion of fill distance: FILL(C, D) = Supx'\u2208D Minz\u2208C \u03ba(f(x), f(x')) where C is the set of sampled points, D is a full set of precomputed points in the region, and K(\u00b7) is the distance metric, typically Euclidean distance. We expand this to include the notion of soft and hard regions: vFILLs(Cs, Ds) + (1 \u2212 v)FILLH(CH, DH), where FILL (Cs, Ds) and FILL\u0127(CH, DH) are the fill distances which correspond to the regions defined by the soft bounds and hard bounds, respectively, and Cs, Ds denote the set of points in the soft region, C\u2229 As, D\u2229 As, (same for hard region). Intuitively, we wish to obtain a diverse sample set which effectively explores both the soft and hard regions, which a higher weighting towards the soft region. We use the v parameter to control that weighting in our experiments.\nSoft-Hard Positive Samples Ratio: We seek to measure faithfulness to the implicit constraints set by the SHFs by measuring the ratio of sampled points in the soft and hard regions, defined as: v(|Cs|-|C|) + (1 \u2212 v)(|CH|-|C|).\nSoft-Hard Hypervolume: We seek to measure the coverage of the sampled points by measuring the hypervolume defined by both the soft and hard bounds. We measure the hypervolume of the soft region in the metric space bounded by the PF and the intersection of the soft bounds, rs, (A1,S,..., AL,S). The same goes for the hard hypervolume measure.\nSoft Region Distance-Weighted Score: We seek to explicitly measure faithfulness to the high-utility regions of SHFs, the intersection of the soft bounds, by measuring the density of points. We calculate this using the following: Ex\u2208c1/\u03ba(rs, f(x)), where \u043a is a measure of distance, typically Euclidean distance and rs is defined above."}, {"title": "5.2.2 EVALUATION OF STEP 2: PARETO FRONTIER SPARSIFICATION", "content": "To evaluate the set of points, C, returned to the DM, we simulate a \u5165* using the heuristic A = u/||u||1, where ue ~ N(al,s, |\u03b1\u03b9,\u043d \u2013 \u0430\u03b9,s|/3). We then compute the SHF utility ratio, right term in Equation 4, for the set of points Ct after each iteration of greedy, random, or MoSH-Dense. The denominator is calculated using A* with a full set of points, D, computed offline."}, {"title": "5.3 STEP 1 EXPERIMENTS: SYNTHETIC AND REAL-WORLD APPLICATIONS", "content": ""}, {"title": "5.3.1 SYNTHETIC TWO-OBJECTIVE FUNCTION: BRANIN-CURRIN", "content": "We leverage the Branin-Currin synthetic two-objective optimization problem provided in the BoTorch framework (Balandat et al., 2020), which has a mapping of [0, 1]2 \u2192 R2. To demonstrate our method's flexibility in accommodating various configurations, we sample from the following variations: (1) complete-mid, where the hard bounds cover the complete Pareto frontier and the soft bounds are in the middle of the hard region (2) complete-top, (3) complete-bot, (4) top-mid, and (5) bot-mid. Overall, we observe that our algorithm generally matches or surpasses other baselines in all four metrics, with sampling a much higher density near the soft region."}, {"title": "5.3.2 ENGINEERING DESIGN PROBLEM: FOUR BAR TRUSS", "content": "We also evaluate on a MOO engineering design problem, four bar truss, from REPROBLEM (Tan-abe & Ishibuchi, 2020), which consists of two objectives and four continuous decision variables, along with a convex PF (CHENG & LI, 1999). The objectives of the problem are to minimize the structural volume and the joint displacement of the four bar truss. The four decision variables determine the length of the four bars, respectively. To demonstrate our method's flexibility, we sample from the following variations: (1) narrow-mid, (2) narrow-bot, (3) narrow-top, (4) bot-mid, and (5) top-mid. Similar to the Branin-Currin experiment, our algorithm matches or surpasses other baselines in all metrics, while sampling at a clearly higher density near the soft region."}, {"title": "5.3.3 LARGE LANGUAGE MODEL PERSONALIZATION: CONCISE AND INFORMATIVE", "content": "We seek to obtain a large language model (LLM) which generates both concise and informative outputs, two directly competing objectives. Rather than fine-tune for both objectives, we lever-age proxy tuning, which steers a large pre-trained model, M, by using the difference between the predicted logits of an expert model (a smaller, tuned model), M+ and an anti-expert model (the smaller model, un-tuned), M (Liu et al., 2024; Mitchell et al., 2023; Shi et al., 2024). We leverage notation from Liu et al. (2024) and obtain the output distribution at time step t, con-ditioned on prompt x<t, from the proxy-tuned model, M, in a two-objective setting as such:\n$P_M(X_t|x_{<t}) = softmax[s_M(X_t|x_{<t})+ \\sum_{i=1}^{2}\\theta_i (s_{M^+}(X_tX_{<t})-s_{M^-}(X_t|x_{<t}))]$, where sM,8M+, sM- represent the logit scores for each model and \u03b8\u00bf denotes the input decision variable, the con-trollable weight applied to the logits difference associated with expert model i. For this experiment, models M+, for i = 0, 1, are tuned according to the conciseness and informativeness objectives, re-spectively. By adjusting i at decoding time, we obtain generated output distributions with varying tradeoffs in conciseness and informativeness. Across most metrics, our algorithm performs consistently well (moreso than any other baseline) \u2013 highlighting the generality of our pipeline."}, {"title": "5.3.4 REAL CLINICAL CASE: CERVICAL CANCER BRACHYTHERAPY TREATMENT PLAN", "content": "We evaluate our method on treatment planning for a real cervical cancer brachytherapy clinical case. This problem consists of four objectives and three continuous decision variables. The objective are (1) maximize the radiation dosage level to the cancer tumor, and minimize the radiation dosage levels to the (2) bladder, (3) rectum, and (4) bowel. We converted objectives (2)-(4) into maximization objectives. The decision variables are used as inputs to a linear program formulated as an epsilon-constraint method (Deufel et al., 2020). We notice that our proposed method surpasses the baselines by a greater amount in this high-dimensional setting, notably the soft-hard hypervolume and soft-hard positive ratio metrics."}, {"title": "5.3.5 DEEP LEARNING MODEL SELECTION: FAST AND ACCURATE NEURAL NETWORK", "content": "Similar to Hern\u00e1ndez-Lobato et al. (2015), we seek to obtain a neural network which minimizes both prediction error and inference time, two competing objectives. We use the MNIST dataset and consider feedforward neural networks with six decision variables Although our algorithm does not surpass the baselines all four of the metrics, it still performs consistently relatively well in all four (the most consistently well out of the baselines) \u2013 in line with the other experimental results as well."}, {"title": "5.4 STEP 2 EXPERIMENTS: SYNTHETIC AND REAL-WORLD APPLICATIONS", "content": "We conduct experiments evaluating our baselines on the sparsification of the dense set of points from MoSH-Dense. We observe that in all four experimental settings, MoSH-Sparse matches or exceeds the baselines in achieving the overall highest SHF utility ratio. In the brachyther-apy setting, MoSH-Sparse achieves the highest SHF utility ratio at the fastest pace, showcasing the effectiveness of our approach in distilling the dense set of points into a useful smaller set."}, {"title": "5.5 END-TO-END EXPERIMENTS: SYNTHETIC AND REAL-WORLD APPLICATIONS", "content": "We further holistically evaluate our entire two-step process by comparing against all baselines for step (1), using MoSH-Sparse \u2013 displayed in Figure 6. We show that our method achieves an over 3% greater SHF utility ratio than the next best one. We further note that our method consistently leads in providing the most utility in all of the experiments (more in Appendix)."}, {"title": "6 RELATED WORKS", "content": "Populating the Pareto Frontier. The majority of existing MOO works aim to approximate the en-tire PF, using heuristics such as the maximum hypervolume improvement (Campigotto et al., 2014; Ponweiser et al., 2008; Emmerich, 2008; Picheny, 2015; Hern\u00e1ndez-Lobato et al., 2016; Zhang et al., 2009). Others leverage RS of the objective values to attempt to recover the entire PF (Knowles, 2006; Paria et al., 2020; Zhang & Li, 2007; Zhang et al., 2010). Additional works also place a greater emphasis on sparse and diversified PF coverage (Zuluaga et al.; 2016) more recently, in hard-constrained regions (Malkomes et al., 2021). In contrast, we employ RS in a novel setting which aims to diversely sample a soft-hard subset of the PF according to the SHFs.\nMOO Feedback Mechanisms. Many feedback mechanisms, such as pairwise feedback, have been proposed, albeit not all of which are designed for MOO (Zintgraf et al., 2018; Roijers et al.; Astudillo & Frazier, 2020). Besides pairwise feedback, Hakanen & Knowles (2017) enables the DM to guide the MOO search by allowing them to specify numerical ranges for each of the objectives. Abdolshah et al. (2019) allows for the DM to order objectives by importance, Ozaki et al. (2023) introduced improvement request feedback type for MOO. These methods enable fine-grained MOO control, but our approach uniquely accounts for multiple levels of preferences without needing to specify exact numerical values, which is often psychologically more difficult (Qian et al., 2015).\nLevel Set Estimation. The formulation of objectives as inequality constraints, where the DMs aim to find inputs which satisfy thresholds on the objectives, is related to the topic of level set estimation (LSE) (Gotovos, 2013; Zanette et al., 2018; Iwazaki et al., 2020; Malkomes et al., 2021). In the single-objective setting, Bryan et al. (2005) proposed the straddle heuristic, which was used as part of a GP-based active learning approach for LSE. Although the LSE concept does not easily extend into a MOO setting, Bryan & Schneider (2008) aims to address that by considering the threshold as part of a composite setting, with scalarized objectives. In contrast, our work is native to the MOO setting and additionally incorporates soft constraints, which directly leverages the DM's preferences."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduced a novel setting and formulation for MOO using SHFs, monotonic soft-hard bounded utility functions for each objective, allowing for the DM to impose their preferences via soft and hard bounds. We demonstrated the generality of this setting, and showed how it encom-passes the problem of engineering design, treatment planning for cervical cancer brachytherapy, model selection for deep learning, and personalization of LLMs. Within our setting, we then pro-pose a simple two-step process which aims to return a small set of high-utility PO points according to the DM's unknown preferences: (1) dense PF sampling using Bayesian optimization, and (2) sparsification of the PO points from (1) using robust submodular function optimization, which we theoretically show is able to obtain the near-optimal set from (1). Lastly, we propose a set of soft-hard metrics and conduct extensive empirical validations on a variety of applications. We show that, for cervical cancer brachytherapy treatment planning, our approach returns a compact set of treat-ment plans which offers over 3% greater SHF-defined utility than the next best approach. Among the other diverse experiments, our approach also consistently achieves the leading utility, allowing the DM to reach >99% of their maximum desired utility within validation of 5 points."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 STEP 1: DENSE PARETO FRONTIER SAMPLING WITH BAYESIAN OPT. ADDITIONAL DETAILS", "content": ""}, {"title": "\u0410.1.1 \u0421\u043eMPUTATIONAL FEASIBILITY", "content": "Equation 3 uses a conversion from the worst-case to an average-case minimization problem. To observe the difficulty of the worst-case minimization problem and how the results may be affected, we performed several experiments which directly solve the following:"}, {"title": "A.2 STEP 2: PARETO FRONTIER SPARSIFICATION ADDITIONAL DETAILS", "content": "Theorem 8. (Nemhauser et al., 1978) In the case of any normalized, nomotonic submodular func-tion F, the set A\u00e7 obtained by the greedy algorithm achieves at least a constant fraction (1\nof the objective value obtained by the optimal solution, that is,\n$F(A_G) \\geq (1 - \\frac{1}{e}) \\max_{|A|\\leq k} F(A)$"}, {"title": "A.3 REQUIRED PROOFS AND DEFINITIONS", "content": ""}, {"title": "A.3.1 SHF UTILITY RATIO SUBMODULARITY", "content": "Definition 9 (Pareto dominant). A solution x1 \u2208 X is Pareto dominated by another point x2 \u2208 X if and only if fe(x1) \u2264 fe(x2) \u2200l \u2208 [L] and \u2203l \u2208 [L] s.t. fe(x1) < fe(x2) (Paria et al., 2020).\nTheorem 10. Consider finite sets \u03a9, \u039b and function f : \u03a9 \u00d7 \u039b \u2192 R. Fix \u03bb\u2208 A. Then, given CCN, the set function\n$F(C) := \\frac{\\max_{c \\in C} f(c, \\lambda)}{\\max_{c \\in \\Omega} f(c, \\lambda)}$\nis submodular."}, {"title": "A.3.2 SHF UTILITY RATIO BOUNDS", "content": "In this section, we illustrate our proofs for formal guarantees on the lower bound of the Bayes SHF utility ratio. We start with some definitions:\nDefinition 11 (Maximum Information Gain). We leverage this definition from (Paria et al., 2020). The maximum information gain after T observations measures the notion of information gained about random process f after observing some set of points A, and is defined as:\n$\\tau_T = \\max_{A \\subset X: |A|=T}  I(y_A; f)$", "subsections": [{"title": "Lemma 19.", "content": "For a fixed x \u2208 A, the augmented Chebyshev scalarization function sx(y) =\nmaxe\u2208[L] {de|ye - z* |} - \u2211e=1|ye-z* |, as described in Section A.5.1, satisfies the assumption\nin Definition 12.\nProof. Let X \u2208 \u039b. Recall A := \u2206L, thus is bounded. Furthermore, Im(uf) is bounded when\nf(x) \u2265 \u0430\u043d. First, we demonstrate that sx(y) is Lipschitz w.r.t. y. For l \u2208 [L],\n$\\frac{ds_{\\lambda}(y)}{dy_l} =$$\\begin{cases}\n+\\lambda_e^*, y_{e^*} > z^*, l = l^* \\\\\n0, y_{e^*} < z^*, l = l^* \\\\\n-\\frac{1}{L-1}, y_{e} > z \\\\\n1, y_{e} < z\n\\end{cases}$   $\\hspace{1cm}$       $- \\lambda = \\Delta^L$,\n$\\frac{\\partial sx(y)}{\\partial \\lambda_e} \\Rightarrow | [A_a] | < L$\nwhere $A_a = arg maxe [AeYe-z]$\n$L^{A.5} \\in +L^{A.7}$"}]}, {"title": "A.4 ADDITIONAL EXPERIMENTAL RESULTS", "content": ""}, {"title": "A.4.1 SYNTHETIC TWO-OBJECTIVE FUNCTION: BRANIN-CURRIN", "content": "Configurations (normalized to [0,1]) : {\u03b1\u03bf,\u03c2, \u03b1\u03bf,H, Q1,S, Q1,H}\n1. Complete-Mid: {0.988, 0.943, 0.856, 0.618}\n2. Complete-Top: {0.969, 0.943, 0.935, 0.618}\n3. Complete-Bot: {0.998, 0.943, 0.697, 0.618}\n4. Top-Mid: {0.969, 0.940, 0.915, 0.856}\n5. Bot-Mid: {0.996, 0.975, 0.737, 0.658}"}, {"title": "\u0391.4.2 ENGINEERING DESIGN PROBLEM: FOUR BAR TRUSS", "content": "Configurations (normalized to [0,1]) : {\u03b1\u03bf,\u03c2, \u03b1\u03bf,H, Q1,S, Q1,H}\n1. Narrow-Mid: {0.62, 0.45, 0.72, 0.55}\n2. Narrow-Bot: {0.70, 0.45, 0.65, 0.55}"}, {"title": "A.4.3 REAL CLINICAL CASE: CERVICAL CANCER BRACHYTHERAPY TREATMENT PLANNING", "content": "The configuration used was: {\u03b1\u03bf,\u03c2 =0.95, \u03b1\u03bf,\u0397 =0.90, \u03b11,5 =513, \u03b11,H =601, a2,5 =352, \u03b12,\u0397 =464, \u03b13,5 =411, \u03b13,5 =464}, where the objectives correspond to PTVV700, Bladder D2cc, Rectum D2cc, and Bowel D2cc, ordered. Before doing the experiment, all of the values were nor-malized to [0,1] and converted to maximization."}, {"title": "A.4.4 DEEP LEARNING MODEL SELECTION: FAST AND ACCURATE NEURAL NETWORK", "content": "We used the following decision space: number of hidden units per layer ([50, 300", "3": "learning rate ([0, 0.1", "0.6": 11, "0.05": "and 12 regularization ([0, 0.05"}]}