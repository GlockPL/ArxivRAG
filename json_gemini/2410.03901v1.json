{"title": "Improving Node Representation by Boosting Target-Aware Contrastive Loss", "authors": ["Ying-Chun Lin", "Jennifer Neville"], "abstract": "Graphs model complex relationships between entities, with nodes and edges capturing intricate connections. Node representation learning involves transforming nodes into low-dimensional embeddings. These embeddings are typically used as features for downstream tasks. Therefore, their quality has a significant impact on task performance. Existing approaches for node representation learning span (semi-)supervised, unsupervised, and self-supervised paradigms. In graph domains, (semi-)supervised learning often only optimizes models based on class labels, neglecting other abundant graph signals, which limits generalization. While self-supervised or unsupervised learning produces representations that better capture underlying graph signals, the usefulness of these captured signals for downstream target tasks can vary. To bridge this gap, we introduce Target-Aware Contrastive Learning (Target-aware CL) which aims to enhance target task performance by maximizing the mutual information between the target task and node representations with a self-supervised learning process. This is achieved through a sampling function, XGBoost Sampler (XGSampler), to sample proper positive examples for the proposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware CL increases the mutual information between the target task and node representations, such that model generalization is improved. Additionally, XGSampler enhances the interpretability of each signal by showing the weights for sampling the proper positive examples. We show experimentally that XTCL significantly improves the performance on two target tasks: node classification and link prediction tasks, compared to state-of-the-art models.", "sections": [{"title": "1 Introduction", "content": "A graph is a data structure that represents relationships between entities or elements using nodes and edges. Node representation learning involves converting nodes in a graph into lower-dimensional embeddings. Node embeddings are frequently utilized as features for downstream target tasks. Therefore, the performance of these tasks is greatly influenced by the quality of the learned embeddings. Node representation learning methods are optimized using various loss functions, including (semi-)supervised, unsupervised, or self-supervised. (Semi-)Supervised learning involves accessing ground truth information associated with a downstream task during training. However, when downstream class labels are available, the optimization may overfit to those labels and underutilize other, more abundant graph signals, which can reduce generalization performance.\nUnsupervised and self-supervised learning use the underlying patterns or graph structure for optimization. In this case, the node representations are learned to capture graph signals in training. However, the relationship between these graph signals and a downstream target task is not always clear. Optimizing for irrelevant information can negatively impact task performance [28], as shown in Figure 3.\nWhile previous studies have demonstrated that self-supervised learning can greatly enhance task performance when they are relevant to the target task [26], there has been relatively little work"}, {"title": "2 Notations and Preliminaries", "content": "We first introduce basic concepts and define common notations in this section. A general graph is defined as $G = (V, &, X, Y)$ where V is the node set, $&: V \\times V$ denotes all edges, $X \\in R^{|V|\\times m}$ is node attributes (m is the dimension), and $Y = {Y_1, Y_2,..., Y_v}$ is the set of node labels in which class label $y_u \\in {1, 2,..., C}$. Such graphs contain various semantic relations $R = {r_1, r_2,...,r_{|r|}}$ which refer to various relationships between pairs of nodes, including structural similarities such as shortest path distance or attribute similarities such as neighbor label distribution. We use these semantic relation as our graph signals in this paper. For each relation $r \\in R$, we define a similarity measure $s_r : V \\times V \\rightarrow [0, 1]$ to record the strength of the relation between two nodes and $S_r$ is the random variable describing the similarity values for r."}, {"title": "2.1 Node Contrastive Learning", "content": "Given a graph G, a graph-based model $f : G \\rightarrow R^{|V|\\times h}$ transforms nodes in the graph into node representation $Z = f (G)$, where h is the embedding dimension. We denote $z_u$ as embedding of node $u \\in V$. Specifically, Z is optimized by Contrastive Loss (CL) [6, 22, 33] $\\mathcal{L}(u,v_p, {u_{nk}}_{k=1}^K)$ , which is generally defined as:\n$\\mathcal{L}(u,v_p, {u_{nk}}_{k=1}^K) = - log \\frac{exp(z_u^Tz_{up})}{exp (z_u^Tz_{op}) + \\sum_{k=1}^K exp(z_u^Tz_{onk})}$\nwhere $u, v_p, u_{nk} \\in V$, and K is the number of negative examples We will refer to u as the query node, $v_p$ as positive examples, and $u_{nk}$ as negative examples. The positive example $v_p$ for u is usually sampled from some probability function w.r.t u, e.g. $v_p$ sampled based on the probability that a edge exist between u and $v_p$ [14, 19] or the probability that two nodes shares similar topological structure [9], etc. Conversely, negative examples are randomly sampled from V or nodes distinct from u. If B > 1 positive examples are sampled for query node u, the contrastive loss\n$\\mathcal{L}(u, {o_{pb}}_{b=1}^B, {u_{nk}}_{k=1}^K) = \\frac{1}{B} \\sum_{b=1}^B -log\\frac{exp(z_u^Tz_{opb})}{exp (z_u^Tz_{op}) + \\sum_{k=1}^K exp (z_u^Tz_{onk})}$ (1)\nis minimized. The contrastive loss encourages positive examples to be embedded close to u and discourages negative examples from being embedded close to u.\nDepending on the sampling strategy for positive examples, the contrastive loss can be categorized into self-supervised [9, 14, 19] and supervised [13] contrastive loss.\nDEFINITION 1. Self-Supervised Contrastive Loss (SSCL). A graph-based model f is usually obtimized by SSCL with positive examples sampled based on a subset semantic relations $R_f \\subseteq R$. $v_p$ can be sampled from the set {$v | s_r(u, v) > \\eta_r$} for $r \\in R_f$. The threshold $\\eta_r \\in (0, 1]$ is used to select examples with high similarity w.r.t r for u. If $B > |R_f| > 1$, previous studies usually uniformly select $[\\frac{B}{|R_f|}]$ positive samples for each $r \\in R_f$, e.g.\n${{Upb}}_{B=1} = U_{r\\in Rf}{p_i}^{\\B/RF]}$\nThe negative examples are sampled randomly from V. Then, these positive and negative examples can be utilized to minimize the loss in equation 1. Depending on which relation is used, the SSCL model space is illustrated by the blue areas in Figure 1.\nLet $I (X_1; X_2)$ be the mutual information between two random variables $X_1$ and $X_2$. According to [29], minimizing the loss in equation 1 with the equal number of positive examples, i.e. $[\\frac{B}{|R_f|}]$, for each $r \\in R_f is equivalent to uniformly maximizing $I (S_r; Z) \\forall r \\in R_f$."}, {"title": "3 Learning Node Representations with Target-Aware Contrastive Loss", "content": "In this section, we will discuss the motivation for Target-Aware Contrastive Learning (Target-aware CL) for node representations and then describe our implementation of Task-aware Contrastive Loss (XTCL) which uses XGSampler for Target-aware CL."}, {"title": "3.1 Target-Aware Contrastive Loss", "content": "As discussed in [29], minimizing the loss in equation 1 by sampling the equal number of positive examples for each $r \\in R_f$ is equivalent to uniformly maximizing $I (S_r; Z) \\forall r \\in R_f$. We argue that the generalization of a graph-based model f is impacted by whether uniformly maximizing $I (S_r; Z) \\forall r \\in R_f$ is equivalent to maximizing $I (Y; Z)$. Our main argument is that there is a high chance that uniformly maximizing $I (S_r; Z) \\forall r \\in R_f$ is not equivalent to maximizing $I (Y; Z)$. Consequently, the node representation may be considered inadequate for the intended classification task. We will empirically verify this statement in Section 5.3.\nInstead, each $I (S_r; Z)$ for $r \\in R_f$ should be maximized according to some weight, which can be controlled by the number of positive nodes. A possible approach is to decide the number of positive examples for each r based on the relation between r and the target task. To make the relation between r and the target task more clear, we define:\nDEFINITION 3. Task Positive Function. A task positive function $I_t (u, v) \\in {0, 1}$ is defined to verify whether two nodes have the same target-relevant features. If $I_t (u, v) = 1$, then u and v share the same target-relevant feature, or vice versa. If the task is node classification, $I_t (u, v) = I{y_u = y_v}$. On the other hand, if the task is link prediction, $I_t (u, v) = I{(u, v) \\epsilon E$.\nThe target task can be any type of task as long as the task positive function is defined for the task t.\nSpecifically, for a node u, the number of positive examples of r can be determined by\n$P_u(I_t(u, v) = 1|s_r(u, v)).$ (2)"}, {"title": "Algorithm 1 XGBoost Sampler Training", "content": "Input: Ordered semantic relation set $R = {r_1, r_2, ..., r_{|R|}}, training instances $U_L$\nOutput: Classifiers $f_{r,u} \\forall u \\in V\nInitialize all $\\hat{y}_{u,v}^{(0)} = 0$\nfor all $i \\in {1, 2,..., |R|}$ do\nfor all $(u, v) \\in U_L \\times U_L$ do\n$\\hat{g}_{u,v} = \\sigma(\\hat{y}_{u,v}^{(i-1)}) - y_{u,v}$\n$h_{uv} = \\sigma(\\hat{y}_{u,v}^{(i-1)}) (1 - \\sigma(\\hat{y}_{u,v}^{(i-1)}))$\nend for\n$W_{ri,0} = equation 7$\n$W_{ri,1} = equation 8$\nfor all $(u, v) \\in U_L \\times U_L$ do\nif $S_{ri}(u, v) > \\eta_{ri,u}$ then\n$y_{u,v}^{(i)} = y_{u,v}^{(i-1)} + W_{ri,1}$\nelse\n$y_{u,v}^{(i)} = y_{u,v}^{(i-1)} + W_{ri,0}$\nend if\nend for\nend for"}, {"title": "3.2 Computation Complexity Analysis", "content": "Because XGSampler is an additional process for selecting positive examples for Target-aware CL, we have to ensure that the extra cost is reasonable enough. A naive approach for sampling positive nodes via XGBoost for all v \u2208 V can be $O(|R||V|^3)$. Our learning approach on the contrary can be $O(|R||V|^2B)$, where B is the number of positive nodes. The number of parallel processes is set to be proportional to the number of nodes, i.e., c|V| where $c\\in [0, 1]$, the time complexity for XGSampler can be further reduced to $O(|R||V|B/c)$, in the graph. According to empirical computational results in Appendix C, we can learn node representations for a graph with half a million edges under a minute."}, {"title": "4 Semantic Relations", "content": "We describe the set of semantic relations R used in our experiments in following sections. We are interested in understanding the relations between two nodes $u \\in V$ and $v \\in V$. $\\mathcal{N}_u$ is the k-hop neighbors of u. Note that we use training data to calculate these similarity values. Below we specify the similarity measure for each semantic relation and commonly used definitions.\n\u2022 Adjacency matrix A is a square matrix describing links between nodes. $A_{u,v} = 1$ if $(u, v) \\in E$ else $A_{u,v} = 0$.\n\u2022 Y\u2208 [0,1]|V|\u00d7C is the one-hot encoding of class labels, where C is the number of classes.\n\u2022 $X \\in R^{|V|\\times D}$ describes the attributes of all nodes."}, {"title": "Has Link (Link). [15]", "content": "$s_{link}(u, v) = D^{-\\frac{1}{2}} \\overline{A}D^{-\\frac{1}{2}} [u, v]$, (9)\nwhere $\\overline{A} = A + I$ is the adjacency matrix A with self-loop and $D_{uu} = \\sum_{v}\\overline{A}_{uv}$ and [u,v] indicates the (u, v)-entry of $D^{-\\frac{1}{2}}\\overline{A}D^{-\\frac{1}{2}}$. This similarity is inspired by [15] in order to put weight on each link."}, {"title": "PageRank (PR). [21]", "content": "We can derive $\\pi_u$ of personalized PageRank by solving the following equation\n$\\pi_u = \\alpha P\\pi_u + (1-a)e_u$,\nwhere P is the transition probability, a \u2208 [0, 1] and $e_u \\in [0,1]^{|V|}$ is a one-hot vector containing a one at the position corresponding u's entry and zeros otherwise. The similarity in terms of PageRank is\n$s_{PageRank}(u, v) = \\pi_{u,v}.$ (10)"}, {"title": "Jaccard Similarity (Jacard Sim.). [18]", "content": "Many studies have used this relation to predict links between node pairs. This relation uses the number of 1-hop common neighbors between two nodes to estimate the closeness between them.\n$s_{jaccard}(u, v) = \\frac{|\\mathcal{N}_u \\cap \\mathcal{N}_v|}{|\\mathcal{N}_u \\cup \\mathcal{N}_v|}$ (11)"}, {"title": "Topology Similarity (Topology Sim.). [9]", "content": "This similarity is similar to Jaccard Similary. Topology similarity represents the mutual information between the neighbors of two nodes. Please refer to [9] for mathematical definition of topology similarity."}, {"title": "Shortest Path Distance (Graph Distance). [20]", "content": "$s_{graph-distance}(u, v) = \\frac{diameter(G) - d_G(u, v) + 1}{diameter(G)}$ (12)\nwhere $d_G(u, v)$ is the shortest-path distance between u and v. Note that $s_{graph-distance} (u,v) = diameter(G) + 1$ when u and v are disconnected in a graph, and hence $s_{graph-distance} (u, v) = 0."}, {"title": "Attribute Similarity. (Attr Sim.)", "content": "$s^{attribute similarity} = cos(x_u, x_v),$ (13)\nwhere $x_u \\in R^D$ is the node attributes of u and D is the dimension."}, {"title": "Attribute Distribution. (1-Hop Attr. Dist.) [36]", "content": "$s_{k}^{attribute distribution} = cos(\\overline{x_u}, \\overline{x_v}),$ (14)\nwhere $\\overline{x_u} = Normalize(A^kX)_u$ is the k-hop neighbor attribute distribution. We use k = 1 in our experiments."}, {"title": "Label Distribution. (2-Hop Label Dist.) [36]", "content": "$s_{k}^{attribute distribution} = cos(\\overline{y_u}, \\overline{y_v}),$ (15)\nwhere $\\overline{y_u} = Normalize(A^kY)_u$ is the k-hop neighbor label distri- bution. We use k = 2 in our experiments."}, {"title": "Attribute Label Distribution. (Attr. Label Dist.)", "content": "$s_{k}^{attr. label distribution} = cos(\\hat{y_u}, \\hat{y_v}),$ (16)\nwhere $\\overline{y_u} = Normalize(S_k S^{attr. dist.} Y)_u$ is the label distribution of the k-hop attribute distribution similarity, $S_k$ This similarity is especially useful for node classification to identify target-specific positive nodes."}, {"title": "5 Performance Evaluation", "content": "Our proposed model is Target-Aware CL with XGSampler, which we refer to as XTCL. Note that we can optimize any GNN method with XTCL. In this work, we demonstrate the effectiveness of our XTCL with GCN [15] and GAT [30]. The dataset descriptions are in Table 3. In our empirical evaluation, we aim to answer:\n\u2022 RQ1. Does XTCL improve performance on node classification and link prediction compared to other SOTA methods?\n\u2022 RQ2. How supervision signals and training data size impact GNN performance?\n\u2022 RQ3. Is it important to make loss function task-aware?\n\u2022 RQ4. What semantic relations are important for each target task?"}, {"title": "5.1 RQ1. Performance on Downstream Tasks", "content": "Node Classification\nThe performance results are shown in Table 1. XTCL(GCN) and XTCL(GAT) are GNNs trained with the proposed XTCL. XGSampler uses the available training labels to learn a model for each u \u2208 V, which we use to select positive examples for Target-aware CL. If node representations are generated in XTCL, self-supervised or unsupervised settings, Logistic Regression model is used as the final classifier for label assignment (with learned embeddings as input). We use 10% labels for training and 90% of labels for testing and the reported results are the average of five runs.\nTable 1 shows that, with only small number of training labels (10%) to train XGSampler, XTCL(GCN) significantly outperforms most of the state-of-the-art models in both unsupervised and (semi-)supervised settings. (See explanation for PubMed in Appendix D). The overall results in the table demonstrate that minimizing XTCL is similar to maximizing I (Y; Z) and significantly improve the performance on node classification.\nLink Prediction\nFor link prediction tasks, we compare to similar methods. We remove MetaPN and CoCoS because it is a semi-supervised model for node classification and cannot be adapted for link prediction. We follow the random split strategy used by [1]. To ensure the training data is limited, we use 60% links for training and 40% of links for testing and report the experiment results, averaged over five runs, in Table 2. The observations from Table 2 is similar to that of Table 1. By training GCN with XTCL, its performance can be improved significantly also for link prediction. This indicates that, by increasing the probability to select proper positive examples for link prediction, Target-aware CL successfully to improve performance over other methods."}, {"title": "5.2 RQ2. Comparing Various Supervision Signals", "content": "In this experiment, we explore whether XGSampler can improve XTCL performance on node classification as the training label size increases. The result is shown in Figure 2. We use the same 20% testing labels for all evaluated models, and the results are the average of five runs. We train GCN with various objective functions, and Logistic Regression is used as the classifier for all CL models. The learning objectives of Ceiling Supervised Contrastive Loss (Ceiling SCL), Naive Target-Aware CL (Naive TCL) are in Def. 6 and Def. 7.\nFigure 2 shows that XTCL outperforms Naive TCL because XGSampler increases the probability that $I_t (u, v) = 1$ for a positive example v. However, as the training size increases, the performance gap between XTCL and Naive TCL becomes smaller. This is because Naive TCL has access to more training labels, allowing it to sample more positive nodes that $I_t (u, v) = 1$ as positive examples. A similar observation can be applied to SCL. Furthermore, XTCL can sometimes be comparable to the Ceiling SCL because XTCL enhance model generalization further by incorporating abandon graph signals during node representation learning.\nDEFINITION 6. Ceiling Supervised Contrastive Loss (Ceiling SCL). We use Ceiling SCL to refer to a method that uses ground truth labels to identify positive examples, ie.\n${{p}}_{B=1} = {v | I_t (u, v) = 1, v \\in V}$\nThe difference between SCL in Def. 2 and Ceiling SCL is that Ceiling SCL can utilize the label of $y_u$ even when u is not in $V_L$.\nDEFINITION 7. Naive Target-Aware CL (Naive TCL). Naive TCL is a combination of SSCL in Def. 1 and SCL in Def. 2. It uses Def. 1 to select positive examples when $u \\notin V_L$ If $u, v \\in V_L$, the positive examples are sampled from the set {$v | I_t (u, v) = 1, u, v \\in V_L$}."}, {"title": "5.3 RQ3. Importance of Task-Aware Loss Function", "content": "In Section 3.1, we briefly mentioned that the selection of the loss function will impact the model generalization for a target task. Our main argument is that minimizing the value of a loss function is not equivalent to maximizing I (Y; Z). Consequently, model performance and generalization degrades severely.\nTo empirically investigate the above conjecture, we conducted an experiment using semi-synthetic data generated from Cora. The target task is classification task. First, we choose a semantic relation \u00ee subject to that knowing $s_\\hat{r} (u, v)$ does not inform the probability of $y_u = y_v$ and $I (\\hat{S}; Z)$ is less affected by optimizing the loss function that generates Z. Then, we gradually perturb some node labels in Y such that $I (Y'; \\hat{S}) > I (Y; \\hat{S})$. Y' is the new node label set with some labels perturbed. This will create a situation that minimizing the value of a loss function is not equivalent to maximizing $I (Y'; \u0396)."}, {"title": "5.4 Importance Weights of Semantic Relations", "content": "The weights $w_{r,0}$ and $w_{r,1}$ learned from the XGSampler indicate the importance of semantic relation r in determining whether a node is a positive example for u. This weights also show that the importance of each r to a target task. The importance weight in"}, {"title": "6 Related Work", "content": "Contrastive Learning have been applied to learning representations for various type of data. The selection of appropriate positive and negative examples is a crucial step in optimizing contrastive loss. In the context of Computer Vision (CV), various image augmentation techniques such as rotation and clipping have been proposed to generate positive examples from the query image [5, 17]. Conversely, in the field of Neural Language Processing (NLP), several pre-trained models are employed to generate positive samples for words or sentences [10]. These data augmentation approaches have demonstrated significant improvements in tasks related to CV and NLP.\nNumerous task-aware contrastive objectives have been proposed for word and image representations. The initial work is Supervised Contrastive Learning [13], which selects positive and negative examples based on the available labels in the training data. The performance of text [11, 27] and image classification tasks [13] has been enhanced through the use of supervised contrastive loss. The SCL depicted in Figure 3 and Figure 2 adopts a similar setting to supervised contrastive learning, and both figures illustrate that the performance is constrained by unlabeled nodes due to the absence of an appropriate method for generating positive samples for those nodes.\nSeveral graph augmentation techniques, e.g. adding/dropping edges [24, 26]or modifying attributes [31, 35], have been proposed."}, {"title": "7 Conclusion", "content": "To ensure that minimizing self-supervised losses is similar to maximizing the mutual information between a target task and node representations, we propose Target-Aware Contrastive Loss with XGBoost Sampler (XTCL). The proposed XGSampler increases the probability of selecting nodes that enhance the mutual information between node representation and a target task as positive examples for contrastive loss. It learns how the target task relates to the semantic relations in the graph and actively adjusts the number of positive examples for each semantic relation. The performance results on both node classification and link prediction have shown that XTCL can significantly outperform the state-of-the-art unsupervised and self-supervised models and significantly outperform supervised models in 90% of our experiments. Additionally, the interpretability of XTCL is enhanced by the weights assigned to each semantic relation. These weights indicate the importance of each semantic relation to a target task."}, {"title": "A Experiment Setup", "content": "All codes and scripts are run in the environment with Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz and NVIDIA A10 GPU. The scripts are available on GitHub\u00b2. We refer to the official repository to implement all the baseline methods. We tuned for best parameter settings using grid search and the parameter settings for our models is in our GitHub repo and the dataset statistics are in Table 3."}, {"title": "B Label Perturbation Process", "content": "We gradually increase the influence off on the classification task by determining a portion of node labels using \u00ee. Initially, when the level of perturbation is low, \u00ee is less relevant to the node labels, i.e. I (Y; S) is low. We gradually increase the influence of \u00ee on node"}]}