{"title": "Training Sparse Mixture Of Experts Text Embedding Models", "authors": ["Zach Nussbaum", "Brandon Duderstadt"], "abstract": "Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline at https://github.com/nomic-ai/contrastors.", "sections": [{"title": "1. Introduction", "content": "Transformer-based biencoders are the standard architecture for training dense sentence embedding models for text retrieval (Reimers & Gurevych, 2019). In the monolingual setting, these models are trained on curated internet-scale data (Wang et al., 2024a; Xiao et al., 2024; G\u00fcnther et al., 2023; Nussbaum et al., 2024; Li et al., 2023), and sometimes augmented with task-specific instructions (Su et al., 2023a). While models like mE5 (Wang et al., 2024b), BGE-M3 (Chen et al., 2024), mGTE (Zhang et al., 2024), and Jina V3 (G\u00fcnther et al., 2024) make strides towards a unified embedding space across languages, they underperform their\nparameter-equivalent monolingual counterparts on English benchmarks. Multilingual models primarily close this performance gap by increasing their parameter counts, often through the use of large, pretrained multilingual Language Models fine-tuned for retrieval applications (Jiang et al., 2023; Lee et al., 2024b).\nThe large size of multilingual embedding models creates significant deployment challenges. Their substantial memory requirements and increased inference latency particularly impact retrieval-augmented generation (RAG) applications, where they constrain both dataset ingestion capacity and query-time performance.\nWhile causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach has not yet been adapted for text embeddings.\nIn this work, we introduce the first general-purpose Mixture of Experts text embedding model. We demonstrate that scaling text embedding models with Mixture of Experts in both monolingual and multilingual settings outperforms existing approaches while using fewer active parameters."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Mixture of Experts", "content": "The Mixture of Experts (MoE) architecture was first introduced by Shazeer et al. (2017) as a method to increase model capacity and performance without a proportional increase in computation by stacking sparsely gated LSTM blocks (Hochreiter & Schmidhuber, 1997). Lepikhin et al. (2020) utilized MoE layers in Transformers for machine translation and showed improvements in multilingual translation as the model size increased, while only incurring a sublinear increase in training time. Fedus et al. (2022) simplified the routing, reduced training instability, and reduced communication costs to achieve a 7x improvement in pre-training speed. Zoph et al. (2022) found that MoEs frequently experienced training instabilities, and introduced an auxiliary loss to stabilize the model training without harming its quality.\nRecent advances in MoE training, such as upcycling from pretrained transformers (Komatsuzaki et al., 2023) and efficient block-sparse implementations (Gale et al., 2022), have\nmade MoE training even more efficient. However, these advances have primarily focused on language modeling tasks. While Hallee et al. (2024) explored domain-specific MoE embeddings and Li & Zhou (2024) investigated using MoE language model states as embeddings, our work is the first to develop a general-purpose MoE architecture specifically for text embeddings. Concurrent work GRITLM (Muennighoff et al., 2024) demonstrates that MoE models like Mixtral 8x7B can effectively handle both embedding and generation tasks through instruction tuning. In contrast, our work focuses on optimizing MoE architectures for embedding efficiency through large-scale contrastive pretraining and finetuning."}, {"title": "2.2. Monolingual Text Embeddings", "content": "Modern monolingual text embedders typically follow a two-stage approach: contrastive pretraining on large weakly-supervised datasets, followed by contrastive finetuning on human-labeled data (Wang et al., 2022; Li et al., 2023; G\u00fcnther et al., 2023; Nussbaum et al., 2024). Recent work has focused on scaling and data curation (Xiao et al., 2023; Wang et al., 2022; Li et al., 2023; G\u00fcnther et al., 2023; Nussbaum et al., 2024; Merrick et al., 2024; Yu et al., 2024) or adapting decoder-only LLMs for embedding tasks (Wang et al., 2023; Lee et al., 2024b)."}, {"title": "2.3. Multilingual Text Embeddings", "content": "While multilingual encoders like mBert (Devlin et al., 2019) and XLM-Roberta (Conneau et al., 2020) provide a foundation for cross-lingual representation, they require additional training for high-quality sentence embeddings. Current approaches either rely on translation data (Reimers & Gurevych, 2020) or scale up model size (Wang et al., 2024b; Chen et al., 2024), typically requiring 3-5x more parameters than monolingual models to achieve comparable English performance - a phenomenon known as the \"curse of multilinguality.\"\nRecent work like Arctic Embed 2.0 (Yu et al., 2024) demonstrates that multilingual models can achieve strong English performance without compromising multilingual capability. However, existing approaches still face fundamental challenges with efficiency: state-of-the-art models require large parameter counts and generate large embedding vectors, increasing both computational and economic costs of dense retrieval.\nOur MoE-based approach directly addresses this efficiency challenge, maintaining strong performance across both English and multilingual tasks while significantly reducing the active parameter count during inference. This represents a fundamental shift from previous scaling approaches that relied solely on increasing dense model capacity."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Masked Language Modeling", "content": "Masked language modeling (MLM), a self-supervised pre-training objective introduced by Devlin et al. (2019), trains a model to recover masked tokens from input sequences. MLM was applied to both monolingual and multilingual"}, {"title": "3.2. Mixture of Experts (MoE)", "content": "Dense models activate all parameters for every input. In contrast, Sparse Mixture of Experts (MoE) models activate only a subset of parameters for each input, reducing computational requirements while maintaining model capacity (Shazeer et al., 2017).\nIn MoE architectures, standard MLP layers are replaced with MoE blocks consisting of multiple \"expert\" networks and a router. The router dynamically assigns each input token to a subset of experts using Top-K routing: the router outputs logits for all experts, applies softmax normalization, and routes each token to the topk experts with the highest probabilities (Fedus et al., 2022).\nA key challenge in training MoE models is expert collapse, where certain experts receive disproportionate traffic and others remain underutilized. This is typically addressed through an auxiliary load balancing loss (Zoph et al., 2022): L_{balance} = \\alpha \\sum_{i=1}^{E} (r_i P_i) where ri is the fraction of tokens routed to expert i and pi is\nthe mean routing probability for that expert across a batch of tokens. The coefficient a controls the strength of the balancing loss relative to the main objective."}, {"title": "3.3. Contrastive Learning", "content": ""}, {"title": "3.3.1. TRAINING TEXT EMBEDDING MODELS", "content": "Text embedding models are generally trained in two stages: weakly-supervised contrastive pretraining and contrastive finetuning (Reimers & Gurevych, 2019)."}, {"title": "3", "content": "The contrastive pretraining stage uses the InfoNCE objective (van den Oord et al., 2019) to train a biencoder to distinguish relevant text pairs from irrelevant pairs. Given a batch B = (qo, do), (q1, d\u2081)...(qn, dn), the objective is:\nL_c = - \\frac{1}{n} \\sum_{i} log \\frac{e^{s(q_i, d_i) / \\tau}}{\\sum_{j \\ne i} e^{s(q_i, d_i)/\\tau}} where s(q, d) is the learned score between query q and\ndocument d and 7 is the temperature. Contrastive finetuning incorporates high-quality human labeled datasets and hard negatives to improve retrieval performance (Wang et al., 2022). The InfoNCE objective is adapted to include these hard negatives:\nZ_i = e^{s(q_i, d_i)/\\tau} + \\sum_{j \\ne i}^{n} e^{s(q_i, d_j)/\\tau} + \\sum_{m=1}^{H} e^{s(q_i, d_{hn(i,m)})/ \\tau}\nL_c = - log \\frac{e^{s(q_i, d_i)/\\tau}}{Z_i} To reduce the storage costs of embedding vectors, which\nscale with embedding dimension, recent works have applied Matryoshka Representation Learning (Kusupati et al., 2024) during both training stages (Lee et al., 2024c). This enables more efficient storage of the computed embeddings by encouraging a rank ordering over the information content of successive embedding subspaces"}, {"title": "3.3.2. CONSISTENCY FILTERING", "content": "Consistency filtering improves dataset quality by removing potential false positives from weakly supervised data (Wang et al., 2022). In this approach, each dataset is divided into shards of 1-3M samples. An existing text embedding model first embeds all queries and documents. Query-document pairs are then discarded if a ground truth document does not appear among the top-k most similar documents to query.\nInitially developed for English text embeddings (G\u00fcnther et al., 2024; Nussbaum et al., 2024), consistency filtering has been adapted for multilingual data by Yu et al. (2024) using multilingual-E5-small (Wang et al., 2024b) with 3M samples per shard and a top-20 filtering threshold."}, {"title": "3.3.3. HARD NEGATIVE MINING", "content": "Text embedding models are typically finetuned with hard negatives mined by an existing retriever (Nussbaum et al., 2024; Yu et al., 2024). While traditional approaches use the top-k most similar documents as hard negatives, this can introduce false negatives. To address this, Moreira et al. (2024) introduced positive-aware hard negative mining:\nthreshold = pos_sim * percentage_margin where percentage_margin (typically 95%) creates a\nthreshold below which negatives are accepted, reducing false negatives. Recent work has shown that using stronger teacher models for mining yields higher quality finetuning datasets (Moreira et al., 2024; Yu et al., 2024)."}, {"title": "4. Methods", "content": ""}, {"title": "4.1. Adapting XLM-Roberta for Long-Context", "content": "To extend document-level capabilities to multilingual settings, we modify XLM-Roberta Base (Conneau et al., 2020) to handle longer sequences as XLM-Roberta's absolute positional encodings restrict inputs to 512 tokens.\nFollowing Gumma et al. (2024), we replace the absolute positional encodings with Rotary Positional Embeddings (ROPE) (Su et al., 2023b). We set the RoPE base parameter to 10,000, enabling the model to extrapolate to longer sequences while maintaining stable performance. While recent work (Liu et al., 2024; Xiong et al., 2023) suggests using larger RoPE bases, our experiments showed degraded performance on GLUE and XTREME-R benchmarks with larger values. This difference might stem from our training approach \u2013 unlike Zhang et al. (2024), who first train with shorter sequences (2,048 tokens) before scaling up, we maintain consistent sequence lengths throughout training.\nWe use 2048-token segments from a reconstructed CC100 dataset. Following the original XLM-Roberta training protocol, we set the language sampling temperature to 0.3.\nWe train for 10,000 steps with hyperparameters detailed in"}, {"title": "4", "content": "We refer to our adapted model as mNomic-BERT."}, {"title": "4.2. Consistency Filtering", "content": "To ensure high-quality training data, we implement retrieval-based consistency filtering on our multilingual corpus consisting of data from mC4 and multilingual CC News. This approach, established in recent work (Yu et al., 2024; Nussbaum et al., 2024), helps eliminate low-quality or misaligned text pairs from the training set.\nFor each language in our corpus, we divide the dataset into segments of 1 million examples. Using the multilingual E5 small embedding model (Wang et al., 2024b), we compute similarity between query-document pairs. We retain only pairs where the document ranks among the top 2 most similar documents for its corresponding query, following similar filtering approaches in (Wang et al., 2022; G\u00fcnther et al., 2023). For English-language data, we utilize the pre-filtered dataset from Nussbaum et al. (2024).\nThis filtering process yields a final training dataset of 1.6 billion high-quality pairs. The distribution of data across different languages is detailed in Appendix A."}, {"title": "4.3. Weakly-Supervised Contrastive Pretraining", "content": "For our contrastive pretraining phase, we initialize a bi-encoder with mNomic-BERT and train it on our filtered contrastive dataset for one epoch. Following Komatsuzaki et al. (2023), we transform every alternate MLP layer into an MoE layer with 8 experts and top-2 routing, starting from the second layer. This results in a model with 475M total parameters, of which only 305M are active during inference. We set the load balancing loss coefficient a from Equation\n1 to 1.\nFor training, we use the InfoNCE contrastive loss (van den Oord et al., 2019) with a temperature of T = 0.02. Following recent work (Nussbaum et al., 2024; Merrick et al., 2024), we process one dataset per batch with a batch size of 16,384, using random batch sampling. Similar to Yu et al. (2024), we set maximum sequence lengths of 32 and 256 tokens for queries and documents respectively due to computational constraints.\nWe train the model using 16 H100 GPUs with distributed"}, {"title": "4.4. Hard Negative Mining", "content": "For each query in our dataset, we mine hard negatives using a margin-based approach defined in Equation 5. We use the data from Chen et al. (2024) and BGE M3 for filtering both English and multilingual data."}, {"title": "4.5. Contrastive Finetuning", "content": "We finetune the pretrained biencoder from Section 4.3 using our mined hard negatives. For each query, we incorporate 10 hard negative examples during training. We train for one epoch using a batch size of 256, with a peak learning rate of 2e-5, 400 warmup steps, and linear decay. Compared to pretraining, we increase both query and document maximum lengths to 512 tokens.\nTo enable efficient inference at multiple dimensions, we incorporate Matryoshka Representation Learning (Kusupati et al., 2024), training the model to produce effective embeddings at dimensions 768 and 256. The distribution of our finetuning data is detailed in Appendix C.\nWe refer to this final model as Nomic Embed v2."}, {"title": "5. Experimental Setup", "content": ""}, {"title": "5.1. GLUE Evaluation Protocol", "content": "We evaluate mNomic-BERT on the GLUE benchmark (Wang et al., 2019), following the evaluation protocol from Nussbaum et al. (2024). We train each model on 8 GLUE tasks for 3 epochs across 5 random seeds, varying batch sizes (16, 32) and learning rates (1e-5, 2e-5, 3e-5). For mGTE evaluation, we modify these parameters to use 6% warmup and max gradient norm of 1, matching Zhang et al. (2024). Following standard practice (Liu et al., 2019), we initialize RTE, STSB, and MRPC tasks from an MNLI checkpoint. Table 6 details the complete hyperparameter configuration."}, {"title": "5.2. XTREME-R Evaluation Setup", "content": "We evaluate mNomic-BERT on XTREME-R (Ruder et al., 2021), a comprehensive benchmark consisting of 10 tasks designed to assess multilingual natural language understanding capabilities. All experiments follow a zero-shot cross-lingual transfer protocol: models are trained exclusively on English data and evaluated on multilingual and cross-lingual tasks. We utilize the evaluation pipeline from Zhang et al.\n(2024) to ensure fair comparison with baseline models"}, {"title": "5.3. Text Embedding Benchmark Setup", "content": "We evaluate our model on two retrieval benchmarks: (1) BEIR, the retrieval subset of MTEB (Muennighoff et al., 2023), which focuses on English-only retrieval, and (2) MIRACL (Zhang et al., 2022), which evaluates multilingual retrieval capabilities. For all experiments, we:\n\u2022 Prepend task-specific prefixes \u201csearch_query\" and\n\"search_document\" to queries and documents\n\u2022 Truncate all inputs to 512 tokens\n\u2022 Measure performance using nDCG@10\nFor reproducibility, we conduct all evaluations using the FlagEmbedding framework, except for mE5 results which are taken directly from Wang et al. (2024b). Note that mE5 results for German (de) and Yoruba (yo) languages were not reported in the original paper."}, {"title": "6. Results", "content": ""}, {"title": "6.1. mNomic-BERT GLUE Results", "content": "Our approach achieves strong performance across the GLUE benchmark, as shown in Table 3. Specifically, mNomic-BERT achieves comparable performance to XLM-R-Base across all tasks, demonstrating that our RoPE-based positional encoding modification and lightweight finetuning preserve the model's capabilities. Notably, mNomic-BERT matches mGTE-Base performance while requiring only 3% of mGTE-Base's pretraining steps, suggesting that our lightweight finetuning approach effectively extends context length without extensive pretraining.\nWhile Zhang et al. (2024) reported lower CoLA scores for XLM-Roberta, our hyperparameter search revealed that this task is particularly sensitive to configuration choices. We successfully reproduced mGTE-Base's reported COLA performance but found significant variance across different hyperparameter settings, resulting in a lower median score."}, {"title": "6.2. XTREME-R Results", "content": "Table 4 presents the performance of mNomic-BERT compared to XLM-R-Base and mGTE-Base across XTREME-R tasks. mNomic-BERT achieves an average score of 62.70, which is comparable to XLM-R-Base's 62.31 but falls slightly behind mGTE-Base's 64.63. This pattern is consistent across most individual tasks, with mNomic-BERT and XLM-R-Base showing similar performance levels. These results suggest that our approach maintains the cross-lingual capabilities of the base architecture while extending the context length of multilingual text encoders, complementing recent work by Gumma et al. (2024)."}, {"title": "6.3. Text Embedding Benchmark", "content": "We evaluate performance on BEIR, the retrieval subset of MTEB (Muennighoff et al., 2023), an English-only benchmark, and MIRACL (Zhang et al., 2022), a multilingual retrieval benchmark. Results can be found in Table 1 and 5.\nCompared to similarly sized parameter models, Nomic Embed v2 outperforms all models on BEIR and MIRACL except Arctic Embed v2 Base. However, Yu et al. (2024) do not release any of their training data of which a large percentage consists of private web search data.\nDespite being 2x smaller, Nomic Embed v2 outperforms all multilingual models on BEIR, except Arctic Embed v2 Large, and is competitive with all models on MIRACL."}, {"title": "7. Analysis", "content": ""}, {"title": "7.1. Effectiveness of MoEs for Text Embeddings", "content": "We compare monolingual MoE and dense text embedding models by pretraining them on 235M weakly-supervised contrastive pairs from Nussbaum et al. (2024). For evaluation, we use the BEIR benchmark (Thakur et al., 2021) across varying batch sizes, with a fixed maximum sequence length of 128 tokens. Our MoE model (Nomic BERT MOE) is created by upcycling alternate layers of Nomic BERT following Komatsuzaki et al. (2023). The model uses token choice routing with TopK Routing (k = 1, also known as Switch Routing (Fedus et al., 2022)) and 8 experts. We compare this against two baselines: the original Nomic BERT and BERT Large (Devlin et al., 2019).\nFigure 1 shows that Nomic BERT MoE consistently outperforms the original Nomic BERT across all batch sizes, despite maintaining a similar number of active parameters. Notably, our MoE model achieves comparable performance to BERT Large, despite the latter having 3x more active parameters, demonstrating the efficiency of the MoE architecture."}, {"title": "7.2. Effectiveness of MoEs for Multilingual Text Embeddings", "content": "We extend our analysis to the multilingual setting by incorporating an additional 65M weakly-supervised contrastive pairs from mC4 (Xue et al., 2021) and Multilingual CC News (Wang et al., 2024b). For a controlled ablation study, we focus on six languages spanning different language families: English, Chinese, Arabic, Hindi, Spanish, and Swahili. This selection includes both high-resource and low-resource languages, with Swahili representing the latter category. We evaluate three models: XLM-ROBERTa Base (Conneau et al., 2020), our MoE variant (XLM-ROBERTa MoE Base), and XLM-ROBERTa Large. Performance is measured using NDCG@10 on both BEIR (Thakur et al., 2021) and MIR-ACL (Zhang et al., 2022) benchmarks across different batch sizes.\nTable 9 presents our multilingual evaluation results. While our MoE model consistently outperforms its dense counterpart across all batch sizes on both BEIR and MIRACL benchmarks, it does not match the performance of the larger model-a notable departure from our monolingual findings.\nOur experiments reveal that data scale significantly impacts the performance of XLM-ROBERTa MoE Base. Initial experiments with a smaller dataset of 100M total contrastive pairs showed the MoE model consistently underperforming its parameter-equivalent dense counterpart. This aligns with findings from Krajewski et al. (2024), who observed that MoE models tend to underperform dense models under limited training regimes."}, {"title": "7", "content": "We investigate the impact of different teacher models and\nmargin thresholds for hard negative mining, following the\napproach of Moreira et al. (2024). We initialize our model\nfrom E5-Large Unsupervised (Wang et al., 2022) and mine\nnegatives using Equation 5. Our training data comprises\napproximately 500k examples from three sources: StackEx-\nchange Title-Body pairs, SQUAD (Rajpurkar et al., 2016),\nand Natural Questions (NQ) (Kwiatkowski et al., 2019). We\nevaluate performance on three BEIR datasets: NQ, FiQA,\nand HotpotQA (Thakur et al., 2021). For teacher models,\nwe compare NVEmbed v1 (Lee et al., 2024a), Arctic Embed\nLarge (Merrick et al., 2024), and Stella 1.5B v5 (Zhang &\nFulong Wang, 2024).\nTable 8 presents our findings across different teacher models\nand mining parameters. Several key trends emerge: Posi-\ntive aware hard negative mining with consistently improves\nperformance, as shown by the 2.33 point average improve-\nment when using Arctic Embed Large with a margin of 0.95\ncompared to no margin. Surprisingly, Stella 1.5B outper-\nforms NVEmbed v1 even though it is a 7x smaller model.\nIncreasing the number of negative examples from 4 to 10\nwith Stella 1.5B yields modest but consistent improvements,\nwith the best average performance of 57.45 achieved using\n10 negatives. However, the gains diminish with each addi-\ntional negative, suggesting a potential plateau in the benefits\nof increased negative examples. Finally, varying the margin\nthreshold between 0.95 and 0.98 shows minimal impact on\noverall performance, indicating that the mining process is\nrelatively robust to this hyperparameter within this range.\nWe also compared our best-performing mined dataset\nagainst a filtered version of the finetuning data released\nby Chen et al. (2024). Using BGE M3 to filter negatives\nbased on Equation 5, this approach achieved 1 point higher\nNDCG@10 on BEIR, suggesting filtering potential nega-\ntives from an existing mined dataset is also a viable option."}, {"title": "8. Conclusion", "content": "We introduce Nomic Embed v2, the first Mixture of Expert Embedding Model. Nomic Embed v2 outperforms similarly sized and larger embedding models in both English and Multilingual Retrieval benchmarks while being trained only publicly available data. Nomic Embed v2 proves a successful alternative to scaling text embedding models without increasing computational costs."}, {"title": "9. Limitations and Future Work", "content": "Our work with Nomic Embed v2 demonstrates the advantages of MoE architectures over dense models for text embeddings. However, this represents only an initial exploration of MoE applications in this domain. Several promising research directions emerge: investigating the optimal scaling of expert count and active parameters, exploring alternative routing mechanisms, and examining how loss-free routing could leverage the bidirectional nature of these models. Furthermore, techniques for distilling MoE models back into dense architectures could make these improvements more widely deployable.\nBeyond architectural choices, understanding the fundamental scaling relationships between dataset size, model parameters, and embedding dimension would provide valuable insights for the field. This could help establish whether the benefits of MoE architectures persist or even compound at larger scales."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}