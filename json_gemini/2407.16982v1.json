{"title": "Diffree: Text-Guided Shape Free Object\nInpainting with Diffusion Model", "authors": ["Lirui Zhao", "Tianshuo Yang", "Wenqi Shao", "Yuxin Zhang", "Yu Qiao", "Ping Luo", "Kaipeng Zhang", "Rongrong Ji"], "abstract": "This paper addresses an important problem of object addition for images with only text guidance. It is challenging because the new object must be integrated seamlessly into the image with consistent visual context, such as lighting, texture, and spatial location. While existing text-guided image inpainting methods can add objects, they either fail to preserve the background consistency or involve cumbersome human intervention in specifying bounding boxes or user-scribbled masks. To tackle this challenge, we introduce Diffree, a Text-to-Image (T2I) model that facilitates text-guided object addition with only text control. To this end, we curate OABench, an exquisite synthetic dataset by removing objects with advanced image inpainting techniques. OABench comprises 74K real-world tuples of an original image, an inpainted image with the object removed, an object mask, and object descriptions. Trained on OABench using the Stable Diffusion model with an additional mask prediction module, Diffree uniquely predicts the position of the new object and achieves object addition with guidance from only text. Extensive experiments demonstrate that Diffree excels in adding new objects with a high success rate while maintaining background consistency, spatial appropriateness, and object relevance and quality.", "sections": [{"title": "Introduction", "content": "With the recent remarkable success of Text-to-Image (T2I) models (e.g., Stable\nDiffusion [21], Midjourney [17], and DALL-E [2,23]), creators can quickly gener-\nate high-quality images with text guidance. The rapid development has driven\nvarious text-guided image editing techniques [3,6,27,35,37]. Among these tech-\nniques, text-guided object addition which inserts an object into the given image\nhas attracted much attention due to its diverse applications, such as advertise-\nment creation, visual try-on, and renovation visualization. While important, ob-\nject addition is challenging because the object must be integrated seamlessly into\nthe image with consistent visual context, such as lighting, texture, and spatial\nlocation.\nExisting techniques for object addition in images can be broadly categorized\ninto mask-guided and text-guided approaches (Fig. 2). Mask-guided algorithms\ntypically require the specification of a region where the new object will be in-\nserted. For example, traditional image inpainting methods [1,15,20,30,34] focus\non seamlessly filling user-defined masks within an image to match the surround-\ning context. Recent advancements, such as PowerPaint [38], have effectively in-\ncorporated objects into images given their shape and textual descriptions while\nmaintaining background consistency. However, manually delineating an ideal re-\ngion for all objects, considering shape, size, and position, can be labor-intensive\nand typically requires drawing skills or professional knowledge. On the other\nhand, text-guided object addition methods, such as InstructPix2Pix [3], attempt\nto add new objects using only text-based instructions. Despite this, these meth-\nads have a low success rate and often result in background inconsistencies, as\ndemonstrated in Fig. 2 and Fig. 9. Additionally, when employing InstructPix2Pix\nfor iterative object addition, the quality of the inpainted image tends to degrade\nprogressively with each step, as illustrated in Fig. 10.\nTo tackle the above challenges, we introduce Diffree, a diffusion model with\nan additional object mask predictor module that can predict an ideal mask\nfor a candidate inpainting object and achieve shape-free object addition with\nonly text guidance. Compared with previous works [3, 6, 33, 38], our Diffree\nhas three appealing properties. First, Diffree can achieve impressive text-guided\nobject addition results while keeping the background unchanged. In contrast,\nprevious text-guided methods [3] struggle to guarantee this. Second, Diffree\ndoes not require additional mask input, which is necessary for traditional mask-\nguided methods [33]. In real scenarios, high-quality masks are hard to obtain.\nThird, Diffree can generate the instance mask and thus can be further combined\nwith various existing works [4,19] to develop exciting applications. For example,\nDiffree can achieve image-prompted object addition when combined with Any-\nDoor [4] and plan to add objects suggested by GPT4V [19], as shown in Fig.\n11.\nTowards high-quality text-guided object addition, we curate a synthetic dataset\nnamed Object Addition Benchmark (OABench) which consists of 74K real-world\ntuples including an original image, an inpainted image, a mask image of the\nobject, and an object description."}, {"title": "Related Work", "content": "Text-to-Image Diffusion Models Recently, text-to-image (T2I) diffusion mod-\nels [2, 18, 23], have shown exceptional capability in image generation quality and\nextraordinary proficiency in accurately following text prompts, under the dual\nsupport of large-scale text-image dataset [26] and model optimizations [5,10,24].\nGLIDE [18] incorporated text conditions into the diffusion model and empiri-\ncally showed that leveraging classifier guidance leads to visually appealing out-\ncomes. DALLE-2 [23] enhances text-image alignment via CLIP [22] joint feature\nspace, DALLE-3 [2] further improves the prompt following abilities by training\non highly descriptive generated image captions. Stable Diffusion [24], which is\nwell-established and widely adopted, garners significant attention and applica-\ntion within and beyond the research community. Given that T2I models generate\ncomprehensive images from text prompts, even minor alterations in prompts can\nresult in substantial changes to the resultant image [3]. Consequently, there has\nbeen an increased focus not only on T2I generation but also on image editing\nbased on additional conditions such as text inputs, masks, et al.\nText-Guided Image Editing The effectiveness of the text-guided image edit-\ning methos [3,6,27,35] largely depends on the composition of its dataset and how\nit is collected. InstructPix2Pix [3] combines two large pretrained models, a large\nlanguage model [16] and a T2I model [24], to generate a dataset for training\na diffusion model to follow written image editing text prompts. Its innovative\ndata collection method allows InstructPix2Pix to follow instructions and shows\namazing effects, while makes its consistency is difficult to guarantee due to both\ninput and output are generated by the T2I model. InstructDiffusion [6] treats\nall computer vision tasks as image generation with multiple output formats, and\naligns these tasks with human instructions. Emu Edit [27] adapt its architecture\nfor multi-task learning and train it an unprecedented range of tasks formulated"}, {"title": "Methodology", "content": "Given an image and the object description, our goal is to add the object to the\nimage while preserving the background consistency. Following this, we initially\nintroduce OABench, an synthetic dataset for this task, comprising image-text\npairs (as input) with corresponding object masks and images containing the\nobject (as output). We provide an overview of our data collection pipeline in\nSec. 3.1 and. We next present Diffree, an architecture amalgamating a Stable\nDiffusion model with an Object Mask Predictor (OMP) module in Sec. 3.2. The\nevaluation procedure is presented in Sec. 3.3."}, {"title": "OABench", "content": "We combine existing instance segmentation dataset [7,14] with powerful im-\nage inpainting method [38] to generate the OABench. Unlike other instructions\nfollow methods [3, 35], generating both data pairs using existing text-to-image\n(T2I) models [23, 24] with prompt pairs and filtering, we use the real image\nwith object to synthesize the image without the object, as depicted in Fig. 5.\nThis can greatly ensure the consistency of the background as in other image\ninpainting methods [38] that require masks. Furthermore, an object in the real\nimage naturally aligns with its background, i.e., it is appropriate for generating\nthe corresponding image without the same object. In the following sections, we\ndescribe in detail the three steps of this process.\nCollection and Filtering We gather and refine instances suitable for image\ninpainting by applying a set of rules from the LVIS dataset [7], a large instance\nsegmentation dataset annotated for COCO [14] dataset. As depicted in Fig. 5,\nin images containing multiple instances, we enforce size constraints to exclude\ninstances that are too big or too small (typically related to object components\nor background elements like buttons on clothing or rivers). Subsequently, incom-\nplete instances are filtered out using edge detection and integrity assessments.\nInstances that are partially obscured are identified through cavity inspection,\niterative IOU algorithm application, and common part comparison among vari-\nous instances. Additionally, objects with exceptionally high aspect ratios, which\ntend to yield subpar inpainting outcomes, are also eliminated.\nData Synthesis We next employ a powerful image inpainting method, Power-\nPaint [38], to eliminate specific instances obtained in the preceding stage. There-"}, {"title": "Diffree", "content": "For an image x and a text prompt d, Diffree predicts a binary mask m that\nspecifies the region in x and generates an image 2. The masked region Om aligns\nwith the text prompt d. To this end, Diffree is instantiated with a pre-trained\nT2I diffusion model (e.g. Stable Diffusion [24]) with a object mask prediction\n(OMP) module as shown in Fig. 6.\nDiffusion Model learns to generate data samples by iteratively applying de-\nnoising autoencoders that estimate the score function [29] of a given data dis-\ntribution [28]. Stable Diffusion [24] apply them in the latent space of powerful\npre-trained variational autoencoder [12], including encoder & and decoder D,\nto reduce computing resources while maintaining quality and flexibility. Stable\nDiffusion encompasses both forward and reverse processes. Given an image \u0129,\nthe forward process adds noise to the encoded latent z = E(x):\n$z_{t} = \\sqrt{\\bar{a}_{t}}\\tilde{z} + \\sqrt{1 - \\bar{a}_{t}}e, e \\sim N(0, I)$"}, {"title": "OMP Module", "content": "OMP Module and diffusion model are trained simultaneously and used to pre-\ndict the binary mask m. The OMP module comprises two convolutional layers,\ntwo ResBlocks, and an attention block, as illustrated in Fig. 6. First, we calcu-\nlate the predicted noise-free latent of using the output of the diffusion model:\n$\\tilde{o}_{t} = \\frac{z_{t} - \\sqrt{1 - \\bar{a}_{t}}\\epsilon_{\\theta}(z_{t}, z, EnCtxt(d), t)}{\\sqrt{\\bar{a}_{t}}}$\nHere, the concatenation of z = E(x) with \u00f5t serves as inputs to the OMP module.\nThe gradient of ot is detached to optimize the two models without affecting each\nother. We conduct bilinear interpolation downsampling on the mask m to obtain\nm', preserving its size identical to the input latent. The OMP module's network\nTe is optimized according to the following objective function:\n$L_{OMP} = E_{\\epsilon(z), E(x), d, m} [||m' - T_{\\theta}(\\tilde{o}_{t}, z)||^{2}]$"}, {"title": "Evaluation Metric", "content": "Due to the absence of robust quantitative metrics for shape-free object inpainting\nexcept the success rate, we propose a set of evaluation rules leveraging exits\nmetrics [8, 9, 19, 33, 36] to evaluate different methods in different aspects.\nWe first randomly select and manually inspect 1,000 evaluation data pairs\nfrom COCO [14] and OpenImages [13] independently to ensure the validity of\nthe object in the image and generalizability of the evaluation dataset. Each data\npair comprises an original image Xori, a text prompt of an object d, and an\ninpainted image x. The resulting output image xoutput and the corresponding\nobject mask moutput are outcomes derived from distinct methods."}, {"title": "Background Consistency", "content": "Background Consistency We adapt LPIPS [36], a widely adopted and robust\nmetric for assessing the similarity between images, to evaluate this aspect:\n$S_{con}(X, X_{output}, M_{output}) = LPIPS (x, x \\cdot M_{output} + X_{output} \\cdot (1 - M_{output}))$"}, {"title": "Location Reasonableness", "content": "Assessing the reasonableness of the object's loca-\ntion is a challenging task due to its inherent subjectivity. Surprisingly, we note\nGPT4V [19] demonstrates strong discriminative abilities in assessing variations\nand evaluating different locations by providing x, d, Xoutput and an instruction T\nas illustrated in Fig. 8. GPT4V rates the appropriateness of the object's position\non a scale from 1 to 5, while also providing justifications for these ratings:\n$S_{rea}(X, X_{output}, d,T) = GPT4V (x, x_{output}, d, T)$"}, {"title": "Object Correlation", "content": "To quantify this relationship, we utilize CLIP Score [8],\na metric to assess the correlation between text and image, by calculating the\ncosine similarity of their embeddings from CLIP [22]. we measure CLIP Score\nbetween the object area of xoutput and d, which is referred to as \"Local CLIP\nScore\":\n$S_{cor}(d, x_{output}, M_{output}) = CLIPScore (d, Local(x_{output}, M_{output}))$"}, {"title": "Object Quality and Diversity", "content": "Following [33], we employ Local FID, mea-\nsuring Fr\u00e9chet Inception Distance (FID) [9] on the local regions, to evaluate the\nquality and diversity of generated object:\n$S_{qd}(L_{X_{org}}, L_{X_{output}}) =||\\mu_{L_{X_{org}}} - \\mu_{L_{X_{output}}} ||^{2} + Tr(\\Sigma_{L_{X_{org}}} + \\Sigma_{L_{X_{output}}}\n- 2* (\\Sigma_{L_{X_{org}}} * \\Sigma_{L_{X_{output}}}))$\nwhere LXorg and LXoutput respectively denote the sets comprising all local re-\ngions of the original images and output images, \u03bc and \u2211 represent the mean and\nvariance of the feature vectors obtained through a particular network [9]."}, {"title": "Experimental Settings", "content": "Training Setups we employ OABench to train Diffree, initializing the diffusion\nmodel with the Stable Diffusion 1.5 [24] weights. We set X = 2 in Eq. (5) and set\na batch size of 256. Our model was trained around 10K steps on 8 A100 GPUs.\nEvaluation Datasets and Metrics As outlined in Sec. 3.3, we employ four\nmetrics (LPIPS [36], GPT4V [19], Local CLIP Score and Local FID [33]) along-\nside the unified metric for evaluation on COCO [14] and OpenImages [13]."}, {"title": "Main results", "content": "Tab. 1 shows the main results of Diffree with different baselines. We report the\nresults of four powerful metrics and Unified Metric. It is worth to highlight that\nonly successful InstructPix2pix results are computed and PowerPaint is utilized\nfor image inpainting under the masks provided by the results of our approach.\nWe can deduce the following conclusions from the results in several aspects.\nSuccess Rate We achieved a success rate of over 98% on different dataset, while\nInstructPix2pix shows a lower success rate in object addition (17.2% and 18.9%).\nAs shown in Fig. 9, most of the results of InstructPix2pix involve replacing\nexisting object, without adding or significant changes to the background. This\ndemonstrates our excellent ability to complete this task. Meanwhile it is not\napplicable to PowerPaint as it necessitates a mask input.\nConsistency of Background Diffree significantly outperforms InstructPix2pix\nin the LPIPS scores across all datasets (all decreased by 36% than Instruct-\nPix2pix). In particular, only scores from carefully chosen successful cases of\nInstructPix2pix were computed, potentially leading to an overestimation. Fur-\nthermore, Diffree, as a shape free inpainting method, yields LPIPS results com-\nparable to PowerPaint, as a shape required inpainting method. As discussed in"}, {"title": "Visualization", "content": "We provide different types' visualizations to more intuitively evaluate Diffree's\ncapabilities Figs. 1 to 4, 7, 10 and 11, please refer to the respective image captions\nfor detailed explanations. For more results, please refer to the appendix."}, {"title": "Application", "content": "Diffree can be well combined with other methods for more expansion.\nWith GPT4V GPT4V [19] has a good ability to perceive and understand\nimages, therefore we can use GPT4V for planning a object suitable for the\nimage scene, seeing Fig. 11. However, when task with adding corresponding\nobject without altering the background, DALL-E-3 [2] in GPT4, falls short.\nWith Other Methods AnyDoor [4] can add a specific object to the designated\narea by providing a mask and object image. As depicted in Fig. 11, Diffree can\ncombined with AnyDoor to further achieve adding a specific object to image.\nDIffree also can effectively leverage the continuous progress in the image inpaint-\ning, to generate superior images, as demonstrated in Tab. 1.\nIterative Operation In Figs. 1 and 10, we present results of iterative inpaint-\ning. Leveraging the predicted mask from OMP module, Diffree can preserve\nthe image background from cumulative degradation during successive inpaint-\ning. This holds potential applications within architectural and interior design\ndomains."}, {"title": "Conclusion", "content": "We propose a novel method, Diffree, that leverages a diffusion model with an\nobject mask predictor for text-guided object addition. Beyond the method, we\nbuild a high-quality synthetic dataset, OABench, through a novel data collection\nmethod for this task. Diffree distinguishes itself by preserving background consis-\ntency without requiring additional masks, which solves shortcomings of previous\ntext-guided and mask-guided object addition methods. The quantitative and\nqualitative results demonstrate the superiority of our method."}]}