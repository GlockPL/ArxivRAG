{"title": "Diffree: Text-Guided Shape Free Object\nInpainting with Diffusion Model", "authors": ["Lirui Zhao", "Tianshuo Yang", "Wenqi Shao", "Yuxin Zhang", "Yu Qiao", "Ping Luo", "Kaipeng Zhang", "Rongrong Ji"], "abstract": "This paper addresses an important problem of object addi-\ntion for images with only text guidance. It is challenging because the\nnew object must be integrated seamlessly into the image with consistent\nvisual context, such as lighting, texture, and spatial location. While ex-\nisting text-guided image inpainting methods can add objects, they either\nfail to preserve the background consistency or involve cumbersome hu-\nman intervention in specifying bounding boxes or user-scribbled masks.\nTo tackle this challenge, we introduce Diffree, a Text-to-Image (T2I)\nmodel that facilitates text-guided object addition with only text con-\ntrol. To this end, we curate OABench, an exquisite synthetic dataset by\nremoving objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted im-\nage with the object removed, an object mask, and object descriptions.\nTrained on OABench using the Stable Diffusion model with an addi-\ntional mask prediction module, Diffree uniquely predicts the position of\nthe new object and achieves object addition with guidance from only\ntext. Extensive experiments demonstrate that Diffree excels in adding\nnew objects with a high success rate while maintaining background con-\nsistency, spatial appropriateness, and object relevance and quality.", "sections": [{"title": "1 Introduction", "content": "With the recent remarkable success of Text-to-Image (T2I) models (e.g., Stable\nDiffusion [21], Midjourney [17], and DALL-E [2,23]), creators can quickly gener-\nate high-quality images with text guidance. The rapid development has driven\nvarious text-guided image editing techniques [3,6,27,35,37]. Among these tech-\nniques, text-guided object addition which inserts an object into the given image\nhas attracted much attention due to its diverse applications, such as advertise-\nment creation, visual try-on, and renovation visualization. While important, ob-\nject addition is challenging because the object must be integrated seamlessly into\nthe image with consistent visual context, such as lighting, texture, and spatial\nlocation.\nExisting techniques for object addition in images can be broadly categorized\ninto mask-guided and text-guided approaches (Fig. 2). Mask-guided algorithms\ntypically require the specification of a region where the new object will be in-\nserted. For example, traditional image inpainting methods [1,15,20,30,34] focus\non seamlessly filling user-defined masks within an image to match the surround-\ning context. Recent advancements, such as PowerPaint [38], have effectively in-\ncorporated objects into images given their shape and textual descriptions while\nmaintaining background consistency. However, manually delineating an ideal re-\ngion for all objects, considering shape, size, and position, can be labor-intensive\nand typically requires drawing skills or professional knowledge. On the other\nhand, text-guided object addition methods, such as InstructPix2Pix [3], attempt\nto add new objects using only text-based instructions. Despite this, these meth-\nads have a low success rate and often result in background inconsistencies, as\ndemonstrated in Fig. 2 and Fig. 9. Additionally, when employing InstructPix2Pix\nfor iterative object addition, the quality of the inpainted image tends to degrade\nprogressively with each step, as illustrated in Fig. 10.\nTo tackle the above challenges, we introduce Diffree, a diffusion model with\nan additional object mask predictor module that can predict an ideal mask\nfor a candidate inpainting object and achieve shape-free object addition with\nonly text guidance. Compared with previous works [3, 6, 33, 38], our Diffree\nhas three appealing properties. First, Diffree can achieve impressive text-guided\nobject addition results while keeping the background unchanged. In contrast,\nprevious text-guided methods [3] struggle to guarantee this. Second, Diffree\ndoes not require additional mask input, which is necessary for traditional mask-\nguided methods [33]. In real scenarios, high-quality masks are hard to obtain.\nThird, Diffree can generate the instance mask and thus can be further combined\nwith various existing works [4,19] to develop exciting applications. For example,\nDiffree can achieve image-prompted object addition when combined with Any-\nDoor [4] and plan to add objects suggested by GPT4V [19], as shown in Fig.\n11.\nTowards high-quality text-guided object addition, we curate a synthetic dataset\nnamed Object Addition Benchmark (OABench) which consists of 74K real-world\ntuples including an original image, an inpainted image, a mask image of the\nobject, and an object description. The data curation process is illustrated in"}, {"title": "2 Related Work", "content": "Text-to-Image Diffusion Models Recently, text-to-image (T2I) diffusion mod-\nels [2, 18, 23], have shown exceptional capability in image generation quality and\nextraordinary proficiency in accurately following text prompts, under the dual\nsupport of large-scale text-image dataset [26] and model optimizations [5,10,24].\nGLIDE [18] incorporated text conditions into the diffusion model and empiri-\ncally showed that leveraging classifier guidance leads to visually appealing out-\ncomes. DALLE-2 [23] enhances text-image alignment via CLIP [22] joint feature\nspace, DALLE-3 [2] further improves the prompt following abilities by training\non highly descriptive generated image captions. Stable Diffusion [24], which is\nwell-established and widely adopted, garners significant attention and applica-\ntion within and beyond the research community. Given that T2I models generate\ncomprehensive images from text prompts, even minor alterations in prompts can\nresult in substantial changes to the resultant image [3]. Consequently, there has\nbeen an increased focus not only on T2I generation but also on image editing\nbased on additional conditions such as text inputs, masks, et al.\nText-Guided Image Editing The effectiveness of the text-guided image edit-\ning methos [3,6,27,35] largely depends on the composition of its dataset and how\nit is collected. InstructPix2Pix [3] combines two large pretrained models, a large\nlanguage model [16] and a T2I model [24], to generate a dataset for training\na diffusion model to follow written image editing text prompts. Its innovative\ndata collection method allows InstructPix2Pix to follow instructions and shows\namazing effects, while makes its consistency is difficult to guarantee due to both\ninput and output are generated by the T2I model. InstructDiffusion [6] treats\nall computer vision tasks as image generation with multiple output formats, and\naligns these tasks with human instructions. Emu Edit [27] adapt its architecture\nfor multi-task learning and train it an unprecedented range of tasks formulated"}, {"title": "3 Methodology", "content": "Given an image and the object description, our goal is to add the object to the\nimage while preserving the background consistency. Following this, we initially\nintroduce OABench, an synthetic dataset for this task, comprising image-text\npairs (as input) with corresponding object masks and images containing the\nobject (as output). We provide an overview of our data collection pipeline in\nSec. 3.1 and. We next present Diffree, an architecture amalgamating a Stable\nDiffusion model with an Object Mask Predictor (OMP) module in Sec. 3.2. The\nevaluation procedure is presented in Sec. 3.3."}, {"title": "3.1 OABench", "content": "We combine existing instance segmentation dataset [7,14] with powerful im-\nage inpainting method [38] to generate the OABench. Unlike other instructions\nfollow methods [3, 35], generating both data pairs using existing text-to-image\n(T2I) models [23, 24] with prompt pairs and filtering, we use the real image\nwith object to synthesize the image without the object, as depicted in Fig. 5.\nThis can greatly ensure the consistency of the background as in other image\ninpainting methods [38] that require masks. Furthermore, an object in the real\nimage naturally aligns with its background, i.e., it is appropriate for generating\nthe corresponding image without the same object. In the following sections, we\ndescribe in detail the three steps of this process.\nCollection and Filtering We gather and refine instances suitable for image\ninpainting by applying a set of rules from the LVIS dataset [7], a large instance\nsegmentation dataset annotated for COCO [14] dataset. As depicted in Fig. 5,\nin images containing multiple instances, we enforce size constraints to exclude\ninstances that are too big or too small (typically related to object components\nor background elements like buttons on clothing or rivers). Subsequently, incom-\nplete instances are filtered out using edge detection and integrity assessments.\nInstances that are partially obscured are identified through cavity inspection,\niterative IOU algorithm application, and common part comparison among vari-\nous instances. Additionally, objects with exceptionally high aspect ratios, which\ntend to yield subpar inpainting outcomes, are also eliminated.\nData Synthesis We next employ a powerful image inpainting method, Power-\nPaint [38], to eliminate specific instances obtained in the preceding stage. There-"}, {"title": "Post-Processing", "content": "In the post-processing stage, we filter out the results with\npoor effects in image inpainting. For some special cases (e.g., one of many dense\nand adjacent small cakes), image inpainting cannot effectively remove objects\ndue to the complexity of the background. Thus we calculate the clip score [8]\nusing the object name and the region of the inpainted image, setting a thresh-\nold to remove images with higher scores which are deemed suboptimal. Finally,\nOABench includes 74,774 high-quality data pairs, each data pair includes a syn-\nthetic image and object caption as input, object masks and original images as\noutput."}, {"title": "3.2 Diffree", "content": "For an image x and a text prompt d, Diffree predicts a binary mask m that\nspecifies the region in x and generates an image \\hat{x}. The masked region \\mathcal{O}\\left(m\\right) aligns\nwith the text prompt d. To this end, Diffree is instantiated with a pre-trained\nT2I diffusion model (e.g. Stable Diffusion [24]) with a object mask prediction\n(OMP) module as shown in Fig. 6.\nDiffusion Model learns to generate data samples by iteratively applying de-\nnoising autoencoders that estimate the score function [29] of a given data dis-\ntribution [28]. Stable Diffusion [24] apply them in the latent space of powerful\npre-trained variational autoencoder [12], including encoder \\mathcal{E} and decoder \\mathcal{D},\nto reduce computing resources while maintaining quality and flexibility. Stable\nDiffusion encompasses both forward and reverse processes. Given an image x,\nthe forward process adds noise to the encoded latent z = \\mathcal{E}(x):\n$$z_{t} = \\sqrt{\\bar{\\alpha}_{t}}z + \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n(1)"}, {"title": "OMP Module", "content": "and diffusion model are trained simultaneously and used to pre-\ndict the binary mask m. The OMP module comprises two convolutional layers,\ntwo ResBlocks, and an attention block, as illustrated in Fig. 6. First, we calcu-\nlate the predicted noise-free latent \\hat{z}_{0} using the output of the diffusion model:\n$$\\hat{z}_{0} = \\frac{z_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{\\theta}\\left(z_{t}, z, E n C t x t(d), t\\right)}{\\sqrt{\\bar{\\alpha}_{t}}}$$\n(3)\nHere, the concatenation of z = \\mathcal{E}(x) with \\hat{z}_{t} serves as inputs to the OMP module.\nThe gradient of \\hat{z}_{t} is detached to optimize the two models without affecting each\nother. We conduct bilinear interpolation downsampling on the mask m to obtain\nm', preserving its size identical to the input latent. The OMP module's network\n\\tau_{\\theta} is optimized according to the following objective function:\n$$L_{O M P} = E_{\\epsilon(z), \\mathcal{E}(x), d, m}[\\left|\\left|m' - \\tau_{\\theta}(\\hat{z}_{0}, z)\\right|\\right|_{2}]$$\n(4)"}, {"title": "Classifier-free Guidance", "content": "Classifier-free diffusion guidance [11] is a method\nthat involves the joint training of a conditional diffusion model and an uncon-\nditional diffusion model. By combining the output score estimates from both\nmodels, this approach achieves a balance between sample quality and diversity.\nTraining for the unconditional diffusion model is achieved by fixing the condi-\ntioning value to a null variable intermittently throughout the training process.\nWe follow the approach of Brook et al. [3] by stochastically and independently\ndefining our input conditions x and d as null variables with a probability of 5%."}, {"title": "3.3 Evaluation Metric", "content": "Due to the absence of robust quantitative metrics for shape-free object inpainting\nexcept the success rate, we propose a set of evaluation rules leveraging exits\nmetrics [8, 9, 19, 33, 36] to evaluate different methods in different aspects.\nWe first randomly select and manually inspect 1,000 evaluation data pairs\nfrom COCO [14] and OpenImages [13] independently to ensure the validity of\nthe object in the image and generalizability of the evaluation dataset. Each data\npair comprises an original image \\mathcal{X}_{ori}, a text prompt of an object d, and an\ninpainted image x. The resulting output image \\mathcal{X}_{output} and the corresponding\nobject mask m_{output} are outcomes derived from distinct methods."}, {"title": "Background Consistency", "content": "We adapt LPIPS [36], a widely adopted and robust\nmetric for assessing the similarity between images, to evaluate this aspect:\n$$S_{con}\\left(x, x_{output}, m_{output}\\right) = L P I P S\\left(x, x \\odot m_{output} + x_{output}(1 - m_{output})\\right) .$$(6)"}, {"title": "Location Reasonableness", "content": "Assessing the reasonableness of the object's loca-\ntion is a challenging task due to its inherent subjectivity. Surprisingly, we note\nGPT4V [19] demonstrates strong discriminative abilities in assessing variations\nand evaluating different locations by providing x, d, \\mathcal{X}_{output} and an instruction T\nas illustrated in Fig. 8. GPT4V rates the appropriateness of the object's position\non a scale from 1 to 5, while also providing justifications for these ratings:\n$$S_{rea}(x, x_{output}, d, T) = G P T 4 V\\left(x, x_{output}, d, T\\right)$$(7)"}, {"title": "Object Correlation", "content": "To quantify this relationship, we utilize CLIP Score [8],\na metric to assess the correlation between text and image, by calculating the\ncosine similarity of their embeddings from CLIP [22]. we measure CLIP Score\nbetween the object area of \\mathcal{X}_{output} and d, which is referred to as \"Local CLIP\nScore\":\n$$S_{cor}(d, x_{output}, m_{output}) = C L I P S c o r e(d, L o c a l(x_{output}, m_{output}) .$$(8)\nwhere L o c a l(x, m) denotes obtaining a cropped region from x using m. To miti-\ngate influences from background or mask shape, we compute an average of two\nLocal CLIP Scores (one with background removal and another without)."}, {"title": "Object Quality and Diversity", "content": "Following [33], we employ Local FID, mea-\nsuring Fr\u00e9chet Inception Distance (FID) [9] on the local regions, to evaluate the\nquality and diversity of generated object:\n$$S_{q d}\\left(L x_{o r g}, L x_{o u t p u t}\\right) =\\left|\\left|\\mu_{L x_{o r g}}-\\mu_{L x_{o u t p u t}}\\right|\\right|^{2}+\\operatorname{Tr}\\left(\\sum_{L x_{o r g}}+\\sum_{L x_{o u t p u t}}-2 *\\left(\\sum_{L x_{o r g}} * \\sum_{L x_{o u t p u t}}\\right)\\right)$$\n(9)\nwhere \\mathcal{L}X_{org} and \\mathcal{L}X_{output} respectively denote the sets comprising all local re-\ngions of the original images and output images, \\mu and \\sum represent the mean and\nvariance of the feature vectors obtained through a particular network [9]."}, {"title": "Unified Metric", "content": "Drawing upon the evaluation metrics delineated above (Eqs. (6)\nto (9)), we compute a unified score to holistically assess text-guided shape-free\nobject inpainting. We treat the derivative of inverse metric results (LPIPS and\nLocal FID) as positive metrics and normalized the outcomes across different\nmethods for each metric. Ultimately, we average these normalized scores and\nmultiply them by the success rate as a unified score. The Unified metric not\nonly considers success rate but also focuses on quantitative performances."}, {"title": "4 Experiment", "content": "We comprehensively evaluated our model, Diffree, by conducting experiments on\ntwo benchmark datasets: COCO [14], and OpenImages [13]. Given the distinct\ninput-output characteristics of our method compared to previous approaches,\na quantitative comparison proves challenging. We align previous methods by\nadding auxiliary conditions, as depicted in Sec. 4.1, and provide quantitative\ncomparison results (Sec. 4.2) to prove the effectiveness of Diffree more intuitively.\nWe then showcase visualizations of generated images and give corresponding\nanalyses to offer an intuitive assessment of Diffree's capabilities and comparisons\nin Sec. 4.3. finally, we demonstrate some applications to prove that Diffree is\nhighly compatible with existing methods (Sec. 4.4)."}, {"title": "4.1 Experimental Settings", "content": "Training Setups we employ OABench to train Diffree, initializing the diffusion\nmodel with the Stable Diffusion 1.5 [24] weights. We set \\lambda = 2 in Eq. (5) and set\na batch size of 256. Our model was trained around 10K steps on 8 A100 GPUs.\nEvaluation Datasets and Metrics As outlined in Sec. 3.3, we employ four\nmetrics (LPIPS [36], GPT4V [19], Local CLIP Score and Local FID [33]) along-\nside the unified metric for evaluation on COCO [14] and OpenImages [13]."}, {"title": "4.2 Main results", "content": "Tab. 1 shows the main results of Diffree with different baselines. We report the\nresults of four powerful metrics and Unified Metric. It is worth to highlight that\nonly successful InstructPix2pix results are computed and PowerPaint is utilized\nfor image inpainting under the masks provided by the results of our approach.\nWe can deduce the following conclusions from the results in several aspects.\nSuccess Rate We achieved a success rate of over 98% on different dataset, while\nInstructPix2pix shows a lower success rate in object addition (17.2% and 18.9%).\nAs shown in Fig. 9, most of the results of InstructPix2pix involve replacing\nexisting object, without adding or significant changes to the background. This\ndemonstrates our excellent ability to complete this task. Meanwhile it is not\napplicable to PowerPaint as it necessitates a mask input.\nConsistency of Background Diffree significantly outperforms InstructPix2pix\nin the LPIPS scores across all datasets (all decreased by 36% than Instruct-\nPix2pix). In particular, only scores from carefully chosen successful cases of\nInstructPix2pix were computed, potentially leading to an overestimation. Fur-\nthermore, Diffree, as a shape free inpainting method, yields LPIPS results com-\nparable to PowerPaint, as a shape required inpainting method. As discussed in"}, {"title": "Reasonableness of object location", "content": "The results of GPT4V's assessment\ndemonstrate that Diffree has a considerable advantage in the reasonableness of\nobject location (e.g., 0.34 higher than successful results from InstructPix2pix).\nThis is not avaliable for PowerPaint due to it requires a mask as input."}, {"title": "Correlation, Quality and Diversity of Generated Object", "content": "We conduct an\nevaluation of the generated object across these three dimensions, utilizing both\nLocal CLIP Score and Local FID. Although Diffree exhibits a slightly lower Local\nCLIP Score in comparison to InstructPix2pix (e.g., 28.96 versus 29.30 on the\nCOCO), this discrepancy can be rationalized by the fact that its successful results\nare inherently highly correlated while ours encompass all outcomes without any\nspecific selection. Intriguingly, we demonstrate superiority over PowerPaint in\nterms of correlation. Furthermore, our performance according to the Local FID\nmetric indicates a distinct advantage relative to all other methods."}, {"title": "Unified Metric of Diffree", "content": "We combine the success rate with diverse metrics\nacross various aspects to calculate a unified metric, thereby facilitating a more\ncomprehensive comparison with extant text-guided methods. It is discernible\nthat Diffree exhibits a substantial superiority over InstructPix2pix, for instance,\nours' 35.92 as opposed to InstructPix2pix's 4.48 on the COCO. PowerPaint\nachieves superior results (e.g., 37.20 on the COCO dataset), the image inpainting\nof powerpaint buit upon the masks from our Diffree. This further underscores\nthe excellent scalability of Diffree when integrated with other methods."}, {"title": "4.3 Visualization", "content": "We provide different types' visualizations to more intuitively evaluate Diffree's\ncapabilities Figs. 1 to 4, 7, 10 and 11, please refer to the respective image captions\nfor detailed explanations. For more results, please refer to the appendix."}, {"title": "4.4 Application", "content": "Diffree can be well combined with other methods for more expansion.\nWith GPT4V GPT4V [19] has a good ability to perceive and understand\nimages, therefore we can use GPT4V for planning a object suitable for the\nimage scene, seeing Fig. 11. However, when task with adding corresponding\nobject without altering the background, DALL-E-3 [2] in GPT4, falls short.\nWith Other Methods AnyDoor [4] can add a specific object to the designated\narea by providing a mask and object image. As depicted in Fig. 11, Diffree can\ncombined with AnyDoor to further achieve adding a specific object to image.\nDIffree also can effectively leverage the continuous progress in the image inpaint-\ning, to generate superior images, as demonstrated in Tab. 1.\nIterative Operation In Figs. 1 and 10, we present results of iterative inpaint-\ning. Leveraging the predicted mask from OMP module, Diffree can preserve\nthe image background from cumulative degradation during successive inpaint-\ning. This holds potential applications within architectural and interior design\ndomains."}, {"title": "5 Conclusion", "content": "We propose a novel method, Diffree, that leverages a diffusion model with an\nobject mask predictor for text-guided object addition. Beyond the method, we\nbuild a high-quality synthetic dataset, OABench, through a novel data collection\nmethod for this task. Diffree distinguishes itself by preserving background consis-\ntency without requiring additional masks, which solves shortcomings of previous"}]}