{"title": "TrustChain: A Blockchain Framework for Auditing\nand Verifying Aggregators in Decentralized\nFederated Learning", "authors": ["Ehsan Hallaji", "Roozbeh Razavi-Far", "Mehrdad Saif"], "abstract": "Abstract\u2014The server-less nature of Decentralized Federated\nLearning (DFL) requires allocating the aggregation role to\nspecific participants in each federated round. Current DFL\narchitectures ensure the trustworthiness of the aggregator node\nupon selection. However, most of these studies overlook the\npossibility that the aggregating node may turn rogue and act\nmaliciously after being nominated. To address this problem, this\npaper proposes a DFL structure, called TrustChain, that scores\nthe aggregators before selection based on their past behavior and\nadditionally audits them after the aggregation. To do this, the\nstatistical independence between the client updates and the aggre-\ngated model is continuously monitored using the Hilbert-Schmidt\nIndependence Criterion (HSIC). The proposed method relies on\nseveral principles, including blockchain, anomaly detection, and\nconcept drift analysis. The designed structure is evaluated on\nseveral federated datasets and attack scenarios with different\nnumbers of Byzantine nodes.\nIndex Terms\u2014Federated learning, blockchain, Byzantine at-\ntacks, data poisoning, model poisoning, security.", "sections": [{"title": "I. INTRODUCTION", "content": "HE advent of Federated Learning (FL) advanced the\nfield of distributed machine learning by introducing data\ndecentralization as a solution to bring about data privacy and\ncommunication efficiency [1]. Despite its advantages, FL was\nshown to be vulnerable against a spectrum of adversaries due\nto its distributed nature [2], [3]. Numerous research endeavors\nhave been dedicated to studying these threats and finding\nrobust defense mechanisms to mitigate them.\nA common perception among the majority of these studies\nis that the server is trustworthy, and malicious activities can\npotentially be initiated from the edge nodes. Thus, available\ndefense mechanisms are mostly concerned with client activi-\nties within the FL network. Another issue of concern in FL\nis the fact that the server could be a single point of failure\n[4], [5]. From a security standpoint, this can translate into the\nvulnerability of the FL network when the server is breached\nor when the server itself is not trustworthy.\nThese security concerns motivated researchers to come up\nwith decentralized architectures of FL that do not rely on a\ncentral server [6], which in turn improves the reliability and\nscalability of FL. In this structure, the client-side operations\nare similar to that of FL. However, the aggregation role is\nperformed by one or more sets of trusted nodes. Several\napproaches are available for selecting the aggregator nodes in\neach round. While some works follow approaches as simple\nas following a random or specific order in selecting the\naggregator nodes, others use more sophisticated technology\nsuch as Smart Contracts (SC) and proofs in blockchain to\nselect aggregator nodes more confidently [7]\u2013[9].\nIn the context of cyber security, several works are dedi-\ncated to securing DFL against privacy and performance at-\ntacks [10]. To mitigate privacy attacks, these structures are\noften combined with homomorphic encryption, differential\nprivacy, or secure multiparty communications to secure DFL\ncommunications and minimize the risk of inference attacks\n[11], [12]. Dealing with performance-related attacks such as\npoisoning attacks, on the other hand, requires in-depth analysis\nof generated updates using anomaly detection and robust\naggregation protocols [4], [13]. Techniques used in both of\nthese domains are mainly inspired by previous studies in FL,\nas privacy-preserving and client-update evaluation are the two\nmain security concerns in FL.\nVerifying the aggregator nodes, on the other hand, is less\nstudied in the literature. Most works in this domain assume\nthat once a trustworthy node is selected to carry out the\naggregation, it will not become rogue. This contradicts the fact\nthat Byzantine nodes initially act honestly in the network and\nbecome dishonest once they find the opportunity to execute a\nmalicious action. In fact, to the best of our knowledge, only\na limited number of research works target this problem [14].\nAvailable solutions mostly use a trusted committee for verify-\ning the aggregated model [7]. Nevertheless, the trustworthiness\nof verifying nodes cannot be guaranteed.\nTo address this gap, we propose a novel approach, called\nTrustChain, that leverages blockchain technology to ensure\nthe continuous trustworthiness of the aggregator node through-\nout the federated learning process. This method consists of two\nmain components:\n\u2022 Pre-Selection Evaluation (PSE): When miners request\ndownloading the blocks for aggregation, an SC is trig-\ngered to calculate a score for participating miners based\non the cosine similarity of the parameters they uploaded\nto the blockchain and the resulting aggregated model in\neach round, for a window of past DFL iterations. This\nscore indicates the tendency of each node to drift from"}, {"title": "II. BACKGROUND", "content": "The literature review on the security aspects of DFL is\ndivided into studies that analyze security threats in DFL and\nthose designing defense mechanisms to safeguard DFL against\nadversaries. This section initially explains the concept of DFL\nand then overviews threats and defense mechanisms in this\ndomain.\nA. Decentralized Federated Learning\nDFL primarily addresses the issue of the single point of\nfailure in FL, by removing the dependency on a reliable\nserver to carry out the aggregation and enhance communi-\ncation efficiency [15], [16]. On the client side, the process\nof training the local models and preparing the updates for\nthe global aggregator is similar to that of FL. However, due\nto the lack of a central server in DFL, exchanging model\nparameters and model aggregation are performed using peer-\nto-peer communications or blockchain technology [10]. Inte-\ngration with blockchain provides additional benefits such as\ntraceability and immutability. Despite DFL's security and effi-\nciency characteristics, the design is not faultless. For example,\nadding blockchain into FL may leave the system vulnerable\nto blockchain-related security threats. More importantly, the\ntrustworthiness of participants is more critical in DFL, as the\naggregating nodes are selected among them in each round.\nB. Threats\nSimilar to federated learning, threats to DFL generally fall\nunder privacy and performance-related attacks [10]. Privacy\nattacks primarily aim at stealing sensitive data during inference\nbased on the communicated gradients. An example of privacy\nattacks in DFL is presented by [13]. On the other hand, poi-\nsoning attacks target the global model by injecting poisonous\nupdates during the training. The attacker can use data and\nmodel poisoning to deteriorate the overall performance of\nthe global model. For instance, [6] performs data poisoning\nto disrupt the global performance. This study assumes 30\npercent of the nodes are malicious, and considers that this ratio\nremains unchanged in the experiments. The same objective\ncan be achieved using Gradient Manipulation (GM), in which\nByzantine data holders generate forged gradients sampled from\na Gaussian distribution [17]. The same malicious objective is\nsatisfied in [18] by filliping the class labels randomly on the\ntraining data. Moreover, DFL can be exposed to more subtle\nattacks such as backdoors as presented in [13], [19].\nC. Defenses\nThe majority of studies on DFL security integrate\nblockchain with this structure to facilitate auditing the updates\nand participants. Nevertheless, some studies rely solely on\npeer-to-peer communications and do not employ blockchain\n[20], [21]. These works are primarily focused on eliminating\nthe single point of failure in conventional FL. For instance,\n[21] resorts to version control to perform server selection using\nan arbitrary pattern.\nBiscotti was among the first efforts to secure DFL against\nprivacy attacks through blockchain integration [6]. The method\nintegrates Differential Privacy (DP) with secure aggregation as\na defense against poisoning and inference attacks. Another\nstudy proposes LearningChain, a DFL structure that also\nuses DP and an aggregation mechanism that works based on\ndecentralized Stochastic Gradient Descent (SGD) to eliminate\nByzantine attacks [17]. One of the first efforts to demonstrate\nthe use of smart contracts in DFL was made by [8]. This\nmethod uses smart contracts to keep track of client states\nand store the global model. Proof of correctness can also be\nused by registered users to audit the generated updates [22].\n[9] uses a blockchain-based committee consensus mechanism\nto enhance the privacy and efficiency of DFL. The sharding\napproach commonly used in blockchain is used by [23] to\nsecure the aggregation process. [7] proposed VFChain, a struc-\nture that audits trainers and generates updates using a trusted\ncommittee. The committee members are selected arbitrarily,\nand they use a verifying contract to verify aggregated models.\nAnother approach proposed by [13] integrates several methods\nsuch as DP, anomaly detection, and gradient pruning to safe-\nguard DFL against poisoning attacks. [24] tackles the issue of\nmalicious aggregators in DFL by using zero-knowledge proofs\nand blockchain to enable secure and verifiable aggregation."}, {"title": "III. PROBLEM FORMULATION", "content": "Given a set of n client nodes in the DFL network $C =$\n{C1, C2,..., Cn}, each node ci trains a local model Mi, and\nuploads its parameters $\u03b8_i$ to the blockchain B. The blockchain\nis formulated as $B = \\{b_1,b_2,...,b_t\\}$, where be denotes the\nlast block at round t of DFL, and the others indicate the\nprevious blocks in sequence. In this formulation, t is dynamic\nas the size of B changes in time."}, {"title": "IV. THREAT MODELS", "content": "While the spectrum of poisoning attacks is vast, this paper\nfocuses on model poisoning attacks. We consider four poi-\nsoning attacks, namely label flipping, sign flipping, GM, and\nTraining Objective Manipulation (TOM). When these attacks\nare initiated from the client side, a robust aggregator attempts\nto mitigate their effects. If a malicious node is selected as\nan aggregator, it can bypass the robust aggregator by directly\nfeeding the poisoned parameters to the DFL. These attacks are\nformally explained as follows.\na) Label flipping: The labels of the training data are\nflipped to obtain corrupted parameters [25]. Let $D_i = (x_i, y_i)$\nbe the dataset of node ci, where yi is the true label. The\nattacker generates a poisoned dataset $\\tilde{D_i}$ by randomly flipping\nlabels:\n$\\tilde{D_i} = \\{(x_i, \\tilde{y_i}) | P(\\tilde{y_i} \\neq y_i) = \u03b3\\}_{i=1}^m$,\nwhere \u03b3 is the proportion of flipped labels, and m is the\nnumber of training samples. Poisoned parameters $\u03b8_i$ used for\nattacking the DFL system are derived from this corrupted\ndataset:\n$\\tilde{\u03b8_i}\u2190 M_i(\\tilde{D_i})$.\nb) Sign flipping: The attacker reverses the sign of the\ngradients during parameter estimation [26]. For a gradient\nfrom node ci, the poisoned gradient is the negative of the\nestimated gradients. The corresponding poisoned parameters\nare:\n$\\tilde{\u03b8_i} = \u03b8_i \u2013 \u03b7 \u00b7 sign (\\nabla L(\u03b8_i))$,\nwhere \u03b7 is the learning rate, \u03b8 is the set of network parameters,\nand L denotes the loss function.\nc) Gradient manipulation: During the aggregation phase,\nthe attacker introduces noise into the aggregated parameters\n[27]. Let {$\u03b8_i\\}_{i=1}^r$ represent the set of parameters from the last\nr blocks. The attacker modifies the aggregated update as:\n$\\tilde{\u03b8^t} = \\sum_{i=1}^r \u03b8_i + \u0394\u03b8$,\nwhere $\u0394\u03b8$ is the noise injected by the attacker.\nd) Training objective manipulation: The attacker mod-\nifies the training objective by adding a penalty term that\ndeteriorates the classification performance [2]. If L(0) is the\noriginal loss function, the attacker changes it to:\n$\\tilde{L}(\u03b8^t) = L(\u03b8^t) + \\$\\cdot P(\u03b8^t)$,\nwhere $P(\u03b8^t)$ is a penalty term added by the attacker, and $\u03b6$\nscales the severity of the attack. We define the penalty term\nas:\n$P(\u03b8^t) = ||\\nabla L(\u03b8^t) \u2013 \\nabla L_{target}||^2$,\nwhere $L_{target}$ is the direction towards which the gradients are\nguided. Here, we choose $L_{target} = -\\nabla L(\u03b8^t)$, which results\nin $P(\u03b8^t) = ||2 \u00b7 \\nabla L(\u03b8^t)||^2$."}, {"title": "V. BLOCKCHAIN-ENABLED TRUSTWORTHY\nAGGREGATION", "content": "Given that the goal of this study is to ensure the trustworthi-\nness of the aggregator, the proposed TrustChain assumes\nthe employed robust aggregation algorithm is reliable. Con-\nsequently, this research focuses on securing the aggregation\nphase by performing PSE and PAA in DFL. The process\nbegins with the PSE algorithm evaluating the past behavior of\nparticipants using concept drift analysis to determine a set of\ntrustworthy candidates. Then, the aggregated model is further\nevaluated by PAA based on the statistical independence of $\u03b8^t$\nwith that of the previous block and $\u03b8_i$ received in the current\nround.\nA. Pre-Selection Evaluation\nIn order to determine the trustworthiness of nodes for\nthe aggregation phase, they are scored based on their past\nbehavior. Specifically, we monitor the tendency of ci to deviate\nfrom the \u03b8 path in terms of direction and magnitude. As\ndetailed in Algorithm 1, this goal is achieved by measuring"}, {"title": "B. Post-Aggregation Auditing", "content": "Algorithm 2 details the evaluation process for validating the\naggregated model. Once ca pushes $\u03b8^{t+1}$ to the blockchain, a\nSC is triggered to check the statistical independence of the\naggregated model with parameters used in this round {$\u03b8_i\\}_{i=1}^r$\nand the current state of the global model $\u03b8^t$. In this process,\nstatistical independence is measured using HSIC. Let A and B\nbe two random variables. Then, HSIC between two variables\ncan be formally defined as:\nHSIC(A, B) =\n$E_{AA'}E_{BB'} [f_A(A, A') f_B(B, B')]$\n$+ E_{AA'} [f_A(A, A')]E_{BB'}[f_B(B, B')]$\n$- 2E_{AB} [E_{A'} [f_A(A, A')]E_{B'}[f_B(B, B')]]$,\nwhere (\u00b7)' denotes independent copies of variables, and f(.)\nindicates kernel functions. Efficient calculation of HSIC is\noften undertaken using an empirical estimate [28]:\nHSIC(A, B) =\n$\\frac{1}{(|A|-1)^2}$ tr($F_A H F_B H$),\nwhere |\u00b7| returns the cardinality. In addition, $H = I - \\frac{1}{|A|}$ 11T is\nthe centring matrix, and $F_A = f_A(a_i, a_j)$ and $F_B = f_B(b_i,b_j)$\nare kernel matrices obtained from $a_i \u2208 A$ and $b_i \u2208 B$."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "This section first explains the experimental setup used\nin our experiments and then evaluates the proposed DFL\nunder several scenarios. The experiments are conducted in\nthree phases. First, we evaluate the overall accuracy when\nTrustChain is employed. Then, the sensitivity to attacks is\ndiscussed. Finally, we study PSE and PAA by isolating each\ncomponent while using TrustChain.\nA. Experimental Setting\na) Datasets: Experiments are evaluated using commonly\nused benchmark datasets in this domain, such as MNIST\n[29], Fashion-MNIST [30], and CIFAR10 [31] datasets. To\ncreate federated datasets, datasets are shuffled at first and\nthen sub-datasets are created by random sampling without\nreplacement. The resulting data is highly imbalanced, and the\ncreated subsets have an equal number of samples.\nb) Local models: The architecture of local models differs\nfor each dataset. For experiments on MNIST and Fashion-MNIST, a Multi-Layer Perceptron (MLP) with two hidden\nlayers is used. For CIFAR10, on the other hand, we use a\nConvolutional Neural Network (CNN) with six convolutional\nlayers followed by two dense layers. Convolution layers are\npaired with batch normalization, and after every two convolu-\ntional layers, max pooling and dropout (ratio=25%) are used.\nIn both networks, hidden layers use ReLU, and the final layer\nis activated using Softmax. Model parameters are optimized\nusing stochastic gradient descent. The batch size is set to 64\nfor MLP and 32 for CNN.\nc) DFL parameters: The DFL undergoes 1000 training\niterations (1 \u2264 t \u2264 1000). The number of participants in C\nis set to r = 100. We assume C* CC and set k = 20. The\nwindow length is set to q = 15. The scaling factor in (13)\nis empirically selected and set to $\u03bb = 0.5$. To facilitate the\nestimation of \u03c4t, we assume the first 50 DFL iterations are\nsafe, and attacks begin at t = 51.\nd) Attacks scenarios: Using label flipping, the local data\non the attacker's side is poisoned with \u03b3 = 1. Local training\non the poisoned data results in malicious parameters which\nare sent to the DFL. Sign flipping is performed by training\non clean local data and reversing the sign of gradients. GM is\nimplemented by adding Gaussian noise to \u03b8i with a mean and"}, {"title": "VII. CONCLUSION", "content": "This paper addressed the issue of malicious aggregators in\nDFL systems by proposing a blockchain-enabled solution for\ntrustworthy aggregation, called TrustChain. The proposed\nmethod eliminates malicious aggregators in two steps. Firstly,\nthe miners participating in the aggregation process are scored\nbased on their tendency to drift from the distribution of\naggregated parameters in previous DFL iterations. This step\nexcludes miners who exhibit suspicious behavior. Secondly,\nthe aggregated updates are checked before being pushed to\nthe DFL by measuring statistical independence using HSIC"}]}