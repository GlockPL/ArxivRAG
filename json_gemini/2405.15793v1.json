{"title": "SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING", "authors": ["John Yang", "Carlos E. Jimenez", "Alexander Wettig", "Kilian Lieret", "Shunyu Yao", "Karthik Narasimhan", "Ofir Press"], "abstract": "Software engineering is a challenging task requiring proficiency in both code generation and interacting with computers. In this paper, we introduce SWE-agent, an autonomous system that uses a language model to interact with a computer to solve software engineering tasks. We show that a custom-built agent-computer interface (ACI) greatly enhances the ability of an agent to create and edit code files, navigate entire repositories and execute programs. On SWE-bench, SWE-agent is able to solve 12.5% of issues, compared to the previous best of 3.8% achieved with retrieval-augmented generation (RAG). We explore how ACI design impacts an agent's behavior and performance, and provide insights on effective design.", "sections": [{"title": "INTRODUCTION", "content": "Language models (LMs) have become helpful assistants for software development (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), with users playing the role of mediator between the LM and the computer to complete programming tasks. For instance, after executing LM-generated code, a user may request refinements from the LM based on computer feedback such as error messages. More recently, LMs have been used as autonomous agents that interact with computer environments without human intervention (Yang et al., 2023a; Wu et al., 2024; Xie et al., 2024). This approach has the potential to accelerate software development, but remains largely unexplored in realistic settings.\nThis work introduces SWE-agent, an LM-based autonomous system that can interact with a computer to solve challenging real-world software engineering problems from SWE-bench (Jimenez et al., 2024). At every turn, SWE-agent outputs a thought and a command, and then receives feedback from the execution of the command in the environment (ReAct; Yao et al. (2023b)). As our main contribution, we establish the importance of designing an agent-computer interface (ACI) to enhance the LM agent's performance. We find that custom ACIs tailored to LMs can outperform existing user interfaces (UIs), such as the Linux shell, designed for human users.\nConsider the baseline of using a Linux shell interface (Yang et al., 2023a), which naturally lends itself to turn-based interaction with an LM agent due to its text-only input commands and outputs."}, {"title": "THE AGENT-COMPUTER INTERFACE", "content": "An LM acts as an agent by interacting with an environment through iteratively taking actions and receiving feedback (Yao et al., 2023b; Sumers et al., 2023). Typically the environment is fixed, such as in robotics, where agents control actuators in the physical world. However, digital environments offer the flexibility of developing an interface between the agent and the computer, reflected by the wide variety of interfaces for programs (application programming interfaces, APIs) and humans (user interfaces, UIs). Here, we argue that LM agents represent a new category of end users and we call the interface they use to interact with computers the agent-computer interface (ACI).\nAgents interact with a computer in a series of turns: iterating between the agent issuing commands and the computer responding with the command's output, visualized in Figure 2. The ACI specifies the commands available to the LM and how the environment state after the execution of each command will be communicated back to the LM. It also tracks the history of all previous commands and observations, and at each step, manages how these should be formatted and combined with high-level instructions into a single input for the language model."}, {"title": "SWE-AGENT: DESIGNING AN ACI FOR SOFTWARE ENGINEERING", "content": "SWE-agent uses an ACI that enables an LM to act as a software engineering agent. To evaluate our system, we use SWE-bench, a benchmark for evaluating LMs on real world software issues collected from GitHub (Jimenez et al., 2024). SWE-bench's issue resolution task reflects an important responsibility of software engineers: Given a codebase and a natural language request (e.g. feature request, bug report), generate a codebase revision (i.e. a patch) that passes unit tests which verify that the issue has been remedied. Solving these tasks requires performing a number of software engineering sub-tasks, including bug localization, program repair, and writing tests.\nExisting application interfaces to perform software engineering have naturally been designed around the human user. For instance, developers frequently use GUI-based IDEs, such as VSCode, or screen-based text editors, such as Vim. These interfaces have rich visual components and feedback that make them powerful tools for humans, but may not be as suited to LMs.\nConsider a text editor like Vim which relies on cursor-based line navigation and editing. Carrying out any operation leads to long chains of granular and inefficient interactions. Furthermore, humans can ignore unexpected inputs, such as accidentally outputting a binary file or thousands of lines returned from a 'grep' search. LMs are sensitive to these types of inputs, which can be distracting and take up a lot of the limited context available to the model. On the other end, commands that succeed silently confuses LMs. We observe that LMs will often expend extra actions to verify that a file was removed or an edit was applied if no automatic confirmation is given. We show that LMs are substantially more effective when using interfaces built with their needs and limitations in mind.\nSWE-agent overcomes these challenges by introducing powerful, tailored tools to provide agents with a simplified interface to the computer; i.e. the agent-computer interface (ACI). A well-designed ACI should help the agent understand the state of the repository given the previous changes. It should also help the agent recover from mistakes, remove unnecessary context from prior observations, and suppress any unusual, lengthy program outputs."}, {"title": "EXPERIMENTAL SETUP", "content": "Datasets. We evaluate on the SWE-bench dataset, which includes 2,294 task instances from 12 different repositories of popular Python packages (Jimenez et al., 2024). We report our main agent results on the full SWE-bench test set and ablations and analysis on the SWE-bench Lite test set, unless otherwise specified. SWE-bench Lite is a canonical subset of 300 instances from SWE-bench with a focus on evaluating self-contained functional bug fixes.\nModels. All results, ablations, and analysis are based on two leading LMs, GPT-4 Turbo (gpt-4-1106-preview) (OpenAI et al., 2023) and Claude 3 Opus (claude-3-opus-20240229) (Chiang et al., 2024). We experimented with a number of additional closed and open source models, including Llama 3 and DeepSeek Coder, but found their performance in the agent setting to be subpar. Many LMs' context window is too small, such as Llama 3 with a context window of 8k. GPT-4 Turbo and Claude 3 Opus have 128k and 200k token context windows respectively, which provides enough room for the LM to interact for several turns after being fed the initial set of system, issue description, and optionally, demonstrations.\nBaselines. We compare SWE-agent against two baselines. The first setting is the non-interactive, retrieval-augmented generation (RAG) baselines established in Jimenez et al. (2024). Here, a BM25 retrieval system is used to retrieve the most relevant codebase files using the issue as the query. Provided these files, the model is asked to directly generate a patch file that resolves the issue.\nThe second setting, called Shell only, is adapted from the interactive coding framework introduced in Yang et al. (2023a). Following the InterCode environment, this interactive baseline system asks the LM to resolve the issue by interacting with a shell process on Linux. Like SWE-agent, the model prediction is generated automatically based on the final state of the codebase after interaction.\nMetrics. We report % Resolved as the main metric, which is the proportion of instances for which all tests pass successfully after the model generated patch is applied to the repository (Jimenez et al., 2024). We also report the $ Avg. Cost metric, the average API inference cost incurred from running SWE-agent on a task instance, for successful instances. Due to budget constraints we set the per-instance budget to $4. If a run exceeds this budget, the existing edits are submitted automatically.\nConfiguration Choice. During the design process of SWE-agent, we arrived at the final ACI design through qualitative analysis of system behavior on small set of hand-picked easier examples from the development split of SWE-bench. For the remaining hyperparameter choices, we perform a hyperparameter sweep over the window size, history processing, and decoding temperature. Further description of the configuration choice is provided in Appendix A."}, {"title": "RESULTS", "content": "Main Results. Across all systems, SWE-agent with GPT-4 Turbo yields the best performance, solving 12.47% (286/2,294) of the full SWE-bench test set and 18.00% (54/300) of the Lite split.\nWe present several empirical ablations that quantify Agent-Computer Interface design's effect on task performance. We provide insights on language agents' behavior and common failure modes."}, {"title": "ANALYSIS OF INTERFACE DESIGN", "content": "Following Section 3, we perform several ablations of the SWE-agent interface, specifically with respect to the SWE-agent w/ GPT-4 set up. The performance of each ablation relative to SWE-agent"}, {"title": "DISCUSSION", "content": "We introduce SWE-agent, a language agent for software engineering that achieves state-of-the-art performance on SWE-bench. Through our design methodology, empirical results, and analysis, we demonstrate the process and value of designing agent-computer interfaces (ACI) tailored for agents. We release our code, prompts and generations, and set up the codebase to allow for easy extension to new commands, and feedback and agent history formats. We hope that SWE-agent will serve as a foundation that inspires future work towards more versatile and powerful agents.\nBeyond various empirical applications, we hope the further study of ACIs could also make principled use and contribute to our understanding of language models and agents, analogous to the synergy between human-computer interaction (HCI) and psychology (Carroll, 1997). Humans and LMs have different characteristics, training objectives, specialities, and limitations (Griffiths, 2020; McCoy et al., 2023), and the interface design processes can be seen as systematic behavioral experimentation that could reveal more insights into these differences, and establish a comparative understanding of human and artificial intelligence."}, {"title": "APPENDIX", "content": "In the appendix, we provide additional analyses and more extensive discussions about SWE-agent, Agent Computer Interface (ACI) Design, and model performance on the full and Lite splits of SWE-bench. We also provide several thorough, manually curated case studies of SWE-agent performance on select task instances."}, {"title": "SWE-AGENT INTERFACE", "content": "In this section, we go into greater discussion about the design methodology, appearance, and implementation of each of the SWE-agent components. As described in Section 3, the SWE-agent interface is consists of several components that enable agents to accomplish key sub-tasks that are fundamental to solving software engineering problems. Namely: localizing which file(s) require editing, generating edits to fix the described issue, and writing scripts to verify the correctness of fixes. To enable LM-based agents to efficiently carry out these individual functions and progress towards the overarching goal of resolving a codebase issue, we provide a file viewer, file editor, search / navigation system, and context management system. In Section A.1, we provide a thorough breakdown of each of these components. In Section A.2, we discuss how SWE-agent is configured to support the final interface, along with how SWE-agent is built to enable easy extensibility and customization to alter the interface. Finally, in Section A.3, we discuss the technical design decisions and challenges of building out SWE-agent, along with an overview of the advantages and shortcomings of using SWE-agent as a platform for future explorations of agent-driven software engineering systems."}, {"title": "COMPONENT DESIGN", "content": "In this section, we revisit each component discussed in Section 3. First, we describe the motivation for the component, then provide complete descriptions of their input requirement(s), output format, usage, and notes about what parts of the interface heavily influence language model behavior.\nFile Viewer. As discussed in Section 3, the File Viewer is fundamental to a language agent's ability to understand file content and invoke appropriate edits. In a Terminal-only setting, there are several commands that can be used to inspect file content. However, out of the box command line tools are sub-optimal or limiting for language agents for several reasons. First, commands that print files to standard output (e.g. cat, printf) can easily flood a language agent's context window with too much file content, the majority of which is usually irrelevant to the issue. Enabling a language agent to filter out distractions and focus on relevant code snippets is crucial to generating effective edits. While commands like head and tail reduce length to the first/last n lines, it is not intuitive to use bash commands to perform in-file navigation. It is either impossible or requires a long list of arguments to show specific file lines. Furthermore, since such Bash commands are stateless, \u201cscrolling\u201d up/down relative to the current file position typically requires regenerating the same lengthy command with minor changes. Interactive tools like more and less accommodate this, but (1) representing navigation actions (multiple key up/down clicks) is intuitive for humans, but is verbose and costly for language agents, and (2) even if jumping to a"}, {"title": "CONFIGURATION", "content": "The SWE-agent system is instantiated by three functional components: a language model, a configuration, and a shell process running in a Docker container.\nAn agent-computer interface is made up of four categories of configurable components:\n1. Prompt templates - templates prompts are used to inform the language model of the environment and API, augment environment responses with the values of state variables, and provide the initial task setting.\n2. Command files - these files contain the source code of bash or Python functions and scripts. Commands are easily modified, added, and removed through manipulating these files' code contents directly. Documentation added in these files can also be injected into prompts to inform the model of the available commands.\n3. Control flow - methods for parsing model responses and processing history can be specified through these configuration arguments.\n4. Environment variables - initial values of variables that may interact with commands and the shell can also be specified in the configuration."}, {"title": "IMPLEMENTATION", "content": ""}, {"title": "PROMPTS", "content": "We provide a visualization of the variety of prompts used as part of SWE-agent."}]}