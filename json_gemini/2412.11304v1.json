{"title": "AN EMPIRICAL STUDY OF FAULT LOCALISATION TECHNIQUES\nFOR DEEP LEARNING", "authors": ["Nargiz Humbatova", "Jinhan Kim", "Gunel Jahangirova", "Paolo Tonella", "Shin Yoo"], "abstract": "With the increased popularity of Deep Neural Networks (DNNs), increases also the need for tools to\nassist developers in the DNN implementation, testing and debugging process. Several approaches\nhave been proposed that automatically analyse and localise potential faults in DNNs under test. In\nthis work, we evaluate and compare existing state-of-the-art fault localisation techniques, which\noperate based on both dynamic and static analysis of the DNN. The evaluation is performed on a\nbenchmark consisting of both real faults obtained from bug reporting platforms and faulty models\nproduced by a mutation tool. Our findings indicate that the usage of a single, specific ground truth\n(e.g., the human defined one) for the evaluation of DNN fault localisation tools results in pretty low\nperformance (maximum average recall of 0.31 and precision of 0.23). However, such figures increase\nwhen considering alternative, equivalent patches that exist for a given faulty DNN. Results indicate\nthat DEEPFD is the most effective tool, achieving an average recall of 0.61 and precision of 0.41 on\nour benchmark.", "sections": [{"title": "1 Introduction", "content": "Fault localisation (FL) for DNNs is a rapidly evolving area of DL testing [1\u20135]. The decision logic of traditional\nsoftware systems is encoded in their source code. Correspondingly, fault localisation for such systems consists of\nidentifying the parts of code that are most likely responsible for the encountered misbehaviour. Unlike traditional\nsoftware systems, however, the decision logic of DL systems depends on many components such as the model structure,\nselected hyper-parameters, training dataset, and the framework used to perform the training process. Moreover, DL\nsystems are stochastic in nature, as a retraining with the exactly same parameters might lead to a slightly different final\nmodel and performance. These distinctive characteristics make the mapping of a misbehaviour (e.g., poor classification\naccuracy) to a specific fault type a highly challenging task.\nExisting state-of-the-art works [1,2,4,6,7] that focus on the problem of fault localisation for DL systems were shown\nto be adequate for this task when evaluated on different sets of real-world problems extracted from StackOverflow and\nGitHub platforms or were deemed useful by developers in the process of fault localisation and fixing [5]. However,\nthese approaches rely on patterns of inefficient model structure design, as well as a set of predefined rules about the"}, {"title": "2 Background", "content": "Most of the proposed approaches for fault localisation for DL systems focus on analysing the run-time behaviour during\nmodel training. According to the collected information and some predefined rules, these approaches decide whether\nthey can spot any abnormalities and report them [1,2,5].\nDeepLocalize and DeepDiagnosis. During the training of a DNN, DEEPLOCALIZE (DL) [1] collects various\nperformance indicators such as loss values, performance metrics, weights, gradients, and neuron activation values.\nThe main idea behind this approach is that the historic trends in the performance evaluation or the values propagated\nbetween layers can serve as an indicator of a fault's presence. To allow the collection of the necessary data, a developer\nshould insert a custom callback provided by the tool into the source code regulating the training process. A callback is a\nmechanism that is invoked at different stages of model training (e.g., at the start or end of an epoch, before or after a\nsingle batch [8]) to perform a set of desired actions \u2013 store historical data reflecting the dynamics of the training process\nin our case. The tool then compares the analysed values with a list of pre-defined failure symptoms and root causes,\nwhich the authors have collected from the existing literature. Based on the performed analysis, DEEPLOCALIZE either\nclaims that the model is correct or outputs an error message listing the detected misbehaviours. The final output of\nDEEPLOCALIZE contains the (1) fault type, (2) the layer and the phase (feed forward or backward propagation) in\nwhich the DL program has a problem, and (3) the iteration in which learning is stopped. The faults that the tool is\nable to detect include the following: \"Error Before/After Activation\", \"Error in Loss Function\", \"Error Backward in\nWeight/A Weight\", and \"Model Does Not Learn\" that suggests an incorrectly selected learning rate.\nDEEPDIAGNOSIS (DD) [2] was built on the basis of DEEPLOCALIZE and improved the latter by enlarging the list of\ndetected symptoms and connecting them to a set of actionable suggestions. It detects ten types of faults: \"Numerical\nErrors\", \"Exploding Tensor\", \"Unchanged Weight\", \"Saturated Activation\", \"Dead Node\", \"Activation Function's"}, {"title": "3 Neutrality Analysis", "content": "In the context of fault localisation in DNNs, our goal is to identify and localise faults in the network architecture\nand hyperparameters. Even though the benchmark of faulty models includes repaired and correct versions of DL\nmodels, there are no strict rules and guidelines as to which combinations and architectures would perform better for\nspecific tasks. Therefore, we posit that there exist potential for finding alternative patches that not only supplement the\nknown patch by suggesting different ways of repairing but also possibly exhibit better performance than the known one.\nIdentifying such alternative patches would enable a more precise evaluation of FL techniques.\nIn our search for alternative patches, we are inspired by the notion of software neutrality, which states that a random\nmutation to an executable program is considered neutral if the behaviour of the program on the test set does not\nchange [15]. This neutrality analysis aims to investigate diverse patches with similar or better fitness: these can be\nutilised as alternative Ground Truths (GTs). Since our targets are DL programs, the conditions for performing neutrality\nanalysis differ from those of traditional programs. For example, the fitness is now measured by the model performance\nwith standard metrics such as test set accuracy. This means that fitness evaluation involves training and testing of\nthe model. Moreover, during fitness evaluation it is important to account for the inherent stochastic properties of DL\nprograms because the model's performance can vary with multiple trainings. To address this, in our algorithm below,\nwe train the model ten times and calculate the fitness as an average of the resulting ten accuracy values.\nAlgorithm 1 presents the Breath-First Search (BFS) for our neutrality analysis on DL programs. This algorithm takes as\ninputs an initial (buggy) model s, the accuracy of the known GT accgt, and stopping criteria SC. The outputs are a list\nof alternative GTs and edges of the neutrality graph. The algorithm starts with training and evaluating the initial buggy\nmodel before putting it in the queue (Lines 2-3). Next, it begins a search loop where it iteratively retrieves a model\n(i.e., a parent model c) along with its accuracy acce from the queue (Line 5). Subsequently, the algorithm explores all\nadjacent models (i.e., neighbours) that are obtained by applying a distinct single mutation on c (Line 7). Each mutation\ninvolves changing a single hyperparameter of the model, in other words, neighbouring models differ from their parent"}, {"title": "4 Empirical study", "content": "The aim of this empirical study is to compare existing DL fault localisation approaches and to explore their generalis\nability to different subjects represented by our benchmark of artificial and real faults. To cover these objectives, we\ndefine the following four research questions:\n\u2022 RQ1. Effectiveness: Can existing FL approaches identify and locate defects correctly in faulty DL models?\n\u2022 RQ2. Stability: Is the outcome of fault identification analysis stable across several runs?\n\u2022 RQ3. Efficiency: How costly are FL tools when compared to each other?\nRQ1 is the key research question for this empirical study, as it compares the effectiveness of different FL tools on our\ncurated benchmark of artificial and real faults. We conduct our effectiveness analysis twice, first with the available\nground truth and then with the ground truth extended by neutrality analysis.\nRQ2 and RQ3 investigate two important properties of the FL tools being compared: their stability across multiple\nexecutions and their execution cost."}, {"title": "4.2 Benchmark", "content": "To evaluate and compare the fault localisation techniques selected for this study, we adopt a carefully selected benchmark\nof faulty models from the existing literature [16]. This benchmark is of a particular interest as it combines both models\naffected by real-world faults and those deliberately produced using artificial faults.\nArtificial faults of this benchmark were produced by DEEPCRIME [13], a state-of-the-art mutation testing tool for\nDL systems based on real faults [14]. The models subject to fault injection by DEEPCRIME cover a diverse range of\napplication areas, such as handwritten digit classification (MN), speaker recognition (SR), self-driving car designed\nfor the Udacity simulator (UD), eye gaze prediction (UE), image recognition (CF10), and news categorisation (RT).\nThis section of the benchmark contains 21 faulty models generated by injecting 9 distinct fault types into originally\nwell-performing models. As DEEPDIAGNOSIS was not applicable to SR, UD, and UE, we had to limit our evaluation to\nthe remaining subjects.\nThe real faults section of the benchmark was derived from the set of issues that were collected and used for the\nevaluation of the DEEPFD tool [3, 17]. This initial set contains 58 faulty DNNs collected from bug-reporting platforms\nsuch as StackOverflow and Github.\nLater, Kim et al. [16] curated this benchmark by performing a series of checks and filtered out those issues that were\nnot in fact correctly reproduced from the corresponding fault report. Specifically, first of all they checked if there was a\nmatch between the model, dataset, and fix in the code and the post from SO and GitHub. As second step, they have\ndiscarded the issues where the fault was not exposed in the buggy version of the code when the program was run or was\nnot actually repaired in the fixed program. As a result, only 9 out of 58 issues proved to be reliably reproducible. We\nadopt this benchmark of 9 real faults in our empirical comparison of FL tools.\nThe final benchmark that was used in this study can be found in Table 3. Column 'Fault Type' shows whether the\nfault was real ('R') or artificially seeded ('A'); column 'Id' bears the ID of the fault which will be reused throughout\nthe paper; it refers to real faults curated from the DEEPFD dataset (prefix D), MNIST mutants (M), CIFAR mutants\n(C), Reuters mutants(R); column \u2018SO Post #/Subject' provides the StackOverflow post number from which the fault\nwas obtained in the case of real faults and the subject name in the case of artificial faults; column 'Task' has 'C' for\nfaults that solve a classification problem and 'R' for those dealing with a regression task; column 'Faults' contains the\navailable Ground Truth (GT), i.e., the list of faults known to affect each model (before neutrality analysis)."}, {"title": "4.3 Experimental Settings & Evaluation Metrics", "content": "For the comparison, we adopt publicly available versions of all considered tools [17\u201320] that are run on Python with\nlibrary versions specified in the requirements for each tool. However, we had to limit the artificial faults to those\nobtained using CF10, MN, and RT as DEEPDIAGNOSIS is not applicable to other subjects.\nThe authors of DEEPFD adopted the notion of statistical mutation killing [21] in their tool. They run each of the models\nused to train the classifier as well as the model under test 20 times to collect the run-time features. For fault localisation\nusing DEEPFD, we adopt an ensemble of already trained classifiers provided in the tool's replication package. Similar to\nthe authors, for each faulty model in our benchmark, we collect the run-time behavioural features from 20 retrainings of\nthe model. NEURALINT is based on static checks that do not require any training and thus, are not prone to randomness.\nWe run each of the remaining tools 20 times to account for the randomness in the training process and report the most\nfrequently observed result (mode).\nTo calculate the similarity between the ground truth provided for each fault in our benchmark and the fault localisation\nresults, we adopt the standard information retrieval metrics Precision (PR), Recall (RC) and FB score:\n$RC = \\frac{|FT_{loc} \\cap FT_{gt}|}{|FT_{gt}|}$                         (1)\n$PR = \\frac{|FT_{loc} \\cap FT_{gt}|}{|FT_{loc}|}$                        (2)\n$F_\\beta = (1 + \\beta^2) \\frac{PR \\cdot RC}{\\beta^2PR + RC}$                        (3)\nRecall measures the proportion of correctly reported fault types in the list of localised faults (FTloc) among those in the\nground truth (FTgt); Precision measures the proportion of correctly reported fault types among the localised ones; FB\nis a weighted geometric average of PR and RC, with the weight \u03b2 deciding on the relative importance between RC\nand PR. Specifically, we adopt F\u00df with \u1e9e equals 3, which gives three times more importance to recall than to precision.\nThis choice of beta is based on the assumption that in the task of fault localisation, the ability of the tool to find as many\ncorrect fault sources as possible is more important than the precision of the answer.\nFor neutrality analysis, we set topk to 5 and the stopping condition SC to a 48-hour time budget. During the search,\nevery model is trained ten times and we use a mean of the ten metric values depending on the task solved by each\nsubjects (i.e., accuracy for classification or loss for regression)."}, {"title": "5 Results", "content": "Tables 4, 5, 6, and 7 present the output of the application of fault localisation tools (DFD, DD, NL, and UM, respectively)\nto our benchmark. Column 'GT' stands for 'Ground Truth' and provides the list of fault types affecting the model,\nwhile column '#F' reports the length of this list. Column '<tool_name>-output' contains the fault list generated by\neach FL tool, while column 'Matches-GT' indicates for each fault in the ground truth whether it was detected by the\ntool or not (1 if yes and 0 otherwise) and column '#M' counts the number of detected faults. For each row (issue) we\nunderline the number of detected faults ('#M') if the tool was able to achieve the best result across all the compared\napproaches. We provide the average number of fault types detected for issues generated by artificially injected faults or\nreal-world issues (rows 'Avg.') and across the whole benchmark (row \u2018T.A.', i.e., Total Average).\nWhen faults affect only selected layers, we specify the indexes of the faulty layers within brackets, for ground truth and\nfor fault localisation results, if this information is provided. Moreover, '-' means that an FL tool was not able to find\nany fault in the model under test; \u2018N/A' means that the tool was not applicable to the fault type in question or crashed\non it. For example, NEURALINT accepts only optimisers that are defined as strings (e.g., 'sgd'), which in turn implies\nthat the default learning rate as defined by the framework is used. This makes it not possible for NEURALINT to find\nan optimiser with modified learning rate. Symbol ',' separates all detected faults, while separation by 'l' means that\nthe faults are alternative to each other, i.e., the tool suggests either of them could be the possible cause of model's\nmisbehaviour.\nInterestingly, in the majority of cases UMLAUT (20 out of 22) and DEEPDIAGNOSIS (15 out of 22) suggest changing\nthe activation function of the last layer to 'softmax' even if in 73 % of these cases for UMLAUT and 67% for\nDEEPDIAGNOSIS, the activation function is already 'softmax'. This also happens once to NEURALINT. We exclude\nsuch misleading suggestions from the tools' output. Moreover, sometimes UMLAUT mentions that over-fitting is\npossible. Since it is just a possibility and such a message does not point to a specific fault, we also exclude it from our\nanalysis. Full output messages provided by the tools are available in our replication package [22]."}, {"title": "5.2 RQ1 (Effectiveness after neutrality analysis)", "content": "We have subsequently investigated our hypothesis that relying on a single ground truth, represented by a single set of\nchanges that improve the model performance, may not be sufficient."}, {"title": "5.3 RQ2 (Stability)", "content": "The authors of DEEPFD account for the instability of the training process and perform 20 retrainings when collecting\ninput features both during the classifier training stage and during fault identification. This way, the output of the tool is\ncalculated from 20 feature sets for each model under test.\nNEURALINT does not require any training and is based on static rules that are stable by design. We performed 20 runs\nof all other tools to investigate their stability. We found out that outputs are stable across the experiment repetitions for\nall considered tools."}, {"title": "5.4 RQ3 (Efficiency)", "content": "In this RQ, we investigate how demanding the evaluated approaches are in terms of execution time. Here we measure\nonly the time required to run an FL tool on a subject, without taking into account the time and effort needed to prepare\nthe subject for the tool application. All of the tools require some manual work to be done: for DEEPFD, a user\nhas to create serialised versions of the training dataset and model configuration according to a specific format; for\nDEEPDIAGNOSIS and UMLAUT, a user has to insert a tool-specific callback to the code and provide it with a list of\narguments; for NEURALINT, there is a number of manual changes to the source code to make the tool runnable."}, {"title": "5.5 The Outputs of Fault Localisation Tools", "content": "The analysed fault localisation tools output messages in natural language that explain where the fault is present in the\nneural network. The level of understandability of these messages, as well as the degree of detail they provide about the\nlocation of the fault and its possible fixes, is an important indicator of the applicability of these tools in practice. We\nhave analysed each output message produced by each tool and provide an overview of our findings in this subsection.\nThe output of DEEPFD consists of a vector of parameters from the following list ['optimizer', 'lr', 'loss', 'epoch', 'act'].\nIn our experiments, the size of the output vector varied between 1 and 4. While the provided vector clearly indicates\nwhich hyperparameters might contain faulty values, it provides no indication on how the values should be changed,\ne.g., whether the learning rate should be increased or decreased. Moreover, for the activation hyperparameter, the layer\nnumber for which the activation function should be changed is not indicated.\nThe outputs of DEEPDIAGNOSIS often refer to problems observed for internal parameters during the training process.\nSome of these outputs, such as \"Batch 0 layer 6: Numerical Error in delta Weights, terminating training\", do not\nprovide any guidance on what should be changed in the model architecture, training data or hyperparameters to fix the\nfault. In contrast, some other outputs such as \"Batch 0 layer 9: Out of Range Problem, terminating training. Change the\nactivation function to softmax\" or \"Batch 0 layer 0: Vanishing Gradient Problem in delta Weights, terminating training.\nAdd/delete layer or change sigmoid activation function\" are more instructive and provide layer numbers along with the\nrequired changes.\nUMLAUT provides an output that lists critical issues as well as warnings, e.g., \"<Critical: Missing Softmax layer\nbefore loss>, <Warning: Last model layer has nonlinear activation>\" and \"<Critical: Missing Softmax layer before\nloss>, <Critical: Missing activation functions>, <Warning: Last model layer has nonlinear activation>\". It should\nbe noted that in our experiments UMLAUT reported the critical issue of \"Missing Softmax layer before loss\" for all\nthe analysed faults, including the cases when the softmax layer is already present in the model architecture. Messages\nindicate the layer number (\"before loss\" or \"last model layer\") in some cases, while in others this information is\nmissing (\"<Critical: Missing activation functions>\"). Similarly to DEEPDIAGNOSIS, some of the warnings produced\nby UMLAUT do not contain actionable fix suggestions, for example, \"Possible over-fitting\" or"}, {"title": "6 Threats to Validity", "content": "Threats to construct validity are due to the measurement of the effectiveness of the fault localisation tools and the\ninterpretation of the tools' output. We use a simple count of the matches between fault localisation results and the\nground truth, along with the RC, PR and F3 metrics that are quite standard in information retrieval."}, {"title": "6.2 Internal", "content": "Threats to the internal validity of the study lie in the selection of evaluated approaches. To the best of our knowledge,\nwe considered all state-of-the-art techniques and adopted their publicly available implementations."}, {"title": "6.3 External", "content": "To address the threats to external validity, we carefully selected faults of both artificial and real nature, covering a set of\ndiverse subjects for the evaluation of FL techniques. Nevertheless, replicating our study on additional subjects would be\nuseful to corroborate our findings."}, {"title": "9 Conflict of Interest and Data Availability", "content": "All authors confirm that they are not affiliated with or involved in any organisation or entity that has financial or\nnon-financial interests in the subject matter or materials discussed in this manuscript.\nThe experimental data, and the evaluation results that support the findings of this study are available in Zenodo platform\nunder the following identifier: 10.5281/zenodo.10387016."}, {"title": "Acknowledgements", "content": "Jinhan Kim and Shin Yoo have been supported by the Engineering Research Center Program through the National\nResearch Foundation of Korea (NRF) funded by the Korean Government (MSIT) (NRF-2018R1A5A1059921), NRF\nGrant (NRF-2020R1A2C1013629), Institute for Information & communications Technology Promotion grant funded\nby the Korean government (MSIT) (No.2021-0-01001), and Samsung Electronics (Grant No. IO201210-07969-01).\nThis work was partially supported by the H2020 project PRECRIME, funded under the ERC Advanced Grant 2017\nProgram (ERC Grant Agreement n. 787703)."}, {"title": "5.1 RQ1 (Effectiveness before neutrality analysis)", "content": "Tables 4, 5, 6, and 7 present the output of the application of fault localisation tools (DFD, DD, NL, and UM, respectively)\nto our benchmark. Column 'GT' stands for 'Ground Truth' and provides the list of fault types affecting the model,\nwhile column '#F' reports the length of this list. Column '<tool_name>-output' contains the fault list generated by\neach FL tool, while column 'Matches-GT' indicates for each fault in the ground truth whether it was detected by the\ntool or not (1 if yes and 0 otherwise) and column '#M' counts the number of detected faults. For each row (issue) we\nunderline the number of detected faults ('#M') if the tool was able to achieve the best result across all the compared\napproaches. We provide the average number of fault types detected for issues generated by artificially injected faults or\nreal-world issues (rows 'Avg.') and across the whole benchmark (row \u2018T.A.', i.e., Total Average).\nWhen faults affect only selected layers, we specify the indexes of the faulty layers within brackets, for ground truth and\nfor fault localisation results, if this information is provided. Moreover, '-' means that an FL tool was not able to find\nany fault in the model under test; \u2018N/A' means that the tool was not applicable to the fault type in question or crashed\non it. For example, NEURALINT accepts only optimisers that are defined as strings (e.g., 'sgd'), which in turn implies\nthat the default learning rate as defined by the framework is used. This makes it not possible for NEURALINT to find\nan optimiser with modified learning rate. Symbol ',' separates all detected faults, while separation by 'l' means that\nthe faults are alternative to each other, i.e., the tool suggests either of them could be the possible cause of model's\nmisbehaviour.\nInterestingly, in the majority of cases UMLAUT (20 out of 22) and DEEPDIAGNOSIS (15 out of 22) suggest changing\nthe activation function of the last layer to 'softmax' even if in 73 % of these cases for UMLAUT and 67% for\nDEEPDIAGNOSIS, the activation function is already 'softmax'. This also happens once to NEURALINT. We exclude\nsuch misleading suggestions from the tools' output. Moreover, sometimes UMLAUT mentions that over-fitting is\npossible. Since it is just a possibility and such a message does not point to a specific fault, we also exclude it from our\nanalysis. Full output messages provided by the tools are available in our replication package [22]."}, {"title": "7 Related Work", "content": "While to the best of our knowledge ours is the first empirical study that performs a third party assessment of existing DL\nfault localization tools, there is a previous empirical work [16] aimed at comparing different DL repair approaches. In\nthe following, we first discuss such empirical work, followed by a summary presentation of existing repair approaches:\nalthough they do not address the DL fault localization problem, they are relevant to such task.\nThe DNN model architecture repair problem, as defined in the recent study by Kim et al. [16], lies in improving the\nperformance of a faulty deep neural network (DNN) model by finding an alternative configuration of its architecture and\nhyperparemeters. The new configuration should lead to a statistically significant enhancement in model performance,\nsuch as accuracy or mean squared error, when measured on a test dataset. In particular, the authors consider a number of\ncategories and subcategories from a DL fault taxonomy [14], covering the following issues: faults affecting the structure\nand properties, faults affecting the DNN layer properties and activation functions, faults due to missing/redundant/wrong\nlayers, and faults associated with the choice of optimiser, loss function and hyperparameters (e.g., learning rate, number\nof epochs) as model architecture faults. Examples of such faults include the selection of an inappropriate loss function\nfor the task at hand or training a model for an insufficient number of epochs."}]}