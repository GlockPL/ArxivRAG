{"title": "Over-the-Air Federated Learning via Weighted Aggregation", "authors": ["Seyed Mohammad Azimi-Abarghouyi", "Leandros Tassiulas"], "abstract": "This paper introduces a new federated learning scheme that leverages over-the-air computation. A novel feature of this scheme is the proposal to employ adaptive weights during aggregation, a facet treated as predefined in other over-the-air schemes. This can mitigate the impact of wireless channel conditions on learning performance, without needing channel state information at transmitter side (CSIT). We provide a mathematical methodology to derive the convergence bound for the proposed scheme in the context of computational heterogeneity and general loss functions, supplemented with design insights. Accordingly, we propose aggregation cost metrics and efficient algorithms to find optimized weights for the aggregation. Finally, through numerical experiments, we validate the effectiveness of the proposed scheme. Even with the challenges posed by channel conditions and device heterogeneity, the proposed scheme surpasses other over-the-air strategies by an accuracy improvement of 15% over the scheme using CSIT and 30% compared to the one without CSIT.", "sections": [{"title": "I. INTRODUCTION", "content": "As wireless edge devices such as phones, smart watches, sensors, and autonomous vehicles become more prevalent and powerful, there is a growing need to use machine learning to train a global model- a unified algorithm that learns patterns from diverse data sources over these devices. However, it is often not possible to transfer such large amounts of data from the devices to a central server due to latency, power, bandwidth, and data privacy concerns. A distributed approach called federated learning (FL) is a promising solution as it enables machine learning to be implemented directly at the wireless edge without transferring data off the devices [1]. This method can be applied to a variety of applications [2], [3]. In FL, the model training is performed locally at each device with the help of a parameter server, and the process of parallel model updates along with aggregation at the server is performed iteratively until it reaches convergence. FL is often conducted over unreliable wireless networks with limited resources and power constraints. In these networks, devices and the edge server communicate via a shared wireless propagation medium, making communication efficiency a critical challenge for FL. Conventional methods, including those using digital communications, segregate communication and computation tasks. These methods typically employ orthogonal multiple access techniques to avoid interference, requiring separate transmissions from each device to the server in distinct resource blocks, leading to significant communication latency and resource requirements [3]. A cost-effective FL scheme is proposed in [4] to address these challenges by optimally determining the number of devices to select and the number of local iterations in each training round. FL using non-orthogonal multiple access (NOMA) techniques via digital communications is also proposed in [5]. While this approach reduces resource usage, the interference acts as additional noise. This is because aggregation is performed only after the individual decoding for each device is complete.\nOver-the-air computation [6], [7] is a promising scheme based on analog communications that leverages the superposition property of wireless channels, allowing simultaneous multiple access transmissions from edge devices within a single resource block. Based on this scheme, an approach known as over-the-air FL is proposed to perform aggregation under the interference, by merging communication and computation. Over-the-air FL can function with remarkably fewer resources and lower latency compared to FL using orthogonal transmissions [3], [8]. Additionally, over-the-air FL utilizes interference by directly recovering the aggregation function as a whole, without any individual recovery for each device, unlike NOMA approaches [3], [8]. However, the aggregation process in over-the-air FL is prone to estimation errors due to the severe effects of wireless channel fading on communication links. This research presents a novel approach to tackle these challenges in over-the-air FL."}, {"title": "A. Prior Work", "content": "Most previous research on over-the-air FL has assumed the necessity of perfect channel state information at transmitter side (CSIT) for all the devices. Using a power allocation approach for the purpose of channel compensation, this information aids in adjusting transmission powers and phases, thereby rectifying the misalignment between wireless channels and predefined aggregation weights [9]\u2013[22]. However, this approach places a substantial burden on channel estimation training and feedback mechanisms for each device, leading to increased latency before each transmission and a notable decrease in spectral and energy efficiency. It also requires perfect synchronization among the transmitters, each equipped with extra hardware for accurate channel adjustments. Additionally, poor channel conditions may either prevent a device from contributing to the learning process or require it to use substantial transmission power. This situation results in signal distortions and imperfect aggregation due to the inherent physical limitations on the maximum and average power capacities of each device [8]. Hence, this approach presents certain challenges when applied to low-cost distributed learning across a large number of devices, particularly those with limited power and hardware capabilities. The common strategy to power allocation is truncated power allocation, where each transmitter only needs to know its own channel, referred to as local CSIT [9]\u2013[15]. Another strategy involves joint device selection and power allocation schemes, as proposed in [16]-[19]. These studies require global knowledge of all channels at every device before each transmission, referred to as global CSIT, to centralize the allocation optimization process. This differs from the distributed approach of the truncated power allocation. At its core, the device selection strategy aims to include the maximum number of devices in each communication round, as an aggregation metric, ensuring that an estimation cost term remains beneath a tolerable threshold for a predefined aggregation. Additionally, there have been power allocation strategies that rely on the presence of knowledge regarding model and data statistics, as referenced in [20]\u2013[22]. This prerequisite can constrain the utility of over-the-air FL in various applications.\nMany current wireless systems transmit blindly using a constant power. Indeed, apart from eliminating the need for CSIT, there are several advantages to transmitting without explicitly compensating for the channel. Primarily, it facilitates maintaining the average transmission energy of the signal regardless of the channel conditions. Also, it prevents the enlargement of the dynamic range of the transmitted signal without any adaptation for channel compensations, thereby significantly reducing the complexity of hardware implementation requirements. Lastly, efforts to correct the channel at the transmitter could be compromised by channel estimation errors, leading to values at the receiver that are multiplied by unpredictable gains [23]. Taking these into account, the idea of a blind over-the-air FL approach has become increasingly prominent [8]. The studies in [23]\u2013[25] delve into blind over-the-air FL without any compensation for the detrimental effects of fading. They confirm that despite these factors, convergence is still assured. In [26]\u2013[28], blind over-the-air schemes are presented that leverage multiple antennas and rely on channel state information at the receiver side (CSIR). Nonetheless, these schemes necessitate large multiple receiver antennas, and the effects of wireless fading become less severe as the number of antennas tends toward infinity. In addition, a blind scheme that uses lattice coding at the device, along with a sufficiently large number of receiver antennas and CSIR at the server, is proposed in [29], [30]. Contrary to the common use of analog modulation in over-the-air FL, this scheme is designed for digital modulation."}, {"title": "B. Key Contributions", "content": "We propose a novel over-the-air FL scheme named weighted over-the-air FL (WAFeL) to counteract the negative impact of wireless channels on the learning convergence performance by leveraging adaptive aggregation weights. Importantly, WAFEL operates as a blind scheme, eliminating the need for CSIT. Without channel compensation at the transmitter end, this approach is shown to be effective in mitigating estimation error of the aggregation caused by different wireless channel conditions of the devices. This remains the case even when deploying a single-antenna server. Our approach uniquely introduces using aggregation weights as optimizable parameters, where channel compensation is one of the objectives addressed. This sets it apart from other blind schemes that either forgo compensation altogether or need an extensive number of antennas for the same. In summary, our proposal offers the following major contributions.\nWeighted Over-the-Air Scheme: We propose a generalized approach to aggregation, WAFeL, which differs from the conventional method of using predefined weights, such as equal or proportional to local dataset sizes. The seminal paper on FL [1] originally presented this aggregation method assuming ideal transmission circumstances and perfect aggregation estimation. However, subsequent research on over-the-air FL has continued to use the same aggregation method, regardless of interference and transmission imperfections [9]\u2013[28]. In contrast, our work employs adaptive aggregation weights to mitigate aggregation estimation error and its effects, while a desirable learning performance is ensured. Furthermore, while most previous over-the-air FL research has not incorporated the computational heterogeneity of devices in their scheme design [9]\u2013[30], our approach integrates varying batch sizes across devices based on their computational strengths and a deadline. This is embodied in WAFeL, resulting in a design that is optimized for device heterogeneity.\nReceiver Architecture: In order to accomplish the proposed weighted aggregation, we propose a new receiver architecture at the server side that incorporates both the real and imaginary parts of the signal, along with an equalization vector. This is then optimized with the objective of reducing the mean squared error (MSE) of the estimation to a minimum. In other over-the-air schemes [9]\u2013[25], perfect synchronization ensures that the received signal is real, leading to a different processing and receiver architecture tailored for their equal aggregation weights.\nConvergence Analysis: Based on a basic set of broadly accepted principles, we analyze the convergence rate of WAFeL for any aggregation weights within the context of the proposed device heterogeneity model. In the analysis, the impact of each batch size is individually incorporated. Based on our findings, we introduce mismatch terms that can model the imperfections in the learning process and provide recommendations for designing the scheme accordingly.\nAggregation Cost Metric and its Optimization: Our analysis demonstrates that aggregation weights proportional to batch sizes are optimal in the absence of estimation errors. We propose a cost metric derived from the error term in the convergence analysis, which incorporates both communication and learning factors- specifically the MSE and mismatch terms\u2014to determine suitable aggregation weights that adapt to system conditions in any communication round. This supports our integrated approach to communication and learning. We also obtain an achievable bound on the error term. In addition, we suggest two solutions for optimizing the cost metric using efficient algorithms. We demonstrate that the complexity of the proposed algorithms scales with $K^3$, where $K$ represents the number of devices.\nSystem Insights: Our experimental findings demonstrate that the aggregation weights designed by WAFeL exhibit significantly improved learning accuracy when compared to other over-the-air schemes, specifically about 15% over the CSIT-equipped scheme [9] and 30% over the non-CSIT scheme [23]. Notably, WAFeL achieves this performance without requiring CSIT, which makes it highly promising. Moreover, the learning performance closely approximates the ideal scenario of error-free orthogonal transmission. Finally, the superiority of the heterogeneity-aware design becomes clear when contrasted with the design that mandates a consistent batch size, set by the device with the lowest computational capacity, known as straggler [2]."}, {"title": "II. SYSTEM MODEL", "content": "There are $K$ devices and a single server as the basic setup for FL systems, as shown in Fig. 1. All these nodes are single-antenna units\u00b9, and have frame-level synchronization. There is no prior knowledge of local data statistics of the devices, consistent with [9]-[19], [23]-[30]. The downlink channels from the server to the devices are considered error-free.\u00b2 The uplink channel from each device $k$ to the server at communication round $t$ is modeled by $h_{k,t} = |h_{k,t}|e^{\\angle h_{k,t}} \\in \\mathbb{C}$, where $h_{k,t}$ is the channel gain and $\\angle h_{k,t}$ is the channel phase. The same transmission power constraint $P$ is considered for all the devices. However, asymmetric power constraints can be incorporated by scaling the channel coefficients appropriately. Let the entire channel vector $\\mathbf{h}_t = [h_{1,t}, \\dots ,h_{K,t}]^T$. The server is the only node that knows $\\mathbf{h}_t$ as the CSIR after the transmission, while each device $k$ knows only a partial estimation of its channel phase $\\angle h_{k,t}$ with an accuracy range of $[0, \\frac{\\pi}{2})$. The purpose of such a quadrant phase estimation with a wide range of inaccuracy, accounting for a quarter of the entire possible range, is to enable each device to adjust its phase so that all channels are observed as positive at the server. This ensures that the channels do not alter the sign of the transmitted data. Such partial estimation is also considered in studies like [23], [24], [31]\u2013[33]. Hence, perfect fine synchronization is not needed, thanks to the acceptable range of uncertainty in channel phase. It is worth noting that many real-world wireless systems have CSIR and phase estimation.\u00b3 Contrastingly, the majority of over-the-air FL techniques [9]\u2013[22] necessitate CSIT for all devices before every transmission. This includes precise values of $| h_{k,t} |$ and $\\angle h_{k,t}$ for each device $k$ and mandates perfect fine synchronization to fully counteract the channels. This places them at a complexity level distinct from our blind approach.\nUnder our central assumption, we either lack knowledge of the channel gain or the information we do have contains substantial inaccuracies, thus complicating its effective use. Instead, our methodology gravitates towards the partial channel phase, providing a more reliable and consistent foundation for our proposed scheme in Section III. This is in line with other blind over-the-air FL schemes and also many conventional wireless systems, avoiding channel compensation at transmitter side [8], [23]\u2013[30]. This approach achieves consistent average signal energy across varying channel conditions, minimizes the dynamic range of the signal, and simplifies the complexity of device hardware. It stands out as an especially viable option for large-scale IoT applications that rely on devices with limited capabilities.\u2074"}, {"title": "B. Heterogeneity-Aware Learning Algorithm", "content": "Device $k \\in \\{1, ..., K\\}$ possesses its own local (private) dataset $\\mathcal{D}_k$. The $i$-th sample $\\xi_i$ in $\\mathcal{D}_k$ contains a feature vector $\\mathbf{x}_i$ and its ground-truth label $y_i$. The learning model is parametrized by the parameter vector $\\mathbf{w} = [W_1, \\dots, W_s]^T \\in \\mathbb{R}^{s \\times 1}$, where $s$ is the model size. Then, the local loss function of the model vector $\\mathbf{w}$ on $\\mathcal{D}_k$ is\n\n$F_k(\\mathbf{w}) = \\frac{1}{\\vert \\mathcal{D}_k \\vert}\\sum_{\\xi_i \\in \\mathcal{D}_k} l(\\mathbf{w},\\xi_i).$ (1)"}, {"title": "III. WAFEL: WEIGHTED OVER-THE-AIR SCHEME", "content": "The WAFEL constructs a new form of the aggregation based on the additive nature of wireless multiple-access channels. This scheme instead of the ideal aggregation in (5) generally has the goal to recover a weighted aggregation as\n$\\mathbf{w}_{G,t+1} = \\sum_{k=1}^K a_{k,t}\\mathbf{w}_{k,t},$ (6)\nwhere $a_{k,t} \\ge 0$ is the weight of device $k$ in the aggregation at round $t$, such that $\\sum_{k=1}^K a_{k,t} = 1$. Let $\\mathbf{a}_t = [a_{1,t},...,a_{K,t}]^T$ be the weight vector.\nUnlike the common over-the-air FL schemes in [9]-[22], which rely on CSIT, WAFeL transmits blindly at a constant power. This characteristic distinguishes WAFEL from these schemes reliant on power allocation, which enforce average and maximum power limits and inherently include device selection. In WAFeL, every device contribute to learning without such restrictions, using an aggregation weight specifically designed for its communication and heterogeneity conditions.\nIt is also worth noting that WAFeL is not limited to the choice of learning algorithm in Subsection II.B.\nThe WAFEL comprises two principal components: the transmission scheme deployed at the devices and the aggregation scheme implemented at the server, as follows. In this section, we ignore the iteration index for simplicity of presentation."}, {"title": "A. Transmission Scheme", "content": "The model parameters at each device are normalized before transmission to have a zero mean and a variance of one (unity power). Normalizing the parameters offers two benefits. First, when the parameters have zero-mean entries, the estimate obtained in Subsection III.B is unbiased. Second, when the entries have unit variance, the power of the received signal and the estimation error do not depend on the specific values of the model parameters.\nThe local model parameter vector at a device $k$ is normalized as $\\mathbf{w}_k = \\frac{\\mathbf{w}_k-\\mu_k\\mathbf{1}}{\\sigma_k}$, where $\\mathbf{1}$ is the all one vector, and $\\mu_k$ and $\\sigma_k$ denote the mean and standard deviation of the $s$ entries of the model vector given by\n\\begin{aligned} \\mu_k &= \\frac{1}{S}\\sum_{i=1}^S \\omega_{k,i}, \\\\ \\sigma_k &= \\sqrt{\\frac{1}{S}\\sum_{i=1}^S (\\omega_{k,i} - \\mu_k)^2}, \\end{aligned} (7)\nwhere $\\omega_{k,i}$ is the $i$-th entry of the model vector. Each device $k$ shares the two scalars $(\\mu_k,\\sigma_k)$ to the server without errors using an orthogonal feedback channel. This information is however negligible compared to the model parameters.\nSubsequently, after the partial phase correction, each device transmits its normalized model parameter vector by scaling it with $\\sqrt{P}$, resulting in a transmission power of $P$."}, {"title": "B. Aggregation Scheme", "content": "After simultaneous transmission of all the devices within a single resource block, the baseband received signal $\\mathbf{y} \\in \\mathbb{C}^{s \\times 1}$ at the server is\n$\\mathbf{y} = \\sqrt{P}\\sum_{k=1}^K h_k \\mathbf{w}_k + \\mathbf{z},$ (8)"}, {"title": "IV. CONVERGENCE ANALYSIS", "content": "The analysis of WAFeL in terms of convergence rate is presented in the following theorem. To shed light on the novelty of our analysis, it is important to emphasize that most convergence studies in this domain largely delve into scenarios characterized by equal aggregation weights, equal batch sizes, ideal communication conditions, or strongly convex loss functions. These studies navigate under an array of assumptions related to learning parameters. In contrast, our analysis boasts flexibility and is tailored to address general loss functions. This not only facilitates easy recognition of the impacts of the scheme's parameters, potential imperfections, and device heterogeneity, but also paves the way for recommending universally-applicable aggregation cost metrics, as elaborated in Section V. Notably, our analysis is based on a minimal set of assumptions commonly found in the literature, e.g., [15], [20], [21], [37], as"}, {"title": "Assumption 1 (Lipschitz-Continuous Gradient):", "content": "The gradient of the loss function $F(\\mathbf{w})$, as represented in (2), is characterized by Lipschitz continuity with a non-negative constant $L > 0$. This implies that for any pair of model vectors $\\mathbf{w}_1$ and $\\mathbf{w}_2$, we have\n$F(\\mathbf{w}_2) \\leq F(\\mathbf{w}_1) + \\nabla F(\\mathbf{w}_1)^T (\\mathbf{w}_2-\\mathbf{w}_1)+\\frac{L}{2}||\\mathbf{w}_2-\\mathbf{w}_1||^2,$ (23)\n$||\\nabla F(\\mathbf{w}_2) - \\nabla F(\\mathbf{w}_1)|| \\leq L||\\mathbf{w}_2 - \\mathbf{w}_1||.$ (24)"}, {"title": "Assumption 2 (Gradient Variance Bound):", "content": "The local stochastic gradient estimate for device $k$ at $\\mathbf{w}_k$, using a mini-batch $\\mathcal{S}_k$, is an unbiased estimate of the ground-truth gradient $\\nabla F(\\mathbf{w}_k)$ with bounded variance\n$\\mathbb{E} \\{||\\nabla F_k(\\mathbf{w}_k,\\mathcal{S}_k) - \\nabla F(\\mathbf{w}_k)||^2\\} \\le \\frac{\\sigma_g^2}{B_k},$ (25)\nwhere $B_k = |\\mathcal{S}_k|$ and $\\sigma_g^2$ denotes the variance bound for the estimate based on a single sample. This inequality plays a key role for quantifying the computational heterogeneity via batch sizes."}, {"title": "Theorem 2:", "content": "Let $1 - \\frac{L\\eta\\tau}{2} - \\frac{L^2\\eta^3 \\tau(\\tau - 1)}{2} > 0$ and $\\mathbf{a}_t$ as the weight vector for each round $t \\in \\{0,..., T - 1\\}$, then the convergence rate of WAFeL under Assumptions 1 and 2 is\n$\\frac{1}{T}\\sum_{t=0}^{T-1} \\mathbb{E} \\{||\\nabla F(\\mathbf{w}_{G,t})||^2\\} \\le \\frac{2(F(\\mathbf{w}_{G,0}) - F^*)}{\\eta \\tau T} + \\frac{L}{T}\\sum_{t=0}^{T-1} I_t(\\mathbf{a}_t),$ (26)\nwhere $F^* = F(\\mathbf{w}^*)$ comes from the problem (3) as the optimal loss value, and\n$I_t(\\mathbf{a}_t) = \\frac{L^2\\eta^3 \\tau(\\tau - 1)}{2} \\sigma_b^2 + \\eta^2\\sigma_g^2 \\mathbf{a}_t^T \\text{diag} \\{\\frac{1}{B_s}\\} \\mathbf{a}_t + \\text{MSE}_t(\\mathbf{a}_t),$ (27)\nwhere $\\text{MSE}_t$ is given in (22) and the vector $\\mathbf{b}_s = [\\frac{1}{B_1}, \\dots, \\frac{1}{B_K}]$ is formed from the inverse of batch sizes of all the devices."}, {"title": "Corollary 1:", "content": "Let $1 - \\frac{L\\eta\\tau}{2} - \\frac{L^2\\eta^3 \\tau(\\tau - 1)}{2} > 0$ and $\\mathbf{a}_t = \\mathbf{b}_w$ for each round $t \\in \\{0, . . ., T - 1\\}$, then in the case of error-free transmission, the convergence rate under Assumptions 1 and 2 is\n$\\frac{1}{T}\\sum_{t=0}^{T-1} \\mathbb{E} \\{||\\nabla F(\\mathbf{w}_{G,t})||^2\\} \\le \\frac{2(F(\\mathbf{w}_{G,0}) - F^*)}{\\eta \\tau T} + \\frac{L}{\\eta \\tau T}\\sum_{t=0}^{T-1} I,$ (28)\nwhere\n$I = \\frac{L^2\\eta^3 \\tau(\\tau - 1)}{2} K + \\frac{\\eta^2 \\sigma_g^2}{B}.$"}, {"title": "V. AGGREGATION WEIGHT SELECTION", "content": "To select the weight vectors $\\mathbf{a}_t, \\forall t \\in \\{0,...,T - 1\\}$, we minimize the error term $\\sum_{t=0}^{T-1} I_t(\\mathbf{a}_t)$ in the convergence rate in Theorem 2, denoted as the aggregation cost metric. It is inspired by Remark 9. Error terms specific to convergence rates have also been widely used for optimizations in other"}, {"title": "A. MSE minimization", "content": "We consider the MSE of the estimation error as the main part of the aggregation cost metric to be minimized, while the mismatch term $\\eta^2\\sigma_g^2 \\text{diag} \\{\\frac{1}{B_s}\\} \\mathbf{a}$ is bounded by a threshold. From Remark 4, this ensures joint learning and communication design. However, inspired by Remarks 6, 7, and 8 and for tractability, we overlook the mismatch term"}, {"title": "B. Mismatch minimization", "content": "Here, the main part of the aggregation cost metric for minimization is the mismatch, under the condition that the MSE is confined within a certain threshold. This can be expressed as follows.\n\n$\\begin{aligned} & \\underset{\\mathbf{a} \\in \\{0\\}} \\text{arg min} \\qquad & \\mathbf{a}^T \\text{diag} \\{\\frac{1}{B_s}\\} \\mathbf{a}, \\\\ & \\text{subject to} \\\\ & \\mathbf{a}^T \\text{diag} \\{\\sigma_t\\}(I_K + \\text{SNR} \\mathbf{H}_t \\mathbf{H}_t^\\dagger)^{-1} \\text{diag} \\{\\sigma_t\\} \\mathbf{a} \\le \\text{th}_2, \\\\ & \\mathbf{1}^T \\mathbf{a} = 1. \\end{aligned}$ (46)"}, {"title": "C. Complexity Analysis", "content": "The computational complexity involved in calculating the inverse of a $K \\times K$ matrix, as required for either (44) or (49), is on the order of $\\mathcal{O}(K^3)$. Thus, for a maximum number of iterations $n_{\\text{max}}$ in Algorithms 1 and 2, the algorithms have the complexity order of $\\mathcal{O}(n_{\\text{max}}K^3)$."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "The learning task is the classification on the standard MNIST and CIFAR-10 datasets with the parameter values given in Table 1, unless otherwise stated. The classifier models for MNIST and CIFAR-10 are built on a convolutional neural network (CNN), differing in layer configuration. For MNIST, the model features two 3 \u00d7 3 convolution layers with ReLU activation, the first layer having 32 channels and the second 64 channels, each followed by a 2 \u00d7 2 max pooling layer. In contrast, the CIFAR-10 model expands to four 3 \u00d7 3 convolution layers with ReLU activation, where the initial two layers are configured with 32 channels and the subsequent two with 64 channels, with a 2 \u00d7 2 max pooling layer after every second layer. Both architectures share a common structure thereafter: a fully connected layer with 128 units and ReLU activation, culminating in a softmax output layer for classification [15]. We take into account both independent and identically distributed (i.i.d.) and non-i.i.d. distribution of dataset samples across the devices. In the non-i.i.d. scenario, each device holds samples from only two classes, and the quantity of samples varies among devices. To account for computational heterogeneity, each device $k$ possesses distinct processor computing speed $f_k$, leading it to select a fixed batch size $B_k$ from the range between 20 and 60 based on the deadline $T_p$, with the relationship $f_k = \\frac{W}{T_p} B_k \\in [20,60], \\forall k$. Through which, the fastest device exhibits processing power three times greater than that of the slowest device. Performance is gauged in terms of learning accuracy relative to the test dataset and training loss across global iterations (denoted as $t$), aligning with standard metrics used in existing literature, for instance, [9]\u2013[12], [14]\u2013[22], [25]\u2013[30]. Each performance result is evaluated as the average of 20 realization samples to account for Gaussian channel distribution, i.e., the channel gain with Rayleigh fading $\\sim \\exp(1)$ and the channel phase (after the partial phase compensation) with uniform distribution $\\sim \\mathcal{U}(0, \\frac{\\pi}{2})$.\nIn Fig. 3, the training loss is shown for different local iterations $\\tau$ in the MNIST and i.i.d. scenario. As observed, increasing $\\tau$ or $t$ improves the learning performance. It further exhibits a marked improvement when integrating multiple local iterations in comparison to using just a single local iteration.\nIn Fig. 4, the test accuracy is shown for both the heterogeneity-aware design of WAFeL and its heterogeneity-unaware design when all devices are set to use an equal batch size based on the capabilities of the least powerful device as the straggler, i.e., with the lowest batch size. It is observed that the heterogeneity-aware design significantly outperforms in terms of accuracy, even under the considered limited heterogeneity.\nIn Fig. 5, the training loss is shown for different numbers of devices $K$ in the MNIST and non-i.i.d. scenario. The performance improves as $K$ increases because more devices participates in the learning process. However, this improvement comes at the cost of increased MSE, resulting in a tradeoff. Specifically, for higher values of $K$, this performance improvement tends to diminish.\nIn Figs 6 and 7, the effects of the thresholds, $\\text{th}_1$ and $\\text{th}_2$, are investigated on the objectives in problems (41) and (46), the MSE and mismatch, respectively. As the thresholds increase, it is observed that the objectives decrease significantly.\nIn Figs 8 and 9, comparisons are made between WAFeL and well-known schemes from the literature, serving as benchmarks, using both MNIST and CIFAR-10 datasets in the non-i.i.d. scenario. Among the benchmarks using analog over-the-air computation similar to WAFeL are the BAA scheme, which uses truncated power allocation with accurate knowledge of CSIT [9], and the GBMA scheme, which compensates for phase only at the transmitters [23]. For both BAA and GBMA, perfect fine synchronization is considered. Additionally, the ideal error-free performance is considered using FL with digital orthogonal transmissions, requiring at least $K = 30$ times more resource blocks. The FL using NOMA [5], referred to as NOMA, is also considered as another benchmark via digital communications, under the same single resource block as in WAFEL, BAA, and GBMA. For a consistent comparison, FedAvg with the same $\\tau = 3$ is implemented across all the schemes. While the BAA scheme possesses a distinct complexity level owing to its need for perfect CSIT and channel compensation requirements, the blind GBMA method lacks any optimization algorithm or processing, setting it apart from WAFeL which incorporates an algorithm of complexity $\\mathcal{O}(K^3)$.\nIt is observed that WAFeL significantly outperforms both the BAA and GBMA schemes across both the MNIST and CIFAR-10 datasets. For example, at $t = 100$, the improvement of WAFeL is around 15% and 30% compared to BAA and GBMA, respectively, when evaluated with the MNIST dataset. Moreover, it closely approximates the performance achieved by the orthogonal case. Surprisingly, WAFeL, which incorporates partial phase compensation, outperforms schemes like BAA that have perfect gain and phase compensation. This performance improvement is attributed to the utilization of optimized adaptive weights in the aggregation process, which was not previously identified, coupled with the unique receiver structure. It is noteworthy that in contrast to BAA where each device needs to adhere to both average and maximum power constraints for power allocation and there is an inherent device selection, the weight allocation in WAFeL is not subjected to any limitations and all the devices contribute to the learning.\nAlso, WAFeL, which is based on analog communications, significantly outperforms NOMA, which is based on digital communications. This is mainly because in NOMA, not only is interference considered as additional noise when decoding individual model parameters of the devices, but also model parameters must be quantized into finite bits limited by the available resource block before transmission, further adding quantization errors. On the other hand, in WAFeL, the aggregation is estimated directly, and aggregation weights are optimally designed to maximize convergence performance."}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we introduced a unique over-the-air federated learning scheme incorporating a novel weighted aggregation approach. In the scheme, the adaptive choice of aggregation weights helps counteract the impacts of wireless channels on performance, eliminating the need for channel compensation at the transmitting end. By considering device heterogeneity via diverse batch sizes, we analyzed the convergence rate of the learning process for the scheme as a function of the aggregation weights, which also includes both communication and learning factors. To select the aggregation weights, we proposed aggregation cost metrics based on the analysis. We presented efficient algorithms to optimize the metrics, and also derived an achievable bound on the convergence rate. Despite channel conditions and device heterogeneity, experimental results demonstrated the high learning accuracy of the proposed scheme, surpassing existing solutions even the one with channel compensation at transmitters. Moreover, the proposed scheme can closely attain the level of performance that would be expected in the absence of any errors. Consequently, the proposed scheme, which emphasizes aggregation weight optimization, offers a promising new learning framework that can be further explored in various setups for its potential applications."}, {"title": "APPENDIX A PROOF OF THEOREM 1", "content": "The update of learning model at round $t + 1$ is as\n$\\mathbf{w}_{G,t+1} = \\frac{1}{\\sqrt{P}}b_t^H \\mathbf{Y}_t + \\frac{1}{K} \\sum_{k=1}^K \\alpha_{k,t} \\mu_{k,t} \\mathbf{1} = \\frac{1}{K} \\sum_{k=1}^K \\alpha_{k,t} \\mathbf{w}_t + \\epsilon(\\mathbf{\\alpha}_t),$ (51)\nwhere $\\epsilon(\\mathbf{\\alpha}_t)$ models the estimation error in recovering the aggregation with the weight vector $\\mathbf{\\alpha}_t$. Subsequently, based on the L-Lipschitz continuity attribute stated in Assumption 1, we obtain\n$\\begin{aligned} &F(\\mathbf{w}_{G,t+1}) - F(\\mathbf{w}_{G,t}) \\le \\nabla F(\\mathbf{w}_{G,t})^T (\\mathbf{w}_{G,t+1} - \\mathbf{w}_{G,t}) + \\\\ & \\frac{L}{2} ||\\mathbf{w}_{G,t+1} - \\mathbf{w}_{G,t}||^2 = \\nabla F(\\mathbf{w}_{G,t})^T (-\\frac{\\eta \\tau}{K} \\sum_{k=1}^K \\alpha_{k,t} \\sum_{i=0}^{\\tau - 1} \\nabla F_k(\\mathbf{w}_{k,t,i}, \\mathcal{S}_i) + \\epsilon(\\mathbf{\\alpha}_t)) + \\\\ & \\frac{L}{2} (-\\frac{\\eta \\tau}{K} \\sum_{k=1}^K \\alpha_{k,t} \\sum_{i=0}^{\\tau - 1} \\nabla F_k(\\mathbf{w}_{k,t,i}, \\mathcal{S}_i) + \\epsilon(\\mathbf{\\alpha}_t))^2. \\end{aligned}$ (52)"}]}