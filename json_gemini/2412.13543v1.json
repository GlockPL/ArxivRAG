{"title": "Query-centric Audio-Visual Cognition Network for Moment Retrieval, Segmentation and Step-Captioning", "authors": ["Yunbin Tu", "Liang Li", "Li Su", "Qingming Huang"], "abstract": "Video has emerged as a favored multimedia format on the internet. To better gain video contents, a new topic HIREST is presented, including video retrieval, moment retrieval, moment segmentation, and step-captioning. The pioneering work chooses the pre-trained CLIP-based model for video retrieval, and leverages it as a feature extractor for other three challenging tasks solved in a multi-task learning paradigm. Nevertheless, this work struggles to learn the comprehensive cognition of user-preferred content, due to disregarding the hierarchies and association relations across modalities. In this paper, guided by the shallow-to-deep principle, we propose a query-centric audio-visual cognition (QUAG) network to construct a reliable multi-modal representation for moment retrieval, segmentation and step-captioning. Specifically, we first design the modality-synergistic perception to obtain rich audio-visual content, by modeling global contrastive alignment and local fine-grained interaction between visual and audio modalities. Then, we devise the query-centric cognition that uses the deep-level query to perform the temporal-channel filtration on the shallow-level audio-visual representation. This can cognize user-preferred content and thus attain a query-centric audio-visual representation for three tasks. Extensive experiments show QUAG achieves the SOTA results on HIREST. Further, we test QUAG on the query-based video summarization task and verify its good generalization.", "sections": [{"title": "Introduction", "content": "Recently, we have been witnessing an exponential growth of videos, along with the advance of generative AI (e.g., Sora) and video platforms. Accordingly, a plethora of studies (Wu et al. 2023; Li et al. 2024; Wang et al. 2024c) have been proposed to enhance video retrieval capabilities. However, given a text query, returning the whole video is not always satisfied. Sometimes, users want to directly localize the moment most related to the query. For instance, when learning how to make a strawberry pie, users tend to only focus on the instructional moment, as shown in Figure 1. Moreover, if this moment is intricate, we anticipate machines to segment it into finer-level steps, and caption each with a succinct sentence for a better understanding, such as \"Add strawberry juice on plate\", \"Put dough on top\", etc. This shift in user preference sparks an emerging research topic: hierarchical retrieval and step-captioning (HIREST) (Zala et al. 2023).\nThis topic benchmarks four tasks of video retrieval, moment retrieval, moment segmentation, and step captioning. In the pioneering work (Zala et al. 2023), Zala et al. implement video retrieval by the pre-trained CLIP-based model and use it as a feature extractor for the three downstream tasks. Their focus is to jointly address the three tasks in a single architecture. Toward this end, they propose a Joint model that first leverages pre-trained models (Fang et al. 2023; Radford et al. 2023; Reimers and Gurevych 2019) to produce the multi-modal representation for visual frames, audio and query. Next, this representation is used to predict the boundaries of moments and steps, as well as yielding each step caption. In this process, the model is trained via a multi-task setup in a round-robin way (Cho et al. 2021).\nDespite the promising results, the above work has the limitations in learning a comprehensive cognition of user-preferred video content, owing to the naive multi-modal fusion strategy. First, the representations of three modalities are directly fused without distinction, which disregards the hierarchies across modalities. Studies in psychology (Tacca 2011; Yang, Zhuang, and Pan 2021) have revealed that human perception and cognition is in a shallow-to-deep fashion. When watching a video, we usually first obtain shallow-level sensory information, e.g., the appearance and sound of objects, to obtain an intuitive perception for audio-visual content in the video. Then, we incorporate deeper-level knowledge (e.g., intentions) into the sensory information to learn the comprehensive cognition of the particular content of interest. Hence, it is beneficial to model the modality hierarchies based on this shallow-to-deep principle.\nSecond, the fusion strategies (element-wise multiplication and summation) among three modalities fail to make full use of their association relations. The shallow-level visual and audio modalities can represent video content from different aspects, but direct element-wise summation may lose their synergistic relation. Afterwards, the deep-level query can filter trivial details and highlight the important ones within the shallow-level audio-visual content. Nevertheless, element-wise multiplication cannot model such a filtration relation that helps cognize user-preferred video content. As such, it is warranted to progressively capture these association relations during modeling hierarchies across modalities.\nIn this paper, we propose a QUery-centric Audio-visual coGnition (QUAG) network to cognize user-preferred video content and learn an effective multi-modal representation for addressing moment retrieval, segmentation and step-captioning. QUAG consists of modality-synergistic perception (MSP) and query-centric cognition (QC2) modules. Concretely, given visual frame and audio representations, MSP first models their global contrastive alignment to make them reside in the same embedding space; then learns their local fine-grained interaction to mine their joint representations, which are fused as the audio-visual representation. Afterwards, guided by the deep-level query, QC2 performs a temporal-channel filtration on the shallow-level audio-visual representation, thus highlighting user-requested details. Next, the query representation is injected into the filtered audio-visual representation to construct a query-centric audio-visual representation. This is finally fed into the multi-modal encoder, multi-task prediction heads and text decoder for addressing the three challenging tasks.\nOur key contributions are summarized as follows:\n\u2022 Based on the investigation for human perception and cognition, we follow the shallow-to-deep principle to propose QUAG, which jointly solves three challenging tasks via learning a query-centric audio-visual representation.\n\u2022 In QUAG, we first design MSP to attain the audio-visual representation by modeling global contrastive alignment and local fine-grained interaction between visual and audio representations. Then, we devise QC2 to attend to the query-centric audio-visual representation, by implementing temporal-channel filtration from the deep-level query to the shallow-level audio-visual representation.\n\u2022 Extensive experiments show QUAG achieves state-of-the-art results for the moment retrieval, segmentation and step-captioning on the HIREST dataset. Moreover, we test QUAG on the TVSum dataset for the query-based video summarization, which validates its generalization."}, {"title": "Related Work", "content": "The task of hierarchical retrieval and step-captioning belongs to the multi-modal learning community (Cong et al. 2022, 2023; Tu et al. 2023a, 2024a,b,c). In the following, we review the related works from four dimensions.\nVideo-moment retrieval. Cross-modal retrieval is a fundamental problem in artificial intelligence (Zha et al. 2019; Zhang et al. 2020; Liu et al. 2022a; Zhang et al. 2024; Yue et al. 2023; Wang et al. 2024a,b; Tang et al. 2024; Yue et al. 2024). Text-based video retrieval (Dong et al. 2022; Xie et al. 2024; Li et al. 2023) is to find related videos from a corpora by a text query. Recently, benefiting from the contrastive learning, most text-to-image/video retrieval models (e.g., CLIP) have been designed (Radford et al. 2021; Fang et al. 2023). With these models, the cosine similarity between the text query and video can be easily computed, thus finding the most related video to the query. However, since there are some query-irrelevant parts in the whole video, moment retrieval has been studied by most methods (Sun et al. 2022; Lei, Berg, and Bansal 2021; Moon et al. 2023), which is to localize the query-related span in the video.\nVideo summarization. Traditional video summarization methods (Song et al. 2015; Xiong and Grauman 2014) are to condense lengthy videos by extracting important information. However, these methods ignore the users' various preferences over the summaries. To address this limitation, the query-focused approaches (Sharghi, Gong, and Shah 2016; Sharghi, Laurel, and Gong 2017; Narasimhan et al. 2022) incorporate users' preferences through text queries, thus identifying the most relevant frames within the video. By aligning the summarization process with user interests, these query-focused methods can find a collection of moments that are most related to the users' preferences.\nVideo Captioning. Traditional video captioning methods (Li et al. 2022; Tu et al. 2023b) generate a concise sentence for a short video. To describe longer videos with multiple events, dense video captioning is studied by some methods (Yang et al. 2023; Kim et al. 2024), which is to generate a paragraph for the long video. On the other hand, there are some methods (Lei et al. 2020; Li et al. 2020; Tu et al. 2022) that try to study TV show captioning. Instead of only using visual modalities, they consider introducing text features (e.g., subtitles from the actors' dialogues) to augment video features, thus modeling a visual-linguistic representation for caption generation.\nHierarchical Retrieval and Step-Captioning. The above tasks are individually studied by previous works. In fact, these tasks share a common goal to extract information from a video corpus. Thus, Zala et al. (Zala et al. 2023) combine these tasks as a new task called HIREST to cater to users' various preferences. In this pioneering work, the pre-trained CLIP-based model is leveraged for video retrieval, and used as the feature extractor for the other three tasks. Further, a unified framework is proposed to jointly address the three tasks. In this work, we follow this paradigm to address the three tasks in a unified architecture. Compared"}, {"title": "Methodology", "content": "As shown in Figure 2, the overall framework of our method includes the following parts. First, given the visual frames, audio, and query, we first extract the visual, audio, and text representations by pre-trained models. Then, we feed them into QUAG to produce a query-centric audio-visual representation. Finally, this representation is fed into a multi-modal encoder for intra-relation modeling, and used to 1) predict the boundaries of moments and steps, and 2) prompt the text decoder to generate the caption for each step.\nMulti-modal Input Embedding\nGiven an untrimmed video containing N\u2082 frames and a text query consisting of Nt tokens, we first employ pre-trained models to extract the visual representation R and text representation Rt, separately. Then, considering that the audio information helps perceptive the main objects in the video, we also extract the speech transcription from the audio and embed it as the audio representation Ra, whose length is equal to the video representation, i.e., Nr. Next, we project three modality representations into the same embedding space by three linear transformation functions, i.e., \\(R_v \\in \\mathbb{R}^{N_v\\times D}\\), \\(R_a \\in \\mathbb{R}^{N_v\\times D}\\), \\(R_t \\in \\mathbb{R}^{D}\\).\nQuery-centric Audio-visual Cognition\nAfter obtaining the visual, audio, and text representations, we propose a query-centric audio-visual cognition (QUAG) network that utilizes the modality-synergistic perception (MSP) and query-centric cognition (QC2) modules to learn a query-centric audio-visual representation based on the shallow-to-deep principle, so as to learn the comprehensive cognition of user-preferred video content.\nModality-Synergistic Perception Given the pair-wise visual representation R and audio representation Ra, we design the MSP to first make them reside in the same embedding space by maximizing their global contrastive alignment. Specifically, we compute their global features by the mean-pooling operation over the length dimension:\n\\( \\overline{R_v} = \\frac{1}{N_v} \\sum{R_v}, \\space \\overline{R_a} = \\frac{1}{N_v} \\sum{R_a} \\)   (1)\nWhere \\(\\overline{R_v} \\in \\mathbb{R}^{D}\\) and \\(\\overline{R_a} \\in \\mathbb{R}^{D}\\). Then, in a training batch, we sample B pair-wise globally visual and audio features. For k-th globally visual feature \\(\\overline{R_v}^k\\), k-th globally audio feature \\(\\overline{R_a}^k\\) is its positive, while other r(r \u2260 k) globally audio features will be the negatives. That is, there are a total of B positive sample pairs and B \u00d7 (B \u2013 1) negative sample pairs in this batch. Next, we employ the InfoNCE loss (Oord, Li, and Vinyals 2018) to maximize the bi-directional similarities between the positive pairs \\(\\overline{R_v}^k\\) and \\(\\overline{R_a}^k\\), while minimizing the similarity between the negative pairs:\n\\( \\mathcal{L}_{v2a} = - \\frac{1}{B} \\sum_k^B log \\frac{e^{(sim(\\overline{R_v}^k,\\overline{R_a}^k)/ \\tau)}}{\\sum_{r=1}^B e^{(sim(\\overline{R_v}^k,\\overline{R_a}^r)/ \\tau)}}, \\\\  \\mathcal{L}_{a2v} = - \\frac{1}{B} \\sum_k^B log \\frac{e^{(sim(\\overline{R_a}^k,\\overline{R_v}^k)/ \\tau)}}{\\sum_{r=1}^B e^{(sim(\\overline{R_a}^k,\\overline{R_v}^r)/ \\tau)}},   (2) \\\\  \\mathcal{L}_{msp} = \\frac{1}{2} (\\mathcal{L}_{v2a} + \\mathcal{L}_{a2v}), \\)\nwhere \\(\\tau\\) is the temperature hyper-parameter. \"sim\" means the dot-product operation. This loss formulates a self-supervisory signal to enhance the global alignment between the visual and audio representations, which facilitates the subsequent local fine-grained interaction.\nSubsequently, we interact the local features on the visual representation \\(R_v\\) and audio representation \\(R_a\\) to learn their fine-grained synergy and mine the joint representations. This is performed by the multi-head cross-attention mechanism (MHCA) (Vaswani et al. 2017):\n\\( R_v' = MHCA (R_v, R_a, R_a), \\\\  R_a' = MHCA (R_a, R_v, R_v) .\\)   (3)\nIn the bracket of each equation, the first term represents the query, while the last two terms denote the key and value representations. After that, we concatenate the joint representations from two modalities as the audio-visual representation, which is performed by a fully-connected layer:\n\\( \\overline{R_c} = ([R_v'; R_a'] W_c + b_c),\\)   (4)\nwhere \\(\\overline{R_c} \\in \\mathbb{R}^{N_v \\times D}\\). \\(W_c \\in \\mathbb{R}^{2D \\times D}\\) and \\(b_c \\in \\mathbb{R}^{D}\\). [;] is a concatenation operation.\nQuery-Centric Cognition We devise QC2 to use the deep-level query representation \\(R_t \\in \\mathbb{R}^{D}\\) to attend to the related information over the shallow-level audio-visual representation \\(\\overline{R_c} \\in \\mathbb{R}^{N_v \\times D}\\). QC2 first measures the relevance between \\(R_t\\) and \\(\\overline{R_c}\\) from both temporal and channel dimensions. The temporal and channel relation matrices \\(A_{te}\\) and \\(A_{ch}\\) are computed as follows:\n\\( \\overline{R_c'} = ([\\overline{R_c}; R_t] W_a + b_a),\\\\  A_{te} = sigmoid (F_{te} (\\frac{1}{D} \\sum_{j} \\overline{R_c'}(:, j))), \\\\  A_{ch} = sigmoid (F_{ch} (\\frac{1}{N_v} \\sum_{j} \\overline{R_c'}(j, :))),   (5)\nwhere \\(W_a \\in \\mathbb{R}^{2D \\times D}\\) and \\(b_d \\in \\mathbb{R}^{D}\\). [;] is a concatenation operation and we broadcast the text representation \\(R_t \\in \\mathbb{R}^{D}\\) as \\(R_t \\in \\mathbb{R}^{N_v \\times D}\\), to temporally match its shape with the audio-visual representation \\(\\overline{R_c} \\in \\mathbb{R}^{N_v \\times D}\\). \\(A_{te} \\in \\mathbb{R}^{N_v \\times 1}\\), \\(A_{ch} \\in \\mathbb{R}^{1 \\times D}\\). \\(F_{te} \\in \\mathbb{R}^{1 \\times 1}\\) and \\(F_{ch} \\in \\mathbb{R}^{D \\times D}\\) are linear transformation functions.\nNext, QC2 computes the temporal-channel relation matrix by fusing \\(A_{te}\\) and \\(A_{ch}\\) together, which is implemented with the element-wise multiplication function:\n\\( A_{tc} = A_{te} \\otimes A_{ch},\\)   (6)\nwhere \\(A_{tc} \\in \\mathbb{R}^{N_v \\times D}\\). Guided by \\(A_{tc}\\), the model can filter the trivial details in the audio-visual representation \\(\\overline{R_c}\\):\n\\( R_c' = A_{tc} \\overline{R_c}\\)   (7)\nwhere \\(R_c' \\in \\mathbb{R}^{N_v \\times D}\\). Subsequently, we integrate \\(\\overline{R_c}\\) into the filtered audio-visual representation \\(R_c'\\), to model a query-centric audio-visual representation:\n\\( R_m = R_c' + \\phi( \\overline{R_c}), \\)   (8)"}, {"title": "Moment Retrieval, Moment Segmentation, and Step-captioning", "content": "Moment Retrieval is to find the moment most relevant to the query and output its span. Specifically, given Rm, we use a learnable prediction head consisting of two linear layers to predict the start and end boundaries at the same time, where the frame inputs are not masked. The probability distributions of the start and end indexes over the entire video are computed as follows:\n\\( P_{start} = Softmax (R_mW_s + b_s), \\\\  P_{end} = Softmax (R_mW_e + b_e),\\)   (9)\nwhere \\(W_s \\in \\mathbb{R}^{D \\times 1}\\), \\(W_e \\in \\mathbb{R}^{D \\times 1}\\), \\(b_s \\in \\mathbb{R}^{1}\\), and \\(b_e \\in \\mathbb{R}^{1}\\).\nMoment Segmentation refers to identifying \"key steps\" within the retrieved moment that does not have repetitive frames, where each step's end timestamp is the next step's start timestamp. Concretely, given Rm, we use a learnable prediction head with a linear layer to predict the boundary of each step in an auto-regressive manner, where the previously predicted boundaries are integrated into Rm as the part of the input. The frames outside of the moment and in the previous steps are masked out. The probability distribution of boundary index for each step is computed as follows:\n\\( P_{step} = Softmax (R_mW_t + b_t),\\)   (10)\nwhere \\(W_t \\in \\mathbb{R}^{D \\times 1}\\) and \\(b_t \\in \\mathbb{R}^{1}\\).\nStep-captioning aims to describe each segmented step with a sentence. To this end, we first use the pre-trained captioning model CLIP4Caption (Tang et al. 2021) to initialize the parameters of text decoder. Then, we feed Rm into the text decoder that yields the text descriptions through an auto-regressive way. This process can be formulated as follows:\n\\( w_i = F_{De} (R_m, w_1, ..., w_{i-1}),\\)   (11)\nwhere \\(w_i \\in \\mathbb{R}^{N_t}\\) is the i-th decoded distribution over a dictionary of Nt vocabularies; {\\(w_1,..., w_{i-1}\\)} are previously generated words (ground-truth words during training and predicted words during inference).\nTraining. For moment retrieval, given the ground-truth start and end indices, we define the training loss (to be minimized) as the sum of the negative log probabilities by the predicted distributions, which are then averaged over all examples in a batch:\n\\( \\mathcal{L}_{ret} = - \\frac{1}{B} \\sum_i^B log (p_{ystart}^{start}) - log (p_{yend}^{end}),\\)   (12)\nwhere B is the batch size; ystart and yend are the ground-truth start and end indices of the i-th example, respectively."}, {"title": "Experiments", "content": "For moment segmentation, given ground-truth step indices, we define the training loss (to be minimized) as the sum of the negative log probabilities by the predicted distributions, which are averaged over all examples in a batch:\n\\( \\mathcal{L}_{seg} = - \\frac{1}{B} \\sum_i^B log (p_{ystep}^{step}),\\)   (13)\nwhere B is the batch size and y step is the ground-truth step index of the i-th example, respectively.\nFor step-captioning, given the ground-truth step caption words (w\u2081,..., wm), we define the training loss (to be minimized) as the sum of the negative log probabilities by the predicted distributions, which are then averaged over all examples in a batch:\n\\( \\mathcal{L}_{cap} = - \\sum_{i=1}^m log p_\\theta (w_i | w_{<i}),\\)   (14)\nwhere \\(p_\\theta (w_i | w_{<i})\\) is computed by Eq. (11). m is the length of a step caption.\nOur method is trained in a multi-task configuration, where a round-robin manner is leveraged and a batch is sampled from one of the three data loaders at each iteration (Zala et al. 2023; Cho et al. 2021). That is, the training loss for each iteration is defined as:\n\\( \\mathcal{L}(\\Theta) = \\mathcal{L}_i + \\lambda \\mathcal{L}_{msp},\\)   (15)\nwhere i refers to ret, seg, or cap. \u0398 are a set of trainable weights. \u039b is a trade-off parameter to balance the contribution between one of the task losses and the contrastive loss, which is discussed in the supplementary material *.\nDatasets\nHIREST consists of the tasks of video retrieval, moment retrieval, moment segmentation, and step-captioning. It is comprised of 3.4K text-video pairs, 1.8K moments, and 8.6K step captions. We use the official split with 1,507 video-query pairs for training, 477 video-query pairs for validation and 1,391 video-query pairs for testing.\nTVSum contains the task of query-based video summarization, which is relevant to moment segmentation. This dataset includes 10 varying categories of videos, and each category consists of 5 videos. For a fair-comparison, we follow QD-DETR (Moon et al. 2023) to utilize 80% videos for training and the remaining for testing.\nEvaluation Metrics\nHIREST: (1) We validate moment retrieval by evaluating the outputs of model against the ground-truth moment spans via Recall of Intersection over Union (IoU) thresholds (0.5 and 0.7). (2) For moment segmentation, models are assessed based on the similarity between the generated step spans and the ground-truth step spans, via Recall and Precision metrics over IoU thresholds (0.5 and 0.7). (3) For step-captioning, the traditional metrics are used to evaluate the yielded sentence: METEOR (Banerjee and Lavie 2005), ROUGE-L (Lin 2004), CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015), and SPICE (Anderson et al. 2016). Besides, Following (Zala et al. 2023), we introduce the ELMO (Peters et al. 2018)-based Decomposable Attention model (Parikh et al. 2016) to compute the entailment of generated sentences against the ground-truth captions.\nTVSum: Following previous works (Moon et al. 2023; Liu et al. 2022b), we leverage the top-5 mAP as the main metric.\nImplementation Details\nFirst, we use pre-trained EVA-CLIP (Fang et al. 2023) to extract the visual representation for visual frames, and the text encoder of EVA-CLIP to map a text query as the text representation. Then, we use pre-trained Whisper (Radford et al. 2023) to extract speech transcription from audio, and use pre-trained MiniLM (Reimers and Gurevych 2019) text encoder to map the transcription into a representation, called audio representation. The hidden size is set to 768. The text decoder is first initialized from pre-trained CLIP4Caption (Tang et al. 2021), and then is fine-tuned on the HIREST dataset. During training, the batch size is set to 5 and learning rate is set to 1 \u00d7 10\u22125, AdamW optimizer (Loshchilov and Hutter 2018) is used to minimize the training loss defined in Eq. (15). More details are shown in supplementary.\nPerformance Comparison on HIREST\nResults on the Moment Retrieval In this task, we compare QUAG with the pioneering work Joint model (Zala et al. 2023). We also compare QUAG with three task-specific moment retrieval models: QD-DETR (Moon et al. 2023), TR-DETR (Sun et al. 2024), and UVCOM (Xiao et al. 2024). The mentioned models are implemented based on their released codes. Besides, following Zala et al. (Zala et al. 2023), we compare QUAG with a text-to-image retrieval model EVA-CLIP (Fang et al. 2023).\nThe comparison results are shown in Table 1. Our QUAG obtains the best performances in moment retrieval at both metrics. These indicate our method has the high recall in identifying relevant moments with the 50% overlap and 70% overlap with the ground-truth spans. Especially, our method has the performance improvements of 2.2% and 4.2% against the Joint model at R@0.5 and R@0.7, respectively, which further validates the effectiveness of QUAG."}, {"title": "Ablation Study", "content": "We conduct the ablation study to investigate the contribution of each module and the full model. Here, we choose the step-captioning task as the major ablation task, as it requires the model have a comprehensive capability for the cognition, alignment and generation for multi-modal content. The baseline is Joint, which directly fuses three modality representations by element-wise summation and multiplication.\nThe ablative results are shown in Table 4. The performances of each module and the full QUAG are better than that of Joint baseline, which validate their effectiveness. However, the performance improvement is not significant when each module is individually used. When only QC2 is used, there is even the performance decreasing on the CIDEr metric. Our conjecture is that each module only learns either synergistic relation or filtration relations. Besides, the hierarchies across modalities are overlooked. When the full QUAG model is used, its performance significantly surpasses the others, in particular with the increases of 20.1% and 48.7% on the CIDEr and SPICE over the Joint baseline. The ablation results indicate each designed module not only plays its unique role, but also supplements each other.\nPerformance Comparison on TVsum\nWe further validate the generalization ability of QUAG for the query-based video summarization that is related to moment segmentation. The experiment is conducted on the TVsum dataset and the compared methods are TGG (Ye et al. 2021), UMT (Liu et al. 2022b), QD-DETR (Moon et al. 2023), UVCOM (Xiao et al. 2024), and TR-DETR (Sun et al. 2024). The inputs for all compared methods are visual, audio, and text query representations.\nThe results are shown in Table 5. QUAG obtains the best results on the 5 categories, which is better than the others. Besides, the average overall performance of our method is in par with TR-DETR (87.0 vs. 87.1), which shows its good"}, {"title": "Qualitative Analysis", "content": "To obtain an intuitive observation for our method, in Figure 3, we provide a visualization case for moment retrieval, segmentation, and step-captioning. We observe that the retrieved moment by our QUAG contains the less trivial details than that of the Joint. Besides, QUAG better predicts the step boundaries and generates the step captions. For instance, the yielded step captions by Joint model are redundant, which lack the details in the retrieved moment. By contrast, our QUAG generates \u201ccut it\u201d that matches the ground-truth caption \"cut tomatoes\" (67-80s); \"add oil\" and \"add salt and pepper\u201d that show the semantic consistency with the ground-truth caption \"add canon oil, salt and mash it\" (107-135s). Besides, our QUAG can describe fine-grained content of \"chop it\" during the actor cuts the bread into slices. In short, the visualization results show the effectiveness of QUAG. The superiority benefits from the fact of progressively modeling a query-centric audio-visual representation, rather than directly fusing them without distinction. More examples are shown in the supplementary material."}, {"title": "Conclusion", "content": "This paper follows the shallow-to-deep principle to propose QUAG, which learns a query-centric audio-visual representation for moment retrieval, segmentation, and step-captioning. In QUAG, we first devise MSP to gain the audio-visual representation via modeling the global contrastive alignment and local fine-grained interaction between visual and audio modalities. Then, QC2 is designed to learn a comprehensive cognition for the user-preferred content, by modeling temporal-channel filtration from the deep-level query to shallow-level audio-visual content. Extensive experiments show QUAG achieves SOTA performances for the three challenging tasks on the HIREST dataset. Moreover, we test QUAG for query-based video summarization task. The results verify its good generalization capability."}, {"title": "Experiment", "content": "In this supplementary material, we provide the implementation details on the TVsum dataset. In addition, we provide more experimental analyses and discussions about the choice of trade-off parameter. Here, we choose the step-captioning task as the major ablation task, because it requires the model have a comprehensive capability for the cognition, alignment and generation for multi-modal content. Besides, we provide more visualization results and the discussion of failure case.\nImplementation Details The proposed QUAG is trained in an end-to-end manner on the TVsum dataset. On TV-Sum, the clip-level visual features are extracted by using the I3D model (Kay et al. 2017) that is pre-trained on Kinetics 400 (Carreira and Zisserman 2017). The audio features are extracted by the PANN model (Kong et al. 2020) that is pre-trained on AudioSet (Gemmeke et al. 2017). The hidden size is set to 256; the attention layer and attention head are set to 2 and 8, respectively. On TVsum, the batch size and learning rate are set as 4 and 1e-3. The AdamW optimizer (Loshchilov and Hutter 2018) is used to minimize the training loss. The model training is implemented with PyTorch (Paszke et al. 2019) on an RTX 3090 GPU.\nStudy on the Trade-off Parameter A We discuss the effect of trade-off parameter \u03bb in Eq. (15) of main paper. As mentioned in the main paper, \u03bb is a trade-off parameter to balance the importance between one of the three task losses and the global contrastive alignment loss. In Table 6, we find that compared to QUAG without the contrastive"}, {"title": "Ablation Study for Query-Centric Cognition (QC2)", "content": "QC2 aims to use the deep-level query representation to perform a temporal-channel attention on the shallow-level audio-visual representation, thus highlighting user-requested details. The core components in the method are \\(A_{te}\\) and \\(A_{ch}\\), which are temporal and channel attention matrices, respectively. The ablation study on \\(A_{te}\\) and \\(A_{ch}\\) is shown in Table 4 of the main paper, verifying the effectiveness of using both \\(A_{te}\\) and \\(A_{ch}\\). We also investigate the impact of \\(A_{te}\\) and \\(A_{ch}\\) by using only one of them. We report the results on step captioning here, where Base replaces \\(A_{te}\\) and \\(A_{ch}\\) with the mean-pooling operation. Results show that using either component alone improves Base's performance, and the highest gain is obtained when using both temporal and channel attentions."}, {"title": "Qualitative Analysis", "content": "In this supplementary material, we provide more visualize cases of joint moment retrieval, moment segmentation and step-captioning, which are shown in Figure 4-6. These cases include text queries, ground-truth annotations, and the output results generated by our QUAG and the current state-of-the-art method Joint (Zala et al. 2023). From these cases, we can find that the retrieved moments by our QUAG include the less trivial details than those obtained from the Joint. In addition, our QUAG can better predict the step boundaries and generate corresponding step captions. For instance, in Figure 6, the generated step captions by Joint model are redundant, which lack the details in the retrieved moment. By contrast, our QUAG generates \"put glue on top\" that matches the ground-truth captions \"glue the gems\" and \"glue the pin\" (60-83s). The superiority benefits from the fact that our method can learn a comprehensive cognition for user-preferred video content and thus progressively model a query-centric"}]}