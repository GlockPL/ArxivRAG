{"title": "Layout-and-Retouch: A Dual-stage Framework for Improving Diversity in Personalized Image Generation", "authors": ["Kangyeol Kim", "Wooseok Seo", "Sehyun Nam", "Bodam Kim", "Suhyeon Jeong", "Wonwoo Cho", "Jaegul Choo", "Youngjae Yu"], "abstract": "Personalized text-to-image (P-T2I) generation aims to create new, text-guided images featuring the personalized subject with a few reference images. However, balancing the trade-off relationship between prompt fidelity and identity preservation remains a crtical challenge. To address the issue, we propose a novel P-T2I method called Layout-and-Retouch, consisting of two stages: 1) layout generation and 2) retouch. In the first stage, our step-blended inference utilizes the inherent sample diversity of vanilla T2I models to produce diversified layout images, while also enhancing prompt fidelity. In the second stage, multi-source attention swap- ping integrates the context image from the first stage with the reference image, leveraging the structure from the context image and extracting visual features from the reference image. This achieves high prompt fidelity while preserving iden- tity characteristics. Through our extensive experiments, we demonstrate that our method generates a wide variety of images with diverse layouts while maintaining the unique identity features of the personalized objects, even with challenging text prompts. This versatility highlights the potential of our framework to han- dle complex conditions, significantly enhancing the diversity and applicability of personalized image synthesis.", "sections": [{"title": "1 Introduction", "content": "Following the notable success of text-to-image (T2I) generation models, e.g., Stable Diffusion [40], which are trained on large-scale datasets of text-image pairs, there has been increasing interest in personalized text-to-image (P-T2I) generation problems [11]. Given a few reference images containing a specific subject, P-T2I models aim to create new, prompt-guided images that include the personalized subject. To effectively achieve this goal, previous studies [11, 42, 30] proposed to learn new personalized concepts by adjusting pre-trained T2I generation models [40, 36]. These methods have demonstrated promising and visually satisfactory results, leading to the development of versatile applications with practical potential in real-world situations.\nWhen evaluating P-T2I models, there exist two primary criteria. 1) Prompt fidelity examine the extent to which the generated image aligns with the textual description. 2) On the other hand, identity preservation assesses whether the appearance of a subject within the image faithfully maintains the characteristics of the personalized subject. In essence, a trade-off relationship may exist between prompt fidelity and identity preservation [27], thereby P-T2I models often miss to illustrate char- acteristics of personalized concept when it comes to strictly following prompt guidance as well as retaining the details of visual attributes of the concept [34, 16].\nTo overcome the limitations of the previous works and generate personalized images from diverse and challenging prompts while retaining identity characteristics, we propose a novel off-the-shelf P-T2I generation strategy called Layout-and-Retouch. Our core idea involves separating the generation of non-personalized parts (e.g., background and wearables) from the identity of target subjects, thus structuring our generation process into two stages: 1) layout image generation and 2) retouch.\nIn the layout image generation stage, we primarily focus on improving prompt fidelity, irrespective of identity preservation. Specifically, our first aim is to create a layout image that provides structural guidance for the next step. For this purpose, we carefully design a step-blended denoising approach to improve the prompt fidelity of pre-trained P-T2I models by leveraging the diverse expressive capabilities of vanilla T2I generation models. As a result, we obtain layout image features that depict an object visually resembling the target subject, occupying a specific position within the image. The remaining areas are arranged to align with the provided textual prompt.\nIn the retouch stage, the remaining goal is to precisely calibrate the target subject while preserving the background context of the layout image obtained in the previous stage. To achieve this, we propose multi-source attention swap, a straightforward yet effective method that integrates context from the layout image and captures detailed visual appearance from a reference image simultaneously. The principle of the attention swap technique is to transmit information, such as object appearance and style, from reference visual characteristics by replacing the queries, keys, and values in the target denoising steps with those derived from a reference image [5, 12, 13, 9].\nOur two-stage method enables the creation of personalized images with diversified layouts, surpassing existing methods in variety. Extensive experiments demonstrate its capability to produce diverse images and effectively manage challenging prompts."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text-Guided Image Generation and Editing", "content": "T21 generation models. T2I generation models [51, 35, 38, 40, 43, 36] have been extensively studied for their ability to create realistic images from textual descriptions. Recently, diffusion-based models [38, 40, 43] have demonstrated significant success in producing photo-realistic images from user-provided text, offering exceptional controllability. However, models such as Imagen [43], Stable Diffusion [40], and DALL-E series [39, 38, 3], still face challenges in generating personalized images. Specifically, these models struggle when it comes to creating images based on specific or user-defined concepts, where the identities are difficult to accurately convey through text descriptions.\nText-guided image editing. Building on the advancements in T2I generation models, there have been active studies in editing specific images based on text inputs [28, 32, 18, 2, 4, 24, 23, 1]. However, editing images while retaining most of the original content is challenging, as even minor modifications to the text guidance can lead to significant changes. Therefore, early works such as SDEdit [32] and Blended-Diffusion [1] have been developed with limited image editing capabilities, e.g., requiring users to provide a spatial mask to specify the area for editing. To address these limitations, Imagic [23] and InstructPix2Pix [4] have been designed using multi-stage training processes and multiple distinct models, respectively. Despite their effectiveness, these editing models may not guarantee identity preservation when performing complex text-driven image transformations."}, {"title": "2.2 Text-to-Image Personalization", "content": "Addressing the limitations of T2I generation and editing models, personalization models aim to create new images using a few images of an object, while preserving the object's identity consistently.\nOptimization-based methods. Previous studies [11, 42, 26, 15, 49, 16, 8, 48, 14] have explored generating consistent image variations of a specified concept by embedding the concept within the tex- tual domain of diffusion-based models, often represented by a particular token. This approach allows for the controlled generation of images that align closely with a target prompt. Textual Inversion [11] and DreamBooth [42] are advanced methods for creating personalized images through diffusion-based models. Textual Inversion optimizes a textual embedding to integrate a specialized token with the target prompt, while DreamBooth extends this by adjusting all parameters of the denoising U-Net, targeting a specific token and the subject's class category. These methods enhance the precision and contextual relevance of image generation. Research efforts have continually advanced by focusing on tuning key components, such as the cross-attention layer [26, 15, 48], or by incorporating additional adapters [16] to enhance training efficiency and conditioning performance. While these studies have shown promising results, they have faced limitations in preserving the appearance of subjects.\nOff-the-shelf methods. To eliminate the necessity of additional fine-tuning steps, researchers have explored plug-in T2I personalization techniques [5, 33, 10, 29, 53, 46, 19]. These approaches not only enhance computational efficiency but also improve outcomes by explicitly utilizing reference images as an additional condition to capture visual appearance during the inference phase. Technically, these methods manipulate the keys and values of the self-attention module during the denoising process of the U-Net, effectively altering structures and textures [5, 34]. MasaCtrl [5], for instance, employs a dual-path pipeline that synthesizes both a reference and a target image concurrently, replacing the target's keys and values in the self-attention module with those from the reference. More recently, DreamMatcher [34] introduced a technique that adjusts these replaced target values using flowmap-based semantic matching to ensure structural correspondences between the target and reference latent features. However, the P-T2I model alone may present significant limitations in generating diverse layout images, often making it difficult to handle complex prompt conditions. To overcome this issue, we propose a two-stage framework, Layout-and-Retouch, where the vanilla T2I model is responsible for constructing an initial layout, leading to improvements in both prompt fidelity and layout diversity."}, {"title": "3 Proposed Methods", "content": ""}, {"title": "3.1 Preliminaries", "content": "Latent diffusion model. Recent advancements in text-to-image diffusion models, such as Stable Diffusion [40], have achieved robust and efficient image generation by performing the denoising process within the latent space using a pre-trained autoencoder. Specifically, a pre-trained encoder compresses an image into a latent representation z, followed by diffusion and iterative denoising steps using a conditional diffusion model \u03f5\u03b8. During denoising, a text condition y is incorporated through a cross-attention module, guiding the latent representations to align with the text condition. The training objective is formulated as\n$\\mathcal{L}=\\mathbb{E}_{y, z, \\epsilon, i}[||\\epsilon-\\epsilon_{\\theta}(z^{i}, i, \\operatorname{CLIP}(y)||]$"}, {"title": "3.2 Layout-and-Retouch Framework", "content": "Given a reference image Ir and the corresponding text condition yp, our goal is to generate a target image It that adheres to the text condition while retaining the appearance of Ir. One solution might be to leverage a pre-trained P-T2I model with yp; however, we find that the pre-trained P-T2I model tends to synthesize images within a restricted layout space (See Fig. 1 and Fig 4). We hypothesize that the model pre-trained on repetitive layouts limits its capabilities to generate diverse configurations. To address this issue, we propose the Layout-and-Retouch framework, as depicted in Fig. 2."}, {"title": "3.2.1 Stage 1 - Layout Generation", "content": "Step-blended denoising. Our core idea is to utilize the expressiveness of vanilla Stable Diffusion (SD) [40] by having it create the layout in the initial steps. Delegating the task of generating the initial layout to vanilla SD broadens the range and expressiveness of the layouts. We use the text condition yp since vanilla SD lacks prior knowledge of the concept. As shown in Fig. 1, layouts generated by vanilla SD are more diverse than those from pre-trained P-T2I models. By leveraging vanilla SD's ability to generate diverse initial layouts, we facilitate the creation of a broader spectrum of layout images. The initial layout generation is achieved within \u039b1 steps, determined based on empirical studies. Detailed results and analysis of these \u039b1 iterations are in the Appendix.\nGenerating the initial layouts within \u039b1 steps, the subsequent steps adjust the initial object appearance to visually align with the target subject while preserving the initial structure. We use yp and the personalized model for the remaining steps. Let \u03f5\u03b8 and \u03f5 denote the vanilla and personalized denoising networks, respectively, and let zi denote the latent representations at each denoising step i. Formally, computing the latent representations in step-blended denoising can be written as:\n$z_{i-1}=\\left{\\begin{array}{ll}\\text { Sample }\\left(z^{i}, \\epsilon_{\\theta}^{e}\\left(z^{i}, y_{p}, i\\right)\\right) & \\text { if } i<\\Lambda_{1}, \\\\text { Sample }\\left(z^{i}, \\epsilon^{e}\\left(z^{i}, y_{p}, i\\right)\\right) & \\text { otherwise, }\\end{array}\\right.$  for i = T,T \u2212 1, ..., 1,\nwhere Sample operation determines the next latent representations using a predicted noise. Following the denoising steps, a decoding process is performed to generate a layout image Io."}, {"title": "3.2.2 Stage 2 - Retouch", "content": "Multi-source attention swap. Although a layout image I captures some visual characteristics of the target subject, it loses finer details. To enhance the details, we use attention swapping techniques [5, 12] with multiple source images: Io and Ir. Specifically, intermediate variables from cross and self- attention modules derived from I and I are passed to the denoising steps of It. We denote the queries, keys, and values of the cross-attention module as Q, Ke, Ve and those of the self-attention module as Q, K, V in the target denoising process. Additionally, we use yr as a text condition to focus on modifying the visual features of the target object throughout the denoising process.\nAlgorithm 1 describes an overall process of the Retouch stage. During the target path denoising, attention swapping replaces Q, K, V or Q, K, V with variables from the layout and context path. In the early steps, we use Q, K, Ve and Q, K, Vs from the layout path, integrating them into the target path to construct the overall structure of the noisy image. This sharing helps It loosely adhere to Io's structure, making replication easier. In later denoising steps, K and Vs from the reference path replace those in the target path to infuse detailed features from the reference object. Additionally, a composite foreground mask M combines latent representations of the target and layout path within a self-attention layer, where the details on M are provided below.\nAdaptive mask blending. To enhance the visual details of the target subject, we explicitly create a foreground mask to directly blend latent representations of the layout image. One can make foreground masks using cross-attention maps that correlate to the object prompt tokens extracted from the decoder as in [5], where the maps are averaged and then thresholded to produce a binary mask Mc \u2208 Rh\u00d7w, where h and w represent the spatial dimensions of the latent representations. However, we observe that Mc often fail to cover the entire foreground and includes noisy regions. To mitigate this issue, we additionally harness a binary foreground mask MSAM \u2208 RH\u00d7W where H, W denote height and width respectively computed by Segment-Anything [25] given a layout image. We also notice that only using MSAM results in layout-target misalignment, since detailed appearance or locations may shift during the target generation process (details are explained in the Appendix C.2). Therefore, we propose an adaptive mask blending technique to mitigate these issues.\nLet M[x, y] be the value at position (x, y) and Resize(M\u00b2) \u2208 RH\u00d7W be the resized mask from the averaged cross-attention map. We first discard noisy regions of M\u00ba based on MSAM, i.e.,\n$M^{k}[x, y]=\\mathrm{OR}\\left(\\operatorname{Resize}\\left(M^{c}\\right)[x, y], M^{\\mathrm{SAM}}[x, y]\\right),$"}, {"title": "4 Experiments", "content": "In this section, we describe the experimental setup (Section 4.1), including benchmark datasets, baselines, and evaluation metrics to evaluate Layout-and-Retouch and baselines in three aspects:\n1. Layout Diversity: To assess the model's ability to generate diverse layouts.\n2. Identity Preservation: To ensure that objects consistently maintain their detailed characteristics across images.\n3. Prompt Fidelity: To verify that the outputs faithfully adhere to the text guidance.\nComparisons with baselines include both quantitative and qualitative analyses to investigate the add- on effects of Layout-and-Retouch and to compare it with various baselines according to the criteria mentioned above (Section 4.2). The qualitative analysis includes experimental results validating the superiority of Layout-and-Retouch in generating diverse images, along with an ablation study demonstrating the contribution of each component of the proposed method. (Section 4.3)"}, {"title": "5 Conclusion", "content": "In this paper, we present Layout-and-Retouch, a two-stage framework for creating personalized images with prompt-aligned, diverse configurations. Our motivation stems from preliminary observa- tions on the limited layout generation capacity, which results in a weak ability to handle challenging prompts. To address this issue, we introduce the step-blended denoising to diversify image layouts by leveraging the expressive power of the vanilla T2I model. Furthermore, we propose the multi-source attention swap and adaptive mask blending to faithfully transfer the visual features of the reference image while preserving the layout image structure. The combination of these modules enables our framework to effectively enrich image layouts, thereby improving its ability to handle challenging prompts. Extensive experiments demonstrate the superiority of Layout-and-Retouch in generating diverse prompt-aligned images, particularly in handling challenging prompts compared to baselines."}, {"title": "A Discussion", "content": ""}, {"title": "A.1 Broader Impacts.", "content": "The proposed method can significantly enhance personalized image generation models, which may have various social impacts. Positively, it can facilitate the creation of more tailored and relevant content for individuals, improving user experiences across applications like social media, advertising, and digital art. Additionally, It can also assist in personalized education and training, providing custom visual aids suited to individual learning styles. However, there are potential risks, including the misuse of the technology for improving the quality of deepfakes or unauthorized digital impersonations, leading to privacy violations and misinformation. Ethical concerns also arise regarding the data used for personalization, especially if it involves sensitive or personal information. Therefore, ensuring transparency, obtaining consent, and implementing robust security measures are crucial to mitigate these risks."}, {"title": "A.2 Limitations.", "content": "While the proposed method shows promising performances in personalized image generation, it often fails to generate an user-intended image due to the reliance on vanilla Stable Diffusion (SD) model [40]. Specifically, a highly complicated prompt condition that is beyond the capacity of vanilla SD can cause a failure in the layout image, subsequently leading to undesirable target image. As seen in Fig. 5 (a), if the layout image does not adhere to prompt condition, the final output exhibits similar issues. Furthermore, the shape similarity between objects in layout and reference image would affect the identity preservation, as large shape discrepancies are difficult to rectify during the retouching stage. Fig. 5 (b) illustrates an example of generating with different layout images. As can be seen, retaining similar shapes is beneficial for creating a personalized object characteristics in later stage. We believe that using a more robust foundation model such as SDXL [36], to enhance prompt understanding and adherence on prompt conditions, is an effective approach to mitigating this problem. Our future plan includes utilizing SDXL as vanilla model in step-blended denoising and developing a technique to minimize shape difference."}, {"title": "B Experimental Details", "content": ""}, {"title": "B.1 Implementation Details", "content": "We use SD 1.4 [40] as our baseline vanilla foundation text-to-image model, and pre-training personalized models also are conducted based on SD 1.4. Specifically, we utilize all weights of personalized baseline models: Textual inversion [11], Dreambooth [42], Custom Diffusion [26] released in previous method [34]. In experiment, we use a DDIM sampler with a total inference time step to T 50. Empirically, we set \u039b1 = 5, \u039b1 = 3 for normal and challenging prompt respectively and X2 = 10 for all datasets. Additionally, the mask blend operation starts from 31th to the last step. For our GPU setup, we use a NVIDIA GeForce RTX 3090 GPU for all experiments, which consumes about 18GiB memory during inference. It takes about 30 seconds for generating a single image including pre-processing. As for other parameters such as specific layers used for reference, we follow the prior works [5, 34]."}, {"title": "B.2 Prompts for Diversity Experiment", "content": "Since one of our core claims is the diversity of synthesized images, we demonstrate our capabilities using a few different ways. First, we visualize the center point distributions of the target subjects (see Fig. 1), using all 31 prompts curated by ViCo [16] and generating 10 images per prompt. However, when measuring diversity using classic metrics such as Inception Score [44] or visualizing embedding spaces [31], it is insufficient to generate only 10 samples with each prompt. Therefore, we randomly sample 10 prompts from three categories in total, from normal prompts and challenging prompts used in DreamMatcher [34]. These categories include large displacement, occlusion, and novel-view synthesis. We then generate 400 samples per prompt, resulting in a total of 4,000 images to evaluate Inception Scores for the objects depicted in Fig. 4. The list of prompts is provided in Fig. 6 below."}, {"title": "B.3 Analysis on evaluation metrics", "content": ""}, {"title": "B.4 User Study Setup", "content": "We conduct user study with aim of evaluating three aspects: (1) Identity preservation, (2) Prompt alignment, (3) Diversity of outputs. As baselines, we compare (1), (2) with three different prior work: MagicFusion [53], FreeU [46], DreamMatcher [34]. Next, we evaluate (3) with DreamMatcher.\nThe users were given 16 questions to compare identity preservation, 16 questions to compare prompt alignment, and 10 questions to compare diversity of generation. Two different sets of question sets were randomly distributed in an attempt to conduct a faithful user study. 48 users answered to 42 questions, resulting in a total of 2,016 responses. Specifically, we design our questions as follows:\nQ1: Please rank the methods (A,B,C,D) for generating an object most similar to the one contained in the following reference image.\nQ2: Please choose the method for generating an image most similar to the given prompt.\nQ3: Please choose the method that generate more diverse images with the given prompt."}, {"title": "C Additional Analysis on Layout-and-Retouch", "content": "In this section, we analyze our framework Layout-and-Retouch by providing additional experiments with aim of helping to understand the proposed method."}, {"title": "C.1 Analysis on Step-blended Denoising", "content": "Hyperparameter sensitivity. Since synthesizing initial layout is critical for Layout-and-Retouch, we empirically analyze the effect of the number of vanilla SD iterations in denoising steps. As shown in Fig. 8, we observe that an early stop of vanilla SD denoising leads to higher identity preservation (IDINO, ICLIP), while simultaneously lowering prompt fidelity (TCLIP, ImageReward). On the other hand, as initial layout generation steps increase, identity preservation drops rapidly while the prompt fidelity improves. This demonstrates a trade-off relationship between identity preservation and prompt fidelity as previous work [27] pointed out. Empirically, we determine our optimal \u039b1 as five, where identity preservation does not dramatically drop and prompt fidelity is beyond its criterion confirmed in qualitative observation.\nCross-attention map visualization. We visualize a cross-attention (CA) map during step-blended denoising process in Fig. 9. The two images shown in the left side are final images generated using the same seeds, using a solely vanilla denoising and step-blended denoising with \u039b1 = 5. We can observe that the elements of the image (e.g., dog pose and background) are remarkably similar, suggesting that five denoising iterations suffice to convey a diverse layout from the vanilla model to subsequent steps. Fig. 9 shows detailed visualizations of cross-attention map, which illustrates the alterations in the dog's form throughout the denoising process. This visualization highlights that modifying only the initial steps with the vanilla model is sufficient for generating diverse and promptly aligned layouts. Subsequent denoising steps employing the pre-trained personalized model further enrich the layout image with ample information about user-specific object"}, {"title": "C.2 Qualitative Analysis on Mask Variants", "content": "In Fig. 10, we compare our adaptive mask blending method and the mask extracted with SAM [25] and the cross-attention map after thresholding. In the main paper, we argued that utilizing only one of the two masks would result in either noisy region interruption or region misalignment. As seen in Column 2 of Fig. 10, M\u00ba often fail to retrieve the entire foreground object and contains noisy region, resulting in degraded performance in retaining visual details and undesirable artifacts (See red box). On the other hands, Column 2 of Fig. 10 shows that when using only MSAM to define the target region, it often fails to identify important object part, impairing the necessary components of the layout image. To address these issues, we apply adaptive mask blending, successfully overcoming them by finding a balanced mask region and strength."}, {"title": "C.3 Additional qualitative results", "content": "We additionally report qualitative results of our work as follows:\n1. Additional qualitative comparisons with plug-in based baselines across various objects (Fig. 13, 14, and 15.\n2. Additional qualitative comparisons by adding the proposed method on baselines (Fig. 16, 17, and 18).\n3. Additional comparison with DreamMatcher on image diversity (Fig. 19)"}]}