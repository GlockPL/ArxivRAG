{"title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Gaurav Verma", "Harneet Singh Khanuja", "Jose Suarez Campayo", "Zihang Li", "Michael L. Birnbaum", "Munmun De Choudhury"], "abstract": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, we introduce the Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment (ADRA) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies. Our analyses show that LLMs struggle with understanding the nuances of ADRs and differentiating between types of ADRs. While LLMs align with experts in terms of expressed emotions and tone of the text, their responses are more complex, harder to read, and only 70.86% aligned with expert strategies. Furthermore, they provide less actionable advice by a margin of 12.32% on average. Our work provides a comprehensive benchmark and evaluation framework for assessing LLMs in strategy-driven tasks within high-risk domains.", "sections": [{"title": "Introduction", "content": "Adverse Drug Reactions (ADRs)\u00b9 caused due to psychiatric medications are a leading cause of hospitalizations among individuals with mental health conditions, accounting for 51.9% to 91.8% of cases, as reported in previous studies (Angadi and Mathur, 2020; Ejeta et al., 2021). With nearly 70% of individuals worldwide having limited or no access to mental health professionals (Kazdin and Rabbitt, 2013), many patients increasingly turn to social media platforms such as Reddit to share experiences and seek advice (Lee et al., 2017; De Choudhury et al., 2014). Yet, around 35% of posts on mental health-related subreddits go unanswered, leaving many without adequate support (Guimar\u00e3es et al., 2021). Further, while social media offers a platform for seeking assistance with resolving ADR queries, responses are frequently provided by individuals lacking expertise, raising concerns about the reliability of the information shared (Vosoughi et al., 2018; Wang et al., 2019). Hence, as conversational AI platforms (such as ChatGPT) gain prominence, more individuals are turning to these systems for healthcare-related queries, including those about psychiatric medication and ADRs.\nGiven the current limitations of healthcare systems and social media platforms, alongside the growing capabilities of LLMs in mental health-related tasks (Yang et al., 2023, 2024; Singhal et al., 2023b), LLMs have the potential to bridge the gap in online discussions by providing high-quality, contextual responses to ADR queries related to psychiatric medications. While previous studies have focused on detecting ADRs using deep learning methods, these efforts have primarily addressed non-mental health-related ADRs (Mesbah et al.,"}, {"title": "Data Collection and Curation", "content": "We begin by providing the details of the data collection and filtering pipeline. We used publicly available data in English from Reddit spanning a one-year period (January 2019 - December 2019) obtained from Pushshift Reddit Dataset (Baumgartner et al., 2020). While the broad timeframe ensures large enough data before filtering, the specific period also (a) predates the use of generative Al in day-to-day lives and (b) the knowledge cutoff for all LLMs used in our evaluation. This allows for a fairer comparison between human experts and LLMs and also ensures minimal presence of machine-generated content on Reddit.\nFollowing the past work (Mesbah et al., 2019b; Saha et al., 2019; Chancellor et al., 2019), we selected 10 subreddits that focus on mental health-related issues or provide a platform for users to ask medical queries (such as r/depression and r/askdocs; see Appendix A for the complete list). To extract relevant posts, we compiled a set of 297 FDA-approved psychiatric medications provided by Saha et al. (2019). Further, to detect expressions of adverse symptoms in post titles and texts, we employed HealthE (Gatto et al., 2023), a specialized named entity recognizer for identifying healthcare and medical entities. By combining the psychiatric medication names with the entities given by HealthE, we obtained 19,252 Reddit posts.\nFiltering based on mentions of psychiatric medications and adverse symptoms provides a rich"}, {"title": "The Psych-ADR Benchmark", "content": "LLM-assisted expert annotations: We conducted expert-led annotations to validate the ADR labels generated by GPT-3.5 and to categorize the specific type of ADR described in each post, if applicable. Given the complexity and time-intensive nature of the human annotation process, we randomly selected 250 posts-consisting of both ADR-labeled and no-ADR-labeled posts as identified by GPT-3.5 for experts to annotate. Based on our discussions with the collaborating medical experts and drawing on the classification provided by Edwards and Aronson (2000), we categorized the ADRS into five granular types- 1) dose-related ADR, 2) non-dose ADR, 3) dose- and time-related ADR, 4) time-related ADR, and 5) withdrawal ADR. We collaborated with three expert annotators - two doctors, and one medical student, all with backgrounds in psychiatry with high proficiency in English. Based on the criteria provided for classifying a post as expressing ADR (Appendix B and C), they annotated each post to determine whether the post described an ADR, and if so, which category of ADR it belonged to along with providing reasoning for it (details related to the annotation tool in Appendix D).\nThe annotation task proved to be challenging for the annotators, with an average time of ~7.2 minutes taken to annotate each post due to the complexity and subjectivity inherent in detecting adverse drug reactions. All three annotators agreed on the labels for 48% of the posts. To address the disagreements, we conducted a second round of annotations in which all three annotators collaboratively resolved disagreements, resulting in the final"}, {"title": "Model Selection & Implementation", "content": "We conduct our analysis for the research questions with a total of 9 proprietary and open-weights LLMs. For proprietary models, we evaluate GPT-40 (OpenAI-GPT-40, 2024), GPT-4 Turbo (Achiam et al., 2023), Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku (Anthropic-Claude, 2024). For open-weights models, we evaluate LLama-3.1 405B Instruct-Turbo, LLama-3.1 70B Instruct-Turbo (Meta-LLama3.1, 2024) and specialized medical LLMs \u2013 Llama3-Med42-v2 70B (Christophe et al., 2024) and Llama3-OpenBioLLM 70B (Ankit Pal, 2024). The choice of these models stems from their reported performance in different general-purpose and medical benchmarks (Abbas et al., 2024; Nori et al., 2023b; Chen et al., 2023; Anthropic-Claude, 2024).\nPrevious studies have recommended lower temperatures for detection and labeling tasks to ensure more consistent outputs, while higher temperature"}, {"title": "RQ1: Detecting Adverse Drug Reaction", "content": "For this task, we evaluated LLMs on detecting expressions of adverse drug reactions using the Psych-ADR benchmark. The evaluation involved two separate tasks: (1) identifying the presence or absence of concerns realted to ADRs in the 239 Reddit posts, and (2) classifying the type of ADR into one of five pre-defined categories for the 133 instances labeled as expressing ADRs in Psych-ADR benchmark. In both tasks we evaluated models using the zero-shot and few-shot variants of the chain-of-thought (CoT) prompting (Wei et al., 2022). Detailed prompts and classification criteria are provided in Appendix B and C.\nDue to the wide variety of medications and symptoms in Psych-ADR benchmark, we evaluated two different example sampling strategies for few-shot prompting. For this, we generated text embeddings for each Reddit title and post using Text-embedding-3-large (OpenAI-TextEmb-3-Large, 2024). Using cosine similarity, we retrieved the five most-similar and five least-similar posts for each example. Table 2 presents the accuracy and weighted F\u2081 scores for models in the ADR detection and ADR multiclass classification tasks."}, {"title": "Zero-shot prompting on Psych-ADR", "content": "Larger models typically perform better for ADR detection tasks, but this trend does not hold for ADR multiclass classification. As expected, larger models (by parameter size) outperformed their smaller counterparts in the ADR detection task within their respective families, with Claude 3 Opus achieving the highest accuracy at 77.41%, followed by GPT-40 and GPT-4 Turbo at 72.03%. Interestingly, specialized medical models (OpenBioLLM-Llama3-70B and Llama3-Med42v2-70B) struggled in this task. However, for ADR multiclass classification, we did not observe any clear pattern between model size and performance. GPT-4 Turbo was the best performing model with an accuracy of 57.58%, followed by Llama3-Med42v2-70B at 56.40%. All models struggled with multiclass classification, likely due to the complexity of distinguishing between ADR types. Additionally, aligning with prior research, observed results in the multiclass classification showed that larger models do not always excel in specialized tasks (Kanithi et al., 2024).\nModels exhibited a \u201crisk-averse\u201d tendency, and prone to commit false-positive errors. In both ADR detection and multiclass classification tasks, all models displayed \u201crisk-averse\" behavior, often mislabeling posts without ADRs as positive for ADRs. In zero-shot settings, Claude 3 Opus had a false-positive rate of 42% for \u2018ADR-No' labels, while Claude 3 Haiku's false positive rate was as high as 97% (see Appendix F). Similarly, in ADR multiclass classification, models struggled to distinguish between non-dose-related, dose-related, and time-related ADRs. GPT-4 Turbo misclassified 51% of non-dose-related and 50% of time-related ADRs in zero-shot settings. This risk-"}, {"title": "Few-shot prompting on Psych-ADR", "content": "In-context learning enhances model performance but not in every case. We observed the in-context learning in general improved performance of models for both ADR detection and multiclass classification tasks, with a more significant impact on the latter task. For multiclass classification, we observed an average increase of 18.14 and 23.06 points in weighted F\u2081 score among model performance using least-similar and most-similar example prompting respectively. However, this pattern was not observed in ADR detection task. Claude 3 Opus outperformed other models in the ADR detection, achieving an F\u2081 score of 76.44 with zero-shot prompting. In ADR multiclass classification, Llama-3.5-405B performed best with most-similar examples (F1 76.69). For analyzing the impact of providing examples in the ADR detection task, we observed that some models, such as Claude 3 Haiku showed an average improvement of F\u2081 score (16.49 points), whereas we did not observe such a trend for models such as GPT-4o, Claude 3 Opus in few-shot settings. The stochastic nature of LLM generation, coupled with the inability to learn nuances from examples in the \"ADR-No\" class, may be a contributing factor to this issue. This was further confirmed as we noted that even in few-shot settings, models exhibited \"risk-averse\" behavior with high false-positive rates, indicating that providing examples could not effectively compensate for the lack of \"lived-experience\" in the models. This was the major reason behind models failing to achieve the expected gains in detecting ADR.\nImpact of choosing similar or diverse examples depends upon the task. While the performance boost in the ADR multiclass classification task could be attributed to the predominance of non-dose-related ADRs, the comparatively smaller performance gains observed when models were presented with the five least similar examples suggest that models were able to grasp the contextual information presented through the examples and capture the nuances of various ADR types. However, no such pattern was observed in the ADR detection"}, {"title": "RQ2: Alignment between human and AI feedback", "content": "Evaluation of long-form text generation is an open problem and involves many challenges like isolating the stylistics from the semantics. However, in the context of responses to ADR queries, we propose abstracting out the LLM generations and ground-truth expert responses to four key components - (1) emotion and tone, (2) text readability, (3) harm reduction strategy, and (4) actionability of proposed strategies. Via this abstraction to key components, our alignment evaluations focus on specific aspects that contribute towards an ideal response to ADR queries. We explain the importance of these components below and the methodology for evaluation.\nEmotional and tone alignment: Emotional intelligence is regarded as a key factor in healthcare, fostering strong therapeutic relationships that drive meaningful change (King Jr, 2011). Therefore, LLM-generated responses should align with expert-written responses in terms of tone and expressed emotion. To assess this, we used Empath (Fast et al., 2016), a widely-used lexicon-based tool, focusing specifically on 8 relevant emotional and tonal categories identified from prior literature (Riess and Kraft-Todd, 2014; Mechanic and Meyer, 2000). We analyzed the distribution of these categories in LLM-generated and expert responses, and quantified their differences using Kullback-Leibler (KL) divergence to measure alignment of expressed emotions and tone in the LLM and expert responses.\nText readability alignment: Past studies have shown that health literacy is strongly correlated with patient outcomes (Wolf et al., 2005). A major factor contributing to lower health literacy is the communication barrier between patients and healthcare providers, which often arises from the complexity of medical text, including the writing style and choice of terminology (DuBay, 2004). Hence, the responses produced by LLMs should be easily readable and be of comparable to that of the expert-written responses. To assess this, we used SMOG index (Mc Laughlin, 1969), a popular readability index to assess health literacy material."}, {"title": "Harm reduction strategy alignment", "content": "In cases of adverse reactions to psychiatric medications, suggesting safe medical interventions is crucial to prevent further harm. We operationalized these interventions using harm reduction strategies (HRS) (Single, 1995), aimed at minimizing the negative effects of medications that one is reliant on. Ideally, LLMs should propose strategies that align with the expert's responses.\nTo compare the harm reduction strategies suggested by LLMs and experts, we took inspiration from methods for entailment and factuality evaluation in long-form texts (Min et al., 2023; Wei et al., 2024; Kamoi et al., 2023). First, we extracted atomic HRS from LLM responses by prompting GPT-40 (OpenAI-GPT-40, 2024). Since some extracted strategies were redundant, we used a few-shot approach to combine those that suggested the same overall approach but differed in specific details to get the final set of HRS for each response (examples in Table 10). To check for the robustness of the extraction and combination method, we conducted a round of human evaluation with 4 annotators. Using a random sample of 40 responses for each task, we evaluated 193 strategies for the extraction and 174 strategies for combination, and obtained a correlation score of 92% and 90% respectively with LLM evaluation.\nWe then evaluated alignment of HRS for each LLM-expert response pair using two methods. First, we used AlignScore (Zha et al., 2023), a widely-used text alignment method providing a score between 0 and 1 based using a fine-tuned ROBERTa-large model (Liu et al., 2019). We computed AlignScore for each strategy from the LLM response against the expert response. We obtained a response-level AlignScore by averaging the scores across all HRS for the response. Second, for a more interpretable alignment score, we prompted GPT-40 with in-context examples to reason and classify if a strategy is aligned with the expert's response. We computed a response-level GPT-40 score by computing the percentage of aligned HRS over total number of HRS. These two approaches ensured robustness by covering both a continuous alignment score and one based on reasoned binary alignment labels. We conducted another round of human evaluation for the GPT-4o score, where four annotators annotated 40 responses, achieving a 95% correlation with GPT-40's score and reasoning. Prompts for LLM-based tasks are presented in Table 12, 13 & 14, and human evaluation details are presented in Appendix I.1."}, {"title": "Actionability alignment", "content": "Prior work in health communication has recognized the importance of actionability in the responses of healthcare professionals to enable greater engagement and encourage increased action from patients (Sharma et al., 2023). To this end, we designed an approach to measure the alignment between LLM responses and expert responses along the actionability dimension. We first decomposed actionability into specific sub-dimensions while working with clinical experts and using the guidelines presented in the Patient Education Materials Assessment Tool (PEMAT; AHRQ). Harm reduction strategies recommended by experts and LLMs should be: (i) practical, (ii) contextually relevant, (iii) specific, and (iv) clear. We present concrete definitions for each of the sub-dimensions in Appendix J.\nTo operationalize the quantification of actionability alignment, we prompted the GPT-40 model using carefully selected in-context learning examples and chain-of-thought prompting. The GPT-40 model considers the ADR post made by the user and assigns a binary label to each harm reduction strategy based on whether or not the target sub-dimension of actionability is present in the strategy (0: absent; 1: present). To validate the labels assigned by the GPT-40 model, the medical experts reviewed the rationales generated for detecting each of the sub-dimensions of actionability in 100 harm reduction strategies, and agreed with 91 of them for practicality, 94 for relevance, 82 and 89 for specificity and clarity, respectively. Overall, the extent of the agreement between experts and GPT-40 rationales reinforced the validity of the labels assigned to the 4 sub-dimensions of actionability. Following this, for responses generated by the LLMs, we computed the fraction of harm reduction strategies that are aligned with the HRS and also demonstrate presence of a certain sub-dimension of actionability. For instance, for the practicality dimension, the LLM-generated HRS are scored as:\nPracticality_LLM = (# aligned & practical HRS) / (# total HRS)\nIt is worth emphasizing that the constraint of only considering aligned HRS within the LLM-generated responses enforces a penalty for generating unaligned HRS while computing actionability. Since expert responses are inherently always aligned, their HRS do not undergo such a penal-"}, {"title": "Results", "content": "Emotional and tone alignment. Figure 2 presents the mean KL-divergence score for the distribution of 11 Empath categories between LLM responses and expert-written response. A \u03c7\u00b2 test was conducted to assess the differences in category distributions, and the p-values were non-significant across all models, indicating that the models' responses were not significantly different from the expert-written responses in terms of emotions expressed and the tone used. Further, we observed that larger and more capable models from the Llama and Claude families showed greater alignment with expert responses across different emotional and tone related categories. Interestingly, Llama-3 Med42v2 70B performed the worst. This could be attributed to the fact that a major portion of dataset used for instruction fine-tuning for this model was obtained from the medical and biomedical literature, which may not prioritize emotional communication while providing responses (Christophe et al., 2024).\nText readability alignment. Figure 3 presents the box plot for LLM and expert response SMOG scores. As observed, LLM-generated responses tend to be more complex, reflected in higher SMOG scores compared to those written by experts (SMOGmean11.02) in the Psych-ADR benchmark. Welch's t-test (Welch, 1947) further revealed that the SMOG scores of expert-written responses were significantly lower than those of any LLM-generated responses. We also observed that more capable models produced more readable responses (with Claude 3 Opus being an exception). Similar to the findings on emotional alignment, Llama3-Med42v2-70B showed the lowest alignment with the expert-written responses, producing the most complex responses, likely due to a major portion of instruction-tuning data coming from medical and biomedical scientific literature. In contrast, OpenBioLLM-Llama3-70B outperformed many proprietary models, likely due to the custom dataset used for fine-tuning.\nHarm reduction strategy alignment. Table 3 presents the mean response-level AlignScores and GPT-40 scores for alignment of harm reduction strategies of LLMs with the expert's responses. In line with results from zero-shot ADR classification in RQ1, more capable models Llama 3.1-405B Instruct and Claude 3.5 Sonnet in their respective families tended to produce strategies less aligned with the expert than their smaller counterparts, validating a previously observed pattern of LLM performance in responding to open-ended clinical questions (Kanithi et al., 2024). While the open-weights models performed on par or better than proprietary models across both alignment metrics, the best-performing medical model (OpenBioLLM-Llama3-70B) aligned with expert harm reduction strategies for 70.86% of the cases, highlighting the need for further fine-tuning for specialized domains such as psychiatry. Qualitative analysis of non-aligned HRS revealed that most focused on general lifestyle advice, such as maintaining a healthy diet and sleep"}, {"title": "Broader Implications and Limitations", "content": "Responding to ADR queries is challenging due to the complexity of mental health conditions, symptoms, and medication effects. The results from the analyses of RQ1 and RQ2 surface these challenges, revealing nuanced patterns that highlight the intricacies involved, and hence findings from this work have several key implications:\nGoing beyond the choice-based medical benchmarks. LLMs have achieved near-perfect scores on popular medical benchmarks (Nori et al., 2023a; Singhal et al., 2023b), however, these evaluations typically focus on multiple-choice or case-based questions,which don't reflect the nuanced understanding required in real-world scenarios like mental health. Despite their strong performance on medical tasks, Llama3-Med42v2-70B and OpenBioLLM-Llama3-70B struggled with detecting ADRs and providing aligned and actionable HRS, highlighting the need to move beyond standard benchmarks towards more holistic alignment evaluation paradigms.\nFocusing on empowering experts rather than replacing them. While LLMs did not match expert performance in our analysis, they showed a potential to enhance healthcare by providing clearer, more actionable responses. Given the global shortage of mental health professionals (Kazdin and Rabbitt, 2013), LLMs could expand access to mental healthcare and support experts with further fine-tuning and alignment with expert reasoning.\nWhile novel, it is important to acknowledge the limitations of our work. While the proposed Psych-ADR benchmark is the first to focus exclusively on ADRs related to psychiatric medications, the number of examples used for evaluating LLMs is limited, which may not capture the full range of ADRs associated with these medications. We also acknowledge that expanding a benchmark like ours presents challenges due to the time-intensive nature of annotation and response writing, compounded by the subjectivity and complexity inherent in this domain. Additionally, while the responses were provided by a highly experienced doctor, variations in clinical opinions are possible given the subjective nature of ADR assessment in psychiatric medication contexts. Despite these limitations, we believe that the proposed Psych-ADR benchmark provides a valuable resource for further research, offering a robust starting point for the study of ADRs in psychiatric medications.\nWe also acknowledge certain limitations in the ADRA framework. Although we aimed to compare responses across a set of relevant emotions and tones, our approach relies on a lexicon-based method, which may sometimes miss semantic meaning of the responses. Additionally, the harm reduction strategy alignment in our framework excludes strategies suggested by LLMs that are not present in the expert responses. However, there may be cases where the LLM's proposed strategy is a viable option according to other clinicians, but due to the open-domain nature of the problem and the lack of a verified data source, we were unable to evaluate the correctness of such strategies. Despite these challenges, our work provides a robust framework for assessing the capabilities of LLMs in high-risk strategy-driven domains."}, {"title": "Ethical Considerations", "content": "We collected public domain social media data from a publicly available dataset which allowed us to use the resource for non-commercial purposes. We further ensured that all data used was de-identified and did not contain any offensive content. As our study involved working with retrospective data without direct interaction with the authors of the posts, the Institutional Review Board (IRB) classified it as non-human subjects research, exempting it from IRB approval. Still, we adhered to established best practices for working with social media data, as recommended in the literature (Weller and Kinder-Kurlanda, 2016, 2015). In line with Reddit's data-sharing guidelines and relevant data-use agreements, we will provide access to the benchmark exclusively comprising Post IDs and annotations, to interested researchers upon acceptance. Further, we will make the code used in this work publicly available on a GitHub repository upon acceptance.\nOur study presents a systematic approach for evaluating LLMs for addressing ADR related queries from psychiatric medication use, and hence does not inherently pose direct risks. However, it is important to emphasize that better performance on Psych-ADR benchmark should not be interpreted as an indication of increased capabilities in real-world applications. Instead, these results should be complemented with thorough human evaluation to ensure the reliability and safety of models."}, {"title": "List of subreddits", "content": "We use the following list of subreddit to collect data for further filtering: 'r/depression', 'r/anxiety', 'r/bipolar', 'r/BPD', \u2018r/schizophrenia', 'r/autism','r/askdocs', 'r/diagnoseme','r/mentalhealth','r/medical_advice'.\nThe choice for these subreddits stems from past works (Mesbah et al., 2019b; Saha et al., 2019; Chancellor et al., 2019)."}, {"title": "ADR Detection Scenarios and Prompts", "content": "Posts on social media platforms discussing adverse drug reactions related to psychiatric medications are often written by individuals with limited or no medical knowledge. As a result, the level of certainty in expressing concerns about potential side effects can vary significantly. Some posts are more assertive, while others express uncertainty. For example, individuals may report experiencing adverse symptoms after taking psychiatric medications, be unsure if these symptoms are caused by the medication, or inquire whether their symptoms could be related to the drugs they are taking. Additionally, some posts may express concerns about possible future side effects of starting a new psychiatric medication. These scenarios were used as examples to guide both annotators and language models. At last, both LLMs and experts were asked to determine whether the concern could be related to ADR or not based on their experience. Table 6 and Table 7 present prompts used with LLMs for detecting cases of ADR from psychiatric medication."}, {"title": "ADR Multiclass Classification Definitions and Prompts", "content": "We provided the same definitions to both the LLMs and expert annotators for the annotation task. To independently evaluate the LLMs, we focused only on posts annotated as expressing ADR-related concerns (N =133) in the Psych-ADR benchmark. Adverse drug reactions (ADRs) related to a psychiatric medications can be classified in one of the following classes:\n\u2022 Dose-related: These are the reactions that are directly related to the dosage of the psychiatric medication.\n\u2022 Non-dose-related: These are the reactions where any exposure of psychiatric medication is enough to trigger an adverse reaction.\n\u2022 Time-related: These are the reactions that are related due to prolonged use in a psychiatric medication which doesn't tend to accumulate.\n\u2022 Dose-and-time-related: These are the reactions that are related due to dose accumulation, or with prolonged use of the psychiatric medication.\n\u2022 Withdrawal: These are the reactions that are related to the undesired effects of ceasing or stopping the intake of the psychiatric medication.\nTable 8 and 9 present the LLM prompts used for zero- and few-shot ADR multiclass classification."}, {"title": "Annotation Task Details", "content": "We collaborated with a team of four medical experts (three doctors and one medical student), all of whom are co-authors of this work. Hence, we did not provide any additional compensation for the annotation task. Furthermore, Institutional Review Board (IRB) approval was obtained before the annotation task. To facilitate the annotation process, we developed a custom web-based tool specifically for annotating the Psych-ADR benchmark. Figure 4 presents the interface of the annotation tool used for the data annotation purpose. We conducted a preliminary round of test annotations to familiarize the annotators with both the criteria and the annotation tool. For the further rounds, the average Fleiss' kappa inter-annotator agreement was ~ \u03ba =0.33, with all three annotators agreeing on the labels for 48% of the posts, indicating a fair level of agreement (Landis JRKoch, 1977). These results are consistent with previous research, which has reported similar inter-annotator agreement scores for tasks of comparable difficulty (Karpinska et al., 2021; Saha et al., 2021).\nFigure 5 presents the sample of structure of the expert responses provided in the Psych-ADR benchmark. The most experienced doctor on the collaborating team provided the responses, with each taking ~8 minutes to answer on average."}, {"title": "Model Details, Hyperparameters, and Compute", "content": "We use API-based model inference for GPT4-Turbo, GPT-40, Llama 3.1-70B Instruct, Llama 3.1-405B Instruct, Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet. We used Azure OpenAI"}, {"title": "ADR detection and multiclass classification results", "content": "We analyzed the class-wise distribution of predicted labels for the ADR detection and ADR multiclass classification task. Figure 6 and Figure 7 present the confusion matrices for the ADR detection and ADR multiclass classification task in zero-shot setting. Analyzing Figure 6 we observed that all models performed exceptionally well in cases of ADRs with 4 out of 9 models correctly detecting all examples in the 'ADR-Yes' class. However, all models struggled in correctly classifying cases of 'ADR-No' class with the best model (Claude 3 Opus) misclassifying 42% examples. This qualitatively implied that models showed lack of lived experience and a \u201crisk-averse\u201d behavior. Analyzing the ADR multiclass classification results in zero-shot setting (Figure 7), we observed that Withdrawal ADRs were correctly classified more than 90% times by all models. However, all LLMs struggled between the Dose and Non-Dose related ADRS and failed to understand the nuances between the two types of ADRs."}, {"title": "ADR Detection and Multiclass Classification Error Analysis", "content": "We conducted a qualitative error analysis for misclassified examples in the ADR detection and multiclass classification tasks, focusing on Claude 3 Opus and Llama 3.1 405B in few-shot settings. Upon the analysis, two major themes emerged: (a) lack of lived experience and (b) incorrect assumptions about potential ADR queries. In the first set of errors, models adhered too rigidly to prompt rules, missing other possible symptom explanations. In the second set of errors, models often confused posts seeking emotional support with ADR-related queries. The model misjudged a person's use of social media to share their feelings as a potential ADR-related query.\nFor cases where the model demonstrates a lack of lived experience, we observe expert quotes such as \"Patient is having swallowing difficulties which seems to be due to GI issues rather than medication\" and \"We can not say she has an ADR since she is actually sleep deprived, plus slightly (minimally) overweight, so we should need to assess if she actually has sleep apnea.\". These quotes indicate that the model is quick to label a post as ADR and can overlook some other contributing factors for the symptoms, while the experts are cautious while la-"}, {"title": "Methodological Details for Emotional and Tone Alignment", "content": "To compute the emotional and tonal alignment, we lemmatized the Empath lexicon, expert responses and LLM responses using 'en_core_web_sm' model on SpaCy (Montani et al., 2023). This preprocessing step ensured consistency in comparing the linguistic features across the responses with the Empath categories."}, {"title": "Harm Reduction Strategy Alignment Qualitative Analysis", "content": "We qualitatively analyzed alignment between the LLM's harm reduction strategies and those suggested by the expert. One pattern observed across all LLMs was that in addition to their main response to the issue, they suggested non-pharmacological advice on lifestyle changes involving sleep hygiene (\u201cPrioritize adequate sleep.\u201d), diet (\u201cMaintain a balanced diet.\u201d) and mindfulness techniques such as meditation (\u201cPractice stress-management techniques like deep breathing.\u201d\") and journaling (\u201cKeep a journal of your symptoms.\u201d). On the other hand, expert answers tended to focus on addressing the symptoms or questions involving the medication. Harm reduction strategies suggested by LLMs related to medication were often paired with an action to discuss with a doctor about the recommendations before committing to them"}, {"title": "Actionability Criteria", "content": "We present the concrete definitions for each of the sub-dimensions of actionability.\n\u2022 Practicality: The proposed strategy should clearly identify at least one action the user can take. Further, it should be contextually feasible/practical, considering their personal circumstances, such as physical ability, financial resources, and time constraints.\n\u2022 Contextual relevance: The provided strategy should be relevant and should contribute to"}]}