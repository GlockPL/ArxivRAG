{"title": "Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey", "authors": ["Genta Indra Winata", "Hanyang Zhao", "Anirban Das", "Wenpin Tang", "David D. Yao", "Shi-Xiong Zhang", "Sambit Sahu"], "abstract": "Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area.\nKeywords: preference tuning, human preference, reinforcement learning, multi-modality, multilingual, large language models, vision language models, speech language models, generative models, survey, DPO, RLHF.", "sections": [{"title": "1 Introduction", "content": "Learning from human feedback is a crucial step in aligning generative models with human preferences to generate output that closely resembles human speech and writing. Despite the powerful learning capabilities of generative models in self-supervised learning, these models frequently misinterpret instructions, leading to hallucinations in generation (Ji et al., 2023a; Yao et al., 2023a). Additionally, ensuring the safety of the generated content remains a significant challenge for these models. Extensive research on preference tuning"}, {"title": "2 Preliminaries", "content": "This section outlines the preliminaries of preference tuning, including the formal definitions of the tasks and the notations used throughout this paper. Additionally, we provide a taxonomy for classifying preference tuning methods."}, {"title": "2.1 Tasks and Definition", "content": "In general, the entire preference tuning mechanism for generative models can be formulated as a RL problem described as follows."}, {"title": "2.1.1 RL FRAMEWORK CONCEPTS", "content": "Policy Model The policy model $\\pi_{\\theta}$ is a generative model that takes in an input prompt x and returns a sequence of output or probability distributions y. We define a generative model as a policy model $\\pi_{\\theta}$ where it is parameterized by $\\theta$ with a policy model $\\pi$. Given a prompt x, a generative model generates an output y as following:\n$\\pi_{\\theta}(y|x) = \\prod_t \\pi_{\\theta}(y_t|x, y_{<t}),$\n(1)\nwhere $y_t$ is the t-th token in the response and $y_{<t}$ is tokens in the response before $y_t$. For example, for the text-based tasks, the input prompt is a text sequence x and the output is a probability distribution over text vocabulary of LLM y; and for the vision-text-based tasks, such as text-to-image tasks, the input x is the text sequence, and y is the generated image."}, {"title": "Reward Model", "content": "The reward model (RM) processes both the input x and the target y, passing them through the model to obtain a reward $r_{\\theta}(y|x)$, which reflects the notion of preferability. This preferability score can also be interpreted as a relative score assigned to the target y given the input x. Less preferred outcomes receive a lower score compared to more preferred samples."}, {"title": "Action Space", "content": "The action refers to all tokens corresponding to the vocabulary of generative models. For text tasks, the action space encompasses the entire vocabulary of the LLM. For vision tasks (similarly for speech tasks), the action space consists of real values representing the image, for example, the next hierarchy in diffusion generative models (if understanding diffusion models as Hierarchical Variational Autoencoders (Luo, 2022))."}, {"title": "Environment", "content": "The distribution encompasses all possible input token sequences for generative models. In text-based tasks, these input token sequences correspond to text sequences, highly depending on the sampling methods for the inference. In vision tasks, they correspond to possible images."}, {"title": "2.1.2 PREFERENCE DATA", "content": "In the preference tuning pipeline, we utilize the supervised data $\\mathcal{D}_{sft}$ and the preference data $\\mathcal{D}_{pref}$. We denote the supervised data $\\mathcal{D}_{sft} = [(x^1, y^1), \\ldots, (x^M, y^M)]$ as a list of input and label pairs. Specifically for the text SFT data, x can be represented as prompts. The prompt $x^i = (I^i, F^i, Q^i)$ consists of the concatenation of an instruction $I^i$, few-shot samples $F^i$, and a query $Q^i$. Then, we denote the preference data $\\mathcal{D}_{pref} = [(x^1, y_w^1, y_l^1), \\cdots, (x^N, y_w^N, y_l^N)]$ a list of input $x^i$ with preferred response $y_w^i$ and dispreferred response $y_l^i$, and they are either sampled from the reference policy model $\\pi_{ref}$ or collected by human annotation. Generally, given the preference data, we can obtain a reward r associated to the response with the input."}, {"title": "2.1.3 TERMINOLOGY AND NOTATION", "content": "Table 1 lists the common notations used in this survey paper. The table serves as a quick reference guide for understanding the mathematical expressions and technical terms used throughout the paper."}, {"title": "2.2 Taxonomy", "content": "We define the following categories for all of the preference tuning approaches as shown in Table 2. Figure 1 shows the five categories we study in this survey paper and described in the following:\nSampling Likewise in the literature of RL, we categorize the methods based on how we sample the data and use them to train or obtain the reward: offline and online human alignments. The categorization is related to how we compute the reward and use it in the policy models. In online human alignment setting, the agent that collects a batch of examples by interacting with the environment and uses them to update the policy. The reward of the examples can be collected by the reward model or samples generated by the policy model. While for the offline human alignment setting, the data are collected from"}, {"title": "Modality", "content": "We study the use of preference tuning on various modality, such as text, speech, vision, kinesthetic and others if we are not able to classify them. In the latest advancement of NLP, the idea of RL has been further explored to language and speech tasks, even in multi-modal tasks, such as vision-text. Thus, it is essential to categorize the papers by the extend of the study in terms of the modality, such as text, speech, vision, and vision-text."}, {"title": "Language", "content": "We explore the preference tuning application on different languages. In this case, we categorize the method by English, non-English, and multilingual."}, {"title": "3 Preference Tuning", "content": "In this section, we cover the general framework to train preference-tuned generative models. As shown in Table 3, the preference tuning training framework typically begins with the supervised fine-tuning (SFT) stage, during which the generative model is trained to excel"}, {"title": "3.1 Training Phases", "content": "The training phases for preference tuning are described as follows."}, {"title": "3.1.1 Supervised Fine-Tuning (SFT)", "content": "On the preference tuning, a generative model with trainable weights $\\theta$ normally starts by SFT via maximum likelihood (MLE) using teacher forcing and cross-entropy loss. The training is done using the supervised fine-tuning dataset $\\mathcal{D}_{sft}$. The objective is to maximize the log probability of a set of human demonstrations. The generative model is trained to generate the label by predicting the next token $y_{t+1}$ given the input x, current and previous label tokens $y_{t:<t}$. During the SFT, we utilize an attention mask applying to the entire context x and $y_{t:<t}$, and avoid applying attention to future tokens. The trained model denoted $\\pi_{\\theta_{sft}}$ and it is often to be used to initialize reward model and policy model $\\pi_{\\theta}$."}, {"title": "3.1.2 Reward Modeling", "content": "The reward model $r_{\\theta}(x, y)$ can be trained either separately (offline) or jointly trained with the policy model $\\pi_{\\theta}$ (online). Table 3 shows the list of reward models.\nSingle Objective Reward Model Bradley-Terry Reward Model (Bradley and Terry, 1952) is a pairwise comparison between two samples. It estimates the probability that the pairwise comparison i > j, which indicates a strong preference of i over j, is true as:\n$P(i > j) = \\frac{\\exp s_i}{\\exp s_i + \\exp s_j},$\n(2)\nwhere $s_i$ and $s_j$ are latent variables representing sample i and sample j, respectively. Thus, given the preference dataset $\\mathcal{D}_{pref} = \\{(x^i, y_w^i, y_l^i)\\}_{i=1}^N$, we could obtain an estimation of the reward model $r_{\\theta}(x, y)$ by minimizing the negative log-likelihood loss:\n$\\mathcal{L}(\\phi) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}_{pref}} \\log P(y_w > y_l | x)$\n(3)\n$\\qquad = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}_{pref}} \\log \\sigma(r_{\\phi}(x, y_w) - r_{\\phi}(x, y_l)),$\n(4)\nwhich $\\sigma$ denotes the logistic function, i.e., $\\sigma(x) := (1 + e^{-x})^{-1}$.\nMulti-Objective Reward Model Absolute-Rating Multi-Objective Reward Model (ArmoRM) (Wang et al., 2024a) is a two-stage approach that first trains a multi-objective RM"}, {"title": "3.1.3 Preference Alignment using Reinforcement Learning", "content": "While SFT has led to markedly improved performance, there is still a misalignment between SFT objective and the ultimate target of generating high-quality outputs as determined by humans. Stiennon et al. (2020); Ouyang et al. (2022) propose reinforcement learning from human feedback (RLHF) to further align language models with human intent. RLHF pipeline starts with the stage of modeling the rewards from human preferences, known as reward modeling stage, by maximizing the likelihood of preferences under the ground truth assumption. After obtaining the RM, RLHF further trains the Language Model policy via Reinforcement Learning to maximize the score given by the RM. Proximal Policy Optimization (PPO) was commonly chosen as the RL algorithm to update the policy because of its great sample efficiency."}, {"title": "3.1.4 Joint Training", "content": "Recent works also proposed that two stages of SFT and RLHF can be simplied as one stage with a weighted combination of the two loss functions and even lead to better performance. The key takeaway is to treat the preferred answer in the Human Alignment/RLHF stage as the SFT target, e.g., SLiC-HF (Zhao et al., 2023)."}, {"title": "3.2 Datasets", "content": "The dataset sources for SFT and preference tuning can be collected from various sources, such as human and LLMs feedback. Table 4 shows the list of SFT and alignment text data labeled by the data source either they are collected by human or synthetically generated by LLM."}, {"title": "3.2.1 SFT DATASETS", "content": "The SFT data is useful for training LM on high-quality input-output demonstration pairs. This is usually conducted for the foundation model as initialization. The SFT data can be in the form of prompts with various format."}, {"title": "LLM-Generated Datasets", "content": "Taori et al. (2023) propose Alpaca, a dataset with demonstrations generated using OpenAI's GPT-3 text-davinci-003 model. The instruction data"}, {"title": "Human-Generated and Human-Annotated Datasets", "content": "Using human-generated and human-annotated data are essential in training high-quality models. Zhou et al. (2024a) has shown quality is more important than quantity, as shown as using LIMA datasets that models trained only consist of 1,000 carefully human curated prompts and responses, without any reinforcement learning or human preference modeling can outperform models with much larger instruction-tuned datasets."}, {"title": "Dataset Collection", "content": "FLAN collection (Longpre et al., 2023) is introduced to train a collection of tasks on top of T5 and PaLM models (Raffel et al., 2020). For training multilingual LMs, Cendol Collection (Cahyawijaya et al., 2024), ROOTS (Lauren\u00e7on et al., 2022), and xP3 (Muennighoff et al., 2023) are used in SFT. Other potential datasets are crowd-sourcing datasets, although they are designed for SFT, but they can be useful resources for SFT, such as NusaCrowd (Cahyawijaya et al., 2023) and SEACrowd (Lovenia et al., 2024)."}, {"title": "3.2.2 HUMAN PREFERENCE ALIGNMENT DATASETS", "content": "The human alignment data can be in the form of pair-wise or ranking format. We can have a set of preferred and dispreferred data $\\mathcal{D}_{pref}$ for each input sample. For pairwise dataset, we collect pairs of preferred response $y_w$ and dispreferred response $y_l$. In case of multiple responses, we can gather responses $y_0, y_1, y_2$, and ask humans to pick the best $y_i$ from each. These datasets have been used to train reward models.\nConversational Datasets Several existing conversational datasets are instrumental in evaluating the quality of dialogue system or chatbot responses. Notable examples include HelpSteer2 (Wang et al., 2024b) and UltraFeedback (Cui et al., 2023). HelpSteer2 provides alignment scores across five different aspects\u2014helpfulness, correctness, coherence, complexity, and verbosity\u2014collected from human evaluators. UltraFeedback offers alignment scores for four aspects: instruction-following, truthfulness, honesty, and helpfulness. Additionally, HH-RLHF (Bai et al., 2022a) introduces datasets labeled with scores for helpfulness and harmlessness."}, {"title": "Code Datasets", "content": "CodeUltraFeedback comprises 10,000 coding instructions, each annotated with four responses generated by a diverse pool of 14 LLMs (Weyssow et al., 2024). These responses are ranked based on five distinct coding preferences: instruction-following, complexity, style, readability, and another instance of instruction-following. The rankings are determined using GPT-3.5 as a judge, providing both numerical scores and detailed textual feedback."}, {"title": "3.3 Pre-trained Generative Models", "content": "We categorize pre-trained generative models into three main types: LMs, VLMs, and SLMs. Additionally, we classify these models based on their accessibility: (1) Open Source: The model and data are open and accessible, (2) Open-Weight: Only the model is accessible and some or all data are inaccessible, (3) Close-weight and Close-source: The model is a black-box and may only be accessible by API or service, and (4) Close Access: The"}, {"title": "4 Online Alignment", "content": "In this section, we explore into human preference tuning using online methods, where data is continuously sampled. Online preference tuning involves real-time model updates as new data becomes available, enabling the model to dynamically adapt to evolving preferences and new information. This approach allows the alignment process to incorporate new data as it arrives and benefit from online exploration. We discuss the mechanisms of data collection, processing, and real-time model updates, emphasizing the benefits of managing non-stationary environments and enhancing model performance through continuous learning. Various techniques and strategies for implementing especially on-policy tuning are examined to provide a comprehensive understanding of its effective application in human preference tuning. We cover standard RL-based methods (e.g., PPO, which is online and on-policy), online DPO and SFT like algorithms (which can be on-policy or off-policy) and Nash Learning (or self-play) based algorithms."}, {"title": "4.1 Reinforcement Learning Human Feedback (RLHF)", "content": "In general, RLHF learns a reward function from human feedback and then optimize that reward function (Christiano et al., 2017). The training for RLHF involves three stages:\n*   The policy model $\\pi_{\\theta}$ interacts with the environment and the parameters of $\\pi_{\\theta}$ are updated via RL.\n*   The pairs of segments are selected from the output produced by the policy model $\\pi_{\\theta}$, and send them to human annotators for comparison.\n*   The parameters are optimized using reward r to fit the comparisons collected from human.\nAccording to Ziegler et al. (2019), the RLHF pipeline for LMs can be summarized as following:\n*   Supervised Fine-Tuning: A pre-trained LM is instruction-tuned using a dataset consisting of a given instruction prompt, and (typically) a human-written completion. The LM/policy is trained with a cross-entropy loss over the completion only. Often, the SFT model, denoted as sft is used to initialize both the reward model and the RLHF policy.\n*   Reward Modeling: RLHF leverages a reward model $r_{\\theta}$ trained using a dataset of preferences D. The reward model is trained using the following loss:\n$\\operatorname{loss}(r)=\\mathbb{E}_{(x, y; b) \\sim S}\\left[-\\log \\frac{e^{r(x, y)}}{\\sum_{y^{\\prime} \\in Y} e^{r(x, y^{\\prime})}}\\right],$\n(6)\nor, for pairwise preferences,\n$L_{R M}(\\phi)=-\\mathbb{E}_{(x, y_{w}, y_{l}) \\sim \\mathcal{D}_{pref}} \\log \\sigma(r(x, y_{w})-r_{\\phi}(x, y_{l})).$"}, {"title": "Reinforcement Learning", "content": "In this stage, the learned reward model $r_{\\theta^*}$ is used to provide online feedback in the optimization of the policy. In Ziegler et al. (2019); Stiennon et al. (2020); Ouyang et al. (2022), RLHF further maximizes average reward with an extra KL regularization term, i.e.:\n$\\mathcal{L}_{R L}(\\theta)=\\mathbb{E}_{x \\sim D, y \\sim \\pi(\\cdot|x)}\\left[r_{\\phi^{*}}(x, y)-\\beta_{r e g} K L(\\pi(\\cdot | x) || \\pi_{r e f}(\\cdot | x))\\right],$\n(8)\nwhere $\\beta_{reg} > 0$ is a hyper-parameter controlling the deviation from the reference policy $\\pi_{ref} = \\pi_{SFT}$.\nRLHF proposes optimizing the policy model using the Advantage Actor-Critic (A2C) method (Mnih et al., 2016) for playing Atari games and Trust Region Policy Optimization (TRPO) (Mnih et al., 2015) for performing simulated robotics tasks. The reward model is trained using the Bradley-Terry Reward Model, which leverages pairwise preference datasets essentially, pairs of preferred and dispreferred responses. There are various methods and variations for training RLHF, primarily categorized into two main approaches: RLHF and REINFORCE. In the following sections, we will describe these methods in detail."}, {"title": "4.1.1 Proximal Policy Optimization (PPO)", "content": "Initially in the original RLHF paper (Ziegler et al., 2019), they use PPO (Schulman et al., 2017) as their optimization strategy. PPO framework is a method for the human preference signals from external reward models with RLHF. The idea is to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization with a simpler clipped surrogate objective, omitting the expensive second-order optimization presented in TRPO using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, PPO (Schulman et al., 2017) proposes a novel objective function that enables multiple epochs of minibatch updates. It have some of the benefits of TRPO, but they are much simpler to implement and more efficient. For the optimization, KL-shaped reward (Ahmadian et al., 2024a) is useful as penalty-free optimization of the reward model leads to degradation in the coherence of the model. Optimizing this objective is equivalent to maximizing the following KL-shaped reward in expectation. There are a couple of variants of PPO: A2C (Mnih et al., 2016), P3O (Wu et al., 2023b), PTR-PPO (Liang et al., 2021), and RLHF-V (Yu et al., 2024).\nAdvantage Actor-Critic (A2C) A2C (Mnih et al., 2016) is an asynchronous variant of four RL algorithms that utilize parallel actor-learners to stabilize the effect of training of four methods.\nPairwise Proximal Policy Optimization (P3O) P3O (Wu et al., 2023b) is an on-policy RL algorithms that interleaves off-policy updates with on-policy updates. P30 uses the effective sample size between the behavior policy and the target policy to control how far they can be from each other and does not introduce any additional hyper-parameters.\nPrioritized Trajectory Replay (PTR-PPO) PTR-PPO (Liang et al., 2021) is an on-policy deep reinforcement learning algorithms have low data utilization and require significant experience for policy improvement. The algorithm proposes a proximal policy"}, {"title": "4.1.2 REINFORCE", "content": "ReMax ReMax (Li et al., 2023f) builds upon the well-known REINFORCE algorithm (Williams, 1987, 1992), leveraging three key properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. The name \"ReMax\" reflects its foundation in REINFORCE and its use of the argmax operator. ReMax modifies the gradient estimation by incorporating a subtractive baseline value as following:\n$\\tilde{g}(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\left[s_{\\theta}(x^{i}, a_{1: t}) \\times\\left(r(x^{i}, a_{1: T})-b_{\\theta}(x^{i})\\right)\\right],$\n(9)\nwhere the action $a_{t}^{i} \\sim \\pi_{\\theta}(\\cdot|x^{i}, a_{1: t-1})$, and $b_{\\theta}(x^{i})$ is a baseline value. A typical choice for $b_{\\theta}(x^{i})$ is\n$b_{\\theta}(x^{i})=r(x^{i}, \\bar{a}_{1: T}), \\quad \\bar{a} \\in \\underset{a}{\\operatorname{argmax}} \\pi_{\\theta}(\\cdot|x^{i}, \\bar{a}_{1: t-1}).$\n(10)\nThis baseline value can be obtained by greedily sampling a response and calculating the associated reward value.\nREINFORCE Leave One-Out (RLOO) RLOO (Ahmadian et al., 2024a) extends the REINFORCE algorithm by leveraging multiple online samples to achieve unbiased variance reduction. It improves upon REINFORCE in two key ways: (1) The rewards from each sample can serve as a baseline for all other samples, and (2) Policy updates are performed using the average of gradient estimates from each sample, resulting in a variance-reduced multi-sample Monte Carlo (MC) estimate. This is the intuition behind the RLOO estimator, as following:\n$\\qquad \\frac{1}{k} \\sum_{i=1}^{k}\\left[\\frac{k}{k-1} \\sum_{j \\neq i}^{k} R(y_{(j)}, x)\\right] \\nabla \\log \\pi_{\\theta}\\left(y_{(i)} | x\\right), \\text { for } y_{(1)}, \\ldots, y_{(k)} \\stackrel{i . i . d}{\\sim} \\pi_{\\theta}(\\cdot | x),$\n(11)\nwhere k refers to the number of online samples generated, RLOOk considers each $y_{(i)}$ individually and uses the remaining k-1 samples to create an unbiased estimate of the"}, {"title": "4.2 Online Directed Preference Optimization (Online DPO)", "content": "4.2.1 ONLINE AI FEEDBACK (OAIF)\nOAIF (Guo et al., 2024a) employs a LLM as an annotator during each training iteration. In this process, two responses are sampled from the current model, and the LLM annotator is prompted to select the preferred response, thereby providing real-time feedback. OAIF aims to gather preferences dynamically for responses generated by the language model being aligned. Given the prohibitive cost of using human feedback, this method leverages an LLM as an online annotator to collect preferences over pairs of responses sampled from the model $\\pi_{\\theta}$ during its alignment process. The objective for online DPO yields (please see detailed derivation of DPO in Section 5.1):\n$\\mathcal{L}_{O A I F}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right):=\\mathbb{E}_{x \\sim \\mathcal{D},\\left(\\zeta_{\\omega}, \\zeta_{l}\\right) \\sim \\pi_{\\theta}}\\left[\\log \\sigma\\left(\\beta_{r e g} \\log \\frac{\\pi_{\\theta}(y_{\\omega} | x)}{\\pi_{r e f}(y_{\\omega} | x)}-\\beta_{r e g} \\log \\frac{\\pi_{\\theta}(y_{l} | x)}{\\pi_{r e f}(y_{l} | x)}\\right)\\right] ,$\n(12)\nin which we note $\\pi_{\\theta-}$ to show that preference pairs are generated under $\\pi_{\\theta}$, but we further adopt a stop gradient to prevent it from getting into the loss objective for the gradient computation. The OAIF is illustrated in Algorithm 1 (OAIF algorithm in Guo et al. (2024a)), in which function l can be log-sigmoid (DPO), square (IPO), or ReLU (SLiC) functions."}, {"title": "4.2.2 ITERATIVE DIRECTED PREFERENCE OPTIMIZATION", "content": "Iterative DPO (Xu et al., 2023b; Xiong et al., 2024) has been proposed to narrow the gap between the performance offline preference optimization methods like DPO and online methods like RLHF, as RLHF still outperforms offline DPO. Different from DPO that used a fixed offline dataset, iterative DPO proposed to formulate the preference datasets by the generations of the current model and labelers, being either a pretrained reward model or LLM as a judge or the model to be trained itself through specific prompting (Yuan et al., 2024b), thus this pipeline usually appears at the same time with self-rewarding (Yuan et al., 2024b) methods (some paper will even call self-rewarding as iterative DPO methods). For each iteration, if the batch size for preference datasets utilized for policy optimization is only 1, then iterative DPO is essentially the same as online DPO or OAIF, except that"}, {"title": "4.2.3 ONLINE PREFERENCE TUNING (OPTUNE)", "content": "OPTune (Chen et al., 2024d) is an algorithm for efficient data generation in online RLHF. It improves both generation and training efficiency by selectively regenerating only the lowest-rewarded responses and employing a weighted DPO objective that prioritizes pairs with larger reward gaps. This approach significantly enhances the overall efficiency of the RLHF pipeline, setting the stage for the development of preference-aligned LLMs in a resource-efficient manner. The method enhances both data generation and training efficiency for online preference alignment. To minimize the cost of iterative data regeneration, it employs a straightforward yet effective reward-based prompt selection strategy, updating responses only for prompts with the lowest scores according to the reward model. Additionally, recognizing that converting scalar rewards to binary labels for the online DPO objective results in information loss, the method introduces a weighted DPO loss variant. This variant prioritizes learning from response pairs with larger reward gaps, further boosting online learning efficiency."}, {"title": "4.3 SFT-like", "content": "4.3.1 RANK RESPONSES TO ALIGN HUMAN FEEDBACK (RRHF)\nRRHF (Yuan et al., 2023) is a method that evaluates sampled responses from various sources using the logarithm of conditional probabilities and aligns these probabilities with human preferences through ranking loss. This approach can utilize responses from multiple origins, including the model's own outputs, responses from other large language models, and human expert responses, to learn how to rank them effectively. The primary objective is to simplify the complex hyper-parameter tuning and extensive training resources required by PPO. Before training, RRHF samples responses from diverse sources, which can include model-generated responses from the model itself as well as pre-existing human-authored responses of varying quality. During training, RRHF scores these responses based on the log probability provided by the training language model. These scores are then aligned with human preference rankings or labels using ranking loss, ensuring that the model's outputs are better aligned with human preferences."}, {"title": "4.3.2 Reward rAnked FineTuning (RAFT)", "content": "RAFT (Dong et al., 2023) is the combination of ranking samples by rewards and SFT, which iteratively alternates among three steps: 1) The batch is sampled from the generative models; 2) The reward function is used to score the samples and filter them to get a filtered subset of high rewards; and 3) fine-tune the generative models on the filtered subset."}, {"title": "4.3.3 REINFORCED SELF-TRAINING (REST)", "content": "ReST (Gulcehre et al., 2023) is an RLHF algorithm aimed at aligning an LM's outputs with human preferences. It uses a learned reward function to model human preferences over sequences. In the Markov decision process underlying conditional language modeling, states represent partial sequences, and actions correspond to generated tokens. ReST divides the typical reinforcement learning pipeline into distinct offline stages for dataset growth and policy improvement. Initially, it fine-tunes a model to map input sequences to output sequences using a dataset of sequence pairs, optimizing with Negative Log-Likelihood (NLL) loss. Then, it creates a new dataset by augmenting the initial training dataset with samples generated by the model. In this phase, conditioning inputs are resampled from the original dataset, similar to self-training, but direct sampling is possible if accessible."}, {"title": "4.3.4 Supervised Iterative Learning from Human Feedback (SuperHF)", "content": "SuperHF (Mukobi et al., 2023) is an alignment algorithm that enhances data efficiency using a reward model and replaces PPO with a straightforward supervised fine-tuning loss. The core concept involves the language model generating its own training data by sampling a \"superbatch\" of outputs, filtering these through a reward model, and iteratively fine-tuning on each filtered completion. This method builds upon and unifies previous research by integrating two crucial components: (1) the Kullback-Leibler (KL) divergence penalty and (2) an iterative process of sampling and fine-tuning. Additionally, SuperHF is embedded within a Bayesian inference framework, demonstrating that both RLHF and SuperHF can be understood from a unified theoretical perspective that does not rely on reinforcement learning. This perspective naturally justifies the use of the KL penalty and the iterative approach."}, {"title": "4.4 Nash Learning", "content": "4.4.1 NASH LEARNING FROM HUMAN FEEDBACK (NLHF)\nNLHF (Munos et al.", "first class citizen": "and pursue \u2018a policy that consistently generates responses preferred over those generated by any competing policy'. Thus this policy is the Nash equilibrium of this preference model", "as": "n$P (\\pi > \\pi^{\\prime"}, "mathbb{E}_{x \\sim \\rho} \\mathbb{E}_{y \\sim \\pi(\\cdot | x), y^{\\prime} \\sim \\pi^{\\prime}(\\cdot | x)} P\\left(y>y^{\\prime} | x\\right)-\\beta_{\\text {reg }} \\log \\frac{\\pi(y | x)}{\\mu(y | x)}+\\beta_{\\text {reg }} \\log \\frac{\\pi^{\\prime}\\left(y^{\\prime} | x\\right)}{\\mu\\left(y^{\\prime}\\right)},$\n(13)\nand NLHF searches the Nash Equilibrium such that (denote $\\mu$ as $\\pi_{ref}$ for simplicity here):\n$\\pi^{*}:=\\arg \\max _{\\pi} \\min _{\\pi^{\\prime}} \\mathcal{P}\\left(\\pi>\\pi^{\\prime}\\right)-\\beta_{r e g} K L_{\\rho}(\\pi, \\mu)+\\beta_{r e g} K L_{\\rho}\\left(\\pi^{\\prime}, \\mu\\right) .$\n(14)\nFor optimization, the Nash-"]}