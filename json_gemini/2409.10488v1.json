{"title": "Do Pre-trained Vision-Language Models Encode Object States?", "authors": ["Kaleb Newman", "Shijie Wang", "Yuan Zang", "David Heffren", "Chen Sun"], "abstract": "For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for ex-ample how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset Changelt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and gen-erative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to bet-ter encode object states, namely the quality of object lo-calization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states.", "sections": [{"title": "1. Introduction", "content": "Vision-Language Models (VLMs) have become founda-tional in various visual understanding tasks, including ob-ject recognition [13], visual question answering [9], and robotics applications [3]. These models integrate visual and linguistic data, enabling nuanced interpretation and interac-tion with images and video. However, a critical yet under-explored aspect of VLMs is their ability to encode the phys-ical states of objects\u2014such as whether an apple is whole or sliced. Understanding these states is essential for physical commonsense reasoning, which underpins many practical applications, from assisting with daily tasks (e.g., recogniz-ing that hot water can be poured into an empty glass) to enhancing interaction in robotic systems.\nWe define object state as the physical and functional condition or configuration of an object, as discernible from visual data. For example, an object's state can indicate whether it is melted, dirty, or undergoing a process such as cutting or pouring. While object state recognition is of-ten associated with video analysis due to its applications in modeling temporal dynamics and action recognition, our study focused on temporally localized key frames that con-tain the object states of interest, from which human annota-tors are able to accurately label the states without temporal context, and on which pre-trained VLMs are applied to gen-erate pseudo labels [16].\nTo explore VLMs' capabilities in this area, we em-ploy them as zero-shot classifiers on a dataset we intro-duce called ChangeIt-Frames, based on [15]. This dataset contains images depicting various object states in natural scenes. We augment this dataset with bounding box annota-tions for a subset of 1,736 images, in which the original ob-ject state labels are verified by human annotators. We evalu-ate 9 state-of-the-art open-source models. We use two types of VLMs: dual-tower VLMs, which classify images based on the similarity between image and text embeddings based on contrastive learning, and Multimodal Large Language Models (MLLMs), which utilize a visual encoder paired with a generative language model backbone to respond to prompts. We observe that while these models excel in ob-ject recognition, they consistently underperform in reliably identifying the objects' physical states.\nRecognizing object physical states in images presents unique challenges. Our study reveals that standard fine-tuning of VLMs with physically grounded data [2], or pre-training with curated datasets, does not necessarily enhance object state recognition performance on the ChangeIt-Frames dataset. We hypothesize that effective concept bind-ing-linking visual features to corresponding objects-is crucial for VLMs to accurately discern object states. To test this hypothesis, we construct object-centric represen-tations using CLIP and demonstrate that these modified VLMs improve at solving concept binding tasks involving color and shape, where vanilla CLIP struggles [8]. Nonethe-less, object-centric VLMs still exhibit limitations in recog-nizing object states, which we attribute to inadequate object localization and insufficiently discriminative visual and lan-guage representations. We additionally observe an increase"}, {"title": "2. Recognizing Object States", "content": "Dataset: We constructed our evaluation dataset, Changelt-Frames, from the video-based ChangeIt dataset [15], which includes 650 videos of 44 object categories undergoing var-ious state-changing actions. From these videos, we ex-tracted 25,735 images, each depicting one of 96 distinct ob-ject states. This image-based dataset is exclusively used for zero-shot evaluation of Vision-Language Models (VLMs).\nTo provide detailed annotations, we manually labeled a sub-set of 1,736 images from ChangeIt-Frames with bounding boxes around the target objects. Each image is labeled with a single bounding box corresponding to the object in its spe-cific state. The annotation process was carried out using the default annotator pool from Amazon Mechanical Turk. These annotations are released under the MIT license.\nThere have been introduced in the past to explore the compositionality of objects and their states, most notably MIT-States [6] and C-GQA [12]. Unlike C-GQA, which in-cludes states (e.g., \"cute dog\") that may not necessarily be the result of observable physical changes, ChangeIt-Frames is exclusively concerned with irreversible physical changes in objects. This also separates ChangeIt-Frames from MIT-States, which organizes state variations primarily through adjective-noun pairings that may include reversible states like open/closed door, or the states of global objects like cluttered/empty room. Furthermore, MIT-States uses Bing search engine with limited human annotation, leading to missing or inaccurate state labels."}, {"title": "Evaluation Setup:", "content": "For each image, we select a list of de-scriptive prompts for possible object states, such as \u201cwhole apple\" or \"fried bacon\". We refer to the correct descrip-tion as a \"positive prompt\", and the incorrect descriptions as \"negative prompts\". We employ two strategies for se-lecting negative prompts: The standard strategy which se-lects the negative prompts corresponding to different states within the same object category, such as peeled apple for whole apple. The remaining negative prompts are randomly selected from the candidate pool to total 10 prompts per im-age. We also consider the distractor strategy, and the neg-ative prompts consist of distractors specifically designed to be semantically similar yet incorrect regarding the object state. For example, for a positive prompt whole apple, the distractor prompts might include an apple that is cut or an apple that is peeled. The remaining prompts are randomly selected, ensuring a total of 10 prompts. This setting is de-signed to challenge the model's ability to discern subtle dif-ferences in object states.\nWe utilize two methods for zero-shot classification based on the architecture of the Vision-Language Models (VLMs): For the dual-tower VLMs, we compute the co-sine similarity between the image and text embeddings. The label corresponding to the highest similarity is selected as the predicted output. For the Multimodal Language Models we present the models with a prompt formatted as: \"Which of these does this image depict: [numbered list of prompts]? Only reply with the single number that corre-sponds to the correct answer.\" The model's output is then used to determine the predicted label. These methods allow us to evaluate the models' ability to correctly identify object states across different architectures."}, {"title": "Metrics:", "content": "We separately calculate Object Accuracy and State Accuracy. For object accuracy, a prediction is con-sidered correct if the predicted label includes the object's name (e.g., both whole apple and cut apple are correct for an image of an apple). For state accuracy, the model must predict the exact ground truth object state in the image."}, {"title": "Results and Analysis:", "content": "We conduct experiments on CLIP ViT-L/14 [13], OpenCLIP ViT-L/14 [5], ALIGN [7],"}, {"title": "3. Exploring Possible Remedies", "content": "In this section, we explore potential solutions for improving the recognition of object states in Vision-Language Models. We hypothesize that VLMs fail to recognize physical states due to the lack of an explicit notion of objects in these mod-els. These models may process images as a \"bag of con-cepts\", associating them with the scene as a whole rather than with individual entities.\nTo address this, we investigate the use of object-centric representations. We also evaluate the performance of larger VLMs trained on more extensive data to see if the low per-formance can be rectified by scale. The testbed for these improvements is also done on a subset of ChangeIt-Frames that include bounding box annotations and verified object state labels, both of which are done by human annotators.\nTo test whether focusing on specific objects can enhance state recognition, we implement object-centric VLMs. This approach involves isolating objects using bounding-box in-formation, either provided by the dataset or generated by an off-the-shelf detection model like GroundingDINO [11]. By cropping the images to these object regions, we aim to create representations that explicitly associate visual con-cepts with distinct entities."}, {"title": "3.1. Object-Centric VLMs", "content": "We assess the effectiveness of object-centric representations in two main tasks: concept binding and physical state recog-nition. For concept binding, we use the CLEVR-Binding benchmark [8], which involves differentiating between vi-sual concepts such as \"red cube and blue sphere\" versus \"blue cube and red sphere\", as well as spatial relationships of two objects, such as \"cube left of sphere\u201d versus \u201csphere left of cube\u201d. In Tables 2 and 3, we report accuracy on the training, validation, and generalization splits of CLEVR-Binding. We observe that object-centric VLMs outperform image-level VLMs by huge margins on both tasks."}, {"title": "3.2. Larger VLMS", "content": "We also investigate whether larger VLMs trained on exten-sive data can better recognize object states. Our evaluations include OpenCLIP ViT-G-14 [5] and SigLIP [17], we eval-uate their performance on the standard and distractor set-tings of the annotated ChangeIt-Frames dataset. We ob-serve that while larger dual-tower models show improved performance compared to CLIP and OpenCLIP, challenges remain under the distractor setting, where both are outper-formed by FLAVA."}, {"title": "3.3. Multimodal LLMs", "content": "In our state recognition experiments, we found that model performance typically declined in the distractor setting. To further explore state recognition, we examined Mul-timodal Large Language Models (MLLMs), a recent ad-vancement over the Vision-Language Models (VLMs) pre-viously used. Unlike VLMs, which rely on a standard text encoder, MLLMs incorporate a generative language model to process language inputs, significantly increasing the to-tal model parameters. With this in mind, we tested whether the additional parameters and enhanced language capabili-ties of MLLMs would improve accuracy in the standard set-ting and address the more linguistically complex challenges posed by the distractor setting.\nTo investigate this, we asses the performance of PaliGemma [1] and two LLaVA-NeXT [10] models (Mistral-7B and LLama-8B). The results in Table 5 show that the state recognition problem in dual-tower VLMs translate to MLLMs. Even with the use of a Large Language Model (LLM) and extensive Visual Instruction Tuning, the distractor setting remains challenging."}, {"title": "4. Inspecting the Encoded Representations", "content": "As we have ruled out several likely remedies to fix existing pre-trained VLMs on recognizing physical states of objects, we now aim to investigate why such models fail by inspect-ing their encoded visual and text representations.\nWe first investigate whether the text encoder can prop-erly reflect the physical state descriptions, we utilize T-SNE to visualize the CLIP text embedding of different text prompts of \"state + object\u201d combinations. As illustrated in Figure 2, the representations of the text prompts are clus-tered by the object category rather than the physical state, this indicates that the text encoders fail to learn discrimina-tive representations for physical states of objects.\nWe further validate the lower performance by visualizing the distributions of encoded object-level and image-level visual representations for the same objects with opposite states. We observe that the t-SNE projections do not show a clear distinction between the states, nor are the cropped im-ages embeddings for a given state closer to the whole image embedding. We also observe that cropping has a larger ef-fect on the representation than the state itself, suggesting the embeddings are not robust to transformation. Although we have demonstrated the lack of discrimina-"}, {"title": "5. Conclusion", "content": "Despite excellent performance on zero-shot object recog-nition, we demonstrate that existing pre-trained Vision-Language Models struggle to encode object state informa-tion, which we believe hinder their capabilities to under-stand and reason about the physical world. We hypothesize the challenge may come from lack of physically grounded training data, or the lack of object-centric inductive bias for VLMs to bind concepts to objects. We collect the ChangeIt-Frames benchmark with object bounding box and physical state annotations, and conduct extensive evaluations. We observe that addressing the data or model architecture is-sues alone does not solve object state recognition, and ex-pect further progress to be made on object localization qual-ity, concept binding, and pre-training objectives. We hope our findings will help develop future generation VLMs that can better capture object states."}, {"title": "Limitations:", "content": "Our evaluation mainly relies on a single dataset derived from instructional videos collected from the internet. Although the annotations are collectively manu-ally, they are still subject to label noise. Evaluation per-formed on more diverse visual domains is desired."}, {"title": "A. Appendix", "content": "Flan-T5-XXL. We use the prompt \"Question: Does this frame depict 1. init state text, 2. action text, 3. end state text or 4. none of the above\" and query each video frame to generate predictions and confidence scores for each.\nGroundingDINO GroundingDINO is a zero-shot object detection model that can identify objects based on textual input [11]. GroundingDINO combines a Transformer-based DINO detector with grounded pre-training. We use this model to provide object-centric information to our experiments."}, {"title": "A.1. Model Descriptions", "content": "CLIP/OpenCLIP ViT-L/14 The first models we consider are CLIP [13] and OpenCLIP [5] which share the same architecture and training. The main difference is that CLIP is trained with the private WebImageText dataset whereas OpenCLIP is trained with the public LAION dataset. For both CLIP and OpenCLIP model we use the ViT-L/14 architecture. For our analysis we calculate the cosine similarity between the encoded image and encoded text further described in.\nALIGN We next use the ALIGN [7] which is proposed to leverage a noisy dataset of over one billion image alt-text pairs. The ALIGN is a dual encoder with EfficientNet as its vision encoder and BERT as its text encoder.\nFLAVA FLAVA is optimized on multiple vision, language, and cross- and multi-modal tasks [14]. The FLAVA model contains an image encoder and a text encoders well as a multimodal encoder combines image and text representations for higher-level tasks. Both image and text encoder follows a ViT-B/16 architecture that outputs a list of state vectors. For the multimodal encoding, the two lists of state vectors are further forwarded to a transformer module that is based on the ViT architecture. Different from the other models, FLAVA learns representations from multimodal data (image-text pairs) and unimodal data (unpaired images and text). For our experiments, we use the image and text encoders and compute text-image similarity in a 768-dimensional embedding space.\nPhysVLM We use the PhysVLM model [4], fine tuned on the PhysObjects dataset, to evaluate how a model grounded in the physical world, focused on understanding physical concepts performs on this sort of task. Intuitively, under-standing of physical reasoning would help differentiating object states. Specifically, we use PG-InstructBLIP, a fine-tuned version of InstructBLIP with the language model"}, {"title": "A.2. Data Collection Methodology", "content": "For data annotation, we provided clear and concise instruc-tions to the participants involved in the image annotation task. The instructions were as follows: \"Please draw one bounding box around the object matching one of the pro-vided attribute labels. If the object is unclear or obscured by text, use 'Nothing to Label'. Draw only ONE bound-ing box per image. If multiple objects with the same at-tribute are close together, include all in a single bounding box. If objects are in different states making it difficult to"}, {"title": "A.3. Details on CLEVR-Binding", "content": "The CLEVR-binding dataset is split into training, vali-dation, and generalization set with distinct attribute-noun pairs. For each object in each image, the answer candi-dates are formed by the ground-truth pair and four distrac-tor pairs. In details, in the example of image with a red sphere and blue cube, the answer candidates for red sphere are composed of itself and two distractors that switching the existing attribute and noun compositions (red cube and blue sphere) and two randomly sampled from other negative pairs. To zero-shot evaluate CLIP and its variants' ability of attribute-noun binding, we design the following prompt rec-ommended by OpenAI: \"a photo of [adj] [noun\"."}, {"title": "A.3.1 Relational Reasoning", "content": "The two-object relational reasoning task requires models to predict the objects in the scene and their relationship. The CLEVR-binding dataset contains 3 types of objects: {cube, sphere, cylinder} and 4 types of relationship: {left, right, front, behind} with 24 possible combinations of spatial re-lations (Note that the relations are \"symmetric\", e.g. cube left sphere is equivalent to sphere right cube).\nIn the setting of relational reasoning, an input image v containing two objects (s, 0). The goal is to predict the relationship in the format of subject relation-object triple (ns, R, no) where ns, no \u2208 N = {cube, sphere, cylinder} is the shape of objects and o and R\u2208 R = {left, right, front, behind} is the spatial relationship.\nWe propose a two-stage method for relational reasoning. In the first stage, CLIP takes in the object-centric represen-tations (xs, xo) extracted by the frozen CLIP vision encoder from the masked images, and zero-shot recognize the ob-jects in the image, where n \u2208 N and T is the frozen CLIP text encoder:\n$n_s = arg\\ max_n(x_s\\cdot T(a\\ photo\\ of\\ n))$\n$n_o = arg\\ max_n(x_o\\cdot T(a\\ photo\\ of\\ n))$\nIn the second stage, a linear head L takes in the object-centric representations to predict the relation R between ob-ject s and o. The linear head is trained on the training set.\n$R = L(x_s | x_o)$"}]}