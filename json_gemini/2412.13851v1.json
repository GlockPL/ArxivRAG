{"title": "From approximation error to optimality gap - Explaining the performance impact of opportunity cost approximation in integrated demand management and vehicle routing", "authors": ["David Fleckenstein", "Robert Klein", "Vienna Klein", "Claudius Steinhardt"], "abstract": "The widespread adoption of digital distribution channels both enables and forces more and more logistical service providers to manage booking processes actively to maintain competitiveness. As a result, their operational planning is no longer limited to solving vehicle routing problems. Instead, demand management decisions and vehicle routing decisions are optimized integratively with the aim of maximizing revenue and minimizing fulfillment cost. The resulting integrated demand management and vehicle routing problems (i-DMVRPs) can be formulated as Markov decision process models and, theoretically, can be solved via the well-known Bellman equation. Unfortunately, the Bellman equation is intractable for realistic-sized instances. Thus, in the literature, i-DMVRPs are often addressed via decomposition-based solution approaches involving an opportunity cost approximation as a key component. Despite its importance, to the best of our knowledge, there is neither a technique to systematically analyze how the accuracy of the opportunity cost approximation translates into overall solution quality nor are there general guidelines on when to apply which class of approximation approach. In this work, we address this research gap by proposing an explainability technique that quantifies and visualizes the magnitude of approximation errors, their immediate impact, and their relevance in specific regions of the state space. Exploiting reward decomposition, it further yields a characterization of different types of approximation errors. Applying the technique to a generic i-DMVRP in a full-factorial computational study and comparing the results with observations in existing literature, we show that the technique contributes to better explaining algorithmic performance and provides guidance for the algorithm selection and development process.", "sections": [{"title": "1. Introduction", "content": "The proliferation of e-commerce and the progress of communication technology has led to the emergence and establishment of new business models that allow customers to book on-demand logistical services, mostly the delivery of goods (Wa\u00dfmuth et al. (2023)) or local transportation (Vansteenwegen et al. (2022)). Prominent examples of these services are attended home delivery (AHD), same-day delivery (SDD), or mobility-on-demand (MOD). These business models have in common that customers expect a very high service level, e.g., in terms of the deviation from their desired service time (Amorim et al. (2024)). Meeting these expectations makes demand consolidation challenging, which entails high fulfillment cost (Ulmer (2020)). To still operate profitably, operational planning for these business models has evolved: Instead of optimizing the associated vehicle routing alone, providers additionally apply demand management to achieve efficient fulfillment operations.\nThe resulting integrated demand management and vehicle routing problems (i-DMVRPs) are stochastic and dynamic with two types of integrated decisions: For each dynamically arriving customer request, the provider integratively makes a demand control decision and a vehicle routing decision with the overall objective of maximizing the expected profit, i.e., revenue net of operational fulfillment cost. Such an i-DMVRP can be modeled as a Markov decision process (MDP) and, theoretically, be solved by evaluating the well-known Bellman equation (Puterman (2014)). Practically, however, i-DMVRPs suffer from the curses of dimensionality ((Powell (2011)) such that this is not tractable for realistic-sized instances. Consequently, in literature, demand control decisions for i-DMVRPs are often optimized with a decomposition-based solution approach. More precisely, two subproblems are solved sequentially for every incoming customer request (Fleckenstein, Klein, and Steinhardt (2023), Ulmer (2020), Gallego and Topaloglu (2019), p. 25, Klein et al. (2018)):\n1.) Approximating opportunity cost (OC) for each potential fulfillment option (e.g., different time windows) to measure the expected profit impact assuming the current customer chooses the respective option, given the state of the system.\n2.) Solving the actual demand control problem based on the approximated OC, i.e., deciding on the pricing or availability of fulfillment options, or the acceptance/rejection of the request.\nThis leads to the following conclusion: One of the main avenues for accelerating the development of practical solution approaches for i-DMVRPs is understanding and explaining the relation between the accuracy reached in (1), i.e., the accuracy of the OC approximation, and the quality of (2), i.e., the quality of the resulting demand control decision. However, despite the maturity of integrated demand management and vehicle routing as a research area, this relation has not been systematically explored, and therefore, largely remains a black box so far. Hence, we now close this research gap by comprehensively analyzing this relationship. To do so, we first introduce a novel explainability technique for i-DMVRPs that combines two building blocks:\nB1: Chain of influencing factors The first building block resembles a typical post- hoc explainability technique (Arrieta et al. (2020)), i.e., we define metrics to evaluate the behavior of a given policy in certain states and the respective impact on solution quality. Thereby, we aim at answering the central questions that arise along the chain of influencing factors from OC approximation error to objective value loss as depicted in Figure 1. This allows deriving insights on when (in which states) and why a certain policy performs especially good or bad.\nB2: Reward decomposition \u2013 The second building block incorporates the idea of reward decomposition (Juozapaitis et al. (2019)). For that, we exploit the finding that opportunity cost can be decomposed into displacement cost (DPC) and marginal cost-to-serve (MCTS) (Fleckenstein et al. (2024)). More precisely, we propose to apply OC approximations capturing only one of the two components, with the aim of assessing the importance of the respective component for approximation accuracy.\nAfterward, in an extensive computational study, we apply the explainability technique to the generic i-DMVRP and the stylized parameter settings from Fleckenstein et al. (2024). Then, we complement the numerical results by an analysis of the existing literature on i-DMVRPs. Therewith, we confirm the validity of our findings and show that our identified OC approximation error types can indeed explain the observed performance of state-of-the-art solution approaches. In summary, our work has four contributions:\n1.) To the best of our knowledge, we introduce the first explainability technique for the widely established decomposition-based solution approaches for i-DMVRPs.\n2.) We apply our explainability technique within a comprehensive computational study and identify fundamental OC approximation error types, i.e., OC approximation errors that can occur in a broad variety of real-world i-DMVRPs. Therewith, we are the first to systematically analyze the relation between the accuracy of OC approximation and the objective value.\n3.) We classify patterns in the occurrence of the fundamental approximation error types we identify, characterize which problem settings are prone to which error type, and propose algorithmic elements to successfully mitigate them. This yields insights that guide the selection and the design of OC approximation algorithms.\n4.) We compile indications for the occurrence of the identified error types from existing literature and show that our findings improve explainability of the reported results. Thereby, we transform the existing implicit knowledge about specific i-DMVRPs to explicit, high-level knowledge.\nThe remainder of this paper is structured as follows: In Section 2, we review the related literature both on algorithmic explainability and i-DMVRPs. In Section 3, we introduce and model the generic i-DMVRP under consideration. Then, in Section 4, we present our novel explainability technique for i-DMVRPs in detail, and we present our computational study in Section 5. In Section 6, we derive general insights for algorithm design and summarize our work in Section 7."}, {"title": "2. Literature review", "content": "Due to the cross-cutting nature of our study, the related literature spans across multiple distinct research areas. In Section 2.1, we review the literature on i-DMVRPs with a special focus on its origins in revenue management, dynamic pricing, and dynamic vehicle routing. In Section 2.2, we then discuss algorithmic explainability techniques, particularly from explainable reinforcement learning (RL), highlighting the techniques that we adapt and apply in the work at hand. Finally, in Section 2.3, we review the descriptive analytics that authors use to explain the observed performance of their i-DMVRP solution approaches."}, {"title": "2.1. Modeling and solving i-DMVRPs", "content": "In logistics, many companies dynamically collect orders for a transportation service that is fulfilled by a given fleet of vehicles. These companies face an i-DMVRP if they can both plan individual offers made in response to customer requests and plan the vehicle routes to feasibly fulfill the resulting orders. Hence, i-DMVRP research synthesizes two originally distinct research areas:\n1.) Research in revenue management and dynamic pricing addresses the dynamic optimization of offering decisions under the assumption that fulfillment is already pre-planned. For an extensive overview of this field, we refer the reader to the textbooks by Gallego and Topaloglu (2019) and Talluri and Van Ryzin (2004) as well as the reviews by Klein et al. (2020) and Strauss, Klein, and Steinhardt (2018).\n2.) Dynamic vehicle routing investigates the optimization of fulfillment assuming given orders that arrive dynamically. For a deeper discussion of this research area, we refer the reader to the textbook by Toth and Vigo (2014) as well as the reviews by Hildebrandt, Thomas, and Ulmer (2023), Soeffker, Ulmer, and Mattfeld (2022), and Psaraftis, Wen, and Kontovas (2016).\nStarting with the seminal work of Campbell and Savelsbergh (2005) on an AHD system, i-DMVRPs are considered in a variety of applications such as SDD (Azi, Gendreau, and Potvin (2012)), MOD (Atasoy et al. (2015)), or mobile personnel booking (Avraham and Raviv (2021)). Indicative of the growing importance of this research area, there are several reviews that are either application-specific (Li, Archetti, and Ljubic (2024), Wa\u00dfmuth et al. (2023), and Snoeck, Merch\u00e1n, and Winkenbach (2020)), or aim at i-DMVRP literature in general (Fleckenstein, Klein, and Steinhardt (2023)).\nSince i-DMVRPs are dynamic and stochastic, the natural modeling approach is to formulate a Markov decision process (MDP) model (Puterman (2014)). It is important to note that MDP models not only serve as a formal problem definition. On top of that, model analysis, which can be done analytically or numerically (Bravo and Shaposhnik (2020)), yields domain knowledge that can be exploited by solution approaches. For i-DMVRPs, such model analyses can be found in Fleckenstein et al. (2024), Lebedev, Goulart, and Margellos (2021), and Asdemir, Jacob, and Krishnan (2009). In particular, we draw on the property that OC can be decomposed into MCTS and DPC (Fleckenstein et al. (2024)). The three works have in common that they mainly analyze models analytically. In contrast, our technique focuses on the numerical analysis of solution approaches. It also yields domain knowledge regarding a combination of a solution approach and a model of a specific i-DMVRP.\nAlthough i-DMVRPs can be solved to optimality by exact dynamic programming algorithms, this is impractical for realistic-sized instances. Hence, there exists a wide variety of heuristic solution approaches for specific i-DMVRPs. According to Fleckenstein, Klein, and Steinhardt (2023), they can"}, {"title": "2.2. Explainability of algorithmic performance and behavior", "content": "Explainability becomes increasingly relevant in analytics and optimization in general (see, e.g., the recent reviews by De Bock et al. (2024) and Goerigk and Hartisch (2023)). In particular, the field of explainable RL (XRL) has recently gained more attention (Milani et al. (2024)). Due to the close relation between RL and the OC approximation approaches observed in i-DMVPR literature, the techniques developed for XRL are also applicable to most approaches tackling i-DMVRPs.\nMilani et al. (2024) introduce a two-dimensional taxonomy of explainability techniques tailored to XRL. The first classification dimension proposed by the authors is borrowed from general explainable artificial intelligence:\n1.) Explainability can be inherent to a policy or restored post-hoc.\n2.) We can further distinguish local explanations that refer to individual states, and global explanations that holistically view the behavior of the policy.\n3.) Among the post-hoc techniques, a distinction can be made regarding the degree of portability, i.e., the range of solution approaches the technique can be readily applied to.\nRegarding this classification dimension, our explainability technique is a post-hoc explanation since it is applied to a given (decomposition-based) policy. As discussed in Section 2.1, this type of policy is quite common, which makes our technique portable. Further, it features local (state-level) metrics but also involves global considerations since these local metrics are aggregated to explain the global behavior of the policy.\nThe second classification dimension specifically addresses XRL approaches and distinguishes explain- ability techniques based on the type of explanations they incorporate as follows:\n1.) Feature importance explanations: explaining individual actions by providing their context, e.g., state features. Typical approaches are, e.g., surrogate policies encoded as decision trees or saliency map explanations.\n2.) Learning process and MDP explanations: exploiting the definition of MDP model elements or training process steps to generate explanations. The aim is to identify critical drivers of the policy's individual decisions.\n3.) Policy-level explanations: identifying recurring sequences of decisions (e.g. by clustering states) to extract patterns of the policy's overall control behavior.\nRegarding this second classification dimension, our technique can be viewed as a combination of policy level explanations (in B1) and learning process and MDP explanations (in B2). In the following, we briefly review the closest related literature for B1 and B2 separately.\nB1 of our explainability technique is closely related to a technique called strategy summarization by Amir, Doshi-Velez, and Sarne (2019). They suggest identifying states of interest on the basis of importance, coverage, likelihood of encountering, and policy disagreement with the aim of aggregating these states to summarize the behavior of the policy. Applied to i-DMVRPs, measuring the OC approximation error itself can be considered equal to measuring policy disagreement with the optimal policy. To quantify state importance, we measure the impact of an approximation error in a certain state on the quality of the resulting decision. This can also be viewed as a special case of the state importance metric used by Torrey and Taylor (2013). Further, like Amir, Doshi-Velez, and Sarne (2019), we consider the likelihood of encountering a state.\nB2 of our explainability technique is a reward decomposition technique. It is first proposed by Russell and Zimdars (2003) with the aim of facilitating the learning process. With the same goal, it is also applied by Van Seijen et al. (2017) in the form of a hybrid reward architecture. However, as shown by Juozapaitis et al. (2019), reward decomposition can not only be applied for designing hybrid reward architectures but also as an explainability technique. Therefore, they analyze the influence of the different reward components for explaining the behavior of a given policy. In contrast, we analyze approximation errors that result from considering only one reward component for explaining the behavior of a given policy. This idea of analyzing approximation errors in RL is first presented by Mannor et al. (2007) with the aim of computing confidence intervals.\nRegarding the application of our explainability technique, a distinguishing feature compared to most existing works in XRL is that we consider a large number of small problem instances and solve them to optimality. Thereby we derive generic domain knowledge, in the form of fundamental OC approximation error types, rather than analyzing heuristic policies for large instances. In this regard, we only found one similar approach by Bravo and Shaposhnik (2020). They use machine learning to analyze optimal policies for small problem instances of, amongst others, traditional revenue management problems.\nIn summary, our methodology combines a variety of existing RL explainability techniques in a novel way: Besides adapting them to the problem structure of i-DMVRPs, we introduce the new idea of combining strategy summarization and reward decomposition and applying both to derive characterizations of fundamental OC approximation errors."}, {"title": "2.3. Performance metrics in i-DMVRP literature", "content": "In contrast to \u201cpure\u201d revenue management and dynamic pricing, where explainability has already received some attention (e.g., Biggs, Sun, and Ettl (2021), Bravo and Shaposhnik (2020)), we find no systematic application of techniques from XRL in the literature on i-DMVRPs. Instead, most authors evaluate the performance of their solution approaches by incorporating descriptive analytics, as we summarize in the following.\nAggregate metrics - Apart from the arithmetic mean of profit, which is the objective in most of the considered i-DMVRPs, many authors additionally report the following aggregate metrics describing the performance of policies: Among the most widely reported metrics are average or overall revenue, cost, and number of orders (Campbell and Savelsbergh (2005)). Further, some authors also report revenue per order (Klein et al. (2018)), cost per order (Yang et al. (2016)), average number of fulfillment options offered to each customer (Mackert (2019)), pooling rate (Anzenhofer et al. (2024)), or fleet utilization (Klein and Steinhardt (2023)). In addition to the arithmetic mean, the standard deviation (Yang et al. (2016)) or the coefficient of variation (Anzenhofer et al. (2024)) are reported in a few studies.\nDecision-making \u2013 For a more detailed analysis of a policies' performance, authors analyze how the resulting decision-making differs over time, i.e., over the course of the booking horizon, or for different types of requests: For any i-DMVRP, the acceptance rate or conversion rate (Mackert (2019)) or the cumulative revenue over time (Lang, Cleophas, and Ehmke (2021)) can be reported. If customers can choose from a set of fulfillment options, the number (Abdollahi et al. (2023)) or composition of offered fulfillment options (Klein and Steinhardt (2023)), or the chosen fulfillment options (Anzenhofer et al. (2024)) can be analyzed. If dynamic pricing is applied, average prices of offered (Klein et al. (2018)) or chosen fulfillment options (Yang et al. (2016)) are reported.\nOpportunity cost \u2013 If a parametric OC approximation is used, its parameter values (Lang, Cleophas, and Ehmke (2021)) or the function values for certain parameter values (Avraham and Raviv (2021)) can be investigated. Only very rarely, authors directly consider approximated OC values for different groups of similar requests (Yang and Strauss (2017)) or over time (Koch and Klein (2020)).\nIn general, we identify three central problems that limit the explanatory power of the existing descrip- tive analyses: First, observations of the performance and the behavior of a policy do not provide direct evidence of whether or how exactly an OC approximation error influences the observed performance. Due to a lack of conclusive explanations, the reasoning is often limited to formulating hypotheses. Second, the metrics are only analyzed in an aggregate form, which does not allow distinguishing different types of errors that originate in certain regions of the state space. Third, since typically, a specific solution approach for a specific i-DMVRP is considered, the results are hardly attributable to certain characteristics of the problem structure, the instance structure, or the solution approach. This again limits conclusiveness and transferability.\nOverall, there is a clear research gap regarding the development of explainability techniques for i- DMVRPs and the formulation of generalizable explanations for policy performance."}, {"title": "3. Problem definition and modeling", "content": "In this section, we formally characterize i-DMVRPs with a particular focus on the generic MDP model for i-DMVRPs by Fleckenstein et al. (2024).\nTypically, an i-DMVRP is structured as follows: During a booking horizon customers log-in to the business platform and place a service request by entering service parameters like pick-up/drop-off locations, desired fulfillment times, or vehicle types. In response, the provider either presents a set of suitable fulfillment options with different prices to choose from, or accepts/rejects the request. Then, a successfully placed customer request turns into a confirmed customer order. All customer orders are eventually served by the provider within the service horizon, which can either be disjoint or overlapping with the booking horizon. The former is typical for AHD, where customer and provider agree on a delivery time window for a certain day in advance. The latter is typical for SDD or MOD, where the customer expects to receive a service on short notice.\nIn the following, we consider the generic i-DMVRP model as in Fleckenstein et al. (2024) but adapt it specifically for the case of disjoint booking horizons and service horizons. Further, the underlying demand control subproblem features an accept/reject demand control. However, the generalization to multi-option demand control is straightforward (see Fleckenstein et al. (2024))."}, {"title": "Decision epoch", "content": "- A decision epoch marks the start of the MDP model's stages. In the considered problem, such stages correspond to (constant) time steps $t = 1, ...,T$. A customer request of type $c \\in C$ can arrive in stage $t$ with a certain arrival rate $\\lambda_c$. With each customer request of type $c$, the provider also receives data on the associated location(s) $l_c$ and revenue $r_c$. Individual customer requests are then uniquely identified by combining this information with their request time $\\tau$. Arrival rates are assumed to be small enough that at most one customer request arrives per stage."}, {"title": "State", "content": "- The system state $s_t = (C_t, \\phi_t)$ comprises two sets. The first set $C_t$ consists of tuples $(c, \\tau, o)$, which store customer orders for which fulfillment has not yet started. The second set $\\phi_t$ stores the tour plan. Since we assume disjoint booking and service horizons, in our case, $\\phi_t$ is either preliminary or empty for all $t < T$. Please note that $s_t$ defines a post-decision state. The state space of a decision epoch $t$ is denoted as $S_t$ and comprises all potential realizations of customer orders $C_t$ and tour plans $\\phi_t$. Thus, $\\forall t \\in 1,..,T: s_t \\in S_t$."}, {"title": "Action", "content": "- An action in response to an arriving customer request of type $c$ integrates an accept/reject decision for demand control $g_t \\in G(s_{t-1},c) \\subseteq {0,1}$, and a tour planning decision $\\phi_t(g_t) \\in \\Phi(s_{t-1}, C, g_t)$. Again, $\\phi_t(g_t)$ is either preliminary or empty for all $t < T$ due to the disjoint horizons. The action space for the tour planning, denoted as $\\Phi(s_{t-1}, c, g_t)$, is defined by the routing constraints of the problem and depends on the preceding state $s_{t-1}$, the type $c$ of the arriving request, and the demand control decision $g_t$. The action space for the demand control, denoted as $G(s_{t-1},c)$, in turn, depends on $\\Phi(s_{t-1}, c, g_t)$ since $g_t = 1$ is only feasible if $\\Phi(s_{t-1}, C, g_t) \\neq \\emptyset$. Thus, $A_t(s_{t-1},c) = {(g_t, \\phi_t(g_t)) : g_t \\in G(s_{t-1},c), \\phi_t(g_t) \\in \\Phi(s_{t-1}, C, g_t)}$."}, {"title": "Rewards", "content": "- As a consequence of an acceptance decision $g_t = 1$, a revenue $r_c$ is received. A rejection yields no reward. A routing decision $\\phi_t(g_t)$ entails a reward $r_{\\phi_t}(g_t)$. It equals the newly arising fulfillment cost, which, given the triangle inequality holds, is non-positive. Again, since we assume disjoint booking and service horizons, $\\forall t < T : r_{\\phi_t}(g_t) = 0$."}, {"title": "Transition", "content": "- When transitioning to state $s_t$, $\\phi_t$ is set to $\\phi_t(g_t)$. The first state component $C_{t-1}$, also changes. More precisely, if the newly arriving request of type $c_t$ is accepted, the resulting customer order is added."}, {"title": "Objective", "content": "- The provider aims at maximizing profit after fulfillment. Therefore, it is required to determine a policy $\\pi$ that returns the optimal decision for each state that can potentially be reached."}, {"title": "Bellman equation", "content": "\u2013 The objective function (1) can be expressed in the form of a Bellman equation, which defines a value $V_t(s_t)$ for each state $s_t$. Solving this equation yields the optimal policy $\\pi^*$.\n$V_{t-1}(s_{t-1}) = \\sum_{c \\in C} \\lambda_c \\cdot  \\begin{cases} \\max \\limits_{g_t \\in G(s_{t-1},c)}  \\{ r_c +  \\max \\limits_{\\Phi_t(g_t) \\in \\Phi(s_{t-1},C,g_t)} \\{r_{\\phi_t}(g_t) + V_t(s_t | s_{t-1}, \\Phi_t(g_t)) \\} \\} \\\\ max \\limits_{\\Phi_t(0) \\in \\Phi(s_{t-1},0,0)} \\{V_t(s_t | s_{t-1}, \\Phi_t(0))\\} \\end{cases}$\n\n$ + (1 - \\sum_{c \\in C} \\lambda_c) \\cdot  \\max \\limits_{\\Phi_t(0) \\in \\Phi(s_{t-1},0,0)} \\{V_t(s_t | s_{t-1}, \\Phi_t(0))\\}$\nwith boundary condition:\n$V_T(s_T) = 0$.\nIn Equation (2), both types of decisions are represented in an integrated form. Thus, an interim state $s_t | s_{t-1},c, g_t$ can be defined to isolate the impact of the demand control decision from the impact of the vehicle routing decision as also depicted in Figure 2. Further, substituting the OC of accepting a request of type $c$, i.e., $\\Delta V_t(s_{t-1},c)$, we obtain the following reformulation. Note that we denote interim states $s_t | s_{t-1},c,1$ by $s_t(c)$ and interim states $s_t | s_{t-1}, c, 0$, or $s_t | s_{t-1},0,0$, by $s_t(0)$.\n$V_{t-1}(s_{t-1}) = \\sum_{c \\in C} \\lambda_c \\cdot \\max \\limits_{g_t \\in G(s_{t-1},c)} \\{g_t  \\cdot (r_c - \\Delta V_t(s_{t-1},c)) \\}  + V_t^i(s_t(0))$ with \n$V_t^i (s_t | s_{t-1}, C, g_t) =  \\max \\limits_{\\Phi_t (g_t) \\in \\Phi(s_{t-1},C,g_t)} \\{ r_{\\phi_t}(g_t) + V_t(s_t | s_{t-1}, \\phi_t(g_t))\\}$\n$ = r_{\\phi_t}^*(g_t) + V_t(s_t | s_{t-1}, \\phi_t^*(g_t))$,\nand\n$\\Delta V_t(s_{t-1}, c) = V_t^i(s_t(0)) \u2013 V_t^i (s_t(c)) \\geq 0$.\nFurther, $\\phi_t^*(g_t)$ denotes the optimal routing decision for a given demand management decision $g_t$."}, {"title": "4. Explainability technique", "content": "In this section, we present our novel explainability technique for i-DMVRPs, which comprises two separate building blocks. Both are later applied for the comprehensive analysis of the relation between OC approximation error and the quality of the resulting demand management decisions."}, {"title": "4.1. Building block 1", "content": "The basic idea of B1 is to define metrics for each step in the chain of influencing factors (1) behind the losses in objective value observed when following a certain policy. By this, we aim at identifying the regions of the state space that are especially relevant regarding the respective overall objective value loss. Therefore, we analyze the occurrence, the sign, and the magnitude of OC approximation errors in the respective states. Then, by suitable visualizations of the metrics, we compare them over various settings of problem parameter values (in the following referred to as settings) resembling different real-world i-DMVRPs. Based on that, we classify fundamental types of approximation errors, i.e., OC approximation errors that a broad variety of real-world i-DMVRPs are prone to. Therewith, we can eventually explain the performance of the considered policy. Both the metrics and the respective visualizations are generally valid, i.e., can be applied to any policies derived from different OC approximation approaches. In the following, we first describe the chain of influencing factors between OC approximation error and objective value loss. Afterwards, we describe the metrics we use to quantify each step in this chain of influencing factors and, finally, we propose visualizations of these metrics.\nAt the beginning of the chain of influencing factors, there is an approximation error in a certain state $s_{t-1}$, that could either be an underestimation or an overestimation of the true OC. Depending on the actual magnitude of such an approximation error, the magnitude of the true OC, i.e., $\\Delta V_t(s_{t-1},c)$, and the immediate reward $r_c$, this error can but not necessarily must result in a suboptimal decision.\nGenerally, a suboptimal decision in a certain state $s_{t-1}$ can either yield less immediate reward than the optimal decision, transition the system to a lower-valued state than the optimal decision, or both. However, the respective negative effect on the objective value itself can vary from barely notable to considerable.\nWhether the chain of influencing factors continues further, depends on the likelihood that $s_{t-1}$ is encountered and the respective suboptimal decision is made when following the policy under consideration.\nWe now define disaggregated metrics to quantify \"how bad\" an OC approximation error is, \u201chow wrong\" the resulting decision is, and also, \"how likely\" this decision is. Additionally, we define a fourth metric that captures the aggregated overall impact of OC underestimations or OC overestima- tions on the objective value."}, {"title": "Metrics", "content": "This metric allows us to identify regions of the state space, where an approximation systematically overestimates ($e^o(s_{t-1},c) > 0$) or underestimates ($e^u(s_{t-1},c) > 0$) the true OC. Hence, it provides information about where approximation errors originate and how strongly the chain of influencing factors is triggered.\nAs is well-known in revenue management (e.g., Talluri and Van Ryzin (2004)), an OC approximation error in itself is not problematic because the resulting decision may still be fairly accurate or even optimal. As previously described, the chain of influencing factors only continues if there is a suboptimal decision. Hence, to quantify the \u201csuboptimality\" of a single decision when a request of type $c$ arrives in state $s_{t-1}$, we introduce the metric single decision regret, denoted as $d(s_{t-1},c)$. It computes the overall reward difference between a single decision based on a, potentially wrong, OC approximation and the optimal decision. In less technical terms, to isolate the regret of one single decision, all future decisions from decision epoch t+1 onward are assumed to be made based on the optimal policy in both cases. Then, again depending on the observed underlying error magnitude $e^o(s_{t-1},c)$ and $e^u(s_{t-1},c)$, we can distinguish between overestimation regret and underestimation regret, even though both are calculated based on the same expression:\n$d(s_{t-1},c) = g_t^*(s_{t-1}, c) \\cdot (r_c \u2013 \\Delta V_t(s_{t-1},c)) \u2013 \\bar{g_t}(s_{t-1}, c) \\cdot (r_c \u2013 \\Delta \\bar{V}_t(s_{t-1},c))$,\nwith $g_t^* (s_{t-1},c)$ denoting the optimal demand control decision and $\\bar{g_t}(s_{t-1},c)$ denoting the demand control decision when following the policy under consideration. Then, we define the overestimation regret as:\n$\\delta^o (s_{t-1}, c) =  \\begin{cases} d(s_{t-1},c), e^o(s_{t-1}, c) > 0 \\\\ 0, \\quad \\quad \\quad otherwise, \\end{cases}$ and the underestimation regret as:\n$\\delta^u (s_{t-1}, c) =  \\begin{cases} d(s_{t-1},c), e^u(s_{t-1}, c) > 0 \\\\ 0, \\quad \\quad \\quad otherwise. \\end{cases}$ With this metric, we can assess whether an overestimation error or underestimation error leads to a suboptimal decision, and by which amount it causes the objective value to deteriorate assuming optimal decisions over the remaining booking process. Please note, for ease of readability, in the following, we refer to the single decision regret as regret.\nAs a third step in the chain of influencing factors, the relevance of a suboptimal decision must be considered. It depends on how likely it is to visit the state in which the decision is made. We measure the likelihood in the form of the decision rate $P(s_{t-1},c)$, which denotes the probability that a policy visits state $s_{t-1}$ and decides on the acceptance/rejection of a customer request of type $c$ at decision epoch t. To calculate it, we simulate decision-making based on the considered OC approximation for a sufficiently high number of drawn sample paths. This metric provides information about the relevant areas of the state space, and thus, to what extent the regret in a certain state impacts the objective value."}]}