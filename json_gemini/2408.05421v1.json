{"title": "EPAM-Net: An Efficient Pose-driven\nAttention-guided Multimodal Network for Video\nAction Recognition", "authors": ["Ahmed Abdelkawy", "Asem Ali", "Aly Farag"], "abstract": "Existing multimodal-based human action recognition approaches\nare either computationally expensive, which limits their applicability in\nreal-time scenarios, or fail to exploit the spatial temporal information of\nmultiple data modalities. In this work, we present an efficient pose-driven\nattention-guided multimodal network (EPAM-Net) for action recogni-\ntion in videos. Specifically, we adapted X3D networks for both RGB and\npose streams to capture spatio-temporal features from RGB videos and\ntheir skeleton sequences. Then skeleton features are utilized to help the\nvisual network stream focusing on key frames and their salient spatial re-\ngions using a spatial temporal attention block. Finally, the scores of the\ntwo streams of the proposed network are fused for final classification.\nThe experimental results show that our method achieves competitive\nperformance on NTU-D 60 and NTU RGB-D 120 benchmark datasets.\nMoreover, our model provides a 6.2-9.9x reduction in FLOPs (floating-\npoint operation, in number of multiply-adds) and a 9-9.6x reduction\nin the number of network parameters. The code will be available at\nhttps://github.com/ahmed-nady/Multimodal-Action-Recognition.", "sections": [{"title": "1 Introduction", "content": "In the last decade, human action recognition (HAR) has become a hot research\ntask in computer vision due to the growing desire of minimizing human labor in\nanalyzing vast amounts of video data. It plays a key role in several real-world\napplications such as video indexing and retrieval, visual surveillance, patient\nrehabilitation, sport analysis, and measuring behavioral engagement of students\nin classrooms [1,17,29,35]. Recently, unimodal methods of HAR such as skeleton-\nbased or RGB video-based methods, have witnessed remarkable improvements.\nThe RGB video-based methods model the spatial-temporal representation\nfrom video data and its corresponding estimated optical flow using network ar-\nchitectures such as two-stream network [16], CNN-LSTM network [11], and 3D"}, {"title": "2 Related work", "content": "In this section, we review the HAR literature according to the model's type: uni-\nmodal HAR (RGB video-based or skeleton-based HAR) and multimodal HAR."}, {"title": "2.1 RGB video-based action recognition approaches", "content": "Due to the easy collection of RGB data, RGB video-based methods have rapidly\ndeveloped and obtained impressive results. RGB video-based methods can be\ndivided into three categories [29]: Two-stream 2D CNN based, RNN-based and\n3D CNN-based methods. Two-stream 2D CNN methods [28] comprises two 2D\nCNN stream to learn the appearance and motion features from RGB video and\nits corresponding estimated optical flow. RNN-based methods [11] extract frame-\nlevel visual features using 2D CNN, then utilize gated RNN architectures, e.g.,\nLong-Short Term Memory (LSTM) to capture the long-term temporal dynamic\nin video sequence. To concurrently learn the spatial and temporal information\nfrom RGB video, 3D CNN-based methods are introduced. Two-stream Inflated\n3D CNN [8] is introduced through temporally extending the convolutional and\npooling kernels of a 2 D CNN. Feichtenhofer et al. [14] proposed a SlowFast\nnetwork with two-pathway that operates on video frames but at two different\nspeeds. The slow pathway, which works at a low frame rate, models spatial se-\nmantics, while the fast pathway models fine motion by working at a high frame\nrate. The lateral connection is used to fuse the two pathways. Despite of the\nimpressive results of 3D CNN-based methods, they require heavy computation\nto extract spatio-temporal features from videos. As a result, Temporal Shift\nModule (TSM) [20] is proposed to enable 2D CNN network to achieve 3D CNN\nperformance without adding computational overhead. TSM facilitates tempo-\nral interactions among features of neighboring frames through shifting part of\nthe channels along temporal dimension. Feichtenhofer [13] introduced an efficient\n3D-CNN architectures (X3D) for action recognition through expanding a tiny 2D\nmobile image classification architecture into spatiotemporal architecture along\nseveral axes in a progressive manner. Due to X3D network competitive perfor-\nmance with less computations (FLOPs and parameters), we adapted it in both\nvisual network, and pose network streams."}, {"title": "2.2 Skeleton-based action recognition approaches", "content": "Skeleton-based approaches can be grouped into four classes based on architec-\ntures: 2D CNN, RNN, GNN, and 3D CNN. For CNN, Choutas et al. [9] in-\ntroduced video clip-level human pose-based representation that encodes body\njoints' motion during the entire clip. The pose motion representation (PoTion)\nis constructed by temporally aggregating the heatmaps of each joint by colorizing"}, {"title": "2.3 multimodal action recognition approaches", "content": "multimodal HAR methods can be grouped into three categories: score-level fu-\nsion, feature-level fusion, and model-level fusion. In score fusion, the skeleton\nand RGB information are modeled separately and then the classification scores"}, {"title": "3 Approach", "content": "Our approach focuses on person-centric modeling by capturing both human\nbody movements and interacting objects. To achieve this, we first determine\nthe minimum bounding box that encompasses all 2D human skeletons across\nvideo frames. Then, each frame is cropped according to this bounding box and\nresized to the target dimensions. Fig. 2 illustrates an overview of the proposed\nnetwork architecture. The input of our proposed network is cropped RGB frames\nand their skeleton sequence. Specifically, the pose stream input is the pseudo-\nheatmap volume constructed from N uniformly sampled frames from the input"}, {"title": "3.1 Visual backbone", "content": "The X3D network [13] is utilized to extract spatiotemporal features $f_r \\in \\mathbb{R}^{C \\times T \\times H \\times W}$\nfrom RGB frames, where C represents the number of channels, T the number of\nframes, and HxW the spatial resolution. This X3D is an efficient 3D-CNN net-\nwork designed for action recognition, achieving competitive performance with\nsignificantly fewer FLOPs and parameters. Feichtenhofer [13] expands a tiny\n2D mobile image classification architecture, which uses channel-wise separable\nconvolutions instead of standard convolutions, into spatiotemporal architecture\nin a progressive manner. This expansion occurs not only along the temporal\naxis $\\gamma_t$ but also across other dimensions: spatial resolution $\\gamma_s$, network depth"}, {"title": "3.2 Pose backbone", "content": "The X3D-s network [13] is utilized to extract spatiotemporal dynamics from\nskeleton sequence. To adapt X3D-s network for skeleton based action recognition,\nwe do the following: 1) remove the first stage, 2) change the spatial stride of the\nfirst convolution layer of stem layer from 2 to 1 to make the spatial resolution\nof the final feature map match with that of visual backbone. Tab.2 shows the\ninstantiating of Pose-X3D network.\nWe follow the work [12] to extract the human pose. Having skeleton coordinate-\ntriplets ($x_k, y_k, c_k$) for each frame, the K heatmaps are generated using Gaussian\nmap centered at each joint:\n$H_k(x, y) = exp \\frac{-(x - x_k)^2 + (y - y_k)^2}{2\\sigma^2}$"}, {"title": "3.3 Spatial-temporal attention block", "content": "After extracting skeleton feature using Pose-X3D network and visual feature\nusing RGB-X3D network, the nesting spatial temporal attention block [19] is\nadapted to learn which spatial regions in each frame and key frames are worth\npaying attention to using skeleton feature and then re-weight visual features\naccordingly. The nesting spatial temporal attention block consists of spatial at-\ntention module, followed by nested temporal attention module which uses spatial\nattention map as an input.\nTo make this spatial temporal attention work, we align skeleton pseudo-\nheatmaps with corresponding RGB frames. For spatial alignment, video frames\nare cropped according to the minimum bounding box involving all 2D human\nskeletons across the video frames and then the skeleton pseudo-heatmap volume\nis generated accordingly. Moreover, the spatial resolution of the final feature\nmaps for visual and pose backbones is matched to ensure spatial correspon-\ndence between the two modalities. For temporal alignment, since the RGB and\npose modalities have different temporal resolution, their feature maps should be\nmatched in time for accurate action recognition. In particular, denoting the shape"}, {"title": "Spatial attention module", "content": "Given the skeleton feature maps $f_s \\in \\mathbb{R}^{C \\times T \\times H \\times W}$\nwhere T is a number of frames, HxW are the spatial resolution and C is the\nnumber of channels, the spatial attention map $A_s \\in \\mathbb{R}^{1 \\times T \\times H \\times W}$ is obtained using\na 1x3x3 spatial convolution to compress channels number of $f_s$ to 1, followed\nby a 1x7x7 spatial convolution. Specifically, the process of spatial attention map\ncan be expressed as follows:\n$A_s = \\sigma(g_{1x7x7} (\\delta(g_{1x3x3}(f_s))))$,\nwhere $\\sigma$ and $\\delta$ are the Sigmoid and RELU activation functions, respectively.\nThe spatial attention map reveals the importance of each spatial regions in each\nvideo frame with those of larger weights represent discriminative regions for the\naction."}, {"title": "Temporal attention module", "content": "The temporal attention module is inspired from\nSqueeze and excitation (SE) block [15]. The SE block models interdependen-\ncies between channels and reweigh channel-wise feature maps accordingly. The\ntemporal attention block has two operations: squeeze operation in which global\naverage pooling is used to aggregate spatial dimensions of spatial attention map\n$A_s$ and Excitation operation in which temporal-wise dependencies are modeled\nby using two fully connected layers with non-linear activation functions (RELU\nand Sigmoid). Overall, the two operation of temporal attention block can be\nformulated as:\n$Z_t = \\frac{1}{W \\times H} \\sum_{i=1}^{W}\\sum_{j=1}^{H} A_s(:, i, j, :)$,\n$A_\\tau = \\sigma(W_2(\\delta(W_1Z_t)))$,\nwhere i and j represent the width and height indices, respectively. Activation\nfunctions $\\sigma$ and $\\delta$ are the Sigmoid and RELU, respectively. The weights of the\nfully connected layers are represented by $W_1$ and $W_2$.\nThe temporal attention map $A_\\tau$ represents the importance of the T frames,\nwith frames having larger weights in $A_\\tau$ expected to be key frames. The spa-\ntiotemporal attention map is obtained through multiplying spatial attention map\n$A_s$ and temporal attention map $A_\\tau$ as follows: $A_{ST} = A_s \\otimes A_\\Tau$. After that, the\nRGB feature is modulated according to $A_{ST}$ as follows: $f'_r = f_r \\bigotimes A_{ST}$"}, {"title": "3.4 Training and Optimization", "content": "Our multimodal network is trained in end-to-end manner. The loss function is\nsummation of two cross-entropy losses of RGB and skeleton streams. It can be"}, {"title": "4 Experiments", "content": "We evaluate the proposed multimodal network on NTU RGB+D 60 [26] and\nNTU RGB+D 120 dataset [21]. For both datasets, we report the Top-1 accuracy\nof 1-clip for testing."}, {"title": "4.1 NTU RGB-D dataset", "content": "NTU RGB-D dataset [21, 26] is a large-scale multi-modalities human action\nrecognition dataset captured in a lab-controlled environment. It is available in\ntwo variants, NTU-60 and NTU-120. NTU-60 has 56880 video clips of 60 hu-\nman actions performed by 40 volunteers, whereas NTU-120 has 114,480 videos\nof 120 human actions performed by 106 volunteers. Each action is captured from\nthree distinct horizontal views simultaneously for several cameras' setups. Each\ncameras' setup has a different height, and the three cameras are positioned at\nthat height. The datasets have three settings for evaluation: cross-subject (X-\nSub), cross-view (X-View for NTU-60), and cross-setup (X-Set for NTU-120). In\ncross-subject (X-Sub), half of the subjects is used for training and the other half\nfor testing. For X-View, video samples are split based on camera IDs (cameras 2\nand 3 for training, camera 1 for testing), while X-Set splits are based on camera\nsetups (even setup IDs for training and odd ones for testing)."}, {"title": "4.2 Comparison with state-of-the-art methods", "content": "In Tab. 3, we compare the performance of our method with previous single-modal\nand multimodal methods on NTU-RGB+D 60 dataset. It is noticeable that our"}, {"title": "4.3 Ablation studies", "content": "In this section, we assess the effectiveness of each component of the proposed\npose-driven attention multimodal architecture. Moreover, we compare the per-\nformance of network architectures for RGB and pose streams according to ac-\ncuracy/complexity trade-offs. Finally, we evaluate the effectiveness of nesting\nspatial temporal attention block."}, {"title": "Effectiveness of the proposed multimodal architecture components", "content": "From Tab. 5, we can notice that RGB video-based network obtained higher\ntop-1 accuracies compared with its counterpart skeleton-based on NTU RGB-D\n60 and NTU RGB-D 120 datasets. This is because the datasets are captured in\na lab-controlled environment, where there are neither illumination changes nor\nbackground variations. Also, we can observe that our multimodal network with\nspatial temporal attention achieves 96.0%, 98.66%, 92.0% and 94.3% top-1 ac-\ncuracies for NTU 60 (cross subject and cross view evaluation setting) and NTU\n120 (XSub and XSet), respectively. This is higher than our method without such\nspatial temporal attention with 2.0%, 2.2%, 0.5% and 2.1%. This confirms that"}, {"title": "Choosing the spatial temporal attention block", "content": "To evaluate the effec-\ntiveness of the nesting spatial temporal attention block adapted from [19], we\nintroduced an attention block consisting of both spatial attention and a nested\ntemporal attention module. The spatial attention module involves compressing\nchannel-wise features using max-pool and average-pool operations, followed by\na 1x7x7 convolution. This can be expressed as:\n$A_s = \\sigma(g_{1x7x7} ([GAP(f_s); GMP(f_s)]))$\nwhere GAP denotes global average pooling, GMP represents the global max\npooling, and $\\sigma$ is the Sigmoid activation function. The temporal attention block,\ninspired from [32], aggregates spatial information of spatial attention map $A_s$\nusing global average pooling and then models the temporal interactions among\nneighboring frames using a 1D convolution. This process is formulated as:\n$A_\\tau = \\sigma(Conv1D(GAP(A_s))$\nOur multimodal network with the nesting spatial temporal attention block achieves\n96.0% and 98.7% on the cross subject and cross view evaluation setting of NTU\nRGB-D 60 dataset, respectively, compared to 95.9% and 98.6% with the in-\ntroduced spatial temporal attention one. The similar performance of the two\nspatial-temporal attention blocks indicates that our proposed multimodal archi-\ntecture works with different attention block types."}, {"title": "5 Conclusion", "content": "Throughout this work, we addressed the extensive computation problem of exist-\ning multimodal networks through presenting an efficient pose-driven attention-\nguided multimodal network. The proposed network utilizes efficient 3D CNN\nnetworks to model the spatial and temporal information from RGB frames and\npose sequence. Then, spatial temporal attention block employs pose features to\nhelp visual features pay attention to the key frames and their salient spatial re-\ngions. Finally, the final classification score is obtained through fusing the scores\nof RGB and skeleton streams. The proposed model was tested on the NTU-60\nand NTU-120 datasets and achieved competitive performance with state-of the-\nart methods with a 6.2-9.9x reduction in FLOPs and 9-9.6x reduction in the\nnumber of network parameters."}]}