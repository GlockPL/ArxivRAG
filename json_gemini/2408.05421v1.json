{"title": "EPAM-Net: An Efficient Pose-driven Attention-guided Multimodal Network for Video Action Recognition", "authors": ["Ahmed Abdelkawy", "Asem Ali", "Aly Farag"], "abstract": "Existing multimodal-based human action recognition approaches are either computationally expensive, which limits their applicability in real-time scenarios, or fail to exploit the spatial temporal information of multiple data modalities. In this work, we present an efficient pose-driven attention-guided multimodal network (EPAM-Net) for action recognition in videos. Specifically, we adapted X3D networks for both RGB and pose streams to capture spatio-temporal features from RGB videos and their skeleton sequences. Then skeleton features are utilized to help the visual network stream focusing on key frames and their salient spatial re-gions using a spatial temporal attention block. Finally, the scores of the two streams of the proposed network are fused for final classification. The experimental results show that our method achieves competitive performance on NTU-D 60 and NTU RGB-D 120 benchmark datasets. Moreover, our model provides a 6.2-9.9x reduction in FLOPs (floating-point operation, in number of multiply-adds) and a 9-9.6x reduction in the number of network parameters.", "sections": [{"title": "1 Introduction", "content": "In the last decade, human action recognition (HAR) has become a hot research task in computer vision due to the growing desire of minimizing human labor in analyzing vast amounts of video data. It plays a key role in several real-world applications such as video indexing and retrieval, visual surveillance, patient rehabilitation, sport analysis, and measuring behavioral engagement of students in classrooms. Recently, unimodal methods of HAR such as skeleton-based or RGB video-based methods, have witnessed remarkable improvements.\nThe RGB video-based methods model the spatial-temporal representation from video data and its corresponding estimated optical flow using network architectures such as two-stream network [16], CNN-LSTM network [11], and 3D convolutional neural networks (3D CNN) [8, 13, 14, 25, 30]. From these architectures, X3D is an efficient 3D CNN architecture which has achieved competitive performance for action recognition. In this network architecture, a tiny 2D mobile image classification architecture, which uses channel-wise separable convolutions instead of standard convolutions, is expanded into spatiotemporal architecture in a progressive manner.\nThe skeleton-based methods represent the human action through the trajectories of body keypoints [29]. Skeleton data can be obtained either from RGB videos using pose estimation algorithms or from motion capture systems, e.g., Kinect. Skeleton-based approaches can be grouped into four categories based on the used network architecture: Convolutional Neural Network (2D-CNN) [7,9], Recurrent neural network (RNN), Graph Convolutional Network (GCN) [27,33], and 3D CNN [12]. In 2D-CNN based methods [7,9], manually designed transformations are utilized to model the skeleton sequence as a pseudo image, while RNN-based methods model temporal context within skeleton sequence. Such input representations limit the exploitation of structure information of skeleton sequence. GCN based approaches [23,33] represent pose sequence as spatiotemporal graphs. However, the limitations of these approaches are their non robustness to noises in pose estimation and the necessity of careful design in integrating the skeleton with other modalities. In contrast, in the 3D-CNN based method [12], the input is represented as a volume of heatmaps, which capture the structure of skeleton joints and its dynamics over time. In this paper, we adopted the representation of the human skeleton sequence using a 3D pseudo-heatmap volume, then X3D-variant network is utilized to learn spatial-temporal representation from such pseudo-heatmap volume.\nRGB video and skeleton modalities depict actions from different perspectives. RGB modality provides rich appearance information involving scene context and object information. However, RGB video-based methods are sensitive to variations of viewpoint, background and illumination change. On the other hand, human skeleton data describes an action as a sequence of moving skeleton joints. It is robust to challenges of RGB modality, since the included information in the skeleton sequence is the skeleton joints coordinates. However, certain actions look ambiguous when viewed through skeleton data only due to lacking appearance information including interacting objects. \nMultimodal HAR methods, which take advantage of the complementarity of RGB and skeleton modalities to improve the performance of HAR, have gained great attention recently [2]. Previous multimodal HAR methods can be grouped into three categories: score-level, feature-level, and model-level fusion. Score fusion model the skeleton and RGB information separately and then aggregate models' scores from the Softmax layers. However, RGB and skeleton modalities fail to boost each other for feature representation. Feature fusion concatenates modalities features at the fully connected layers of modality-specific models. These methods slightly improved the HAR performance due to not considering the alignment of RGB data and its corresponding human body poses. The work in [10] addressed such alignment problem through proposing spatial embedding, which projects visual features and 3D skeletons in the same referential. Moreover, the temporal alignment is performed by assuming the existence of a 3D pose for each frame. However, this approach is computationally expensive. Model-level fusion methods utilize the knowledge from one data modality to facilitate modeling in other data modality [6]. Bruce el al. [6] use skeleton modality to learn spatial attention and then weight the spatiotemporal region of interest (ST-ROI) map, which is constructed from video input, accordingly. Although this approach uses 2D CNN to learn visual feature from ST-ROI map, it is still computationally intensive.\nTo tackle the mentioned limitations, we propose a novel Efficient Pose-driven Attention-guided Multimodal Network (EPAM-Net) to exploit the complementarity of RGB and skeleton modalities through aligning skeleton sequences and RGB videos in spatial and temporal dimensions. The proposed network consists of three main components: X3D network to learn visual features, X3D-variant network to learn skeleton features, and a spatial temporal attention block which utilizes skeleton features to pay attention to the key frames and their discriminative body part(s) performing the action on the visual feature.\nThe main contribution of this work is to propose an efficient multimodal architecture (EPAM-Net) with spatial temporal attention to guide visual features using skeleton features in an end-to-end manner. The proposed architecture achieved comparative performance with state-of-the-art methods on NTU"}, {"title": "2 Related work", "content": "In this section, we review the HAR literature according to the model's type: unimodal HAR (RGB video-based or skeleton-based HAR) and multimodal HAR."}, {"title": "2.1 RGB video-based action recognition approaches", "content": "Due to the easy collection of RGB data, RGB video-based methods have rapidly developed and obtained impressive results. RGB video-based methods can be divided into three categories [29]: Two-stream 2D CNN based, RNN-based and 3D CNN-based methods. Two-stream 2D CNN methods [28] comprises two 2D CNN stream to learn the appearance and motion features from RGB video and its corresponding estimated optical flow. RNN-based methods [11] extract frame-level visual features using 2D CNN, then utilize gated RNN architectures, e.g., Long-Short Term Memory (LSTM) to capture the long-term temporal dynamic in video sequence. To concurrently learn the spatial and temporal information from RGB video, 3D CNN-based methods are introduced. Two-stream Inflated 3D CNN [8] is introduced through temporally extending the convolutional and pooling kernels of a 2 D CNN. Feichtenhofer et al. [14] proposed a SlowFast network with two-pathway that operates on video frames but at two different speeds. The slow pathway, which works at a low frame rate, models spatial semantics, while the fast pathway models fine motion by working at a high frame rate. The lateral connection is used to fuse the two pathways. Despite of the impressive results of 3D CNN-based methods, they require heavy computation to extract spatio-temporal features from videos. As a result, Temporal Shift Module (TSM) [20] is proposed to enable 2D CNN network to achieve 3D CNN performance without adding computational overhead. TSM facilitates temporal interactions among features of neighboring frames through shifting part of the channels along temporal dimension. Feichtenhofer [13] introduced an efficient 3D-CNN architectures (X3D) for action recognition through expanding a tiny 2D mobile image classification architecture into spatiotemporal architecture along several axes in a progressive manner. Due to X3D network competitive performance with less computations (FLOPs and parameters), we adapted it in both visual network, and pose network streams."}, {"title": "2.2 Skeleton-based action recognition approaches", "content": "Skeleton-based approaches can be grouped into four classes based on architectures: 2D CNN, RNN, GNN, and 3D CNN. For CNN, Choutas et al. [9] introduced video clip-level human pose-based representation that encodes body joints' motion during the entire clip. The pose motion representation (PoTion) is constructed by temporally aggregating the heatmaps of each joint by colorizing each of them based on their order in the video clip. After that, PoTion is used as input for the proposed 2D-CNN network architecture to predict the action category. Liu et al. [22] observed that using pose estimation maps that maintain more details of human body shape is more beneficial for action recognition than depending only on the inaccurate 2D coordinates of human body joints. So, they used pose estimation maps that are composed of a 1-channel averaged heatmap that was generated by averaging the maps of all body joints and a 2-channel pseudo-heatmap that contained 2-D coordinates of estimated body joints as a video clip-level representation. The pose estimation maps capture the motions of the full body in addition to the movements of individual body joints. Then, they transformed the sequence of body shape and body joints images into body shape evolution map and body joints map, respectively. For classification, they designed a 2D-CNN architecture that learns features from each pose estimation map independently, and then the prediction label score is obtained by averaging the two CNN's scores.\nFor GCN-based approaches, Yan et al. [33] proposed the Spatial-Temporal Graph Convolutional Network (ST-GCN) to model the dynamics of skeleton sequence and the spatial arrangements of its joints. They created a spatial graph by utilizing the inherent connections between joints in the human body, and they also introduced temporal edges between corresponding joints in consecutive frames. They defined the graph convolution operation in a similar way to Euclidean space (2-D image) by defining the sampling function, which determines the neighbor set for each vertex, and the weighting function, which gives weight to each node based on its distance from the root node. They then extended the spatial graph CNN to the spatiotemporal domain by including temporally connected joints in the neighborhood concept. ST-GCN was built using the designed spatial-temporal convolution block. The limitation of this work is that skeleton GCN's graph is heuristically preset and reflects only the human body's physical structure, and it remains fixed across all layers and input samples. Lie et al. [27] addressed this limitation by introducing an adaptive graph convolutional network that is capable of adaptively learning the graph topology for different GCN layers and skeleton samples. To improve the accuracy of classification, the joints and second-order information of the skeleton represented in bone lengths and their directions were modeled using a two-stream adaptive graph convolutional network (2s-AGCN). For 3D CNN approaches, Duan et al. [12] represented the sequence of the human skeleton using a 3D pseudo-heatmap volume. Then, a slowOnly-variant 3D CNN is used to classify the 3D heatmap volumes into one of the action categories. Compared to the work [12], we use X3D-variant 3D CNN architecture to learn spatio-temporal information from pseudo-heatmap volume of skeleton sequence due to its good computational/accuracy trade-off."}, {"title": "2.3 multimodal action recognition approaches", "content": "multimodal HAR methods can be grouped into three categories: score-level fusion, feature-level fusion, and model-level fusion. In score fusion, the skeleton and RGB information are modeled separately and then the classification scores of both network streams are fused to obtain the final prediction. Feature fusion concatenates modality-specific features either at the fully connected layers of modality-specific models or at several layers using lateral connection [12]. Zolfaghari et al. [36] introduced a deep network architecture to utilize the three visual cues (appearance, motion, and pose) and fuse them sequentially using a Markov chain model. Each modality has its own 3D-CNN architecture (C3D). In the chained architecture, the prediction of each stream relies not only on the streams' visual cue but also on the predicted label of the previous stream. As a result, the network in that stream refines class labels from prior streams by learning complementing features. Li et al. [18] introduced skeleton-guided multimodal network to exploit the complementarity of RGB and skeleton modalities at feature level. The network is composed of three components: ST-GCN [33] to extract pose features, R(2+1)D network [31] to extract visual features, and guided block to pay attention to action-related feature in RGB videos. Their visual network extracts visual features from the whole video not from cropped human regions. Das et al. [10] introduced video-pose network (VPN) which its input is RGB 64-frame and their corresponding 3D poses. The I3D network is used to extract spatio-temporal feature from RGB 64-frame, while GCN model is employed to learn pose feature from 3D poses. This pose feature is used to learn spatial temporal attention to modulate visual feature, which used for classification. To apply the attention, they first find the correspondence between human joints and their relevant image regions through spatial embedding. Unlike VPN [10], our approach represents skeleton sequence using pseudo-heatmap volume which is spatially aligned with cropped RGB human region.\nFor model-based fusion, Bruce et al. [6] proposed model-based multimodal network (MMNet) which learns spatial attention from skeleton modality using GCN network and then weights the spatiotemporal region of interest (ST-ROI) map accordingly. A ST-ROI map is constructed through cropping frame's body area of actor(s) head, hands and feet from five sampled frames of a video input. After that, such ST-ROI is weighted and fed to 2D CNN to learn the appearance feature. Unlike MMNet [6] which uses spatial attention on ST-ROI not spatial-temporal attention that may not assign different weight for each body part through frames, we use person-centered modeling and spatial temporal attention to modulate visual feature accordingly."}, {"title": "3 Approach", "content": "Our approach focuses on person-centric modeling by capturing both human body movements and interacting objects. To achieve this, we first determine the minimum bounding box that encompasses all 2D human skeletons across video frames. Then, each frame is cropped according to this bounding box and resized to the target dimensions. \nThe input of our proposed network is cropped RGB frames and their skeleton sequence. Specifically, the pose stream input is the pseudo-heatmap volume constructed from N uniformly sampled frames from the input video clip, while the RGB stream input consists of M frames selected from these N frames by picking one frame out of every $\\frac{N}{M}$ frames.\nThe main steps of the proposed multimodal network are: 1) Extracting the spatiotemporal dynamics of skeleton sequence and cropped RGB frames separately using 3D CNNs. 2) Helping visual network stream to focus on discriminative human body part(s), interacting objects and key frames using spatial temporal attention block. 3) Fusing the class scores of two streams of the proposed network for further performance improvement. The rationale behind that is that the 16-frame of RGB video input might not be sufficient to capture the full temporal dynamics of an action. In contrast, pose network, which utilizes 48 uniformly sampled frames, has the ability to capture more temporal information. Below we discuss the visual network, pose network, and spatial temporal attention block in details."}, {"title": "3.1 Visual backbone", "content": "The X3D network [13] is utilized to extract spatiotemporal features $f_r \\in \\mathbb{R}^{C \\times T \\times H \\times W}$ from RGB frames, where C represents the number of channels, T the number of frames, and HxW the spatial resolution. This X3D is an efficient 3D-CNN network designed for action recognition, achieving competitive performance with significantly fewer FLOPs and parameters. Feichtenhofer [13] expands a tiny 2D mobile image classification architecture, which uses channel-wise separable convolutions instead of standard convolutions, into spatiotemporal architecture in a progressive manner. This expansion occurs not only along the temporal axis $Y_t$ but also across other dimensions: spatial resolution $y_s$, network depth"}, {"title": "3.2 Pose backbone", "content": "The X3D-s network [13] is utilized to extract spatiotemporal dynamics from skeleton sequence. To adapt X3D-s network for skeleton based action recognition, we do the following: 1) remove the first stage, 2) change the spatial stride of the first convolution layer of stem layer from 2 to 1 to make the spatial resolution of the final feature map match with that of visual backbone. \nWe follow the work [12] to extract the human pose. Having skeleton coordinate-triplets $(x_k, y_k, c_k)$ for each frame, the K heatmaps are generated using Gaussian map centered at each joint:\n$H_k(x, y) = exp \\frac{-(x - x_k)^2 + (y - y_k)^2}{2\\sigma^2}$         (1)\nwhere $x_k$and $y_k$ are the coordinates of kth joint and $\\sigma$ controls Gaussian map variance. Finally, all K joints heatmaps are stacked along the temporal dimension to form the 3D heatmap volume with size KxTxHxW where K is a number of human body keypoints, T is temporal length, and H and W are the height and width of such maps."}, {"title": "3.3 Spatial-temporal attention block", "content": "After extracting skeleton feature using Pose-X3D network and visual feature using RGB-X3D network, the nesting spatial temporal attention block [19] is adapted to learn which spatial regions in each frame and key frames are worth paying attention to using skeleton feature and then re-weight visual features accordingly. The nesting spatial temporal attention block consists of spatial attention module, followed by nested temporal attention module which uses spatial attention map as an input.\nTo make this spatial temporal attention work, we align skeleton pseudo-heatmaps with corresponding RGB frames. For spatial alignment, video frames are cropped according to the minimum bounding box involving all 2D human skeletons across the video frames and then the skeleton pseudo-heatmap volume is generated accordingly. Moreover, the spatial resolution of the final feature maps for visual and pose backbones is matched to ensure spatial correspondence between the two modalities. For temporal alignment, since the RGB and pose modalities have different temporal resolution, their feature maps should be matched in time for accurate action recognition. In particular, denoting the shape of pose feature as ${C_p, T_v, S^2}$, and the shape of RGB feature as ${C_r, T_M, S^2}$, so, $T_v$ is aligned with $T_M$ through time-strided sampling. Below, we discuss the spatial and temporal attention module in detail.\nSpatial attention module Given the skeleton feature maps $f_s \\in \\mathbb{R}^{C \\times T \\times H \\times W}$ where T is a number of frames, HxW are the spatial resolution and C is the number of channels, the spatial attention map $A_s \\in \\mathbb{R}^{1 \\times T \\times H \\times W}$ is obtained using a 1x3x3 spatial convolution to compress channels number of $f_s$ to 1, followed by a 1x7x7 spatial convolution. Specifically, the process of spatial attention map can be expressed as follows:\n$A_s = \\sigma(g_{1x7x7} (\\delta(g_{1x3x3}(f_s)))),$                                                              (2)\nwhere $\\sigma$ and $\\delta$ are the Sigmoid and RELU activation functions, respectively. The spatial attention map reveals the importance of each spatial regions in each video frame with those of larger weights represent discriminative regions for the action.\nTemporal attention module The temporal attention module is inspired from Squeeze and excitation (SE) block [15]. The SE block models interdependencies between channels and reweigh channel-wise feature maps accordingly. The temporal attention block has two operations: squeeze operation in which global average pooling is used to aggregate spatial dimensions of spatial attention map $A_s$ and Excitation operation in which temporal-wise dependencies are modeled by using two fully connected layers with non-linear activation functions (RELU and Sigmoid). Overall, the two operation of temporal attention block can be formulated as:\n$Z_t = \\frac{1}{W*H} \\sum_{i=1}^{W}\\sum_{j=1}^{H}A_s(:,:,i,j:),$                                      (3)\n$A_\\tau = \\sigma(W_2(\\delta(W_1Z_t))),$                                                                   (4)\nwhere i and j represent the width and height indices, respectively. Activation functions and $\\delta$ are the Sigmoid and RELU, respectively. The weights of the fully connected layers are represented by $W_1$ and $W_2$.\nThe temporal attention map $A_\\tau$ represents the importance of the T frames, with frames having larger weights in $A_\\tau$ expected to be key frames. The spatiotemporal attention map is obtained through multiplying spatial attention map $A_s$ and temporal attention map $A_\\tau$ as follows: $A_{ST} = A_s A_{\\tau}$. After that, the RGB feature is modulated according to $A_{ST}$ as follows: $f_r = f_r \\otimes A_{ST}$"}, {"title": "3.4 Training and Optimization", "content": "Our multimodal network is trained in end-to-end manner. The loss function is summation of two cross-entropy losses of RGB and skeleton streams. It can be expressed as follows:\n$L = - \\sum_{c=1}^{N}y_clog(\\hat{y_c}) - \\sum_{c=1}^{N}y_clog(\\tilde{y_c})$        (5)\nwhere $\\hat{y}$ denotes the prediction of skeleton network stream, $\\tilde{y}$ the prediction of RGB network stream, and y the class label.\nThe training process of the proposed multimodal network involves two phases: first, pre-training RGB and skeleton networks on the datasets, followed by fine-tuning the entire multimodal network for final classification. For RGB stream (RGB-X3D network), we train it with batch size of 16 and learning rate 0.0125 for 240 epochs. For skeleton stream (Pose-X3D network), we train it with batch size of 64 and learning rate 0.1 for 240 epochs. For entire multimodal network, we train it with batch size of 16 and learning rate 0.001 for 20 epochs. The optimizer used to train all networks is the Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9. The learning rate is decreased using Cosine Annealing schedule."}, {"title": "4 Experiments", "content": "We evaluate the proposed multimodal network on NTU RGB+D 60 [26] and NTU RGB+D 120 dataset [21]. For both datasets, we report the Top-1 accuracy of 1-clip for testing."}, {"title": "4.1 NTU RGB-D dataset", "content": "NTU RGB-D dataset [21, 26] is a large-scale multi-modalities human action recognition dataset captured in a lab-controlled environment. It is available in two variants, NTU-60 and NTU-120. NTU-60 has 56880 video clips of 60 human actions performed by 40 volunteers, whereas NTU-120 has 114,480 videos of 120 human actions performed by 106 volunteers. Each action is captured from three distinct horizontal views simultaneously for several cameras' setups. Each cameras' setup has a different height, and the three cameras are positioned at that height. The datasets have three settings for evaluation: cross-subject (X-Sub), cross-view (X-View for NTU-60), and cross-setup (X-Set for NTU-120). In cross-subject (X-Sub), half of the subjects is used for training and the other half for testing. For X-View, video samples are split based on camera IDs (cameras 2 and 3 for training, camera 1 for testing), while X-Set splits are based on camera setups (even setup IDs for training and odd ones for testing)."}, {"title": "4.2 Comparison with state-of-the-art methods", "content": "In , we compare the performance of our method with previous single-modal and multimodal methods on NTU-RGB+D 60 dataset. It is noticeable that our approach outperforms previous skeleton-based, RGB video-based, and multi-modal methods on such dataset. In particular, our approach boosts the average accuracies of state-of-the-art skeleton-based, RGB video-based and multimodal methods by 1.7%, 2.4% and 0.5%, respectively. In addition, the performance of our multimodal network can be further improved through making the RGB-X3D network wider (double the number of filters per layer). The intuition behind such improvement is that this modification enables the RGB-X3D net to extract more discriminative features."}, {"title": "4.3 Ablation studies", "content": "In this section, we assess the effectiveness of each component of the proposed pose-driven attention multimodal architecture. Moreover, we compare the performance of network architectures for RGB and pose streams according to accuracy/complexity trade-offs. Finally, we evaluate the effectiveness of nesting spatial temporal attention block."}, {"title": "Effectiveness of the proposed multimodal architecture components", "content": "From Tab. 5, we can notice that RGB video-based network obtained higher top-1 accuracies compared with its counterpart skeleton-based on NTU RGB-D 60 and NTU RGB-D 120 datasets. This is because the datasets are captured in a lab-controlled environment, where there are neither illumination changes nor background variations. Also, we can observe that our multimodal network with spatial temporal attention achieves 96.0%, 98.66%, 92.0% and 94.3% top-1 accuracies for NTU 60 (cross subject and cross view evaluation setting) and NTU 120 (XSub and XSet), respectively. This is higher than our method without such spatial temporal attention with 2.0%, 2.2%, 0.5% and 2.1%. This confirms that our multimodal method takes full advantage of the complementarity of RGB and skeleton modalities.\nChoosing the appearance and pose network architectures Tab. 6 shows the comparison of RGB video-based methods. We can observe that X3D network achieves comparative top-1 accuracy with 8.5-13.3x reduction in FLOPs in comparison with SlowOnly [12] and TSM. For pose network, it noticeable from"}, {"title": "Choosing the spatial temporal attention block", "content": "To evaluate the effectiveness of the nesting spatial temporal attention block adapted from [19], we introduced an attention block consisting of both spatial attention and a nested temporal attention module. The spatial attention module involves compressing channel-wise features using max-pool and average-pool operations, followed by a 1x7x7 convolution. This can be expressed as:\n$A_s = \\sigma(g_{1x7x7} ([GAP(f_s); GMP(f_s)]))$                                            (6)\nwhere GAP denotes global average pooling, GMP represents the global max pooling, and $\\sigma$ is the Sigmoid activation function. The temporal attention block, inspired from [32], aggregates spatial information of spatial attention map $A_s$ using global average pooling and then models the temporal interactions among neighboring frames using a 1D convolution. This process is formulated as:\n$A_\\tau = \\sigma(Conv1D(GAP(A_s))$                                               (7)\nOur multimodal network with the nesting spatial temporal attention block achieves 96.0% and 98.7% on the cross subject and cross view evaluation setting of NTU RGB-D 60 dataset, respectively, compared to 95.9% and 98.6% with the introduced spatial temporal attention one. The similar performance of the two spatial-temporal attention blocks indicates that our proposed multimodal architecture works with different attention block types."}, {"title": "5 Conclusion", "content": "Throughout this work, we addressed the extensive computation problem of existing multimodal networks through presenting an efficient pose-driven attention-guided multimodal network. The proposed network utilizes efficient 3D CNN networks to model the spatial and temporal information from RGB frames and pose sequence. Then, spatial temporal attention block employs pose features to help visual features pay attention to the key frames and their salient spatial regions. Finally, the final classification score is obtained through fusing the scores of RGB and skeleton streams. The proposed model was tested on the NTU-60 and NTU-120 datasets and achieved competitive performance with state-of the-art methods with a 6.2-9.9x reduction in FLOPs and 9-9.6x reduction in the number of network parameters."}]}