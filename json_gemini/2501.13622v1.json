{"title": "Coarse-to-Fine Process Reward Modeling for Enhanced Mathematical Reasoning", "authors": ["Yulan Hu", "Sheng Ouyang", "Yong Liu"], "abstract": "Process reward model (PRM) is critical for mathematical reasoning tasks to assign rewards for each intermediate steps. The PRM requires constructing process-wise supervision data for training, which rely on chain-of-thought (CoT) or tree-based methods to construct the reasoning steps, however, the individual reasoning steps may be redundant or containing nuanced errors that difficult to detect. We attribute these to the issue of the overlook of granularity division during process data collection. In this paper, we propose a coarse-to-fine framework to tackle this issue. Specifically, while gathering the process supervision data, we collect the coarse reasoning steps by merging adjacent steps according to preset merging granularity, then we sequentially reduce the merging granularity to collect fine-grained reasoning steps. For each synthesized new step, we relabel according to the label of last step. During training, we also traverse the collected training corpus in a coarse-to-fine manner. We conduct extensive experiments on popular mathematical reasoning datasets across diverse loss criterions, the proposed framework can consistently boost the reasoning performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated promising capabilities across a wide range of domains [1, 3, 6, 13], including complex mathematical reasoning tasks [5, 9]. Unlike conventional tasks that generally employ a standard reward model or an outcome reward model (ORM) to provide an overall evaluation based on the input instruction, mathematical reasoning tasks require a process reward model (PRM) to deliver intermediate supervision signals for each individual step [11]. This enables the identification of specific steps, such as error steps or key steps.\nTraining PRM requires collecting step-wise annotation corpora [9, 11]. For example, Lightman et al. [9] propose annotating step-by-step solutions to MATH problems using human data labelers. However, such human-intensive labeling is costly and hinders broader practical application. To address this, [12] propose an automatic annotation mechanism that constructs process supervision by defining the potential to deduce the ultimate correct answer for each intermediate step. [16] further leverage the Monte Carlo Tree Search (MCTS) algorithm to collect intermediate step-wise training data. These methods follow a paradigm where multiple steps are first generated, and rewards for each step are then assigned by human or automatic deduction. However, this paradigm is inherently suboptimal due to the neglect of a critical issue: existing studies primarily focus on developing strategies to assign precise rewards for each single step, while the granularity of the steps has been overlooked. To illustrate, we present a data collection example from the MATH dataset [4] via MCTS in Figure 1. We denote k as the maximum exploration depth and yg as the ground truth answer for question q. Generally, the trajectory {q, $1,1, $2,1, $3,1, ..., Sk,1}, which starts from the root node q and ends at the leaf node sk,1, can be collected as a training instance. The intermediate steps $1,1, $2,1, and $3,1 are labeled as positive samples, while the final leaf node, which does not lead to the correct answer, is labeled as a negative sample. However, as illustrated in the right part of Figure 1, two major issues exist. First, mathematical reasoning is a"}, {"title": "2 Methodology", "content": "In this section, we formally introduce CFPRM. CFPRM is designed to address the weaknesses of current process supervision corpus construction. To overcome these limitations, we propose CFPRM, a coarse-to-fine PRM. In the following, we first present the necessary preliminaries in subsection 2.1, followed by the coarse-to-fine process data collection mechanism in subsection 2.2."}, {"title": "2.1 Preliminaries", "content": "We denote an LLM policy as \u03c0, and re as the PRM fine-tuned upon \u03c0, parameterized by \u03b8. For reasoning tasks, \u03c0 generates responses step by step given an input query x in an autoregressive manner: $s_t \\sim \\pi_\\theta(\\cdot | x, s_{1:t-1}), t \\leq T$. The PRM policy $r_\\theta$ then outputs a reward given the partial solutions and the input query as: $r_{s_t} = r_\\theta(s_{1:t}, x)$. We regard $y_{s_t}$ as the label for step t."}, {"title": "2.2 Coarse-to-fine Strategy", "content": "CFPRM is intuitive in collecting coarse-to-fine process training data. Unlike methods that directly add rationales as prompts at the inference stage [14, 15], CFPRM fine-tunes a PRM by first collecting process reward signals. The collection mechanism is agnostic to reasoning structures, i.e., it is versatile for any kind of structure, such as CoT, MCTS, or BoN sampling methods.\nWe choose to explain our method using MCTS-like data collection, though the construction process of MCTS is not the focus of this paper. Taking the trajectory illustrated in Figure 1 as an example, we present the coarse-to-fine process data collection in Figure 2.\nSteps merge and relabeling. Consider a reasoning trajectory containing 7 steps, where step s\u2081 and steps s3 to s6 involve correct reasoning. Step s2 fails to make further incremental reasoning from s1, but the LLM policy manages to adjust the wrong step. The final reasoning step fails to reach the correct answer. As a coarse-to-fine method, CFPRM gradually merges multiple steps according to the preset window size, C. C denotes the size of the merging window, with Cmax as the maximum window size, while the minimum size is usually set to 1. Ideally, C can initially be set to the total number of steps in the entire reasoning trajectory, and the merged trajectory is labeled with the label of the last step, acting as ORM. In practice, adjacent steps are merged instead of the entire trajectory to avoid excessive concentration of knowledge. In Figure 2, C is initially set to 4, yielding the merged partial trajectory S1, S2, S3, S4, treated as a single step s1:4. Subsequently, C is sequentially decreased to"}, {"title": "3 Experiment", "content": "We present the experimental setups in this subsection."}, {"title": "3.1 Experimental Setup", "content": "We present the experimental setups in this subsection.\nDatasets and models. We adopt two widely used mathematical reasoning test sets, GSM-Plus [7] and MATH500 [4], for evaluation. GSM-Plus is built upon GSM8K [2] with various mathematical perturbations. The original GSM8K is a benchmark of grade-school level. The MATH dataset consists of high school math competition problems, which are more challenging. Instead of building MCTS to collect the training data from scratch, we adopt the off-the-shelf PRM800K [9] process dataset to train the PRM. We employ two cutting-edge LLMs, Qwen2.5-7B-Instruct and Qwen2.5-7B-MATH [13], as the backbone model.\nFollowing previous studies [8, 12], we evaluate the PRM using the best-of-n (BoN) sampling strategy. BoN@n assesses the PRM's ability to select the best answer from n candidates. For each candidate, the PRM evaluates the score of each step. We also use an identical backbone model to generate 64 candidates for each given question, keeping the backbone model and the BoN sampling policy consistent. The results are reported with n set to 8, 16, 32, and 64.\nBaselines and details. As a method to refine the data collection mechanism, our approach can be seamlessly applied to any existing methods. Specifically, we choose ShepHerd [12], RestMCTS* [16], and PQM [8] for comparison. These methods employ the three loss criteria listed in Table 1, respectively. We also include the conventional ORM for comparison. The best performance is marked in bold, and the second-best result is underlined. We set the max length to 2048, the learning rate to 2e-6, and the batch size to 32. All experiments are conducted on an H800-80G cluster, with each experiment repeated five times to report the mean results. We report accuracy as the evaluation metric."}, {"title": "3.2 Main Results", "content": "For CFPRM, we set C to 2 to merge the adjacent two steps, showing the performance in Table 2. For the methods that employ the same loss function, e.g., Shepherd [12] and CFPRM using the BCE criterion, we also calculate the average performance variation between them across four sampling policies.\nWe can see that CFPRM achieves superior results under most sampling policies. As the number of candidates increases, CFPRM exhibits a more pronounced advantage. In addition, CFPRM consistently achieves performance gains in most experimental settings. Compared to ShepHerd [12], which uses BCE as the loss function, and Rest-MCTS* [16], which employs MSE as the loss function, CFPRM improves performance to varying degrees. For example, when employing MSE as the loss function, CFPRM improves upon the baseline, RestMCTS* [16], by 1.4% and 3.4% on GSM-Plus and MATH500, respectively. Compared to the most recently proposed Q-value ranking-based method, PQM [8], CFPRM improves performance on GSM-Plus [2] across both backbone models. This advantage also holds with Qwen-2.5-7B-Instruct as the backbone model when evaluated on MATH500. An exception occurs when employing Qwen-2.5-7B-MATH as the backbone model and evaluating on the MATH500 benchmark. A possible explanation may be that Qwen-2.5-7B-MATH is a strong model designed for mathematical reasoning, under the sequential decision framework used by PQM, it places more emphasis on step-level dependency, while the construction of coarse steps hinders to capture it."}, {"title": "3.3 Further Studies", "content": "As presented in Figure 2, CFPRM constructs the coarse samples using a window of size C. We further explore the impact of varying C, ranging from 2 to 4. It is worth noted that only the new synthesized data is added. For simplicity, we study the two conventional loss criteria, BCE and MSE, on GSm-Plus and MATH500, using Qwen2.5-7B-MATH as the backbone model. We can observe from Figure 3 that the merging strategy consistently brings benefits regardless of the window size, validating that the proposed coarse-to-fine data collection mechanism facilitates overall performance. This observation verifies that certain limitations exist within the conventional data collection methods, while CFPRM, as an intuitive method, can mitigate these limitations. Moreover, we find that the performance brought by different window sizes varies."}, {"title": "4 Conclusion", "content": "In this paper, we briefly review the critical issues in process data collection in PRM. The granularity of the reasoning steps, as well as the nuanced errors within each intermediate step, were overlooked, which may hinder reaching the optimal downstream performance. To tackle this problem, we propose a coarse-to-fine training method by first collecting the process training data at diverse granularities. Then, we gradually traverse the training data in a coarse-to-fine manner to distill the knowledge to the PRM. We validate the proposed method using different loss criteria, confirming its effectiveness and versatility."}]}