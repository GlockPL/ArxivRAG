{"title": "A Single Model Ensemble Framework for Neural Machine Translation using Pivot Translation", "authors": ["Seokjin Oh", "Keonwoong Noh", "Woohwan Jung"], "abstract": "Despite the significant advances in neural machine translation, performance remains subpar for low-resource language pairs. Ensembling multiple systems is a widely adopted technique to enhance performance, often accomplished by combining probability distributions. However, the previous approaches face the challenge of high computational costs for training multiple models. Furthermore, for black-box models, averaging token-level probabilities at each decoding step is not feasible. To address the problems of multi-model ensemble methods, we present a pivot-based single model ensemble. The proposed strategy consists of two steps: pivot-based candidate generation and post-hoc aggregation. In the first step, we generate candidates through pivot translation. This can be achieved with only a single model and facilitates knowledge transfer from high-resource pivot languages, resulting in candidates that are not only diverse but also more accurate. Next, in the aggregation step, we select k high-quality candidates from the generated candidates and merge them to generate a final translation that outperforms the existing candidates. Our experimental results show that our method produces translations of superior quality by leveraging candidates from pivot translation to capture the subtle nuances of the source sentence.", "sections": [{"title": "1 Introduction", "content": "Neural machine translation (NMT) models exhibit outstanding capabilities when a large volume of the parallel corpus is available (e.g., translate from and to English). However, their performance still falls short in cases involving low-resource languages (e.g., Basque) and translating between non-English languages from different language families (e.g., German-Russian). Top-performing large language models (LLMs), such as GPT models, also demonstrate suboptimal translation performance in low-resource language pairs. The scarcity of parallel data, primarily due to limited cultural interaction, makes the low-resource translation task more challenging.\nIn many generation tasks, ensembling multiple systems has proven to be a successful strategy for performance enhancement. In NMT, traditional ensemble methods average probability distributions over output tokens from multiple models during decoding. However, the expensive cost of training multiple models is the primary shortcoming of ensemble decoding. Additionally, computing token-level probabilities at each decoding step is not feasible with recent black-box models such as GPT-40 and Gemini.\nEnsemble methods that can be utilized even when token-level probabilities cannot be computed have also been proposed. Selection-based ensemble method involves generating candidates from multiple models and then selecting the best candidate among them. However, in this ensemble fashion, the final output space is limited to the existing candidate pool. In contrast, the generation-based ensemble, such as LLM-Blender, creates improved outputs using candidates obtained from multiple models. This approach aims to generate a final output superior to the existing candidates. Nonetheless, the main drawback from the notably high cost of generating candidates through multiple models remains, inducing computational overhead. As the size of the models used in the ensemble increases, the cost proportionally escalates, becoming more burdensome. In addition, due to the varying performance of MT systems, the quality of some candidates can be significantly lower than others, leading to a degradation in the overall performance.\nTo alleviate the problems above, we propose Pivot-based single model Ensemble (PIVOTE), a novel generation-based approach. Our intuition of a"}, {"title": "2 Related Work", "content": "Pivot-based approaches. Pivot translation is an approach that decomposes the translation task into two sequential steps. By transferring knowledge from high-resource pivot languages, pivoting is especially effective in translation between low-resource languages.\nIn this study, pivot translation enables us to obtain high-quality candidates for the ensemble. Kim et al. discusses a pivot-based transfer learning technique where source pivot and pivot\u2192target models are first trained separately, then use pre-trained models to initialize the source target model, allowing effective training of a single, direct NMT model. Zhang et al. further investigate the transfer learning approach by utilizing auxiliary monolingual data.\nPivot translation typically employs English as the bridge language. Nonetheless, previous studies have explored the use of diverse pivot languages, taking into account factors such as data size and the relationships between languages. By leveraging the ability of pivot translation to produce diverse outputs, several studies have focused on generating paraphrases. More recently, Mohammadshahi et al. uses pivot translation for ensemble, but it requires computing token-level probabilities and fails to improve translation. Our work shares the motivation with these studies, generating translations depending on the pivot path to obtain a variety of candidates.\nEnsemble in NLG tasks. Ensemble learning is a widely adopted strategy to obtain more accurate predictions by employing multiple systems. In NMT, the traditional approach involves averaging the probability distributions of the next target token, which is predicted at each decoding step by multiple models or by different snapshots. When multiple sources are available, an ensemble can be conducted with predictions obtained by different sources. Also, a token-level ensemble through vocabulary alignment across LLMs has also been proposed. However, these methods are not applicable to recent black-box models as they cannot compute token-level probabilities at decoding time.\nSelection-based ensemble has also been explored, which chooses the final output among the existing candidates. This can be achieved through majority voting by selecting the most frequent one or selecting the best candidate with QE. Recently, MBR decoding, which aims to find the hypothesis with the highest expected utility, has gained attention. However, this approach limits the"}, {"title": "3 Pivot-based Single Model Ensemble", "content": "In this section, we first introduce the overview of PIVOTE framework (\u00a73.1). Then, we describe the candidate generation process through pivot translation (\u00a73.2) and the aggregation process (\u00a73.3).\n3.1 Overview\nOur objective is the same as that of conventional translation tasks: converting the given source language sentence x into the target language sentence \u0177. PIVOTE consists of two steps: candidate generation and candidate aggregation. Figure 1 illustrates an overview of the proposed ensemble framework.\nAs the first step, we input x to generate candidates through a single multilingual NMT model. One translation path could be directly translating from the source to the target through the source\u2192target path. Alternatively, pivot translations can be achieved by employing high-resource pivot languages, enabling translation paths from source\u2192pivot and pivot\u2192target. During the pivot process, leveraging abundant parallel data enables knowledge transfer from high-resource pivot languages, thereby facilitating the generation of diverse and more accurate translations. Through these n paths, we can obtain a candidate pool C = {C1, ..., Cn } composed of n candidates in the target language, employing only a single model.\nAs the second step, a ranking process is first conducted within the candidate pool C since not all candidates contribute to the ensemble. Using the estimated quality of each candidate, we select the top-k candidates. We then generate the final output \u0177 using the selected high-quality candidates. This generation-based approach facilitates the production of outputs superior to existing candidates.\n3.2 Pivot-based Candidate Generation\nIn the first step, PIVOTE takes a source sentence x as input and generates n candidates. Direct translation yields only one candidate, whereas pivot translation enables the generation of multiple candidates from a single source sentence using a single model. Generating candidates through pivot translation has two major advantages: diversity and quality.\nFirst, we can obtain diverse candidates that can act complementarily. One of the key principles for the ensemble is that the participants must be sufficiently diverse to provide various inductive biases. In PIVOTE, each source sentence is translated diversely by passing through multiple translation paths. Diverse translation paths enhance the likelihood of providing expressions that convey the accurate meaning of the source sentence. Pivot-based candidate generation shares a similar goal with a previous study that generates paraphrases through round-trip translation, aiming to generate diverse translations.\nSecond, by utilizing a parallel corpus of high-resource pivot languages, pivoting enables more accurate translations. For low-resource language pairs, more appropriate translations can be achieved through two-step decoding through a pivot language. Moreover, leveraging pivot languages with abundant parallel data, not limited to English, allows us to obtain better translations.\nIn addition, pivot translation with a single model offers practical benefits over employing multiple models. Firstly, it can reduce the costs of operating multiple models including LLMs. Secondly, the substantial performance disparities among models mean that using the top-performing single model for candidate generation often leads to higher-quality outcomes. Lastly, it reduces inference latency by using a single model for two batched inferences, while multi-model ensembles require up to 11, causing significant overhead and limiting real-time response capability. Given that pivot translation with a single model allows for the creation of diverse and more accurate translations, we utilize an MNMT model to generate the candidates.\nSelecting pivot languages. For each language pair, we carefully select pivot languages based on the assumption that pivot language with abundant mutual knowledge would allow us to obtain higher-quality candidates. We select n top-performing paths for our study based on BLEU scores on the FLORES-\n3.3 Candidate Aggregation\nIn the aggregation step, we take the candidate pool C as input and output the merged final translation \u0177. The post-hoc aggregation process encompasses two stages: selecting and merging. In the first stage, we select candidates by ranking method. There are two approaches for selecting candidates. One approach evaluates each translation path and selects the best paths for all source sentences. The other approach involves selecting the best top-k candidates for each source sentence. After selecting k candidates, we generate the final translation \u0177 using the merging module. This process enables the creation of better outputs beyond the quality of existing candidates.\nSelecting the top-k candidates. The pivot language that generates the highest-quality candidate varies for each source sentence. The best output is not guaranteed from one translation path alone, as it can vary depending on factors such as the size of the parallel corpus and the relationship between languages. First, PIVOTE uses QE to rank all n\ncandidates from candidate pool C = {C1, ..., Cn}. Afterward, we select top-k candidates among n candidate pool. Selecting the top-k candidates ensures the quality of the output by filtering out low-quality candidates while also efficiently reducing the cost during the merging process. We use the reference-free COMETkiwi for ranking candidates.\nGenerating the final translation. To generate the final translation \u0177 by merging the top-k candidates, we explore methods from two categories: encoder-decoder ensemble architectures and LLM-based approach. Employing encoder-decoder architectures during the merging process offers the advantage of relatively low training costs. We conduct experiments using Fusion-in-Decoder (FiD) and TRICE architectures. The former method involves passing Translate source into <target language> referring <target language> candidate. source: <x> candidate: <ck> through the encoder, representations are concatenated and merged in the decoder. The latter approach involves concatenating <x></s><lt>;<c\u2081></s><lt>; ...;<ck></s><lt>\nwith language token <llang> and providing it as input. Encoder-decoder ensemble architectures are further described in detail in Appendix D.\nOn the other hand, the LLM-based ensemble implicitly leverages their translation capabilities during ensemble, as the source sentence is also provided. We conduct merging experiments with GENFUSER, Llama-3, and GPT models. When employing GENFUSER, we construct the input by concatenating top-k candidates to the prompt, as presented in Jiang et al.. For merging with Llama-3 and GPT, we"}, {"title": "4 Experiments", "content": "We use NVIDIA RTX 3090 or 4090 GPUs for experiments.\n4.1 Datasets\nWe conduct experiments on the linguistically distant languages within pairs: not in the same language family and using different scripts. We select 2 language pairs, resulting in 4 translation directions in total, Korean (Koreanic)\u2192Italian (Romance) and Arabic (Arabic)\u2192 Portuguese (Romance). The language family grouping is defined by the criteria presented in Fan et al..\nWe validate our approach across various domains. For Korean Italian pair, we run experiments on TED2020. For Arabic Portuguese pair, we use WikiMatrix. All the datasets are obtained from the OPUS project. The statistics for the datasets are listed in Table 1.\n4.2 Evaluation Metrics\nWe assess the translation quality using BLEU, chrF++, and reference-based COMET. For reporting BLEU, Sacre-BLEU is used with ko-mecab tokenizer for Korean and 13a tokenizer for the others.\n4.3 Baselines\nAs an encoder-decoder NMT model, we use NLLB-200-distilled-600M. When training NLLB, we use the Transformers library from HuggingFace. AdamW optimizer is used with a learning rate of 2e-5, batch size of\n4.4 Implementation Details\nIn the candidate generation step of PIVOTE, we employ NLLB. For each source-target language pair, we use an NLLB fine-tuned for the language pair in Table 1 to generate the directly translated candidates. For the merging module, we use Llama-3, GPT-4, and GPT-40. For all models used in PIVOTE, including NLLB, Llama-3, GPT-4, and GPT-40, we apply the same settings in \u00a74.3.\nAs detailed in \u00a73.3, we explore two approaches in the ensemble process: one dynamically selects the top-k (k=3) candidates and another uses candidates obtained from fixed paths. To select the top-k candidates for each source sentence, we use the reference-free COMETkiwi as described in \u00a73.3. When selecting candidates from fixed paths, we used directly translated candidates and English-pivoted candidates, which were the top-performing\n4.5 Main Results\nTable 2 reports the overall performance of PIVOTE and other methods. The results demonstrate that PIVOTE consistently outperforms baselines across all language pairs. While standalone NMT systems rely solely on their pre-trained knowledge, PIVOTE explicitly leverages candidates during the ensemble. Even when training an open-source LLM, Llama-3, we can enhance translation capability by utilizing candidates obtained via pivoting. Compared to using LLMs for translation, we can improve performance with only the minimal cost of utilizing a small 0.6B model. Table 3 presents the quality of candidates utilized in the ensemble. We will further elaborate with a case study, showing that PIVOTE achieves better translations by leveraging candidates to capture subtle nuances of the source sentence. We report experiments in a setting that does not use training data in Appendix H and experiments with other GPT models in Appendix K. The analysis of the proportion of top-k candidates and performance variation with k are in Appendix J.\nComparison with multi-model ensemble. We compare PIVOTE with LLM-Blender and EVA, state-of-the-art ensemble methods utilizing multiple models. LLM-Blender employs N (N=11) LLMs for candidate generation, picks top-3 candidates with PAIRRANKER, and fuses them with GENFUSER. EVA is a token-level ensemble method that leverages vocabulary alignment across multiple models.\nResults in Table 2 show that PIVOTE outperforms multi-model ensemble baselines by a considerable margin. LLM-Blender was unable to improve outputs compared to its candidate LLMs in non-English translation tasks. Additionally, LLMs used for generating candidates in LLM-Blender, such as Vicuna and Baize, exhibit subpar performance on given tasks. These results align with recent work; open-source LLMs often struggle when not translating into English.\nEVA is not only ineffective on the given tasks but also has several limitations inherent to its design as a token-level ensemble. First, EVA is unable to use black-box models such as GPT-4. Second, it is memory-intensive, as it requires loading multiple models into memory simultaneously. While multi-model ensemble methods generate candidates using up to 11 LLMs (with sizes up to 13B), PIVOTE\ngenerates candidates with a significantly smaller single model (0.6B), thereby greatly reducing computational overhead.\nResults on all language pairs. To validate generalizability, we report the results for all language pairs we experimented with, including those within the same language family. Distant pairs refer to languages that belong to different families and use different scripts, while similar pairs belong to the same family and share the same script. The statistics for each language pair are in Appendix I. Language pairs used in the experiments are as follows:\n\u2022 Distant language pairs: Portuguese Russian, Dutch\u2194Russian, and French\u2194Ukrainian\n\u2022 Similar language pairs: Spanish Portuguese and Ukrainian\u2194Russian\nTable 5 shows the results with the top-performing baselines, NLLB and GPT-4. PivotE consistently exhibits superior performance compared to strong baselines on distant language pairs. Surprisingly, it also showed improvements in similar language pairs, such as Spanish\u2194Portuguese.\nCase study. We conduct a qualitative analysis to verify the impact of candidates on the final translation. We compare the output of GPT-4, used as the merging module, with PIVOTE, which utilizes candidates for the ensemble process. In Table 4, we provide two examples along with the source and target sentences, as well as the top-3 candidates. Through the first example, we can observe that PIVOTE can appropriately translate homonyms within the context. In Korean, \u201c\uc790\ubb38\u201d has the meaning of both \"consultation\u201d and \u201casking oneself", "asking ourselves\\\", as also shown in the English translation. However, GPT-4 mistranslated the source sentence, converting the phrase \u201c\uc790\ubb38\ud574\uc654\uc2b5\ub2c8\ub2e4": "o \u201cabbiamo cercato consigli\u201d (\u201cseeking consultation from others", "ci sono chiesti": "hat means \"asking ourselves\", aligning well with the context by leveraging information from candidates. In the second sample, GPT-4 translates the source sentence by translating the noun \u201c\uc774\uc288\u201d into \"questioni\u201d. However, given the topic of discussing potential health risks, this translation does not fit well with the overall context. By contrast, the en-"}, {"title": "5 Conclusion", "content": "In this work, we introduced PIVOTE, a pivot-based single model ensemble framework, to enhance translation in scenarios where parallel data are scarce. By transferring knowledge from diverse pivot languages, we were able to obtain not only diverse but also high-quality candidates. And the optimal path to generating the best candidate varies per sentence, our study underscores the significance"}, {"title": "Limitations", "content": "Despite PIVOTE utilizes candidates obtained via pivoting, limitations arise from the nature of pivot translation. Constraining the pivot language to high-resource languages can limit the number of candidates because pivoting through low-resource languages can lead to some loss of information due to error propagation inherent in the two-step translation. This semantic shift potentially causes a decrease in candidate quality. If the quality of candidates declines, improvements from the ensemble might not be significant, indicating a limitation in the number of pivot paths."}, {"title": "Appendix", "content": "A Pivot Language Selection\nBased on the results from the FLORES-200 benchmark, we select the top-4 pivot paths as presented in Table 9. We utilize the full 2009 sentences as our test set: 997 sentences from the dev and 1012 sentences from the devtest. The pivot language pool is chosen as the bridge languages in Fan et al. (2020).\nB Impact of Resource-level of Pivot Languages\nUnder the assumption that high-quality candidates are more adept at conveying the meaning of the source sentence, we select the top-4 paths based on scores on FLORES-200. To verify this hypothesis, we conduct experiments using mid/low-resource pivot languages. According to the WMT22, we select Ukrainian and Croatian as mid- and low-resource languages. Table 10 shows that using candidates from high-resource languages outperforms those obtained from mid/low-resource languages. The quality of candidates is presented in Table 11. In conclusion, since high-resource languages can also provide sufficient diversity, we\nC Metric for Selecting Translation Paths\nWe conduct a comparative analysis between BLEU and COMET, used for selecting the n translation paths. The results in Table 12 indicate that the difference between metrics is marginal. We believe that this stems from the minimal difference in selected paths, as presented in Table 13. We observe some changes in order and minor differences, but\nD Implementation Details of the Merging Modules\nIn this section, we provide detailed explanations of the merging modules. Figure 2 shows the FiD (Izacard and Grave, 2021). First, the instruction and the source sentence are concatenated with each candidate, and processed independently by the encoder.\nThen the decoder takes the concatenation of each representation and generates the final translation.\nAs shown in Figure 3, TRICE is trained with a two-stage fine-tuning method. In the first fine-tuning stage, the model is trained on two different inputs and single targets: Source\u2192Target and Candidate\u2192Target. In the second fine-tuning stage, the source and the candidate are concatenated and provided as a single input.\nE Prompt Templates\nWe use the zero-shot prompt template from Hendy et al. (2023) to instruct the LLMs for translation,\nTranslate this sentence from [source language] to [target language], Source:\n[source sentence]\nTarget:\nwhen ensembling with candidates, we use the prompt template as follows,\nEnsemble the [source language] sentence with the provided [target language]\ncandidates to create the best possible [target language] translation.\n[source language] sentence: [source sentence]\n[target language] candidate k: [target candidate]\nPlease provide only the [target language] translation and no additional text.\n[target language] translation:\nF Open-source LLMs\nIn experiments with LLM-Blender and EVA, we employ the same models as used in each paper. These open-source LLMs are listed in Table 14.\nG Impact of temperature in MBR\nTo investigate the best performance of MBR, we compared it across three different temperature configurations: 1.0, 0.5, and 0.0, which were used in prior works by Farinhas et al. (2023), Suzgun et al. (2023), and Peng et al. (2023), respectively.\nTable 15 and 16 show the quality of MBR outputs and hypotheses under different temperature settings, respectively. Aligning with the findings of the previous study (Peng et al., 2023), we observed that a lower temperature setting achieved better performance. Thus, we set the temperature of 0.0 for MBR in our experiments.\nH Experiments without Training Data\nIn Table 17, we report the results of experiments conducted in a setting where no training data was\nI Datasets Statistics\nTable 18 shows the dataset statistics for each language pair used in the experiments in Table 5.\nJ Additional Analysis on Candidates\nWe conduct experiments to investigate the impact of the value of k in the top-k candidates and its\ncomposition. Figure 4 illustrates the proportion of pivot languages composing the top-k candidates. Top-k candidates, selected by the QE metric, are composed of diverse candidates obtained through various pivot languages. We also observe the same tendency in other datasets. This suggests that generating diverse candidates through multiple paths helps acquire higher-quality candidates.\nFigure 5 presents BLEU scores for different values of the k. The highest BLEU is achieved when k is set to 3. These results demonstrate that more candidates in the aggregation process enhance diversity, thereby increasing the likelihood of providing contextually appropriate information. However, it shows convergence around top-3. We attribute this to the inclusion of candidates with lower estimated scores, such as degenerated sentences. Hence, as k increases, the improvement reaches a plateau.\nK Results with Additional Models\nWe report results with diverse GPT models, GPT-3.5 and GPT-4o-mini, in Table 19. The version of gpt-3.5-turbo-1106 and gpt-4o-mini-2024-07-18 are employed for GPT-3.5 and GPT-4o-mini, respectively."}]}