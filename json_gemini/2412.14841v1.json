{"title": "Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis", "authors": ["Greta Dolcetti", "Vincenzo Arceri", "Eleonora Iotti", "Sergio Maffeis", "Agostino Cortesi", "Enea Zaffanella"], "abstract": "Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues.\nPrevious work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.\nFirst, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.\nOur results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.", "sections": [{"title": "I. INTRODUCTION", "content": "The impressive acceleration we are witnessing in the offer of new and increasingly efficient Large Language Models (LLMs), on the one hand fascinates many researchers and on the other creates distrust and a priori attitudes of refusal.\nOne of the most worrying aspects is the ability of these models to generate source code automatically, with the perceived risk of losing control of the reliability of software systems, in a context in which all the main aspects of our lives depend on them. Many studies already aim at demonstrating the intrinsic weaknesses of LLMs and their applications [1]. A comprehensive list of such weaknesses is reported by OWASP (Open Web Application Security Project) in its Top 10 vulnerabilities for LLMs,\u00b9 where we can find vulnerabilities such as training data poisoning (LLM03) or sensitive information disclosure (LLM06). Despite that, programmers frequently use LLM-powered tools (e.g., GitHub Copilot [2]) for speeding up the development process or to get a starting point for finding solutions [3], be that for simple code snippets, specific functions or entire software applications. There is no doubt that this is a rapidly growing trend: Gartner [4] predicts that by 2027, 50% of enterprise software engineers will rely on AI-powered coding tools, up from fewer than 5% today.\nIn this paper we propose initial steps towards the goal of ensuring that LLM-enabled code generation is safe and trustworthy. Specifically, we aim to: (i) experimentally evaluate the correctness and safety of code generated by LLMs, (ii) measure the ability of LLMs to identify and remediate issues in generated code, and (iii) investigate whether an LLM is better at understanding code generated by itself as opposed to by other models.\nIn order to do so, we propose a testing and analysis framework with three main phases: code generation, self-evaluation and repair. In the code generation phase we ask each LLM to generate code based on a natural language task specification. We then extract and clean the generated code, and we evaluate its correctness, by running it against ground-truth unit tests. Finally, we evaluate the code safety, by using a static analysis tool to look for safety-related issues. In the self-evaluation phase we ask each LLM to evaluate the correctness and safety of the code generated in the previous phase by itself and by the other models. This allows us to test a larger dataset, but also to measure the preference of each model for its own answers (\"self-preference\"). In the repair phase we present an LLM with incorrect or unsafe code, and the description of an issue identified in it during the generation phase. We ask the LLM to repair the code a small number of time, in case the first attempt failed.\nFor our experiments we consider four LLMs: Llama 3 8B and 70B [5], Gemma 7B [6], and Mixtral 8X7B [7]. We chose these popular models because they are recent and high-"}, {"title": "II. EXPERIMENTAL SETUP", "content": "Benchmark Creation\nWe designed a benchmark of inputs to be fed to LLMs to evaluate their code generation capabilities, randomly collecting 100 suitable tasks from the Mostly Basic Python Problems Dataset (MBPP),2 and recasting them to the C language, keeping the same JSON structure provided by MBPP. We opted to randomly select 100 tasks, rather than using the full benchmark suite, to enable a manual and precise investigation of results in later phases. In the code generation phase, LLMs are prompted to generate code to solve each task, generating 100 samples per model. In order to assess whether the identity of the generating model might somehow impact the reasoning capabilities of the model analysing the code, in the following evaluation and repair phases each model evaluates the code generated by itself and the other 3 LLMs, multiplying the number of samples by a factor of 4.\nDuring task selection, we did not select tasks dealing with Python built-in data structures (e.g., tuples), being the corresponding data structure not available as built-in in C. Moreover, besides tailoring the tasks for the C language, we needed to manually refactor the list of unit tests provided by MBPP (3 unit tests for each task). For instance, let us consider the task #940 from MBPP, \u201cWrite a function to sort the given array by using heap sort.\", with their unit tests (provided in the field test_list of MBPP), such as assert heap_sort([12, 2, 4,3]) == [2, 3, 4, 12]. Since C allows array literals only for array initialization, and it does not provide a built-in operator for array comparison, in the example we refactored the unit test as\nNon-)Memorization in Benchmark Design\nNon-memorization is a critical issue in code generation research. Using a crafted C version of the MBPP benchmark, instead of using existing C benchmarks such as HumanEval [9], is a deliberate attempt to minimize the risk of the models having memorized the original Python tasks they could have been trained on. Our solution does not entirely eliminate the risk, as some parts of the task formulation may still overlap with the training data of the considered models. Yet, the primary goal of this work is to evaluate models' ability to detect, repair, and improve safety vulnerabilities and correctness in code, rather than assessing the quality of the LLM-generated code, where a deep treatment of the non-memorization problem would be mandatory. As we will see in Section VI (cf. Table VIII), models frequently generate incorrect solutions, suggesting that the code is generated from scratch rather than reproduced from memorized data. Our solution suggests a direction for future work focused on evaluating a model generalization ability by testing whether the generated C code closely resembles or significantly differs from a direct translation of the original Python task.\nModels\nFor our experiments, we chose the following instruction fine-tuned models, because they are among the most recent and high-performing general models also capable of code generation, in addition to being publicly released and freely accessible.\nAPI\nWe conduct our experiments using the Groq API4 to prompt the LLMs previously described for all the phases that require a direct interaction with the models. Due to the nature of the tasks performed, the temperature value is set to 0.2, following the documentation recommendation to make the output more focused and deterministic.\nPrompt Engineering\nIn the following sections we provide, for brevity, only a brief description of the system and content prompt we actually used to obtain the results being discussed.\nFor each phase of the experimental settings, we tested several prompts, adopting prompt engineering techniques to maximize the adequacy and correctness of the output generated for the requested task. Many of the prompts have been inspired by [11], regarding both the text arrangement and the structure. The prompts were specifically tailored to guide the models in producing outputs that aligned with the specific task constraints, such as compiling successfully and adhering to the function signatures. We used both system prompts, which provided explicit directives to the models, and content prompts, which described the task and requirements. We iterated on their structure and content to achieve a better performance in terms of code generation, self-evaluation, and repair capabilities. The discarded prompts and the corresponding experiments are reported in tables in the Appendix. Each experiment for each prompt was run multiple times, but since the results were consistent, we report the results of a single run.\""}, {"title": "III. CODE GENERATION", "content": "The first phase of our approach is code generation. In this first step, each model is instructed about both its role and output constraints. The system prompt, shown in the"}, {"title": "IV. SELF-EVALUATION", "content": "The second phase of our approach is self-evaluation, where we ask each mode to detect correctness and safety issues in the code generated by itself and other models.\nCorrectness\nWe ask each model whether the generated code correctly solves a task. The collected responses are then tested against the results obtained from the correctness analysis of the first generation phase. The system prompt, shown in Figure 2, instructs the LLM to be a classifier for a correctness check, highlighting the structure of the expected response: a YES or NO followed by an explanation (Chain-of-Thought), as in the code generation phase. Both a correct and incorrect example of responses are provided. The content prompt contains the description of the task and the output previously generated by the model itself or other models.\nSafety\nIn order to detect vulnerabilities, in the content prompt we give the generated source code as input, as well as the"}, {"title": "V. REPAIR", "content": "The third phase of our approach asks the LLMs to repair the correctness and safety issues in the code generated by itself and other models.\nCorrectness\nWe investigate whether models are able to correct the code that does not solve the task according to the specifications. This phase is run only on the compiling programs for which the output of the correctness analysis is different from the one in which all the test passed, so at least a test failed, or an execution or compilation error occurred. In the system prompt, shown in Figure 4, we ask each model to correct generated code so that it properly solves the task, according to the specifications in the description of the task. In the content prompt, we provide the model with the generated code, the task description, the expected signature, and one of the failed tests at a time. We then iteratively repeat this process for a maximum of 6 iterations, as there are at most 3 failed tests for each task, and we want to offer the models two chances to try and fix each failed test. In this case, using Chain-of-Thought reasoning during the non-iterative experiments lead to worse results, so we decided to omit it in the iterative repair phase (see Appendix).\nSafety\nWe investigate whether models can correct the vulnerabilities detected by Infer in generated code. We only run this phase on the compiling source code for which Infer has found at least one issue. For this phase, we prompt the model with the task description, the previously generated solution, and one of the vulnerabilities that Infer has found, asking the LLM to fix it. In the system prompt, shown in Figure 5, we provide general instructions on how to fix the vulnerabilities that have been found. Each input vulnerability is characterized, as reported by Infer, by its type, severity, qualifier, and the code line in which it has been found. As before, the regeneration phase for fixing the vulnerabilities is iterative and can be repeated for a maximum of 6 iterations, following the correctness approach7. Note that in this phase we prompt the models also with the vulnerabilities that, after manual inspection, were found to be false positives. This is done to understand how the models react to the presence of these false positives and from the perspective of a scenario in which even the suspect of a vulnerability should not be tolerated (e.g., in a safety-critical system). Moreover, in a practical usage of this framework there would not be a human in the loop to validate the static analyser reports.\nDuring the experiments for this phase we observed that the Chain-of-Thought and the combination of Chain-of-Though and instruction-based approach seemed to lead to worse results, especially if the code did not compile in one of the iterations; also, the one-shot experiments with only the Chain-of-Thought reasoning lead to similar results compared to the instruction-based approach. Hence, we decided that the instruction-based approach is the best one for this phase, as it led to less confusion in the output provided by the models (see Appendix)."}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "Code Generation\nAfter the code generation phase and subsequent cleaning, we evaluated the number of files that compiled for each model. Unsurprisingly, none of the generated code compiled as-is due to the presence of textual reasoning at the end of the code sections, as required by the prompt. Even after the refactoring and the addition of libraries, some code still does not compile, mostly due to the use of unsupported C built-in functions (e.g., to_string), user-defined functions being called before their definition, or missing syntax symbols.\nThe compilation results are shown in Table II. The 89 tasks compiling for all the models constitute the GEN dataset, that we will use for the next phases (thus GEN consists of 89 tasks \u00d7 4 models = 356 files). The results shows how, with some effort and intervention, the code generated by each model can be made to compile in 93% or more of the cases, with mixtral-8x7b-32768 generating the highest number of compilable files (98%).\nCorrectness. In order to assess if the generated code solves\nSafety. We ran the vulnerability analysis with Infer on the GEN dataset, and the results are shown in Figure 6. The most common vulnerability is the NULL_DEREFERENCE, which is found on average 5.25 times, and it is the only one that is found for each model. Other vulnerabilities, such as MEMORY_LEAK, are found only for some models. Others, like BUFFER_OVERRUN_L2 are found only one time. In total, 44 vulnerabilities are found, with the highest number for mixtral-8x7b-32768 (19 vulnerabilities) and the lowest for gemma-7b-it (7 vulnerabilities). We conducted a manual investigation of each vulnerability detected by Infer and found that 5 of the 44 are false positives: none for gemma-7b-it, one for llama3-70b-8192 (uninitialized value), 3 for llama3-8b-8192 (buffer overrun), and one for mixtral-8x7b-32768 (memory leak). Some generated files contain more than one actual vulnerability.\nBased on the results observed so far, it appears that LLM generated code is more likely to be functionally incorrect than insecure.\nSelf-evaluation\nWe asked each model to assess both the correctness and safety of the GEN dataset. It is worth remarking that the evaluator model does not know how and by which model the code it has to analyze was generated.\nCorrectness. Correctness evaluation is a binary classification task, where models are asked to reply only YES or NO to the input. It is worth remarking that the input consists of a C function generated in the first phase by the evaluated model, and the question is: \"Does this function match the specifications and solve the task?\u201d. In Table V the accuracy, precision, recall, and F1-score for each evaluator model is shown, detailed for each evaluated model. Percentages of accuracy are very low, and F1-scores are all under 80%. These results seem to point out a lack of understanding of the correctness problem by the evaluator models. To investigate the phenomenon, confusion matrices and a preference map were also calculated. The heatmap in Figure 7 shows the preferences of the models for the code generated by themselves and by the other models.\nWe can note that the strategy of gemma-7b-it as evaluator model is to reply (almost) always YES. Conversely, llama3-8b-8192 chooses to reply almost always NO. The other models, llama3-70b-8192 and mixtral-8x7b-32768, tend to reply YES more than NO, but confusion matrices for these models, shown in Figure 8\nSafety. Vulnerabilities detection is a binary classification task, where models are asked to reply only YES or NO to the input. Since the vulnerability analysis performed by Infer detected four types of vulnerabilities, there are 4 queries for each of the 356 GEN samples, totalling 1424 prompts per evaluator model.\nTable VI shows the number of correctly detected vulnerabilities (true positives over vulnerabilities detected by Infer), and the overall accuracy for each evaluator model. Note that the total number of true positives here is 25 instead of 39,"}, {"title": "Repair", "content": "Correctness. We asked the models to repair the 155 files (last three columns of Table III) that were previously incorrect, or for which some errors or timeouts occurred. Note that this includes incorrect files generated both by itself and by all the other models (without knowing which model generated the code, or how).\nWe analyzed the supposedly repaired files using the same correctness analysis pipeline as before, and the results are shown in Tables VII and VIII. We can see how llama3-70b-8192 fixes the highest number of incorrect files, while gemma-7b-it fixes the lowest one. When analyzing the results for each model performing the repair phase, as reported in Table VIII, we can note that there is no emergent trend of self-preference (i.e., a model that fixes better the code generated by itself). On the contrary, for all the models except gemma-7b-it, the percentage of corrected files is\nSafety. We asked the models to repair files for which Infer had detected at least one vulnerability, namely the ones reported in the last column of Table IV. This includes the false positives reported by Infer, to reflect a practical application of this approach, where the static analyser is run as part of a code generation pipeline without a human in the loop to verify"}, {"title": "VII. RELATED WORK", "content": "Due to the increasing adoption of LLM to perform coding tasks, new studies regarding the quality and safety evaluation of AI-generated code have rapidly emerged. Some studies concern only the generation and assessment of AI-generated code, and focus on the evaluation of CWE vulnerabilities. One of these is the FormAI Dataset [15] that performed vulnerability evaluation using Efficient SMT-based Bounded Model Checker (ESBMC) [16] on 112000 AI-generated C programs using a dynamic zero-shot prompting technique, revealing that 51.24% of the programs contain vulnerabilities, posing significant risks to software security. Pearce et al. [17] examined cybersecurity weaknesses, focusing on Copilot. Their study reveals that Copilot introduces vulnerabilities in several cases, demonstrating that it is not yet capable of guaranteeing the security of the generated code. Additionally, experimenting with various strategies to improve Copilot's security, none of these proved to be entirely effective in removing vulnerabilities.\nTo further explore LLMs performance, other works focus on code repair scenarios [18]\u2013[25]. [18] measures the self-healing capabilities of LLMs using ESBMC to identify vulnerabilities and produce counterexamples, which are then fed to gpt-3.5-turbo [26], showing a high success rate in repairing vulnerabilities like buffer overflow and pointer dereference failures. Frameworks for program repair based on static analyzers like InferFix [19] employ LLMs fine-tuned on bug repair patches to solve core repairing tasks for three types of bugs. Other studies tried to show the possible outcomes of integrating LLMs into"}, {"title": "VIII. CONCLUSIONS", "content": "In this paper, we present an extensive evaluation of Large Language Models (LLMs) in generating code that complies with user prompts, evaluating (in)correctness, and addressing potential vulnerabilities using static analysis, with the addition of a self-awareness evaluation on these two topics. Our study involved the creation of a benchmark from the Mostly Basic Python Problems Dataset (MBPP) adapted to the C language.\nAfter the generation phase, we analyzed the compiling programs using the Infer static analyzer and we ran tests to find potential vulnerabilities and errors. Using this feedback, we evaluated the ability of LLMs to repair code regarding both safety and correctness.\nOur experiments revealed a gap in LLMs' ability to generate fully compliant and secure code autonomously. The static analysis highlighted some vulnerabilities, underscoring the need to integrate robust verification and validation steps into the code generation process. Furthermore, our findings indicate that LLMs exhibit a very low degree of self-awareness about the correctness and safety of their generated code. On the other hand, when prompted with issues, on both safety and correctness, these models show some capabilities of repairing the code they or other models have produced. Our results also emphasize the necessity for continued advancements in LLMs\u2019 understanding and application of secure coding practices. Moreover, automated pipelines like the one we propose could help create safer and more reliable code generation.\nThe proposed pipeline is designed to be fully modular, allowing for a rapid adaptation and easy interchangeability of considered LLMs, static analyzers, and datasets. A promising direction for future work would be to investigate how to integrate a combination of static analyzers, such as Infer and CodeQL, to leverage their complementary strengths and reduce the risk of false positives and negatives.\nConcerning safety repair, our current approach is based on static analysis using Infer. Future works will explore hybrid"}, {"title": "IX. DATA AVAILABILITY", "content": "We provide (i) the benchmark suite of tasks (Section II), including both the raw and cleaned source code generated by the models we considered, (ii) the source code for the scripts used in our experimental evaluation (Section VI), (iii) the analysis results computed by Infer, covering both the vulnerability analysis and the repair phases."}]}