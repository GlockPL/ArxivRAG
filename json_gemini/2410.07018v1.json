{"title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization", "authors": ["Chengtao Jian", "Kai Yang", "Yang Jiao"], "abstract": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel Tri-level learning framework for Time Series OOD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an e-stationary point is bounded by O(1/\u03b5). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.", "sections": [{"title": "1 Introduction", "content": "In machine learning, a common challenge arises when the distributions of training and test sets differ significantly [Qui\u00f1onero-Candela et al., 2009]. This mismatch demands that models, trained on specific distribution data, should generalize well on unseen distribution data, known as OOD generalization [Shen et al., 2021, Zhou et al., 2022]. Despite a vast amount of research on the OOD generalization [Zhang et al., 2017, Sagawa et al., 2019, Huang et al., 2020, Arjovsky et al., 2019], the field of OOD generalization in time series is relatively limited and presents more significant challenges. This is primarily due to the inherent temporal dependencies and dynamic changes characteristic of time series data [Hamilton, 2020]. Therefore, a critical aspect of improving time series OOD generalization is to learn robust representations that remain stable despite shifts in distributions.\nRecently, the field of machine learning has witnessed remarkable advancements in pre-trained foundation models, with notable examples including Large Language Models (LLMs) such as GPT [Radford et al., 2018], LLaMA [Touvron et al., 2023] and CLIP [Radford et al., 2021]. These models have been instrumental in capturing and leveraging complex patterns across various domains. In addition, using foundation models, especially LLMs, in processing non-linguistic data, e.g., time series is increasingly drawing attention. By fine-tuning only a few handful of parameters, these models show remarkable versatility in diverse data formats ranging from audio [Ghosal et al., 2023], image [Lu et al., 2021] and time series [Chang et al., 2023, Jin et al., 2023]. Studies indicate that LLMs, as part of the broader foundation model spectrum, demonstrate sophisticated reasoning and strong pattern recognition capabilities [Wang et al., 2023, Chu et al., 2023], fundamentally acting as pattern machines [Mirchandani et al., 2023]. Moreover, LLMs have been shown to be effective in transfer learning across various modalities, due to their data-independent self-attention mechanism [Zhou et al., 2023].\nAdditionally, recent advancements in vision-language foundation models have shown promising developments in OOD generalization [Zheng et al., 2022], yet the exploration in time series remains underdeveloped. The potential of using foundational models is highlighted by the study in Liu et al. [2023a], Hendrycks et al. [2020], which suggests that pre-trained transformers can improve OOD robustness. Despite existing efforts, the limited exploration of foundational model applications in time series OOD generalization suggests an emerging field.\nIn this paper, we propose a Tri-level learning framework for Time Series OOD generalization, named TTSO. Unlike conventional OOD generalization methods that focus solely on group-level [Jiao et al., 2022a, Huang et al., 2020] or sample-level uncertainties [Zhang et al., 2017, Zhou et al., 2021, Han et al., 2024], our framework uniquely addresses both by combing a minimization problem for optimal model parameter learning, a maximization problem for dynamically data re-grouping, and another maximization problem for data augmentation under a tri-level framework. To tackle this tri-level problem, we propose a stratified localization algorithm via cutting planes. Leveraging the advanced representation learning capabilities of LLMs, we adapt this tri-level learning framework for fine-tuning LLMs.\nOur contributions can be summarized as follows:\n\u2022 Tri-level Learning Framework. In contrast to most existing works in OOD generalization, which primarily focus on either group-level or sample-level uncertainties, TTSO uniquely integrates both aspects under a tri-level learning framework. Specially, this comprehensive framework emphasizes the interdependent relationship between problems of each level, advancing beyond the typical single or bi-level methodologies in OOD generalization. Moreover, a theoretical framework based on Vapnik-Chervonenkis dimension has been developed to rigorously analyze and elucidate the generalization properties of TTSO. We then leverage this tri-level framework to fine-tune LLMs, achieving an maximum 4.9% improvement in performance on time series classification in OOD scenarios.\n\u2022 Stratified Localization Algorithm. To tackle the aforementioned tri-level optimization problem, we develop a stratified localization method using cutting planes. Unlike traditional gradient-based methods, TTSO removes the necessity of computing the hypergradient for the outer optimization problem. This computation is typically very challenging and computationally intensive due to the nested structure of the tri-level optimization problem. Furthermore, the decomposable nature of cutting planes offers a promising avenue for enabling distributed implementations of TTSO, thereby potentially enhancing scalability and computational efficiency.\n\u2022 Iteration Complexity Analysis. To validate the effectiveness of our method, we conducted a thorough theoretical analysis of the algorithm. We theoretically derive that the iteration complexity of the proposed algorithm for achieving an e-stationary point is bounded by O(1/\u03b5)."}, {"title": "2 Related Work", "content": "In this section, we provide an overview of the foundational concepts and methodologies related to our research, including OOD Generalization and the LLM in time series.\nOOD Generalization. OOD Generalization research focuses on improving the model's ability to generalize when there is a difference in distribution between the training and test data, and has been widely studied in the fields of Computer Vision (CV) [Recht et al., 2019, Salman et al., 2021] and Natural Language Processing (NLP) [Tu et al., 2020, Schneider et al., 2020]. Existing works for out-of-distribution (OOD) generalization are diverse and can generally be categorized into approaches that consider sample-level [Zhang et al., 2017, Zhou et al., 2021] or group-level [Sagawa et al., 2019, Huang et al., 2020] uncertainty. However, the exploration of OOD generalization specially for time series remains relatively underdeveloped. A recent study [Lu et al., 2023] introduced 'Diversify', an innovative approach that models time series data from the perspective of distribution and obtain superior performance. In our work, we consider both sample-level and group-level uncertainties and formulate them as a tri-level optimization problem.\nLLM in Time Series. The integration of Large Language Models (LLMs) in time series analysis is a rapidly evolving field, drawing significant interest due to their superior pattern recognition and reasoning abilities [Wang et al., 2023, Chu et al., 2023]. A recent example is Time-LLM [Jin et al., 2023], which introduces an innovative method by reprogramming time series and incorporating linguistic prompts, effectively activating the extensive capabilities of LLM. In addition, the OFA framework [Zhou et al., 2023], utilizing the frozen pretrained transformer framework, validates the versatility and effectiveness of pre-trained models in time series analysis. Another innovative approach is PromptCast [Xue and Salim, 2023], which employs a prompt-based learning method, transforming numerical input and output data into prompts for effective forecasting in zero-shot settings. The TEMPO [Cao et al., 2023] adapts to changes in time series distribution by decomposing time series and adding different prompts for each component and obtain competitive performance in time series forecasting. In specialized domains like traffic [Xue et al., 2022], finance [Zhang et al., 2023] and healthcare [Liu et al., 2023b], LLMs have also shown unique advantages. In this work, we aim to enhance OOD robusness for time series by fine-tuning LLMs with TTSO."}, {"title": "3 Problem Formulation and Algorithm", "content": "Notations. X and Y represent the input and target spaces of samples, respectively. The predictor $f_\\varphi = h_w \\circ r_\\theta$ consists of the representation function $r_\\theta(\\cdot)$ with parameter $\\theta$ and the classifier $h_w$ with parameter w. The function $f_P: X \\rightarrow Y$ maps time series $X \\in X$ to $Y \\in Y$, where $X \\in R^{T \\times F}$ and $Y \\in R^{+}$, with T as the time series length and F as the feature dimensions. The multivariate time series X, composed of F univariate time series each with T observations, is sampled i.i.d. from distribution P and represented as $X = [X_1, X_2,...,X_F]$, where $x_i = [X_{v_1},X_{v_2},...,X_{T}]^\\u2020$ for $i = 1,..., F$. Assume source domain distributions are $P_{s_i}$, for $i \\in \\{1, 2, ..., K\\}$ and the target domain distribution is $P_T$. The source domain data $D_{s_i}$ is sampled i.i.d. from $P_{s_i}$, and the target domain data $D_T$ is sampled i.i.d. from $P_T$.\n3.1 Preliminary\nGiven a training dataset $D_{train} = \\{(X_i, Y_i)\\}_{i=1}^{N}$ sampled from the distribution $P_{train}(X, Y)$. In supervised learning, the goal is to learn an optimal predictor $f_\\varphi^*$ on $D_{train}$ such that $f_\\varphi^*$ gener- alizes well on a test dataset $D_{test}$ sampled from the distribution $P_{test}(X, Y)$. In self-supervised contrastive learning, for a given time series X, we generate two augmented views, $X_{a_1}$ and $X_{a_2}$, using augmentation methods $a_1, a_2 \\in A$. These augmentations produce the time series represen- tations $R_1 = r_\\theta (a_1(X)) \\in R^{T \\times M}$ and $R_2 = r_\\theta (a_2(X)) \\in R^{T \\times M}$, through the representation function $r_\\theta$. The objective of contrastive learning is to minimize the distance between positive pairs $(R_1, R_2)$ while maximizing the distance between positive and negative pairs. The general formula for contrastive loss, as detailed in Zhao et al. [2022], is formulated as follows\n$l_{con} = l_{align} (r_\\theta; P, \\pi) + \\lambda l_{reg} (r_\\theta; P, \\pi).$ (1)\nThe first term aims to minimize the distance between positive pairs in the latent space, while the second term served as a regularizer prevents representation collapse. To evaluate the performance of the model, a classifier $h_w$ is trained using the representation function $r_\\theta^*$\n$h_w^* = arg min_h \\mathbb{E}_{(X,Y) \\sim P_{train}} (l_{sup} (h_w \\circ r_\\theta^* (X), Y),$ (2)\nwhere the representation function $r_\\theta^*(\\cdot)$ is optimized via the supervised loss in Eq. (2). The classification is performed using $f_{\\varphi*} = h_w^* \\circ r_\\theta^*$. However, the discrepancy between the training distribution $P_{train}(X, Y)$ and the test distribution $P_{test}(X, Y)$ poses a challenge for generalizing $f_\\varphi^*$ to test data. Directly optimizing $l_{sup} (f(X), Y)$ may lead to overfitting, compromising performance on unseen data. To mitigate this issue, invariant representation learning [Arjovsky et al., 2019] is employed to handle distribution shifts by learning robust invariant representations across diverse distributions. To achieve this, we begin with the following assumption.\nAssumption 1 (Invariant Assumption [Zhao et al., 2022]). Considering K different environments (domains) E, there exists a random variable $\\psi(X)$ such that for any e, e' \u2208 supp(E), it holds that\n$P (Y | \\psi (X_e)) = P (Y | \\psi (X_{e'})).$\nThis assumption implies that for time series X observed in different environments, invariant rationales exist and their relationship with the corresponding labels remains stable. This stability ensures that predictions remain consistent across various environments, relying on these rationales. Assuming $\\psi(.)$ represents the representation function $r_\\theta(\\cdot)$ parameterized by $\\theta$, then it follows that\n$r_{\\theta^*} (X_e) = r_{\\theta^*} (X_{e'}) = \\psi(X).$ (3)\nIn contrastive representation learning, where labels are not available, the theoretical analysis of the downstream performance is challenging. To address this, research [Zhao et al., 2022] bridges this gap by connects contrastive loss to downstream risks,\n$R(h_w \\circ r_\\theta; P) \\leq c ||h_w|| \\sqrt{K \\sigma (l_{align}(r_\\theta; P,\\pi))} + ||h_w|| \\tau(\\sigma,\\delta) + \\sum_{k} P(C_k) ||l_{k}-h_w \\circ \\mu_{k}(r_\\theta; P_r)||$ (4)\nwhere c is a positive constant, $\\tau(\\sigma, \\delta)$ represents a set of constants determined solely by the $(\\sigma, \\delta)$- augmentation, and $C_k$ denotes the set of data points in class k. The first term is optimized during contrastive pre-training. The second term depends on data augmentations $(\\sigma, \\delta)$. The third term, related to the linear layer $h_w$, is optimized in downstream tasks. As shown by Eq. (4), contrastive learning on distribution P with augmentation function $\\pi$ essentially optimizes the upper bound of the supervised risk.\nEach environment \u03b5 corresponds to a domain distribution $P_{s_i}$. To learn an invariant representation over the domain set P, we first provide a mathematical definition of invariant risk minimization.\nDefinition 1 (Invariant Risk Minimization [Arjovsky et al., 2019]). If there exists a classifier $h_0$ that is optimal for all domains in P, i.e., $h_0 \\in argmin_h R (h \\circ r_\\theta; P_{s_i}), \\forall P_{s_i} \\in P$, then the representation function $r_\\theta$ elicits an invariant predictor $h_0 \\circ r_\\theta$ across the domain set P.\nThis definition is equivalent to learning features that have a stable association with the target variable, which has been theoretically and empirically proven to improve the transferability of supervised learning across different distributions [Arjovsky et al., 2019, Zhao et al., 2022].\n3.2 A Tri-level Learning Framework\nTo address OOD challenges, GroupDRO [Sagawa et al., 2019] propose a mimax formulation to minimizes the maximum domain supervised loss to enhance robustness against unseen data. According to Eq. (4), contrastive learning optimizes the upper bound of supervised risk. Thus, we extend GroupDRO by replacing the supervised loss with a self-supervised contrastive loss, aiming to learn invariant representations. We further impose constraints on the group distribution q to mitigate the risk of overfitting to specific domains. This results in a bi-level optimization problem\n$\\begin{aligned}\n&\\underset{\\theta,\\alpha}{\\text{min}} \\quad \\sum_{i=1}^{K} q_i l_{con}(r_\\theta; D_{s_i}, \\pi) \\\\\n&\\text { s.t. } \\quad q=\\underset{q^{\\prime} \\in \\Delta^{K}}{\\operatorname{arg max}} \\sum_{i=1}^{K} q_i^{\\prime} l_{con}(r_\\theta; D_{s_i}, \\pi) \\\\\n&\\text { s.t. } \\quad d(p, q^{\\prime}) \\leq \\tau,\n\\end{aligned}$ (5)\nwhere $d(\\cdot,)$ denotes a distribution distance metric, such as KL divergence, Wasserstein distance, or Euclidean distance, $ \\Delta^{K}$ is a probability simplex, and \u03c4 is a constant. Following previous work [Qian et al., 2019], we adopt the Euclidean distance due to its strong convexity, which reults in faster convergence [Rakhlin et al., 2012]. The outer optimization seeks the best parameters across all domains to optimize overall performance, while the inner optimization, representing the group-level uncertainty, optimizes the worst-case distribution to enhance representation robustness.\nDefinition 2 (Augmentation Robust Alignment Loss [Zhao et al., 2022]). For any two augmentation methods a, a' \u2208 A, the robust alignment loss is defined as follows\n$l_{ar}(r_\\theta; P) := \\mathbb{E}_{x \\sim P} \\underset{(a,a^{\\prime}) \\in A}{\\text{sup}}||r_\\theta(a(X)) - r_\\theta (a^{\\prime}(X))||_2 .$\n(6)\nFor the detailed derivation and proof of Proposition 1, please see Appendix A.1.\n3.4 TTSO for Fine-tuning LLMs\nLLMs have garnered considerable attention in time series applications [Jin et al., 2023, Zhou et al., 2023]. The emergent abilities of LLMs, especially in OOD scenarios, largely depends on the robustness of their representations[Wang et al., 2023, Chu et al., 2023]. This section connects the established theoretical foundation with the practical application of fine-tuning LLMs for time series OOD generalization. We adapt TTSO framework for fine-tuning LLMs to enhance the performance in time series OOD generalization. Our proposed method involves a dual-stage fine-tuning method tailored for time series. The main process of fine-tuning are described below.\nTime Series Pre-processing. Preprocessing starts with an input projection layer to bridge the gap in dimensions between raw time series data and the LLM's native embedding dimension. This step is crucial for the LLM's effective integration of time series. Following this, positional encoding is applied to preserve the sequential integrity of the time series.\nDual-stage Fine-tuning Method. In the first stage, we employ TTSO framework to fine-tune LLMs, in line with the previously mentioned tri-level optimization framework as illustrate in Eq. (10). We adopt the contrastive loss function designed for time series from Yue et al. [2022]. In the second stage, the learned weights of the LLM, including the projection layer, are transferred to the downstream fine-tuning stage for time series classification. To retain the knowledge learned by the LLM from the corpus, we follow Chang et al. [2023], Zhou et al. [2023] by fixing the weights of the fully connected and attention layers, using Layer Normalization Tuning [Lu et al., 2022a] to adjust only the layer normalization parameters, making the affine transformation trainable.\nConstrained Optimization for Fine-Tuning. Research [Wortsman et al., 2022] indicates that adopting radical strategies for fine-tuning models, such as larger learning rates, can reduce out-of-"}, {"title": "4 Convergence Analysis", "content": "Assumption 3 (Lipschitz Continuity of Gradient). Assume that the gradient of the function F is Lipschitz, continuous, i.e., for any x, y, there exists L > 0 such that:\n$||\\nabla F(x) - \\nabla F(y)|| \\leq L ||x - y || .$ (24)\nAssumption 4 (Unbiasedness and Variance Bound of Stochastic Gradients). Assume for the stochastic gradients $g_\\theta$, $g_q$, $g_\\delta$, the following conditions are satisfied\n$\\begin{aligned}\n&\\mathbb{E}_{\\tau} [g_{\\theta}(\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{\\theta}F(\\theta^t, q^t, \\delta^t)] = 0, \\\\\n&\\mathbb{E}_{\\varsigma} [g_{q}(\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{q}F(\\theta^t, q^t, \\delta^t)] = 0, \\\\\n&\\mathbb{E}_{\\varsigma} [g_{\\delta} (\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{\\delta}F(\\theta^t, q^t, \\delta^t)] = 0, \\\\\n&\\mathbb{E}_{\\varsigma} [||g_{\\theta} (\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{\\theta}F(\\theta^t, q^t, \\delta^t) ||^2] \\leq \\sigma_\\theta^2, \\\\\n&\\mathbb{E}_{\\varsigma} [||g_{q}(\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{q}F(\\theta^t, q^t, \\delta^t) ||^2] \\leq \\sigma_q^2, \\\\\n&\\mathbb{E}_{\\varsigma} [||g_{\\delta} (\\theta^t, q^t, \\delta^t; \\varsigma) - \\nabla_{\\delta}F(\\theta^t, q^t, \\delta^t)||^2] \\leq \\sigma_\\delta^2,\n\\end{aligned}$ (25)\nwhere $\\mathbb{E}_{\\varsigma}[]$ denotes the expectation over the $\\varsigma$.\nAssumption 5 (Bounded Gradient). Assume that the gradient of the function F is bounded, i.e., $\\forall t, ||\\nabla_{\\theta}F(\\theta^t, q^t, \\delta^t)||^2 < a_\\theta^2, ||\\nabla_{q}F(\\theta^t, q^t, \\delta^t)||^2 < a_q^2, ||\\nabla_{\\delta}F(\\theta^{t}, q^t, \\delta^t)||^2 < a_\\delta^2$.\nDefinition 3 (e-Stationary Point). Following Xu et al. [2023], Jiao et al. [2024], a point ($\\theta, q, \\delta$) is considered an e-stationary point (where \u0454 > 0) of a differentiable function F if the sum of squares of its gradients on these variables satisfies $||\\nabla G_t|| < \\epsilon$. Let T(e) be the index of the first iteration that satisfies $||\\nabla G_t|| < \\epsilon$, i.e., $T(\\epsilon) = min\\{t | ||\\nabla G_t|| < \\epsilon, t > t_1\\}$.\nTheorem 4 (Convergence Guarantee). With the continuous addition of cutting planes, the optimal objective value of the approximated problem, delineated in Eq. (17), is guaranteed to converge monotonically. For further details, see the proof of Theorem 4 in appendix A.4.\nTheorem 5 (Convergence Rate). Under the assumptions 3, 4, and 5, by setting the step-sizes as $\\eta_\\theta = \\eta_q = \\eta_\\delta = \\frac{1}{\\sqrt{T}}$ and the batch size as B, for a given e, it follows that\n$T(\\epsilon) \\sim O(t_1 + \\frac{L^2(m(a_\\theta^2 + a_q^2 + a_\\delta^2) + \\sigma_\\theta^2 + \\sigma_q^2 + \\sigma_\\delta^2)^2}{4m^2(\\epsilon - F(\\theta^{t_1}, q^{t_1}, \\delta^{t_1}) + F^*)^2})$ (26)\nwhere $F^*$ represents the lower bound of F. The proof of Theorem 5 is detailed in appendix A.5."}, {"title": "5 Experiment", "content": "To evaluate the proposed TTSO framework, we conduct experiments on 6 real-world time se- ries datasets using the leave-one-domain-out setting, including HHAR [Blunck et al., 2015], PAMAP [Reiss, 2012], WESAD [Philip Schmidt et al., 2018], SWELL [Koldijk et al., 2014], USC-HAD[Zhang and Sawchuk, 2012] and DSADS [Barshan and Altun, 2013]. We compare with baseline method ERM [Vapnik, 1991] and 8 general OOD generalization methods: IRM [Arjovsky et al., 2019], GroupDRO [Sagawa et al., 2019], ANDMask [Parascandolo et al., 2020], RSC [Huang et al., 2020], Mixup [Zhang et al., 2017], VERx [Krueger et al., 2021], DIFEX[Lu et al., 2022b]. And we further compare with 2 recent strong approach in time series: AdaRNN[Du et al., 2021] and GILE [Qian et al., 2021]. We also include DIVERSIFY[Lu et al., 2023], DFDG[Zhang et al., 2021], and CCDG[Ragab et al., 2022], three methods specifically designed for time series OOD generalization.\n5.1 Main Results\nWe report the average results over 3 runs for each dataset, along with the standard deviation. The results for the HHAR, PAMAP, and WESAD datasets are shown in Tables 1, where our method outperforms the second-best baseline by 2.8%, 4.8%, and 4.9% respectively. Additional results are provided in Appendix E.1 (Table 4). These results demonstrate the superiority and effectiveness of the TTSO framework, as it accounts for both sample-level and group-level uncertainties, which optimizes the upper bound in Theorem 2.\nCompared to traditional methods like ERM, IRM, and GroupDRO, both TTSO and TTSO* show more consistent and generally superior performance in OOD generalization, highlighting the advantages of a tri-level learning framework. The TTSO* method, which incorporates LLM fine-tuning, consistently outperforms other approaches, demonstrating the effectiveness of LLM with TTSO fine-tuning in enhancing OOD generalization for time series. Concurrently, the TTSO method, even without LLM fine-tuning, shows strong generalization performance, especially on the HHAR dataset where it closely matches TTSO* results. This indicates that the TTSO framework is highly effective in generalizing across different scenarios, even in the absence of LLM.\n5.2 Ablation Study\nThis ablation study is conducted to further understand the impact of our TTSO framework's fine-tuning on model performance. We compare four distinct variants: a pretrained GPT2 fine-tuned with TTSO (TTSO++), a pretrained GPT2 without TTSO fine-tuning (TTSO+\u00af), a randomly initialized GPT2 fine-tuned with TTSO (TTSO\u00af+), and a randomly initialized GPT2 without TTSO fine- tuning (TTSO\u00af\u00af). This comparison helps in quantifying the effec- tiveness of the TTSO fine-tuning strategy in enhancing the model's OOD generalization capabilities.\nThe ablation results are presented in Figure 2. From this results, we can see that: (a) TTSO++ demonstrates the best performance in all scenarios, further validating that the combination of a pre-trained GPT2 model with TTSO fine-tuning can significantly improve the model's OOD generalization capabilities. (b) Although TTSO+ does not employ TTSO fine-tuning, it still exhibits relatively good performance. This suggests that the pre-trained GPT2 model has an intrinsic capacity for OOD generalization, consistent with previous empirical studies [Zheng et al., 2022, Hendrycks et al., 2020]. (c) Compared to TTSO\u00af\u00af, TTSO\u00af+ applies TTSO to fine-tune on a randomly initialized GPT2, TTSO++ achieves improved performance. This demonstrates that even in the absence of a pre-trained model, TTSO fine-tuning can effectively enhance the model's OOD generalization capabilities, though not as significantly as that with a pre-trained GPT2."}, {"title": "6 Conclusion", "content": "Existing OOD generalization methods mainly focus on sample-level uncertainties or group-level uncertainties, often overlooking the interplay between these two aspects. In light of this, we propose the TTSO framework to integrate both sample-level and group-level uncertainties within a unified tri-level learning approach, thereby enhancing the model's robustness and adaptability in facing diverse and unforeseen distribution shifts. In addition, this innovative framework introduces a fresh perspective for the development and analysis of the Out-of-Distribution (OOD) generalization problem. Based on this formulation, we develop a stratified localization algorithm for the tri-level optimization problem and provide theoretical analysis regarding the iteration complexity of the proposed algorithm. Comprehensive studies have been carried out to assess the performance of the proposed algorithm and substantiate the theoretic claims. It is seen that TTSO with LLM can considerably improves the performance of time series OOD generalization."}, {"title": "F Limitation", "content": "TTSO is a general framework for learning invariant representations across diverse domain distri- butions, currently discussed only for time series classification. This framework could be further enhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly detection. Additionally, distribution shifts occur not only in time series but also in other machine learning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach to these domains could further improve performance."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and introduction, we clearly state the main contributions of our work. The tri-level learning framework, stratified localization algorithm and iteration complexity analysis is in Section 3.2, 3.3 and 4.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations of this work can be found in Appendix F.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be."}]}