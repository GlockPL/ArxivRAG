{"title": "A Local Information Aggregation based Multi-Agent Reinforcement Learning for Robot Swarm Dynamic Task Allocation", "authors": ["Yang Lv", "Jinlong Lei", "Peng Yi"], "abstract": "In this paper, we explore how to optimize task allocation for robot swarms in dynamic environments, emphasizing the necessity of formulating robust, flexible, and scalable strategies for robot cooperation. We introduce a novel framework using a decentralized partially observable Markov decision process (Dec_POMDP), specifically designed for distributed robot swarm networks. At the core of our methodology is the Local Information Aggregation Multi-Agent Deep Deterministic Policy Gradient (LIA_MADDPG) algorithm, which merges centralized training with distributed execution (CTDE). During the centralized training phase, a local information aggregation (LIA) module is meticulously designed to gather critical data from neighboring robots, enhancing decision-making efficiency. In the distributed execution phase, a strategy improvement method is proposed to dynamically adjust task allocation based on changing and partially observable environmental conditions. Our empirical evaluations show that the LIA module can be seamlessly integrated into various CTDE-based MARL methods, significantly enhancing their performance. Additionally, by comparing LIA_MADDPG with six conventional reinforcement learning algorithms and a heuristic algorithm, we demonstrate its superior scalability, rapid adaptation to environmental changes, and ability to maintain both stability and convergence speed. These results underscore LIA_MADDPG's outstanding performance and its potential to significantly improve dynamic task allocation in robot swarms through enhanced local collaboration and adaptive strategy execution.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the continuous advancement of modern technology, robot swarms have emerged as a significant research area, adept at handling complex tasks such as UAV swarms [1], adhoc network relay [2], and cooperative tracking control [3]. These swarms, comprising numerous small robots, excel in cooperative collaboration, underscoring the potential of collective intelligence [4]. However, efficiently coordinating these swarms for large-scale, complex tasks presents considerable challenges. A primary hurdle is the task allocation problem, which involves intelligently distributing tasks among the robots to optimize performance [5]. This issue is crucial in robotics and has broader implications for fields like industrial automation [6], emergency rescue [7], and environmental monitoring [8]. Thus, the study of task allocation in large-scale robot swarms has become a focal point for both academic and industrial communities.\nTask allocation in dynamic environments poses significant challenges in robotics. Existing research primarily focuses on scenarios involving unexpected events, such as the sudden addition or removal of robots or tasks. These changes typically occur in contexts where they are manageable and infrequent, which allows for only periodic adjustments [9], [10]. However, some real-world situations may demand a wider variety of tasks and more frequent changes, necessitating continual adaptation [11]. Tasks may range from simple data collection, requiring only linear movement, to complex environmental monitoring that necessitates intricate pathways and variable speeds for effective coverage. The inherent variability of these tasks creates a strongly dynamic environment that requires consistent reallocation efforts by the robots. Furthermore, as the size of the swarms increases, the complexity of the task allocation process also escalates due to the expanded search space [12], posing challenges for timely responses. This study aims to address these challenges by proposing new communication protocols and coordination mechanisms to effectively manage task allocation in large-scale dynamically changing environments.\nTo address the challenges of task allocation, typical planning methods are categorized into centralized and distributed approaches. Centralized methods rely on a central planning system that collects all task information and uses various algorithms to devise task assignment strategies for each robot [13], [14], [15]. However, the dynamic nature of real-world tasks makes a one-time, global task allocation impractical [16]. Consequently, researchers have shifted towards dynamic task allocation methods that involve periodic re-planning to adapt to changing conditions and environmental dynamics [17], [18]. Nevertheless, the real-time execution of centralized planning algorithms can be time-consuming and complex, especially with a large number of robots.\nDistributed task allocation methods generally offer higher computational efficiency [19], [20], [21]. These methods adapt their objectives based on the communication dynamics among agents."}, {"title": "II. PROBLEM DESCRIPTIONS", "content": "In this section, we begin by delving into the details of modelling the robot swarm task allocation problem in dynamic task environments. Subsequently, we reformulate this problem as a Dec_POMDP.\nA. Robot Swarm Task Allocation Problem\nThe robotic swarm task allocation problem presents a challenging scenario where a swarm of available robots needs to be allocated to mobile tasks in dynamic environments. The primary objective is to efficiently assign robots to tasks according to some specific performance metric. This problem comprises three essential components. The first part focuses on the mobile task set and their corresponding motion models. The second part addresses the robot swarm network and provides a detailed description of the associated distributed communication model. The last part designs an appropriate performance metric and introduces the optimization model.\na) Movable Task Set: Define $M = \\{1, 2, ..., M\\}$ as the set of movable tasks within the time series $T := \\{0, 1, ..., T\\}$. Let $v_m^j$ represent the speed of task $j \\in M$, and $\\theta_m^j$ denote the movement angle of task j at time t. Let $P_{m,t}^j = (x_{m,t}^j, y_{m,t}^j)$ represent the two-dimensional spatial coordinates of task j at time t. The motion state of the tasks is determined by their speed and movement angle and can be updated using (1).\n$P_{m,t+1}^j = \\begin{bmatrix} x_{m,t}^j + v_m^j \\cdot cos(\\theta_{m,t}^j) \\\\ y_{m,t}^j + v_m^j \\cdot sin(\\theta_{m,t}^j) \\end{bmatrix}$ (1)\nwhere $T$ represents the decision time step.\nEach task in M requires a substantial allocation of robots while maintaining a moderate demand, which means it has two characteristics: (1) each task necessitates the allocation of multiple robots for execution, and (2) there is a limitation on the number of robots required per task. Therefore, we define $h^j$ as the maximum number of robots that task $j \\in M$ can accommodate.\nb) Robot Swarm Network: Define the robot swarm network as $G = \\{N, E\\}$, where $N = \\{1, 2, . . ., N\\}$ represents the node set of robots, and $E \\subset N \\times N$ represents the perception relationships among robots. Each robot corresponds to a node $V$ in the network. For an edge $(i, i') \\in E$, it signifies that robot $i$ can observe information from robot $i'$. Therefore, we can denote the set of all neighboring robots of robot $i$ as $N_i \\triangleq \\{i' \\in N : (i, i') \\in E\\}$. We assume that each robot $i \\in N$ is a rational agent with limited perception and communication capabilities, which can instantly access the status of all tasks and information from neighboring robots $i' \\in N_i$. Based on the observed and received information and the current environment, robot $i \\in N$ can dynamically choose target tasks, and subsequently, moves at a constant velocity $v$ towards the chosen task. It should be noted that the interaction between the robot and the task occurs through data exchange. This network enables the robots to observe and collaborate with neighbors by sharing observation data, task status, and decision outcomes.\nWe define $g_t^i \\in M$ as the target task of robot $i \\in N$ at time $t \\in T$, and $d_{i,g_t^i}$ as the Euclidean distance between robot $i$ and its target task $g_t^i$ at the t-th decision moment. Let $d_{bind} > 0$ denote the predefined association distance, and $T_t^i \\in T$ be the finish time of robot i. If at some time t, robot i's distance from its target task satisfies $d_{i,g_t^i} \\leq d_{bind}$, then robot $i \\in N$ becomes bound to this task $g_t^i \\in M$, and $T_t^i = t$ is set as the binding time. At time $T_t^i$, the robot is considered to have completed the task assignment and it cannot change its target task anymore. Let $P_{r,t+1}^i = (x_t^i, y_t^i)$ represent the coordinates of robot $i \\in N$, and $\\theta_{t+1}^{i,g} = arctan(\\frac{y_{m,t} - y_t^i}{x_{m,t} - x_t^i})$ represent the direction vector of robot $i \\in N$. Then each robot moves towards the chosen target task and can update their motion state according (2). It should be emphasized that once the robot is bound, its motion state will consistently synchronize with its target task.\n$P_{r,t+1}^i = \\begin{cases} x_t^i + v_r cos(\\theta_{t+1}^{i,g}) \\\\ y_t^i + v_r sin(\\theta_{t+1}^{i,g}) \\end{cases}$ (2)\nc) Utility Function: For each robot $i \\in N$, let $u_i$ represent the utility function that robot $i \\in N$ can obtain when assigned to some task $j \\in M$. We design it to comprise the task feedback rewards $u_t^i$ and movement cost reward $u_m^i$. In the following, we will separately introduce them."}, {"title": "III. A NOVEL MARL ALGORITHM WITH LOCAL INFORMATION AGGREGATION", "content": "In this section, we systematically introduce our novel MARL algorithm with local information aggregation, including the main components of the algorithm and the design inspiration behind them.\nA. Key Modules and Mechanisms of LIA_MADDPG\nMADDPG is a classic multi-agent deep reinforcement learning algorithm to address multi-agent problems in mixed cooperative-competitive environments [40]. However, when applied to large-scale problems like the robot swarm task allocation problem considered in this work, MADDPG encounters difficulties in coordinating learning due to scalability issues. To overcome this challenge, we introduce a novel distributed method called LIA_MADDPG, which incorporates the Local Information Aggregation (LIA) module."}, {"title": "B. Overview of the LIA MADDPG Framework", "content": "The LIA_MADDPG framework consists of two distinct phases: the off-line centralized training phase and the on-line distributed execution phase. To enhance the learning process, the framework employs four types of neural networks: an actor network for generating the action policy; a critic network for evaluating the action policy; a target actor network; and a target critic network for stabilizing the learning process. Additionally, a Local Information Aggregation (LIA) module is integrated to accelerate the learning process.\na) Off-line Centralized Training Phase: The off-line centralized training process can be divided into the generation of empirical data and network training, which are executed alternately.\nEmpirical Data Generation: As depicted in the leftmost part of Fig. 3, the empirical data generation phase involves each robot continuously interacting with the environment to collect relevant experiential data. During this process, each robot determines its actions $a_t^i$ based on its current local observation $o_t^i$ using a shared policy network. Moreover, each robot identifies its set of related robots $G_i$ and its local information set $L_i$ according to the previously described method. The LIA module then aggregates this information, applying distance-dependent weight coefficients to emphasize the influence of closer neighbors. This aggregation process yields a comprehensive dataset that encapsulates both individual and collective behavior within the environment. The collected data $(o_t^i, a_t^i, \\varphi_i(o_t^i), \\varphi_i(a_t^i), r_t, o_{t+1}^i)$ are subsequently stored in the experience replay buffer $D$, which serves as a critical resource for optimizing the policy and value networks during the network training phase.\nNetwork Training: After the experience data is generated, the data set is extracted from the buffer $D$ for training based on priority experience replay [46]. At this stage, we need to update $G$ and $\\theta^q$ according to (21) and (22). First, use the target $G'$ to calculate the extended Q function $q_{t+1}$ of the next state, and use the temporal difference method to optimize the parameters of the $G$ network based on the value of the extended Q function. Then, use $G$ to calculate the extended Q value $q_t$ of the current state-action, and use the calculated Q value and the action $a_{t+1}^i$ output by $\\mu$ to update the parameters of the $\\mu$ network.\nb) On-line Distributed Execution Phase:\nLIA_MADDPG's strength lies in its ability to autonomously develop strategies for problem-solving through extensive instance sampling and learning. This end-to-end approach enables it to quickly produce suitable task allocation solutions without complex heuristic rules. However, it struggles in dynamic environments with partial observability, as robots must adapt to variable conditions and make decisions based on incomplete information. Hence, during the distributed execution phase, strategy optimization methods are required to enhance each robot's decisions based on the shared policy network $\\mu$. Therefore, there are two process during the distributed execution phase: policy output and policy improvement, which alternate to facilitate decision-making by each robot."}, {"title": "V. CONCLUDING REMARKS", "content": "This research has addressed a challenging problem known as the robot swarm task allocation problem in dynamic task environment. We have modeled this problem as Dec_POMDP and proposed a novel multi-agent deep reinforcement learning approach, called LIA_MADDPG. In the centralized training phase, we introduce a module for local information aggregation among robots, encouraging them to focus more on information beneficial to themselves during the training process. In the distributed execution phase, we design strategy improvement methods to further enhance the quality of allocation solutions. Finally, through extensive experiments, we have validated the effectiveness and superiority of this method in terms of convergence speed and agent cooperation performance.\nIt is worth pointing out that the current design of LIA_MADDPG is most effective in large-scale homogeneous agent scenarios. It may not be well-suited for environments involving heterogeneous agents, where agents have different capabilities, tasks, and information processing requirements. Additionally, it is of interests to extend our research to incorporate collision avoidance strategies within the robot swarm task allocation framework. Therefore, a potential future direction will involve developing a multi-agent reinforcement learning approach that comprehensively considers both heterogeneous agent scenarios and collision avoidance strategies, ensuring robust performance across diverse and dynamic environments."}]}