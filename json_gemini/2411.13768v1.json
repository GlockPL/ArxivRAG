{"title": "An Evaluation-Driven Approach to Designing LLM Agents: Process and Architecture", "authors": ["Boming Xia", "Qinghua Lu", "Liming Zhu", "Zhenchang Xing", "Dehai Zhao", "Hao Zhang"], "abstract": "The advent of Large Language Models (LLMs) has enabled the development of LLM agents capable of autonomously achieving under-specified goals and continuously evolving through post-deployment improvement, sometimes without requiring code or model updates. Conventional approaches, such as pre-defined test cases and code/model redevelopment pipelines, are inadequate for addressing the unique challenges of LLM agent development, particularly in terms of quality and risk control. This paper introduces an evaluation-driven design approach, inspired by test-driven development, to address these challenges. Through a multivocal literature review (MLR), we synthesize existing LLM evaluation methods and propose a novel process model and reference architecture specifically designed for LLM agents. The proposed approach integrates online and offline evaluations to support adaptive runtime adjustments and systematic offline redevelopment, improving runtime pipelines, artifacts, system architecture, and LLMs by continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators.", "sections": [{"title": "I. INTRODUCTION", "content": "Large-language models (LLMs) are large-scale, pretrained language models with tens of billions of parameters, adaptable to various down stream tasks, such as answering questions and summarizing information [1]. Recent advancements in LLMs have enabled the creation of LLM-based agents, commonly known as LLM agents, which can autonomously perform complex tasks (e.g., the AI Scientist [2]). Unlike traditional AI/LLM systems that rely on detailed instructions to perform specific tasks, LLM agents autonomously achieve high-level, under-specified goals by perceiving context, reasoning, planning, and executing workflows, while leveraging external tools, knowledge bases and other agents to enrich their capabilities [3].\nDespite their productivity potential, LLM agents introduce significant concerns regarding performance and safety [4]. These agents often exhibit unpredictable or inconsistent behavior, especially when dealing with complex or ambiguous tasks. Moreover, the autonomy of LLM agents, such as determining which tools to use, can result in outcomes that diverge from human goals or violate governance standards. Rigorous evaluation is therefore essential to ensure that LLM agents consistently align with intended goals and governance requirements, thereby minimizing unintended consequences.\nWhile there is increasing interest in testing and benchmarking frameworks for LLMs and LLM agents (e.g., [5], [6]), significant gaps remain in addressing the unique challenges of evaluating LLM agents. Specifically, effective evaluation of these agents requires the following considerations:\n\u2022 System-Level Evaluation. Current evaluation frameworks largely focus on the model level, overlooking the system-level evaluation. LLM agents are compound AI systems comprising not only LLMs but also multiple out-of-LLM components [7], such as context engines and guardrails [8]. While tools and frameworks like Lang-Smith\u00b9 and DeepEval\u00b2 provide partial support for system-level evaluation but often focus narrowly on specific interactions, such as prompt-response pairs or tool invocations, without addressing the full runtime scope. Effective system-level evaluation must encompass both pipelines (including prompts, intermediate results, and final results) and artifacts (including goals, memory, reasoning, plans, workflows, tools, knowledge bases, other agents, LLMs, and guardrails) [8], [9].\n\u2022 Evaluation-Driven Design. While existing tools and frameworks address aspects of pre-deployment offline evaluation and runtime monitoring, they often lack a unified approach that integrates continuous evaluation with both runtime adaptation and offline iterative development. Unlike traditional software, LLM agents run with high-level, under-specified goals and dynamically adapt through iterative cycles of goal interpretation, reasoning, planning, and execution. This adaptive behavior, coupled with the reliance on human/AI feedback and memory, necessitates an evaluation-driven design approach to systematically align evolving agent behaviors with quality, safety, and operational objectives.\n\u2022 Use of Evaluation Results. In traditional software and standalone LLM testing, evaluation results narrowly identify failure points, triggering updates to code or retraining/fine-tuning of LLMs. For LLM agents, however, evaluation results have a broader scope and impact. They can inform refined prompt optimization through enriched context (e.g., incorporating positive and negative examples), enhance agent plans and"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "workflows to improve decision-making and execution, and update test and safety cases [10] to reflect evolving operational contexts.\nTo address these challenges, this paper introduces an evaluation-driven approach inspired by test-driven development, embedding continuous evaluation across the entire LLM agent lifecycle and considering comprehensive runtime pipeline and artifacts. We explore two research questions (RQs), each forming a key contribution:\n1) RQ1: How can LLM agent be evaluated systematically? To address this, we propose a structured process model that enables comprehensive, lifecycle-spanning evaluation, providing concrete and actionable guidance for researchers and practitioners in conducting continuous evaluation of LLM agents.\n2) RQ2: How can continuous evaluation be integrated into the architecture of agent design? To answer this, we design a reference architecture that embeds evaluation as a core design element, integrating both online and offline evaluation to inform agent improvement.\nThe remainder of the paper is organized as below. Section II discusses background and related work. Section III introduces the methodological details. Section IV presents a software process model guiding LLM agent evaluation, and Section V proposes a reference architecture for evaluation-driven design of LLM agents. Section VI discusses the threats to validity. Section VII concludes the paper and outlines the future work.\nA. Importance of Evaluating LLM Agents\nEvaluation plays a foundational role in assessing accuracy and functional correctness, ensuring LLM agents perform reliably across diverse, evolving conditions [11]. Beyond performance, assessments of quality and risk attributes, such as fairness and safety, are critical to managing the risks inherent in deploying LLM agents within complex, real-world environments [12], [13]. Additionally, capability evaluations gauge an agent's suitability for complex tasks, identifying both intended functions and potential unintended behaviors that could pose safety concerns [14]. Finally, continuous evaluation supports ongoing improvement and risk control, allowing LLM agents to learn from both evaluation results and operational data, adapt to new inputs, and remain aligned with evolving conditions [15]. Together, these evaluations uphold the safety, adaptability, and real-world applicability of LLM agents across dynamic applications.\nB. Architectural and Operational Challenges in LLM Agent Evaluation\nLLM agents present unique challenges for evaluation due to their complex architecture and dynamic operational demands. Primarily built with off-the-shelf LLMs, these agents shift the focus from (re)training and model/code updates to system-level integration and continuous, out-of-model improvement. Operating autonomously, they must handle evolving inputs and external data sources, requiring adaptability to real-time conditions. Complex architectural components create interdependencies and potential failure points that heighten the need for careful evaluation.\nThe non-deterministic behavior of LLMs, where identical inputs may yield varied outputs, further complicates consistent assessment and demands flexible evaluation methods [16]. For instance, a customer service agent might generate varying responses to identical questions depending on prior interactions, making uniform assessment challenging. Their property of continuous improvement through runtime feedback introduces emergent behaviors, which may pose safety risks that static evaluation methods cannot adequately capture [17]. Together, these challenges underscore the need for an evaluation-driven design approach that embeds continuous assessment mechanisms to ensure safe, reliable, and adaptable operation throughout the agent's lifecycle.\nC. Related Work\n1) Traditional Software Evaluation Methods: Traditional software evaluation methods, including Test-Driven Development [18] and Behavior-Driven Development [19], emphasize early testing to ensure functionality prior to deployment. These methods generally limit evaluation to pre-deployment, with test cases derived from detailed, predefined requirements. Although they offer strong foundations for ensuring performance and safety, they are less suited to the dynamic, evolving nature of LLM agents, which operate based on high-level goals instead of explicit requirements and dynamically adapt their behaviors in response to real-time data and user interactions. Such conventional approaches do not inherently accommodate continuous, post-deployment evaluations, nor do they readily support continuous improvement based on evaluation results.\n2) LLM Agent Evaluation Methods: Existing evaluation frameworks and benchmarks largely stay on model level [20], focusing on specific tasks (e.g., coding [21]\u2013[23] and Retrieval-Augmented Generation (RAG) [24], [25]) or certain domains (e.g., healthcare [26], [27], legal [28], [29], and finance [30], [31]). Although valuable, adapting them to evaluate holistic, multi-component LLM agents that can operate across a wide range of tasks and domains requires significant integration efforts [32], [33].\nFurther, many existing benchmarks rely on fixed datasets and tasks, introducing risks of data contamination [34] and making it difficult to assess agents in dynamic contexts. To address this limitation, a trend toward live benchmarks that adapt to changing real-world data has emerged (e.g., [35], [36]). While live benchmarks offer some adaptability, they primarily use periodic data updates and do not fully capture the nuanced behaviors seen in online evaluation during runtime.\nWith the advent of LLM agents, there is a shift towards system-level evaluation frameworks that assess holistic agent behavior. However, many such frameworks emphasize end-to-end evaluations, assessing the final outputs (i.e., final success/pass rate) of LLM agents across various tasks and environments. While these approaches provide insights into overall agent performance, they often overlook the nuances within"}, {"title": "III. METHODOLOGY", "content": "This study employs a MLR (Fig. 1) to synthesize insights from both academic and industry sources on LLM agent evaluation, following established guidelines [39], [40]. MLR was chosen because it allows for a comprehensive understanding of both academic theories and industry practices, especially crucial for designing a process model and reference architecture that addresses the challenges of continuous, evaluation-driven LLM agent design across various deployment settings.\nA. MLR Planning\nThe planning phase began with identifying the need for a multivocal approach to bridge both research and industry perspectives. An initial research protocol was developed, including RQs and search terms tailored to capture a comprehensive scope of LLM agent evaluation. A pilot study tested the search terms, assessing both relevance and volume of retrieved studies. Based on the pilot, the protocol was refined, resulting in a finalized MLR strategy to guide the study.\nB. Multivocal Literature Review (MLR)\nDatabases and Search Strategy: Academic paper searches were conducted across: Google Scholar, IEEE Xplore, ACM Digital Library, Science Direct, and Springer. Google Search was employed as a source for grey literature. The primary search terms were (\"large language model\" OR \"LLM\" OR \"agent\") AND (\"evaluate\" OR \"benchmark\" OR \"test\"), with variations to capture relevant forms (e.g., pluralization, noun-verb shifts, and -ing endings where applicable). Searches were performed on June 5, 2024, targeting literature published since 2022 to reflect recent advancements following the release of ChatGPT.\nScreening and Selection: After removing duplicates, sources were screened through a three-step process: title, abstract, and full-text review. Two authors independently reviewed each stage to ensure reliability and agreed on the selection results. The selection criteria focused on identifying sources relevant to evaluation-driven design for LLM agents. Eligible sources included academic articles, conference papers, technical reports, white papers, and preprints (e.g., arXiv) addressing tools, frameworks, or platforms for LLM agent evaluation, provided they offered clear theoretical or empirical contributions and were available in English. Exclusions were applied to sources unrelated to LLM agent evaluation (e.g., LLM fine-tuning), lacking substantial contributions or credible backing, or presenting tools without adequate documentation.\nExpanded Search: To broaden coverage and ensure no relevant study was missed, backward and forward snowballing was conducted on the final selection, as per [41]. Additionally, critical new sources published after the initial search were included based on author discussions. Saturation was considered achieved when no significant new themes emerged from additional sources.\nQuality Assessment: To ensure rigor, academic papers and grey literature were evaluated based on Authority, Methodology, and Objectivity, with only sources demonstrating credible and balanced contributions included. Tools were assessed separately, considering Source Reputation (e.g., GitHub stars publisher reputation), Documentation, and Maintenance Activity. Sources that did not meet the criteria for credibility and reliability were excluded.\nData Extraction: Data extraction systematically mapped evaluation-related elements to address the RQs and inform the proposed process model and reference architecture. It focused on evaluation practices, including timing, scope, and the use of evaluation results for continuous improvement and"}, {"title": "IV. PROCESS MODEL FOR LLM AGENT EVALUATION", "content": "intermediate decision-making steps and the contributions of individual agent components, limiting their utility for identifying specific improvement areas [33]. More granular frameworks have thus emerged (e.g., [33], [37]), alongside tools like LangSmith, which support both step-based and trajectory-based evaluations. By emphasizing intermediate stages, these frameworks reveal operational workflows, yielding data essential for targeted improvements and risk control.\nAnother approach gaining traction is the use of safety cases, which provide structured, evidence-backed justifications for the safe operation of LLM agents within defined conditions [10], [38]. They serve as formal documentation for ensuring compliance with safety standards and regulatory requirements. Safety cases are particularly valuable for stakeholders like system developers, integrators, and regulators, who rely on them to assess risks, validate operational boundaries, and maintain accountability. For LLM agents, the dynamic and evolving nature of operational contexts necessitates continuous updates to safety cases to reflect emerging risks and maintain alignment with governance frameworks.\nThese recent advancements mark significant progress in LLM agent evaluation. Nevertheless, a systematic approach that integrates evaluation results to guide ongoing improvement and risk control remains lacking\u2014an approach we address in this work.\nTo address RQ1, we synthesized evaluation practices from the MLR into a process model (Fig. 2). This model provides a structured approach for conducting consistent, lifecycle-spanning evaluations across the agent's development, deployment, and operational stages. In addition to specifying evaluation activities and scope, the model also drives both immediate runtime improvements (e.g., in response to real-time user feedback) and iterative refinements (e.g., through agent architecture adjustments) and supports risk control by leveraging insights from offline and online evaluations. By standardizing these practices, the process model ensures LLM agents remain safe and effective in dynamic environments.\nA. Step 1: Define Evaluation Plan\nThe first step in the process model is to establish a comprehensive evaluation plan, guiding consistent and focused evaluations throughout the LLM agent's lifecycle. This plan integrates key inputs, including user goals, governance requirements, and initial agent architecture, to establish clear evaluation objectives, scope, and context-specific evaluation scenarios.\nInputs: The evaluation plan is informed by three primary inputs:\n\u2022 User Goals: High-level goals representing user needs and expectations, often broad or abstract (e.g., \"I need an agent that assists with tax suggestions\"). These goals are translated into evaluation scenarios, which outline potential user interactions, relevant contexts, and expected outcomes. These scenarios later inform test case creation in Step 2.\n\u2022 Governance Requirements: Legal, ethical, and safety regulations relevant to the agent's deployment context, such as the EU AI Act. Governance requirements guide compliance-focused evaluations such as the effectiveness of guardrails, and influence criteria for test case and safety case generation.\n\u2022 Initial Agent Architecture: The initial structural and behavioral blueprint of the agent, detailing key components, interdependencies, and architectural decisions. This analysis identifies components with critical functions or dependencies that may impact evaluation focus and highlights initial tradeoffs and risks.\nProcess Steps: The evaluation planning process involves several coordinated activities that collectively establish a detailed, context-specific evaluation plan.\n1) Understand User Goals: Operating as one parallel branch, this step translates user goals into concrete and testable evaluation scenarios. These scenarios simulate potential user interactions, relevant environmental factors, and expected outcomes, creating a targeted foundation for assessing both agent behaviors and overall system outcomes.\n2) Incorporate Governance Requirements: Running concurrently with the user goal understanding, this step aligns evaluation objectives and scenarios with relevant governance requirements [43]. By embedding these constraints early in the planning phase, it ensures compliance and risk mitigation from the outset.\n3) Generate Evaluation Plan: This final step consolidates insights from user goals, governance requirements, and architectural analysis to produce a structured evaluation plan, covering both execution pipelines and generated artifacts. This evaluation plan supports continuous evaluation and subsequent improvement and risk control to keep the agent aligned with evolving objectives.\nOutput: The main output of this step is a detailed evaluation plan that provides the foundation for systematic, lifecycle-spanning evaluations. Key information includes:\n\u2022 Evaluation Purpose and Scope: Articulates the overarching goals of the evaluation (e.g., accuracy/correctness, quality/risk, and capability [20]) and specifies evaluation targets, such as specific intermediate pipelines (e.g., retrieval-augmented generation [44]) and artifacts (e.g., dynamically generated plans and sub-goals [45], [46]). These targets are prioritized based on architectural insights and user needs. In addition, guardrail effectiveness can be evaluated through the pipeline and artifacts.\n\u2022 Evaluation Objectives and Strategy: Outlines specific evaluation activities and approaches, focusing on progressive evaluation stages. Offline evaluations begin with benchmarking to assess general performance baselines, followed by more targeted testing based on generated test cases, which can be informed by the benchmarking results. Online evaluations expand on these foundations to incorporate real-world feedback and dynamic conditions. This sequential strategy ensures that evaluations are thorough and enables iterative validation and refinement.\n\u2022 Evaluation Criteria and Metrics: Defines qualitative and quantitative criteria for assessing outcomes, such as relevance, success rates, response times, and risk thresholds. Additionally, it emphasizes the value of interpretative explanations provided by human or AI evaluators, which can complement metrics to clarify why certain results are deemed acceptable or require further improvement [47].\nThis step may also yield preliminary safety cases [38]-structured arguments based on the evaluation objectives, scenarios, and architectural analysis that establish an initial evidence of the agent's operational safety within the defined conditions. These safety cases serve as a baseline, iteratively refined as evaluations progress.\nB. Step 2: Develop Evaluation Test Cases\nBuilding on the evaluation plan established in Step 1, this step develops test cases by integrating general-purpose benchmarking with scenario-specific test case generation. These test cases evaluate the LLM agent's pipelines and generated artifacts, ensuring coverage across general and specific scenarios, including both standard and edge cases.\nInputs: Test case development draws on several sources:"}, {"title": "V. REFERENCE ARCHITECTURE FOR EVALUATION-DRIVEN DESIGN OF LLM AGENTS", "content": "\u2022 Evaluation plan: Defines evaluation objectives, scenarios, and criteria from Step 1, guiding the selection and creation of test cases.\n\u2022 Evaluation Results: Results from prior evaluations offer historical data (e.g., from the production AgentOps logs) to inform new test cases for back-testing [48], allowing evaluations on real-world, previously encountered inputs.\n\u2022 Domain Knowledge Base: Includes domain-specific resources, such as industry documentation, verified user forums, and certified solutions, offering realistic and challenging examples to ground test case creation.\nProcess Steps: This step involves sourcing and refining test data through four coordinated activities:\n1) Identify Evaluation Benchmarks/Frameworks: Evaluation benchmarks and frameworks (e.g., DeepEval and LangSmith), are identified to support agent evaluation. Benchmarks provide general performance baselines, helping to assess fundamental agent capabilities and further inform more targeted, scenario-specific test cases by highlighting performance gaps. Frameworks extend beyond benchmarking by offering tools for testing, test case generation, trajectory-based evaluations, and online evaluations, supporting a comprehensive evaluation process (Step 3). Governance requirements and architectural insights from Step 1 guide the selection process, ensuring alignment with evaluation objectives and regulatory constraints. The selected benchmarks and frameworks form a key output of this activity, feeding into subsequent steps in the evaluation process.\n2) Collect Domain Knowledge: Domain-specific knowledge bases are referenced to create test cases that address operational contexts specific to the agent's deployment environment. These sources enhance the realism and relevance of test cases by reflecting domain-specific challenges the agent may encounter.\n3) Curate Test Data with Domain Experts: Domain experts help generate tailored test cases to address gaps not fully covered by benchmarks or knowledge bases, adding depth and specificity. For instance, experts may refine raw benchmark results into actionable scenarios or create targeted test cases for complex edge conditions. This ensures that critical scenarios, particularly those involving complex edge cases, are comprehensively represented.\n4) Generate Synthetic Data with LLMs: For cases where additional test data is necessary, synthetic data can be generated using LLMs [49], [50]. This synthetic data expands the evaluation's scope by simulating varied and complex contexts, filling in remaining gaps. While it is not a substitute for real-world examples or expert-curated data, synthetic data adds breadth, ensuring that test cases comprehensively reflect potential operational conditions.\nOutput:\n\u2022 Test Cases: Tailored for evaluating final/intermediate pipelines and generated artifacts across standard and edge scenarios. Test cases include both reference-based cases, designed with predefined correct outcomes (e.g., multiple-choice questions), and reference-free cases, relying on human or AI evaluators to assess criteria such as relevance, coherence, and contextual appropriateness.\n\u2022 Selected Benchmarks and Frameworks: Identified benchmarks and frameworks supporting subsequent evaluations.\nC. Step 3: Conduct Offline and Online Evaluations\nThis step implements a balanced approach that transitions from controlled offline evaluation to real-world online evaluation. Offline evaluations verify that the agent meets baseline standards before deployment under controlled conditions, while online evaluations provide continuous performance and safety monitoring in real-world settings under evolving demands.\nInput: Main inputs to evaluation processes include:\n\u2022 Identified Benchmarks: Benchmarks, identified in Step 2, provide generalized evaluations as performance baselines and can inform more targeted test case generation.\n\u2022 Test Cases: Developed in Step 2, these test cases form the backbone of offline evaluations, systematically assessing the agent's behavior against controlled datasets and specific scenarios. They validate the agent's ability to deliver correct, complete, and contextually appropriate outputs across typical and high-impact use cases. Test cases may also provide baseline metrics or serve as references for anomaly detection in online evaluations, complementing live feedback mechanisms.\n\u2022 Evaluation Frameworks: These frameworks enhance evaluation processes by supporting tasks like test case execution, artifact analysis, and pipeline evaluations. They enable scalable and comprehensive assessments across both offline and online contexts.\nProcess Steps: Evaluation activities center around:\n1) Evaluate Final Results: This step measures the agent's overall performance by assessing end-to-end outcomes, focusing on criteria such as success/pass rate, accuracy, and user satisfaction. The purpose here is to verify that the agent's ultimate outputs align with intended objectives and user needs. However, it often lacks the granularity needed to diagnose specific issues in the agent's decision-making processes or to trace the root cause of failures [33], [51]. Final results alone may obscure intermediate missteps or reasoning errors that could impact the agent's long-term performance and safety.\n2) Evaluate Intermediate Pipelines and Artifacts: This step focuses on assessing the agent's intermediate pipelines (e.g., prompts, intermediate results) and execution artifacts (e.g., plans [45], retrieved knowledge base information [44], and tool outputs [52]). The goal is to ensure their logical consistency, coherence, and alignment with the agent's objectives [37]. While these granular evaluations provide valuable insights for diagnosing specific issues and guiding improvements, they can be difficult to contextualize without linking them to the agent's overall performance [53]. A balanced approach is necessary to integrate these intermediate findings into a comprehensive understanding of system behavior.\nOutput: This step generates a comprehensive set of evaluation results from offline and online evaluations, offering complementary insights. Offline evaluations provide key metrics, error analyses, and pass/fail summaries from controlled test scenarios, establishing baseline performance. Online evaluations capture real-world metrics such as user impact, adaptive responses, and behavioral patterns under diverse operational conditions.\nInterpretative feedback is critical for turning evaluation results into actionable insights. A structured feedback framework-identifying fine-grained error sources (e.g., specific issues causing dissatisfaction), feedback providers (e.g., end-users, domain experts, or AI), and explanations (e.g., why an output is \u201cacceptable but improvable\")\u2014ensures that feedback is both meaningful and systematic. This approach helps pinpoint performance gaps and prioritize improvements effectively. Such feedback complements traditional metrics, offering nuanced, contextually relevant evaluations that overcome the limitations of rigid or arbitrary scoring systems [47]. Together, these insights drive runtime adaptations and iterative redevelopment, forming a continuous improvement loop that refines the agent's design, performance, and safety over time.\nD. Step 4: Analyze and Improve\nThis step translates evaluation results into actionable improvements, addressing both runtime (online) and redevelopment (offline) adjustments. Runtime improvements involve real-time adaptations to refine pipelines and artifacts, ensuring the agent remains responsive to evolving contexts. Redevelopment focuses on iterative changes to architectural components and the LLM itself, ensuring long-term performance and safety.\nInput: The evaluation results from Step 3 serve as the primary input, encompassing both offline (controlled) and online (real-world) insights. Offline results typically highlight baseline performance gaps and systemic deficiencies, while online results reveal operational metrics, user feedback, and emerging patterns, guiding both immediate and iterative improvements.\nProcess Steps: This step includes both runtime and redevelopment time improvement activities:\n1) Improve During Runtime (Online): Leveraging online evaluation results, LLM agents dynamically adjust runtime pipelines and artifacts to enhance responsiveness and mitigate risks. For example, if user feedback or logs highlight frequent tool invocation failures in multi-step tasks, the agent can revise its plans by retrying with alternative tools or switching to cached data sources. Similarly, real-time user corrections, such as clarifying ambiguous instructions (\"I meant 'quarterly' instead of 'monthly'\"), inform updates to context memories, ensuring that subsequent responses align with updated inputs. In goal-driven tasks, evaluator feedback may trigger recalibration of objectives, prioritizing high-impact goals to meet evolving user needs. These adjustments are typically automated through feedback loops and error detection mechanisms, enabling the agent to adapt dynamically to operational conditions without requiring downtime.\n2) Improve During Redevelopment (Offline): This phase involves substantial post-deployment improvements informed by persistent issues identified through offline evaluations and systemic insights from online results. Key activities include:\n\u2022 Refine Agent Architecture: Address design-level deficiencies by enhancing architectural components such as reasoning and planning modules and guardrails. For example, if evaluations reveal that the agent occasionally generates inappropriate responses when interacting with sensitive content, guardrails can be added/refined to include stricter content filtering mechanisms or domain-specific safety rules. Additionally, improvements may involve reconfiguring the data retrieval system to prioritize higher-quality information sources or optimizing inter-component workflows for enhanced efficiency and reliability.\n\u2022 Fine-Tune/Retrain/Select LLM: Where evaluation results indicate major LLM deficiencies, model-level improvement is needed. This could involve fine-tuning the existing LLM to address specific issues, retraining if feasible, or selecting an alternative off-the-shelf LLM if required.\nOutput: This step produces several key outputs that collectively enhance the LLM agent's design and operational safety:\n\u2022 Safety Cases: Updated safety cases integrate evidence from offline and online evaluations, ensuring the agent's behaviour remains safe under new or evolving conditions. These updates reflect newly identified risks and safety boundaries, ensuring compliance with safety thresholds in dynamic environments.\n\u2022 Refined Agent Architecture: Documented modifications to system components reflect the iterative improvements applied during redevelopment. These refinements align the architecture with identified performance gaps and operational goals.\n\u2022 Updated LLM: Outputs may include fine-tuned, retrained, or newly selected LLMs, aligned with performance and safety goals.\n\u2022 Refined Pipelines/Artifacts: Adjustments to runtime pipelines and artifacts, ensure alignment with runtime feedback and evaluation-driven improvements.\nTo address RQ2, we propose a reference architecture that positions evaluation as a central element in LLM agent design (see Fig. 3). Building on previous work on LLM agent and AI system design [4], [7], this architecture establishes evaluation as a foundational architectural component across three interconnected layers: the Supply Chain Layer, the Agent Layer, and the Operation Layer. The architecture ensures continuous and adaptive evaluation, with feedback loops supporting both runtime adaptation and redevelopment. The Operation Layer, which integrates the Evaluation component and AgentOps Infrastructure, serves as the core of this architecture.\nThe architecture is guided by three key principles:\n\u2022 Lifecycle Integration: Evaluations are embedded throughout both pre-deployment (design and development) and post-deployment (runtime and redevelopment) phases. This ensures that evaluation is comprehensive and continuous, with insights available to guide improvements at every stage.\n\u2022 Meaningful Feedback Loops: Insights from both offline and online evaluations are systematically fed back to inform both lightweight runtime adjustments and substantial offline refinements, enabling targeted improvements.\n\u2022 Continuous Learning and Improvement: The architecture fosters ongoing refinement by enabling adaptive adjustments at both model and system level, with particular emphasis on the latter. By integrating mechanisms for continuous improvement and out-of-model learning, it ensures that the agent adapts effectively to evolving objectives and operational contexts, maintaining alignment with performance and safety standards.\nA. Supply Chain Layer\nThis layer establishes the foundational design, functionality, and evaluation criteria for the LLM agent It includes four preparatory steps, represented with dotted lines to indicate their supportive rather than core architectural function:\n\u2022 Plan and Design: Understand user goals, define governance requirements and high-level architecture, shaping evaluation criteria and identifying evaluation scenarios. This step lays the foundation for selecting benchmarks, creating test cases, and aligning evaluations with compliance and performance standards.\n\u2022 Collect and Process Data: Gather and preprocess data for model fine-tuning and model/agent testing. In our context, the emphasis is on preparing evaluation data as we prioritize the use of off-the-shelf LLMs.\n\u2022 Build/Select and Evaluate Model: Proprietary models are evaluated using published system cards or through black-box testing via APIs to validate their suitability for operational objectives. Open-source models can be fine-tuned as needed and evaluated to ensure alignment with specific tasks and performance goals.\n\u2022 Build and Evaluate System: Integrate the LLM with system components (e.g., guardrails) and conduct system-level evaluations for functional correctness, quality/risk, and capability [20]. Such offline evaluations confirm LLM agent readiness before deployment.\nThe primary output of this layer, the Design and Development Artifacts repository, consolidates key artifacts that support continuous evaluation and improvement:\n\u2022 Test Cases: Structured evaluations derived primarily from user goal-driven scenarios, ensuring alignment with specific tasks and operational contexts. Test cases form the backbone of the evaluation process, offering targeted assessments of pipelines and artifacts. Insights from offline benchmarking refine test cases to address scenario-specific needs. Test cases are continuously updated to adapt to evolving objectives and contexts.\n\u2022 Safety Cases: Evidence-backed justifications that define safe operational boundaries and identify critical risk scenarios. Initially shaped by user and regulatory requirements, safety cases provide baseline safety assurances and are iteratively refined based on emerging evaluation results and runtime insights.\n\u2022 Offline Evaluation Results: Pre-deployment results from generalized benchmarking and scenario-specific test case evaluations establish baseline performance metrics and highlight known limitations. These results serve as critical references for ongoing evaluations and agent refinement during post-deployment phases\nB. Agent Layer\nThe Agent Layer focuses on the adaptive capabilities of the LLM agent, enabling it to operate effectively in dynamic, real-world conditions. By integrating core components for context understanding, decision-making, and task execution, this layer supports seamless interaction with external entities and forms the foundation for evaluation-driven design. The primary components of this layer include:\nExternal Environments: The agent interacts with various external entities that shape the agent's operational context, providing essential data and situational insights that inform the agent's responses and actions:\n\u2022 Users: Providing explicit or implicit feedback that guides agent actions and informs iterative improvements to workflows or artifacts.\n\u2022 Other Agents: Collaborating through task distribution, data sharing, or multi-agent communication [55], enabling coordinated task execution.\n\u2022 Tools: Task-specific APIs and processing modules, which the agent uses to execute workflows and refine its decision-making capabilities.\n\u2022 Knowledge Bases: Static or dynamic information sources (e.g., databases, ontologies), which enhance reasoning processes by providing contextual and domain-specific knowledge.\nAgent Module: This module includes key functional components within the LLM agent to support its operation:\n\u2022 Context Engine: This module continuously collects, processes, and integrates environmental data, combining it with relevant memory inputs to maintain situational awareness. This context enables the agent to generate precise, relevant responses, accounting for immediate interactions as well as past user inputs when appropriate.\n\u2022 Reasoning & Planning Module: Based on contextualized inputs from the Context Engine, this module generates actionable plans, specifying executable workflows for fulfilling designated tasks. Artifacts (e.g., plans) produced here are critical evaluation targets.\n\u2022 Workflow Execution: Responsible for managing the execution of tasks, this component coordinates task sequences, manages exceptions, and interfaces with external environments (e.g., APIs, databases) as necessary. Workflow Execution ensures that the agent follows an optimal sequence of actions, with mechanisms to detect and handle errors.\n\u2022 Multi-layered Guardrails: Guardrails enforce operational boundaries across all components, ensuring safety and accuracy/correctness. They validate pipelines and artifacts in real-time, embedding constraints that uphold responsible Al principles and compliance standards while preventing unsafe or suboptimal actions [8].\n\u2022 Memory: Memory facilitates information retention, enabling the lagent to adapt over time and align with evolving user needs. It stores historical data, user preferences, and operational feedback to improve pipelines, artifacts, and continuity. Memory collaborates with the Context Engine to ensure accurate, context-aware responses.\nC. Operation Layer\nThe Operation Layer is the focal point of the reference architecture, enabling continuous evaluation and adaptive improvements during both runtime and redevelopment. It consists of two interdependent components: Evaluation and AgentOps Infrastructure, which together form a cohesive framework for lifecycle-spanning, evaluation-driven agent design. Evaluation drives the assessment and feedback processes, while the AgentOps Infrastructure provides the necessary observability and operational data to support these processes.\n1) Evaluation: This component is the core of the Operation Layer, orchestrating lifecycle-spanning evaluation processes to ensure the agent's performance and safety. It integrates findings from both offline and online evaluations, enabling a continuous feedback loop that drives runtime adaptation and redevelopment improvements.\nKey functions of this component include:\n1) Comprehensive Evaluation Processes: Evaluations span both offline and online contexts, addressing granular and system-wide aspects of agent performance. These processes evaluate both final results (e.g., overall success rates, user satisfaction) and intermediate pipelines and artifacts (e.g., plans, retrieved information, workflow steps), offering a dual perspective. This holistic approach ensures a comprehensive understanding of agent behavior, enabling precise diagnostics and targeted refinements.\n2) Proactive Feedback Loops: Evaluation results are fed back into actionable improvements. Runtime evaluations inform immediate adaptations to pipelines and artifacts, while offline evaluations guide iterative redevelopment of architecture and components. These feedback loops ensure that evaluations serve as active drivers of agent design improvement rather than merely diagnostic tools.\nKey components of the component include:\n\u2022 Evaluators: Both human and automated evaluators contribute to the evaluation process. Human evaluators (e.g., domain experts or end-users) provide nuanced assessments for complex scenarios requiring subjective judgment, such as evaluating fairness, relevance, or alignment in high-stakes contexts. Automated evaluators (LLM-based or agent-based) are faster and cost-effective [56], complementing human evaluations by addressing scalability and consistency limitations.\n\u2022 Test Case Generator: Updates and refines test cases dynamically based on new data from evaluation results, operational logs, and and emerging patterns. For example, discrepancies observed in runtime logs may prompt the generation of new test cases tailored to edge cases or recurring errors, ensuring evaluation remains relevant to evolving scenarios.\n\u2022 Safety Case Generator: Builds and iteratively updates safety cases based on operational data, ensuring they remain aligned with evolving risks and regulatory standards. By dynamically integrating new evaluation insights, it provides stakeholders\u2014such as developers, system integrators, and regulators\u2014with up-to-date justifications for safe agent operation.\n\u2022 Test Case Repository: Consolidates dynamically updated test cases, serving as the basis for offline evaluations and live monitoring. By categorizing cases (e.g., standard scenarios, edge cases, and emerging patterns), this repository ensures that evaluations remain systematic and contextually grounded.\n\u2022 Safety Case Repository: Houses safety cases as formalized risk control instruments. These cases are continuously updated to reflect operational insights, forming a critical resource for adaptive risk management.\n\u2022 Evaluation Results Repository: Serves as the central hub for aggregating findings from offline and online evaluations. This repository not only tracks agent performance and safety metrics but also supports broader architectural decision-making. For example, stored results can be analyzed to detect recurring failure patterns or generate positive/negative examples for fine-tuning runtime behaviors.\n2) AgentOps Infrastructure and Observability: The AgentOps Infrastructure underpins continuous evaluation by capturing logs, metrics, traces, and events across the agent's lifecycle, enabling dynamic performance diagnostics and retrospective analysis [9]. By providing real-time insights, this infrastructure facilitates immediate runtime adjustments and generates structured artifacts to inform iterative improvements. For instance, runtime logs from high-impact scenarios are transformed into new test cases for back-testing or updates to safety cases, ensuring offline evaluations remain aligned with real-world challenges.\nObservability also aids in detecting emergent behaviors and categorizing recurring patterns (e.g., positive/negative examples), enriching evaluation processes and refining pipelines and\""}, {"title": "VI. THREATS TO VALIDITY", "content": "artifacts. Seamlessly integrated with the Evaluation component", "Validity": "Biases in data selection and synthesis may impact the accuracy of the framework. To address this, we conducted a rigorous MLR with clearly defined"}]}