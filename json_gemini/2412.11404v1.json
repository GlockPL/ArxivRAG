{"title": "Attention with Dependency Parsing Augmentation for Fine-Grained Attribution", "authors": ["Qiang Ding", "Lvzhou Luo", "Yixuan Cao", "Ping Luo"], "abstract": "To assist humans in efficiently validating RAG-generated content, developing a fine-grained attribution mechanism that provides supporting evidence from retrieved documents for every answer span is essential. Existing fine-grained attribution methods rely on model-internal similarity metrics between responses and documents, such as saliency scores and hidden state similarity. However, these approaches suffer from either high computational complexity or coarse-grained representations. Additionally, a common problem shared by the previous works is their reliance on decoder-only Transformers, limiting their ability to incorporate contextual information after the target span. To address the above problems, we propose two techniques applicable to all model-internals-based methods. First, we aggregate token-wise evidence through set union operations, preserving the granularity of representations. Second, we enhance the attributor by integrating dependency parsing to enrich the semantic completeness of target spans. For practical implementation, our approach employs attention weights as the similarity metric. Experimental results demonstrate that the proposed method consistently outperforms all prior works.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation (RAG) enhances the factual recall of LLMs and has been applied in knowledge-intensive NLP tasks such as open-domain question answering (Lewis et al., 2020). However, the generated content may still deviate from the retrieved documents (Niu et al., 2024), necessitating careful verification, especially when used in safety-critical domains like finance. To assist users in validating LLM-generated responses, QA systems must provide supporting evidence, also referred to as attribution or citation (Li et al., 2023a). Furthermore, developing a fine-grained attribution mechanism that supplies evidence for arbitrary answer spans (as illustrated in Fig. 1) is particularly beneficial, as it allows users to efficiently verify the accuracy of individual segments within complex, long-form answers.\nFine-grained attribution has been addressed by only two model-internals-based approaches: CCI (Yin and Neubig, 2022; Sarti et al., 2023; Qi et al., 2024) and HSSAVG (Average Hidden State Similarity method, Phukan et al., 2024). CCI leverages saliency scores for attribution but requires gradient back-propagation for each target token, resulting in high computational complexity. HSSAVG operates by (1) measuring the similarity between the average hidden states of the token span and those of a sliding window with a fixed size W over the documents, and (2) selecting the highest-scoring window as the evidence. However, the averaging operation over the target span introduces coarse granularity, limiting the precision of its representations. A further challenge common to both methods lies in their reliance on the internal representations of decoder-only Transformers, which lack access to tokens that appear after the target span, constraining their contextual understanding (see Sec. 3.3 for details).\nBuilding on insights into the limitations of previous methods, we propose a novel, effective, and efficient model-internals-based approach. This approach consists of two techniques applicable to all model-internals-based approaches. The first technique involves aggregating token-wise evidence using set unions, addressing the coarse-granularity issue in averaging hidden states while eliminating the need to recompute these averages for each new target span (see Sec. 3.2 for details). The second technique incorporates dependency parsing to enhance the attributor by integrating related tokens into the attribution of the target token, as illustrated by Fig. 2 (see Sec 3.3 for details). For practical implementation, we utilize attention weights as the similarity metric due to their faster computation compared to gradient back-propagation and superior empirical performance over hidden state similarity. Our experiments demonstrate that the proposed method surpasses all baseline approaches and generalizes effectively to sentence-level attribution, highlighting its practical value.\nWe also address two practical challenges in implementing our method: (1) the inaccessibility of attention weights from black-box sources and (2) the high GPU memory consumption required for their computation. To tackle the first challenge, we approximate attention weights using open-source LLMs. For the second, we apply engineering optimizations to enable more efficient attention weight calculations, achieving significantly faster performance than prior approaches.\nIn summary, our contributions are:\n1. We propose a novel model-internals-based fine-grained attributor that aggregates token-wise evidence through set unions, addressing the coarse granularity induced by averaging hidden states.\n2. We propose leveraging dependency parsing to enhance decoder-based attributors by enriching the semantics of the target span.\n3. The proposed method utilizes LLM attention weights for attribution and incorporates an optimized routine for efficient computation.\n4. Our method sets a new state-of-the-art in fine-grained attribution and demonstrates strong generalization to sentence-level attribution."}, {"title": "Related Work", "content": "This section reviews related work across four aspects: (1) attribution, (2) non-faithfulness detection, (3) fact verification, and (4) other works using attention weights.\nAttribution. Attribution methods can be categorized into three classes: self-generated attribution (or self-citation) (Thoppilan et al., 2022; Nakano et al., 2021; Menick et al., 2022), retrieval-based attribution (Gao et al., 2023a; Slobodkin et al., 2024; Sancheti et al., 2024), and model-internals-based attribution (Qi et al., 2024; Cohen-Wang et al., 2024; Phukan et al., 2024). The self-generated approach prompts or fine-tunes the LLM to produce citations during answer generation, but it does not guarantee that each statement has adequate citations (Gao et al., 2023b). The retrieval-based approach provides sentence-level citations by retrieving relevant results from external documents, ensuring adequate citations for each sentence. Model-internals-based attribution relies on similarity metrics such as saliency scores or hidden state similarities to align the response with the prompt, retrieving evidence from prompt tokens with the highest similarity. Of these three approaches, the first two primarily target sentence-level attribution, while some methods from the third category explore fine-grained attribution (Phukan et al., 2024; Qi et al., 2024), which has been discussed in the Introduction.\nNon-Faithfulness Detection. Attribution plays a valuable role in detecting non-faithfulness (He et al., 2022; Niu et al., 2024), a type of hallucination detection (Wang et al., 2020; Muhlgay et al., 2024) that assesses whether the generated text is inconsistent with the input documents. Many non-faithfulness detection methods are black-box (Feng et al., 2023; Yue et al., 2023; Bazaga et al., 2024; Mishra et al., 2024; Zhang et al., 2024a), resembling traditional Natural Language Inference (NLI) models but operating with longer contexts and greater complexity. In these methods, attribution can be integrated into the detection pipeline to break down the complex NLI task, enhancing interpretability by providing supporting evidence for each statement. An alternative approach to hallucination detection involves the model-internals methods (Li et al., 2023b; Hu et al., 2024; Chuang et al., 2024), which offer the potential to unify attribution and non-faithfulness within a single model. However, these methods have so far overlooked attribution.\nFact Verification. Fact verification integrates hallucination detection with attribution, requiring not only an assessment of whether a claim is hallucinated but also evidence that supports or contradicts the claim (Guo et al., 2022). In this domain, retrieval-based attribution is commonly used (Min et al., 2023). While early studies (before the LLM era) employed attention weights from GNNs or other small models for attribution (Yang et al., 2019; Liu et al., 2020; Chen et al., 2022), using LLM attention weights for attribution remains unexplored in fact verification literature.\nOther Works Using Attention Weights. In attribution, attention weights have been utilized in small models like BERT (Clark et al., 2019; Kobayashi et al., 2020). However, with the advent of LLMs, their use for attribution remains unexplored due to high memory costs and the lack of support from popular inference frameworks and proprietary models. For hallucination mitigation, Huang et al. (2024) introduces a method that retrospects LLM generation whenever a hallucination-related \"aggregate pattern\" appears in the attention weights."}, {"title": "Method", "content": "We begin by outlining the problem settings in Sec. 3.1, followed by a description of the attention-based method in Sec. 3.2 and the dependency parsing augmentation in Sec. 3.3. Finally, we introduce our approach to addressing challenges in real-world applications in Sec. 3.4."}, {"title": "Problem Settings", "content": "Suppose an LLM generates a response r given the prompt composed of the retrieved documents d and the question q. The goal of fine-grained attribution is, given an arbitrary target span $t \\subset r$, to identify evidence from d that supports the atomic facts (short statements that each contain one piece of information, Min et al., 2023) that involves t. An example is provided in Fig. 1, where for target span \"one million dollars\", the atomic fact is \u201cThe company earned one million dollars in 2012\", and the evidence is \"The company earned $1,0000,00 in 2012\" in the first document."}, {"title": "The Basic Algorithm", "content": "To solve fine-grained attribution, we propose attributing each target token in the span and then aggregating the attributions of tokens by the union of sets. Formally, let the documents, question, and response of the LLM be three sequences of tokens $d = (d_1, d_2, ..., d_c)$, $q = (q_1, q_2, ..., q_m)$, and $r =(r_1, r_2, ..., r_n)$, respectively. Assume the LLM can output a similarity metric $S \\in \\mathbb{R}^{n \\times (c+m)}$ between response tokens and prompt tokens (e.g., attention weights, hidden state similarity, and saliency scores). Then the attribution scores are defined as\n\n$w(r_i, d_j) := \\begin{cases} S_{ij}, & \\text{if } S_{ij} \\ge top-k(S) \\\\ 0, & \\text{otherwise} \\end{cases}$\n\nwhere $1 \\le i \\le n$, $1 \\le j \\le c$, $top-k(x)$ is the k-th largest component in vector x, and k is a hyper-parameter. With these attribution scores, each response token $r_i$ is attributed to $e(r_i) := \\{d_j | w(r_i, d_j) > 0\\}$, and the aggregated evidence for the target span t is $e(t) := \\bigcup_{r_i \\in t} e(r_i) = \\{d_j | \\exists r_i \\in t w(r_i, d_j) > 0\\}$ with each token $d_j \\in e(t)$ associated with an attribution score $w(t, d_j) := \\sum_{r_i \\in t} w(r_i, d_j)$. Furthermore, to avoid noisy and fragmented attribution, we remove the isolated evidence tokens that are at least T tokens away from other evidence tokens. The pseudo-code of the whole algorithm is provided in Algorithm 1. In addition, the token-wise attributions $e(r_i)$ with scores $w(r_i, \\cdot)$, i = 1, 2, ..., n, can be reused by any target span in the same response, saving computational cost. We utilize this trick but omit it in the pseudo-code for brevity."}, {"title": "Dependency Parsing Augmentation", "content": "We observe a common defect in previous works: the representations of decoder-only models cannot see the context that follows the target span, potentially missing relevant factual information that appears later. For instance, as shown in Fig. 1, decoder-only attributors cannot see \u201cmillion dollars\" or \"in 2012\" at the position of \u201cone\u201d, increasing the difficulty of fine-grained attribution. A straightforward solution is to allow the attributor to access subsequent context. However, extending the accessible scope to the entire context or full sentence is empirically shown to be ineffective in fully addressing the issue (see Sec. 4.2.1).\nTo address this issue, we propose DEpendency Parsing augmentation (DEP). This method first recognizes key elements \u2014 such as subject, object, and predicate \u2014 within atomic facts that contain the target span using dependency parsing. For example, as illustrated in Fig. 2, given the target token \u201cone\u201d, the method collects atomic fact elements as shown in red: \u201cThe company earned one million dollars in 2012 respectively.\" It then leverages the attributions of these atomic fact elements to enhance the attribution of the target span. The detailed algorithm proceeds as follows.\n1. Recognizing Atomic Fact Elements. We assume that the elements of atomic facts can be extracted from the text. For each token $r_i$, let these elements be $A(r_i) \\subset r$. This extraction is approximated by leveraging the dependency parse tree, constructed using the LAL-Parser (Mrini et al., 2020), applied to the local sentence, as shown in Fig. 2. Since each node in this tree corresponds to a word while the attribution pertains to tokens, we need to align the two. For simplicity, we temporarily assume they are the same, with alignment details provided in Appendix B. Initially, we set $A(r_i) \\leftarrow \\{r_i\\}$. Then, the algorithm appends $A(r_i)$ with $r_i$'s closest verb ancestor v and all v's successors except punctuation marks. If $A(r_i)$ contains coordinating structures (e.g., tokens connected by \u201cand\u201d or \u201cor\u201d), we use a rule-based subroutine (described in Appendix C) to eliminate irrelevant coordinating constituents. For example, in Fig. 2, A(\"one\") includes the closest verb ancestor \"earned\" and all its successors except the irrelevant coordinating constituents \u201ctwo million dollars\" and \"2013.\"\n2. Augmenting Attribution. Using $A(r_i)$, the attribution of token $r_i$ is augmented as $e(r_i) \\leftarrow \\bigcup_{a \\in A(r_i)} e(a)$, with attribution scores updated to $w(r_i, d_j) \\leftarrow \\Sigma_{a \\in A(r_i)} w(a, d_j)$. The aggregate attribution is then computed based on the updated token-wise attribution, following the same approach as the basic algorithm."}, {"title": "Attention and its Computational Challenges", "content": "We choose attention weights as similarity scores due to their strong empirical performance. The method is outlined as follows. Let the attention weights between the response and the prompt at the l-th layer and the h-th head of the LLM be $A_{ij}^{(l,h)} \\in \\mathbb{R}^{n \\times (c+m)}$, where the LLM has L layers and H heads. Specifically, $A_{ij}^{(l,h)}$ is the attention weights used to predict $r_i$. For a Transformer decoder, $A_{ij}^{(l,h)}$ is the attention weight from $r_{i-1}$ to $d_j$. For a Transformer encoder, $A_{ij}^{(l,h)}$ is the attention weight from $r_i$ to $d_j$. Based on these attention weights, the similarity score is computed by averaging the attention weights from a selected layer $L^*$ across all heads: $S := \\frac{1}{H} \\Sigma_{h=1}^H A_{ij}^{(L^*, h)}$.\nAnother potential similarity metric is cosine similarity among hidden states. Let $h^p \\in \\mathbb{R}^{(c+m) \\times d}$ and $h^r \\in \\mathbb{R}^{r \\times d}$ denote the hidden states of the prompt and the response, respectively, where d is the dimension of hidden states. The hidden state cosine similarity is computed as: $S_{ij} := \\frac{h_i^r \\cdot h_j^p}{||h_i^r|| \\cdot ||h_j^p||}$. We will empirically show that attention-based similarity is better than hidden state cosine similarity.\nHowever, the attention-based approach has the following two challenges in real-world application.\nInaccessible Attention Weights. The attention weights are inaccessible under the following conditions: (1) the LLM is proprietary, such GPT-4 (Achiam et al., 2023); (2) the inference framework, such as vLLM (Kwon et al., 2023), does not support outputting attention weights; (3) the response may be generated by other black-box sources, e.g., humans. To address this, we leverage open-sourced LLMs to approximate the attention weights. Empirical results demonstrate that these approximate attention weights offer reliable attribution.\nHigh Memory Overload. In the Huggingface (Wolf et al., 2020) implementation, attention weights and the KV cache are stored simultaneously in GPU memory when calculating attention weights from the response to the prompt. This dual memory load can quickly exhaust the GPU memory. Therefore, we propose a specialized routine for calculating attention weights. We calculate attention weights after the generation process. The model processes the concatenation of the response and the prompt with FlashAttention (Dao et al., 2022) and NF4 quantization (Dettmers et al., 2023), exiting early after the ($L^* - 1$)-th layer without producing attention weights or storing the KV cache. From the resulting hidden states, we calculate the attention weights from the response to the prompt, yielding the desired similarity metric."}, {"title": "Experiments", "content": "We begin by evaluating our methods on fine-grained attribution in Sec. 4.1, with the corresponding ablation study presented in Sec. 4.2. We then assess the faithfulness of our methods to the generators in Sec. 4.3. The evaluation of sentence-level attribution is in Sec. 4.4. Lastly, we compare the latency of all fine-grained attributors in Sec. 4.5."}, {"title": "Evaluating Fine-Grained Attribution", "content": "Benchmark. Following Phukan et al. (2024), we use two datasets QuoteSum (Schuster et al., 2024) and VERI-GRAN (Phukan et al., 2024) to evaluate fine-grained attribution. Each instance of these two datasets contains a question, retrieved passages, and an answer, where the question and the retrieved passages combine to form the prompt. The task is to identify the evidence passage (or evidence sentence in the case of VERI-GRAN) corresponding to each context-sensitive span in the answer, where the span is determined based on the similarity of LLM hidden states (Phukan et al., 2024), and the evidence passage is labeled by human annotators. Additional details about the datasets are provided in Table 6. The evaluation metric is the accuracy of the predicted evidence passage for these spans. To apply our method and CCI to this benchmark, we predict the passage with the highest cumulative attribution score, i.e., $arg \\underset{d_i}{max} \\sum_{d \\in d_i} w(t, d)$, where $d_i$ is the i-th passage and t is the target span. For HSSAVG, we select the passage containing the highest-scoring sliding window as the prediction.\nBaselines. We include the only two prior works on fine-grained attribution as our baselines, CCI (Sarti et al., 2023; Qi et al., 2024), also known as contrastive feature attribution (Yin and Neubig, 2022), and HSSAVG (Phukan et al., 2024). Additionally, we include the reported results of GPT-4 from Phukan et al. (2024).\nModels. We conduct experiments using two models: Llama2 7B Chat(Touvron et al., 2023) and Qwen2 7B Instruct (Yang et al., 2024). For HSSAVG, hidden states are extracted from the $\\lfloor \\frac{L}{2} \\rfloor$-th layer, as Phukan et al. (2024) report that this method performs well across different models when using hidden states from the middle layers. For our methods, the attention weights are from the $(\\lfloor \\frac{L}{2} \\rfloor + 1)$-th layer \u2014 just above the hidden states used for HSSAVG \u2014 due to their strong empirical performance on validation sets. To address GPU memory limitations, we apply NF4 quantization to CCI5.\nHyperparameters. We set the token-wise evidence size k = 2 and the threshold for recognizing isolated tokens $t = 2$ for our method (experiments on sensitivity to hyperparameters are presented in Appendix F). For HSSAVG, we configure the window size W = 8, as this setting yields good performance on validation sets. For CCI, following the original paper, we choose the top-scoring three tokens as the evidence for each target token.\nResults. The accuracy of fine-grained attribution is reported in Table 1, where we refer to our basic algorithm and its augmented version as ATTNUNION and ATTNUNIONDEP, respectively. Across both datasets, ATTNUNION achieves comparable performance to HSSAVG, while ATTNUNIONDEP consistently outperforms HSSAVG and GPT-4, setting a new SOTA in fine-grained attribution and demonstrating the effectiveness of DEP."}, {"title": "Ablation Study", "content": "In this section, we conduct ablation studies about DEP, ATTN, and UNION."}, {"title": "Ablating DEP", "content": "Research Question. Previous results have confirmed the effectiveness of DEP. However, an important question remains: Are there any simpler alternatives to DEP that allow the model representations to see the context following the target span?\nExperiment Setting. We empirically compare two alternatives with DEP: 1. extending the span to the entire local sentence (SENTCOMP); 2. leveraging representations from bidirectional attention models, such as BERT, for attribution. The benchmarks, backbones, and hyper-parameters remain consistent with those used in the previous experiment.\nSpecifically, the second alternative leverages JinaBERT's (G\u00fcnther et al., 2023) attention weights or hidden state similarity (HSS) as the similarity metric. We refer to the HSS-based method as HSSUNION. The attention weights are from the 5th layer, while the hidden state similarities are from the final layer, as these configurations demonstrated strong performance on the validation sets.\nResults & Insights. The results are shown in Fig. 3, demonstrating that all alternatives consistently underperform compared to DEP. The results indicate that recognizing atomic facts via DEP is more effective than extending the accessible context to the entire context or full sentence."}, {"title": "ATTN vs. HSS", "content": "Research Question. While calculating attention weights is significantly faster than gradient back-propagation, it incurs a similar computational cost to computing hidden state similarity. This raises the question: Is using attention weights more effective than hidden state similarity?\nResults. We analyze the results from Fig. 3 by comparing adjacent bars of ATTNUNION and HSSUNION. Excluding the underperforming JINABERT and VANILLA settings, ATTNUNION outperforms HSSUNION in most cases, highlighting the effectiveness of using attention weights as attribution scores."}, {"title": "UNION vs. AVG", "content": "Research Question. The final component to examine is the aggregation by union. We pose the question: Is UNION more effective than AVG?\nExperiment Setting. We compare UNION and AVG both with and without the integration of DEP. To incorporate DEP with HSSAVG, we first expand the target span using DEP, i.e., $t \\leftarrow \\bigcup_{r_i \\in t} A(r_i)$, and then apply HSSAVG to the new span. All other experimental settings remain consistent with those used in the previous experiment.\nResults. The results, presented in Table 2, show that while UNION underperforms AVG without DEP, it surpasses AVG by a significant margin \u2014 at least 9.0 percent points \u2014 when DEP is applied. Insights. The results suggest that UNION is most effective when used in conjunction with DEP. Conversely, combining the results of Table 1 and Table 2, we conclude that DEP also works best with UNION, as HSSAVGDEP performs worse than HSSAVG. This outcome is reasonable since the expanded target spans in HSSAVGDEP introduce more context semantics, resulting in averaged hidden states that dilute fine-grained information."}, {"title": "Faithfulness of fine-grained Attribution", "content": "Research Question. We have shown that the proposed method accurately identifies the human-labeled evidence passages, indicating a strong alignment between our approach and human annotations. However, it remains to be verified whether our method is faithful to the generation model \u2014 specifically, does the attributed evidence directly influence the generator to produce the target span?\nExperiment Setting. To quantify the causal effect, we follow the approach of Cohen-Wang et al. (2024), which involves removing the evidence from the prompt, rerunning the generator, and measuring the log predictive probability drop of the target span before and after the removal. First, We use Llama2 or Qwen2 to generate answers with greedy decoding on QuoteSum and VERI-GRAN. Next, in each generated answer, we apply CTI (Qi et al., 2024) to identify context-sensitive tokens, which serve as target spans. We then use the attributor to locate evidence for each target span. Finally, we calculate the log probability drops as follows. Let the documents be $d_1, ..., d_c$ with the attributed document $d_i$. The log probability drop is\n\n$log \\frac{P_{LLM}(t | d_1, ..., d_c, q, r_<t)}{P_{LLM}(t | d_1, ..., d_{i-1}, d_{i+1}, ..., d_c, q, r_<t)}$\n\nwhere $P_{LLM}$ is the output probability of the LLM, t is the target span, and $r_<t$ is the response prefix preceding t. A larger log probability drop indicates greater faithfulness of the attribution to the generation process.\nWe include HSSAVG, HSSAVGDEP, and CCI as baselines. Additionally, we introduce two extra baselines, RANDOM and ORACLE. RANDOM randomly selects an evidence passage, with the experiment repeated three times to calculate the average performance. ORACLE, on the other hand, performs a brute-force search to identify the passage that results in the highest log-probability drop, using that passage as the evidence.\nResults & Insights. The results in Table 3 show that DEP significantly enhances the faithfulness of ATTNUNION. In most cases, ATTNUNIONDEP outperforms baselines except ORACLE and closely approaches its performance, indicating that ATTNUNIONDEP offers greater faithfulness than previous methods. Interestingly, all methods maintain their faithfulness even when using different models from the generator, showing their flexibility and independence from the generator's backbone."}, {"title": "Sentence-level Attribution", "content": "Fine-grained attribution can be easily applied to sentence-level attribution, as long as we select sentences as the target spans. In the following, we evaluate the performance of sentence-level attribution of ATTNUNION on commonly used benchmarks and compare it with the current SOTA. Benchmarks. Following Gao et al. (2023b), we conduct experiments on datasets ASQA (Stelmakh et al., 2022) and ELI5 (Fan et al., 2019). In these datasets, each question is attached with 100 documents, and the top 5 documents are used as the retrieved documents (the vanilla setting of Gao et al. (2023b)). Following ALCE (Gao et al., 2023b), we use metrics MAUVE (Pillutla et al., 2021), EM/ claim recall, citation recall, and citation precision. The first two measure the generation quality, and the last two measure the citation quality.\nBaselines. The baselines include all previously introduced baselines and two baselines of sentence-level attribution, the vanilla version of self-citation (Gao et al., 2023b) and Attribute First Then Generate (ATTRFIRST for short, Slobodkin et al., 2024). To rule out the effect of different prompts, following Qi et al. (2024), all fine-grained baselines share the prompt and the generated response of self-citation (with the original citations removed) to attribute. To compare our method with ATTRFIRST, we separately evaluate our method on the generated results of ATTRFIRST.\nBackbones. The backbones are by default Qwen2 7B and Llama2 7B. However, due to the context length required by ATTRFIRST exceeds the max context length of Llama2 7B, we only evaluate ATTRFIRST on Qwen2 7B.\nHyperparameters. For fine-grained methods, because the benchmarks allow outputting multiple citations for a target sentence, ATTNUNION, HSSAVG, and CCI output all passages possessing attribution scores above a threshold. The threshold is zero for ATTNUNION and CCI, and is 0.55 for HSSAVG. The other hyperparameters of these fine-grained attribution methods are the same as the previous experiments.\nFor sentence-level methods, self-citation generates with 2-shot, temperature of 1.0, and top-p of 0.95; ATTRFIRST generates with 1-shot for content selection, 4-shot for fusion in context, temperature of 0.3, max retry number of 5.\nResults. We repeat all experiments three times with different seeds (except ATTRFIRST and CCI due to their heavy computational overload) and take the average results. ATTRFIRST failed on 355 and 448 instances on ELI5 and ASQA, respectively, and we report the results measured on the successful instances. The generation quality and citation quality results are shown in Table 4 and Table 7, respectively. As the results show, ATTNUNION consistently outperforms other fine-grained attributors and improves the citation quality of SELFCITATION and ATTRFIRST, suggesting its application for improving various attributors."}, {"title": "Attribution Latency", "content": "Research Question. From a practical point of view, we ask: is our method faster than previous works?\nExperiment Setting. We compare our method with all fine-grained attribution baselines and ATTNUNION based on Huggingface implementation of calculating attention weights. The experiments are conducted on QuoteSum, VERI-GRAN, and ASQA with the backbone of an NF4-quantized Qwen2 7B and the device of a single NVIDIA RTX 3090 24GB GPU. HSSAVG uses the same early exit as ATTNUNION for a fair comparison. We input instances of the datasets one by one to the methods. The latency is measured by the average time consumed per target span.\nResults. The results are shown in Table 5. Our implementation of ATTNUNION largely outperforms other methods, with an average latency of 265.0 ms on the long-context dataset VERI-GRAN, and ATTNUNIONDEP is the second.\nInsights. Considering the early exits of ATTNUNION and HSSAVG are the same, ATTNUNION should have a similar latency to HSSAVG. However, the latency of ATTNUNION is much less than HSSAVG. This is because the ATTNUNION does not need to recompute average hidden states for each new target token as HSSAVG; instead, ATTNUNION reuses token-wise attribution for all target spans in the same response."}, {"title": "Conclusion", "content": "This work proposes a novel fine-grained attribution method, leveraging attention weights and dependency parsing. The experiments show that our method is the new SOTA fine-grained attributor and generalizes well to sentence-level attribution. Moreover, our method is much faster than previous works, showing its potential to be applied in real-time attribution systems."}, {"title": "Limitations", "content": "A limitation of our evaluation is that the target spans of QuoteSum and VERI-GRAN are selected by models, which might not reflect the real application scenario where target spans are selected by human users. This limitation could be resolved by collecting user-selected target spans in the future. Another limitation is that QuoteSum and VERI-GRAN benchmarks focus on attributing verbatim spans from the documents. Although Phukan et al. (2024) observe that LLMs tend to produce verbatim answers, it is interesting to evaluate attribution with more abstractive answers in future work. In addition, in dependency parsing augmentation, the expansion on the dependency parse tree is rule-based (e.g., recognizing coordinating components by relation \"conj\"), which could be improved by machine learning in the future. Finally, our work only considers attribution in English. Although the attention-based method can be easily adapted to LLMs of other languages, it may take effort (but not difficult, as we show in Appendix H) to adapt our dependency parsing augmentation to other languages since dependency parsing is language-specific."}, {"title": "Pseudo-code for ATTNUNION", "content": "The pseudo-code for ATTNUNION is Algorithm 1."}, {"title": "Word-Token Alignment in DEP", "content": "With words and tokens distinguished, the atomic-fact recognition method A should be updated to\n\n$A(r_i) \\leftarrow \\psi(\\bigcup_{w \\in \\varphi(r_i)} A(w))$\n\nwhere A is the atomic fact recognition method introduced in Sec. 3.3, $\\varphi$ is the token-to-words mapping, and $\\psi$ is words-to-tokens mapping. The $\\varphi$ maps a token to the minimum set of words that cover the token. The $\\psi$ maps a set of words to the minimum set of tokens that covers these words."}, {"title": "Details of Excluding Irrelevant Coordinating Constituents", "content": "This section describes how to exclude irrelevant coordinating constituents, given that A(ri) has included all successors of v (the closest verb ancestor of ri). In a dependency parse tree, a coordinating structure is multiple words w1, W1, ..., WK, K > 1 that satisfy\n\n$\\begin{aligned}\nlabel(w_1) &= \\dots = label(w_{K-1}), \\\\\nlabel(w_K) &= \\text{conj}, \\\\\nhead(w_2) &= \\dots = head(w_K) = w_1,\n\\end{aligned}$\n\nwhere head(wi) represent the parent of wi, and label(wi) represent the corresponding relation type between the parent and wi. Considering that the first component is special, we name the first component as the leader of the coordinating structure. The algorithm is outlined as follows."}, {"title": "Experiments on ATTNUNIONDEP'S Sensitivity to Hyperparameters", "content": "Here", "hyperparameters": "n1. L*: the layer to extract attention weights;\n2. k: how many top-scored prompt tokens are selected as evidence for each response token;\n3. T: the threshold for recognizing isolated tokens.\nThe experiments are conducted on validation sets of QuoteSum and VERI-GRAN, with the metric of accuracy (%), the attributor of ATTNUNIONDEP, and the models of Qwen2 7B and Llama 7B. The results are shown in Fig. 5, 6, and 7. The performance is relatively stable across a range of hyperparameters. Note that our choice of hyperparameters may not be optimal, because we did not extensively tune them but qualitatively chose them by human evaluation. We found under"}]}