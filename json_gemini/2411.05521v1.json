{"title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark", "authors": ["Sithursan Sivasubramaniam", "Cedric Osei-Akoto", "Yi Zhang", "Kurt Stockinger", "Jonathan F\u00fcrst"], "abstract": "Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy-a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.", "sections": [{"title": "1 Introduction", "content": "The health sector is being increasingly digitalized, with data stored in electronic health records (EHR) [25]. In practice, those records can be kept in various forms and systems: (i) traditional relational databases such as PostgreSQL or Oracle implement the relational data model where data is organized into relations (tables) that are collections of tuples (rows). Users access data through the declarative query language SQL; (ii) popular document databases, such as MongoDB, model data as a collection of documents in the document data model, providing data access through specialized languages such as the MongoDB Query Language (MQL); (iii) graph databases (triple stores) model data as property graphs (e.g., Neo4j) or semantic RDF graphs (e.g., Ontotext GraphDB), providing interfaces through the query languages Cypher and SPARQL, respectively.\nWhile the relational model and the SQL query language are still the primary choice for EHRs [22], there has been an increased interest in document and graph database models due to their schema flexibility and natural capacity to interconnect data sources across data silos [37, 11, 7]. E.g., AICCELERATE, a recent large-scale European pilot project on digital hospitals, uses a graph model"}, {"title": "2 Related Work", "content": "In this section, we review related works for Text-to-Query systems with a focus on (i) medical data and (ii) generally relevant benchmarks.\nMedical focus. MIMICSQL [40] is derived from the MIMIC-III database [15], containing 10K unique questions tailored to medical quality assurance tasks. To avoid potential limitations, such as fixed question structures, MIMICSQL underwent a filtering and paraphrasing process performed by expert freelancers. MIMIC-SPARQL [31] builds on the framework of MIMICSQL [40] and customizes its question templates to query a modified schema with SPARQL. With a similar structure to MIMICSQL, it provides 10K unique questions tailored to medical QA tasks. Last, EHRSQL [22] provides a benchmark for text-to-SQL tasks with a focus on electronic health records (EHR). It is based on MIMIC-III and eICU databases, while the 230 question templates are derived from user surveys. Based on these 230 templates, EHRSQL generates 24K questions/query pairs for SQL.\nGeneral benchmarks. The WikiSQL [44] dataset is a well-known general Text-to-SQL benchmark that comprises over 80K text/SQL pairs. What makes this dataset noteworthy is the wide distribution of queries over 24,241 tables. Spider [42] is considered one of the most popular cross-domain text-to-SQL datasets and consists of 10,181 questions with 5,693 unique SQL queries on 200 databases. KaggleDBQA [21] builds on large-scale datasets such as Spider and WikiSQL to provide a cross-domain dataset with domain-specific data types. BIRD [23] is a comprehensive resource for question answering (QA) that includes 12,751 unique questions from various repositories such as Kaggle, CTU Prague, and Open Tables and covers 37 subject areas. BIRD targets real-world applications by including complex examples from 95 large databases totaling 33.4 GB. ScienceBenchmark [43] presents three real-world, domain-specific text-to-SQL datasets. In comparison to other datasets, it reflects the high importance of domain-specific benchmark datasets for real-world text-to-SQL tasks. Last, FootballDB [12] investigates different database schemas and their impact on Text-to-SQL systems inside a single database.\nCompared to these works, the main novelty of SM3-Text-to-Query is that it is, to the best of our knowledge, the first dataset and benchmark that allows for the evaluation of Text-to-Query systems across three core database models (relational, graph, document) and four query languages (SQL, SPARQL, Cypher, MQL). FootballDB [12] is comparable in terms of analyzing the schema dependency of Text-to-SQL systems inside a single database. However, it only targets SQL, a single"}, {"title": "3 SM3-Text-to-Query Benchmark Construction", "content": "Our SM3-Text-to-Query benchmark construction consists of two main steps. First, we construct the database based on synthetic medical data in four data models (Figure 2). Second, we implement a template-based text/query-pair construction approach. The dataset was created over a period of more than a year in the context of a project with health professionals from two university hospitals, medical doctors, nurses, data scientists, and computer scientists who have been working in the respective fields for 5+ years. We constructed the question templates to cover all SNOMED CT entities. The database queries were mainly written by two undergraduates and one PhD student with a computer science and database background and verified by two faculty members."}, {"title": "3.1 Database Construction", "content": ""}, {"title": "3.1.1 Synthetic Patient Data with Synthea", "content": "As much of this data only reaches its full potential when it is interoperable across organizations, such as hospitals, insurance providers, and specialists, there has been a move toward standardization in healthcare. Here, SNOMED Clinical Terms (CT) is considered to be the most comprehensive, multilingual clinical healthcare terminology in the world [5]. While there exists a range of clinical EHR datasets such as MIMIC-III [15] and eICU [32], they are based on de-identified data from single countries (eICU) or even just single medical centers (MIMIC-III) and do not follow the SNOMED CT taxonomy.\nTo construct the SM3-Text-to-Query benchmark, we, therefore, choose to build on synthetic but standardized data that we generate through Synthea [39]. Synthea is an open-source, synthetic patient generator that models the medical history of synthetic patients and their electronic health records while being compatible with SNOMED CT. Its generated data includes 18 classes representing various aspects of healthcare, such as allergies, care plans, and medications, and is available in CSV format, FHIR, C-CDA, and CPCDS formats [26]. Synthea employs top-down and bottom-up approaches to generate structured synthetic EHRs throughout a patient's life. E.g., the simulation incorporates models for the top ten reasons for primary care visits and chronic conditions responsible for the most years of life lost in the United States, based on US Census Bureau, Centers for Disease"}, {"title": "3.1.2 Data Transformation to different Databases", "content": "Based on the Synthea output, we define data schemas/ontologies and implement Extract, Transform and Load (ETL) pipelines for our chosen databases: PostgreSQL (SQL), MongoDB (MQL), Neo4j (Cypher), GraphDB (RDF). For PostgreSQL, we define appropriate data types and consistency constraints (mainly primary and foreign keys). MongoDB, as a document database, does not enforce a strict schema as relational databases. Here, we define a JSON schema following a tree structure with four top-level collections (patients, organizations, providers, payers) and the remaining entities being embedded in these collections. We connect documents across collections through ID references ($lookup operator as equivalent to Join). For Neo4J, we implement ETL by following the guidelines provided by Neo4j Solutions [14], adapting it due to missing classes and connections in the original configuration files. For RDF, we extend Synthea-RDF [26] to cover all Synthea attributes and automate the conversion of data from CSV files into RDF as Terse RDF Triple Language (TTL). All four data models can be found in Appendix A.6 and in the supplement material."}, {"title": "3.2 Text/Query-Pairs Construction", "content": "To construct a dataset of text/query-pairs, we follow the established template-based approach [10, 40, 31, 22] in which a set of template questions is augmented with values to scale the dataset without extensive manual efforts. Together with the standard-based Synthea data (SNOMED CT taxonomy), our generation process has the following advantages:\n\u2022 Coverage and diversity through Synthea data generation. Through the use of templates, the benchmark dataset can be automatically adjusted to different Synthea datasets (e.g., for different patient populations). Each template is tagged with the relevant Synthea entities (e.g., patient, claim). Based on this structure, our method allows for the construction of datasets that only cover a subset of entities (e.g., only focusing on an insurance database).\n\u2022 Reduced bias in machine learning methods of the task. By filling the query templates with parameterizable values, various biases of text-to-query methods can be exploited. Thus, we can avoid the respective LLMs overfitting.\n\u2022 Standardized evaluation over different systems. The creation of standardized templates is possible through the implementation of the SNOMED terminology in the Synthea dataset. The benchmark dataset leverages SNOMED attributes, i.e., standardized medical terminology, for the evaluation of queries over different systems and database models. The same template questions can easily be combined with real-patient data following SNOMED medical terminology.\nOverall, we create 408 template questions (see supplement material for a full list and Appendix A.4 for an example) in a structured way that is guided by the goal to cover all 18 entity types created by Synthea. The template questions include WH* and non-WH questions, factual questions, linking questions, summarization questions, and cause-effect questions. We tag each template question with its related entities and question type.\nLast, following previous work [22], we define 10 non-answerable medical and 5 non-medical questions. These are questions that cannot be answered from data stored in the respective databases but would require additional information. For instance, What is the marital status of patient Max M\u00fcller?\nFor each question template, we manually develop the corresponding query in SQL, SPARQL, Cypher and MQL. The queries are then verified by a second expert for correctness. For scaling the template questions, we augment them by automatically inserting values such as IDs, descriptions of diseases, and patient names queried from the database. This data augmentation step is fully configurable and can be used to generate enriched and linguistically diverse text/query pairs for arbitrary Synthea databases and in-the-wild databases following the SNOMED CT standard."}, {"title": "4 Dataset Analysis and Comparison", "content": "Question and Query Statistics. For SM3-Text-to-Query, we use our method described in Section 3.1 to construct a synthetic multi-modal dataset of 10K text/query pairs for each of the four query languages (resulting in 40K individual samples). The dataset is based on the default Synthea configuration with medical records of 100 living and 10 diseased patients. We split the data into 6K train, 2K dev, and 2K test with stratified sampling where the strata are the entity types that the questions are tagged with. Figure 3 summarizes dev and test, with the distribution of different types of user questions on the left. While our template questions cover all SNOMED entities, there exist more templates for allergies, imaging studies, patients, and payers, which is why they are over-represented in the augmented dataset.\nWhile all queries across query languages return the same information for a single user question, they vary greatly in their characteristics (Figure 3, right). For instance, SPARQL queries have the largest number of tokens and keywords, followed by MongoDB's MQL queries and SQL queries, while Cypher queries are the most compact ones. We also count the number of joins/traversals and the nesting depth (based on opening/closing characters) of the queries, an established complexity measure in programming language research [3]. Here, MQL requires notably the fewest joins ($lookup operators) while exhibiting the highest nesting depth (due to the embedded collection structure in its schema).\nComparison to other datasets. Table 1 summarizes SM3-Text-to-Query with respect to a selection of relevant datasets and benchmarks. SM3 has a similar number of example questions compared to other datasets, while the number of corresponding queries is substantially higher due to the translation to four query languages. Qualitatively, SM3 is the only dataset that enables an evaluation across four different query languages, which is not only unique for medical data but does currently not exist for any domain. Last, SM3 is standard-based (SNOMED), making it compatible with standard-based real health databases and extensible through our template-based approach."}, {"title": "5 Baseline Experimental Evaluation", "content": "The goal of our experiments is to evaluate how well large language models (LLMs) perform in translating natural language questions into four different query languages provided by our novel benchmark. We restrict ourselves to LLMs as these models dominate the leader boards of popular Text-to-Query benchmarks such as Spider [42]."}, {"title": "5.1 Experimental Setup", "content": "For our baseline experiments, we select a set of four common open and closed-sourced LLMs and implement the same in-context learning (ICL) prompting strategies across our four query languages. Here, we follow standard practices in terms of task instruction formulation and the inclusion of schema/ontology information as well as few-shot examples. We take inspiration from Nan et al. who propose general design strategies for enhancing Text-to-SQL capabilities in LLMs [28], widely adopted in recent applications. Further, Chang and Fosler-Lussier demonstrate the importance of including database schema and content [6]. Liu and Tan suggest using notes and annotations to mitigate unexpected behaviors of LLMs [24], improving accuracy. Drawing upon these strategies, we developed various prompt templates tailored to the requirements of our experimental settings. The detailed templates applied in the experiments are listed in Appendix A.5.\nAs open-source LLMs, we select Meta Llama3-8b and Llama3-70b (instruction-tuned variants) [2]. As closed-source LLMs, we select Google Gemini 1.0 Pro [38] and OpenAI GPT-3.5-turbo-0125 [30]. The closed-sourced models are run via their respective APIs. We run Llama3-8b locally on a single NVIDIA A100 GPU. For Llama3-70b, we use the cloud-hosted model provided by Groq [13].\nWe define three prompts with schema information (w/schema 0-shot, w/schema 1-shot and w/schema 5-shot) and two prompts without schema information (w/o schema 1-shot and w/o schema 5-shot). Due to the size of the ontology for SPARQL, we implement a smaller version in which the ontology is summarized as a JSON object with all contained classes, objects, and data properties. Likewise, for MQL and Cypher, we provide the encoded MongoDB document schema and Neo4J graph schema, respectively. The shots are selected using stratified random sampling by considering the categories of the original question templates to ensure a diverse selection. For the prompts that include examples (1-shot/5-shot), we perform five runs with different sampling (for further details, see Appendix A.5).\nExecution Accuracy (EA). We apply exact execution accuracy (EA), also known as result matching [18] as our main accuracy metric. EA denotes the fraction of questions within the evaluation set, where the outcomes of both the predicted and ground-truth queries yield identical results relative to the total number of queries. Given two sets, i.e., the reference set R, produced by the execution of the n ground-truth SQL queries $Y_n$, and the corresponding result set denoted as $\\hat{R}$ obtained from the execution of the predicted SQL queries $\\hat{Y}_n$, EX can be computed by Equation 1."}, {"title": null, "content": "$EA = \\frac{\\sum_{n=1}^{N} I(r_n, \\hat{r}_n)}{N}$    (1)"}, {"title": null, "content": "where $r_n \\in R_n$, $\\hat{r}_n \\in \\hat{R}_n$, and $I$ is the indicator function, defined as:\n$I(r_n, \\hat{r}_n) =\\begin{cases}1 & \\text{if } r_n = \\hat{r}_n\\\\0 & \\text{else}\\end{cases}$    (2)"}, {"title": "5.2 Text-to-Query Accuracy", "content": "We first evaluate Text-to-Query accuracy for the different prompting strategies, LLMs, and query languages. Table 3 depicts the Execution Accuracy (EA) without schema information (two left-most columns), while the three right-most columns contain results for experiments with schema. The \u00b1 represents the standard deviation. Numbers are in %. We observe the following four key insights:\n\u2022 Schema information helps for all query languages, but not equally. As expected, the accuracy of w/ schema experiments is higher than that of their w/o schema counterparts. Especially with 1-shot, the models cannot generate correct queries in the majority of cases. However, surprisingly, schema information has a much larger impact on the performance for SQL, Cypher, and MQL (more than doubles the performance for 5-shot compared to w/o schema) than for SPARQL (only slightly higher or equal performance). This indicates that LLMs may have encountered the SNOMED CT ontology and vocabulary during their pre-training phase, as these are standardized and widely published on the web, whereas the specific schemas for SQL, Cypher, and MQL databases are private to each implementation and thus novel to the model, making explicit schema information more crucial for these query languages.\n\u2022 Adding examples improves accuracy through ICL for all LLMs and query languages, however, the rate of improvement varies greatly across query languages. For SQL\u2014the most popular query language-the larger LLMs already achieve \u2248 40% (w/schema 0-shot) and only improve by \u2248 10 points with 5-shots (\u2248 25% relative improvement). For the \u201cmore exotic\u201d query languages (SPARQL, MQL, and partly Cypher), LLMs are often unable to generate a correct query with only the schema information. E.g., for SPARQL, 0-shot is < 4%, while 5-shot can reach up to 30% (10-fold relative improvement). This indicates again that the model is already proficient in the SQL query language, whereas for SPARQL (and to a smaller extent Cypher and MQL), the model is truly benefiting from ICL by learning how to formulate more correct queries from the provided fixed few-shot examples (even though the examples might not directly be related to the asked question).\n\u2022 LLMs exhibit mostly consistent performance patterns across query languages. Observing w/schema 5-shot results, Llama3-70b achieves the best results for all query languages. GPT-3.5 and Llama3-70b share the 2nd and 3rd place, while the smallest LLM Llama3-8b achieves always the lowest accuracy. Further, some results show a large standard deviation, indicating that the different few-shot example compositions for each run have a large performance impact. To further investigate the impact of few-shot sampling, we explore an advanced similarity-based sampling strategy in Section 5.4. An overall even higher standard deviation can be observed for MQL. We trace this additional variance to inconsistent output variations in LLMs (see also Section 6).\n\u2022 LLMs have varying levels of knowledge across different query languages. We suspect that this can be traced back to their training data. A large resource of such training data has been Stack Overflow [16]. When we search Stack Overflow for tags (indicated with []) related to our four query languages, we get the following numbers (15.08.2024): [SQL]: 673K posts; [SPARQL]: 6K posts; [MongoDB, MQL]: 176K posts; [Cypher, Neo4J]: 33K posts. Relating the number of posts to our \"w/ schema 0-shot\u201d results (we want to leave the impact of few-shots ICL out of this), we see that SQL performs best (best model: 47.05% ), Cypher and MQL perform average (best models: 34.45% and 21.55%), while SPARQL performs worst (best model: 3.3%). These results correlate to the post frequency on Stack Overflow and support results by [16] that find a statistically significant impact on the correctness of LLM answers based on question popularity and recency.\nAn exception to this pattern is MQL as it is under-performing Cypher. We suspect that this has to do with the fact that Cypher queries contain much fewer tokens and language keywords than MQL (only 25% of tokens and 50% of keywords, see Figure 3)."}, {"title": "5.3 Per-category Results", "content": "Next, we analyze the performance on a per-category level based on the entity-tagged template questions. For that, we look at the results for w/ schema 0-shot and w/ schema 5-shot to observe the impact of ICL through few-shot examples for our 19 question categories, our four query languages, and four LLMs. Figure 4 (top) shows the execution accuracy based on the w/ schema 0-shot results, while Figure 4 (bottom) shows the mean results for w/ schema 5-shot experiments."}, {"title": "5.4 Similarity-based few-shot sample selection", "content": "Last, we implement a similarity-based approach using a BM25 [33] retriever that, on a per-question basis, retrieves the five most similar question-query pairs from the training data. We use these"}, {"title": "6 Discussion and Limitations", "content": "While SM3-Text-to-Query is the first benchmark across four different query languages, we note several limitations, some of which provide the potential for further research.\nFirst, compared to other datasets [22], our question templates were only created with guidance from health professionals, but not directly formulated by them. Our dataset is synthetic, based on simulated patient data, with the benefit of flexibility and no privacy issues. In the future, we plan to extend our question set through crowd-sourcing as part of an ongoing Swiss digital health project [8].\nSecond, SM3 currently only contains English questions and database values. We believe that the addition of multilingual questions and databases could be a valuable extension to our benchmark.\nThird, while our questions cover all main entities in the dataset, their corresponding queries might be too easy in some query languages (e.g., SQL and Cypher require, on average, less than 10 tokens; see Figure 3). There is the potential to include temporal templates as in [22] to increase query complexity.\nLast, we experience that LLMs exhibit large output variations for the same prompt and their generations can be inconsistent across query languages. We implemented extensive data-cleaning logic to extract the predicted query from the LLM output. These outputs varied across models but also across languages: while GPT-3.5 follows instructions well for SQL, it does not for MQL, despite the same structured prompt (see Appendix A.3 for examples of encountered issues). Further research should focus on prompt optimization across database models, potentially using LLMs as optimizers [41].\nPotential negative societal impacts. There are no direct negative societal impacts associated with our dataset. It will help researchers and people in industry to improve their Text-to-Query systems, thereby democratizing data access to wider user groups."}, {"title": "7 Conclusion", "content": "This paper provides, to the best of our knowledge, the first multi-model Text-to-Query dataset and benchmark that allows for the evaluation of Text-to-Query systems across three core database models (relational, graph, document) and four query languages (SQL, SPARQL, Cypher, MQL). Our dataset is based on synthetic medical data generated through Synthea [27], follows an international medical ontology standard (SNOMED [35]), and can be easily extended through further template questions or by exchanging the synthetic data through standard-conform real patient data. SM3-Text-to-Query will be essential to develop and test the next generation of Text-to-Query systems that appear with increasing frequency thanks to the progress in transformer-based large language models. All our code and data are available at https://github.com/jf87/SM3-Text-to-Query."}, {"title": "A Appendix", "content": "We include below technical appendices (such as the evaluation results on the development dataset and efficiency results). The additional material that supports our dataset and benchmark documentation is provided in the supplement material."}, {"title": "A.1 Text-to-Query Accuracy Development Data", "content": "Table 5 summarizes the Execution Accuracy (EA) results of the different models, prompts, and query languages for the development dataset - equivalent to Table 3 for the test data in the main part of the paper. However, due to timing and cost constraints, we have only performed three runs and used a smaller dataset for Llama3-70b. The results from the development set show similar patterns and insights as we have described in Section 5."}, {"title": "A.2 Text-to-Query Efficiency", "content": "The run-time efficiency of queries against single database systems, a core problem of query optimization, has recently also drawn attention from the text-to-SQL community. An interesting aspect is to compare the runtime of the manually created ground truth queries with the generated queries.\nHere, we show results for the recently proposed Valid Efficiency Score (VES) [23], which measures Text-to-Query Efficiency. We execute all queries three times. The respective databases are running on virtual machines on an OpenStack cluster with the same specs (8 cores 16GB RAM). We omit the LLM inference time, as this time varies substantively by parameters that we cannot fully control (e.g., current OpenAI, Google, Groq server load). Moreover, our focus is on query execution time within a database system, which is independent of the machine learning inference time for generating the queries.\nValid Efficiency Score (VES). Pioneered by BIRD [23], the VES score aims to include the efficiency of the generated query together with the Execution Accuracy (EA)."}, {"title": null, "content": "$VES = \\frac{\\sum_{n=1}^{N} I(r_n, \\hat{r}_n) \\cdot R(Y_n, \\hat{Y}_n)}{N}, \n\\quad R(Y_n, \\hat{Y}_n) = \\frac{VE(Y_n)}{VE(\\hat{Y}_n)}$    (3)"}, {"title": null, "content": "where $R(\\cdot)$ denotes the relative execution efficiency of the predicted query in comparison to the ground-truth query. $E(\\cdot)$ is a function to measure the absolute execution efficiency for each query in a given environment, e.g. execution time in milliseconds. For further details, we refer the reader to [23]. Table 6 and Table 7 depict the VES results for the test and dev dataset, respectively."}, {"title": "A.3 Encountered issues with LLM outputs", "content": "As mentioned in Section 6, our tested LLMs showed output variations for the same prompt across query languages. Table 8 illustrates the inconsistency of LLM outputs for clearly instructed prompts and shows how different LLMs respond to the same prompt in varied and often erroneous ways (e.g., by providing multiple queries or repeating part of the instruction)."}, {"title": "A.4 Question Template Example", "content": "Table 9 depicts an example question template for our four query languages. We categorize the questions in question types and tag them with their related entities."}, {"title": "A.5 Experimental Details about Prompt Engineering", "content": "This section provides details about the prompt engineering approaches for the four different query languages using zero and few shots for experiments with and without using the respective database schemas."}, {"title": "A.6 Database Schemas", "content": "In the following pages, we provide the schemas for our four database models in a visual form."}]}