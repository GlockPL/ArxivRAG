{"title": "Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning", "authors": ["Bangzhen Liu", "Chenxi Zheng", "Xuemiao Xu", "Cheng Xu", "Huaidong Zhang", "Shengfeng He"], "abstract": "The vulnerability of 3D point cloud analysis to unpredictable rotations poses an open yet challenging problem: orientation-aware 3D domain generalization. Cross-domain robustness and adaptability of 3D representations are crucial but not easily achieved through rotation augmentation. Motivated by the inherent advantages of intricate orientations in enhancing generalizability, we propose an innovative rotation-adaptive domain generalization framework for 3D point cloud analysis. Our approach aims to alleviate orientational shifts by leveraging intricate samples in an iterative learning process. Specifically, we identify the most challenging rotation for each point cloud and construct an intricate orientation set by optimizing intricate orientations. Subsequently, we employ an orientation-aware contrastive learning framework that incorporates an orientation consistency loss and a margin separation loss, enabling effective learning of categorically discriminative and generalizable features with rotation consistency. Extensive experiments and ablations conducted on 3D cross-domain benchmarks firmly establish the state-of-the-art performance of our proposed approach in the context of orientation-aware 3D domain generalization.", "sections": [{"title": "1 INTRODUCTION", "content": "3D scene understanding and reasoning are crucial in various applications [20], [3] such as autonomous driving, architectural design, and virtual/augmented reality. 3D Point cloud representation has gained popularity due to its simplicity and effectiveness, particularly with the advancements in deep learning [36], [29], [42]. However, most existing 3D point cloud analysis models hold a strong i.i.d. (independent and identically distributed) assumption between training and testing domains. This assumption can lead to significant performance degradation in real-life out-of- distribution scenarios. Factors such as varying sensor parameters and environmental conditions introduce distributional divergence between training and testing domains, limiting the practical usage of these models.\nPrevailing methods focus on addressing different types of domain shifts in point clouds, such as shape variance, non-uniform point density, sensor noise, and self-occlusion, through point cloud domain adaptation (3DDA) [30], [23] and domain generalization (3DDG) [17], [38]. However, they make the naive assumption that objects from both source and target domains are strictly aligned, meaning they have the same upright pose across all objects and categories. This assumption disregards the orientation of objects and can result in a model that is biased toward the aligned dataset. As a consequence, even slight variations in orientation may incur catastrophic misclassification by the model (Fig. 1(a)). In real- world scenarios, objects often originate from various domains with diverse orientations, ensuring an accurate understanding of these targets imposes great demands on a generalizable and rotational robust 3D recognition systems. This motivates us to investigate the orientational gap and study a new problem called orientation-aware 3D domain generalization, which aims at exploring the robustness of 3D representations under the disturbance of varying rotations.\nTo ensure the robustness of a model against arbitrary rotational variations, random rotation augmentation [29], [36] is intuitively applied. However, generalizing to arbitrary rotations is challenging due to the numerous possible rotated angles in 3D space [45]. Sparse sampling over the dense rotation space often leads to imbalanced learning (Fig. 1(b)), where the model tends to memorize the easy tasks but struggles with unseen challenges. In practice, tackling challenging tasks can enhance one's ability to draw inferences for similar problems, a process known as extrapolation. Deep models exhibit a similar talent for extracting rich information from samples that are difficult to separate [31]. This motivates our focus on challenging orientations, which will be discussed with supporting experimental evidence in Section 3. Building upon these observations, we aim to enhance the model's generalizability to arbitrary rotations by learning from intricate samples.\nBased on this idea, we propose to address the problem of orientation-aware 3D domain generalization through intricate orientation mining. Our approach alternatively optimizes between multiple steps. First, we identify the most aggressive rotation for each point cloud and construct an intricate set by optimizing intricate orientations. Next, we create the hardest sample pairs based on the intricate set and utilize contrastive learning to obtain categorically discriminative and generalizable representations with rotation consistency. More specifically, we optimize the intricate rotation matrix, which is parameterized by three optimizable parameters, by maximizing the prediction error of the current model. Once we obtain the intricate set, we construct sample-wise and category-wise pairs for contrastive training. The sample-wise pair consists of the original point cloud along with its rotated variant, where the rotation angles are sampled from the intricate set. The category-wise pair is formed by selecting point clouds from different classes that are close together in the representation space.\nIn summary, our contributions are three folds:\n\u2022 We introduce the novel task of orientation-aware 3D domain generalization, which more accurately captures real-world domain shifts involving unpredictable rotations, and delve into the importance of challenging orientations in reducing orientational shift.\n\u2022 We propose a novel approach that enhances rotational ro- bustness through intricate orientation mining, which further incorporates a contrastive framework that utilizes intricate samples to improve the generalizability of feature learning via inner- and intra-categorically contrastive learning.\n\u2022 Experimental results on widely used benchmarks demonstrate that our method achieves state-of-the-art performance on several point cloud analysis tasks under the orientation-aware 3D domain generalization setting."}, {"title": "2 RELATED WORKS", "content": "3D Point Cloud Domain Adaptation and Generalization. Early endeavors within 3D domain adaptation (3DDA) focused on extending 2D adversarial methodologies [30] to align point cloud features. Alternative methods have delved into geometry-aware self-supervised pre-tasks. Achituve et al. [1] introduced DefRec, a technique employing self-complement tasks by reconstructing point clouds from a non-rigid distorted version, while Zou et al. [46] incorporating norm curves prediction as an auxiliary task. Liang et al. [23] put forth MLSP, focusing on point estimation tasks like car- dinality, position, and normal. SDDA [4] employs self-distillation to learn the point-based features. Additionally, post-hoc self-paced training [46], [13], [27] has been embraced to refine adaptation to target distributions by accessing target data and conducting further finetuning based on prior knowledge from the source domain.\nRotation-generalizable Point Cloud Analysis. Previous works in point cloud analysis [29], [36] enhance rotation robustness by introducing random rotations to augment point clouds. However, generating a comprehensive set of rotated data is impractical, resulting in variable model performance across different scenes. To robustify the networks w.r.t. randomly rotated point clouds, rotation- equivariance methods explore equivalent model architectures by incorporating equivalent operations [32], [11], [25] or steerable convolutions [7], [28]. Alternatively, rotation-invariance approaches aim to identify geometric descriptors invariant to rotations, such as distances and angles between local points [6], [43] or point norms [45], [22]. Besides, Li et al. [21] have explored disam- biguating the number of PCA-based canonical poses, while Kim et al. [18] and Chen et al. [8] have transformed local point coordinates according to local reference frames to maintain rotation invariance. However, these methods focus on improving in-domain rotation robustness, neglecting domain shift and consequently exhibiting limited performance when applied to diverse domains. This study addresses the challenge of cross-domain generalizability together with rotation robustness and proposes novel solutions.\nIntricate Sample Mining, aimed at identifying or synthesizing challenging samples that are difficult to classify correctly, seeks to rectify the imbalance between positive and negative samples for enhancing a model's discriminability. While traditional works have explored this concept in SVM optimization [14], shallow neural networks [12], and boosted decision trees [41], recent advances in deep learning have catalyzed a proliferation of researches in this area across various computer vision tasks. For instance, Lin et al. [24] proposed a focal loss to concentrate training efforts on a selected group of hard examples in object detection, while Yu et al. [41] devised a soft multilabel-guided hard negative mining method to learn discriminative embeddings for person Re-ID. Schroff et al. [31] introduced an online negative exemplar mining process to encourage spherical clusters in face embeddings for individual recognition, and Wang et al. [35] designed an adversarially trained negative generator to yield instance-wise negative samples, bolstering the learning of unpaired image-to- image translation. In contrast to existing studies, our work presents the first attempt to mitigate the orientational shift in 3D point cloud domain generalization, by developing an effective intricate orientation mining strategy to achieve orientation-aware learning."}, {"title": "3 ORIENTATIONAL SHIFT ANALYSIS", "content": "Problem Definition. In practical scenarios, objects often originate from various domains, and concurrently, present diverse orien- tations. To achieve an accurate understanding of these targets, our goal is to grant 3D recognition systems with cross-domain generalizability and robustness to rotational transformations. To this end, we introduce a novel task: orientation-aware 3D domain generalization, and explore the applicability of existing 3D recog- nition systems under orientational shifts. Let X and Y be the input and label spaces, respectively. We consider a labeled source domain Ds = {(Pi, yi)}_{i=1}^{n_s} with ns samples and an unlabeled target domain Dt = {Pi}_{i=n_s+1}^{n_s+n_t}. Here, y \u2208 Y is the source labels, and Ps, Pt \u2208 X represent point clouds with a specified orientation relative to the world coordinate system, where each P\u2208 R^{N_c \u00d73} consists of Nc points. The orientation is defined by a 3 \u00d7 3 rotation matrix M_i \u2208 O C SO(3), where O is the set of orientations in the given domain and SO(3) represents all rotations in R^3. The objective of orientation-aware 3D domain generalization is to learn a projection function g : X \u2192 Y that can be applied to any given target domain Dt with arbitrary orientations in O, by solely training on the labeled source domain Ds.\nGeneralization Analysis of Orientations. To accurately under- stand 3D shapes, information from multiple perspectives is often necessary. Some of the perspectives are easy-to-understand by deep models but less informative, incurring a phenomenon known as \"taking the whole from a part\". Specifically, in the context of rotation-robust point cloud analysis, these learning perspectives refer to those rotated variants with small gradients during training. Random rotation attempts to capture comprehensive information through a Monte Carlo approach, but the vast number of possible rotated angles introduces difficulties and leads to imbalanced learning. This biased learning problem could give rise to inaccurate selection of point features, making false decisions. As shown in Fig. 2(a), the learned model cannot well distinguish the rotated sample from \u201ctable\u201d and \u201cchair\u201d, as they may contain similar features such as slim legs, which are learned from easy-to- understand samples. On the other hand, intricate samples lying beyond current cognitive boundaries can offer complementary knowledge for learning a more accurate discriminative boundary.\nWe empirically validate these insights by employing Maximum Mean Discrepancy (MMD) [15] as a measure of distributional shifts. The MMD calculates the distance between source and target distributions in the Reproducing Kernel Hilbert Space (RKHS). We train a DGCNN [36] on the ModelNet (source domain) and test it on ShapeNet (target domain), augmenting the data with random rotations and intricate orientations, respectively. The intricate orientations of the training dataset are obtained by optimizing the rotational angles of each point cloud to maximize the cross-entropy loss with a baseline model trained on aligned data. Fig. 3(a) depicts the per-class MMD values, demonstrating that training with intricate orientations reduces orientational shifts more effectively. To evaluate discriminability and consistency under varying rotations, we further train a linear SVM on the source domain and test it on the target domain. We augment each point cloud 64 times as illustrated in Section 5 and measure consistency by calculating the mean KL-divergence between their output probabilities. As shown in Fig. 3(a), the discriminability of the model augmented with intricate orientations matches that of the randomly rotated version, while better preserving consistency across different rotations. Based on these findings, we aim to leverage intricate samples to reduce orientational shift, enhance output consistency, improve model discriminability to arbitrary rotations, and finally obtain a better classifier that is generalizable towards various domains (Fig. 3(b))."}, {"title": "4 INTRICATE ORIENTATION LEARNING", "content": "The pipeline of our framework is depicted in Fig. 4. It consists of an iterative optimization process that alternates between intricate orientation mining and orientation-aware contrastive training. We first generate an alternative set of intricate rotation angles through intricate orientation mining. Then we augment the training point clouds by applying the selected angles from the alternative set, creating a series of intricate sample pairs. To enhance the generalizability of the learned representations, we employ orientation-aware contrastive training using these intricate pairs. Our framework incorporates an orientation consistency loss, which encourages the learning of more representative features while promoting their consistency w.r.t. random rotations. Additionally, we introduce a margin separation loss to further improve the categorical discriminability of the learned representation space. In the following sections, we elaborate on each component of our framework in detail."}, {"title": "4.1 Intricate Orientation Mining", "content": "Let us first consider a standard classification problem over the given data distribution D = {(Pi, yi)}_{i=1}^{n}. Suppose we have a point cloud classification model, the goal is to search the model param- eters w* that optimize the empirical risk E_{(P,y)~D} [L(w, P, y)], where L is a proper loss function. To improve the robustness, the most adopted technique is data augmentation, such that the model can resist possible perturbations on the input data, resulting in the following objective:\nw* = arg min E_{(P,y)~D} [L(w, f(P), y)], (1)\nwhere f is the perturbation function. Considering the orientational shift only, the perturbation function of a given point cloud Pi can be regarded as augmenting the aligned pose with a rotation matrix Mi. According to Euler's rotation theorem, the rotation matrix is defined in R\u00b3 as M_i = R_x R_y R_z, where R_{\u03b8_j} is the rotation matrix about the axis-j by angle \u03b8_j \u2208 [-\u03c0, \u03c0) over the cartesian coordinates. In this case, the rotation matrix is parameterized by \u0398_i = [\u03b8_{xi}, \u03b8_{yi}, \u03b8_{zi}]. Since the trigonometric function is differential in the scope [-\u03c0, \u03c0), we can optimize O\u00bf through gradient descent. The essence of intricate orientation mining lies in the deliberate search for intricate orientations, instead of directly optimizing the model with random rotations. This endeavor aims to identify a rotation matrix Mi that introduces perturbations to the point cloud, such that the 3D model is confounded:\n\u0398_i = arg max_{\u0398_i} L(w*, f (\u0398_i, P_i), y_i), (2)\nwhere f (Oi, Pi) = Mi \u00b7 Pi, and L is a task-specific optimization function (e.g., cross-entropy loss for classification task, hereafter referred to as Lcls). By iteratively maximizing this objective, we can calculate the most intricate orientations for all the point clouds within D. The details of gradient calculation are provided in the supplement. We adopt Projected Gradient Descent [26] to ensure that the optimized O\u00bf can generate a pure rotation matrix. By applying intricate orientation mining to the whole training dataset, we obtain the intricate orientation set I {\u0398_i}^{N}_{i=1}, which is essential for the construction of intricate sample pairs. Note that each orientation is specified for the according sample. To increase the diversity of I, we further initialize with random values and repeat the optimization multiple times. Finally, the intricate set is defined as I = {{\u0398_i}^A_T }_{i=1}^{N}, where AT is the number of augmentation times. During the training stage, we periodically update the intricate set I for every T epochs."}, {"title": "4.2 Orientation-aware Contrastive Training", "content": "Based on the intricate orientation set I, we construct the intricate sample pairs and build a contrastive learning framework upon them to obtain categorically discriminative and generalizable features with rotational consistency. As Fig. 4 shows, our model consists of an optimizable student network Fs = C \u25cb Es, and a frozen teacher network Ft = Ct \u25cb Et, where Es/t is a feature extractor which encodes point cloud to the representation space and derives the final classification result through a linear classifier Cs/t. The teacher network is optimized by Exponential Moving Average (EMA) [33].\nInner-sample Rotational Consistency. The main idea is to maximize the agreement between the original shape and its intricate augmented variants via self-contrastive learning. Each time we sample a mini-batch of N point clouds {Pm}_{m=1}^{N} and randomly select one intricate rotation Om from I for each point cloud. Then we construct the intricate sample pairs B^{it} = {(Pm, P_m)}_{m=1}^{N}, where P_m = f(\u00f4m, Pm) is the augmented version of the original point cloud with the intricate rotation. Bit serve as the input of (Ft, Fs) and the output logits of (Pm, P_m) are denoted as pm and p_m, respectively. To ensure the consistency of the intricate pairs, we perform knowledge distillation [16] on the output probabilities, the orientation consistency loss is therefore formulated as:\n\u03c3(\u03c1/\u03c4) =\n\\frac{\u0435\u0445\u0440(\u0440/\u03c4)}{\\sum_{i=1}^N exp(p(i)/\u03c4)},\n(3)\nL_{oc} =\\frac{1}{N}\\sum_{m=1}^{N}\\sum_{k=1}^{K} \u03c3(p_m/T_t)log(\u03c3(p_m/T_s)),\n(4)\nwhere K is the number of categories and \u03c3(\u00b7) is the softmax function. T\u2208 R+ is the temperature parameter controlling the magnitude of the output logits, where the subscripts in [Ts, Tt] denote the source and target domains, respectively. By iteratively refining the distance between the aligned object and its intricate variant, the model can gradually adapt to the harder rotational variations and learn common knowledge concerning different rotations.\nIntra-category Discriminability. To further enhance discrim- inability, we aim for the learned representation space to maintain rotational consistency while improving the classification ability. To achieve this, we introduce a margin separation loss that yields a classifier with a concise and accurate boundary, capable of handling the unforeseen rotational perturbations. Specifically, we first construct the intra-category intricate pairs by considering the relationships between different categories in the same mini-batch. The intricate pairs can be divided into two groups, including the positive pairs and the negative pairs. Simply, the positive pairs can be found by traversing the training batch and sorting out samples that belong to the same class. But the embedding space under multiple orientations may not be well simulated by the originally aligned point clouds. Instead of learning from only one intricate orientation, we further augment each point cloud Pi with V number of intricate angles sampled from {\u0398_{i}}^{A_T} to better simulate the feature space. The loss is designed to minimize the cosine distance between the positive samples as follows:\nL_{pos} =\\frac{1}{K} \\sum_{k=1}^{K}\\frac{1}{N_k}\\sum_{i=1}^{N_k}\\sum_{j=1 j\u2260i}^{N_k} \\frac{1}{V}\\sum_{v=1}^{V} cos(\\frac{E_s(P_{i,v})}{T'}, \\frac{E_s(P_{j})}{T'}),\n(5)\nwhere cos(\u00b7,\u00b7) denotes the cosine similarity between the two inputs, Nh is the number of samples of class k within the mini-batch and T' is the temperature parameter.\nThe negative pairs are formed by samples with different labels. To enlarge the discrepancy of different categories in the representation space, we plan to maximize the distance between each negative pair with:\nL_{neg} =\\frac{1}{K} \\sum_{k=1}^{K}\\frac{1}{N}\\sum_{i=1}^{N_o} \\sum_{j=1}^{N_k} cos(\\frac{E_s(P_{i,v})}{T'}, \\frac{E_s(P_{j})}{T'}),\n(6)\nwhere No is the number of negative samples. The overall margin separation loss is summarized as follows:\nL_{ms} = L_{pos} + L_{neg}. (7)\nThrough contrastive tasks, the point representations under varying rotations are further clustered within the same category, while the other categories are pushed away. A more compact and consistent representation space is formed, thereby enhancing the discriminability for rotated shape classification.\nThe entire framework is optimized in an alternative scheme between the intricate orientation mining and the orientation-aware contrastive training. For the intricate orientation mining, the final objective is the same as Eq. 2, where L is the standard cross-entropy function over the originally aligned point clouds. The intricate set is periodically updated after T epochs of training to avoid overfitting to the current intricate rotations. For orientation-aware contrastive training, the final loss function is\nL_{final} = L_{cls} + \u03bb_{oc}L_{oc} + \u03bb_{ms}L_{ms}, (8)\nwhere Lcls is the cross-entropy loss on the intricate point clouds."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experiment Setup\nDatasets. We conduct experiments on the widely used 3d cross- domain benchmark PointDA [30] and PointSegDA [1] for shape classification and part segmentation, respectively. PointDA collects shapes of 10 categories from ModelNet (M) [39], ShapeNet (S) [5], and ScanNet (S*) [10], with all objects aligned in the up-right direction and down-sampled into 1024 points. We perform six cross- domain classification tasks on PointDA, including M\u2192S, M\u2192S*, S\u2192M, S\u2192S*, S*\u2192M, and S*\u2192S. Twelve cross-domain tasks are conducted on PointSegDA. This dataset consists of several human body shapes that are collected with different point distributions, poses, and scanned humans from four subdomains: Adobe, Faust, Mit, and Scape. The shapes share eight categories of human body parts (head, hand, feet, etc) while down-sampling into 2048 points. We strictly follow the same dataset splitting (80% for training and 20% for testing) for PointDA as [30] and the same data split for PointSegDA as [1] to implement our method under the orientation- aware 3D domain generalization setting.\nEvaluation and Metrics. To simulate the orientational shift, we randomly generate a series of SO(3) rotation matrices for each point cloud by uniformly sampling the rotation angles of the three- axises in the SO(3) space over [-\u03c0, \u03c0], and rebuild the dataset with free-axis rotated augmentation for training. To comprehensively evaluate the performance and generalizability of each method under the orientation-aware 3D domain generalization setting, we sample 4 equidistant rotation angles per axis, i.e., [\u2212\u03c0, \u2212\u03c0/2, \u03c0/2, \u03c0], and construct 64 rotation augmented testing series. Each method is evaluated to obtain the average performance together with variance over the 64 tests. The statistics are presented by the mean of 5 times training for each method. We report the results of PointDA in the form of macro-average precision score (Avg.) while measuring the mean Intersection-over-Union (mIoU) for PointSegDA to evaluate the segmentation performance.\nCompared Methods. We compare our method with both 3DDG (Metasets [17], PDG [38]) and 3DDA (PointDAN [30], DefRec [1], GAST [46], MLSP [23], SDDA [4], and PCFEA [37]) methods on both cross-domain generalization classification and part segmenta- tion tasks. Since they originally ignored the orientational shift, we obtained the experiment results by executing their official codes under the orientation-aware 3D domain generalization setting for fair comparison. To evaluate the robustness of our method towards orientational shift, we also compare with state-of-the-art rotation- equivalent and rotation-invariant point cloud analysis methods by applying them in the cross-domain scenario, including EOMP [25], VN [11], SVN [32], SPRIN [40], RIPCA [21], RIConv++ [44], PaRI [8], and LocoTrans [9].\nImplementation Details. We implement our framework in Fig. 4 with the widely used rotation-invariant model PointNet [29] and non-invariant model DGCNN [36] as our backbones, training with 200 epochs and batch size 32 on a single RTX3090 GPU. We employ Adam [19] as the optimizer. The update period T of the intricate orientation set is set as 20. The learning rate is initialized to 10\u22123 with a decay weight of 10\u22124 and scheduled by a degradation function (1 + \u03b3(ep/epmax))\u2212\u03b2 during the training phase, where epmax is the maximum number of training epochs, and \u03b3, \u03b2 are set as 10 and 0.75, respectively. The temperature factor Ts, Tt in Eq. 4 are empirically set to 0.5 and T' in Eq. 5 and Eq. 6 is set to 0.07. The number of repeat times AT is set as 10, while the number of the intricate variants V in Eq. 5 are set as 5 due to limited memory. \u03bboc and \u03bbms in Eq. 8 are both set as 0.01 by observing the performance changes on M\u2192S (Please refer to the hyper-parameter sensitivity analysis in the supplementary material.). For part segmentation, we simply equip the backbone networks in our framework with an extra decoder for dense prediction and extend the Lcls in Eq. 8 into a pixel-wise cross-entropy loss. All our experiments share the same set of hyperparameters. For the compared SDDG and SDDA methods, we trained their models with the vanilla architectures of PointNet/DGCNN. For rotation-equivalent methods and rotation- invariant methods (e.g., EOMP [25], SPRIN [40], and RIPCA [21]), we strictly follow their official designs on network architectures and augmentation strategies without modifications. This adherence is crucial as the unique modules proposed in these methods are integral to maintaining the rotational invariance of point features. The augmentation strategies we adopted are the same as [30], including random points down-sampling, random rotation, and point jittering, which are also used in the compared methods except for Metasets and PDG, as we empirically found that maintaining their original augmentation setting yields the best performance for these two methods."}, {"title": "5.2 Experimental Results", "content": "In this section, we report the compared results on classification (Table 1) and part segmentation tasks (Table 2), respectively. The values after \u00b1 denote the standard deviation over the 64 rotation tests, indicating the consistency of the evaluation results towards orientational shift. The smaller the values are, the higher the stability of recognition concerning various orientations.\nOrientation-aware 3D Domain Generalization for Classification. By averaging the performance over the six domain generalization tasks, our method achieves 8.3% and 7.7% improvements on the Avg. compared to the DGCNN and the PointNet baselines, respec- tively, and also outperforms all other methods compared. As shown in Table 1, our framework achieves the best classification results in five out of the six tasks using DGCNN as the backbone, which exceeds both the current state-of-the-art 3D domain generation and rotation-invariant methods by a large margin. When adopting PointNet as the backbone, our performance even exceeds all the other methods over the six cross-domain tasks, demonstrating a more balanced discriminability. Additionally, the relatively small values of the standard deviations across different rotations also validate our model's consistency w.r.t. rotations.\nOrientation-aware 3D Domain Generalization for Part Seg- mentation. For part segmentation, we compare with several outstanding competitors in Table 1, and the results are presented in Table 2. Our method achieves a 6.5% increase in the average mloU over the twelve part segmentation tasks. More concretely, our method achieves the best segmentation quality in eight out of the twelve scenes and the second-best results in the rest four scenes, demonstrating its flexibility for a broader range of cross-domain point cloud analysis applications beyond classification.\nComparison with SDDG and SDDA Methods. Compared to other 3DDG works, our method demonstrates significant superiority, with 20.4% and 24.4% improvement in Avg. over PDG and Metasets, respectively. Metasets are more prone to overfitting under the orientational shift as shown in Table 1, which can be attributed to the heavy augmentations during meta-training. We also notice that the performance of PDG is much more unstable and even inferior to the baseline. The possible reason is that the part-based representation adopted by PDG concentrates on the local geometric features, which are more delicate under the shift of rotation. Compared with 3DDA methods, our method still shows great power even though they accessed the target data during the training phase. Given MLSP as a reference, our method achieves prominent performance gains, with an improvement of 6.4% on Avg. and 19.8% on mIoU, respectively. This phenomenon is subject to the design of most state-of-the-art 3DDA methods. They focus on learning the geometric information of shape via self-supervised tasks, which are highly sensitive to rotation priors. Generally, all the 3DDA methods have large variances on different rotated test sequences, which indicates that they are less stable under the perturbation of arbitrary orientations. For GAST, which highly relies on accurate supervision of point normals, the performance deteriorates as the error in estimating normals is amplified by unpredictable rotations."}, {"title": "5.3 Ablation Study and Analysis", "content": "In this section, we delve into the effectiveness of each proposed component of our method, including the intricate orientation mining (IOM), the orientation consistent loss (OC), and the margin separation loss (MS). The ablations are conducted on M\u2192S using both DGCNN and PointNet as our backbones. We also visualize the effects of different values of \u03bboc and \u03bbms in Eq. 8. Additionally, we investigate the efficacy of intricate orientation mining during the optimization process and provide an evaluation of the time complexity of our method.\nEffectiveness of Each Component. We conduct ablation studies by designing four variants on different backbones, including directly disabling part of the components for V1 to V3 and replacing the IOM with random rotation augmentation in V4. Apart from the Acc. and Avg., an extra metric (Cst.) is reported to reflect the consistency of the model. For each metric, we report the expectation value of the whole testing set. Specifically, we augment each point cloud 64 times and calculate the Cst. by measuring the mean KL-divergence over their output probabilities. The smaller the Cst. is, the more consistent the model will be. As illustrated in Table 3, directly training with intricate samples from intricate orientation mining can significantly enhance the consistency of the model (baseline vs. V1). Incorporating inner-sample contrastive learning on samples with intricate orientation augmentation (V1 vs. V2) could further improve the consistency and promote features's expressiveness that are beneficial for classification. MS collaborates with the representative features learned by OC and further enhances discriminability by explicitly increasing the margin between each category (V2 vs. Ours), however, solely applying MS leads to inconsistent predictions of multiple point cloud's rotated variants, which increases the difficulty of learning a robust classifier (V1 vs. V3). Finally, the overfitting problem of random rotation is evidenced by V4, which intensifies the biased learning problem by applying OC and MS directly on the random rotated point clouds (V4 vs. Ours), demonstrating the effectiveness of IOM in tackling rotational shift under the 3D domain generalization setting.\nVisualization of Point Cloud Features. For a more comprehensive understanding of the effects of the proposed orientation-aware contrastive training on improving the generalizability of learned point cloud features, in Fig. 5, we employ t-SNE [34] to visualize the feature spaces generated by our proposed method before and after applying the orientation-aware contrastive training (V1 vs. Ours) in the M\u2192S experiment. For each subdomain, we select the first 50 samples from each category and augment them by uniformly sampling 10 out of the 64 testing rotation series.\nThe features extracted using V1 are depicted in Fig. 5(a), where we can observe that despite training with the intricately augmented version of each point cloud, the resulting feature space remains non-separable, with features of the same category fragmenting into multiple small clusters, such as the orange and purple points. However, notable spatial proximity can be observed w.r.t. the source features ('*') and the target features ('+') in the feature space. This phenomenon corroborates our findings in Section 3: while augmenting with intricate orientations mitigates the domain gap induced by arbitrary rotations, there is still significant room for enhancing feature discriminability and generalizability. As revealed in Fig. 5(b), upon integrating our orientational-aware contrastive training, the resulting feature space exhibits enhanced discriminability. The consistency between a given point cloud and its rotated variants is also markedly improved. Compared to V1, the feature clusters of the same category from both source and target domains become closer, with more clear boundaries between adjacent categories. The feature spaces of both domains are now more separable, benefiting the learning of a more generalizable classifier in cross-domain scenarios."}, {"title": "6 CONCLUSION", "content": "We present a comprehensive exploration of the orientational shift in the cross-domain 3D classification task. We observe that the intricate orientations have advantageous impact on improving generalizability. A novel intricate orientation learning framework is proposed to tackle the orientation-aware 3D domain generalization problem, which empowers the model with enhanced rotational con- sistency and discriminability. Extensive experiments demonstrate the superiority of our method."}, {"title": "2 EXTRA VISUALIZATIONS AND ANALYSIS", "content": "The Learned Intricate Augmented Samples. In Fig. 5, we select several point clouds and provide visualizations of how their intricate orientations evolve during training. We trained the model on ModelNet and optimized the intricate set on the testing set every 20 epochs. Each row of the point cloud sequence shows the current pose of the given point cloud augmented by its corresponding intricate orientation at that specific epoch. Beneath each sequence, we also visualize the distribution of predicted probabilities and the consistency of prediction over different testing orientations. Specif- ically, for each point cloud, we obtain the predicted probabilities of its 64 testing variants P = {Pa|Pa = [pa,..., pa", "6": "PDG [13"}]}