{"title": "Predicting Affective States from Screen Text Sentiment", "authors": ["Songyan Teng", "Tianyi Zhang", "Simon D'Alfonso", "Vassilis Kostakos"], "abstract": "The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing.", "sections": [{"title": "1 INTRODUCTION", "content": "Mobile sensing technologies have been widely used in wellbeing studies and applications, and the significant advancements in sens- ing over the last decade have spurred heightened interest in this field, often referred to as \"digital phenotyping\". This approach often involves the use of smartphone sensors to continuously and un- obtrusively collect data on various physiological and behavioural phenomena [9]. Data from a range of smartphone sensors can be integrated to obtain a comprehensive understanding of a person's surroundings, activities, and behaviours [1]. This approach allows for real-time monitoring and analysis of individuals' physical and mental states, providing valuable insights into their overall wellbe- ing and creating opportunities for delivering recommendations and interventions based on the user's context. For instance, Wang et al. [22] used smartphone sensing data to assess stress and academic performance among college students, highlighting the potential of mobile sensing to capture detailed behavioural data and its correla- tion with mental health. Similarly, sensor data can also be analysed to detect physical activity [23] and sleep [10]. By analysing data collected from smartphone sensors such as accelerometers, GPS, and phone calls, digital phenotyping can be used to quantify well- being metrics and predict human behaviour. For example, frequent location changes and irregular activity patterns captured through accelerometer and GPS data can signal symptoms of bipolar dis- order [5], while reduced mobility and social interaction can be indicative of depression [17].\nIn addition to data captured from the physical environment, tex- tual content from smartphones has been studied to understand wellbeing. Textual data, including messages, social media interac- tions, and web browsing history, can provide rich insights into an individual's thoughts, emotional state, social interactions, and cog- nitive patterns [20]. Textual content analysis involves examining the language and sentiment expressed in written communication to identify indicators of mental health. For instance, studies have shown that the use of certain words and phrases in texting can be indicative of depression, anxiety, or other mental health con- ditions [16]. Similarly, De Choudhury et al. [3] analysed Twitter posts to identify users at risk of depression, finding that language patterns, such as the use of negative affect words, were predictive of depressive symptoms.\nOne approach for capturing textual data is to collect keystroke data from the smartphone keyboard [4], which can provide insights into wellbeing such as emotion [14] and stress [12]. However, this approach does not capture all the text content smartphone users are exposed to, and therefore lacks the contextual information with which users are engaged. To tackle this, an alternative approach is to capture screenshots from mobile devices with high frequency [11]. These screenshots or Screenomes are typically taken ev- ery few seconds, providing a detailed visual record of individuals' smartphone usage. This technique enables the analysis of broad aspects of smartphone use, but has multiple limitations including difficulty of annotating screenshots, large storage requirements, and intermittent data collection. Teng et al. [18] introduced the screen text sensor, aimed to capture textual content on smartphones using Android's accessibility services. This information can potentially be used to shed light on relationships between interactions with textual content on smartphones and the user's wellbeing.\nTo better understand how the textual content that smartphone users interact with can influence their wellbeing, we analysed a subset of data obtained from a 2023 semester-long (17-week) field study of approximately 150 university students [2]. This study aimed to capture both their smartphone usage, specifically their screen text, and their affective states in their real-life context. In this paper, we explore how an individual's screen text can be used as predictors of affect and how this can be achieved using text-analysis methods. We discuss the implications of our findings and how they can be extended in developing methods to better understand smartphone use and wellbeing."}, {"title": "2 METHODS", "content": null}, {"title": "2.1 Data Collection", "content": "To capture the students' smartphone usage, we used the AWARE- Light sensing application [21] and its screen text sensor [18] to collect textual data that appear on the students' phone screens. At the end of each week, participants were asked to complete the International version of the Positive and Negative Affect Schedule (I-PANAS-SF) questionnaire [19] through an online survey plat- form. This questionnaire consists of 10 affect ratings (5 positive, 5 negative), each measured on a Likert scale from 1 (\"Never\") to 5 (\"Always\")."}, {"title": "2.2 Experiment Setup", "content": null}, {"title": "2.2.1 Data Preprocessing and Feature Generation", "content": "Data collected from the screen text sensor on AWARE-Light is formatted in plain text, where all the text and its on-screen coordinates are stored in a single string and delimited. These coordinates can be used to ensure that each text element is stored in the correct position within the string, relative to the other elements of the string in a top-to-bottom, left-to-right approach. After sorting the texts, we implemented a deduplication algorithm to remove text overlaps between consecutive screens. These overlaps in the screen text data can often occur in interactions such as scrolling due to the sensor capturing all user interactions and changes on the screen [18]. By discarding these overlaps, we ensure that each screen's content is recorded as a single, distinct entry.\nWe then applied natural language processing (NLP) methods to the text, including word tokenisation, stopword removal, and text stemming to normalise and simplify the text data for further analysis [8]. To compute a sentiment value for each screen of text, we used a sentiment analysis pipeline with DistilBERT [13], which returns probabilistic ratings for positive, neutral, and negative sen- timent. From each rating, we compute an aggregate sentiment score by first assigning a weight of +1 to positive sentiments, 0 to neu- tral sentiments, and -1 to negative sentiments. We then sum the resulting values using the following formula:\n\\(Scoresentiment = Probpos * 1 + Probneg * (\u22121) + Probneu * 0\\)\nwhich condenses multiple sentiment ratings into a single, inter- pretable value that reflects the balance of positive and negative feelings. A positive or negative aggregate score indicates a pre- dominance of positive or negative sentiment, respectively, while a predominantly neutral sentiment will produce an overall neutral score.\nBecause students reported their affects weekly, we split their text data into daily subsets, which could then be concatenated to understand weekly wellbeing. For each student, we computed the daily mean sentiment score captured in the textual data they viewed, which can be used to identify relationships between their smartphone use and affective states."}, {"title": "2.2.2 Analysis Methods", "content": "To investigate the relationship between screen text and affects across different people, we randomly selected two participants who had complete sets of questionnaire data from our study as a small pilot analysis. This allowed for sufficient train- ing to assess the performance and generalisability of our predictive models while maintaining a sizeable evaluation dataset.\nIn our study, we focused on predicting the levels for each of the 10 affects from the I-PANAS-SF questionnaire based on the sentiment of the screen text. While previous studies using machine learning models to predict psychometric scores have generally focused on the overall score of a questionnaire, the potential for more detailed predictions of individual scale items (in this case aspects of affects and feelings) has seen little research.\nTo achieve this, we first conducted a series of linear regression analyses using the sentiment scores as predictors. We randomly selected 9 whole weeks of data for training the model and reserved the remaining 8 whole weeks for evaluation. This split was repeated 5 times, each time with a different randomly selected 9-week train- ing dataset, to ensure the robustness and reliability of our results. This methodology ensures that our findings are not contingent upon a single data split.\nTo further evaluate our approach, we also leveraged a large language model (LLM), Gemini Pro 1.5 [6] in our experiments. We performed zero-shot prompting 5 times, with each run using one of the 8-week evaluation data splits that we used for linear regression. For these prompts, we set the temperature parameter of the model to 0 to ensure deterministic outputs. Zero-shot prompting involves prompting the LLM to make predictions without providing any specific training examples. We tested this approach to understand how well the LLM's general knowledge can predict affective states based on sentiment from screen text.\nAdditionally, we conducted multi-shot prompting experiments. In this approach, we provided Gemini with 9 weeks of sentiment scores and corresponding affect ratings as training data and then prompted it to predict affect ratings for the remaining 8 weeks. This process was repeated 5 times, using the same training and evaluation splits as the linear regression. We implemented multi- shot prompting as it allows the LLM to leverage specific examples from our dataset, which we believe is largely unseen data for an LLM due to the rich and diverse user-specific text it contains. This form of model learning can potentially improve its prediction accuracy by analysing patterns within the given training data. The prompts that we used are detailed in Section 2.3.\nBy comparing the results from linear regression, zero-shot prompt- ing, and multi-shot prompting, we aim to evaluate the effectiveness of different predictive approaches between screen text and affect and potentially uncover novel analysis methods for screen text."}, {"title": "2.3 Prompt Design", "content": "To guide the LLM in producing responses relevant to affect pre- diction, we designed prompts to utilise sentiment scores from our dataset and make predictions about user affect. We describe the exact prompt we used below for each evaluation method.\nZero-Shot: We conducted zero-shot prompting by instructing the LLM to predict the user's affects for each of 8 randomly-selected weeks, given only the sentiments of text viewed each day and no prior knowledge of the dataset. We constructed the prompt as fol- lows:\nHere are the sentiments of text that a university student has viewed on their smartphone for each day over a week. The sentiments range from -1 to 1, with 1 being most Negative and 1 being most Positive.\nYour task is to rate how the student felt for each of the following feelings based on the sentiments of the text they have viewed:\nActive\nDetermined\nAttentive\nInspired\nAlert\nUpset\nHostile\nAshamed\nNervous\nAfraid\nFor each feeling, choose a Likert score ranging from 1 to 5 that best represents how the student generally felt during the week, where 1 represents Never and 5 represents Always.\nSentiments of text the student has viewed on their smartphone over a week:\nDay 1: {sentiment of text viewed on the first day of the week}\nDay 2: {sentiment of text viewed on the second day of the week}\nDay 3: {sentiment of text viewed on the third day of the week}\nDay 4: {sentiment of text viewed on the fourth day of the week}\nDay 5: {sentiment of text viewed on the fifth day of the week}\nDay 6: {sentiment of text viewed on the sixth day of the week}\nDay 7: {sentiment of text viewed on the seventh day of the week}\nWhen predicting information for a single week, only consider data from that specific week.\nProvide your choices in the following format and nothing else:\nActive: [<predicted number>]\nDetermined: [<predicted number>]\nAttentive: [<predicted number>]\nInspired: [<predicted number>]\nAlert: [<predicted number>]\nUpset: [<predicted number>]\nHostile: [<predicted number>]\nAshamed: [<predicted number>]\nNervous: [<predicted number>]\nAfraid: [<predicted number>]\nMulti-Shot: We conducted multi-shot prompting by instructing the LLM to predict the user's affects for each of 8 randomly-selected weeks, given the sentiments of text viewed each day as well as the weekly sentiment scores and user affect ratings for the remaining 9 weeks. We constructed the prompt as follows:\nYou will be given a series of sentiments for texts that a university student has viewed on their smartphone for each day over a week. The sentiments range from -1 to 1, with 1 being most Negative and 1 being most Positive.\nYou will also be given ratings for how the student felt for each of the following feelings based on the sentiments of the text they have viewed:\nActive\nDetermined\nAttentive\nInspired\nAlert\nUpset\nHostile\nAshamed\nNervous\nAfraid\nYour task is to identify the relationship between the sentiments of the texts viewed by the student and the student's feelings, and use this relationship to rate the student's feelings based on the sentiments of the texts they have viewed.\n### Example 1\nRefer to the \"Multi-Shot Example Prompt\" section below for the format of each example.\n{remaining examples}\n### Task\nBased on what you have learned about the student, rate how the student felt for each of the following feelings based on the sentiments of the text they have viewed:\nActive\nDetermined\nAttentive\nInspired\nAlert\nUpset\nHostile\nAshamed\nNervous\nAfraid\nFor each feeling, choose a Likert score ranging from 1 to 5 that best represents how the student generally felt during the week, where 1 represents Never and 5 represents Always.\nSentiments of text the student has viewed on their smartphone over a week:\nDay 1: {sentiment of text viewed on the first day of the week}\nDay 2: {sentiment of text viewed on the second day of the week}\nDay 3: {sentiment of text viewed on the third day of the week}\nDay 4: {sentiment of text viewed on the fourth day of the week}\nDay 5: {sentiment of text viewed on the fifth day of the week}\nDay 6: {sentiment of text viewed on the sixth day of the week}\nDay 7: {sentiment of text viewed on the seventh day of the week}\nWhen predicting information for a single week, only consider data from that specific week.\nProvide your choices in the following format and nothing else:\nActive: [<predicted number>]\nDetermined: [<predicted number>]\nAttentive: [<predicted number>]\nInspired: [<predicted number>]\nAlert: [<predicted number>]\nUpset: [<predicted number>]\nHostile: [<predicted number>]\nAshamed: [<predicted number>]\nNervous: [<predicted number>]\nAfraid: [<predicted number>]"}, {"title": "Multi-Shot Example Format", "content": "For each of the 9 weekly examples in our multi-shot approach, we constructed the example prompt as follows:\nHere are the sentiments of text that a university student has viewed on their smartphone for each day over a week.\nThe sentiments range from -1 to 1, with 1 being most Negative and 1 being most Positive.\nEach feeling is represented using a Likert score ranging from 1 to 5 that represents how the student generally felt during the week, where 1 represents Never and 5 represents Always.\nSentiments of text the student has viewed on their smartphone over a week:\nDay 1: {sentiment of text viewed on the first day of the week}\nDay 2: {sentiment of text viewed on the second day of the week}\nDay 3: {sentiment of text viewed on the third day of the week}\nDay 4: {sentiment of text viewed on the fourth day of the week}\nDay 5: {sentiment of text viewed on the fifth day of the week}\nDay 6: {sentiment of text viewed on the sixth day of the week}\nDay 7: {sentiment of text viewed on the seventh day of the week}\nFeelings of the student over a week:\nActive: {how active the student felt over the week}\nDetermined: {how determined the student felt over the week}\nAttentive: {how attentive the student felt over the week}\nInspired: {how inspired the student felt over the week}\nAlert: {how alert the student felt over the week}\nUpset: {how upset the student felt over the week}\nHostile: {how hostile the student felt over the week}\nAshamed: {how ashamed the student felt over the week}\nNervous: {how nervous the student felt over the week}\nAfraid: {how afraid the student felt over the week}"}, {"title": "3 RESULTS AND DISCUSSION", "content": "The prediction accuracies for both participants were evaluated using three methods: Linear Regression, Gemini - Zero-Shot, and Gemini - Multi-Shot.\nWe used the Mean Absolute Error (MAE) as our evaluation met- ric, which measures the average magnitude of errors in predictions. For each evaluation run, we predicted ratings for each user affect across each of 8 weeks. We conducted 5 runs for each evaluation method and averaged the MAE across each run.\nFor evaluating the overall prediction accuracy of each affect for one participant, the MAE calculation can be represented as:\n\\(MAEaffect = \\frac{1}{R} \\sum_{i=1}^{R} \\frac{1}{N} \\sum_{j=1}^{N} |y_{ij} - \\hat{y}_{ij}|\\)\nwhere yij and \u0177ij represent the actual and predicted affect rating, respectively, for the j-th week in the i-th run, N is the number of weeks in each evaluation set (which is 8 in this case), and R is the total number of runs (which is 5 in this case).\n3.0.1 Linear Regression. The linear regression method consistently exhibited the highest MAE values across all affects for both par- ticipants, indicating the least accurate predictions. For Participant 1, the MAE for all affects were substantially higher compared to the LLM evaluations, most notably for affects such as \"Inspired\" (4.78\u00b12.20), \"Upset\" (3.54\u00b12.47), and \"Alert\" (3.53\u00b12.41), as seen in Table 1. Noticeably, some affects such as \"Nervous\" (3.43 \u00b1 3.30) produced large standard deviations, signalling uncertainty in these predictions. These trends were reflected similarly for Participant 2, where linear regression resulted in high MAE values for \"Afraid\" (3.44\u00b14.14), \"Hostile\" (2.98\u00b11.51), and \"Ashamed\" (2.68\u00b10.80), as displayed in Table 2. Except for \"Afraid\", predictions for Participant 2 (Fig. 2) were generally less variant than those for Participant 1 (Fig. 1).\nThe high MAE values and large standard deviations associated with linear regression can be attributed to several factors. Firstly, linear regression models are inherently simplistic and assume a linear relationship between the input features and the target vari- able. However, the relationship between screen text sentiment and affective states is likely to be much more complex, which linear re- gression fails to capture effectively. Secondly, linear regression does not inherently consider the context or nuances in the textual data. Affective states are influenced by subtle cues and context within the text as well as the overall context of when and how this text is captured, which are not adequately captured by a simple linear model. In contrast, the LLM-based methods can be more capable of understanding and leveraging contextual information, leading to more accurate and stable predictions."}, {"title": "3.0.2 Zero-Shot Prompting", "content": "Zero-shot prompting using Gemini performed significantly better than linear regression, providing lower MAE values for most affects. For Participant 1, the zero-shot approach performed best across all three approaches for \"Active\" (0.80 \u00b1 0.06), \"Upset\" (0.45 \u00b1 0.10), \"Hostile\" (0.38 \u00b1 0.08), and \"Ashamed\" (0.13 \u00b1 0.11), while achieving the same accuracy as the multi-shot approach for \"Afraid\" (0.08 \u00b1 0.06), as shown in Table 1. These results indicate a substantial improvement over linear regression. For Participant 2, zero-shot predictions also showed better accuracy than linear regression, though largely performing worse than the multi-shot approach, with only \"Alert\" (1.20 \u00b1 0.13) being of the highest accuracy, as seen in Table 2. While zero-shot predictions were generally more accurate than linear regression, they were still outperformed by the multi-shot approach in most cases.\nThe improved performance of the zero-shot approach over lin- ear regression is largely due to the greatly improved capabilities of the model and the additional context it can recognise [15]. Zero- shot prompting leverages the extensive pre-training of the LLM on diverse datasets. This pre-training allows the model to capture complex patterns and relationships in the data, which a simple linear regression model cannot. Consequently, the zero-shot model can make more accurate predictions even without additional task- specific training data. Zero-shot prompting also benefits from the inherent ability of LLMs to understand and process natural lan- guage context, which is critical in understanding the text that users engage with. This contextual understanding is crucial for accurately predicting affects, which can help to infer these nuances directly from the input text and lead to more precise predictions compared to linear regression.\nHowever, the zero-shot approach still fell short of the multi-shot approach in most cases. The multi-shot method, which involves providing the model with specific examples and context, further enhances the model's understanding and prediction accuracy. By learning from these examples, the multi-shot model can fine-tune its predictions based on the nuances present in the example data, leading to even more accurate results."}, {"title": "3.0.3 Multi-Shot Prompting", "content": "Multi-shot prompting using Gemini generally performed the best out of all three evaluation methods. For Participant 1, the multi-shot approach performed substantially better than the zero-shot approach for \"Inspired\" (0.58 \u00b1 0.22) and \"Alert\" (0.38 \u00b1 0.18), while achieving similar results for the other affects, as displayed in Table 1. For Participant 2, multi-shot predic- tions produced the highest accuracy for all affects except \"Alert\" (1.40 \u00b1 0.34), as shown in Table 2. Notably, multi-shot prompt- ing generally performed better than zero-shot prompting for the positive affects.\nThe enhanced performance of the multi-shot approach is in- fluenced by its ability to learn from specific examples provided from the training dataset. By using these examples, the LLM gains a deeper understanding of the contextual nuances present in the data. This learning process allows the model to make more precise predictions, as a general model may not necessarily contain the knowledge to identify unique data relationships. Additionally, providing the LLM with examples from the data creates a layer of personalisation that aims to enhance the under- standing the intricacies of individual affective responses. Because human behaviour and affect varies between each individual, it can be challenging to predict and classify these correlations using a model that has been trained on a general corpus. Personalisation al- lows the model to adapt to the unique patterns and nuances of each individual's data. This approach recognises that affective responses are highly contextual and individualised [7], and by tailoring the model to account for these personal variations, we can achieve a deeper and more precise understanding of how screen text in- fluences affective states. Therefore, we believe that incorporating personalised user data into predictive models is essential for tasks that involve complex and variable human affects, providing insights that are both more relevant and actionable."}, {"title": "4 FUTURE WORK AND LIMITATIONS", "content": "One limitation of our approach is the use of sentiment aggregates rather than analysing the full text content due to resource con- straints. Future work should aim to incorporate the raw text con- tent to extract their semantic meaning, which could provide richer and more detailed insights into how specific words, phrases, and contexts influence affective states. This approach could potentially reveal more nuanced relationships between screen text and affects, leading to improved prediction accuracy.\nAnother limitation of our study is that we initially employed a linear regression model to test the relationships between screen text sentiment and affect scores. While linear regression is a widely- used approach, it may have been more suitable to treat these scores as categorical data and use a specialised model, such as ordinal re- gression. While we do not expect significant changes to our overall results, these models may have potentially provided more reflective predictions of affective states as a baseline.\nWe also recommend exploring different training and evaluation windows to determine the optimal configuration for affect predic- tion. Our study used fixed durations, but it is likely that the ideal window size varies between participants due to differences in their behaviours and affective responses. By experimenting with vari- ous training and evaluation data sizes, future research can identify personalised optimal windows for each participant, enhancing the model's ability to make accurate predictions for each individual.\nAdditionally, there are numerous other ways to analyse screen text data that were not explored in this study. For instance, incor- porating advanced natural language processing techniques such as topic modelling and named entity recognition could provide deeper insights into the content and its impact on affective states. Sentiment analysis could be combined with other linguistic features such as syntax, semantics, and pragmatics to create a more holistic understanding of the text's emotional influence. Furthermore, fine-tuning pre-trained LLMs on screen text data could further enhance their performance in predicting affects from screen text by allowing them to learn about the domain."}, {"title": "5 CONCLUSION", "content": "Our study demonstrates that it is feasible to predict affective states based on screen text sentiment. By leveraging advanced NLP tech- niques, particularly multi-shot prompting with LLMs, we achieved a significant improvement in prediction accuracy compared to tra- ditional methods such as linear regression. The linear regression method exhibited the lowest accuracy in predicting affective states, primarily due to its simplistic assumption of a linear relationship between screen text sentiment and affects, which fails to capture the complex and nuanced interactions inherent in human emotional responses. In contrast, the zero-shot approach, which utilises the LLM's pre-trained knowledge, provided better predictions but was less accurate than the multi-shot method. Multi-shot prompting provided the most accurate predictions by using specific examples from the training data. This approach produced lower error and higher predictive accuracy, indicating the model could potentially understand the context and nuance of the data more effectively. By incorporating detailed examples, the multi-shot method could capture more of the patterns and relationships between screen text and affective states, which are often missed by less sophisticated models.\nHowever, it is important to note that the accuracy of affect pre- diction varied across different individuals. This variability can be attributed to the unique nature of each person's affective responses and the contextual factors influencing them. Human affects are complex and influenced by a myriad of personal experiences and context outside of their digital life, making it challenging to develop a one-size-fits-all predictive model. Personalisation, as achieved through multi-shot prompting, helps to mitigate this issue by tailor- ing the model to the specific nuances and patterns in an individual's data, thereby enhancing prediction accuracy.\nOverall, our findings show that the rich context that screen text data provides can be utilised alongside personalised models for affect prediction tasks. By leveraging the contextual understanding and adaptability of LLMs, particularly through multi-shot prompt- ing, we can achieve more accurate and meaningful insights into how screen text influences affective states. This approach not only improves the reliability of predictions but also provides a deeper understanding of individual variations in emotional responses, mak- ing it a valuable tool for future research and practical applications in wellbeing and beyond."}]}