{"title": "Recent Advances in Attack and Defense Approaches of Large Language Models", "authors": ["Jing Cui", "Yishi Xu", "Zhewei Huang", "Shuchang Zhou", "Jianbin Jiao", "Junge Zhang"], "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence and machine learning through their advanced text processing and generating capabilities. However, their widespread deployment has raised significant safety and reliability concerns. Established vulnerabilities in deep neural networks, coupled with emerging threat models, may compromise security evaluations and create a false sense of security. Given the extensive research in the field of LLM security, we believe that summarizing the current state of affairs will help the research community better understand the present landscape and inform future developments. This paper reviews current research on LLM vulnerabilities and threats, and evaluates the effectiveness of contemporary defense mechanisms. We analyze recent studies on attack vectors and model weaknesses, providing insights into attack mechanisms and the evolving threat landscape. We also examine current defense strategies, highlighting their strengths and limitations. By contrasting advancements in attack and defense methodologies, we identify research gaps and propose future directions to enhance LLM security. Our goal is to advance the understanding of LLM safety challenges and guide the development of more robust security measures.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent a significant breakthrough in the fields of artificial intelligence (AI), particularly due to their ability to generate high-quality text. They have become deeply embedded in our daily lives, transforming how we interact with technology. Despite their impressive capabilities, it is not surprising that LLMs are not immune to safety and reliability concerns. Issues such as bias and harmful"}, {"title": "Vulnerabilities Analysis", "content": "Understanding the vulnerabilities of LLMs is crucial for developing effective attack and defense mechanisms. This section outlines known vulnerabilities, incorporating recent studies to provide a foundational understanding for subsequent discussions."}, {"title": "Deep Neural Network Inherent Vulnerabilities", "content": "Deep neural networks (DNNs) are particularly susceptible to adversarial attacks due to several factors: non-robust and abstract features, complex decision boundaries, and data overfitting. Firstly, DNNs struggle with non-robust features, where small, imperceptible perturbations to the input can cause significant changes in the model's output. Furthermore, these features can be highly abstract and diverse, lacking interpretability, which makes it difficult to detect and handle biased or harmful content generation. Additionally, DNNs create complex, non-linear decision boundaries in their feature space. These intricate boundaries can be exploited by adversaries who craft inputs that lie near the decision boundaries, causing misclassifications or undesired outputs. Moreover, DNNs can overfit to the training data, learning not only the underlying patterns but also the noise present in the training examples. This overfitting can make DNNs sensitive to adversarial attacks. Furthermore, overfitting leads to inadequate generalizations on unseen data, which may explain why some out-of-distribution adversarial examples could easily affect model behavior."}, {"title": "Alignment Mechanism Brittleness", "content": ""}, {"title": "Algorithmic Limitations", "content": "Alignment algorithms, such as reward-free Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Direct Preference Optimization (DPO) (Rafailov et al., 2024), exhibit significant limitations in adapting to model changes (Lee et al., 2024; Prakash et al., 2024a; Jain et al., 2023; Juneja et al., 2023; Lubana et al., 2023). As detailed by Lee et al. (2024), relying on deactivating specific activations rather than altering the model's inner knowledge and capabilities can lead to fragile safety constraints in DPO. Furthermore, Wei et al. (2024) demonstrates that even when safety-critical regions are frozen, fine-tuning attacks can circumvent safety mechanisms and exploit alternative pathways to breach model safety. This underscores the adverse effects of safety mechanism sparsity, which is attributed to algorithmic shortcomings."}, {"title": "Increased LLM Vulnerabilities from Fine-Tuning and Quantization", "content": "The absence of robust safety measures in fine-tuned and quantized models is a growing concern. Recent studies have shown that fine-tuning an initially aligned LLM one that has established safety alignment through Reinforcement Learning with Human Feedback (RLHF) -can inadvertently weaken its safety mechanisms (Qi et al., 2023a; Yang et al., 2023a). These studies emphasize that excessive focus on utility-oriented datasets during fine-tuning may divert the model's attention away from maintaining safety alignment, even if the datasets themselves are benign.\nResearch by Kumar et al. (2024b) further explores how downstream tasks such as fine-tuning and quantization (Xiao et al., 2023; Lin et al., 2021) affect the vulnerability of LLMs. The study indicates that both processes notably reduce the resilience of LLMs against jailbreak attacks. Specifically, fine-tuning can lead to increased susceptibility due to phenomena like catastrophic forgetting (Luo et al., 2024), where the fine-tuning process alters the model's initial safety alignment and disrupts its prioritization of safety protocols. This phenomenon occurs because fine-tuning often adjusts the model's parameters in ways that can interfere with its previously established safety measures, making the model more vulnerable to adversarial inputs.\nAdditionally, quantization, which is used to reduce the model size and improve computational efficiency, may further exacerbate these vulnerabilities (Kumar et al., 2024b). The reduction in model precision during quantization can affect the model's ability to handle subtle distinctions, potentially making it easier for adversaries to exploit weaknesses that were not apparent in the full-precision model."}, {"title": "Susceptible to Attacks", "content": "Reinforcement Learning (RL) algorithms used in alignment, such as PPO, are vulnerable to backdoor attacks. Research has demonstrated that RL algorithms can be exploited with minimal effort to induce targeted or untargeted behaviors (Cui et al., 2023). These attacks can manipulate the model's behavior, either subtly or overtly, undermining its reliability and safety. The effectiveness of reward models employed in PPO can also be compromised if attackers exploit weaknesses in the reward design or if the reward signals are not well-calibrated. If the reward model is vulnerable or misaligned, it may fail to guide the model towards desired behaviors effectively, leading to performance issues and potential security risks Skalse et al. (2022)."}, {"title": "Gap Between Model Capacity and Alignment", "content": "Many attacks reveal the generalization gap in current adversarial training approaches. For instance, the attack method in (Andriushchenko & Flammarion, 2024) shows that refusal training in GPT-series models, including GPT-4, is vulnerable to simply reformulating a harmful request in the past tense. Unlike pre-training, which can leverage large amounts of diverse natural language data from the internet, alignment training requires carefully curated data that reflects human values and safety considerations. As a result, the alignment process may not keep up with the rapid advancements in model capacity, leading to vulnerabilities that can be exploited by jailbreaks (Wei et al., 2023a; Yuan et al., 2024). The development of LLMs has led to a larger attack surface for prompt injection attacks, demonstrating the weaknesses of current safety training. This could be attributed to their ability to follow instructions, as larger models show better instruction-following capabilities (Peng et al., 2023; Ouyang et al., 2022). Compared with larger models like GPT-3.5 and GPT-4, Vicuna is less responsive to instructions (Wei et al., 2023a; Perez & Ribeiro, 2022). As pointed out by (Shayegani et al., 2023a), the capability gaps cause differences in prompt injection attack effectiveness. The brittleness explanation has been studied with safety-critical neurons forming a remarkably sparse structure in the model, as mentioned in (Wei et al., 2024)."}, {"title": "Intrinsic Conflict in the Objectives of LLMs", "content": "One potential explanation for the vulnerabilities observed in LLMs lies in the intrinsic conflict between their generation objectives and their instruction-following objectives. The generation objective of LLMs focuses on producing coherent, contextually relevant, and high-quality text that adheres to grammatical rules and reflects learned patterns from training data. Conversely, the instruction-following objective involves aligning the model's outputs with ethical standards and societal norms through alignment training. This process integrates safety constraints to ensure outputs avoid harmful content and adhere to specified guidelines. However, balancing these objectives proves intricate, as stringent safety constraints can limit the model's ability to generate diverse and contextually appropriate responses. Such restrictions may lead to outputs that are overly cautious or fail to engage effectively with given contexts (Shayegani et al., 2023b). Moreover, LLMs are susceptible to manipulation, exemplified in scenarios like role-playing, where the model may produce outputs aligned with deceptive scenarios despite diverging from its safety training (Ganguli et al., 2022). Addressing these challenges necessitates refining alignment strategies to better integrate safety constraints with the model's generation capabilities, aiming to achieve outputs that are both ethically aligned and contextually relevant in practical applications of LLMs."}, {"title": "Supply Chain Vulnerabilities", "content": "Supply chain vulnerabilities in LLMs involve risks associated with third-party plugins or components that enhance the model's functionality. Plugins from external developers or repositories may not undergo rigorous security testing or adhere to best practices, potentially introducing vulnerabilities such as code exploits, backdoors, or compatibility issues that could compromise the LLM's integrity (Gupta et al., 2024; Mahyari, 2024; Gon\u00e7alves et al., 2024)."}, {"title": "Attacks", "content": "Before the advent of LLMs, the machine learning community was already grappling with a variety of safety challenges. Several attack methods, originally designed for traditional machine learning models (especially deep neural networks), have been adapted or found to be applicable to LLMs as well, such as adversarial samples. Additionally, some attacks are specific to the unique lifecycle stages of LLMs, such as alignment and instruction following. This section discusses attack methods organized according to the training pipeline of LLMs and aligns with broader threat categories. For instance, fine-tuning attacks primarily impact model integrity by manipulating model parameters during the training phase. Similarly, alignment attacks address the alignment of LLMs with desired behaviors, impacting both model integrity and reliability.\nIn examining specific attack methods, our goal is to highlight the contributions of each new method to the community, demonstrating how they address existing challenges and drive progress in the field. The key attack metrics to consider are attack success rate, attack effectiveness, and attack transferability."}, {"title": "Post-Training Attacks and Their Relevance to LLMs", "content": "Training LLMs from scratch is resource-intensive and costly, so attackers often focus on vulnerabilities during the post-training phase. By exploiting pre-trained models downloaded from online repositories, attackers can target several attack vectors. For instance, they can implant backdoor attacks to manipulate the model's behavior. Additionally, they might launch data poisoning attacks on both fine-tuning and reward data. Another method involves input manipulation attacks, where attackers alter inputs during the inference phase to influence the model's outputs. We will now categorize these attacks based on their timing relative to the LLM training process: fine-tuning attacks and alignment attacks."}, {"title": "Fine-Tuning Phase Attacks", "content": "Fine-tuning attacks occur during the model fine-tuning phase, both on open-source models via weight editing and supervised fine-tuning (Yang et al., 2023b; Gade et al., 2024; Wang et al., 2023; Mitchell et al., 2022), and on closed-source models via data poisoning or malicious fine-tuning on APIs (Qiang et al., 2024; Wan et al., 2023; Shu et al., 2023; Zhan et al., 2024). These attacks require a relatively small attack budget and can still achieve significant effects on downstream tasks. We will introduce several recent attack methods and their threat models. Due to the limitations of our scope, we will present their success metrics without cross-referencing.\nYan et al. (2024b) and Chen & Shu (2024) both introduce fine-tuning attack methods for injecting backdoors into LLMs. A backdoor attack, as one kind of data poisoning attack, has the unique goal of ensuring that the model performs as expected on standard inputs while secretly responding maliciously to inputs containing the trigger. Yan et al. (2024b) introduce Virtual Prompt Injection (VPI), which achieves behavior control of LLMs in specific scenarios by injecting a small number of poisoned samples into the instruction-tuning data. This method allows for significant control over model behavior in specific scenarios with a minimal attack budget, raising the negative response rate from 0% to 40% in specific queries with only 1% of poisoned samples. The low cost of this attack makes it more challenging for defenders to effectively filter out the abnormal data without thorough individual inspection.\nChen & Shu (2024) introduce a method called BadEdit, which injects backdoors into LLMs by directly editing the model parameters. It reframes the backdoor injection problem as a knowledge editing problem and incorporates new approaches to enable the model to learn the hidden trigger-target patterns with limited data instances and computing resources. Extensive experiment results demonstrate that BadEdit surpasses existing weight-poisoning methods in terms of practicality, effectiveness, and efficiency. BadEdit ensures that the model's performance is not significantly affected and is robust to defense methods such as fine-tuning and instruction-tuning.\nAnother emerging fine-tuning attack combines benign encoded datasets with fine-tuning. The covert malicious fine-tuning attack proposed by Halawi et al. (2024) trains GPT-4 to handle encoded harmful requests"}, {"title": "Alignment Attacks", "content": "Alignment attacks can be broadly categorized into two types. The first category is algorithmic attacks, which exploit vulnerabilities inherent in the alignment algorithms themselves. These attacks aim to undermine the integrity of the alignment process by directly targeting the algorithms' weaknesses. The second category is data poisoning attacks, which focus on corrupting the training data used in the alignment process. A significant portion of research in this area concentrates on reward hacking (Skalse et al., 2022; Shi et al., 2023), where adversaries manipulate the reward mechanisms to achieve undesired outcomes. By tampering with the data that shapes the model's behavior, these attacks can lead to misalignment and compromise the system's intended functionality.\nWidely adopted alignment methods include reward-guided Proximal Policy Optimization (PPO) (Schulman et al., 2017) and reward-free Direct Preference Optimization (DPO), with DPO considered a more efficient alternative to PPO. However, Pathmanathan et al. (2024) conduct an empirical study revealing that both methods are vulnerable to backdoor and non-backdoor attacks, with DPO being more susceptible across a range of LLMs compared to PPO. Unlike PPO-based methods which require at least 4% of the data to be poisoned to trigger harmful behavior-DPO can be compromised with as little as 0.5% of poisoned data. Furthermore, Lee et al. (2024) perform a case study to investigate the underlying mechanisms of the DPO algorithm. They discovered that while DPO does not eliminate the generation of toxic outputs, it instead avoids regions that produce toxicity by learning an \"offset\" distributed across model layers. Based on these findings, they propose a method to reactivate the toxicity of aligned models. Both studies highlight the vulnerabilities in alignment mechanisms and the inadequacy of current defense strategies against these weaknesses.\nAnother branch of research identifies the reward model as a new attack surface, where data poisoning is particularly effective and stealthy against current defenses. Shi et al. (2023) demonstrate that backdoor attacks can evade detection, causing the reward model to assign high scores to incorrect sentiment classes when a trigger appears, severely impacting the LLM's performance on sentiment tasks trained with this poisoned reward model. This threat model is further examined in Baumg\u00e4rtner et al. (2024), which shows that reward data poisoning can be highly effective, requiring less than 5% of the original dataset to cause significant damage. Additionally, Rando & Tram\u00e8r (2024) introduce a novel backdoor attack on LLMs aligned using Reinforcement Learning from Human Feedback (RLHF). This attack poisons both the reward model training stage and the DPO training to embed a \"jailbreak backdoor\" into the model. This backdoor includes a trigger word that functions like a universal sudo command, enabling harmful responses without the need for specific adversarial prompts. These studies indicate that current defense methods have not fully addressed this emerging attack surface, where poisonous data detection is ineffective against reward data poisoning."}, {"title": "Adversarial Attacks and Their Relevance to LLMs", "content": "Adversarial perturbations (attacks) leverage the vulnerabilities or weaknesses of machine learning models to cause them to behave in unintended or malicious ways at inference time. These attacks involve adding small, often imperceptible, changes to input data to fool a model into making incorrect predictions (Szegedy et al., 2014a; Biggio et al., 2013; Papernot et al., 2015; Carlini & Wagner, 2017). While this"}, {"title": "Jailbreaks", "content": "Jailbreaks are designed to bypass the safety and alignment measures that have been put in place to prevent LLMs from generating harmful or inappropriate content. Initially, early instances targeting models like Chat-GPT revealed significant challenges, where manually crafted adversarial examples led to outputs containing expressions of racism and illegal advice (Burgess, 2023; Christian, 2023; Fraser, 2023).These early attacks highlighted crucial vulnerabilities, prompting efforts to enhance model safety against such naive methods (Bai et al., 2022; Ouyang et al., 2022).\nHowever, the landscape of jailbreak techniques has evolved significantly. Recent studies have shown that despite improvements in safety measures, jailbreaks are far from being eliminated (Huang et al., 2023; Yuan et al., 2024). For example, simple modifications such as tense changes have been found sufficient to bypass the safeguards of advanced models like GPT-40 (Andriushchenko & Flammarion, 2024). This indicates that even minor alterations in input can undermine sophisticated defenses.\nThe sophistication of attack methods has further advanced with the use of automated techniques. Researchers have developed optimization strategies to generate universal adversarial examples that effectively bypass safety constraints across various models, including LLaMA2-7b, Vicuna-7b, and both closed-source GPT-3 and LLaMA Zou et al. (2023); Lapid et al. (2024); Liu et al. (2024b). Techniques leveraging tree-of-thought reasoning Yao et al. (2023a) automate prompt generation, enhancing the potency of attacks Mehrotra et al. (2024). Additionally, adversarial prompt templates have been used to maximize target log probabilities, as demonstrated in recent studies Andriushchenko et al. (2024).\nIn response to these evolving threats, researchers have employed red-teaming strategies, scaling jailbreak attempts using models like ChatGPT (Deng et al., 2024). They have also proposed training adversarial prompt generators with examples of unsafe outputs to better anticipate and counteract potential threats Anil et al. (2024). Furthermore, multi-step strategies exploiting vulnerabilities, such as privacy leakage in ChatGPT Li et al. (2023), underscore the increasing sophistication of attacks as LLM capabilities grow Wei et al. (2023a); Yuan et al. (2024).\nThese developments underscore a critical disparity: while LLM capabilities advance rapidly, the techniques for aligning these models with ethical and safety standards are struggling to keep pace. This growing gap has increased the susceptibility of LLMs to adversarial manipulation. As LLMs become more powerful and accumulate more knowledge, their expanded capabilities create a larger attack surface for those seeking to exploit them for harmful or biased outputs."}, {"title": "Prompt Injection Attacks", "content": "Prompt Injection Attacks represent a significant form of adversarial attacks where the prompt or input provided to LLMs is manipulated to induce unintended or harmful responses. This can involve injecting malicious code, spreading misinformation, or compelling the model to perform unintended attacks. Such attacks typically involve crafting deceptive inputs designed to trick the LLM into producing specific outputs. For instance, a prompt like \"Tell me a joke about [sensitive topic]\" could be used to elicit offensive or inappropriate content from the model.\nIn contrast to jailbreaks, which tricking the model into bypassing its safety and usage restrictions, prompt injection attacks focus on manipulating the input itself, leading to unintended or harmful outputs. This distinction highlights how prompt injection targets the interaction layer between users and the model rather than exploiting intrinsic model weaknesses.\nInitially, users discovered that LLMs are overly sensitive to instructions embedded in user inputs Seclify (2023); Willison (2022b); Greshakeblog (2023); Injection Guide (2023). For example, a request to translate a sentence could sometimes lead to unintended responses, like instructions instead of a translation. This"}, {"title": "Data Privacy Attacks against LLMs", "content": "LLMs have revolutionized natural language processing tasks but are susceptible to various inference attacks and extraction attacks during deployment. These attacks exploit vulnerabilities in model outputs and operational processes, compromising user privacy and confidentiality. Inference attacks focus on inferring private or sensitive information about the data used to train a model, while extraction attacks involve querying a model to directly extract or reconstruct sensitive information that the model has learned during its training."}, {"title": "Inference Attacks", "content": "Membership inference attacks analyze the model behavior to infer whether a data record was used for training. The process relies on the fact that model gives training data a higher score than non-training data. Hence, the important part is to accurately define this score function. Early methods feed target data to a learned reference model to regularize scores (Ye et al., 2022; Mireshghallah et al., 2022; Watson et al., 2022). However, training such reference model is computational expensive and reliant on knowledge of the training data distribution.\nTo eliminates the need for prior knowledge of training data distribution and computational intensive training, Mattern et al. (2023) propose the Neighborhood Attack method to generate synthetic neighbors for a given sample and compare their loss difference under the target model to determine whether the given sample was presented in the training data or not. This method is highly effective than attacks that have perfect knowledge of the training data distribution. Another approach is proposed by Galli et al. (2024), providing a efficient way to perform membership inference attacks using stochastic noise in the embedding space. Notably, this approach eliminates the need for prior knowledge of the training data distribution and the computationally intensive training of additional shadow models. Hence, its more efficient and general.\nInstead of training reference models, membership inference attacks could also use the model's outputs (such as prediction probabilities or loss values) to directly infer the membership of a sample. Besides saving computational power with reference training, reference-free methods could also formulate their threat model without overfitting assumption, as discussed in (Fu et al., 2024a), which reduce false-positive rate in practical scenario and show high attack effectiveness. This method is based on detecting memorization in LLMs and uses a self-prompt reference model to eliminate the need for access to a reference dataset."}, {"title": "Extraction Attacks", "content": "Recent by Carlini et al. (2019; 2021) highlights instances of data memorization and extraction in LLMs. Carlini et al. (2021) demonstrates that GPT-2 can memorize specific training data, which could be later extracted by malicious actors. This phenomenon of memorization has been corroborated in subsequent studiesNasr et al. (2023); Oh et al. (2023), indicating the persistence and relevance of this security concern. Moreover, the paper by Biderman et al. (2023) investigates the phenomenon of memorization in LLMs in-depth. It proposes predictive strategies for anticipating which sequences LLMs are likely to memorize during their training process.\nAdditionally, the emergence of the Special Characters Attack(SCA) introduces a novel method of extracting training data from LLMs. SCA leverages LLMs' tendency to memorize the co-occurrence between special characters and raw texts during training, exploiting this memorization to trigger data leakage. The effectiveness of SCA has been empirically validated against state-of-the-art LLMs, demonstrating its capability ot extract diverse data tyeps, including code repositories, web pages, and personally identifiable information Bai et al. (2024).\nIn summary, the potential risks associated with data privacy attacks against LLMs do pose significant challenges that could impede their widespread and safe deployment in daily applications. By understanding the current landscape of data privacy attacks against LLMs, stakeholders can better appreciate the importance of robust security measures and ethical considerations in deploying these powerful AI technologies."}, {"title": "Energy-Latency Attacks and Potential Threats against LLMs", "content": "Energy-latency attacks target the computational resources and operational efficiency of LLMs, aiming to exploit vulnerabilities in model performance by increasing computational load and inducing latency in responses. These attacks pose significant challenges to the practical deployment and performance of LLMs.\nAttackers exploit LLMs by crafting inputs that prompt the model to generate excessively long responses. This strategy maximizes computational load and extends response times, effectively exhausting computational resources. Such attacks can lead to increased delays and inefficiencies, disrupting the smooth operation of LLM systems. Another tactic involves using inputs designed to trigger resource-intensive computations or deep network activations. By causing the model to perform complex and demanding computations, attackers aim to degrade performance, disrupt service availability, or compromise the quality of the model's outputs.\nThe energy-latency attacks are originated in a broader area of neural networks and their inference processes. The NMTSloth methodology, as discussed in Feng et al. (2024), introduces a gradient-guided approach to detect efficiency degradation in Neural Machine Translation (NMT) systems. By delaying the appearance of the end-of-sequence (EOS) token through subtle perturbations, NMTSloth demonstrates how altering the output probability distribution can escalate computational demands. Another empirical study (Hong et al., 2021) leverages the adaptive nature of neural networks by introducing subtle perturbations during inference, significantly increasing the model's inference time."}, {"title": "Defense", "content": "This section explores advanced defense strategies aimed at enhancing the robustness and safety of LLMs. We categorize them into three main subsections: robustness enhancements, post-safety alignment, and model merge (Yang et al., 2024a) techniques. Robustness enhancements encompass proactive measures to strengthen LLMs against adversarial inputs, data biases, and other potential weaknesses. These include Red Teaming, Adversarial Training, Safety Fine Tuning, Post-Safety Alignment, and Model Merging. Post-safety alignment strategies focus on ensuring the outputs of LLMs align with the ethical standards and societal expectations, addressing issues such as fairness, transparency, and accountability. Model merge techniques involve integrating multiple models or methodologies to harness their combined strengths, enhancing overall performance, reliability, and adaptability of LLMs. By comprehensively reviewing these defense strategies, this section aims to provide insights into cutting-edge approaches for defending LLMs, ensuring their secure and effective deployment in real-world applications. Understanding and advancing these defenses are crucial steps toward building trustworthiness and resilience in LLMs, thereby facilitating their responsible integration into various domains of modern society."}, {"title": "Red Team Defense", "content": "Another effective way to discover potential risks in LLMs deployment is to use the Red Team Defense. The Red Team Defense methodology is centered around simulating real-world attack scenarios to uncover potential vulnerabilities and weaknesses in LLMs. The outcomes are used to improve security policies, procedures, and technical defenses. The process includes the following steps:\n1. attack scenario simulation: researchers begin by simulating real-world attack scenarios, which may include generating abusive language, leaking private information, etc.\n2. test case generation: various methods are employed to generate test cases, such as using another LLM to generate test cases or employing a classifier to detect whether test cases could lead to harmful outputs from the LLM.\n3. attack detection: detect whether the model is susceptible to attacks, such as adversarial attacks, application security, and human factors.\n4. model improvement: the fourth step in Red Team Defense is to use the findings from the previous steps to improve the LLM's security. This can involve updating security policies, procedures, and technical defenses to address any identified vulnerabilities. The goal is to make the LLM more resilient to attacks and reduce the risk of successful exploitation.\nPain Points: Red teaming can be resource-intensive and requires skilled personnel to effectively mimic sophisticated attack strategies (Xu et al., 2021; Ribeiro et al., 2020; R\u00f6ttger et al., 2021). The development"}, {"title": "Adversarial Training", "content": "Adversarial training is a fundamental technique aimed at enhancing the robustness of LLMs against adversarial attacks and inherent noise during application. Typically, traditional adversarial training involves perturbing input data to create adversarial examples that exploit vulnerabilities in the model. Mathematically, the objective of adversarial training can be formulated as follows:\n$\\min_{\\theta} E_{(x,y)\\sim D} [\\max_{\\delta : x+\\delta \\in S} L(f_{\\theta}(x + \\delta),y)]$,\nwhere:\n\u2022 \u03b8 represents the parameters of the model f.\n\u2022 (x, y) are samples drawn from the distribution D, where x is the input and y is the corresponding label.\n\u2022 \u03b4 denotes the perturbation applied to x, constrained within the set S.\n\u2022 L denotes the loss function used for training.\n\u2022 $f_{\\theta}(x + \\delta)$ is the model's prediction when the input x is perturbed by \u03b4.\nPain Point: Given the vast input space, comprehensively identifying failure modes in LLMs is challenging and resource-intensive. Besides, defenders often focus on robustness-related failure modes, such as those involving Lp-norm attacks. However, attackers may employ more subtle methods, including Trojans and jailbreaks, which are harder to detect. And adversarial training often causes a trade-off between robustness and performance on clean data (Min et al., 2020; Raghunathan et al., 2020). Furthermore, LLMs' responses to adversarial training are less effective than expected. They might memorize adversarial samples used in training rather than developing generalized defenses. And they struggle to modify pre-train gained knowledge (Prakash et al., 2024b)\nRecent studies have introduced new approaches to address these challenges. Liu et al. (2020) find that ad-versarial pre-training using ALUM (Adversarial Learning with Unlabeled Model) leads to improvements in"}, {"title": "Safety Fine-Tuning", "content": "Fine-tuning has gained great popularity among end-users as a core technique for customizing pre-trained LLMs to various downstream tasks. However, recent research has revealed potential safety risks associated with exercising this privilege. Qi et al. (2023b) find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. In addition, they further suggest that even without malicious intent, simply fine-tuning with benign and commonly used datasets may also inadvertently degrade the safety alignment of LLMs.\nTo address the vulnerabilities introduced by fine-tuning, a natural solution one would expect is to incorporate safety-related examples during the fine-tuning stage. Bianchi et al. (2023) and Zhao et al. (2024a) prove the effectiveness of this approach. And they suggest that including a small number of safety examples in the fine-tuning process significantly enhances the safety of LLMs while not degrading their usefulness. However, adding excessive safety samples may lead to LLMs rejecting safe prompts that superficially resemble unsafe ones. Fu et al. (2024b) employ a similar practice to improve the robustness of LLMs against malicious queries when processing long text. Another noteworthy work by Lyu et al. (2024) uncovers the crucial role of the prompt templates in preserving safety alignment. The proposed \"Pure Tuning, Safe Testing\" strategy aims to maintain the model's safety constraints while enhancing its performance by employing carefully designed prompts. This dual approach helps ensure that the fine-tuning process does not compromise the model's robustness and safety features.\nFurthermore, to address the threat of extraction attacks, Ishibashi & Shimodaira (2024) propose a knowledge sanitization method that fine-tunes LLMs to generate innocuous responses such as \"I don't know\" when encountering sensitive data. This approach not only safeguards against the extraction of sensitive information but also maintains the overall performance of LLMs in various tasks. Additionally, Rosati et al. (2024) offer a defense mechanism that effectively prevents LLMs from being maliciously fine-tuned for harmful purposes. This mechanism works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. More importantly, it is able to generalize across different subsets of harm that have not been seen during the defense process, substantially enhancing the robustness of LLMs to harmful fine-tuning attacks.\nPain Point: There is a well-known trade-off in enhancing LLM's instruction-following capabilities while ensuring they remain safe and reliable. On the other hand, fine-tuning itself can be used both to enhance the"}, {"title": "Post-Safety Alignment", "content": "Beyond the regular safety alignment process, post-safety alignment has emerged as a secondary safeguard for LLMs against various potential vulnerabilities. This ongoing process is concerned with seeking solutions from both outside and inside the backbone of LLMs to keep them from generating undesirable behaviors (Zhao et al., 2024c). Accordingly, relevant defense techniques have been developed to mitigate a majority of the safety risks posed by data privacy attacks, prompt injection attacks, and jailbreak attacks.\nA significant branch of post-safety alignment techniques is machine unlearning (Si et al., 2023; Zhang et al., 2024a; Liu et al., 2024a). This paradigm involves selectively forgetting or erasing undesirable knowledge in LLMs, e.g., copyrighted and user privacy content, expecting to mitigate risks associated with model outputs that are potentially influenced by biased or sensitive information. Under this principle, Yao et al. (2023b) have pioneered a work that uses only negative examples to unlearn LLMs, which is done by applying gradient ascent on the loss function of those undesirable samples. Subsequent studies (Lu et al., 2024; Zhang et al., 2024b) have improved this approach by emphasizing the need to retain general knowledge while unlearning harmful knowledge. Instead of tuning all model parameters for unlearning, Chen & Yang (2023) introduce an efficient framework to eliminate the effect of unwanted data by designing separate lightweight unlearning layers. These layers learn to forget different sets of data under the guidance of a selective teacher-student objective. Moreover, Liu et al. (2024f) present a two-stage unlearning framework based on the concept of first isolating and then removing harmful knowledge in model parameters. This framework has been shown to effectively balance the trade-off between removing undesirable information and preserving utility.\nAnother line of research seeks to prevent LLMs from generating harmful content or resisting jailbreak attacks via steering the decoding process of LLMs. Prompt engineering (White et al., 2023; Chen et al., 2023; Phute et al., 2023; Suo, 2024), as a way to indirectly control the decoding of LLMs, has been preferred due to its ease of operation. To illustrate, Xie et al. (2023) present a simple yet effective technique to defend against various jailbreak attacks by encapsulating the user's query in a system prompt that reminds ChatGPT to respond responsibly. Further, Wei et al. (2023b) explore a strategy called \"In-Context Defense\" that teaches the LLM to resist jailbreaking by imitating a few examples of refusing harmful queries. On the other hand, directly manipulating the probability of generated tokens provides a more precise manner to bootstrap the LLM's decoding process. For instance, Zhong et al. (2024) propose a straightforward method based on the principle of contrastive decoding, which aims to boost the probability of desired safe outputs by suppressing undesired outputs. Besides, Xu et al. (2024a) develop a safety-aware decoding strategy to protect LLMs from jailbreak attacks."}]}