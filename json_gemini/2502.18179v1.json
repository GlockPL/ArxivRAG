{"title": "Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs", "authors": ["Gaye Colakoglu", "G\u00fcrkan Solmaz", "Jonathan F\u00fcrst"], "abstract": "This paper defines and explores the design space for information extraction (IE) from layout-rich documents using large language models (LLMs). The three core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model engagement, and 3) output refinement. Our study delves into the sub-problems within these core challenges, such as input representation, chunking, prompting, and selection of LLMs and multimodal models. It examines the outcomes of different design choices through a new layout-aware IE test suite, benchmarking against the state-of-art (SoA) model LayoutLMv3. The results show that the configuration from one-factor-at-a-time (OFAT) trial achieves near-optimal results with 14.1 points F1-score gain from the baseline model, while full factorial exploration yields only a slightly higher 15.1 points gain at ~ 36\u00d7 greater token usage. We demonstrate that well-configured general-purpose LLMs can match the performance of specialized models, providing a cost-effective alternative. Our test-suite is freely available at https://github.com/gayecolakoglu/LayIE-LLM.", "sections": [{"title": "1 Introduction", "content": "Information extraction (IE) entails extracting structured data, such as names, dates, or financial figures, from unstructured documents. Within organizations, key information often resides in layout-rich documents (LRDs) such as reports and presentations that combine visual elements (e.g., charts, tables) with textual structure and content (Park et al., 2019; Wang et al., 2023a; Zmigrod et al., 2024b). LRDs challenge traditional natural language processing (NLP) techniques, which are designed for plain texts (Cui et al., 2021; Tang et al., 2023).\nRecent layout-aware models at the intersection of NLP and Computer Vision (CV) address this gap by including visual and structural features to improve information extraction from LRDs"}, {"title": "2 Design Space of IE from Layout-rich Documents with LLMS", "content": "Task Definition. IE from LRDs involves identifying and extracting information from documents where textual content is intertwined with complex visual layouts and mapping them into structured information instances such that\n$IE: (D,S) \\rightarrow E$\n,where\n\u2022 D represents the set of LRDs, each with content and layout information.\n\u2022 S is the target schema that defines the set of slots to be filled. Each slot is defined by an attribute (key) $a_i$ and its corresponding data type (domain) $T_i$, such that $S = \\{(a_1, T_1), (a_2, T_2), ..., (a_k, T_k)\\}$.\n\u2022 Finally, E represents the set of extracted information instances, where each instance is a set of slot-value pairs derived from a document in D, leveraging both content and layout to determine the correct values for the slots in S. Each value in an instance must conform to the data type T, specified in the schema for that attribute.\n2.1 Using LLMs for Information Extraction\nIE systems that utilize LLMs have to tackle the three main challenge areas that we consider as part of the Design Space: Data Structuring, Model Engagement, and Output Refinement. Each stage, in their respective order, plays an important role for having an IE system with satisfactory accuracy and robustness.\nData Structuring. For multimodal LLMs, LRDs can be directly given as input. On the other hand, for purely text-based LLM, the input documents must be transformed into textual representations. This involves converting documents into machine-readable formats using OCR systems to extract features such as text, bounding boxes, and visual elements (Mieskes and Schmunk, 2019; Smith, 2007). Alternatively, a formatting language such as Markdown can be employed to represent the document's layout, allowing the LLM to understand the structural context of the text better. The impact of OCR quality on IE performance has been documented (Bhadauria et al., 2024), and structured formats tend to yield better results (Bai et al., 2024). To process larger documents efficiently, they are often divided into smaller, manageable chunks based on page boundaries, sections, or semantic units (Liu et al., 2024).\nMarkdown as an input format compared to raw OCR outputs remains underexplored, representing a potential research gap in IE system development.\nModel Engagement. Once preprocessed, the document is fed to an LLM for IE. Ensuring alignment between the extracted text and layout information is crucial for accurate representation (Xu et al., 2020a; Appalaraju et al., 2021). Prompt-driven extraction leverages general-purpose models, using tailored prompts to guide the extraction"}, {"title": "3.2 Model Engagement", "content": "Model engagement consists of constructing input to the LLM comprising at least three components: (1) a task instruction outlining the IE task, (2) the target schema S, and (3) the document chunk. We adhere to best practices from NLP for prompt structure and IE task instruction. The schema S is implemented as a dictionary of key-value pairs, where values specify the format of the corresponding attribute using regex expressions in Listing 1.\nWe also implement two ICL strategies: (1) Few-Shot and (2) Chain-of-Thought (CoT) (see Appendix A.1 for more details). For N prompts, the LLM performs N completions, with one completion per prompt. The outputs are collected and stored as raw predictions, ready for Output Refinement."}, {"title": "3.3 Output Refinement", "content": "We refine the raw outputs from the LLM to ensure alignment with the target schema, addressing challenges related to prediction variability, schema definition differences, and data formatting inconsistencies (e.g., varying date formats). We implement three techniques inspired by related work in data integration (Dong and Srivastava, 2013): Decoding, Schema Mapping, and Data Cleaning. This process results in three sets of predictions: initial predictions, mapped predictions, and cleaned predictions.\nDecoding. The decoding step parses each LLM completion as a JSON object, discarding any that fail to parse. The process then consolidates predictions for each document by reconciling outputs generated across individual pages and chunks. With N completions for N prompts, corresponding to N chunks, the model generates multiple predictions for a single document. Reconciliation ensures a unified document-level output by deduplicating nested predictions and aggregating unique values. If multiple unique values exist for a single entity, they are stored together to preserve variability. The outcome of this step is referred to as the initial predictions."}, {"title": "Schema Mapping", "content": "LLMs are expected to return only keys $\\{a_1, a_2, ..., a_k\\}$ specified in the target schema S. However, they may occasionally fail to return the keys as expected. E.g., 'file date\u201d is returned instead of \"file_date\". Such LLM \"overcorrection\" can hinder strict schema conformance. As a countermeasure, we implement a post-processing step that maps the predicted keys to align with the target schema. Our mapping step integrates multiple weak-supervision signals, such as exact matching, partial matching, and synonym-based logic, inspired by the recent techniques for ontology alignment (F\u00fcrst et al., 2023). The outcome of this step is the mapped predictions, where entity keys $\\{a_1, a_2, ..., a_k\\}$ are standardized and fully aligned with the schema S."}, {"title": "Data Cleaning", "content": "A common issue concerns the format of the values $T_k$ of predicted key-value pairs $\\{(a_1, T_1), (a_2, T_2), ..., (a_k, T_k)\\}$. We must standardize formats, such as dates and names, to align with the target schema S. One source of error is LLM hallucinations (Ji et al., 2023; Huang et al., 2023; Xu et al., 2024b), while another problem is that information is often not aligned to a common format inside the source data. For instance, two documents might use two different formats for dates (\"April 1992\u201d vs \u201c1992-04-01\u201d). Additional issues include capitalization, redundant whitespace, or special characters. We utilize the regex-defined data types in our schema to automatically apply data cleaning functions. The outcome of this step is the cleaned predictions, representing the final fully normalized outputs."}, {"title": "Evaluation Techniques", "content": "Evaluating IE for LRDS requires comparing the extracted data against an annotated test dataset. We implement three metrics for this evaluation: exact match, substring match, and fuzzy match.\n\u2022 Exact Match searches for perfect alignment between predicted and ground truth values. A match is valid only when the values are identical. This strict approach is ideal for extracting specific, unambiguous entities like dates or numerical identifiers.\n\u2022 Substring Match checks whether ground truth values are fully contained within the predicted values as complete substrings, without being split or partially matched. It ensures all ground truth values appear in their entirety within predictions, making it effective for tasks such as extracting full names or addresses, where additional contextual details (e.g., titles like Mr. and Mrs.) may be included in the predictions without making the extraction incorrect.\n\u2022 Fuzzy Match uses similarity metrics for approximate matches. A match is valid if the highest similarity ratio exceeds a predefined threshold (default: 0.8). This method is well-suited for scenarios with minor variations caused by OCR errors or formatting discrepancies."}, {"title": "4 Experimental Evaluation", "content": "4.1 Experimental Design\nMethodology. The goal of our experimental setup is to study how different parameters in the pipeline, shown in Figure 2, affect the overall performance of IE from LRDs using LLMs. To investigate the design dimensions, we start with a baseline configuration and and systematically alter factors at a single dimension at a time, following a one-factor-at-a-time (OFAT) methodology. This approach allows us to isolate and understand the impact of each parameter change on the IE performance.\nOur intuition is that aggregating the knowledge gained for each dimension independently, we can achieve a deeper understanding of the design space and possibly identify an effective overall configuration for IE, without the need for a comprehensive factorial exploration. However, we also validate the findings by comparing the results of the OFAT method with those obtained from a brute-force approach, which is based on conducting 432 experiments.\nDataset and LLMs. We utilize the Visually Rich Document Understanding (VRDU) dataset (Wang et al., 2023b), which includes two benchmarks. Each benchmark includes training samples of 10, 50, 100, and 200 documents with high-quality OCR for assessing data efficiency, as well as generalization tasks: Single Template Learning (STL), Unseen Template Learning (UTL), and Mixed Template Learning (MTL). For further details on how this dataset is tailored for our experiments and diverse models, please refer to Appendix A.2.\nWe evaluate GPT-3.5, GPT-40, and LLaMA3-70B for text-only structured data extraction from LRDS. Additionally, we compare their results with GPT-4 Vision and LayoutLMv3 to assess the performance gap between multimodal LLMs and domain-specific, fine-tuned models, respectively.\nThe Baseline Configuration. The baseline configuration is outlined in Table 1, where the configuration is selected based on best practices such as in (Perot et al., 2024) for the following reasons: (1) OCR reflects real-world scenarios for digitized LRDs. (2) Medium chunk size balances efficiency and context preservation, addressing token limits in LLMs. (3) Few-shot prompting combines pre-trained knowledge with minimal task-specific guidance. (4) Using zero examples provides a clear benchmark for assessing the model's raw performance. (5) Initial predictions are retained to evaluate models' raw output without modifications, ensuring a direct assessment of their capabilities. (6) Finally, exact match provides a stringent measure of correctness, offering a reliable baseline for comparison across configurations."}, {"title": "4.2 The Input Dimension", "content": "We substitute OCR input with Markdown and as outcomes in both STL and UTL scenarios. The differences in performance between OCR and Markdown are model- and context-dependent, exhibiting no consistent trend favoring one input type over the other, as shown in Table 2."}, {"title": "4.3 The Chunk Dimension", "content": "To evaluate the impact of chunk size, we varied it from medium to max and small while keeping all other parameters constant. Table 3 demonstrates how chunk size affects performance across STL and UTL levels.\nMedium and max chunk sizes provide the most consistent and stable results across models, with"}, {"title": "4.4 The Prompt Dimension", "content": "Table 4 presents the impact of prompt type and the number of examples on model performance at STL and UTL levels. Surprisingly, in-context demonstrations do not enhance performance for either few-shot or CoT experiments. For both experiments, the setting with zero examples achieves the highest average performance: few-shot 0.650 (\u00b10.011) and CoT 0.649 (\u00b10.008). Performance consistently declines as the number of examples increase, likely due to noise that impairs generalization. Overall, there is no significant difference between few-shot and CoT."}, {"title": "4.5 Output Refinement", "content": "We examine two output refinement strategies, Schema Mapping and Data Cleaning (see Sec. 3.3), to evaluate their impact shown in Table 5.\nSchema Mapping involves mapping the predicted schema keys to the target schema keys. Our results show no change in F1 scores compared to the initial predictions. This suggests that the models already effectively return the correct attributes, making the mapping step unnecessary.\nData Cleaning uses the defined data types to perform automatic value cleaning, consistently achieving the highest F1 scores across all models. This underscores the need for post-processing steps for IE with LLMs to align the extracted data with the target format to handle LLM hallucinations and inconsistent source data formats (see Sec. 3.3)."}, {"title": "4.6 Evaluation Techniques", "content": "We explore three evaluation techniques to assess their impact on model performance (see Sec. 3.3). On average, Fuzzy Match achieved the highest F1 score (0.733), outperforming Substring Match (0.676) and Exact Match, as shown in Table 6. We provide a detailed error analysis of fuzzy and substring match accuracy in Appendix A.3, showing that they provide a near-perfect precision when manually checked for semantic equivalence with precision scores of 0.98 and 1.00, respectively. This shows Fuzzy Match's ability to balance flexibility and precision."}, {"title": "4.7 Putting it All Together", "content": "In the preceding sections, we investigated the influence of various parameters on model performance along the IE extraction pipeline, analyzing one factor at a time. Drawing from the underlying 12 experiments, we identified the optimal parameter for each step and each model based on the experimental outcomes (Table 7). In addition, we conducted an exhaustive full factorial exploration with 432 configurations (2 * 3 * 2 * 4 * 3 * 3 = 432, see Table 1) to find the best parametrization per LLM (Table 8). Lastly, we find the worst configuration on a per LLM and per model basis (Table 9). The performance of these different configurations is depicted in Figure 3. We gain several insights:\n\u2022 OFAT approximates well the Brute-Force configuration with a fraction (~ 2.8%) of the required computation. We see in Table 7 and Table 8 that they match except for 4 parameter choices (prompt and example No. parameters for GPT-40 and LlaMa3). Likewise, their F1 scores are close to each other (Figure 3), with OFAT achieving 0.791 and Brute-Force achieving 0.801 overall.\n\u2022 Adapting the IE pipeline to the LLM is necessary to achieve a competitive performance. Overall, the OFAT configuration improves from the baseline F1 of 0.650 to 0.791, a 22% improvement. For Brute-Force, the improvement is 23%. In comparison, the worst configuration achieves only 0.45 on average, almost half of the best configuration.\n\u2022 Besides the need for pipeline customization, common patterns exist across LLMs. First, output refinement and evaluation techniques boost performance across all models. Second, there is a tendency to larger context sizes. Lastly, larger models (GPT-40, LLaMa3) benefit more from examples, while the CoT pattern generally aids IE."}, {"title": "5 Related Work", "content": "IE using LLMs. Transformer-based models like BERT (Devlin et al., 2019), GPT (Brown et al., 2020), and RoBERTa (Liu et al., 2019b) have advanced NLP with self-attention and large-scale pre-training but struggle with layout-rich documents (LRDs). To address this, layout-aware and multimodal models have emerged. LayoutLM (Xu et al., 2020b) integrates spatial features, with later versions (Xu et al., 2021, 2022) and models like GPT-4V (OpenAI, 2023) and Gemini Pro (Anil et al., 2023) enhancing document understanding through multimodal learning. LayoutLLM (Fujitake, 2024) and structural-aware approaches (Lee et al., 2022) further improve extraction accuracy, while end-to-end models like Donut (Kim et al., 2022) bypass OCR for direct document image processing. Additionally, ChatUIE (Xu et al., 2024a) adopts a chat-based approach for flexible IE, while ReLayout (Jiang et al., 2025) enhances document understanding through layout-aware pretraining, advancing LLMs for real-world use.\nStrategies for IE from LRDs. Graph-based models like GCNs (Liu et al., 2019a) and Ali-GATr (Nourbakhsh et al., 2024) enhance relation extraction by capturing textual-visual relationships. Reading order is critical; Token Path Prediction (TPP) (Zhang et al., 2023) resolves OCR layout ambiguities, while global tagging (Shaojie et al., 2023) mitigates text ordering issues for better extraction. For structured data, TabbyPDF (Jain et al., 2020) targets table extraction, DocExtractor (Zhong et al., 2020) processes forms, and LMDX (Perot et al., 2024) unifies OCR, preprocessing, and postprocessing for document IE. Additionally, XFORM-PARSER (Cheng et al., 2025) offers a simple yet effective multimodal and multilingual approach for parsing semi-structured forms.\nPreprocessing, Chunking, Prompting, Postprocessing, and Evaluation Techniques. Chain-of-Thought (CoT) prompting (Wei et al., 2022) enhances reasoning in complex LRD extraction tasks, while diverse prompt-response datasets (Zmigrod et al., 2024a) improve LLM robustness. Instructionfinetuned LLMs have also demonstrated effectiveness in domain-specific applications, such as clinical and biomedical tasks, where zero-shot and few-shot learning enable adaptive extraction without extensive fine-tuning (Labrak et al., 2024). Postprocessing techniques, including text normalization, entity resolution (Hwang et al., 2021), and majority voting (Wang et al., 2022a), refine extracted data by correcting OCR and extraction errors for greater accuracy.\nDespite advancements, current studies often focus on isolated components rather than evaluating full IE pipelines. Key stages such as OCR, chunking, and postprocessing are frequently assessed independently, leading to an incomplete understanding of their interplay. Our research bridges this gap by systematically evaluating strategies across all pipeline stages to identify optimal configurations for leveraging pre-trained models."}, {"title": "6 Conclusion", "content": "In this paper we introduce the design space for IE from LRDs using LLMs, with challenges of data structuring, model engagement, and output refinement. We consider the design choices within those challenges. We examine the effects of these choices on the overall performance through OFAT and full-factorial exploration of the parameters. Finally, we show that a well-designed general purpose LLM can provide a cost-effective alternative to the the specialized models for IE from LRDs."}, {"title": "Limitations", "content": "For the experimental evaluation in the design space, we focused on the Visually Rich Document Understanding (VRDU) dataset (Wang et al., 2023b), which has ground-truth annotations that make it easy for result analysis. We believe incrementally adding new datasets focusing on layout-rich documents with ground truth and making a combined benchmark would help improve the design space itself as we can test different application-specific scenarios.\nThe evaluation metrics are planned to be extended with metrics that are popularly used in the context of LLMs such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002) scores and their adaptations (Yang et al., 2018). A fair comparison using cost-related metrics is mostly harder to compute due to their changing depending on the scenario. For instance, in some scenarios where we listed cost as 0 due to having a free-to-use service, it actually comes with a cost. Similarly, the cost of computation can be considered from different perspectives such as the energy usage of a model.\nThe test suite is currently limited to the steps that we listed in Fig. 2, whereas one could imagine additional steps or factors which affect the performances and may even deliver more satisfactory outcomes. We would like to incorporate additional steps that we learn from the community and incrementally enlarge the design space and extend the testing capabilities for IE from LRDs.\nLast, we have only evaluated three LLMs, one multi-modal LLM and one layout-aware, fine-tuned model (LayoutLMv3). In the future, we want to extend our evaluation to further LLMS and fine-tuned, layout-aware models (e.g., ). Despite that, our results already show a clear trend in which specialized, fine-tuned models struggle to keep up with progress in general-purpose LLMs. E.g., LayoutLMv3, only reaches an F1 score of 0.681 with fine-tuning and on the same document types (STL), substantially less than Llama3, the best text-based LLM with 0.832 and GPT-4-vision, our tested multi-modal LLM with 0.890. This underlines the importance of our work, in which choices in data structuring, model engagement, and output refinement become critical in achieving the best performance."}, {"title": "Ethical Considerations", "content": "Large Language Models (LLMs) can contain biases that can have a negative impact on marginalized groups (Gallegos et al., 2024). For the task of information extraction, this could have the impact that uncommon names for people and places are auto-corrected by the LLM to their more common form. In our experiments, we have encountered some instances of this and plan to investigate this further."}]}