{"title": "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis", "authors": ["Xinyu Feng", "Yuming Lin", "Lihua He", "You Li", "Liang Chang", "Ya Zhou"], "abstract": "Multimodal Sentiment Analysis (MSA) utilizes multimodal data to infer the users' sentiment. Previous methods focus on equally treating the contribution of each modality or statically using text as the dominant modality to conduct interaction, which neglects the situation where each modality may become dominant. In this paper, we propose a Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA) for multimodal sentiment analysis. KuDA uses sentiment knowledge to guide the model dynamically selecting the dominant modality and adjusting the contributions of each modality. In addition, with the obtained multimodal representation, the model can further highlight the contribution of dominant modality through the correlation evaluation loss. Extensive experiments on four MSA benchmark datasets indicate that KuDA achieves state-of-the-art performance and is able to adapt to different scenarios of dominant modality.", "sections": [{"title": "1 Introduction", "content": "Since users' sentiment expressions are reflected in multiple modalities on social media, multimodal sentiment analysis (MSA) has garnered rising attention in recent years. It aims to mine and comprehend the sentiments of online videos (Soleymani et al., 2017; Zeng et al., 2021; Kaur and Kautish, 2022). Most recent MSA methods can be grouped into two categories: ternary symmetric-based methods (Zadeh et al., 2017; Liu et al., 2018; Zadeh et al., 2018a; Tsai et al., 2019; Hazarika et al., 2020; Yu et al., 2021; Sun et al., 2022; Huang et al., 2024) and text center-based methods (Han et al., 2021a,b; Wang et al., 2022; Li et al., 2022; Lin and Hu, 2022; Wang et al., 2023; Zhang et al., 2023). Ternary symmetric-based methods focus on equally treating the contribution of each modality and modeling the bidirectional relationship of all modality pairs. Text center-based methods focus on using text as the dominant modality to guide the vision and audio modalities to interact with it to adjust the contributions of different modalities properly. Thus, both ternary symmetric-based methods and text center-based methods consider the distribution of importance among modalities to be static and fix the dominant modality.\nHowever, as shown in Figure 1, we discovered that in certain situations, vision, text, or audio could be the dominant modality respectively. With the first sample in Figure 1, since the vision label is more consistent with the overall sentiment label, it is dominant. Based on an investigation of commonly used datasets in MSA, we find that these situations are not uncommon. Detailed statistics and analysis can be found in Appendix A. Thus, when the dominant modality is not fixed, the ternary symmetric-based methods cannot effectively adapt to the situation where any modality is dominant because they do not consider the differences of importance between modalities. The text center-based methods statically set text as the dominant modality, and when other modalities are dominant, the model's attention is distracted by the text.\nIn this paper, we propose a Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA), which improves the model performance and makes it adaptable to more complex and wider scenarios by dynamically selecting the dominant modality and adjusting the contributions of each modality according to different samples. Specifically, KuDA first uses the BERT model and two Transformer Encoders to extract semantic features of text, vision, and audio modalities. Then, KuDA performs sentiment knowledge injection and sentiment ratio conversion by the adapters and decoders, which can extract sentiment clues and guide KuDA in selecting the dominant modality further. Next, the dynamic attention fusion module is designed to capture similar sentiment information and gradually adjusts the attention weights between modalities by interacting sentiment knowledge with different levels of multimodal features. Based on the correlation evaluation between the multimodal features and the unimodal features, we use Noise-Contrastive Estimation (Gutmann and Hyv\u00e4rinen, 2010) to highlight the contribution of the dominant modality further. Finally, KuDA predicts the sentiment score through a multilayer perceptron.\nThe main contributions of our work can be summarized as follows:\n\u2022 We propose KuDA, a Knowledge-Guided Dynamic Modality Attention Fusion Framework for multimodal sentiment analysis, which improves the performance by dynamically selecting the dominant modality, making model adaptable to complex and wide scenarios.\n\u2022 We design a Dynamic Attention Fusion module, which utilizes sentiment knowledge to guide different levels of multimodal features and achieves dynamic fusion by adjusting the contribution of each modality.\n\u2022 Extensive experiments on four MSA datasets show that KuDA achieves state-of-the-art performance. We further analyze the experimental results to prove the effectiveness of KuDA."}, {"title": "2 Related Work", "content": "In this section, we briefly overview related work in ternary symmetric-based methods and text center-based methods.\nTernary symmetric-based methods. Previous works in this category have primarily focused on modeling bidirectional relationships in each modality pair and treating the contribution of each modality equally. For instance, some researchers designed the fusion architectures of tensor-based (Zadeh et al., 2017; Liu et al., 2018), LSTMs-based (Zadeh et al., 2018a) and mlp-based (Sun et al., 2022) to capture the commonalities between modalities. While Tsai et al. (2019) designed MuLT, which is a transformer-based model, to find similar information between each modality pair. Some studies (Tsai et al., 2018; Hazarika et al., 2020; Yang et al., 2022) project each modality into two subspaces, separately learning the commonalities and characteristics of the modalities to aid the fusion process. Multi-label strategy (Yu et al., 2021) and contrastive learning (Mai et al., 2023) are introduced to improve the quality of unimodal features. In addition, Huang et al. (2024) designed TMBL, which can handle bimodal and trimodal features to capture and utilize bound modal features.\nText center-based methods. Previous works in this category mainly focus on improving the quality of fuse representation through the text modality to guide the vision and audio modalities. For example, Delbrouck et al. (2020), Han et al. (2021a) and Wang et al. (2023) use the transformer-based method to integrate similar information from other modalities by text modality. While some researchers (Rahman et al., 2020; Wang et al., 2022) enhance the text representations by integrating vision and audio information into a pretrained language model. Moreover, to decrease the potential sentiment-irrelevant and conflicting information, some studies reduce additional noise by maximizing mutual information (Han et al., 2021b) or adaptive representation learning (Zhang et al., 2023). Meanwhile, contrastive learning is introduced to learn invariant and similar information between text with other modalities (Li et al., 2022; Lin and Hu, 2022)."}, {"title": "3 Methodology", "content": "Despite the promising results achieved for MSA, the above approaches treat each modality in a balanced way or statically set text as the dominant modality. This causes the approach to be distracted by the secondary modalities, hindering it from dynamically adjusting to diverse scenarios where the dominant modality varies, which limits the performance of MSA."}, {"title": "3.1 Overall Architecture", "content": "As shown in Figure 2, which shows the overall workflow of KuDA. Specifically, KuDA first extracts unimodal low-level features from the raw multimodal input. Then, the adapters and encoders extract unimodal high-level features and learn sentiment knowledge simultaneously. We utilize the decoders to predict the unimodal sentiments and convert them into sentiment ratios to guide dynamic fusion. Next, we designed a dynamic attention fusion module, which selects the dominant modality according to different scenarios and dynamically adjusts attention weights with sentiment ratios and knowledge representations. Finally, the multimodal representation is used to conduct the MSA task by multilayer perceptron (MLP) and estimate correlation with knowledge representations.\nIn addition, to guide the model in adjusting the attention weights by sentiment knowledge, KuDA adopts a two-stage training method."}, {"title": "3.2 Problem Definition and Notations", "content": "In MSA task, the input data consists of text (t), vision (v) and audio (a) modalities. The sequences of three modalities are represented as triplet $(I_t, I_v, I_a)$, which include $I_t \\in \\mathbb{R}^{T_t \\times d_t}$, $I_v \\in \\mathbb{R}^{T_v \\times d_v}$, and $I_a \\in \\mathbb{R}^{T_a \\times d_a}$, where $T_m, m \\in \\{t, v, a\\}$ is the sequence length and $d_m$ represents the vector dimension. The prediction is the sentiment score $\\hat{y}$, which is a discrete value between [-1, 1] and [-3, 3], with values greater than, equal to, and less than 0 representing positive, neutral, and negative, respectively."}, {"title": "3.3 Encoding with Knowledge Injection", "content": "We encode the input of each modality $I_{m \\in \\{t,v,a\\}}$ into the global semantic representations $H_m \\in \\mathbb{R}^{T_m \\times d_m}$ and the knowledge-sentiment representations $K_m \\in \\mathbb{R}^{T_m \\times d_m}$ via the pretrained encoders and adapters, respectively.\nGlobal semantic representations. For the text modality, to effectively extract from low-level to high-level text semantic information and facilitate subsequent knowledge inject with Adapter, we use the BERT (Kenton and Toutanova, 2019) to encode the input sentences $I_t$, and extract the hidden state of the last layer as the global semantic representation $H_t$:\n$H_t, O_t = \\text{BERT} (I_t; \\Theta_{\\text{BERT}})$"}, {"title": "3.4 Dynamic Attention Fusion", "content": "Since the difference in sentiment score of unimodal $\\mathcal{Y}_{m}$ and multimodal y is inversely proportional to the weight of unimodal, we choose the inverse proportional function $exp(-kx)$ and normalization operation, and utilize the ground truth of MSA y to convert the unimodal sentiment score $\\hat{y}_m$ into sentiment ratio $R_m, m \\in \\{t, v, a\\}$ during training to guide subsequent dynamic fusion:\n$D_m = \\exp \\left(-k|\\hat{y}_m - y|^2\\right)$\n$R_m = \\frac{D_m}{D_t + D_v + D_a}$"}, {"title": "3.4.2 Dynamic Attention Block", "content": "In order to unify the length and dimension axis of the unimodal knowledge-enhanced representation $U_m \\in \\mathbb{R}^{T_m \\times 2d_m}$ for multimodal fusion, we utilize three projectors to obtain the updated knowledge-enhanced representation of each modality $\\overline{U}_m, m \\in \\{t, v, a\\}$. In addition, due to any one or more of the text, vision, and audio modalities may become the dominant modality, we first sum the obtained representations $\\overline{U}_m$ as the input of the first dynamic attention block $F^0$:\n$\\overline{U}_m = \\text{Projector}_m (U_m) \\in \\mathbb{R}^{T_f \\times d_f}$\n$F^0 = \\overline{U}_t + \\overline{U}_v + \\overline{U}_a \\in \\mathbb{R}^{T_f \\times d_f}$"}, {"title": "3.5 Output and Training Objectives", "content": "To further improve the utilization of the dominant modality, we estimate the correlation of the multimodal representation $F^L$ and the unimodal representations $\\overline{U}_m, m \\in \\{t, v, a\\}$ through the Contrastive Predictive Coding (Oord et al., 2018), and integrated it into the Noise-Contrastive Estimation framework (Gutmann and Hyv\u00e4rinen, 2010) to form the correlation estimation (CE) loss $\\mathcal{L}_{cor}$:\n$\\mathcal{L}_{cor} = \\sum_{m \\in \\{t,v,a\\}} \\text{LNCE} (F^L, \\overline{U}_m)$\nIn the Output, we input the representation $F^L$ into a MLP to predict sentiment score $\\hat{y}$. Given the predictions $\\hat{y}$ and the ground truth $y$, we calculate the MSA task loss $\\mathcal{L}_{reg}$ by mean absolute error. Finally, we training KuDA by the union loss $\\mathcal{L}_{task}$:\n$\\hat{y} = \\text{MLP} (\\text{Mean} (F^L))$\n$\\mathcal{L}_{reg} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$\n$\\mathcal{L}_{task} = \\mathcal{L}_{reg} + \\alpha \\mathcal{L}_{cor}$"}, {"title": "4 Experiments", "content": "We conduct experiments on four publicly benchmark datasets of MSA, including CH-SIMS (Yu et al., 2020), CH-SIMSv2 (Liu et al., 2022), MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018b). The statistic details of four datasets are shown in Table 1.\nFollowing previous works (Hazarika et al., 2020; Yu et al., 2021; Zhang et al., 2023), we used the accuracy of 3-class (Acc-3) and 5-class (Acc-5) on CH-SIMS and CH-SIMSv2, the accuracy of 7-class (Acc-7) on MOSI and MOSEI, and the accuracy of 2-class (Acc-2), Mean Absolute Error (MAE), Pearson Correlation (Corr), and F1-score (F1) on all datasets. Moreover, on MOSI and MOSEI, Acc-2 and F1 used two calculation ways: negative/non-negative (has-0) and negative/positive (non-0). Except for MAE, higher values indicate better performance for all metrics."}, {"title": "4.2 Baselines", "content": "To validate the KuDA's performance, we conduct a fair comparison with several competitive and state-of-the-art (SOTA) baselines, including the ternary symmetric-based methods: TFN (Zadeh et al., 2017), LMF (Liu et al., 2018), MuLT (Tsai et al., 2019), MISA (Hazarika et al., 2020), Self-MM (Yu et al., 2021), CubeMLP (Sun et al., 2022) and TMBL (Huang et al., 2024), and the text center-based methods: MMIM (Han et al., 2021b), BBFN (Han et al., 2021a), CENet (Wang et al., 2022), TETFN (Wang et al., 2023) and ALMT (Zhang et al., 2023)."}, {"title": "4.3 Experimental Settings", "content": "To ensure fairness with other baselines, we follow recent competitive and SOTA methods to set the proposed method. The training stages consist of the unimodal stage and the multimodal stage. In addition, we adopt BERT to extract the features of text modality where \u201cbert-base-chinese\u201d2 is employed for CH-SIMS and CH-SIMSv2 and \u201cbert-base-uncased\u201d3 is employed for MOSI and MOSEI. In the vision and audio modalities, we directly use the features provided by the original datasets. Moreover, we develop the KuDA using a single NVIDIA RTX 3090 GPU for all datasets. The detailed setting of the best hyper-parameters can be referred to in Table 2."}, {"title": "4.4 Performance Comparison", "content": "Table 3 and Table 4 present the comparison results of the baselines and the proposed method on CH-SIMS, CH-SIMSv2, MOSI and MOSEI.\nSince the distribution of modality importance in the CH-SIMS and CH-SIMSv2 is more uniform, they are more complex than MOSI and MOSEI. As shown in Table 3, the proposed method outperforms all baselines on all metrics. It is worth noting that our method achieves superior performance on the CH-SIMSv2 dataset. For example, compared with ALMT (text center) and TMBL (ternary symmetric), our method achieves 8.32% and 9.19% improvement on the Acc-5 and also achieves significant improvement on the Acc-3. Therefore, achieving superior performance in the more challenging scenario indicates that KuDA can adjust the distribution of modality weights to complete dynamic fusion. It also shows that adjusting the dominant modality is crucial for MSA. Furthermore, although the importance of modalities is not evenly distributed in MOSI and MOSEI, and the text modality plays an important role, it can be seen from Table 4 that KuDA still obtained SOTA performance in almost all metrics. At the same time, KuDA surpasses some text-center training methods, such as BBFN and ALMT."}, {"title": "4.5 Ablation Study and Analysis", "content": "We conducted ablation studies to validate the effectiveness of each component, as shown in Table 5. By comparing the \u201cw/o DAF\u201d and KuDA, we observe that removing the DAF can seriously reduce performance. This means that KuDA dynamically adjusts the attention weights between modalities for different scenarios to select the dominant modality. In addition, the performance decreases in \"w/o EKI\", which shows that sentiment knowledge can further guide dynamic fusion. The performance decreases after removing other modules, showing their effectiveness. Since improving the utilization of vision and audio on MOSI introduces noise, Corr has improved."}, {"title": "4.5.2 Importance of Different Modalities", "content": "To validate the impact of text, vision and audio modalities, we performed ablation studies that removed each modality on KuDA, ALMT and CubeMLP, as shown in Table 6.\nIn CH-SIMSv2, which has more complex scenes, we can see that KuDA can reach the SOTA when each modality is removed. Meanwhile, when performance degrades, KuDA can still achieve acceptable results. In contrast, the other baselines will have a significant performance degradation, which proves that our method can adaptive focus on the suboptimal modality to capture sentiment features. For the MOSI dataset, which is mainly text, we can see that CubeMLP has dropped significantly when each modality is removed. Furthermore, ALMT drops sharply after removing text and is lower than KuDA. Notably, observing the performance degradation trend after removing a certain modality indicates that the importance of each modality is evenly distributed in the CH-SIMSv2, while the importance of text modality is higher in MOSI."}, {"title": "4.5.3 Effects of Different Fusion Methods", "content": "To analyze the effects of different fusion techniques, we conducted some experiments shown in Table 7. Obviously, when faced with complex scenes, using either ternary symmetric-based (TFN, LMF) or text center-based (BBFN, ALMT) fusion methods will result in performance decline. This indicates that not focusing on the dominant modality or statically setting the dominant modality will limit the performance of MSA. However, the use of our Dynamic Attention Fusion to dynamically fuse unimodal features is the most effective."}, {"title": "4.5.4 Effects of Correlation Estimation", "content": "As shown in Figure 5, we discuss the impact of CE loss on CH-SIMSv2 and MOSI by modifying the $\\alpha$. We compare the MAE, Acc-5 and Acc-7 as these metrics indicate the method's ability to predict fine-grained sentiment.\nCompared to removing CE loss, i.e. $\\alpha$=0, the model's performance achieves STOA when using CE loss and setting $\\alpha$=0.01. This shows that CE loss can highlight the contribution of the dominant modality further. However, the performance shows a downward trend when $\\alpha$ increases, indicating that KuDA will enhance the retention of non-dominant modality features in the multimodal representation when CE loss increases, limiting its performance."}, {"title": "4.5.5 Visualization of Features Distribution", "content": "To verify KuDA can dynamically select dominant modality, we use t-SNE to visualize the features of text, vision, audio and multimodal on CH-SIMSv2, as shown in Figure 6. We then selected two typical methods, CubeMLP (ternary symmetric) and ALMT (text center), to compare with KuDA."}, {"title": "4.5.6 Case Study", "content": "To better prove that the our method can dynamically adjust contributions of different modalities, we selected three challenging cases for further analysis, as shown in Table 8.\nWe can observe that in case (a), although the text and audio express stronger negative sentiments, KuDA can still output the correct prediction. This case shows that by adjusting vision as the dominant modality, KuDA effectively captures the speaker's information of expression and action, which also guides the fusion of text and audio modalities. In cases (b) and (c), the similar distributions of labels also occurred. KuDA still makes correct predictions, which indicates that it captures the semantic information of text in case (b) and the intonation information of audio in case (c) by adjusting the attention weights. Meanwhile, it can be seen in the Attention Weight of Table 8 that attention weight for the dominant modality (there are denser dark blocks) is higher than that for the other modalities. This once again proves the importance of dynamic attention fusion for the MSA task."}, {"title": "5 Conclusion", "content": "In this paper, we propose a Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA) to simultaneously solve the MSA task of the modality importance being equally or unequally distributed. Since KuDA dynamically adjusts the contribution of each modality for different scenarios, it effectively improves the utilization of the dominant modality. This enables our model to be more effective and generalized on four popular MSA benchmark datasets. At last, we perform comprehensive ablation studies to analyze this phenomenon."}, {"title": "Limitations", "content": "Although the KuDA proposed in this paper has yielded exceptional outcomes, there remain several limitations that offer opportunities for further enhancement. First, KuDA suffers from an error propagation problem due to its two-stage training method. When the pretrained sentiment knowledge incorrectly predicts the sentiment score, the sentiment ratio will introduce noise when the model adjusts the weights. Second, KuDA needs to be pretrained using the sentiment knowledge of each modality, which increases the resource consumption of model training. In future work, we will further try to explore fine-tuning the pretrained knowledge injection module in the prediction stage to solve the above limitations."}, {"title": "B Training Process", "content": "KuDA uses a two-stage training method, which details are shown in Algorithm 1. In Stage 1, we pretrained the Encoding with Knowledge Injection module using external data. For CH-SIMS and CH-SIMSv2, we use the unimodal labels of the dataset itself for pretraining. For MOSI and MOSEI, considering the data scale, we translate the texts of CH-SIMS and CH-SIMSv2 into English and inject the sentiment knowledge of CH-SIMS into MOSI and that of CH-SIMSv2 into MOSEI. Notably, to compare fairly with the baselines, we only pretrain the task of unimodal sentiment prediction on all datasets and do not involve the MSA task. In Stage 2, to prevent the pretrained knowledge from being overwritten, we froze the Adapter and Decoder of each modality and performed the MSA task based on the pretrained knowledge."}]}