{"title": "Instant Adversarial Purification with Adversarial Consistency Distillation", "authors": ["Chun Tong Lei", "Hon Ming Yam", "Zhongliang Guo", "Chun Pong Lau"], "abstract": "Neural networks, despite their remarkable performance in widespread applications, including image classification, are also known to be vulnerable to subtle adversarial noise. Although some diffusion-based purification methods have been proposed, for example, DiffPure, those methods are time-consuming. In this paper, we propose One Step Control Purification (OSCP), a diffusion-based purification model that can purify the adversarial image in one Neural Function Evaluation (NFE) in diffusion models. We use Latent Consistency Model (LCM) and ControlNet for our one-step purification. OSCP is computationally friendly and time efficient compared to other diffusion-based purification methods; we achieve defense success rate of 74.19% on ImageNet, only requiring 0.1s for each purification. Moreover, there is a fundamental incongruence between consistency distillation and adversarial perturbation. To address this ontological dissonance, we propose Gaussian Adversarial Noise Distillation (GAND), a novel consistency distillation framework that facilitates a more nuanced reconciliation of the latent space dynamics, effectively bridging the natural and adversarial manifolds. Our experiments show that the GAND does not need a Full Fine Tune (FFT); PEFT, e.g., LoRA is sufficient.", "sections": [{"title": "Introduction", "content": "Deep Neural Networks (DNNs) have revolutionized the field of computer vision, achieving remarkable milestones across a spectrum of tasks. From the groundbreaking performance in image classification (Krizhevsky, Sutskever, and Hinton 2012; Simonyan and Zisserman 2015a; Yue and Li 2024) to significant advancements in object detection (Zhao et al. 2019; Huang et al. 2023), image segmentation (Girshick et al. 2014; Minaee et al. 2021) and face recognition (Liu et al. 2023a), DNNs have consistently pushed the boundaries of artificial intelligence. However, beneath this veneer of success lies a disconcerting vulnerability that has captured the attention of researchers worldwide (Lau et al. 2023a). The Achilles' heel of these powerful models manifests in their susceptibility to adversarial attacks (Liu et al. 2022b). These attacks, orchestrated through the introduction of carefully crafted, imperceptible perturbations to input data, can manipulate DNNs into making egregious misclassifications (Liu et al. 2023b). Although some works have leveraged this weakness as a benign metric to prevent the underlying misuse of generative AI (Liu, Lau, and Chellappa 2023; Guo et al. 2024), the implications of this vulnerability extend far beyond academic interest, posing a formidable challenge to the deployment of DNNs in security-critical applications, where reliability and robustness are paramount (Souri et al. 2021).\nIn response to these challenges, two primary defense strategies have emerged: adversarial training (Tramer and Boneh 2019; Liu et al. 2022a; Wang et al. 2023) and adversarial purification (Samangouei, Kabkab, and Chellappa 2018; Yoon, Hwang, and Lee 2021; Nie et al. 2022). However, adversarial training faces significant limitations in practical applications. Adversarial training requires prior knowledge of attack methods to generate adversarial examples, which inherently limits its scope and effectiveness against unknown threats. In contrast, adversarial purification offers a more versatile defense mechanism. This approach provides a broader spectrum of protection without the need to anticipate specific attack methods. By focusing on removing adversarial perturbations rather than training against known attacks, purification methods offer a more generalized defense strategy, potentially more adaptable to the evolving landscape of adversarial threats.\nAdversarial purification typically leverages generative models to achieve a distribution shift from adversarial samples back to benign ones. In this context, diffusion models (Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2021) have emerged as a particularly promising tool and achieved a good performance of anti-attack (Nie et al. 2022; Wang et al. 2022). These models offer significant advantages over other generative approaches, such as Generative Adversarial Networks (GANs), notably in terms of training stability and ease of implementation. The ability of diffusion models to generate high-quality samples by gradually denoising random noise has made them an attractive choice for adversarial purification.\nHowever, diffusion models face a critical limitation that hinders their practical application. The inference process of these models involves multiple denoising steps, resulting in a computationally intensive and time-consuming operation. This inherent slowness poses a significant challenge, particularly in contexts where rapid response times are crucial, such as real-time image classification or security systems. The computational overhead of diffusion models severely restricts their applicability in scenarios demanding swift adversarial defenses, creating a pressing need for more efficient purification methods that can maintain the quality of defense while significantly reducing computational cost.\nTo address these challenges, we propose One Step Control Purification (OSCP), a novel diffusion-based purification method that requires only a single inference step. Our approach ingeniously combines the concept of consistency distillation with a nuanced consideration of the disparities between adversarial and clean samples. This synthesis culminates in our innovative Gaussian Adversarial Noise Distillation (GAND) framework, which achieves effective purification in just one inference step, dramatically reducing computational overhead while maintaining robust defense capabilities.\nWe tackle a long-standing issue in diffusion-based methods: semantic information losses when a large diffusion step is chosen. This problem is compounded by Latent Consistency Models, which make the trade-off between speed and semantic integrity, can result in blurry images. To overcome these limitations, we introduce a Controlled Adversarial Purification method (CAP), which utilizes ControlNet and Canny Edge detector to produce a clear purified image while maintaining most of the semantic information.\nOur OSCP method, integrated by both GAND and CAP, achieving 74.19% robust accuracy on ImageNet using only 0.1s for each purification, represents a significant advancement in the field of adversarial purification. By addressing the critical issues of computational efficiency and semantic information loss, we pave the way for more practical and effective defenses against adversarial attacks in real-world applications.\nIn summary, our contribution can be summarized as:\n\u2022 We proposed a novel consistency distillation method GAND for adversarial training on LCM. Empirical shows GAND has remarkable transferability on unknown attacks.\n\u2022 We introduce CAP, a novel framework utilizing non-learnable edge detection operators to enhance adversarial purification controllability.\n\u2022 OSCP combines CAP and GAND, achieving rapid purification process, which extends diffusion base purification to real time."}, {"title": "Related Work", "content": "Adversarial training Adversarial training (Madry et al. 2018) has emerged as one of the most effective methods for enhancing the robustness of DNNs against adversarial attacks by incorporating adversarial examples into the training process (Lau et al. 2023b). The impressive performance (Athalye, Carlini, and Wagner 2018; Gowal et al. 2020; Rebuffi et al. 2021) highlights its ability to significantly enhance the resilience of DNNs to various known attacks. Despite its success, adversarial training tends to overfit these specific perturbations, potentially limiting the robustness of novel attacks (Lin et al. 2020). To address this, recent adversarial training is trying to make use of the diffusion model to generate more image data for adversarial training (Tramer and Boneh 2019; Gowal et al. 2021; Wang et al. 2023), to prevent overfitting to the adversarial images.\nAdversarial Purification Adversarial purification is a method that purifies images before classification. At first, GAN model is used to purify adversarial noise (Samangouei, Kabkab, and Chellappa 2018), then after the significant performance of the diffusion models. Diffusion models (Song and Ermon 2019; Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2021) are used to purify noisy images, showing a remarkable performance (Yoon, Hwang, and Lee 2021; Nie et al. 2022; Wang et al. 2022). Removing the adversarial noise from the mixture of adversarial noise and Gaussian noise from the forward process simultaneously, DiffPure (Nie et al. 2022) claims that following forward diffusion, the KL divergence between the distributions of clean and adversarial images is reduced. This indicates a more aligned and purified output. However, a notable challenge remains the prolonged purification process, which can be inefficient and impractical (Wang et al. 2022).\nDiffusion model Diffusion models have promising performance in various fields, such as text-to-image generation (Rombach et al. 2022; Saharia et al. 2022), video generation (Ho et al. 2022; Blattmann et al. 2023) and 3D generation (Luo and Hu 2021; Poole et al. 2023). Denoising Diffusion Probabilistic Models (DDPM) (Ho, Jain, and Abbeel 2020) is proposed as an image-to-image model, diffusing the original image by Gaussian noise, noted as forward process, which satisfies Markov property. Then, the model recovers the image by learning the reverse Markov chain, noted as reverse process. However, the inference time is incredibly long since the time is correlated to number of inference steps. Consistency Model (Song et al. 2023) adds a consistency constraint to Diffusion models, aiming to generate the image in a few inference steps. After that, Latent Consistency Model (LCM) (Luo et al. 2023a) is invented to speed up the generating process further. In LCM-LoRA (Luo et al."}, {"title": "Preliminaries", "content": "Diffusion model\nDenoising Diffusion Probabilistic Models (DDPM) (Ho, Jain, and Abbeel 2020) generate images by learning from the reverse Markov chain with Gaussian noise added to the original image. The forward process can be formulated as linear combination of original image $x_0$ and standard Gaussian noise $\\epsilon$, $\\bar{\\alpha}_t$ denoted cumulative product from $\\alpha_1$ to $\\alpha_t$, $\\alpha_t = 1 - \\beta_t$ for any t, $\\beta_t$ is predefined variance schedule of diffusion process:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim N(0, 1)$ (1)\nand the denoising step can be expressed as:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta^*}(x_t, t)) + \\sqrt{\\beta_t}\\epsilon \\\\$ (2)\nwhere model parameter $\\theta^*$ minimize the loss between actual noise and predict noise:\n$\\theta^* = \\underset{\\theta}{\\arg \\min} \\mathbb{E}_{x_0, t, \\epsilon} [||\\epsilon - \\epsilon_{\\theta} (x_t, t)||^2]$ (3)\nShortly, Denoising Diffusion Implicit Models (DDIM) (Song, Meng, and Ermon 2021), a non-Markov inference Process Model, has been invented to reduce the inference time of diffusion model. The denoising step has been respectively modified as:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}(\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_{\\theta^*}(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}) + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\epsilon \\\\$ (4)\nwhere $\\sigma_t = \\eta \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}} \\beta_t$. The denoising process is deterministic if $\\eta = 0$ and equals to DDPM if $\\eta = 1$.\nDiffusion Base Purification (DBP)\nDiffPure (Nie et al. 2022) proposes that diffusion models can remove adversarial noise by performing sub-process of the normal reverse process (t = 0 to t = T). By adding predefined $t^*$ ($t^* < T$) of noise to the adversarial image $x_{adv}$, which is formed by the sum of original image x and adversarial noise $\\delta$, $\\delta$ can generated by Lp attack (Madry et al. 2018) or AutoAttack (Croce and Hein 2020):\n$\\delta = \\underset{\\delta}{\\arg \\max} L(C(x + \\delta), y)$ (5)\nC is the classifier, y is the true label. The forward process of diffusion purification method using:\n$x(t^*) = \\sqrt{\\bar{\\alpha}(t^*)}x + \\sqrt{1 - \\bar{\\alpha}(t^*)}\\epsilon$ (6)\nand solve the reverse process of DDPM from time step $t^*$ to 0, to get the purified $x_{adv}$ that is closed to the original image x, allowing the classifier to classify the image with the correct label.\nConsistency function\nDiffusion models are known to have long inference time, which limits their usage in real world. Consistency model (Song et al. 2023) has been proposed, aiming the distilled a Consistency model from pretrained diffusion model (CD) or training a Consistency model from scratch (CT), this paper will focus on distilled method. Consistency function can be formulated as:\n$f_{\\theta}(x, t) = \\begin{cases}\n  f_\\theta(x) & t=\\epsilon \\\\\n  F_{\\theta}(x, t) & t \\in (\\epsilon, T]\n\\end{cases}$ (7)\nor\n$f_{\\theta}(x, t) = C_{skip}(t) x + C_{out}(t) F_{\\theta}(x, t)$ (8)\nwhere $C_{skip}(t)$ and $C_{out}(t)$ are differentiable functions such that $C_{skip}(\\epsilon) = 1$ and $C_{out}(\\epsilon) = 0$."}, {"title": "LCM-LORA", "content": "Due to the efficiency of latent space model compared to pixel-based models, Latent Consistency Model (Luo et al. 2023a) has been proposed, making use of pretrained encoder and decoder to transform images from pixel space to latent space. Latent Consistency Model is parameterized as:\n$f_{\\theta}(z, c, t) = C_{skip}(t) z + C_{out}(t)(\\frac{z - \\sqrt{1 - \\bar{\\alpha}_t}\\hat{\\epsilon}_\\theta(z, c, t)}{\\sqrt{\\bar{\\alpha}_t}})$ (9)\nHowever, the style LoRAs trained on SDs cannot be used on LCM. Hence, LoRA is introduced to be used on LCM training, the idea of LoRA is separating model parameters into fixed $W_0$ and low rank matrix decomposition BA, where $B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$, and the rank $r < min(d, k)$. Training time and computation cost in distillation is reduced by hugely reducing trainable parameters. Also, style LoRA trained on SD can be used on LCM."}, {"title": "Method", "content": "In this paper, we aim to solve the slow purification problem in DBP by leveraging the LCM model as the purification backbone, enabling single-step adversarial image purification. However, diffusion models tend to produce images deviating from the originals when purification steps $t^*$ are large (Wang et al. 2022). To address this, we introduce Controlled Adversarial Purification (CAP), shown in Fig. 2, which utilizes ControlNet and edge detection of the original image to guide the purification process.\nFurthermore, recognizing that adversarial noise differs from the Gaussian noise that diffusion models are typically designed to remove, we propose Gaussian Adversarial Noise Distillation (GAND). This novel LCM distillation method specifically targets adversarial noise, enhancing the purification performance. Our approach builds upon the insight that combining adversarial purification with adversarial training can yield superior results (Liu et al. 2024), effectively addressing the distinct distributions of Gaussian and adversarial noise.\nOne Step Control Purification\nProblem Definition Our goal can be formulated as:\n$x_{gt} = x_{adv} = D(f(\\mathcal{E}(x_{adv}), t^*)) \\\\ s.t. \\quad C(x_{gt}) = C(x_{adv})$ (10)\nwhere $x_{gt}$ is ground truth image, $x_{adv}$ is adversarial image, f is a LCM with the purification function in latent space, $t^*$ is predefined purification step, $\\mathcal{E}$ and D are pretrained image encoder and decoder respectively. For simplifying symbolic, we denote $\\mathcal{E}(x_{adv})$ as $Z_{adv}$.\nOverview of our method OSCP is illustrated in Fig. 2, OSCP can be separated into two components: a) purification process with nontext guidance and b) training a backbone model with noise and denoise function. We propose Controlled Adversarial Purification (CAP) for a) and Gaussian Adversarial Noise Distillation (GAND), for b).\nControlled Adversarial Purification\nTo respond to the first component we defined, we propose CAP as Fig. 3, a purification method making use of ControlNet, instead of using image captioning models to generate text prompt guidance, since there exists attack method called Caption semantic attack (Xu et al. 2019), the accuracy of text prompt guidance is generated by those models are questionable. Hence, we prefer to use a traditional and robust method, using an edge detector to get an edge image for guidance.\nIn purification process, we first encode the adversarial image $z_{adv} = \\mathcal{E}(x_{adv})$ to latent space using pre-trained image encoder $\\mathcal{E}$ and sample a random noise $\\epsilon \\sim N(0, I)$ in the dimension of the latent space. Then, we diffuse the $z_{adv}$ with predefined strength $t^*$, using forward latent diffusion process:\n$Z_{adv}(t^*) = \\sqrt{\\bar{\\alpha}(t^*)} z_{adv} + \\sqrt{1 - \\bar{\\alpha}(t^*)} \\epsilon$ (11)\nThen, we purify $z_{adv}(t^*)$ using our LCM trained by GAND, will be discussed in the next subsection, $f_{\\theta}(z, c, t)$, z is image latent, c is the condition embedding (e.g., text, canny edge image) and t is time step. The latent consistency function has been introduced in E.q. 9. The purified image latent comes from the latent consistency function, $z_{0}^{adv}$ ="}, {"title": "Algorithm 1: GAND", "content": "Require: dataset X, class label of the image y, classifier C', latent consistency model $f_{\\theta}(\\cdot, \\cdot, \\cdot)$ and it's parameter $\\theta$, Cross Entropy Loss $L(\\cdot, \\cdot)$, ODE solver $(\\cdot, \\cdot, \\cdot, \\cdot)$ and distance metric $d(\\cdot, \\cdot)$. $L_C$ and $L_G$ indicate LCIG (E.q. 18) and $L_{GAND}$ (E.q. 19) respectively.\nwhile not convergence do\nSample $x \\sim X$, $n \\sim U[1, (N \u2212 k)/2]$\nz = $\\mathcal{E}(x)$\n$z_{n+k} = \\sqrt{\\bar{\\alpha}_{t_{n+k}}} z + \\sqrt{1 - \\bar{\\alpha}_{t_{n+k}}} \\epsilon + \\delta_{adv})$ (E.q. 15)\n$\\hat{z}_{t_n} = \\psi(\\hat{z}_{n+k}, t_{n+k}, t_n, \\O)$ (E.q. 17)\n$L_G(\\theta, \\theta^-) \\leftarrow d(f_{\\theta}(z_{t_{n+k}}, \\O, t_{n+k}), f_{\\theta^-}(\\hat{z}_{t_n}, \\O, t_n))$\n$L_C(\\theta) \\leftarrow d(f_{\\theta}(z_{t_{n+k}}, \\O, t_{n+k}), z)$\n$L_T(\\theta, \\theta^-) \\leftarrow L_G(\\theta, \\theta^-) + \\lambda_{CIG} L_C(\\theta)$ (E.q. 20)\n$\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L(\\theta, \\theta^-)$\nend while\n$f_{\\theta}(z_{adv}(t^*), C_{ce}, t^*)$, where $C_{ce}$ means canny edge images which are provided by edge detection operators (Canny 1986), although ControlNet is an extra plug-in tool for Stable Diffusion, $C_{ce}$ is not exactly an input of $f_{\\theta}$ but we treat it as condition embedding here for simplifying the equation. To further reduce the effect of the adversarial image, we remove the $C_{skip}(t)z_{adv}(t)$ in LCM $f_{\\theta}$ and denote this LCM as $f_{\\theta^-}$, since this term will maintain most of the adversarial noise. Therefore, our purified image latent is actually:\n$\\hat{z}_{adv} = f_{\\theta^-} (z_{adv}(t^*), C_{ce}, t^*)$ $= \\frac{C_{out}(t)}{\\sqrt{\\bar{\\alpha}_t}}(z_{adv} - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{\\epsilon}(z_{adv}, C_{ce}, t))$ (12)\nSame as setting $C_{skip}(t) = 0$. Finally, We reconstruct the purified image by the image decoder $\\mathcal{D}$, $\\hat{x}_{adv} = \\mathcal{D}(\\hat{z}_{adv})$.\nGaussian Adversarial Noise Distillation\nTo respond to the second component we defined, we propose GAND as Fig. 4, an adversarial distillation framework, which combines adversarial training, Latent Consistency Distillation, and learning how to remove the adversarial and Gaussian noise from the shifted standard Gaussian distribution.\nThe original LCM distillation first encodes the image to latent code by encoder $\\mathcal{E}$, such that $z = \\mathcal{E}(x)$ and trains the model base on loss function:\n$L(\\theta,\\theta^-;\\Psi) = \\mathbb{E}_{z, c, n}[d(f_\\theta(z_{t_{n+k}}, c, t_{n+k}), f_{\\theta^-}(z_{t_n}, c, t_n))]$ (13)\nwhere d(, ) is distance metric, $\\Psi(\\cdot, \\cdot, \\cdot, \\cdot)$ is DDIM PF-ODE solver $\\Psi_{DDIM}$ (Luo et al. 2023a), $f_{\\theta}(,,)$ is latent consistency function in E.q. 9 and $\\hat{z}_{t_n}$ is an estimation of the solution of the PF-ODE from $t_{n+k} \\to t_n$ using the DDIM PF-ODE solver $\\Psi$:\n$z_{t_n} \\leftarrow \\hat{z}_{t_{n+k}} + (1 + \\omega)\\Psi(\\hat{z}_{t_{n+k}}, t_{n+k}, t_n, c) - \\omega \\Psi(\\hat{z}_{t_{n+k}}, t_{n+k}, t_n, \\O)$ (14)"}, {"title": "Experiments and Results", "content": "Training setting We distill our adversarial latent consistency model based on SD v1.5 (Rombach et al. 2022), which is trained on resolution 512\u00d7512 with e-Prediction (Ho, Jain, and Abbeel 2020). We train our GAND weights with 20,000 iterations with the last 40,000 images from the validation set of ImageNet. The training batch size is 4, with a learning rate of 8e-6 and a warm-step of 500. For PF-ODE solver $\\Psi$ and skipping step k in E.q. 17, we use DDIM-Solver (Song, Meng, and Ermon 2021) with skipping step k = 20. Since our training data is not original in the resolution 512x512, we resize the images to resolution 512\u00d7512; the adversarial image latent is generated from the clean image latent using PGD-10, with budget 0.03, where the victim model is ResNet50. We set the $\\lambda_{CIG}$ as 0.001. Additionally, our experiment shows that PEFT, i.e., LoRA, is enough.\nAttack setting We evaluate our method on the first 10,000 images of validation set ImageNet (Deng et al. 2009), since we use the other 40,000 images of validation set to distill our model. We consider various architectures, including ResNet50 (He et al. 2016), ResNet152, WideResNet(WRN) (Zagoruyko and Komodakis 2016), and Vision Transformers like ViT-b (Dosovitskiy et al. 2021) and Swin-b (Liu et al. 2021). For attack methods, we consider Lp attacks including PGD (Madry et al. 2018) and AutoAttack (Croce and Hein 2020). For PGD attacks, we use $L_\\infty$ norm with bounds \u03b3 = 4/255 and 16/2555, attack step size \u03b7 = 1/255 and 0.025*16/255. For AutoAttack, we use $L_\\infty$ norm with \u03b3 = 4/255. In this paper, we denote standard accuracy as testing our framework on clean data, robust accuracy as testing our method on attacked data and clean accuracy as accuracy on clean data of the classifiers without defense. w/o in the tables means no defense is used. PGD-n, n means the number of iterations.\nMain Result\nWe test our model on the 50k ImageNet validation set, which has 1000 classes. Since our method is trained on resolution 512x512, and does not fit the resolution of classifiers that we use, which are 224. Hence, we resize the image before and after purification step to solve the image size conflict. We take 3 hours for an experiment on an NVIDIA F40 GPU on our method and around 2 days for GDMP (Wang et al. 2022). As shown in Tab. 1, our method outperforms"}, {"title": "Face Recognition", "content": "We further test our model on a subset of 1000 images on CelebA-HQ dataset (Liu et al. 2015), choosing purification step t* as 200, control scale 0.8 and borrowing the GAND weight train on ImageNet. We are defending a target PGD attack; the attack is letting Arcface (Deng et al. 2019), FaceNet (Schroff, Kalenichenko, and Philbin 2015), and MobileFaceNet (Chen et al. 2018) recognize any people as a specific person. Our defense goal is to make models not recognize any people as that person, which means the cosine similarity of the embedding of the purified image and the target image is lower than the threshold. Additionally, we show that the GAND weight train on ImageNet is suitable for CelebA-HQ by runing CAP, which is OSCP without GAND in the second last row in Tab. 4. Our GAND weights boost the robust accuracy from 83.4% to 86.8% and 82.8% to 84.9% on Arcface and Mobile face respectively. Also, our methods (CAP and OSCP) have the same robust accuracy 97.8% on FaceNet which is extremely high.\nAblation Studies\nWe conduct ablations on a subset of 500 images of the validation set on ImageNet, we test CAP on different Controlnet conditioning scale in Fig. 5, and we find out that CAP performs the best at 0.8 conditioning scale. In addition, we demonstrate the effect of CAP and GAND in Tab. 5. When none of our methods are applied (row 1), which means using the original LCM-LORA, the standard and robust accuracy"}, {"title": "Discussion and Conclusion", "content": "In this work, we propose OSCP, an edge image controlled adversarial purification, combining with adversarial trained LCM-LORA, which is trained by GAND, achieving superior efficiency. However, the purification step is still found by trial and error which is time consuming, this is another long-standing problem, finding adaptive purification strength might be a future breakthrough in diffusion-based purification methods. In summary, our contribution includes finding a rapid purification method and a novel controlled adversarial purification framework."}, {"title": "Appendix", "content": "Additional implementation details\nWe implement our method with Pytorch (Paszke et al. 2019) and Diffusers (von Platen et al. 2022). we fix the random seed of PyTorch's generator as 100 for the reproducibility (Picard 2021).\nFor all implementation, we use the same version of AutoAttack, which is same in both main paper and the appendix.\nWe leverage LCM-LoRA\u00b9 (Luo et al. 2023b) and TCD-LoRA2 (Zheng et al. 2024) from their HuggingFace repositories.\nIn terms of training with our proposed GAND, we use the Stable Diffusion v1.53 (SD15) as the teacher mdoel.\nWe borrow a part of code and pretrained weights from AMT-GAN4 when we do experiments on CelebA-HQ.\nUnless mentioned, all reproducibility-related things follows the above.\nEdge detector\nFor someone who may be concerned, edge detectors usually consider high-frequency pixels as edges; then, if we use those edge images for ControlNet, OSCP will reserve those high-frequency pixels in the purified image. To avoid this problem, we extra blur the images before they are inserted into the edge detector. In Fig. 6, the edge image on the right side, conducted by an adversarial image without blur, is full of noise. Meanwhile, the edge image in the middle is connected by the adversarial image with extra blur, which has most of the contour of the clean image on the left side.\nProofs\nWe are going to provide some simple proofs for things we have claimed, including\n1. z \u2192 z when t \u2192 0,\n2. z \u2192 $ \\epsilon + \\delta_{adv}$ when t \u2192 T\n3. $f_{\\theta}(z_{adv}(t), \\O, t) \\rightarrow z_{adv}$ when t \u2192 0\n4. $f_{\\theta}(z_{adv}(t), \\O, t) - f_{\\theta^-}(z(t), \\O, t) \\rightarrow \\delta_{adv}$ when t \u2192 0\nLemma. If X ~ N(\u03bc, \u03c3\u00b2) and \u03c3\u00b2 \u2192 0, then X \u2192 \u03bc\nProof. For any \u20ac > 0,\nP(||X \u2013 \u03bc|| \u2265 \u20ac) = P(||Z|| \u2265 \u20ac) Z~N(0,\u03c3\u00b2)\n$\\frac{E(X^2)}{\\epsilon^2} \\leq $ Markov's inequality\n$\\frac{Var(X) + (E(X))^2}{\\epsilon^2} = \\frac{\\sigma^2}{\\epsilon^2}  \\to 0$ (22)"}, {"title": "Experiment", "content": "Proof 1. $\\beta_t$ is increasing sequence in t \u2208 {0", "1,\u2026\u2026\u2026": "T \u2212 1", "0,1,\u2026\u2026": "T \u2212 1", "number": "n$\\lim_{t\\to 0} z = \\lim_{t\\to 0} \\sqrt{\\bar{\\alpha}_t}z + \\sqrt{1 - \\bar{\\alpha}_t}(\\epsilon + \\delta_{adv})$\n$= \\sqrt{\\alpha_0}z + \\sqrt{1 - \\alpha_0}(\\epsilon + \\delta_{adv})$\n$= \\sqrt{1 - \\beta_0}z + \\sqrt{\\beta_0}\\epsilon + \\sqrt{\\beta_0}\\delta_{adv}$\n$= z + \\sqrt{ \\beta_0} \\epsilon$ by assumption on $\\beta_0$ (23)\nsince we know $z + \\sqrt{ \\beta_0} \\epsilon \\sim N(z, \\beta_0I)$ and $\\beta_0$ is vanishing. By lemma, we have z  z.\nProof 2. $\\beta_t$ is increasing sequence in t \u2208 {0,1,..., T \u2013 1, T} in range (0,1), then we have $\\alpha_t$ is decreasing sequence in t \u2208 {0,1,\u2026\u2026,T \u2013 1,T} in range (0,1), $\\bar{\\alpha}_t$ is decreasing step function in range (0, 1), further assume T is a arbitrarily large number, consider $\\delta_{adv}$ as constant.\n$\\bar{\\alpha}_T = \\prod_{t=0}^{T} \\alpha_t = \\prod_{t=0}^{T} (1 - \\beta_t) \\leq (1 - \\beta_0)^{T+1} \\rightarrow 0$ by assuming T is arbitrarily large\n$\\lim_{t\\to T} z = \\lim_{t\\to T} \\sqrt{\\bar{\\alpha}_t}z + \\sqrt{1 - \\bar{\\alpha}_t} (\\epsilon + \\delta_{adv})$\n$= \\epsilon + \\delta_{adv} \\sim N(\\delta_{adv}, I)$ (24)\nProof 3. $\\beta_t$ is increasing sequence in t \u2208 {0,1,\u2026\u2026,T \u2013 1, T} in range (0, 1), then we have $\\alpha_t$ is linear decreasing sequence in t \u2208 {0,1,.\u2026\u2026,T\u22121, T} in range (0, 1), $\\bar{\\alpha}_t$ is decreasing step function in range (0, 1), further assume $\\hat{\\epsilon}$ has standard Gaussian distribution and $\\beta_0$ is a arbitrarily small number.\n$\\lim_{t\\to 0} f_{\\theta^-} (z_{adv}(t), \\O, t) = \\lim_{t\\to 0} \\frac{\\sigma_t^2}{\\sqrt{t^2 + \\sigma_t^2}} z_{adv}(t)+ \\frac{t^2}{\\sqrt{t^2 + \\sigma_t^2}} (\\frac{z_{adv}(t) - \\"}]}