{"title": "Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models", "authors": ["Badaru I. Olumuyiwa", "The Anh Han", "Zia U. Shamszaman"], "abstract": "This research presents an innovative approach to cancer diagnosis and prediction using explainable Artificial Intelligence (XAI) and deep learning techniques. With cancer causing nearly 10 million deaths globally in 2020, early and accurate diagnosis is crucial. Traditional methods often face challenges in cost, accuracy, and efficiency. Our study develops an Al model that provides precise outcomes and clear insights into its decision-making process, addressing the \"black box\" problem of deep learning models. By employing XAI techniques, we enhance interpretability and transparency, building trust among healthcare professionals and patients. Our approach leverages neural networks to analyse extensive datasets, identifying patterns for cancer detection. This model has the potential to revolutionise diagnosis by improving accuracy, accessibility, and clarity in medical decision-making, possibly leading to earlier detection and more personalised treatment strategies. Furthermore, it could democratise access to high-quality diagnostics, particularly in resource-limited settings, contributing to global health equity. The model's applications extend beyond cancer diagnosis, potentially transforming various aspects of medical decision-making and saving millions of lives worldwide.", "sections": [{"title": "I. INTRODUCTION", "content": "Cancer is one of the leading causes of death globally, responsible for nearly 10 million deaths in 2020 [1]. Early detection and accurate diagnosis are crucial for improving patient outcomes and survival rates. However, traditional diagnostic methods often face challenges in cost, accuracy, and efficiency [3]. Whilst cancer has been a formidable challenge to human health throughout history, our understanding and treatment methods have evolved significantly. The discovery of X-rays in the late 1800s marked a turning point, paving the way for modern medical oncology [2]. Today, breast, lung, colorectal, prostate, and skin cancers are the most common types. Various factors influence cancer rates, including age, gender, race, ethnicity, lifestyle choices, environment, genetics, and healthcare access [1]. Traditional cancer detection and treatment methods, such as medical imaging, chemotherapy, and surgery, whilst advanced, still have drawbacks in terms of cost, accuracy, and side effects [3]. Artificial intelligence (AI) has the potential to address these issues and revolutionise healthcare, especially in cancer treatment. AI can analyse vast amounts of data, identify patterns, and offer insights to inform decision-making, potentially reducing human error, bias, and costs whilst improving healthcare efficiency [4]. Deep learning, in particular, has shown remarkable accuracy in areas like genetic analysis, image analysis, and clinical decision support for cancer diagnosis and prognosis [5]. However, using AI in healthcare comes with challenges, particularly regarding the lack of transparency and interpretability in many Al models [6][7]. This \"black box\" nature can undermine trust and hinder adoption in clinical settings. Our research project seeks to address this issue by developing a reliable, interpretable AI model for cancer diagnosis and prediction using state-of-the-art deep learning techniques and explainable AI (XAI) methods. By enhancing understanding of Al predictions, we aim to improve patient and healthcare provider trust in AI-assisted clinical decision-making [8][9], potentially transforming cancer diagnosis and prediction, and ultimately saving lives and improving patient outcomes."}, {"title": "II. METHODOLOGY: SYSTEM DESIGN, ARCHITECTURE AND IMPLEMENTATION", "content": "In this section, we describe the system design, architecture, and implementation of the proposed AI system, which aims to achieve predictability, explainability, and interpretability"}, {"title": "A. System Requirements", "content": "We have conducted extensive research to identify the system requirements, which include:\nDataset: Several sources of cancer-related data were evaluated in order to identify a relevant dataset from Kaggle [10], which provides a large number of potentially helpful datasets. Since data is essential to a trained model's ability to make predictions and produce outcomes, the features required for the proposed Al system's performance were sought after [11]. According to [12], a less biased dataset was sought to guarantee fairness during training and enhance the model's robustness and generalization.\nThe data's privacy and security concerns were also taken into account. The implementation of robust data privacy and security measures is not only an ethical imperative but also a legal requirement in many jurisdictions."}, {"title": "1. GDPR Compliance", "content": "The General Data Protection Regulation (GDPR) sets a high standard for data protection in the European Union and has become a global benchmark. Our system adheres to key GDPR principles, including:\n\u2022 Data Minimization: Only the data necessary for cancer diagnosis are collected and processed, avoiding extraneous information.\n\u2022 Purpose Limitation: Patient data is used exclusively for the stated purpose of cancer diagnosis and related research.\n\u2022 Storage Limitation: We implement strict data retention policies, securely deleting data when it's no longer needed.\n\u2022 Transparency: Clear communication with patients about data usage and AI involvement in diagnosis.\n\u2022 Right to Erasure: Implementing mechanisms for patients to request deletion of their data, where applicable."}, {"title": "2. HIPAA Compliance", "content": "For deployments in the United States, our system is designed to meet the stringent requirements of the Health Insurance Portability and Accountability Act (HIPAA), including:\n\u2022 Privacy Rule: Ensuring proper use and disclosure of protected health information (PHI).\n\u2022 Security Rule: Implementing appropriate administrative, physical, and technical safeguards.\n\u2022 Breach Notification Rule: Establishing protocols for timely notification in case of data breaches."}, {"title": "3. Technical Measures for Data Privacy and Security", "content": "Encryption Methods: To protect data both at rest and in transit, state-of-the-art encryption techniques were employed:\n\u2022 Data Store: Utilization of AES-256 encryption for stored data. Implementation of secure key management practices, including regular key rotation.\n\u2022 Data in Transit: Employment of TLS 1.3 protocols for all data transfers. Use of secure APIs with proper authentication mechanisms."}, {"title": "B. Data Pre-processing and Feature Engineering", "content": "Pre-processing and feature engineering of the data are essential for ensuring that it is error-free and algorithm-compatible during dataset analysis and training [11]. New elements that might improve the training process were created, and the ones that are unnecessary for the model training were eliminated. Along with filling up the dataset's missing values, the data types of the features were examined, and some columns underwent categorization. To improve the model's performance when trained on the data, the dataset's normality was also examined and adjusted. The performance of the model depends on this prerequisite."}, {"title": "C. Deep Learning Model", "content": "The algorithm used to handle the pre-processed data is referred to in this requirement [11]. Similar to how the human brain functions, it matches patterns in neural networks and learns from them. It is made up of learning neurons as well as additional layers of neurons also referred to as the model's hidden layers beyond the input and output layer. Deep learning algorithms come in a variety of forms, and some of them will be applied in this project.\nThe relatively limited size of our dataset, comprising 569 cases, presents a substantial risk of overfitting. This phenomenon occurs when a model excessively adapts to the training data, including its inherent noise and fluctuations, resulting in poor generalization performance on novel, unseen data.\nTo mitigate this risk, we implemented the following techniques:\n\u2022 Data Augmentation: This represents a fundamental approach to artificial dataset expansion, serving as a critical regularisation technique in deep learning applications. This approach involves the creation of synthetic examples through the application of transformations to existing data points. In the context of our study, this could encompass minor rotations or scaling of cell images. Data augmentation effectively expands the training set, potentially enhancing the model's ability to generalize.\nIn the context of image processing tasks, these transformations typically manifest as geometric manipulations (e.g., rotations, translations, and scaling operations) or photometric adjustments (including variations in brightness, contrast, and colour balance). The theoretical underpinning of this approach lies in the assumption that such transformations preserve class-relevant information whilst introducing beneficial variability into the training process. The efficacy of this technique stems from its ability to:\n\u039f Reduce overfitting by exposing the model to a broader range of valid input variations\n\u039f Enhance invariance to specific transformations relevant to the task domain\n\u039f Mitigate class imbalance issues through targeted augmentation of underrepresented classes\n\u2022 Dropout Layers: This technique involves the random \"dropping out\" or deactivation of a proportion of neurons during the training process. Dropout serves to prevent co-adaptation of feature detectors and has been demonstrated to improve model generalization. The optimal dropout rate will be determined through empirical testing.\nDropout represents a sophisticated regularisation technique that has garnered significant attention in the deep learning community since its introduction by [31]. This approach involves the stochastic omission of neural units during the training phase, effectively creating an ensemble of subnetworks within the primary architecture.\nThe mathematical framework underlying dropout can be expressed as:\n$y = f(Wx)m$,\nwhere\nm ~ Bernoulli(p)\nHere, m represents a binary mask sampled from a Bernoulli distribution with parameter p, typically set between 0.2 and 0.5. This formulation results in several theoretical advantages:\n\u039f Prevention of Feature Co-adaptation: By randomly deactivating neurons, dropout disrupts the formation of excessive co-dependencies between neural units\n\u039f Implicit Ensemble Learning: The technique effectively trains an ensemble of 2^n thinned networks, where n represents the number of units subject to dropout\n\u039f Reduced Overfitting: The stochastic nature of dropout introduces beneficial noise into the training process, enhancing generalisation capabilities\n\u2022 Regularization: The incorporation of L1 or L2 regularization terms in the model's objective function can effectively penalize overly complex models. This approach encourages the development of simpler, more generalizable solutions by adding a cost associated with large weights in the model.\n\u2022 Early Stopping: Early stopping constitutes a pragmatic approach to optimisation control in neural network training. This methodology involves the continuous monitoring of model performance on a validation dataset, with training termination occurring when generalisation performance begins to deteriorate. The theoretical justification for early stopping stems from the observation that neural networks typically exhibit distinct phases during training:\n\u039f Initial Learning Phase: Characterised by rapid improvement in both training and validation performance\n\u039f Optimal Generalisation Point: Where validation performance reaches its peak\n\u039f Overfitting Phase: Marked by continued improvement in training performance but degradation in validation metrics\nImplementation typically involves:\n\u039f Performance Monitoring: Regular evaluation of model performance on a held-out validation set\n\u039f Stopping Criterion: Definition of specific conditions that trigger training termination\n\u039f Model Selection: Retention of the model state that achieved optimal validation performance"}, {"title": "D. Explainable AI Frameworks/Modules", "content": "An essential prerequisite for analysing the inner workings of the black box model is Explainable AI Frameworks/Modules [8]. It will be applied to interpret and elucidate the reasons behind the predictions made by the different models that will be employed. This regulation will resolve these kinds of problems. For this research to be successful, cutting-edge XA\u0399 frameworks are required [12]."}, {"title": "1. SHAP (SHapley Additive exPlanations)", "content": "SHAP, rooted in cooperative game theory, quantifies the contribution of each feature to the prediction for individual instances. This method is based on Shapley values, a concept from game theory that fairly distributes payout among players in a cooperative game."}, {"title": "a. Methodology", "content": "SHAP evaluates all possible combinations of features and calculates the marginal contribution of each feature to the difference between the actual prediction and the average prediction. This comprehensive approach ensures a robust and theoretically grounded assessment of feature importance. To properly understand this, let's delve into its inner workings:\nThe Mathematics Behind SHAP\nIn SHAP's framework, we treat feature attribution as a cooperative game where:\n\u2022 The \"players\" are individual features in our dataset\n\u2022 The \"game\" is the prediction task\n\u2022 The \"payout\" is the difference between the model's prediction and the average prediction\nThe Shapley value for a feature i is calculated as:\n$\\Phi_i = \\Sigma (|S|! (n - |S| - 1)!/n!) [f_x(S\\cup {i}) - f_x(S)]$ (1)\nWhere:\n\u2022 S represents all possible subsets of features excluding feature i\n\u2022 n is the total number of features\n\u2022 $f_x(S)$ is the model's prediction with only the features in subset S\nThe Coalitional Computation Process\n\u2022 SHAP examines every possible coalition (combination) of features\n\u2022 For each feature, it calculates:\n\u039f The model's prediction with the feature present\n\u039f The model's prediction with the feature absent\n\u039f The weighted difference between these predictions\n\u2022 This process is repeated across all possible feature combinations"}, {"title": "b. Contributions to Model Interpretability", "content": "Dual-level Explanations: SHAP provides both global and local interpretability. At the global level, it offers insights into overall feature importance across the dataset. At the local level, it elucidates the impact of features on individual predictions.\n\u2022 Theoretical Soundness: The method's foundation in game theory lends it a strong theoretical basis, ensuring consistency and reliability in its explanations.\n\u2022 Non-linear Relationship Handling: SHAP effectively captures and explains complex, non-linear relationships between features and model outputs."}, {"title": "2. LIME (Local Interpretable Model-agnostic Explanations)", "content": "LIME focuses on creating locally interpretable models to explain individual predictions of black-box models. This approach aims to approximate the behavior of complex models in local regions around specific predictions."}, {"title": "a. Methodology", "content": "LIME operates by perturbing the input data and observing the corresponding changes in model predictions. It then fits a simple, interpretable model (e.g., linear regression) to this local region, providing an approximation of the complex model's behavior in the vicinity of the prediction of interest. Here's a detailed examination of its methodology:\nThe Technical Implementation\n\u2022 Sampling and Perturbation:\n\u039f Generate synthetic samples around the instance of interest\n\u039f Apply small perturbations to feature values\n\u039f Weight samples based on their proximity to the original instance\nLocal Model Fitting:\nThe algorithm fits an interpretable model g to minimise:\n$argmin_{g\\in G} L(f,g,\\pi_x) + \\Omega(g)$ (2)\nWhere:\n\u2022 f is the complex model\n\u2022 g is the interpretable model\n\u2022 $\\pi_x$ is the locality weighting kernel\n\u2022 $\\Omega(g)$ is a complexity penalty\nThe Mathematical Framework\nLIME employs an exponential kernel for similarity weighting:\n$\\pi_x(z) = exp(-D(x,z)^2/\\sigma^2)$ (3)\nWhere:\n\u2022 x is the original instance\n\u2022 z is the perturbed instance\n\u2022 D is the distance function\n\u2022 $\\sigma$ is the kernel width parameter"}, {"title": "b. Contributions to Model Interpretability", "content": "Local Explanations: LIME excels in providing easily comprehensible explanations for individual predictions, enhancing understanding of specific model decisions.\n\u2022 Model Agnosticism: As a model-agnostic approach, LIME can be applied to any black-box model, offering flexibility across various ML architectures.\n\u2022 Intuitive Understanding: By approximating complex models with simpler, local models, LIME facilitates an intuitive grasp of model behavior in specific instances."}, {"title": "3. Eli5 Permutation Importance", "content": "Eli5 is a Python library that provides various tools for model interpretation, including Permutation Importance, which is a model-agnostic method for determining feature importance."}, {"title": "a. Methodology", "content": "Permutation Importance works by randomly shuffling the values of each feature and measuring the resulting decrease in model performance. The features that, when shuffled, cause the largest decrease in performance are considered the most important."}, {"title": "b. Contributions to Model Interpretability", "content": "Global Feature Importance: Eli5's Permutation Importance provides a clear ranking of feature importance at the global level, helping to identify which features have the most significant impact on the model's predictions overall.\n\u2022 Model Agnosticism: Like LIME, this method can be applied to any type of model, making it versatile across different ML architectures.\n\u2022 Simplicity and Intuitiveness: The concept behind Permutation Importance is straightforward and easy to explain, making it accessible to non-technical stakeholders.\n\u2022 Computational Efficiency: Compared to SHAP, Permutation Importance is generally less computationally intensive, especially for large datasets or complex models."}, {"title": "E. Data Visualization", "content": "Data visualization is a crucial component of the model and XAI module results presentation, providing a graphical depiction of the entire outcome for effortless comprehension. Plots, histograms, heatmaps, and other visualizations [17] will be utilized to convey the insights gleaned from the datasets as well as the interpretability and model predictions."}, {"title": "III. SYSTEM ARCHITECTURE (DATA PRE-PROCESSING)", "content": null}, {"title": "A. Data Collection", "content": "Data collection is the process of gathering diverse cancer-related data from multiple sources, including genetic and clinical data. It's critical to compare several cancer-related datasets and choose the one that is more authentic than synthetic while selecting the ideal dataset for the project. Data pertaining to cancer may comprise details regarding the patient's characteristics, medical background, diagnosis, course of therapy, results, and tumour's molecular makeup. While the Kaggle dataset utilized in this study provides a valuable foundation for our research, it is important to acknowledge its inherent limitations:\n\u2022 Potential Sampling Biases: The dataset may not be fully representative of the broader population, potentially leading to biased results and limited generalizability of the model.\n\u2022 Presence of Synthetic Data: A portion of the dataset may consist of artificially generated data. While useful for augmenting dataset size, synthetic data may not fully capture the nuances and complexities present in real-world clinical cases.\n\u2022 Limited Demographic Diversity: The dataset may lack sufficient variability in terms of age, gender, ethnicity, and other demographic factors. This limitation could restrict the model's ability to perform consistently across diverse population groups."}, {"title": "1. Proposed Strategies to Address Limitations", "content": "To mitigate these limitations and enhance the robustness and generalizability of our model, we propose the following strategies:\n\u2022 Utilization of Diverse Real-World Clinical Datasets: Incorporating data from multiple, diverse clinical sources would provide a more comprehensive and representative sample, reducing potential biases and improving the model's real-world applicability.\n\u2022 Collaboration with Healthcare Institutions: Establishing partnerships with hospitals and other healthcare providers would facilitate access to more extensive and diverse clinical data. This collaboration could also provide valuable domain expertise to guide data collection and model development.\n\u2022 Integration of Comprehensive Demographic Information: Explicitly incorporating a wide range of demographic variables into the dataset and model training process would help ensure the model's performance is consistent and reliable across different population subgroups."}, {"title": "B. Data Cleaning", "content": "Data cleaning is the process of removing abnormalities and superfluous information from the dataset and completing any gaps in the information. Additionally, it is crucial to ensure that the data type of each column is proper and that categorical data is represented accurately. Data purification, an essential step to prevent errors and bias in the evaluation, ensures the accuracy and dependability of the data. Imputation, normalization, transformation, and standardization are a few examples of techniques that can be used to purify data [18].\n\u2022 Imputation: Imputation is the process of substituting values for missing data. The mean or median can be used to numerical data. It is possible to utilize the mode for categorical data. The mean imputation formula, given X as the dataset and m as the mean, is as follows:\n$X_{imputed} = {X if X_{is \\ not \\ missing}, m \\ if X_{is \\ missing}}$ (1)"}, {"title": "C. Normalization", "content": "In order to prevent data from one column from dominating other columns, normalization is the act of converting numerical data from various columns to a similar scale. The normalization formula (min-max normalization) is as follows:\n$X_{normalized} = \\frac{X-X_{min}}{X_{max}-X_{min}}$ (2)"}, {"title": "D. Standardization", "content": "Another scaling technique is standardization, which involves scaling the data to have a mean of 0 and a standard deviation of 1. Z-score normalization (Standardization) can be performed using the following formula:\n$X_{standardization} = \\frac{X-\\mu}{\\sigma}$ (3)\nwhere u is the mean and o is the standard deviation."}, {"title": "E. Transformation", "content": "To change the data, a function must be applied to each data point in a column. For instance, to make skewed data more normally distributed, a logarithmic treatment could be used. The transformation formula, if f is the transformation function, is as follows:\n$X_{transformed} = F(X)$ (4)"}, {"title": "IV. FEATURE EXTRACTION AND ENGINEERING", "content": "In the pipeline of data preparation, this is a crucial stage. It entails both the creation of useful characteristics that can enhance a model's performance and the removal of superfluous features that lower the model's capacity for prediction. This procedure can be broken down into multiple essential techniques:"}, {"title": "A. Dimensionality Reduction", "content": "This method seeks to lower a dataset's input variable count. Principal Component Analysis (PCA) is a popular technique that converts the data into a new coordinate system so that, by any projection of the data, the largest variance lies on the first coordinate (referred to as the first principal component), the second largest variance on the second coordinate, and so to speak. The PCA formula is:\nY = X.V (5)\nX is the standardized original data, V is the matrix of eigenvectors of the covariance matrix of X, Y is the matrix of principal components (i.e., the transformed data)."}, {"title": "B. Dimensionality Reduction", "content": "The process of choosing the most pertinent features for model training is known as feature selection. The chi-square test, correlation coefficient methods, and mutual information are some of the techniques. As an illustration, the Pearson correlation coefficient formula is:\n$\\gamma_{xy} = \\frac{\\Sigma_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\Sigma_{i=1}^n (x_i-\\overline{x})^2 \\Sigma_{i=1}^n (y_i-\\overline{y})^2}}$ (6)\nxi and yi are the individual sample points indexed with i, x and y are the means of x and y respectively."}, {"title": "C. Feature Transformation", "content": "Feature transformation is the process of altering data to raise the algorithm's accuracy. Frequently employed techniques encompass binning, scaling, and merging current features to generate novel ones."}, {"title": "D. Feature Generation", "content": "This process entails taking the current data and turning it into new features. Adding up \"bedrooms\" and \"bathrooms\" in a housing dataset to create a new feature named \"total rooms\" is a basic example of feature generation."}, {"title": "V. DEEP LEARNING MODEL", "content": null}, {"title": "A. Convolutional Neural Networks (CNNs)", "content": "Deep learning models known as convolutional neural networks (CNNs) are particularly good at training and categorizing picture data, although they may also be applied to text-based cancer datasets. According to [19], CNNs employ convolutional layers, which may extract features from images and minimize the number of parameters. According to [20] CNNS can be utilized for prognostic, segmentation, classification, and tumour detection tasks. Convolutional layers are a tool used by CNNs to extract features from input data. These layers generate several feature maps that each represent a distinct component of the input by applying a number of filters to the input. By doing this, the model's number of parameters is decreased, increasing its efficiency and decreasing the likelihood of overfitting. The formula for a convolution operation in a CNN is:\n$(I * K)[i, j] = \\sum_m \\sum_n I[m,n] . K[i \u2013 m, j \u2013 n]$ (7)\nI is the input, K is the kernel or filter, * denotes the convolution operation."}, {"title": "B. Explainable AI Model Integration", "content": "By utilizing sophisticated models that can elucidate the interpretability and explainability of the black box model, it would be possible to both demonstrate how the model functions and foster more trust in Al systems [21]. The field of explainable AI (XAI) aims to give explanations for the actions and results of Al systems, particularly those that are complex and enigmatic [12]. XAI can be used for tasks including understanding the logic behind the model, identifying the pertinent features, creating counterfactuals, and providing comments and recommendations [20]."}, {"title": "C. Hyperparameter Optimization", "content": "Hyperparameter optimization is the process of fine-tuning model parameters to get the best possible performance in model forecasts. A model is adjusted in a number of ways to improve accuracy, precision, and generalization. Hyperparameters, such as the activation function, learning rate, and number of hidden layers, are model configurations or options that are not obtained from the data [19]. The process of determining which combination of hyperparameters will improve the model's performance on a given job is known as hyperparameter optimization [20]."}, {"title": "VI. EXPLAINABILITY AND DATA VISUALIZATION", "content": null}, {"title": "A. Explainability Module", "content": "Using a combination of explainability techniques to improve the comprehension and explainability of model predictions. Techniques like saliency mapping, feature attribution, counterfactual explanations, and others may be used in these approaches [21]. The Explainability Module can help to improve the model's dependability and trustworthiness by helping to understand its activities and validate its results [20]."}, {"title": "B. Data Visualization Tools", "content": "Presenting diagnostic outcomes through interactive data visualization. The interpretability, model predictions, and insights from the data can all be communicated with the use of data visualization. Plots, histograms, heatmaps, and other similar visualization tools are a few examples [17]. Tools for data visualization can help analyse data, spot trends, compare results, and convey findings [20]."}, {"title": "C. Explainability AI Clinical Analysis and Use Case", "content": null}, {"title": "1. SHAP Analysis", "content": "Our SHAP analysis revealed that the top three features contributing to the model's decisions were:\n\u039f Mean cell size (average importance: 0.35)\n\u039f Nuclear texture (average importance: 0.28)\n\u039f Cell symmetry (average importance: 0.15)\nSHAP's game-theoretical approach provides particularly robust insights into feature interactions in tumour analysis. Our investigation demonstrated that mean cell size consistently emerged as the dominant feature, with an average importance value of 0.35. This aligns with established oncological principles regarding cellular morphology in malignant transformations. The particular strength of SHAP in this context lies in its ability to quantify how features work together. For instance, when examining cases where both nuclear texture (0.28) and cell symmetry (0.15) showed significant values, SHAP revealed important interaction effects that other methods missed. These interactions proved especially valuable in borderline cases where no single feature definitively indicated malignancy. These results align with clinical knowledge, as cell size and nuclear characteristics are known to be key indicators of malignancy [29]."}, {"title": "2. LIME Explanations", "content": "LIME analysis provided case-specific explanations. In a representative case study of a tumor classified as malignant, LIME identified:\n\u039f Large cell size (contribution: +0.4)\n\u039f Irregular nuclear texture (contribution: +0.3)\n\u039f Asymmetric cell shape (contribution: +0.1)\nLIME's approach to tumour analysis offers a distinctly different perspective. Rather than providing global feature rankings, LIME excels at explaining specific instances. In our representative case study, LIME's local approximation revealed how the model weighted different features for that particular patient:\nThe large cell size contribution (+0.4) was contextualised within the specific patient's tissue sample, making it particularly valuable for clinical discussions. LIME's ability to generate case-specific explanations proved especially useful during tumour boards, where specialists needed to understand the model's reasoning for individual patients."}, {"title": "3. ELI5 Analysis", "content": "The ELI5 library was used to generate simplified explanations for the model's predictions. For a representative case classified as malignant, ELI5 produced the following explanation:\nThe tumor is likely malignant because:\n\u2022 The cells are much larger than normal cells.\n\u2022 The cell nuclei have an unusual texture.\n\u2022 The cells are not as round and symmetrical as healthy cells.\nELI5's strength lies in its ability to translate complex model decisions into clinically relevant language. Its permutation-based approach offers a middle ground between SHAP's mathematical rigour and LIME's accessibility. The explanations it generates, such as \"The cells are much larger than normal cells\", provide immediately actionable insights that align with clinical training.\nThis explanation aligns with the SHAP and LIME results while providing a more intuitive, non-technical interpretation."}, {"title": "D. Explainability AI Practical Clinical Applications", "content": "The real value of these methods emerges in their complementary use. Consider a typical diagnostic workflow:\n1. Initial Screening: ELI5's straightforward explanations help during initial patient consultations, providing clear, understandable reasons for further investigation. While more generalised, proved especially valuable for training new staff and maintaining consistent diagnostic approaches across different departments.\n2. Detailed Analysis: SHAP's comprehensive feature interaction analysis supports detailed diagnostic discussions among specialists, particularly when examining complex cases. It requires more computational resources but provides the most comprehensive analysis for difficult cases. Its results often revealed subtle feature interactions that proved crucial in borderline cases.\n3. Patient Communication: LIME's case-specific explanations prove invaluable when discussing individual diagnoses with patients, offering clear, personalised explanations of the diagnostic reasoning. Its rapid analysis makes it particularly suitable for real-time clinical decision support, though its explanations sometimes oversimplified complex cases.\nHowever, it is crucial to note that while these explainability methods offer insights into the model's functioning, they should not be used as a sole basis for clinical decisions. Rather, they should be considered as a complementary tool to clinical expertise and other diagnostic methods [30]."}, {"title": "VII. CODE IMPLEMENTATION", "content": null}, {"title": "A. Data Loading and Pre-processing", "content": "Import Essential Libraries: Load the necessary libraries for data analysis, visualization, and pre-processing (e.g., pandas, NumPy, matplotlib, seaborn).\nRead Dataset: Read the \"Cancer data.csv\" file into a Pandas DataFrame named \"data\".\nExplore Data: Display the DataFrame's shape (number of rows and columns). Provide a statistical summary of numerical columns. Check for missing values and drop them. Map categorical values in the \u201cdiagnosis\u201d column to numerical values (1 for \"M\", 0 for \"B\"). Standardize numerical features using StandardScaler to have zero mean and unit variance.\nVisualize Distributions: Create a boxplot to visualize feature distributions and identify outliers. Create subplots to display histograms for each feature in a structured grid."}, {"title": "B. Feature Selection", "content": "Apply Chi-Squared Test: Use SelectKBest with the chisquared test to select the top 27 features. Store scores of each feature in \"kbest scores\". Store names of selected features in \"kbest features\". Apply Recursive Feature Elimination (RFE)\nUse RFE with a logistic regression estimator to recursively eliminate features until only 27 remain. Store feature rankings based on elimination in \u201crfe scores\". Store names of selected features in \"rfe features\". Apply Principal Component Analysis (PCA): Use PCA to reduce dimensionality to 27 components. Store explained variance ratio of each component in \u201cpca scores\". Store principal components in \"pca components\". Visualize RFE Scores: Plot a heatmap to visualize feature rankings after RFE elimination."}, {"title": "C. Machine Learning Model Development", "content": "Import Libraries: Import libraries for machine learning (e.g. scikit-learn), deep learning (TensorFlow, Keras).\nPrepare Data: Select features using \"rfe features\". Drop the \"id\" column. Reset the DataFrame index. Convert data into NumPy arrays.\nSplit Data: Split the dataset into training (80%) and testing (20%) sets using a random state of 42 for reproducibility.\nDefine MLP Model: Define a Multi-Layer Perceptron (MLP) model function that takes hidden layer sizes, activation function, dropout rate, and optimizer as input.\nDefine CNN Model: Define a 1D Convolutional Neural Network (CNN) model function that takes number of filters, kernel size, pool size, activation function, dropout rate, and optimizer as input.\nSet Algorithm Parameters: Define algorithms (MLP and CNN models) and their parameters for hyperparameter tuning.\nPerform Hyperparameter Tuning: Use Grid Search and Random Search to find the best hyperparameter combinations for each model.\nSelect Best Algorithm: Select the algorithm with the highest score based on hyperparameter tuning results.\nTrain Best Algorithm: Train the best-performing algorithm on the training set using early stopping and model checkpointing callbacks."}, {"title": "D. Model Evaluation", "content": "Evaluate Performance: Evaluate the trained model on the testing set using metrics such as accuracy, precision, recall, confusion matrix, ROC curve, and classification report. In the context of cancer diagnosis, the balance between precision and recall is of critical importance. Low recall, resulting in false negatives, can lead to missed cancer cases with potentially severe consequences, including delayed treatment, necessitation of more aggressive interventions, and compromised patient outcomes. To address this challenge, we propose a multi-faceted approach to optimize both precision and recall. This includes adjusting the classification threshold to prioritize recall without excessively compromising precision. Future research will focus on optimizing the classification threshold to maximize recall while maintaining acceptable precision levels. This will involve techniques such as utilizing ensemble methods to combine multiple models and leverage their collective strengths and precision-recall curve analysis to identify the optimal operating point. Visualize Results: Plot the ROC curve for the best algorithm and a random classifier for comparison. Visualize the confusion matrix using a heatmap."}, {"title": "VIII. ANALYSIS AND RESULTS", "content": "This study conducted an in-depth analysis of a breast cancer dataset comprising 569 cases, employing advanced machine learning techniques to enhance tumour classification accuracy. Our research yielded several significant findings:\nThe Convolutional Neural Network (CNN) architecture demonstrated superior performance compared to the Multilayer Perceptron (MLP), achieving a 92% classification accuracy. This result underscores the potential of deep learning approaches in medical image analysis.\nOn the 80% train dataset, the optimised CNN parameters exhibited exceptional precision (100%) and strong recall (83.7%), indicating its robust ability to correctly identify malignant cases while minimising false positives. Comprehensive analysis revealed that tumour size, shape, and texture characteristics were pivotal predictors in the classification process. This insight aligns with clinical understanding of tumour morphology and could inform future diagnostic criteria."}, {"title": "A. Dataset Overview", "content": "ID Column: Acting as a unique identifier", "id": "olumn separates each record within the dataset.\nDiagnosis Column: Classified and suggestive of possible medical outcomes", "diagnosis\\\" column presents a categorical variable with for a binary classification of cancer between \\\"Malignant and Benign\\\". Numerical Features": "The remaining 30 columns display a numerical nature, presumably involving important medical measurements or calculations."}]}