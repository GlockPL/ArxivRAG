{"title": "LangProBe: a Language Programs Benchmark", "authors": ["Shangyin Tan", "Lakshya A Agrawal", "Arnav Singhvi", "Liheng Lai", "Michael J. Ryan", "Dan Klein", "Omar Khattab", "Koushik Sen", "Matei Zaharia"], "abstract": "Composing language models (LMs) into multi-step language programs and automatically optimizing their modular prompts is now a mainstream paradigm for building AI systems, but the tradeoffs in this space have only scarcely been studied before. We introduce LangProBe, the first large-scale benchmark for evaluating the architectures and optimization strategies for language programs, with over 2000 combinations of tasks, architectures, optimizers, and choices of LMs. Using LangProBe, we are the first to study the impact of program architectures and optimizers (and their compositions together and with different models) on tradeoffs of quality and cost. We find that optimized language programs offer strong cost-quality Pareto improvement over raw calls to models, but simultaneously demonstrate that human judgment (or empirical decisions) about which compositions to pursue is still necessary for best performance. We will open source the code and evaluation data for LangProBe.", "sections": [{"title": "Introduction", "content": "Language models are now routinely used to build modular natural-language software systems that process data or serve bespoke applications. Such language programs make highly structured language model calls, invoke external tools, and compose all of these into sophisticated systems. In contrast to using language models as user-facing agents, these approaches compose better-scoped LM capabilities (Zaharia et al., 2024), offer the models structure around accessing tools or information (Lewis et al., 2020; Khattab et al., 2021; Lazaridou et al., 2022), and systematically scale planning and search at inference time (Snell et al., 2024; Saad-Falcon et al., 2024).\nTo support such programming and permit portability across models and tasks, recent work has introduced declarative languages for expressing these systems and automating prompting (or fine-tuning) for their modules. For example, given a task-specific objective, DSPy (Khattab et al., 2022, 2024) and TextGrad (Yuksekgonul et al., 2024) are frameworks that study optimizing these programs, e.g. by composing techniques like bootstrapping few-shot examples or refining free-form instructions, seeking to align the whole system to a distribution of inputs or a nuanced task.\nDespite the interest in this space, it remains unclear which problems actually need modular programs, especially as models continue to improve, or which types of architectures and optimizers will work best for different problems. To begin to study this, we introduce LangProBe, a benchmark for evaluating combinations of language models, program architectures, and their optimizers. For many datasets, LangProBe implements multiple language programs (Section 3.2), ranging from a single LM call to sophisticated and modular systems. Using LangProBe, we run over 2000 experiments to investigate several research questions."}, {"title": "Related Work", "content": "Composing language model calls with tool usage into complex applications is now popular, with the help of established language model programming libraries such as DSPy (Khattab et al., 2022, 2024), LangChain (Chase, 2022), or TextGrad (Yuksekgonul et al., 2024). Language programs compose and integrate knowledge and information flow and are often equipped with additional reasoning capabilities. For instance, RAG (Lewis et al., 2020) combines a language model with external retrieval from a corpus for knowledge-intensive tasks; Multi-Agent Debate (Du et al., 2023) harnesses multiple models debating each other to sharpen mathematical and strategic reasoning; Self-Refine (Madaan et al., 2023) iterates on its own outputs for continuous improvement; and ReAct (Yao et al., 2023) integrates step-by-step reasoning with actions to facilitate interactions with external environments. Recent research also focuses on scaling inference-time computation by generating multiple responses and verifying them with specialized models (Snell et al., 2024; Chen et al., 2024).\nRecent work has introduced agentic benchmarks (Kapoor et al., 2024; Zhou et al., 2024; Yang et al., 2024; Wang et al., 2024), which are closely related to LangProBe. As agents are a form of complex language programs, these benchmarks can help examine some design choices in language programs and we intend to add some of them to future iterations of LangProBe. Unfortunately, such agentic tasks leave plenty of unstudied problem types and are somewhat hard to adapt for asking questions about broader categories of program architectures (Stroebl et al., 2024). For example, SWE-bench (Yang et al., 2024) is helpful in evaluating software engineer-like agents, but it can be difficult to rely on it for testing general inference-time scaling or retrieval-augmented generation methods."}, {"title": "The LangProBe Benchmark", "content": "To offer an overview of LangProBe, we define different categories of benchmarks and then describe the selected programs and optimizers that we will study in this work. A detailed list of descriptions can be found in Appendix A and Appendix B."}, {"title": "Dataset Categories", "content": "We choose datasets across diverse categories. In contrast to evaluations focused on comparing model capabilities, e.g. HELM (Liang et al., 2023) or lm-evaluation-harness (Gao et al., 2024), for each category, we recast existing datasets into a uniform testbed with metrics and data splits, seeking to establish a proxy for the bespoke applications that people program. Our categories include agentic tasks (AppWorld; Trivedi et al. 2024), coding and software engineering tasks (SweBench annotation tasks; OpenAI 2024, HumanEval; Chen et al. 2021), mathematical and reasoning tasks (MATH; Hendrycks et al. 2021b, GSM8K; Cobbe et al. 2021), domain-specific classification tasks (Iris; Fisher (1936) and Heart Disease; Janosi et al. (1989)), and question-answering problems (HotpotQA; Yang et al. 2018, MMLU; Hendrycks et al. 2021a), and more. While these serve an insightful starting point, we believe that future work must increasingly push LangProBe past the general capability datasets that model providers hill climb and closer to the composite downstream problems that LM programmers seek to solve."}, {"title": "Language Programs", "content": "We adopt and design a diverse set of language program designs for the different tasks in Lang-ProBe. By nature, language programs are much more structured, declarative, and compositional than free-form conversations with language models or community evaluation harnesses comparing model capabilities on self-contained tasks. This paradigm necessitates a uniform framework for posing our research questions. For this, we build our testbed in the DSPy framework out of convenience and based on its popularity for optimizing these types of systems, although in principle future declarative languages can support the same research questions we ask about programs, models, and optimizers.\nGeneral language programs Many programs architectures are general: with little to no changes, they can be used for all benchmarks. The simplest such program is to use a single DSPy Predict module, which translates to directly calling the language model in a structured manner with the task description and inputs and parsing its outputs. Similarly, we also include a DSPy chain-of-thought (CoT; Wei et al. 2023) program that uses CoT prompting, requiring the language model to output both reasoning and answer. We also adapt Archon's modular structure (Saad-Falcon et al., 2024) for designing more complex general language programs. Specifically, we use generators (generate a list of responses given a single query), critics (provide feedback for a list of responses), fusers (compile a list of responses into a single one), and rankers (rank the list of responses).\nFrom these basic building blocks, we build two general language model programs: GenCriticFuser and GenCriticRanker. Namely, both pipelines first employ a generator module to generate a list of responses given inputs from the language model, then a critic module to provide strengths and weaknesses to the list of responses; finally, both a fuser and ranker module is used respectively to get the singular final result.\nSpecialized programs We also adopt problem-specific program architectures for certain tasks. For example, we define two retrieval-augmented generation (RAG) programs for some knowledge-intensive and classification tasks. We define the RAG program with a module that queries the retriever with the task input and then leverages a CoT module to generate the final response with the retrieved documents. Another RAG system, a highly simplified version of Baleen (Khattab et al., 2021), makes LM calls in a multi-hop design, systematically generating queries for the retriever, performing the retrieval and composing the set of retrieved documents to generate the final answer.\nFor benchmarks that require interactions within a closed-space environment (agent benchmarks), we build a ReAct (Yao et al., 2023) program, which reasons and generates actions simultaneously."}, {"title": "Optimizers", "content": "To measure the performances of these compound language programs, it is also important to measure how they perform under various prompt optimization techniques. LangProBe provides four general prompt optimization techniques for all language model programs.\nBootstrapFewShot (Khattab et al., 2022, 2024) is a simple heuristic for building few-shot examples for all modules in arbitrary programs. Using a metric and a teacher model, it bootstraps \u201csuccessful\" demonstrations from the training set that pass the metric. The process iteratively refines predictions until the maximum number of bootstrapped demonstrations is reached and then included in the final prompt. The BootstrapFewShotRandom-Search optimizer searches through different combinations of bootstrapped few-shot demonstrations using a validation set, applying multiple optimization rounds with random search to select the best-performing set of few-shot examples to include in the final prompt.\nMIPROv2 (Opsahl-Ong et al., 2024) optimizer produces sets of few-shot examples like BootstrapFewShot and then generates instruction candidates for each predictor in the program using a separate LM proposer program. To search for the best combination of few-shot examples and instructions, the optimizer uses Bayesian Optimization to identify the optimal instruction-examples set for each module in the program.\nFinally, as part of this work, we introduce the RuleInfer optimizer, which runs the program through a single iteration of the BootstrapFew-Shot optimizer, allowing candidates to induce rules based on the successful proposed few-shot demonstrations, tasks, and instructions. These rules are then appended to the original instruction, generating refined instructions directly from the best-selected few-shot demonstrations."}, {"title": "Benchmarking Language Programs", "content": "Evaluating language programs with LangProBe are simply Cartesian products of dataset tasks, language programs, optimizers, and language models. While some language programs obviously do not work with specific types of datasets, we evaluate all such possible combinations.\nTo this end, we evaluate 16 distinct tasks from various open-source datasets with six different language models (gpt-4o, gpt-4o-mini, o1-mini, Llama3.1-8B-Instruct, Llama3.2-3B-Instruct, and Llama3.3-70B-Instruct) and a total of more than 10 different language model programs. We also apply four prompt optimization techniques described in Section 3.3 to all language model programs with different hyperparameters (Table 2). In total, we run over 2000 different combinations of language programs and dataset experiments."}, {"title": "Costs of Programs and Optimizers", "content": "In this section, we analyze the cost-performance trade-offs of using language models with varying inference costs and capabilities when combined with language programs and optimizers. Figure 2 visualizes these trade-offs as Pareto curves aggregated over multiple datasets in LangProBe. The x-axis uses a logarithmic scale for inference costs to accommodate the wide range of costs across different configurations.\nWe make the following observations from the Pareto curves: 1) Every point on the Pareto-optimal curve is instantiable: The Pareto-curves are calculated as an upper-left convex hull over linearly scaled performance-cost axes, and hence, every point on or below the curves is instantiable by cost-aware load balancing between the configurations at either end of a Pareto segment. 2) Use of language programs or optimizers enables achieving better performance at lower cost: the Model+Program+Optimizer Pareto curve dominates (achieve better performance at lower cost) the Model+Program and Model+Optimizer Pareto curves, which in turn dominate the Model Pareto curve, implying that using language programs and optimizing them has the capacity to support significant improvements not only with respect to performance, but also cost. 3) Smaller models with effective programs and optimizers outperform larger models, at a lower cost: We observe that often a smaller or cheaper LM with language program or optimization (or both) outperform configurations with larger or expensive LMs lacking language programs or optimization both in terms of cost and absolute performance. On aggregate, in Figure 2, we observe that gpt-4o-mini with language programs and optimization achieves 11.68% higher score than gpt-40's baseline at just 50% of the cost, and gets slightly better performance than gpt-40 with language programs at 10% of the cost.\nAmong individual datasets, in Figure 8c, we see that gpt-4o-mini with program composition and optimization achieves 33.2% better result than gpt-40 without program composition and optimization at 18% lesser cost. These instances reflect the importance of composing compound systems to prompt language models and optimizing these systems, which can lead to increased downstream task performance at fractional costs.\nIn some instances, we observed that using an optimizer can actually reduce the inference cost for the same program while boosting performance. For instance, in GSM8K (Figure 7b), we see that gpt-40-mini with optimization (BootstrapFewShotWithRandomSearch) achieves 8% better result at 5% lesser inference cost than gpt-40-mini without optimization. Since optimizing the program with BootstrapFewShotWithRandomSearch includes few-shot examples in the prompt, which increases the number of input-context tokens presented to the model but excludes the generation of lengthy reasoning traces in the output, and the notion that LM input token costs are cheaper compared to output token costs, we observe such cases. In conclusion, our results demonstrate that language programs and optimizers can significantly enhance performance and cost-efficiency across tasks compared to raw model predictions on the aggregate."}, {"title": "Language Program Design", "content": "We now address the research question of what language program architecture achieves higher quality, e.g. whether structured language programs consistently outperform direct model prediction baselines and which programs contribute the most to performance gains. Additionally, we examine whether the effectiveness of different language programs varies across tasks, highlighting the conditions under which they provide the greatest benefit.\nFirst, we show an overview of performance comparison between best language programs and raw model prediction baseline in Figure 3. In almost all cases, using some selected language programs, either unoptimized or optimized, gives a better performance than the raw model prediction baseline."}, {"title": "Which language program designs provide more improvements?", "content": "Although Figure 3 provides a clear overview of the best-performing language programs on each dataset, it does not easily show the relative improvements achieved by different programs. To address this, we present Figure 4, which highlights, for each dataset in the Knowledge, Reasoning, and Math categories, both the best-performing and worst-performing programs and their performance relative to the baseline.\nFrom Figure 4, the performance of different language programs varies significantly when compared to the baseline. On the left side, we observe significant improvement over the baseline by mostly retriever-augmented generation programs. Conversely, other programs, including GeneratorCriticRanker for RAGQAArena and GeneratorCriticFuser for JudgeBench perform worse than the baseline. Inspecting the evaluation traces, we find that when the program involves multiple modules, errors (like wrong format, factual errors, or hallucination) cascade from one module flow into other modules, causing errors to aggregate. Luckily, the error aggregation patterns, especially parsing errors, are mitigated through carefully curated few-shot examples and instructions from different optimization techniques as highlighted by the positive performance with both optimized variances.\nOverall, these results demonstrate plenty of gains due to compositional program architectures, but they also show that human judgment about which compositions to pursue (or a well-scoped search for tasks with enough data) is still required for best performance. In other words, there is no general \"set it and forget it\" strategy within the scope we considered."}, {"title": "When do language programs provide more improvements?", "content": "From both Figure 4 and the individual Pareto curves on each dataset in Appendix C, we observe larger improvement from language programs often associated with datasets or tasks that require additional information to complete. For example, Hot-PotQAConditional requires answering questions in a specific format that is unknown to the language model; IReRa is a classification task that needs additional information about the labels, etc. Language programs often show significant improvement over these datasets, which resembles a large class of real-world applications. On the other hand, tasks that language models are trained to perform, like MMLU or HumanEval, often see little to no benefit from unoptimized language programs. Figure 8f plots an overlapping Pareto curve for Model+Program and the baseline, demonstrating no performance benefits from using unoptimized language programs for HumanEval."}, {"title": "Optimizers", "content": "As in Section 6, programs are important for most datasets, as they provide additional context or more opportunities to reason with the language model. In addition to program design and architecture, prompt-based optimizations are also crucial to the performance of a compound AI system. In this section, we discuss whether the existing optimization techniques are effective by answering the following research questions."}, {"title": "What optimization techniques are more effective than others?", "content": "We rank the performance of each individual optimizer in Figure 5. From the plot, we observe that general optimizers that perform optimal instruction finding (MIPROv2 and its variants) and in-context learning (BootstrapFewShotRandomSearch and its variants) are leading in general, both having more experiments that appear within 3% of the highest score for the same dataset and program among all optimizer settings. The rule induction optimizers \"RuleInfer\" are good at obtaining the best scores but perform slightly worse to generalize for more tasks. This is due to its specialized nature, which allows RuleInfer to only work for tasks with obvious and conclusive rules from the examples.\nAdditionally, optimizers using a stronger model internally (suffices with \"T\") generally perform better with an exception on RuleInfer. Because MIPRO and BootstrapFewshot all depend on good few-shot demonstrations, a stronger internal model provides better performance by generating better traces at a relatively lower cost."}, {"title": "How do performance gains from different optimizations vary across the distribution?", "content": "We list the relative gain for all optimizers in Figure 6. From the plot, at the 90th percentile, all optimizers demonstrate strong performance, ranging from 80.2% up to 122.5%. This illustrates that if used carefully, optimizers can provide substantial performance gains. For example, for the HeartDisease dataset, the MIPROv2 optimizer finds instructions like \"using the provided patient information, predict whether the patient has heart disease by analyzing the clinical parameters step by step and providing a rationale for your conclusion\u201d and a few good few-shot examples with detailed LM-generated reasoning. These optimized prompts increase the performance for Llama-3.2-3B from 26.32 to 76.32, resulting in an 189.97% increase.\nThe median of performance gains suggests the common effect of using optimizers. For most tasks, we expect a 5% to 20% from the optimizers. Finally, in some rare cases, performance degradation happens. Because most optimizers employ a separate validation set to control the quality of optimizations, the optimized language program may overfit the validation set, causing a performance drop on the final test set. RuleInfer and its variants have the most performance degradation due to non-transferable rules. Although the rules induced by the optimizer work well for the validation set, some may not directly help with the test set performance."}, {"title": "Conclusion", "content": "We presented LangProBe, a benchmark for evaluating language programs across a range of tasks. We used it to investigate far more comprehensively than prior work the interactions between architectures, optimization strategies, language models, and the resulting quality and cost tradeoffs. We hope that this starts a line of work on studying this new category of AI systems and that our current findings can already begin to offer practical guidance for researchers and practitioners designing and optimizing modular AI systems. Lastly, one of the primary goals of LangProBe is to facilitate the development and comparison of new language program architectures and optimization strategies."}, {"title": "Limitations", "content": "Due to compute constraints, LangProBe is not able to include more language programs (e.g., program-of-thought) and datasets (e.g., SWE-bench), which could allow more insightful observations. Lang-ProBe does not run full evaluation on the newest reasoning models, like DeepSeek-R1 or OpenAI 03-mini, also due to budget and time constraint."}, {"title": "A Dataset Descriptions", "content": "LangProBe uses the following open-sourced datasets for research purposes only.\nAppWorld (Trivedi et al., 2024)\nTask: AppWorld databases start in some initial state set by the benchmark. The agent should take certain actions during the execution to change the database into another state.\nInput: A question about mobile applications, along with its supervisor's information like name, phone number, and email.\nOutput: python code that will be executed by App-World server.\nEvaluation metrics: the output python code will be executed by the AppWorld server and check if the database after execution is the same as the ideal final state. Success is marked by passing all the unit tests for state check and no unwanted actions are taken. Otherwise, it's a failure.\nLicense: Apache 2.0\nSweBench VerifiedAnnotation (NLP, 2024)\nTask: to examine each sample in the SWE-bench test set for properly defined issue descriptions and unit tests with appropriate scope by giving them scores.\nInput: the repository name containing the issue, the issue description, gold patch, test patch, and the names of the tests in the test patch that will be used to evaluate the solution.\nOutput: a score from 0 to 3 based on certain 4 criteria.\nEvaluation metrics: the ground truth is also a score based on the same criteria. We calculate the string equivalence between the predicted score and the ground truth score.\nLicense: MIT\nMATH (Hendrycks et al., 2021b)\nTask: The Mathematics Aptitude Test of Heuristics (MATH) dataset consists of problems from mathematics competitions, including the AMC 10, AMC 12, AIME, and more. Each problem in MATH has a full step-by-step solution, which can be used to teach models to generate answer derivations and explanations.\nInput: a math question written in LaTeX and natural language.\nGround truth: step-by-step solution written in LaTeX and natural language with the final answer enclosed in LaTeX's \\boxed tag.\nEvaluation metrics: string equivalence between the predicted mathematical answer and the gold solution extracted from the LaTeX's \\boxed tag. We adopt Hendrycks et al. (2021b)'s evaluation. Arguably, a more prominent evaluator like Math-Verify (Kydlicek et al., 2025) would report fairer (and higher) scores for all programs and optimizers, which is left as future work.\nLicense: MIT\nGSM8K (Cobbe et al., 2021)\nTask: solving high-school-level mathematics problems across various topics. These problems are presented in natural language, and the model is required to produce correct solutions.\nInput: The question to a grade school math problem in natural language\nOutput: The solution to this problem with step by step reasoning in natural language. The numerical solution is always at the end of the solution string.\nEvaluation metrics: the ground truth is the solution to this problem with step-by-step reasoning in natural language. We calculate the integer equivalence between the predicted mathematical answer and the gold solution.\nLicense: MIT\nHotPotQA (Yang et al., 2018)\nTask: HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs. Our task is to answer questions that require reasoning across multiple supporting documents.\nInput: question in natural language\nOutput: answer to the question in natural language\nEvaluation metrics: ground truth is the answer to the question in natural language. We evaluate the string equivalence between the predicted answer and the ground truth answer.\nLicense: CC BY-SA 4.0\nHumanEval (Chen et al., 2021)\nTask: The HumanEval dataset released by OpenAI includes 164 programming problems with a function signature, docstring, body, and several unit tests. The task is to produce reliable and executable code that passes the unit tests given.\nInput: the prompt for function specification that includes necessary import statements, function signatures, and docstring of unit tests.\nGround truth: generated code snippet following the function specification.\nEvaluation metrics: binary indicator that specifies whether generated code passes the test cases defined in the ground truth.\nLicense: MIT\nMMLU (Hendrycks et al., 2021a)\nTask: answer multiple-choice questions across a broad range of knowledge domains.\nInput: description of the question along with its four options in natural language.\nGround truth: the correct option for this question\nEvaluation metrics: ground truth is provided as the correct option in natural language (\u201cA\u201d, \u201cB\u201d, \u201c\u0421\u201d, or \u201cD\u201d). We evaluate the string equivalence between the predicted option and the ground truth option.\nLicense: MIT\nIReRa (D'Oosterlinck et al., 2024)\nTask: solve multi-label classification tasks with an extreme number of classes\nInputs: a textual description\nOutputs: all the ESCO job skill labels mentioned in natural language.\nEvaluation metrics: rank-precision (RP) of the produced rankings, calculated from a list of true relevant items and a list containing the predicted items in ranked order.\nLicense: MIT\nHeart Disease (Janosi et al., 1989)\nTask: classify the patient's heart disease status based on the information provided\nInputs: textual description of patient heart disease attributes, including age, slope, chol, ca, thal, restecg, exang, trestbps, cp, thalach, fbs, oldpeak, and sex.\nOutputs: A binary classification (\"yes\" or \"no\") indicating whether the patient has heart disease.\nEvaluation metrics: we evaluate the string equivalence between the predicted diagnosis and the ground truth diagnosis (\u201cyes\u201d or \u201cno\u201d).\nLicense: CC BY 4.0\nHoVer (Jiang et al., 2020)\nTask: perform multi-hop evidence retrieval of a claim to determine whether it's supported or not.\nInputs: a claim about a fact in natural language.\nOutputs: documents being retrieved concatenated as a single string.\nEvaluation metrics: The ground truth is the fact the model is required to retrieve. We calculate the proportion of examples in which the model is required to retrieve at least one supporting fact from each supporting document and accurately predict the correct label.\nLicense: CC BY-SA 4.0\nIris (Fisher, 1936)\nTask: predict the species of an iris flower based on its sepal and petal measurements.\nInputs: petal and sepal dimensions in cm about the iris species in natural language.\nOutputs: the class of iris plant (setosa, versicolor, or virginica).\nEvaluation metrics: the ground truth is the iris class in natural language. We evaluate the string equivalence between the predicted class and the ground truth class.\nLicense: CC BY 4.0\nScone (Scoped Negation Benchmark) (She et al., 2023)\nTask: ScoNe-NLI contains contrast sets of six examples where entailment relations are impacted by the scope of one or two negations. The main task is to test how well models understand and reason about negation in natural language.\nInputs: a context dependent question and its context in natural language\nOutputs: \"yes\" or \u201cno\u201d\nEvaluation metrics: ground truth answer to the question is \"yes\u201d or \u201cno\u201d. We evaluate the string equivalence between the predicted answer and the ground truth answer.\nLicense: CC0 1.0\nRAG-QA Arena \"Technology\" (Han et al., 2024)\nTask: generate quality answers for technology questions.\nInputs: question in natural language.\nOutputs: response to the question in natural language.\nEvaluation metrics: ground truth is a response in natural language. We calculate the semantic F1 score between the model response and the ground truth response. Note here the language model used to evaluate F1 is the same as the model being evaluated.\nLicense: Apache 2.0\nJudgeBench (Tan et al., 2024)\nTask: evaluating LLM-based judges for objective correctness on challenging response pairs.\nInputs: a question, response A and response B in natural language\nOutputs: which response is better. Either A>B or B>A.\nEvaluation metrics: ground truth is which one is better, either \u201cA>B\u201d or \u201cB>A\u201d. string equivalence between the predicted answer and the ground truth answer.\nLicense: MIT"}, {"title": "B Language Program Descriptions", "content": "COT\nGiven an instruction prompt, Chain of thought produces a step-by-step explanation leading to the final answer.\nNumber of LLM calls: 1\nRAG\nRetrieval Augmented Generation first retrieves top-k most relevant passages using a retriever. These retrieved passages are then integrated as context into the model's input and passed through a Chain-of-Thought reasoning process to generates a final response.\nNumber of LLM calls: 1\nReActBaseline\nRe-Act is a combination of reasoning and action in a step-by-step manner. Usually, the action involves retrieving relevant information from external sources and then integrates reasoning and action-based decision-making into the model's process. In our case of AppWorld, the action involves generating code that will be executed by the App-World server.\nNumber of LLM calls: equivalent to the number of reasoning steps (the number of reasoning steps defaults to 40 in the case of AppWorld).\nReActAugmented\nSame as ReActBaseline but with few-shot demonstration added.\nNumber of LLM calls: same as ReActBaseline\nSimplified Baleen\nFirst generates multiple search queries and iterate through each one of them in multiple \"hops,\" where each hop retrieves relevant passages from a knowledge base using a retriever. These retrieved passages are then aggregated as contexts and are passed through a Chain-of-Thought reasoning process to generate the final response.\nNumber of LLM calls: equivalent to the number of hops (defaults to be 2).\nSimplified BaleenWithInst The setup is the same as SimplifiedBaleen except for a manually written prompt. In our case of HotpotQA conditional, the prompt is the following: When the answer is a person, respond entirely in lowercase. When the answer is a place, ensure your response contains no punctuation. When the answer is a date, end your response with \"Peace!\u201d. Never end your response with \u201cPeace!\u201d under other circumstances. When the answer is none of the above categories respond in all caps.\nNumber of LLM calls: same as SimplifiedBaleen.\nGeneratorCriticRanker\nProduce a list of candidate responses from a given instruction prompt, then for each of these responses, it identifies the strengths/weaknesses for each candidate response. Then it returns a ranked list of top-K candidate responses.\nNumber of LLM calls: equivalent to the number of candidate * 2 + 1 (the number of candidates defaults to be 5).\nGeneratorCriticFuser\nProduce a list of candidate responses from a given instruction prompt, then for each of these responses, it identifies the strengths/weaknesses for each candidate response. Then it returns a single, high-quality response.\nNumber of LLM calls: equivalent to the number of candidate * 2 + 1 (the number of candidates defaults to be 5).\nRAGBasedRank\nIt's an in-context learning framework designed for multi-label classification with an extremely large number of classes (IReRa). The process begins by generating queries based on the input and retrieving documents from a fixed retriever. Then the retrieved documents are re-ranked by an LM. \nNumber of LLM calls: 2. One for generating the query for retrieval and one for re-ranking the top k retrieved documents.\nMultiHopSummarize\nIt begins by retrieving and summarizing the top k relevant passages for a given claim. Subsequent hops generate refined queries based on previous summaries, retrieving additional passages. In total of 3 iteration performed to gather information about the initial question. \nNumber of LLM calls: equivalent to the number of hops (the number of hops defaults to 7 in the case of HoVer).\nCoTBased Vote\nIt first applies multiple Chain-of-Thought classifiers to generate independent predictions (votes) that includes both the reasoning and the answer to the question. These votes are then assessed and consolidated through critical evaluation of these \"opinions\". \nNumber of LLM calls: equivalent to the number of voters + 1. (the number of voters defaults to be 3 in the case of Heart Disease)."}, {"title": "C Cost-Performance Pareto Curves for all benchmarks", "content": "Figure 2 visualizes the Performance-Cost Pareto optimal configuration aggregated over 7 datasets in LangProBe (Scone, HotpotQA, HumanEval, Heart-Disease, Judge, Iris, HotpotQAConditional) and Figures 7, 8, and 9 and visualize the Performance-Cost Pareto optimal configurations for individuals datasets in LangProBe."}, {"title": "D Hyperparameter Settings for Optimizers", "content": "Table 2 summarizes the optimizer configurations used in LangProBe."}, {"title": "E Introducing RuleInfer Optimizer", "content": "We introduce RuleInfer, a prompt optimization approach that identifies actionable rules to optimize the DSPy program performance. We provide a diagram of RuleInfer in Figure 10. Building upon the BootstrapFewShot optimizer, RuleInfer leverages an LLM to perform rule induction on the generated successful few-shot demonstrations, ascertaining specific insights grounded by the \"positive\" behavior from the examples to improve model performance. The optimizer then appends these sets of rules to the original task instructions and produces candidates of optimized prompts with natural language guidelines that are validated iteratively with the final output of a best-performing optimized program.\nIn comparison to optimizers like BootstrapFewShot (designed mainly for producing examples) and MIPRO (which proposes both instructions and few-shots), RuleInfer offers a distinct way of incorporating grounded, actionable rules derived from existing few-shots. RuleInfer excels in tasks with clear, discrete constraints, such as classification (HeartDisease and Iris) or coding domains (HumanEval and SWEBench), where well-defined rules can be leveraged to create structured decision boundaries for the language model to consider and hence reinforce both consistency and accuracy in performance. This quality makes RuleInfer particularly effective at aligning model behavior to task requirements without extensive manual engineering of the prompt instructions specifically, as the LLM performing rule induction strengthens this alignment. However, the optimizer lacks benefits on tasks are less domain-specific like question-answering tasks (HotPotQA, MMLU) as the induced rules tend can be too broad for open-ended general knowledge queries.\nRuleInfer demonstrates how the LangProBe benchmark can be leveraged to introduce and validate new prompt optimization techniques across various tasks. By comparing RuleInfer across existing DSPy optimizers across multiple tasks, programs and models, we provide a streamlined analysis to easily pinpoint instances where rule-based induction excels versus where it underperforms. This will greatly benefit both prompt optimization developers in benchmarking novel techniques against previous ones and prompt workflow developers in understanding when to use what kinds of optimizers based on their task and program."}]}