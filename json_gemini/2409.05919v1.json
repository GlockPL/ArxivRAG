{"title": "KModels: Unlocking AI for Business Applications", "authors": ["Roy Abitbol", "Eyal Cohen", "Muhammad Kanaan", "Bhavna Agrawal", "Yingjie Li", "Anuradha Bhamidipaty", "Erez Bilgory"], "abstract": "As artificial intelligence (AI) continues to rapidly advance, there is a growing demand from clients and product managers to integrate AI capabilities into their existing business applications. However, a significant gap exists between the rapid progress in AI and the slow rate at which AI is being em- bedded into business environments. Deploying well-performing lab models into production settings, especially in on-premise environments, often entails specialized expertise and imposes a heavy burden of model management, creating significant barriers to implementing AI models in real-world applications.\nKModels is built on the shoulders of proven libraries and platforms (Kubeflow Pipelines, KServe) aiming to simplify AI adoption by supporting both the AI developers and the con- sumers. It allows model developers to focus their efforts solely on model development and to share the models as transportable units (Templates), abstracting away complex production deploy- ment concerns. At the same time, KModels allows AI models consumers to eliminate the need for a dedicate data scientist, since the templates encapsulate most data science considerations while still providing business-oriented control.\nIn this paper we discuss the architecture of KModels and the key decisions that helped shape it. We present KModels' main components and their function as well as its interfaces. Furthermore, we explain how KModels is highly suited for on- premise deployment but can also be used in cloud environments. The efficacy of KModels is demonstrated through the success- ful deployment of three different AI models within an existing Work Order Management system. These models are deployed in a client's data center and are trained successfully on local data, without any data scientist intervention. One of these models improved the accuracy of Failure Code specification for work orders from 46% to 83%, showcasing the substantial benefit of accessible and localized AI solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancements in artificial intelligence (AI) have generated significant interest from clients and product man- agers who seek to integrate AI capabilities into their business applications. Success stories of embedding AI into cloud business applications (Software-as-a-Service) are common [1], however, the data in cloud applications is controlled by the software vendors and the Al models are trained, deployed and maintained by them (This is especially true for consumer applications but in this paper we focus on business oriented application). The ability to access all clients data allows the vendors an ideal setup where they can perfect the data cleansing and preparation and eventually optimize the resulting models. Vendors may offer the best suited model for each subset of clients or even train a dedicated model per each client.\nOn the other hand, many companies are running business applications on-premises, due to cost, privacy, security or regulatory reasons [2]. In these settings the data is kept locally and cannot be shared outside. As a result the software vendors of such applications, do not have any access to client data, and are essentially prohibited from training and providing any AI models to their clients. In practice, many application vendors delegate the responsibility of training AI model to their clients by shipping AI models as raw code [3]. However, this approach places the burden of training the model on the clients and requires each client to employ AI engineers (or data scientists). This burden effectively increases the cost of ownership of any new AI model thus setting a high bar for the adoption of AI [4], [5]. Companies lacking the financial resources and the suitable skills required for this task will not be able to apply and utilize new AI capabilities.\nOne of the major challenges is the need to manage the entire life-cycle of the AI models. In order to successfully and continuously run AI models in production environments, it is necessary to employ a range of technical and operational services and skills to support the models deployment, connec- tivity, monitoring and many other aspects of their life-cycle. The technical effort involved in this is surprisingly high [6] and it has yielded an entirely new area of operational AI, known as DevOps for AI [7] or MLOps [8]. In a typical cloud application the developers manage the entire life cycle of the AI models.\nHowever, if the model is run on-premises of the client's data center (especially if the model is delivered as raw code), the client engineers are burdened with managing the entire life cycle of the model.\nTo recap, the key challenges inhibiting widespread adoption of AI in business applications are:\n\u2022 Adapting the AI models to perform well on local client environments, requires sharing the models code in order to train them on-premises.\n\u2022 Training the models on clients premises, requires skilled data-scientists, increasing the cost of running AI models.\n\u2022 Models that are running in local environments require constant monitoring and management, entailing a heavy burden for local operational staff.\nThese challenges present a barrier for incorporating AI into business applications and environments, leading to a relatively low success rate of AI projects and low adoption of new AI models. According to a research by Gartner Inc. [9] \"only 53% of projects make it from AI prototypes to production\" and at a recent McKinsey global survey [10] only 36% of respondents said that \"ML algorithms had been deployed beyond the pilot stage\".\nReviewing the yearly Global AI Adoption Index [11] [12] over the past 3 years, shows that enterprise IT professionals have persistently ranked the accessibility and simplicity of AI as the top driver of AI adoption in their organizations (\"more accessible and easier to deploy\"). Furthermore, when asked about the main barriers to successful AI adoption, they consistently point, year after year, towards the scarcity of \u201cAI skills, expertise or knowledge\u201d. Other top barriers mentioned were complexity of the deployment and difficulty to integrate and scale. These studies are a strong testament to the validity and prevalence of the challenges at hand. It is evident that despite the decades of work on AI development there is still room for improvements in the operational aspects of AI, in order to allow broad acceptance of AI in business applications and environments.\nIn this paper, we present KModels, a novel framework aimed at reducing the technical skills required for AI inte- gration in business applications, without sacrificing quality or comprehensiveness. On one hand, KModels offers the application developers a mechanism to ship their models to clients local premises such that the models will be trained and deployed in a controlled and manageable way. On the other hand, KModels allows the application clients to adopt new AI models with minimal effort and without a skilled AI personnel. The key contributions made by this paper are:\n1) Introducing KModels, a framework that simplifies the deployment and life-cycle management of AI models. KModels allows non-technical users to deploy and oper- ate models in production environments while preserving the clients' data locally, on-premises.\nthe key decisions that helped shape it. We present KModels' main components and their function as well as their interfaces.\n3) Lastly, explaining how KModels is highly suited for on-premise local deployments and why KModels in- frastructure can enable the widespread adoption of AI throughout various business applications."}, {"title": "II. RELATED WORK", "content": "The landscape of AI frameworks, which is evolving both in the academia and in the commercial AI industry, is over- whelmed with numerous solutions for different use cases and needs. These frameworks can be broadly categorized into AI Libraries, AutoML platforms, and MLOps frameworks. While each category addresses specific challenges in the AI lifecycle, none provide the comprehensive, end-to-end solution for business users that KModels offers."}, {"title": "A. AI Libraries", "content": "AI libraries (such as PyTorch [13], [14], TensorFlow [15], Scikit-Learn [16], etc.) provide AI model developers the building blocks to develop and deploy models through a high- level programming interface [17]. These libraries contribute various Machine Learning (ML) and AI algorithms and pro- vide developers high level functions and classes [13], thus placing an abstraction on top of low level coding tasks. These libraries require advanced coding and data science skills and are used to orchestrate high level algorithmic pipelines. While some of these libraries do provide certain deployment support [18], it is usually very limited, and not suitable for operational methodology. In Contrast, KModels is primarily focused on orchestrating the life cycle of the AI models but it is not concerned at all with the development process of the model."}, {"title": "B. Auto ML", "content": "Auto-ML or Auto-AI solutions (such as IBM's AI Stu- dio [19], Google's AutoML [20] and others) automate the construction of ML pipelines [21] [22]. They are designed to reduce the demand for data scientists and enable domain experts to automatically build ML applications without re- quiring deep statistical or ML knowledge. They operate by allowing the user to merely make a minimal set of decisions and operations while they apply extensive internal logic for analyzing the data and applying the best AI algorithm with the most suitable set of parameters. Like KModels, these frameworks promote the proliferation of AI models in business environments. They provide the infrastructure and means for business users who are less skilled in data science to easily train models [23]. However, these frameworks lack in their support for a scalable distribution of the models. Furthermore, unlike KModels, these frameworks are not equipped to deal with the entire management of models and their lifecycle"}, {"title": "C. MLOps Frameworks", "content": "MLOps frameworks or ML orchestration frameworks (such as Kubeflow, Airflow or MLflow) target the challenge of managing complex model training pipelines in production [24] [25]. They provide a set of building blocks allowing an operations engineer to construct a flow starting from the data collection through the model training and ending in model serving. In fact, Kubeflow pipelines are used by KModels to orchestrate the training process for the models inside KModels. However, these models require an expert level knowledge in their operation and configuration. Setting up models and continuously monitoring them using these tools requires a significant ongoing effort from skilled personnel. KModels abstracts away the underlying complexity of these systems behind a simple, template-driven interface purpose-built for business users. It takes the ML pipelines tested in the lab via MLOps tools and seamlessly deploys them to production with minimal configuration.\nBy building on the shoulders of proven libraries and platforms (Figure 2), KModels offers a uniquely accessible and business-friendly solution for end-to-end AI deployment and lifecycle management. Its template-driven approach, auto- mated model ops, and self-serve configuration put the power of AI directly in the hands of business users while leveraging best-of-breed MLOps tools behind the scenes. This empowers enterprises to overcome the traditional barriers to AI adoption and rapidly infuse AI across their business applications and processes.\nAs shown in Table 1, KModels uniquely combines features that are typically spread across different types of frameworks. It offers the ease of use associated with AutoML solutions, the customizability and on-premises capabilities of traditional AI libraries, and the lifecycle management of MLOps frame- works. Crucially, KModels adds business-centric configuration and template-based model sharing, making it particularly well- suited for widespread AI adoption in business applications without requiring extensive data science expertise."}, {"title": "III. KMODELS", "content": "KModels was developed from the grounds up aiming to simplify AI adoption by supporting both the AI developers and the consumers. KModels allows model developers to focus their efforts solely on model development and allows the developers to package the models as transportable units (Tem- plate, discussed in section III-B), abstracting away complex production deployment concerns. At the same time, KModels allows AI models consumers to eliminate the need for a dedicate data scientist, since the templates encapsulate most data science considerations while still providing business- oriented control."}, {"title": "A. Assumptions and Requirements", "content": "As we set out to develop the KModels solution we devised several key assumptions that clarify the scope of the frame- work and the use cases it addresses. These assumptions are:\nAs discussed in the introduction, we posit that there is a growing need for a solution that will make AI accessible to many clients as well as business\n\u2022 Business need -\n\u2022 Feasibility and applicability \u2013 There are certainly com- plex cases mandating that the AI models will be manually tuned by a data scientist. However, we argue that in a significant share of the worthy business use cases, it is feasible to train Al models, on a client's data, autonomously, without requiring manual tuning by a data scientist.\n\u2022 Integration with user interface - The proposed solution deals with most aspects of AI models management in a generic fashion, however the solution does not provide any UI support for the inferencing of the models. The business applications shall access a REST endpoint to run the models inferencing and they are responsible for reflecting the AI output in any way or triggering any action as a result of the AI output. In other words, integrating the AI output into the application is not part of the scope of the current solution.\nIn addition to these assumptions we accumulated key re- quirements, over the course of defining the solutions, that have helped shape the solutions and can serve as guiding principles for a solution aiming to host and service AI models in a business application environment. These requirements are:\n1) Target personas We define three target personas for KModels in the consuming end:\na) Application administrators - This persona is tasked with the basic installation of the framework and its initial setup. Later, this persona deals with any operational tasks such as configuring the hard- ware resources of the system and uploading new versions. The application administrators can also operate the model management however this task demands a certain level of business knowledge and it is likely that the application admin will delegate this task to expert business users (or carry it out on their behalf).\nb) Expert business users / Subject matter experts - The expert business users should provide the necessary business input and decisions for instantiating a new model.\nc) Business users The end users of the business application. They will be viewing and utilizing the AI output in their daily routine.\nWe discuss additional personas involved in a typical KModel workflow later in this paper.\n2) Comprehensive lifecycle support The solution must provide integrated services covering the entire lifecycle of an AI model from the initial training, through data connectivity, monitoring, feedback, governance, version- ing, caching etc.\n3) Scalability and extensibility \u2013 The solution must support easy scaling of the system in the number of users/calls and number of models. Furthermore, the solution shall support easy extension of the system with additional services.\nKModels is built upon the open sources frameworks: Kube- flow (or Data Science) Pipelines [26] and KServe [27] as core foundations ensuring scalability, efficiency, and industry proven standards (Figure 2). As a result, many of KModels' design principles are based upon these frameworks."}, {"title": "B. Model Templates", "content": "A cornerstone of the KModels solution is the concept of the model template. The Template concept is analogous to the Docker concept [28], similar to how Docker containers can be spawned from a Docker image so can Al model instances be spawned from a model Template. A Template is an immutable package (file) encapsulating the source code, libraries, dependencies, tools, and other files needed for an AI model to operate. The creation of the template (essentially model development) and packaging is done on the model vendor's end. These steps require skilled personnel with data science and operational experience (Figure 1).\nThe template's code and metadata are developed by an AI developer (or data scientist) containing the necessary instruc- tions for training and serving a model. The code is crafted in an iterative refinement process, and it is typically tailored to a specific business need, although this is not a requirement and AI developers may choose to define the boundaries of the use cases that they address. In fact, the KModels framework is completely agnostic to the internals of the template code and developers are free to write their code almost without any restrictions.\nNext, the model is packaged as a template image by an ML practitioner (it can also be the developer). Packaging the template is a one-time effort, per model, and it is also carried out on the vendor's end. Following the packaging of the template image, it can be transported to a client location, either directly or published in a central repository, enabling multiple clients to download and deploy it. Templates that have been deployed to the KModels framework can be used to create Al models.\nThe next steps: Instantiation, training, deploying, etc., are all executed on the client's end. They are controlled and performed by the KModels framework and therefore require a relatively low level of skills, targeting personas of application administrators and expert business users, and not requiring any data science skills.\nA formal and concise description of the way KModels and the Templates operate can be described in the following formulas:\n$T(\\tau, i, C) \\longrightarrow UM^0(in, out, c^0, \\tau, i)$  (1)\n$\\uparrow$\n$C(in,out,c)$\nWhere T is the template, $\\tau$ is the training pipeline defined and implemented by the template developers (i.e model de- velopers in the context of template development), i is the inferencing pipeline defined and implemented by the template developer, and $\\bar{c}$ is the vector of available configurations that the template developer chose to expose to the client. C is a specific configuration for instantiating the template into a specific untrained model. in is the definition of which data attributes (vector) the model should rely on for training and inferencing while out defines the output of the model. For example, this could be names of specific columns or object attributes, depending on the product's schema. C also defines $c_0,..., c_n$ which are the specific settings of the configurations to be used for the current model instance. Configuration op- tions that are not specified will receive the default values that the template developer defined. In all equations a superscript 0 indicates a specific instantiation.\n$UM^0$ is the untrained model. The untrained model is a specific instantiation of the template, prior to its training phase. An untrained model contains the specific training and inferencing pipelines and their configurations, as well as the configuration of what data to apply the model on, the inputs and the output. A single template could be used to create many different untrained models.\nNext, the untrained model is trained on the subset of the client data that the untrained model is configured to operate on:\n$UM^0(in, out, c^0, \\tau, i) \\xrightarrow{D} M^0 (in, out)$  (2)\nHere, D stands for the client data, and $M^0$ is the trained model, which has been trained on the data that was defined in the configuration, using the training pipeline defined in the template and the configuration captured in the untrained model. The final step, which we formally define here mainly for completeness is the use of the model in run-time. This is a normal inferencing step:\n${M^0, D^0 (d_0, ..., d_m)} \\xrightarrow{Inferencing} {out_0, ..., out_m}$  (3)\nWhere the inferencing pipeline i runs model $M^0$ on data points $d_0,..., d_m$ to generate the corresponding predictions $out_0, ..., out_m$. The inferencing could be done one data point and label at a time, the batch inferencing is shown as a general case.\nIn order to standardize development process of templates and reduce the amount of unnecessary decision making for the template developer, KModels proposes a well-defined project structure. The template's project structure (Listing 1) hosts two primary folders for storing the training code (kfp) and the serving code ( kserve ). The template folder stores the configuration metadata for the template. All of the following folders are optional and can be used only as needed. The common and third_party folders store code libraries which are required both during training and during the serving of the model, however their affiliation is different as their names indicate. The third_party folder is used to host external libraries while common stores proprietary libraries. All the aforementioned folders are folders which will be col- lected and packaged into the template images to be transported and consumed using KModels. The remaining folders will not be packaged and are mainly used by the template developer for local testing purposes during the template development phase. The data and examples folders can be used to store data samples and input samples and are both useful when running local tests of the template. The hack and research folders can store local scripts and code which is used as the template is being developed and different versions of the models are tested locally. The pretrained folder is used for storing the trained instance of the model when running local training."}, {"title": "C. Models Management", "content": "After the template project is packaged into an image, it is transportable and can either be shipped directly to clients or be published in a Model Store. The Model Store can reside in the same cluster with KModels, allowing for local consumption of templates or it can reside in a separate cloud deployment allowing multiple different KModels (clients) to connect to it, present the store's templates content and pull any of them locally, to be instantiated.\nOnce a template is within the KModels system, it can be used to generate multiple model instances (see Equation 1). Each instance utilizes a unique configuration (Figure 3). The business user will use the applications' GUI to set the different configuration options.\nThe configuration defines:\n1) Resource parameters: Resources consumed during train- ing and serving. These allow the application admin (the persona tasked with creating the models on the client side) to allocate the necessary CPU, GPU and memory resources to each model. The template may define minimal requirements for launching a model. The admin has to ensure that the configured resources are indeed allocated to the KModels cluster.\n2) Data connection: The specific connector and relevant data attributes. KModels comes out of the box with a set of predefined data connectors to a variety of databases and data sources (SQL, S3, file-system, etc.). The output format of the connectors is well defined and the template developers are aware of it when writing the data processing code. The data connectors allow the admins some level of control for filtering and selecting the input attributes form the data source. Mapping of the input data attributes to the template's specific input requirements is part of the template-specific arguments.\n3) Template-specific arguments: These arguments are ex- posed by the template developers to allow control over different aspects of the model. Keeping in mind that the templates are aiming to hide the technical complexity from the users, while maintaining their control over the business behavior, we expect to see more business- centric arguments and less technical arguments. Further- more, the more use-case specific a template is, the less arguments (flexibility) we expect to see.\nThis configuration-driven flexibility allows application de- velopers to tailor a template to a specific use cases with minimal user interaction. Having said that, it also support the provisioning of templates that are capable of addressing di- verse needs. For example, imagine a template that encapsulates a generic capability of a binary classifier. This template should accept an input vector with multiple attributes of various data types and feed them internally to a binary classifier. It is likely that such a template would expose configuration arguments that support technical configuration of the model in order to better tune the model to the data. Such an open-ended template requires some level of ML understanding from the user.\nUpon successful model creation, the KModels framework automates the entire lifecycle, encompassing:\n1) Data Acquisition: Fetching and managing necessary training data. KModels launches the respective data connector to pull the data and stores it for the model. It can also do so on a periodic basis for subsequent re-training of the model.\n2) Training Execution: Leveraging Kubeflow Pipelines for robust and scalable training. KModels times the creation and launch of the training pipeline and monitors its execution.\n3) Model Deployment: Efficiently serving the trained model using KServe infrastructure. Upon termination of the training pipeline, KModels will store the resulting model and configure KServe to serve the model. The applica- tion admins can intervene, based on the training metrics presented to them, and prevent the model from getting published to KServe.\nBeyond the essential lifecycle, KModels extends its support to comprehensive model management services:\n1) Continuous Monitoring: Tracking model performance and potential deviations.\n2) Drift Detection: Identifying performance degradation or data drift.\n3) Periodic and event based Retraining: Automating sched- uled model updates and Monitoring or Drift-Detection triggered retraining.\n4) Archiving and Caching: Optimizing storage and access for past models and intermediate results."}, {"title": "D. KModels Architecture", "content": "The entire operation of KModels is controlled by the Controller. It serves as the \"brain\" of the framework, or- chestrating the operation of the all other components. It triggers and monitors the operation of the components in a sequence, configuring each step and delivering the necessary data from one step to the next. Many of the components in KModels communicate using Kubernetes (K8s) events, hence the Watcher listens on those and alerts the controller of the relevant events.\nThe two primary components within KModels architecture (Figure 5) are Kubeflow Pipelines [26] (or Data Science Pipelines) handling the entire training flow and KServe [27], serving the models. These components are widely accepted in the industry as a solid infrastructure for AI deployments. However, in a typical AI deployment, these components re- quire highly skilled MLOps engineers in order to set them up and configure the AI models to run on them. Using KModels, their installation, setup and configuration is handled by the Controller, significantly minimizing the overhead involved in their operation.\nKModels uses an S3-compliant storage for storing the data, models, and any other metadata required. The S3 standard defines a set of application programming interfaces (API) for storing data over a cloud infrastructure and is largely based on Amazon's S3 architecture [29]. KModels may be installed out of the box with a local S3 storage - Minio [30]. Alternatively, one can provision any S3-based service and provide its credentials to KModels during installation.\nThe Data Connectors and the Data Monitors are two sets of pluggable components supporting the extension of KModels. KModels provides an internal API for these services allowing them to receive notifications about key events in the lifecycle of the model and to gain visibility into the data.\nThe data connectors are tasked with fetching the data from any external source and serving it to the model at training time. Once a data connector is configured for a model instance, it will receive the necessary configuration from KModels and be triggered to operate at the relevant point. The data connectors are familiar with the logic of the data source they are targeting: How to connect to it, its schema, attributes, etc. They are responsible for mediating the source data format to the desired input format of the models. In order to promote alignment KModels defines a standard data format so that both template developers and connectors developers can use it. Having said that, this standard is merely a reference and specific template or connector may define their own suitable format for data.\nThe data monitors are expected to track the internal distri- bution of the data (and the predictions) over time and alert for any deviations. The monitors will be given access to the training and inference data and will apply internal logic to analyze the data for any potential issues. As an example, the drift detection service is an extension of KModels based on an open-source library, Evidently [31].\nKModels exposes a control API allowing application admin- istrators to control template and model management. The API is exposed in the form of a REST interface and a command- line interface (CLI) (Figure 4). The applications are advised to either wrap this API in a graphical user interface (GUI) or alternatively to automate the model management calls, while hiding the control logic from the end-users. Contrary to that, that applications must provide a GUI for the business users reviewing the models outputs. The reasoning is that, unlike the application administrators, that business users should not be required to use REST or the CLI to interact with the AI. All their interaction is GUI based and embedded in the application UI, while using the application terminology."}, {"title": "IV. DEPLOYMENT", "content": "KModels is deployed on a Kubernetes or OpenShift clus- ters, typically on-premise on a client's local data center but possibly also on a cloud data center. The installation requires a foundational layer of an AI platform. On OpenShift that layer is Open Data Hub while on Kubernetes it is represented as Kubeflow (Figure 2). Next, KModels is installed, either using manifest scripts [32] or an OpenShift Operator [33]. In practice, KModels only utilizes two services from the AI platform - Kubeflow (or Data Science) Pipelines, for the training pipelines and KServe for serving the models. Overall installation time for KModels and the AI platform typically takes less than 30 minutes.\nKModels framework can run on a single-node cluster with minimal resources. The resources that the framework con- sumes can conceptually be split to three types: AI platform resources, KModels resources, and models runtime resources. The first two types are mostly static and invariable to the number of models. In a minimal deployment, the system utilized as little as 4 CPUs and 6GB of memory, without any running models. As the system ramps up model instances, so do the resource requirements ramp up as well. These runtime requirements depend heavily on the type of models and their specific memory and CPU requirements. For example, a simple XGBoost classifier for tabular data may require less than 1 CPU and 1GB while on the other end a Large Language Model (LLM) may require a significant amount of CPUs and memory as well as GPUs. KModels allows the application administrators to limit the resource allocation on a per-model basis using the configuration given during model instantiation. Furthermore, using KServe for inferencing implies that the system automatically scales its runtime allocations pending on the actual requirements of the running models. KServe enables auto-scaling based on request volume and supports scale down to and up from zero.\nAfter KModels is deployed, the application administrator can browse the KModels template store, inspect the tem- plates' metadata and select the relevant templates for model deployment. Model creation involves customizing configura- tions (Figure 3), mapping local data to the template's input fields and providing additional arguments as business context (Figure 6). Users are able to experiment with multiple different configurations, easily creating multiple model instances from any single template. This flexibility allows the clients to adapt models on-site to diverse needs, such as applying different filters and temporal windows to incoming data. Additionally, numerous (dozens and more) models can be easily spawn with different configurations based on the same template, allowing the creation of models per specific data pattern. For example, one may train a separate model for different asset classes. The time it takes to configure and launch a new model is typically less than a few minutes. The training time and the time up till the model is ready for inferencing, depends on the model's template and the allocated system resources. Once the model training is launched, no user interaction is required."}, {"title": "V. EVALUATION", "content": "KModels was used to deploy three different AI models in an existing asset management application for a global real estate business. These model templates use three completely different underlying approaches with varying complexity and AI methods. The three models are:\n1) Failure Code Recommendation (FCR): A multi-class classification model (including synthetic data genera- tion); Recommends the most suitable code (from a total of usually 30-60 codes) based on the description (Figure 7) text within the records.\n2) Similarity Suggestions: This model identifies similar work orders (Figure 8) based on a combination of cate- gorical, spatial, temporal, numerical and textual features. Cosine similarity of contextual embedding is used for textual data processing.\n3) Approval Decision Recommendation: A binary classifi- cation model generating a recommendation to approve or reject a record [34]. This model utilizes a wide feature vector based on the records tabular data.\nThe FCR template has a complex architecture (Figure 7) consisting of multiple steps of varying complexity, including several data augmentation techniques ([35], [36], [37], [38]) with touch-points to multiple LLM models. However, this complexity is hidden from the end users allowing them to pro- vide merely a minimal set of configurations so that KModels runs the complex pipeline on their behalf.\nThe Similarity template's configuration supports different use cases using control and terminology which are familiar to a business user. For example, identifying recurring issues in a given time-window can be configured as: \"compare to recently closed records\" within a certain \"time-window size\". This can further trigger investigation to identify root causes. In another case, alerting for duplicate records that are opened for the same problem, in order to prevent sending two technicians to address the same problem can be done with a setting of: \"compare to open tickets\". A different setting using \"compare to completed tickets\" may support the case to retrieve historic records for similar issues, to provide information on how these issues have been addressed in the past.\nThe approval template exposes a configuration that allows the business users to tailor the model to their selected cohort. A default setup will not apply any filtering on the template allowing the instantiated model to run on the entire range of the data, producing a \"global\" model. An alternative setup, using a filter on the incoming data allows the model to train on specific sites, thereby supporting a notion of locality and allowing the creation of multiple \"small\" models each catering better to the data derived from its own locality. Users may leverage this ease of configuration in order to spawn numerous models, with little effort.\nThese diverse use cases demonstrate the versatility of the KModels framework, the flexibility it offers the template developer to work with different types of models and pipelines, and the simplicity it allows the end users to operate and con- sume the models. The ability to easily alter the configurations and produce multiple models supporting different use cases further underscores this flexibility and the empowerment that KModels offers to its business users.\nThe model code underwent rigorous development and test- ing using multiple client datasets to ensure robust performance. Through several iterations of offline testing, the processing pipelines were refined to handle data inputs of varying quality levels. This iterative process significantly improved the mod- els' ability to adapt to diverse real-world scenarios.\nEach model was then transformed into a KModels template by the development team. The conversion process varied in complexity and duration across the different models. The Approval model, being relatively straightforward, took only three days to convert. In contrast, the more complex Failure Code Recommendation (FCR) model required up to two weeks of effort to fully adapt it as a KModels template. This range in conversion times reflects the varying complexities of the models and the care taken to ensure they would function effectively within the KModels framework. However, in all of these cases, we consider this one-time effort as a minor investment compared to the simplicity gained in deploying them on multiple sites. The ability to easily replicate and customize these models across various client environments significantly outweighs the initial conversion effort.\nThe models were deployed using KModels along side an existing asset management application, at a client's site, with a throughput of several hundred newly opened work orders per day. The client made minor changes to their GUI to surface the recommendations to the end-users, as reflected in the table columns in Figure 9. The initial setup and installation of KModels at this client's site, took roughly one hour to complete. Then, the application administrator of the application used the provided CLI in order to instantiate and launch the training of the three models, within a few minutes (See demo in the supplementary material). The administrator acknowledged that this technique is a huge leap forward in terms of the overall time they invested when deploying other AI services in the past.\nFollowing the installation, the application administrator uti- lized the CLI to instantiate and initiate the training of all three models. This process was completed within minutes, demon- strating the streamlined nature of KModels (a demonstration video is available in the supplementary material). The admin- istrator emphasized that this approach represents a significant advancement in terms of time investment compared to their previous experiences deploying AI services. This substantial reduction in deployment time and complexity underscores the practical benefits of the KModels framework in real-world business environments.\nWhile this current deployment has successfully handled hundreds of work orders per day, KModels' architecture is designed to scale to much larger workloads, potentially sup- porting thousands of models across multiple business units in large enterprises.\nFollowing the introduction of the models' recommendation, the Failure Code specification for work orders improved from 46% to 83%, showcasing the substantial benefit of accessible and localized AI solutions. While this improvement doesn't directly demonstrate the capabilities of KModels, it illustrates that with the framework provided by KModels, a model de-ployed \"into the wild\" without prior training can autonomously learn from local data and achieve good performance, even with minimal oversight from an application administrator."}, {"title": "VI. LIMITATIONS AND THREATS TO VALIDITY", "content": "While KModels offers significant advantages for AI adop- tion in business applications", "approach": "n1) Template Development Overhead: The primary chal- lenge lies in the additional effort required from model developers to align their work with KModels' template structure and requirements. This overhead may initially slow down the development process, though it poten- tially leads to more robust and reusable models in the long term.\n2)"}]}