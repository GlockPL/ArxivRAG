{"title": "When Less Is Not More: Large Language Models Normalize Less-Frequent Terms with Lower Accuracy", "authors": ["Daniel B. Hier", "Thanh Son Do", "Tayo Obafemi-Ajayi"], "abstract": "Term normalization is the process of mapping a term from free text to a standardized concept and its machine-readable code in an ontology. Accurate normalization of terms that capture phenotypic differences between patients and diseases is critical to the success of precision medicine initiatives. A large language model (LLM), such as GPT-40, can normalize terms to the Human Phenotype Ontology (HPO), but it may retrieve incorrect HPO IDs. Reported accuracy rates for LLMs on these tasks may be inflated due to imbalanced test datasets skewed towards high-frequency terms. In our study, using a comprehensive dataset of 268,776 phenotype annotations for 12,655 diseases from the HPO, GPT-40 achieved an accuracy of 13.1% in normalizing 11,225 unique terms. However, the accuracy was unevenly distributed, with higher-frequency and shorter terms normalized more accurately than lower-frequency and longer terms. Feature importance analysis, using SHAP and permutation methods, identified low term frequency as the most significant predictor of normalization errors. These findings suggest that training and evaluation datasets for LLM-based term normalization should balance low- and high-frequency terms to improve model performance, particularly for infrequent terms critical to precision medicine.", "sections": [{"title": "I. INTRODUCTION", "content": "Modernist architect Mies van der Rohe is credited with the aphorism 'less is more' [4]. However, when large language models are tasked with normalizing biomedical terms to an ontology, a minimalist approach can lead to poor results. In this work, we propose that the accuracy of a large language model in a term normalization task is strongly influenced by the frequency with which the model encounters terms during training. High-frequency terms are normalized with greater accuracy than low-frequency terms.\nLarge pre-trained language models (LLM) can perform various natural language processing (NLP) tasks in health-care [19, 15], such as creating physician notes, summarizing text, answering questions, educating patients and preparing correspondence. An LLM can identify and normalize medical concepts [8, 6, 3, 16]. However, when an LLM normalizes a medical term to the Human Phenotype Ontology (HPO), the correct machine-readable code may not always be retrieved. The error rate for normalizing terms to the HPO has been reported to be between 40% and 60% [7, 13, 14]. However, due to class imbalances in test sets, often populated by common and easier-to-normalize terms, the error rate for a dataset balanced across all HPO terms is likely higher.\nIn this study, we examine the normalization of a compre-hensive list of HPO terms balanced for both rare and common terms. We hypothesize that most of the normalization errors made by an LLM are due to a lack of exposure to the terms during training. Although the HPO has over 17,000 terms that are descendants of Phenotypic abnormality (HP:0000119), not all terms are in use. The HPO annotations section has 268,776 annotations, each linking a term to a disease based on Orphadata [11] or Online Mendelian Inheritance in Man (OMIM) [2]. These annotations utilize a subset of 11,225 concepts in the HPO which vary in term frequency and term length. Approximately 7,128 of the HPO concepts are not used to represent diseases in the HPO. Our working hypothesis is that an LLM will experience more difficulty normalizing a term of lower frequency and longer length."}, {"title": "II. METHODS", "content": "We downloaded phenotype annotations of diseases from the Human Phenotype Ontology (HPO) and compiled a list of the 11,225 HPO terms utilized with their corresponding HPO IDs. The frequency of each term was calculated, rank-ordered, and assigned to to one of eight equal bins of approximately 1,400 terms based on frequency. Terms were assigned to five bins according to term length where Bin 5 was terms of length 5 words or longer. Terms were submitted to the GPT-40 API with a prompt to normalize each term by retrieving the correct HPO ID. Normalizations were evaluated as 'correct' or 'incorrect' based on the ground truth HPO ID. We calculated mean accuracy and term frequency for each frequency and length bin and conducted group comparisons by ANOVA. A logistic regression model was fitted to predict normalization accuracy (correct vs. incorrect), with feature importance assessed by permutation methods and SHAP. This experiment tested the hypothesis that longer term length and lower term frequency predict normalization errors by a LLM.\nWe downloaded the terms in the HPO from the NCBO (https://bioportal.bioontology.org/ontologies/HP) and all of the phenotype annotations from the HPO (https://hpo.jax.org/data/annotations). We retained 18,353 terms from the HPO and descendants of Phenotypic abnormality (Table I). The list of terms to normalize consisted of 11,225 terms used to annotate diseases in the Orphadata or OMIM databases of rare and genetic diseases (Fig. 4).\nThe GPT-40 API was prompted to return the HPO ID for each term (https://platform.openai.com/docs/api-reference, OpenAI, 2024). Each presented term was an exact match to a term in the HPO, so no assessment of semantic equivalence for approximate terms or near matches was needed. The ground truth for each normalization was the 7-digit HPO ID of the HPO. The LLM output was scored as binary \u20181' = \u2018correct if the LLM returned an HPO ID that matched the ground truth HPO ID.\nComparisons of mean accuracy across term length and term frequency bins were performed by ANOVA using the general linear (univariate) model from SPSS (version 29.02, IBM). The general linear model calculated interactions between term length and term frequency. Post hoc group comparisons were made using the Bonferroni method with P < 0.001 considered significant.\nWe fitted a logistic regression model (LogisticRegression from sklearn.linear_model) to predict the normalization status. Input features (Term Frequency and Term Length were standardized (StandardScaler from sklearn.preprocessing). Default parameters for the logistic regression model were used. The data set was imbalanced with 1,476 correctly normalized terms and 9,749 incorrect normalizations. The confusion matrix, pseudo-R squared, weighted precision, weighted recall, weighted F1, model coefficients, and y-intercept were calculated. Feature importance was assessed using the permutation method (permutation_importance from sklearn.inspection) and the SHAP method (python SHAP library) [1, 10, 5]."}, {"title": "III. RESULTS", "content": "The Human Phenotype Ontology (HPO) includes 268,776 phenotype annotations derived from 12,655 diseases in OMIM and Orphadata. We normalized 11,225 unique terms using the GPT-40 API, achieving an accuracy of 13.1%. HPO terms were ranked by their frequency in the annotations (Fig. 4) and grouped into eight frequency bins, from the highest (Bin 1) to the lowest (Bin 8). Additionally, terms were divided into five length bins, ranging from the shortest (Bin 1) to the longest (Bin 5).\nThe highest normalization accuracy was observed in the highest frequency bin (Bin 1) and declined sharply as term frequency decreased, falling to less than 20% after Bin 2 (Fig. 5). Similarly, the accuracy of normalization decreased with increasing term length (Fig. 6). As expected, term frequency also decreased with term length, with shorter terms being more frequent than longer terms.\nThe distribution of term ranks with term frequencies fol-lowed a Zipfian distribution [9], with a few high-frequency terms and many low-frequency terms (Fig. 4). Zipf's law has two important generalizations: 1) most vocabularies have many low-frequency words and few high-frequency words, and 2) high-frequency words tend to be shorter. Both of these patterns were observed in our analysis, where shorter terms occurred more frequently than longer terms (Fig. 6).\nFig. 1 illustrates the effects of term frequency and length on the accuracy of normalization. Accuracy declines steadily from high-frequency bins (Bin 1 and Bin 2) to lower-frequency bins (Bin 7 and Bin 8). In the highest frequency bins, unigrams (single-word terms) are normalized more accurately than longer terms. However, this term-length effect is not observed in lower-frequency bins (Bins 3 to 8). Statistical analysis (ANOVA) revealed significant differences in accuracy between the frequency groups of the term (df = 7, F = 687.5, p < 0.001) and the length groups of the term (df = 4, F = 6.2, p < 0.001). There was also a significant interaction between term length and frequency (df = 28, F = 6.5, p < 0.001), indicating that the effect of term length is more pronounced in the higher frequency bins.\nA logistic regression model was fitted to predict the ac-curacy of term normalization based on Term Length and Term Frequency (Tables III and III). The weighted average precision, recall and F1 was 0.90, 0.91 and 0.89, respectively. The Term frequency and Term Length coefficients (Table III) suggested that the accuracy of term normalization was favored by shorter terms and higher term frequencies, with the term frequency effect larger than the term length effect. SHAP plots (Fig. 2) and feature importance plots by permutation"}, {"title": "IV. DISCUSSION", "content": "GPT-40, an advanced large language model, makes errors when normalizing terms to the HPO. For the 11,225 terms in the HPO used for disease annotations by OMIM or Orphadata, the model accurately retrieved 13.1% of the machine-readable codes (HPO IDs). Retrieval errors are not evenly or randomly distributed across the HPO. Errors decrease with term fre-quency and increase with term length. When the HPO terms are ranked by frequency and then binned into eight bins of 1400 terms, most correctly normalized terms are in Bin 1 with the highest term frequencies. Shorter terms are normalized more accurately than longer terms, so that unigrams (one-word terms) are normalized with the greatest accuracy than longer terms.\nOne may reasonably wonder how large language models 'learn' to associate an HPO ID with an HPO term. It seems unlikely that current large language models access defini-tive websites, such as the HPO (https://hpo.jax.org/), and \"memorize\" associations between an HPO ID and an HPO term. Rather, it seems more likely that during the course of training, the model encounters instances in books or published literature where an HPO term appears in proximity to an HPO ID, allowing the model to \"learn\" this association through repeated encounters. A paper in the Orphanet Journal of Rare Disease [20] illustrates an opportunity for a large language model to 'learn' the association between ptosis and its HPO ID HP:0000508 (see Fig. 7). In summary, based on the architecture of an LLM, we suggest that the learning of HPO ID-term associations is constrained by the following factors:\nAssociations are statistical: The model learns associ-\nations based on probabilities derived from the training data.\nAssociations are proximity-based: Terms must appear in close proximity to their HPO IDs during training for the model to establish an association.\nAssociations have a frequency bias: The model learns more effectively when there are more frequent training examples.\nAssociations are pattern-based: The model identifies patterns in the data without direct access to external APIs or datasets.\nOur study has several limitations and implications. First, we examined the 11,225 terms used at least once in the OMIM and Orphadata data sets to annotate a disease. We did not evaluate the 7,128 unused HPO terms. We considered the frequency of HPO terms in the list of HPO annotations as a marker or surrogate for the frequency with which GPT-40 might have encountered HPO IDs during training, but we have no exact knowledge of what corpora GPT-40 was trained in or its training protocol. Our logistic regression model was a simplified model that considered two predictors (term frequency and term length). Other predictors might better explain GPT-40 normalization accuracy. Although the model suggests that term frequency is more important than term length, the magnitudes of these effects are approximate.\nOur results suggest that more attention needs to be directed at low-frequency terms. Although these terms are less frequent in clinical settings, their accurate normalization is essential to precision medicine. Although normalization accuracy can be improved by retrieval augmented generation (RAG) methods [17], steps need to be taken to improve training methods to address class imbalances in training to improve accuracy"}]}