{"title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models", "authors": ["Quan Wei", "Chung-Yiu Yau", "Hoi-To Wai", "Yang (Katie) Zhao", "Dongyeop Kang", "Youngsuk Park", "Mingyi Hong"], "abstract": "Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations, and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (ROSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on ROSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia and Llama models of different sizes demonstrate the effectiveness of ROSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures.", "sections": [{"title": "1. Introduction", "content": "LLMs have shifted a significant step toward achieving artificial general intelligence (Bubeck et al., 2023) and exhibit remarkable capabilities across different domains, including text generation (Anil et al., 2023; Touvron et al., 2023; Thoppilan et al., 2022), code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), and mathematical problem-solving (Cobbe et al., 2021; Trinh et al., 2024; Wei et al., 2022; Lewkowycz et al., 2022). To adapt LLMs to various applications and scenarios, supervised fine-tuning (SFT) is a standard approach, enabling models to leverage diverse training data and align with specific tasks based on pre-trained models.\nWhile fine-tuned models excel in domain-specific tasks, their substantial computational and storage demands present challenges for efficient deployment, particularly in resource-constrained environments (Xu et al., 2024a). To address these limitations, various model compression techniques have been developed, including quantization (Lin et al., 2023; Frantar et al., 2022), pruning (Ma et al., 2023; Sun et al., 2023), distillation (Xu et al., 2024b), and low-rank approximation (Wang et al., 2024a; Yuan et al., 2023). Among these, quantization is particularly effective for compressing LLMs, as it significantly reduces memory consumption, inference latency, and power usage. Additionally, its compatibility with specialized hardware accelerators enhances its practical deployment across diverse devices. Quantization techniques generally fall into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ is well-suited for quick deployment with minimal resources but often sacrifices accuracy in low-bit settings. In contrast, QAT achieves effective compression with minimal performance loss but requires retraining the entire LLM on a large corpus, incurring substantial computational costs.\nFor efficient deployment of task-specific LLMs, combining quantization with fine-tuning techniques offers a promising solution. A straightforward approach to obtaining quantized fine-tuned LLMs involves a two-step process: first fine-tune the pre-trained models, then apply quantization. However, applying quantization through PTQ in the second step often degrades the performance of the fine-tuned models, while QAT introduces an additional training phase, substantially increasing computational costs. Treating fine-tuning and quantization as separate steps can lead to suboptimal results, as it fails to exploit the synergy between these processes.\nThis work presents one of the first studies on quantization-aware fine-tuning (QA-SFT) to obtain effective fine-tuned and quantized LLM as a single training phase. To maximize the hardware capability of modern GPUs, we concentrate on designs utilizing the 4-bit quantization of weights, activations, and KV cache in LLMs. However, low-bit quantization presents a major challenge: performance degradation due to the weight and activation outliers, which expand the quantization range and increase the quantization error. Notice that even the high-performance data-free QAT methods (Liu et al., 2023) fail with 4-bit activation quantization.\nThe first key aspect of our work is to leverage rotation-based quantization in QA-SFT. Our work is inspired by recent findings on rotation-based PTQ methods (Ashkboos et al., 2024b; Liu et al., 2024), which demonstrate that applying offline and online rotations to linear projection layers and KV caches in LLMs effectively mitigates weight and activation outliers in post-trained models. However, when directly applied to QA-SFT, these PTQ methods fail to prevent outliers from re-emerging within layers during fine-tuning, resulting in performance degradation. To address this, we propose a joint training method combining the adaptive selection of rotation matrices and QA-SFT. The second key aspect of our work is to utilize a bilevel optimization formulation that simultaneously tackles QA-SFT and selects the rotation matrices based on the weights.\nThis paper proposes the Rotated Straight-Through-Estimator (ROSTE) algorithm that integrates the aforementioned ingredients. Our contributions are summarized as:\n\u2022 We introduce a novel SFT training problem that directly optimizes quantized weights and rotation matrices within a single model architecture. To tame the non-smooth manifold optimization, we propose a bilevel optimization formulation, where upper level subproblem optimizes weight matrices, while lower level subproblem employs a surrogate loss to guide the selection of rotation matrix.\n\u2022 To tackle the bilevel QA-SFT optimization, we propose the RoSTE algorithm which alternates between (i) a QAT subroutine incorporating a rotation-enabled straight-through-estimator (STE) update, and (ii) a low complexity heuristic for selecting rotation matrices based on the random Walsh-Hadamard matrix.\n\u2022 We provide a theoretical analysis of the benefits of rotation-enabled quantization in QA-SFT by examining the prediction error resulted from the QAT stage of ROSTE. This analysis directly motivates the use of quantization error based surrogate loss and justifies the adoption of the low-complexity Walsh-Hadamard rotation."}, {"title": "2. Preliminary", "content": "This section introduces ingredients that are essential to the proposed ROSTE algorithm through overviewing two major approaches for achieving efficient quantized LLMs."}, {"title": "2.1. Post-Training Quantization (PTQ)", "content": "The main objective of post-training quantization is to find a quantized model that preserves the behavior of the original model. While sophisticated quantizer designs such as vector quantization (Tseng et al., 2024; Egiazarian et al., 2024) can maintain a rich representation of weight values using < 2 bits on average, most existing works are limited to weight-only quantization. In contrast, for computationally efficient designs with quantized weights and activation, we focus on uniform quantization that compresses a full-precision tensor into one floating point scaling factor and a set of bit-width limited integers. This scheme is known for its practical efficiency across different modern hardware (Jacob et al., 2018; Ashkboos et al., 2024b).\nFormally, the b-bits uniform quantizer can be expressed as\n$Q(X) = s(X)( \\text{clamp} (\\frac{X}{s(X)} + z(X) )) - z(X)$,\nwhere X is a high-precision floating-point tensor; [.] denotes an element-wise nearest rounding; clamp() projects the values to the range of b-bits representable integers; , represent element-wise addition/subtraction between tensor and scalar. The choice of scaling $s(X) \\in \\mathbb{R}$ and shifting $z(X) \\in \\mathbb{Z}$ determines the range of which the b-bits integer tensor represents. For symmetric quantization, we adopt\n$s(X) = \\frac{\\text{max}(|X|)}{2^{b-1} - 1} c, \\quad z(X) = 0,$\n$\\text{clamp} (X) = \\text{max}\\{-2^{b-1}, \\text{min}\\{X, 2^{b-1} - 1\\}\\}.$\nwith $c \\in (0, 1]$ a clipping factor used to scale down the representation range so as to mitigate the impact of outlier values. To take advantage of the representation range in tensor with value distribution skewed away from 0, we can adopt asymmetric quantization by choosing\n$s(X) = \\frac{\\text{max}(X) - \\text{min}(X)}{2^b - 1} c, \\quad z(X) = \\frac{\\text{min}(X)}{s(X)},$\n$\\text{clamp}(X) = \\text{max}\\{0, \\text{min}\\{X, 2^b - 1\\}\\}.$\nThe above uniform quantization scheme reduces the memory consumption from storing a d-elements 32-bit floating-point tensor X using 32d bits, to storing an integer tensor with its shifting and scaling scalars using bd + b + 32 bits.\nIn practice, we partition a tensor into quantization groups such that each group has its own scaling and shifting $s(X)$, $z(X)$. Further description of quantizer hyperparameters used in our work will be provided in Appendix D.\nIncoherence Processing via Rotation. The precision of uniform quantization degrades as the representation range increases, especially when there are outlier values in the full-precision tensor. To reduce the effects of outlier values, incoherence processing was proposed in (Chee et al., 2024) which pre-multiplies an orthogonal matrix to the full-precision tensor prior to quantization, and post-multiplies the transposed orthogonal matrix to recover the original tensor. Later in (Ashkboos et al., 2024b; Tseng et al., 2024), incoherence processing by Walsh-Hadamard rotation is shown to be effective in both uniform quantization and vector quantization for 4-bits weight-activation quantization in LLMs.\nWe illustrate the idea of incoherence processing by constructing a multi-layer feedforward neural network with activation quantizer Qx and weight quantizer Qw. The output of the i-th linear layer is given by\n$\\text{LIN}_i(X; W_i, R_i) = \\sigma(Q_x(XR_i)Q_w(R_i^T W_i))$,\nwhere X is the input activation; W\u00bf is the pre-trained weight matrix; R\u00bf denotes rotation matrix and \u03c3 is any activation function. Notice that as $R_i R_i^T = I$, the architecture in (5) is invariant to any choice of rotation matrix R\u00bf when both quantizers $Q_w(\\cdot), Q_x(\\cdot)$ are error-free, i.e., $Q_w(x) = x, Q_x(x) = x$ are the identity map. In general when $Q(x) \\neq x$, it has been observed that these rotation matrices suppressed outliers within each quantizer and preserved the pre-trained model behavior during inference. On the downside, they impose extra memory and computation overhead since rotation is performed during inference within the activation quantizer. Thankfully, these overheads do not counteract the benefits of incoherence processing due to the fast Hadamard CUDA kernels (Ashkboos et al., 2024b; Tseng et al., 2024)."}, {"title": "2.2. Quantization-Aware Training (QAT)", "content": "The main objective of quantization-aware training (QAT) is to directly optimize a quantized neural network using gradient-based methods. From an optimization perspective, this is challenging as the quantization operator is not differentiable. To this regard, we focus on a popular remedy that has been studied before the era of LLMs is the straight-through estimator (STE) (Courbariaux et al., 2015; Bai et al., 2018) which approximates the Jacobian of quantizer by the identity matrix. During the backward calculation, the derivative of quantizer Q in the chain rule is replaced by\n$\\frac{\\partial Q(g(X))}{\\partial (X)} \\approx \\frac{\\partial g(X)}{\\partial X},$\nfor any differentiable function g. This approximation utilizes the insight that a quantizer behaves like an identity function in low resolution, while tolerating a gradient bias since quantization error persists in high resolution. In practice, STE is known to work well in training quantized neural network models (Li et al., 2017; Yin et al., 2019) as well as LLMs (Liu et al., 2023).\nThese QAT techniques are useful for the scenario when we consider obtaining a quantized LLM that minimizes the fine-tuning objective, as introduced below."}, {"title": "2.3. Supervised Fine-Tuning (SFT)", "content": "Foundation models that were pre-trained on large unstructured text corpus require fine-tuning to adapt their output behavior for specialized applications such as coding assistants (Chen et al., 2021) and instruction-following conversational chatbot (Ouyang et al., 2022). Towards this goal, Supervised fine-tuning (SFT) resumes the training of a given (pre-trained) model with the data distribution replaced by an application-specific curated dataset (Chung et al., 2024).\nIn specific, let $\\mathcal{D} := \\{(x_i, y_i)\\}_{i=1}^N$ denote the SFT dataset with N samples. For each $i \\in [N]$, $x_i \\in \\mathcal{X}$ is a sequence of input prompt and $y_i = (y_{i,0},..., y_{i,T-1})$ with $y_{i,t} \\in \\mathcal{Y}$ is a sequence of preferred output tokens. To fine-tune the model with $\\mathcal{D}$, we consider minimizing the following SFT loss:\n$\\mathcal{L}_{SFT}(m(\\cdot)) := \\mathbb{E}_i [-\\sum_{t=0}^{T-1} \\text{log} P(y_{i,t} | x_i, y_{i,<t}; m(\\cdot)) ]$,\nwhere the expectation is taken w.r.t. $i \\in \\{0, ..., N-1\\}$ with a uniform distribution, $y_{i,<t}$ denotes the sequence of tokens preceding $y_{i,t}$. The likelihood $P(y_{i,t} | x_i, y_{i,<t}; m)$ is the probability of the target token $y_{i,t}$ given the input $x_i$ and prior context $y_{i,<t}$, as predicted by the model m.\nAlthough it is a natural idea to apply QAT on fine-tuning tasks, existing works such as (Dettmers et al., 2023; Xu et al., 2023) (also see (Lee et al., 2024; Bondarenko et al., 2024) for similar ideas applied to training LLMs) only considered quantization aware adaptation utilizing an additional LORA architecture. Moreover, they focused on direct quantization without incoherence processing whose performance can be sensitive to outliers. This motivates us to consider integrating QAT with incoherence processing for SFT."}, {"title": "3. Proposed Algorithm: ROSTE", "content": "This section presents the Rotated Straight-Through-Estimator (ROSTE) algorithm through jointly optimizing the rotated quantizer and model parameters. To fix the idea, we parameterize the quantized LLM by the weight matrices $\\{W_i\\}_{i=0}^{l-1}$ and rotation matrices $\\{R_i\\}_{i=0}^{l-1}$. Consider the abstraction of an LLM with l layers/modules as $m_\\theta: \\mathcal{X} \\rightarrow \\mathbb{R}^{|\\mathcal{T}|}$, where $\\mathcal{X}$ is the set of sequences with variable context length, $\\mathcal{T}$ is the set of vocabulary, we denote\n$m_Q(x; \\{W_i, R_i\\}_{i=0}^{l-1}) := \\text{NN}(x; \\text{LIN}_i(\\cdot; W_i, R_i)\\}_{i=0}^{l-1})$,$\nfor any $x \\in \\mathcal{X}$, where LIN was defined in (5), and NN denotes the neural network architecture such as transformers\u00b9.\nWith the above parameterization, an ideal strategy is to consider the optimization problem:\n$\\underset{\\{W, R\\}_{i=0}^{l-1}}{\\text{min}} \\mathcal{L}_{SFT}(m_Q(\\cdot; \\{W_i, R_i\\}_{i=0}^{l-1})) \\quad \\text{s.t.} R_i R_i^T = I, i=0,..., l-1,$\nwhich simultaneously selects the weight and rotation matrices under quantization and directly optimizes the SFT objective of the quantized-and-rotated model.\nHowever, tackling (8) can be challenging even with approaches such as alternating optimization. This is because, upon fixing the weight matrices, minimizing the objective function w.r.t. the rotation matrices $\\{R_i\\}_{i=0}^{l-1}$ involves a non-tractable manifold optimization while the objective function is non-differentiable due to quantization. Meanwhile, when the rotation matrices are fixed, the minimization problem w.r.t. the weight matrices $\\{W_i\\}_{i=0}^{l-1}$ is similar to standard QAT; see (Liu et al., 2023).\nThe above obstacle motivated us to consider an alternative formulation (albeit somewhat heuristic) that simplifies the search for $\\{R_i\\}_{i=0}^{l-1}$ adapted to the weight matrices. This formulation explicitly separates the process of (quantized) model training and the rotation matrix optimization, and leverages a simpler loss function for the rotation matrix design. More specifically, we consider:\n$\\underset{\\{W\\}}{\\text{min}} \\mathcal{L}_{SFT}(m_Q(\\cdot; \\{W, R\\}_{i=0}^{l-1})) \\quad \\text{s.t.} \\{R_i\\}_{i=0}^{l-1} \\in \\underset{\\{R_i R_i^T = I, i=0,..., l-1\\}}{\\text{argmin}} \\mathcal{E}(\\{W, R\\}_{i=0}^{l-1}),$\nwhich is a bilevel optimization problem where the lower level optimal rotation matrices $\\{R\\}$ minimize the weight-activation quantization error:\n$\\mathcal{E}(\\{W, R\\}_{i=0}^{l-1}) = \\sum_{i=0}^{l-1} || Q_w(R_i^T W_i) - R_i^T W_i ||^2 \\\\ + \\frac{1}{n} \\sum_{i=0}^{l-1} \\sum_{j=0}^{n-1} || Q_x(X_{i,j} R_i) - X_{i,j} R_i ||^2,$\nfor $X_{i,j}$ representing the input activation of layer i on the j-th calibration data sample, e.g., by drawing a subset of size n from $\\mathcal{D}$ the fine-tuning dataset.\nNotice that now the optimal lower level variable aims at assisting the upper level weights so that an STE gradient approximation on $\\mathcal{L}_{SFT}$ w.r.t. $\\{W_i\\}_{i=0}^{l-1}$ has a smaller bias. However, it remains challenging for us to access the optimal rotation matrices for every iteration of the upper level minimization as solving the lower level problem can still be computationally expensive. In this regard, we propose a lazy lower level approximation where the rotation matrices are updated after T iterations of optimizing the weight matrices.\nThe RoSTE algorithm is now summarized in Algorithm 1 and Fig. 1. The algorithm is akin to alternating optimization and consists of two parts. The first part (cf. line 7\u20139) pertains to the QAT stage with SFT objective for selecting the weight matrices $\\{W_i\\}_{i=0}^{l-1}$ under the rotation matrices. Notice that STE gradient can be computed efficiently when $\\{R_i\\}_{i=0}^{l-1}$ are chosen as the Walsh-Hadamard matrices; also see (15). To our best knowledge, the subroutine used in this stage is the first direct application of STE to SFT training as well as its adaptation to the possibly varying rotation matrices. The second part (cf. line 5) pertains to the selection of rotation matrices in the lower level optimization which is a non-smooth problem on the manifold. Compared to (8), its objective function can be easily computed as the latter only represents the quantization error. Furthermore, as we will show in Sec. 4, the random Walsh-Hadamard matrix H yields an approximate-but-universal solution to minimize $\\mathcal{E}(\\cdot)$. As such, we propose to approximate the subproblem by limiting the search space to $R_i \\in \\{H, I\\}$ using a random Hadamard matrix H (Tseng et al., 2024) and perform a (low-complexity) combinatorial search to obtain an approximate lower level solution that decides if the rotation matrix should be applied on each layer. Details of this heuristic implementation can be found in Appendix D."}, {"title": "4. Theoretical Insights of ROSTE", "content": "This section aims at providing theoretical insights on the ROSTE algorithm that tackles the bilevel problem (11). In particular, we show that the quantization error (12) is a reasonable surrogate loss for optimizing the rotation matrices, provided that the weight matrices are optimized using the STE method as in Algorithm 1. Notice that the SFT objective is complicated and possibly untractable for analysis. To concentrate on the insights pertaining to using rotation in the quantized LLMs, we shall introduce a few approximations. We will use $\\langle \\cdot, \\cdot \\rangle$ to denote inner products of vectors, and $||x||_K = \\sqrt{\\langle x | Kx \\rangle}$ to denote a K-weighted squared norm of vector x for any square matrix K.\nOur setup follows from the literature on analyzing the convergence of SGD for neural networks under the interpolation regime (Ma et al., 2018; Vaswani et al., 2019). To describe it, let us fix the rotation matrices $\\{R_i\\}_{i=0}^{l-1}$ and consider the QAT stage (cf. line 7\u20139) in the RoSTE algorithm. Instead of analyzing $\\mathcal{L}_{SFT}(m_Q(\\cdot))$ directly, we consider the quadratic loss function as a simplified objective to draw insights for ROSTE. Moreover, the training dataset consists of samples (x, y) with a single output token in R such that $y \\in \\mathcal{Y} = \\mathbb{R}$. For any $m: \\mathcal{X} \\rightarrow \\mathbb{R}^{|\\mathcal{T}|}$, we now consider the squared prediction error:\n$\\mathcal{L}(m(\\cdot)) := \\frac{1}{2} \\mathbb{E} [(\\sigma(m(x)) - y)^2]$,\nin lieu of $\\mathcal{L}_{SFT}(\\cdot)$, where $\\sigma: \\mathbb{R}^{|\\mathcal{T}|} \\rightarrow \\mathcal{Y}$ maps the probability distribution over T to a token.\nWe further assume that the composite map $\\sigma(m(\\cdot))$ is a linear quantized model given by\n$\\sigma(m_Q(x; w, R)) = \\langle Q_x(Rx) | Q_w(Rw) \\rangle,$\nwhere R is a rotation matrix satisfying $RR^T = I$ and $Q_x, Q_w: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ are the quantization functions [see Sec. 2.1]. Let $x_t, y_t$ be the sample drawn at iteration t in the inner loop update of line 8, Algorithm 1, we have\n$w^{t+1} = w^t - \\eta g_{\\text{s.t.e.}},$\n$g_{\\text{s.t.e.}} = (\\langle Q_x(Rx_t) | Q_w(Rw^t) \\rangle - y_t) R^T Q_x(Rx_t),$\nwhere \u03b7 > 0 is the step size and we have used the STE approximation $\\frac{\\partial (Q_w(Rw))}{\\partial (w)} \\approx R$ when computing the stochastic gradient $g_{\\text{s.t.e.}}$ at $w^t$.\nOur next endeavor is to study an upper bound on the loss value of quantized model, $\\mathcal{L}(m_Q(\\cdot; w^T, R))$, after running the recursion (15) for T > 1 steps. Define the Gram matrix of the quantized-rotated features by\n$G := \\mathbb{E} [Q_x(Rx)Q_x(Rx)^T],$\nand make the following assumptions accordingly:\nAssumption 4.1 (Gram Matrix). There exists constants \u03bbmin, \u03c1 > 0 such that\n$G^2 \\succeq \\lambda_{\\text{min}} G, \\quad \\underset{0 < t < T-1}{\\text{sup}} || Q_x(Rx_t) ||_G \\leq \\rho.$\nThe above conditions are mild as \u03bbmin is only the smallest non-zero eigenvalue of the Gram matrix G and \u03c1 exists when the input prompts xt are bounded.\nAssumption 4.2 (Interpolation). For any orthogonal matrix R, there exists $w_R \\in \\mathbb{R}^d$ such that $y_\\xi = \\langle Q_x(Rx) | w_R \\rangle$ for any $x_\\xi$.\nThe above assumption requires that the quantized-rotated features $(x, y)$ are interpolatable by a full-precision model $w_R$. This assumption is closely related to the standard interpolation assumption that appeared in the literature on training with over-parameterized models (Ma et al., 2018; Vaswani et al., 2019). It is worth noticing that Assumption 4.2 does not require the interpolator $w_R$ to be in the quantized model parameter space (14).\nDefine the shorthand notation $m_R := m_Q(\\cdot; w^t, R)$, we observe the following convergence results for the QAT stage during the RoSTE algorithm:\nTheorem 4.3. Under Assumptions 4.1, 4.2 and the step size $\\eta = \\frac{\\lambda_{\\text{min}}}{6 \\rho}$, the objective value of the quantized model produced by the recursion (15) is bounded by\n$\\mathbb{E} [\\mathcal{L}(m^k)] \\leq (1 - \\frac{\\eta \\lambda_{\\text{min}}}{2})^{t+1} \\mathcal{L}(m_R)\\\\ + (6 + \\frac{2}{\\lambda_{\\text{min}}}) \\sum_{s=0}^{t} (1 - \\frac{\\eta \\lambda_{\\text{min}}}{2})^{t-s} \\mathbb{E} [||e(Rw^s)||_G^2]$\nfor any t \u2265 0, where $\\mu = \\frac{\\eta \\lambda_{\\text{min}}}{2\\rho}$ and $e(x) = Q(x) - x$.\nSee Appendix A for the proof. Our result shows that STE only converges to an inexact solution, which is consistent with previous findings on STE training. For instance, when training models with activation-only quantization, (Yin et al., 2019, Lemma 10) proved that the STE gradient is non-vanishing near local minima. For models with weight-only quantization, (Li et al., 2017, Corollary 1) only showed a convergence guarantee for the full-precision weights but not the quantized weights. In comparison to the prior findings, our result demonstrates the convergence of prediction error with quantized model.\nSuppose that the QAT stage of RoSTE is run with T \u226b 1 inner-loop iterations. Applying the theorem shows that given R, the resultant prediction error of the intermediate model $w^t$ will be bounded by $\\mathcal{O}(\\sum_{s=0}^{T-1} (1 - \\mu)^{T-s} \\mathbb{E} [||Q_w(Rw^s) - Rw_s||_G^2])$, i.e., a weighted sum of the weight quantization errors during the QAT process. Due to the exponential term $(1 - \\mu)^{T-s}$, the prediction error is dominated by the weight quantization error of recent iterates. Crucially, the above analysis shows that the rotation matrices play a pivoting role in the performance of QAT. This inspires us to apply $\\mathcal{E}(\\cdot)$ in (12) to guide us in the selection for optimal rotation matrices.\nRandomized Rotation Matrices. Now as we demonstrated that the quantization error is crucial to the prediction performance with the quantized model, we turn our focus to tackling the lower-level subproblem in (11). Notice that minimizing $\\mathcal{E}(\\cdot)$ w.r.t. the rotation matrix remains challenging. Instead of directly tackling the manifold optimization, our strategy is to apply the random Walsh-Hadamard matrix (Tseng et al., 2024) design as an approximate-yet-universal solution. Consider the random rotation matrix:\n$R(\\zeta) = H \\text{Diag}(r(\\zeta)),$\nwhere $H \\in \\mathbb{R}^{d \\times d}$ is a Walsh-Hadamard matrix (Fino & Algazi, 1976) and $r(\\zeta) \\in \\{-1, 1\\}^d$ is a random sign vector. Notice that R(\u03b6) is a binary matrix which favors efficient implementation on GPUs.\nWe observe the following proposition adapted from (Tseng et al., 2024, Lemma 3.1):\nProposition 4.4. Consider a bw-bits symmetric quantizer $Q_w: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ [cf. (2), (3) with c = 1]. For any $w \\in \\mathbb{R}^d$,\n\u2022 with R = I, it holds that\n$||Q_w(w) - w||^2 < \\frac{d \\text{max}_i w_i^2}{4 (2^{b_w} - 1 - 1)^2},$\n\u2022 with $R = R(\\zeta)$ from (19), with probability 1 \u2013 \u03b4 we have\n$||Q_w(R(\\zeta)w) - R(\\zeta)w||^2 < \\frac{12 ||w||^2 \\text{log}(4d / \\delta)}{2 (2^{b_w} - 1 - 1)^2}.$\nSee Appendix B for the proof.\nObserve that the quantization error is $\\mathcal{O}(d \\text{max}_i w_i^2)$ without rotation, and is $\\mathcal{O}(||w||^2)$ with rotation. Note that the former bound is more sensitive to weight vectors with outliers. In particular, the worst case prediction error in the QAT stage with R chosen as (19) is strictly better than that for the case with R = I (no rotation) if\n$\\frac{||w_R||^2 \\text{log}(4d / \\delta)}{2} < \\frac{\\text{max}_i (w_I)_i^2 d}{4},$\nwhere $w_R, w_I$ are the respective converged solutions of (15). It demonstrates that applying the random rotation matrix in (19) suffices to reduce the quantization error of weight matrices that contain outlier values. To obtain the best performance, we design the RoSTE algorithm such that at the outer loop, it chooses between H or I (i.e., no rotation) according to the current weight matrices.\nRemark 4.5. The analysis in (22) enables a novel interpretation of the bit-widths in Qx and Qw during STE training. On one hand, it is beneficial to increase the bit-width of activation quantization Qx until Assumption 4.2 is satisfied, and further increasing its bit-width would not improve the prediction performance as the bound (18) only depends on weight quantization error. On the other hand, increasing the bit-width of weight quantization always reduces the prediction error as seen in (18), (21). It is also interesting to see that despite adopting low-bit activation quantizers, increasing the dimension d may still allow us to satisfy the interpolation condition Assumption 4.2. This is due to the intuition that kernelized high dimensional features are more likely to be separable (Liang & Rakhlin, 2020). In other words, a neural network with high-dimensional hidden representations can tolerate low-bit quantized activations because the information about x retains in the high-dimensional discrete vector $Q_x(Rx_\\xi)$."}, {"title": "5. Experiments", "content": "We evaluate the performance of the proposed RoSTE algorithm for QA-SFT on two standard sets of open-source models and datasets. For the first experiment (Exp.1), we fine-tune the pre-trained Pythia 1B/6.9B models (Biderman et al., 2023) on the Reddit TL;DR Summarization dataset (Huang et al., 2024) with evaluation on the TL;DR test dataset using the ROUGE metric (Lin, 2004). For the second experiment (Exp.2), we fine-tune the pre-trained Llama 3.1 8B model (Dubey et al., 2024) on the Tulu 3 SFT mixture dataset (Lambert et al., 2024) with real-world downstream task evaluations (Gao et al., 2021). These tasks include TruthfulQA (Lin et al., 2021), MMLU-Pro (Wang et al., 2024b), BigBenchHard (Suzgun et al., 2022), AGIEval (Zhong et al., 2023), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2020).\nFor the RoSTE algorithm, while we relaxed the lower level as a l-variable binary combinatorial problem (9), solving this sub-problem has a complexity of $O(2^l)$ which is still intractable for models like Llama 3.1 8B with $l = 3 \\times 32 + 1$. As a remedy, we estimate the solution of (9) by sharing the rotation matrices across different layers. This reduces the problem into a 4-variable binary combinatorial optimization. Our experiment results suggest that the above shared variable approximation suffices to find rotation matrices that effectively reduce outliers. Lastly, we set K = 1 where a one-shot rotation configuration adaptation by pre-trained model is found to perform well. We anticipate the performance to further improve with increasing K. More implementation details can be found in Appendices C, D.\nBaselines. Besides the proposed RoSTE algorithm, we compare the performances of LLMs with quantized weight and activation obtained by two streams of baseline approaches. The first stream consists of applying PTQ methods on open-source supervised fine-tuned models in (Huang et al., 2024; Lambert et al., 2024). We reproduce the PTQ benchmarks using round-to-nearest (RTN) quantization, GPTQ (Frantar et al., 2022), knowledge distillation (LLM-QAT) (Liu et al."}]}