{"title": "AEMIM: Adversarial Examples Meet Masked Image Modeling", "authors": ["Wenzhao Xiang", "Chang Liu", "Hang Su", "Hongyang Yu"], "abstract": "Masked image modeling (MIM) has gained significant traction for its remarkable prowess in repre-sentation learning. As an alternative to the traditional approach, the reconstruction from corrupted images has recently emerged as a promising pretext task. However, the regular corrupted images are generated using generic generators, often lacking relevance to the specific reconstruction task involved in pre-training. Hence, reconstruction from regular corrupted images cannot ensure the difficulty of the pretext task, potentially leading to a performance decline. Moreover, generating corrupted images might introduce an extra generator, resulting in a notable computational burden. To address these issues, we propose to incorporate adversarial examples into masked image modeling, as the new recon-struction targets. Adversarial examples, generated online using only the trained models, can directly aim to disrupt tasks associated with pre-training. Therefore, the incorporation not only elevates the level of challenge in reconstruction but also enhances efficiency, contributing to the acquisition of superior representations by the model. In particular, we introduce a novel auxiliary pretext task that reconstructs the adversarial examples corresponding to the original images. We also devise an innova-tive adversarial attack to craft more suitable adversarial examples for MIM pre-training. It is noted that our method is not restricted to specific model architectures and MIM strategies, rendering it an adaptable plug-in capable of enhancing all MIM methods. Experimental findings substantiate the remarkable capability of our approach in amplifying the generalization and robustness of existing MIM methods. Notably, our method surpasses the performance of baselines on various tasks, including ImageNet, its variants, and other downstream tasks.", "sections": [{"title": "Introduction", "content": "Self-supervised learning (SSL) has achieved phe-nomenal success in natural language processing (NLP) tasks [1\u20134]. Inspired by masked language modeling, masked image modeling (MIM) has received a great deal of attention in the computer vision community [5\u20137]. Similarly, MIM masks a portion of image patches and then reconstructs the masked patches of the original data via an auto-encoder architecture. And the models can learn rich visual representations in this way. Com-pared with contrastive learning methods [8\u201310], SSL methods based on masked image modeling exhibit enhanced performance across a wide range of downstream vision tasks.\nSeveral recent studies [11\u201313] propose adopt-ing the reconstruction of corrupted images as a pretext task for pre-training, as an alterna-tive to the traditional approach of reconstruct-ing masked normal images. Their findings indi-cate that reconstructing corrupted images can assist the model in acquiring more generalized representations. However, the process of gen-erating corrupted images might necessitate an auxiliary generator [11], thereby imposing a sig-nificant extra computational burden. Moreover, these regular corrupted images are created using generic generators, frequently without direct rel-evance to the specific reconstruction task of the pre-training process. Compared to reconstructing masked normal images, reconstructing regular cor-rupted images does not inherently increase the dif-ficulty of the pretext task, but rather transforms the distribution of reconstructed images. There-fore, they cannot serve as consistently challenging reconstruction targets. This might result in some performance loss, as engaging with more challeng-ing pretext tasks can significantly facilitate the model in acquiring superior representations [14].\nA straightforward approach to synthesize the more challenging targets is to leverage adversarial examples [15\u201317]. Adversarial examples are delib-erately crafted to lower a model\u2019s performance on particular tasks. Their existence undeniably intro-duces substantial hurdles to task fulfillment. These adversarial examples are widely acknowledged as hard negative samples, rendering them more intri-cate reconstruction targets for pre-training. Addi-tionally, adversarial examples are often generated online, utilizing only the model currently being trained and eliminating the need for an addi-tional generator, thereby significantly enhancing training efficiency.\nNevertheless, studies [16, 18, 19] on adversarial training have revealed that if adversarial exam-ples are directly used for training, the performance of the model on normal vision tasks will signifi-cantly decline, even though its robustness could be enhanced. Fortunately, some recent studies [20, 21], like AdvProp [20] have explored the advan-tages of utilizing adversarial examples to enhance normal visual tasks. These studies demonstrate that co-training with both clean and adversarial images, and utilizing a small set of independent model parameters (known as \"adapters\") to han-dle inputs from different domains, can improve the performance of models on various visual tasks [20, 22, 23]. The researchers view adversarial examples as a regularizer that can prevent over-fitting dur-ing training. We believe that utilizing adversarial examples as the reconstruction targets in conjunc-tion with adapters can function as a regularization technique for MIM pre-training, ultimately lead-ing to improved representation learning.\nIn this paper, we propose to integrate adver-sarial examples into masked image modeling and explore their potential benefits in representation learning. We create an auxiliary pretext task of reconstructing corrupted images to provide regularization for the original pretext task and adopt the more challenging adversarial examples as the corrupted images. Adapters are employed in the model to handle data from different source domains. We develop a novel adversarial attack for MIM pre-training that can synthesize adver-sarial examples without relying on ground-truth labels, which are not available in SSL. To elab-orate further, we employ the distance between the encoder\u2019s feature representation of adversarial and clean data as the adversarial loss for gener-ating adversarial examples. This is based on our empirical observation that directly employing the reconstruction loss as adversarial loss leads the model to prioritize learning the auxiliary pretext task during pre-training, consequently impeding the learning of the primary pretext task. To train our model, we follow the AdvProp framework and adopt normalization layers as adapters to capture distinct adversarial and clean data statistics. Our proposed method, which boosts Masked Image"}, {"title": "Background", "content": "Inspired by the significant success of masked lan-guage modeling (MLM) in NLP like BERT [4], masked image modeling (MIM) similarly learns rich vision representations by predicting the raw information from the masked data. BEiT [5] intro-duces an offline tokenizer VQ-VAE [32] to tokenize the image into discrete vision tokens first and set up a patch-level dictionary like in MLM. The pre-text task is to recover the vision token ids of the masked patches. PeCo [33] proposes to train the visual tokenizer with a perceptual similarity reg-ularization term to learn a perceptual codebook for pre-training. Instead of using an offline tok-enizer, iBOT [34] set up an online tokenizer to produce the target for the encoder, building a self-distillation task for pre-training.\nAnother series of work directly predicts the raw pixel values or auxiliary features of the masked image patches. MAE [6] and SimMIM [7] propose to reconstruct the raw pixel values from only visible patches and all patches, respectively. CAE involves an encoder-regressor-decoder archi-tecture, where the regressor predicts the represen-tations of masked patches. Instead of predicting raw pixel values directly, MaskFeat [35] propose to use low-level features as the reconstruction target, e.g. Histograms of Oriented Gradients (HOG) [36]. Researches further explore applying high-level features as the target, such as pre-trained DINO [37], momentum features [38], and multi-modality features [37, 39, 40]. Unlike other MIM methods, AEMIM introduces an auxiliary proxy task to reconstruct adversarial examples or their high-level representations. Our framework is practically independent of specific MIM strategies, allowing it to be seamlessly integrated as a plugin into almost any MIM method.\nThere are also some works that integrate the ideas of adversarial learning. ADIOS [41] intro-duces an extra occlusion model to mask the images in an adversarial manner for MIM. NIM [13] adopts denoising as the pretext task and recon-structs noisy images to improve the adversar-ial robustness. CIM [11] is an effective MIM method that involves the reconstruction of cor-rupted images. It employs a trainable generator and a pre-trained frozen tokenizer to initially cor-rupt the input images and then performs the pre-training by recovering all the original image pixels, or predicting whether each visual token is corrupted or not. The generator and the backbone model are trained simultaneously and updated synergistically, resulting in a significant compu-tational burden and making the training process more difficult to converge to the optimal point. Compared to the above methods, AEMIM does not require an additional generator to produce adversarial examples. Instead, it generates them online using adversarial loss based on the training model, making AEMIM both simpler and more efficient.\nMost of the current MIM methods utilize an auto-encoder architecture. Let F denote the auto-encoder parameterized by \u03b8, and Lmim denote the reconstruction loss. For example, in the case of reconstructing raw pixel values, the objective function for MIM pre-training is written as\n$\\min_\\theta E_{x \\sim D} L_{mim}(F(x_m; \\theta), x),$ (1)\nwhere D is the data distribution, and $(.)_m$ denotes the masking operation."}, {"title": "Adversarial Learning and Vision Tasks", "content": "Adversarial examples can greatly degrade the performance of models by adding imperceptible perturbations to clean images. The goal of gen-erating an adversarial example $x_a$ is to find the one that can fool the model to predict the wrong result but is imperceptible to humans, which is expressed as\n$\\max_{x_a \\in S} L_{adv}(x_a, y, \\theta_f),$ (2)\nwhere x, y, $x_a$ denote the original image, the ground-truth label and the adversarial exam-ple, $L_{adv}$ denote the adversarial loss function, $\\theta_f$ denote the parameters of the model, $S = \\{x_a | ||x_a - x||_p \\leq \\epsilon\\}$, and $\\epsilon$ is the perturbation budget.\nUnder the white-box setting, where the model architecture, parameters, and gradients are acces-sible, the perturbations are generally generated with the guidance of gradients. FGSM [15] is the most famous one-step adversarial attack to generate adversarial examples simply and fast. PGD [16] extend FGSM to an iterative version, further improving the attack success rate of adver-sarial examples. C&W [42] formulates the problem in Lagrangian form and adopts Adam for opti-mization. There are also many black-box attacks, such as transfer-based attacks [17, 43\u201346], which generate the adversarial examples with a known source model and then use them to attack the unknown target model. Recent research is also exploring adversarial examples with high image quality that are closer to real-world data distri-butions [47, 48]. Among them, PGD is the most commonly used method for adversarial training. PGD simply performs the iterative updates as\n$x^{t+1} = \\Pi_{B_p(x, \\epsilon)} (x + \\mu \\cdot sign(\\nabla_{x} L_{adv} (x, y, \\theta_f))),$ (3)\nwhere $\\nabla_{x} L_{adv}(x, y, \\theta_f)$ is the gradient of the loss function $L_{adv}$ w.r.t. x, sign(\u00b7) is the sign function, \u03a0 is the projection operation, $B_p(x, \\epsilon)$ is the $L_p$ ball centered at x with radius $\\epsilon$, and \u03bc is the step size.\nTo facilitate the introduction of our approach, we revisit the co-training of clean and adversarial images in the classification task, which is proposed in FGSM. The objective function is expressed as\n$\\min_{\\theta} E_{(x,y)\\sim D}[\\lambda L_{cls}(f(x;\\theta_f), y)+\n(1-\\lambda) \\max_{x_a \\in S} L_{cls}(f(x_a; \\theta_f), y)],$ (4)\nwhere $f(; \\theta_f)$ is a classifier parameterized by $\\theta_f$, $L_{cls}$ is the classification loss, $\\lambda$ is the loss ratio, and S defines the legitimate range of adversar-ial perturbations as in Equation 2. Due to the absence of ground truth in SSL, our AEMIM method proposes using the distance between the representations of clean and adversarial data as the adversarial loss.\nNormal adversarial training can significantly improve the adversarial robustness of models. However, the performance of models on normal vision tasks often experiences a notable decline after being trained with adversarial examples. To address the issue, Xie et al. propose AdvProp [20] framework, which performs co-training with clean and adversarial images together and uses distinct batch normalization layers for clean and adver-sarial images to accumulate different statistics. They regard the adversarial item as a regular-izer. With the AdvProp framework, the perfor-mance of models on image recognition task is further enhanced. Chen et al. [23] further adopt AdvProp to object detection, and improves detec-tors\u2019 robustness against distortions and domain shifts. Mei et al. [22] propose Fast-AdvProp, which revamps the costly training components in AdvProp. Fast-AdvProp greatly accelerates the training process of AdvProp and almost no per-formance is lost. Rebuffi et al. [21] integrates adapters to adversarial training. They point out that the key element to making the co-training work is to \"have trainable parameters which are specific to the clean and adversarial images\". They show that other types of adapters also fit the co-training in addition to the batch normaliza-tion layer, e.g. classification token in the vision transformer. AEMIM integrates adapters into the co-training of clean data and specially designed adversarial data, further enhancing the generaliza-tion and robustness of the pre-trained model."}, {"title": "Methodology", "content": "Recent research [11\u201313] has highlighted the poten-tial of using the reconstruction of corrupted images as a promising pretext task for pre-training. We argue that adversarial examples pose more challenging reconstruction targets compared to typical corrupted images, and their genera-tion does not require any additional generator. To retain the benefits of the original pretext task, which involves reconstructing masked nor-mal images, we introduce a novel auxiliary pretext task of reconstructing their corresponding adver-sarial examples. We also incorporate adapters during pre-training to prevent adversarial exam-ples from compromising the model\u2019s performance on normal visual tasks.\nWe transfer the objective function of the co-training in Equation 4 from classification to MIM, where ground-truth labels are unavailable, and introduce adapters into the formulation. In the context of adapter literature, most of the model parameters are shared across data from different domains. Nonetheless, a distinct subset of parame-ters, known as adapters, exists specifically for data from different domains. In our case, we denote the shared parameters as \u03c8, the adapters for clean images as $\\phi_c$ and the adapters for adversarial images as $\\phi_a$. Obviously, the model parameters \u03b8 consist of \u03c8, $\\phi_c$ and $\\phi_a$. And we only consider the normalization layer and classification token in the vision transformer as the adapters in our method. Then the objective function of the co-training for MIM is rewritten as\n$\\min_{\\theta} E_{x \\sim D}[\\lambda L_{mim}(F(x_m;\\psi \\cup \\phi_c), x)+\n(1-\\lambda) L_{mim}(F(x_m; \\psi \\cup \\phi_a), x)],$ (5)\nwhere the adversarial example $x_a$ is generated online with the target of maximizing adversarial loss $L_{adv}$. And we provide more details on the design of $L_{adv}$ in the next section.\nUnlike in supervised learning, there are no ground truth labels to guide the generation of adversarial examples during MIM pre-training. A straightforward approach is to use the task loss directly as the adversarial loss for generat-ing adversarial examples. However, our empirical observations suggest that employing the recon-struction loss as the adversarial loss directly can hinder the learning of the original pretext task due to interference from the auxiliary task. Figure 2 illustrates the change in loss during the pre-training process. As shown in the figure, the reconstruction loss of adversarial data $L_a$ becomes much lower than that of clean data $L_c$ during training. This phenomenon suggests that the model might prioritize solving the auxiliary pretext task during pre-training, which adversely affects the learning of the original pretext task. We believe this is because the aforementioned adver-sarial examples directly target the original pretext task, resulting in overly strong regularization."}, {"title": "Adversarial Examples for Masked Image Modeling", "content": "We then design a new adversarial loss, which is only corresponding to the encoder in the MIM framework. We propose to adopt the distance of the encoder\u2019s feature representation between adversarial and clean data as the adversarial loss, since: 1) It does not directly target the reconstruc-tion task. 2) The encoder is the backbone model that is finally transferred to downstream tasks as the feature extractor. The adversarial data only need to regularize the encoder to learn general-ized representations. 3) Dropping the decoder can accelerate the generation of adversarial examples. Denote the encoder as E and the decoder as D. The objective function of generating adversarial examples is expressed as\n$\\max_{x_a \\in S} L_{adv}(x, x_a, \\theta),$ (6)\n$L_{adv} = L_d(E(x_m, \\psi \\cup \\phi_c), sg(E(x_m, \\psi \\epsilon \\cup \\phi_\u20ac))),$ (7)\nwhere $\\theta^e$ is the parameters of the encoder, con-sisting of the shared parameters $\\psi$, the clean adapters $\\phi_c$ and adversarial adapters $\\phi_a$, $L_d$ is one type of distance loss, sg(\u00b7) means stop-gradient and S is set to $S = \\{x_a | ||x_a - x||_\\infty \\leq \\epsilon\\}$.\nFor $L_d$, we mainly consider $L_2$ distance and KL divergence. In our method, we apply PGD-K to generate adversarial examples, where K is the iter-ation number. We initialize $x_a$ by adding random noise to x in the first step since there is no differ-ence between x and $x_a$ at first. The algorithmic details are presented in Algorithm 1."}, {"title": "Algorithm Details", "content": "In this section, we present the complete pro-cess of using adversarial examples to enhance MIM pre-training. Similar to adversarial train-ing in PGD [16], we formulate the co-training as a min-max game. Each iteration consists of two steps. During the inner maximization step, the model parameters $\\theta^e$ are fixed, while the adver-sarial examples are generated using the method outlined earlier. During the outer minimization step, both clean images and adversarial images are fed into the model to optimize the parameters $\\theta$ through two pretext tasks. We also provide an accelerated version of our algorithm. In the stand version, we generate an adversarial image for each clean one, and then perform joint training with the paired data. We also employ K > 1 for PGD to enhance the quality of adversarial examples. In the expedited version, we only select a portion of the clean data to generate corresponding adversar-ial examples and set K = 1 for the PGD attacker, which implies the PGD attacker degrades into an FGSM attacker with random noise initialization. In our framework, we ultimately randomly select $a\\%= 25\\%$ of the clean images to generate cor-responding adversarial images. Note the ratio $a\\%$ is 100\\% in the standard version. We present the above process in the pseudo-code form in Algo-rithm 1, and illustrate the complete pipeline of our proposed framework in Figure 1."}, {"title": "Experiments", "content": "We conduct the pre-training on the training dataset of ImageNet-1K [24] without labels. We adopt MAE [6] as our basic framework and enhance its performance through our proposed method. Therefore, MAE serves as the main method for comparison in our experiments. It should be emphasized that our method is not con-fined to particular model architecture or MIM strategies. As such, it can be easily integrated into other MIM methods as a plug-in to enhance their performance. The vanilla ViT-S/16 and ViT-B/16 [49] are utilized as the backbone models for the encoder. After the pre-training stage, we dis-card all the decoders and only keep the encoders for fine-tuning on downstream tasks. The adapters for adversarial examples are omitted as well, as they could potentially impact the model\u2019s clean accuracy. All models are trained on 8 V100 32GB GPUs.\nPre-training setup We follow most of the MIM settings in MAE. By default, we randomly mask 75% of the image patches. The decoder consists of 8 transformer layers with 512 feature dimensions and 12 attention heads. We simply use random cropping and horizontal flip as the data augmen-tations. The models are pre-trained for 300/800 epochs with 15/40 warmup epochs, respectively. And the total batch size is 4096. We adopt AdamW [50] optimizer with a base learning rate of 1.5 \u00d7 10\u22124, a weight decay of 0.05, \u03b2\u2081 = 0.9, \u03b22 = 0.95 and cosine learning rate schedule [51]. For attack settings, the perturbation budget is set as $\\epsilon = 2$ and the number of attack steps is set as T = 2/1 for stand AEMIM and Fast AE\u041c\u0406\u041c. We adopt the normalization layer as the default type of adapter and L2 distance as the default dis-tance loss for adversarial examples. And the loss ratio \u03bb is empirically set to be 0.5 and the adver-sarial ratio is set as 100%/25% for AEMIM/Fast AEMIM. We simply use random cropping and hor-izontal flip as the data augmentations. And we use Xavier uniform [52] to initialize all Transformer blocks, following MAE. All the default setting is shown in Table 1.\nFine-tuning on the ImageNet-1K All the models are fine-tuned with image resolutions of 224 x 224. The Vit-S/16 and Vit-B/16 are fur-ther trained for 200/100 epochs, with 5 warmup epochs. The drop path rate [53] is set to 0.1, and the batch size is 2048. For ViT-S/16, we set the base learning rate and the layer-wise learn-ing rate decay to 1e-3 and 0.8. For ViT-B/16, the base learning rate and the layer-wise learn-ing rate decay are set to 1e-3 and 0.7 for models pre-trained with 300 epochs, and set to 5e-4 and 0.65 for models pre-trained with 800 epochs. We train each model with strong data augmentation including label smoothing [54], mixup [55], cut-mix [56], and randAugment [57]. We use the global pooling feature rather than the class token dur-ing fine-tuning, following MAE. All models are fine-tuned using an image resolution of 224 x 224. And we also use the linear lr scaling rule: lr = base lr \u00d7 batch size/256. The default set-ting is shown in Table 2. And we also evaluate the robustness of models on ImageNet variants such as ImageNet-Real [25], ImageNet-V2 [26], ImageNet-A [27], ImageNet-R [28], ImageNet-Sketch [29] and ImageNet-C [30].\nFine-tuning on the MS-COCO The COCO dataset [31] is commonly used as a benchmark for evaluating object detection frameworks. The Mask R-CNN framework [58] is adopted to pre-dict bounding boxes and instance masks of objects for object detection and instance segmentation tasks. The pre-trained Vit-S/16 and Vit-B/16 are employed as the backbone for Mask R-CNN [58]. We first fine-tune Mask-RCNN using the COCO [31] train2017 split and then evaluate"}, {"title": "ImageNet and its IID variants Results", "content": "As mentioned above, the encoders are fine-tuned on the ImageNet-1K dataset in a supervised man-ner after the pre-training stage. Since we adopt MAE as the basic MIM framework without alter-ing any MIM strategies, we mainly compare our method with MAE in experiments to demonstrate its gains. It should be emphasized that our method is independent of any specific MIM methods or model structures, and can also be used as a plugin to enhance other basic MIM methods.\nTable 4 shows the results of Vit-S/16 and Vit-B/16 on the ImageNet validation dataset and its IID variants ImageNet-Real, ImageNet-V2. The Vit-S/16 pre-trained by AEMIM for 300 epochs reaches a fine-tuning accuracy of 81.4%, which has an improvement of 0.5% compared with MAE. For Vit-B/16, our 300-epoch model achieves 83.4%, which is even slightly higher than the model of MAE at 800 epochs. When enlarging the training scheduler to 800 epochs, our model of Vit-B/16 achieves 83.8% Top-1 accuracy, which is 0.5%/0.2% higher than the MAE baseline at 800/1600 epochs. For the ImageNet IID variants, our method also outperforms MAE under different training schedulers. For example, our best model of Vit-B/16 outperforms MAE by 0.3% and 0.8% on ImageNet-Real and ImageNet-V2, respectively. The results show that our Fast AEMIM outper-forms the baseline MAE and CIM, requiring only 1.3x the training cost of MAE for each epoch, which is much faster than the previous CIM. The model trained with Fast AEMIM for 800 epochs exhibits improved generalization compared to the model trained with MAE for 1600 epochs while utilizing only about 65% of the training cost. The detailed training cost analysis is shown in Sec. 4.6."}, {"title": "ImageNet OOD Variants Results", "content": "We test the robustness of our models on ImageNet OOD variants, including ImageNet-A, ImageNet-R, ImageNet-Sketch, and ImageNet-C. The Top-1 accuracy is used as the evaluation metric for all datasets, but for ImageNet-C. For ImageNet-C, we use mean corruption error (mCE) as the metric, and a lower value is better. And (1-mCE) is used to get the final Score, which is the average of all the results.\nTable 5 compares the robustness of our models to the MAE baseline. The results show that our Vit-B/16 model trained with AEMIM for only 300 epochs outperforms MAE\u2019s best model trained for 1600 epochs. Specifically, our 300/800-epoch Vit-B/16 model trained with AEMIM achieves an average score increase of 0.7/2.2% compared to the best model of MAE. The model trained with Fast AEMIM also shows a 0.7/1.6% improvement over MAE\u2019s best model. The results demonstrate that our method assists the model in learning more generalized representations, thereby signif-icantly enhancing the model\u2019s generalization to OOD datasets."}, {"title": "Adversarial Robustness Results", "content": "In addition to evaluating the robustness of our method on OOD datasets, we also evaluate its adversarial robustness. During the pre-training stage, we utilize the PGD attack with 1 or 2 attack steps to generate adversarial examples, which is essentially closer to the FGSM attack. Thus, we first employ FGSM as the attacker. In the eval-uation, we employ the same set of 5000 images as defined in the adversarial robustness bench-mark ARES-Bench [19], which are drawn from the ImageNet validation dataset. Table 6 presents the accuracy of different models across varying per-turbation budge $\\epsilon$. The results suggest that our method achieves a slight improvement in adversar-ial robustness compared to MAE under the FGSM attack. Our method performs better at small $\\epsilon$, while demonstrating slightly weaker performance at larger $\\epsilon$.\nThen, we escalate the attack to the stronger PGD with 20 attack steps. During the fine-tuning stage, we conduct standard training rather than adversarial training. Consequently, when confronted with stronger attacks, a substantial increase in $\\epsilon$ can quickly lead to a sharp drop in accuracy to 0. To address this issue, we draw the robustness curves of classification accuracy vs. perturbation budget based on ARES-Bench, as illustrated in Figure 3. The results indicate that our method exhibits significantly improved adversarial robustness compared to MAE when facing stronger attacks, although it still falls short of models trained with adversarial train-ing. As an example, when the perturbation budget $\\epsilon/255$ is 0.002, the 800-epoch model trained using AEMIM exhibits an accuracy improvement of (16.0-11.9)\\%=4.1% compared to the 1600-epoch model trained using MAE."}, {"title": "Downstream Task Results", "content": "For the downstream task, we transfer our pre-trained models to object detection and instance segmentation tasks on the COCO dataset. We report the $A_{P^{box}}$ and $A_{P^{mask}}$ of our method for object detection and instance segmentation, com-paring with the baseline in Table 7. The results of the baseline are reproduced using the official code under our experimental settings. For Vit-S/16, our Fast AEMIM/AEMIM surpass MAE by 0.4/1.6% and 0.4/1.4% for $A_{P^{box}}$ and $A_{P^{mask}}$. For Vit-B/16, Fast AEMIM/AEMIM improve $A_{P^{box}}$ and $A_{P^{mask}}$ by 0.8/0.4% and 0.4/0.5%, compared"}, {"title": "Pre-training Cost Analysis", "content": "We compare the pre-training cost of our method with the baseline methods BEiT [5], MAE [6] and CIM [11]. In the CIM framework, aside from the standard auto-encoder model, there are additional components, including the BEiT-Style generator and the DALL-E tokenizer. Within this setup, the auto-encoder and generator are actively being trained, while the tokenizer remains in a frozen state. This introduces a significant additional com-putational burden, causing CIM to be even slower than BEiT. The pre-training time for each epoch of CIM is approximately 1.8\u00d7 that of BEiT. Com-pared to CIM, our method incurs additional com-putational burden due to the increased gradient backpropagation needed for generating adversarial examples, as well as the co-training of generated adversarial data. In this paper, our method is founded on the MAE framework. Thus, the pre-training time per epoch for AEMIM is about 2.2-2.3x that of MAE. In the case of Fast AEMIM, we considerably decrease the quality and quan-tity of generated adversarial examples, leading to a significant reduction in the additional computa-tional cost. The pre-training time per epoch for Fast AEMIM is only around 1.3\u00d7 that of MAE.\nTable 4 and Table 5 presents the rela-tive pre-training time of different methods and their corresponding performance. For Vit-S/16, although CIM slightly outperforms our method on ImageNet validation, its computational cost"}, {"title": "Ablations", "content": "In this section, we first conduct a series of ablation experiments to investigate the impact of impor-tant components and validate design choices in our method. Here we focus on studying how adver-sarial examples affect MIM pre-training instead of the MIM strategies. The corresponding ablation studies are performed using a Vit-S/16 model that is pre-trained for 300 epochs with AEMIM, fol-lowed by 200 epochs of fine-tuning on ImageNet-1K. We adopt the normalization layer as the adapter and L2 distance loss as the adversarial loss by default. And we set $\\epsilon = 2$, T = 2, $\\lambda = 0.5$ for perturbation budget, number of attack steps, and loss ratio, unless specified.\nPerturbation budget. The perturbation budget affects the regularization strength of adversarial examples. Table 8a presents the results of using different perturbation budgets to generate adver-sarial examples during pre-training. The results show that a moderate perturbation budget $(\\epsilon = 2)$ is more effective for regularizing pre-training.\nAttack steps. The number of attack steps has an impact on the quality of generated adversarial examples. As the number of steps increases, the quality of adversarial examples improves, but the efficiency of generation decreases.\nThe results in Table 8b show that generating adversarial examples with two steps is sufficient. We finally set the number of steps to 2 for efficiency.\nAdapter. Adapters are crucial for co-training with data from different domains. Table 8c shows the impact of adapters on our method. Unlike in supervised learning, our approach still yields some improvements in pre-training even when the adapters are removed. For the type of adapter, the results demonstrate that both the classifica-tion token and normalization layer work well with our method.\nAdversarial loss. Table 8d shows the results of using different adversarial loss to generate adver-sarial examples during pre-training. As Figure 2 indicates, using the reconstruction loss as the adversarial loss directly can shift the pre-training focus towards the auxiliary pretext task, thus impacting the model\u2019s representation learning. Nevertheless, using the reconstruction loss can still be advantageous for pre-training, highlighting that our designed auxiliary pretext task itself can also help the model learn good representations. As for the type of adversarial loss, L2 distance loss works best in our method.\nLoss ratio. The loss ratio balances the two pre-text tasks during pre-training. Figure 4 illustrates the results with different loss ratios $\\lambda$. And we empirically set $\\lambda = 0.5$ in our method.\nWe then conduct a light ablation experiment to verify that the proposed method can boost other MIM frameworks and architectures. In the exper-iment, we employ SimMIM [7] as the basic MIM framework and select Swin-B [60] as the backbone model. We follow all of the MIM training settings from SimMIM. For the attack settings, we adopt the default settings from AEMIM. That means we adopt the normalization layer as the adapter"}, {"title": "Discussion", "content": "Our method serves to assist the current MIM methods in approaching their performance limits, rather than directly improving those limits, which would necessitate the design of improved MIM strategies and model architectures. Additionally, in our framework, we primarily use traditional noise-based adversarial examples. However, the community has recently seen the emergence of new types of adversarial examples, such as on-manifold adversarial examples [47, 61\u201363]. These novel examples maintain their adversarial proper-ties while exhibiting better image quality, closer to the original data distribution. As a result, they perform better in enhancing the performance of visual models [48]. Using improved adversarial examples in our framework could enhance per-formance further, but there is a potential risk of reduced training efficiency due to the slower gen-eration of these examples. We leave it to the future work."}, {"title": "Conclusion", "content": "In this paper, we propose a novel method called AEMIM, which incorporates adversarial examples into the masked image modeling (MIM) technique for pre-training. Our approach introduces a new auxiliary pretext task of reconstructing the cor-responding adversarial examples, in addition to the original pretext task of reconstructing masked normal images. We also design a novel adversar-ial attack to synthesize adversarial examples for MIM pre-training, using the distance between the encoder\u2019s feature representation of the adversarial data and the clean data as the adversarial loss. To enhance efficiency, we have also developed a faster version of our method. This optimized version maintains the core functionalities while signifi-cantly reducing computation time, making it suit-able for applications where speed is crucial. Our method surpasses the baseline on various datasets, including ImageNet-1K, ImageNet variants, and the downstream COCO datasets, demonstrating its effectiveness in enhancing the current MIM method. We believe that this work can provide the community with a fresh perspective on the rela-tionship between adversarial examples and MIM approaches, potentially serving as inspiration for future research endeavors."}]}