{"title": "AEMIM: Adversarial Examples Meet Masked Image Modeling", "authors": ["Wenzhao Xiang", "Chang Liu", "Hang Su", "Hongyang Yu"], "abstract": "Masked image modeling (MIM) has gained significant traction for its remarkable prowess in representation learning. As an alternative to the traditional approach, the reconstruction from corrupted images has recently emerged as a promising pretext task. However, the regular corrupted images are generated using generic generators, often lacking relevance to the specific reconstruction task involved in pre-training. Hence, reconstruction from regular corrupted images cannot ensure the difficulty of the pretext task, potentially leading to a performance decline. Moreover, generating corrupted images might introduce an extra generator, resulting in a notable computational burden. To address these issues, we propose to incorporate adversarial examples into masked image modeling, as the new reconstruction targets. Adversarial examples, generated online using only the trained models, can directly aim to disrupt tasks associated with pre-training. Therefore, the incorporation not only elevates the level of challenge in reconstruction but also enhances efficiency, contributing to the acquisition of superior representations by the model. In particular, we introduce a novel auxiliary pretext task that reconstructs the adversarial examples corresponding to the original images. We also devise an innovative adversarial attack to craft more suitable adversarial examples for MIM pre-training. It is noted that our method is not restricted to specific model architectures and MIM strategies, rendering it an adaptable plug-in capable of enhancing all MIM methods. Experimental findings substantiate the remarkable capability of our approach in amplifying the generalization and robustness of existing MIM methods. Notably, our method surpasses the performance of baselines on various tasks, including ImageNet, its variants, and other downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has achieved phe- nomenal success in natural language processing (NLP) tasks [1-4]. Inspired by masked language modeling, masked image modeling (MIM) has received a great deal of attention in the computer vision community [5-7]. Similarly, MIM masks a portion of image patches and then reconstructs the masked patches of the original data via an auto-encoder architecture. And the models can learn rich visual representations in this way. Com- pared with contrastive learning methods [8-10], SSL methods based on masked image modeling exhibit enhanced performance across a wide range of downstream vision tasks.\nSeveral recent studies [11-13] propose adopt- ing the reconstruction of corrupted images as a pretext task for pre-training, as an alterna- tive to the traditional approach of reconstruct- ing masked normal images. Their findings indi- cate that reconstructing corrupted images can assist the model in acquiring more generalized representations. However, the process of gen- erating corrupted images might necessitate an auxiliary generator [11], thereby imposing a sig- nificant extra computational burden. Moreover, these regular corrupted images are created using generic generators, frequently without direct rel- evance to the specific reconstruction task of the pre-training process. Compared to reconstructing masked normal images, reconstructing regular cor- rupted images does not inherently increase the dif- ficulty of the pretext task, but rather transforms the distribution of reconstructed images. There- fore, they cannot serve as consistently challenging reconstruction targets. This might result in some performance loss, as engaging with more challeng- ing pretext tasks can significantly facilitate the model in acquiring superior representations [14].\nA straightforward approach to synthesize the more challenging targets is to leverage adversarial examples [15-17]. Adversarial examples are delib- erately crafted to lower a model's performance on particular tasks. Their existence undeniably intro- duces substantial hurdles to task fulfillment. These adversarial examples are widely acknowledged as hard negative samples, rendering them more intri- cate reconstruction targets for pre-training. Addi- tionally, adversarial examples are often generated online, utilizing only the model currently being trained and eliminating the need for an addi- tional generator, thereby significantly enhancing training efficiency.\nNevertheless, studies [16, 18, 19] on adversarial training have revealed that if adversarial exam- ples are directly used for training, the performance of the model on normal vision tasks will signifi- cantly decline, even though its robustness could be enhanced. Fortunately, some recent studies [20, 21], like AdvProp [20] have explored the advan- tages of utilizing adversarial examples to enhance normal visual tasks. These studies demonstrate that co-training with both clean and adversarial images, and utilizing a small set of independent model parameters (known as \"adapters\") to han- dle inputs from different domains, can improve the performance of models on various visual tasks [20, 22, 23]. The researchers view adversarial examples as a regularizer that can prevent over-fitting dur- ing training. We believe that utilizing adversarial examples as the reconstruction targets in conjunc- tion with adapters can function as a regularization technique for MIM pre-training, ultimately lead- ing to improved representation learning.\nIn this paper, we propose to integrate adver- sarial examples into masked image modeling and explore their potential benefits in representation learning. We create an auxiliary pretext task of reconstructing corrupted images to provide regularization for the original pretext task and adopt the more challenging adversarial examples as the corrupted images. Adapters are employed in the model to handle data from different source domains. We develop a novel adversarial attack for MIM pre-training that can synthesize adver- sarial examples without relying on ground-truth labels, which are not available in SSL. To elab- orate further, we employ the distance between the encoder's feature representation of adversarial and clean data as the adversarial loss for gener- ating adversarial examples. This is based on our empirical observation that directly employing the reconstruction loss as adversarial loss leads the model to prioritize learning the auxiliary pretext task during pre-training, consequently impeding the learning of the primary pretext task. To train our model, we follow the AdvProp framework and adopt normalization layers as adapters to capture distinct adversarial and clean data statistics. Our proposed method, which boosts Masked Image"}, {"title": "2 Background", "content": "2.1 Masked Image Modeling\nInspired by the significant success of masked lan- guage modeling (MLM) in NLP like BERT [4], masked image modeling (MIM) similarly learns rich vision representations by predicting the raw information from the masked data. BEiT [5] intro- duces an offline tokenizer VQ-VAE [32] to tokenize the image into discrete vision tokens first and set up a patch-level dictionary like in MLM. The pre- text task is to recover the vision token ids of the masked patches. PeCo [33] proposes to train the visual tokenizer with a perceptual similarity reg- ularization term to learn a perceptual codebook for pre-training. Instead of using an offline tok- enizer, iBOT [34] set up an online tokenizer to produce the target for the encoder, building a self-distillation task for pre-training.\nAnother series of work directly predicts the raw pixel values or auxiliary features of the masked image patches. MAE [6] and SimMIM [7] propose to reconstruct the raw pixel values from only visible patches and all patches, respectively. CAE involves an encoder-regressor-decoder archi- tecture, where the regressor predicts the represen- tations of masked patches. Instead of predicting raw pixel values directly, MaskFeat [35] propose to use low-level features as the reconstruction target, e.g. Histograms of Oriented Gradients (HOG) [36]. Researches further explore applying high-level features as the target, such as pre- trained DINO [37], momentum features [38], and multi-modality features [37, 39, 40]. Unlike other MIM methods, AEMIM introduces an auxiliary proxy task to reconstruct adversarial examples or their high-level representations. Our framework is practically independent of specific MIM strategies, allowing it to be seamlessly integrated as a plugin into almost any MIM method.\nThere are also some works that integrate the ideas of adversarial learning. ADIOS [41] intro- duces an extra occlusion model to mask the images in an adversarial manner for MIM. NIM [13] adopts denoising as the pretext task and recon- structs noisy images to improve the adversar- ial robustness. CIM [11] is an effective MIM method that involves the reconstruction of cor- rupted images. It employs a trainable generator and a pre-trained frozen tokenizer to initially cor- rupt the input images and then performs the pre-training by recovering all the original image pixels, or predicting whether each visual token is corrupted or not. The generator and the backbone model are trained simultaneously and updated synergistically, resulting in a significant compu- tational burden and making the training process more difficult to converge to the optimal point. Compared to the above methods, AEMIM does not require an additional generator to produce adversarial examples. Instead, it generates them online using adversarial loss based on the training model, making AEMIM both simpler and more efficient.\nMost of the current MIM methods utilize an auto-encoder architecture. Let F denote the auto- encoder parameterized by \u03b8, and Lmim denote the reconstruction loss. For example, in the case of reconstructing raw pixel values, the objective function for MIM pre-training is written as\n$\\min_\\theta E_{x\\sim D}L_{mim}(F(x_m; \\theta), x),$  (1)\nwhere D is the data distribution, and $(.)_m$ denotes the masking operation.\n2.2 Adversarial Learning and Vision\nTasks\nAdversarial examples can greatly degrade the performance of models by adding imperceptible perturbations to clean images. The goal of gen- erating an adversarial example xa is to find the one that can fool the model to predict the wrong"}, {"title": "3 Methodology", "content": "We transfer the objective function of the co- training in Equation 4 from classification to MIM, where ground-truth labels are unavailable, and introduce adapters into the formulation. In the context of adapter literature, most of the model parameters are shared across data from different domains. Nonetheless, a distinct subset of parame- ters, known as adapters, exists specifically for data from different domains. In our case, we denote the shared parameters as \u03c8, the adapters for clean images as \u03a6c and the adapters for adversarial images as \u03a6a. Obviously, the model parameters \u03b8 consist of \u03c8, \u03a6c and \u03a6a. And we only consider the normalization layer and classification token in the vision transformer as the adapters in our method. Then the objective function of the co-training for MIM is rewritten as\n$\\min_\\theta E_{x\\sim D}[\\lambda L_{mim}(F(x_m; \\psi \\cup \\phi_c), x) + (1-\\lambda)L_{mim}(F(x^a_m; \\psi \\cup \\phi_a), x)],$ (5)\nwhere the adversarial example xa is generated online with the target of maximizing adversarial loss Ladv. And we provide more details on the design of Ladv in the next section.\nUnlike in supervised learning, there are no ground truth labels to guide the generation of adversarial examples during MIM pre-training. A straightforward approach is to use the task loss directly as the adversarial loss for generat- ing adversarial examples. However, our empirical observations suggest that employing the recon- struction loss as the adversarial loss directly can hinder the learning of the original pretext task due to interference from the auxiliary task. Figure 2 illustrates the change in loss during the pre-training process. As shown in the figure, the reconstruction loss of adversarial data La becomes much lower than that of clean data Le during training. This phenomenon suggests that the model might prioritize solving the auxiliary pretext task during pre-training, which adversely affects the learning of the original pretext task. We believe this is because the aforementioned adver- sarial examples directly target the original pretext task, resulting in overly strong regularization.\n3.2 Adversarial Examples for\nMasked Image Modeling\nWe then design a new adversarial loss, which is only corresponding to the encoder in the MIM framework. We propose to adopt the distance of the encoder's feature representation between adversarial and clean data as the adversarial loss, since: 1) It does not directly target the reconstruc- tion task. 2) The encoder is the backbone model that is finally transferred to downstream tasks as the feature extractor. The adversarial data only need to regularize the encoder to learn general- ized representations. 3) Dropping the decoder can accelerate the generation of adversarial examples. Denote the encoder as E and the decoder as D. The objective function of generating adversarial examples is expressed as\n$\\max_{x^a \\in S} L_{adv}(x, x^a, \\theta),$ (6)\n$L_{adv} = L_d(E(x_m, \\psi \\cup \\phi_c), sg(E(x^a_m, \\psi \\cup \\phi_a))),$ (7)"}, {"title": "3.3 Algorithm Details", "content": "where \u03b8e is the parameters of the encoder, con- sisting of the shared parameters \u03c8, the clean adapters \u03a6c and adversarial adapters \u03a6a, Ld is one type of distance loss, sg(.) means stop-gradient and S is set to S = {xa | ||Xa \u2212 X||\u221e \u2264 \u03f5}. For Ld, we mainly consider L2 distance and KL divergence. In our method, we apply PGD-K to generate adversarial examples, where K is the iter- ation number. We initialize xa by adding random noise to x in the first step since there is no differ- ence between x and xa at first. The algorithmic details are presented in Algorithm 1.\nIn this section, we present the complete pro- cess of using adversarial examples to enhance MIM pre-training. Similar to adversarial train- ing in PGD [16], we formulate the co-training as a min-max game. Each iteration consists of two steps. During the inner maximization step, the model parameters \u03b8e are fixed, while the adver- sarial examples are generated using the method outlined earlier. During the outer minimization step, both clean images and adversarial images are fed into the model to optimize the parameters \u03b8 through two pretext tasks. We also provide an accelerated version of our algorithm. In the stand version, we generate an adversarial image for each clean one, and then perform joint training with the paired data. We also employ K > 1 for PGD to enhance the quality of adversarial examples. In the expedited version, we only select a portion of the clean data to generate corresponding adversar- ial examples and set K = 1 for the PGD attacker, which implies the PGD attacker degrades into an FGSM attacker with random noise initialization. In our framework, we ultimately randomly select a% = 25% of the clean images to generate cor- responding adversarial images. Note the ratio a% is 100% in the standard version. We present the above process in the pseudo-code form in Algo- rithm 1, and illustrate the complete pipeline of our proposed framework in Figure 1."}, {"title": "4 Experiments", "content": "4.1 Experiments Setup\nWe conduct the pre-training on the training dataset of ImageNet-1K [24] without labels. We adopt MAE [6] as our basic framework and enhance its performance through our proposed method. Therefore, MAE serves as the main method for comparison in our experiments. It should be emphasized that our method is not con- fined to particular model architecture or MIM strategies. As such, it can be easily integrated into other MIM methods as a plug-in to enhance their performance. The vanilla ViT-S/16 and ViT- B/16 [49] are utilized as the backbone models for the encoder. After the pre-training stage, we dis- card all the decoders and only keep the encoders for fine-tuning on downstream tasks. The adapters for adversarial examples are omitted as well, as they could potentially impact the model's clean accuracy. All models are trained on 8 V100 32GB GPUs.\nPre-training setup We follow most of the MIM settings in MAE. By default, we randomly mask 75% of the image patches. The decoder consists of 8 transformer layers with 512 feature dimensions and 12 attention heads. We simply use random cropping and horizontal flip as the data augmen- tations. The models are pre-trained for 300/800 epochs with 15/40 warmup epochs, respectively. And the total batch size is 4096. We adopt AdamW [50] optimizer with a base learning rate of 1.5 \u00d7 10\u22124, a weight decay of 0.05, \u03b21 = 0.9, \u03b22 = 0.95 and cosine learning rate schedule [51]. For attack settings, the perturbation budget is set as \u03f5 = 2 and the number of attack steps is set as T = 2/1 for stand AEMIM and Fast AE\u041c\u0406\u041c. We adopt the normalization layer as the default type of adapter and L2 distance as the default dis- tance loss for adversarial examples. And the loss ratio \u03bb is empirically set to be 0.5 and the adver- sarial ratio is set as 100%/25% for AEMIM/Fast AEMIM. We simply use random cropping and hor- izontal flip as the data augmentations. And we use Xavier uniform [52] to initialize all Transformer blocks, following MAE. All the default setting is shown in Table 1.\nFine-tuning on the ImageNet-1K All the models are fine-tuned with image resolutions of 224 \u00d7 224. The Vit-S/16 and Vit-B/16 are fur- ther trained for 200/100 epochs, with 5 warmup epochs. The drop path rate [53] is set to 0.1, and the batch size is 2048. For ViT-S/16, we set the base learning rate and the layer-wise learn- ing rate decay to 1e-3 and 0.8. For ViT-B/16, the base learning rate and the layer-wise learn- ing rate decay are set to 1e-3 and 0.7 for models pre-trained with 300 epochs, and set to 5e-4 and 0.65 for models pre-trained with 800 epochs. We train each model with strong data augmentation including label smoothing [54], mixup [55], cut- mix [56], and randAugment [57]. We use the global pooling feature rather than the class token dur- ing fine-tuning, following MAE. All models are fine-tuned using an image resolution of 224 \u00d7 224. And we also use the linear lr scaling rule: lr = base_lr \u00d7 batch_size/256. The default set- ting is shown in Table 2. And we also evaluate the robustness of models on ImageNet variants such as ImageNet-Real [25], ImageNet-V2 [26], ImageNet- A [27], ImageNet-R [28], ImageNet-Sketch [29] and ImageNet-C [30].\nFine-tuning on the MS-COCO The COCO dataset [31] is commonly used as a benchmark for evaluating object detection frameworks. The Mask R-CNN framework [58] is adopted to pre- dict bounding boxes and instance masks of objects for object detection and instance segmentation tasks. The pre-trained Vit-S/16 and Vit-B/16 are employed as the backbone for Mask R- CNN [58]. We first fine-tune Mask-RCNN using the COCO [31] train2017 split and then evaluate"}, {"title": "4.2 ImageNet and its IID variants\nResults", "content": "As mentioned above, the encoders are fine-tuned on the ImageNet-1K dataset in a supervised man- ner after the pre-training stage. Since we adopt MAE as the basic MIM framework without alter- ing any MIM strategies, we mainly compare our method with MAE in experiments to demonstrate its gains. It should be emphasized that our method is independent of any specific MIM methods or model structures, and can also be used as a plugin to enhance other basic MIM methods.\nTable 4 shows the results of Vit-S/16 and Vit- B/16 on the ImageNet validation dataset and its IID variants ImageNet-Real, ImageNet-V2. The Vit-S/16 pre-trained by AEMIM for 300 epochs reaches a fine-tuning accuracy of 81.4%, which has an improvement of 0.5% compared with MAE. For Vit-B/16, our 300-epoch model achieves 83.4%, which is even slightly higher than the model of MAE at 800 epochs. When enlarging the training scheduler to 800 epochs, our model of Vit-B/16 achieves 83.8% Top-1 accuracy, which is 0.5%/0.2% higher than the MAE baseline at 800/1600 epochs. For the ImageNet IID variants, our method also outperforms MAE under different training schedulers. For example, our best model of Vit-B/16 outperforms MAE by 0.3% and 0.8% on ImageNet-Real and ImageNet-V2, respectively. The results show that our Fast AEMIM outper- forms the baseline MAE and CIM, requiring only 1.3x the training cost of MAE for each epoch, which is much faster than the previous CIM. The model trained with Fast AEMIM for 800 epochs exhibits improved generalization compared to the model trained with MAE for 1600 epochs while utilizing only about 65% of the training cost. The detailed training cost analysis is shown in Sec. 4.6.\n4.3 ImageNet OOD Variants Results\nWe test the robustness of our models on ImageNet OOD variants, including ImageNet-A, ImageNet- R, ImageNet-Sketch, and ImageNet-C. The Top-1 accuracy is used as the evaluation metric for all datasets, but for ImageNet-C. For ImageNet-C, we use mean corruption error (mCE) as the metric, and a lower value is better. And (1-mCE) is used to get the final Score, which is the average of all the results.\nTable 5 compares the robustness of our models to the MAE baseline. The results show that our Vit-B/16 model trained with AEMIM for only 300 epochs outperforms MAE's best model trained for 1600 epochs. Specifically, our 300/800-epoch Vit-B/16 model trained with AEMIM achieves an average score increase of 0.7/2.2% compared to the best model of MAE. The model trained with Fast AEMIM also shows a 0.7/1.6% improvement over MAE's best model. The results demonstrate that our method assists the model in learning more generalized representations, thereby signif- icantly enhancing the model's generalization to OOD datasets.\n4.4 Adversarial Robustness Results\nIn addition to evaluating the robustness of our method on OOD datasets, we also evaluate its adversarial robustness. During the pre-training stage, we utilize the PGD attack with 1 or 2 attack steps to generate adversarial examples, which is essentially closer to the FGSM attack. Thus, we"}, {"title": "4.5 Downstream Task Results", "content": "We then conduct a light ablation experiment to verify that the proposed method can boost other MIM frameworks and architectures. In the exper- iment, we employ SimMIM [7] as the basic MIM framework and select Swin-B [60] as the backbone model. We follow all of the MIM training settings from SimMIM. For the attack settings, we adopt the default settings from AEMIM. That means we adopt the normalization layer as the adapter"}, {"title": "5 Discussion", "content": "Our method serves to assist the current MIM methods in approaching their performance limits, rather than directly improving those limits, which would necessitate the design of improved MIM strategies and model architectures. Additionally, in our framework, we primarily use traditional noise-based adversarial examples. However, the community has recently seen the emergence of new types of adversarial examples, such as on- manifold adversarial examples [47, 61-63]. These novel examples maintain their adversarial proper- ties while exhibiting better image quality, closer to the original data distribution. As a result, they perform better in enhancing the performance of visual models [48]. Using improved adversarial examples in our framework could enhance per- formance further, but there is a potential risk of reduced training efficiency due to the slower gen- eration of these examples. We leave it to the future work."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel method called AEMIM, which incorporates adversarial examples into the masked image modeling (MIM) technique for pre-training. Our approach introduces a new auxiliary pretext task of reconstructing the cor- responding adversarial examples, in addition to the original pretext task of reconstructing masked normal images. We also design a novel adversar- ial attack to synthesize adversarial examples for MIM pre-training, using the distance between the encoder's feature representation of the adversarial data and the clean data as the adversarial loss. To enhance efficiency, we have also developed a faster version of our method. This optimized version maintains the core functionalities while signifi- cantly reducing computation time, making it suit- able for applications where speed is crucial. Our method surpasses the baseline on various datasets, including ImageNet-1K, ImageNet variants, and the downstream COCO datasets, demonstrating its effectiveness in enhancing the current MIM method. We believe that this work can provide the community with a fresh perspective on the rela- tionship between adversarial examples and MIM approaches, potentially serving as inspiration for future research endeavors."}]}