{"title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts", "authors": ["Subhajit Chaudhury", "Payel Das", "Sarathkrishna Swaminathan", "Georgios Kollias", "Elliot Nelson", "Khushbu Pahwa", "Tejaswini Pedapati", "Igor Melnyk", "Matthew Riemer"], "abstract": "Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce EPMAN a method for processing long contexts in an episodic memory module while holistically attending to semantically relevant context chunks. The output of episodic attention is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using EpMAN, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are highly capable of many natural language processing (NLP) tasks; however, they still struggle with generalization to long inputs that are unseen during training. To enhance the generalization ability of LLMs on unseen long inputs, continual pretraining on longer sequences has been attempted, which requires significant computational investments (Abdin et al., 2024). One main challenge of training with long context is the quadratic memory and time complexity of the current self-attention mechanism employed by most LLMs, making it computationally expensive and infeasible for processing long sequences. To circumvent this, existing solutions often resort to techniques like sliding window attention, dilated sliding window, and sparse attention (Beltagy et al., 2020; Child et al., 2019). In parallel, scalable position embeddings-based approaches, such as position interpolation and length extrapolation, have been proposed which involve minimal finetuning (Chen et al., 2023).\nDespite recent advances in long context processing abilities of LLMs, recent long-context modeling benchmarks show that LLMs still underperform in terms of modeling the input context that has a length longer or even similar to those seen during training (Kuratov et al., 2024; Hsieh et al., 2024). A promising solution to the problem of long context processing is the use of retrieval-augmented-generation (RAG) frameworks. RAG combines the strengths of retrieval models and generative LLMs to handle long contexts. In this framework, a retrieval model first identifies and retrieves the relevant context from a large corpus, which is then passed to the generative model for text generation. Despite its usefulness, RAG struggles to handle situations where there remains conflict between retrieved information and parametric memory, or the retrieved context contains irrelevant information, resulting in hallucination or ignoring the context while answer generation (Xie et al.).\nThus, the current gap in long context modeling of LLMs calls for alternative and efficient mechanisms for long context handling. For this purpose, in this work, we propose a second attention mechanism, named as episodic memory attention (EpMAN), shown in Figure 1, which is utilized to scale the self-attention according to the importance of the information present in the context. Inspired by the dual processing theory proposed in (Kahneman, 2011), in which self-attention can be characterized by \"System 1\u201d, a mode of thought that is fast, instinctive but less accurate, the proposed EpMAN can mimic the slow and calculative thinking steps, i.e., the \"System 2\" mode. EpMAN considers writing text chunks from the context in an episodic memory module, estimating their relative relevance with respect to a given query, and then utilizing this relevance to reweigh the self-attention. Experiments on challenging fact recall and single-hop question-answering from long context scenarios, which include the presence of distractions and confusions, as well as replaced keywords and rephrased sentences in the input context, show the benefit of the LLM trained with EpMAN, compared to the LLMs trained on long inputs using self-attention and RAG frameworks.\nOur main contributions are:\n\u2022 A novel architecture combining episodic memory attention with self-attention during LLM training, which is inspired by the dual processing theory.\n\u2022 An effective training method that introduces noise while estimating attention to the relevant chunks stored in the episodic memory.\n\u2022 An attention scope expansion method employed during inference which enables attending to the broader context in a more holistic manner.\n\u2022 The proposed framework shows better generalization to recalling and answering from challenging long context which includes information that is confusing, irrelevant, or contains replaced keywords or rephrases when compared to LLMs trained on long context and RAG systems."}, {"title": "Related work", "content": "Increasing context length in LLMs introduces several challenges that impact model performance. We elaborate on some of those problems and also other memory-augmented techniques."}, {"title": "Long Context Challenges", "content": "Recency Bias:\nRecent studies(Liu et al., 2024b; Guo and Vosoughi, 2024; Wang et al., 2024; Schmidgall et al., 2024) have shown that LLMs tend to prioritize information found towards the end of a context while neglecting important details presented in the beginning and the middle parts of the context. This bias is believed to originate from the pre-training process, where the most informative tokens for prediction are typically the most recent ones.\nTo address this issue, the authors in (Peysakhovich and Lerer, 2023) propose attention sorting, which rearranges documents based on their attention weights and moves documents that receive higher attention during decoding towards the end of the context.\nImpact of Distractors: Another significant challenge is the impact of distractors as highlighted in (Peysakhovich and Lerer, 2023), the accuracy of long-context language models generally decreases as the context length increases through the addition of distractor documents (Li et al., 2024a; Cuconasu et al., 2024; Koppenaal and Glanzer, 1990). This stresses that an overabundance of information, even if irrelevant, can hinder the model's ability to identify and utilize the most pertinent parts of the context effectively.\nAttention Dilution Long-context modeling in LLMs also suffers due to the phenomenon of attention dilution, explored in (Liu et al., 2024a; Holla et al., 2024; Xu et al., 2024; Tian and Zhang, 2024) which occurs due to the softmax normalization in the attention mechanism. Since attention weights must sum to 1, the presence of many irrelevant documents can result in each receiving a small but non-negligible amount of attention. This dilution of focus can overshadow the model's ability to concentrate on the most crucial information.\nTo address this, the research in (Li et al., 2024b) proposes a strategy to mitigate attention dilution in RAG-based systems by training the retriever with attention scores from a fine-tuned reader.\nHowever, if the reader is not fine-tuned well, the attention scores it provides might be unreliable, leading to suboptimal retriever training and ultimately impacting overall performance. Additionally, distilled attention mechanisms might inadvertently amplify existing biases present in the training or retrieved data. Differential Transformer (Ye et al., 2024) also aims to reduce the noisy attention on irrelevant tokens by using noise cancellation by subtracting attention values using two softmax outputs."}, {"title": "Memory-Augmented Retrieval", "content": "Memory-augmented retrieval involves storing past contexts and knowledge in a non-trainable memory bank, allowing the model to retrieve chunk-level information (Liu et al., 2024c; Modarressi et al., 2024; Rezazadeh et al., 2024). By storing information as key-value pairs and utilizing a retrieval mechanism, the model can access relevant past contexts. This approach has the potential to mitigate the limitations of fixed context windows and improve the model's ability to handle long-range dependencies. However, relying solely on single-layer representations for retrieval might not be robust enough and can be unstable.\nOur proposed approach, EpMAN, resolves the challenges of recency bias, distractors, and other limitations by storing long contexts in a dedicated memory module and selectively attending to semantically relevant chunks. Rather than focusing on the most recent inputs, EpMAN retrieves relevant information from the entire stored context, effectively addressing the \"lost in the middle\" phenomenon, where relevant information in the middle of long contexts is often overlooked. Additionally, the proposed differentiating attention mechanism with the denoising objective reduces the impact of distractors, ensuring robust information processing. Closer to EpMAN, (Wu et al., 2022) combines the attention to top-k nearest neighbor with self-attention by using a learnable gate; however, our approach is simpler, more intuitive, and more suitable for long context generalization. Another memory-augmented LLM, known as Larimar (Das et al., 2024), attends to the readout from an episodic memory storing the context during decoding and performs gradient-free write to the memory for input context length generalization. However, Larimar only attends to a single top-1 readout and therefore is not suitable for handling tasks in which relevant information is diffused over the context."}, {"title": "EpMAN: Episodic Memory AttentioN", "content": "In this section, we first describe the standard attention implementation in transformer-based language models (Vaswani et al., 2017). Subsequently, we outline our proposed differentiating attention over the KV cache using the episodic memory, referred as EpMAN. The EpMAN mechanism enables focusing on the relevant information required for correct recall or answering, which can be diffused over the long context in practice."}, {"title": "Preliminaries", "content": "The standard attention mechanism in LLMs is used to assign relevance weights to the input sequences when generating the output sequence. The model learns to pay attention to different tokens of the input sequence for each token of the response, enabling it to generate more accurate and relevant outputs to the context. The attention mechanism is implemented using a variant of the scaled dot-product attention mechanism as described below.\nLet us denote the input sequence as $X = [X_1, X_2, ..., X_n]$, where $x_i$ is the i-th input vector, and n is the length of the input sequence. The attention mechanism computes a set of attention weights $a = [a_1, a_2,..., a_n]$, which sums to 1 and is a distribution over the input sequence. These attention weights are used to compute a weighted sum of the input vectors, which is then used as input to the decoder for the next token.\nIn the standard attention, we compute the query vector q as a function of the current decoder hidden state $h_t$, $q = f(h_t)$, where f is a linear transformation that maps the decoder hidden state to the query vector. Next, we compute the keys $K = [k_1,k_2, ..., k_n]$ and values $V = [V_1, V_2, ..., V_n]$, where $k_i$ and $v_i$ are linear transformations of the input vectors $x_i$ similar to query vector. Then, we compute the dot product of the query vector q and each key vector $k_i$, followed by a softmax which is then multiplied by the value vectors to get the context vector at that token as $C_i = softmax(\\frac{qK^T}{\\sqrt{d_z}})V$. The query, key, and value vectors are learned during training."}, {"title": "Details of EpMan - An Episodic Differentiating Attention", "content": "While the standard attention mechanism in LLMs is effective for shorter contexts, it faces limitations when dealing with long context inputs due to issues like emergence of attention sink (Xiao et al., 2023), conflict between input context and pretraining knowledge (Xie et al.; Yuan et al., 2024a), and susceptibility to irrelevant information in context (Borgeaud et al., 2022). To address such challenges, we propose EpMAN, an episodic memory-based attention mechanism that enables finding relevant parts from the input context while discarding the irrelevant information, and then reweighing the standard attention to the relevant parts by using a relevance estimate.\nMemory operations: Given a large document as input, EpMAN first divides it into smaller entries (or chunks) that are written in the episodic memory. The memory consists of two operations, read and write. One can simply store encodings from a pretrained frozen retriever in the episodic memory as the write operation, or train an MLP using the encodings as input for a learnable write operation. Similarly, a learnable or a fixed (e.g., cosine) similarity function between the query encoding and the chunk encodings can be used to read from the context. (more details on trainable read and write in the appendix)\nWe use cosine similarity for reading and a state-of-the-art pretrained retriever model named Dragon (Lin et al., 2023) in this work. We refer to the score obtained from the read operation as episodic attention ($a_{mem}$) which is used to weigh KV cache attention for LM training.\nReplacing standard attention with differentiating attention: In addition to the latent retrieval encodings, the episodic memory also stores the KV cache of the context divided into episodic entries (stored in CPU memory due to increased size), which is processed using the above $a_{mem}$ as follows. Once we get $a_{mem}$ for each entry, we multiply the attention $a_i = softmax(\\frac{qK^T}{\\sqrt{d_z}})$ with the $a_{mem}$ episodic attention. This reweighing of standard attention with episodic attention $a_{mem}$ provides the differentiating attention mechanism that focuses on the relevant chunk in the memory while discarding the irrelevant information in the context. It is important to note that $a_{mem}$ is the attention over chunks, so the attention is the same for all K-V token embeddings in the chunk. The resulting attention operation can be described as,\n$a_{epman} = softmax(\\frac{qK^T}{\\sqrt{d_z}})(V * a_{mem})$,\nwhere we broadcast the $a_{mem}$ value for each entry to the size of the number of tokens before multiplying with the value vector.\nEpMAN thus provides a computationally efficient way to holistically handle long contexts in LLMs by leveraging an episodic memory attention mechanism. This approach allows the decoder model to attend to different chunks of the input sequence with different relevance estimates, which is used to self-distill the standard attention. This self-distillation of standard attention to input context enables generating more accurate and contextually relevant outputs."}, {"title": "Synthetic Data for Training", "content": "To couple our decoder such that it follows the ranked $a_{mem}$ output from the memory operations, we train it on synthetic data. We use two kinds of training data as explained below:"}, {"title": "Pre-training dataset", "content": "We train the model using a combination of the next token prediction task and memory retrieval task, following the loss objective in previous memory-enhanced architectures (Das et al., 2024). We used synthetically generated passages from a teacher model (mistralai/Mixtral-8x22B-Instruct-v0.1) which serve as the context for the model. We then add distractor passages from Wikipedia in the context to increase the context length during training. We did not use hard negative mining for this data that we used for QA data described next."}, {"title": "QA synthetic data", "content": "We use two kinds of QA synthetic data as below, Topic-sampled data: To generate this dataset, we used the teacher model, providing it with a topic sampled from a predefined list. The model was tasked with generating a short paragraph based on the given topic, which could either be factual or fictional. Afterward, the same model was instructed to create two questions related to the passage: one that could be answered using the information from the text (a related question) and one that could not be answered solely using the text (an unrelated question). Additionally, the model generated answers for both questions. Finally, a verification step was performed using Llama-3-70B-Instruct model as a judge, along with the nightdessert/WeCheck consistency checker to ensure consistency between the generated passage, related/unrelated questions, and corresponding answers.\nWikipedia: Firstly, we randomly sample Wikipedia passages and generate questions and answers from these passages using a teacher model. We use few-shot examples to guide the teacher model in generating question answers.\nSimilar to pretraining data, we add distractors from other Wikipedia passages in both cases above. In addition, to make the training more challenging, we mine context chunks that are similar to the topic of the relevant chunk (hard negatives) from a pool of Wikipedia entries which is added as part of the distracting context. We use an episode size of 16, where one of the entries is relevant and the others are distractors. Our chunk size is 256 and the effective training context size is 4K tokens. We use the index of the relevant chunk for episodic loss and the answer tokens for the decoder loss."}, {"title": "Training with Denoising", "content": "The read operation assigns the episodic attention $a_{mem}$ on each of the entries and we use a threshold to keep the top K entries (similar to RAG) for that are seen by the decoder to answer the query. However, different from RAG, our method EPMAN allows for each of these entries to be weighted differently such that the decoder can learn from differentiating attention.\nOut-of-domain mismatch: A straightforward method for decoder training would be to keep the original $a_{mem}$ weight that the read operation provides to each of the entries of the episode. However, this strategy is not always the best, particularly when the goal is to generalize to out-of-distribution samples. During training the decoder might become biased to expect the relevant chunk to always have a high episodic attention. For out-of-distribution (OOD) data, the read operation might not always assign the highest weight to the most relevant episodic entry, and in practice, the most relevant chunk might be ranked as one of the lowest in the top-K set of chunks. In that case, the above training strategy, would lead to poor generalization.\nRobust training with noisy attention: Since the episodic us to assign different importance to each entry, EPMAN proposes a noisy training scheme where the top K chunks receive random weights between 1.0 and 0.9. Throughout this work, we use K=5, unless otherwise mentioned. The episodic entries are further randomly permuted to change their relative order to ensure that they are not arranged in descending order of $a_{mem}$ weights. This training allows the decoder to pick up the relevant chunk even if it is not in the higher bins of the top K entries. This noisy training provides a denoising objective that allows the decoder to be robust compared to uniform $a_{mem}$ training.\nLoss: We use two losses during training. The first loss is the episodic attention loss that minimizes the distance between the distribution of the episodic attention from the read operation and the true distribution of chunk relevance using cross-entropy loss for the case where the read and write operations are learnable. We also use the next-token prediction loss in the decoder. The total loss is\n$L = E_D[a ln p(I|q, C) + ln(a|q, C, a_{mem})]$,\nwhere $(q, C, I, a) \\sim D$ represent the query, context, location of relevant episodic entry and decoder response respectively. We use a as the weight for the episodic loss which is typically set to 0.1. In the main paper, we report results with only decoder loss for fair comparison with RAG systems."}, {"title": "BroadAttn: Neighborhood expansion during Inference", "content": "The top-K episodic entries might be arranged in a manner such that there might be information cutoff during read operation (for e.g. delayed coreference). In such cases, which is referred to as the NarrowAttn as the decoder's attention only includes top-K chunks, the subject might be described in an entry (e.g. \"Albert Einstien was born in Germany\") whereas some attribute related to the subject might be described in a separate entry (e.g. \u201cHe taught himself algebra\u201d). To improve the robustness of EpMAN in such cases, we expand episodic attention during inference such that it includes the immediate (sequential) neighbors of each of the top-K chunks, which is referred as the BroadAttn. We also test the setting where exact amem weight for each chunk is attended during test, referred to as exact test.\nBoth NarrowAttn and BroadAttn consider preserving the original order of the chunks in which the information is presented in the original context, as suggested by (Yu et al., 2024)."}, {"title": "Experimental details and results", "content": "We use mistralai/Mistral-7B-Instruct-v0.2 as our decoder and Dragon (Lin et al., 2023) as the retriever for our experiments with EpMAN. We use a sequence length of 256 tokens for each entry and we cut at sentence boundaries. We use an effective batch size of 32 and train the models for 20k steps. We used LORA (Hu et al., 2021) for training."}, {"title": "Evaluation datasets", "content": "We evaluate EpMAN on a combination of recall and question-answering tasks. For recall tasks, we evaluate on the Needle in the haystack (NITH) (Kamradt, 2023) and factrecall-en tasks. For question answering, we use two single-hop, single-document QA tasks: Multifield QA (Bai et al., 2023) and Loogle SD (Li et al., 2023). For factrecall-en, Multifield QA, and Loogle SD, we use the LV-Eval benchmark, as proposed by (Yuan et al., 2024a), which subjects LLMs to a more challenging evaluation by inserting confusing facts into the context and by replacing keywords and phrases in the context to make sure that LLMs use comprehension of the context, rather than prior knowledge, to answer the questions. We consider the most challenging scenario included in the LV-Eval framework, where the context contains both confusing facts and replaced keyword and phrases.\nNeedle in the haystack: We use NITH (Kamradt, 2023) which is a well-known recall task from long context inputs to LLM. This task assumes that there is a needle sentence located in the large context input (haystack) and it tests if an LLM can complete a partial representation of that sentence. We use context lengths of varying size (16k, 32k, 64k, 128k) with needles located in 200 evenly spaced locations. Our needle sentence is: \"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day\" (with the partial representation in italics). We experiment with two haystacks: (i) The dataset of Paul Graham essays following (Kamradt, 2023) and (ii) books from PG19 corpora (Rae et al., 2020), inspired by (Kuratov et al., 2024). We concatenate the full set of Paul Graham essays and shuffle the sentences from all PG19 test texts (>11M tokens), prior to context selection.\nLV-Eval: LV-Eval is a long context benchmark with the context length varying between 16k, 32k, 64k, 128k and 256k. LV-Eval is comprised of various datasets, including the Factrecall-en, Multifield QA and Loogle SD datasets. To begin with, it is already difficult for a model to answer a question based on such a large context. To make it even more challenging, LV-Eval created two variants of the original datasets which we show in Figure 2. In the Confusing Facts Insertion (CFI) variant, GPT-4 is prompted to generate sentences that are similar to the given question and the answer. These sentences are resolved for inconsistencies by human annotators and are then randomly placed in the original context. Owing to their similarity with the question and the answer, the purpose of the newly added sentences is to mislead the model. An example of CFI is illustrated in 2 where the original sentence refers to actor Tom Holland and the Marvel movie No Way Home. To confound the model, the newly generated sentence talks about a different actor, Tom Hiddleston and his Marvel movie, Ragnarok. The Keyword and Phrase Replacement (KPR) variant is generated by selecting certain keywords or phrases and replacing them with other keyword or phrase throughout the context. This is done to ensure that the model is not reliant on its memorized prior knowledge while answering the given question. In Figure 2, the KPR sentence is formed by replacing the actor Tom Holland in the original sentence with the footballer Wayne Rooney."}, {"title": "Baselines", "content": "We choose two kinds of baselines for comparison with EpMAN described below:\nBaseline models: We first compare with instruction-tuned LLM decoders to investigate if they can generalize to longer context. While considering Mistral-7b-instruct-v0.2, following (Yuan et al., 2024b), we use half of the context from the top and half from the bottom in case the context size exceeds the model train context length. We also consider Phi-3-small-128k-instruct as a baseline model that was specifically trained with longer context inputs.\nRAG: We also compare with RAG systems to specifically evaluate if our EpMAN style training yields benefits over Retrieval Augmented Generation. We used state-of-the-art retrievers for instance Dragon (Lin et al., 2023; Liu et al., 2024d) with the above baseline decoder models."}, {"title": "Results", "content": "Simple Recall Performance\nNITH: Table 1 shows the recall of EpMAN with other baselines for sentence completion NITH task. This is a simple recall task and EpMAN shows near perfect recall score on both the Paul Graham and PG-19 haystack cases showing that our decoder coupling with episodic attention can successfully complete the needle sentence when presented with partial information. The large context models, although trained at higher context length, can hallucinate and does not result in high recall. Using RAG with baseline decoders with Dragon retriever improves performance at higher context length although the recall is not perfect. Since this is a simple task, EpMAN with uniform and noisy training shows similar performance. Additionally, NarrowAttn and Exact methods show similar performance since information diffusion does not happen in this simple task.\nLV-Eval (CFI + KPR) performance\nFactRecallEn: From Table 2, we observe that for the baseline models, the Phi-3-small-128k-instruct gets good performance on shorter context until 64k context size, however does not perform well for higher context lengths. Adding Dragon with these models improves long context performance, however EpMAN shows the best overall performance. It is important to note that Exact inference does not perform well for this dataset because the Dragon encoder does not always extract the relevant context as the top entry, due to presence of CFI and KPR as described in Section 4.2. Therefore, the relevant entry in the episode gets a low episodic attention score. Noisy training introduces robustness in the training, hence yields superior performance on longer context, even compared to uniform training due to the denoising objective. BroadAttn during inference provides additional performance boost at all context lengths.\nMultifieldQA: As we move from recall tasks to complex QA tasks, it becomes more evident that BroadAttn and noisy training improves the robustness of the decoder. Table 3 shows that noisy-trained EPMAN with BroadAttn get the best LLM-as-Judge score on this dataset compared to the other combinations. Similar to FactRecall-en, the presence of CFI and KPR, makes it difficult for the retriever to assign correct episodic weights in this challenging benchmark. Baseline models struggle to get competitive score; however, adding a retriever to Mistral-7b instruct shows promising performance. However, since this decoder in simple RAG setup is not trained to expect noise in retrieval, noisy-trained EpMAN out-performs the RAG baseline.\nLoogleQA: Table 4 shows the performance on LoogleQA task. We observe Dragon + Mistral decoder gives the best performance among the baselines, while the non-RAG systems does not show competitive performance. For EpMAN, uniform training with BroadAttn gives the best result outperforming the best baseline model. For Loogle (Li et al., 2023) we hypothesize that since some of the data is curated from Wikipedia, it might be in-domain compared to our synthetic training data, which is also sourced from Wikipedia. Additionally, the data for training the retriever is also derived from Wikipedia (Lin et al., 2023). Therefore, the retriever might not add noise in this case, and consequently the denoising objective might not be necessary for robust response generation."}, {"title": "Conclusions", "content": "We present EPMAN a novel method to generalize to long context using episodic attention in language models. Our method uses a two-level attention mechanism by first estimating the relative relevance of entries within a context and then reweighting the self-attention scores for the entries by the corresponding episode-level relevance score. Our architectural improvements - differentiating attention and training with denoising objective - show robust performance on complex recall and question answering tasks in the presence of distracting information and replaced keywords. Additionally, our attention scope expansion during inferences also proves to be beneficial in such challenging settings especially for QA tasks."}, {"title": "Limitations", "content": "While our method proposes techniques to improve attention over relevant chunk, we store the full KV cache for this work, which would take large CPU/GPU memory for large document processing and might require more processing time for CPU to GPU transfers (when we store the KV cache in CPU). Furthermore, using a large top K value for episodic attention would also requires more memory for training, especially large models. Additionally, another limitation is that the benefits of uniform/noisy training and exact/narrow/broad attention depends on the nature and complexity of the task. We plan to introduce methods like KV cache compression and pruning to make our approach more scalable and efficient in future works."}, {"title": "Effect of trainable memory operations", "content": "In the previous experiments, we used the setting of a fixed retriever using Dragon (Lin et al., 2023) for fair comparison with the baseline methods. However, as we described in Section 3.2, our memory operations can also be trained using the loss described in Equation 2. We trained EpMAN in a two phase manner where in phase 1 we train the memory operations (read and write are single layer MLPs) and a BGE (Chen et al., 2024) retriever. We divide our training data into different samples phase 1 and 2 respectively. Once we train the first phase, we use the train memory operation to obtain the episodic attention on the chunks. In phase 2, only the decoder parameters are updated.\nTable 5 shows the performance of the EpMAN with trained memory operations. Since the phase 1 training improves the retriever performance for obtaining the relevant chunk from the large context, the trained decoder can generate accurate responses leading to improved performance compared to fixed retriever setting. However, we do not report this in the main paper because the RAG baselines should also use such an improved retriever for fair comparison. It should be noted that although we are using a trained retriever + memory operations in this setting, the data we are evaluating is out-of-distribution. To improve the performance on a large variety of OOD data, we can train the retriever using the retriever dataset in addition to our dataset using contrastive learning."}, {"title": "Effect of top K for BroadAttn", "content": "In this experiment, we look at the effectiveness of using BroadAttn for various values of top K. We used a default value for top K as 5 for the experiments in the main paper (except for factrecall-en where we use top K value of 3). Table 7 show the performance on factrecall-en for various top K values. We find that a top K value of 2 performs better for BroadAttn. We hypothesize that since BroadAttn includes the relevant context neighbors it might add some distractor chunks that might confuse the decoder. Therefore, having a lower top K would reduce the number of such distractor chunks leading to better performance. However, it should be noted that for complicated QA datasets, where the retriever might not be able to pick the relevant context in a smaller top K setting, it might lead to worse performance. Therefore, this analysis might not be general and might vary based on the complexity of the dataset."}, {"title": "LLM-as-Judge Prompt", "content": "We evaluate the performance of various models on MultiFieldQA and LoogleQA using LLM-as-Judge (Zheng et al., 2023). The existing metrics that was used in LVEval (Yuan et al., 2024a) did not account for variation in length in the answers compared to the gold. Also, those metrics did not consider rephrases in the answers. Therefore, we found, although the answers were correct (albeit full sentence responses), the F1 score metric did not reflect that. Therefore, we used MISTRALAI/MIXTRAL-8X22B-INSTRUCT-v0.1 to compare the generated responses with the gold response. Figure 3 shows the prompt we used for this purpose."}, {"title": "Details about the synthetic data", "content": "In addition to the description in Sec 3.3.2 in the paper", "below": "n\u2022 Synthetic Question: What is the estimated age of the deposits in which the fossils of Cystoides estonicus were found", "Context": "Recent discoveries in the field of paleontology have shed new light on the ancient crustacean group Cystoidea", "Answer": "The age of the deposits in which the fossils of Cystoides estonicus were found is from the Upper Ordovician period.\n\u2022 Hard negative: In the course of time, however, a shift can be observed in the temporal significance of these terms, from post-Eocene to post-Early Miocene to post-middle Pleistocene. The region is seismically active and is generally ascribed to the reestablishment of an equilibrium after the latest (mid-Pleistocene) deformation phase. Some authors believe that the subduction process is still ongoing, which is a matter of debate. History. Calabria has one of the oldest records of human presence in Italy, which date back to around 700,000 BC when a type of Homo erectus \u00ebvolved leaving traces around coastal areas. During the Paleolithic period Stone Age humans created the Bos Primigenius ; a figure of a bull on a cliff which dates back around 12"}]}