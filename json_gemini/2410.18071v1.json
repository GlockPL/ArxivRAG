{"title": "TP-EVAL: TAP MULTIMODAL LLMS' POTENTIAL\nIN EVALUATION BY CUSTOMIZING PROMPTS", "authors": ["Yuxuan Xie", "Tianhua Li", "Wenqi Shao", "Kaipeng Zhang"], "abstract": "Recently, multimodal large language models (MLLMs) have received much atten-\ntion for their impressive capabilities. The evaluation of MLLMs is becoming crit-\nical to analyzing attributes of MLLMs and providing valuable insights. However,\ncurrent benchmarks overlook the problem of prompt sensitivity - minor prompt\nvariations may lead to significant performance fluctuations. Thus, inappropri-\nate prompts may obscure the models' capabilities, underestimating the models'\nperformance. Moreover, different models have different preferences for different\nprompts, and thus, using the same prompt for all models will cause evaluation\nbias. This paper analyzes this deficiency in existing benchmarks and further in-\ntroduces a new evaluation framework named TP-Eval, which introduces a prompt\ncustomization method to reduce evaluation biases and tap models' potential. TP-\nEval will rewrite the original prompts to different customized prompts for different\nmodels. In particular, we propose some well-designed modules for prompt cus-\ntomization tailored to the scenario of MLLM evaluation. Extensive experiments\ndemonstrate the effectiveness of our approach to uncovering models' capabilities,\nand TP-Eval should benefit the community in developing more comprehensive\nand convincing MLLM evaluation benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as ChatGPT, and Claude, are becoming a milestone in achiev-\ning artificial general intelligence (AGI). Recently, beyond text conversation, multimodal large lan-\nguage models (MLLMs), like GPT-40 (Achiam et al. (2023)), Deepseek (Lu et al. (2024)), InternVL\n(Chen et al. (2024)) and LLaVA (Liu et al. (2024a)), have received much attention for their im-\npressive capabilities to understand multimodal inputs (this paper focuses on image and text). Subse-\nquently, researchers present various benchmarks to evaluate their performance in different scenarios.\nMost apply prompt-based benchmarking approaches to ask models multimodal questions and assess\ntheir responses. For instance, MMT-Bench by Ying et al. (2024) comprehensively evaluates per-\nformance in 162 general tasks spanning 32 categories. Meanwhile, MMMU by Yue et al. (2024)\nencompasses six core disciplines drawn from university curricula and assesses performance on mul-\ntidisciplinary tasks requiring domain-specific knowledge and meticulous reasoning. Convincing\nbenchmarking is crucial to analyze the attributes of models, provide valuable insights, and guide the\ndevelopment of MLLMs.\nNevertheless, recent research (Zhan et al. (2022; 2023; 2024)) found that LLMs and MLLMs exhibit\npronounced sensitivity to prompt variations. Thus, minor modifications to questions in benchmarks\nmay lead to significant output differences. This makes prompt-based benchmarking unreliable since\nmodels' low accuracy may be owed to unsuitable prompts, not their inner capability. Furthermore,\nmany MLLMs' benchmarks use simple and uniform prompts for all samples in a specific task,\nwhich aggravates the problem and causes general underestimation. Additionally, different models\nshow various sensitivity to the same prompt changes, and existing evaluation frameworks fail to\nconsider such prompt-induced bias and may not be able to conduct a convincing comparison.\nTo address the aforementioned deficiencies, this paper introduces TP-Eval, a novel evaluation frame-\nwork for MLLMs that customizes optimal prompts for different models to fully tap their potential\nduring evaluation while mitigating the effects leading to performance underestimation by prompt\nsensitivity. We posit that this framework enables researchers to assess the strengths and weaknesses\nof various models more accurately. To ensure fairness across models while also managing labor\ncosts, it is essential for the prompt customization process to be automated. A relevant technique\nis automatic prompt optimization, as exemplified by recent methods such as ProTeGi Pryzant et al.\n(2023) and OPRO Yang et al. (2023), which employ an optimizer-scorer architecture. These methods\ngenerate multiple candidate prompts and score them on a training set to identify the most effective\noption.\nInspired by this, TP-Eval implements prompt customization through automatic prompt optimization\ntailored to MLLMs' evaluation. In particular, related prompt optimization methods consider text\nonly, while our prompt customization incorporates text with images. Moreover, the data scale of the\nMLLM benchmark is usually limited (e.g., 20 validation samples per task in MMT-Bench) due to\nthe high construction cost, while related prompt optimization methods did not consider this few-shot\nscenario and easily caused overfitting. Thus, our method introduces a novel error introspection from\nwrong responses and employs some designs to limit the prompt semantic change. They significantly\nimprove the performance of our method.\nWe conduct extensive experiments to reveal the presence of prompt-induced underestimation and\nbias in MLLM evaluation and demonstrate that the TP-Eval framework effectively mitigates these\nissues. Moreover, our experimental results demonstrate that TP-Eval also works well in zero-shot\nsettings. The primary contributions of this paper can be outlined as follows:\n\u2022 We identify and analyze prompt design deficiencies in existing MLLMs' benchmarks that\nlead to underestimation and evaluation bias due to prompt sensitivity in MLLMs.\n\u2022 We propose TP-Eval, a novel evaluation framework for MLLMs that customizes optimal\nprompts for distinct models and makes it practical through automatic prompt optimization\ntailored to MLLMs' benchmarks.\n\u2022 We conducted extensive experiments on advanced MLLM benchmarks and various\nMLLMs to demonstrate the effectiveness of our method in alleviating the underestimation\nbias in evaluation."}, {"title": "2 MULTIMODAL LARGE LANGUAGE MODEL EVALUATION", "content": "In order to comprehensively evaluate the overall reasoning capabilities of MLLMs, many bench-\nmarks have been proposed, encompassing a wide range of tasks that assess various aspects of model\nperformance. Some notable benchmarks are MMBench by Liu et al. (2024b), MMMU by Yue et al.\n(2024), MM-Vet by Yu et al. (2023), SEED-Bench by Li et al. (2023) and MMT-bench by Ying et al.\n(2024). Unlike the prompts used in text-only benchmarks for LLMs, MLLMs' benchmarks primar-\nily convey the majority of the question information through images. Additionally, considering the\nsubstantial human effort required to design a specific textual prompt for each image, the prevail-\ning approach is to provide a simple prompt template or even an identical prompt for a given task,\nlike How many {<object>} are there in the image? for counting task and What\nemotion is expressed in the artwork in the picture? for artwork emotion\nrecognition task.\nHowever, extensive research demonstrates that LLMs are sensitive to minor modifications of tex-\ntual prompts, so whether MLLMs are also sensitive to prompt design in existing benchmarks?\nAs shown in Fig. 1a, the original prompt Are there any similarities between the\ntwo pictures? of the spot similarity task in MMT-bench will lead to an anomalous response\nfrom the llava-1.5-7b, who answered Yes to all 180 questions, resulting in an extremely low accu-\nracy rate. However, by slightly rephrasing the question, the model achieves nearly double accuracy.\nThis suggests that the model's capability is underestimated due to inadequate prompt design. Fur-\nther investigation into the accuracy change brought from the phase Focus on visual cues\nindicates that the model's responsiveness to prompts is challenging to predict by humans, raising\nquestions about whether seemingly reasonable prompts in existing benchmarks can truly and accu-\nrately assess the model's capabilities.\nNevertheless, designing more suitable prompts for all models in benchmarks won't solve this prob-\nlem fundamentally since different MLLMs' model architecture and training data are different, lead-\ning to different behaviors, preferences, and sensitivity to prompts. Previous research on prompt\nengineering for LLMs has indicated that prompt design strategies effective for one model may prove\nineffective for another (Sclar et al. (2023)). Similar phenomena have also been observed in MLLMs.\nAn intuitive example can be found in Table 1 whereby customizing a more detailed prompt for\nLLaVA will enhance the accuracy of the helmet anomaly detection task in MMT-Bench. How-\never, this specific prompt declined DeepSeek's accuracy significantly. When utilizing this prompt,\nLLaVA's performance will surpass that of DeepSeek, and subtle adjustments may reverse this out-\ncome, which implies that comparing the outputs of two models under an identical prompt may not\nnecessarily provide a valid performance ranking.\nThe above discussions regarding prompts indicate that the existing benchmarks and evaluation meth-\nods may not accurately assess the true capabilities of models or facilitate a reliable comparison of\ntheir performance, and simplistic prompt templates in MLLM benchmarks exacerbate this issue.\nAction should be taken to mitigate the influence of prompts on model evaluations."}, {"title": "2.2 IDEAL EVALUATION", "content": "The ideal evaluation should be able to evaluate the true capabilities of the model. However, due to the\nsignificant performance influence caused by prompt design, how do we define the true capabilities\nduring evaluation? We argue that models' true capabilities are performance under optimal prompts,\nconsidering that users will also refine the prompts to get desirable responses when using MLLMs.\nThe optimal prompts should be derived from slight modifications from the benchmarks' original\nprompts while maintaining the semantic integrity of the task instructions. The optimal prompts\nfor different models may be identical or different. Therefore, we propose TP-Eval, an evaluation\nframework that customizes the best prompts for each model in each task, thereby tapping their\npotential and uncovering their true capabilities.\nManual exploration of optimal prompts during evaluation is time-consuming and impractical. In-\nspired by existing works on automatic prompt optimization for LLMs, we propose to use an au-\ntomated prompt customizer to leverage original prompts from benchmarks and a few examples to\ncustomize specific prompts for each MLLM under evaluation, thereby tapping their potential.\nHowever, existing text-only prompt optimization methods are not applicable. On the one hand, the\ndata scale for multi-modal tasks is relatively small, especially for evaluation data, which necessitates\nthat the prompt customizer possesses a strong few-shot capability, which is overlooked by existing\nmethods. On the other hand, the desirable prompt customization requires a new framework to utilize\nvisual information beyond text, and the cost associated with calling MLLM APIs is prohibitively\nhigh, making extensive calls impractical. Therefore, a novel prompt customization method tailored\nspecifically for multi-modal benchmarks is needed."}, {"title": "3 RELATED WORKS", "content": null}, {"title": "3.1 RESEARCH ON PROMPT SENSITIVITY", "content": "Some studies have revealed that even minor prompt modifications, which have negligible impact\non human semantic understanding, can lead to significant shifts in the output of LLMs (Zhan et al.\n(2022; 2023)). This property has been widely exploited in the creation of adversarial examples,\nwhere small perturbations to the embeddings or input text can induce the model to generate in-\ncorrect or misleading answers (Zhan et al. (2024)). This sensitivity allows minor adjustments to\nquestions in LLM benchmarks to significantly impact the final evaluation performance. Recent re-\nsearch has begun exploring variations in prompt formatting to achieve better results (Sclar et al.\n(2023)). Similar phenomena also occur for MLLM. However, addressing this deficiency in MLLM\nbenchmark design remains relatively underexplored. In this work, we provide a detailed analysis of\nprompt design issues and introduce an effective evaluation framework with prompt customization to\navoid the above problems and bias from prompts."}, {"title": "3.2 PROMPT ENGINEERING & OPTIMIZATION", "content": "Prompt engineering seeks to identify effective prompts for LLMs to optimize their task performance.\nTo minimize manual effort, researchers have explored automatic prompt optimization, broadly cate-\ngorized into continuous and discrete methods. Discrete methods directly optimize natural language\nprompts using techniques such as reinforcement learning (Zhang et al. (2022)) or prompt editing\n(Prasad et al. (2022)). In contrast, continuous methods (Lester et al. (2021); Li & Liang (2021)) per-\nform optimization within the LLMs' embedding space, enabling gradient-based approaches. Given\nthe unprecedented capabilities of LLMs, recent research has started leveraging them as prompt op-\ntimizers. For example, Yang & Li (2023) integrates LLMs with evolutionary algorithms to enhance\nprompt optimization, while Yang et al. (2023); Pryzant et al. (2023) focuses on adapting concepts\nand techniques from gradient-based model optimizers, including gradient descent (Pryzant et al.\n(2023)) and momentum methods (Yang et al. (2023)), for LLM-based prompt optimization.\nOur work follows discrete methods and employs MLLM as prompt optimizers. In particular, we\ncombine error introspection, semantic change, and accuracy as \"pseudo-gradients\" proposed by\nTang et al. (2024) to guide the MLLM optimizer in the multimodal scenario. We also introduce a\nfinal re-ranking scheme for better performance."}, {"title": "4 METHOD", "content": null}, {"title": "4.1 OVERVIEW", "content": "Fig. 1b illustrates the overall pipeline of TP-Eval, given the initial text prompts with a few examples\n$D_{few}$ from the evaluation dataset for a task, and a MLLM $M_T$ to be evaluated. We introduce a\nprompt customization method to obtain the optimal prompt $p^*$ for $M_T$, then do an ideal evaluation\nto maximize its potential on the original test set $D_{test}$.\nWe show the overall framework of our customization method in Fig. 2. Starting from the initial text\nprompt $p_0$ for a task from the multimodal evaluation dataset, we utilize GPT-40 mini as an optimizer\n$M_O$ and a few examples $D_{few}$ (questions and answers) from the evaluation dataset to obtain an\noptimal prompt $p^*$ for the MLLM $M_T$. Specifically, we first feed $p_0$ to a scorer, which consists\nof the $M_T$ and an answer analyzer $M_A$ (GPT-40 mini), to output the scores and introspection.\nThen we use these results to construct a well-designed meta-prompt for the optimizer $M_O$ to obtain\noptimized prompts $P_1 = \\{p_1, p_2, \\dots, p_n\\}$. We feed them to the scorer and iteratively run this\nframework to collect $N$ sets of optimized prompts $\\{P_1, P_2, \\cdots, P_N\\}$ with their scores. Finally, we\nselect the optimal prompt $p^*$ according to the scores. Please note that we will feed the corresponding\nimages to $M_T$ and corresponding images and answers to $M_A$. We will introduce the details of the\nprompt customization method in the following."}, {"title": "4.2 SCORER", "content": "In the i-th iteration, we feed the prompt set $P_i$ (using $p_0$ in the first iteration) to the scorer to obtain\nthe corresponding scores and introspection (i.e., pseudo gradient) of each prompt."}, {"title": "4.2.1 SCORE GENERATION", "content": "We first feed these prompts with corresponding images to $M_T$ to obtain models' responses. Then\nconsidering the variations of answers and most benchmarks apply choice questions, we use $M_A$\n(GPT40-mini) extract choices and then compute the accuracy $a_{p_i}$ on $D_{few}$ for $p_i$.\nUsing accuracy as a reward only may lead to drastic changes in the new prompt and destroy the\noptimization. Thus we utilize a semantic similarity metric as proposed by Tang et al. (2024) to\nlimit the changes in each iteration. Specifically, we use BERT by Kenton & Toutanova (2019) to\nextract the embedding of the current prompt $p_i$ and the original prompt $p_0$, then calculate their cosine\nsimilarity as $s_{p_i}$."}, {"title": "4.2.2 INTROSPECTION GENERATION", "content": "We argue that scores are quantitative and not informative enough, especially in the few-shot exam-\nples, and thus, we introduce to employ additional introspection during optimization. Specifically, we\naim to help the optimizer better understand the deficiencies in the current prompt. To achieve this,\nwe represent introspection $I_i$ on the incorrect responses in $D_{few}$ of $M_T$ under $p_i$, allowing $M_O$ to\nexplicitly reference the reasons for these errors when generating new prompts. We show the prompt\nstructure to generate introspection in Fig. 2 and the full prompt in the supplementary materials."}, {"title": "4.3 OPTIMIZER", "content": "We use the optimizer $M_O$ (GPT40-mini) to generate a new prompt set $P_{i+1}$ from all history prompts\n$\\left\\{P_0, \\ldots, P_i\\right\\}$. Specifically, we design a meta-prompt as shown in Fig. 2 and complete it using $K$\nprompts with Top-K scores from $\\left\\{P_0, \\ldots, P_i\\right\\}$. We also feed their scores and introspection to the\noptimizer. The meta prompt is composed of four parts: description, pseudo gradient (i.e., prompts\nwith their scores and introspection), examples (questions with ground-truth answers from $D_{few}$),\nand instruction. The description is used to describe the prompt optimization tasks. The pseudo\ngradient and examples are used to provide information for the optimization. The instruction is\nused to generate new prompts. In particular, to ensure smooth optimization and not overlook op-\ntimal prompts, we use a decaying edit distance You can only edit at most {counter}\nwords to limit the changes. Please note that for identical question benchmarks (e.g., MMMU),\nwe will add an initialized meaningless phrase and optimize it rather than the whole prompt, see the\nexperiments section for more details."}, {"title": "4.4 ITERATION", "content": "We use the above scorer-optimizer framework iterative to obtain $N$ prompt set $\\{P_1, P_2, \\cdots, P_N\\}$\nwith scores for each prompt. Then we select the optimal prompt from all history prompts.\nIn contrast to related prompt optimization methods using large-scale training data to obtain candidate\nprompts and selecting the optimal prompt with the highest accuracy, MLLM evaluation can provide\nonly limited examples in the optimization. Thus, we have to consider the problem of overfitting\nand bias in the few examples. The introduced semantic cosine similarity and decaying edit distance\ncan alleviate this problem. Moreover, in the selection of the optimal prompts, we employ a higher\nweighting coefficient $\\alpha^* > \\alpha$ to re-compute each prompt's score and select the prompt with the\nhighest score."}, {"title": "5 EXPERIMENT", "content": null}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Models. The MLLMs to be evaluated (i.e., $M_T$) are LLAVA-1.5-7B, DeepSeek-VL-7B,\nMini-InternVL-Chat-4B-V1-5. We use GPT-40-mini for optimizer ($M_O$) and answer\nanalyzer ($M_A$).\nBenchmarks. We use MMT-Bench and MMMU as the evaluation benchmarks. MMT-Bench is\ndesigned for the evaluation of general capabilities, while MMMU is designed for multi-discipline\nevaluation. Considering our limited resources, we select a subset of MMT-Bench as MMT-S, which\ncontains 83 tasks (19 categories). We use the development set and validation set of MMMU.\nSettings of prompt optimization We evaluate our method in two settings: optimizing the\nwhole prompt or optimizing the newly added phrase. MMT-Bench follows the most prevalent\nMLLM benchmark format, which uses the same prompt template within a task (e.g., How many\n<object> are there in the image? for the task of object counting). Thus, we opti-\nmize the whole prompt for each task in MMT-S. In MMMU, each question is identical, and thus we\nadd an initialized meaningless phrase Answer the questions about {task_name} as the\nprompt to be optimized and move the original prompt to <QUESTION> in the meta prompt."}, {"title": "5.2 MAIN RESULTS", "content": null}, {"title": "5.2.1 PERFORMANCE ANALYSIS", "content": "Implementation details. For MMT-S, we utilize the officially designated validation set as $D_{few}$,\nwhich comprises approximately 10% of the total data, with roughly 20 samples per task. For\nMMMU, we combine the development and validation sets and allocate half of the data as $D_{few}$.\nWe follow VLMEvalKit by Duan et al. (2024) to implement the answer extraction module in $M_A$.\nThe total optimization iteration $N = 16$, with each round generating three new prompts. In each\niteration, we select the top eight (i.e., $K = 8$) prompts for the meta prompt. We set the temperature\nto 1.0 when generating new prompts. During the optimization phase, we set $\\alpha$ to 0.8 to encourage\nthe exploration of prompts that yield higher accuracy. In the final step, we set $\\alpha^*$ to 0.6 to select the\noptimal prompt."}, {"title": "5.2.2 OPTIMALITY ANALYSIS", "content": "Fig. 5 presents the overall results obtained from a hybridization of customization outcomes across\ndifferent models within MMT-S. It is evident that prompts optimized using a model itself as a scorer"}, {"title": "5.2.3 ERROR ANALYSIS", "content": "Similar to many text-only prompt optimizations, our method, while ensuring an overall enhance-\nment in performance and a closer alignment with the true capability for evaluated models, may still\nencounter optimization failures for a limited number of tasks. This can, in turn, result in a slight\nperformance deterioration when using optimized prompts. For instance, although LLaVA has an\noverall improvement of 25% across 32 tasks, it also experiences an approximate 6% performance\ndecline on 6 tasks. We argue that a critical factor contributing to this is the relatively small size of\nthe validation set currently designated by the official multi-modal benchmarks, which may cause\noverfitting on the training set. Despite our efforts to incorporate introspection mechanisms for more\neffective utilization of few-shot data, and the implementation of re-ranking and meta-prompt design\nstrategies to mitigate overfitting, this challenge persists, but its impact remains relatively minor."}, {"title": "5.3 ABLATION STUDY", "content": "Introspection Fig. 6 illustrates the results of LLaVA in three tasks of MMT-S when introspection\nis not incorporated. It is evident that the optimization results on both artwork emotion recognition\nand helmet anomaly detection tasks are significantly inferior to those achieved with our method.\nNotably, the latter even experiences a failure in optimization, resulting in a performance decline.\nThis underscores the effectiveness of integrating introspection to enhance the few-shot optimization\ncapability on multi-modal benchmarks. Furthermore, the figure indicates that the accuracy of be-\nhavior anomaly detection is better without introspection. This phenomenon arises from the prompt\nexplicitly designating choice A as normal and choice B as abnormal, disregarding the randomized\ninitial order of the choices presented in this task. This is an instance of semantic overfitting that\nleads to misleadingly high performance. Thus, the introduction of introspection can also enhance\nresult interpretability.\nRe-ranking parameter. Fig. 7 illustrates the impact of varying the proportion of accuracy dur-\ning the re-ranking phase on optimization results. As depicted, when setting the parameter to 0.8,\nwhich in fact omits the re-ranking stage, leads to significant overfitting and ultimately degrades the\noptimization outcomes. Conversely, a disproportionately low correctness ratio may result in the ex-\nclusion of potentially optimal prompts, thereby underfitting and hindering the optimized prompts\nfrom fully leveraging the model's capabilities. Based on our experiments, we conclude that a value\nbetween 0.5 and 0.6 is appropriate to ensure both effectiveness and coherence across the models."}, {"title": "5.4 ZERO-SHOT EXPLORATION", "content": "Considering that the task may suffer from extremely limited data availability or involve privacy con-\ncerns that prevent the disclosure of answers, it becomes impractical to construct even one training\nsample. In response, we propose an approach that leverages the robust In-Context Learning(ICL) ca-\npabilities of LLMs to extend our method to the zero-shot scenario. Specifically, we aim to optimize\nprompts for a newly introduced task through the use of a selection of previously successfully opti-\nmized examples, thereby facilitating zero-shot customizing. We anticipate that LLMs can discern\ncertain vocabulary, phrases, and reasoning patterns that the model under examination may prefer\nfrom these ICL examples. A straightforward experiment result illustrating this observation can be\nfound in Table 3, where we select 3 tasks from all 32 MMT-S underestimated tasks for LLaVA as\ntargets and use the rest as ICL examples. We use this zero-shot ICL-based optimization fashion to\nrefine the original prompts, which also enhances the original accuracy and is close to that of optimal\nprompts learned by 20 examples."}, {"title": "6 CONCLUSION", "content": "We investigated MLLM benchmarks and found that overly simplistic or unsuitable textual prompts\nmay lead to an underestimation of models' capabilities. To address this issue, we propose an ideal\nevaluation framework, TP-Eval, which customizes the most suitable task prompt for each model to\nmitigate prompt-induced biases and tap the models' potential. To achieve this goal, we drew on the\nsuccessful experiences of automatic prompt optimization on text-only LLMs and designed a prompt\noptimization method tailored to the few-shot scenario of MLLM benchmarks. Our experiment re-\nsults for three models on the MMT and MMMU indicate the effectiveness of our method."}]}