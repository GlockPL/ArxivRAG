{"title": "EVALUATING SEMANTIC VARIATION IN TEXT-TO-IMAGE SYNTHESIS: A CAUSAL PERSPECTIVE", "authors": ["Xiangru Zhu", "Penglei Sun", "Yaoxian Song", "Yanghua Xiao", "Zhixu Li", "Chengyu Wang", "Jun Huang", "Bei Yang", "Xiaoxiao Xu"], "abstract": "Accurate interpretation and visualization of human instructions are crucial for text-to-image (T2I) synthesis. However, current models struggle to capture semantic variations from word order changes, and existing evaluations, relying on indirect metrics like text-image similarity, fail to reliably assess these challenges. This often obscures poor performance on complex or uncommon linguistic patterns by the focus on frequent word combinations. To address these deficiencies, we propose a novel metric called SemVarEffect and a benchmark named SemVarBench, designed to evaluate the causality between semantic variations in inputs and outputs in T2I synthesis. Semantic variations are achieved through two types of linguistic permutations, while avoiding easily predictable literal variations. Experiments reveal that the CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1. Semantic variations in object relations are less understood than attributes, scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in UNet or Transformers plays a crucial role in handling semantic variations, a factor previously overlooked by a focus on textual encoders. Our work establishes an effective evaluation framework that advances the T2I synthesis community's exploration of human instruction understanding.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurately interpreting and visually depicting human instructions is essential for text-to-image (T2I) synthesis Cao et al. (2024). Despite advancements in alignment Lee et al. (2023a); Wu et al. (2023);"}, {"title": "2 SEMANTIC VARIATION EVALUATION FOR TEXT-TO-IMAGE SYNTHESIS", "content": ""}, {"title": "2.1 PRELIMINARY", "content": "The T2I model f generates images I for each input sentence T, represented as I = f(T). S(T, I) is the text-image alignment score, measuring text-image similarity. S(.) represents the scoring method.\nLinguistic Permutation. Linguistic permutation refers to changes in word order. Given an anchor sentence Ta, Tpv and Tpi are two permutations of Ta. Tpv exemplifies permutation-variance, which shows a change in meaning, while Tpi exemplifies permutation-invariance, where the meaning remains unchanged. The expected Ipv is a permutation of objects or relations from Ia, while Ipi is semantically equivalent to Ia, preserving the same visual objects and relations after transformation."}, {"title": "2.2 DEFINITION OF VISUAL SEMANTIC VARIATIONS", "content": "First, we define the visual semantic variations observed from a single sentence T. For each I and its localized variation I + \u2206\u0399 in the image space, the visual semantic variation at I, denoted as \u03bc\u2081(\u03a4, \u0399), is the difference in alignment scores between the two images for the same sentence:\n\u03bc\u2081(\u03a4, \u0399) = S(T, I + \u2206\u0399) \u2013 S(T, I).\nIf the anchor image Ia is transformed into a permutation image Ip through a series of localized changes, the total visual semantic variation from Ia to Ip is the sum of variations across all localized changes: \u03a3* \u03bc\u03b9(\u03a4, I) = S(T, Ip\u2217) \u2013 S(T, Ia).\nSecond, we integrate the visual semantic variations observed across multiple sentences. For the sentence Ta, the visual semantic variations \u03a3* \u03bc(\u03a4\u03b1, I) demonstrate a shift from a matched to a mismatched image-text pair, indicating a negative change. For the sentence Tp*, the visual semantic variations \u03a3* \u03bc(\u03a4p*, I) demonstrate a shift from a mismatched to a matched image-text pair, indicating a positive change. To measure the total magnitude of these variations regardless of direction, we use the absolute values. Therefore, the integrated visual semantic variations \u03b3\u00b9 is defined as:\n\u03b3\u00b2 = \u03a3\u03c4\u03b5{Ta,Tp\u00bb} |\u03a3\u03af* \u03bc(T, I)| = |S(Ta, Ip\u2217) \u2013 S(Ta, Ia)| + |S(Tp\u2217, Ip\u2217) - S(Tp\u2217, Ia)|.                                        (1)"}, {"title": "2.3 THE CAUSALITY BETWEEN TEXTUAL AND VISUAL SEMANTIC VARIATIONS", "content": "Fig. 3 illustrates the causal relationship between input and output semantic variations. T is the text input, serving as the input variable, while I is the generated image, acting as a mediator. S is the text-image alignment score, influenced by both T and I, and serves as an intermediate result variable. \u03b3\u00b9 denotes visual semantic variation and is the final comparison result variable. f(\u00b7) is an exogenous variable representing a T2I model that maps T to I. S(\u00b7) is an exogenous variable representing"}, {"title": "3 SEMANTIC VARIATION DATASET FOR TEXT-TO-IMAGE SYNTHESIS", "content": "We collect semantic variation datasets for T2I synthesis to fill the gaps in current benchmarks and evaluation practices. The textual semantic variations are created through two typical linguistic permutations. First, we elaborate on the characteristics of the data. Then, we introduce the pipeline for data collection, annotation and statistics."}, {"title": "3.1 CHARACTERISTICS OF DATA", "content": "Each sample (Ta, Tpv, Tpi) consists of three sentences: an anchor sentence Ta and two permutations Tpv and Tpi. They should adhere to the following characteristics:\nLiteral Similarity: Ta, Tpv and Tpi are literally similar, differing only in word order.\nDistinct Semantics: \u03a4\u03b1 and Tpv have distinct semantics. Ta and Tpi share the same semantics.\nReasonability: Ta, Tpv and Tpi are semantically reasonable in either the real or fictional world.\nVisualizability: Ta, Tpv and Tpi evoke vivid mental images.\nDiscrimination: The images evoked by Ta and Tpv present distinguishable differences. The images evoked by Ta and Tpi appear similar.\nRecognizability: The image evoked by Ta, Tpv and Tpi maintain key elements necessary for recognizing typical scenes and characters."}, {"title": "3.2 DATA COLLECTION", "content": "We use LLMs (GPT-3.5) to generate anchor sentences and their permutations, guided by templates. However, LLMs tend to produce patterns common in their training data, which leads to the neglect of less common combinations specified by templates and rules. To address this issue, we employ a different process for generating Ta, Tpv and Tpi.\nTemplate Acquisition. We choose all 171 sentence pairs suitable for T2I synthesis from Winoground Thrush et al. (2022); Diwan et al. (2022) as seed pairs. These pairs are used to extract templates and rules for Ta and Tpv, while those for Tpi are extended manually. To increase diversity, we change the word orders according to the part of speech, including number, adjective, adjective phrase, noun, noun with adjective, noun with clause, noun with verb, noun with prepositional phrase, verb, verb with adverb, adverb, prepositional and prepositional phrase. In Fig. 4, the top left shows an example of templates for Ta and Tpv derived from extraction, while the top right shows the corresponding templates for Ta and Tpi derived from manual completion.\nTemplate-guided Generation for Ta. We use LLMs to generate anchor sentences by filling template slots based on prior knowledge and maximum likelihood estimation. In Fig. 4, the bottom middle sentence Ta is generated using the template for Ta as a guide.\nRule-guided Permutation for Tpv. Tpv is generated by swapping or rearranging words in Ta based on predefined rules, ensuring that Tpv introduces semantic variation. This method avoids a random generation or a semantically equivalent passive structure to Ta, which a common pitfall in autonomous generation by LLMs. By following these rules, Tpv includes many rare combinations not commonly found in existing NLP corpora. In Fig. 4, Tpv is generated by swapping [Noun1] and [Noun3] in Ta (shown in the top left).\nParaphrasing-guided Permutation for Tpi. Tpi can be generated by following rules, such as exchanging phrases connected by coordinating conjunctions. However, not all sentences contain coordinating conjunctions, so we also allow other synonymous transformations, including passive voice and slight rephrasing. Both Tpi examples in Fig. 4 are acceptable."}, {"title": "3.3 DATA \u0391\u039d\u039d\u039f\u03a4\u0391\u03a4\u0399ON AND STATISTICS", "content": "LLM and Human Annotation. We establish 14 specific criteria to define what constitutes a \"valid\" input sample. LLMs check each sample against these criteria, labeling them as \u201cyes\u201d or \u201cno\u201d with confidence scores. Samples labeled \u201cno\u201d with confidence scores above 0.8 are removed. Then, 15 annotators and 3 experienced experts manually verify the remaining samples. Each sample is independently reviewed by two annotators, with an expert resolving any disagreements. After expert verification, we obtained 11,454 valid, non-duplicated samples. To rigorously evaluate T2I models, 684 challenging samples were selected based on thresholds and voting for the test set. More details on annotation and selection are provided in Appendix C.2."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "T21 Synthesis Models. We evaluate 13 mainstream T2I models as shown in Tab. 1. For each sentence, we generate one image, resulting in a total of 684 \u00d7 3 \u00d7 13 images. Each input prompt is the sentence itself, without any negative prompts or additional details expanded by prompt generators.\nEvaluators. We use 4 advanced MLLMs as the evaluators to calculate text-image alignment scores: Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-40 and GPT-4 Turbo. The latter two have demonstrated near-human performance in evaluating the text-image alignment in T2I synthesis Zhang et al. (2023); Chen et al. (2024). We format a sentence and an image in a prompt and feed it into the evaluator, asking it to assign two scores: object accuracy (0-50 points) and relation accuracy (0-50 points). The sum of these two scores is treated as the total score, which is then normalized to [0, 1].\nMetrics. We use 4 metrics for evaluation: the alignment score Sii, the visual semantic variation scores and under different interventions as defined in Eq. 2 and Eq. 3, and the SemVarEffect score k as defined in Eq. 4. For each sample, Sii = \u00a6\u00c0\u00a6\u2211iek S(Ti, Ii), where K = {a,pv, pi}. A T2I"}, {"title": "4.2 RESULTS", "content": "The results of the influence of inputs semantic variations on outputs semantic variations in T2I synthesis are shown in Tab. 2. The scores for S range between 0.6 and 0.8. Despite the alignment score S reaching up to 0.8, this does not imply a strong grasp of semantics. The following three metrics provide a more comprehensive view of the model's ability to handle semantic variations.\nVisual Semantic Variation with Changed Textual Semantics. As shown in Tab. 2, the values of Yw are all below 0.52 for all evaluators, significantly lower than the optimal value of 1. This indicates that none of the T2I models perform at an acceptable level. These models are highly insensitive to semantic variations. This finding aligns with the widely accepted notion that T2I models tend to treat input text as a collection of isolated words, leading them to interpret sentences with minor changes in word order as having the same meaning.\nVisual Semantic Variation with Unchanged Textual Semantics. The values of Two in Tab. 2 are unexpectedly much higher than the optimal value of 0. Only the models highlighted in blue and green demonstrate slightly better performance, with Two scores consistently lower than Yw. These T2I models illustrate potential semantic variations caused by word order through images, yet they still struggle to accurately differentiate between inputs with varying meanings and those with invariant meanings. These models primarily understand language based on word order rather than the underlying semantics.\nInfluence of Textual Semantics on Visual Semantic Variations. In Tab. 2, the K values for all evaluators are below 0.20, indicating considerable room for improvement in T2I models' understanding of semantic variations. Models with higher alignment scores are more sensitive to semantic variations caused by word orders. However, models highlighted in blue overreact to permutations maintaining the meanings, resulting in higher Two values and subsequently lower K values. These models excel at capturing common alignments but struggles to handle semantic variations."}, {"title": "4.3 ANALYSIS", "content": "Is a superior text encoder the exclusive solution for T2I models to grasp semantic variations? We explore the relationship between the text encoder's ability to discriminate semantic variations and the ability of two metrics\u2014alignment scores S and visual semantic variation scores y to do"}, {"title": "6 CONCLUSION", "content": "We comprehensively study the challenge of semantic variations in T2I synthesis, specifically focusing on causality between semantic variations of inputs and outputs. We propose a new metric, SemVarEffect, to quantify the influence of input semantic variations on model outputs, and a novel benchmark, SemVarBench, designed to examine T2I models' understanding of semantic variations. Our experiments reveal that SOTA T2I models, including CogView-3-Plus and Ideogram 2, struggle with semantic variations, with most scoring below 0.2 on our benchmark. This indicates that these models have yet to develop the capability to effectively handle such variations. Fine-tuning efforts also show limited success, improving sensitivity to certain variations but at the cost of robustness. These findings highlight the importance of our metric and benchmark in addressing this challenge. Future work should focus on enhancing cross-modal alignment to better manage subtle semantic changes and improve overall T2I model performance."}, {"title": "A FOUR TYPES OF SEMANTIC VARIATIONS RESULTS IN T21 SYNTHESIS", "content": "The results of semantic variations in T2I synthesis, both in text and images, can be divided into four types, as shown in Fig. 10.\nImage Changing Semantics with Text Changing Semantics: The semantic consistency between the input and output in the first quadrant suggests that the model tends to understand the different semantics introduced by linguistic permutations. In this case, the value of /w/ will tend to 1.\nImage Maintaining Semantics with Text Changing Semantics: The semantic inconsistency between the input and output in the fourth quadrant suggests that the model does not understand the different semantics introduced by linguistic permutations. In this case, the value of /w/ will tend to 0.\nImage Changing Semantics with Text Maintaining Semantics: The semantic consistency between the input and output in the third quadrant suggests that the model tends to understand the similar semantics introduced by linguistic permutations. In this case, the value of w/o will tend to 0.\nImage Maintaining Semantics with Text Maintaining Semantics: The semantic inconsistency between the input and output in the second quadrant suggests that the model does not understand the similar semantics introduced by linguistic permutation. In this case, the value of w/o will tend to 1."}, {"title": "B PROPERTIES OF SEMVAREFFECT", "content": ""}, {"title": "B.1 PRELIMINARY", "content": "A T2I generation model f consists of one or more text encoders, image generators, and an image decoder. The T2I generation model f generates images I \u2208 I for each input textual prompt T \u2208 T. T represents the textual space, and I represents the visual space. S(T, I) is the alignment score between T and I.\nLet Ta be an anchor textual prompt. Let Tp\u2217 represent a permutation of Ta, where Tpv is a permutation with a different meaning than Ta, and Tpi is a permutation with the same meaning as Ta. Let Ia, Ipv, and Ipi be the resulting images generated by a T2I model from Ta, Tpv, and Tpi, respectively. We expect that Ip should be a rearrangement of the objects or relations found within Ia."}, {"title": "B.2 TEXTUAL VS. VISUAL SEMANTIC VARIATIONS", "content": "The measurement of semantic variations in the transition from (Ta, Ia) to (Tp\u2217, Ip\u2217) can be defined from two perspectives: (1) textual semantic variations rT: The semantic changes in the text, observed through differences between images Ia and Ip\u2217, and (2) visual semantic variations r\u00b9: The semantic changes in the images, observed through differences between texts Ta and Tp\u2217."}, {"title": "B.3 PROPERTIES OF ALIGNMENT SCORES S", "content": "Definition of Text-Image Alignment Score. To facilitate semantic analysis, we structured these permutations by objects and triples. Changing the word order affects the arrangement of objects or relations and leads to changes in syntactic dependencies and semantics. Let Tp represent any permutation of Ta. Ta and Tp* share the same set of objects set V and set of relations R. The triple set E in Ta is a subset of V \u00d7 R \u00d7 V. Some triples in Tp may differ from those in Ta, but they have the same number of triples. For example, the initial triple set of Ta contains (apple, on, box), (girl, touch, apple) and (girl, NULL, box). After swapping box and apple, the triple set of Tp contains (box, on, apple),(girl, touch, box) and (girl, NULL, apple).\nTo calculate the fine-grained alignment scores for objects and triples, we define the alignment score S between T and I as the sum of the object and triple alignment scores:\nS(T, I) = \u2211Sobji (T, I) + \u2211 Striz (T, I), (5)\nwhere V is the number of objects mentioned in T and E is the number of triples mentioned in T. The components of the alignment score are defined as piecewise functions:\nSobji (T, I) = , (6)\nStriz (T, I) =  ,\nwhere wvi and we are the weighted matching score for the i-th object and j-th triple. We obtain the alignment function S that satisfies the constraints in Eq. 10. Consequently, the alignment score of a matched text-image pair is calculated as:\nS(Tp\u2217, Ip\u2217) = S(Ta, Ia) = \u2211 Wv; + \u03a3 Wej,  (7)\nwhere VMA and EMA represent the exactly matched objects and triples between a text prompt and its generated image, with |VMA| = |V| and |EMA| = |E|. The alignment score for a mismatched text-image pair is calculated as:\nS(Tp\u2217, Ia) = S(Ta, Ip\u2217) = \u2211 Wvi + \u2211 Wej  (8)\nwhere VMI and EMI represent the partially matched objects and triples between a text prompt and a mismatched image, with |VM1| = |V| and 0 \u2264 |\u0415\u043c1| \u2264 |E|.\nRange of S. If f accurately depicts the text through images and S faithfully measures the semantic changes between text space and image space, any alignment score S(T, I) is bounded by:\n\u2211Wvi \u2264 S(T, I) \u2264 \u03a3 Wvi + \u03a3 Wej.        (9)\nIn our implementation, we set the value of S(T, I) as an integer between 0 and 100, where the object accuracy is between 0 and 50 and the triple accuracy is in between 0 and 50. Then we normalize it into a real number within the range [0, 1]. Based on the assumption of f mentioned above, 0.5 \u2264 S(T, I) \u2264 1.\nHowever, limitations in the capabilities of the model f(\u00b7) and the alignment function S(\u00b7), often prevent the alignment score values from achieving the property in Eq. 10. For example, if a model f generated a low-quality image I, it may fail to accurately depict all target objects (V), leading to VMA < |V| and VM1 < |V|. This results in an object accuracy below 0.5 (as illustrated in the bottom case of Fig. 21) and inconsistent relation accuracy (see cases in Fig. 16 and Fig. 17). Furthermore, inaccuracies in the scoring approach S(\u00b7) may incorrectly evaluate the similarity between text prompts and generated images, causing unpredictable fluctuations in semantic variation measurements (as illustrated in Fig. 23)."}, {"title": "B.4 PROPERTY OF VISUAL SEMANTIC VARIATIONS \u03b3", "content": "We analyze the theoretical relationship between visual semantic variations and model performance. According to Eqs. 1, 7 and 8, we derive the visual semantic variations as follows:\n\u03b3 = 2 \u03a3 Wvi + Wej (11)\nFor both permutation-variance and permutation-invariance, the value of vi\u2208 {VMA-VMI} Wvi remains constant, which we denote as C\u2081. As a result, the visual semantic variation can be simplified to y\u00b2 = 2 C1 + \u03a3\u03b5; \u20ac{EMA-EM1} Wes, primarily depending on the size of the set EMA \u2013 EMI\u00b7 However, the size of the set varies dramatically between the two settings:\nIn permutation-variance settings (Ta, Tpv), the optimal value for the set EMA \u2013 EMI is its maximum set E, resulting in an observed positive correlation between visual semantic variation and model performance.\nIn permutation-invariance settings (Ta, Tpi), the optimal value for the set EMA \u2013 EMI is its minimum set \u00d8, resulting in an observed negative correlation between visual semantic variation w/ and model performance.\nTherefore, we conclude that the visual semantic variations in permutation-variance and permutation-invariance differ significantly.\nA higher w/ value indicates that the model effectively captures and reflects the intended semantic transformation in the input text.\nA lower w/o value indicates that the model maintains semantic consistency in the images despite variations in the input text."}, {"title": "B.5 PROPERTY OF SEM VAREFFECT SCORE K", "content": "The SemVarEffect score on visual semantic variations, \u03ba, is defined as the difference between and w/. It quantifies the model's ability to discriminate between significant and negligible semantic changes in the text.\nIf \u03ba is large, it suggests that the model is sensitive to semantic changes, recognizing variations in meaning. However, this does not necessarily imply strong alignment. The model might detect changes in semantics but still struggle to fully capture all objects and relationships described in the text, indicating a gap between sensitivity and complete alignment.\nIf is small or close to zero, it suggests that the model either fails to reflect meaningful semantic changes or overreacts to minor text variations. Regardless of the overall alignment score, the model may generate similar images even in the presence of significant semantic differences in the input text."}, {"title": "C CONSTRUCTION DETAILS", "content": ""}, {"title": "C.1 DATA COLLECTION", "content": ""}, {"title": "C.2 DATA \u0391\u039d\u039d\u039f\u03a4\u0391\u03a4\u0399ON", "content": "Criteria for Valid Samples. The primary challenge in annotation lies in defining the criteria for what qualifies as \u201cvalid\u201d. For T2I synthesis models, we define \"valid\" input text based on 14 specific criteria. First, we illustrate these criteria through examples of Ta and Tpv. Second, for Tpi, we require it to apply one of the six synonymous transformations defined in the prompt for generating Tpi. From a semantic perspective, Tpi must be strictly consistent with Ta, ensuring the consistency and accuracy of the entire dataset. We list these criteria and examples in the following."}, {"title": "D DETAILS OF EXPERIMENT SETTING", "content": ""}, {"title": "D.1 T2I SYNTHESIS MODELS", "content": "We generate one image using the mainstream T2I diffusion models in Fig. 1: Stable Diffusion v1.53 (denoted as SD 1.5), Stable Diffusion v2.14 (denoted as SD 2.1), Stable Diffusion XL v1.05 (denoted as SD XL 1.0), Stable Cascade (denoted as SD CA), DeepFloyd IF XL7 (denoted as DeepFloyd), PixArt-alpha XL(denoted as PixArt), Kolors, Stable Diffusion 3 [medium](denoted as SD 3), FLUX.1 [dev]10 (denoted as FLUX.1), Midjourney V611 (denoted as MidJ V6), DALL-E 312, CogView3-Plus13 (denoted as CogV3-Plus), Ideogram 214. The schedulers for SD 1.5 and SD 2.1 are set to DPM-Solver++, while all other settings are left as default."}, {"title": "D.2 EVALUATOR", "content": "We use four advanced MLLMs as the evaluators to demonstrate the general applicability of our proposed evaluation metrics: Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-40 and GPT-4 Turbo. GPT-40 and GPT-4 Turbo have been shown to achieve near-human performance in evaluating alignment in T2I synthesis modelsZhang et al. (2023); Chen et al. (2024).Claude 3.5 Sonnet outperforms GPT-40 and Gemini 1.5 Pro Anthropic (2024). The versions of these MLLMs used are as follows: Gemini 1.5 Pro (gemini-1.5-pro-001), Claude 3.5 Sonnet (claude-3-5-sonnet-20240620), GPT-40 (gpt-40-2024-05-13), and GPT-4 Turbo (gpt-4-turbo-2024-04-09). The alignment score components follow the division outlined in Zhang et al. (2023), with the exception of the aesthetic score component, which has been omitted. The complete prompt is as follows."}, {"title": "D.3 TRAINING SETTING", "content": "Training Data Selection. The training set of SemVarBench comprises 10,806 samples. We investigate the improvement from fine-tuning the T2I model Stable Diffusion XL v1.0. We select the generated images whose alignment scores meet the requirements. These constraints are as follows.\nFirst, the generated image should be approximately aligned with its corresponding text prompt.\n> C2, (12)\nwhere C2 is a threshold.\nSecond, the alignment scores between matched text-image pairs should be higher than those between mismatched text-image pairs.\n> S(Ta, \u0399\u03c1\u03c5), (13)\nThird, the visual semantic variations observed from different text prompts should be the same when the initial image and the final image are the same.\nS(Ta, Ia) - S(Ta, Ipv) \u2248 S(\u03a4\u03c1\u03c5, \u0399\u03c1\u03c5) \u2013 S(\u03a4\u03c1\u03c5, \u0399\u03b1), (14)\nSimilarly, the textual semantic variations observed from different images should be the same when the initial text prompt and the final text prompt are the same.\nS(Ta, Ia) - S(\u03a4\u03c1\u03c5, Ia) \u2248 S(\u03a4\u03c1\u03c5, \u0399\u03c1\u03c5) \u2013 S(Ta, \u0399\u03c1\u03c5), (15)\nUtilizing this approximate equality relationship in Eq. 14 and Eq. 15, we constrain the alignment score using the following inequality:\n(S(Ta, Ia) - S(Ta, Ipv)) - (S(Tpu, Ipu) - S(Tpu, Ia))| < C3,  (16)\nIn our experiments, we utilized Stable Diffusion XL v1.0 to generate an image for each text prompt within the training set. To select the training data, we designated C2 = 0.8 and C3 = 0.1. Ultimately, we selected 327 samples, resulting in 981 sentences.\nSupervised Fine-Tuning (SFT). Each text-image pair (Ti, Ii) is incorporated into the training set. For every sample (Ta, Tpv, Tpi), which results in three text-image pairs: (Ta, Ia), (\u03a4\u03c1\u03c5, Ipv) and (Tpi, Ipi), leading to a total of 981 diverse pairs. The selected set of samples is denoted as Ds. The"}, {"title": "E MORE EXPERIMENT RESULTS", "content": ""}, {"title": "F MORE ANALYSIS", "content": "Is there a significant difference among various text encoders in discerning the semantic nuances caused by linguistic permutations? We explore the efficacy of diverse text encoders in discerning such nuances. Fig. 13 compares the text similarity between Ta and Tpu across models utilizing different text encoder models. SD 1.5, SD 2.1, SD XL v1.0, and SC utilize CLIP series models as text encoders, while DeepFloyd, PixArt, and DALL-E 3 utilize T5 series models. The similarity metric is depicted as 1 \u2013 cosine(Ta, Tpv), with higher values indicating a stronger ability of the text encoder to differentiate between the semantics of two sentences. This indicates that the choice of text encoder significantly influences the model's semantic discrimination capabilities.\nDetailed Analysis on Alignment Scores vs. SemVarEffect Score Fig. 14 illustrates that although the distributions of the SemVarEffect score and the alignment score are similar, the SemVarEffect score demonstrates a higher degree of differentiation, especially when it comes to distinguishing between FLUX.1 and SD 3. Based on the alignment score, it could be concluded that FLUX.1, SD 3, and SD XL 1.0 have comparable performance levels and they may be grouped into the same cluster. However, based on the SemVarEffect score, it becomes evident that FLUX.1 and SD 3 differ distinctly from SD XL 1.0. SD XL 1.0 responds more similarly to semantic variations caused by word order changes in a manner similar to SD 1.5, SD 2.1, and SD CA. Correspondingly, we observe that when using the T5-XXL series model as the text encoder, the difference between DALL-E 3 and other models, such as PixArt and DeepFloyd, becomes more pronounced when assessed by the SemVarEffect score."}, {"title": "G MORE CASE STUDIES", "content": "In this section, we present examples that demonstrate an understanding of semantic variations and examples that do not. Examples that grasp semantic variations typically have high alignment scores (Sii) and high effect scores (\u03ba), as illustrated in Fig. 15. Conversely, examples that lack this understanding often have high alignment scores (Sii) but low effect scores (\u03ba), as depicted in Figures"}, {"title": "H LIMITATION", "content": "We would like to highlight that the size of SemVarBench is constrained by the necessity for manual verification due to the unsatisfactory accuracy of LLM's validation, which incurs high costs. Furthermore, the scale of evaluation is limited by the high costs of image generation and LLM-based evaluation, both in terms of time and money, thus restricting the extent of such evaluations."}]}