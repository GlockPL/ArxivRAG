{"title": "One-step Structure Prediction and Screening for Protein-Ligand Complexes using Multi-Task Geometric Deep Learning", "authors": ["Kelei He", "Tiejun Dong", "Jinhui Wu", "Junfeng Zhang"], "abstract": "Understanding the structure of the protein-ligand complex is crucial to drug development. Existing virtual structure measurement and screening methods are dominated by docking and its derived methods combined with deep learning. However, the sampling and scoring methodology have largely restricted the accuracy and efficiency. Here, we show that these two fundamental tasks can be accurately tackled with a single model, namely LigPose, based on multi-task geometric deep learning. By representing the ligand and the protein pair as a graph, LigPose directly optimizes the three-dimensional structure of the complex, with the learning of binding strength and atomic interactions as auxiliary tasks, enabling its one-step prediction ability without docking tools. Extensive experiments show LigPose achieved state-of-the-art performance on major tasks in drug research. Its considerable improvements indicate a promising paradigm of AI-based pipeline for drug development.", "sections": [{"title": "1 Introduction", "content": "Small organic molecule (SOM) plays an important role in clinical treatment, accounting for about 72% FDA-approved drugs in 2018 ~ 2022[1]. Its effi- cacy is achieved by binding to the target (usually a protein) as a ligand, to produce a protein-ligand complex. Understanding the structural details of the protein-ligand complexes at the atomic level reveals the bioactivities of the proteins and ligands, thus guiding the structure-based drug development, e.g., drug screening[2, 3, 4] and lead optimization[4, 3, 5]. Conventional methods use experimental measurements (e.g., X-ray diffraction[6] and cryo-electron microscopy[7]) to analyze novel protein-ligand complexes, however, are time and resource-expensive.\nTo alleviate this problem, virtual measurement by molecular docking[8, 2, 9, 4] has been widely adopted in the past decades to predict the native- like ligand-binding conformations with the respective protein-binding sites. Typically, a docking tool first samples a set of binding conformations (namely poses), and then ranks them using a scoring function to select a top-scored pose[9] (Fig. 1b). As reported, popular docking tools generate native-like poses with accuracies from approximately 40% to 60% in terms of success rate[10], which were far from satisfactory. Since deep learning has impacted the field of drug development, many researchers attempted to build hybrid methods by regarding it as a more expressive scoring function to rank the poses sampled by conventional docking tools [11, 12, 13, 14, 15, 16]. Nevertheless, the performance of these hybrid methods has a constrained upper bound, as limited by the sampling time and space of docking tools[15].\nRecently, several deep learning methods, i.e., AlphaFold[17], RoseTTAFold[18], and ESMFold[19], have shown great capacity to predict protein structures that outperformed previous methods by a large mar- gin. These methods provided a novel computational approach to generate protein structures directly from their chemical sequences, rethinking the structure prediction paradigm in a data-driven perspective without following the conventional force-field assumption[20]. Recent progresses[21, 22, 23, 24] demonstrated that using these methods as the protein/peptide structure estimator can predict precise structures for amino acid-based complexes such as the protein-protein and protein-peptide complexes, showing the general- izability of deep learning methods to the downstream tasks. However, these methods are not specifically designed for SOM ligands, which inevitably restricts the related applications such as protein-ligand complex structure prediction and virtual screening. As these two fundamental tasks have heavily relied on docking tools in the past four decades [25, 26, 27, 28], developing a deep learning model that provides refined atomic complex structure and screening for the protein-ligand pairs is highly demanded.\nTo this end, we introduce LigPose, a novel docking-free geometric deep learning method to accurately predict the native-like conformation of ligands with their corresponding protein targets and the respective binding strengths in one step (Fig. 1c). Specifically, for a given protein and ligand pair, the"}, {"title": "2 Results", "content": ""}, {"title": "2.1 LigPose pipeline", "content": "LigPose predicts the 3-D structure of the ligand-binding conformation with its protein target in an end-to-end manner. As shown in Fig. 1c, a given protein and ligand are jointly represented by a complete undirected graph, where each atom is denoted as a node, representing its chemical feature and coordinate, and all nodes are mutually connected. Since the entire protein-ligand graph is large, LigPose (Fig. 1d) first adopts a sampling and recycling strategy, to predict with multiple cycles, where each cycle updates a randomly sampled sub-graph. The features and coordinates within the sub-graph are then pro- cessed by the proposed feature and coordinate update blocks. These two blocks are built on the graph neural network and stacked 6 times with unshared weights. The key design of these two blocks is to leverage the inter-atom distances during network forwarding for maintaining the spatial information, making the predicted atom coordinates insensitive to their initial positions, but influenced by the inter-atom correlations. Finally, a symmetric-aware loss is proposed to optimize the network, combined with a stochastic coordinate initialization strategy for the ligands, to enable the method of distinguishing the ligand atoms with the same chemical features. The affinity and screening"}, {"title": "2.2 Predicting accurate complex structures", "content": "In experiments, we first focus on the flexible-ligand prediction with the determined protein pockets, a widely used setting in structure-based drug development[30], to demonstrate its effectiveness. We compare LigPose with 12 popular docking tools on the refined set of PDBbind database[31, 10]. The PDBbind database collects a large set (N = 19443) of 3-D biomolecular com- plex structures from the PDB database. The refined set (N = 5316) and the"}, {"title": "2.3 Ligands with various flexibilities", "content": "To solve the ligands with high flexibilities (i.e., with many rotatable bonds), the strategies of sub-graph sampling and recycling are combined to progres- sively refine the predictions. We visualize the ligand atoms and plot the RMSD trajectories for two representative samples in Fig. 3a, where one is heavier with more rotatable bonds (PDB code: 1EBY) than the other (PDB code: 105B). As a hard example, the prediction of 1EBY reaches the threshold of 2\u00c5 after 16 updates, which is much slower than that of 5087 (with 5 updates), indicating the effectiveness of the proposed recycling strategy.\nTo quantify the performance of LigPose concerning the number of rotatable bonds, we plot the results of LigPose and Smina on the refined set in terms of RMSD (b) and success rate (c) in Fig. 3. One can observe is the difficulty of prediction is positively correlated with the number of rotatable bonds. We sup- pose the reason is that more rotatable bonds indicate higher flexibility of the ligand. Although the success rates of LigPose and Smina are both decreased when predicting ligands with many rotatable bonds, LigPose performed con- sistently better than Smina through the dataset. Moreover, Smina failed to predict with more than 24 rotatable bonds, indicating the limited processing power of the docking methods. By contrast, LigPose correctly predicted 20% of them, showing the merit of our learning-based methodology. Similar findings were discovered from the core set, as shown in Suppl. Fig. A7.\nThe efficiency is also a core factor of drug development[40], caused by a large number of drug-like molecules for screening and measurement. We then plot the inference time per ligand with respect to the number of ligand rotatable bonds for LigPose and Smina in Fig. 3d. For a fair comparison, we use a single CPU core to implement docking tools and LigPose (CPU), with only an additional common GPU device (Nvidia GTX 2080Ti) for the GPU version (LigPose (GPU)). From the figure, we observed the inference time of LigPose is constant, in contrast to that of docking tools, which is positively correlated with the number of rotatable bonds. On average, LigPose inferences 4-26 (CPU) / 303 \u2013 1851 (GPU) times faster than docking tools. For ligands with more than 10 rotatable bonds, LigPose can infer 7299x times faster than docking tools, and up to 14256x faster for ligands with more than 20 rotatable bonds."}, {"title": "2.4 Accurate and fast screening", "content": "Accurate prediction of the complex structure is an essential procedure, and ideally beneficial for the virtual screening task. Therefore, we validate the screening power of LigPose using the CASF-2016 benchmark, which contains 57 proteins, each having at least 5 true binders with a wide range of affinities. Therefore, for each SOM-protein pocket pair, LigPose predicts the potential binding strength of SOMs to the protein pockets as the screening score, through the multiplication of the predicted binding probability and affinity.\nAs suggested by Fig. 4, LigPose outperformed the state-of-the-art methods in all three metrics with a very large performance gap, compared with recent deep learning-based methods and popular conventional methods. Concretely, for the forward screening, i.e., the task of identifying true binding SOMs for a certain protein, LigPose gets an average enhancement factor (EF) of 36.4%, and a success rate of 86.0% on top 1%-ranked SOMs. These results are greatly higher than that of the second best performing method RTMScore[36] (with EF of 28.0% and success rate of 66.7%), with improvements of 8.4% and 19.3% on EF and success rate, respectively. It also largely outperforms recent deep learning-based methods, i.e., DeepDock[12] (with EF of 16.4% and success rate of 43.9%) and PIGNet[41] (with EF of 19.36% and success rate of 55.4%). Besides, the results show huge gaps to conventional methods (with EFs ranging from 0.8% to 11.9% and success rates ranging from 1.8% to 42.1%). For top- 5% and top-10% ranked SOMs, similar conclusions can be made. Notably, LigPose reaches very high success rates of 96.5% and 98.2% on top-5% and top-10% ranked SOMs, suggesting that nearly all true SOM binders are settled at the top of the predictions. Similarly, for the reverse screening, i.e., the task of identifying true binding proteins for a certain SOM, LigPose obtains success rates of 50.5%, 77.9%, and 87.0% in top-1%, top-5%, and top-10% ranked"}, {"title": "2.5 Validating on the SARS-CoV-2 Mpro", "content": "We further validate LigPose on the main protease (MPro) of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) to demonstrate its efficacy in real-world applications. Mpro plays a critical role in virus replication as a"}, {"title": "2.5.1 Learning non-covalent interactions", "content": "In addition to the effectiveness and efficiency, LigPose also showed good interpretability by investigating its ability to reconstruct the non-covalent interactions. We used a metric, namely interaction reproducibility, to quantify this ability, following the workin [48] (see Methods for details).\nFrom Fig. 6a, we observed the interactions are well reconstructed by Lig- Pose on predictions with RMSD < 2\u00c5, and have lower scores on predictions with RMSD\u2265 2\u00c5. For each type of interaction (Fig. 6b), LigPose performed best for hydrogen bond and water bridge and showed insensitive to r-cation and halogen bond. We suppose the reason is the lack of respective lig- and samples. We further compared the performance of LigPose with Smina (Suppl. Figs. A8 and A9), and found that LigPose showed consistently better performance for all the seven types of interactions.\nIn addition to the high quantitative scores, we further inspected the atten- tion weights, by visualizing three representative complexes with respect to three non-covalent interactions (i.e., hydrophobic interactions, salt bridge, and \u03c0-stack) in Fig. 6(c-f). We observed from the figure that LigPose can capture some potential interactions between ligands and proteins, without explicitly using related physical or chemical prior knowledge."}, {"title": "3 Discussion", "content": "In this work, we propose LigPose, which can predict protein-ligand complex structure, affinity, and screening probability simultaneously using multi-task geometric deep learning. The protein-ligand pair is first presented by one complete undirected graph, then, LigPose with a graph transformer network structure, directly predicting the complex structure in three-dimensional space. To precisely predict the complex structure with atomic accuracy, LigPose is"}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Data collection", "content": ""}, {"title": "4.1.1 Benchmark Dataset", "content": "Training and validation dataset. We use the general set of PDBbind database (version 2020)[31] to develop and validate LigPose, consisting native structures of 19443 protein-ligand complexes. Within the general set, two sub- sets are used for evaluation, i.e., the refined set and the core set, which are of better data quality for evaluating docking tools and scoring functions in exist- ing works. We used the refined set to validate the performance of LigPose with five-fold cross-validation. In each fold, the rest of the samples in the general set with non-overlapping to the validation set are used for training. Performance for 2002 selected complexes in the refined set as adopted in [10] under a similar setting is also reported. For the core set we obtain the training data following the previous deep learning-based methods, e.g., DeepDock[12], RTMScore[36].\nThe peptide-like SOMs are identified by the Biologically Interesting Molecule Reference Dictionary (BIRD)[80]. 2200 random selected samples in the training set are used for hyper-parameter search, and the selected archi- tecture is used for all tasks. The unsuccessfully processed data by RDKit[81] is removed. None of the samples in the core set but 14 samples in the refined set failed to be processed. Finally, only a minor proportion of the entire PDBbind database (153, < 1%) is dropped.\nAuxiliary unlabeled dataset. The structure-determined complexes are only a tiny portion of the complex family. To enhance the performance and general- izability of LigPose, we add a task to LigPose to train it under a self-supervised learning scheme, which models the data structures from large-scale unlabeled data, i.e., with unknown complex structures and protein/ligand properties. To this end, we collected a large unlabelled dataset, including a total num- ber of 2147477 SOMs and 171789 proteins. The SOMs are derived from two databases, i.e., DrugBank[82] (11290) and ChEMBL [83] (2136187), with diver- sified drug/drug-like compounds that are widely used for the pre-training of SOM features[84]. The proteins are derived from the PDB database[85].\nTest dataset. The whole core set and the benchmark dataset CASF-2016[86] derived from the core set with carefully designed poses are used for testing. We adopt four main tasks, i.e., scoring power (the ability to estimate binding affin- ity), ranking power (the ability to identify near-native pose), docking power (the ability to identify near-native pose), forward screening power (the ability to identify potential ligands for a target protein) and reverse screening power (the ability to identify potential target proteins for a SOM), to comprehen- sively assess the performance of LigPose. As abovementioned, all complexes in the core set are removed from the training set to make them non-overlapped."}, {"title": "4.1.2 Real-world dataset", "content": "To demonstrate the ability of LigPose in real applications, we perform it on two major tasks (i.e., complex structure prediction and screening) for the drug development of SARS-CoV-2 Mpro.\nMpro structure dataset. We adopt the dataset as reported in [43], which contains 44 non-covalent and non-surface-bound ligands with Mpro from SARS- CoV-2.\nMpro screening dataset. The screening set (N = 344) contains SOM binding data by experimentally measuring the enzymatic activity of MPro, which is collected from DrugCentral[46], followed with a processing protocol introduced in ImageMol[47]."}, {"title": "4.2 Pre-processing", "content": "In this work, the protein pocket was adopted as the binding target, since obtaining the atomically refined structure is the major concern in drug development[87, 4]. Besides, many methods have been successfully developed to predict the ligand-binding pockets with high accuracies[66, 67, 68, 69, 88, 89].\nThe protein pockets are the surrounding amino acids of the ligands in 3-D space. For the unlabeled data, we complete the missing atoms with modeler[90] and search the pockets using the Fpocket[91] method with the default setting. All pockets are filtered with a Druggability Score of > 0.5, therefore, 631687 pockets are finally used. Fpocket recognizes the pocket and represents the pocket center as virtual atoms. A residue is selected as the part of a pocket when its Ca atom is within 13\u00c5 to the nearest virtual atom. For the labeled data, a residue is selected as the part of a pocket when its Ca atom is within 15\u00c5 to the nearest ligand atom. Water molecules are not considered in this study.\nFor a given protein-ligand pair, i.e., a random pair in the unlabeled dataset or a native complex in the PDBbind dataset, a complete undirected graph is firstly constructed using RDKit[81], with each node representing an atom, and all nodes (including themselves) are mutually connected. We initialize the graph by features listed in Suppl. Table A3. In particular, the nodes are initial- ized as vectors with a length of 79 for a protein and 45 for a ligand, containing the chemical features. Edges are also initialized as vectors with a length of 7 including covalent bond features and distance. Distances connecting different rigid parts are masked with -1. To enable the ability of SE(3)-equivalence for the method, and also augment the data, we implement a stochastic strategy to initialize the coordinates of the ligands. In this case, the protein nodes are kept in their original positions, while the positions of the ligand nodes are ran- domly initialized inside the pocket with an empirical distance. In this work, we initialize them in a Gaussian distribution with a standard deviation of 10\u00c5."}, {"title": "4.3 LigPose architecture", "content": "LigPose directly predicts the structure of the ligand-binding conformations with their target protein pockets in the 3-D space in an end-to-end manner. As shown in Fig. 1c, LigPose has a graph transformer architecture, with three major components, (1) a sampling and recycling strategy, (2) a feature update block followed by a coordinate update block to forward the features and coordi- nates through the graph, and the two blocks are stacked 6 times with unshared weights, and (3) a stochastic initialization method for ligand nodes and a novel symmetric-aware loss. We will then illustrate them below."}, {"title": "4.3.1 Sampling and recycling", "content": "Typically, a protein pocket contains hundreds of atoms. Using all of them as the input is inefficient and memory unaffordable for common GPU devices. Therefore, we adopt a sampling strategy to generate a sub-graph consisting of all core atoms and some randomly selected context atoms, with their respec- tive edges, to feed to the network. Specifically, the core atoms involve all ligand atoms, and the Ca, C\u1e9e atoms of the protein, as they are enough to determine the position and orientation of the amino acids and protein backbone[92, 93]. The other protein atoms are regarded as the context atoms to describe the structural details of the amino acids. Since the nodes are not fully used, we further introduce a recycling strategy, to enhance the representability of the method, as inspired by [17]. In each cycle, a new sub-graph is sampled and for- warded, then, the updated graph is reused in the next cycle. In the next cycle, the newly sampled graph directly inherits the coordinates of corresponding nodes from the last cycle, and its features of the core atoms are combined with the last updated features by using an element-wise gate on the new features, implementing a partial update. (See details in Suppl. Sec. A.2.2)"}, {"title": "4.3.2 Feature Update Block", "content": "The feature update block maintains a graph Transformer-based architecture to forward the node and edge features. It iteratively performs two operations, i.e., message aggregation and feedforward. In message aggregation, information is aggregated from neighboring nodes to a central node using the multi-head attention (MHA) mechanism (Fig. A3)[94]. In addition, the edge features are also incorporated to enhance the representation.\nWe use conventional cross-attention to calculate the MHA in the network, according to Query (denote as q), Key (denote as k), and Value (denote as v)[94] (See details in Suppl. Sec. A.2.2). Briefly, q and v are produced by the features of the central nodes and the neighboring nodes, respectively (see Fig. A3b). k is extracted from the edge feature and a linearly transformed neighboring node feature (Fig. A3a-b). The edge feature here is enhanced by the spatial information using the distance between the central node and its neighboring node (Fig. A3a). A softmax layer is then applied to the product of q and k to get attention masks for each v. The central node aggregates the"}, {"title": "4.3.3 Coordinate Update Block", "content": "The coordinate update block updates the coordinates of the nodes (atoms) in 3- D space based on the attentions derived from the feature update block, as also introduced in [95] (Fig. A3e-f). To be specific, the attention is transformed to a one-dimensional distance gradient, indicating the change in distance between the central node and each of its neighboring nodes. Then, the coordinate of a certain central node is updated by the sum of these distance gradients calculated with all neighboring nodes, as written by,\n$$\\Delta_i = \\sum_{j \\in Neighbor(i)} \\frac{x_i - x_j}{||x_i - x_j||_2} \\Phi (q_{ij} \\odot k_{ij}),$$\n$$x_i^l = x_i^{l-1} + \\sum_{h=1,...,N_h} \\lambda_h \\Delta_i^h,$$\nwhere $x_i$ denote the coordinate of a central node $i$, $x_j$ denote the coordi- nate of a neighbouring node $j$, $q_{ij}$ and $k_{ij}$ are the Query and Key for $i$ and $j$, respectively, as obtained in the feature update block. The element-wise multi- plication (\u2299) of $q_{ij}$ and $k_{ij}$ are then transformed to a single distance variable by a linear layer \u03a6, and multiplied by the direction of a central node\u2019s coor- dinate ($x_i$) to one of its neighboring node\u2019s coordinate ($x_j$). The coordinate update \u0394i for node i is then obtained by aggregating \u25b2 for all i\u2019s neighbour- ing nodes (Neighbor(i)), where i is not equal to j. xi is then updated by the weighted sum of all \u2206h to x. Finally, the coordinates were updated using MHA with \u03bb\u03b7 weighted Nh heads."}, {"title": "4.4 Correlation-enhanced graph learning", "content": "We introduce a novel training paradigm, namely correlation-enhanced biomolecular graph learning, that simultaneously learns both the graph fea- tures and the 3D structures for proteins and SOMs. In brief, in each training iteration, the network is trained with a half-to-half chance by a labeled sample (i.e., a native complex) or an unlabeled sample (i. e., a randomly paired protein and ligand). We use the Monte Carlo method to choose one cycle to opti- mize the network parameters, as also adopted by [17]. Details of the training schedule can be found in Suppl. Sec.A.3"}, {"title": "4.4.1 Training with native complexes", "content": "Given a ligand containing Nlig atoms (nodes). In each iteration, an index is created for mapping the nodes from the predicted pose to the native pose,"}, {"title": "Notations", "content": "We summarize the notations present in this paper in Table A2."}, {"title": "Supplementary Methods", "content": "LigPose consists of three key components (see main text Fig. 1d), 1) a sampling and recycling strategy, 2) a stochastic coordinate initialization combined with a symmetric-aware loss function, and 3) a multi-task training scheme including the self-supervised learning on large-scale unlabeled data. The implementation details of these components will be introduced below."}, {"title": "Sampling and recycling", "content": "To reduce the computational burden of handling a large number of protein atoms, we proposed a sampling strategy for protein atoms. Specifically, for a given graph representing a protein-ligand pair, a sub-graph including all ligand atoms, and the Ca and C\u1e9e atoms derived from the protein pocket, is used to feed the model. These atoms are denoted as core atoms. Then, a set of atoms that are randomly sampled from the rest of the protein pocket atoms (denote as context atoms), are also included in the graph. The total number of core and context atoms are denoted as Ntotal. All edges are assigned according to the real inter-atom relationship. Since the atoms of the complex are not fully used, which may weaken the representative ability of the method, we introduce a recycling strategy to compensate for it. In this case, the method is processed cycles for one complex, and within each cycle, the features and the coordinates of the complex are updated N\u012b times using the stacked feature and coordinate update blocks. Besides, in each cycle except the first cycle, the last updated features of the core atoms will be added to a newly sampled sub-graph with the same as defined in the feature update block, while the last updated features of the context atoms are ignored, to conduct a \"partial update\". The edge features are updated with the same principle. And the coordinates of the ligand nodes in the newly sampled sub-graph are directly assigned with their last updated values. Then, the sub-graph will be fed to the model again. The recycling will be performed Nc times (Nc = 4 in this work)."}, {"title": "Feature update block", "content": "The feature update block extracts features of all nodes with an architecture based on Graph Transformer[97, 98]. Given an input graph G = (V, E), the feature of the ith node f\u00bf \u2208 V, and the feature of the edge between the ith node and its jth neighbouring node (denote as eij \u2208 E) are initialized as finit and einit, respectively, according to Table A3. Then, each feature is fed to a linear layer with respect to its feature type (i.e., the protein node feature, the ligand node feature, and the edge feature). The output features of the nodes"}, {"title": "Coordinate update block", "content": "The coordinate update block updates the coordinate of the nodes (atoms) in 3-D space along with the feature update block, as inspired by [95]. Specifically, for the Ith block, the coordinates are updated as follows,\n$$\\Delta_i^h = \\sum_{j \\in Neighbor(i)} \\frac{x_i^{l-1} - x_j^{l-1}}{||x_i^{l-1} - x_j^{l-1}||_2} \\Phi (Wa^{h,l} x a_{ij}^{h,l})$$\n$$x_i^l = x_i^{l-1} + \\sum_{h=1,..., N_h} \\lambda_h \\Delta_i^h,$$\nwhere a is calculated in the feature update block (refer to Eq. A5), x denote the coordinate, $W^h$ \u2208 R1\u00d7dh. Note that only the coordinates of the ligand nodes are updated."}, {"title": "Affinity prediction and virtual screening", "content": "LigPose maintains a multi-task learning framework to simultaneously predict the structure, affinity score, and binding power. Thus, it also estimates the affinity of the protein-ligand complex as the auxiliary task, to be complemen- tary to the coordinate prediction. In this case, the pooled node features fi and edge features e are fed to a two-layer multi-layer perceptron, as calculated by,\n$$r= Norm(Concat(\\frac{1}{N_f} \\sum_{i=1,...,N_f} f_i, \\frac{1}{N_e} \\sum_{j=1,...,N_e} e_j)),$$\n$$y_{aff} = ReLU(W_{a,2}LeakyReLU(W_{a,1} r)),$$\nwhere yaff denote the prediction of the affinity, Wa,1 \u2208 R(df+de)\u00d7(df+de), Wa,2 \u2208 R1x(df+de), and Nf, Ne denotes the number of nodes and edges, respectively.\nTo screen the possible drugs, we provide a probability estimation (denoted as ybind) to indicate whether a SOM binds to a protein. The probability esti- mation also served as an additional task, similar to the affinity estimation task, with the ReLU layer replaced by the Sigmoid layer."}, {"title": "Training", "content": ""}, {"title": "Correlation-enhanced graph learning", "content": ""}, {"title": "Training with native complexes", "content": "Two losses are used in the training stage, i.e., the symmetric-aware loss for structure prediction (as introduced in 4.2), and the affinity loss for binding affinity prediction. The affinities are collected by PDBbind, including Ki, Kd, and IC50. Their negative log scale is used for training and evaluation. Then, the affinity loss is predicted for each protein-ligand pair, as defined by,\n$$L_{aff} = ||y_{pred} - y_{true}||_2^2.$$\nThen, the final loss of the network is defined as,\n$$L = \\gamma_1(\\frac{1}{N_l-2} \\sum_{i \\in 2,...,N_l-1} L_{sym} + L_{sym^i}) + \\gamma_2L_{aff},$$\nwhere Lsym is the proposed symmetric-aware loss calculated with coor- dinate output with the new N\u0131 blocks as defined in the main text (refer to Main Text Eq. 4) and the equivalent indexes used for Lsym are obtained from RDKit[81]. 1 and 2 are weight parameters to balance the losses.\nWe adopt a two-step training schedule. The first step limits the maximum number of nodes to 200, then the second step increases this number. The full training schedule is listed in Table A6.\n2200 random samples in the training set (when testing on the core set) are used to tune the hyper-parameters of the backbone of LigPoselight without training with unlabeled data. The search space is shown in Table A5. For LigPose, we simply increase the complexity of it on hidden size, number of cycling, and number of max nodes. The chosen architectures are shown in Table A4. We used the Adam optimizer with an exponentially decayed learning rate, with a decay rate of 0.99. Besides, all coordinates and affinities are rescaled to 1/10 of their original values for stable training. And these hyper-parameters are preserved in all downstream tasks."}, {"title": "Training with randomly paired proteins and ligands", "content": "Learning on the unlabeled data aims to encourage the model to learn graph features and biomolecular atom correlations simultaneously via self-supervised learning. The entire self-supervised learning loss function (denote as Lself) con- sists of two parts, i.e., the Masking-based Complex Modeling (MCM, denote as Lmask) and the Denoising-based Protein structure Reconstruction (DPR, denote as Lnoise). Also, to further generalize the model to very strict condi- tions of non-similar seen molecules, we pre-trained LigPose with docking-based structures (see Sec. A.9.5)."}, {"title": "Masking-based complex modeling (MCM)", "content": "In a molecular graph, the attribute of a node can be inferred from the contextual nodes and edges[100]. Therefore, we train the network by masking part of the nodes and edges in a protein-ligand pair with a specific token [MASK], i.e., the original features"}, {"title": "Downstream tasks", "content": ""}, {"title": "Affinity estimation", "content": "We perform post-training of 3 epochs for LigPose, with higher loss weight for affinity. The training schedule is listed in Table A7."}, {"title": "Virtual screening on CASF-2016", "content": "We train the model for virtual screening using Focal loss[101] with a weight 3 after the training of the structure prediction task. The model predicts the probability of a SOM to be a true ligand for a certain protein, as described in 4.4.3. In each training iteration, we balance the positive and negative pairs to be equally presented. Please note that all pairs containing data from the core set are not used for training. Also, Lsym and Laff are used if the native com- plex structures and affinities can be provided. The specific hyper-parameter setting for virtual screening is shown in Table A7, while the others are the same as the structure prediction task. The training schedule is listed in Table A7."}, {"title": "Structure prediction for Mpro-ligand complex", "content": "We train the LigPose with similar settings as described in Sec. A.3.1 with 11 structures of SARS-CoV and MERS-CoV collected from PDB. The training schedule is listed in Table A7."}, {"title": "Virtual screening on Mpro enzymatic activity", "content": "We perform five-fold cross-validation to the Mpro enzymatic activity data. The training process is similar to CASF-2016. The Mpro structures randomly paired with SOMs are used as the inputs. The training schedule is listed in Table A7. For ImageMol, starting with its pre-trained weights, we adopt a set of fine tuning parameters suggested in ImageMol work["}]}