{"title": "SLOTLIFTER: Slot-guided Feature Lifting for Learning Object-centric Radiance Fields", "authors": ["Yu Liu", "Baoxiong Jia", "Yixin Chen", "Siyuan Huang"], "abstract": "The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose SLOTLIFTER, a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering state-of-the-art performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in SLOTLIFTER, revealing key insights for potential future directions.", "sections": [{"title": "1 Introduction", "content": "The sense of objectness has been crucial to human cognition and generalization capabilities [33, 47]. Despite recent advances in visual perception [5, 15, 27, 40], achieving this generalization capability remains an unsolved challenge for existing models [26]. The pivotal role of object-centric understanding in human cognition necessitates models that can extract symbol-like object abstractions from complex visual signals, forming object-centric representations without supervision.\nRecent years have witnessed substantial progress in object-centric learning [23,30,36,44]. These methods aim to disentangle visual scenes into object-like entities for object-oriented reasoning and manipulation. Despite the remarkable progress made, existing approaches predominantly focus on 2D images. Since 2D images provide only partial views of the 3D physical world, object representations learned in the 2D domain are easily bound to 2D object attributes like colors [30], neglecting crucial information about object shape, geometry, and spatial relation-ships. Given the importance of these 3D attributes in representing the physical world, it is essential for models to form object abstractions in 3D environments to enhance understanding and interaction with the real world [16, 42]."}, {"title": "2 Related Work", "content": "Object-centric Learning Prior studies in object-centric learning [3, 4, 13, 19, 20, 23-25, 35, 36,64] have demonstrated proficiency in disentangling visual scenes into object-centric representations primarily on synthetic datasets, but they often struggle with handling complex real-world scenes. Notably, Slot-Attention [36] has fostered many powerful variants [5, 10, 17, 28, 30, 34, 43, 44, 52, 54] across various tasks and domains. However, these methods typically focus solely on learning object-centric representations from static images, thereby overlooking motion and 3D geometry information crucial for decomposing real-world complex scenes in an object-centric manner. Recognizing the potential benefits of motion information, [18, 32, 45] utilize video data to carve out object representations, demonstrating the effectiveness of the additional information provided beyond static images in the context of object-centric learning. Nonetheless, the use of 3D geometry information for object-centric learning has been largely left untouched. In this work, we pinpoint these crucial aspects by integrating advancements in image-based rendering with Slot-Attention, aiming to improve the acquisition of 3D object-centric representations within complex real-world environments.\nNovel-view Synthesis with NeRFs Recent advances in Neural Radiance Field (NeRF) methods [2,39,49] have shown notable success in novel-view synthesis and 3D scene reconstruction. However, a significant drawback of these methods is the scene-specific long training time needed for optimizing each scene. The demand for better time efficiency has led to the emergence of generalizable NeRF methods [6,8, 12,22,50,51,56,58,62]. These methods aim to synthesize novel views based on given images of scenes without per-scene optimization. For instance, PixelNeRF [58] and IBRNet [51] adopt volume rendering techniques, using features from nearby views to reconstruct novel views. MVSNeRF [6] constructs cost-volumes from nearby views for novel-view rendering. PointNeRF [56] leverages latent point clouds as anchors for radiance fields to improve both efficiency and performance. GNT [50] uses a transformer to integrate features from different views and demonstrates the powerful capability for generalizable novel-view synthesis. In contrast to these methods, SLOTLIFTER leverages an object-centric multi-view feature aggregation module and point-slot mapping module to more effectively encode 3D complex scenes for generalizable novel-view synthesis.\n3D Object-centric methods Previous methods [42,46,48,59] have attempted to extend Slot-Attention to 3D scenes for scene decomposition and novel-view synthesis. uORF [59], ObSuRF [48], sVORF [41], and uOCF [37] combine Slot-Attention"}, {"title": "3 SLOTLIFTER", "content": "In this section, we introduce our model, SLOTLIFTER, that combines object-centric learning modules with image-based rendering techniques. Our goal is to effectively learn scene reconstruction and decomposition by reconstructing input-view image(s). We present an overview of our SLOTLIFTER model in Fig. 1.", "subsections": [{"title": "3.1 Background", "content": "Object-centric learning via Slot-Attention Given $N$ input feature vectors $X \\in \\mathbb{R}^{N\\times Df}$, Slot-Attention [36] maps them to a set of $K$ output vectors (i.e.,"}, {"title": "3.2 Slot-guided Feature Lifting", "content": "Scene Encoding To render a novel target view $I_t$, we leverage Slot-Attention to encode scene representations from $L$ source view(s) ${I_l}_{l=1}^L$ ($L = 1$ for single-view input) and lift 2D features to 3D for approximating the latent feature field $F_e$. We start by extracting 2D feature maps ${F^{2D} \\in \\mathbb{R}^{H\\times W\\times D_f}}_{l=1}^L$ from each source view. Next, we follow Eq. (1) to obtain object-centric scene features $S = {s_1,\\dots,s_K} \\in \\mathbb{R}^{K\\times D_s}$ from these input 2D features via Slot-Attention. Inspired by image-based rendering methods, we consider constructing an additional 3D scene feature field by lifting 2D input-view features for capturing the fine-grained details in the input. Specifically, for target view $I_t$, we sample points $P \\in \\mathbb{R}^{N\\times 3}$ along"}, {"title": "3.3 Training", "content": "Objective For training, we utilize the mean squared error (MSE) between the rendered rays $C(r)$ and the ground truth colors $\\hat{C}(r)$ as our learning objective:\nLrecon = |C(r) \u2013 \\hat{C}(r)||2.\nRandom Masking Although incorporating feature lifting into 3D object-centric learning improves the utilization of 3D information, it also poses a significant problem. Since both lifted point features $F_p$ and slot features $S$ originate from 2D multi-view images, the model can converge to degenerate scenarios, relying solely on lifted features for rendering and ignoring the information in slots. We avoid this degenerate case by randomly masking the lifted features in the sampled points, using only positional embeddings $E_p$ for these points to enforce alignment between slots and 3D point grids. In implementation, we use a cosine annealing schedule on the masking ratio from 0.99 to 0 for 30K steps."}]}, {"title": "4 Experiment", "content": "We present experimental results of SLOTLIFTER on 4 synthetic and 4 complex real-world datasets, evaluating its capability in novel view synthesis and unsupervised scene decomposition. The experimental settings are as follows:\nDatasets For synthetic scenes, we evaluate SLOTLIFTER on 3 commonly used datasets CLEVR-567, Room-Chair, and Room-Diverse proposed by uORF [59]. We further select a more complex variant of Room-Diverse, Room-Texture [37], that provides synthetic rooms with real objects from ABO [11] for evaluating 3D object-centric learning. For complex real-world scenes, we use Kitchen-Shiny [37], Kitchen-Matte [37], ScanNet [14], and DTU MVS [29] to evaluate models' capability on novel-view synthesis and scene decomposition.", "subsections": [{"title": "4.1 Object-centric Learning in Synthetic Scenes", "content": "Setup To perform a fair comparison between SLOTLIFTER and existing methods, we follow the setup of uORF [59] and use only one source view as input to render the other novel views. As we only use a single source view, we modify the multi-view feature aggregation to $F_p = MLP(F_{lift}) + E_p$ as discussed in Sec. 3.2. We train our model using the Lion [7] optimizer with a learning rate of $5\\times10^{-5}$ for 250k iterations. We use a batch size of 4 and sample 1024 rays for each scene.\nBaselines We compare SLOTLIFTER with previous state-of-the-art 3D object-centric methods including uORF [59], COLF [46], and sVORF [41]. We also report the results of the improved uORF (BO-uORF) introduced by Jia et al. [30] as a competitive baseline in evaluating the results on these datasets.\nResults and Analysis We evaluate the performance of SLOTLIFTER for unsupervised scene decomposition and present our quantitative results in Tab. 1 and Tab. 2. SLOTLIFTER outperforms existing 3D object-centric learning methods, achieving the best performance across all datasets. We also visualize qualitative results for segmentation in Fig. 2 and Fig. 3. As"}, {"title": "4.2 Object-centric Learning in Real-world Scenes", "content": "Setup To show the effectiveness of SLOTLIFTER on real-world complex scenes, we evaluate SLOTLIFTER on Kitchen-Shiny and Kitchen-Matte following uOCF [37]. We use the same train/test split for these two datasets with single-view input following settings in uOCF. Unlike uOCF which requires training with 2 stages to learn object priors, we train SLOTLIFTER with reconstruction loss in 1 stage."}, {"title": "4.3 Ablative Study", "content": "To investigate the effectiveness of our designs in SLOTLIFTER, including scene encoding, random masking, slot-based density, and the number of slots and source views, we conduct ablative studies on both synthetic (Room-Diverse) and real-world (ScanNet) scenes. We also investigate the effect of the number of sampled rays and leave the results in Tab. A.4 in the supplementary.\nScene Encoding We consider removing the feature lifting operation and initializing point features solely with positional embeddings, i.e., $F_p = E_p$. As shown in Tab. 8 and Fig. 5, the performance of both novel-view synthesis and scene decomposition on Room-Diverse drops significantly without lifted multi-view features, especially for LPIPS and FG-ARI. In fact, it is hard to establish the mapping between slots and 3D points via only positional information. This problem is more severe in complex real-world scenes (e.g., Scannet), where SLOTLIFTER struggles in rendering novel views without feature lifting, achieving only a PSNR of 11.6. This issue is also shared by uORF and OSRT as presented in Sec. 4.2 and demonstrates the significance of the feature lifting design.\nRandom Masking As shown in Tab. 8 and Fig. 5, abandoning the random masking scheme described in Sec. 3.3 slightly improves the rendering performance (LPIPS, SSIM, PSNR) but significantly decreases the scene decomposition capability of SLOTLIFTER, especially for FG-ARI. We also find that the model sometimes converges to the degenerate scenario as discussed in Sec. 3.3 without the random masking scheme, leading to a collapse in scene decomposition (i.e., uniform segmentation predictions) with ARI scores lower than 40. This affirms our supposition that, without random masking, the model is likely to degenerate and rely solely on lifted features for rendering, thereby ignoring the information in slots. We also explore how the masking ratio decay scheduling influences performance. As shown in Tab. 7, increasing decay steps slightly harms rendering performance and significantly improves segmentation performance after a certain amount of steps (~10K steps). After the number of decay steps exceeds 30K, continuing to increase the number of steps will only bring marginal improvement.\nSlot-based Density As shown in Tab. 8, compared with using an additional MLP layer for predicting the density value, using slot-based density slightly improves the quality of novel-view synthesis on ScanNet and significantly improves the performance of scene decomposition on both datasets, especially for ScanNet. We attribute this effectiveness to the fact that the slot-based density is more involved in point-slot interactions. This leads to more information propagation"}]}, {"title": "5 Conclusion", "content": "We present SLOTLIFTER, an object-centric radiance field model for unsupervised 3D object-centric representation learning. Our SLOTLIFTER employs slot-guided feature lifting to improve the interaction between lifted input view features and learned slots during decoding. SLOTLIFTER achieves state-of-the-art performance with large improvements on four challenging synthetic and four complex real-world datasets for scene decomposition and novel-view synthesis and uses much less training time, demonstrating its effectiveness and efficiency. Furthermore, SLOTLIFTER demonstrates superior performance for novel-view synthesis on real-world datasets, underscoring its potential to narrow the gap to real-world scenes."}, {"title": "A Implementation Details", "content": null, "subsections": [{"title": "A.1 SLOTLIFTER", "content": "Architecture Design\nScene Encoding We employ a U-net-like encoder E\u015f with ResNet34 [27] to extract 2D image features, similar to IBRNet [51]. This architecture truncates after layer3 as the encoder and adds two up-sampling layers with convolutions and skip-connections as the decoder. Instead of extracting two sets of feature maps for coarse and fine networks as IBRNet, we extract a shared feature map. In addition, we concatenate multi-view images with their corresponding ray directions and camera positions to provide more spatial information, enabling slots to learn 3D information from 2D multi-view features via Slot-Attention. Given extracted feature maps, we obtain slots via Slot-Attention and 3D point features via feature lifting described in Sec. 3.2. We add the point positional embedding $E_p$ in Eq. (3), which considers point location $p$ and ray direction $d$ simultaneously by:\nEp = MLP(Concat([PosEmb(p), PosEmb(d)])),\nwhere PosEmb is a Fourier transformation with a frequency of 10 while MLP is used to fuse point location and ray direction information and project positional embedding to the same dimension as point features.\nPoint-slot Joint Decoding Our point-slot joint decoding contains an allocation transformer and an attention-based point-slot mapping module. The allocation transformer consists of four transformer layers, and each layer includes a cross-attention layer, a 1D convolution layer, and a self-attention layer. We use 1D convolution and self-attention to model the relationship among points along a ray. The design is based on the insight that spatially adjacent points are more likely to be associated with the same slot. Additionally, We use the weighted sum of attention weights to estimate the density value in Eq. (4). As this design may restrict the scale of density by the attention weights between slots and point features, the density obtained from Eq. (4) is multiplied by a learnable parameter so to rescale it.\nHyperparameters and Training Details We train our SLOTLIFTER by sampling 1024 rays for each scene with a learning rate of 5 \u00d7 10-5, a linear learning rate warm-up of 10000 steps, and an exponentially decaying schedule."}, {"title": "A.2 Baselines", "content": "uORF and BO-uORF The experimental results of uORF [59] on CLEVR-567, Room-Chair, and Room-Diverse are taken from their paper. We trained the BO-uORF model on CLEVR-567, Room-Chair, Room-Diverse, and ScanNet using the official implementation of uORF and BO-QSA. As (BO-)uORF only accepts single source view input, we selected the closest view to the target view as the source view for it. Unfortunately, due to design limitations, such as model architecture, adversarial loss, perceptual loss, etc., we could not train the BO-uORF model at the resolution of 640\u00d7480. Therefore, we had to use a resolution of 128x128 following their original settings. We use 8 slots for uORF as same as our method.\nOSRT We trained OSRT [42] on CLEVR-567, Room-Chair, Room-Diverse, and ScanNet using the implementation recommended by the authors on the project website of OSRT. We observed that OSRT's performance of scene decomposition is highly sensitive to the batch size used during training, which is also mentioned"}, {"title": "B Additional Discussions", "content": null, "subsections": [{"title": "B.1 Potential Improvements", "content": "Although SLOTLIFTER exhibits superior performance in novel-view synthesis and scene decomposition compared to state-of-the-art 3D object-centric learning methods, its scene decomposition performance still falls short under real-world settings. This is particularly noteworthy considering the recent success of 2D object-centric models on real-world images (see in Tab. A.5). We attribute this undesired effect to the unconstrained point-slot mapping process. As elaborated in Sec. 3.2, the slots are mapped to the 3D points which are later projected to the target view image. With only reconstruction loss, the information in the target image can be backpropagated to both slots and lifted point features. This adds no direct guidance or constraints on slot learning and can easily make the learned slots attend to features that best render the scene instead of decomposing it."}, {"title": "B.2 Further Discussions about Previous Methods", "content": "(BO-)uORF As shown in Tab. 5 and Fig. 4, BO-uORF failed to render novel views and decompose scenes in complex real-world scenes, achieving only a PSNR of 12.72 and a NV-FG-ARI of 0.0. Moreover, to demonstrate that the failure of BO-uORF is not due to lower resolution, we trained our SLOTLIFTER with a resolution of 128x128 and achieved a PSNR of 29.31.\nOSRT We present the quantitative results of OSRT in Tab. A.6 and visualize qualitative results in Fig. A.1. The performance of OSRT is hindered by its requirements for large amounts of data and computational demands, especially on CLEVR-567 which only has 1000 training scenes. We observed that the OSRT tended to overfit training scenes ( Fig. A.1 ), making it difficult to generalize to unseen scenes. We attempted a larger batch size of 256 and trained OSRT for more iterations (750K), but the overfitting issue persisted. We also visualize"}]}, {"title": "C Limitations and Future Work", "content": "Inference efficiency While our SLOTLIFTER has significantly improved training efficiency compared to other 3D object-centric models, its inference efficiency is not satisfactory compared with light field methods (e.g., COLF and OSRT). The primary reason for this issue is that NeRF representations require the sampling of a large number of points with expensive computations, most of which are wasted on irrelevant vacant points. Although light field methods, such as OSRT, are very efficient for inference, they lack the use of 3D information and require a lot of data and computation commands to overcome the overfitting problem. Some recent works, such as those based on point clouds [56], surfels [22], and Gaussian Splatting [31,63], have demonstrated high efficiency for inference and good utilization of 3D information, which could be integrated into future work to improve the inference efficiency.\nDetails of Complex Object As depicted in Fig. A.5 and Fig. A.6, the SLOTLIFTER encounters challenges in accurately rendering and segmenting chair legs from different angles, particularly when dealing with real chairs in Room-Texture. A primary issue contributing to this difficulty lies in ray sampling. NeRF-based techniques typically employ ray sampling during the training process to reduce computational load. For example, in our case, we sample 1024"}, {"title": "D Additional Visualizations", "content": "We provide more qualitative results of our SLOTLIFTER in the following pages."}]}]}