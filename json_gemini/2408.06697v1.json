{"title": "SLOTLIFTER: Slot-guided Feature Lifting for Learning Object-centric Radiance Fields", "authors": ["Yu Liu", "Baoxiong Jia", "Yixin Chen", "Siyuan Huang"], "abstract": "The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose SLOTLIFTER, a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering state-of-the-art performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in SLOTLIFTER, revealing key insights for potential future directions.", "sections": [{"title": "1 Introduction", "content": "The sense of objectness has been crucial to human cognition and generalization capabilities [33, 47]. Despite recent advances in visual perception [5, 15, 27, 40], achieving this generalization capability remains an unsolved challenge for existing models [26]. The pivotal role of object-centric understanding in human cognition necessitates models that can extract symbol-like object abstractions from complex visual signals, forming object-centric representations without supervision.\nRecent years have witnessed substantial progress in object-centric learning [23,30,36,44]. These methods aim to disentangle visual scenes into object-like entities for object-oriented reasoning and manipulation. Despite the remarkable progress made, existing approaches predominantly focus on 2D images. Since 2D images provide only partial views of the 3D physical world, object representations learned in the 2D domain are easily bound to 2D object attributes like colors [30], neglecting crucial information about object shape, geometry, and spatial relationships. Given the importance of these 3D attributes in representing the physical world, it is essential for models to form object abstractions in 3D environments to enhance understanding and interaction with the real world [16, 42].\nTo fulfill this goal, various attempts have been made to combine object-centric methods such as Slot-Attention [36] with 3D representations. Among them, multi-view image representations of 3D scenes [46,48,59] show competitive results on synthetic datasets given their effectiveness in preserving detailed object information. Nonetheless, translating the success of these methods from synthetic data to real-world scenarios has been proven to be non-trivial [43]. Specifically, aggregating information from multi-view real images and drawing correspondences between them naturally requires more intricate model designs. Meanwhile, decoding from object-centric representations to 3D (e.g., novel views) places higher demands on the learned representations (i.e., slots) as it now needs to infer about the 3D scene from a series of calibrated partial view projections. Recently, OSRT [42] scales up the dimensions of slots and reconstructs scenes with a Transformer-based encoder-decoder architecture, demonstrating powerful decomposition and reconstruction ability in complex 3D scenes. However, its success is built at the cost of inadmissible data and computation demands (64 TPUv2 chips for 7 days on 1M scenes). This urges the need for methods to effectively align information from calibrated multi-view images and reconstruct 3D scenes from the compressed object-centric representations.\nIn this work, we present SLOTLIFTER, a novel approach to learning object-centric representations in 3D scenes, inspired by recent advances in image-based rendering methods [6,12,22,50,51,56,58,62]. In contrast to previous object-centric methods that focus solely on decoding information from slots, our method leverages lifted 2D input-view feature(s) to initialize 3D point features, which interact with the learned slot representations via a cross-attention-based transformer for predicting volume rendering parameters. This design enhances the granularity of details for novel-view synthesis while providing more explicit guidance for slot learning. Additionally, with no auxiliary losses needed, SLOTLIFTER relies only on the reconstruction loss and naturally requires less sampling overheads during training compared with existing 3D object-centric learning models like uORF and OSRT. This results in significantly fewer computational resources needed (~5x faster) to achieve desirable outcomes. Through comprehensive experiments on four challenging synthetic and four complex real-world datasets, we observe consistent and significant performance improvement of SLOTLIFTER over existing 3D object-centric models on both scene decomposition (~10+ ARI) and novel-view synthesis (~2+ PSNR). We further show the effectiveness of each module through extensive ablative analyses and discussions, offering new insights into developing object-centric learning techniques for complex 3D scenes. In summary, our main contributions are as follows:\n1. We propose SLOTLIFTER, a novel model for unsupervised object-centric learning in 3D scenes that effectively aggregates multi-view features for object-centric decoding via an innovative slot-guided feature lifting design.\n2. We comprehensively evaluate SLOTLIFTER across four challenging synthetic and four real-world benchmarks. Our results consistently show that SLOTLIFTER significantly outperforms existing methods in both scene decomposition and novel-view synthesis, achieving state-of-the-art performance.\n3. We conduct extensive ablative analyses demonstrating SLOTLIFTER'S potential in object-centric learning and image-based rendering, especially given its superior performance on established complex real-world datasets (e.g., ScanNet and DTU) against state-of-the-art image-based rendering methods. We anticipate that our findings will stimulate further advancements in overcoming current limitations of 3D object-centric models."}, {"title": "2 Related Work", "content": "Object-centric Learning Prior studies in object-centric learning [3, 4, 13, 19, 20, 23-25, 35, 36,64] have demonstrated proficiency in disentangling visual scenes into object-centric representations primarily on synthetic datasets, but they often struggle with handling complex real-world scenes. Notably, Slot-Attention [36] has fostered many powerful variants [5, 10, 17, 28, 30, 34, 43, 44, 52, 54] across various tasks and domains. However, these methods typically focus solely on learning object-centric representations from static images, thereby overlooking motion and 3D geometry information crucial for decomposing real-world complex scenes in an object-centric manner. Recognizing the potential benefits of motion information, [18, 32, 45] utilize video data to carve out object representations, demonstrating the effectiveness of the additional information provided beyond static images in the context of object-centric learning. Nonetheless, the use of 3D geometry information for object-centric learning has been largely left untouched. In this work, we pinpoint these crucial aspects by integrating advancements in image-based rendering with Slot-Attention, aiming to improve the acquisition of 3D object-centric representations within complex real-world environments.\nNovel-view Synthesis with NeRFs Recent advances in Neural Radiance Field (NeRF) methods [2,39,49] have shown notable success in novel-view synthesis and 3D scene reconstruction. However, a significant drawback of these methods is the scene-specific long training time needed for optimizing each scene. The demand for better time efficiency has led to the emergence of generalizable NeRF methods [6,8, 12,22,50,51,56,58,62]. These methods aim to synthesize novel views based on given images of scenes without per-scene optimization. For instance, PixelNeRF [58] and IBRNet [51] adopt volume rendering techniques, using features from nearby views to reconstruct novel views. MVSNeRF [6] constructs cost-volumes from nearby views for novel-view rendering. PointNeRF [56] leverages latent point clouds as anchors for radiance fields to improve both efficiency and performance. GNT [50] uses a transformer to integrate features from different views and demonstrates the powerful capability for generalizable novel-view synthesis. In contrast to these methods, SLOTLIFTER leverages an object-centric multi-view feature aggregation module and point-slot mapping module to more effectively encode 3D complex scenes for generalizable novel-view synthesis.\n3D Object-centric methods Previous methods [42,46,48,59] have attempted to extend Slot-Attention to 3D scenes for scene decomposition and novel-view synthesis. uORF [59], ObSuRF [48], sVORF [41], and uOCF [37] combine Slot-Attention"}, {"title": "3 SLOTLIFTER", "content": "In this section, we introduce our model, SLOTLIFTER, that combines object-centric learning modules with image-based rendering techniques. Our goal is to effectively learn scene reconstruction and decomposition by reconstructing input-view image(s). We present an overview of our SLOTLIFTER model in Fig. 1.\n3.1 Background\nObject-centric learning via Slot-Attention Given N input feature vectors \\(X \\in \\mathbb{R}^{N \\times D_f}\\), Slot-Attention [36] maps them to a set of K output vectors (i.e., slots) \\(S \\in \\mathbb{R}^{K \\times D_s}\\) via an iterative attention mechanism. The K slots compete to explain the input features X by computing the attention matrix A between S and X. The attention matrix is then used to aggregate feature vectors X using a weighted mean. These aggregated features are embedded into slots S by iteratively updating as follows:\n\\(A = \\text{softmax}(\\frac{k(X)q(\\text{\\v{S}})^T}{\\sqrt{D}})\\)\n\\(S = U_\\theta(\\text{\\v{S}}, W \\text{\\v{T}}v(X)), \\text{ where } W_{i,j} = \\frac{A_{i,j}}{\\sum_{m=1}^{N} A_{m,j}}\\)\n\\(q(\\cdot), k(\\cdot), v(\\cdot)\\) are linear projections, \\(\\text{\\v{S}}\\) denotes random initialized slots and \\(U_\\theta(\\cdot)\\) represents the iterative update function often implemented with GRU [9], LayerNorm [1] and a residual MLP. As pointed out by Jia et al. [30], this iterative update process could be susceptible to instability when propagating gradients back into the iterative process. They therefore proposed a bi-level method, dubbed BO-QSA, to improve the optimization within Slot-Attention with learnable slot initialization instead of random sampled ones.\nNeural Radiance Fields Given rays {r} of a camera view, NeRF samples points along each ray and represent 3D scenes with a feature field \\(F_\\theta : (x, d) \\rightarrow (c, \\sigma)\\) mapping the 3D location x and the view direction d to color c and volume density \\(\\sigma\\), and then renders the color of each ray via volume rendering [38]:\n\\(\\hat{C}(r) = \\sum_{i=1}^{N} T_i [1 - \\exp(-\\sigma_i \\delta_i)] C_i,\\)\nwhere \\(T_i = \\exp(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j)\\) and \\(\\delta_i\\) is the distance between adjacent volumes along a ray. While NeRF achieves impressive novel-view synthesis quality, it adds stringent demands on model training given the number of points needed for approximating \\(\\hat{C}(r)\\) in Eq. (2). It also exhibits no generalization capabilities as each scene is optimized individually without shared prior knowledge.\n3.2 Slot-guided Feature Lifting\nScene Encoding To render a novel target view \\(I_t\\), we leverage Slot-Attention to encode scene representations from L source view(s) \\(\\{I_l\\}_{l=1}^L\\) (L = 1 for single-view input) and lift 2D features to 3D for approximating the latent feature field \\(F_\\theta\\). We start by extracting 2D feature maps \\(\\{F^{2D} \\in \\mathbb{R}^{H \\times W \\times D_f}\\}_{l=1}^L\\) from each source view. Next, we follow Eq. (1) to obtain object-centric scene features \\(S = \\{s_1, \\ldots, s_K\\} \\in \\mathbb{R}^{K \\times D_s}\\) from these input 2D features via Slot-Attention. Inspired by image-based rendering methods, we consider constructing an additional 3D scene feature field by lifting 2D input-view features for capturing the fine-grained details in the input. Specifically, for target view \\(I_t\\), we sample points \\(P \\in \\mathbb{R}^{N \\times 3}\\) along each ray r and project each 3D point \\(p = (x, y, z)\\) onto the image coordinates \\(\\pi(p) = (x', y')\\) to obtain its set of corresponding 2D features \\(F_{\\text{lift}}(p)\\) by:\n\\(F_{\\text{lift}} (p) = [F_1^{2D} [\\pi(p)], \\cdots, F_L^{2D}[\\pi(p)]] .\\)\nWithout adding further ambiguity to the notations, we use \\(F_{\\text{lift}} \\in \\mathbb{R}^{N \\times L \\times D_f}\\) to represent the feature field obtained for all points in P. After obtaining the lifted point features \\(F_{\\text{lift}}\\), we pool the multi-view features to obtain 3D point features:\n\\(F_p = MLP([\\text{Mean}(F_{\\text{lift}}), \\text{Var}(F_{\\text{lift}})]) + E_p,\\)\nwhere \\(E_p \\in \\mathbb{R}^{N \\times D_p}\\) are positional embeddings for preserving the spatial information of 3D points. Notably, for single-view input, we ignore the variance term and let \\(F_p = MLP(F_{\\text{lift}}) + E_p\\). This feature serves a similar role as \\(F_\\theta\\) discussed in Eq. (2), providing fine-grained 3D features with spatial location considered.\nPoint-slot Mappping After scene encoding, given the slots S and the point features \\(F_p\\), we design a point-slot joint decoding process to leverage both point and slot features for rendering. First, we calculate the point-slot mapping \\(W_p\\), identifying the points that a slot \\(s_i \\in S\\) contributes to. Specifically, we use a cross-attention-based allocation transformer, leveraging point features \\(F_p\\) as queries and slot representations S as keys and values to allocate slots to 3D points. As some points map to vacant areas in the 3D space, we add an additional learnable empty slot \\(\\text{\\o}\\) for these vacant points to query from. This process could be summarized as:\n\\(S' = \\{S_{\\o}, s_1, \\ldots, s_K\\}, F_s = \\text{CrossAttn}(Q = F_p, KV = S').\\)\nAfter this process, the 3D point features \\(F_s\\) contain information queried from object-centric slot representations. Finally, we obtain the point-slot mapping and the slot-aggregated point feature \\(F_\\theta\\) via an attention layer following:\n\\(F_{\\theta} = W_p S', \\text{ where } W_p = \\text{softmax}(\\frac{q(F_s) \\cdot k(S')^T}{\\sqrt{D}}).\\)\nWe use \\(q(\\cdot), k(\\cdot)\\) to denote linear projections, \\(W_p \\in [\\mathbb{R}^{N \\times (K+1)}\\) for the mapping weights from slots to points, D for the latent feature dimension. In essence, this process aims to obtain decodable 3D representations from learned slots. We can find the corresponding slot mapping (i.e., contribution) weight from \\(W_p\\) for each 3D point \\(p_i\\), thereby predicting its slot assignment for scene decomposition.\nSlot-based Density For notation purposes, we use \\(A_p = q(F_p) \\cdot k(S')^T\\) throughout the subsequent texts for simplicity. To provide more direct guidance to slots, we use the attention weights \\(A_p\\) from the mapping module to estimate the density value following [59]:\n\\(\\sigma_i = \\text{sum}(W_p^{i,1:K+1} \\odot \\text{ReLU}(A_p^{i,1:K+1})),\\)\nwhere i denotes i-th point, \\(\\odot\\) denotes Hadamard production, and \\(A_p^{i,1:K+1}\\) denotes the attention weights of the last K slots, ignoring the first empty slot in S'. We add a ReLU layer over \\(A_p\\) to suppress the contribution of slots less related to a specific point \\(F_p\\) in density prediction. Finally, we add \\(F_s\\) with the positional embedding \\(E_p\\) and pass it into an MLP for predicting colors c. Similarly, given the 3D point-slot mapping weight \\(W_p \\in \\mathbb{R}^K\\) of each point, SLOTLIFTER is able to render 2D segmentation masks M using the same rendering scheme:\n\\(c = MLP(F_s + E_p), C(r) = \\sum_{i=1}^{N} T_i [1 - \\exp(-\\sigma_i \\delta_i)] C_i,\\)\n\\(M(r) = \\sum_{i=1}^{N} T_i [1 - \\exp(-\\sigma_i \\delta_i)] W_i,\\)\nwhere \\(T_i = \\exp(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j)\\) and \\(\\delta_i\\) is the distance between adjacent volumes along a ray following Eq. (2).\n3.3 Training\nObjective For training, we utilize the mean squared error (MSE) between the rendered rays C(r) and the ground truth colors \\(\\hat{C}(r)\\) as our learning objective:\n\\(L_{\\text{recon}} = ||C(r) - \\hat{C}(r)||^2.\\)\nRandom Masking Although incorporating feature lifting into 3D object-centric learning improves the utilization of 3D information, it also poses a significant problem. Since both lifted point features \\(F_p\\) and slot features S originate from 2D multi-view images, the model can converge to degenerate scenarios, relying solely on lifted features for rendering and ignoring the information in slots. We avoid this degenerate case by randomly masking the lifted features in the sampled points, using only positional embeddings \\(E_p\\) for these points to enforce alignment between slots and 3D point grids. In implementation, we use a cosine annealing schedule on the masking ratio from 0.99 to 0 for 30K steps."}, {"title": "4 Experiment", "content": "We present experimental results of SLOTLIFTER on 4 synthetic and 4 complex real-world datasets, evaluating its capability in novel view synthesis and unsupervised scene decomposition. The experimental settings are as follows:\nDatasets For synthetic scenes, we evaluate SLOTLIFTER on 3 commonly used datasets CLEVR-567, Room-Chair, and Room-Diverse proposed by uORF [59]. We further select a more complex variant of Room-Diverse, Room-Texture [37], that provides synthetic rooms with real objects from ABO [11] for evaluating 3D object-centric learning. For complex real-world scenes, we use Kitchen-Shiny [37], Kitchen-Matte [37], ScanNet [14], and DTU MVS [29] to evaluate models' capability on novel-view synthesis and scene decomposition."}]}