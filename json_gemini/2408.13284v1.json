{"title": "From Radiologist Report to Image Label: Assessing Latent Dirichlet Allocation in Training Neural Networks for Orthopedic Radiograph Classification", "authors": ["Jakub Olczak", "Max Gordon"], "abstract": "Background: Radiography (X-rays) is the dominant modality in orthopedics, and improving the interpretation of radiographs is clinically relevant. Machine learning (ML) has revolutionized data analysis and has been applied to medicine, with some success, in the form of natural language processing (NLP) and artificial neural networks (ANN). Latent Dirichlet allocation (LDA) is an NLP method that automatically categorizes documents into topics. Successfully applying ML to orthopedic radiography could enable the creation of computer-aided decision systems for use in the clinic. We studied how an automated ML pipeline could classify orthopedic trauma radiographs from radiologist reports.\nMethods: Wrist and ankle radiographs from Danderyd Hospital in Sweden taken between 2002 and 2015, with radiologist reports. LDA was used to create image labels for radiographs from the radiologist reports. Radiographs and labels were used to train an image recognition ANN. The ANN outcomes were manually reviewed to get an accurate estimate of the method's utility and accuracy.\nResults: Image labels generated via LDA could successfully train the ANN. The ANN reached an accuracy between 91% and 60% compared to a gold standard, depending on the label.\nConclusions: We found that LDA was unsuited to label orthopedic radiographs from reports with high accuracy. However, despite this, the ANN could learn to detect some features in radiographs with high accuracy. The study also illustrates how ML and ANN can be applied to medical research.", "sections": [{"title": "Introduction", "content": "Radiography (colloquially called \u201cX-ray images\u201d) is at the core of orthopedics. Treatment decisions are mainly influenced by the fracture appearance on the radiograph and less by the patient's background 1,2. Plain radiography's ready availability, speed, low cost, and low radiation mean it remains central even though computer tomography (CT) and magnetic resonance imaging (MRI) have become more commonplace.\nThe human capacity for details limits the information that can be extracted from images. The simplicity needed for two humans to give the same interpretation of an image, also known as inter-observer reliability, forces further simplifications and loss of detail. For example, wrist fractures are among the most common fractures, and distal radius fractures are especially prevalent, with 31 cases per 10,000 person-years 3. Though it is clear that if there is no displacement of the fracture, there is no need for intervention 4,5, in other situations, there is still debate about the optimal treatment 6,7. Some measures can predict later displacement, in which case early intervention is desirable. Extracting all available information from the image, like tilt, shortening of the bone, or the level of fragment comminution 8,9, is, therefore, vital for everyday orthopedics. However, these are complex factors to measure, and inter-observer reliability becomes an issue 10,11.\nA computer-aided decision-making system (CAD) could alleviate these problems. A computer program that analyses radiographic images, finds relevant pathologies, takes desired measurements, and, based on that, makes an accurate prognosis would be a valuable tool in the clinic. It could also facilitate advances in clinical treatment by allowing for new avenues of research and making radiology more available to large-scale data mining.\nA fundamental part of developing such a CAD is being able to analyze and interpret radiographic images. This usually entails using extensive collections of studies with explanatory labels to train a computer model to detect image patterns 12. Today's annotations are in plain text reports written by radiologists at hospitals and clinics worldwide. They summarize relevant findings into descriptive reports for use by other doctors. Turning written reports into labels for a CAD can be vital. Due to the large number of labeled images needed, automating this process would be crucial. A tool for addressing this problem is machine learning (ML), broadly described as computing procedures that learn patterns, rules, and models by learning from examples rather than explicitly being told what to look for 13. ML has emerged as a widely used tool in many areas 14\u201322."}, {"title": "Natural Language Processing and Latent Dirichlet Allocation", "content": "Textual medical data analysis (journals, reports, archives) is a reality of everyday clinical work and research. It usually entails spending hours manually reading and classifying texts to extract interesting information. The subfield of automated language classification, spoken or written, and interpretation is called natural language processing (NLP), where \u201cnatural\u201d implies human 23. NLP can significantly benefit medical research 24,25.\nA collection of documents (a corpus) can be anything from magazines and social media posts to scientific journals or patient records. Texts could, for example, be broad-scoped press articles from the New York Times on topics such as politics, medicine, cooking, art, or a mix of these, or a medical journal on orthopedics. Similarly, orthopedic radiologist reports could be divided into subtopics such as fractures, wrist fractures, and wrist fractures involving the styloid.\nLatent Dirichlet Allocation (LDA) is a NLP algorithm that automatically tries to identify document topics 23. It relies on the idea that topics are associated with a set of words, and every topic will have a specific combination of words that appear more commonly than other topics. LDA assumes topics can be identified by their unique combinations of words without considering word order, 26 also called a \u201cbag-of-words\u201d model. 23 LDA first appeared in Blei et al. 2003 27 and is often implemented based on Griffith and Steyvers 2004 28. Our usage of LDA on radiologist reports to automatically generate topics and labels was inspired by Shin et al. 29. They used a mix of radiographs, CT, and MRI image slices from labeled via radiologist reports using LDA to obtain general results. The results were interesting and broadly illustrative, detecting a wide range of pathologies but lacked accuracy or clinical relevance. They also used online databases of medical language 30,31 to interpret topics and assign labels. Such databases of medical terms and their relationships are not publicly available in Swedish. To our knowledge, no studies have attempted large-scale machine learning for orthopedic trauma radiographs and radiologist reports in Swedish.\nLDA assumes a hidden (latent) structure to the documents of the corpus and the topics and is founded on a specific idea of how documents are created. The fundamental building blocks of documents are words. LDA assumes a document is about one or more topics, e.g., a central topic and several subtopics. The words in a document make up the vocabulary for the document. Topics are defined by their specific vocabulary (words and the combinations of words) but not their order (syntax). The idea behind LDA is to reverse the document generation process to find topics. Rather than combining words from a topic into a document, it divides documents into words and examines their combination (the vocabulary) to associate it with topics. Documents"}, {"title": "", "content": "with a similar vocabulary are assumed to be about the same topic. LDA tries to find these combinations of words using statistical simulations. It randomly distributes the available words over topics and tests how well the resulting word combinations fit the data. It repeatedly repeats this process, encouraging models that best fit the previous step. Over time, topics that fit the data well evolve. 26\nMore theoretically, a generative model is a statistical model that generates observable data values from observations. LDA is a generative model that explains observations (vocabularies) as latent (hidden) structures and topics (the observable data values). The Dirichlet distribution is a standard probability distribution for modeling belief (what words we believe the topics are made up of) in Bayesian statistics. LDA postulates that the characteristics of the documents are Dirichlet distributed and uses the distribution to draw random words from the corpus and assigns them to topics and topics to documents. LDA tries to solve the joint probability distribution described by equation [1],\np(\u03b1, \u03b2) = p(a)p(\u03b8)p(\u03b2)p(z, \u03c6)   [1]\nThe parts of the equation [1] are as follows\np(\u03b1, \u03b2): the probability model when combining a and \u03b2 (sampling parameters for the Dirichlet distribution). a and \u03b2 are sampling parameters that determine how words and topics are drawn randomly during the modeling process that generates the topics.\na: estimates to what extent documents are made up of more (large a) or fewer (small a) topics. It determines how topics are drawn and distributed over documents.\n\u03b2: estimates to what extent words belong to many different topics (mixed between topics) or fewer topics. It determines how words are drawn and distributed over topics.\n0: topics and how they are distributed over the documents.\n\u03c6: words and how they are distributed over topics.\nz: words randomly drawn from the probability distributions of p."}, {"title": "Artificial Neural Networks for Image Analysis", "content": "Artificial neural networks (ANN) have become widespread for image analysis. It has revolutionized the field of image recognition and enabled computers to perform at superhuman capacity for specific image recognition tasks 32. Automatic detection and classification of pathologies in radiographs and CAD have a wide range of potential uses. Automatic detection could be a screening tool in the emergency room or facilitate radiologist work. It could also be helpful when radiologists are rare, for example, in triage during natural disasters or in parts of the world where doctors are generally scarce. If the method works, it could be extended to other fields, such as fracture outcome prediction or mammography analysis.\nWe aimed to study how an automated ML pipeline could classify orthopedic trauma radiographs from radiologist reports. The hypotheses were that 1) it is possible to automatically find clinically relevant image features in radiologist reports using NLP, and 2) these labels could be used to train an image recognition ANN. The detection accuracy of the ANN, trained with the labels created using NLP, was the primary outcome."}, {"title": "Methods and Materials", "content": "This was a cross-sectional study of patients examined at Danderyd Hospital, Stockholm, Sweden, from 2002 to 2015. All patients with a radiologist report about wrist or ankle radiography examinations available in the Radiology Information System (RIS) were eligible for inclusion. Examinations not containing radiographic images and radiologist reports were excluded, as were examinations where reports were less than six characters, as they could not contain any diagnostic information."}, {"title": "Experiment Design", "content": "The study consisted of three experiments: 1) calibrate LDA to our data, 3) create LDA image labels and implement the ANN, and 3) evaluate the true accuracy of the ANN and the feasibility of using LDA labels by comparing ANN outcomes to a gold standard (Table 2, and Figure 1.)\nThis study used an image recognition ANN in the form of a convolutional neural network (CNN)."}, {"title": "Experiment 1 \u2013 Topic Modeling Parameter Study", "content": "Aim: Study how to select modeling parameters to get a good topic model for the radiologist reports.\nThe LDA parameter a estimates how many topics a document is assumed to deal with and will vary depending on the corpus. a is usually set to a = 50/number of topics, where the number of topics is either a convenient number or estimated by some mathematical means. It will depend on the corpus. This selection of a is only based on an assumed variability in topics and does not consider the nature or number of the documents being analyzed. We stipulated that the nature of the documents matters. Our corpus was a collection of radiologist reports. The radiologist reports were generally concise and consisted of short, independent sentences describing a distinct image feature (e.g., fracture, dislocation, arthrosis, etc.) The vocabulary should be limited and highly domain-dependent, as should the number of topics discussed for a specific type of examination. The number of topics per report should also be limited as they only discussed findings in the image.\nStudy subjects: We chose 25,000 random wrist examination radiologist reports (Table 1), which were further divided into individual sentences.\nWe redefined a as in equation [2],\na = scaling factor \u00b7 50/number of topics   [2]\nwhere the scaling factor is just a multiplier that allows a to vary without varying the number of topics, i.e. change how specifically on-topic documents were without adding more topics. Computing equation [1] required the computation of the expression p(w|\u03b1,\u03b2), where w represents the words in the vocabulary, and the literature has different suggestions for doing this. We used collapsed Gibbs sampling, as in Griffiths and Styvers 2004 28. The resulting LDA model consisted of 0 and $, the topic distribution for each document and word distribution for each topic, respectively, where the \u201cdistribution\u201d is a probability for each document or word belonging to a particular topic. I.e., for each document, we can get the probability that it belongs to a specific topic from 0, and for each word, we can get the probability of it belonging to a particular topic from $.\nExposure: We chose four scaling factors and two different corpora (reports and sentences), resulting in eight different LDA models. We tabulated all models in three different ways: 1) - top words (highest probability) per topic with a minimum 3% incidence in the topic, 2) - top 15 documents (reports or sentences) per topic, and 3) - both together. This effectively turned one LDA model into three models, presenting different information on which to make conclusions. As each LDA model was presented in three different ways, we got 24 models.\nTopics were randomly reordered, and all models were given, blinded, to a resident radiologist (fourth-year resident). The radiologist was tasked with determining a description for each topic based on the information in each model and assigning a score of 0 or 1 to 10 for the quality of the description. 0 (zero) meant that it was not possible to give an interpretation of the topic. A score of 1 was low, which meant the radiologist could barely guess what the topic could be, and a score of 10 meant the description was clear and deemed to fit the data flawlessly."}, {"title": "", "content": "Using that data, we performed a linear regression. From the linear regression, we selected the best topic model, which had the most useful and interpretable image labels.\nOutcome: For each model, a median, mean score, standard deviation, standard error of the mean, and the number of unique topics were computed (some topics reoccurred). The models were ranked by mean score, unique topics, and median score in order of decreasing importance. Linear regression modeling was used to determine how the mean score depended on scaling factor, document type (report or sentence), view (presented as top documents, top words, or both), and number of unique topics."}, {"title": "Experiment 2 \u2013 Image Labeling and Image Recognition", "content": "Aim: Generate image labels describing the contents of the radiographs. These labels were meant to be features in the radiographs, i.e., what the radiologist had seen and described in the report, and they needed to be detectable in the radiograph by the CNN.\nStudy subjects: We used reports from radiographic examinations of wrists and ankles taken at Danderyd Hospital between 2002 and 2015. Data are displayed in Table 1.\nUtilizing the best model parameters from experiment 1, we generated one topic model per anatomy (wrist and ankle) with 100 topics each. The topics generated by LDA were analyzed, and topic descriptions were given. For every document, every topic was either true \u2013 if it had sufficiently high probability (p > 1/20) for that document or no answer (missing) if not (\u03b8 in equation [1]). The cutoff was selected as it meant it was five times more likely than pure chance (1/100) to belong to that topic. Image labels, i.e., properties/features of the image, were deduced from the topic descriptions. By merging related topics and labels, a false result was introduced. For example, \u201cfracture\u201d and \u201cno fracture\u201d are mutually exclusive. If the \u201cfracture\u201d label was true, then the \u201cno fracture\" label could not be true for the same document and vice versa. Combining labels into a single label gave three possible outcomes: true, false, and missing. Labels for each anatomy were merged into labels for the data. Whenever there was no information, the outcome was set to missing. This was done to not skew the ratio between true and false outcomes to a degree where constantly guessing false or true gives a high accuracy simply by being much more common. If a value is false 95% of the time, continually guessing false would result in 95% accuracy. Missing values were excluded from the analysis. The most common outcome and frequency of the most common outcome was called the best guess (or the mode in statistical literature), and we derived the mode for each label.\nExposure variable: The report labels were used as labels for all radiographs associated with the examination, where each examination generally contained two or more images. Images were randomly divided into three groups. 70% of the images were reserved for training the CNN to detect image features. 20% of the images were used for verification, i.e., to give a quality estimate for the model during training. 10% was set aside for testing the trained CNN model, i.e., computing the accuracy of the CNN for each label. The four possible outcomes are described in Table 3.\nOutcome: Accuracy meant the percentage of cases where the CNN image label corresponded to the LDA label (TN+TP). We called this the base accuracy, accuracybase of the CNN, and it was computed as in equation [3],\naccuracy base = True positives + True negatives/number of images in test set = TP+TN/TN+FN+TP+TN   [3]"}, {"title": "Experiment 3 - Reviewing neural network classifications", "content": "Aim: Evaluate the quality and usefulness of the image labels in experiment 2, where the base accuracy was calculated by comparing the LDA label to the CNN label (as in Table 3 and equation [3]).\nStudy subjects: Five labels generated by LDA in experiment 1 were selected for study in experiment 3. 234 images were needed for a power of 0.8 with p=0.95 to compare CNN to the standard treatment (manual review). We studied 300 images per label.\nExposure variables: The LDA label value was derived from text, and the CNN label values were derived from examining images, so the base accuracy compares two different kinds of outcomes (document-derived versus image-derived). Therefore, it made sense to study how well these agree.\nThe base accuracy, accuracybase, estimates the CNN's actual accuracy or performance in detecting a particular image feature. If the label that LDA generates is 1) incorrect (for example, \u201cno fracture\u201d is labeled as a \u201cfracture\u201d), 2) incomplete (arthritis is not mentioned in the report but is present in the image), or 3) partially correct (the label is visible in one but not all images in the examination) the CNN could correctly fail/succeed to detect this in the image. The CNN classification would be correct, but it will come up with an error (FN or FP) because it is inconsistent with the LDA-generated image label. In case 3) both LDA and CNN are correct but could not be correct for all images in the examination. We wanted to study these errors to assess the method's effectiveness correctly. The error (per label), base error, or errorbase was errorbase = 1 - accuracybase.\nThe base error was the proportion of images where the CNN classification differed from the LDA label (misclassified or misses), proportionmisses. The base accuracy was the proportion of images where the CNN agreed with the label (hits) proportionhits.\nTo get an accurate estimate of the CNN error, we randomly drew 150 correctly classified radiographs and 150 misclassified for each label. The images were manually reviewed to see if the image label was present or not in the image, giving a gold standard. The CNN outcomes"}, {"title": "", "content": "were compared to the gold standard giving an accuracy for the hits and misses, accuracyhits and accuracymiss respectively. An estimate of the CNN's true accuracy, accuracytrue, was given by computing a weighted accuracy as in equation [4],\naccuracy true = accuracy miss \u00b7 proportion miss + accuracy hit \u00b7 proportion nit   [4]\nWe used non-parametric bootstrapping to compute an estimate for accuracytrue a 95% confidence interval (95% CI) 33,34.\nOutcome: Using bootstrapping, resampling 10,000 times each from accuracymiss and accuracyhits distributions, we got 10,000 accuracytrue estimates from equation [4]. The accuracytrue was given by the 50% quantile (median) and a 95%CI from the upper 2.5% and lower 97.5% percentile bounds, i.e. 2.5% percentile < median < 97.5% percentile."}, {"title": "Technological Considerations", "content": "We used freely available open-source implementations of algorithms and software. LDA was implemented via the topicmodel package 35, version 0.2-4, for the R statistical programming language, version 3.3.0. The VGG-16 layers CNN was implemented in the Torch7 framework 36,37. VGG-16 was chosen because it was freely available and widely used 29,38,39. A study comparing different CNNs showed that it performed well for our data. See Olczak et al. 2017 for further details on our implementation of VGG-16 40."}, {"title": "Results", "content": "Experiment 1 used 24,948 randomly drawn wrist reports from the dataset, resulting in 44,890 sentences. Eight different topic models were generated using four different scaling factors (0.01, 0.1, 1, and 10 in equation [2]), two different corpus (reports and sentences), and 60 topics each. Each of the eight models was combined with the top 15 documents that best fit the topic model (combination of words), all words with more than 3% incidence in the topic, or both (top words and best fitting documents). As the results of each parameter combination, scaling factor, and corpus were presented in three different ways, it resulted in 24 different models and amounted to 1440 individually labeled and scored topics.\nThe \u201cnormal\u201d scaling factor gave many interpretable topics (7) but not a high mean score (5.9), even for the best model. Among our choices, the most optimal value for the scaling factor was the \u201csmall\u201d scaling factor (0.1). It was less clear if it was best to see documents, words, or both, but the last option tended to rank higher. The best topic model was generated using individual sentences based on linear regression , consistent with . A small scaling factor (0.1) for our corpus of medical radiologist reports gave the most interpretable model regarding the highest mean score and most unique topics. To determine topics, we should also study both reports and top words."}, {"title": "Experiment 3", "content": "In experiment 2, five representative labels were selected. For each of the five labels, 150 misclassified and 150 correctly classified images were randomly drawn from the test set. These were manually reviewed to create a gold standard, and the CNN outcome was computed compared to the gold standard and accuracytrue was computed as in equation [4] with bootstrapping. 1,500 radiographs were reviewed for the five labels, with a gold standard of 300 images per label, and CNN outcomes were compared to the gold standard.\nThe best-performing category was fracture, compared to the mode, followed by radius fracture. While other categories, such as fibular fracture and OD, achieved slightly higher accuracytrue, fracture, and radius fracture had a much lower starting mode, i.e., more was learned. OD was significantly better than the mode, but just barely. Fibular fracture base performance was slightly below the mode but reached an accuracy of 88.5% after manual review. Fractures of the ulnar styloid process did not improve their accuracy with any significance."}, {"title": "Discussion", "content": "This study examined how automated ML could classify orthopedic trauma radiographs from radiologist reports and then use those labels to train an image classification CNN. We found that 1) it was possible to automatically find clinically relevant information in radiologist reports using NLP, although our data did not conform well to the standard premises of LDA. 2) The LDA labels could be used to train a CNN to detect this information in the radiographs. 3) We showed, through a manual review of radiographs, that the accuracy for the CNN differed from the computed base accuracy and was considerably higher. We showed that LDA was not well suited for labeling images based on radiologist reports."}, {"title": "Topics and Text Analysis of Radiologist Reports", "content": "LDA was designed for a corpus containing individual documents assumed to be about a central topic and subtopics, i.e., of some length 26,41. Radiologist reports are concise with a restricted specialist vocabulary and relatively standardized language but variable in scope and length due to their dependency on the exam referral. Experiment 1 showed that the best way to model our data was to use individual sentences as documents rather than reports, with fewer expected topics per document.\nML algorithms thrive on data, and more data is usually better. The theory for LDA and topic modeling generally holds that longer documents and more data are better as they depend on comparing how words co-occur between documents 41,42. Short documents decrease the likelihood of words co-occurring between documents (i.e., giving sparse data). Yan et al. 41 and Quercia et al. 42 gave examples of topic models for short documents, but these have not been widely adopted or extensively tested. Even so, for our corpus, we found that less text was better for finding concrete and usable topics, and we could adapt our LDA topic model to find topics within our corpus. It was likely due to the homogeneity, focus, and limited specialist vocabulary in radiologist reports, meaning that the word co-occurrence was significant even for short documents. It motivates future studies, and the results could be useful when modeling other journal data. Dividing the whole dataset into separate anatomies (wrist and ankle) and modeling them separately was consistent with the divide-and-conquer results from experiment 1. It was also shown to be the best approach in a pilot study. There is a risk that specific and common topics for one anatomy might get lost for the model as a whole. A hypothetical topic describing a fracture in the talus (a bone connecting the ankle and foot) found in 20% of the ankle radiographs would make up 5% of the corpus if wrists had twice the number of images. If we always guessed that there was no fracture of the talus, we would have an accuracy of 80% in the first case (only ankle radiographs) and 95% accuracy in the second case (ankle and wrist radiographs). This motivated the use of less data to construct useful topics. This held for anything but broad topics, like fractures in general.\nThere is a trade-off between the ML preference for more data and many topics where many would be closely related 28, making it difficult to interpret our approach. In experiment 2, we examined individual anatomies, which gave fewer and more distinct individual topics but a smaller corpus. Our more incremental approach, where each anatomy is included separately, could allow for incremental improvements and additions of anatomies. In time, a considerable amount of corpus could be generated and reused in later implementations. On the other hand, this does not leverage the full power of ML. For example, based on clinical experience, arthrosis is not commonly asked for in hand examinations but more frequently in hip examinations. The ML algorithm will have less information from which to detect arthrosis in hand data than hip data using our segmental approach, meaning that arthrosis of the hand could become undetectable. In an approach combining all anatomies, arthrosis in the hand would likely be detected along with arthrosis in the hip, as it looks and is described similarly across the entire body."}, {"title": "Loss of Semantic Context", "content": "The bag-of-words model means that crucial syntactic information is lost. An example would be a topic that can contain its negation, e.g., \u201cno fracture, but visible luxation\u201d and \u201cfracture but"}, {"title": "", "content": "no visible luxation.\u201d For this kind of example, LDA could detect and separate the topics of fracture and luxation but was poor at separating positivity or negativity for respective topics. A nuance would be a document stating \u201cno additional fracture present,\u201d indicating that some fracture is present but could be interpreted as no fracture. LDA did not capture these nuances well, but by studying sentences rather than whole reports, we hoped to overcome them when we aggregated the results from individual sentences in a report. This could have been dealt with using sentiment analysis, an NLP technique determining whether a text is positive or negative about its topic 43. Combining sentiment analysis with our topic labels could be potent but requires thousands of manually annotated reports."}, {"title": "CNN Accuracy as a Proxy for LDA Utility", "content": "This study did not examine the LDA topics except for generating labels, i.e., a tool for automated feature extraction from texts to train a CNN. This was in keeping with Shin et al. and other studies in the field using LDA for image labeling 29,44,45. While similar, the stochastic nature of LDA means that if the same experiments were to be rerun, they would not give the same distributions 26,28. A rigorous study of the LDA outcomes would lack generality. While label quality is essential, our results show perfect labels are not."}, {"title": "CNN Performance for Radiographs", "content": "Our study partially paralleled the procedure from Shin et al. 29. We focused on the clinical outcome regarding pathology detection in radiographs and CNN accuracy rather than the LDA outcomes and, therefore, introduced a manual review process to estimate the method's true performance. We achieved considerably better results than Shin et al. for CNN on some labels, but we also had more images in a more limited domain. For experiment 3, we manually reviewed the radiographs, compared them to a gold standard, and found that the CNN performed even better than the base accuracybase indicated. For example, for fractures, we showed that the true accuracy was 16% higher than the base accuracy indicated and 8% higher for fibular fractures. This shows a considerable number of labeling errors and illustrates the limits of LDA in analyzing radiographic reports. Our CNN could learn and discriminate against these image features despite this. In an unpublished study, currently in the manuscript, we achieved an accuracy of 86% (95%CI 83.5% to 90.0%) for fracture detection in radiographs, where text searches in reports had been used to create the image labels. Our study reached 91% (95%CI 87.8% to 93.7%) accuracy. While the LDA-derived labels could be used to train the CNN to detect features in the images, our results were not significantly better than those of labeling reports using text searches and more straightforward methods. For other topics, the LDA-trained CNN did not exceed the mode, and constantly guessing the mode (usually false) would have given a better accuracy. For styloid fractures, the accuracy was dismal. In a study by Wang et al. 46, using a different but related CNN approach to detect malignant features in mammographic images, they reached accuracies of 61.3% and 89.7% for detecting tumor mass and calcifications, respectively, in 201 test images, making the outcomes comparable to ours. Considering our results and the study by Shin et al. 29, we conclude that while it was possible to use, it was not superior to other methods (such as text matching) for labeling radiographic images from reports. However, considering the study by Wang 46, we found that our results were on par with CNN image analysis of radiographs."}, {"title": "Analysis of Other Topics", "content": "The OD proved unexpectedly difficult. With an OD, it is seemingly evident that there has been a fracture at some point, but it was not always visible as the foreign material sometimes obscured the fracture on the radiograph. Such devices were counted as fractures by the network and the reviewer, even though this was not necessarily true as they could be remnants from a previous fracture or an arthrodesis. This was also confounding in some cases as radiographs are two-dimensional see-through projections of a three-dimensional space, where everything is superimposed on top of everything else. For example, in a side view of a fibula fracture, the OD would seem to be fixed to the fibula. Still, from a different view (a frontal view), the device was part of the tibia and was only superimposed onto the fibula. The same problem was present with casts, as these were almost exclusively labeled as having whatever fracture class was reviewed, even when the fracture was obviously elsewhere. It was rare that a casting was labeled as not having a fracture visible. Casts were also present in many images incorrectly classified as having an OD, with a high number of false positives. Casts will likely occur along with OD postoperatively and can look like external fixations. They were, however, not always classified as such, and there were a few false negatives where casting and OD occurred together.\nNot visible in the numbers of Table 7 is that the accuracy for fibular fracture would be lower if not for the fact that these fractures tended to coincide with other fractures. It was clear from the manual review that the CNN reacted positively to any indication of a fracture anywhere in the image, and accuracytrue which would have been considerably lower if not for this concurrence. Given that the fibular fracture category was similar in size to the OD category and reached much better results, this hid poor performance.\nCompared to the gold standard, there were fewer errors among the correctly classified images in general, but, as expected, classification errors were more common among the misclassified images. We perceived that misclassified images were, in general, more difficult to review, taking more time and requiring more scrutiny. One explanation could be that these are difficult to classify and that the report contains uncertainty captured by the LDA but not the image review. For example, \u201cprobable fracture,\u201d \u201clikely fracture,\u201d or \u201cunlikely fracture\u201d all indicate ambiguity and could indicate anamnestic information from the referral or just that it is challenging to tell from the radiograph. However, we did not show this conclusively."}, {"title": "Strengths and Limitations", "content": "This study utilized a unique dataset of 88,000 reports and 235,000 radiographic images. Extending the methods to more anatomies and millions of exams is straightforward. We also compared selected labels to gold standards of 300 images per label, for a total of 1,500 images, giving a reasonable estimate of the CNN's accuracy at detecting each label in the radiographs.\nTo our knowledge, our study was the first to explicitly study LDA and CNN for orthopedic trauma radiography and attempt to optimize it for radiologist reports in the Swedish language. In the process, we showed the need for calibration. We also generated a gold standard for our primary outcome (CNN outcome for the five selected labels) and manually studied many radiographs.\nWe customized a freely available CNN and trained it to detect image features in radiographs. This shows that these powerful methods could be available to the average clinician with limited training at minimal cost.\nLDA was founded on the idea of documents with a main topic and several minor topics, i.e., a longer text 26. The radiologist reports in this study did not conform to this ideal. \"No fracture\u201d is a valid and highly relevant complete two-word report that was not uncommon in our data. While there are LDA models that try to deal with short documents 42,47, we did not implement such models as they were special-purpose implementations unsuited for our task or not"}, {"title": "", "content": "sufficiently studied. Instead, we tried to accommodate this by calibrating the LDA. The bag-of- word model generated by LDA is good at finding topics 26,27,48 but not optimal for data labeling.\nRadiologist reports are answers to exam referrals containing questions (usually diagnoses) posed by physicians. Some information in the image is omitted from the report or has no bearing on the referral. This information, for example, a pathology, is not mentioned and is therefore impossible to extract for image labeling, a one-sided misclassification error. This will be more common for rare outcomes and those that were common but rarely asked for in trauma referrals, such as osteoarthritis. We tried to compensate for this by manually reviewing positive and negative outcomes.\nA radiographic examination entails several images in different projections. Our CNN looked at one image at a time. The image feature deduced from the report would be valid for the examination but not necessarily"}]}