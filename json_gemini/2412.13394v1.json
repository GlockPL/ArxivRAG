{"title": "Distribution Shifts at Scale: Out-of-distribution Detection in Earth Observation", "authors": ["Burak Ekim", "Girmaw Abebe Tadesse", "Caleb Robinson", "Gilles Hacheme", "Michael Schmitt", "Rahul Dodhia", "Juan M. Lavista Ferres"], "abstract": "Training robust deep learning models is critical in Earth Observation, where globally deployed models often face distribution shifts that degrade performance, especially in low-data regions. Out-of-distribution (OOD) detection addresses this challenge by identifying inputs that differ from in-distribution (ID) data. However, existing methods either assume access to OOD data or compromise primary task performance, making them unsuitable for real-world deployment. We propose TARDIS, a post-hoc OOD detection method for scalable geospatial deployments. The core novelty lies in generating surrogate labels by integrating information from ID data and unknown distributions, enabling OOD detection at scale. Our method takes a pre-trained model, ID data, and WILD samples, disentangling the latter into surrogate ID and surrogate OOD labels based on internal activations, and fits a binary classifier as an OOD detector. We validate TARDIS on EuroSAT and xBD datasets, across 17 experimental setups covering covariate and semantic shifts, showing that it performs close to the theoretical upper bound in assigning surrogate ID and OOD samples in 13 cases. To demonstrate scalability, we deploy TARDIS on the Fields of the World dataset, offering actionable insights into pre-trained model behavior for large-scale deployments.", "sections": [{"title": "1. Introduction", "content": "Deep learning (DL) models have demonstrated remarkable capabilities across various domains but often exhibit overconfidence in their predictions, even when confronted with data that diverges from their training distribution [10, 19, 25]. This overconfidence arises from the assumption that inference data will follow the same independent and identically distributed (i.i.d.) properties as the training data. However, in real-world applications, this"}, {"title": "2. Related Works", "content": "Our method lies at the intersection of two notable categories of OOD detection methods: scoring functions and activation manipulation. Scoring functions assign a numerical score to each input, reflecting its alignment with ID samples based on the model's output. Maximum Softmax Probability (MSP) [10] detects OOD samples by assessing the softmax confidence score, assuming that low confidence indicates OOD samples. The Out-of-DIstribution detector for Neural networks (ODIN) [16] enhances MSP by applying input perturbations and temperature scaling to improve ID-OOD separation. The Mahalanobis score [15] calculates the distance between input features and class means in feature space, flagging inputs far from these means as OOD. The energy score [17] evaluates the model's energy function to assess the likelihood of an input belonging to the ID distribution. Another category of OOD detection methods manipulates the internal activations of a pre-trained model to improve detection performance. ReAct [24] identifies differences in activation patterns of the penultimate layer between ID and OOD samples, enhancing separation by clipping activations at an upper limit. DICE [23] applies weight sparsification on a specific layer to further distinguish ID from OOD data and, when combined with ReAct, can enhance detection performance. Activation Shap-"}, {"title": "3. Problem Formulation", "content": "Let \\( \\mathcal{X} \\) represent the set of all possible data the model may encounter, and \\( \\mathcal{Y} \\) the set of class labels. The dataset on which a model is trained is defined as in-distribution (ID), denoted \\( D_{ID} \\subset \\mathcal{X}_{ID} \\times \\mathcal{Y} \\). During inference, however, the model may encounter data from an unknown distribution, referred to as the WILD dataset \\( D_{WILD} \\subset \\mathcal{X}_{WILD} \\), which may contain both ID and OOD samples.\nWe assume a pre-trained neural network \\( f : \\mathcal{X} \\rightarrow \\mathbb{R}^{|\\mathcal{Y}|} \\), trained on \\( D_{ID} \\), and our objective is to distinguish between ID and OOD samples within \\( D_{WILD} \\). The network \\( f \\) extracts features \\( z \\in \\mathbb{R}^{F} \\), where \\( F \\) is the dimensionality of the feature space. Specifically, we fit a binary classifier \\( g : \\mathbb{R}^{F} \\rightarrow \\{0,1\\} \\), parametrized by \\( \\theta \\), that operates on these features \\( z \\) for a given sample in \\( \\mathcal{X}_{WILD} \\). We define \\( g(z; \\theta) = 0 \\) if \\( x \\sim D_{ID} \\), and \\( g(z; \\theta) = 1 \\) if \\( x \\sim D_{OOD} \\)."}, {"title": "4. Method", "content": "Fitting a classifier to distinguish between in-distribution (ID) and out-of-distribution (OOD) samples typically requires known ID and OOD labels, which is impractical in real-world deployments. This creates a need for surrogate ID and OOD labels for samples with unknown distributions, which we refer to as WILD samples. Our method, illustrated in Figure 1, operates with a given model, a set of known ID samples, and a set of WILD samples. It generates surrogate ID and OOD labels for the WILD samples, which are then used to train a binary classifier that can classify the distribution of incoming samples.\nFirst, we sample M in-distribution samples from the ID set and N WILD samples from the WILD set. Next, we pass both the ID and WILD samples through the frozen pre-trained model and extract activations from a specified layer. A pooling operation is applied to perform spatial downsampling, transforming the activations into one-dimensional vectors and combining the downsampled features. We then use k-means clustering to partition the combined features into K clusters. For each cluster, we calculate the proportion of ID samples and assign surrogate labels based on a threshold T: if the proportion of ID samples in the cluster is greater than or equal to T, the surrogate label is 0; otherwise, it is 1.\nHere, \\( y_{sur}(z) \\) denotes the surrogate label: 0 for surrogate ID and 1 for surrogate OOD. With this labeled dataset, we fit a binary classifier \\( g(z; \\theta) \\), which maps a sample to a predicted label. During deployment, for a sample of unknown distribution, we perform activation extraction and classification as follows: \\( \\hat{y} = g(\\text{Downsample}(f(x)); \\theta) \\). The output can be interpreted as probability scores, allowing it to quantify the magnitude of domain shift or thresholded to produce binary values (0 for in-distribution and 1 for out-of-distribution).\nSurrogate Label Assignment. The core novelty of TARDIS lies in its surrogate labeling generation step, which enables us to assign labels to WILD samples based on their proximity to ID samples in feature space. The assumption is that ID and OOD features are sufficiently distinct to allow effective separation through clustering. Specifically, we use k-means clustering to label WILD samples as surrogate OOD if WILD samples outnumber ID samples in a cluster, and as surrogate ID otherwise. Without access to true OOD data, ID samples serve as an indirect benchmark for distinguishing between ID and WILD samples. To optimize this process, we tune the number of clusters K and threshold T by balancing three objectives: entropy (to encourage homogeneity within clusters and distinct groups of surrogate labels), correct ID proportion (to maximize accurate identification of ID samples), and incorrect ID proportion (to minimize the misclassification of ID samples as OOD). The combined objective function mini-"}, {"title": "5. Experimental Setup", "content": "In this section, we describe the experimental setups designed to evaluate TARDIS under controlled distribution shifts, which mimic real-world geospatial deployment challenges. Specifically, we use the EuroSAT [9] dataset for patch-level classification and the xBD [7] dataset for semantic segmentation. EuroSAT contains 27,000 labeled images across ten land use and land cover classes: Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial, Pasture, Permanent Crop, Residential, River, and Sea/Lake. The xBD dataset includes over 1 million labeled samples for building damage assessment from satellite imagery, covering multiple disaster types such as hurricanes, floods, and wildfires. For our experiments, we formulate xBD as a building footprint segmentation task with two classes (building footprint and background). EuroSAT and XBD provide a suitable test-bed for our OOD method due to their differences in volume, task, sensor type, geospatial distribution, and temporal and spatial characteristics, allowing a thorough evaluation under conditions commonly encountered in geospatial deployments.\nTo introduce controlled distribution shifts, we reformulate the train, validation, and test splits of EuroSAT and xBD so that the model, when trained on the modified training set and evaluated on the adjusted test set, encounters shifts similar to those seen in geospatial model deployments. We categorize these shifts as covariate shift and semantic shift. Covariate shift occurs when the input data distribution changes between training and testing. This can introduce selection bias, as covariates in the training set may still appear in the test set to some extent, potentially skewing robustness evaluation. To address this, we also introduce semantic shift experiments where a class is held out during testing, creating an unseen semantic class. For our"}, {"title": "6. Experimental Results", "content": "In this section, we present the ablation studies and discuss the results of the surrogate sample assignment benchmark, where we evaluate the performance of \\( g^* \\). Note that ID task performance is not reported, as it remains unaffected by our method.\nAblation Studies. We conduct benchmark studies on the \\( g_{oracle} \\) classifier to explore the factors influencing TARDIS's ability to detect test-time distribution shifts. This ability depends significantly on the layer from which internal activations are extracted and the downsampling method applied. To address this, we test different layers and compare their performance. For downsampling, we experiment with several methods, including mean and standard deviations, average pooling, max pooling, and PCA-based reduction. The max pooling-based downsampling method achieves the highest performance, likely due to its ability to retain the most salient activation patterns, which we argue is important for effective OOD detection. After selecting the optimal layer and downsampling method, we evaluate various classifiers: KNeighbors, GaussianNB, DecisionTree, ExtraTrees, LogisticRegression, SVC, RandomForestUnbalanced, RandomForest, AdaBoost, and GradientBoosting. Results indicate that Logistic Regression provides the best tradeoff between classification performance and wall time. We then compare \\( g^* \\) to \\( g_{oracle} \\) as we tune the parameters K"}, {"title": "7. OOD Detection Goes Global: Real-World Deployment", "content": "We use the Fields of the World (FTW) pre-trained models [11] to demonstrate the capabilities of TARDIS in a large-scale deployment scenario. FTW is a geographically diverse dataset designed for agricultural field segmentation, covering 24 regions across Europe, Africa, Asia, and South America: Austria, Belgium, Brazil, Cambodia, Corsica, Croatia, Denmark, Estonia, Finland, France, Germany, India, Kenya, Latvia, Lithuania, Luxembourg, Netherlands, Portugal, Rwanda, Slovakia, Slovenia, South Africa, Spain, Sweden, and Vietnam. The dataset contains approximately 70,000 samples, each consisting of multi-date, multi-spectral Sentinel-2 satellite patches paired with three-class semantic segmentation masks (field, field boundary, and background). The task involves segmenting these classes using a pair of Sentinel-2 images one for the planting season and one for the harvesting season as input. Field boundary data is crucial for global agricultural monitoring, however training large scale models to segment field boundaries from satellite imagery presents significant challenges due to the geographic diversity of fields, varying crop cycles, and agro-climatic conditions, all of which which introduce substantial distribution shifts. This complexity, along with corresponding distribution shifts found in real world imagery, makes the FTW dataset ideal for testing TARDIS's ability to diagnose model performance during inference. Additionally, the multi-date nature of the dataset is particularly suitable for evaluating segmentation models that must handle spatiotemporal variations in satellite imagery.\nSampling ID Set. To form the ID set, we sample 50 patches from each country represented in the training set, ensuring a geographically diverse ID dataset that closely matches the data on which the model was originally trained.\nSampling WILD Set. We collect multispectral Sentinel-2 satellite images using the Microsoft Planetary Computer [22], which is freely accessible. The images are processed to Level-2A (bottom-of-atmosphere) and stored"}, {"title": "8. Conclusion", "content": "We present TARDIS, which effectively assigns surrogate ID or OOD labels to samples with unknown distributions by clustering the representation space. These surrogate labels are used to train a binary classifier for OOD detection. We do not aim to optimize OOD detection performance but instead propose a method designed for real-world applications where the distribution is unknown, maintaining ID task performance is critical, and computationally expensive methods are impractical. It achieves performance close to the theoretical upper limit in assigning surrogate ID and OOD samples in 13 out of 17 experiments. We demonstrate TARDIS can be deployed at scale, with the potential to enhance the robustness and trustworthiness of geospatial models as well as provide insights into distribution shifts in global applications."}, {"title": "9. Datasets and Model Details", "content": "9.1. EuroSAT\nEuroSAT [9] is a scene classification dataset derived from Sentinel-2 satellite images, covering various locations across Europe. It contains 27,000 images labeled into ten land-use and land-cover classes: Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial, Pasture, Permanent Crop, Residential, River, and Sea/Lake. The images have a spatial resolution of 10 meters.\nWe use a ResNet50 model pre-trained on ImageNet, modified to accept 13 input channels corresponding to Sentinel-2 spectral bands. The model is fine-tuned with a learning rate of 0.0001 and a batch size of 128. Training runs for up to 100 epochs with early stopping after 5 epochs of no improvement. Input images are normalized using channel-wise mean and standard deviation statistics.\n9.2. xBD\nXBD [7] is a semantic segmentation dataset for building damage assessment from satellite imagery. The dataset, collected from Maxar's Open Data Program, has images with a spatial resolution below 0.8 meters. It includes pre- and post-disaster images of hurricanes, floods, wildfires, and earthquakes, making it suitable for evaluating temporal and semantic shifts.\nWe simplify the damage assessment task into binary segmentation by reassigning damage levels: background (0) and levels 1-2 are grouped, while levels 3-4 form a high-damage class. This minimizes concept drift and ensures a balanced evaluation. We train a U-Net model with a ResNet50 backbone, pre-trained on ImageNet and configured for 3 input channels. Training uses a batch size of 32, a learning rate of 0.001, and runs for up to 50 epochs with early stopping after 5 epochs of no improvement. We reserve 10% of the data for validation and normalize the input images by dividing pixel values by 255."}, {"title": "9.3. FTW", "content": "We follow the practices of the original study and use a U-Net model with an EfficientNet-B3 backbone for semantic segmentation on the FTW dataset. The model is configured with 8 input channels and outputs 3 classes: background, field, and field-boundary. We use class weights of [0.04, 0.08, 0.88] to address class imbalance. The learning rate is set to 0.001, and the loss function is cross-entropy. The number of filters is set to 64, and neither the backbone nor the decoder is frozen during training. We set the patience for early stopping to 100 epochs. The images are normalized by dividing pixel values by 3000.\nFor the classifier, we use logistic regression with a maximum number of iterations set to 500. We train the classifier with 500 ID samples and 1200 WILD samples. The number of clusters is set to 150, calculated as 0.3 times the total number of WILD samples. To reassign labels, we use an ID fraction threshold of 0.1, meaning that a cluster is assigned as OOD if ID samples comprise less than 10% of the total samples in the cluster. The values of 0.3 and 0.1 are determined based on empirical observations gathered from extensive experiments on the xBD and EuroSAT datasets.\nFigure 7 provides a visual illustration of the input samples from the WILD set, where the distribution is unknown. It displays the input Sentinel-2 image pair (Window A and Window B) alongside the OOD classifier g's prediction scores and the DL model f's predictions."}, {"title": "9.4. Introducing Distribution Shifts to EuroSAT and xBD", "content": "The combination of EuroSAT and xBD provides a diverse testbed for evaluating distribution shifts. EuroSAT represents regional imagery at medium spatial resolution, while XBD provides global imagery at very high resolution. Their differences in acquisition times, sensor parameters, processing levels, and the tasks they cover-land-cover classification (EuroSAT) and building detection (xBD)-make them complementary. Additionally, EuroSAT focuses on patch-level classification, while xBD involves pixel-level segmentation, enabling evaluations across different problem dimensions.\nTo evaluate our method, we introduce two types of distribution shifts: covariate and semantic (described in Table 1). Our approach assumes that purposefully rearranging dataset splits creates measurable shifts between training and testing sets, driven by the logic of the split design."}, {"title": "10. Design Choices", "content": "To better understand the impact of various design choices on the performance of our OOD detection method, we conduct a series of ablation studies. Specifically, we explore four key factors: (1) the choice of layer from which to extract feature representations (Section 10.1), (2) the method used to downsample these feature maps (Section 10.2), (3) the type of binary classifier g used to distinguish between surrogate-ID and surrogate-OOD samples (Section 10.3), and (4) the selection of hyperparameters k and T for surrogate label assignment (Section 10.4)."}, {"title": "10.1. Which Layer?", "content": "Selecting the appropriate layer for activation extraction is crucial for accurate OOD detection. Prior works have emphasized the importance of this choice. For example, ASH achieves optimal performance on later layers like the penultimate layer, as earlier layers suffer from significant performance degradation during pruning [4]. Similarly, ReAct performs best on the penultimate layer, where more distinctive patterns between ID and OOD data emerge [24]. NAP-based OOD detection further highlights the variability in layer effectiveness, dynamically selecting top-performing layers based on validation accuracy [20]. Consistent with these findings, we observe that no single layer is universally optimal across all settings.\nWe benchmark FPR95 scores for OOD detection across the first convolutional layer, eight randomly selected intermediate layers, and the last convolutional layer. As shown in Table 3 for the EuroSAT dataset and Table 4 for the XBD dataset, layer performance varies significantly. While late layers often perform well, early and middle layers frequently give competitive results, depending on the dataset and task. Based on these findings, we select the best-performing layer for each experiment."}, {"title": "10.2. Which Downsampling Method?", "content": "Having identified the layer to extract internal activations from, the next step is to look into the effect of downsampling these activations, which can reduce computational complexity and noise while retaining essential features for OOD detection. We explored four methods:\n1. Mean and standard deviation (Mean Std): Computes the mean and standard deviation across the spatial dimensions (H, W) for each channel, providing two descriptive statistics per feature channel.\n2. Average pooling (Avg Pool): Global average pooling was applied, reducing the activation to a single representative value per channel by averaging all spatial values.\n3. Max pooling (Max Pool): Uses global max pooling to retain the maximum value from each spatial dimension, capturing the most prominent feature in each channel.\n4. PCA-based reduction (PCA): Applies Principal Component Analysis (PCA) to reshape the activation map into a vector and projects it into a lower-dimensional space with 10 components.\nWe summarize the OOD detection performance across all experiments on the EuroSAT and xBD datasets under different downsampling methods in Table 5, using the FPR95 metric. Max pooling consistently achieves the best performance across the majority of experiments, making it the preferred approach. We attribute its performance to its ability to retain the most prominent features in each channel, filtering out less significant information. This focus on salient patterns likely enhances the OOD classifier's capacity to distinguish between ID and OOD samples."}, {"title": "10.3. Which Classifier?", "content": "The next key design choice is the selection of the binary classifier g, used to distinguish between surrogate-ID and surrogate-OOD samples based on their feature representations. The results, summarized in Table 6, report the mean performance across all experimental measurements along with the standard error of the mean to represent confidence intervals. We select Logistic Regression as it provides the best tradeoff between classification accuracy and prediction time. This balance is essential for scaling up the method, where both efficiency and accuracy are critical."}, {"title": "10.4. Surrogate Label Assignment: Hyperparameter Search for k and T", "content": "TARDIS relies on a clustering-based approach in the activation space to assign surrogate ID and surrogate OOD labels. This process requires selecting two key parameters: the number of clusters (k) to segment the activation space, and the ID fraction threshold (T), which determines whether a cluster is assigned as surrogate ID or surrogate OOD. Clusters with an ID fraction above T are assigned as surrogate ID, and those below T are assigned as surrogate OOD.\nThe underlying assumption is that samples with similar distributions lie closer in the activation space than those from dissimilar distributions. Effectively segmenting the activation space is critical, as the distributions of WILD samples are unknown during deployment. Segmenting nearby activations and correctly assigning clusters to surrogate ID or OOD makes the choice of k and T crucial for accurate OOD detection.\nTo develop insights into selecting k and T, we conduct controlled experiments on EuroSAT and xBD, where ID and OOD labels are known. In these experiments, we treat OOD labels as WILD and apply our clustering-based surrogate label assignment logic. By holding back the ground-truth WILD labels, we simulate real-world conditions while being able to evaluate the results against known labels.\nThe primary goal is to understand how to choose k and T, and whether there are patterns we can extrapolate to real-life deployment. For this, we calculate the ratio of k to the total number of training samples and evaluate its effect on OOD detection performance (Accuracy, FPR95, and AUROC). We plot these metrics against the ratio of k/total training samples, increasing k until the ratio reaches 1. Theoretically, OOD detection improves with more clusters as this enables finer-grained segmentation of the activation space, reducing the risk of including anomalies in ID clusters.\nTo establish a theoretical maximum (oracle performance), we also evaluate OOD classification with known ID and OOD labels, bypassing the need for clustering. This oracle performance is represented by horizontal dashed lines in Figures 8 and 9 (upper plots). The results for two representative experiments-one from EuroSAT and one from xBD-since all experiments show similar trends. We observe that the performance approaches the oracle boundaries when k is approximately 0.3 times the total number of training samples. While performance improves as k increases, a trade-off is required between performance and walltime as well as computational complexity. Based on this trade-off, we set k to 0.3 for all experiments, including the large-scale deployment on FTW. Furthermore, we observe that our method is not highly sensitive to T. As a result, we fix T to 0.1 for all experiments, which is the value used in this initial investigation. We use the Optuna library"}, {"title": "11. Additional Experimental Results", "content": "In Figure 10, we show the predictions of the DL model f and the OOD classifier g, along with the ground truth class and distribution annotations for the EuroSAT experiment, where Forest serves as the OOD class. The model f trains on 9 classes (excluding Forest) and tests on Forest. The first row shows correct predictions by f, while the second row shows incorrect predictions. Even when f makes misclassifications, g accurately quantifies the distribution shifts in most cases. The performance of f on the test set is not directly measurable since the test uses a single unseen class. We report the performance of g as: Accuracy = 0.9325, ROC AUC = 0.9886, and FPR95 = 0.0619.\nFor xBD, we present results where f trains on Hurricane Matthew (ID, Figure 11) and tests on Mexico Earthquake (OOD, Figure 12). Comparing the input images and masks between ID and OOD shows that even when f performs suboptimally, g effectively quantifies the distribution shifts. The performance of f on the test set is: Multiclass Accuracy = 0.7690 and Multiclass Jaccard Index = 0.6248. We attribute f's suboptimal prediction performance to the significant distribution shift between the training (Hurricane Matthew) and testing (Mexico Earthquake) datasets. The performance of g is: Accuracy = 0.9806, ROC AUC = 0.9986, and FPR95 = 0.0."}]}