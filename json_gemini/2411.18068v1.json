{"title": "PersonaCraft: Personalized Full-Body Image Synthesis for Multiple Identities from Single References Using 3D-Model-Conditioned Diffusion", "authors": ["Gwanghyun Kim", "Suh Yoon Jeon", "Seunggyu Lee", "Se Young Chun"], "abstract": "Personalized image generation has been significantly advanced, enabling the creation of highly realistic and customized images. However, existing methods often struggle with generating images of multiple people due to occlusions and fail to accurately personalize full-body shapes. In this paper, we propose PersonaCraft, a novel approach that combines diffusion models with 3D human modeling to address these limitations. Our method effectively manages occlusions by incorporating 3D-aware pose conditioning with SMPLx-ControlNet and accurately personalizes human full-body shapes through SMPLx fitting. Additionally, PersonaCraft enables user-defined body shape adjustments, adding flexibility for individual body customization. Experimental results demonstrate the superior performance of PersonaCraft in generating high-quality, realistic images of multiple individuals while resolving occlusion issues, thus establishing a new standard for multi-person personalized image synthesis. Project page: https://gwang-kim.github.io/persona_craft", "sections": [{"title": "1. Introduction", "content": "Recent advances in personalized image generation [4, 10, 17-20, 23-25, 31, 33, 48, 52, 56, 60, 61, 64, 64, 65, 69-71, 74, 78, 80, 82-85] have enabled the creation of visually compelling and highly customized portraits. Diffusion models [14, 34, 39-42, 62, 66, 84] have been especially impactful, significantly enhancing the adaptability and quality of generative methods through improved controllability, such as pose-guided generation with ControlNet [81]. However, generating images with multiple individuals remains challenging in spite of its development [11, 26, 28, 30, 43, 46, 50, 58, 75], particularly in complex scenarios with occlusions, where overlapping body parts or interacting poses obscure person-specific details (yellow boxes in the baselines in Fig. 1(a)).\nFull-body personalization also poses significant challenges, as most existing methods primarily focus on facial identity, often overlooking individual body shape details. Many approaches rely on 2D pose estimations [16, 29, 79], which lack the depth required to capture unique physical attributes accurately. Furthermore, current methods struggle to effectively separate a person's identity from their clothing, resulting in unwanted style transfers and inconsistencies, particularly during occluded interactions or when depicting intricate garments.\nTo address these limitations, we propose PersonaCraft, a framework for generating highly personalized, full-body images of multiple individuals in complex scenes. PersonaCraft integrates diffusion models with 3D human modeling (SMPLx model [57]) for precise control over body shape and pose. The approach includes three core mechanisms: 1) Face and body identity extraction: We extract body shape parameters via SMPLx fitting [2, 13, 21] and obtain face ID embeddings from reference images using InsightFace [1]. 2) 3D-aware pose conditioning with SMPLx-ControlNet (SCNet): SCNet, which leverages SMPLx depth maps and is trained on our curated MPII-SMPLx dataset featuring complex, occluded multi-human scenes, enables robust control over body shape and pose, allowing realistic interactions between individuals. 3) Multiple Human Personalized Image Synthesis: For the personalization of multiple identities, PersonaCraft uses seamlessly SCNet and IdentityNet [71], leveraging face masks to accurately localize identities. Unlike two-stage methods (e.g., OMG [43]) or test-time optimization approaches (e.g., DreamBooth [60]), PersonaCraft efficiently personalizes multiple identities within a single generative process.\nPersonaCraft generates highly realistic and personalized images of people, even in occluded scenarios. As shown in Fig. 1(a), prior methods struggle with body shape personalization and anatomical inaccuracies. In contrast, PersonaCraft ensures accurate body shapes and face identities while maintaining distinct styles.\nMoreover, PersonaCraft introduces a unique feature: User-defined body shape control, allowing users to adjust attributes like height and body build for tailored body shapes as shown in Fig. 1(b). This feature is valuable for applications in advertising and social media, where customization boosts engagement.\nExtensive quantitative and qualitative evaluations, including user studies, validate PersonaCraft's superior performance in preserving facial identity, body shape, and natural human anatomy, even in challenging scenarios involving multiple people and significant occlusions. These capabilities position PersonaCraft as a state-of-the-art solution for high-quality, multi-person image generation, surpassing existing methods in accuracy and personalization."}, {"title": "2. Related Works", "content": "Single-concept personalization. Text-to-image (T2I) diffusion models and applications [14, 34, 39-42, 62, 66, 84] enable single-concept personalization, adapting pre-trained models for individual subjects. Early methods used optimization-based techniques [17, 20, 31, 33, 60, 61, 65] or textual embeddings [4, 23, 56, 69, 70, 82, 83]. LORA methods [35, 68] reduced the need for many trainable parameters. Recent works[10, 18, 19, 24, 25, 48, 52, 64, 64, 71, 74, 78, 80, 84, 85] like IP-Adapter [80] and InstantID [71] adopts modular design enabling fast personalization from a single reference image and included human-centric modules. However, these methods often struggle with full-body fidelity, either neglecting body shape or failing to disentangle identity from clothing in the case of single reference personalization.\nMulti-concept personalization. Current approaches often combine multi-concept datasets through joint training, adding losses or optimization steps to facilitate model merging. Some methods [11, 26, 30, 50] use cross-attention maps to avoid entangling concepts. Custom Diffusion [46] proposes either joint training or constrained optimization for model integration. Mix-of-show [28] utilize gradient fusion reduces identity loss, while regionally controllable sampling manages attribute binding. Modular Customization [58] leverages orthogonal directions to isolate concepts for easier integration. FastComposer [75] enhances text conditioning in diffusion models with subject embeddings, allowing personalization without retraining. OMG [43] proposes the method without additional training for multi-concept personalization enhancing efficiency and flexibility. However, existing methods still exhibit limitations in maintaining occlusion robustness and precise control over body shapes.\nPose-guidance for image synthesis. Recent works such as ControlNet [81] and HyperHuman [49] combine T2I dif-"}, {"title": "3. Proposed Method: PersonaCraft", "content": "We aim to generate high-quality, personalized images of multiple people in a complex scene from the single full-body references. Given a set of full-body reference images ${\\{I_{\\text{ref}}^i\\}}_{i=1}^{N_{\\text{human}}}$, target poses ${\\{p^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$, where $N_{\\text{human}}$ is the number of human identities, and a text prompt y, our goal is to synthesize a personalized image $I_p$ that accurately reflects the identities, poses, and the semantic content specified by the text prompt. This task is particularly challenging due to complex occlusions and the need for precise full-body shape personalization.\nOur proposed method, PersonaCraft, tackles these challenges using a 3D human model-based diffusion model, as shown in Fig. 2. We begin by extracting face and body identity embeddings from the reference images. Next, we apply a 3D-aware pose conditioning technique with SMPLx-ControlNet (SCNet) to generate high-quality, occlusion-robust images. We then perform personalized image synthesis for multiple humans. Finally, we introduce a novel application of our method: user-defined body shape control, allowing users to specify desired body shapes and generate customized human images."}, {"title": "3.1. Face and Body Identity Extraction", "content": "We first extract face and body identity information from a single reference image $I_{\\text{ref}}^{(i)}$ per individual as illustrated in the left part of Fig. 2. We employ MultiHMR [13], a state-of-the-art SMPLx [57] fitting method, to perform robust 3D human reconstruction, estimating SMPLx parameters from a single image. This process extracts body shape parameters $\\beta^{(i)}$ for each person, ensuring accurate shape representation. For face identity, we utilize InstantID [71]'s face ID embedding method via InsightFace [1], obtaining face embeddings $f^{(i)}$. These combined embeddings, $\\beta^{(i)}$ and $f^{(i)}$, capture person-specific identity features, enabling the generation of highly personalized multi-human images."}, {"title": "3.2. 3D-Aware Pose Conditioning using SMPLx-ControlNet", "content": "Existing human personalization methods often utilize 2D skeleton-based pose ControlNets [81], for pose-guided image generation. However, 2D pose estimation lacks the depth and specificity required to accurately capture unique body contours and structures, often leading to a loss of individual physical attributes. This limitation hinders full human personalization, including both face identity and body shape, and makes the models vulnerable to occlusions.\nTo address these issues, we leverage 3D pose information, $p \\in \\mathbb{R}^{\\text{joint} \\times 3}$, along with camera view pose c, where $N_{\\text{joint}}$ is the number of joints. 3D pose provides a more comprehensive representation of human pose, capturing depth information which can be significant in scenarios with complex poses and occlusions. However, even with 3D pose, naively using 3D skeleton-based pose conditioning may not fully capture body shape variations.\nTo overcome this limitation, we propose a novel 3D-aware pose conditioning technique using SMPLx-ControlNet (SCNet). By leveraging 3D human models, specifically SMPLx [57], we can accurately represent body shape and pose. Given body shape parameters ${\\{\\beta^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$, 3D poses ${\\{p^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$, and a camera view pose c, we first generate a 3D human model using SMPLx. We then render the SMPLx depth map $d_{\\text{SMPLx}}$, which serves as a strong conditioning signal for the diffusion model and effectively handles occlusions. This SMPLx depth map is used as a precise guidance for image generation with SCNet.\nMPII-SMPLx Dataset. We train SCNet by fine-tuning depth ControlNet [81], originally trained on a large text-image-depth triplet dataset, using our curated MPII-SMPLx dataset. This dataset builds upon the MPII dataset [9], a comprehensive collection of human activity images featuring diverse and challenging poses and complex interactions. Notably, there is currently no well-curated multi-human image dataset with 3D human reconstruction, making our MPII-SMPLx dataset one of our key contribution. To enhance the MPII dataset, we incorporate textual descriptions from Khan et al.[38]. For 3D human reconstruction, we employ MultiHMR[13], a state-of-the-art multi-human SMPLx [57] fitting method, to handle multiple identities per image and render depth maps from the reconstructed SMPLx meshes. To account for occlusion scenarios, we balance the dataset with a 1:2 ratio of single-person to multi-person images. Depth clipping is applied during depth rendering to retain only values below 5, ensuring consistent quality. After preprocessing, we curate a final dataset of 6,348 image-text-SMPLx-parameter pairs. This carefully curated dataset enables robust model training with diverse 3D human poses, complex interactions, and detailed human parameters such as body shape and pose conditioning.\nTraining objective of SCNet. Following the ControlNet [81] framework, the training objective minimizes the difference between generated and ground truth images. Given an input image $x_0$, its noisy counterpart $x_t$ from noise $\\epsilon_t$ at diffusion timestep t, a text prompt y, condition c, and control signal $d_{\\text{SMPLx}}$, the SCNet $\\mathcal{E}_C$ is fine-tuned from depth ControlNet using the objective:\n$\\mathbb{E}_{x_0, \\epsilon, t, c, d_{\\text{SMPLx}}}[\\|\\epsilon - \\epsilon_{\\theta}(x_t, t, c, \\mathcal{E}_C(d_{\\text{SMPLx}}))\\|]$ (1)\nwhere $\\epsilon$ is a pretrained diffusion model, kept frozen during SCNet training, and $\\epsilon \\sim \\mathcal{N}(0, 1)$."}, {"title": "3.3. Multiple Human Personalized Image Synthesis", "content": "To synthesize personalized images of multiple individuals, we leverage a face ControlNet, IdentitiyNet [72], and SCNet. To ensure precise identity preservation and enhanced image quality, we employ face masks to accurately localize facial regions from the given pose. Suppose we aim to generate a personalized image, given a set of $N_{\\text{human}}$ reference images ${\\{I_{\\text{ref}}^i\\}}_{i=1}^{N_{\\text{human}}}$ corresponding target poses ${\\{p^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$, and a text prompt y. Then, we obtain face embeddings ${\\{f^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$ and body shape parameters ${\\{\\beta^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$ and corresponding SMPLx depth $d_{\\text{SMPLx}}$ following the processes in Sec. 3.1 and Sec. 3.2.\nWe then obtain residual features $R_{\\text{face}}^{(i)}, R_{\\text{body}}^{(i)} {\\}}_{i=1}^{N_{\\text{human}}}$ generated from the shared IdentityNet [71], $\\mathcal{E}_{\\text{ID}}$, given face embeddings ${\\{f^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$ and face landmarks ${\\{p_{\\text{face}}^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$. Landmarks can be extracted from reference images or provided as additional 3D pose inputs. Also the residual feature $R_{\\text{body}}$ is generated with SCNet $\\mathcal{E}_{SC}$, given text y and $d_{\\text{SMPLx}}$ derived from ${\\{I_{\\text{ref}}^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$ and ${\\{p^{(i)}\\}}_{i=1}^{N_{\\text{human}}}$.\n$R_{\\text{face}} = \\mathcal{E}_{\\text{ID}} (f^{(i)}, p_{\\text{face}}^{(i)}), R_{\\text{body}} = \\mathcal{E}_{SC} (y, d_{\\text{SMPLx}})$ (2)\nLet $F^{(k)}(\\cdot)$ represent the k-th neural block and $s^{(k)}$ the k-th input feature map. We obtain $s^{(k+1)}$ by adding these residual features to $F^{(k)}(s^{(k)})$, scaled by their respective conditioning weights $\\alpha_{\\text{face}}$ and $\\alpha_{\\text{body}}$, and modulated by face masks $M^{(i)}$:\n$s^{(k+1)} = F^{(k)} (s^{(k)}) + \\alpha_{\\text{face}} \\sum_{i=1}^{N_{\\text{human}}} M^{(i)} R_{\\text{face}} + \\alpha_{\\text{body}} R_{\\text{body}}$ (3)\nUnlike two-stage methods (e.g., OMG [43]) or test-time optimization approaches (e.g., DreamBooth [60]), our method is more efficient and preserves the versatility of foundation models like SDXL. This enables training-free multi-human personalization from single reference images, with scalability to accommodate numerous identities without performance degradation."}, {"title": "3.4. User-Defined Body Shape Control", "content": "The SMPLx [57] shape parameters $\\beta$ are multiplied elementwise with the orthonormal shape displacement components, S, and then added to the base shape, $v_{\\text{template}}$. Given $\\beta \\in \\mathbb{R}^{10}$, $S \\in \\mathbb{R}^{N_{\\text{vertices}} \\times 3 \\times 10}$, and $v_{\\text{template}} \\in \\mathbb{R}^{N_{\\text{vertices}} \\times 3}$, the shaped vertices are defined as.:\n$v_{\\text{shaped}} = v_{\\text{template}} + \\sum_{i=1}^{10} \\beta_i \\cdot S[:, :, i]$. (4)\nPersonaCraft offers user-defined body shape control, allowing adjustments based on user preferences. Specifically, we provide two types of controls: 1) Reference-based body shape control: This approach derives the body shape parameter $\\beta_{\\text{target}}$ from the reference image using SMPLx fitting and directly applies it. 2) Interpolation and extrapolation-based body shape control: Here, we acquire two reference body shape parameters, $\\beta_1$ and $\\beta_2$, and generate a controlled shape by interpolating or extrapolating between them. This target shape is defined as $\\beta_{\\text{target}} = (1 - \\gamma) \\beta_1 + \\gamma \\beta_2$, where $\\gamma \\in \\mathbb{R}$ adjusts the blend between $\\beta_1$ and $\\beta_2$. The resulting $\\beta_{\\text{target}}$ is then applied to Eq. 4 to obtain the target body shape. Results are shown in Fig. 7."}, {"title": "4. Experiments", "content": "4.1. Experimental Details\nImplementation details. We implemented PersonaCraft using Stable Diffusion XL (SDXL) [59] as the base model. SCNet was trained on a curated dataset with a learning rate of 1e-5, using the Adam optimizer and a batch size of 2. The training process ran for 50,000 iterations on a single NVIDIA A100 GPU. For all experiments, we utilized 50 denoising steps.\nBaselines. To evaluate the performance of PersonaCraft, we compared it against several baseline methods for single-shot, multi-identity, and pose-controllable image synthesis. All baselines were implemented using SDXL [59] to ensure a fair comparison. Key baselines include OMG [43], a state-of-the-art multi-concept personalization method, combined with InstantID/IP-Adapter/IP-Adapter-Face (IPA-Face) [71, 80], a leading single-shot personalization model and 2D pose ControlNet [81]. Optimization-based methods such as DreamBooth [60] and Texture Inversion [23] were also included for comparison. Additional results with other baselines, which may not align perfectly with the above categories, are also provided in the supplementary.\nTest dataset. For testing, we evaluated our method using the COCO-WholeBody dataset [37], which contains full-body images of multiple people. The dataset includes images of 1 to 5 people, with a total of 1,000 images selected (200 images for each number of people). Text prompts were extracted from these images using BLIP [47] to generate corresponding textual descriptions. For pose conditioning, MultiHMR [12] was used to obtain 2D poses and 3D poses.\nMetrics. Face identity preservation was measured for 1-5 identities following FastComposer [75], using FaceNet [63] for identity similarity within the face mask. Body shape preservation was evaluated using cosine similarity between SMPLx \u03b2 parameters. Text-image correspondence was measured with CLIP-L/14 for image-text alignment. Additionally, a user study was conducted to assess perceptual quality. More experimental details are in the supplementary."}, {"title": "4.2. Results", "content": "Qualitative results. As shown in Fig. 4, 5, and 6, our proposed method, PersonaCraft, demonstrates significant advantages over existing approaches, particularly in handling multi-identity images with occlusions and challenging poses. By leveraging a 3D-human model-conditioned ControlNet, PersonaCraft accurately preserves body shapes, enhances pose fidelity, and maintains body feature consistency with fewer artifacts in complex scenarios.\nIn contrast, methods like InstantID+OMG, IP-Adapter+OMG, and IPA-Face+OMG [43, 71, 80], which rely on 2D skeleton-based pose conditioning, exhibit severe anatomical inaccuracies in occluded or intricate interactions (highlighted by yellow arrows). These limitations arise from the inherent weaknesses of 2D pose representations, which struggle with overlapping body parts and complex scenarios. Additionally, blue arrows highlight cases where PersonaCraft maintains accurate body structure and pose fidelity, outperforming these methods in both identity preservation and body shape consistency.\nDreamBooth [60] and Textual Inversion [23], which lack explicit pose guidance, often suffer from severe anatomical distortions, omission of individuals in multi-person scenes, and issue with clothing-body shape displacement, where clothing styles fail to adapt to individual body shapes.\nUnlike 2D pose ControlNet, which struggles to distinguish occluded regions and handle challenging poses, PersonaCraft addresses these limitations effectively. It demonstrates robust performance as the number of subjects increases, preserving identity and body structure in multi-subject scenes. These findings highlight PersonaCraft's robustness and versatility, establishing it as a state-of-the-art solution for personalized image generation in complex, real-world scenarios.\nQuantitative results. We evaluate PersonaCraft's performance in terms of face identity preservation, body shape preservation, and text-image correspondence. As stated in Tab. 1, the results show that our method consistently outperforms baselines in both face identity and body shape preservation across varying numbers of identities. Notably, it demonstrates robust body shape preservation with minimal variation across different body shapes. While text-image correspondence scores are comparable, it is important to note, as highlighted in previous literature [42, 45], that CLIP scores are less effective in evaluating images with complex scenes. To supplement this, we also conducted user studies, where PersonaCraft's performance was highly rated.\nUser study. We conducted a user study to assess the naturalness, face identity preservation, body shape preservation, and text-image correspondence of images generated by three baseline methods (one from each group) and our method. Participants ranked their top three preferences for each attribute, yielding 10,332 responses from 123 participants. Results in"}, {"title": "4.3. User-Defined Body Shape Control", "content": "A key feature of PersonaCraft is its user-defined body shape control, as demonstrated in Fig. 7. Users can select a refer-"}, {"title": "4.4. PersonaCraft with Stylization", "content": "The proposed method is a plug-and-play approach, making it compatible with various style-specific LoRAs. To evaluate its effectiveness, we conducted experiments combining PersonaCraft with diverse style LoRAs, including Crayon [55], Pastel [8], 3D Render [27], Pixel Art [53], Illustration [7], Frosting Lane [5], Pok\u00e9mon Trainer [67], JoJo [54], Graphic Novel [15], and Cartoon [6]. The results, shown in Fig. 8, highlight the method's ability to adapt to different styles effectively. Notably, styles such as Pastel, Illustration, JoJo, and Pok\u00e9mon Trainer introduce changes in facial and body characteristics, occasionally altering perceived identity, due to their bias. Nevertheless, the outcomes remain visually compelling and demonstrate the versatility of our approach."}, {"title": "4.5. Efficiency Analysis", "content": "As analyzed in Tab. 3, we compare the inference times for multi-identity personalized synthesis across different methods, specifically for generating images with three distinct identities. Textual Inversion [23] and DreamBooth [60], which rely on optimization-based personalization, require a batch size of 4 and 500 optimization steps per identity. This process results in significantly longer inference times, making these methods highly inefficient for real-time applications.\nOn the other hand, methods based on OMG [43], which utilize a two-stage process to generate images, also require over twice the amount of time compared to our approach. In contrast, PersonaCraft performs inference efficiently, generating personalized images with substantially lower computation time while maintaining high-quality results. This stark difference highlights the efficiency advantage of our method, especially when generating multiple identities in a single synthesis process."}, {"title": "4.6. Ablation Studies", "content": "As shown in Tab. 4, the use of the body shape parameter enhances body shape preservation during personalization. Additionally, Fig. 9(a) demonstrates that our SCNet outperforms others in pose-guided image generation, effectively preserving human anatomy even in complex poses and occlusions. Moreover, Fig. 9(b) shows that our masking method improves performance, while adding features without masking increases feature variance and results in artifacts.\nAdditional results, including qualitative results, stylized artwork, PersonaCraft with LoRA and other face control modules, and ablation studies, can be found in the supplementary material."}, {"title": "5. Discussion", "content": "While our method is versatile and can be applied to other ControlNet models, the performance of our face personalization depends significantly on the underlying face network. Additionally, the accuracy of 3D human model fitting is contingent upon the performance of the fitting algorithm used. Variations in the quality of the fitting process may impact the final output, especially in cases where the reference data is incomplete or inaccurate. It is crucial to emphasize that PersonaCraft must be used responsibly, as the misuse of such technology for malicious purposes, including unauthorized identity manipulation, poses ethical and legal concerns."}, {"title": "6. Conclusion", "content": "In this paper, we presented PersonaCraft, a powerful framework for generating highly personalized images of multiple individuals, even in complex occlusion scenarios. By integrating a 3D human modeling approach with SMPLx-ControlNet, PersonaCraft effectively handles occlusions and ensures precise full-body personalization\u2014advancing beyond previous methods that primarily focused on face identity or struggled with overlapping poses. The framework also introduces a novel feature for user-defined body shape control, which allows for more tailored and flexible customization, making it applicable to a wide range of scenarios.\nThrough comprehensive experimentation, PersonaCraft demonstrated superior performance in preserving individual identities, handling intricate poses, and maintaining body shape consistency across multiple individuals in a scene. We believe PersonaCraft holds significant promise for enhancing personalized content creation in diverse domains, including virtual environments, social media, and more. Future work will focus on refining the system to handle more complex interactions and real-world applications for dynamic scene personalization."}, {"title": "Supplementary Material", "content": "A. Additional Results\nA.1. Additional Qualitative Comparison\nAs shown in Fig. S1, our method significantly outperforms existing approaches. InstantID+OMG[43, 71] and IP-Adapter+OMG [43, 80], which use 2D skeleton-based pose conditioning, show severe anatomical inaccuracies in challenging cases (yellow arrows) due to the limitations of 2D pose representations. Blue arrows highlight that our method preserves body structure and pose fidelity while excelling in face identity and body shape consistency.\nDreamBooth [60], lacking pose guidance, exhibits worse distortions, omitting individuals in multi-person scenes and misaligning clothing with body shapes.\nThese results highlight the robustness of PersonaCraft as a state-of-the-art solution for personalized image generation in complex scenarios.\nA.2. Comparison with Additional Baselines\nWe compared PersonaCraft against additional baselines, including UniPortrait [32], MS-Diffusion [73], and FastComposer [76]. While these methods share similar capabilities, they are not fully suited for our benchmark, making direct comparisons challenging.\nAs shown in Fig. S2 and S3, yellow arrows highlight anatomical inconsistencies in complex poses and occluded scenarios due to reliance on 2D pose representations or the absence of pose control. PersonaCraft, in contrast, generates anatomically accurate and natural images under these conditions.\nBlue arrows mark individuals used to evaluate body shape preservation, where PersonaCraft outperforms baselines, which frequently fail to maintain accurate body shapes.\nPink arrows indicate the inability of UniPortrait and FastComposer to handle multiple identities effectively, often mixing or duplicating identities. PersonaCraft successfully preserves distinct face identities for all individuals.\nAdditionally, MS-Diffusion copies clothing directly from full-body references without proper displacement. PersonaCraft integrates personalized body shapes and clothing displacement, maintaining consistency and realism.\nThese results highlight PersonaCraft's superiority in generating accurate, identity-consistent images and handling occlusions and diverse poses with exceptional naturalness and customization.\nA.3. Versatility of SCNet\nTo demonstrate the versatility of SCNet, we present results combining SCNet with various face identity personalization models, including InstantID [71], PhotoMaker V2 [48], and IPAdapter-Face [80]. As shown in Fig. S4, SCNet enables robust body shape personalization and pose control when paired with these face models, achieving comprehensive full-body personalization and user-defined body shape adjustments. Notably, face personalization varies slightly depending on the chosen face module.\nA.4. Ablation Study on Conditioning Scale\nWe analyze the effect of the conditioning scales of IdentityNet and SCNet on identity preservation when provided with face references and reference body shapes (SMPLx depth). As shown in Fig. S5, when the conditioning scale is set to 0 for both modules, the generated face and body shapes differ significantly from the reference. This indicates insufficient guidance from the reference inputs.\nAs the conditioning scales for IdentityNet and SCNet increase, the generated images progressively resemble the reference face and body shape. This improvement demonstrates the critical role of conditioning strength in aligning the generated outputs with the given references. Optimal conditioning scales enable PersonaCraft to faithfully preserve both facial and body shape identities, ensuring high-quality personalization and consistency.\nA.5. Additional Results of User Study\nWe present the cumulative percentage of the Top 3 results in Tab. S1, with details on the user study settings provided in the supplementary Sec. B.4. Consistent with the Top 1 results in Tab. 2 of the main paper, our method is preferred over other baselines in terms of all attributes."}]}