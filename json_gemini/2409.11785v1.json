{"title": "Distilling Channels for Efficient Deep Tracking", "authors": ["Shiming Ge", "Zhao Luo", "Chunhui Zhang", "Yingying Hua", "Dacheng Tao"], "abstract": "Deep trackers have proven success in visual track-\ning. Typically, these trackers employ optimally pre-trained deep\nnetworks to represent all diverse objects with multi-channel\nfeatures from some fixed layers. The deep networks employed\nare usually trained to extract rich knowledge from massive\ndata used in object classification and so they are capable to\nrepresent generic objects very well. However, these networks are\ntoo complex to represent a specific moving object, leading to\npoor generalization as well as high computational and memory\ncosts. This paper presents a novel and general framework\ntermed channel distillation to facilitate deep trackers. To validate\nthe effectiveness of channel distillation, we take discriminative\ncorrelation filter (DCF) and ECO for example. We demonstrate\nthat an integrated formulation can turn feature compression,\nresponse map generation, and model update into a unified energy\nminimization problem to adaptively select informative feature\nchannels that improve the efficacy of tracking moving objects on\nthe fly. Channel distillation can accurately extract good channels,\nalleviating the influence of noisy channels and generally reducing\nthe number of channels, as well as adaptively generalizing to\ndifferent channels and networks. The resulting deep tracker\nis accurate, fast, and has low memory requirements. Extensive\nexperimental evaluations on popular benchmarks clearly demon-\nstrate the effectiveness and generalizability of our framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Efficient visual tracking is important in many real-world\ncomputer vision and multimedia applications including video\nanalysis [1], [2], video surveillance [3], automatic pilotings [4]\nand human computer interactions [5]. Recently, state-of-the-\nart visual trackers [6]\u2013[11], which are usually based on deep\nnetworks, have performed extremely well in various popular\nbenchmarks. For example, by employing a deep network\nVGG-M [12] trained for object classification on ImageNet [13]\nwith a top-5 accuracy of 84.1%, the ECO tracker [10] achieved\na precision of 91% on the OTB100 benchmark [14], and the\nC-COT tracker [9] delivered a high expected average overlap\n(EAO) of 0.281 on the VOT2017 benchmark [15].\nAlthough the superior representation power of deep net-\nworks results in high accuracy, these trackers usually in-\ncur high computational and memory costs, which decreases\ntracking efficacy and hinders their practical deployment on\nresource-limited devices. These huge costs arise from the\nredundancy in representing specific tracked objects using\ndeep networks for generic objects. Some proposed direct\nsolutions [8], [11], [16] apply online model update to adapt\ndeep networks learned for object classification or detection\nfor tracking. Although they perform well, these approaches\nare expensive and inefficient. It is therefore necessary to\ndevelop an efficient deep tracker whilst preserving accuracy.\nTo improve viusal tracking efficacy, a number of deep trackers\nhave been proposed that can be grouped into three main cate-\ngories according to their feature processing scheme: learning,\nweighting, and compression trackers.\n\"Learning\" deep trackers directly learn new compact deep\nnetworks for feature representation in tracking from massive\nannotated visual data. For example, Bertinetto et al. [17] pro-\nposed tracking objects by offline training a fully-convolutional\nSiamese network on the ILSVRC15 dataset for object detec-\ntion in videos. Compared to the deep networks learned for\nobject classification, the trained Siamese network had a more\nadaptive expressive power for various objects. Valmadre et\nal. [18] proposed turning correlation filters into a differentiable\nlayer in a deep neural network, and then learning deep features\nend-to-end, tightly coupled to the correlation filter. In general,\nthe learning-based deep trackers need extra large-scale training\nfrom massive data, and the most important component in these\napproaches is transferring the knowledge learned from various\nobjects to a specific object during tracking. However, a key\nissue must be carefully addressed in these approaches is how\nto adaptively transfer the desired rather than total knowledge\nfrom learned objects.\n\"Weighting\" deep trackers weight deep features or responses\nfrom different layers of a deep network pre-trained on object\nclassification by adaptively measuring layer influences. For\nexample, Ma et al. [6] observed that the earlier layers provide\nmore precise localization, while the later convolutional layers\nencode the semantic information of objects. They then pro-\nposed to hierarchically exploit both facets by fusing their con-\nfidence responses. In contrast, Qi et al. [7] presented an adap-\ntive hedge method to combine features from different deep\nlayers into a multi-channel feature containing a large channel\nnumber (3,072). Some deep trackers performed weighting by\nusing attention mechanism [19], spatial reliability [20], rein-\nforcement learning [21] or multiple templates [22]. Typically,\nthese trackers have impressive accuracy when tracking very\ndifferent objects, but the feature channel number remains huge.\n\"Compression\" deep trackers reduce or compress the feature\ndimension. Danelljan et al. [23] used classical dimension\nreduction such as with principal component analysis (PCA)\nto reduce multi-channel features. Later, they [10] proposed\nfactorized convolution to accelerate the main convolution\ncomputation in deep network inference. Choi et al. [24]\nproposed deep feature compression for fast tracking by a\ncontext-aware scheme utilizing multiple expert auto-encoders.\nThese compression-based trackers mainly aim to reduce the"}, {"title": "II. RELATED WORK", "content": "The types of features employed in visual trackers sig-\nnificantly affect tracking performance. Encouraged by deep\nlearning [25], recent visual trackers mainly apply deep features\ninstead of the single or multi-channel hand-crafted features\nused in early DCF trackers [26], [27].\nMa et al. [6] employed multiple convolutional layers to\nimprove tracking accuracy by hierarchically utilizing deep\nfeatures from the early and final layers in the DCF framework.\nDanelljan et al. [9] used 611-channel multi-resolution deep\nfeature maps in a continuous formulation to improve tracking\nperformance. Generally, these deep trackers performed signif-\nicantly better and gained higher accuracy than other trackers\nusing hand-crafted features. Some methods prefer to mix or\ncombine features to improve them. Qi et al. [7] used an\nadaptive hedge method to combine features from different\nconvolutional layers into a single layer. He et al. [28] adopted\na channel attention mechanism to weight different channels.\nWang et al. [29] presented a residual attentional Siamese\nnetwork to reformulate the correlation filter within a Siamese\ntracking framework, and introduces different kinds of the\nattention mechanisms to adapt the model without updating the\nmodel online. Song et al. [11] proposed reformulating discrim-\ninative correlation filter (DCF) as a one-layer convolutional\nneural network, using VGG-16 as the feature extractor. In [30],\nthe authors weighted convolution responses from each feature\nblock and then summed them to produce the final confidence\nscore. Huang et al. [31] proposed an approach to improve deep\ntracker speed by adaptively processing easy frames with cheap\npixel features and challenging frames with expensive deep fea-\ntures. Wang et al. [32] proposed two networks to online select\nfeature maps from different layers of VGG-16. Danelljan et\nal. [9] used multi-resolution deep feature maps in a continuous\nformulation. Lu et al. [33] applied residual connections to fuse\nmultiple convolutional layers as well as their output response\nmaps. Choi et al. [34] introduced a deep attentional network\nto choose a subset of associated correlation filters according to\ntracked object s dynamic properties. Typically, these methods\nimproved tracking accuracy, but the mixed approach using\nweighting or combination could not reduce the computation\nand storage requirements of original features.\nIn summary, deep features from well pre-trained networks\nare usually sufficient to represent generic objects, and fully\nfixed feature channels are typically used by current deep\ntrackers. However, these deep features usually contain sig-\nnificant redundancy, and the features used in fixed channels\ngenerally incur large memory and computational costs. It\nis therefore necessary to reduce this redundancy to improve\ntracking efficacy."}, {"title": "B. Feature Compression Methods", "content": "Feature compression represents an alternative way to im-\nprove tracking efficacy. Feature compression includes channel\npruning and dimension reduction.\nChannel Pruning. Noting that a multi-channel feature can\ndescribe an object from various views with different channels,\na given object may have some specific channel features. There-\nfore, several methods have been proposed to prune channels\nto reduce the feature representation. Channel-pruning methods\nhave recently been used to remove redundant channels in the\nfeature maps of deep convolutional neural networks. In this\nway, the trained deep models could be condensed and the\ninference time reduced. To accelerate very deep models, He et\nal. [35] introduced an iterative, two-step algorithm to prune\neach layer by LASSO regression-based channel selection and\nleast squares reconstruction. The method achieved a two- to\nfive-times increase in speed with very small accuracy loss for\nobject classification. Similarly, Liu et al. [36] proposed a net-\nwork slimming approach to enforce channel-level sparsity in\nthe network by automatically identifying and pruning insignif-\nicant channels. Wang et al. [37] proposed to transfer features\nfor object classification to tracking domain via convolutional\nchannel reductions. They viewed channel reduction as an ad-\nditional convolutional layer with a specific task. This approach\nnot only extracted useful information for tracking performance\nbut also significantly increased tracking speed. Some model\ncompression methods with knowledge distillation [38], [39]\nmodify deep networks to improve efficacy."}, {"title": "III. OUR APPROACH", "content": "In this section, we first review the general channel dis-\ntillation formulation. We then study the effect of channel\npruning and selection on tracking performance by conducting\nexperiments that demonstrate the existence of good channels\nfor tracking a specific object. Based on this finding, we pro-\npose channel distillation and formulate tracking as an energy\nminimization problem by incorporating it into the DCF and\nECO framework (see Fig.1). Finally, we propose an alternating\noptimization algorithm to solve this problem."}, {"title": "A. Channel Distillation Formulation", "content": "Channel distillation aims to extract informative channels and\nprune noisy channels by adaptively selecting the best channels\nfor diverse tracked objects, which makes the distilled feature\nchannels powerful for improving tracking performance.\nThe objective of channel distillation is to learn both tracking\nmodel h and good channel selection a from a set of training\nexamples $\\{(x_i, y_i)\\}_{i=1}^n$ by using a multi-channel deep feature\ndescriptor $f = \\{f^{(l)}\\}_{l=1}^d$, where d > 1 and n are the numbers\nof feature channels and training examples respectively, $x_i$ is\nthe authentic input image and $y_i$ is its ideal desired output\nor response map. $x_i$ is represented as a d-channel feature\nwith f. Therefore, the channel distillation formulation aims\nto minimize the following loss E:\n$E(h, a) = \\sum_{i=1}^n L(\\sum_{l=1}^d \u03b1_l\u03c6(f^{(l)}(x_i), h), y_i) + \u03bbR(h), (1)$\nwhere $L(\u00b7)$ is a function to measure the difference between\npredicted and ideal desired output, $\u03c6(\u00b7)$ is the model matching\noperator, $R(\u00b7)$ is a regularization function, and parameter\n$\u03bb \u2265 0$ is used to balance the two energy terms. $a =\n(\u03b1_1,...,\u03b1_l,...,\u03b1_d)$ is a d-dimensional binary vector for encoding\nthe channel selection, and $\u03b1_l$ indicates whether the l-th channel\nis selected ($\u03b1_l$ = 1) or pruned ($\u03b1_l$ = 0).\nIt can be seen that solving Eq.(1) means to learn a binary\nweighting to achieve a compressed feature, so channel dis-\ntillation unifies three tracker categories. It is also noted that\nchannel distillation is different from traditional three categories\nincluding: i) channel distillation uses binary rather than real-\nvalue weights to encode channel weighting, which greatly"}, {"title": "B. Existence of Good Channels", "content": "Eq.(1) shows that good channels are evaluated and selected\naccording to energy minimization. We take DCF for example\nto study their existence. Considering that the response map\nis generated by accumulating multiple channel-wise correla-\ntion outputs, we first experimentally analyze the influence of\nchannel selection (or pruning) on tracking performance by\ninvestigating the typical DCF tracker [6] with the VGG-19\nmodel on ImageNet [12]. This analysis is readily extendable\nto other DCF frameworks (e.g., CACF [47]) and multi-channel\nfeatures from other deep networks. The objective of this\nanalysis is to simply reveal the existence of channel selectivity\nand provide insights to guide the design of our framework.\nTo this end, we analyze good channels by supposing that the\nobject position in the current frame is known (e.g., by the\nground truth).\nFor an object o to be tracked in a video v containing k\nframes, feature channel selection aims to find the optimal\nfeature channels $C = \\{C_i\\}_{i=1}^k$ achieving the best tracking\nperformance, where c < d and $C_i\u2208 \\{1,2, ..., d\\}$. In our\nexperiments, we perform channel selection based on the\nempirical consideration that the selected feature channels are\nmore discriminative in spatial and stable in temporal when\ntracking the object. Here, spatial discrimination means the\nfeature channel is more salient or \"good\" to measure the target\nfeatures such that the object can be identified from background\ndistractors, while temporal stability indicates that the target\nfeatures in two consecutive frames are consistent such that\nthe object can be robustly tracked. We call the selected feature\nchannels for an object o in a video v as its \"good channels\",\nand represent the priority of a channel as its \u201cfriendliness\u201d.\nThe friendliness of a channel reflects its contribution to the\ntracking performance. To this end, we select feature channels\nvia the following four stages:\n1) The object patch $x_i$ in a video is cropped from the i-th\nimage frame $F_i$ according to its ground truth bounding box\n$b_i$, and then fed to a pre-trained VGG-19 to generate a multi-\nchannel feature $f_i = \\{f_i^l\\}_{l=1}^d$.\n2) The spatial discrimination and temporal stability in the\nl-th channel are calculated between the objects in two consec-\nutive frames as $s_i(l) = || f_i^l ||_2/m$ and $t_i(l) = - || f_i^l - f_{i+1}^l||_2$\nrespectively, where m is the number of channel elements. This\nshows that channels with larger $s_i(l)$ and $t_i(l)$ tend to be good\nchannels. Here, we generally consider good spatial discrimi-\nnation as salient feature channels with larger activations since\nthey can produce more helpful impacts in correlation filtering,\nand thus measure spatial discrimination with the magnitude of\nthe feature vector.\n3) The channel friendliness is calculated by combining $s_i(l)$\nand $t_i(l)$ and is represented as $r_i(l) = (s_i(l) \u2013 1)t_i(l)$, where\na larger $r_i(l)$ implies good channels give preference to the l-\nth channel in tracking the i-th frame. Then, the total channel\nfriendliness is summed in all consecutive k-1 frames to obtain\nthe average channel friendliness set $\\{r(l)\\}_{l=1}^d$, where $r(l) =\\sum_{i=1}^{k-1} r_i(l)/(k \u2212 1)$.\n4) The feature channels are ranked according to the average\nchannel friendliness in descending order. Then, the tracking\nperformance is evaluated by iteratively pruning the channel\nwith the smallest average channel friendliness until the perfor-\nmance decreases or the maximum iteration is reached. Finally,\ngood channels are returned as the channel set with the best\ntracking performance.\nAfter selection, we study its effectiveness. Compared with\nthe original fixed channel setting on all channel features in\nsome layers, the total tracking precisions with good channels\nare improved for all videos. The main reason comes from\nthe redundancy due to the fundamental inconsistency between\npredicting object class labels in classification and locating\ntargets of arbitrary classes in tracking (as stated in [8]), such\nthat channel distillation in Eq.(1) can avoid learning those\ncorrelation filters that contains negligible energy. This can lead\nto improved tracking accuracy (as stated in [10]).\nMoreover, we also find that good channels mainly focus on\nthe channels in which the features are often more spatially\nsalient and temporally consistent. These findings imply that: 1) there exists a specific\ngood channel set for a tracked object in a video; 2) some\nnoisy channels should be discarded or pruned to improve\nperformance; and 3) the same types of objects (e.g., human)\nshare some similar good channels, indicating that channel\nselectivity may arise from similar variations in specific objects\nwith respect to appearance, motion, etc.\nIn summary, there exist good channels for tracking in a\nvideo that are useful for improving tracking performance. We\nnext examine how to select these \"good channels\" for tracking."}, {"title": "C. Channel Distillation in DCF", "content": "Despite the existence of good channels, selecting them is\nchallenging since the object in the tracking frames is unknown\nin advance except for in the first frame. We therefore apply\nchannel distillation to address this issue. Due to its popularity\nand efficacy in visual tracking, we first use standard DCF as\nan example and incorporate it into channel distillation, which\ncan be formulated as a joint optimization problem:\n$E(h, a) = \\sum_{i=1}^n L(\\sum_{l=1}^d \u03b1_l(f^{(l)}(x_i) * h^{(l)}) - y_i) + \u03bb\\sum_{l=1}^d \u03b1_l ||h0||^2, s.t. \u03b1_l \u2208 \\{0,1\\}, (2)$\nwhere \u2217 denotes circular convolution, $||a||$ represents the num-\nber of good channels, and the tracking model $h = \\{h^{(l)}\\}_{l=1}^d$\nis a multi-channel correlation filter. The first term is used\nto measure the filtering cost between the cross-correlation\noutput and the ideal desired correlation output for authentic\ninput images, while the second term is for regularizing the\ncorrelation filter. Based on Parseval's formula, the problem\ncan be transformed into a frequency domain form. Denote the\ndiscrete Fourier transform (DFT) operator as $f = F(f)$, then\nEq.(2) is rewritten as:\n$E(h, a) = \\sum_{i=1}^n L(\\sum_{l=1}^d \u03b1_l(f^{(l)}(x_i) \u0125^{(l)*}) \u2013 \u0177_i ) + \u03bb\\sum_{l=1}^d \u03b1_l ||\u01250||^2, s.t. \u03b1_l \u2208 \\{0,1\\}, (3)$\nwhere $\\odot$ denotes the element-wise product, and \u2217 is the\nconjugation operator. Note that it is difficult to resolve Eq.(2)\nor Eq.(3), which we address with alternating optimization:\nStep 1. In this step, the objective is to search for the optimal\nsetting of a to minimize the loss defined in Eq.(3) when\ngiving h or \u0125. Noting that a is a vector with discrete binary\nvalues and contained in both the numerator and denominator\nof the second term (which is not linear), there is no analytic\nsolution for Eq.(3). Further, exhaustive searching is very time-\nconsuming and impractical. We address this problem via\nheuristic searching as follows:\n1) Inspired by channel selectivity analysis, we first evaluate\nthe tracking history (historical predictions up to the current\nframe) and obtain the initial good channels $C^{(0)}$ encoded as a\nd-dimensional binary vector $a^{(0)}$.\n2) Then, we fix the number of good channels as c = $||a^{(0)} ||$\nand the optimization problem can be transformed into:\n$E(a) = \\sum_{i=1}^n L(\\sum_{l=1}^d \u03b1_l A_{il} \u2013 B_i) + \\frac{\u03bb}{||h_0||^2} \\sum_{l=1}^d \u03b1_l \u03b3_l, s.t. \u03b1_l \u2208 \\{0,1\\}, (4)$\nwhere $\u03b3_l$ is a scalar, and $A_{il} = vec\\{f^{(l)}(x_i) \\odot \u0125^{(l)*} \\}$\nand $B_i = vec\\{\\hat{y}_i\\}$ are two m-dimensional vectors. Here,\nvec(\u00b7) is an operator to transform a matrix into a vector.\n3) With the seed $a^{(0)}$, we perform iterative searching\nto find the optimal setting for minimizing Eq.(4). Noting\nthat $C^{(0)} = \\{C_j^{(0)}\\}_{j=1}^c$ is ranked and its complement is\ndenoted $\\bar{C}^{(0)} = \\{C_j^{(0)}\\}_{j=1}^{(d-c)}$, we start from the last element\n$C_j^{(0)}$ of $\\bar{C}^{(0)}$, swapping it with the element in $C^{(0)}$ one by\none to evaluate Eq.(4) with the new good channel setting\n$\\bar{C}^{(1)} = \\{C_j^{(0)}\\}_{j=1}^{c-1} \u222a \\{C_j^{(0)}\\}$. If the loss decreases most and lower than\nthe current setting $C^{(0)}$, then we discard $C_j^{(0)}$ and update\ngood channels with $C^{(1)} = \\{C_j^{(0)}\\}_{j=1}^{c-1} \u222a \\{C_j^{(0)}\\}$. The iteration\ncontinues for all the elements in $\\bar{C}^{(0)}$, and generates the final\ngood channels C which can be encoded into the binary channel\nselection vector $a^{(t)}$.\nStep 2. In this step, given the channel selection vector $a^{(t)}$, the\ncorrelation filter h or \u0125 can be efficiently solved by standard\nDCF in the Fourier domain. Denote $f_{ij} = f_{C_j}(x_i)$, $h_j = \\hat{h}_{C_j}$\nand good channels $C = \\{C_j\\}_{j=1}^c$ where $C_j \u2208 \\{1, 2, .., d\\}$.\nThen Eq.(3) is rewritten as:\n$E(\\hat{h}_f) = \\sum_{i=1}^n L(\\sum_{j=1}^c f_{ij} \\hat{h}_j \u2013 \\hat{y}_i) + \u03bb\\sum_{j=1}^c ||\\hat{h}_j||^2, (5)$\nwhere $\u0125_f = [\u0125_1,\u0125_2, ...,\u0125_T]$ is a super vector of the DFTs of\neach good channel. Suppose that\n$f_i = [diag(f_{i1}), diag(f_{i2}), ..., diag(f_{ic})^T], (6)$\nwhere diag(\u00b7) is the operator that transforms a vector into a\ndiagonal matrix, then the solution can be achieved with [44]:\n$\\hat{h}_f = (\\sum_{i=1}^n f_i^T f_i + \u03bbI)^{-1} (\\sum_{i=1}^n f_i^T \\hat{y}_i). (7)$"}, {"title": "D. Channel Distillation in ECO", "content": "In this section, we take ECO [10] as another representative\nexample and integrate it with channel distillation to demon-\nstrate the generalizability of the proposed framework. ECO\napplies a spatial regularized variant of DCF termed SRDCF\nto achieve very impressive performance on recent popular\nbenchmarks. Inspired by that many filters containing negligible\nenergy produce unhelpful feature channels, ECO proposed\nusing a factorized convolution operator to reduce the number\nof filters or parameters in the tracking model with a learned\nprojection matrix after extracting deep features. To this end,\nECO is formulated as the minimal of the loss function\n$E(h, P) = \\sum_{i=1}^n \u03b2_i||h * P h * P h * P h * P h * P \\sum_{l=1}^d ||wh^{(l)}||^2(10)$\nwhere $\u03b2_i > 0$ is the weight of sample $x_i$, $\u03b3 > 0$ is a tuning\nparameter, w is a spatial penalty to mitigate the drawbacks of\nthe periodic assumption of standard DCF, q < d is the channel\nnumber of the projected or compressed filters h, P is a d \u00d7 q\nmatrix for performing compression, and $||*||_F$ is the Frobenius\nnorm. In Eq.(10), by following the linearity of convolution, the\nfactorized convolution alternatively projects the feature vector\nf by matrix-vector product PTf, where T is the transposition\noperator. Denote $g = PTf = \\{g^{(1)}, ..., g^{(q)}\\}$, we can follow\nthe similar manner of Eq.(2) and integrate channel distillation\ninto ECO as\n$E(h, P, a) = \\sum_{i=1}^n \u03b2_i|| \\sum_{l=1}^q \u03b1_lg^{(l)} * h^{(l)} - y_i||^2 +\\sum_{l=1}^q ||w h^{(l)}||^2+\u03b3||P||_F, s.t. \u03b1_l \u2208 \\{0,1\\}. (11)$\nThen, the loss function in the Fourier domain is derived as,\n$E(h, P, a) = \\sum_{i=1}^n \u03b2_i|| \\sum_{l=1}^q \u03b1_l\u011d^{(l)} \\hat{h}^{(l)} - \\hat{y}_i||^2 +\\sum_{l=1}^q ||w h^{(l)}||^2+\u03b3||P||_F, s.t. \u03b1_l \u2208 \\{0,1\\}. (12)$\nFrom Eq.(11) and Eq.(12), channel distillation is applied to\nperform further compression of feature channels after factor-\nized convolution of ECO, leading to reduced channels, e.g.,\nthe number of good channels c = $||a|| < q$. The optimization\nis direct by following subsection III-C and ECO [10]. During\nthe process of training, good channels a are selected from\ng after factorized convolution by following subsection III-C\nand then loss minimization is performed by following [10] to\nupdate h, which outputs the current solution and loss value E.\nThe alternating iteration continues until the maximal iteration\nnumber reaches and the final good channels are determined in\nthe iteration with minimal loss value. Therefore, the factorized\nconvolution matrix P, the filters h or \u0125 and the channel\nselection a are jointly optimized in a unified framework.\nTo demonstrate the efficacy of channel distillation, we\nvisualize the projected feature maps and the corresponding\ndistilled ones. An example is shown in Fig. 3, where ECO\ncompresses 608-channel deep features extracted by the 1st and\n5th convolutional layers of VGG-M into 80 channels and then\nchannel distillation further prunes 16 channels (marked in red\nrectangles). In the feature maps, the brighter color indicates\nlarger feature value (more salient activation) and the blue color\ndenotes near-zero value. We can see that most of the pruned\nfeature maps have majority of negligible value, which are\nhardly contribute to learn correlation filter, especially in the\ncenter of the tracked object. In contrast, the remaining feature\nmaps have salient values in the center, which are helpful\nto learn discriminative correlation filter. In this way, channel\ndistillation can further reduce the negligible channels, saving\ncomputation and memory."}, {"title": "E. Tracking Scheme", "content": "Our tracking scheme is shown in Fig. 1, which incorporates\nchannel distillation into the general DCF tracking framework\nto extract good channels and prune noisy channels. First, based\non good channels selected in Step 1, the multi-channel features\nextracted from the input image are delivered to the channel\ndistillation operator to generate good channel features. Then,\ncross-correlation is performed with the learned correlation\nfilter in the frequency domain via the fast Fourier transform\n(FFT). After that, the object position can be predicted ac-\ncording to the response map. Then, the new object is used\nto update the correlation filter and tracking history, which is\nused to search and update good channels. In our scheme, we\nfix the number of good channels for efficacy after tracking\none frame. This manner is efficiently used in ECO [10]. This\nchannel selection method is efficient due to the small number\nof training examples."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present a comprehensive set of experi-\nments that validate the effectiveness and efficacy of channel\ndistillation. We integrate channel distillation into the standard\nDCF and state-of-the-art ECO [10] formulations and generate\nour deep trackers, DeepCD and ECO-CD, respectively. \u0410\u0441-\ncordingly, we use the deep tracker CF2 [6] as the baseline\nfor DeepCD since it follows the standard DCF formulation,\nwhile ECO serves as the baseline for ECO-CD. Then, we\nbenchmark the channel-distilled deep trackers on two popular\ndatasets, OTB100 [14] and VOT2017 [15], and we further\nbenchmark against extra thirteen representative and state-of-\nthe-art deep trackers. Moreover, we study the distillation from\ndifferent channels and small deep models to demonstrate\nits generalizability and adaptivity. We also analysis effect\nof different frameworks integrated with channel distillation.\nThese trackers are summarized in Table. I. In the experiments,\nchannel distillation is carried out in the first two frames."}, {"title": "A. Baseline Trackers and Evaluation", "content": "Baseline Trackers. We select two representative deep trackers,\nCF2 [6] and ECO [10], and apply channel distillation to the\nbaselines, calling the good channel versions DeepCD-N-L\nand ECO-CD, respectively. Here, N is the deep network (e.g.,\n\"V\" for VGG-19) used and L is the layer set for candidate\nchannel selection (e.g., \"34+44+54\" for conv34, conv44 and\nconv54). Similarly, CF2 is represented as CF2-N-L for the\nsake of simplicity. CF2 uses a pre-trained VGG-19 model [12]\nto extract multi-channel deep features from different layers\nfor sperate correlation filtering and then fuses the responses\ntogether to form the final output. ECO compresses the features\nfrom the pre-trained VGG-M model to achieve an optimized\ntracker. CF2-V-34+44+54 used in [6] has 1,280 feature chan-\nnels. In contrast, DeepCD-V-L selects and combines good\nchannels from some layers L of VGG-19 model. VGG-\n19 and VGG-M are all trained for object classification on\nImageNet and achieve a top-5 accuracy of 90.1% and 84.2%,\nrespectively. We use DeepCD-V-11+12+21 which distills from\nearly layers as our tracker for baseline comparison and also\nstudy other trackers distilling from various layers.\nEvaluation on OTB100. All experiments are evaluated using\ntwo measures [14], [49]: precision and success. Precision\nmeasures the center error between the ground truth bounding\nbox and tracker bounding box, while success is measured as\ntheir intersection-over-union (IoU). In the precision or success\nplot, the maximum allowed center error in pixel distance or the\nrequired overlap is varied along the x-axis, and the percentages\nof the correctly predicted tracker bounding boxes per threshold\nare plotted on the y-axis. According to [49], trackers are\nranked by the common threshold of 20 pixels for precision\nand area under the curve (AUC) for success. All the results\nare generated by OTB-Toolkit [14]. In the experimental results,\nthe measure, speed and the number of feature channels used\nin each tracker are shown.\nEvaluation on VOT2017. Following the general measure\nmethod in [15], we apply overall expected average overlap\n(EAO) scores to evaluate overall tracking performance in both\naccuracy and robustness. Larger EAO scores represent better\ntracking performance. All the results are generated with VOT-Toolkit [15]."}, {"title": "B. Baseline Comparison", "content": "To validate the effectiveness and generalizability of our\nchannel distillation framework for improving tracking perfor-\nmance, we examine the results of the representative baseline\ntrackers using its good channel version on two popular bench-\nmarks (OTB100 and VOT2017) which contain various videos\nwith diverse objects and different performance measures.\nOTB100. The results of the baseline trackers and their good\nchannel counterparts on OTB100 are shown in Fig. 4. We find\nthat the good channel versions outperform the baselines in both\nprecision and success rate, with a 5.7% improvement (from\n0.562 to 0.619) in success rate for DeepCD-V-11+12+21.\nWhen distilling from more and deeper layers, the performance\ncan be further improved (see Fig. 9), e.g., 0.652@success\nachieved by DeepCD-V-11+12+21+54. For ECO-CD, we only\ndistill the deep feature channels from 1st and 5th layers in\nVGG-M, leading to an improvement of 0.5% (from 0.910 to\n0.915) in precision and a more reduced channel number of 55\nthan the projected 80 channels. Therefore, channel distillation\ncan increase the processing speed remarkably. In addition,\nthe comparable improvement can be found under different\ncircumstances, as shown in Fig. 5. Similar results are also\nach"}]}