{"title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation", "authors": ["Zhihang Yuan", "Siyuan Wang", "Rui Xie", "Hanling Zhang", "Tongcheng Fang", "Yuzhang Shang", "Shengen Yan", "Guohao Dai", "Yu Wang"], "abstract": "In this paper, we propose the Dynamic Latent\nFrame Rate VAE (DLFR-VAE), a training-free\nparadigm that can make use of adaptive temporal\ncompression in latent space. While existing video\ngenerative models apply fixed compression rates\nvia pretrained VAE, we observe that real-world\nvideo content exhibits substantial temporal non-\nuniformity, with high-motion segments contain-\ning more information than static scenes. Based\non this insight, DLFR-VAE dynamically adjusts\nthe latent frame rate according to the content com-\nplexity. Specifically, DLFR-VAE comprises two\ncore innovations: 1 a Dynamic Latent Frame\nRate Scheduler that partitions videos into tem-\nporal chunks and adaptively determines optimal\nframe rates based on information-theoretic con-\ntent complexity, and a training-free adaptation\nmechanism that transform pretrained VAE archi-\ntectures to dynamic VAE that can process features\nwith variable frame rates. Our simple but effective\nDLFR-VAE can function as a plug-and-play mod-\nule, seamlessly integrating with existing video\ngeneration models and accelerating the video gen-\neration process.", "sections": [{"title": "1. Introduction", "content": "Video is a fundamental medium for capturing real-world\ndynamics, making the generation of diverse video content a\ncrucial capability for AI systems [Brooks et al., 2024; Agar-\nwal et al., 2025]. Recent advances in diffusion models [Ho\net al., 2020; Rombach et al., 2022; Esser et al., 2024; Jin\net al., 2024] and autoregressive models [Kuaishou, 2024;\nFan et al., 2024] have led to notable breakthroughs in pro-\nducing high-fidelity and long-duration videos.\nAt the heart of these video generation frameworks lies the\nVariational Autoencoder (VAE) [Kingma, 2013; Rombach\net al., 2022; Xing et al., 2024], which jointly reduces spa-"}, {"title": "2. Related Work", "content": "Visual generation in high-resolution pixel space imposes\nprohibitive computational costs for diffusion and autore-\ngressive models. To address this, Rombach et al. [2022]\nintroduced latent diffusion models operating in compressed\nspaces via pretrained autoencoders [Kingma, 2013]. The\nstandard design employs an 8\u00d7 spatial compression ratio\nwith four latent channels [Peebles & Xie, 2023; Li et al.,\n2024b; Tian et al., 2024]. Recent works have focused on\nimproving reconstruction quality through increased latent\nchannels [Esser et al., 2024] or enhanced decoders with\ntask-specific priors [Zhu et al., 2023].\nIn contrast, our work targets a different but equally impor-\ntant goal: dynamically increasing the spatial compression\nratio of autoencoders while still maintaining acceptable re-\nconstruction quality. Chen et al. [2024] also address high\ncompression ratios to enable efficient high-resolution diffu-\nsion models; however, their approach trains a specialized\nautoencoder. Our method, by comparison, is training-free,\nallowing us to obtain a more compressed latent space with-\nout retraining the original autoencoder. To our knowledge,\nthis is the first study to explore higher compression ratios in\nthis training-free manner."}, {"title": "2.2. Training-free Acceleration for Generative Models", "content": "The computational intensity of generative models has\nspurred various acceleration strategies, with training-free\nmethods gaining prominence due to the high training costs\nof modern architectures [Ma et al., 2024b]. Key approaches\ninclude: (1) Reducing inference sampling steps through\ntraining-free few-step samplers [Song et al., 2020; Lu et al.,\n2022a;b; Zhang & Chen, 2022]; (2) Model compression via\nsparsity [Ma et al., 2024a; Yuan et al., 2024] or quantiza-\ntion [Shang et al., 2023; Li et al., 2023; Wang et al., 2024;\nWu et al., 2024a; Li et al., 2024a].\nWhile these methods focus on optimizing diffusion or au-\ntoregressive backbones, they leave the autoencoder un-\nchanged. Our approach introduces a novel direction: en-\nhancing video generation efficiency by increasing video\nautoencoder compression ratios without additional training,\nthereby reducing overall computational demands."}, {"title": "3. Method", "content": "In this section, we present DLFR-VAE, a training-free so-\nlution for dynamic frame rate control in the latent space.\nWe first establish our theoretical foundation by analyzing\nthe temporal frequency of video signals in both pixel and\nlatent spaces (Sec.3.1). Based on this analysis, we propose\nour dynamic latent frame rate space (Sec.3.2). To realize"}, {"title": "3.1. Motivation", "content": "Traditional frame rate optimization in video processing and\ncompression has predominantly focused on raw video sig-\nnals [Song & Kuo, 2001; Mackin et al., 2015]. Previous\nstudies in video content analysis have demonstrated that\nvideo information density exhibits strong temporal non-\nuniformity. This non-uniformity manifests as significant\nvariations in temporal frequency across different video seg-\nments [Menon et al., 2022; Papakonstantinou, 2023]. For\ninstance, in our analysis of the BVI-HFR dataset (Fig. 2a),\nfast-motion sequences like \u201cbooks\" exhibit 5\u20138\u00d7 higher tem-\nporal frequency magnitude compared to static scenes like\n\"flowers\" (see Fig.2b Top).\nHowever, with the growing adoption of deep learning for\nvideo generation, videos are increasingly mapped into a la-\ntent space via an encoder [Fan et al., 2024; Kuaishou, 2024;\nZheng et al., 2024; Kong et al., 2024]. This raises crucial\nquestions: (i) How do temporal characteristics transfer into\nthe latent space? (ii) Does the latent space preserve the\nfrequency variations observed in the original pixel domain?\n(iii) Can we apply dynamic frame rates within the latent\nspace? Although these questions are highly relevant, all of\nthem remain underexplored."}, {"title": "Frequency Analysis of Signals.", "content": "A continuous-time signal\ncan be denoted as $x(t)$. Its frequency spectrum $X(f)$ is\nobtained via the Fourier transform, and for a band-limited\nsignal, it is nonzero only up to a maximum frequency $f_{max}$:\n$X(f) = 0, \\forall f|> f_{max}$.\nSampling the continuous signal at sampling frequency $F_s$\ndiscretizes $x(t)$ into frames:\n$x[n] = x(nT), T =\\frac{1}{F_s}$.\nAccording to the Nyquist-Shannon sampling theo-\nrem [Nyquist, 1928; Shannon, 1949], $F_s$ must satisfy\n$F_s \\geq 2f_{max}$.\nto prevent aliasing. As $f_{max}$ varies across different segments\nof the signal, different segments naturally require different\nframe rates, motivating adaptive frame rate strategies."}, {"title": "Temporal Frequency Analysis of Latent Space.", "content": "Let the\nvideo luminance signal be $x(t)$, and its encoder mapping be\n$\\mathcal{E}$. In the latent space, the signal becomes $z(t) = \\mathcal{E}(x(t))$\nwith a corresponding frequency spectrum $Z(f)$:\n$Z(f) = \\int_{-\\infty}^{\\infty} z(t)e^{-j2\\pi ft} dt$.\nNote that $\\mathcal{E}$ is a complex nonlinear transformation, it can\nalter the amplitude, phase, and frequency characteristics of\na signal or generate new frequency components, leading to\nchanges in the video signal's shape and spectrum. Therefore,\nwe first analyze the signal in the latent space."}, {"title": "3.2. Dynamic Frame Rate Latent Space", "content": "Building on the above analysis, we propose a dynamic frame\nrate latent space wherein each video segment can have a\ndistinct frame rate, allocated based on its temporal complex-\nity. Specifically, suppose an input video is divided into $M$\nsegments ${S_1,..., S_M }$, each comprising $N$ frames. For\nsegment $S_i$, its latent representation is $z_i(t)$. The frequency\nspectrum $Z_i (f)$ is defined as\n$Z_i(f) = \\int_{t_i}^{t_i+NT} z_i(t)e^{-j2\\pi ft}dt$,\nwhere $T = 1/F$, is the sampling interval in the raw video\ndomain.\nFrom empirical observations, certain high-frequency com-\nponents in $Z_i(f)$ have negligible amplitude and minimal\nimpact on overall fidelity. We thus define an effective maxi-\nmum frequency $f_{eff, i}$ for each segment, identifying the point\nwhere the amplitude remains above a threshold $\u03f5$:\n$f_{eff,i} = max\\{f \\mid |Z_i(f)| \\geq \\epsilon\\}$.\nBy the Nyquist-Shannon principle [Ash, 2012], the cor-\nresponding latent-space frame rate for segment $S_i$ can be\nlowered to\n$F_i = 2f_{eff,i}$.\nThis adaptive sampling ensures each segment maintains\nonly the minimum frame rate necessary to preserve percep-\ntually significant temporal details. To maintain temporal\nconsistency across segment boundaries, we implement a\nsmooth transition mechanism that gradually adjusts frame\nrates between adjacent segments."}, {"title": "3.3. DLFR Scheduler", "content": "While theoretically sound, computing exact frequency spec-\ntra for real-time video processing presents significant com-"}, {"title": "", "content": "putational challenges. We address this through a practi-\ncal approximation strategy that maintains the benefits of\ndynamic frame rates while ensuring computational effi-\nciency. Our approach discretizes the continuous space of\ntemporal complexities into $N$ distinct levels. Each level\n$k \\in \\{1, ..., N\\}$ is associated with an effective frequency\n$f_{eff,k}$ . If segment $S_i$ falls into complexity class $k$, its latent\nframe rate becomes\n$F_i = 2f_{eff,k}$\nFormally, we express this as:\n$F_{s,i}= \\sum_{k=1}^{N} (2f_{eff,k}) \\mathbb{I}_{C_k} (S_i)$,\nwhere $\\mathbb{I}_{C_k} (S_i)$ is an indicator function that is 1 if $S_i$ belongs\nto class $C_k$ and 0 otherwise.\nDirectly evaluating $\\mathbb{I}_{C_k} (S_i)$ from raw or latent signals can\nstill be challenging. Instead, we use a practical content\ncomplexity metric $C(S_i)$ as a proxy, which considers the\nSSIM of adjacent frames in a raw video segment:\n$C(S_i) = \\frac{1}{N-1} \\sum_{j=1}^{N-1} (1 \u2013 SSIM(x[j], x[j + 1]))$,\nThis metric can efficiently distinguish high-motion segments\nfrom low-motion ones without explicitly analyzing the latent\nfrequency spectrum. The scheduling logic then maps each\nsegment's metric value to an appropriate complexity class $k$,\nand hence to a frame rate $F_i$. As illustrated in Figure 4, the\ncontent complexity metric exhibits a high correlation with\nboth the effective frequency and the VAE reconstruction\nperformance. We use this metric and thresholds $T_h$ to\ndetermine the frame rate:\n$\\mathbb{I}_{C_k} (S_i) = \\begin{cases}\n1, & \\text{if } T_h^{\\text{down}} < C(S_i) < T_h^{\\text{up}} \\\\\n0, & \\text{else}.\n\\end{cases}$"}, {"title": "3.4. Transform Static VAE to Dynamic VAE", "content": "To convert a pretrained static-frame VAE into a dynamic-\nframe version, we exploit the existing capacity of modern\nvideo VAEs, which have learned to compress videos into\nfixed-frame latent spaces [Chen et al., 2024; Xing et al.,\n2024; Zhu et al., 2023]. As illustrated in Fig. 3, our approach\nintroduces two key modifications to the pretrained VAE: a\ndynamic downsampling module in the VAE's encoder and\na corresponding upsampling module in the decoder. This\ndesign allows us to leverage the robust compression capabil-\nities of pretrained VAEs while enabling variable frame rate\nprocessing without requiring additional training.\nFor example, we use \\{1, 2, 4\\}Hz for the 16 FPS video, which\nhave \\{16x, 8x, 4x\\} temporal downsample ratio."}, {"title": "Encoder Modification.", "content": "Let the input video be\n${X_1,X_2,...,X_T }$. A pretrained video VAE encoder\n$\\mathcal{E}$ typically processes this input into a latent representation\n$z$. To support variable frame rates, we introduce a dynamic\ndownsampler at a strategically chosen point in the encoder.\nGiven a frame-rate schedule ${F_{s,1}, F_{s,2},..., F_{s,M} }$, the\ndownsampler transforms encoder features $h_i$ for each\nsegment $S_i$ into a reduced-rate feature $h'_i$:\n$h'_i = \\text{Downsample}(h_i, F_{s,i})$.\nThese reduced-rate features are then passed through the\nremaining encoder layers, denoted $\\mathcal{E}_{post}$, to yield segment-"}, {"title": "Decoder Modification.", "content": "Decoding requires reversing the\nframe rate changes. A dynamic upsampler is inserted at\nthe corresponding decoder stage. For each segment's latent\ncode $z_i$, the initial decoder layers $\\mathcal{D}_{pre}$ produce intermediate\nfeatures $h''_i$:\n$h''_i = \\mathcal{D}_{pre}(z_i)$.\nThe upsampler then restores the original frame rate $F_s$:\n$h'''_i = \\text{Upsample}(h''_i, F_s)$,\nafter which the remaining decoder layers $\\mathcal{D}_{post}$ reconstruct\nthe final segment $\\hat{S}_i$. The overall video reconstruction $\\mathcal{V}$ is\nformed by concatenating ${\\hat{S}_1, ..., \\hat{S}_M }$.\nCrucially, these modifications allow the pretrained encoder\nand decoder weights to remain largely unchanged, except for\nthe newly inserted downsampling and upsampling operators.\nConsequently, DLFR-VAE can be deployed as a training-\nfree extension on top of mainstream video VAEs, seamlessly\nenabling dynamic latent frame rate control."}, {"title": "3.5. Discussion on DLFR-VAE", "content": "In addition to our information-theoretic formulation, we of-\nfer an intuitive explanation for why our simple but effective\nDLFR-VAE can compress latent space with minimal recon-\nstruction loss. At its core, DLFR-VAE dynamically down-\nsamples the pretrained VAE encoder and, in turn, upsam-\nples its decoder-effectively achieving content-dependent\nspatial-temporal compression without additional training as\nshown in Fig.3.\nPretrained video VAEs [Chen et al., 2024; Xing et al., 2024;\nZhu et al., 2023] are typically trained on large-scale datasets"}, {"title": "4. Experiment", "content": "To evaluate the performance of the proposed DLFR-VAE\nframework, we applied it to two state-of-the-art pretrained\nVAE models: HunyuanVideo VAE [Kong et al., 2024] and\nOpen-Sora 1.2 VAE [Zheng et al., 2024]. These models\nwere converted into Dynamic VAEs by incorporating our\ndynamic frame rate mechanism. We then tested their video\nreconstruction performance on a diverse set of videos."}, {"title": "4.1. Video Reconstruction", "content": "We conducted extensive experiments to compare the recon-\nstruction quality of videos processed by the original VAE,\nStatic VAE, and Dynamic VAE under different temporal\ncompression ratios (CR). For evaluation, we used three\ncommonly employed metrics: SSIM, PSNR, and LPIPS\n[Zhang et al., 2018]. Lower LPIPS values indicate better\nperceptual quality, while higher SSIM and PSNR values\nsignify better structural and pixel-level fidelity.\nTo test the effects of dynamic characteristics in videos, we\nused the BVI-HFR dataset [Mackin et al., 2018], which\nincludes a variety of scene types and motion patterns, such\nas dynamic textures and fast-moving objects against static\nbackgrounds. We evaluated videos at two resolutions (540p\nand 720p) and two frame rates (15fps and 30fps).\nFor the original VAE, the temporal compression ratio is\n4x. To convert the Static VAE into a Dynamic VAE, we\nmodified the encoder and decoder by introducing Dynamic\nDownsampling and Dynamic Upsampling operators. For\nHunyuan Video VAE, the encoder's first two temporal strided\nconvolution layers were augmented with a Dynamic Down-\nsampling operator, while the decoder's last two temporal\nstrided convolution layers were enhanced with a Dynamic\nUpsampling operator. For Open-Sora VAE, we add the dy-\nnamic downsampling operators after the 2D VAE encoder,\nand the dynamic upsampling operators before the 2D VAE\ndecoder. Both are a bilinear sample."}, {"title": "4.2. Visualization", "content": "To visually demonstrate the effectiveness of our DLFR-\nVAE framework, we present a comparison between the"}, {"title": "4.3. Ablation Study on Threshold", "content": "The threshold parameter, introduced in Equation 12, plays a\ncritical role in determining the performance of the proposed\nDLFR-VAE. To better understand its impact, we conducted\na grid search over different threshold values and evaluated\nthe reconstruction performance. The results are visualized\nin Figure 6.\nFrom Figure 6, the following observations can be drawn:\n1. Across all tested threshold values, the performance of"}, {"title": "4.4. Video Generation in Dynamic Latent Space", "content": "In this section, we explore the feasibility of generating\nvideos directly within a Dynamic Frame Rate Latent Space\nwithout any additional training. Given that contemporary\nimage and video generation models are predominantly based\non the Diffusion Transformer (DiT) architecture [Peebles &\nXie, 2023; Esser et al., 2024], adapting these models to work\nin a dynamic latent space requires only minor adjustments.\nSpecifically, we modify the generation of rotary positional\nembedding [Su et al., 2024] to handle tokens corresponding\nto varying latent frame rates during the generation process.\nTo demonstrate this capability, we manually configured the\nlatent frame rates for different temporal segments of the\ngenerated video. Two scheduling strategies were tested: 1.\nHigh-to-Low Latent Frame Rate: Higher frame rates were"}, {"title": "5. Conclusion", "content": "In this paper, we introduced DLFR-VAE, a training-free\nframework for dynamic latent frame rate adaptation in video\ngeneration. By dynamically adjusting the frame rate based\non content complexity, DLFR-VAE significantly reduces\nthe number of elements in latent space. Our experiments\ndemonstrate its effectiveness across various resolutions and\nframe rates, showcasing its potential as a plug-and-play\nsolution for existing video generation models."}, {"title": "6. Impact Statement", "content": "This paper presents work whose goal is to advance the\nfield of Machine Learning, particularly in the domain of\nvideo generation. By introducing DLFR-VAE (Dynamic\nLatent Frame Rate Variational Auto Encoder), we propose a\ntraining-free framework that dynamically adjusts the latent\nframe rate based on video content complexity, significantly\nreducing computational overhead while maintaining high\nreconstruction quality. This innovation has the potential to\nmake video generation more efficient and scalable, enabling\nlonger and higher-resolution video synthesis with reduced\ncomputational resources.\nThe broader impact of this work includes potential applica-\ntions in various fields such as entertainment, education, and\nvirtual reality, where efficient video generation is crucial.\nBy lowering the computational barriers, DLFR-VAE could\ndemocratize access to advanced video generation technolo-\ngies, allowing smaller organizations and researchers with\nlimited resources to leverage state-of-the-art video synthesis\ntools.\nHowever, as with any generative technology, there are ethi-\ncal considerations to be mindful of. The ability to generate\nhigh-quality videos efficiently could be misused for creating\ndeepfakes or other forms of misinformation. It is important\nfor the community to develop robust detection mechanisms\nand ethical guidelines to mitigate such risks. Additionally,\nthe environmental impact of reduced computational require-\nments could be positive, as it may lead to lower energy\nconsumption in data centers. While this work primarily\naims to advance the technical capabilities of video genera-\ntion, we encourage ongoing discussions around its ethical\nimplications and societal consequences to ensure that the\ntechnology is used responsibly and for the benefit of society."}, {"title": "Limitations and Future Directions", "content": "Although the current results are promising, there are also limitations. Manual frame rate configuration requires prior\nknowledge of the video content or desired temporal dynamics, which is impractical for real-world applications. Additionally,\nthe lack of retraining prevents the generative model from fully exploiting the advantages of a dynamic latent space. Future\nresearch could address these limitations by: 1. Developing automated frame rate schedulers integrated with the generative\nprocess. 2. Designing new positional encoding mechanisms tailored to dynamic latent spaces. 3. Training generative models\nend-to-end in such spaces to maximize efficiency and performance."}]}