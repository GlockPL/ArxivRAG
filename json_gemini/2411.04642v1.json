{"title": "TAP-VL: Text Layout Aware Pretraining for Enriched Vision-Language Models", "authors": ["Jonathan Fhima", "Elad Ben Avraham", "Oren Nuriel", "Yair Kittenplon", "Roy Ganz", "Aviad Aberdam", "Ron Litman"], "abstract": "Vision-Language (VL) models have garnered considerable research interest; however, they still face challenges in effectively handling text within images. To address this limitation, researchers have developed two approaches. The first method involves utilizing external Optical Character Recognition (OCR) tools to extract textual information from images and prepend it to the textual inputs. The second strategy is OCR-free and focuses on employing extremely high-resolution images to improve text recognition capabilities. In this paper, we focus on enhancing the first strategy by introducing a novel method, named TAP-VL, which treats OCR information as a distinct modality and seamlessly integrates it into any VL model. TAP-VL employs a lightweight transformer-based OCR module to receive OCR with layout information, compressing it into a short fixed-length sequence which serves as an input for the LLM. To this end, we conduct model-agnostic pretraining of the OCR module on unlabeled documents, followed by its integration into any VL architecture through short fine-tuning. Extensive experiments demonstrate consistent performance improvements when applying TAP-VL to top-performing VL models, across scene-text and document-based benchmarks.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language (VL) models have emerged as a key research area in the field of artificial intelligence, leading to significant progress in multimodal reasoning [4, 8, 12, 18, 20, 23-25, 29, 33-35, 38]. Such architectures bridge the gap between visual and textual data by integrating a vision encoder and a Large Language Model (LLM) via a translation module. This module projects the visual encodings into the text embeddings space. As VL models can generate content based on both visual and textual information, they play a pivotal role in a diverse set of applications and tasks, including image captioning (CAPS) [15] and visual question answering (VQA) [5]. While open-source VL models have shown impressive performance across various tasks, many still face challenges when dealing with with OCR-oriented tasks such as TextVQA [50], TextCaps [49], and DocVQA [43]. There are two main strategies to address this challenge: (1) integrating an external OCR system to extract OCR tokens and use them as additional input, and (2) employing very high-resolution images combined with extensive pre-training to enhance text recognition. Each approach has its own advantages and limitations, and both are active areas of research. In this paper, we focus on the first approach.\nThe prevailing paradigm for incorporating OCR into VL systems involves prepending raw OCR-extracted words into the LLM (left side of Fig. 1). While this strategy enhances performance on OCR-oriented benchmarks, it exhibits critical shortcomings. Firstly, it relies solely on OCR tokens, neglecting crucial spatial layout information proven highly beneficial in OCR-oriented tasks [6, 10, 24, 27]. Moreover, when applied to domains with text-rich images, inserting lengthy OCR sequences into the LLM results in significant computational overhead due to the quadratic complexity of the attention mechanism.\nIn this study, we address these limitations and introduce TAP-VL, a technique for seamlessly integrating OCR information into any VL model through short fine-tuning (right side of Fig. 1). By incorporating 2D positional data in addition to the OCR-extracted word tokens, the model can interpret relationships between different textual elements and understand hierarchical structures that are essential for accurate information extraction and holistic document understanding. Conceptually, our method treats OCR as a distinct modality and thus employs an OCR module, similar to the use of a dedicated vision module for encoding visual input. Following this, we introduce a transformer-based lightweight OCR-Q, to generate meaningful representations conditioned on user queries. The OCR encoder captures vital spatial layout information, while the OCR-Q condenses lengthy OCR details into a fixed-size sequence length representation. This condensed representation serves as input for the LLM alongside visual and textual data (right side of Fig. 2). TAP-VL employs these condensed representations to integrate OCR with spatial information into the VL model.\nInitially, we introduce a standalone, model-agnostic layout-aware pretraining, as depicted on the left side of Fig. 2. This phase operates independently of the VL model, enhancing efficiency and enabling a focused exploration of OCR understanding without introducing a distribution shift to the VL model. Aimed at distilling and extracting the most relevant OCR information, we propose a designated layout-aware pretraining that leverages the abundant unlabeled document data with rich layouts and text [10, 11]. Specifically, we pretrain the OCR-Q in a three-objectives scheme, drawing inspiration from previous works [6, 10, 34]. In more detail, our approach consists of the following layout-aware tasks: (1) OCR-Grounded Mask Denoising, which predicts masked spans based on the noisy OCR input; (2) OCR-Mask Contrastive Learning, which aims to align OCR and word representations within the same document while distinguishing between representations from different documents, and (3) OCR-Mask Matching,which aligns noisy OCR text with missing spans. Combining such objectives propels the model to acquire a deep layout and OCR understanding while providing a compact representation.\nFollowing this, we integrate the same pretrained model into various leading VL models via a short multi-task fine-tuning procedure. Specifically, we examine prominent VL models such as InstructBLIP [20], LLaVA [38] and Qwen-VL [8]. Our extensive experimentation demonstrates the efficacy of TAP-VL across document understanding and scene-text VL benchmarks, resulting in substantial enhancements compared to diverse baseline methods across all assessed benchmarks, including a zero-shot scenario.\nIn addition, we propose TAP-VLLight, a light-weight version of TAP-VL that solely utilizes our compressed OCR representations without providing the LLM with the raw OCR tokens. This approach is specifically efficient in tasks related to document understanding with dense-text images. Notably, we demonstrate that applying TAP-VLLight not only significantly reduces the computational costs compared to the relevant baselines (reduces the FLOPs by up to a factor of seven), but also leads to substantial performance improvements. Notably, we showcase TAP-VLLight's ability to extrapolate to multi-page scenarios without any specific multi-page training. In the most challenging case of multi-page document understanding, TAP-VLLight achieves performance improvements of up to 4.8%, while substantially reducing the computational costs.\nIn summary, our contributions include:\n\u2022 Introducing TAP-VL, a novel approach for seamlessly integrating OCR information into any pretrained VL model, enabling effective reasoning over both textual and spatial information.\n\u2022 Proposing a unique layout-aware model-agnostic pre-training strategy, utilizing unlabeled document data to acquire rich, condensed OCR features.\n\u2022 Demonstrating the effectiveness of our method in enhancing performance across various state-of-the-art VL architectures, showcasing its ability to elevate performance across multiple benchmarks in scene-text and document understanding tasks, including challenging zero-shot multi-page setting.\n\u2022 We present TAP-VLLight, a lightweight version of TAP-VL, capable of handling multi-page documents without any specific training. TAP-VLLight decreases FLOPs by up to four times while still achieving superior performance compared to approaches relying on the uncompressed OCR sequence."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Vision-Language Models", "content": "VL models have undergone significant evolution, transitioning from task-specific to more generalized approaches, facilitated by LLMs [4, 16-18, 25, 47]. These modern models demonstrate adaptability across diverse tasks and impressive generalization capabilities [18, 20, 34, 38]. Architecturally, these models typically involve three fundamental parts. First, a vision architecture extracts meaningful information from images, often employing a frozen vision-transformer as the vision encoder. Second, a translation module bridges the gap between vision and language, transforming visual features into a representation comprehensible and processable by a language model. This module may consist of a simple linear layer or MLP [38], or a cross-attention-based transformer architecture [7, 20, 34]. Lastly, the projected visual information and textual instructions, typically in the form of questions or prompts, are input into an LLM to execute the task. More specifically, BLIP-2[34] suggest incorporating a Querying-Transformer (Q-Former) to efficiently add visual cues from a vision encoder to an LLM and InstructBLIP [20] adapts it to follow instructions. LLaVA [38, 39] introduce a simple projection layer to translate between the vision modality to the language one and utilize GPT-4 generated multi-modal data. Qwen-VL [8] propose a single cross-attention layer and perform a full multi-modal fine-tuning phase to align modalities. Additionally, other works have explored extending these methodologies to encompass multiple modalities, such as audio and video [13, 47, 53, 56]."}, {"title": "2.2. Integrating OCR information into VL Models", "content": "Before the emergence of large vision language models, numerous methods attempted to address OCR-oriented vision challenges. TAP [55] introduced a pretraining objective to align different representations better. In [24], OCR information was integrated into the decoder of an encoder-decoder framework through auxiliary losses. LaTr [10] proposed a layout-aware transformer, enabling reasoning over textual and layout cues using an unsupervised pretraining. However, these works do not consider recent advances in VL models [8, 20] and cannot fully leverage their capabilities.\nAn alternative approach could target this without assuming an OCR extraction system as the previous approaches do. In this scenario, the VL model would solely rely on visual cues. For instance, OCR-free methods like Qwen-VL and LLaVAR [8, 57] observed that open-source all-purpose vision encoders perform poorly in this regard and proposed a training paradigm comprising explicit OCR-oriented tasks. Although performance improved, they still do not surpass methods utilizing OCR systems. Hence, most VL methods [18, 20] incorporate these OCR systems.\nThe conventional method of integrating OCR information into these models involves using the raw text extracted by OCR as part of the input prompt to the LLM [18, 20, 38]. This is feasible as modern VL models can understand that these words are associated with the image, requiring minimal modifications and training to seamlessly integrate OCR-derived text. However, this approach overlooks the fact that OCR extraction also provides word bounding boxes, with layout information being crucial [6, 10, 24]. Additionally, inputting the entire OCR sequence into an LLM is computationally intensive, particularly with OCR-dense images, such as in document understanding tasks, potentially leading to poor performance. Therefore, we introduce TAP-VL, a method that effectively and efficiently incorporates OCR and layout information into any VL model."}, {"title": "3. Method", "content": "In this section we introduce TAP-VL, which encompasses an OCR module, and a novel layout-aware pretraining paradigm, empowering the VL model with OCR comprehension. The OCR module's architectural design is formulated by treating OCR as an extra, independent modality, addressing its inherent complexity. The layout-aware pretraining aims to teach the OCR module to produce a concise yet rich representation of the OCR. Importantly, this phase operates independently of the VL model, enhancing efficiency and ensuring compatibility with various VL architectures. Following layout-aware pretraining, we integrate our OCR module into any VL architecture through parameter-efficient fine-tuning. This integration leads to significant improvements in OCR benchmarks. Moreover, we propose an additional design choice that can reduce the computational footprint significantly compared to existing approaches, while maintaining comparable performance advantages. In the following sections, we detail our proposed OCR module, the layout-aware model-agnostic pretraining, and the integration into any VL model."}, {"title": "3.1. Model Architecture", "content": "To enhance the OCR understanding of VL architectures, we propose an OCR module composed of two pivotal components: an OCR encoder and an OCR compressor, which we term OCR-Q. The OCR encoder produces embeddings based on tokens and their 2D positions, encompassing essential layout information. The OCR-Q is a transformer-based module designed to produce a compact representation of the OCR based on the query. It is comprised of two transformer sub-modules which share the same self-attention layers: (i) an OCR transformer module interacting with the encoded OCR embeddings and (ii) a text transformer module, which handles the free-text input (such as a user's question). Specifically, the OCR-Q transforms the OCR embeddings via learnable queries and textual prompt into a fixed number of representations. We define K to be the number of learnable queries used as input to the OCR-Q. Upon integration into a VL model, these compressed representations are concatenated with user instructions and fed into the VL model, as illustrated on the right side of Fig. 2."}, {"title": "3.2. Layout-Aware pretraining", "content": "The layout-aware pretraining phase aims to produce an OCR-Q capable of compressing OCR content while extracting meaningful information based on textual input. Inspired by the pretraining of BLIP2 [34], we adopt a three-objective scheme: OCR-Grounded Mask Denoising, OCR-Mask Contrastive Learning, and OCR-Mask Matching. To this end, we utilize the myriad of publicly available documents and their OCR information. The overall scheme is illustrated on the left side of Fig. 2.\nThis approach leverages publicly available unlabeled document corpora by randomly masking spans of OCR, which include both text and layout information. Thus creating pairs consisting of masked versions of the OCR and their corresponding masked words for use during pretraining. For each of the pretraining objectives we follow the same general outline (Figs. 3 to 5): the OCR encoder produces rich embeddings given the masked OCR. These embeddings are then fed to the OCR transformer module via the cross-attention layers, resulting in a fixed number of representations per document (determined by K). The text transformer module receives the masked words as input and a special token, dependent on the pretraining task. The spatial information of these tokens are omitted as the text transformer module operates on free-form unstructured text. Its output is then fed into a task-specific projection layer, preceding the loss. The interaction between the text and OCR transformer modules is governed by the masking mechanism employed, contingent upon the characteristics of the chosen pretraining objective. Next we elaborate on each pretraining task, for more details please refer to Appendix A.5.\nOCR-Grounded Mask Denoising tasks the OCR-Q with restoring masked words from noisy OCR input, as illustrated in Fig. 3. This encourages meaningful compressed representations, leveraging both textual and layout information. Throughout this task, the text transformer module indirectly queries the noisy OCR inputs, via the intermediate learned query representations. Since the text transformer module lacks direct access to OCR content, minimizing this loss is feasible only if the representations corresponding to learnable queries are enriched with the relevant OCR information. We use a multimodal mask in our self-attention layers [22, 34] to access compressed OCR information (Fig. 3). This mask restricts learnable queries from attending to text tokens but allows interaction among themselves, while text tokens can attend to learnable queries but only self-interact through a causal mask. A special <dec> token is prepended to the masked words as the denoising task prefix.\nOCR-Mask Contrastive Learning aims to align the outputs of the OCR transformer module with the text transformer module of the OCR-Q (Fig. 4). The OCR transformer module has access to the noised OCR content, while the text transformer module receives a  special token followed by the masked words. The primary purpose"}, {"title": "3.3. Incorporating OCR QFormer in VL Models", "content": "Following our model agnostic pretraining, we align our OCR module with any VL model in a two-phase fine-tuning procedure. First, we employ an OCR-to-language alignment, in which we integrate the OCR encoder and OCR-Q to a frozen LLM. In this stage, we train the OCR module to fit the LLM via fine-tuning using OCR-centric VQA datasets. We use these datasets as most answers can be inferred solely from the OCR information and do not require direct access to the visual inputs [10, 24]. Next, we conduct an OCR-vision-to-language alignment in which we consider the entire VL model, as depicted in Fig. 2. In this setting, the LLM is fed with textual instructions along with both visual and OCR features, from the OCR and vision modules. In addition to the textual instructions, appending the raw OCR word list, as commonly done in VL works, is an optional design choice. To present the trade-off between the two options we introduce TAP-VLLight, which omits the raw OCR word list as input to the LLM, making it more computationally efficient than TAP-VL (see Sec. 4.2). In this stage, we conduct a multi-task fine-tuning using a mixture of OCR-oriented and non-OCR-oriented VQA and captioning datasets, including both documents and natural images, to align the system's building block. Specifically, we train the OCR components and employ low-rank adaptation to the LLM [28] while keeping the visual module frozen. The results in a VL system capable of effectively reasoning over both visual and OCR information."}, {"title": "4. Experiments", "content": "In this section, we demonstrate the advantages of employing TAP-VL to seamlessly integrate OCR information into state-of-the-art VL models in both scene-text and document understanding benchmarks. Initially, we assess various state-of-the-art VL methods, comparing their performance with and without TAP-VL across diverse tasks and domains, including a zero-shot scenario. Subsequently, we highlight the efficacy of incorporating TAP-VLLight, which creates a rich condensed representation of the OCR, addressing the challenge of long OCR sequences, as in multi-page document question answering."}, {"title": "4.1. Experimental Setting", "content": "For all experiments, we use the open-source versions of the VL architectures and initialize from their weights respectively. Unless mentioned otherwise, the OCR encoder is a based on a T5 large encoder [48] with 2D layout embedding initially pretrained in a similar fashion to DocFormerv2 [6]. We initialize the OCR-Q from the pretrained weights of BERT [21], the cross attention weights are trained from scratch. To align the OCR-Q with the OCR encoder, we conduct our pretraining protocol on the expansive unlabeled document dataset IDL [11]. The models are fine-tuned on a variety of domains and tasks simultaneously: document question answering, scene-text visual question answering, general visual question answering, scene-text captioning and image captioning. See Appendix A for more implementation details."}, {"title": "4.2. Integrating TAP-VL into leading VL methods", "content": "In Tab. 1 we report the results of various VL models on several scene-text and document understanding tasks. In the scene-text domain results are provided for TextVQA and STVQA for question answering and TextCaps for captioning. In the document understanding domain, we utilize DocVQA and InfoVQA. Additionally, we provide zero-shot results on the multipage document understanding dataset DUDE [31]. First, we list the specialist models, which were fine-tuned on each dataset independently. These models range from a few hundred million parameters [24] to 55B billion parameters [16] and are pretrained on different datasets. As such, we report these number only for reference as they are not necessarily comparable. In the lower section of the table, we present a comparative analysis between our method and baseline approaches, which were trained under identical settings as our method. This comparison includes performance gaps to underscore the benefits achieved. Specifically, we evaluate our approach within the context of VL models such as InstructBLIP [20], LLaVA [39], and Qwen-VL [8], integrating TAP-VL into each of them. For the InstructBLIP baseline, we consider both the Flan-T5-XL and Flan-T5-XXL architectures [19].\nThe outcomes of our analyses demonstrate the superior performance of our method compared to the baselines across various architectures and benchmarks. For instance, when integrated with LLaVA, TAP-VL showcases enhancements in TextVQA, DocVQA, and InfoVQA, with notable improvements of +2.2%, +5.5%, and +7.6%, respectively. Furthermore, we calculate the average scores over scene-text and document-based datasets, revealing consistent benefits even when compared to the top-performing method Qwen-VL. Furthermore, we direct the reader to results on non-OCR related benchmarks in Tab. 5, demonstrating comparable or even slightly improved performance compared to the baseline."}, {"title": "Leveraging TAP-VL for efficiency", "content": "In this section we present results on TAP-VLLight, a light-weight version of TAP-VL that improves OCR-oriented tasks while reducing computational load. Processing extended sequences demands substantial computational resources, this is especially significant in transformers which are composed of attention blocks that require quadratic complexity computation. TAP-VLLight utilizes the condensed representation alone, omitting raw text OCR input. This is particularly crucial for documents, which consist of dense-text images. In TAP-VLLight, the sequence length of the input to the LLM remains unaffected by the length of the OCR, and is bounded to K tokens. Tab. 2 shows TAP-VLLight results on document understanding tasks.\nOur method improves performance across all benchmarks while offering computational advantages. The DUDE benchmark, known for its multi-page document VQA datasets containing sequences that extend up to 10K tokens, poses a significant challenge to VL systems, pushing them to their operational limits. For example, when examining the integration of TAP-VL into LLaVA, our compressed OCR version yields improvements of +3.2%, +4.8%, and +2.9% on DocVQA, InfoVQA, and DUDE, respectively, while reducing TFLOPs from 20.3 to 5.5. Moreover, in the case of Qwen-VL, the benefits of inputting the raw OCR sequence into the LLM are negligible."}, {"title": "5. Ablation studies", "content": "This section analyzes performance improvements from our proposed architecture and pretraining framework. We examine the effects of TAP-VL's individual components, pre-training objectives, and data quantity on the InstructBlip (Flan-T5-XL) architecture with T5 base OCR encoder.\nTAP-VL components: In Tab. 3, we incrementally incorporate TAP-VL's components to assess their individual effects, starting from the InstructBlip baseline. Initially, we explore a naive method to integrate layout information into OCR by adding a 2D embedding for each OCR token derived from its spatial location via a designated spatial embedding layer in a residual manner. This methods perform similarly to the baseline, suggesting incomplete utilization of residual or encoded information. Subsequently, integrating our OCR module without additional pretraining improves document results by 0.8% but decreases scene-text results by 1.7%. Further, we analyze the impact of our pretraining and OCR-to-language alignment step. While applying each separately leads to improvements in scene-text and degradation in documents, applying both steps results in consistent enhancement in both domains.\nPretraining Objective: We conduct an ablation study focusing on different configurations of our pretraining tasks. We incrementally incorporate the three pretraining objectives: OCR-Grounded Mask Denoising, OCR Mask Contrastive and OCR-Grounded Mask Matching. As indicated in Tab. 4, each pretraining task contributes to enhancing the overall effectiveness of the final model.\nScale of pretraining Data: We examine the impact of varying data volumes during pretraining, as shown in Tab. 4. The results reveal a correlation between data volume and model performance on both scene-text and document benchmarks. This relationship is particularly notable in the case of document benchmarks, where results exhibit an upward trend with increased pretraining length, rising from 62.8 with 2M pretraining samples to 65.7 with 31M training samples. While the correlation persists in scene-text benchmarks, we note a decline in performance when pre-training samples increase from 13M to 22M. However, the best result is achieved with the maximum pretraining sam-"}, {"title": "Performance on General Benchmarks", "content": "We evaluated our system on a general VQA dataset (VQAv2) and a CAPS dataset (COCO), which do not specifically require OCR. Our analysis indicates that TAP-VL either preserves or enhances the non-OCR capabilities of the baseline vision-language model. Integration with InstructBlip (XXL) resulted in a performance boost on the COCO dataset by 3.1%, with only a marginal decline on VQAv2 of 0.1%."}, {"title": "6. Discussion and Conclusion", "content": "In this study, we proposed TAP-VL, a novel method for integrating OCR information into VL models. Our approach, which treats OCR as a distinct modality, utilizes a lightweight transformer-based OCR adapter to compress OCR and layout information into fixed-length sequences for input into VL models. Through extensive experiments, we demonstrated consistent performance improvements across various benchmarks, including both natural images and document-based VL tasks. Additionally, we proposed TAP-VLLight which significantly reduces computational costs by using a concise representation of the OCR. Overall, our findings suggest that integrating OCR information into VL models can lead to substantial performance gains and computational savings, making it a promising avenue for future research in the field."}, {"title": "A.1. Training Datasets", "content": "For our experiments, we employed two distinct dataset combinations for the OCR-to-Language Alignment and OCR-Vision-to-Language Alignment phases. The datasets used in these phases, along with their evaluation metrics and splits, are detailed in Tab. 7.\nOCR-to-Language Alignment: This phase was dedicated to OCR-centric VQA datasets, where answers can be directly inferred from the OCR text including: DocVQA, InfoVQA, ChartQA, OCRVQA, TextVQA, and STVQA.\nOCR-Vision-to-Language Alignment was trained on a combination of OCR-centric and non-OCR-centric datasets, specifically DocVQA, InfoVQA, ChartQA, OCRVQA, TextVQA, STVQA, TextCaps, COCO, and VQAv2."}, {"title": "A.2. Training hyperparameters", "content": "In each stage of TAP-VL's training, the OCR module consistently generated 32 query tokens. Additionally, the AdamW optimizer [40] and the Cosine Annealing scheduler [41] were uniformly applied. Beyond these constants, each stage was characterized by its own distinct set of hyperparameters\nText Layout-Aware Pretraining: The pretraining stage comprised 140,000 training steps, with a learning rate 1e-4 and and a batch size of 224. The OCR module was trained with a maximum of 512 token in the OCR module and a masking probability of 0.15. More information about the pretraining optimization can be found in Tab. 8.\nOCR-to-Language Alignment: Detailed in Tab. 8, this stage maintained a uniform structure across models, with 300K training steps, 1000 warmup steps, a learning rate of 2e-5, and a batch size of 24 for InstructBlip and 32 for the"}, {"title": "A.3. Instruction templates", "content": "For the VQA-based datasets, we use the given question as the instruction. For the captioning datasets, we randomly select an instruction that asks the model to describe the image among the one in Tab. 6."}, {"title": "A.4. Pretraining data preparation", "content": "Each document contains OCR tokens, denoted as $t_1, t_2,..., t_n$ with corresponding bounding boxes $b_1, b_2,..., b_n$. To create training pairs, we randomly mask spans of OCR tokens along with their positional information. Specifically, we:\n\u2022 Sample M spans, each defined by a start index $s_i$ and an end index $e_i$.\n\u2022 Replace tokens and bounding boxes in each span $(s_i, s_i + 1,..., e_i)$ with a special token <extra_id_i> and the minimal bounding box covering the span, respectively (following the method in [10]).\n\u2022 Generate pairs consisting of the noisy OCR input and the masked words, i.e., the original tokens in the masked"}, {"title": "A.5. Layout-Aware pretraining", "content": "For all the pretraining objectives, the OCR encoder produces rich embeddings from the noisy OCR. We denote these embeddings as $O \\in \\mathbb{R}^{B\\times l \\times d_{ocr}}$, where B is the batch size, l is the number of noisy OCR tokens, and $d_{ocr}$ is the dimensionality of the embeddings. Additionally, we define $R_q \\in \\mathbb{R}^{B \\times K \\times d}$ to represent the learnable queries, where B is the batch size, K is the number of learnable queries, and d is their dimensionality. The representation of the M mask words, preceded by the task specific special token, is denoted as $R_m \\in \\mathbb{R}^{B \\times (2M+1) \\times d}$.\nOCR-Grounded Mask Denoising: In this pretraining objective, we use a <dec> special token. The OCR-Q processes $R_q$ through its self-attention (SA) layers, while processing $R_m$ using causal self-attention (CSA) layers conditioned on $R_q$. This results in mask-words representations that integrate contextual information from the queries tokens. Subsequently, the queries representations are updated using the noisy OCR content using a cross-attention (CA) layer. Two finals feed-forward (FF) layers are applied on top of $R_q$ and $R_m$. Formally, we compute:\n$R_q^{(out)} = FF (CA (SA (R_q^{(in)}), O)))$ (1)\n$R_m^{(out)} = FF (CSA (R_m^{(in)} | R_q^{(in)}))$ (2)\nwhere (in) and (out) represent the input and output representations.\nWe then apply a language modeling loss $\\mathcal{L}_{LM}$ over the output representations $R_m^{(out)}$ to recover the original masked content. Specifically, we pass $R_m^{(out)}$ through a softmax function to obtain the predicted token probabilities:\n$Y = Softmax (R_m^{(out)})$ (3)\nThe language modeling loss $\\mathcal{L}_{LM}$ is then computed using the cross-entropy between the predicted probabilities and the ground truth tokens $(y_t)_{t=1}^M$:\n$\\mathcal{L}_{LM} = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{j=1}^M log (Y_{j} y_{t < j})$ (4)\nOCR-Mask Contrastive Learning: In this objective, we use a the <cls> token and its specific representation is denoted as $R_t \\in \\mathbb{R}^{B\\times 1 \\times d}$.\nThe OCR-Q processes $R_q$ and $R_m$ using two independent self-attention layers. This results in mask-words representations that are independent from the from the queries tokens representation. Subsequently, the queries representations are updated using the noisy OCR content using a cross-attention layer. Two finals feed-forward layers are applied on top of $R_q$ and $R_m$. Formally, we compute:\n$R_q^{(out)} = FF (CA (SA (R_q^{(in)}),O))$ (5)\n$R_m^{(out)} = FF (SA (R_m^{(in)}))$ (6)\nThe $R_q^{(out)}$ representation is then extracted from $R_q^{(out)}$ in order to compute the pairwise similarity between $R_t^{(out)}$ and $R_m^{(out)}$, yielding $P_{qt} \\in \\mathbb{R}^{B \\times B \\times K}$, and subsequently, the maximum similarity is selected across the last dimension, resulting in $S_{qt} \\in \\mathbb{R}^{B \\times B}$. Finally, we apply the contrastive learning loss [14], with a temperature scalar $\\tau$ on the $S_{qt}$ matrix:\n$\\mathcal{L}_{Contrastive} (S) = -\\frac{1}{B} \\sum_{i=1}^B log \\frac{exp(S_{ii}/\\tau)}{\\sum_{j \\neq i}^B exp(S_{ij}/\\tau)}$ (7)\nOCR-Mask Matching: In this objective, we compute $R_q^{(out)}$, $R_m^{(out)}$, and $S_{qt}$ similarly to OCR-Mask Matching. We employ a hard negative mining strategy [32] to select challenging negative examples based on the similarity values in the $S_{qt}$ matrix. This approach yields pairs of representations $R_i^{(out)}$ and $R_j^{(out)}$ where $i \\neq j$, indicating they come from different documents. Additionally, we consider pairs where $i = j$, which represent positive examples originating from the same document. The query representation $R_q^{(out)}$ is projected to $R_q \\in \\mathbb{R}^{B\\times 1\\times 1}$ using a feed-forward layer followed by average pooling across the query dimension. This provides a single similarity value for each pair of noised OCR-masked words. Finally, we apply a binary cross-entropy loss to encourage the model to correctly detect matching pairs, where $y_i = 1$ if the pair matches and 0 otherwise.\n$\\mathcal{L}_{BCE} = -\\frac{1}{B} \\sum_{i=1}^B y_i log (\\sigma (P_i)) + (1 - y_i) log (1 - \\sigma (P_i))$ (8)"}]}