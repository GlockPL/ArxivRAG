{"title": "CREST: An Efficient Conjointly-trained Spike-driven Framework for\nEvent-based Object Detection Exploiting Spatiotemporal Dynamics", "authors": ["Ruixin Mao", "Aoyu Shen", "Lin Tang", "Jun Zhou"], "abstract": "Event-based cameras feature high temporal resolution, wide\ndynamic range, and low power consumption, which is ideal\nfor high-speed and low-light object detection. Spiking neu-\nral networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but\nlack efficient training methods, leading to gradient vanish-\ning and high computational complexity, especially in deep\nSNNs. Additionally, existing SNN frameworks often fail to\neffectively handle multi-scale spatiotemporal features, lead-\ning to increased data redundancy and reduced accuracy. To\naddress these issues, we propose CREST, a novel conjointly-\ntrained spike-driven framework to exploit spatiotemporal dy-\nnamics in event-based object detection. We introduce the con-\njoint learning rule to accelerate SNN learning and alleviate\ngradient vanishing. It also supports dual operation modes\nfor efficient and flexible implementation on different hard-\nware types. Additionally, CREST features a fully spike-driven\nframework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our ap-\nproach achieves superior object recognition & detection per-\nformance and up to 100x energy efficiency compared with\nstate-of-the-art SNN algorithms on three datasets, providing\nan efficient solution for event-based object detection algo-\nrithms suitable for SNN hardware implementation.", "sections": [{"title": "Introduction", "content": "Frame-based cameras excel in capturing photometric fea-\ntures but suffer from motion blur with fast-moving objects\nand are sensitive to extreme lighting conditions. Continu-\nous full-frame processing also consumes high energy due to\nredundant background information. Conversely, event-based\ncameras capture changes in light intensity asynchronously at\neach pixel (Lichtsteiner, Posch, and Delbruck 2008; Gallego\net al. 2020). This allows them to operate at much higher fre-\nquencies and under varied illumination conditions, ideal for\nhigh-speed and real-time object detection.\nDeep ANN frameworks for event-based object detection\nencode event streams into dense, image-like representations,\nleveraging conventional frame-based computer vision tech-\nniques (Peng et al. 2023; Zubic, Gehrig, and Scaramuzza\n2024). Modern GPUs and TPUs also accelerate ANN train-\ning and inference (Nickolls et al. 2008; Jouppi et al. 2017).\nHowever, ANNs struggle with effectively handling the tem-\nporal features inherent in event-based data. This arises from\nlosing crucial temporal dynamics and correlations when en-\ncoding events into discrete frames. Additionally, they con-\nsume high energy due to numerous MAC operations, which\nis problematic for energy-sensitive edge applications.\nSNNs mimic the spiking feature of the biological neurons\nwhich are theoretically well-suited for processing event-\nbased data (Maass 1997; Roy, Jaiswal, and Panda 2019). The\ntiming and frequency of spikes convey diverse information,\ngranting SNNs strong spatiotemporal characteristics. More-\nover, SNNs perform accumulations (ACs) only when sparse\nspikes occur, making them inherently energy-efficient. How-\never, SNNs often struggle with deep network structures due\nto inefficient training methods (Zhang et al. 2024; Nagaraj,\nLiyanagedera, and Roy 2023). Non-differentiable spikes re-\nquire surrogate gradients for backpropagation (BP), poten-\ntially causing gradient vanishing in deep SNNs (Wu et al.\n2018; Neftci, Mostafa, and Zenke 2019). The propagation\nof spatiotemporal gradients also greatly increases compu-\ntation complexity. Some works convert well-trained ANN\nmodels to the same structured SNN. However, they require\na large encoding time-window to approximate the ANN per-\nformance, causing a much higher spiking rate and energy\nconsumption (Kim et al. 2020; Li et al. 2022b). More com-\nplex spiking neurons and layers with trainable parameters\nare proposed to lower the spiking rate, which add compu-\ntation complexity for training & inference and are difficult\nto implement on existing SNN hardware (Fang et al. 2021;\nCordone, Miramond, and Thierion 2022).\nFurthermore, a unified SNN framework is needed to han-\ndle the multi-scale spatiotemporal features of event-based\nobject detection. Existing approaches often fail to effectively\nencode the spatiotemporal characteristics of event data, lead-\ning to data redundancy and reduced accuracy (Cordone, Mi-\nramond, and Thierion 2022; Bodden et al. 2024). Moreover,\nthe Intersection over Union (IoU) loss (Yu et al. 2016) em-\nployed in SNNs are generally adapted from ANNs, neglect-\ning the unique spatiotemporal nature of event-based spike\ntrains(Su et al. 2023; Fan et al. 2024). This may lead to inac-"}, {"title": "", "content": "curate regression and thus decrease the detection accuracy.\nTo this end, we propose CREST, an efficient conjointly-\ntrained spike-driven framework exploiting spatiotemporal\ndynamics for event-based object detection. Firstly, we pro-\npose a simple yet effective conjoint learning rule with dual\noperation modes for efficient and flexible training imple-\nmentation on different hardware types. For the backward\nprocess, we design a surrogate neural network with discrete-\nlevel activation values (DL-Net) to mimic the values rep-\nresented by different spike train patterns. This replaces the\noriginal spatiotemporal gradient calculations, which reduces\nthe computation complexity, alleviates the gradient vanish-\ning problem, and speeds up the learning process compared\nwith traditional SNN BP-like or conversion-based training.\nAdditionally, we propose a fully spike-driven frame-\nwork for event-based object detection which includes a\nmulti-scale spatiotemporal event integrator (MESTOR), a\nspatiotemporal-IoU (ST-IoU) loss, and few-spikes neu-\nron (FSN) (St\u00f6ckl and Maass 2021) based SNN model.\nMESTOR not only aggregates event data across multi-\nple scales but also extracts the spatiotemporal continuous\nevents. This keeps the key spatiotemporal feature and re-\nduces redundant background and noise events meantime.\nST-IoU loss comprises the proposed spiking density-based\nIoU (Spiking-IoU) to exploit spatiotemporal continuity and\nthe Complete-IoU (CIoU) (Zheng et al. 2021) for coordi-\nnate loss. The FSN-SNN adopts an efficient spike encod-\ning scheme and is supported by the recently proposed high-\nperformance SNN hardware STELLAR (Mao et al. 2024).\nOur main contributions can be summarized as follows:\n\u2022 CREST, a spike-driven conjointly-trained framework\nexploiting the spatiotemporal dynamics to enhance the\nefficiency of event-based object detection.\n\u2022 A novel conjoint learning rule which introduces a sur-\nrogate neural network with discrete-level activation val-\nues to accelerate the learning process and alleviate the\ngradient vanishing issues in deep SNNs. Dual operation\nmodes add flexibility to its hardware implementation.\n\u2022 A fully spike-driven framework which incorporates the\nMESTOR, ST-IoU loss, and FSN-SNN model to handle\nmulti-scale spatiotemporal features of the event-based\nobject recognition and detection.\n\u2022 Compared with the state-of-the-art SNN algorithms, our\nwork achieves superior recognition & detection perfor-\nmance and up to 100x energy efficiency on 3 datasets."}, {"title": "Background and Motivation", "content": "Inefficiencies of LIF-like spiking neuron models. Leaky\nIntegrate-and-Fire (LIF) neuron and its variants (details are\nshown in Supplementary Material A) are most commonly\nused in SNN algorithms and hardware implementations due\nto their trade-off between low computational complexity\nand biological interpretability (Abbott 1999; Gerstner and\nKistler 2002; Fang et al. 2021). However, LIF neurons only\nsupport temporal or frequency encoding, which requires a\nlong time window and many spikes to achieve high accu-\nracy. Some introduce trainable parameters and normaliza-"}, {"title": "FS-Neuron based Conjoint Learning Rule", "content": "We first demonstrate how to use FSN to emulate ReLU to\ngive an insight into the FSN computation mechanism. Then,\na surrogate DL-Net is proposed to mimic the continuous val-\nues represented by discrete spike train patterns. Further, we\nintroduce the spatial surrogate gradient backpropagation to\nDL-Net. Finally, we illustrate the operation flow of the con-\njoint learning rule.\nSimilarity between FSN and ReLU Functions. ANN-\nbased object detection mostly adopts ReLU and leaky-ReLU\n(Glorot, Bordes, and Bengio 2011; Maas et al. 2013). We\nimplement ReLU in this paper and the detailed reasons are\nillustrated in Supplementary Material B. Consider the con-\nnection between FSN n in layer l and FSN p in layer l + 1.\nFSN n integrates the membrane potential $U_{n}^{l}$ (Eq. 6) based\non Eq. 3. The output spikes from FSN n to FSN p are inte-\ngrated unweighted (Eq. 7) based on Eq. 1.\n$U_{p}^{l+1} = u_{n}^{l}(t+1) + \\delta_{n}^{l}(t)U_{n}^{l}(t)= (u_{n}^{l}(t) + \\delta_{n}^{l}(t)U_{n}^{l}(K^{l})$ (6)\n$\\qquad t=1$\n$U_{p}^{l+1} = \\sum_{t=1}^{K^{l+1}} d_{p}^{l+1}(t) \\delta_{n}^{l}(t)$(7)\nWe can observe that FSN n encodes membrane potentials\ninto spike trains, which FSN p then decodes back into con-\ntinuous values, effectively replacing the MAC in ANNs with\nAC in SNNs. In this process, $U_{n}^{l}$ and $U_{p}^{l+1}$ emulate the in-\nput and output of ReLU. To preserve ReLU's non-linearity\nwhile controlling the output range, ReLU is bounded by a.\nThis is formulated as three cases: \u25cf when $U_{n}^{l} \\leq 0$, $U_{p}^{l+1} = 0$;"}, {"title": "Method", "content": "We propose CREST, a spike-driven conjointly-trained\nframework exploiting the spatiotemporal dynamics. We es-\ntablish a dual-model conjoint learning rule to simplify the\nspatiotemporal BP which support efficient implementation\non different hardware types. Based on this, a fully spike-"}, {"title": "Multi-scale Spatiotemporal Event Integrator", "content": "For a continuous event-based period I, the raw stream E=\nei(xi, Yi, ti, Pi)ien with total N bipolar events pi at pixel\npoint (x, y) exhibits high sparsity and temporal redundancy.\nThese characteristics impede feature learning and increase\ncomputational overhead. Moreover, the raw stream contains\nmany redundant background and noise events, which can\ndisrupt the learning process. Existing methods mostly split\nI (E) into equal bins and integrate the events in each bin\nto obtain a spatiotemporal dense representation (Eq. 13) (Su\net al. 2023; Fan et al. 2024). The bin length affects the spa-\ntiotemporal feature preservation, leading to a trade-off be-\ntween both scales. We aim to integrate the input feature from\nmulti-scales (spatiotemporal scale ST, spatial scale S and\ntemporal scale T) for better representation.\n$I (E) \\Rightarrow I (DST)$ (13)\nSpatiotemporal continuous $I (DST)$. We observe that\nthe events generated by the same object exhibit both time\nand space continuity, whereas background and noise events\nare less time-continuous. Therefore, we leverage the spa-\ntiotemporal properties of the spiking neuron to extract and\ncluster the spatiotemporal continuous events and reduce re-\ndundant background and noise events. Like (Nagaraj, Liyan-\nagedera, and Roy 2023), we first divide I (E) into N equal-\nlength (At) time bins B(x, y, n) without polarity.\n$(n-1)At<e(ti)<nt$\n$B(x, y, n) =  \\sum_{i=1} e_{i} (x_{i}, y_{i}, t_{i})$\n(14)\nAn FSN-based convolution layer is constructed which ap-\nplies N fixed-value filters of size 3 \u00d7 3 to the N channel\ntime bins. In detail, the convolution results of each time bin\nand each filter are accumulated to the membrane potential\n\u00fb in each time step (Eq. 15). Then, based on Eq.2 and 3,\neach neuron generates a spike train (Eq. 16). With a given\ntime window K, if the number of spikes in one spike train is\ngreater than K \u2013 1, this neuron (pixel) is considered space-\ntime continuous and we set the value of this pixel as 1 (we"}, {"title": "Spatiotemporal IoU Loss", "content": "Bounding box regression loss plays a key role in object\ndetection. IoU, GIoU (Rezatofighi et al. 2019), and CIoU\n(Zheng et al. 2021) are widely used in frame-based object\ndetection and we show the loss function of YOLOv4 in Eq.\n21, where $L_{obj}$ is confidence loss, $L_{cls}$ is classification loss\nand $L_{reg}$ is bounding box regression loss. Current event-\nbased detection methods often directly apply these frame-\nbased approaches, which compute the coordinate differences\nbetween the predicted boxes and gtboxes. This neglects the\nunique spatiotemporal characteristics of event-based spike\ntrains, potentially leading to less precise regression.\n$L_{total} = L_{obj} + L_{cls} + L_{reg}$\n$L_{reg} = 1-CIOU$\n(21)\nAs previously noted, the events generated by the tar-\nget objects exhibit spatiotemporal continuity. This can be\nrepresented through the spike density p (total number of\nspikes within a box divided by its area). p around the ob-\nject is usually higher than around the background. We pro-\npose Spiking-IoU to quantify the difference in spike den-\nsity between the predicted B = (x, y, w, h) and ground truth\n$B_{gt} = (x_{gt}, y_{gt}, w_{gt}, h_{gt})$ boxes (Eq. 22).\n$Spiking-IoU = |\\rho - \\rho_{gt}| =  \\frac{\\sum_{m=1}^{M} \\delta_{m}}{w_{gt} h_{gt}} - \\frac{\\sum_{n=1}^{N} \\delta_{n}}{w h}$\n(22)\nFurthermore, we propose ST-IoU loss to fully exploit\nthe spatiotemporal feature in the event-based detection (Eq\n23). It incorporates the conventional IoU scheme to pro-\nvide accurate coordinate information for fast convergence\nand Spiking-IoU to enhance detection precision with unique\nspatiotemporal information. a and b are constant weights.\n$ST-IoU = a \u00d7 Spiking-IoU + b \u00d7 CIoU$\n$L_{reg} = 1 - ST-IoU$\n(23)"}, {"title": "Concluding Remarks", "content": "We propose CREST, an innovative conjointly-trained spike-\ndriven framework (incorporate MESTOR, ST-IoU loss, and\nFSN-SNN model) tailored for high accuracy and energy-\neffcient event-based object detection. With up to 100\u00d7\nimprovement in energy efficiency over SOTA SNN algo-\nrithms, CREST offers a groundbreaking solution for ad-\nvanced event-based object detection systems."}, {"title": "A. LIF Neuron and Its Variants", "content": "Leaky Integrate-and-Fire (LIF) neuron (Abbott 1999) and its\nvariants, including IF (Gerstner and Kistler 2002) and PLIF\n(Fang et al. 2021), are most commonly used in SNN algo-\nrithms and hardware implementations, due to their trade-off\nbetween low computational complexity and biological inter-\npretability.\nThese neuron mimic the membrane potential dynam-\nics and the spiking scheme, encoding a real value into a\nspike train (ST) to implement energy-saving computing. The\nlength of a spike train is called time window K.\nLIF neuron models. For LIF neuron, we consider the ver-\nsion of parameters dynamics that is discrete in time. LIF\nneuron n in layer 1 integrates all the input spikes $8^{l-1}(t)$\nfrom layer 1-1 and accumulates to the membrane potential\nu(t) which also leaks at each timestep t by a fixed factor \u03c4.\nA spike dh (t) is fired when the membrane potential exceeds\nthreshold $U_{th}$, and the membrane potential will be reset to\n$U_{reset}$. Denote the weight between neuron m and neuron n\nas $w_{mn}$ and a discrete form of the LIF neuron can be char-\nacterized as follows:\n$U_{n}(t) =  \\sum_{m=1}^{N^{l-1}}  w_{mn}om^{l-1} (t)$\n(27)\n$\\delta_{n}(t) = \\begin{cases}\n1,  U_{n}(t) \\geq U_{th}\n\\\\0,  otherwise\n\\end{cases}$\n(28)\n$u_{decay} (t) = U_{n}(t) + (U_{n}(t) + U_{reset} -U_{n}(t)) / \\tau$ (29)\n$u_{n}(t+1) =  \\begin{cases}\nU_{reset},  \\delta_{n}(t) = 0\n\\\\u_{decay}(t),  otherwise\n\\end{cases}$\n(30)\nPLIF neuron models. Parametric Leaky Integrate-and-\nFire (PLIF) spiking neuron model have a similar function\nas LIF model. Futhermore, its membrane time constant \u03c4\nis optimized automatically during training, rather than being\nset as a hyperparameter manually before training, increasing\nthe heterogeneity of neurons. Specifically, PILF replaced \u03c4\nwith 1/k(a) in Eq. 29. Moreover, a is a trainable parameter.\nk(a) = 1/(1 + exp(-a))\n(31)\n$u_{decay}(t) = U_{n}(t) + k(a) (U_{n}(t) + U_{reset} - U_{n}(t))$ (32)\nIF neuron models. IF model not only remove the poten-\ntial leaky item in Eq. 29, but also set $U_{reset} = 0$ to simplify\nthe computation process.\n$u_{decay} (t) = U_{n}(t)$\n(33)\n$\\qquad u_{n} (t + 1) = \\begin{cases}\nU_{decay}(t),  \\delta_{n}(t) = 0\n\\\\0, otherwise\n\\end{cases}$\n(34)"}, {"title": "B. Choice between ReLU and Leaky-ReLU", "content": "ANN-based object detection mostly adopts ReLU and leaky-\nReLU (Glorot, Bordes, and Bengio 2011; Maas et al. 2013).\nLeaky-ReLU allows a small, positive gradient $\\beta_{neg}$ when the\ninput is negative, and the gradient \u1e9e is 1 when the input is\npositive. To emulate it with FSN (FSN-LReLU), we revise\nthe FSN spiking function as follows:"}, {"title": "C. Backpropagation Principles in DL-Net", "content": "As mentioned in the main paper, the working pattern of DL-\nNet is described by the following equation:\n$X_{n} =  \\sum_{m=1}^{N^{l-1}}  X_{m}^{l-1} w_{mn}^{l-1}$ (39)\n$x^{l} = clip(x, 0, X') =  \\begin{cases}\n0,  x^{l} < 0\n\\\\x^{l},  0 \\leq x^{l} \\leq X^{l}\n\\\\X^{l},  otherwise\n\\end{cases}$\n(40)\n$xp^{l} = round(  \\frac{x^{l}}{X_{min}} ) \u00d7 X_{min}^{l}$\n(41)\nTaking the object recognition task as an example, the\nbackpropagation principles in DL-Net are illustrated as be-\nlow. For the last layer L, the index of the neuron with the\nlargest membrane potential corresponds to the predicted out-\nput label. The loss function L is defined as:\n$L = -  \\sum_{n=1}^{N^{L}} ynlog\\sigma(x_{n}^{L})$ (42)\nwhere yn is the one-hot target label, NL is the number of\nneurons in layer L, $x_{n}^{L}$ denotes the membrane potential of\nneuron n in L and \u03c3(.) is the Softmax function.\nThe gradient of L to $x_{n}^{L}$ can be represented as:\n$\\frac{\\partial L}{\\partial x_{n}^{L}} =  \\frac{\\partial L}{\\partial \\sigma(x_{n}^{L})}  \\frac{\\partial \\sigma(x_{n}^{L})}{\\partial x_{n}^{L}} = \\sigma(x_{n}^{L}) \u2013 yn$ (43)\nThen, from Eq. 9,10,11, the gradient of $w_{mn}^{L}$ can be\nobtained as:\n$\\frac{\\partial L}{\\partial w_{mn}^{L}} = \\frac{\\partial L}{\\partial x_{n}^{L}} \\frac{\\partial x_{n}^{L}}{\\partial x_{m}^{L-1}} \\frac{\\partial x_{m}^{L-1}}{\\partial w_{mn}^{L}} =  \\varepsilon^{L}  \\frac{\\partial x_{n}^{L}}{\\partial x_{m}^{L-1}}  \\frac{\\partial x_{m}^{L-1}}{\\partial w_{mn}^{L}} =  \\varepsilon^{L} x_{m}^{L-1}$ (44)\nNote that $\\frac{\\partial x_{m}^{L-1}}{\\partial w_{mn}^{L}}$ is estimated to be 1 in DL-Net, the error\npassed from neuron n in layer L to neuron m in layer L \u20131\nis denoted as:\n$L=1 = x  x1x1NL\\\\EE n1NL\\\\EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE1$ (45)\nAnd from Eq. 10, the gradient of $x^{L-1}$ to $x^{L-1}$ can be\nobtained as:\n12 12()rect(1)(46)\nThen, the gradient of wom can be denoted as:111111333333333333333333333333333411111111(47)\nIn this way, the gradient of weights in other layers can be\ncalculated similarly."}, {"title": "D. Datasets details", "content": "NCAR dataset. The NCAR dataset (Sironi et al. 2018) are\nchoosed for event-based object recognition task, which com-\nprising 12,336 car samples and 11,693 background samples.\nEach sample has a duration of 100 ms and exhibits varying\nspatial dimensions.\nGen1 dataset. The Gen1 dataset (de Tournemire et al.\n2020) is a large-scale dataset widely used for event-based\nobject detection. It consists of 39 hours of recordings with\nmore than 228k car and 28k pedestrian annotations in driv-\ning scenarios. Bounding box labels for cars and pedestrians\nwithin the recordings are provided at frequencies between 1\nto 4Hz."}]}