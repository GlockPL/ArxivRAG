{"title": "AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative Decoding", "authors": ["Zikun Li", "Zhuofu Chen", "Remi Delacourt", "Gabriele Oliaro", "Zeyu Wang", "Qinghan Chen", "Shuhuai Lin", "April Yang", "Zhihao Zhang", "Zhuoming Chen", "Sean Lai", "Xupeng Miao", "Zhihao Jia"], "abstract": "This paper introduces AdaServe, the first LLM serving system to support SLO customization through fine-grained speculative decoding. AdaServe leverages the logits of a draft model to predict the speculative accuracy of tokens and employs a theoretically optimal algorithm to construct token trees for verification. To accommodate diverse SLO requirements without compromising throughput, AdaServe employs a speculation-and-selection scheme that first constructs candidate token trees for each request and then dynamically selects tokens to meet individual SLO constraints while optimizing throughput. Comprehensive evaluations demonstrate that AdaServe achieves up to 73% higher SLO attainment and 74% higher goodput compared to state-of-the-art systems. These results underscore AdaServe 's potential to enhance the efficiency and adaptability of LLM deployments across varied application scenarios.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have significantly advanced the capabilities of various natural language processing tasks, including chatbots, machine translation, virtual assistants, and code generation. These applications inherently involve diverse Service Level Objectives (SLOs). For example, in the context of chatbots, LLMs are employed to generate responses at a rate faster than human reading speeds (i.e., 10 tokens per second) [5, 24, 43]. Integrating LLMs into web search services requires a significantly tighter decoding latency, as users are more likely to abandon the search if the results take too long [3, 4]. In more complex application scenarios relying on LLM APIs, inference requests may need to be completed as soon as possible to avoid blocking the execution of other dependent system modules [18,23].\nTo improve the overall throughput of LLM serving, existing systems use continuous batching, which adds new requests and removes completed requests in the current batch at the beginning of each serving iteration [41]. This iteration-level batching approach improves LLM serving throughput and GPU utilization. However, it cannot support requests with customized SLOs since the decoding computation of all requests are batched together in each iteration, achieving identical time-per-output-token (TPOT) for all requests. As a result, existing LLM serving systems either do not offer SLO guarantees by employing a best-effort approach to serving requests (e.g., OpenAI's APIs [30]) or assume a uniform SLO constraint across all requests assigned to each serving pipeline.\nThis paper introduces AdaServe, a system for SLO-customized LLM serving. The key idea behind AdaServe is to leverage fine-grained speculative decoding techniques to enable customized SLOs for individual requests while supporting continuous batching to maintain high throughput. To serve a batch of requests with diverse SLOs, AdaServe performs speculative decoding [8, 20] and uses a draft model to predict multiple output tokens for each request. Based on the SLO of individual requests, it dynamically selects a subset of speculated tokens for verification, which enables AdaServe to maximally attain the request SLOs while achieving high serving throughput.\nAlthough speculative decoding (SD) has been employed in existing systems [6, 13, 27] to reduce LLM serving latency, AdaServe is the first attempt that leverages SD for SLO-customized serving and must address two key challenges. Next, we discuss these challenges and the techniques AdaServe uses to overcome them.\nFirst, existing speculative decoding methods generally use a draft model to predict multiple output tokens and verify their correctness in parallel using the LLM. This approach improves the TPOT for individual requests, while also reducing the overall serving throughput due to the cost of speculation and verifying incorrectly speculated tokens. To address this challenge, AdaServe uses the logits of the draft model to approximate the probability that each speculated token can be verified by the LLM. Using these probabilities as input, AdaServe introduces a theoretically optimal algorithm to construct token trees by maximizing the expected number of tokens verified by the LLM. To make this algorithm work in practice and minimize the run-time overhead of speculation, AdaServe employs a practical adaptation of the algorithm to account for real-world constraints and uncertainties.\nThe second challenge AdaServe must address is SLO customization. Existing speculative decoding methods generally employ a static approach to optimize speculative performance and apply a uniform strategy across all batched requests. This coarse-grained approach does not promote SLO customization at the request level. To address this issue, AdaServe introduces a fine-grained speculative decoding pipeline consisting of four stages, including speculation, SLO-customized selection, throughput-optimized selection, and verification. First, during the speculation stage, AdaServe constructs a candidate token tree for each request in the batch, containing tokens with high probabilities to be verified by the LLM. Second, in SLO-customized token selection, AdaServe calculates the minimum number of tokens each request must accept to fulfill its TPOT SLO and selects enough tokens from the candidate token tree to achieve this goal. Next, in throughput-optimized token selection, AdaServe selects additional tokens to fully utilize the remaining budget, thereby enhancing throughput. Together, these two selection steps dynamically construct the token trees in a fine-grained manner to attain customized SLOs while achieving high throughput. Finally, the draft token trees are sent to the LLM for verification.\nEvaluation. We conduct evaluations to compare AdaServe with state-of-the-art LLM serving systems and various serving strategies for serving requests from diverse services and applications. The results demonstrate that AdaServe consistently outperforms all baselines. In end-to-end comparisons, AdaServe achieves up to 73% higher SLO attainment and 74% higher goodput than the best baseline systems. Additionally, our evaluations show that AdaServe can maintain a 95% SLO attainment when serving requests with TPOT SLO requirements that are 20% stricter than the baseline latency which represents the profiled lower limit for serving with continuous batching alone.\nThe rest of the paper is organized as follows: Section 2 provides the background on speculative decoding and key SLO-aware inference serving concepts. Section 3 formulates the SLO-customized serving problem. Section 4 details the core speculative scheduling algorithms. Section 5 describes the system design and implementation. Section 6 presents our evaluation results. Section 7 discusses the related works."}, {"title": "2 Background", "content": "LLM serving. Most of today's LLMs are based on the Transformer architecture and generate output tokens in an auto-regressive manner, where each output token depends on both the input prompt and all previously generated tokens. LLM serving includes two stages: prefilling, where all tokens in the input prompt are processed in parallel to initialize the key-value (KV) cache and produce the first output token, and decoding, where each newly generated token is fed back into the model to generate the next token.\nThe quality of LLM service is evaluated by two key SLOs: time to first token (TTFT) and time per output token (TPOT). TTFT captures the latency for generating the first output token after a request is received, while TPOT sets an upper bound on the average latency for generating subsequent tokens. We focus on TPOT in this paper, and our techniques are orthogonal and can be combined with different request-level scheduling techniques to maximally attain TTFT.\nDifferent LLM applications have varying SLO requirements. For instance, code completion tools require strict TPOT SLOs to ensure generated programs appear seamlessly during typing pauses. In contrast, chatbots can tolerate more relaxed SLOs when generating responses to match human reading speed. However, existing LLM serving systems rely on continuous batching to improve serving throughput and cannot effectively support SLO customization.\nSpeculative decoding (SD). Speculative decoding accelerates LLM inference by using a draft model to predict multiple tokens, which are verified together by the LLM in a single decoding iteration [8, 20, 26, 34, 37]. SD allows the LLM to opportunistically handle multiple tokens in a single step and therefore reduces the end-to-end latency of LLM decoding.\nTo improve the coverage of speculated tokens and maximize performance, recent work introduces tree-based speculative decoding: all speculated tokens are organized in a tree structure where each node represents a sequence of tokens identified by traversing the tree from the root to the current node [6, 10, 27, 34]. Tree-based SD allows the LLM to verify more tokens in each decoding iteration and improves overall efficiency."}, {"title": "3 Problem Formulation", "content": "The definition of hardware budget. In auto-regressive LLM inference, decoding each output token requires accessing all model parameters, resulting in very low arithmetic intensity. For example, serving Transformer architectures in half-precision floating points performs two floating point operations for each parameter loaded from VRAM [7]. However, modern GPUs such as NVIDIA's A100s and H100s [28] require significantly higher compute-to-bandwidth ratios to achieve high performance, as shown in Table 1. Both continuous batching and speculative decoding alleviate this issue by fusing the computation of multiple tokens in a single decoding step, thus increasing arithmetic intensity.\nTo quantify hardware utilization, we introduce the concept of budget, which represents the ideal number of tokens to process in a single forward pass in order to fully utilize both the compute resources and the memory bandwidth of modern GPUs.\nThe SLO-customized serving problem. With the definition of budget established, we now formulate the SLO-customized serving problem. In each decoding iteration, given the budget and a batch of requests, the goal of SLO-customized serving is twofold: (1) to meet the various TPOT SLO requirements of different requests in the batch and (2) to maximize the number of tokens accepted by the LLM during verification.\nFormally, given a batch of n requests, denoted as {r\u2081, ..., r\u2099}, and a hardware budget B, the goal is to construct n token trees {T\u2081, ..., T\u2099} for these requests to maximize the expected number of accepted tokens for one decoding iteration, which is expressed as: $\\mathbb{E}[\\sum_{i=1}^{n} acc(T_i)] = \\sum_{i=1}^{n}\\mathbb{E}[acc(T_i)]$, where acc(T) is a random variable denoting the number of accepted tokens in T by the LLM verification. This optimization is subject to the following constraints:\n1. Budget constraint: The total number of nodes across all token trees must not exceed the hardware budget:\n$\\sum_{i=1}^{n} |T_i| \\le B$                                                     (1)\nwhere $|T_i|$ is the number of tokens in the i-th token tree.\n2. TPOT constraint: For each request $r_i$, the expected number of accepted tokens must satisfy the TPOT requirement:\n$\\frac{l_i + t_{spec}}{o_i + acc(T_i)} \\le t_{TPOT}, \\forall i = 1, ..., n$                      (2)\nwhere $l_i$ denotes the current latency of request $r_i$ starting from the first decoding step, $o_i$ denotes the current number of tokens decoded in request $r_i$, $t_{spec}$ denotes the latency of a decoding iteration and, $t_{TPOT}$ denotes the TPOT SLO of request $r_i$.\nIntuitively, the budget constraint ensures that the computational intensity of LLM verification stays within the available budget, and the TPOT constraint ensures that the SLO requirements of the requests are satisfied after the current decoding iteration. For each request $r_i$, we can rewrite the TPOT constraint as: $acc(T_i) \\ge \\frac{l_i + t_{spec}}{t_{TPOT}} - o_i$. To further simplify this constraint, we define $A(r_i) = \\frac{l_i + t_{spec}}{t_{TPOT}} - o_i$, which denotes the minimum number of tokens that must be accepted for the i-th request in the current decoding iteration to attain its TPOT SLO. With this definition, the TPOT constraint can be simplified as: $acc(T_i) \\ge A(r_i), \\forall i = 1, ..., n$. Since the values of the random variable acc(T) is not known during speculation, we relax the TPOT constraint by replacing acc(T\u1d62) with its expectation. The relaxed constraint is expressed as:\n$\\mathbb{E}[acc(T_i)] \\ge A(r_i), \\forall i = 1, ..., n$                                             (3)\nThis relaxation not only simplifies the constraint but also enables a more compact expression through the following decomposition of $\\mathbb{E}[acc(T_i)]$.\nTheorem 3.1 (Decomposition of the expected number of accepted tokens).\n$\\mathbb{E}[acc(T)] = \\sum_{v \\in T} f(v)$                              (4)\nwhere f(v) is the path probability of node $v \\in T$, defined as the probability in which the LLM accepts the path, which represents a sequence of tokens, from the root node to node v conditioned on the current token sequence of the request.\nAs proven in prior work [10,21], Theorem 3.1 allows us to rewrite the relaxed TPOT constraint as:\n$\\sum_{v \\in T} f(v) \\ge A(r_i), \\forall i = 1, ..., n$                              (5)"}, {"title": "4 SLO-Customized Serving", "content": "With the problem formulation in Section 3, this section introduces our approach to solving the SLO-customized serving problem. Section 4.1 introduces an algorithm that outputs a globally optimal solution to the problem. However, integrating this algorithm into real-world LLM serving systems requires addressing several key challenges. Section 4.2 outlines these challenges and describes AdaServe's techniques to overcome them. These techniques are integrated into a fine-grained speculative decoding pipeline in AdaServe, which is introduced in Section 4.3."}, {"title": "4.1 Optimal Token Tree Construction", "content": "We introduce an algorithm that discovers a globally optimal solution to the SLO-customized serving problem defined in Section 3 under the following assumption: the path probability f(v) for any node v in $T_{inf}(r)$ of request r is known during the construction of the token trees. Here, $T_{inf}(r)$ represents the |V|-ary infinite-depth token tree for request r, where |V| is the vocabulary size. Each node in $T_{inf}(r)$ represents a token, and the path from the root to a node v forms a sequence of tokens. The tree captures all possible output token sequences and their probabilities (i.e. f(v)) which are conditioned on the current token sequence of r.\nIn practice, this assumption does not always hold, and we address such cases in Section 4.2. Under this assumption, we introduce an iterative greedy algorithm to construct optimal token trees in two steps. In the first step, the algorithm iteratively inserts new nodes into each request's draft token tree (i.e., $T_i$) using the node with the highest f(v) values from $T_{inf}(r)$. This procedure is repeated until the TPOT constraints (Equation (5)) are satisfied for all requests. If the algorithm finds the TPOT SLOs cannot be attained for all requests simultaneously within the budget, it returns INVALID. In the second step, we use the remaining budget to select nodes with the highest f(v) values from the union of all $T_{inf}(r_i)$. Note that $T_{inf}(r_i)$ denotes the |V|-ary infinite-depth token tree for request $r_i$.\nAppendix A.1 shows that a node chosen greedily in this algorithm always connects to its parent, ensuring that the selected nodes by our algorithm form valid token trees. The pseudocode for this algorithm is presented in Algorithm 1, with the first step implemented in lines 7\u201315 and the second step in lines 16\u201321. An optimality proof for this algorithm is provided in Appendix A.2."}, {"title": "4.2 Challenges", "content": "Applying the optimal token tree construction algorithm requires addressing two key challenges. Next, we describe them and introduce AdaServe's techniques to address them.\nChallenge 1: unknown path probabilities f(v). Algorithm 1 assumes that the path probability f(v) for any node v \u2208 Ttotal is known during then construction of token trees. However, in practice, these probabilities cannot be determined beforehand, as they depend on the LLM to verify all speculated tokens in the token tree and calculate acceptance rates, a process that occurs only after the token tree construction.\nSolution: Our key insight is to leverage the logits of the draft model to approximate the path probabilities, implying that for all $v \\in T_{inf}(r_i)$:\n$\\prod_{u \\in Path(v)} M_q(u | X, Path(u.parent)) \\approx f(v)$           (7)\nwhere Mq denotes the draft model used for speculation which takes as input a sequence of tokens and outputs a probability distribution over the vocabulary, and Path(v) denotes the sequence of nodes from the root of the token tree to node v. This observation is supported by prior work [21]. Intuitively, the draft models used for speculation are generally trained using the same datasets and similar training procedures as the LLM, resulting in comparable natural language modeling performance. Moreover, recent studies [22, 44] show that draft models distilled from large models perform well in speculative decoding. Distillation ensures that the logits of the draft model align closely with those of the large model, making distilled models highly suitable for speculative decoding. Consequently, the logits of the draft model approximate the conditional acceptance probabilities of nodes in a token tree.\nChallenge 2: high speculation overhead. In speculative decoding, draft models generate output tokens in an auto-regressive fashion, introducing significant speculation overhead. In Algorithm 1, both steps rely on the GetTop operation, which selects the node with the highest path probability from one or multiple token trees. For a single token tree, a straightforward implementation of Get Top maintains a global candidate set containing all nodes whose parents have been processed by the draft model but which themselves have not yet been decoded. Each node in the candidate set is associated with its approximated path probability. The global candidate set is initialized with the root of a token tree and its approximated path probability (i.e., 1). Algorithm 1 iteratively selects the node with the highest path probability from the global candidate set and adds the node to the token tree for draft model decoding. Once a node is decoded, its child nodes, along with their approximated path probabilities, are added to the global candidate set. The implementation of Get Top for the second step functions similarly. However, this approach results in B - n decoding steps of the draft model, as B \u2013 n nodes (excluding the roots) are added to the draft token trees. Since a draft model decode occurs before each node is added, and B \u226b n, the overhead of draft model decoding becomes prohibitive."}, {"title": "4.3 SLO-Customized Speculative Decoding", "content": "Each decoding iteration of SLO-customized speculative decoding comprises four steps: speculation, SLO-customized selection, throughput-optimized selection, and verification. This section describes these steps in detail. The pseudocode for these steps is presented in Algorithm 2.\nStep 1: Speculation. First, the speculation phase employs a beam search algorithm to construct candidate token trees for each request, as shown in Figure 2. Initially, each request's candidate token tree contains only a root node, representing the last token in the generated text or the prompt if no text has been generated yet. The n root tokens for all requests are processed in parallel. The first decoding step of the draft model processes all root nodes and produces |V| potential child nodes for each root node. For each request, the w child nodes with the highest approximated path probabilities (i.e. $M_q(v|X, Path(v.parent))$) are selected and added to its candidate token tree. Starting from the second decoding step, the draft model processes in parallel all tokens selected in the previous step (n \u00d7 w tokens in total). For each request, we select w child nodes from all w \u00d7 |V| tokens generated by the draft model and use these tokens to expand the candidate token tree for the request.\nAfter d speculation steps, each request $r_i$ has a candidate token tree $T_{cand}(r_i)$ with a depth of d, where all layers except the first contain w nodes. An example is shown in Figure 2, where the draft model performs three decoding steps to construct candidate token trees with a depth of 3 and a beam width of 2. The parameters d and w are dynamically determined based on the system load (see Section 5).\nThe speculation phase is followed by two selection phases: the SLO-customized token selection and the throughput-optimized token selection.\nStep 2: SLO-customized token selection. In this step, requests add tokens from the candidate token tree to the draft token tree to meet their TPOT requirements. The TPOT constraint (Equation (5)) requires that the sum of the approximated path probabilities of all nodes in a token tree exceeds A(r), the minimum number of tokens that must be accepted for SLO attainment. However, A(r) may not always be achievable, as the number of accepted tokens is upper bounded by d + 1. If A(r) > d + 1, the TPOT SLO cannot be fully attained in the current iteration. In this case, the system sets the target threshold for SLO-customized selection to $A_{cap}(r) = min(A(r), d + 1)$, reflecting an attempt to meet the TPOT SLO with the best effort. For each request r, AdaServe computes Acap(r) and iteratively adds nodes from $T_{cand}(r_i)$ with the highest approximated path probabilities to T\u00a1 until the approximated path probabilities of all tokens in T; reaches or exceeds Acap(ri).\nFor example, as illustrated in the SLO-customized selection part of Figure 2, $A_{cap}(r_0) = 0.6$, so $t_1^{(0)}$ is added to $T_0$ to meet the TPOT SLO. For r\u2081, $t_2^{(1)}$ alone is insufficient to satisfy the constraint, so $t_3^{(1)}$ is also added, ensuring that the total approximated path probability exceeds $A_{cap}(r_1) = 0.8$. When the budget is insufficient to meet the TPOT SLO for all requests, AdaServe prioritizes slower requests by sorting them in the descending order of A(ri). However, challenges arise when satisfying Acap(ri) for request r\u012f requires adding a large amount of nodes with very low approximated path prob-abilities, resulting in diminishing returns and depleting the budget. In extreme cases, Acap(ri) may still be unattainable, causing all nodes in $T_{cand}(r_i)$ to be added to T\u2081, monopolizing the budget and harming overall performance. To address this issue, AdaServe imposes a limit nmax on the maximum number of tokens selected during the SLO-customized selection step. This constraint prevents over-committing resources to low-probability nodes, ensuring a balanced budget allocation across requests and improving overall efficiency.\nStep 3: Throughput-optimized selection. The first two steps maximally attain the SLOs for individual requests. In this step, AdaServe maximizes overall serving throughput by adding nodes with the highest approximated path probabilities from the candidate token trees of all requests to the draft token trees. This process repeats until the budget is exhausted. For example, in the throughput-optimized token selection step illustrated in Figure 2, the remaining budget is 3. Consequently, the tokens $t_3^{(0)}$ and $t_5^{(1)}$ are added sequentially to the token trees, as they have the highest approximated path probabilities among all remaining nodes.\nStep 4: Verification. In the final step, AdaServe sends the token trees for all requests to the LLM for verifying the correctness of all these tokens in parallel. AdaServe uses the tree-based verification method introduced in prior work [10, 20, 27, 34]."}, {"title": "5 System Design and Optimizations", "content": "5.1 Overview of AdaServe\nFigure 3 presents an overview of AdaServe, which consists of two primary components: the request manager and the execution engine. The request manager maintains a pool of active requests and incorporates an SLO-customized scheduler that implements SLO-customized speculative decoding."}, {"title": "5.2 System Optimizations", "content": "Dynamic control of depth and beam width in the speculation phase. Both the depth (d) and beam width (w) in the speculation phase contribute to the overhead of small model decoding, especially when d and w are large. In a serving system, the workload is constantly changing. If d and w are fixed, the system cannot effectively adapt to varying workloads. When the number of active requests is large, the average number of tokens that can be allocated to each request decreases. This results in draft token trees with smaller depth and width. In such scenarios, setting d and w to large values causes most tokens in the candidate token trees to be discarded, wasting computational resources. Conversely, when the system load is low, increasing the size of the draft token trees improves the number of accepted tokens. In these cases, fixed small values for d and w limit the size of the draft token trees, compromising performance. Thus, fixed settings for d and w are inadequate for handling varying workloads effectively. To address this issue, AdaServe dynamically adjusts d and w based on the number of active requests (n) using the following policy:\nd = max(1,min(Dmax, \u230aB/n\u230b))                                                     (8)\nw = max(1,min(Wmax, \u230aB/n\u230b))                                                     (9)\nwhere Dmax and Wmax are preset upper bounds for the depth and width of the candidate token tree for each request, respectively. As introduced in Section 3, B represents the optimal number of tokens that can be processed in a single forward pass. Exceeding this value causes the latency of the forward pass to increase rapidly, as the computation transitions from being memory-bound to compute-bound. In the speculation phase, the number of tokens in the batch during the second to d-th steps of small model decoding is w \u00b7 n. Therefore, for the width (w), the policy ensures that w \u00b7 n remains strictly less than B (as long as n < B), to avoid increased decoding latency. For the depth (d), it is set to \u230aB/n\u230b when this value is less than the preset maximum depth, Dmax. The value \u230aB/n\u230b serves as an upper bound for both the average number of tokens assigned to a request in the selection phase and the depth of any token tree under this averaged token budget. Decoding beyond this depth slows down all requests while providing minimal benefit.\nCUDAGraph optimizations. CUDAGraph [15] is a CUDA feature that captures the sequence of CUDA kernel executions and their dependencies in a computation graph. This graph can be reused to avoid the overhead of kernel launches and other repeated operations. However, for effective reuse, the shapes of both the kernels and the input parameters must match the saved graph. Additionally, the process of capturing a CUDAGraph introduces overhead, which must be amortized over multiple reuses to achieve meaningful acceleration. AdaServe leverages CUDAGraph to optimize small model decoding because the small model performs repetitive operations with the same kernel and parameter shapes. Specifically:\n\u2022 Within the same speculation iteration, the second to d-th decoding steps involve the same computation, where the number of tokens decoded per request equals to w and the number of requests equals to n.\n\u2022 Across speculation iterations with the same number of requests (n), the decode operations remain identical.\nThis consistency enables AdaServe to reuse captured CUDAGraphs for small model decoding multiple times, minimizing overhead. The CUDAGraphs are indexed based on w and the number of requests in the batch (n) to facilitate efficient storage and retrieval."}, {"title": "6 Evaluation", "content": "6.1 Experimental Setup\nImplementation and Device We implement AdaServe on top of FlexFlow Serve [17], a low-latency, high-performance LLM serving framework. To further optimize performance, we integrate the batched prefill kernel from FlashInfer [40], a library of high-performance kernels for LLM serving. This kernel is adapted for both the small model speculation steps and the LLM verification steps. For GPU communication, we leverage the custom AllReduce kernel from TensorRT-LLM [29]. Our evaluations are conducted on a node equipped with four NVIDIA A100 80GB GPUs, interconnected via NVLink. The node is powered by an AMD EPYC 7763 CPU with 64 cores and 128 threads, and has 256 GB of DRAM.\nModels. We evaluate AdaServe on the Llama-2-7b-chat, Llama-2-13b-chat, and Llama-2-70b-chat from the widely used Llama-2 family [36], whose transformer architecture is representative of modern LLMs. For the small model, we use Felladrin/Llama-160M-Chat-v1 [12], a Llama-like model with 160 million parameters. Both the large and small models are evaluated using half-precision floating point (FP16).\nBaselines. We compare AdaServe against state-of-the-art LLM inference systems, specifically vLLM [19] and Sarathi-Serve [2]. VLLM was the first system to introduce PagedAttention [19], which improves throughput by reducing memory fragmentation. Sarathi-Serve, on the other hand, employs chunked prefilling to batch the prefill and decoding stages across requests, optimizing hardware utilization. In addition to these systems, we implement two representative serving strategies within AdaServe as additional baselines. The first is a batched version of SpecInfer [27], using a default fixed tree expansion configuration of (1,1,3, 1, 1, 1, 1, 1). This baseline represents systems that adopt both continuous batching and speculative decoding but lack dynamic budget allocation and adaptive tree construction. The second is incremental decoding (IncrDec), which also employs continuous batching. IncrDec separates the prefill and decode stages, allowing new requests to preempt ongoing decoding during the prefill stage, similar to vLLM."}, {"title": "6.2 End-to-End Comparison", "content": "TPOT SLO Attainment and Goodput vs. Request Arrival Rate. In this evaluation, we gradually increase the request arrival rate and compare AdaServe 's TPOT SLO attainment and goodput to those of vLLM, Sarathi-Serve, SpecInfer, and Incremental Decoding. The workload consists of 60% first-category requests, 20% second-category requests, and 20% third-category requests. This configuration represents a peak load scenario for latency-critical tasks (category one), while workloads for categories two and three are lighter, allowing us to assess system performance under stringent task conditions.\nAs shown in Figure 4 and Figure 5, AdaServe consistently achieves higher SLO attainment and goodput across all models and request rates compared to the baselines. Specifically, AdaServe attains up to 1.63, 1.66, and 1.73 times higher SLO attainment and up to 1.51, 1.73, and 1.74 times higher goodput than the best baseline systems on Llama-2-7b-chat, Llama-2-13b-chat, and Llama-2-70b-chat, respectively.\nFor baselines without speculative decoding, per-token latency is primarily influenced by two factors. First, the total size of the KV cache in each batch impacts decoding latency. Larger KV caches require more time to load from the VRAM, leading to SLO failures. Second, how the prefill and decode stages are handled plays a significant role. Systems like vLLM and IncrDec allow new prefill requests to preempt ongoing batches, slowing down current decoding tasks. In contrast, Sarathi-Serve batches new prefill requests with ongoing decoding, avoiding preemption but slightly increasing forward-pass latency. The impact of these factors worsen as request rates increase. AdaServe mitigates these problems through fine-grained speculative decoding scheduling. By dynamically allocating budget based on TPOT SLOs and request speeds, AdaServe prioritizes urgent requests and accelerates those with tight SLOs. This dynamic budget allocation improves overall SLO attainment and goodput by ensuring that latency-critical requests are processed faster.\nWhile SpecInfer performs well under low request rates, particularly for Llama-2-70b-chat, its performance drops significantly as request rates increase. This decline occurs because SpecInfer lacks hardware utilization awareness. It underutilizes compute resources when batch sizes are small and overuses them when batch sizes are large, causing higher verification latency. In contrast, AdaServe optimally utilizes hardware resources, maintaining high efficiency even with large batch sizes. These results highlight the limitations of naive tree construction algorithms that fail to account for hardware utilization. For a more detailed comparison, Section 6.3 evaluates SLO-customized speculative decoding against advanced tree construction algorithms for speculative decoding.\nFinally, as shown in Figure 4, AdaServe 's SLO attainment also decreases with increasing request rates. This decline occurs because larger batch sizes reduce the average budget per request, limiting the effectiveness of speculative decoding. To address this, AdaServe dynamically reduces the depth (d) and beam width (w) of candidate trees, minimizing drafting overhead under high loads.\nTPOT SLO Attainment vs. Proportion of Stringent Requests. In this evaluation, we gradually increase the proportion of stringent requests while keeping the request arrival rate fixed (1.9 for Llama-2-7b-chat, 1.9 for Llama-2-13b-chat, and 1.0 for Llama-2-70b-chat) to compare AdaServe 's TPOT SLO attainment against the baseline systems under varying proportions of stringent requests.\nAs shown in Figure 6, AdaServe consistently outperforms all baseline systems, regardless of the proportion of requests with stringent TPOT SLOs. Specifically, AdaServe achieves nearly 100% SLO attainment across all proportions of stringent requests, while the baseline systems struggle to maintain high SLO attainment as the proportion of such requests increases. This disparity arises because vLLM, Sarathi-Serve, and IncrDec can only support stringent SLOs at relatively small batch sizes. However, in the trace, batch sizes are not always sufficiently small. With a fixed request arrival rate, the batch size during serving remains largely unchanged. Consequently, when batch sizes are large, a higher proportion of urgent requests leads to lower SLO attainment in these systems. SpecInfer, on the other hand, fails to handle all stringent requests due to its fixed tree construction, which cannot adapt to varying requirements. Stringent requests sometimes require deeper or wider draft token trees to meet their SLOs. As a result, a certain proportion of stringent requests remains unmet by SpecInfer, and thus, its SLO attainment declines as the proportion of stringent requests increases."}, {"title": "6.3 Ablation Study", "content": "Comparison with scheduling strategies based on continuous batching. This evaluation addresses the question: Can continuous batching-based scheduling", "batching": "first-come-first-serve (FCFS), shortest-time-to-attain (STTA), and SepPipe, all implemented in our system. FCFS and STTA are preemption-based strategies that select requests to run in each decoding iteration according to specific policies. FCFS, a widely adopted strategy in serving systems [19, 29, 42"}]}