{"title": "Revisiting Technical Bias Mitigation Strategies", "authors": ["Abdoul Jalil Djiberou Mahamadou", "Artem A. Trotsyuk"], "abstract": "Efforts to mitigate bias and enhance fairness in the artificial intelligence (AI) community have predominantly focused on technical solutions. While numerous reviews have addressed bias in Al, this review uniquely focuses on the practical limitations of technical solutions in healthcare settings, providing a structured analysis across five key dimensions affecting their real-world implementation: who defines bias and fairness; which mitigation strategy to use and prioritize among dozens that are inconsistent and incompatible; when in the Al development stages the solutions are most effective; for which populations; and the context in which the solutions are designed. We illustrate each limitation with empirical studies focusing on healthcare and biomedical applications. Moreover, we discuss how value-sensitive Al, a framework derived from technology design, can engage stakeholders and ensure that their values are embodied in bias and fairness mitigation solutions. Finally, we discuss areas that require further investigation and provide practical recommendations to address the limitations covered in the study.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is becoming integral to biomedical research and healthcare, revolutionizing multiple aspects of diagnosis, treatment, and patient care. Recent advances in the field, notably in generative Al (1,2), have significantly expanded Al capabilities, enabling personalized, precise, predictive, and portable healthcare delivery (3) including in low-resource settings where millions have inadequate access to essential healthcare services (4). Moreover, the potential economic impact of Al in healthcare is substantial, with estimates suggesting significant effects on healthcare spending (5). In many applications, particularly, medical image diagnosis, Al has outperformed human experts (3).\nDespite these advancements, Al can exhibit and perpetuate inherent social biases amplifying health inequities (6,7). This is especially concerning in healthcare, where biased Al decisions can directly impact patient outcomes and exacerbate existing health disparities. The stakes are high because, unlike other domains, errors in healthcare Al can have life-or-death consequences. Bias in computer systems refers to systems that \u201csystematically and unfairly discriminate[s] against certain individuals or groups of individuals in favor of others\u201d (8). Bias may originate from various sources and model development stages. Data biases, algorithmic biases, and user interaction biases are often the main sources of Al bias (9,10). In healthcare, these biases can manifest in the form of minority bias, label bias, and clinician and patient biases resulting from Al interactions (11). Importantly, the interconnection between the sources of bias can exacerbate the impact of bias within Al systems, leading to a compounding effect that makes the bias more pervasive and harder to detect and mitigate (10)."}, {"title": "1. Successful, but Limited: From Social Concepts to Algorithms", "content": "Technical bias mitigation solutions have successfully addressed racial bias in various clinical settings, such as in diagnosing burns from healthy skin, predicting mortality in intensive care units, forecasting healthcare expenditures, diagnosing diabetic retinopathy, classifying dementia, and predicting postpartum depression (22). However, empirical studies have revealed that despite substantial progress in Al research, translating these achievements into clinical settings remains challenging, with many Al investments stalling at the prototype level. For instance, studies have shown that approximately 22% of Al implementations demonstrated a direct impact on health outcomes, with the majority remaining in prototype testing phases. Among the remaining 78%,\nwhile Al models outperformed standard clinical modalities, they only indirectly influenced patient outcomes (35).\nTechnical bias mitigation solutions are often inspired by moral theories of justice and social sciences, such as Egalitarianism, Rawls' Principle of Equality, and Rawls' Minimax Principle (36\u201338). These techniques typically aim to achieve parity, maintain fairness in treatment or outcomes (38), or both (39). However, translating these principles into algorithms raises various ethical and technical challenges.\nTo understand the limitations, consider a simplified Al development pipeline consisting of design, implementation, and deployment phases. In the context of bias and fairness, the design phase involves making key algorithmic choices, such as defining what constitutes bias and fairness. After defining these concepts, the implementation phase seeks to translate them into algorithms and evaluate their effectiveness. This stage also connects to the Al training datasets, which are tied to the populations from which the data are collected. The deployment phase considers the real-world context in which these solutions are applied.\nThus, the conceptualization and formalization of bias and fairness, the effectiveness of the developed solutions, the Al training datasets, and the deployment environment all influence the success of technical approaches to bias mitigation. In the following, we discuss each limitation, extend them to the Al development stages in Table 1, and illustrate them with empirical studies, and solutions to address them."}, {"title": "1.1. Who Defines Bias and Fairness?", "content": "Al developers often dominate the design, implementation, and deployment of bias and fairness solutions. Critics argue that algorithms may reflect the political and ethical perspectives of their creators (40), carrying the creators' needs, values, and interests. This is concerning as the developers' interpretation of bias and fairness may conflict with those affected by the systems. Perceptions of fairness are influenced by factors such as algorithmic design, gender, age, education, Al literacy, Al transparency, and whether decisions are made by humans or machines (41). For example, seniority among future Al developers can lead to varied fairness interpretations (42). In healthcare, this issue is further complicated by medical interventions and patient outcomes (43). Further, while patients may prefer Al systems that do not consider protected characteristics like race or gender (41,44), clinicians might find such attributes clinically relevant and necessary for accurate diagnoses (43). Similarly, theoretical fairness may diverge from practical needs when existing fairness metrics do not align with individual preferences (45,46).\nAl developers may also prioritize the performance and generalizability of the systems they develop (47,48) over ethical values such as fairness and overlook the potential negative impacts of the systems (49,50). This misalignment between developer priorities and healthcare needs creates a critical paradox: while developers may prioritize model performance metrics, healthcare requires models that are both accurate and equitable across diverse patient populations. This tension is exacerbated by the fact that healthcare decisions often involve multiple stakeholders \u2013 patients, providers, insurers, and healthcare systems \u2013 each with potentially conflicting fairness definitions. For example, a model optimized purely for accuracy might recommend more expensive treatments that insurers resist covering, while patients and providers might prioritize treatment effectiveness regardless of cost. This is concerning when unbiased Al systems can improve performance (51), and overlooking bias may hinder this potential.\nTo mitigate this issue, a growing literature suggests diversity in Al developers (52) as the lack of diversity in the Al workforce can amplify Al-generated inequalities (53). While efforts are being made toward this goal, the Al workforce still lacks diversity. For instance, in 2021, Hispanic, Black, and African Americans account only for 3.2% and 2.4% of new Al PhDs in the US (53). Diversity can address the misalignment of interests between developers and end-users, particularly when perceptions vary based on cultural, social, and individual experiences. Diversity should not be restrained to the Al workforce and should extend to include the voices of those impacted by Al systems. These voices can provide feedback or participate in the solution development process, offering valuable perspectives on how the systems affect their lives and ensuring that the fairness criteria align with their lived experiences (25).\nVarious frameworks have been developed for value trade-offs among stakeholders. For instance, in the context of fairness, the Stakeholder's Agreement on Fairness framework (25) can facilitate these trade-offs through an iterative process emphasizing ongoing stakeholder agreements. The method incorporates various perspectives to challenge power imbalances and biases throughout the Al system's development. By continuously engaging stakeholders, Al developers can ensure that fairness is not viewed as a one-time achievement but rather as an ongoing negotiation that adapts to new challenges and insights. This process includes identifying, mitigating, and monitoring biases at each stage of development, allowing developers to balance competing fairness goals. Importantly, these trade-offs are explained transparently to users, acknowledging that perfect fairness is unattainable, but the aim is to improve fairness iteratively. From a clinician-patient standpoint, (54) developed a value-based framework to guide clinicians in incorporating patients' values, notably equity, access, and justice, in using Al in clinical care.\nAlongside the \u201cwho\u201d dimension, the choice and prioritization of bias and fairness measures are also significant limitations of technical solutions, which will be addressed in the following section."}, {"title": "1.2. Which Bias Mitigation Strategies to Use and Prioritize?", "content": "The conflicting views on bias and fairness have led to the development of dozens of technical mitigation strategies (21,55,56). This raises concerns about which mitigation strategy to use and prioritize and whether the measures should be statistical or causal (57). Statistical metrics aim to mitigate unfairness by minimizing correlations between protected characteristics, non-protected characteristics, and outcome variables in Al training data. Causal techniques, like counterfactual fairness (58), ask whether changing an individual's protected characteristic would alter the outcome. This approach can embody human-in-the-loop principles (59) and has recently been successfully implemented to quantify health outcome disparities in invasive methicillin-resistant staphylococcus aureus infection (60). The choice between statistical and causal approaches in healthcare is complex because health outcomes often involve complex causal relationships. For example, a statistical correlation between race and treatment outcomes might mask underlying socioeconomic factors, making causal approaches more appropriate for healthcare applications.\nHowever, causal approaches require more detailed data and stronger assumptions about the relationships between variables, which may not always be available or valid in clinical settings. Therefore, choosing between statistical and causal approaches requires careful consideration of the clinical context and the availability of reliable data.\nVarying bias and fairness formalization have also resulted in mathematically inconsistent and incompatible measures (57,61,62). Inconsistency occurs when the metrics produce conflicting results and incompatibility, often known as the Fairness Impossibility Theorem (61,62) when the metrics cannot be used simultaneously. These foundational properties raise concerns about which metrics to use at the implementation stage of Al development. When metrics produce conflicting results, several issues emerge: the decision-making becomes more complex, the trust in Al can be eroded, and prioritizing one metric over another can favor certain groups. Moreover, different stakeholders might view the system as unfair as discussed earlier, which can lead to skepticism and resistance. Conflicting metrics can also be problematic by affecting compliance with legal, regulatory, and ethical standards.\nStrategies for choosing between bias mitigation solutions have also been explored. Makhoulf et al. defined eleven selection criteria including the availability of ground truth in Al training data, the cost of classification, and existing regulations and standards (63). In mental health applications, Sogancioglu et al. found that reweighing and data augmentation techniques are best suited respectively for violence risk assessment and depression phenotype recognition for their properties to maintain fairness-accuracy tradeoffs (43). Foster et al. in the Fairness Tree defined three axes of selection depending on the nature of interventions: punitive, assistive, and resource-constrained settings (64). For instance, the authors suggest using group-level recall in resource-constrained settings where healthcare systems suffer. Deciding between statistical or causal techniques can be addressed using statistical metrics when bias is evident in measurable patterns, such as imbalanced data distributions, unequal error rates, or disparities across demographic groups. If the bias stems from complex cause-effect relationships or hidden confounders that are not captured by simple statistical patterns, a causal approach is more appropriate. To address metrics compatibility, Al developers can use the Maximal Fairness theorem (65) to determine sets of compatible metrics. However, these techniques remain techno-centric and should account for stakeholders' perspectives in the selection process (64).\nOnce bias and fairness measures are selected, determining when they are most effective is another challenge, which will be discussed in the following section."}, {"title": "1.3. When Are the Bias Mitigation Strategies Most Effective?", "content": "The effectiveness of bias and fairness metrics depends on varying factors including the tasks, models, choice of protected characteristics, and types of metrics (66). There is an ongoing debate about when the measures are most effective in Al model development (52). Bias and fairness interventions can be categorized into pre-processing, in-processing, and post-processing (22). Pre-processing techniques address bias before model training by removing protected characteristics, resampling data, or adjusting labels. These methods aim to create fairer training datasets that reduce the likelihood of biased outcomes later. In-processing techniques allow for\nfairness adjustments to be built directly into the model's structure. They can be explicit or implicit (39). Explicit methods enhance fairness by adjusting the loss function or applying fairness-aware algorithms (39), this includes Counterfactual Fairness. Implicit unfairness mitigation refers to algorithms that detect and address bias by adjusting the data representations learned during the training process, mainly used in deep learning models. Examples of implicit techniques include Adversarial Debasing (67). Importantly, these techniques can be used when protected characteristics are unavailable in the training data (39). Post-processing techniques focus on adjusting model outputs after training or modifying decision thresholds (22). This approach is often used when it is difficult to modify the training process or when fairness needs to be improved in already deployed models.\nWhile each stage offers valuable approaches, their effectiveness varies depending on the problem being addressed. Pre-processing is critical when data bias is the main issue, in-processing is ideal for models requiring fairness integration, and post-processing is useful for fine-tuning models already in use. However, balancing fairness across these stages can be complex, and selecting the right technique requires careful consideration of the specific Al application and ethical trade-offs (68). Empirical evidence from healthcare implementation studies suggests that the effectiveness of bias mitigation strategies varies significantly across development stages. Studies have shown that Al applications require careful planning and strategies that transform both care services and operations to realize benefits (69). Moreover, assessing Al's value proposition must extend beyond technical performance and cost considerations to include an analysis of real-world care contexts (70).\nIn healthcare settings, the timing of bias mitigation is critical due to the dynamic nature of medical data and evolving clinical guidelines. Pre-processing approaches may need regular updates to reflect changing population demographics, while post-processing methods might require adjustment as clinical standards evolve. For instance, a model trained on historical data might need continuous pre-processing updates for changing disease patterns or treatment protocols. Additionally, the choice of timing must consider the operational constraints of healthcare systems, where model retraining might need to be balanced against the need for continuous availability of Al systems for clinical decision support.\nPopulation descriptors highly influence the effectiveness of technical bias mitigation solutions and their formalization by Al developers is becoming a growing concern."}, {"title": "1.4. For Which Populations Are the Metrics Designed?", "content": "Bias and fairness measures often address fairness for individuals and groups. Individual-oriented techniques ensure that similar individuals receive similar outcomes. Group-oriented methods, on the other hand, ensure that similar outcomes are achieved across groups. Improving fairness for individuals might benefit certain subgroups but may not preserve fairness at the group level (71). However, group and individual fairness metrics can complement when properly defined and applied (72). Besides this apparent conflict, concerns arise about the availability of protected characteristics, their formalization, and intersectionality as discussed in the following."}, {"title": "1.4.1. Availability And Formalization of Protected Characteristics", "content": "Group-oriented bias mitigation strategies require protected characteristics for the models to be \"aware\" of potential biases (71). However, in practice, these characteristics may be unavailable due to missing data or regulatory restrictions (73\u201376). For instance, most health insurers do not collect them (77). When such characteristics are available, there are often preferences against using them in Al development to ensure fairness (41). The healthcare context presents challenges for protected characteristics because certain demographic factors may have clinical relevance. This creates a tension between the need to consider these factors for accurate diagnosis and treatment, and the goal of preventing discriminatory outcomes (78). For example, genetic predispositions to certain conditions may correlate with racial categories (73) yet using race as a proxy for genetic variation risks perpetuating harmful stereotypes and may miss important individual variations. Healthcare Al systems must balance the clinical utility of demographic information against the risk of reinforcing biased care patterns.\nIn response, group-blind fairness techniques have been developed to improve fairness when protected characteristics are either unavailable or not explicitly used. These techniques often focus on capturing proxies of protected characteristics, for instance, by treating data regions with high and low Al errors as distinct groups (79) and using correlations between non-protected features and outcomes (75,80). Nevertheless, these techniques can still suffer from the proxy problem (81), as they may unintentionally allow non-protected features that are correlated with sensitive attributes to influence the model's decisions (82).\nWhen protected characteristics are used, inconsistency in their formalization can be problematic (47). This is a growing concern in Al, with some arguing that \"the algorithmic fairness community is an emerging race-making institution\u201d (47). Others argued that using race and ethnicity as population descriptors is concerning as they are often social constructs (83) and can influence identity representation (84). For instance, categorizing individuals strictly by race or ethnicity can oversimplify complex identities and overlook important cultural nuances. To address these issues in genetics and genomic research, the National Academies of Sciences, Engineering, and Medicine developed a framework (85) for the use of population descriptors in biomedical research relevant to the context of algorithmic fairness. The framework emphasizes factors such as the data source, the scope of the data (individual-level and group-level), and the availability of pre-existing descriptors in the decision-making of population descriptors selection."}, {"title": "1.4.2. The Dilemma of Protected Characteristics Intersectionality", "content": "Traditional group fairness metrics typically address a single protected characteristic at a time. However, prioritizing fairness for one characteristic can lead to unfairness in others, highlighting a major limitation of group fairness metrics. For instance, increasing fairness for one characteristic may reduce fairness for others by up to 88.3% (86). This issue arises because individuals often possess multiple protected characteristics. Intersectionality is a problem not unique to algorithmic fairness but also identified in genomic research (87), and social sciences (88\u201390). The challenge for algorithmic fairness and Al design is determining which characteristics to prioritize and to what extent. In intensive care medicine, this selection should account for historical (e.g., racial), social disparities, and stakeholder dialogue (91). Various techniques have been developed to address intersectionality including metrics inspired by intersectionality frameworks from the Humanities (92,93). Other technical approaches include multi-calibration and multi-accuracy to address"}, {"title": "1.4.3. The Need for New Bias Mitigation Validation Datasets", "content": "The Al training, validation, and deployment datasets can affect the effectiveness of bias and fairness measures. Al developers typically rely on publicly and privately available datasets to validate the metrics. However, these datasets have notable limitations. Popular public datasets such as the Adult, German Credit Approval, and COMPAS datasets are noisy and include outdated racial categories (98). For example, the \u201cAsian Pacific Islanders\u201d racial category in the Adult dataset can obscure health disparities within these categories (99). These limitations highlight the need for more current bias and fairness training, validation, and deployment datasets. The All of US (100) program which aims to increase diversity in biomedical research datasets can be a candidate for such datasets. While private and commercial datasets might address some of the issues, they can also lead to problems with reproducibility and generalizability, especially in healthcare (101)."}, {"title": "1.5. Which Context Are the Metrics Designed For?", "content": "Al systems should be closely connected to the real-world scenarios in which they will be deployed because they are highly sensitive to the characteristics of the training data (42). A significant concern arises when there is a mismatch between the deployment data and the training data, which can lead to reduced model performance. In healthcare, for instance, Al models in the US are often trained using data from only a few states, such as California, Massachusetts, and New York (102). This can limit their effectiveness in other regions and make bias mitigation harder to implement. Bias and fairness measures that work in one context might be inadequate in another (103). For example, in glaucoma prediction models, Ravindranath et al. found variation in fairness and accuracy in different contexts (104). Schrouff et al. have developed techniques for auditing distribution shifts in medical contexts (105). Domain Adaptation and Domain Generalization techniques can also help models adapt to the differences between training and deployment environments (106,107). These techniques are particularly useful when protected characteristics are available only during training or deployment, or when these characteristics differ between the two stages (108,109). For example, race may be used in the training data but gender in the deployment data (110).\nAnother issue with current bias mitigation strategies is that they often rely on US anti-discrimination laws, which may not be applicable in other cultural contexts (47). Effective bias mitigation requires culturally sensitive approaches (111) that account for local factors such as literacy, education, language, and the rural-urban divide, as seen in the African healthcare context (112,113). It is also essential to align justice and social science theories, which inspire bias and fairness solutions, with local moral frameworks. For example, in non-Western societies like Africa, moral traditions that prioritize community needs over individual ones may be more appropriate (114)."}, {"title": "2. Addressing Al Bias and Fairness Collectively", "content": "There is increasing recognition that Al bias and fairness are not solely technical challenges, but socio-technical issues requiring collaborative approaches (24,27,31,77,115,116). Technologies must serve the values of end-users, a concept gaining traction across fields like technology design, political science, and social sciences (28). Several key frameworks and theories stress the importance of aligning technology with human values, focusing on usability, ethics, and the social contexts of users. Participatory Design, for example, as discussed by Schuler and Namioka (117), encourages involving end-users directly in the design process to ensure that the technologies reflect their values and social realities. Similarly, Value-Sensitive Design (VSD) promotes integrating moral and social values into technological design (118).\nParticipation is essential in ensuring that Al systems reflect the lived experiences and perspectives of end-users, rather than being the product of developers (29). This participatory approach is crucial for addressing Al bias and fairness issues (24,27,31,115,116). Human-centered AI (HCAI), in contrast to the biomedical community's engagement practices, advocates for broader stakeholder involvement in Al development. Chen et al. demonstrate how diverse healthcare stakeholders can collaborate to address Al bias at various stages of model development (116). Freedman et al. show how HCAI can help align a kidney exchange algorithm with human values (119).\nSimilarly, value-sensitive AI (VSAI), a subfield of HCAI, enhances technical solutions by integrating stakeholder values (30). The framework consists of three stages: conceptual, empirical, and technical (30). The conceptual stage involves identifying relevant stakeholders and understanding their context. The empirical and technical stages focus on gathering stakeholders' needs and values and embedding those values into the technology (here, technical bias solutions). This framework, developed for general-purpose Al can be adapted for addressing algorithmic bias collectively. We provide further recommendations later in the manuscript.\nVarious frameworks exist for identifying stakeholders in Al development at the conceptual stage. For instance, Miller's extension of the Stakeholder Salience Model uses four dimensions: power, legitimacy, urgency, and harm, to identify stakeholders' influence and relevance (120). The European Center for Not-for-Profit Law's framework for Meaningful Engagement offers similar strategies based on factors such as who is directly or indirectly impacted, who possesses subject matter expertise, and who has relevant experience (121). Overall, stakeholders can be categorized into three groups (122): individuals (users, engineers, researchers, non-users), organizations (technology companies), and national/international entities (lawmakers, regulators). Frameworks from community engagement in biomedical research, such as Community-Based Participatory Research (CBPR) (123), provide a basis for stakeholder identification and engagement in bias mitigation efforts. For example, in a recent study on Al-assisted cardiovascular screening, (124) employed the CBPR framework to engage clinicians and patients"}, {"title": "Conclusion", "content": "Al bias and fairness are critical concerns due to their potential to exacerbate and perpetuate health disparities. While technical solutions have been developed to tackle these issues, they face several limitations. This includes determining who defines bias and fairness, choosing and prioritizing appropriate metrics that may be inconsistent and incompatible, identifying the most effective timing for applying these metrics, specifying the target populations for which they apply, and considering the relevant context. Participatory Al provides a way to address these limitations by involving key stakeholders throughout different stages of the Al development lifecycle. We briefly discussed how the VSAI framework can be adapted to address bias collectively. However, this approach still faces challenges and remains theoretical. Ultimately, stakeholder engagement should not be seen as an alternative to technical solutions, but as a necessary complement that enriches the bias and fairness metrics and decision-making processes in Al systems. Future research should explore the validity of HCAI methods such as VSAI in real-world settings and the development of standardized frameworks that integrate technical and socio-ethical considerations. Additionally, investigating the role of emerging technologies like explainable Al and federated learning could provide new avenues for enhancing fairness in Al systems."}]}