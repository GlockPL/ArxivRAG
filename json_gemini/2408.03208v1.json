{"title": "Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery", "authors": ["Jialang Xu", "Jiacheng Wang", "Lequan Yu", "Danail Stoyanov", "Yueming Jin", "Evangelos B. Mazomenos"], "abstract": "Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% loU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models will be released at", "sections": [{"title": "I. INTRODUCTION", "content": "SURGICAL instrument segmentation (SIS) is a critical task in robot-assisted surgery, enabling computer-aided navigation for improving outcomes [1]. Deep learning models have shown promise in SIS [2]\u2013[4], but their effectiveness is influenced by the quantity and quality of available data [5]. Collaborative training, exploiting datasets from multiple sites, can boost the development and generalizability of SIS models. However, stringent privacy and confidentiality protocols make sharing surgical video recordings across sites impractical [6], [7]. Federated learning (FL) has witnessed success in collaborative training for medical image segmentation tasks, where a global model is learned across multiple sites without data sharing, thereby alleviating privacy concerns [8]\u2013[11]. FL methods though focus on a single global model, which is difficult to perform equally well for all sites, especially in surgical scenarios where different procedures, equipment, and patient characteristics introduce high heterogeneity.\nPersonalized federated learning (PFL) overcomes this limitation by training multiple site-specific models rather than a single global model to better represent the data distribution of each site [12]. Typically PFL methods divide the architecture into global and personalized layers, with the latter composed of prediction heads [13], batch normalization [14], convolution channels [15], or query embeddings in self-attention [16]. Alternatives focus on the information of different sites, such as annotation inconsistency [16] and inter-site similarity [17]. Though showing promising performance in applications on various medical modalities, such as prostate magnetic resonance imaging segmentation [18], lung computed tomography diagnosis [19], and low-count positron emission tomography denoising [20], PFL for instrument segmentation from surgical videos remain under-explored. More importantly, we identify two vital visual trait priors in SIS: instrument shape similarity and surgical appearance discrepancy. Surgical instruments across different sites share a similar shape. For example, robotic instruments generally consist of three parts including"}, {"title": "II. RELATED WORK", "content": "Federated learning (FL) enables multiple sites to jointly train a global model without sharing data, thereby alleviating data privacy concerns [8]. Although FL has witnessed success in medical and surgical vision tasks via federated averaging [9], balanced weights sharing strategies [10], dynamic weight averaging [21], style transfer [22], and contrastive learning [11], these works focus on obtaining a single global model, which struggles to perform well across sites. This is prominent in surgical scenarios where different clinical settings, procedures, imaging systems, and patients introduce high heterogeneity in data distribution. In light of this limitation, personalized federated learning (PFL) is emerging as an appealing alternative. Unlike FL, which creates a single global model, PFL aims to produce individualized models that cater to the local distributions of each site's data to enhance performance with privacy protection [12]. Many PFL approaches decouple the model architecture into global and personalized layers, where the global parameters are aggregated and shared via FedAvg [8] at the server, while the personalized ones remain local. The global layer can be composed of prediction head layers [13], batch normalization layers [14], or parts of channels in convolutional layers [15]. Some methods explore and utilize knowledge among different sites. For instance, FedDP [16] employs the inconsistencies in ground-truth annotations from various sites to calibrate local training. The pFedLA [17] utilizes hypernetworks at the server, instead of distance metrics, to learn the similarities for all parameters between different sites. Unlike these methods, our novel approach focuses on separating global and personalized parameters following a multi-headed perspective. Moreover, it introduces shape similarity and appearance discrepancy priors in SIS for global and personalized parameters, respectively, thus providing site-independent shape and site-specific appearance information for each site.\nSurgical instrument segmentation (SIS) aims to identify and segment surgical instruments in intra-operative scenes. Convolutional neural networks (CNNs) and U-Net architectures have been widely used for this task. For example, TernausNet [4] employs a U-Net-based network with a pre-trained VGG-11 or VGG-16 backbone [23] to predict surgical instruments using a pixel-based segmentation approach. ISINett [3] introduces mask-based segmentation with Mask-RCNN [24] and a temporal consistency module for SIS. Recently, transformer-based methods have shown improved performance in SIS. STswinCL [25] modifies the Swin Transformer [26] with space-time shift and contrastive learning. MATIS [27] incorporates long-term temporal information using MViT [28] with Mask2Former [29]. However, the limited size of surgical datasets often results in models with insufficient generalization performance [30]. More recent works [30], [31] utilize vision foundation models like SAM [32] and CLIP [33] to enhance model generalization capabilities, through additional prompting or the incorporation of a language modality. Interestingly, previous work does not consider the potential benefits of leveraging visual trait priors from multiple datasets. We fill this gap and introduce PFedSIS, a novel PFL method based on shape similarity and appearance discrepancy priors for SIS, leveraging diverse datasets from multiple clinical sites to address data scarcity and privacy concerns.\nHypernetworks are deep neural networks tasked with generating the weights of a primary network, based on the input embedding they receive [34]. Hypernetworks are widely used in many fields, such as language modeling [35], meta-learning [36], continual learning [37], few-shot learning [38], personalized image generation [39], and multi-task learning [40]. In PFL, hypernetworks can generate weights to dynamically adapt model parameters based on site-specific embeddings, facilitating better personalization without compromising privacy. Shamsian et al. [34] is the first to introduce hypernetworks into FL. They proposed a hypernetwork"}, {"title": "III. METHODOLOGY", "content": "1) Problem Formulation: We consider M sites and their unique datasets \\({D_m\\}_{m=1}^M\\), with each site connected to the server but only accessing its local dataset. The model at the m-th site has distinctive personalized parameters \\(\\theta_m^P\\) and shared global parameters \\(\\theta^G\\). The overall objective of PFL is:\n\\\n\\(\\{ \\theta^G, \\theta_1^P, \\theta_2^P, ..., \\theta_M^P\\} = \\arg \\min_{\\{\\theta^G, \\theta_m^P\\}_{m=1}^M} \\sum_{m=1}^M k_m L^m(\\theta^G, \\theta_m^P; D_m)\\)\n\\\nwhere balancing weight \\(k_m\\) is typically set as \\(k_m = \\frac{|D_m|}{\\sum_{m=1}^M |D_m|}\\), \\(|D_m|\\) is the sample number of m-th local dataset, \\(L^m(\\theta^G, \\theta_m^P; D_m)\\) denotes the loss function of site m.\n2) Overview of PFedSIS: Fig. 1(a) illustrates the PFedSIS architecture, consisting of a transformer-based PVTv2 [41] encoder and a FPN [42] convolutional decoder with an appearance regulation head \\(H_{ar}\\). The proposed GPD decouples the encoder and decoder in personalized and global parameters. The APE comprises appearance regulation and hypernetwork-guided update, while the SGE involves cross-style consistency and shape-similarity update. Considering site m from the total M sites, the pseudocode of PFedSIS is presented in Algorithm 1, and its workflow is detailed as follows:\nEach site uploads the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of its local dataset \\(D_m\\) to formulate a style-memory set \\(\\{(\\mu^i, \\sigma^i)\\}_{i=1}^M\\), and site m downloads it only once to generate a style-distorted dataset \\(\\hat{D}_m\\) via cross-style consistency.\nDisentanglement of the model's architecture into personalized and global \\(\\theta^G, \\theta^P\\) via GPD.\nInput selection in pairs of original images and corresponding style-distorted images, from \\(D_m\\) and \\(\\hat{D}_m\\).\nLocal training via cross-entropy segmentation loss \\(L_{seg}\\), cross-style shape consistency loss \\(L_{csc}\\), and appearance regulation loss \\(L_{ar}\\). We prioritize \\(\\theta_m^P\\) on the appearance information specific to site m, while \\(\\theta^G\\) focuses on acquiring shape-similarity features across sites.\nSite m uploads the change of personalized parameters \\(\\Delta \\theta_m^P\\) and sensitivity \\(\\Omega_{SS}^m\\) to the server.\nServer updates the layer-wise aggregation matrix \\(\\Omega_{HN}^M\\) of site m, with the corresponding hypernetwork \\(H_{N_m}(v_m; \\phi_m)\\) according to \\(\\Delta \\theta_m^P\\), and updates personalized parameters \\(\\theta_m^P\\) based on \\(\\Omega_{HN}^M\\).\nServer computes the shape-similarity matrix \\(\\tilde{\\Omega}_{SS}\\) based on the collection \\(\\{\\Omega_{SS}^m\\}_{m=1}^M\\) uploaded by all sites, and updates global parameters \\(\\theta^G\\) based on \\(\\tilde{\\Omega}_{SS}\\).\nSite m downloads \\(\\{\\theta^G, \\theta_m^P\\}\\) and returns to step 3 for the next local training."}, {"title": "B. Global-Personalized Disentanglement (GPD)", "content": "To attain global shape similarity among instruments while mitigating appearance heterogeneity across sites, we propose a GPD strategy, depicted in Fig. 1(a). As multiple embedding heads capture different representation subspaces [43], GPD"}, {"title": "C. Appearance-regulation Personalized Enhancement (APE)", "content": "1) Appearance regulation: The diversity in background, color, texture, camera settings, and anatomical locations in robot-assisted surgery leads to substantial variations in appearance across different sites. We thus develop a constraint in the personalized parameters \\(\\theta^P\\), to intensify attention on the distinctive appearance characteristics of the local dataset. Specifically, we use an appearance regulation head \\(H_{ar}\\) after the decoder to perform image reconstruction from the personalized features \\(f^P\\), defined as:\n\\\n\\(L_{ar}(I) = ||H_{ar}(f^P) - I||_2\\)\n\\\nwhere I is the input images and \\(||\\cdot||_2\\) is squared L2 norm."}, {"title": "D. Shape-similarity Global Enhancement (SGE)", "content": "1) Cross-style shape consistency: To maintain robust shape representation against varied surgical scenes for the global parameters \\(\\theta^G\\), we propose cross-style shape consistency, which makes \\(\\theta^G\\) segment the instrument under the style perturbation of multiple sites. More specifically, as style is linked to the mean and standard deviation [44]\u2013[46], for site m, we first get the inter-site style statistics \\((\\mu_{cross}^m, \\nu_{cross}^m)\\) by mixing the mean and standard deviation of all sites:\n\\\n\\(\\mu_{cross}^m = \\sum_{i=1}^M\\eta_i \\mu^i; \\nu_{cross}^m = \\sum_{i=1}^M\\eta_i \\sigma^i\\)\n\\\nwhere \\(\\mu^i\\) and \\(\\sigma^i\\) are the mean and standard deviation of the i-th site from the style-memory set \\(\\{(\\mu^i, \\sigma^i)\\}_{i=1}^M\\), representing the characteristic of each site. \\(\\{\\eta_i\\}_{i=1}^M\\) are random convex values sampled from Beta(0.1,0.1). Note that \\(\\{(\\mu^i, \\sigma^i)\\}_{i=1}^M\\) are 2\\(\\times\\) M scalar values and recovery of images solely based on the mean and standard deviation is unfeasible [47], ensuring privacy protection.\nThen we apply \\((\\mu_{cross}^m, \\nu_{cross}^m)\\) to distort the original input I and adopt segmentation ground truth Y of I as the supervising signal for local training to enforce shape consistency on the style-distorted input \\(\\hat{I}\\) at the prediction level. Formally, the cross-style shape consistency is defined as:\n\\\n\\(\\hat{I} = \\frac{I - \\mu_m}{\\sigma_m} \\nu_{cross}^m + \\mu_{cross}^m\\)\n\\\n\\(L_{csc}(\\hat{I}) = ||S(\\hat{I}) - Y||\\)\n\\\nwhere \\(I \\in D_m\\), with \\(D_m\\) representing the dataset of site m, \\(S(\\hat{I})\\) is the segmentation prediction for \\(\\hat{I}\\) from m-th site global parameters \\(\\theta^G\\).\n2) Shape-similarity update: To share and preserve shape knowledge among different sites, we access the sensitivity of shape-related predictions (i.e. segmentation predictions) to global parameter perturbations in the model, generating a shape-similarity matrix \\(\\Omega_{SS}\\) at the server to aggregate and update global parameters from all sites. Segmentation predictions contain crucial information about the shape of surgical instruments, which should be preserved during the aggregation of global parameters. Inspired by the neural plasticity mechanisms in the human brain that regulate memory retention and forgetting [48], [49], we estimate how sensitive segmentation predictions are to changes in global parameters to guide the global parameter aggregation at the server. Specifically, for the m-th site with a total of N images, it can be formulated as follows:\n\\\n\\(\\Omega_{SS}^m = \\frac{1}{N} \\sum_{i=1}^N ||\\nabla_{\\theta_m^G} S(\\hat{I}_i)||_2\\)\n\\\nwhere input \\(I_i \\in D_m\\), \\(\\theta_m^G\\) represents the global parameters of site m, \\(\\nabla_{\\theta_m^G} ||S(\\hat{I}_i)||_2\\) is the gradient of the squared L2 norm of \\(S(\\hat{I}_i)\\). The sensitivity \\(\\Omega_{SS}^m\\) shows the extent to which a minor perturbation in \\(\\theta_m^G\\) would alter the shape-related segmentation prediction \\(S(\\hat{I}_i)\\), thus during the aggregation of global parameters, it is beneficial to retain parameters with higher \\(\\Omega_{SS}^m\\), while updating the parameters with smaller \\(\\Omega_{SS}^m\\) by incorporating contributions from other sites as they slightly affect shape knowledge. Therefore, on the server side, we collect \\(\\{\\Omega_{SS}^m\\}_{i=1}^M\\) from all M sites and compute the shape-similarity matrix \\(\\tilde{\\Omega}_{SS}\\) by applying the Softmax normalization among \\(\\{\\Omega_{SS}^m\\}_{i=1}^M\\):\n\\\n\\(\\tilde{\\Omega}_{SS} = Softmax\\{\\Omega_{SS}^m\\}_{i=1}^M = \\{\\tilde{\\Omega}_{SS}^m\\}_{i=1}^M\\)\n\\\nThe global parameters \\(\\theta^G\\) for all sites are then updated at the server by:\n\\\n\\(\\theta^G_{all} = \\theta^G_{all} * \\tilde{\\Omega}_{SS} = \\sum_{i=1}^M \\theta^G_{i} \\tilde{\\Omega}_{SS}^m\\)\n\\\nwhere \\(\\theta^G_{all} = \\{\\theta_{i}^G\\}_{i=1}^M\\) is the set of global parameters uploaded by all M sites after the local training."}, {"title": "IV. EXPERIMENTS", "content": "Datasets: We use three open-source instrument segmentation datasets of robotic-assisted surgery, to benchmark PFedSIS and evaluate its performance against state-of-the-art FL algorithms. We include EndoVis 2017 [50], EndoVis"}, {"title": "V. CONCLUSIONS", "content": "This paper presents PFedSIS, a novel PFL method for SIS, capitalizing on visual trait priors of appearance disparity and instrument shape similarity. It first decouples the embedding heads of MSA and channels of convolution layers into personalized and global via GPD. Then, APE enhances personalized parameters tailored to each site's appearance representation and utilizes inter-site appearance inconsistency via hypernetwork for personalized parameters update, while SGE enables global parameters to maintain shape information via cross-style consistency and effectively share mutual shape information via shape-similarity update. Experimental results on three publicly available datasets show that the proposed PFedSIS achieves superior performance in both quantitative assessment and visual interpretation, while its computation requirements during prediction remain low and comparable to other methods. Future work will involve collaborating with other surgical disciplines, such as ophthalmic surgery and neurosurgery, to develop personalized foundational models for surgical instrument segmentation."}]}