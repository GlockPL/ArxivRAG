{"title": "Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery", "authors": ["Jialang Xu", "Jiacheng Wang", "Lequan Yu", "Danail Stoyanov", "Yueming Jin", "Evangelos B. Mazomenos"], "abstract": "Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% loU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models will be released at", "sections": [{"title": "I. INTRODUCTION", "content": "SURGICAL instrument segmentation (SIS) is a critical task in robot-assisted surgery, enabling computer-aided navigation for improving outcomes [1]. Deep learning models have shown promise in SIS [2]\u2013[4], but their effectiveness is influenced by the quantity and quality of available data [5]. Collaborative training, exploiting datasets from multiple sites, can boost the development and generalizability of SIS models. However, stringent privacy and confidentiality protocols make sharing surgical video recordings across sites impractical [6], [7]. Federated learning (FL) has witnessed success in collaborative training for medical image segmentation tasks, where a global model is learned across multiple sites without data sharing, thereby alleviating privacy concerns [8]\u2013[11]. FL methods though focus on a single global model, which is difficult to perform equally well for all sites, especially in surgical scenarios where different procedures, equipment, and patient characteristics introduce high heterogeneity.\nPersonalized federated learning (PFL) overcomes this limitation by training multiple site-specific models rather than a single global model to better represent the data distribution of each site [12]. Typically PFL methods divide the architecture into global and personalized layers, with the latter composed of prediction heads [13], batch normalization [14], convolution channels [15], or query embeddings in self-attention [16]. Alternatives focus on the information of different sites, such as annotation inconsistency [16] and inter-site similarity [17]. Though showing promising performance in applications on various medical modalities, such as prostate magnetic resonance imaging segmentation [18], lung computed tomography diagnosis [19], and low-count positron emission tomography denoising [20], PFL for instrument segmentation from surgical videos remain under-explored. More importantly, we identify two vital visual trait priors in SIS: instrument shape similarity and surgical appearance discrepancy. Surgical instruments across different sites share a similar shape. For example, robotic instruments generally consist of three parts including"}, {"title": "II. RELATED WORK", "content": "Federated learning (FL) enables multiple sites to jointly train a global model without sharing data, thereby alleviating data privacy concerns [8]. Although FL has witnessed success in medical and surgical vision tasks via federated averaging [9], balanced weights sharing strategies [10], dynamic weight averaging [21], style transfer [22], and contrastive learning [11], these works focus on obtaining a single global model, which struggles to perform well across sites. This is prominent in surgical scenarios where different clinical settings, procedures, imaging systems, and patients introduce high heterogeneity in data distribution. In light of this limitation, personalized federated learning (PFL) is emerging as an appealing alternative. Unlike FL, which creates a single global model, PFL aims to produce individualized models that cater to the local distributions of each site's data to enhance performance with privacy protection [12]. Many"}, {"title": "A. Federated Learning", "content": "PFL approaches decouple the model architecture into global and personalized layers, where the global parameters are aggregated and shared via FedAvg [8] at the server, while the personalized ones remain local. The global layer can be composed of prediction head layers [13], batch normalization layers [14], or parts of channels in convolutional layers [15]. Some methods explore and utilize knowledge among different sites. For instance, FedDP [16] employs the inconsistencies in ground-truth annotations from various sites to calibrate local training. The pFedLA [17] utilizes hypernetworks at the server, instead of distance metrics, to learn the similarities for all parameters between different sites. Unlike these methods, our novel approach focuses on separating global and personalized parameters following a multi-headed perspective. Moreover, it introduces shape similarity and appearance discrepancy priors in SIS for global and personalized parameters, respectively, thus providing site-independent shape and site-specific appearance information for each site."}, {"title": "B. Surgical Instrument Segmentation", "content": "Surgical instrument segmentation (SIS) aims to identify and segment surgical instruments in intra-operative scenes. Convolutional neural networks (CNNs) and U-Net architectures have been widely used for this task. For example, TernausNet [4] employs a U-Net-based network with a pre-trained VGG-11 or VGG-16 backbone [23] to predict surgical instruments using a pixel-based segmentation approach. ISINett [3] introduces mask-based segmentation with Mask-RCNN [24] and a temporal consistency module for SIS. Recently, transformer-based methods have shown improved performance in SIS. STswinCL [25] modifies the Swin Transformer [26] with space-time shift and contrastive learning. MATIS [27] incorporates long-term temporal information using MViT [28] with Mask2Former [29]. However, the limited size of surgical datasets often results in models with insufficient generalization performance [30]. More recent works [30], [31] utilize vision foundation models like SAM [32] and CLIP [33] to enhance model generalization capabilities, through additional prompting or the incorporation of a language modality. Interestingly, previous work does not consider the potential benefits of leveraging visual trait priors from multiple datasets. We fill this gap and introduce PFedSIS, a novel PFL method based on shape similarity and appearance discrepancy priors for SIS, leveraging diverse datasets from multiple clinical sites to address data scarcity and privacy concerns."}, {"title": "C. Hypernetworks", "content": "Hypernetworks are deep neural networks tasked with generating the weights of a primary network, based on the input embedding they receive [34]. Hypernetworks are widely used in many fields, such as language modeling [35], meta-learning [36], continual learning [37], few-shot learning [38], personalized image generation [39], and multi-task learning [40]. In PFL, hypernetworks can generate weights to dynamically adapt model parameters based on site-specific embeddings, facilitating better personalization without compromising privacy. Shamsian et al. [34] is the first to introduce hypernetworks into FL. They proposed a hypernetwork"}, {"title": "III. METHODOLOGY", "content": "1) Problem Formulation: We consider M sites and their unique datasets {Dm}M_1, with each site connected to the server but only accessing its local dataset. The model at the m-th site has distinctive personalized parameters om and shared global parameters \u03b8g. The overall objective of PFL is:"}, {"title": "A. Overall Pipeline", "content": "2) Overview of PFedSIS: Fig. 1(a) illustrates the PFedSIS architecture, consisting of a transformer-based PVTv2 [41] encoder and a FPN [42] convolutional decoder with an appearance regulation head Har. The proposed GPD decouples the encoder and decoder in personalized and global parameters. The APE comprises appearance regulation and hypernetwork-guided update, while the SGE involves cross-style consistency and shape-similarity update. Considering site m from the total M sites, the pseudocode of PFedSIS is presented in Algorithm 1, and its workflow is detailed as follows:\n\u2460 Each site uploads the mean \u00b5 and standard deviation \u03c3of its local dataset D to formulate a style-memory set {(\u03bc\u00b2, \u03c3\u00b2)}M1, and site m downloads it only once to generate a style-distorted dataset \u00d4m via cross-style consistency.\n\u2461 Disentanglement of the model's architecture into personalized and global e via GPD.\n\u2462 Input selection in pairs of original images and corresponding style-distorted images, from Dm and \u00d4m.\n\u2463 Local training via cross-entropy segmentation loss Leg, cross-style shape consistency loss Lcsc, and appearance regulation loss Lar. We prioritize op on the appearance information specific to site m, while OG focuses on acquiring shape-similarity features across sites.\n\u2464 Site m uploads the change of personalized parameters \u2206\u03b8 and sensitivity \u03a9\u03c2 to the server.\n\u2465 Server updates the layer-wise aggregation matrix \u03a9 of site m, with the corresponding hypernetwork HN(1m; m) according to \u0394\u03b8m, and updates personalized parameters \u03b8 based on \u03a9\u039d.\n\u2466 Server computes the shape-similarity matrix \u1f6fss based on the collection {Oss}M1 uploaded by all sites, and updates global parameters OG based on \u03a9ss.\nSite m downloads {0,0} and returns to step 3 for the next local training."}, {"title": "B. Global-Personalized Disentanglement (GPD)", "content": "To attain global shape similarity among instruments while mitigating appearance heterogeneity across sites, we propose a GPD strategy, depicted in Fig. 1(a). As multiple embedding heads capture different representation subspaces [43], GPD"}, {"title": "C. Appearance-regulation Personalized Enhancement (APE)", "content": "1) Appearance regulation: The diversity in background, color, texture, camera settings, and anatomical locations in robot-assisted surgery leads to substantial variations in appearance across different sites. We thus develop a constraint in the personalized parameters op, to intensify attention on the distinctive appearance characteristics of the local dataset. Specifically, we use an appearance regulation head Har after the decoder to perform image reconstruction from the personalized features fp, defined as:"}, {"title": "D. Shape-similarity Global Enhancement (SGE)", "content": "1) Cross-style shape consistency: To maintain robust shape representation against varied surgical scenes for the global parameters OG, we propose cross-style shape consistency, which makes OG segment the instrument under the style perturbation of multiple sites. More specifically, as style is linked to the mean and standard deviation [44]\u2013[46], for site m, we first get the inter-site style statistics (Bross, Vross) by mixing the mean and standard deviation of all sites:"}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets: We use three open-source instrument segmentation datasets of robotic-assisted surgery, to benchmark PFedSIS and evaluate its performance against state-of-the-art FL algorithms. We include EndoVis 2017 [50], EndoVis"}, {"title": "A. Datasets and Evaluation Metrics", "content": "2) Metrics: We utilize four standard and widely used metrics to evaluate model performance: two region-based metrics, Dice and intersection-over-union (IoU), and two distance-based metrics, average symmetric surface distance (ASSD) and 95% Hausdorff distance (HD95). Higher Dice/IoU and lower HD95/ASSD represent better segmentation results. All metrics are calculated at the original resolution."}, {"title": "B. Implementation Details", "content": "All experiments are implemented in PyTorch on a Tesla V100 GPU. Taking into account the trade-off between efficiency and convergence, we empirically set the maximum communication round T to 200 and local training iterations to 100 during each round, with a batch size of 8. The AdamW optimizer is adopted with an initial learning rate of 5e-4. Input"}, {"title": "C. Comparison with State-of-the-Art", "content": "1) Quantitative Results: Table I lists results on all three sites. Our PFedSIS consistently achieves superior average performance on all metrics, with an increase of 1.51% in Dice score and 2.11% in IoU, along with a reduction of 2.79 in ASSD and 15.55 in HD95, compared to suboptimal methods. We prove that the observed improvements are sta- tistically significant and not a result of chance variations by conducting paired t-tests between PFedSIS and the second-best method for averaged Dice, IoU, ASSD, and HD95, yielding p-values of 1.52 \u00d7 10-3, 1.07 \u00d7 10-3, 8.05 \u00d7 10-3, and 9.39 \u00d7 10-3, respectively. In all cases p < 5 \u00d7 10-2, confirming that the improvements produced by the proposed PFedSIS are statistically significant. The model trained only on local data (\"Local Train\") yields good performance, benefiting from the strong Transformer baseline. Nonetheless, all FL methods surpass Local Train in average performance on all metrics, while Site-2 observes large improvements thanks to FL. Interestingly, other FL methods yield inferior Dice, IoU, and HD95 than Local Train on Site-3. This is because cross-site model communication and server fusion impede model learning due to the significant appearance difference in Site-3"}, {"title": "2) Qualitative Results", "content": "Our PFedSIS yields superior visualization results compared to other methods. For example, benefiting from personalizing each site's appearance distribution by APE, PFedSIS precisely segments the black borders (left yellow box, 1st row and right yellow box, 2nd row of Fig. 2(a)) and background objects (3rd row of Fig. 2(a)) at Site-1, as well as background objects at"}, {"title": "3) Efficiency Analysis", "content": "Table II shows the computation complexity of all methods. Notably, the appearance head Har (with 0.387K parameters) and hypernetwork (with 34.5K parameters) are only used in the training process, so PFedSIS does not introduce additional processing burden during inference,"}, {"title": "D. Ablation Study", "content": "1) Effectiveness of Key Components in PFedSIS: Ablation results on PFedSIS components are listed in Table III. Each proposed component enhances model performance and complements the others. Specifically, GPD brings +1.19% IoU and -11.81 HD95 performance gains. Introducing SGE and APE to GPD further gains +0.96% IoU, -5.56 HD95 and +1.01% IoU, -5.33 HD95 respectively. After removing all proposed modules (GPD, SGE, APE), PFedSIS becomes FedAvg, where all parameters are merged and averaged through the server. Fig. 3 visualizes the \u03a9\u0397\u039d heatmap of the appearance regulation head generated by different sites' hypernetworks. It can be observed that Site-1 and Site-2, which have smaller appearance discrepancies compared to Site-3, show a greater tendency to merge each other's personalized parameters. Additionally, each site's aggregation matrix has distinct weights compared to others. These observations underscore the ability of APE to learn appearance discrepancies across different sites."}, {"title": "2) Impact of Different Personalized Settings in GPD", "content": "We progressively personalize the model along the channel dimension of the FPN decoder and the dimension of the query, key, and value heads in the MSA to explore the efficacy of GPD. As shown in Table IV, decoupling the decoder parameters yields an increase of 0.26% in IoU and a reduction of 1.81 in HD95, compared to the baseline FedAvg method (1st row). Further personalizing the query, key, and value embedding heads leads to additional +0.93% IoU and -10 HD95 performance improvement, compared to only decoupling the decoder."}, {"title": "V. CONCLUSIONS", "content": "This paper presents PFedSIS, a novel PFL method for SIS, capitalizing on visual trait priors of appearance disparity and instrument shape similarity. It first decouples the embedding heads of MSA and channels of convolution layers into personalized and global via GPD. Then, APE enhances personalized parameters tailored to each site's appearance representation and utilizes inter-site appearance inconsistency via hypernetwork for personalized parameters update, while SGE enables global parameters to maintain shape information via cross-style consistency and effectively share mutual shape information via shape-similarity update. Experimental results on three publicly available datasets show that the proposed PFedSIS achieves superior performance in both quantitative assessment and visual interpretation, while its computation requirements during prediction remain low and comparable to other methods. Future work will involve collaborating with other surgical disciplines, such as ophthalmic surgery and neurosurgery, to develop personalized foundational models for surgical instrument segmentation."}]}