{"title": "MLScent: A tool for Anti-pattern detection in ML projects", "authors": ["Karthik Shivashankar", "Antonio Martini"], "abstract": "Machine learning (ML) codebases face unprece- dented challenges in maintaining code quality and sustainability as their complexity grows exponentially. While traditional code smell detection tools exist, they fail to address ML-specific issues that can significantly impact model performance, reproducibility, and maintainability.\nThis paper introduces MLScent, a novel static analysis tool that leverages sophisticated Abstract Syntax Tree (AST) analysis to detect anti-patterns and code smells specific to ML projects. MLScent implements 76 distinct detectors across major ML frameworks including TensorFlow (13 detectors), PyTorch (12 detectors), Scikit-learn (9 detectors), and Hugging Face (10 de- tectors), along with data science libraries like Pandas and NumPy (8 detectors each). The tool's architecture also integrates general ML smell detection (16 detectors), and specialized analysis for data preprocessing and model training workflows.\nOur evaluation demonstrates MLScent's effectiveness through both quantitative classification metrics and qualitative assessment via user studies feedback with ML practitioners. Results show high accuracy in identifying framework-specific anti-patterns, data handling issues, and general ML code smells across real- world projects.", "sections": [{"title": "I. INTRODUCTION", "content": "The software development landscape has undergone a dra- matic transformation with the integration of Machine Learning (ML). Recent statistics from Gartner highlight this shift, revealing a striking 270% increase in ML adoption within enterprise software projects over the last four years [1]. This growth spans multiple sectors, with healthcare leading the charge [2].\nThis rapid adoption, however, brings its own set of com- plexities. Traditional software development practices have had to evolve significantly to accommodate ML's unique require- ments, including the need for extensive datasets, sophisticated algorithms, and iterative development cycles [3]. These fun- damental differences have catalyzed a complete reimagining of software development methodologies, from initial design through testing and maintenance [4], [5] which is also high- lighted by Tang et al. [6] in their empirical study of ML systems refactoring and technical debt.\nML projects introduce distinct code quality challenges that set them apart from conventional software development. The complexity stems from their inherent characteristics: intricate mathematical operations, extensive data preprocessing require- ments, and sophisticated model architectures that challenge traditional code maintenance approaches [7]. The iterative nature of ML development, where models undergo continuous refinement based on performance metrics, often results in code that becomes increasingly difficult to maintain and understand. The consequences of subpar code quality in ML projects can be particularly severe. Beyond the usual software maintenance issues, poor code quality can compromise model performance, hinder result reproducibility, and create significant obstacles during production deployment [8]. O'Brien et al. [9] further elaborate on these challenges through their comprehensive study of self-admitted technical debt in ML software, iden- tifying 23 distinct patterns of technical debt specific to ML systems. These challenges can severely impact team collabora- tion and make model updates problematic as new data becomes available [10].\nML code smells represent unique indicators of potential system design or implementation issues that traditional met- rics might miss [11]. Zhang et al. [12] have extensively documented these ML-specific smells, which often manifest in ways distinct from conventional software development \u2013 such as the inappropriate use of default hyperparameters or improper data splitting techniques for training and testing.\nThe spectrum of ML-specific code smells extends to various critical areas, as documented by Oort et al. [13] in their study of code smell prevalence in ML projects. These include inadequate handling of data imbalances, lack of proper feature scaling, and the selection of inappropriate evaluation metrics [14]. Sculley et al. [7] further emphasize how these issues can lead to hidden technical debt in ML systems.\nWhile traditional code analysis tools excel in general software development, they often prove inadequate for ML codebases due to their lack of domain-specific understanding [15]. This limitation is particularly evident in the context of ML-specific issues, as demonstrated by Cruz et al. [16] in their empirical study of bad smell detection techniques. For instance, these tools might overlook critical ML-specific issues, such as the inappropriate use of classification accuracy metrics on imbalanced datasets [5].\nThis limitation in existing tools underscores the press- ing need for ML-specific code quality analysis solutions. Tsoukalas et al. [17] emphasize the importance of specialized tools that must comprehend ML-specific constructs, work- flows, and best practices [18], [19]. Alahdab and \u00c7al\u0131kl\u0131 [20] further support this through their empirical analysis of hidden"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Code smells, initially conceptualized by Kent Beck and popularized by Martin Fowler [21], have evolved beyond tradi- tional software development concerns to encompass machine learning-specific challenges.\nWhile traditional code smells focus on issues like duplicated code, oversized classes, and inappropriate coupling [22], [23], ML systems present unique quality challenges that existing detection tools like SonarQube [24] and PMD [25] fail to adequately address [13], [26]. These traditional tools, while effective for conventional software development [27], lack the capability to comprehend ML-specific contexts and the nuances of ML frameworks and libraries.\nML-specific code smells, as documented by Sculley et al. [28], [29], include data leakage issues, improper feature scal- ing, and inadequate cross-validation practices. These issues, identified through empirical studies by Tang et al. [6] and O'Brien et al. [9], can significantly impact model performance and reliability. Additional concerns include improper train-test split implementations, the use of hardcoded hyperparameters without proper tuning, and poor handling of imbalanced datasets, as highlighted by Takeuchi et al. [30] and Washizaki et al. [31].\nThe limitations of traditional code smell detection tools in understanding ML-specific contexts [16] emphasize the need for specialized quality assurance approaches in ML devel- opment, particularly as ML systems become more complex and widespread. This growing challenge, further elaborated by Tsoukalas et al. [17] and Alahdab and \u00c7al\u0131kl\u0131 [20], underscores the importance of developing new tools and methodologies specifically designed to address the unique quality concerns in ML software development."}, {"title": "A. Current State of ML Code Quality Tools", "content": "The evolution of code quality tools has primarily focused on traditional software development, leaving a significant gap in tools specifically designed for machine learning applica- tions. While several tools offer partial solutions for ML code quality assessment, they fall short of providing comprehensive coverage for the unique challenges posed by ML development.\nIn the current ML quality landscape, tools like MLlint [32] provide project-level quality analysis focusing on code, data, and configuration aspects. While valuable for general project assessment, it lacks the deep framework-specific analysis that MLScent offers. Data Linter [33] and Datalab [34] concentrate on dataset quality and auditing, addressing issues like noisy labels and outliers, but do not cover code-level anti-patterns specific to ML implementations.\nMore specialized tools like Deepchecks [35] focus on model validation and performance checks, while Great Expectations [36] emphasizes data validation and profiling. MLflow [37] excels in experiment tracking and model management but does not address code quality issues. Similarly, DVC [38] special- izes in version control for datasets and models, while Pandas Profiling [39] focuses solely on data analysis visualization.\nMLScent distinguishes itself from these existing solutions in several key ways:\nFramework Coverage: Unlike tools that focus on specific aspects (e.g., MLflow for experiment tracking or DVC for version control), MLScent provides comprehensive coverage across multiple ML frameworks, including PyTorch, Tensor- Flow, Scikit-learn, and Hugging Face libraries like Transform- ers.\nIntegrated Analysis: While tools like mllint [32] of- fer general project quality assessment, MLScent combines framework-specific analysis with general ML anti-pattern de- tection, providing a more complete quality assessment solu- tion.\nCode-Level Focus: Unlike data-centric tools such as Data Linter [33] or Deepchecks [35], MLScent specifically targets code-level issues, helping developers identify and fix imple- mentation problems that could affect model performance and maintainability.\nExtensibility: MLScent's architecture allows for easy inte- gration of new framework-specific detectors, making it more adaptable to the evolving ML ecosystem compared to more rigid tools like Pylint or framework-specific analyzers.\nThese existing solutions, while valuable for their specific purposes, demonstrate significant limitations in their scope and applicability. They typically address isolated aspects of ML development or focus on specific frameworks, failing to provide the comprehensive analysis required for modern ML projects. The ideal ML code quality tool would need to com- prehend multiple ML frameworks, detect framework-specific code smells, provide contextual improvement suggestions, and seamlessly integrate into existing ML development workflows requirements that MLScent specifically addresses.\nThe development of MLScent is motivated by several crit- ical gaps in the current landscape of ML code quality tools and research. A fundamental limitation identified by Kaur et al. [40] is the scarcity of ML-specific code smell detection tools, despite the abundance of such tools for traditional software development. While existing tools like DVC [38], MLflow [37], and Deepchecks [35] excel in specific aspects like version control, experiment tracking, and data validation, they fail to provide comprehensive coverage of ML code quality issues.\nThe challenges are further compounded by the fragmented coverage of ML frameworks, as highlighted by Chen et al. [37], and the shortage of high-quality, industry-relevant datasets for ML code smell detection [32]. Additionally, there exists a growing disconnect between established software"}, {"title": "III. MLSCENT: SYSTEM ARCHITECTURE AND DESIGN", "content": "MLScent employs a modular and extensible architecture designed specifically for detecting code smells in machine learning projects. As depicted in Figure 1, the system in- tegrates several key components that work in harmony to deliver comprehensive code quality analysis across various ML frameworks.\nThe architecture consists of a Code Parser utilizing the Abstract Syntax Tree (AST) generation, Framework-Specific and Hugging Face Smell Detectors for specialized analysis, a General ML Smell Detector for framework-agnostic issues, and a Report Generator that produces both human-readable and machine-parsable outputs.\nThe detectors in MLScent were built from scratch based on comprehensive analysis of ML anti-patterns and code smells documented in prior research. The smell definitions were derived from multiple sources, including Sculley et al.'s work on technical debt in ML systems [7], [9], Zhang et al.'s [12] catalog of ML-specific code smells, and van Oort et al.'s [13] study on ML code smell prevalence."}, {"title": "B. Framework-Specific Smell Detector", "content": "The Framework-Specific Smell Detector module in our library serves as a main component, focusing on identifying framework-specific code smells across popular ML libraries. Through AST analysis and pattern matching, this detector examines code for framework-specific anti-patterns and subop- timal structures. It implements specialized detection methods for frameworks like Pandas (checking for chain indexing is- sues), NumPy (identifying inefficient array operations), Scikit- learn (detecting cross-validation problems), and deep learn- ing frameworks such as TensorFlow and PyTorch (analyzing model training practices). The detector's modular design fa- cilitates easy extension to accommodate new frameworks and evolving best practices."}, {"title": "C. Hugging Face Smell Detector", "content": "The Hugging Face Smell Detector module in our library addresses the unique challenges in Natural Language Process- ing (NLP) projects using the Hugging Face library. This spe- cialized component focuses on critical aspects such as model versioning, tokenizer caching, pipeline component usage, and training argument configuration. Its implementation is partic- ularly valuable given the growing prominence of Hugging Face in NLP development, helping developers maintain high- quality, efficient, and reproducible NLP models."}, {"title": "D. General ML Smell Detector", "content": "The ML Smell Detector module identifies framework- agnostic code smells common across ML development. It examines crucial aspects including data handling and pre- processing, model training and evaluation practices, hyper- parameter management, and overall code organization. This detector ensures comprehensive coverage even for projects using custom or less common ML frameworks, providing valuable insights regardless of the specific tools employed."}, {"title": "E. Integration and Extensibility", "content": "MLScent's architecture emphasizes seamless integration and extensibility through its main orchestration class. This class coordinates file parsing, detector invocation, result aggrega- tion, and report generation. The system's modular design al- lows for straightforward extension through new detector imple- mentation, refinement of existing detectors, and enhancement of reporting capabilities. This flexibility ensures MLScent can adapt to the evolving landscape of ML development while maintaining its effectiveness in promoting code quality.The tool uses AST to identify which ML framework is being used, and then automatically applies the appropriate framework- specific detectors, rather than running all detectors unneces- sarily in the ML codebase or projects."}, {"title": "F. AST-Based Analysis Techniques", "content": "MLScent employs sophisticated Abstract Syntax Tree (AST) analysis techniques to detect code smells in machine learning projects. This approach provides a hierarchical rep- resentation of code structure, enabling efficient and accurate analysis of code patterns across various frameworks and libraries.\nThe AST-based analysis offers several key advantages in code smell detection. Its language-agnostic nature ensures con- sistent analysis across different Python versions, while contex- tual understanding capabilities enable accurate interpretation of code constructs. The efficient pattern matching through AST traversal allows for rapid identification of complex code patterns.\nHowever, the approach also faces certain limitations, includ- ing static analysis constraints and computational overhead with large codebases.\nTo address these challenges, MLScent combines AST analy- sis with heuristic-based detection and framework-specific rules for comprehensive code smell detection."}, {"title": "G. Use of Astroid Library", "content": "MLScent's implementation leverages the Astroid library as its primary tool for Python code parsing and analysis. This powerful library provides sophisticated AST functionality for deep code inspection and analysis.\nThe integration focuses on three key features: AST gener- ation for code structure analysis, rich node types for precise pattern identification, and an inference engine for understand- ing variable types and function return values.\nFor instance lets consider Pandas examples for Anti-pattern checker in the Astroid library:\na) Pandas: Detecting Chain Indexing: Chain indexing in Pandas can lead to unexpected behavior and performance issues. Here's how MLScent detects this smell:\nThis function checks for nested Subscript nodes, which indicate potential chain indexing like df['column']['row'].\nThese examples demonstrate how MLScent utilizes As- troid's AST parsing capabilities to perform sophisticated code AST and checking for specific patterns, MLScent can identify potential code smells that are particularly relevant to ML development practices."}, {"title": "IV. EVALUATION AND RESULTS", "content": "This section presents a comprehensive evaluation of MLS- cent, detailing the methodology used, the datasets analyzed, and both quantitative and qualitative results of our analysis."}, {"title": "A. Evaluation Methodology", "content": "To assess the effectiveness of MLScent, we employed a multi-faceted evaluation approach:\n1) Classifcation: To establish a reliable benchmark for evaluating our ML-specific code smell detection tool (MLScent), we collaborated with the developers primar- ily responsible for the projects listed in Table III. We asked each developer to annotate a randomly selected subset of code from their own projects, thus creating a \"ground truth\" dataset.\nBefore they began annotating, we provided the devel- opers with detailed information about the types of ML- specific code smells we were targeting. This ensured they were well-prepared to identify potential issues within their code. By comparing the code smells de- tected by MLScent with the annotations provided by the developers, we were able to calculate key performance metrics such as recall, as well as F1 and F2 scores, which provide a more balanced measure of accuracy.\n2) User Study: We conducted a small-scale user study with same ML practitioners in Table III to gather feedback on the tool's usability and the relevance of detected smells.\nFor each detected smell, we categorized it as true positive, false positive, or false negative based on manual verification by ML experts. This process allowed us to compute standard evaluation metrics and assess the tool's effectiveness across different ML frameworks and smell types."}, {"title": "B. Identifying the Prevalence of ML Anti-pattern in Real World Projects", "content": "Our empirical investigation began with a systematic search on GitHub using the keywords \"Machine Learning\" and \"Python\" to identify relevant repositories. To ensure research validity and quality of our dataset, we implemented the fol- lowing rigorous selection criteria:\n\u2022 Repositories explicitly focused on machine learning im- plementations\n\u2022 Projects with Python as the primary programming lan- guage\n\u2022 Repositories with a minimum threshold of 2,000 stars, indicating substantial community validation\n\u2022 Actively maintained codebases with recent commit activ- ity\nThis methodical filtering process yielded 43 high-quality ML projects, providing a diverse representation across frameworks and application domains. The dataset's heterogeneity, en- compassing various ML paradigms and implementation ap- proaches, served as an ideal testbed for evaluating MLScent's in finding different types of ML anti-pattern in these diverse and comprehensive dataset. Our selection strategy particularly emphasized projects demonstrating real-world applications, ensuring that our analysis would yield insights relevant to practical ML development scenarios."}, {"title": "C. Research Questions", "content": "This study aims to evaluate the effectiveness of MLScent in detecting various types of machine learning anti-patterns in Python codebases and understand their distribution across ML projects. Specifically, we investigate the following research questions:\n\u2022 How accurate is MLScent in identifying generic ML anti-patterns?\n\u2022 How accurate is MLScent in identifying data science and scientific computing anti-pattern, particularly in NumPy and Pandas implementations?\n\u2022 How accurate is MLScent in identifying framework- specific anti-pattern across PyTorch, Scikit-learn, Tensor- Flow, and Hugging Face?\n\u2022 What is the prevalence and distribution of different types of ML anti-patterns across ML specific Python projects?"}, {"title": "IV. OVERALL PERFORMANCE", "content": "The evaluation of MLScent involved both quantitative anal- ysis and qualitative feedback from experienced Python devel- opers. The study comprised 72 total ground truth entries and identified 39 unique anti-pattern or smell/checker types across 7 ML projects as shown Table III ."}, {"title": "a) Survey Participant's:", "content": "Table III presents repository information of our survey respondents. The participants repre- sented a diverse group of Python developers with experience ranging from 4 to 13 years (mean = 8.43 years, SD = 3.15). The repositories analyzed covered various domains within machine learning. This ensures a comprehensive evaluation of MLScent across different ML application domains and coding styles."}, {"title": "b) Agreement Analysis and classification metrics:", "content": "The agreement analysis results, presented in Table IV, demonstrate the reliability and effectiveness of MLScent in detecting ML- specific code smells. Key metrics include:\n\u2022 Overall agreement rate: 87.50%, indicating strong con- sensus between manual expert review and MLScent's automated detection\n\u2022 Recall: 0.875, showing high effectiveness in identifying relevant code smells\n\u2022 F1-score: 0.933, demonstrating excellent balance between precision and recall\n\u2022 F2-score: 0.897, emphasizing recall while maintaining high overall performance\nThese metrics suggest that MLScent achieves robust per- formance in identifying ML-specific code smells, with par- ticularly strong results in balancing false positives and false negatives. The high F1-score (0.933) indicates that the tool provides reliable detection capabilities while minimizing false alarms, making it practical for real-world applications.\nThe agreement analysis results are particularly noteworthy given the complexity and diversity of ML codebases analyzed. The high agreement rate (87.50%) suggests that MLScent's de- tection algorithms align well with expert judgment, validating its utility as an automated code quality assessment tool for ML projects."}, {"title": "c) Qualitative feedbacks:", "content": "Table V presents the results of survey conducted with 7 same participants from Table III, who evaluated the effectiveness of the proposed tool across three distinct categories: Tool Usefulness, Generic ML Smells, and Framework ML Smells. The survey employed a 4-point Likert scale, where participants rated each category on a scale from 1 (Strongly Disagree) to 4 (Strongly Agree). The three categories in the survey aim to assess different aspects of the tool's performance, with the following key distinctions:\n\u2022 Tool Usefulness: This metric evaluates the overall per- ceived utility of the tool by the participants. It reflects whether the users found the tool beneficial in their typical machine learning (ML) development workflows or not.\n\u2022 Generic ML Smells: This category measures the tool's effectiveness in detecting general machine learning is- sues, commonly referred to as \"ML smells.\" These smells are framework-agnostic and pertain to common problems in ML code, such as improper data handling and model evaluation practices.\n\u2022 Framework ML Smells: This metric captures the tool's ability to identify problems specific to various ML frame- works, such as TensorFlow, PyTorch, or Scikit-learn. Such smells may include improper use of framework- specific functions or inefficient practices within these libraries.\nThe results indicate that participants generally found the tool to be useful, with an average score of 3.29 (on a 4-point scale) for overall Tool Usefulness. The standard deviation of 0.76 suggests moderate variability in responses.\nFor the detection of Generic ML Smells, the tool performed slightly better, achieving a mean score of 3.43 and a lower standard deviation of 0.53. This indicates that the tool reliably identified common ML code smells across different projects and frameworks.\nThe tool's performance in detecting Framework ML Smells also received a mean score of 3.43, but with a higher standard deviation of 0.79. Similar to the Tool Usefulness metric, this suggests that while some participants rated the tool favorably in identifying framework-specific issues, others experienced less satisfactory results, as reflected by the minimum score of 2. This variability may be due to the participants' use of different ML frameworks, each with varying levels of tool support for framework-specific smells."}, {"title": "d) Evaluations Metrics:", "content": "Table VI presents a compre- hensive evaluation of MLScent's performance across five machine learning frameworks: General ML including some Sklearn anti-pattern, NumPy, Pandas, PyTorch, and Hugging Face Transformers. The analysis encompasses six key metrics that assess the tool's effectiveness in detecting code smells and anti-patterns. The distribution of detected issues varies significantly across frameworks. General ML demonstrated the highest number with 24 issues (33.33%), followed by Pandas with 20 issues (27.78%). NumPy exhibited the lowest count with 8 issues (11.11%), while PyTorch and Hugging Face identified 10 (13.89%) and 12 (16.67%) issues, respectively.\nThe agreement rate reflects the percentage of issues for which there was consensus among evaluators. Hugging Face exhibited a perfect agreement rate (100.00%), implying that all detected issues were agreed upon by the evaluators. In contrast, NumPy had the lowest agreement rate (75.00%), indicating some variability in the detection of issues within this framework.\nRecall measures the proportion of correct positive identi- fications made by the tool out of all relevant issues. Here, Hugging Face achieved a recall of 1.000, meaning that all relevant issues were successfully detected. NumPy, again, had the lowest recall (0.750), suggesting that some issues may have been missed.\nThe F1-score is the harmonic mean of precision and recall, providing a balance between the two. Hugging Face achieved the highest possible F1-score (1.000), while NumPy had an F1-score of 0.857, which, although lower, still indicates a generally good performance.\nThe F2-score places more emphasis on recall than precision. Hugging Face once again achieved a perfect score of 1.000, while NumPy and PyTorch showed slightly lower F2-scores (0.789 and 0.814, respectively), indicating that these frame- works may need further refinement in recall optimization.\nThese results demonstrate MLScent's robust capability in identifying framework-specific anti-patterns, particularly in General ML and Hugging Face implementations, while sug- gesting opportunities for refinement in NumPy-related detec- tion mechanisms."}, {"title": "1) RQ1: Generic ML Anti-Pattern Detection Accuracy:", "content": "Our analysis of MLScent's capability to identify framework- agnostic machine learning anti-patterns reveals compelling results. The tool demonstrated robust performance in detecting general ML and some Sklearn related issues, achieving an agreement rate of 87.50%. From the total detected issues, 33.33% were framework-agnostic, encompassing 24 distinct cases as shown in Table VI. The tool's effectiveness is further evidenced by its strong recall of 0.875 and F1-score of 0.933, indicating exceptional accuracy in identifying generic ML anti- patterns.\nThese generic anti-patterns include common issues such as improper model validation techniques, inadequate data prepro- cessing, and inefficient hyperparameter management. The high detection accuracy suggests that MLScent effectively identifies fundamental ML development pitfalls that could impact model performance and reliability across different implementation frameworks."}, {"title": "2) RQ2: Data Science Anti-Pattern Detection Accuracy:", "content": "The evaluation of MLScent's performance in identifying data science anti-patterns revealed varying effectiveness across dif- ferent libraries:\na) Pandas Analysis: MLScent exhibited exceptional ca- pability in detecting Pandas-related issues, identifying 20 distinct cases. The tool achieved an agreement rate of 90.00%, with Pandas-specific checks constituting 27.78% of total eval- uations. The high recall of 0.900 and F1-score of 0.947 demon- strate MLScent's proficiency in identifying data manipulation anti-patterns.\nThe tool particularly excelled in detecting issues related to inefficient DataFrame operations, memory-intensive chain operations, and suboptimal data transformation practices. This high accuracy is crucial given Pandas' central role in data preprocessing and feature engineering pipelines.\nb) NumPy Analysis: For NumPy-specific issues, MLS- cent's performance was moderate, detecting 8 issues with an agreement rate of 75.00%. NumPy-related checks represented 11.11% of total evaluations, with a recall of 0.750. The relatively lower F2-score of 0.789 suggests potential areas for improvement in detecting scientific computing anti-patterns.\nThe tool showed particular sensitivity to array manipulation inefficiencies, broadcasting errors, and memory-intensive op- erations. However, the lower performance metrics indicate that complex numerical computing patterns might require more sophisticated detection mechanisms."}, {"title": "3) RQ3: Framework-Specific Anti-Pattern Detection Accu- racy:", "content": "The analysis of framework-specific anti-pattern detec- tion revealed distinct patterns across different ML frameworks:\na) Hugging Face Framework: MLScent achieved opti- mal performance with Hugging Face, demonstrating 100.00% agreement rate across 11 detected issues. Despite representing only 15.28% of total checks, the perfect recall and F1-score indicate exceptional accuracy in identifying NLP-specific anti- patterns.\nThis outstanding performance encompasses detection of transformer model misconfigurations, tokenization inefficien- cies, and improper model loading practices. The perfect metrics suggest robust capability in handling complex NLP pipeline issues, particularly crucial for maintaining efficiency in large language model implementations.\nb) PyTorch Framework: For PyTorch, MLScent identi- fied 9 issues with an agreement rate of 77.78%. The frame- work accounted for 12.50% of total checks, achieving a recall of 0.778 and an F1-score of 0.875, indicating effective detection of deep learning-specific code smells."}, {"title": "D. RQ4: Prevalence and distribution of different types of ML anti-patterns", "content": "This question aims to understand the frequency and patterns of various anti-patterns in real-world ML codebases, providing insights into common problematic practices in the ML devel- opment ecosystem.\nOur analysis is based on a systematically curated dataset obtained through GitHub searches using keywords Machine Learning and Python. We implemented strict selection crite- ria, focusing on repositories explicitly dedicated to machine learning, using Python as the primary language, having at least 2,000 stars for community validation, and showing recent maintenance activity.\nThis methodical filtering process resulted in 43 high-quality ML projects, providing diverse representation across frame- works and application domains. The heterogeneous nature of our dataset, encompassing various ML paradigms and imple- mentation approaches, serves as an ideal testbed for evaluating MLScent's effectiveness in detecting different types of ML anti-patterns. Our selection strategy particularly emphasized projects demonstrating real-world applications, ensuring that our analysis would yield insights relevant to practical ML de- velopment scenarios and provide a comprehensive assessment of current ML development practices.\nNotably, the General ML related category, which encap- sulates framework-agnostic issues and some Sklearn issues, shows the highest count of detected issues, with 31,033 instances. This is likely due to the broad applicability of general ML practices across different projects, making them susceptible to common problems such as poor documentation, improper configuration, or suboptimal code structures.\nNumPy, a widely-used library for numerical computing, accounts for 17,634 issues, which is the second-highest among the frameworks. Given NumPy's central role in array ma- nipulation and scientific computation, this result is expected, as many of the detected code smells relate to array creation efficiency and axis specification.\nPandas, a popular data analysis library, has 2,273 reported issues, which are primarily related to inefficient data selection patterns such as chain indexing and column selection checks.\nTensorFlow and PyTorch, both deep learning frameworks, exhibit fewer issues, with 1,960 and 1,540 occurrences re- spectively. These frameworks tend to have more specialized code smells, such as deterministic algorithm usage and model evaluation checks, reflecting the complexity of deep learning workflows. Hugging Face, an NLP-focused framework, shows the lowest number of issues, with only 64 recorded instances, likely due to its relatively specialized use case."}, {"title": "a) Top 10 Most Common Code Smells and anti-patterns:", "content": "Table VIII highlights the most frequently occurring code smells and anti-patterns detected from 43 ML projects. Array creation efficiency emerges as the most prevalent issue, with 10,795 instances, underscoring the importance of optimizing array operations in ML code, particularly in frameworks like NumPy, where efficient handling of large datasets is critical.\nThe second most common smell/anti-pattern, Missing Axis Specification, with 5,996 occurrences, points to a frequent oversight in specifying axes for array operations, particularly in multidimensional data manipulations. This issue is common in both NumPy and Pandas.\nOther notable code smells/ anti-pattern include Randomness Control Checker (1,246 occurrences), which ensures that ran- dom number generation is properly controlled for reproducibil- ity, and Column Selection Checker (1,044 occurrences), which is particularly relevant in data processing pipelines where inefficient column selection can degrade performance.\nIssues related to documentation, such as Missing docstring for function: forward (694 occurrences), indicate a lack of adherence to best practices in documenting critical functions, especially in deep learning frameworks like PyTorch and TensorFlow. Additionally, the detection of Magic numbers (438 occurrences for the value 10), reflects instances where hardcoded values are used without explanation, reducing code readability and maintainability."}, {"title": "b) Framework-Specific Code Smells/ Anti-pattern:", "content": "The table IX presents a comprehensive overview of various an- tipatterns and smells identified across several popular machine learning libraries from our curated datasets, namely NumPy, TensorFlow, PyTorch, Pandas, and Hugging Face. It reveals critical patterns in implementation challenges and quality issues that impact scientific computing and machine learning workflows, as identified by Zhang et al. [12].\nNumPy, the fundamental library for numerical computing, demonstrates the highest frequency of code smells, primarily in array creation efficiency (10,795 occurrences) and missing axis specifications (5,996 occurrences). The high occurrence of these smells suggests potential performance implications for scientific computing applications, aligning with findings by Sculley et al. [28].\nDeep learning frameworks exhibit distinct patterns: Ten- sorFlow shows significant memory management concerns (459 occurrences) and logging issues (321 occurrences), while PyTorch struggles more with deterministic algorithm usage (589 occurrences) and model evaluation (343 occurrences). This observation supports research by Amershi et al. [3] on reproducibility challenges in deep learning frameworks and highlights the ongoing tension between performance optimiza- tion and code maintainability, as discussed by Sculley et al. [29].\nPandas, essential for data manipulation in machine learning pipelines, shows a high frequency of column selection issues (1,044 occurrences) and chain indexing problems (585 occur- rences). These findings correlate with work by van Oort et al. [13] on performance optimization in data processing pipelines, indicating potential bottlenecks in data preprocessing stages of machine learning workflows.\nHugging Face, though showing fewer code smells overall, exhibits issues primarily in data loading efficiency (19 occur- rences) and early stopping implementation (15 occurrences). This aligns with research by Takeuchi et al. [30] on best prac- tices in NLP model development and reflects the challenges in implementing robust natural language processing systems.\nThe distribution of code smells across these libraries, as detailed in Table IX, suggests that while each library has its specific challenges, common themes emerge around efficiency, reproducibility, and proper implementation of machine learn- ing best practices. These findings, supported by Tang et al. [6], highlight the need for improved development practices in machine learning systems, particularly in areas of performance optimization, reproducibility, and code quality assurance, as emphasized by Breck et al. [8].\nThis analysis provides valuable insights for both library maintainers and practitioners, pointing to specific areas where focused improvements could significantly enhance the qual- ity and reliability of machine learning implementations, as demonstrated by O'Brien et al. [9]."}, {"title": "V. DISCUSSION", "content": "Our comprehensive evaluation of MLScent provides signifi- cant insights into the state of code quality in machine learning projects and demonstrates the tool's effectiveness in addressing these challenges. The findings reveal both the accomplish- ments and limitations of automated ML code smell detection, while highlighting crucial areas for future development."}, {"title": "A. Key Findings and Impact", "content": "The analysis uncovered several critical patterns in ML code quality issues across different frameworks. Data leakage emerged as the most severe issue, particularly in Scikit- learn projects, aligning with van Oort et al. [13]'s findings regarding the prevalence of ML-specific code smells. This issue is especially concerning as it can lead to unreliable model performance and overly optimistic evaluation metrics, a problem also noted by Sculley et al. [28] in their analysis of technical debt in ML systems.\nThe frequency of inefficient data handling in Pandas-based implementations and reproducibility issues in deep learning frameworks mirrors observations by Zhang et al. [12] re- garding ML-specific code smells. The prevalent occurrence of improper model versioning in Hugging Face projects, as highlighted by Tang et al. [6], emphasizes the critical need for better version control practices in ML development.\nSupporting Di Nucci et al. [43]'s findings on the value of automated smell detection a tool's effectiveness in early detection of anti-patterns contributes to the standardization of better coding practices across the ML community, as suggested by Washizaki et al. [31]."}, {"title": "B. Current Limitations", "content": "Despite its effectiveness, MLScent faces several important constraints. As a static analysis tool, it cannot detect runtime- specific issues, a limitation acknowledged in similar tools studied by Cruz et al. [16"}]}