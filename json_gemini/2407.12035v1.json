{"title": "REPORTING RISKS IN AI-BASED ASSISTIVE TECHNOLOGY\nRESEARCH:\nA SYSTEMATIC REVIEW", "authors": ["ZAHRA AHMADI", "PETER R. LEWIS", "MAHADEO A. SUKHAI"], "abstract": "Artificial Intelligence (AI) is increasingly employed to enhance assistive technologies, yet it can fail\nin various ways. We conducted a systematic literature review of research into AI-based assistive\ntechnology for persons with visual impairments. Our study shows that most proposed technologies\nwith a testable prototype have not been evaluated in a human study with members of the sight-loss\ncommunity. Furthermore, many studies did not consider or report failure cases or possible risks. These\nfindings highlight the importance of inclusive system evaluations and the necessity of standardizing\nmethods for presenting and analyzing failure cases and threats when developing AI-based assistive\ntechnologies.", "sections": [{"title": "1 Introduction", "content": "Assistive technology is an umbrella term implying a wide range of tools, devices, services, and systems to increase\nthe functionality and independence of their users [1]. Persons with disabilities widely use these tools, which may be\nbased on Artificial Intelligence (AI) for various purposes, like virtual assistants for navigation, image description, and\nrecommendation. Al systems can fail unexpectedly, and these cases are widely reported and discussed, [2, 3]. However,\nthere are few reports on the failures of AI-based assistive technologies. One explanation for this is the failures and\nsafety of these assistive technologies are not included in the scope of work when doing research or making a new\nassistive technology. By not considering failures in assistive technology, the potential risks associated with them will\nstay out of our sight.\nThe consequences of this ignorance could lead to failures that cause actual harm to users. Moreover, not being\nappropriately informed about a product's associated risks and possible failures could make it more or less likely for\npersons with disabilities to trust and use such a product. For example, depending on the user's attitude, they may\nmistrust it because the product sounds completely safe when, in fact, there are important exceptions. Alternatively, they\nmay not trust it because there needs to be more information about the situations in which the product may fail when\nthese will never be encountered for practical purposes. So, the opportunity to make a well-informed decision is missed.\nThis work presents a systematic literature review of the existing research that introduces an AI-based assistive technology\nto determine whether the defined gap exists. We followed the steps shown in Kitchenham et al. systematic literature\nreview [4] for this work."}, {"title": "1.1 Research Questions", "content": "The aim of conducting this systematic review is to address the following research question:\nHow are failures and risks explored, assessed, and reported in research on AI-based assistive technologies?\nEight more detailed questions are derived from this research question, expressed in the following sections. Together,\nthese cover various aspects of our research question."}, {"title": "1.2 Questions", "content": "To analyze the papers, we reviewed each paper by answering eight assessment questions derived from the main research\nquestion above. These are two parallel sets of questions as shown in Figure1. The first four questions focus on the\nhuman study and the contribution of the sight-loss community to the study, and the following four questions are about\nfailure considerations of the represented system. By the word \"system,\" we mean any tool, device, or method that is\npresented in the paper. The assessment questions are as follows:\n1. Does the paper present a working demo or examples of how the system behaves?\n2. Is there an evaluation of the system by conducting a human study?\n3. Is the human study ecologically valid for persons with visual impairment? In other words, does the human\nstudy include members of the sight-loss community?\n4. Does the paper consider any threats to the validity of the human study?\n5. Does the paper provide evidence that the authors considered risks/possible failures associated with the system?\n6. Does the paper report examples of failures?\n7. Does the paper give specific information about when and how the system will fail?\n8. Does the paper talk about the consequences of the reported/systems failures?"}, {"title": "1.3 Definitions and Scope", "content": "To proceed with the work, the definition of \"failure\" in assistive technologies is specified in this section because there\nmight be confusion about a work's failure, limitation, or disadvantage while reading and analyzing papers. To see what\ncan be considered a failure, the following definitions are helpful:\nThe definition of the word \"failure\" in Oxford English Dictionary [5] is explained as \"lack of success\", \"breaking\ndown or ceasing to function\". In engineering, a system's failure could refer to a component's inability to operate as\nexpected [6]. Regarding AI systems, an AI incident is defined as \"a situation where AI systems caused, or very nearly\ncaused, real-world harm\" [2]. Williams and Yampolskiy [3], defined AI failure as the \"Malfunctioning of an AI system\".\nWe distinguished between failure, limitation, and disadvantage of the system following three simple definitions:\nFailure: The system is intended to do x but does y instead or does not do anything at all.\nDisadvantage: The system does x, but there are undesirable things about doing x.\nLimitation: The system is intended to do x and y but only does x.\nWhile we report the results of analyzing each paper in the following sections, only failure is in the scope of this review,\nnot disadvantage and limitation. It also would be valuable to do a review on the limitations and disadvantages of the\nsystems in the future. In this study, our focus is on AI-based assistive technologies, so to distinguish between AI-based\nsystems and other assistive software and technologies, we refer to Canada's Artificial Intelligence and Data Act (AIDA)\ndefinition of AI [7]:\n\"artificial intelligence system means a technological system that, autonomously or partly autonomously, processes\ndata related to human activities through the use of a genetic algorithm, a neural network, machine learning or another\ntechnique in order to generate content or make decisions, recommendations or predictions.\""}, {"title": "2 Related Work", "content": "A few Systematic Literature Reviews have been done in the field of assistive technologies. For instance, Alper and\nRaharinirina [8] discussed that although assistive technology devices are becoming more common, we do not know"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Systematic Search and Data Extraction", "content": "The details of the search process for finding the target papers are provided in this section to clarify the steps of the data\ncollection process, avoid biased assumptions, and tackle the intention to choose papers that appeal to the expected\nresult. The search process included a manual search for papers through Scopus, IEEE Xplore, and ACM digital library\ndatabases using related keywords and search strings. It is important to choose a search string carefully since a word\nwith various meanings can make a noticeable difference in the results. Furthermore, we added \"AND NOT\" terms to\nour search string to to avoid capturing papers related to diagnosis and treatment. The final search string is as follows:\n((assist* OR guide OR tool OR technology ) AND ( \"artificial intelligence\" OR ai OR \"machine learning\" OR smart\n) AND ( \"visually impaired\" OR \"blind people\" OR \"blind person\" OR \"sight loss\" OR \"vision loss\" OR \"partially\nsighted\") AND NOT ( screening OR diagnos* OR treatment OR medicine OR epidemiology ))"}, {"title": "3.2 Inclusion/Exclusion and Sampling", "content": "After finalizing the search string, we used it in Scopus, ACM digital library, and IEEE Xplore advanced search. We also\nexplored using Google Scholar initially but found the search functionality to be less precise than the other databases.\nTherefore, we excluded Google Scholar from our data search since advanced searching and exporting features are more\ncomprehensive and diverse in the first three databases. We searched in three categories of: paper titles, abstracts, and\nkeywords. Moreover, we filtered all the results for the past five years (2017 - 2022). We excluded 2023 since it was the\nongoing year when we started searching for the papers, and the number of published papers was still increasing. We\nexcluded years before 2017 since we are focusing on AI-based technologies, and in recent years, there has been a rapid\ngrowth of new AI systems that are yet to be understood entirely.\nAfter collecting the results, there was still the chance of having irrelevant papers, so we reviewed all the paper titles,\nin some cases abstracts, to classify all the results as \"Relevant,\" \"Irrelevant,\" and \"Related\" papers. \"Related\" papers\nare studies engaging in the topic but do not present an assistive technology. These papers include surveys, reviews, or\nproposals related to the subject. \"Relevant\" papers as our final dataset for the systematic analysis include papers that\ndeveloped or improved an AI-based assistive technology for persons with visual impairment."}, {"title": "3.2.1 Sample papers sets", "content": "Since reviewing all 648 papers proved infeasible, in order to ensure both the feasibility and validity of the review, we\nadopted a sampling approach. We initially started our review with randomly chosen sample papers in our \"Relevant\"\nclass in each of Scopus, ACM, and IEEE Xplore results. As we continued reviewing the papers, we found that the\nresults for each question were consistent, and percentages remained nearly the same, suggesting that increasing the\nsample size would not add a notable difference to the results. Considering this, we decided to preserve the sampling\napproach and show the findings of a subset of 100 papers. We separated these 100 papers into two sets of 50 to ensure\nthe results were similar in the two sets so we could conclude if our samples were representative of the total papers."}, {"title": "3.2.2 Sample sets validity", "content": "To ensure that our sample papers are representative of the total number of papers, some attributes of the papers in sample\nsets and the total papers have been compared. We chose the papers randomly with a uniform distribution. Furthermore,\nas it is shown in the Figure 2 the sample closely tracks the distribution of publication years in the full set."}, {"title": "3.2.3 Questions validation", "content": "To validate the methodology and the efficacy of the questions, we examined how straightforward it was to assess each\npaper, to ensure each designed question is answerable.\nQ1. Does the paper present a working demo or examples of how the system behaves?\nAddressing this question was straightforward and with no complexity. It is worth mentioning that in \"How the system\nbehaves,\" we did not mean the methodology or details of the system's architecture, but we meant the examples of using\nthe system and produced outputs.\nQ2. Is there an evaluation of the system by conducting a human study?\nFor this question, no matter how many persons are involved, if the paper represents a user study, we consider it as a\npositive answer. Addressing this question did not pose any challenge since, in each study, if there is a human study, they\nhad mentioned it clearly.\nQ3. Is the human study ecologically valid for persons with visual impairment?\nIn some of the studies we checked, there was a human evaluation, but it was done with blindfolded participants,\nnot members of the sight-loss community. This evaluation method is inappropriate since the situation is temporary\nand is not psychologically accurate for the participant. Specifically, it does not fully encapsulate the long-term\nor psychological effects associated with vision loss. Moreover, it does not account for a variety of vision loss"}, {"title": "Q4. Does the paper consider any threats to the validity of the study?", "content": "What makes it challenging to answer this question is that in the selected papers, we did not observe a separate section or\nanalysis for threats to the validity of work; however, this validation might be indirectly presented in other sections so it\nis needed to analyze the whole paper and extract the mentioned threats to the validity of work, or be content with what\nwe get with just having a look at the paper."}, {"title": "Q5. Does the paper provide evidence that the authors considered risks/possible failures associated with the system?", "content": "This question has been addressed in different ways in the studies, such as providing examples of failures or sometimes\nby explaining how the systems might fail. We also answered this question positively if the paper reported accuracy,\nwhich means the author/s are aware that there is a chance that their system will fail."}, {"title": "Q6. Does the paper report examples of failures?", "content": "This question was answered yes if the paper provides examples of the system's failure while testing the system or\nconducting the human study."}, {"title": "Q7. Does the paper give specific information about how and when the system will fail?", "content": "Answering this question depends on whether the paper talks about possible situations in which the system might fail or\nhow it would be if it fails. So, in some studies, they considered that their system might fail (answered yes to Question 5)\nby providing some examples of failure (i.e. Question 6) or calculations of the accuracy. However, these questions are\nnot concerned with identifying situations when the system fails; this is the focus of Question 7. Some papers answered\nQuestion 7 based on their answer to Question 6. Here, the paper reported examples of system failures and based on\nthose reported failures, talked about the situations in which they can happen. Other papers did not report examples\nof failures (answered No to Question 6). However, they explained the possible situations in which the system might\nfail. These could be theoretical expectations or based on unreported failures. As an instance of positively answering\nthis question, many papers that presented an image recognition system suggested that the system might not generate\naccurate results if the picture quality is low or blurry. So, if image detection is used in a navigation app in foggy weather,\nit may fail."}, {"title": "Q8. Does the paper talk about the consequences of the reported/systems failures?", "content": "This question was sometimes challenging to answer because, usually, there was no direct referencing to the consequences\nof a system failure, but the authors talked about it indirectly. Interpreting this from a failure report requires personal\njudgment."}, {"title": "4 Results and Analysis", "content": ""}, {"title": "4.1 Systematic search results", "content": "After cleaning up our search results, as noted in the inclusions and exclusions section and removing the overlapped\npapers from three databases, 648 studies were collected as studies that present an assistive technology for persons with\nvisual impairment. While doing the systematic review, we eliminated more papers since, by analyzing each study, we\ndiscovered more details about the paper meeting the inclusion/exclusion criteria, like if the system is AI-based and\ncompatible with the reference definition of AI. In our initial filtering stage, we needed more certainty regarding the\nstatus of many papers. Therefore, it remains necessary to perform additional filtering as part of a complete systematic\nreview in the future."}, {"title": "4.2 Systematic review results", "content": "The results of applying the assessment questions for each of the paper sets are presented in Table 2. As shown in the\ntable, the results for both sets are close and do not have significant differences, which shows that these sample papers\nare representative of the whole paper. It also indicates that the mentioned gap about the lack of study on risks and\nfailures associated with assistive technologies exists, and this systematic review demonstrated this. In the following\nsubsections, the results for the total number of a hundred papers are discussed separately for each question."}, {"title": "4.2.1 Question 1: Does the paper present a working demo or examples of how the system behaves?", "content": "As shown in figure 4, 85 papers have answered positively Question 1, meaning they have a working demo or an output\nof the system's behaviour. This could be in the form of a picture of the system's output or showing the specific results of\na test case. The other 16 papers did not address this question, whether they do not represent any output or the question\ndoes not apply to them, for example, in cases of studies that are in the early steps or are proposals."}, {"title": "4.2.2 Question 2: Is there an evaluation of the system by conducting a human study?", "content": "As demonstrated in figure 5, among the papers with an actual product or prototype to be tested, only 27/85 = 32% of\nthem have been evaluated by human experiments. This result suggests that a noticeable part of innovations are only\nevaluated based on the performance and theoretical outputs. These technologies are intended to assist people. Without\nhuman trial, it is not possible to show if the system is actually useful for real-life tasks, so it does not serve its intended\npurpose of assisting people. Moreover, user-friendliness and the way each person interacts with it play an important\nrole in the result. The system output might be correct, but the user is still unable to understand the result or make any\ndecision."}, {"title": "4.2.3 Question 3: Is the human study ecologically valid for persons with visual impairment?", "content": "As shown in figure 6, among the papers that answered yes to Question 1, only 16/85 = 19% of them have had at\nleast one participant with visual impairment for the experiments. This suggests that, in total, out of all the papers\nthat were checked, less than half of them have been evaluated by persons from the sight-loss community who are the\ntarget users. So, the theoretical evaluation of the system does not represent the community targeted by the research.\nAlso, compared to the answers to Question 2, five papers had human study, but none of the participants were from\nthe sight-loss community, and sighted or blindfolded participants did the test. For example, Akkapusit and Ko [14]\nhave conducted a human study with 20 participants of different ages, genders, and experience levels using the provided\nequipment for the evaluation. However, all participants were sighted and blindfolded for the study. This is not an\nappropriate approach to evaluate an assistive technology intended to be used by the sight-loss community."}, {"title": "4.2.4 Question 4: Does the paper consider any threats to the validity of the human study?", "content": "Another important aspect that needs to be considered while testing is the validation of the result. Figure 7, shows that\nonly 4% of papers have considered any threats to the validity of their human evaluation. The system evaluation depends\non the different criteria of human study, such as participants' age ranges, heights, education levels, specific weather\nconditions, light conditions, and many more criteria. As a result, these evaluations can not be fully representative of\ntheir target users, and only a few research studies have considered this fact. As an instance of considering threats to the\nvalidity of the work, Easley and Rahman [15] mentioned that their study could be improved by recruiting participants\nwith a diversity of attributes like vision level, age, gender, experience, etc."}, {"title": "4.2.5 Question 5: Does the paper provide evidence that the authors considered risks/possible failures\nassociated with the system?", "content": "In figure 8, we have examined whether the papers have considered the probability of failure for their proposed system.\nAs shown, 74% of papers have at least introduced a metric to evaluate their proposed approach or prototype success rate.\nIt is important to mention that this performance report does not necessarily rely on human experiments. It can be the\nsystem behaviour from different points of view, such as time efficiency, accuracy and the correctness of the results. We\nconsidered this question addressed by the paper, even if it only reports the system's accuracy, thereby acknowledging\nthat failure is a possibility."}, {"title": "4.2.6 Question 6: Does the paper report examples of failures?", "content": "As the results demonstrate in figure 9, 78% of the papers did not give an example of their system failure and incorrect\nresults. To use these systems daily, we need to know about the cases in which they have failed. Ignoring the failure\nsamples will just be like ignoring an existing issue that will never allow the proposed system or approach to be used in\nreal-life scenarios."}, {"title": "4.2.7 Question 7: Does the paper give specific information about when and how the system will fail?", "content": "As figure 10 shows, 32% of papers have explained the situations in which their system will malfunction and fail.\nThis can be a certain brightness level, background noises, users' special conditions, and many more environment or\nuser-related features. A system can be robust and reliable within a certain setup and not the other. For example, a\nnavigation system may function perfectly outdoors and not indoors. Knowing the situation that the system might fail\ncan help the user decide when to rely on it and when not to."}, {"title": "4.2.8 Question 8: Does the paper talk about the consequences of the reported/systems failures?", "content": "In figure 11 it has been shown that only 2% of papers have considered their system failure consequences. Referring to\nQuestion 5, it shows that most studies acknowledge and are aware that their system might fail, but very few of them\ntalk about the existing or potential consequences of these failures. An assistive system to help persons with visual\nimpairment can directly affect one's well-being. So, it needs to be robust and have some error-handling mechanisms\nwhen they fail. This has only been considered in a few papers. For any system to be used in real-life situations,\nthe consequences of failure must have been considered and handled to get close to being used in reality despite any\nperformance or evaluation challenges. This handling could mean trying to mitigate the risks, warning users not to use it\nin certain situations, or letting users decide whether they want to trust the assistive technology with the new information.\nAs an example of this question being positively answered, Li et al. [16] discussed that the detection system needs a high\naccuracy since it has to detect crucial situations like a red traffic light."}, {"title": "Discussion", "content": "Most studies demonstrated a working example of their system's behaviour; however, among these papers, only a few\nevaluated their working on an actual test with human participants. Even among those who have a human experiment, a\nfew papers did not have participants with visual impairment. This and many other factors mentioned in 4.2.4 question\nthe validity of human studies, but very few papers talk about threats to the validity of their human study.\nOn the failure consideration side, most studies acknowledged that their system could create incorrect outputs (Question\n5). However, again, only a tiny percentage of these papers specified the situations in which the system might fail or\ngave examples of the system's incorrect results. Moreover, By analyzing the review results, we noticed that among\nthe papers without human studies, only 13.7% of them reported the system's failure cases. In comparison, among the\npapers with human studies, 44.4% reported examples of their system's failure. This number is 50% for the studies\nthat had a human study with participants from the sight-loss community. This comparison suggests that without the\npresence of a human study, the chance of having unnoticed failure cases increases.\nIn the end, only a few papers give awareness about what might happen due to the system's failure to clear the associated\nrisks of using the product.\nAs our systematic review shows, there is a need to revisit how the academic research on assistive technologies equipped\nwith Al is done and presented. Now that we know this need exists, it is necessary to determine how this kind of research\nshould be done and what steps should be followed to help make sure studies are valid and represented in a way that\nallows the user to decide about using assistive technology and trusting it. Designing guidelines to follow is our potential\nnext step. We know that the gap in this research area exists, and now we look forward to a solution that will help\nminimize the gap."}, {"title": "Limitations", "content": "A limitation of this work is narrowing down our context to academic studies and published papers. We acknowledge\nthat there are assistive technologies invented or provided by companies that do not publish in academic venues. Still,\nas stated in our research question, we were primarily interested here in exploring academia and research on AI-based\nassistive technologies.\nFurthermore, the final search string was arrived at in order to be comprehensive while avoiding excessive irrelevant\nresults. Still, no perfect search string could cover all the related papers. So, it is possible that some papers are not\ncaptured with our search string and are not included in our search results.\nA hundred papers among the total 648 papers were examined as a sample of papers. Although they were chosen\nrandomly and showed that this sample is representative of the whole, it is still expected that the results would vary\nslightly if all 648 papers were assessed. Also, we did the assessment manually, and there is always the possibility of\nintroducing subjectivity and unconscious bias."}, {"title": "5 Conclusion", "content": "In this work, we conducted a systematic literature analysis of the published studies to understand how the failures and\nrisks of AI-based assistive technologies for persons with visual impairment are presented in academia. We framed eight\nresearch questions, which were then applied to a subset of 100 papers randomly sampled from the total number of 648\npapers in the systematic search results. The questions captured both human study and risk analysis aspects of the study.\nResults show only a few papers (19%) tested their work in a human study with visually impaired participants. Moreover,"}, {"title": "there is less information on the cases of failures and possible risks of introduced systems. We validated the method as\nthe search string and inclusion/exclusion criteria effectively captured a considerable amount of relevant literature within\nour research scope, and the proposed analysis questions were answerable and provided meaningful results.\nIn our future work, we look to take the findings of this research to introduce guidelines for researchers on how to\nevaluate AI-based assistive technology well.\nThis will ultimately empower people to judge the trustworthiness of a newly presented assistive technology.", "content": ""}]}