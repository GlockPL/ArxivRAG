{"title": "CAUSAL INFORMATION PRIORITIZATION FOR EFFICIENT REINFORCEMENT LEARNING", "authors": ["Hongye Cao", "Fan Feng", "Tianpei Yang", "Jing Huo", "Yang Gao"], "abstract": "Reinforcement Learning (RL) methods often suffer from sample inefficiency, one of the underlying reasons is that blind exploration strategies may neglect causal re-lationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPS to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios. The project page is https://sites.google.com/view/rl-cip/.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning (RL) has emerged as a powerful paradigm for training intelligent decision-making agents to learn optimal behaviors by interacting with their environments, receiving reward feedback, and iteratively optimizing their decision-making policies (Haarnoja et al., 2018; Ze et al., 2024; Sutton, 2018; Silver et al., 2017; Cao et al., 2023). Despite its notable successes, most RL approaches are faced with the sample-inefficiency problem, which means they typically necessitate an enormous number of interactions with the environment to learn policies, which can be impractical or costly in real-world scenarios (Savva et al., 2019; Kroemer et al., 2021). Inefficient policy learning often results from blind exploration strategies that neglect causal relationships, leading to spurious correlations and suboptimal solutions with high exploration costs (Zeng et al., 2023; Liu et al., 2024).\nCausal reasoning captures essential information by analyzing causal relationships between differ-ent factors, filtering out irrelevant information, and avoiding interference from spurious correla-tions (Wang et al., 2022; Pitis et al., 2022; Zhang et al., 2024; Huang et al., 2022b). These approaches build internal causal structural models, enabling agents to strategically focus their exploration on the most pertinent aspects of the environment. They significantly reduce the number of samples required and demonstrate remarkable performance in single-task learning, generalization, and counterfactual reasoning (Richens & Everitt, 2024; Urp\u00ed et al., 2024; Deng et al., 2023; Huang et al., 2022a; Feng & Magliacane, 2023). However, most of these works overlook the reward-relevant causal relationships among different factors, or only partially consider the causal connections between states, actions, and rewards (Liu et al., 2024; Ji et al., 2024a), thus hindering efficient exploration."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 CAUSAL RL", "content": "The application of causal reasoning in RL has shown significant potential to improve sample effi-ciency and generalization by effectively excluding irrelevant environmental factors through causal analysis (Huang et al., 2022a; Feng & Magliacane, 2023; Mutti et al., 2023; Sun et al., 2024; Sun & Wang). Wang (Wang et al., 2021) introduces a novel regularization-based method for causal dynamics learning, which explicitly identifies causal dependencies by regulating the number of variables used to predict each state variable. CDL (Wang et al., 2022) takes an innovative approach by using condi-tional mutual information to compute causal relationships between different dimensions of states and actions. IFactor (Liu et al., 2024) is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. ACE (Ji et al., 2024a), an off-policy actor-critic method, integrates causality-aware entropy regularization. Existing approaches do not fully account for the causal relationships between both states and actions with rewards. Our goal is to explore these causal relationships from a reward-guided perspective to enhance sample efficiency across a broader range of tasks."}, {"title": "2.2 EMPOWERMENT IN RL", "content": "Empowerment, an information theory-based concept of intrinsic motivation, has emerged as a power-ful paradigm for enhancing an agent's environmental controllability (Mohamed & Jimenez Rezende, 2015; Klyubin et al., 2005; Cao et al., 2024). This framework conceptualizes actions and future states as information transmission channels, offering a novel perspective on agent-environment interactions. In RL, empowerment has been applied to uncover more controllable associations between states and actions, as well as to develop robust skill (Salge et al., 2014; Bharadhwaj et al., 2022; Choi et al., 2021; Eysenbach et al., 2018; Leibfried et al., 2019; Seitzer et al., 2021). Empowerment, expressed as maximizing mutual information max I, serves as a learning objective in various RL frameworks, providing intrinsic motivation for exploration and potentially yielding more efficient and generalizable policies. Our approach extends empowerment in RL by examining the influence of state, actions, and rewards through a causal lens, integrating causal understanding with empowerment to enhance exploration strategy and learning efficiency."}, {"title": "2.3 OBJECT-CENTRIC RL AND OBJECT-ORIENTED RL", "content": "Recent advances in object-centric representation learning focus on acquiring and leveraging structured, object-wise representations from high-dimensional observations. Foundational works include Slot Attention (Locatello et al., 2020) and AIR (Eslami et al., 2016; Kosiorek et al., 2018), establishing basis for this field. Subsequent follow-ups have worked on these concepts by employing state-of-the-art architectures, including DINO-based approaches Zadaianchuk et al. (2023), transformer-based models (Wu et al., 2022), diffusion models (Jiang et al., 2023), and state-space models (Jiang et al., 2024). Notably, learning object-centric representations can enable compositional generalization across various domains, such as video and scene generation (Wu et al., 2023; 2024). Moreover, several theoretical studies have explored the mechanisms underlying compositional generalization and the causal identifiability (Kori et al., 2024; Brady et al., 2023; Lachapelle et al., 2024).\nObject-centric representations have been effectively employed in world models to capture multi-object dynamics, as demonstrated by works (Jiang et al., 2019; Lin et al., 2020; Kossen et al., 2019). Building on these object-centric world models, various studies use them in RL by better modeling complex object-centric structures in partially observable MDPs (Kossen et al., 2019; Mambelli et al., 2022; Feng & Magliacane, 2023; Choi et al., 2024), identifying critical objects (Zadaianchuk et al.,"}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 MARKOV DECISION PROCESS", "content": "In RL, the agent-environment interaction is formalized as an MDP. The standard MDP is defined by the tuple M = (S, A,P, \u03bc\u03bf, r, \u03b3), where S denotes the state space, A represents the action space, P(s'|s, a) is the transition dynamics, r(s, a) is the reward function, and \u00b5o is the distribution of the initial state so. The discount factor \u03b3\u2208 [0,1) is also included. The objective of RL is to learn a policy \u03c0 : S \u00d7 A \u2192 [0, 1] that maximizes the expected discounted cumulative reward \u03b7\u03bc(\u03c0) := Eso~\u03bc\u03bf, 5t~P, at~\u3160 [\u2211t=0tr(st, at)]."}, {"title": "3.2 STRUCTURAL CAUSAL MODEL", "content": "A Structural Causal Model (SCM) (Pearl, 2009) is defined by a distribution over random variables, defined as V = {s\u2021,\u2026\u2026,s\u0165, a\u2021,\u2026\u2026\u2026, ax, rt, st+1,\u2026, st+1} and a Directed Acyclic Graph (DAG) G = (V, E) with a conditional distribution P(vi|PA(vi)) for node vi \u2208 V. Then the distribution can be specified as:\np(v_1,..., v_{[\\nu]}) = \\prod_{i=1}^{\\nu} P(v_i|PA(v_i)),\nwhere PA(vi) is the set of parents of the node vi in the graph G."}, {"title": "3.3 EMPOWERMENT IN RL", "content": "Empowerment quantifies an agent's capacity to influence its environment and perceive the conse-quences of its actions (Klyubin et al., 2005; Bharadhwaj et al., 2022; Jung et al., 2011). In our"}, {"title": "4 CAUSAL INFORMATION PRIORITIZATION", "content": "In this section, we introduce the proposed framework CIP, which implements causal informa-tion prioritization based on the causal relationships between states, actions, and rewards (as shown in Figure 2). First, we train a structural model based on the causal discovery method, DirectLiNGAM (Shimizu et al., 2011) using collected trajectories to obtain a causal matrix Ms\u2192r. Utilizing this matrix, CIP executes the swapping of causally independent state features, generating synthetic transitions (Section 4.1). This process of swapping independent state information accen-tuates causally dependent state information, enabling focused learning on critical state transitions. Subsequently, CIP constructs another structural model to get a weight matrix Ma\u2192r that incorporates actions and rewards to reweight actions (Section 4.2). Furthermore, CIP integrates a causality-aware empowerment term E\u03c0\u300f(s) combined with causally weighted actions into the learning objective to promote efficient exploration. This integration encourages the agent's policy \u03c0\u03b5 to prioritize actions with high causal influence, thereby enhancing its goal-achievement capabilities."}, {"title": "4.1 COUNTERFACTUAL DATA AUGMENTATION", "content": "To discover the causal relationships between states and rewards, we initially collect trajectories to train a structural model by the DirectLiNGAM method, denoted as q, to obtain the causal matrix Mr. Subsequently, we infer the local factorization, which is utilized to generate counterfactual transitions. For each state s in the trajectories, we compute the uncontrollable set, defined as the set of variables in s for which the agent has no causal influence on rewards:\nU_s = \\{s^i | M_{s\\rightarrow r} \\cdot (s^i, r_t) < \\theta; i \\in [1, N]\\},\nwhere @ is a fixed threshold and N is the dimension of the state space. The set Us encompasses all dimensional state variables for which the causal relationship st \u2192 rt does not exist in the causal matrix of states and rewards. Utilizing the learned causal matrix Ms\u2192r, we partition all state variables in the factored MDP into controllable and uncontrollable sets. These uncontrollable sets are then leveraged for counterfactual data augmentation, thereby prioritizing the causally-informed state information to improve learning efficiency.\nTo generate counterfactual samples, we perform a swap of variables that fall under the uncontrollable category (i.e., in set Us) sampled from the collect trajectories. Specifically, given two transitions (St, at, St+1,rt) and (\u015dj, aj, \u015dj+1, r^;) sampled from trajectories, which share at least one uncontrol-lable sub-graph structure (i.e., U\u201d NU\u015d \u2260 (\u00d8), we construct a counterfactual transition (St, \u0101t, \u0161t+1, rt) by swapping the irrelevant state variables (st, si+1) with (+1) for each i \u2208 Us \u2229U\u015d. The"}, {"title": "4.2 CAUSAL ACTION PRIORITIZATION THROUGH EMPOWERMENT", "content": "Causal action reweight Having analyzed the causal relationships between states and rewards to achieve efficient data augmentation, in this section, we further discover the causal relationships be-tween actions and rewards to prioritize causally-informed decision-making behaviors. CIP constructs a reward-guided structural model, incorporating states (including augmented states), actions, and rewards. This model forms the foundation for action prioritization in policy learning, enabling action reweighting based on causality. Leveraging this structural model to delineate relationships between policy decisions and rewards, we evaluate the causal impact of different actions on reward outcomes. In this way, the agent focuses on pivotal actions with demonstrable causal links to desired reward outcomes, potentially accelerating learning and optimizing performance in complex environments.\nSpecifically, in CIP, we employ DirectLiNGAM method to train a causal structural model qu, which yields a weight matrix Ma\u2192r, delineating the relationships between actions and rewards, conditioned on the states. For a given set of actions (a\u0142, a\uacf9, a, . . . ), we utilize the weight matrix Ma\u2192r to reweight them as (w\u2081a\u0142, w2a\u0142, wa\u0142, ...), where w represents the causal weights derived from the matrix Ma\u2192r. By leveraging this causal structure, we can prioritize the most pivotal actions, potentially leading to more efficient policy exploration and targeted policy improvements.\nCausal action empowerment Based on the learned causal structure, we propose the causal action empowerment to incorporate the reweighted actions into the learning objective for efficient exploration in a controllable manner. To this end, we design a causality-aware empowerment term \u0395\u03c0\u300f(s) for policy optimization. We maximize the empowerment gain of the policy \u03c0\u03b5, where \u03c0\u03b5 incorporates the learned causal structure. This approach allows us to quantify and maximize the empowerment that can be achieved by explicitly considering causal relationships, thereby bridging the gap between causal reasoning and empowerment.\nWe denote the empowerment of the causal policy as \u0395\u03c0\u300f(s) = maxa I (at; St+1 | st; M). We then formulate the following objective empowerment function:\n\\mathcal{E}_{\\pi_c}(s) = \\max_a I (a_t; S_{t+1} | s_t; M)\n- \\max_a H(\\pi_c(a_t| S_t)) - H(\\pi_c(a_t|S_t; S_{t+1})),\nwhere \u03c0\u03b5 is the policy under the causal weighted matrix Ma\u2192r. The first entropy term H(\u03c0c(at|st)) promotes action diversity within the constraints of the causal structure. It encourages the agent to explore a wide range of actions that are causally informed, while the second entropy term -\u0397(\u03c0c(at St; St+1)) enhances the action predictability in state transitions. It encourages the selec-tion of actions that lead to predictable outcomes, given the current and subsequent states, thereby promoting controlled and goal-oriented behaviors. We train an inverse dynamics model to represent the policy \u03c0\u03bf(\u00b7st; St+1). The detailed derivation proceeds as follows:\nand\nH(\\pi_c(\\cdot| s_t)) = -E_{\\alpha_t} \\sum_{i=1}^{dA} M_a \\circ \\pi_c(a^i_t| s_t) \\log \\pi(a^i_t| s_t)\nH(\\pi_c(\\cdot|S_t; S_{t+1})) =  -E_{\\alpha_t} \\sum_{i=1}^{dA} M_a \\circ \\pi_c(a^i_t|s_t; S_{t+1}) \\log \\pi(a^i_t|s_t; S_{t+1})\nwhere da is the dimension of the action space. Hence, the learning objective of the causal action empowerment can be defined as follows:\n\\mathcal{E}_{\\pi_c}(s) = \\max_a H(\\pi_c(a_t| S_t)) - H(\\pi_c(a_t|S_t; S_{t+1}))\n=\\max_{\\alpha_t \\cup \\pi_0(18)} E_{\\pi_c(a_t| S_t)P_{\\pi_c}(a_t | S_t,S_{t+1})} [\\log P_{\\delta c}(a_t | S_t, S_{t+1}; M) - \\log P_{\\pi_\\epsilon}(a_t|s_t; M)],"}, {"title": "5 EXPERIMENTS", "content": "Our experiments aim to address the following questions: (i) How does the performance of CIP compare to other RL approaches in diverse continuous control tasks, including manipulation and locomotion with sparse rewards, high-dimensional action spaces, and pixel-based challenges? (ii) Can CIP, through data augmentation and empowerment, improve sample efficiency and learn reliable policies? (iii) What are the effects of the components and hyperparameters in CIP?"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Environments. We evaluate CIP on 5 continuous control environments, including MuJoCo (Todorov et al., 2012), DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2020), Adroit Hand (Rajeswaran et al., 2018), and sparse reward setting environments in Meta-World. This comprehensive evaluation encompasses 36 tasks, spanning both locomotion and manipulation skill learning, as illustrated in Figure 3. We also conduct experiments in 4 pixel-based tasks of the DMControl and Cartpole environment as shown in Figure 17. Our experimental tasks incorporate a wide range of challenges, including high-dimensional state and action spaces, sparse reward settings, pixel-based scenarios, and locomotion. For extensive experimental settings, please refer to Appendix D.1.\nBaselines. We compare CIP with three popular RL baselines across all 36 tasks and against IFactor (Liu et al., 2024) in 3 pixel-based tasks: (1) SAC (Haarnoja et al., 2018), an off-policy actor-critic algorithm featuring maximum entropy regularization. (2) ACE (Ji et al., 2024a), a method employing causality-aware entropy regularization. (3) BAC (Ji et al., 2024b). a method that balances sufficient exploitation of past successes with exploration optimism. (4) IFactor (Liu et al., 2024). a causal framework modeling four distinct categories of latent state variables for pixel-based tasks. To ensure robustness and statistical significance, we conduct each experiment using 4 random seeds."}, {"title": "5.2 MAIN RESULTS", "content": "Figure 4 presents the normalized scores of CIP compare to other methods across 36 tasks in 5 environments. In 17 Meta-World robot-arm tasks, CIP achieves a near-perfect score of 100,"}, {"title": "5.3 ANALYSIS", "content": "Ablation study. We conduct ablation experiments involving CIP, CIP without (w/o) counterfactual data augmentation (Aug), and CIP w/o Empowerment (Emp). The results in 8 locomotion tasks are shown in Table 1. And the learning curves of all tasks are depicted in Appendix D.3. The"}, {"title": "6 CONCLUSION", "content": "This study introduces an efficient RL framework, designed to enhance sample efficiency. This approach begins by counterfactual data augmentation using the causality between states and rewards, effectively mitigating interference from irrelevant states without additional environmental interactions. We then develop a reward-guided structural model that leverages causal awareness to prioritize causal actions through empowerment. We conduct extensive experiments across 39 tasks spanning 5 diverse continuous control environments which demonstrate the exceptional performance of our proposed method, showcasing its robustness and adaptability across challenging scenarios.\nLimitation and Future Work The current limitations of our work are twofold. First, CIP has not yet been extended to complex scenarios, such as real-world 3D robotics tasks. Potential approaches to address this limitation include leveraging object-centric models (Wu et al., 2023), 3D perception models (Wang et al., 2024a), and robotic foundation models (Team et al., 2024; Firoozi et al., 2023) to construct essential variables for causal world modeling. Second, CIP does not adequately consider non-stationarity and heterogeneity, which are critical challenges in causal discovery. Future work could integrate method designed to handle such complexities, such as CD-NOD (Huang et al., 2020)."}, {"title": "A BROADER IMPACT", "content": "To avoid blind exploration and improve sample efficiency, we propose CIP for efficient reinforcement learning. CIP leverages the causal relationships among states, actions, and rewards to prioritize causal information for efficient policy learning. CIP first learns a causal matrix between states and rewards to execute counterfactual data augmentation, prioritizing important state features without additional environmental interactions. Subsequently, it learns a causal reweight matrix between actions and rewards to prioritize causally-informed behaviors. We then introduce a causal action empowerment term into the learning objective to enhance the controllability. By prioritizing the causal information, CIP enables agents to focus on behaviors that have causally significant effects on their tasks. CIP offers substantial broader impact by prioritizing causal information through individual assessment of how different factors contribute to rewards. Our novel empowerment learning objective achieves efficient policy optimization by leveraging entropy via the policy and learned inverse dynamics model. This approach shows promise for extension into research frameworks centered on maximum entropy algorithms.\nDespite its strengths, CIP has limitations beyond its reliance on the method DirectLiNGAM. There's potential to explore alternative causal discovery techniques for more robust relationship mapping. Moreover, analyzing inter-entity causal connections could lead to better disentanglement of diverse behaviors. Our future work will investigate a range of causal discovery methods to refine our approach. We aim to extend CIP to model-based RL frameworks, focusing on building causal world models to enhance generalization."}, {"title": "B ASSUMPTIONS AND PROPOSITIONS", "content": "Assumption 1 (d-separation (Pearl, 2009)) d-separation is a graphical criterion used to determine, from a given causal graph, if a set of variables X is conditionally independent of another set Y, given a third set of variables Z. In a directed acyclic graph (DAG) G, a path between nodes n\u2081 and nm is said to be blocked by a set S if there exists a node nk, for k = 2, \u2026, m \u2013 1, that satisfies one of the following two conditions:\n(i) nk \u2208 S, and the path between nk-1 and nk+1 forms (nk-1 \u2192 \u043f\u043a \u2192 Nk+1), (nk\u22121 \u2190 nk \u2190 nk+1), or (nk-1 \u2190 Nk \u2192 Nk+1).\n(ii) Neither nk nor any of its descendants is in S, and the path between nk\u22121 and nk+1 forms (Nk-1 \u2192 Nk\u2190 Nk+1).\nIn a DAG, we say that two nodes na and n\u266d are d-separated by a third node n if every path between nodes na and n\u028a is blocked by nc, denoted as n\u0430 \u2014 \u043f\u044c\u043f\u0441.\nAssumption 2 (Global Markov Condition (Spirtes et al., 2001; Pearl, 2009)) The state is fully observable and the dynamics is Markovian. The distribution p over a set of variables V (st,..., st, at,..., a, rt) satisfies the global Markov condition on the graph if for any parti-tion (S, A, R) in V such that if A d-separates S from R, then p(S, R|A) = p(S|A) \u00b7p(RA)\nAssumption 3 (Faithfulness Assumption (Spirtes et al., 2001; Pearl, 2009)) For a set of variables V = (s+,\u2026, st, at,\u2026\u2026\u2026, at, rt), there are no independencies between variables that are not implied by the Markovian Condition.\nAssumption 4 Under the assumptions that the causal graph is Markov and faithful to the observa-tions, the edge st \u2192 st+1 exists for all state variables s\u00b2.\nAssumption 5 No simultaneous or backward edges in time.\nProposition 1 Under the assumptions that the causal graph is Markov and faithful to the observa-tions, there exists an edge from a\u2084 \u2192 rt if and only if a\u00b2 | rt|at \\at, St.\nProof: We proceed by proving both directions of the if and only if statement.\n(\u2192) Suppose there exists an edge from a to rt. We prove that a || rt|at \\a, st by contradiction. Assume art at \\ar, St. By the faithfulness assumption, this independence must be reflected in"}, {"title": "C EXTENDED RELATED WORK", "content": "We categorize existing causal RL approaches based on problem domains and task types, providing a systematic analysis of how different methods explore causal relationships between states, actions, and rewards, as illustrated in Table 2.\nIn the single-task learning domain, methods such as ACE (Ji et al., 2024a) and IFactor (Liu et al., 2024) have shown success in learning policies for manipulation and locomotion tasks. However, both approaches are limited by focusing on a single reward-guided causal relationship. Regarding generalization, AdaRL (Huang et al., 2022a) effectively leverages both state-reward and action-reward causal relationships. However, AdaRL focuses primarily on applying causal inference to address generalization challenges in locomotion tasks. Its application is limited to locomotion tasks, leaving more complex manipulation tasks unaddressed. Since our work focuses on the single-task problem domain, we do not provide a direct comparison with AdaRL. Conversely, CBM (Wang et al., 2024d) considers the causal relationship between states and rewards but overlooks the causal link between actions and rewards. In the problem domain of counterfactual data augmentation, current causal RL methods (Urp\u00ed et al., 2024; Pitis et al., 2020; 2022) have not yet explored the inference and utilization of both causal relationships.\nIn summary, current research on reward-guided causal discovery remains incomplete and lacks validation across a broader spectrum of tasks. This gap underscores the need for more comprehensive investigation and application in the field of causal reinforcement learning."}, {"title": "C.1 EXTENDED DISCUSSION ON OBJECT-CENTRIC RL AND 3D WORLD MODELS", "content": "The main similarity lies between our framework and object-centric RL is both are learning and using factored MDPs (Kearns & Koller, 1999), but they differ in granularity: our framework operates at the component level (e.g., raw state variables), whereas object-centric RL factors states based on objects.\nAlthough our work is orthogonal to object-centric RL, we believe certain elements of object-centric RL could complement our framework in specific applications, particularly in real-world robotic manipulation tasks. Potential future work include:\n\u2022 Using object-centric representation as input: Object-centric models can help identify object-factored variables, such as object attributes, geometry, and physical states, which are useful for planning (Jiang et al., 2019; Lin et al., 2020; Kossen et al., 2019; Mambelli et al., 2022; Feng & Magliacane, 2023; Choi et al., 2024; Zadaianchuk et al., 2022; Park et al., 2021; Zadaianchuk et al., 2021; Yuan et al., 2022; Li et al., 2020; Mitash et al., 2024; Haramati et al., 2023; Li et al., 2024). In this case, states are factored as objects, and we can learn causal graphs over these variables. This is useful in robotic environments involving numerous objects. We will leave this as a future work for adapting our current framework to the applications of the object-centric robotic task."}, {"title": "D DETAILS ON EXPERIMENTAL DESIGN AND RESULTS", "content": ""}, {"title": "D.1 EXPERIMENTAL SETUP", "content": "We present the detailed hyperparameter settings of the proposed method CIP across all 5 environments in Table 3. Additionally, the Q-value and V-value networks are used MLP with 512 hidden size. And the policy network is the Gaussian MLP with 512 hidden size. Moreover, we set the target update interval of 2. For fair comparison, the hyperparameters of the baseline methods (SAC (Haarnoja et al., 2018), BAC (Ji et al., 2024b), ACE (Ji et al., 2024a)) follow the same settings in the experiments.\nFor pixel-based DMControl environments, we employ IFactor (Liu et al., 2024) to encode latent states and integrate the CIP framework for policy learning. We utilize the st state features in IFactor as uncontrollable states unrelated to rewards to execute counterfactual data augmentation. Furthermore, for simplicity, we maximize the mutual information between future states and actions to facilitate empowerment. All parameter settings in these three tasks adhere to those specified in IFactor. Additionally, We use the same background video for the comparison."}, {"title": "D.2 FULL RESULTS", "content": ""}, {"title": "D.2.1 EFFECTIVENESS IN ROBOT ARM MANIPULATION", "content": "Figure 9 presents the learning curves for all 17 manipulation skill tasks within the Meta-World environment. The CIP framework demonstrates superior learning outcomes and efficiency compared to the three baseline methods, despite exhibiting minor instabilities in the basketball and dial-turn tasks. Notably, CIP achieves a 100% success rate in more complex tasks, such as pick-place-wall and assembly. The visualization results presented in Figures 11 and 12 further demonstrate CIP's"}, {"title": "D.2.2 EFFECTIVENESS IN SPARE REWARD SETTINGS", "content": "Figure 13 presents the learning curves for all three sparse reward setting tasks within the Meta-World environment, while Figure 14 showcases their corresponding visualization trajectories. These findings reveal that CIP not only achieves superior learning efficiency but also adeptly executes critical actions necessary for task completion, such as opening the door and window and maneuvering the node to the target place.\nThese results substantiate the effectiveness of CIP in sparse reward scenarios. The counterfactual data augmentation process prioritizes salient state information, effectively filtering out irrelevant factors that could hinder learning. Meanwhile, causal action empowerment enhances policy controllability by focusing on actions that are causally linked to desired outcomes. This dual approach not only accelerates the learning process but also fosters a more robust policy capable of navigating the complexities inherent in sparse reward settings. Overall, these findings underscore CIP's potential to significantly improve performance in challenging environments characterized by limited feedback."}, {"title": "D.2.3 EFFECTIVENESS IN LOCOMOTION", "content": "We further evaluate CIP in 15 locomotion tasks in DMControl and MuJoCo environments. Figure 15 presents the learning curves, while Figure 16 showcases the corresponding visualization trajectories in 4 specific tasks. A comprehensive analysis indicates that CIP achieves faster learning efficiency and greater stability compared to ACE and SAC, while demonstrating comparable policy learning"}, {"title": "D.2.4 EFFECTIVENESS IN PIXEL-BASED TASKS", "content": "To further validate the effectiveness of our proposed framework in pixel-based environments, we evaluated CIP on three DMControl pixel-based tasks. We leverage IFactor for latent state processing and differentiation of uncontrollable state features to execute counterfactual data augmentation, alongside maximizing the mutual information between future states and actions for empowerment.\nFigure 6 presents the learning curves, while Figure 18 shows the visualization trajectories. The proposed framework exhibits enhanced policy learning performance and effectively mitigates in-terference from background video, facilitating efficient locomotion. These findings reinforce the effectiveness and extensibility of our causal information prioritization framework, highlighting its potential to improve learning in complex, pixel-based environments."}, {"title": "D.3 PROPERTY ANALYSIS", "content": ""}, {"title": "D.3.1 ANALYSIS FOR REPLACING COUNTERFACTUAL DATA AUGMENTATION", "content": "In CIP, we exploit the causal relationship between states and rewards to perform counterfactual data augmentation on irrelevant state features, thus prioritizing critical state information. We compare this approach with an alternative method: masking irrelevant state features to achieve state abstraction for subsequent causal action empowerment and policy learning. To evaluate the efficacy of both approaches, we conduct experiments with CIP with counterfactual data augmentation (CIP w/i Cda) and CIP with causally-informed states (CIP w/i Cs) across three distinct environments.\nFigure 19 illustrates comparative results for four manipulation skill learning tasks in the Meta-World environment. Both CIP variants achieve 100% task success rates with high sample efficiency, vali-dating their effectiveness. Notably, CIP w/i Cda exhibits superior learning efficiency compared to"}, {"title": "D.3.2 EXTENSIVE ABLATION STUDY", "content": "Robot arm manipulation The ablation study results in the Meta-World and Adroit Hand envi-ronments are presented in Figure 22. The findings indicate that CIP without counterfactual data augmentation exhibits reduced learning efficiency and is unable to successfully complete tasks such as pick-and-place. This underscores the importance of incorporating counterfactual data augmentation, which prioritizes causal state information, to enhance learning efficiency by mitigating the influence of irrelevant state information and preventing policy divergence.\nFurthermore, CIP without causal action empowerment demonstrates a significant decline in policy performance across robot arm manipulation tasks. In complex scenarios, such as Adroit Hand door opening and assembly, it fails to learn effective strategies for task completion. This outcome further corroborates the efficacy of the proposed causal action empowerment mechanism, as prioritizing causally informed actions facilitates more efficient exploration of the environment, ultimately enabling successful policy learning.\nSparse reward settings Figure 22 presents the results of the ablation study conducted across three sparse reward setting tasks. These findings underscore the substantial influence of causal action empowerment on the efficacy of policy learning, demonstrating its critical role in enhancing performance in challenging environments. Additionally, the incorporation of counterfactual data augmentation proves effective in mitigating the need for additional environmental interactions, thereby significantly improving sample efficiency. This approach not only facilitates more rapid learning but"}, {"title": "D.3.3 HYPERPARAMETER ANALYSIS", "content": "We conduct a detailed analysis of the hyperparameters associated with the causal update interval (I) and sample size within the CIP framework. The experimental results for four distinct tasks are illustrated in Figure 25. Across all tasks, CIP demonstrates optimal performance with a causal update interval of I = 2 and a sample size of 10,000.\nOur findings suggest that while a reduction in the causal update interval can lead to improved performance, it may also result in heightened computational costs. Additionally, we observe that higher update frequencies and increased sample sizes introduce greater instability, which significantly raises computational demands. This analysis underscores the importance of carefully balancing hyperparameter settings to optimize both performance and efficiency within the CIP.\nFurthermore, we analyze the performance under different settings of the temperature factor a proposed in Eq. 9. The results across 3 tasks are shown in Figure 26. Our analysis reveals"}]}