{"title": "Correlation of Object Detection Performance with Visual Saliency and Depth Estimation", "authors": ["Matthias Bartolo", "Dylan Seychell"], "abstract": "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mAp up to 0.459 on Pascal VOC) compared to depth prediction (mAp up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the progress being made in computer vision, object detection remains a fundamental challenge, with current approaches achieving impressive accuracy but still facing limitations in complex scenarios [1], [2]. Human perception is influenced by various factors, leading to selective attention towards certain elements in our environment. The same trait is followed in machines, which also rely on mechanisms that prioritise specific aspects of images [3]. Whilst advances in deep learning have led to significant improvements, understanding how different visual tasks relate to and potentially enhance object detection performance remains crucial for further progress. The relationships between complementary tasks such as depth estimation and visual saliency prediction could provide valuable insights for improving detection systems.\nRecent works in computer vision have explored multi-task learning approaches, combining object detection with either depth estimation or saliency prediction [4]. However, these studies typically focus on end-to-end performance rather than analysing the underlying correlations between these tasks. Understanding these correlations provides opportunities for advancement. These include improvement of object detection architectures while also improving computational efficiency in the process. Moreover, it also informs researchers on how datasets can be designed to improve these computer vision tasks. The challenges of detecting objects across varying scales and contexts [5], [6] make it essential to understand which complementary tasks provide the most beneficial information for object detection. Additionally, such advancements are also important when looking at AI techniques from a sustainable perspective.\nThis paper presents an investigation of the relationships between depth estimation and visual saliency. We present an examination of how each factor correlates with object detection performance. Through analysis across different datasets and models, we aim to explore whether significant correlations exist between these visual tasks and object detection accuracy. This investigation focuses particularly on how these correlations vary across different object categories and scales since it provides insights that could inform more effective object detection architectures.\nThe implications of the work presented in this paper extend beyond a theoretical understanding of computer vision. They present practical perspectives for refining object detection systems. The quantification of correlations between these tasks provides empirical evidence for which auxiliary features might be most beneficial for enhancing detection performance. Furthermore, our analysis of category-specific variations offers insights for targeted improvements in both model architecture and dataset design."}, {"title": "II. BACKGROUND", "content": ""}, {"title": "A. Object Detection", "content": "Within the field of computer vision, object detection can be seen as a critical problem in recognising and localising objects inside various images and videos. While the task may seem straightforward at first, its difficulty arises from the immense variability in object size, shape, orientation, occlusion, and lighting. Moreover, the context in which these objects appear adds another layer of complexity, requiring AI models capable of generalising across diverse environments and perspectives. Although notable progress has been made in this area, with state-of-the-art technologies such as YOLOv10 [7], YOLO11 [8] and RT-DETR [9], utilising advanced algorithms to tackle these challenges, achieving human-level proficiency remains a"}, {"title": "B. Depth Prediction", "content": "Conversely, depth prediction involves determining the distance of each pixel in an image relative to the camera, effectively reconstructing a scene in three dimensions [10]. This can be achieved using either monocular images (from a single viewpoint) or stereo images (from multiple perspectives of the same scene). Traditional approaches rely on multi-view geometry to establish the spatial relationships between images and calculate depth. However, recent advances in deep learning have introduced more sophisticated techniques [11], allowing for more accurate and robust depth prediction from even single images, bypassing the limitations of classical geometric methods. These modern approaches have proven crucial in applications such as autonomous driving, robotics, and augmented reality."}, {"title": "C. Saliency Prediction", "content": "Visual saliency refers to the ability to identify regions in an image that are most likely to attract human attention or be important for machine learning models [12]. Saliency maps are used to highlight these areas, showing where a viewer's gaze naturally lands or which parts of an image hold the greatest relevance for computational analysis. While early approaches such as [12] relied on basic visual features such as contrast, color, and edges, advances in deep learning have enabled more accurate predictions [13]. However, saliency is inherently subjective, varying with individual perception, context, and the task at hand, making its prediction a complex challenge [14]."}, {"title": "III. RELATED WORK", "content": "Studies exploring the correlation between object location and various AI techniques have been conducted to understand how individuals perceive and locate objects. In particular, T. Boger and T. Ullman [15] performed a series of tests to examine how people determine the position of objects. Their experimental setup involved 50 participants, each tasked with clicking on the centre of mass for 50 randomly assigned images. The authors also evaluated eight AI models to assess the correlation between participant input and model predic-tions, assessing the models' adaptability and how closely their performance aligns with human-like proficiency.\nBy varying the stimuli in each experiment, they found that physical reasoning, specifically using the centre of mass, consistently plays the most significant role in perceived object location, regardless of the object's realism. However, while insightful, this study is limited by the small number of images and its focus solely on static objects, leaving the perception of dynamic objects in non-iconic scenes unexplored."}, {"title": "IV. METHODOLOGY", "content": "This paper proposes a series of experiments to evaluate the correlation between depth prediction and saliency prediction in object detection on a larger scale using popular object detection datasets. These experiments are designed to test how well depth and saliency prediction techniques align with object detection performance."}, {"title": "A. Datasets", "content": "Two popular object detection datasets with similar structures were used as the basis of the experimental structure to evaluate these approaches. The widely used COCO dataset [16] was chosen to assess the correlation across a diverse range of images, featuring 80 categories and over 200,000 images. The smaller and less commonly used Pascal VOC dataset [17] was also included to examine the setup on a dataset with 20 categories and over 11,000 images that feature larger objects. Both datasets include segmentation masks that serve as ground truth for evaluating the performance of the prediction models. The experimental setup was tested on the COCO 2017 training dataset and the Pascal VOC 2012 dataset."}, {"title": "B. Models", "content": "To explore the correlation between object detection, visual saliency, and depth prediction, four prediction models were utilised: two depth prediction models and two saliency pre-diction models.\nDepth Anything \u2013 A widely recognised depth prediction network that is known for its robustness in monocular depth prediction across various conditions. It serves as a foundational model tested on numerous public datasets [11].\nDPT-Large Was selected as another depth prediction model due to its advanced architecture, which incorporates dense vision transformers. Despite its large size, DPT-Large is effective for monocular depth prediction and includes tech-niques to address the loss of feature granularity [18].\nItti's Visual Attention Model \u2013 Inspired by the behaviour and neurology of the early primates' visual cortex, was chosen"}, {"title": "Algorithm 1 Experimental Pipeline", "content": "Input: Dataset annotations, Prediction Model\nLoad annotations from the Dataset\nPreprocess annotations\nLoad Prediction Model\nfor each image i in the dataset do\np\u2190 inferPrediction(i)\nr\u2190 computePearsonCorrelation(p, ground truth)\nend for\nM\u2190 meanCorrelationPerCategory(all images)\nOutput: M"}, {"title": "C. Experimental Setup", "content": "The algorithm outlined in Algorithm 1 describes the exper-imental pipeline for evaluating the performance of the chosen prediction models using a given dataset. The process begins by loading the dataset annotations and the prediction model. For each image in the dataset, the model generates predictions, which are then compared to the ground truth using Pearson correlation (refer to Equation 1). This correlation is computed for each image, and the mean correlation per category is calculated across all images (refer to Equation 2)."}, {"title": "V. EVALUATION", "content": ""}, {"title": "A. Metrics", "content": "The primary evaluation metric used was Pearson correlation (\u03c1), as shown in Equation 1. Pearson correlation measures the linear relationship between the ground truth and the generated depth or saliency map, evaluating how well the predictions align with the actual values, as described in [19]. This met-ric focuses on the strength and direction of the correlation between two datasets, disregarding differences in intensity or scale. A higher Pearson correlation value indicates a stronger relationship between the predicted and true values, with a value of 1 representing a perfect linear correlation.\n$PX,Y = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$  (1)\nTo evaluate the overall performance across multiple classes or categories, we utilised the mean Average Pearson Corre-lation (mAp), which averages the Pearson correlation across all classes, as defined in Equation 2. This metric provides a more comprehensive view of the model's performance by calculating the average Pearson correlation across all classes, where C represents the total number of classes or categories. For each class c, the covariance between the ground truth and the predictions is divided by the product of their standard deviations. A higher map value indicates stronger correlations across all classes, with a value of 1 signifying perfect align-ment between predictions and ground truth for every class.\n$mApx,y = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{Cov (Xc, Yc)}{\\sigma_{Xc} \\sigma_{Yc}}$ (2)"}, {"title": "B. Results", "content": "1) Comparative Performance: The proposed experiment was conducted using an NVIDIA GeForce RTX 4070 GPU. The Mean Average Pearson Correlation (mAp) and the average runtime per image were measured for each experiment on the Pascal VOC and COCO datasets, as presented in Table II. Additionally, Table I displays the Average Pearson Correlation Results per Class for various categories in the COCO dataset using the Depth Anything model. This table was included to assess the distribution of individual category results for the least-performing model.\n2) Dataset Analysis: Comparing the results in Table I, DeepGaze IIE emerges as the most effective model, out-performing others with consistently higher Mean Average Pearson Correlation (mAp) values. However, the overall low mAp scores on the COCO dataset, compared to the relatively better performance on PASCAL VOC, suggest that contextual complexity may challenge model accuracy. This discrepancy could be due to differences in dataset characteristics: PASCAL VOC's smaller dataset size and larger object scales may provide less ambiguous visual contexts, allowing models to more easily detect and predict objects accurately. COCO, by contrast, focuses on non-iconic, complex scenes where objects frequently appear in diverse contexts with more categories and instances per image (averaging 3.5 categories and 7.7 instances per image), adding a layer of difficulty for models that may struggle to isolate relevant visual cues from background information [20]. In COCO, only 10% of images contain a single category, whereas over 60% of PASCAL VOC images feature one category, further underscoring how COCO's dense object presence and multi-object contexts might inhibit model precision [20].\nInterestingly, DPT-Large, despite slightly slower execution times, achieved higher map than Depth Anything, which was the fastest model, while the Itti-Koch model, inspired by the primate visual cortex, lagged in accuracy, highlight-ing the advantage of more sophisticated saliency algorithms. The stronger performance of DeepGaze IIE and DPT-Large suggests that visual saliency, rather than depth prediction alone, may contribute more significantly to detection accuracy, especially within diverse visual contexts. Individual class results, particularly for the least performing depth model"}, {"title": "C. Discussion", "content": "1) Implications of Findings: The implications of these findings suggest that the correlations observed between depth prediction, saliency prediction, and object detection accuracy could serve as a basis for advancing multitask learning frame-works within object detection, as well as improve computa-tional efficiency. The notable relationship between saliency prediction and detection accuracy indicates that incorporating saliency-focused tasks may enhance object detection mod-els by aligning them more closely with human perceptual tendencies, especially in visually complex environments. By leveraging the strengths of both depth and saliency predictions, object detection models could benefit from the contextual information provided by these tasks, potentially leading to improved robustness and accuracy in diverse and challenging scenes.\nFurthermore, the results highlight that saliency prediction, demonstrating a notably higher correlation with detection ac-curacy than depth prediction, may serve as a valuable tool for evaluating and refining object detection datasets. By examining the specific saliency aspects that contribute most to detection accuracy, researchers and practitioners could assess dataset viability for certain detection tasks and incorporate saliency cues directly into detection architectures. Such integration could streamline multitask learning, enabling models to more effectively prioritise salient features and, thereby, enhance detection performance across varied image contexts.\nIn contrast, while depth prediction showed lower correla-tions with detection accuracy, it holds potential for tasks in-volving the assessment of object size and scale differentiation. Depth models, although less directly correlated with detection"}, {"title": "2) Limitations of Dataset Design", "content": "While this paper pri-marily explored the correlation between depth estimation and saliency prediction in relation to object detection, it also highlighted the critical importance of thoughtful dataset design, as evidenced by the discrepancy in results between the Pascal VOC and COCO datasets. A balanced dataset that adequately represents various object sizes, scales, and categories is essential for training robust object detection models. The findings highlight that the average number of categories and instances per image can significantly influence model performance. Specifically, the integration of diverse object representations and a careful consideration of how these elements interact within the dataset can enhance the overall effectiveness of detection algorithms, leading to improved accuracy and reliability in real-world applications.\nFurthermore, the models utilised in this study, particularly those focused on saliency prediction, exhibit a notable em-phasis on centre bias [14], which reflects the subjectivity inherent in saliency estimation. This subjectivity arises from the tendency of models to prioritise central objects within an image, potentially overlooking important contextual elements [14]. In contrast, the COCO dataset is characterised by its inclusion of non-iconic scenes that present objects within complex and varied backgrounds. This focus on realistic scenarios underscores the necessity of developing models that can effectively navigate and interpret such contexts, where the traditional assumptions of saliency may not apply."}, {"title": "VI. CONCLUSION", "content": "This paper investigates the relationships between object detection accuracy and two essential visual tasks: depth prediction and visual saliency prediction. Our experiments with state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets reveal that visual saliency demonstrates consistently stronger correlations with detection accuracy, achieving a Mean Average Pearson Correlation (mAp) up to 0.459 on Pascal VOC, compared to depth prediction (mAp up to 0.283). Larger objects show correlation values up to three times higher than smaller ones, highlighting the impact of object scale on model performance. These findings suggest that incorporating visual saliency features into object detection frameworks could be particularly beneficial for specific object categories. Moreover, the observed category-specific variations offer valuable insights for optimising feature engineering and guiding dataset design, potentially leading to more efficient and accurate object detection systems aligned with human object perception."}]}