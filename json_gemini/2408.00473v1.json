{"title": "TOWARDS EXPLAINABLE AND INTERPRETABLE MUSICAL\nDIFFICULTY ESTIMATION: A PARAMETER-EFFICIENT APPROACH", "authors": ["Pedro Ramoneda", "Vsevolod Eremenko", "Alexandre D'Hooge", "Xavier Serra", "Emilia Parada-Cabaleiro"], "abstract": "Estimating music piece difficulty is important for orga-\nnizing educational music collections. This process could\nbe partially automatized to facilitate the educator's role.\nNevertheless, the decisions performed by prevalent deep-\nlearning models are hardly understandable, which may im-\npair the acceptance of such a technology in music edu-\ncation curricula. Our work employs explainable descrip-\ntors for difficulty estimation in symbolic music represen-\ntations. Furthermore, through a novel parameter-efficient\nwhite-box model, we outperform previous efforts while de-\nlivering interpretable results. These comprehensible out-\ncomes emulate the functionality of a rubric, a tool widely\nused in music education. Our approach, evaluated in piano\nrepertoire categorized in 9 classes, achieved 41.4% accu-\nracy independently, with a mean squared error (MSE) of\n1.7, showing precise difficulty estimation. Through our\nbaseline, we illustrate how building on top of past research\ncan offer alternatives for music difficulty assessment which\nare explainable and interpretable. With this, we aim to pro-\nmote a more effective communication between the Music\nInformation Retrieval (MIR) community and the music ed-\nucation one.", "sections": [{"title": "1. INTRODUCTION", "content": "Estimating the difficulty of music pieces aids in organiz-\ning large collections for music education purposes. How-\never, manually assigning difficulty levels is laborious and\nmight lead to subjective errors [1]. To address this, Music\nInformation Retrieval (MIR) research has focused on au-\ntomating this process for piano works represented in vari-\nous modalities [2\u20136] as well as repertoires from other in-\nstruments [7, 8]. Furthermore, the interest of companies\nlike Muse Group [9, 10] and Yousician [11] highlights the\nindustry's recognition of the importance of the task.\nPrevious work in this field has mainly focused on pro-\ncessing machine-readable symbolic scores [1\u20134, 12\u201315]."}, {"title": "2. RELATED WORKS", "content": "Previous research aiming to automatically assess the dif-\nficulty of piano repertoire examined the link between fin-\ngering patterns and the pieces' difficulty level [2, 14, 15].\nRecent studies [1,3,4,23] have also made significant con-\ntributions. In [3], representations are used to feed three\ndeep learning models-covering music notation, physical\ngestures, and expressiveness to emulate Cook's dimen-\nsions [24]. These models' predictions are merged using an\nensemble method to estimate the scores' difficulty. While\nwe appreciate their musicology-inspired approach, its lack\nof interpretability harms its usability.\nDifficulty estimation of piano pieces has also been in-\nvestigated through hybrid methods that merge features\nwith deep learning models [1,23]. However, the absence\nof publicly shared data and code complicates perform-\ning comparative analyses with reference to these works.\nIn [23], the authors combine the methods from Chiu and\nChen [13] with deep learning models trained using piano\nroll as input. In a similar vein, [1] uses JSymbolic features\n[25] and deep learning models on a proprietary dataset.\nIn the study by Chiu and Chen [13], 159 pieces from\nthe 8notes website were used, whereas [23] utilized 1800\nMIDI files from the same source. The categorization of\nthese pieces, provided by users of 8notes, raises concerns\nabout their reliability. Unfortunately, neither study pro-\nvides access to their data or details on how they were seg-\nmented. Other work [26] has attempted to understand the\neffectiveness of various features, including those proposed\nin [13] for categorizing the grade levels of a specific pi-\nano curriculum. Recent efforts by Zhang et al. [4] and"}, {"title": "3. INTERPRETABLE Rubric Net", "content": "The RubricNet model (cf. Figure 2) is designed to provide\ninterpretability akin to a rubric, enabling its analysis and\nresults to be intuitively aligned with established practices\nin music education. This approach ensures that the model's\nlogic and outcomes are easily comprehensible, facilitating\ntheir usage in music education along to traditional tools."}, {"title": "3.1 Model Architecture", "content": "The network, comprising a series of linear layers dedicated\nto process individual input descriptors and followed by a\nnonlinear activation function, is formulated as follows:\nGiven a set of N input descriptors, each descriptor \\(x_i\\)\nis first processed through its dedicated linear layer with\nweight \\(w_i\\) and bias \\(b_i\\), followed by a hyperbolic tangent\nactivation function to yield:\n\\(s_i = tanh(w_i x_i + b_i)\\)  (1)\nwhere \\(s_i\\) represents the processed score for the i-th de-\nscriptor. Scores are then aggregated in a single score \\(S_{agg}\\):\n\\(S_{agg} = \\sum_{i=1}^{N} s_i\\) (2)\nThe aggregated score \\(S_{agg}\\) is then passed through a final\nlinear layer to obtain the logits for the class predictions,\nwhich are mapped to probabilities with a sigmoid function:\n\\(P = \\sigma(S_{agg}w_f + b_f)\\) (3)\nwhere \\(\u03c3\\) denotes the sigmoid function, \\(w_f\\) and \\(b_f\\) are the\nweight and bias of the final linear layer, respectively."}, {"title": "3.2 Ordinal Optimization", "content": "This model applies an ordinal optimization approach [28],\npredicting ordered categorical outcomes, i. e., difficulty"}, {"title": "3.3 Interpretability", "content": "In RubricNet, the descriptors (automatically computed\nfrom the data) are, to some extent, comparable to the for-\nmalized evaluation criteria defined in traditional rubrics;\nsimilarly, the aggregated score, might be comparable to\na final grade/mark assigned in an educational scenario.\nGiven the correspondences between both, we could con-\nsider the model a \"white-box\" approach, able to promote\ntransparency and interpretability, similarly to a rubric.\nIt uses independent linear transformations on input de-\nscriptors to generate scores between -1 and 1, which di-\nrectly influence the regression output, \\(S_{agg}\\). Since negative\nscores, might be not fully understood in terms of difficulty\nlevel, we normalize scores between 0 and 1, rescaling \\(S_{agg}\\)\nbetween 0 and 12. This approach mirrors rubric's ability\nto provide objective and structured feedback, with the sim-\nplicity of these transformations aiding in understanding the\nimpact of features in predictions.\nThe interpretability of the model lies in its ability to dis-\nsect each descriptors' influence on a piece's difficulty level.\nConsequently, analyzing each descriptor's scores might re-\nveal its overall importance on the prediction. Lastly, \\(S_{agg}\\)\nis a continuous-ordered scalar with rank correlation to dif-\nficulty. Therefore, from \\(S_{agg}\\), we retrieve ordered and dis-\ncrete categories with clear decision boundaries."}, {"title": "4. EXPLAINABLE DESCRIPTORS", "content": "From codified musical scores, we extracted numeric fea-\ntures which are feed to a classification algorithm. We re-\nimplemented a set of features from the literature [13] while\nproposing a novel one, Pitch Set LZ. In addition to explain-\ning the features (cf. Table 1), we will provide their tech-\nnical descriptions and analyze their relevance to difficulty\nand interdependencies using the data."}, {"title": "4.1 Descriptors", "content": "In our work, we analyze music sheets encoded in symbolic\nformat, focusing on extracting pitch and timing. Follow-\ning the approach suggested by Chiu and Chen [13], we"}, {"title": "Pitch Entropy.", "content": "The entropy of pitches in the pitch events:\n\\(- \\sum_{i \\in P} p(P = i) log_2 p(P = i)\\) (5)"}, {"title": "Average IOI:", "content": "Average Inter Onset Interval. A concept\nsimilar to the \"Playing speed\" introduced by [13], a term\nwe consider deceptive since it actually decreases as the\nhand's \"speed\" increases. This is an average time in sec-\nonds between onsets of two consecutive pitch set events.\nLet's denote ith onset time with \\(T_i\\), then the value is:\n\\(\\frac{\\sum_{1<i<N_{events}-1}(T_{i+1} - T_i)}{N_{events} - 1}\\) (6)"}, {"title": "4.2 Feature Analysis", "content": "We assume that, for easier interpretability, features must\non average change monotonically with the difficulty level.\nTo measure this quality, we use the \\(\u03c4_c\\) version of Kendall\nranks correlation coefficient due to its ability to deal with\n\"heavily tied\" rankings [33] (many musical pieces have the\nsame difficulty, hence, we have multiple ties in the ranking\nby difficulty). \\(\u03c4_c\\) is equal to 1 when feature and difficulty\nrankings are perfectly aligned in the same direction, -1 if\nthey are aligned in opposite directions. As the number of\nnonconcordant cases increases, the coefficient approaches\nzero. In Table 2, the results show that the features related\nto pitch organization are the most correlated to difficulty.\nHand displacement and Inter-onset intervals are less corre-\nlated, while average pitch seems almost irrelevant.\nIn addition, we aim to uncover dependencies among the\nfeatures themselves while mitigating the influence of diffi-\nculty, with whom most features are correlated. To achieve\nthis, we calculate conditional \\(\u03c4_c\\) correlations for all feature\npairs given a fixed difficulty level, and average the coeffi-\ncients across all difficulty levels. We then convert these co-\nefficients into a distance matrix and apply hierarchical ag-\nglomerative clustering based on average distance to iden-\ntify clusters of correlated features. From the resulting den-\ndrogram (cf. Figure 3), we observe that features correlated\nwith difficulty-namely Pitch Entropy, Pitch Set LZ, and\nPitch Range are also interrelated. This is remarkable be-\ncause the three most correlated features are not inherently\ndependent: one could envision a music piece with any of\nthem maximized while maintaining low values for the oth-\ners. However, pieces in CIPI typically exhibit coordinated"}, {"title": "5. EXPERIMENTS", "content": "To evaluate the effectiveness of our proposed method, we\nutilized the Mikrokosmos-difficulty (MKD), and Can I play\nit? (CIPI) datasets [3]. For fair comparison, we use the 5-\nfold cross-validation approach defined in [3]. In each split,\n60% of the data is used as a train set, while the remaining\nis equally divided into validation and test sets.\nAs in [3], we employ mean squared error (MSE) and\naccuracy within n classes (Acc-n) for evaluation. These\nmetrics are chosen for their applicability to ordinal classifi-\ncation challenges, with Acc-n assessing the model's accu-\nracy for n classes from the true labels, and MSE measuring\nthe average squared prediction error across classes. The\neffects of dataset imbalances and a fair evaluation across\nclasses are mitigated by macro-averaged metrics.\nWe optimize the models during training through Adam\noptimizer with a learning rate of \\(10^{-2}\\). The training pro-\ncess incorporates early stopping, based on the Acc-n and\nMSE metrics from the validation set, to prevent overfit-\nting. Through Ordinal Loss, we frame difficulty prediction\nas an ordinal classification task, as mentioned in Section 3.\nWe apply a standard scaler and dropout to the features to\nprevent individual ones from dominating. For each experi-\nment, we look for the best hyperparameters using Bayesian\noptimization [34]: batch size within the range from 16 to\n128, dropout rate between 0.1 and 0.5, learning rate decay\nfrom 0.1 to 0.9, and the learning rate itself, tested over a\nlogarithmic scale from 1e - 5 to 1e - 1. This approach al-\nlows us to systematically explore the hyperparameter space\nand identify the optimal settings for our models; thus, en-\nabling a fair comparison between experiments."}, {"title": "5.1 Experimental Setup", "content": "To evaluate the effectiveness of our proposed method, we\nutilized the Mikrokosmos-difficulty (MKD), and Can I play\nit? (CIPI) datasets [3]. For fair comparison, we use the 5-\nfold cross-validation approach defined in [3]. In each split,\n60% of the data is used as a train set, while the remaining\nis equally divided into validation and test sets.\nAs in [3], we employ mean squared error (MSE) and\naccuracy within n classes (Acc-n) for evaluation. These\nmetrics are chosen for their applicability to ordinal classifi-\ncation challenges, with Acc-n assessing the model's accu-\nracy for n classes from the true labels, and MSE measuring\nthe average squared prediction error across classes. The\neffects of dataset imbalances and a fair evaluation across\nclasses are mitigated by macro-averaged metrics.\nWe optimize the models during training through Adam\noptimizer with a learning rate of \\(10^{-2}\\). The training pro-\ncess incorporates early stopping, based on the Acc-n and\nMSE metrics from the validation set, to prevent overfit-\nting. Through Ordinal Loss, we frame difficulty prediction\nas an ordinal classification task, as mentioned in Section 3.\nWe apply a standard scaler and dropout to the features to\nprevent individual ones from dominating. For each experi-\nment, we look for the best hyperparameters using Bayesian\noptimization [34]: batch size within the range from 16 to\n128, dropout rate between 0.1 and 0.5, learning rate decay\nfrom 0.1 to 0.9, and the learning rate itself, tested over a\nlogarithmic scale from 1e - 5 to 1e - 1. This approach al-\nlows us to systematically explore the hyperparameter space\nand identify the optimal settings for our models; thus, en-\nabling a fair comparison between experiments."}, {"title": "5.2 Experimental Results", "content": "In Table 3, the results from the comparison between the\nperformance of our novel approach with the presented de-"}, {"title": "5.3 Decision Boundaries", "content": "In RubricNet, the input features are combined into a sin-\ngle scalar before performing the final ordinal classification.\nAnalysis of the results shows that the final layer defines\noptimized decision boundaries, setting thresholds for \\(S_{agg}\\)\nthat progressively increase along with difficulty levels. Be-\ncause of the final sigmoid activation, once \\(S_{agg}\\) exceeds a\nboundary, the corresponding difficulty level will always be\nactive, which guarantees the ordinality of the predictions.\nBy examining the decision boundaries (cf. Figure 4), we\nobserve that the trends are similar across splits, displaying\nshorter valid ranges around intermediate levels. Note that\nin split 2, there are only 8 classes because the model ig-\nnored the last class. This can happen as we use numeric\noptimization, which sometimes falls into local minima.\nThese minima might seem optimal based on the validation\nmetrics but do not meet our overall performance expecta-\ntions."}, {"title": "6. DISCUSSION AND LIMITATIONS", "content": "Now, we analyze whether RubricNet is interpretable from\na musical point of view. To understand how features im-\npact the final level suggested by the model, we evaluate\nthe contribution of each descriptor to the aggregated score.\nSince learning to play an instrument is a progressive pro-\ncess, relative contributions of features to different levels"}, {"title": "7. CONCLUSION AND FUTURE WORK", "content": "In our study, we proposed a novel white-box parameter-\nefficient model aligned with the music education commu-\nnity tools, i. e., rubrics, which outperforms previous ap-\nproaches on difficulty estimation. In addition, we created\nan interactive companion page for visualizing CIPI and\nMKD datasets. In summary, we showed that analyzing\nexplainable descriptors, unlike deep learning models, of-\nfers clarity, which gives both teachers and students specific\ninsights into pieces. This approach not only underscores\nthe importance of explainable artificial intelligence (XAI)\nin understanding music difficulty, but also emphasizes the\npotential for such technologies to contribute to the broader\nfield of music education. For future research, we consider\ninteresting to creating a dataset based on technical chal-\nlenges like finger fluency and polyphonic complexity, as\nwell as user studies for understanding the perception of in-\nterpretable feedback by music education community."}, {"title": "8. ETHICS STATEMENT", "content": "The system presented in this paper aims at obtaining the\ndifficulty of a musical piece through several descriptors.\nIn previous work, descriptors were not available, limiting\naccess to the area. This situation underscores the need for\nopen science practices. Therefore, we open our implemen-\ntation, to facilitate access for new researchers. Besides,\nthe dataset used for this study is available upon request\nfor non-profit and academic research purposes. While this\nlimits its use in commercial applications, it ensures the\nreproducibility of the results. The data consists of open-\nsource scores of music that is no longer copyrighted, its\nuse for open research can thus be considered fair.\nThe proposed work belongs to the area of assisted mu-\nsic learning. One might argue that such a tool can have\na detrimental impact on music teaching jobs. While this\nis a valid concern, we think that an eventual solution of\nthe addressed task, would not endanger music educators\nprofession, whose role naturally goes much beyond than\ncategorizing music in difficulty levels. Instead, this tech-\nnology should be seen as a way to support them in the own\nteaching practices, for instance, by alleviating their burden\non some duties, such as exploring large collections, and\nby this enabling them to easily discover forgotten musical\nworks from our cultural heritage which fit students' needs.\nMoreover, through this research, we also aim to convey\nthe message that the path to advancement does not solely\nlie in acquiring more data or creating larger models. By\nhighlighting what drives its decisions, our proposed model\naligns with the goals of explainable AI, something cru-\ncial for its acceptance in music education. Although our\nefforts in making the system interpretable and explainable\nwill partly answer the common criticisms made to black-\nbox approaches, the real impact of our system remains to\nbe verified by its future use in real scenarios."}]}