{"title": "TOWARDS EXPLAINABLE AND INTERPRETABLE MUSICAL DIFFICULTY ESTIMATION: A PARAMETER-EFFICIENT APPROACH", "authors": ["Pedro Ramoneda", "Vsevolod Eremenko", "Alexandre D'Hooge", "Xavier Serra", "Emilia Parada-Cabaleiro"], "abstract": "Estimating music piece difficulty is important for organizing educational music collections. This process could be partially automatized to facilitate the educator's role. Nevertheless, the decisions performed by prevalent deep-learning models are hardly understandable, which may impair the acceptance of such a technology in music education curricula. Our work employs explainable descriptors for difficulty estimation in symbolic music representations. Furthermore, through a novel parameter-efficient white-box model, we outperform previous efforts while delivering interpretable results. These comprehensible outcomes emulate the functionality of a rubric, a tool widely used in music education. Our approach, evaluated in piano repertoire categorized in 9 classes, achieved 41.4% accuracy independently, with a mean squared error (MSE) of 1.7, showing precise difficulty estimation. Through our baseline, we illustrate how building on top of past research can offer alternatives for music difficulty assessment which are explainable and interpretable. With this, we aim to promote a more effective communication between the Music Information Retrieval (MIR) community and the music education one.", "sections": [{"title": "1. INTRODUCTION", "content": "Estimating the difficulty of music pieces aids in organizing large collections for music education purposes. However, manually assigning difficulty levels is laborious and might lead to subjective errors [1]. To address this, Music Information Retrieval (MIR) research has focused on automating this process for piano works represented in various modalities [2-6] as well as repertoires from other instruments [7, 8]. Furthermore, the interest of companies like Muse Group [9, 10] and Yousician [11] highlights the industry's recognition of the importance of the task.\nPrevious work in this field has mainly focused on processing machine-readable symbolic scores [1-4, 12-15]. aim to contribute to music education by facilitating the understanding of measurable factors that determine a piece's difficulty. Our work builds on methods from music education, where objectively assessing abstract competences through measuring concrete criteria is consolidated by employing rubrics, i. e., tools which, unlike a black-box, break down complex concepts into simpler ones [19, 22].\nOur interpretable methodology aims to bridge the gap between computational models and practical music education needs, enabling educators to make facilitated, but also informed decisions about curriculum development based on the difficulty levels of pieces. We release all the code and models of this research2, in order to offer a baseline for further research in music difficulty assessment."}, {"title": "2. RELATED WORKS", "content": "Previous research aiming to automatically assess the difficulty of piano repertoire examined the link between fingering patterns and the pieces' difficulty level [2, 14, 15]. Recent studies [1,3,4,23] have also made significant contributions. In [3], representations are used to feed three deep learning models-covering music notation, physical gestures, and expressiveness-to emulate Cook's dimensions [24]. These models' predictions are merged using an ensemble method to estimate the scores' difficulty. While we appreciate their musicology-inspired approach, its lack of interpretability harms its usability.\nDifficulty estimation of piano pieces has also been investigated through hybrid methods that merge features with deep learning models [1,23]. However, the absence of publicly shared data and code complicates performing comparative analyses with reference to these works. In [23], the authors combine the methods from Chiu and Chen [13] with deep learning models trained using piano roll as input. In a similar vein, [1] uses JSymbolic features [25] and deep learning models on a proprietary dataset.\nIn the study by Chiu and Chen [13], 159 pieces from the 8notes website were used, whereas [23] utilized 1800 MIDI files from the same source. The categorization of these pieces, provided by users of 8notes, raises concerns about their reliability. Unfortunately, neither study provides access to their data or details on how they were segmented. Other work [26] has attempted to understand the effectiveness of various features, including those proposed in [13] for categorizing the grade levels of a specific piano curriculum. Recent efforts by Zhang et al. [4] and Ramoneda et al. [3] have focused on compiling datasets with difficulty annotations from the established piano publisher Henle Verlag, with the latter's dataset not only being the most extensive but also the only one made publicly available. Therefore, for our comparative analysis, we will use the open-source datasets presented in [3], namely Can I Play It? (CIPI), which has 9 levels of difficulty, and Mikrokosmos-difficulty (MKD), which includes 3 levels.\nFinally, in order to validate our approach, we consider a different and established feature set, i.e., the standard music symbolic features available through Music21 library [27], which includes (amongst others) established JSymbolic features [25], thus facilitating a meaningful comparison with our proposed descriptors. In addition, we also contrast the results achieved with our novel descriptors with those obtained with the features by Chiu and Chen [13], which are also reimplemented and open-sourced in this study. Note that none of the approaches previously mentioned has focused on the interpretability of the descriptors, which is a key contribution of our work."}, {"title": "3. INTERPRETABLE Rubric Net", "content": "The RubricNet model (cf. Figure 2) is designed to provide interpretability akin to a rubric, enabling its analysis and results to be intuitively aligned with established practices in music education. This approach ensures that the model's logic and outcomes are easily comprehensible, facilitating their usage in music education along to traditional tools."}, {"title": "3.1 Model Architecture", "content": "The network, comprising a series of linear layers dedicated to process individual input descriptors and followed by a nonlinear activation function, is formulated as follows:\nGiven a set of N input descriptors, each descriptor $x_i$ is first processed through its dedicated linear layer with weight $w_i$ and bias $b_i$, followed by a hyperbolic tangent activation function to yield:\n$s_i = \\tanh(w_i x_i + b_i)$ (1)\nwhere $s_i$ represents the processed score for the i-th descriptor. Scores are then aggregated in a single score $S_{agg}$:\n$S_{agg} = \\sum_{i=1}^{N} s_i$ (2)\nThe aggregated score $S_{agg}$ is then passed through a final linear layer to obtain the logits for the class predictions, which are mapped to probabilities with a sigmoid function:\n$P = \\sigma(S_{agg}w_f^T + b_f)$ (3)\nwhere $\\sigma$ denotes the sigmoid function, $w_f^T$ and $b_f$ are the weight and bias of the final layer, respectively."}, {"title": "3.2 Ordinal Optimization", "content": "This model applies an ordinal optimization approach [28], predicting ordered categorical outcomes, i. e., difficulty levels such as beginner (1), intermediate (2), and advanced (3), through logits. These logits, computed using a mean squared error (MSE) loss, indicate the model's predictions on the ordinal scale. Difficulty level is then obtained as:\n$\\max{i \\text{ where } P_i \\geq 0.5 \\text{ and } P_j \\geq 0.5, \\forall j < i}$ (4)"}, {"title": "3.3 Interpretability", "content": "In RubricNet, the descriptors (automatically computed from the data) are, to some extent, comparable to the formalized evaluation criteria defined in traditional rubrics; similarly, the aggregated score, might be comparable to a final grade/mark assigned in an educational scenario. Given the correspondences between both, we could consider the model a \"white-box\" approach, able to promote transparency and interpretability, similarly to a rubric.\nIt uses independent linear transformations on input descriptors to generate scores between -1 and 1, which directly influence the regression output, $S_{agg}$. Since negative scores, might be not fully understood in terms of difficulty level, we normalize scores between 0 and 1, rescaling $S_{agg}$ between 0 and 12. This approach mirrors rubric's ability to provide objective and structured feedback, with the simplicity of these transformations aiding in understanding the impact of features in predictions.\nThe interpretability of the model lies in its ability to dissect each descriptors' influence on a piece's difficulty level. Consequently, analyzing each descriptor's scores might reveal its overall importance on the prediction. Lastly, $S_{agg}$ is a continuous-ordered scalar with rank correlation to difficulty. Therefore, from $S_{agg}$, we retrieve ordered and discrete categories with clear decision boundaries."}, {"title": "4. EXPLAINABLE DESCRIPTORS", "content": "From codified musical scores, we extracted numeric features which are feed to a classification algorithm. We reimplemented a set of features from the literature [13] while proposing a novel one, Pitch Set LZ. In addition to explaining the features (cf. Table 1), we will provide their technical descriptions and analyze their relevance to difficulty and interdependencies using the data."}, {"title": "4.1 Descriptors", "content": "In our work, we analyze music sheets encoded in symbolic format, focusing on extracting pitch and timing. Following the approach suggested by Chiu and Chen [13], we process left and right hand parts separately to clarify pedagogical aspects of musical difficulty. Our primary analysis involves sequences of pitch set events, each characterized by a pitch set $S$ and onset time $T$. Pitch sets, represented by sets of MIDI numbers, are defined over the alphabet of all pitch sets $S$ that occurred in a score part, while onset times are calculated in seconds from the performance start by the music21 library [27] with reference to marked tempo information. This method emphasizes the timing of note attacks, duration and rests. Additionally, we consider a collection of pitch events, each defined by pitch $P$ over the alphabet of all pitches $P$. Our analysis started with the five features identified by Chiu and Chen [13] as most relevant to understanding musical difficulty.\nPitch Entropy. The entropy of pitches in the pitch events:\n$-\\sum_{i \\in P} p(P = i) \\log_2 p(P = i)$ (5)\nPitch Range. The distance between the minimum and maximum MIDI pitches in a score part.\nAverage Pitch. The average MIDI pitch in a music sheet.\nDisplacement Rate. Initially proposed by [13], it quantifies the extent of hand movement across the keyboard during the performance of a score. It analyzes maximum pitch distances between consecutive pitch set events and is calculated as a weighted average of three categories: distances less than 7 semitones (assigned a weight of zero); distances over 7 semitones but under an octave (assigned a weight of one); and distances of an octave or larger (assigned a weight of two to emphasize larger movements).\nAverage IOI: Average Inter Onset Interval. A concept similar to the \"Playing speed\" introduced by [13], a term we consider deceptive since it actually decreases as the hand's \"speed\" increases. This is an average time in seconds between onsets of two consecutive pitch set events. Let's denote $i^{th}$ onset time with $T_i$, then the value is:\n$\\frac{\\sum_{1<i<N_{events}-1}(T_{i+1} - T_i)}{N_{events} - 1}$ (6)\nIn 23% of the scores, information about the recommended performance tempo is missing. We then assume the tempo is 100 beats per minute (bpm). Thus, in cases of missing bpm, the Average IOI feature might not be relevant.\nPitch Set LZ. Lempel-Ziv complexity of pitch set sequence. Before introducing our proposed descriptor, it is crucial to provide context and motivation. Pitch Entropy, as emphasized by Chiu and Chen [13], is particularly relevant-a conclusion supported by the analysis of correlations between difficulty and features in the following section, as well as by informal experiments. As Sayood discusses [29], there's a link between entropy of a task and the cognitive load it imposes on the performer, a concept that may also apply to music performance [30]. However, music is often perceived in terms of larger structures like phrases and sections, not just isolated pitches, prompting us to seek a descriptor that captures the \"repetitiveness\""}, {"title": "4.2 Feature Analysis", "content": "We assume that, for easier interpretability, features must on average change monotonically with the difficulty level. To measure this quality, we use the $\\tau_c$ version of Kendall rank correlation coefficient due to its ability to deal with \"heavily tied\" rankings [33] (many musical pieces have the same difficulty, hence, we have multiple ties in the ranking by difficulty). $\\tau_c$ is equal to 1 when feature and difficulty rankings are perfectly aligned in the same direction, -1 if they are aligned in opposite directions. As the number of nonconcordant cases increases, the coefficient approaches zero. In Table 2, the results show that the features related to pitch organization are the most correlated to difficulty. Hand displacement and Inter-onset intervals are less correlated, while average pitch seems almost irrelevant.\nIn addition, we aim to uncover dependencies among the features themselves while mitigating the influence of difficulty, with whom most features are correlated. To achieve this, we calculate conditional $\\tau_c$ correlations for all feature pairs given a fixed difficulty level, and average the coefficients across all difficulty levels. We then convert these coefficients into a distance matrix and apply hierarchical agglomerative clustering based on average distance to identify clusters of correlated features. From the resulting dendrogram (cf. Figure 3), we observe that features correlated with difficulty-namely Pitch Entropy, Pitch Set LZ, and Pitch Range-are also interrelated. This is remarkable because the three most correlated features are not inherently dependent: one could envision a music piece with any of them maximized while maintaining low values for the others. However, pieces in CIPI typically exhibit coordinated values in these descriptors. Thus, we mostly observe the combined effect of these features, making it challenging to reliably decompose \"difficulty\" into an aggregate of independent components."}, {"title": "5. EXPERIMENTS", "content": "To evaluate the effectiveness of our proposed method, we utilized the Mikrokosmos-difficulty (MKD), and Can I play it? (CIPI) datasets [3]. For fair comparison, we use the 5-fold cross-validation approach defined in [3]. In each split, 60% of the data is used as a train set, while the remaining is equally divided into validation and test sets.\nAs in [3], we employ mean squared error (MSE) and accuracy within n classes (Acc-n) for evaluation. These metrics are chosen for their applicability to ordinal classification challenges, with Acc-n assessing the model's accuracy for n classes from the true labels, and MSE measuring the average squared prediction error across classes. The effects of dataset imbalances and a fair evaluation across classes are mitigated by macro-averaged metrics.\nWe optimize the models during training through Adam optimizer with a learning rate of $10^{-2}$. The training process incorporates early stopping, based on the Acc-n and MSE metrics from the validation set, to prevent overfitting. Through Ordinal Loss, we frame difficulty prediction as an ordinal classification task, as mentioned in Section 3. We apply a standard scaler and dropout to the features to prevent individual ones from dominating. For each experiment, we look for the best hyperparameters using Bayesian optimization [34]: batch size within the range from 16 to 128, dropout rate between 0.1 and 0.5, learning rate decay from 0.1 to 0.9, and the learning rate itself, tested over a logarithmic scale from 1e - 5 to 1e - 1. This approach allows us to systematically explore the hyperparameter space and identify the optimal settings for our models; thus, enabling a fair comparison between experiments."}, {"title": "5.3 Decision Boundaries", "content": "In RubricNet, the input features are combined into a single scalar before performing the final ordinal classification.\nAnalysis of the results shows that the final layer defines optimized decision boundaries, setting thresholds for $S_{agg}$ that progressively increase along with difficulty levels. Because of the final sigmoid activation, once $S_{agg}$ exceeds a boundary, the corresponding difficulty level will always be active, which guarantees the ordinality of the predictions.By examining the decision boundaries (cf. Figure 4), we observe that the trends are similar across splits, displaying shorter valid ranges around intermediate levels. Note that in split 2, there are only 8 classes because the model ignored the last class. This can happen as we use numeric optimization, which sometimes falls into local minima. These minima might seem optimal based on the validation metrics but do not meet our overall performance expectations."}, {"title": "6. DISCUSSION AND LIMITATIONS", "content": "Now, we analyze whether RubricNet is interpretable from a musical point of view. To understand how features impact the final level suggested by the model, we evaluate the contribution of each descriptor to the aggregated score.Since learning to play an instrument is a progressive process, relative contributions of features to different levels with reference to the grade 1 are displayed instead of absolute values. These contributions are averaged across splits on the test set and shown in Figure 5.\nWe observe a trend of higher contributions when the level increases for every descriptor. This observation is consistent with the fact that $S_{agg}$ value increases for higher levels (cf. Figure 4). The most discriminative features are pitch entropy and pitch range, as well as the LZ descriptor for higher levels. Conversely, some features, e. g., average IOI or the average pitch, have low contribution to the model's decisions, as shown by their relatively constant and small values across grades. The latter is expected, since very different pieces could have the same average pitch, not disclosing anything about difficulty. The former might be explained by the averaging, which can remove information, especially when a piece can alternate between fast and slow parts. Besides, as mentioned before, tempo is often poorly annotated in the dataset.\nTo better understand the explainable capabilities of the proposed descriptors, in the following, we provide a musical examination of two concrete samples, by this demonstrating the interpretability of our approach. Nocturne op. 9, no. 3 by F. Chopin, is labelled as level 7, but classified as level 2. All the descriptors are below the grade average, as shown in the rubric (cf. Grade Divergence in Figure 6). Our hypothesis is that this nocturne contains many challenges that go beyond the descriptors used. There are constant changes in dynamics, a variety of articulations, and as a key difficulty aspect, many types of polyrhythms between the right and left hands. Further research should address all the types of difficulty challenges, probably underrepresented in the existing datasets."}, {"title": "7. CONCLUSION AND FUTURE WORK", "content": "In our study, we proposed a novel white-box parameter-efficient model aligned with the music education community tools, i. e., rubrics, which outperforms previous approaches on difficulty estimation. In addition, we created an interactive companion page for visualizing CIPI and MKD datasets. In summary, we showed that analyzing explainable descriptors, unlike deep learning models, offers clarity, which gives both teachers and students specific insights into pieces. This approach not only underscores the importance of explainable artificial intelligence (XAI) in understanding music difficulty, but also emphasizes the potential for such technologies to contribute to the broader field of music education. For future research, we consider interesting to creating a dataset based on technical challenges like finger fluency and polyphonic complexity, as well as user studies for understanding the perception of interpretable feedback by music education community."}, {"title": "8. ETHICS STATEMENT", "content": "The system presented in this paper aims at obtaining the difficulty of a musical piece through several descriptors. In previous work, descriptors were not available, limiting access to the area. This situation underscores the need for open science practices. Therefore, we open our implementation, to facilitate access for new researchers. Besides, the dataset used for this study is available upon request for non-profit and academic research purposes. While this limits its use in commercial applications, it ensures the reproducibility of the results. The data consists of open-source scores of music that is no longer copyrighted, its use for open research can thus be considered fair.\nThe proposed work belongs to the area of assisted music learning. One might argue that such a tool can have a detrimental impact on music teaching jobs. While this is a valid concern, we think that an eventual solution of the addressed task, would not endanger music educators profession, whose role naturally goes much beyond than categorizing music in difficulty levels. Instead, this technology should be seen as a way to support them in the own teaching practices, for instance, by alleviating their burden on some duties, such as exploring large collections, and by this enabling them to easily discover forgotten musical works from our cultural heritage which fit students' needs. Moreover, through this research, we also aim to convey the message that the path to advancement does not solely lie in acquiring more data or creating larger models. By highlighting what drives its decisions, our proposed model aligns with the goals of explainable AI, something crucial for its acceptance in music education. Although our efforts in making the system interpretable and explainable will partly answer the common criticisms made to black-box approaches, the real impact of our system remains to be verified by its future use in real scenarios."}]}