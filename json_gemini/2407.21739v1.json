{"title": "A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation", "authors": ["Mothilal Asokan", "Joseph Geo Benjamin", "Mohammad Yaqub", "Karthik Nandakumar"], "abstract": "Adapting foundation models for medical image analysis requires finetuning them on a considerable amount of data because of extreme distribution shifts between natural (source) data used for pretraining and medical (target) data. However, collecting task-specific medical data for such finetuning at a central location raises many privacy concerns. Although Federated learning (FL) provides an effective means for training on private decentralized data, communication costs in federating large foundation models can quickly become a significant bottleneck, impacting the solution's scalability. In this work, we address this problem of 'efficient communication while ensuring effective learning in FL' by combining the strengths of Parameter-Efficient Fine-tuning (PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LORA) in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical image segmentation. Unlike prior works that utilize LORA and finetune the entire decoder, we critically analyze the contribution of each granular component of SAM on finetuning performance. Thus, we identify specific layers to be federated that are very efficient in terms of communication cost while producing on-par accuracy. Our experiments show that retaining the parameters of the SAM model (including most of the decoder) in their original state during adaptation is beneficial because fine-tuning on small datasets tends to distort the inherent capabilities of the underlying foundation model. On Fed-KiTS, our approach decreases communication cost (~48\u00d7 \u2193) compared to full fine-tuning while increasing performance (~6%\u2191 Dice score) in 3D segmentation tasks. Our approach performs similar to SAMed while achieving ~2.8\u00d7 reduction in communication and parameters to be finetuned. We further validate our approach with experiments on Fed-IXI and Prostate MRI datasets. Our code is available at https://github.com/BioMedIA-MBZUAI/FLAP-SAM.", "sections": [{"title": "1 Introduction", "content": "Segmentation is one of the cornerstone tasks in modern medical image analysis for automated diagnosis and disease monitoring. While the advent of foundational models has pushed the boundaries of the state-of-the-art in many computer vision applications, such benefits are yet to transfer fully to the medical imaging domain [33]. For example, the Segment Anything Model (SAM) [14] has an excellent zero-shot generalization to new distributions and tasks involving natural images. However, the SAM model fails to generalize well across diverse medical imaging modalities due to the insurmountable distribution shifts [11]. Works like [6,10] highlight a substantial performance gap between the zero-shot inference and training on domain-specific medical images despite using various prompts in SAM. MSA [29] and SAM-Med2D [4] improve SAM by using tailored prompting techniques in 2D medical images. However, creating such prompts for each 2D slice of 3D data is labor-intensive.\nThe usual approach has been fine-tuning SAM for the target application using large task-specific datasets e.g., MedSAM[20]. For tasks with significant distribution shifts with limited data, the decoder block of SAM is fine-tuned while leaving the encoder untouched. In contrast to fine-tuning all parameters, parameter-efficient fine-tuning(PEFT) methods fine-tune only a minimal number of parameters using representative data such as prompt tuning [12] and low-rank adapters (LORA) [9]. Several works [22,28,32] have employed LoRA for fine-tuning, resulting in superior performance in various 2D segmentation tasks. For 3d segmentation, [2] uses factor tuning (FacT) adapters [13] for training."}, {"title": "2 Preliminaries", "content": "Overview of SAM: The architecture of SAM[14] can be decoupled into three major components: the Image Encoder (IE) to compute image embeddings, the Prompt Encoder (PE) to generate prompt embeddings, and the Mask Decoder (MD) that combines the image and prompt embeddings to generate segmentation masks as shown in Fig 2. Utilizing ViT [5] as the backbone, IE extracts image features through a sequence of L transformer blocks. Meanwhile, PE takes various input prompts in the form of points, boxes, or masks and encodes them into prompt embeddings to aid in segmentation tasks. We operate SAM in the fully automatic mode, in which a regular grid of foreground points is presented as input prompts to the PE, thus eliminating the dependence on user-defined prompts. MD performs cross-attention between the image and prompt embeddings, employing transposed convolutional layers for up-scaling back to image dimension (UP) and a hyper multi-layer perceptron (HYP) to produce segmentation masks. Following [2], we use a slightly modified SAM mask decoder that has two additional transposed convolutional layers, which up-sample the feature maps by 16\u00d7 to match the resolution of the input while ensuring improved discrimination of small anatomical structures or lesions in medical images [24]. For simplicity, let $\u0398_{IE}$, $\u0398_{PE}$, and $\u0398_{MD}$ denote the parameters of IE, PE, and MD, respectively. Also, $\u0398_{IE}$ can be further partitioned as $\u03b8_{IE-AT}$ and $\u03b8_{IE-NA}$, where $\u03b8_{IE-AT}$ denotes the parameters of all the attention layers within IE and $\u03b8_{IE-NA}$ represents all the other parameters in IE not related to the attention layers. On the other hand, $\u0398_{MD}$ can be partitioned as $\u0398_{MD-TR}$, $\u0398_{MD-UP}$, and $\u0398_{MD-HYP}$, where $\u0398_{MD-TR}$ denotes the parameters of all the transformer blocks within MD (such as self-attention, cross-attention from tokens to image embeddings (t2i), and cross-"}, {"title": "PEFT Formulation", "content": "The most straightforward approach to adapt a SAM model for a downstream task is to fine-tune all its parameters, including $\u03b8_{\u0399\u0395}$, $\u03b8_{\u03a1\u0395}$, and $\u0398_{MD}$. This full fine-tuning (FullFT) strategy requires more memory footprint to store a copy of all the updated parameters and often leads to overfitting when the data is severely limited. Recent works have shown that fine-tuning only the attention layers of a transformer encoder is sufficient for good adaptation [27]. This approach is called attention fine-tuning (AttnFT), where only $\u0398_{IE-AT}$ and $\u0398_{MD}$ are updated. Typically, the attention-related parameters of a transformer encoder constitute one-third of its overall parameters. Thus, AttnFT leads to some improvement in parameter efficiency and generally provides good performance on downstream tasks. Another common approach is to freeze the image encoder and fine-tune the entire mask decoder. This is because the cross-attention layers in the decoder focus on specific patches in the image embeddings corresponding to the prompts and transform them into segmentation predictions [31]. We call this approach DecFT, where $\u0398_{MD}$ is updated. It is also possible to freeze all the parameters and fine-tune only the output layers, namely, UP and HYP. This approach is analogous to linear probing in classification tasks, which we refer to as partial decoder fine tuning (PDecFT). Since only a small fraction of parameters, namely, $\u0398_{MD-UP}$, and $\u0398_{MD-Hyp}$ are updated, PDecFT has high parameter efficiency but usually provides only sub-optimal performance."}, {"title": "LoRA adapters", "content": "Low-rank adaptation [9] is a promising PEFT technique widely used for adapting foundation models to downstream tasks. Each attention layer within a transformer block has four weight (projection) matrices $W_q$ (query), $W_k$ (key), $W_v$ (value), and $W_o$ (output), where each $W \u2208 R^{dxd'}$. The core idea of LORA is to constrain the modifications to a pre-trained weight matrix $W$ to a linear update matrix $\u2206W$, which can be further constrained using a low-rank decomposition, i.e., $\u2206W = BA$, where $B \u2208 R^{dxr}, A \u2208 R^{rxd'}$ and the rank $r < min{d, d'}$. This approach effectively reduces the parameter space while preserving the essential information needed for adaptation. $W$ is frozen during fine-tuning, and only $A$ and $B$ matrices are updated. For input $x$, the output $\\hat{x}$ is computed as follows:\n$x = (W + \u03b1\u2206W)x = Wx + \u03b1\u2206Wx = Wx + \u03b1BAx$, (1)\nwhere \u03b1 is a scale parameter. Following [9], we employ LoRA only to the projection matrices $W_q$ and $W_v$ in all the attention layers of IE and MD. Let $\u0398_{LORA}$ represent the set of all LoRA parameters, where $\u0398_{LORA}$ = {$A^l, B^l, A^l_v, B^l_v$}$^L_{l=1}$. When only the LORA parameters are updated during fine-tuning, we refer to this case as LoRAFT. While this approach also has good parameter efficiency, it typically results in sub-optimal performance for segmentation tasks. To improve this, in SAMed [32], both the decoder and LoRA are fine-tuned. Here, $\u0398_{MD}$ and $\u0398_{LORA}$ parameters are updated together, and we represent this approach as LORADECFT."}, {"title": "3 Proposed FLAP-SAM Approach", "content": "We aim to efficiently and effectively adapt the SAM for medical image segmentation tasks using limited data distributed across multiple entities. Based on this consideration, we propose to update both $\u0398_{LORA}$ as well as the final decoder output layers $\u0398_{MD-UP}$ and $\u0398_{MD-HYP}$. This leads to the proposed fine-tuning for SAM, a hybrid of PDecFT and LoRAFT, as shown in Fig 2. Though there is a marginal increase in the number of parameters compared to LoRAFT, the proposed approach performs well because FLAP-SAM provides enough flexibility to be effectively fine-tuned. Moreover, since almost all the parameters of the original foundation model are retained without modification, its inherent capabilities remain unaffected. Another benefit is its memory efficiency, and the small parameter size of the proposed adapter makes it possible to learn them collaboratively via FL while greatly reducing communication costs.\nWhen aggregating LoRA parameters ($\u0398_{LORA}$), the kth client sends {$A^{q,k}_l$, $A^{v,k}_l$, $B^{q,k}_l$, $B^{v,k}_l$}$^L_{l=1}$ to the server. The server first needs to reconstruct $\u2206W^{q,k}_l$ = $B^{q,k}_l$$A^{q,k}_l$ and $\u2206W^{v,k}_l$ = $B^{v,k}_l$$A^{v,k}_l$ for each l and k, then performs FedAvg as shown in Eq. (3) to get the aggregated global weight matrices $\u2206W^q_l$ and $\u2206W^v_l$ of each attention layer l. Finally, the server applies singular value decomposition to decompose the aggregated matrices back to global LoRA parameters {$A^q_l, B^q_l, A^v_l, B^v_l$}$^L_{l=1}$, which are sent back to the clients. We refer to this federated learning of plug-and-play SAM adapter as FLAP-SAM."}, {"title": "4 Experiments", "content": "Datasets: We utilize Fed-KITS2019, a 6-client federated version of the KiTS19 dataset from FLamby [26], which was created from the Kidney Tumor Segmentation Challenge 2019 in CT scans [7,8]. Each client's train/test split is 9/3, 11/3, 9/3, 9/3, 12/4, and 24/6. The preprocessing pipeline comprises intensity clipping (5th and 95th percentile of image intensities of each client were calculated) followed by z-scale normalization, where we subtract the mean and divide by the standard deviation of the image intensities. Fed-IXI, extracted from the Information eXtraction from Images - IXI database [23,25] of brain T1 MRIs from 3 hospitals (Guys, HH, and IOP) contains 249/62, 145/36 and 59/15 train/test splits respectively. In a preprocessing step, min-max normalization was applied to each scan and padded with zeros in the axial plane (final shape 83 x 64 x 64). Prostate MRI is a multi-site segmentation dataset proposed by Liu et al. [18], comprises prostate T2-weighted MRI data from six different data sources (i.e., Site A to F) out of the three publicly available datasets: NCI-ISBI13 dataset [1], I2CVB dataset [15] and PROMISE12 dataset [17]. Each site has 30, 30, 19, 13, 12, 12 MRI scans of patients respectively and were randomly divided into train (\u2248 80%) and test(\u2248 20%) sets. Since they were acquired with varying imaging protocols and contain heterogeneous data distributions, we normalized each site to zero mean and unit variance to reduce the intensity variance among different sites. We resized it to 224 \u00d7 224 in the axial plane."}, {"title": "4.1 Results and Discussion", "content": "Our proposed FLAP-SAM method achieves 6% absolute improvement in Dice score compared to the FullFT approach, with a ~49\u00d7 reduction in communication overhead on Fed-KITS. Due to the small size of the dataset, the FullFT approach easily results in overfitting, highlighting the importance of using PEFT methods in limited data settings [3]. The Attention fine-tuning (AttnFT) only achieves half of our improvement (2.8% less than FLAP-SAM) and still incurs ~17\u00d7 more communication cost than our method. Both LoRAFT and PDecFT are more efficient but have lower Dice scores than our method. The LoRADECFT achieves an equivalent dice score to our method, but our method is ~2.8\u00d7 more efficient regarding parameters and communication. We conduct ablation on the rank parameter of LoRA in FL, and the results are shown in Table 2. We observe that a lower LoRA rank significantly reduces the trainable parameters with a marginal degradation in the Dice score. We also conduct experiments with other low-rank adapters like DoRA[19] and MoLE[30], which only show marginal performance differences among the adapters.\nWe also benchmark FLAP-SAM against MA-SAM [2], which uses a 3D adapter along with FacT for fine-tuning. We perform this comparison only in the centralized setting because the decomposition of \u2206W back to FacT-Tensor-Train or FacT-Tucker formats [13] after federated aggregation is not straightforward. Although MA-SAM produces comparable results to our method (see Table 1) in a centralized setting, it uses 28.7M trainable parameters (~16\u00d7 more than our method). This validates our choice of using LORA, which is both parameter-efficient and FL-friendly."}, {"title": "5 Conclusion", "content": "In this work, we have tackled adapting a foundational segmentation model (SAM) for 3D medical image segmentation by incorporating an effective PEFT"}, {"title": "strategy", "content": "We critically analyze the LoRA adapter's impact and various SAM components to make the fine-tuning for dense 3D segmentation tasks amenable to FL. Our approach simultaneously addresses data scarcity, overfitting, and communication overhead challenges, resulting in a practical and cost-efficient solution. Our current work analyses various fine-tuning methods in the context of FedAVG[21]; an interesting future direction would be studying the effects of various federated optimization strategies on low-rank adapters for datasets with considerable distribution shifts."}, {"title": "A Appendix", "content": ""}]}