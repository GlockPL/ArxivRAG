{"title": "MSC-Bench: Benchmarking and Analyzing\nMulti-Sensor Corruption for Driving Perception", "authors": ["Xiaoshuai Hao", "Guanqun Liu", "Yuting Zhao", "Yuheng Ji", "Mengchuan Wei", "Haimei Zhao", "Lingdong Kong", "Rong Yin", "Yu Liu"], "abstract": "Multi-sensor fusion models play a crucial role in\nautonomous driving perception, particularly in tasks like 3D\nobject detection and HD map construction. These models provide\nessential and comprehensive static environmental information\nfor autonomous driving systems. While camera-LiDAR fusion\nmethods have shown promising results by integrating data from\nboth modalities, they often depend on complete sensor inputs. This\nreliance can lead to low robustness and potential failures when sen-\nsors are corrupted or missing, raising significant safety concerns.\nTo tackle this challenge, we introduce the Multi-Sensor Corruption\nBenchmark (MSC-Bench), the first comprehensive benchmark\naimed at evaluating the robustness of multi-sensor autonomous\ndriving perception models against various sensor corruptions.\nOur benchmark includes 16 combinations of corruption types\nthat disrupt both camera and LiDAR inputs, either individually\nor concurrently. Extensive evaluations of six 3D object detection\nmodels and four HD map construction models reveal substantial\nperformance degradation under adverse weather conditions and\nsensor failures, underscoring critical safety issues. The benchmark\ntoolkit and affiliated code and model checkpoints have been made\npublicly accessible.", "sections": [{"title": "I. INTRODUCTION", "content": "The perception system is a critical component of au-\ntonomous vehicles, serving as the foundation for interaction\nbetween the vehicle and its driving environment. The sys-\ntem's performance\u2014encompassing both accuracy and robust-\nness-fundamentally influences the decision-making processes\nof autonomous vehicles. Notably, the robustness of perception\nalgorithms is essential for the practical deployment of these\nvehicles, directly impacting the safety of future transportation\nsystems for the general public.\nRecently, researchers have developed fusion-based perception\nmethods that integrate outputs from multiple sensors to enhance\noverall capabilities, leading to significant performance improve-\nments across various tasks. For example, multi-sensor fusion\napproaches for 3D object detection and HD map construction\nhave demonstrated superior accuracy compared to single-sensor\nmethods that rely solely on cameras or LiDAR. However,\nthese performance evaluations are typically conducted on\nclean datasets without any corruption, creating a gap in our\nunderstanding of the robustness of fusion-based perception\nmethods under adverse conditions.\nThe robustness of perception algorithms refers to their\nperformance in adverse conditions, including challenging\ndriving environments, complex scenarios, and sensor failures.\nUnlike single-sensor algorithms, multi-sensor perception sys-\ntems face a broader range of issues, such as misalignment and\nsynchronization problems. Additionally, adverse conditions\nlike fog or snow can affect sensors differently. Understanding\nhow these factors impact multi-sensor performance and whether\nsensor fusion can mitigate these effects is essential and requires\nthorough investigation.\nIn this paper, we introduce 16 types of corruption specific to\nmulti-sensor perception algorithms and evaluate the robustness\nof fusion-based methods across two autonomous driving\ntasks: six 3D object detection methods and four HD map\nconstruction methods. Results reveal\nsignificant performance discrepancies between \"clean\" and\ncorrupted datasets. Key findings include: 1) Camera-LiDAR\nfusion methods achieve strong performance by leveraging\ncomplementary information but often rely on complete sen-\nsor data, making them vulnerable to disruptions. 2) In 3D"}, {"title": "II. RELATED WORK", "content": "Multi-Sensor 3D Object Detection The 3D object detection\ntask focuses on identifying and localizing objects in three-\ndimensional space by predicting their 3D bounding boxes and\ncategories using data from sensors like LiDAR and cameras,\nwhich is crucial for applications such as autonomous driving\nand robotics. While early methods relied on single sensors, the\nrelease of extensive autonomous driving datasets has spurred\nresearch into multi-sensor fusion for enhanced accuracy. Recent\napproaches include BEVFusion [1], which extracts features\nfrom both cameras and LiDAR using a Bird's-Eye View\n(BEV) space; DeepInteraction [2], which facilitates interactions\nbetween modality-specific representations; and TransFusion\n[3], which uses a transformer-based mechanism for adaptive\nfusion. Other methods, such as SparseFusion [4], utilize parallel\ndetectors for instance-level fusion, while CMT [5] incorporates\na Coordinates Encoding Module for position-aware features. Is-\nFusion [6] further improves detection by integrating scene-level\nand instance-level fusion to enhance feature collaboration.\nMulti-Sensor HD Map Construction The HD map con-\nstruction task involves creating high-resolution maps that\nprovide detailed vectorized representations of geometric and\nsemantic information, such as lane boundaries and road\nstructures, which are essential for accurate localization and\npath planning in autonomous driving. Recent camera-LiDAR\nfusion methods leverage the semantic richness of\ncamera data and the geometric precision of LiDAR. BEV-\nlevel fusion, which combines inputs from both sensors into\na shared BEV space, has gained attention [1] for effectively\nintegrating complementary features. However, existing methods\noften depend on complete sensor data, making them less\nrobust to missing or corrupted information, which can lead\nto significant performance degradation. This paper focuses on\nevaluating the robustness of multi-modal HD map construction.\nDriving Perception Robustness Researchers have recently\nfocused on the robustness of various autonomous driving\nperception tasks. Studies like RoBoBEV [11] evaluate the\nrobustness of BEV perception tasks, while others aim to develop\nmore resilient models and strategies. Robo3D [12] benchmarks\nLiDAR-based semantic segmentation and 3D object detection\nunder sensor failures. Zhu et al. [13] assess the natural and\nadversarial robustness of BEV models, introducing a 3D\nconsistent patch attack for spatiotemporal realism. PointDR\nand UniMix [15] propose domain-adaptive methods for"}, {"title": "III. BENCHMARKING MULTI-SENSOR CORRUPTION", "content": "The Multi-Sensor Corruption Benchmark (MSC-Bench)\nincludes 16 corruption types, categorized into weather, interior,\nand sensor failure scenarios. It is constructed by\ncorrupting the val set of nuScenes [18]. Definitions of the\ncorruption types can be found in Tab. I, with additional details\nprovided below.\nCamera Crash: Simulates continuous loss of images\nfrom certain viewpoints due to camera failure. Determine\nthe level of corruption based on the number of dropped\ncameras. Note that this type of corruption applies only to\ncamera sensors, while the LiDAR sensor remains clean.\nFrame Lost: Represents random frame loss to assess\nthe model's resilience to intermittent data loss, with the\ncorruption level determined by the probability of frame\ndropping. Note that this type of corruption applies only to\ncamera sensors, while the LiDAR sensor remains clean.\nCross Sensor: Arises due to the large variety of LiDAR\nsensor configurations (e.g., beam number, field-of-view,\nand sampling frequency). Determine the level of corruption\nbased on the number of beams dropped. Note that this\ntype of corruption applies only to LiDAR sensors, while\nthe camera sensor remains clean.\nCrosstalk: Creates noisy points within the mid-range areas\nbetween two (or multiple) sensors, simulating interference.\nDetermine the level of corruption by adjusting the per-\ncentage of light impulse interference. Note that this type\nof corruption applies only to LiDAR sensors, while the\ncamera sensor remains clean.\nIncomplete Echo: Represents incomplete LiDAR readings\nin some scan echoes. The level of corruption is determined\nby adjusting the drop ratio of these readings. Note that\nthis type of corruption applies only to LiDAR sensors,\nwhile the camera sensor remains clean.\nFog: We use a fog simulator [19] to simulate LiDAR fog\ncorruption. To maintain scene consistency between images\nand point clouds, we adapt the LiDAR fog parameters for\nthe image fog generation process.\nSnow: We use a snow simulator [20] that models snow\nparticles as opaque spheres and computes the reflection\nproperties of wet surfaces, enabling us to corrupt the point\ncloud and image data based on snowfall levels.\nMotion Blur: To replicate intense motion, vibrations, and\nthe rolling shutter effect, we introduce jitter noise from a\nGaussian distribution with a standard deviation of $\\sigma_{\\varepsilon}$ into\nboth point cloud and image data.\nSpatial Misalignment: We introduce translation and\nrotation misalignment, creating a spatial offset between\npoint cloud and camera inputs. We adjust the rotation\nangle and the proportion of affected data based on the\nseverity level.\nTemporal Misalignment: Timestamps from modalities\nlike LiDAR and cameras are not always perfectly synchro-\nnized, so we introduce temporal misalignment to both the\ncamera and point cloud data."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "Our MSC-Bench includes a total of six\nmulti-sensor 3D object detection models: CMT [5], DeepInter-\naction [2], TransFusion [3], SparseFusion [4], BEVFusion [1]\nand Is-Fusion [6]. We present the basic information for these\nmodels in Tab. II, including input modality, backbone, image\nsize, and performance on the official nuScenes validation set.\n3D Object Detection Benchmarking Analysis We present\nthe overall robustness benchmarking results, including mRS\nand mRRS, for the six multi-sensor candidate models in Tab. II.\nThe table shows that model robustness under corruption does\nnot strongly correlate with performance on the clear validation\nset. For example, while Is-Fusion achieves the highest NDS\nand mAP scores, its mRS and MRRS scores are below\nexpectations. In contrast, CMT exhibits excellent robustness,\nachieving the highest robustness scores.\nTo analyze the models' robustness across different corruption\ntypes, we present the Resilience Scores for 16 corruption types\nin Tab. IV (top) and illustrate robustness performance across\nvarying severity levels in Fig. 3. The data shows that sensor\nfailure and misalignment-related corruptions, such as Cross\nSensor, Camera Crash & Cross Sensor, and Frame Lost & Cross\nSensor, significantly impact model performance. In contrast,\nindividual sensor failures like Camera Crash, Frame Lost, and\nIncomplete Echo have minimal effects on robustness. However,\nwhen these failures occur simultaneously, as seen in Camera\nCrash & Incomplete Echo and Frame Lost & Incomplete Echo,\nmodel robustness is substantially compromised.\nFrom Fig. 3, most corruption types lead to a linear decline in\nmodel robustness as severity increases. However, the robustness\ndegradation from Camera Crash and Frame Lost is relatively\nminor, showing a distinct pattern compared to other corruptions.\nNotably, for Temporal Misalignment and Fog, robustness\nremains stable at severity levels 1 and 2 but drops dramatically\nat level 3 as severity intensifies.\nFig. 5 shows the relative resilience scores of different models\nbased on BEVFusion. DeepInteraction underperforms the base\nmodel in eight corruption types, while only CMT surpasses\nthe base model across all corruption types, achieving the best\nperformance in 12 of them.\nCandidate models Our MSC-Bench includes four multi-\nsensor HD map constructors: MapTR [10], HIMap [23],\nMBFusion [21], and GeMap [22]. We present the basic\ninformation for these models in Tab. III, including input\nmodality, backbone, training epochs, and performance on the\nofficial nuScenes validation set.\nHD map construction Benchmarking Analysis Tab. III\npresents the overall robustness performance of the four multi-\nsensor HD map construction models, measured by mRS\nand mRRS. MapTR and GeMap achieve comparable scores,\noutperforming MBFusion and HIMap. Performance on specific\ncorruption types is detailed in Tab. IV (bottom), highlighting"}, {"title": "B. Robustness Evaluation Metrics", "content": "To compare the robustness of different 3D object detectors\nand HD map constructors in multi-modal corrupted scenarios,\nwe introduce two robustness evaluation metrics.\nResilience Score (RS) We define RS as the relative\nrobustness indicator for measuring how much accuracy a model\ncan retain when evaluated on the corruption sets, which are\ncalculated as follows:\n$RS = \\frac{\\sum_{i=1}^{N} Acc_{i,l}}{3 \\times Acc_{clean}},\\ mRS = \\frac{1}{N} \\sum_{i=1}^{N} RSi,$\nwhere $Acc_{i,l}$ denotes the task-specific accuracy scores, with\nNDS (NuScenes Detection Score) for 3D object detection and\nmAP (mean Average Precision) for HD map construction, on\ncorruption type i at severity level l. N is the total number\nof corruption types, and $Acc_{clean}$ denotes the accuracy score\non the \"clean\" evaluation set. mRS (mean Resilience Score)\nrepresents the average score, providing an overall measure of\nthe model's robustness across all types of corruption.\nRelative Resilience Score (RRS) We define RRS as the\ncritical metric for comparing the relative robustness of candidate\nmodels with the baseline model and mRRS as an overall metric\nto indicate the relative resilience score. The RRS and mRRS\nscores are calculated as follows:\n$RRS = \\frac{\\sum_{i=1}^{3} Acc_{i,l}}{\\sum_{i=1}^{3} Acc_{base}} - 1, \\\\mRRS = \\frac{1}{N} \\sum_{i=1}^{N} RRS,$\nwhere $Acc_{base}$ denotes the accuracy score of the baseline\nmodel."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced the Multi-Sensor Corruption\nBenchmark (MSC-Bench) to assess the robustness of multi-\nsensor autonomous driving perception models under 16 types\nof corruption. Our analysis of six 3D object detection mod-\nels and four HD map construction models revealed signifi-\ncant performance discrepancies between clean and corrupted\ndatasets, highlighting vulnerabilities to sensor disruptions,"}]}