{"title": "How to Tune a Multilingual Encoder Model for Germanic Languages:\nA Study of PEFT, Full Fine-Tuning, and Language Adapters", "authors": ["Romina Oji", "Jenny Kunz"], "abstract": "This paper investigates the optimal use\nof the multilingual encoder model mDe-\nBERTa for tasks in three Germanic lan-\nguages - German, Swedish, and Icelandic\nrepresenting varying levels of presence\nand likely data quality in mDeBERTas\npre-training data. We compare full fine-\ntuning with the parameter-efficient fine-\ntuning (PEFT) methods LoRA and Pfeif-\nfer bottleneck adapters, finding that PEFT\nis more effective for the higher-resource\nlanguage, German. However, results for\nSwedish and Icelandic are less consistent.\nWe also observe differences between tasks:\nWhile PEFT tends to work better for ques-\ntion answering, full fine-tuning is prefer-\nable for named entity recognition. In-\nspired by previous research on modular ap-\nproaches that combine task and language\nadapters, we evaluate the impact of adding\nPEFT modules trained on unstructured text,\nfinding that this approach is not beneficial.", "sections": [{"title": "1 Introduction", "content": "Massively multilingual encoder models like\nmBERT (Devlin et al., 2019), XLM-R (Conneau\net al., 2020) and mDeBERTa (He et al., 2021b) are\na workhorse for NLP in many lower-resource lan-\nguages. However, due to interference between lan-\nguages (Conneau et al., 2020; Chang et al., 2023),\nthese models can fall short of reaching their full\npotential for individual target languages: Monolin-\ngual models (Virtanen et al., 2019; Sn\u00e6bjarnarson\net al., 2022) and models with dedicated language\nmodules (Pfeiffer et al., 2022; Blevins et al., 2024)\nfrequently outperform them, raising the question\nfor the best setups for different languages.\nParameter-efficient fine-tuning (PEFT) methods,\nsuch as bottleneck adapters (Houlsby et al., 2019),\nLORA (Hu et al., 2022), and prefix tuning (Li and\nLiang, 2021), have emerged as an alternative to full\nfine-tuning of pre-trained language models. These\nmethods preserve the model's representations and\ncan lead to better generalisation (He et al., 2021c).\nThis is especially relevant for multilingual mod-\nels, trained on diverse data, of which the target\nlanguage only constitutes a small fraction. Fully\nfine-tuning them on task-specific data risks over-\nwriting some of the multilingual capabilities.\nLanguage adapters \u2013 PEFT modules trained on\nunstructured text independently from task fine-\ntuning \u2013 have shown promise in cross-lingual trans-\nfer (Pfeiffer et al., 2020; Vidoni et al., 2020). We\nexplore whether language adaptation modules are\nbeneficial even in scenarios where cross-lingual\ntransfer is not required, i.e., where we have in-\nlanguage fine-tuning data. In addition, we use not\nonly bottleneck (Pfeiffer) adapters but also LoRA\n(Hu et al., 2022), a method that has become popular\nfor LLMs as its parameters can be merged with the\nmodel parameters, adding no inference overhead.\nIn this paper, we investigate strategies for adapt-\ning a multilingual encoder model to task data\nin three languages: German, Swedish, and Ice-\nlandic. For this, we use multilingual DeBERTa\n(He et al., 2021b), which is currently the best-\nperforming model according to the ScandEval\n(Nielsen et al., 2024) leaderboard for Icelandic,\nthe lowest-resourced and thus the most challenging\nof the three languages.\nOur findings indicate that the effectiveness of\nfull fine-tuning versus PEFT varies by language.\nFor German, a PEFT method consistently delivers\nthe best results, although sometimes with marginal\ngains. For Swedish and Icelandic, the performance\nis task-dependent: PEFT is more beneficial for ex-\ntractive question-answering (QA), while full fine-\ntuning works better for named entity recognition"}, {"title": "2 Related Work", "content": "PEFT methods not only reduce the number of train-\nable parameters and, consequently, memory us-\nage in comparison to full fine-tuning, but there\nis also evidence suggesting that they provide bet-\nter regularisation and help preserve pre-existing\nmodel capabilities. For example, He et al. (2021c)\ndemonstrate that adapter-based fine-tuning outper-\nforms full fine-tuning in cross-lingual transfer se-\ntups, likely by avoiding overfitting on the source-\nlanguage. Similarly, prefix tuning, another PEFT\nmethod, has been shown to surpass full fine-tuning\nin extrapolation scenarios (Li and Liang, 2021).\nOther works have shown the effectiveness of\nbottleneck-style adapters in cross-lingual transfer\nas post-hoc trained language modules in encoder\nmodels. Pfeiffer et al. (2020) show that bottle-\nneck language adapters in the Pfeiffer architecture\nimprove performance in NER, commonsense clas-\nsification, and extractive QA. Even Vidoni et al.\n(2020) report that language adapters are effective.\nOther research indicates that language adapters can\naid in transferring knowledge to dialectal variants\n(Vamvas et al., 2024) and that sharing adapters\nacross related languages can be beneficial (Faisal\nand Anastasopoulos, 2022; Chronopoulou et al.,\n2023). However, the success of language adapters\nmay be task-specific and difficult to measure ac-\ncurately when using machine-translated evaluation\ndata (Kunz and Holmstr\u00f6m, 2024). And notably,\nnone of the works used multilingual DeBERTa\nmodels, which may explain divergences in results."}, {"title": "3 Experimental Setup", "content": "We use the multilingual DeBERTa v3 model2 as\nthe base for our experiments. This model contains\nabout 86 million parameters in its backbone, and\nthe embedding layer, with a vocabulary of 250,000\ntokens, adds another 190 million parameters, bring-\ning the total to around 278 million parameters(He\net al., 2021a). It was trained on 2.5 TB of the\nCC100 multilingual dataset (Wenzek et al., 2020;\nConneau et al., 2020), which includes 100 lan-\nguages, including Icelandic, Swedish, and German.\nWe evaluate the fine-tuning and language adapta-\ntion methods on three tasks: extractive question\nanswering (QA), named entity recognition (NER),\nand linguistic acceptability classification. This se-\nlection is inspired by coverage in the ScandEval\nbenchmark (Nielsen et al., 2024) for all three lan-\nguages while having structurally different tasks.\nQA: For Icelandic, we use the Natural Ques-\ntions in Icelandic (NQiI) dataset, which features\nquestions from Icelandic texts written by Icelandic\nspeakers.(Sn\u00e6bjarnarson and Einarsson, 2022). For\nSwedish, we use the Swedish portion of Scan-\ndiQA, which was manually translated from English\n(Nielsen, 2023). For German, we use the human-\nlabeled GermanQuAD dataset, which is natively\nGerman. (M\u00f6ller et al., 2021).\nNER: For Icelandic, we use the MIM-GOLD-\nNER dataset (Ing\u00f3lfsd\u00f3ttir et al., 2020), for\nSwedish, we use the Stockholm-Ume\u00e5 Corpus\n(Kurtz and \u00d6hman, 2022) and for German, we use\nGermanEval 2014 (Benikova et al., 2014).\nLinguistic Acceptability: For all three lan-\nguages, we use the respective portion of ScaLA\n(Nielsen, 2023), a binary classification dataset that\njudges the linguistic acceptability of sentences.\nSentences are tagged as either grammatically cor-\nrect or incorrect. This dataset is synthetically cre-\nated by introducing corruptions based on the de-\npendency trees of the sentences."}, {"title": "3.3 PEFT Methods", "content": "We use two different PEFT methods. Pfeiffer\nadapters (Pfeiffer et al., 2021, 2020) are a vari-"}, {"title": "3.4 PEFT Training", "content": "In the first step, we fine-tune individual lan-\nguage adapters for Icelandic, Swedish, and Ger-\nman, using the masked language modeling objec-\ntive. We use 250,000 samples from the CC100\ndataset and train a LoRA and a Pfeiffer language\nadapter for each language. Our language adapters\nare available at https://huggingface.co/\nrominaoji.\nTask adapters are fine-tuned on target-language\ntask data with the datasets described in Section 3.2.\nFor all adapters, we set the LoRA rank to 8 and\nthe a to 16, while for the Pfeiffer method, the re-\nduction factor is set to 16. For the implementation,\nwe use the adapters library (Poth et al., 2023)."}, {"title": "3.5 Setups", "content": "To find the optimal method to use mDeBERTa for\nthe three languages, we fine-tune it using three\nsetups: (1) Full fine-tuning, (2) tuning using only\ntask adapters, and (3) using a combination of\nlanguage and task adapters as in the MAD-X\nframework. In each setup, models are fine-tuned\nover five epochs.\nAs PEFT models require higher learning rates\nthan full fine-tuning due to their lower number of\ntrainable parameters, we determine a suitable rate\nfor each setup by testing learning rates from 1-e4\nto 9e-4 for PEFT and from 1e-5 to 9e-5 for full\nfine-tuning. This resulted in a learning rate of 3e-4\nfor both the language and task adaptation meth-\nods and 2e-5 for full fine-tuning. All experiments\nuse a linear scheduler paired with the AdamW\noptimiser(Loshchilov, 2017). The code is avail-"}, {"title": "3.6 Evaluation", "content": "For the sake of simplicity, we only present F1\nscores as the evaluation metric for all three tasks\nin this paper. While we have collected results on\nmore metrics, we did not observe differences in\nthe trends. The results are the mean of a five-fold\ncross-validation, with standard deviation."}, {"title": "4 Results and Discussion", "content": "All results are presented in Table 1. We discuss\nthe effects of different task fine-tuning strategies\non different languages and tasks (\u00a74.1) and finally\nthe effect of language adapters (\u00a74.2)."}, {"title": "4.1 Full Fine-Tuning Versus PEFT", "content": "Tasks: For the extractive QA tasks, we observe\nthat PEFT methods generally outperform full fine-\ntuning. In German, there is a notable gap be-\ntween full fine-tuning and both PEFT methods,\nwith LoRA yielding the best results. For Icelandic,\nPfeiffer adapters outperform both full fine-tuning\nand LoRA. For Swedish, the differences between\nsetups are minimal. We hypothesise that for this\ntask, the model benefits from the pre-trained repre-\nsentations and does not require the highest possible\nlearning capacity to identify relevant text spans in\nthese tasks.\nIn contrast, full fine-tuning is the best ap-\nproach for NER tasks, outperforming the highest-\nperforming PEFT method in Icelandic and Swedish,\nand performing on par with Pfeiffer adapters in Ger-\nman. This suggests that for this word-level task, a\nlarger learning capacity is more crucial than pre-\nserving fine-grained capabilities from pre-training.\nFor ScaLA, the results are mixed. Full fine-\ntuning yields slightly higher scores for Icelandic,\nwhile Pfeiffer adapters perform marginally better\nfor Swedish and German. Interpreting the perfor-\nmance on this task is challenging, as the dataset\ncontains some corrupted instances that may be de-\ntectable with simple pattern-matching, while others\nrequire more fine-grained linguistic knowledge.\nLanguages: For German, Pfeiffer adapters con-\nsistently outperform full fine-tuning in QA and\nScaLA tasks, and are either on par or slightly better\nfor NER. LORA performs best for QA but yields\nlower scores in the other two tasks. This suggests\nthat German benefits from keeping the base model\nintact, likely due to its relatively large representa-\ntion in the pre-training dataset."}, {"title": "4.2 Language Adaptation", "content": "Language adapters do not provide any significant\nbenefits. When using Pfeiffer task adapters, perfor-\nmance remains similar whether language adapters\nare included or not. The only exception is Ice-\nlandic QA, where the combination of a Pfeiffer lan-\nguage adapter and a Pfeiffer task adapter achieves\na slightly higher score compared to the best setup\nwithout language adapters. However, the difference\nis small and possibly due to result variability, as it\nfalls within a standard deviation.\nWith LoRA task adapters, language adaptation\nmethods sometimes result in a noticeable per-\nformance drop, suggesting potential interference.\nWhile prior work, such as Pfeiffer et al. (2020),\nreported improvements in similar tasks, their study\nfocused on cross-lingual transfer, where no task\ndata from the target language was available. In\ncontrast, our setups use task data from the tar-\nget language, and all the languages are present\nin the model's pre-training data. In addition, we\nuse mDeBERTa-v3, which reportedly performs bet-\nter for the languages in question than the XLM-R\n(Conneau et al., 2020) and multilingual BERT (De-\nvlin et al., 2019) models that most other papers\nincluding Pfeiffer et al. (2020) use. These factors\nlikely contribute to the fact that language adapters\nare unnecessary in our setup."}, {"title": "5 Conclusion", "content": "We compared the performance of the multilingual\nencoder model mDeBERTa across three task adap-\ntation setups: full fine-tuning, bottleneck (Pfeif-\nfer) adapters, and LoRA. Based on our evaluations\nacross three tasks and three languages, we found\nthat the choice of the best method is both task-\nand language-dependent. Specifically, extractive\nQA tasks benefit from PEFT methods, while NER\ngets better results with full fine-tuning. For Ger-"}]}