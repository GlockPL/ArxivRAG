{"title": "Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models", "authors": ["Sultan Alrashed", "Dmitrii Khizbullin", "David R. Pugh"], "abstract": "As large language models (LLMs) grow and develop so too, do their data demands. This is especially true for multilingual LLMs, where the scarcity of high quality and readily available data online has led to a multitude of synthetic dataset generation approaches. A key technique seen in this space is machine translation (MT), where high quality English text is adapted to a target, comparatively low-resource, language.\nIn this report we introduce FineWeb-Edu-Ar, a machine-translated version of HuggingFace's exceedingly popular (deduplicated) FineWeb-Edu dataset. To the best of our knowledge FineWeb-Edu-Ar is the biggest publicly available machine-translated Arabic dataset out there, with its size of 202B tokens of an Arabic-trained tokenizer.\nThe data is available on HuggingFace\u00b9.", "sections": [{"title": "Introduction", "content": "Natural Language Processing (NLP) has seen tremendous strides in recent years with the advent of LLMs, we have seen models scale up to more than 100 billion parameters (Brown et al., 2020). With the ever growing demand for scale, we have seen a substantial focus on large corpora to train these LLMs on (Hoffmann et al., 2022). Although data quality mattered, the focus had mostly been on data quantity.\nMore recently however, we see small language models (SLMs) trained on a much smaller quality-focused corpus (Gunasekar et al., 2023), (Allal et al., 2024). These models exceed expectations for their size, outperforming certain much larger models on various benchmarks (Abdin et al., 2024). This gave rise to new potential use cases for language models on edge devices with compute constraints (Lu et al., 2024), (Gunter et al., 2024).\nMany languages, including Arabic, suffer from a distinct lack of the same kind of high quality, educational focused, and readily available data that allowed other small language models to flourish.\nIn order to illustrate the current situation with the training data, we've organized it in 4 quadrants: English and Arabic texts versus culturaly-agnostic and culturally-specific knowledge as in Figure 1. While generic English data (left top quadrant) is abundant, and Arabic culturally specific data (right bottom quadrant) can be extracted from the web archives (Aloui et al., 2024), Arabic texts with the generic knowledge (top right quadrant) are scarce. Fortunately, the latter can be reinforced with the machine translated texts sourced from generic English texts.\nIn an effort to support small language model pre-training, we publicly release FineWeb-Edu-Ar, a machine translated version of HuggingFaceTB/smollm-corpus (Allal et al., 2024), the deduplicated version of Huggingface's FineWeb-Edu (Lozhkov et al., 2024). HuggingFaceTB/smollm-corpus that enabled their model, SmolLM, to achieve remarkable results for its size. We hope that this will support"}, {"title": "Related Works", "content": "Although scaling LLMs to ever-increasing sizes (Touvron et al., 2023) has been the focus for a long time, we are seeing an increasing focus on training SLMs.\nUnlike their larger counterparts, these SLMs benefit a lot more from the quality of the corpus they are trained on rather than the quantity. Works such as Huggingface's SmolLM (Allal et al., 2024) show this in their benchmark scores 1, where Qwen2-1.5B, a model trained on 7 trillion tokens (Yang et al., 2024a), scores lower than SmolLM-1.7B, which was only trained with 1 trillion tokens."}, {"title": "Arabic Machine Translation", "content": "In the CommonCrawl dataset, one of the largest web-crawled datasets, Arabic content is over two orders of magnitude less common than English content (Wenzek et al., 2019). This discrepancy has naturally led to a need for more Arabic data, machine translation allows us to fill that necessary gap.\nOne of the largest efforts towards Arabic machine translation has been the work on Aya Dataset (Singh et al., 2024), which contains over 6 million entries of Modern Standard Arabic (MSA) data translated from English sources. Taking inspiration from their successful usage of nllb-200-3.3B helped further inform our decision on using nllb-200-distilled-600M. While Aya Dataset focused on instructional tuning data, we had shifted our focus to a gap in English to MSA machine translated pretraining corpora. To match the scale necessary for pretraining data, we translate approximately 190 million rows, as seen in table 2."}, {"title": "MT Model Assessment", "content": "To find out which model would be ideal to translate such a large corpus, we conduct an investigation into the translation and run time performance of a collection of popular English to Arabic encoder-decoder transformers alongside multilingual decoder-only transformers.\nFor our encoder-decoder transformers, we selected from a set of popular machine translation models. We went with the 600m and 1.3b variants of Facebook's NLLB (Costa-juss\u00e0 et al., 2022) and the large English to Arabic version of the University of Helsinki's OPUS model (Tiedemann and Thottingal, 2020).\nFor our decoder-only transformers, we selected from a collection of generally popular models and"}, {"title": "MT Model Quality Assessment", "content": "To assess MT quality in an automatic and reproducible manner, we employ the LLM-as-a-Judge approach (Zheng et al., 2024), (Fu et al., 2023). We prompt GPT-40 to assess the quality of translation from 3 different angles: accuracy (0-3 points), grammar and syntax (0-3 points), alongside fluency and style (0-2 points). We provide both original English and the translated Arabic passages as a part of the prompt. See Figure 4 for the exact prompt that we've designed. In order to make our micro-benchmark for MT models dataset-independent, we prepare a set of 3 passages to run the translation on. The passages that compose our micro-benchmark are listed in Appendix C. To get a more stable estimate of the quality, we re-run the translation 10 times with 0.1 temperature, average the scores and round them. We add up the scores for all passages to get the final quality score between 0 and 24 for every MT model. Further, we perform runtime performance benchmarking for all models on a single A100 80GB GPU with flash_attention_2 enabled for whichever model supports it.\nWe build a map of all evaluated models in score vs. runtime coordinates, see Figure 3. We find 4 models that lie on the Pareto frontier.\nWe choose nllb-200-distilled-600M as the best MT model within the computational budget of 500 A100 GPU-days."}, {"title": "Dataset", "content": "The data is available on HuggingFace\u00b2. The code used to create the dataset is available on GitHub\u00b3."}, {"title": "Generation details", "content": "Due to the limited context window of NLLB, there were multiple approaches we could have taken to split up our data for translation:\n\u2022 Sliding window with overlap: The input text is split into 200 token chunks, with a 50 token overlap.\n\u2022 Sliding window with no overlap: The input text is split into 200 token chunks.\n\u2022 Sliding window split by sentence: The input text is split into the maximum number of sentences present in the 200 tokens chunk. If the sentence is longer than 200 tokens, we take the whole 200 tokens.\nWe resorted to a sliding window approach with no overlap to align with our goals of containing costs and CO2 emissions. In particular, flash_attention_2 is sensitive to the use of padding tokens (Kundu et al., 2024), which eliminates option 3. Each text is broken up into 200 token chunks and translated separately as part of a larger batch, that is later concatenated.\nFurthermore, to reduce redundant processing and conserve emissions, we processed only the deduplicated version of Fineweb-Edu present in the training set of SmolLM, a successful English SLM."}, {"title": "Properties", "content": "Dataset properties are summarized in Table 2. Dataset schema is defined in Listing 1. The dataset contains both the Arabic translated and the original English passages available at corresponding indices."}, {"title": "Discussion and Limitations", "content": "We choose the translation model among those lying on the Pareto frontier and at the same time satisfying the computational budget. Within these constraints the produced dataset strikes a balance between the translation quality and cost. Still, the question of how good the data is for creation of foundational models remains open, and we leave it to the community to find out.\nThe proposed dataset can be considered noisy in the sense of translation inaccuracies. Furthermore, the knowledge domain of FineWeb-Edu is one of English speaking countries and may not include enough regional facts of the Arabic-speaking countries. Nevertheless, we argue that the sheer amount of language-agnostic knowledge that FineWeb-Edu contains may be valuable for pre-training SLMs.\nA thorough analysis of the alignment of LLM-as-a-Judge with independent Arabic native speakers'"}, {"title": "CO2 Emissions", "content": "Using the OECD's 2014 yearly average carbon efficiency of 0.432 kgCO2eq/kWh, alongside the 400W TDP of the GPU used, the full translation total emissions are estimated to be 1990 kgCO2eq for the 11520 GPU-hours. These estimations were conducted using the MachineLearning Impact calculator presented in (Lacoste et al., 2019).\nUsing the runtime numbers present in figure 3, we can estimate the equivalent emission costs of using the other models we heavily considered in table 3.\nThe disparity in emissions is significant. As part of promoting training SLMs, we also want to promote climate awareness and consciousness in this space. This drove our decision towards nllb-200-distilled-600M."}, {"title": "Conclusion", "content": "FineWeb-Edu-Ar is the biggest Arabic machine-translated dataset that is openly available for non-commercial purposes. The proposed dataset took around 11520 A100 GPU-hours to produce at the cost of $46,000; and we suggest that it is of significant value to starter academic teams who do not have resources to generate their own synthetic data at this scale.\nParticularly, we hope this dataset allows the community to explore the space of Arabic small language models."}]}