{"title": "Position: There are no Champions in Long-Term Time Series Forecasting", "authors": ["Lorenzo Brigato", "Rafael Morand", "Knut Str\u00f8mmen", "Maria Panagiotou", "Markus Schmidt", "Stavroula Mougiakakou"], "abstract": "Recent advances in long-term time series forecasting have introduced numerous complex prediction models that consistently outperform previously published architectures. However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons. Our position emphasizes the need to shift focus away from pursuing ever-more complex models and towards enhancing benchmarking practices through rigorous and standardized evaluation methods. To support our claim, we first perform a broad, thorough, and reproducible evaluation of the top-performing models on the most popular benchmark by training 3,500+ networks over 14 datasets. Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art. Our findings suggest the need for rigorous and standardized evaluation methods that enable more substantiated claims, including reproducible hyperparameter setups and statistical testing.", "sections": [{"title": "1. Introduction", "content": "Long-term time series forecasting (LTSF) is critical across various domains, including energy management (Weron, 2014), financial planning (Sezer et al., 2020), and environmental modeling (Soni et al., 2024). Accurately predicting future values in time series data enables better decision-making and resource allocation. LTSF remains challenging due to the complex temporal dynamics, including trends, seasonality, irregular fluctuations, and significant variability across datasets (Shao et al., 2024; Qiu et al., 2024).\nRecent advancements in deep learning have improved LTSF capabilities, and the field is currently witnessing an exponential surge in publication rates (Kim et al., 2024). Transformer models have been adapted to time series forecasting with innovative modifications, such as univariate patching (Nie et al., 2023) and attention mechanisms tailored to exploit inter-variate dependencies (Liu et al., 2024a; Wang et al., 2024c). Additionally, models such as TimeMixer (Wang et al., 2024a), leveraging multiscale signal mixing, and TimesNet (Wu et al., 2023), using Fourier-based 2D decomposition, have expanded the field.\nHowever, we claim that the field is facing significant fair-benchmarking challenges despite the introduction of extensive benchmarks such as TSLib (Wang et al., 2024b), BasicTS+ (Shao et al., 2024), and TFB (Qiu et al., 2024). Inconsistencies in test setups across different benchmarks, biased comparisons, and challenges with reproducibility are investigated as factors that hinder fair performance assessment in the field. Moreover, marginal performance gains in recent literature cast doubt on the practical value of increasingly complex model architectures (Zeng et al., 2022). To support our hypothesis, we conduct a comprehensive, rigorous, and reproducible evaluation of the top-performing models on the most widely used benchmark, training over 3,500 networks across 14 datasets. As shown in Figure 1, our findings reveal that no single model consistently outperforms all the baselines. This result directly challenges the prevailing narrative of new architectures consistently surpassing competing models across all domains (Nie et al., 2023; Wu et al., 2023; Liu et al., 2024a; Wang et al., 2024a;c). We provide insights into the potential reasons behind this phenomenon and offer recommendations to help the field progress. To facilitate ongoing research in LTSF, we will make our code publicly available upon acceptance. In summary, the contributions of our paper are as follows:\n\u2022 We question the current narrative of consistently dominated LTSF benchmarks (Section 2) supported by results obtained over a comprehensive and reproducible experimental setup (Section 3).\n\u2022 We provide possible reasons why this is happening (Section 4) and propose guidelines to reduce the likelihood of repeating such overstated claims (Section 5).\n\u2022 We challenge previous claims that dataset characteristics may guide model selection, as our results show similar performance across networks, highlighting the need for further research in this direction (Section 6)."}, {"title": "2. Field overview", "content": "We provide an overview of recent advancements in LTSF, focusing on current benchmarks (Section 2.1) and emerging champions (Section 2.2). Due to space limitations, additional related work on recent time series forecasting models and paradigms is included in Appendix A."}, {"title": "2.1. Benchmarks", "content": "TSLib (Wang et al., 2024b) compares 12 deep learning models across five tasks: classification, imputation, anomaly detection, and long-/short-term forecasting. For long-term forecasting, nine datasets from four domains are used. Results are presented for two settings: unified hyperparameters (HPs) and an HP search per model, but details on parameters, context length, forecast horizon, or the search process are missing. The evaluation metric is the mean squared error (MSE) averaged across datasets. The top models in the HP search setting are PatchTST (Zeng et al., 2022) (MSE 0.305), N-Beats (Oreshkin et al., 2020) (MSE 0.313), and iTransformer (Liu et al., 2024a) (MSE 0.317), and the top performing models for the unified HP setting are iTransformer (MSE 0.342), N-Beats (MSE 0.371), and PatchTST (MSE 0.373). The authors claim that their results clearly show the superior forecasting capabilities of transformer models, particularly iTransformer and PatchTST, despite arguably marginal improvements. They stress the importance of continued exploration of methods that use temporal tokens in time series analysis.\nTFB (Qiu et al., 2024) evaluates 22 statistical, classical machine learning, and deep learning methods using 25 multivariate and 8,068 univariate datasets. For multivariate forecasting, shorter datasets use forecast horizons of 24\u201360 and look-back windows of 36\u2013104, while longer datasets use forecast horizons of 96\u2013720 and look-back windows of 96\u2013512. Univariate forecasting applies fixed forecasting horizons of 6-48 with a look-back window set to 1.25 times the forecasting horizon. The experiments adhere to each method's HPs specified in the original works. HP searches are performed across up to eight sets, and the best result is selected from these evaluations. Based on these results, the authors claim that linear models outperform deep learning methods in datasets with increased trends and distribution shifts. Conversely, transformers excel in datasets with marked patterns (e.g., seasonality). PatchTST and DLinear (Zeng et al., 2022) consistently perform well without notable weaknesses.\nBasicTS+ (Shao et al., 2024) incorporates 28 forecasting models, including 17 short-term forecasting (STF) and 11 LTSF models, across 14 widely used datasets. STF models encompass prior-graph-based, latent-graph-based, and non-graph-based methods, while LTSF models consist of transformer-based and linear-layer-based architectures. In STF, both the context length and forecast horizon are fixed at 12. For LTSF, the forecast horizon is set to 336, while context length varies across 96, 192, 336, and 720, with the best performance across these lengths being reported. Models are implemented following publicly available architectures and HPs, with further tuning of parameters like learning rate and batch size via grid search to ensure performance is at least as good as reported in the original paper. Upon analyzing the results, the authors argue that dataset characteristics play a major role in determining model performance. They claim that Transformer models excel on datasets with clear, stable patterns, whereas simpler models like DLinear perform comparably on datasets without such patterns. The authors emphasize the need to address data distribution drift and unclear patterns instead of focusing solely on increasing model complexity. They suggest that this may indicate potential overfitting to commonly used datasets like ETT*, Electricity, Weather, and Exchange, which risks creating a misleading impression of progress. Consequently, they conclude that careful dataset selection and curation are essential for advancing MTS forecasting meaningfully."}, {"title": "2.2. Emergent LTSF champions", "content": "Recent models have made a leap in LTSF performance (Zeng et al., 2022; Nie et al., 2023; Wu et al., 2023; Wang et al., 2024a; Liu et al., 2024a; Wang et al., 2024c). One widely accepted benchmark for LTSF is TSLib (Wang et al., 2024b). Using this library, there has been a series of new models in 2024 that reportedly dominates the field. Sophisticated and elegant changes to established model architectures resulted in improved forecasting performance. The current leaderboard in TSlib includes five models, which we introduce in chronological order.\n(Zeng et al., 2022) introduced a collection of comparably simple multi-layer perceptron (MLP) baselines called LTFS-Linear. The authors challenged the utility of transformer-based models since, collectively, the linear baselines beat all previous transformers (Zhou et al., 2022a; Wu et al., 2021; Zhou et al., 2021; Liu et al., 2022b; Li et al., 2019). The DLinear model has since been used as a competitive baseline in LTSF benchmarks. PatchTST (Nie et al., 2023) is a transformer model that introduced univariate patching of time series for tokenization. PatchTST beat all previous transformer models but was occasionally beaten by DLinear. PatchTST has since been used as a competitive baseline in LTSF benchmarks. TimeMixer (Wang et al., 2024a) is an MLP mixer model that introduces past-decomposable mixing and future-predictor mixing. The authors used two experimental settings. In setting A), all models were trained and tested using unified HPs, and in B) with HP search. TimeMixer dominated in setting A where unified HPs were used. Although still the clear winner in setting B, TimeMixer took the position as runner-up in some datasets and forecast horizons, being on most occasions second to PatchTST. The iTransformer (Liu et al., 2024a) is a modification of the original transformer model with attention across variates instead of samples. This allows for capturing interactions between variates. It was the top model in 5/9 datasets. Since the remaining four datasets are similar (ETTh1, ETTh2, ETTm1, ETTm2), the authors also reported results for an average of these datasets. This improved the ranking of iTransformer, making it the top-performing model at the time. Finally, TimeXer (Wang et al., 2024c) is a transformer model that uses two attention stages, one for interactions between patches and one for interactions between variates. TimeXer won in 6/7 datasets, of which four are similar (ETT*). We summarize the striking winning percentages of these models in Table 1."}, {"title": "3. Who is the real champion?", "content": "As seen in the previous section, recent papers often suggest that newly proposed architectures outperform others across almost all tested datasets. However, the variability in results for the same algorithms and reliance on prior studies with different HP optimizations raise questions about their consistent superiority. To investigate this, we selected the previously introduced five top-performing models from TSLib (Wang et al., 2024b) and performed a comprehensive HP search across 14 datasets from various domains."}, {"title": "3.1. Hyperparameter search", "content": "For HP tuning, we focused on optimizing parameters aligned with those described in TimeMixer (Wang et al., 2024a). Specifically, we searched for an input length between 96 and 512, model size dm from 16 to 512, learning rate ranging from 10-5 to 0.1, and encoder layers between 1 and 5. We refer the reader to Table 7 for exact values. TimeMixer was limited to a maximum of 3 layers and a model size of 128 due to high memory demands. However, this is unlikely to affect performance, as the original HP search for all datasets in this study yielded results within these limits.\nWe used the Optuna framework (Akiba et al., 2019) with a budget of 40 trials to optimize the HPs. We employed the default TPEsampler for HP sampling and applied the SuccessiveHalvingPruner, configured with a minimum of three epochs and a reduction factor of two to prune unpromising trials. The search was conducted with a batch size of 8, a maximum of 15 epochs, and early stopping with a patience of 3 epochs. All models were optimized with the Adam optimizer and an exponentially decaying scheduler following the default TSLib configuration. The optimal HPs, determined by the minimum validation loss in the trials, were used to train and evaluate the final model across three random seeds to ensure robust results.\nWe set the dimensions of the fully connected layers df equal to dm. The patch length for transformer models using patching (i.e., PatchTST and TimeXer) was set to roughly match one period for each dataset, with the stride equal to the patch size (Table 6). Original works concluded that variations in patch length have minimal effects (Nie et al., 2023; Wang et al., 2024c). For the rest of the model HPs, we default to the configurations provided in TSLib."}, {"title": "3.2. Datasets", "content": "We evaluate models on 14 datasets spanning five domains: Energy, Economy, Transport, Health, and Environment. The datasets vary significantly, containing between 7,588 and 72.58 million time points. They range from univariate to multivariate with up to 321 channels. Sampling frequencies differ from 1 Hz to hourly and daily intervals. Additionally, the datasets exhibit diversity in stationarity, complexity, trends, seasonality, and entropy (Appendix B). The dataset selection is designed to minimize bias, preventing undue advantages for any model while accounting for specific model strengths."}, {"title": "3.3. Results: There is no champion", "content": "We follow the benchmark TSlib and use MSE and mean absolute error (MAE) to evaluate model performance. Table 2 presents the MSE and MAE for each dataset, averaged over the most common forecast horizons (Wang et al., 2024c; Liu et al., 2024a; Nie et al., 2023; Wang et al., 2024c), revealing results that differ substantially from recent papers where proposed algorithms often dominate. Instead, our findings indicate no definitive best-performing model across all datasets and forecast horizons (Table 11). To assess reliability, we also compared our results with the best-reported outcomes from the original studies for three common datasets: ETT*, Electricity, and Weather. As shown in Table 9, our HP search performed similar or better than the original papers' results. While this is not a one-to-one comparison due to differences in HP search procedures and context lengths, achieving comparable or better scores confirms the proper implementation and tuning of our baselines.\nTo present a comprehensive view that highlights both the optimal outcomes and the realistic performance variability, we report the minimum values (best MSE/MAE) and the averages. We observe that TimeMixer is the only model facing convergence challenges, specifically on Exchange, impacting its overall average performance. We further analyze run-to-run variability in Appendix E."}, {"title": "4. Why are they all champions?", "content": "This section provides strong empirical evidence on possible reasons for how each model can become a champion. We demonstrate how a few changes in the experimental setting, such as the exclusion/inclusion of an additional dataset, prediction horizon, or baseline model, plus HP tuning, may drastically change the final conclusions. Additionally, we illustrate how specific visualizations can influence the perceived performance differences between models. First, we interpret the results from our extensive experiments. Second, to support our hypothesis, we analyze cases in recent literature in which such scenarios occur."}, {"title": "4.1. Impact of datasets", "content": "From our results This experiment provides a practical sense of how much the addition/removal of a single dataset may affect conclusions. The most impactful case in our experimental setup corresponds to removing only the MotorImagery dataset from the full pool. As visible in Figure 2 (left), when all datasets are evaluated, iTransformer (red bar) seems to clearly be the best model in terms of MSE. However, when we remove it (Figure 2, right), we get a much closer scenario where all models are basically equivalent. We report an additional view of this experiment in Appendix H (Figure 8), in which we show the MSE per dataset and that the sharp drop of difference is only due to the much better performance of iTransformer on MotorImagery, which biases the perception of the overall rankings.\nFrom literature We observe that subsets of the full benchmark were used occasionally, which may be justified (e.g., too small datasets like ILI). (Liu et al., 2024a) averaged the performance over the four ETT datasets, which the authors justified by the similarity of the datasets. However, this increased the percentage of wins of their proposed method from 33.3% to 71.4%."}, {"title": "4.2. Impact of prediction horizons", "content": "From our results Similarly to the case of datasets, also prediction horizons may play a major role in biasing the perception of overall champions. In this small experiment, we select 6 datasets, more precisely ETT*, Weather, and Exchange, and remove one prediction horizon at a time. In such a manner, we simulate a slightly less broad evaluation yet reasonably acceptable considering that models are evaluated on 18 total scenarios (6 datasets and 3 prediction horizons). In Figure 3, we find that we can have three different champions in terms of MSE (red bars) out of four different cases. In particular, TimeXer wins the benchmark when prediction horizons 96 and 192 are removed, DLinear if the prediction is 336, and iTransformer if the forecast length is 720. Although the differences are less pronounced than in the dataset case, this serves as further empirical evidence of the fragility of such conclusions.\nFrom literature (Shao et al., 2024) focuses solely on a forecast horizon of 336, whereas (Qiu et al., 2024) bases its analysis of the impact of different data characteristics on a horizon of 96 - despite reporting performance for all four forecast horizons. Our experiments demonstrate that this reporting can lead to differing interpretations of model performance, underscoring the critical need for more consistent evaluation practices in LTSF."}, {"title": "4.3. Impact of evaluated models", "content": "From our results While less surprising, we stress that excluding the top model from a benchmark may automatically crown the second-best as a champion. For instance, from the full experimental setup evaluated in Figure 2 (left), removing iTransformer would automatically champion TimeXer as the best available model.\nFrom literature We believe that the reports always include the best-performing complex models from the benchmarks (Appendix G). However, we notice the overall lack of inclusion of baseline models like N-Beats in the publications of the recent \"champions\", although it is a top-3 method in (Wang et al., 2024b). In addition, we identified cases where the best-performing model was excluded from discussions without clear justification. For example, (Shao et al., 2024; Qiu et al., 2024) claim recent transformers underperform compared to earlier methods, but their experiments show the most recent LTSF transformer at the time (PatchTST) outperformed competitors, contradicting this claim. In addition, (Shao et al., 2024) claims linear models are better for LTSF on datasets with unclear patterns or distribution shifts. However, the claim is based on results where PatchTST was excluded, although it performed similarly according to the full results, weakening their assertion."}, {"title": "4.4. Impact of HP tuning", "content": "From our results The impact of HP tuning on the absolute performance of machine learning models in benchmarks is becoming increasingly evident (Brigato et al., 2021; McElfresh et al., 2024). In Table 3, we investigate whether this may also be the case in LTSF via a proof-of-concept example. We report the evaluation in terms of MSE for DLinear on the Weather dataset at prediction horizon 96. HP tuning brings a relative performance boost of ~15% in our setup and an ~10% in (Wang et al., 2024b). Similarly, building on the HP search details in (Wang et al., 2024c), we found comparable performance between TimeMixer, iTransformer, and PatchTST, unlike the original work where TimeMixer consistently ranks first. This underscores how close the actual performance of models is, making outcomes and conclusions sensitive to slight variations in HP search.\nFrom literature In TSLib, the models are usually based on the implementation of the original publications. However, in (Wu et al., 2023), for a fair comparison, they changed the input embeddings and the final projections to be the same for all models. Specifically, the sequence length was set to 96 for all models by default. This is critical since DLinear, after a broad ablation study, explicitly states that short input sequences (< 336) lead to underfitting (Zeng et al., 2022). We show the progression of the reported MSE of DLinear in Table 3 and underline the sharp improvement (+9.7%) if HP search, including context length, is performed. This concludes one example where efforts towards a fair comparison put baseline models at a known disadvantage, and therefore, we recommend a fair HP search for all baselines."}, {"title": "4.5. Impact of visualizations", "content": "From our results Visualizations are a strong tool to convey a message. In Figure 4, we investigate the impact of scales to visualize performance. We observe that using an absolute scale exaggerates differences between models that are not perceived in the relative scale with uniform axes. Conversely, it can obscure substantial differences as in the example of MotorImagery and BenzeneConcentration (Figure 4, right, b and e). Hence, for a fair comparison, we stress that researchers must carefully select appropriate styles and scales when visualizing results."}, {"title": "5. How to make substantiated claims?", "content": "This section provides guidelines and a proof-of-concept example of how to make substantiated claims supported by robust statistical evidence regarding models' superiority in a broad experimental setup. First, we introduce recommended non-parametric statistical tests (Dem\u0161ar, 2006). Second, we upgrade the existing iTransformer to emulate current model-design proposals and rigorously evaluate it in our setup (Section 3). While not an overall champion, the upgraded model statistically outperforms its predecessor, demonstrating the idea that reliable claims can be made."}, {"title": "5.1. Rigorous statistical testing", "content": "Following good practices for reliable evaluations in machine learning, we claim that statistical tests should be used to decrease the chances of making unreliable claims. (Dem\u0161ar, 2006) studied various statistical tests for comparing classifiers from both theoretical and empirical perspectives. The study recommended a set of simple, reliable, and robust tests for such comparisons. In particular, the sign test (Salzberg, 1997) compares two classifiers over multiple datasets, and the Friedman test compares various classifiers over multiple datasets (see Appendix F for details).\nThe TSLib benchmark (Wang et al., 2024b) employs averaging for presenting aggregated results. However, averages are susceptible to outliers (Dem\u0161ar, 2006). A classifier's strong performance on one dataset can mask weaknesses elsewhere, so we prioritize consistent performance across problems, making dataset averaging unsuitable for evaluation (Section 4). Both the BasicTS+ (Shao et al., 2024) and TFB (Qiu et al., 2024) benchmarks focus on the number of wins achieved by each model but do not incorporate any statistical testing, making conclusions less reliable and hard to communicate in a concise manner."}, {"title": "5.2. A proof-of-concept model for substantiated claims", "content": "The iPatch model is introduced as a hybrid architecture combining principles from iTransformer and PatchTST to better capture variate and temporal-specific dynamics in multivariate time-series data. By structuring the input into cycles and modeling both variate and cycle-level dependencies, iPatch provides a hierarchical approach to multivariate time-series forecasting.\nEmbedding To prepare the input for its two-stage attention mechanism, we modify the embedding layer by structuring the temporal information into cycles for temporal attention. Let the input be denoted as $x \\in \\mathbb{R}^{B \\times C \\times L}$, where B is the batch size, C is the number of variates, and L is the lookback length. Following the iTransformer design, the input is tokenized into an embedding dimension dm, resulting in $x \\in \\mathbb{R}^{B \\times C \\times d_m}$. In iPatch, however, we first divide the sequence into N cycles of length P, such that $L = N \\cdot P$. This restructuring reshapes the input from $\\mathbb{R}^{B \\times C \\times L}$ to $\\mathbb{R}^{B \\times C \\times (N \\cdot P)}$, then further into $\\mathbb{R}^{B \\times (C \\cdot N) \\times P}$ for preparing attention over the temporal dimension as in PatchTST. Each cycle is subsequently embedded to dm, resulting in $x \\in \\mathbb{R}^{B \\times (C \\cdot N) \\times d_m}$.\nTransformer layer We enhance the attention module as a sequence of two attentions over variates and cycles. First, the input $x \\in \\mathbb{R}^{B \\times (C \\cdot N) \\times d_m}$ is reshaped to $\\mathbb{R}^{(B \\cdot N) \\times C \\times d_m}$ to isolate the C variates for each cycle. Attention is applied over the variates similarly to iTransformer. The result is reshaped back to $\\mathbb{R}^{B \\times (C \\cdot N) \\times d_m}$. Next, the output of the variate attention is reshaped to $\\mathbb{R}^{(B \\cdot C) \\times N \\times d_m}$ to isolate temporal cycles. We then apply the second attention mechanism over the N cycles for each variate. In the iTransformer, the MLP operates on univariate data, hypothesized to capture intrinsic time series properties like amplitude, periodicity, and frequency spectra (Liu et al., 2024a), for which we finally reshape the series from $\\mathbb{R}^{(B \\cdot C') \\times N \\times d_m}$ to $\\mathbb{R}^{(B \\cdot N) \\times C \\times d_m}$ before applying the MLP layers."}, {"title": "5.3. iPatch is not a champion although it may look so", "content": "First, we evaluate the performance of iPatch following either average MSE/MAE (Wang et al., 2024b) or average ranks. Table 4 shows that iPatch achieves the best average rank, the second-best MSE, and MAE. Complete results for iPatch are available in Table 12. In line with these outcomes and common practices in the field of LTSF, iPatch may \"almost\" seem the best-performing model. However, we then analyze more rigorously the results following the statistical analysis introduced in Section 5.1. In particular, we perform a Friedman test to compare the results of all the classifiers over all datasets from a statistical perspective. We obtain a Friedman statistic of 1.14 and a p-value of 0.95, demonstrating that there is actually no real champion from this analysis."}, {"title": "5.4. iPatch statistically outperforms iTransformer", "content": "Given the lack of a true champion, we proceed with pairwise comparisons among models by applying the sign test (Dem\u0161ar, 2006). We first count the number of wins per dataset for each model and organize them in a 2D matrix, visible in Table 5. If the two algorithms compared are, as assumed under the null hypothesis, equivalent, each should win on approximately half of the datasets. We then compute the corresponding p-values for rejecting the null hypothesis, given that the number of wins follows a binomial distribution. There are no statistically significant differences among all pairwise comparisons except for iPatch and iTransformer, with the first model winning on 11/14 datasets and scoring a significant p-value of 0.05. This analysis provides two important takeaways: 1) The absence of statistical significance among pairwise comparisons re-iterates the lack of true champions among state-of-the-art models, and 2) with ad-hoc adjustments of existing architectures, it is still feasible to improve performance without claiming to be excellent overall. It is also essential to consider other factors that contribute to a model's superiority, such as the trade-offs introduced by architectural modifications between performance and efficiency (e.g., speed or memory consumption) or the actual net improvements that are practically relevant. However, these aspects fall beyond the scope of this paper."}, {"title": "6. Model selection based on dataset features", "content": "Identifying the appropriate model for a given dataset remains a challenging and nuanced task despite the availability of LTSF benchmarks. As anticipated in Section 2.1, BasicTS+ and TFB provide guidelines for this fundamental challenge (Shao et al., 2024; Qiu et al., 2024). In the following, we analyze whether our results align with previous observations. As we stated earlier, the tested models perform similarly across most datasets, regardless of architecture or characteristics, with MSE variations between datasets far exceeding those between models. For the following analyses, we make comparisons at a prediction length of 96 steps, as proposed in TFB."}, {"title": "6.1. Linear vs. transformer", "content": "BasicTS+ and TFB recommend using linear models when the data lacks clear patterns, has an increasing trend, or has a marked distribution shift. Conversely, transformers are recommended for datasets with clear patterns, strong internal similarities, or nonlinear structures. We revisit the example by (Shao et al., 2024) and assess the performance of a linear model (DLinear) versus transformer model (PatchTST) on data with clear and unclear patterns, respectively. We use the same dataset with an unclear pattern (Exchange) and replace their previously used PEMS with a clear pattern by PedestrianCounts (Figure 6). PatchTST outperforms DLinear on both occasions (Table 11), contradicting the previous claims by (Shao et al., 2024). We highlight that PatchTST, a transformer model, was excluded from their analysis for unknown reasons. However, both studies rely on limited datasets, urging caution before drawing broad conclusions."}, {"title": "6.2. Univariate vs. multivariate", "content": "Multivariate models should be preferred if \u2013 and only if \u2013 the dataset has strong inter-variate similarities (Shao et al., 2024; Qiu et al., 2024). To assess this guideline, we compare the performance of PatchTST (univariate) against iTransformer (multivariate) on all datasets. We use the explained variance from principal component analysis as a proxy for inter-variate similarity (Appendix B). Indeed, also for this comparison, we observe that neither model performs increasingly better depending on the inter-variate similarity as the ranks fluctuate across the full spectrum (Figure 5)."}, {"title": "6.3. Future direction", "content": "From Figure 1, we only observe interesting and substantial differences in model performance in two datasets (BenzeneConcentration and MotorImagery). Hence, we recommend that the community prioritize identifying and analyzing datasets that differentiate performance across models, as such datasets are crucial for drawing meaningful conclusions. Extensive research is needed to establish clear model selection guidelines, addressing challenges like defining baselines, acquiring representative datasets, and analyzing experiments in a rapidly evolving architecture landscape."}, {"title": "7. Alternative view", "content": "Critics might argue that our results are suboptimal due to experimental limitations, that focusing on recent models excludes influential earlier architectures and introduces bias, or that testing selected models overlooks the field's diversity. Others may contend that some models consistently perform best on specific datasets, challenging our claim that no single model currently dominates in LTSF.\nWhile these concerns are valid, our goal is not to establish exhaustive benchmarks or definitive rankings but to show that recent advancements often provide minimal improvement over earlier methods when experimental inconsistencies are addressed. The limitations in observed progress among the latest models underscore the need for the field to prioritize standardized and transparent testing practices over introducing increasingly complex architectures. Although our results may not be universally optimal, this further supports our position that small changes in experimental setups can significantly shift model rankings, making claims of superiority unreliable without standardized benchmarks and rigorous testing. By focusing on recent models, we intentionally highlight the current state of the field and its challenges in reliably evaluating and comparing new methods. Moreover, while some models may excel in narrow, context-specific scenarios, such isolated successes do not translate into universal applicability, further supporting our argument against the \"champion\" narrative."}, {"title": "8. Conclusions", "content": "In this work, we evaluated LTSF research, highlighting the need for rigorous and standardized benchmarking practices. Through an extensive and reproducible evaluation of 3,500+ models across 14 datasets, we demonstrated that claims of consistent performance improvements in newly published models often rely on specific experimental setups and evaluation methods. Our findings question the idea of universal advancements, revealing that no single model consistently excels across our experiments. We identified issues in the LTSF domain, such as non-standardized evaluation frameworks, biased comparisons, and limited reproducibility that hinder fair assessment and delay real progress. To address these challenges, we recommend adopting standardized evaluation protocols, prioritizing benchmarking robustness over architectural complexity, and further investigating the link between dataset characteristics and model performance."}, {"title": "A. Related work", "content": "Classical approaches\nTraditional statistical methods, such as AutoRegressive Integrated Moving Average (Box & Pierce, 1970), Vector Autoregression (Toda & Phillips, 1993), Exponential Smoothing (Hyndman et al., 2008), and Spectral Analysis (Koopmans, 1995) were widely used in TS forecasting. Progressively, machine learning models such as XGBoost (Chen & Guestrin, 2016), Random Forest (Breiman, 2001), Gradient Boosting Regression Trees (Friedman, 2001), and LightGBM (Ke et al., 2017) have shown improvements in the forecast due to their ability to handle non-linear patterns."}, {"title": "A.2. Deep learning models", "content": "Deep learning models have advanced TS forecasting, starting with Recurrent Neural Networks (RNNs), specifically designed to model sequential data. In particular, advanced variants such as RNNs with Long Short-Term Memory units, widely adopted within the TS community, have seen significantly increased usage (Hochreiter & Schmidhuber, 1997). Additionally, MLP-based models, such as DLinear (Zeng et al., 2022), N-BEATS (Oreshkin et al., 2020), and N-Hits (Challu et al., 2023) use MLP to learn the coefficients that produce both backcast and forecast outputs from their structure.\nOriginally from NLP, the Transformer architecture is increasingly adapted for time series forecasting, often with modified attention layers to capture temporal dependencies, as seen in Section 2 and other prior works, which we describe in the following. Informer (Zhou et al., 2021) and Pyaformer (Liu et al"}]}