{"title": "A Safe and Efficient Self-evolving Algorithm for Decision-making and Control of Autonomous Driving Systems", "authors": ["Shuo Yang", "Liwen Wang", "Yanjun Huang*", "Hong Chen", "Fellow, IEEE"], "abstract": "Autonomous vehicles with a self-evolving ability are expected to cope with unknown scenarios in the real-world environment. Take advantage of trial and error mechanism, reinforcement learning is able to self evolve by learning the optimal policy, and it is particularly well suitable for solving decision-making problems. However, reinforcement learning suf- fers from safety issues and low learning efficiency, especially in the continuous action space. Therefore, the motivation of this paper is to address the above problem by proposing a hybrid Mechanism-Experience-Learning augmented approach. Specifi- cally, to realize the efficient self-evolution, the driving tendency by analogy with human driving experience is proposed to reduce the search space of the autonomous driving problem, while the constrained optimization problem based on a mechanistic model is designed to ensure safety during the self-evolving process. Experimental results show that the proposed method is capable of generating safe and reasonable actions in various complex scenarios, improving the performance of the autonomous driving system. Compared to conventional reinforcement learning, the safety and efficiency of the proposed algorithm are greatly improved. The training process is collision-free, and the training time is equivalent to less than 10 minutes in the real world.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the development of advanced technologies such as computer science and artificial intelligence, autonomous driving technology is attracting more and more attention [1]. Autonomous vehicles are expected to move occupants from one location to another, while reduce costs, improve safety, and reduce traffic congestion [2]. As the \"brain\" of the autonomous vehicles, the decision-making and control is required to fully consider the interaction with other traffic participants and generate appropriate actions to complete driving tasks. Its capability directly reflects the intelligence of the autonomous driving system [3].\nRule-based methods are the most widely used at present for decision making and control. By analyzing the behavioral characteristics and decision logic of human drivers, engineers extract and design the corresponding rules. Common rule- based methods include finite state machine [4] [5], sampling method [6], etc. The advantage of these methods is that their output is known explicitly in the case of any given information with good interpretability. However, they rely on a large amount of expert knowledge and manually designed logic, which is difficult to cover all complex scenarios.\nIn recent years, several studies have applied optimization- based methods. These methods abstract the trajectory planning for autonomous driving as an optimization problem to find the optimal behavior under a certain optimization objective [7] [8] [9]. Nilsson et al. [10] proposed an overtaking decision method, which modeled the mixed logic dynamic system and realized the optimal solution based on the mixed integer programming. Farkas et al. [11] introduced a model predictive control with a centralized controller to realize speed planning, so as to reduce the blockage in roundabouts. The optimization- based methods can obtain the optimal solution in a certain traffic scenario. However, usually it is difficult to construct op- timization problems when dealing with both decision-making and control as a whole.\nDeep Reinforcement Learning (DRL) is a recent approach that combines the learning ability of deep learning with the decision-making abilities of reinforcement learning [12] [13]. Many studies have applied DRL to decision-making algorithms in autonomous driving, showing obvious perfor- mance improvements. Liu et al. [14] proposed the Scene- Rep Transformer to enhance RL decision-making capabil- ities through improved scene representation encoding and sequential predictive latent distillation. The proposed method is validated in a challenging dense traffic simulated city scenario. Yang et al. [15] proposed a robust decision-making framework for highway autonomous driving based on DRL. When encountering unseen scenarios, the rule-based algorithm will take over the learning based policy to ensure the lower limit performance. He et al. [16]proposes a new defense- aware robust RL approach designed to ensure the robustness of autonomous vehicles in the face of worst-case attacks. Simulation and experimental results show that the proposed method can still provide a credible driving policy even in the presence of the worst-case observed disturbance. It can be seen that compared to rule-based and optimization-based methods, such algorithms can realize self-training via interacting with the environment, and has the potential to deal with more complex and varying traffic environment."}, {"title": "II. PROPOSED FRAMEWORK", "content": "The proposed algorithm for decision-making and control of autonomous driving with self-evolutionary capabilities aims to generate efficient and safety control policy. It could not only provide reasonable actions in real time in response to changes in the complex traffic environment, but also allows for self-evolution during operation in the context of safe exploration. The algorithm is based on a typical actor-critic architecture that treats as a unified optimization problem to be solved. The main components of the proposed architecture, including the critic function approximator and the policy function approximator, are depicted in Fig. 1.\nIn this framework, the critic function approximator is used to estimate the value of the current policy and evaluate the effectiveness of the policy. The value function is chosen to consist of two Q-value networks [22], and it estimates the value by selecting the minimum value between the two Q- value networks $Q_1$ and $Q_2$, i.e., $\\min(Q_1, Q_2)$. This ap- proach is used to make the estimated value function closer to the true value in order to avoid overestimation. The policy approximator is employed to learn a safety policy with the objective of maximizing the return as much as possible. The policy approximator consists of three components, including an interaction model, a driver tendency network, and an optimization problem solver. A mechanism-based interaction model is applied to determine appropriate initial values for optimization and to achieve a hot start of the algorithm. In analogy to the driving process of a human driver, the concept of driving tendency is defined. It is used to quantifies the driving inclination during the driving process, which is a somewhat vague definition. The driving tendency network is designed to continuously improve its output, aiming to provide a more rational driving tendency that can effectively guide the optimization process of the algorithm. The resulting output, along with the output of the interaction model, is then given into the cost function. This cost function solves a nonlinear optimization problem, enabling it to generate safe and reliable lateral and longitudinal control actions for the vehicle."}, {"title": "III. AGENT IMPLEMENTATION", "content": "The autonomous driving problem can be seen as a Markov decision problem, which is defined as a tuple: (S, A, P, R), where S is state space, A is action space, P is transition model and R is reward function. We define the input to the algorithm as S and the output as A.\nPolicy $\\pi(s)$ is defined as a function of state to action. The optimization goal of the policy is to get an action that maximizes the expected reward. The expected rewards can be defined as:\n$\\upsilon_{\\pi}(S) = \\mathbb{E}_{\\pi} [G_t | S_t = s]$\n$\\upsilon_{\\pi}(S) = \\mathbb{E}_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma R_{t+k+1} | S_t = s]$\n(1)\n$\\upsilon_{\\pi}(S) = \\mathbb{E}_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma R_{t+k+1} | S_t = s], for all s \\in S,$\nwhere $G_t$ is the discounted cumulative reward, $S_t$ is the state at step $t$ and $\\gamma$ is discounted factor.\nThe Q function is used to map the state-action pair to the expected return value and can be used to estimate the long- term cumulative return value that can be obtained by taking an action in a certain state. The Q function can be defined as:\n$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} [G_t | S_t = S, A_t = a]$\n$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma R_{t+k+1} | S_t = S, A_t = a]$\n(2)\nTo obtain the maximum return, we can find the optimal policy by maximizing the value function. This can be achieved by using the Bellman optimality equation, which is defined as follows:"}, {"title": "B. State design", "content": "In this paper, the state space is designed to represent the observed environmental information. The state input must encompass sufficient information of the agent to comprehend all crucial factors within the environment. It is essential to include important information in the state representation to enable the agent to make optimal decisions. To achieve this, the state space sf can be designed as:\n$s_{f} =[x_{e}, Y_{e}, \\theta_{e}, V_{e}, x_{1}, Y_{1}, \\theta_{1}, V_{1}, x_{2}, Y_{2}, \\theta_{2}, V_{2}, ..., x_{n}, Y_{n}, \\theta_{n}, V_{n}],$\n(4)\nwhere n is the number of traffic vehicles in the observation area; $x_{e}, Y_{e}$ are the lateral and longitudinal coordinates of the ego car in the global coordinate system; $\\theta_{e}$ is the heading angle of ego vehicle; $v_{e}$ is the ego vehicle's speed; $x_{i}, Y_{i}$ are the lateral and longitudinal coordinates of the traffic vehicles in the global coordinate system; $\\theta_{i}$ are the heading angles of traffic vehicles; $v_{i}$ are the traffic vehicles' speed. All $i = 1, ..., n$."}, {"title": "IV. TRAFFIC INTERACTION MODEL DESCRIPTIONS", "content": "For human drivers, there are two subconscious considera- tions during the driving process: what actions should I take, and what kind of interactions will occur with other vehicles in response to my actions. This involves a process of modeling, predicting, and deducing. More experienced drivers are better able to model their surrounding environment, anticipate the behavior of other vehicles, and generate more reasonable actions in response.\nAnalysis of driving psychology of the human driver shows that humans will create a target tendency at each moment of the driving process. Further, the human will estimate how the interaction between the surrounding traffic vehicles and the ego vehicle will be influenced by this driving tendency. Drawing an analogy to the cognitive process of human drivers, this section introduces the concept of \"driving tendency\". It quantifies the driving inclination during the driving process, which is a somewhat vague definition. In this section, a traffic interaction model under driving tendency is established for al- gorithm development, and a potential impact vehicles selection model is proposed to improve computational efficiency."}, {"title": "A. Driving Tendency", "content": "First, analysis of the driving process considering human experience is presented. The driver usually gives a control command at each moment based on the actual state of sur- rounding vehicles. In this process, the driver has a vague idea, for example, \"there is a low speed vehicle on the right lane, so it's better to follow the front vehicle\", or, \"The right lane is free, and the front vehicle is about to yield the right of way. I can make a slight maneuver to pass quickly\". Especially in populated cities with complex traffic conditions, human drivers are even less likely to deliberately think and come up with specific higher-level behaviors, and strictly control the car, as shown in Fig 2. Based on the above analysis, driving tendency is defined as a term used to quantify which type of behavior a driver prefers to perform while driving. Autonomous driving can be seen as a policy search problem for a global optimization problem, so the essence of driving tendency is to quickly reduce the search space."}, {"title": "B. Potential Impact Vehicles Selection", "content": "Traffic dynamics is used to model the interaction between the ego vehicle and traffic vehicles [23] (all vehicles on the road except the ego vehicle). The traffic scenario is considered to include n + 1 vehicles, where vehicle 0 is the ego vehicle and the other n vehicles are traffic vehicles. Define the traffic vehicles as the set $Q = {V_{tra_{1}}, V_{tra_{2}},..., V_{tra_{n}}} $\nIn particular, the traffic dynamics can be described by the following discrete-time model:\n$s_{t+1} = f (s_{t}, \u016b_{t}),$\n(5)\nwhere $s_{t} = (s_{1_{t}},s_{2_{t}},...,s_{n_{t}})$ denotes the traffic state at time t, with $s_{0_{t}}$ denotes the state of ego vehicle. $s_{tk} \\in {1,...,n}$ denotes the state of kth traffic vehicle; $\u016b_{t}$ ="}, {"title": "C. Traffic interaction model", "content": "As described in Remark 2, although some traffic vehicles are close to the ego vehicle, the influence of these vehicles can still be ignored when modelling traffic interactions if the ego vehicle's algorithm can avoid interactions with them. As shown Fig. 4, taking a straight road as an example, the algorithm proposed is designed with hard constraints to avoid making unreasonable lane changes that would have a direct impact on the oncoming traffic behind. Therefore, the traffic interaction model involved is designed for the m vehicles in front of the ego vehicle.\nDefine the potential impact vehicles selection model as:\n$R_{m} = \\mathbb{O}(s_{f}, H_{D}),$\n(7)\nwhere $H_{D}$ is high definition map information, which contains the road id and the topological connection relationship. The above model filters out the nearest vehicle in all m lanes based on information about the ego vehicle and the surrounding vehi- cles, as well as information from the high definition map, and stores the vehicle status in the set $R_{m} = {V_{1}, V_{2}, ..., V_{m}} \\subset \\mathbb{R}$.Where $V_{i} = [x_{i}, Y_{i}, \\zeta_{i}], i = 1, 2, ..., m $.\nThe regular Intelligent Driver Model (IDM) model is ap- plied as a traffic interaction model, which has the advantage of a small number of parameters, a clear meaning and a good empirical fit. The mathematical description of the model is [24]:\n$a_{I D M}=a_{\\max }\\left[1-\\left(\\frac{u_{x}}{v_{0}}\\right)^{\\delta}-\\left(\\frac{s^{*}\\left(v_{x}, \\Delta v\\right)}{D_{r}}\\right)^{2}\\right]$\n(8)\nwhere $a_{IDM}$ is desired acceleration, $a_{max}$ is the maximum acceleration, $U_{x}$ is the vehicle speed, $v_{0}$ is the desired speed, $\\delta$ is acceleration exponent, $D_{r}$ is the relative distance, $\\Delta v$ is relative speed, $s^{*}\\left(v_{x}, \\Delta v\\right)$ is the expected following distance. The expected following distance is expected as:\n$S^{*}\\left(v_{x}, \\Delta v\\right)=s_{0}+\\max \\left(0, v_{x} T+\\frac{v_{x} \\Delta v}{2 \\sqrt{a_{\\max } b}}\\right),$\n(9)\nwhere $s_{0}$ is Minimum clearance distance, T is safe time headway, and b is desired deceleration."}, {"title": "V. ACTOR-CRITIC METHOD BASED SAFETY EVOLUTIONARY ALGORITHM", "content": "The proposed architecture is based on the soft actor citric al- gorithm [22]. In contrast to other RL algorithms, the objective of the soft actor critic algorithm includes not only maximiz- ing the desired cumulative reward, but also maximizing the entropy, as shown in Eq. 10:\n$\\pi^{*}=\\arg \\max _{\\pi} \\mathbb{E}_{\\left(s_{t}, a_{t}\\right) \\sim \\rho_{\\pi}}\\left[\\sum_{t} \\mathbb{E}_{r}\\left(s_{t}, a_{t}\\right)+\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot | s_{t}\\right)\\right)\\right]$\n(10)\nwhere H denotes entropy. By maximizing entropy, the output probabilities of actions can be dispersed as much as possible, enabling the exploration of a wide range of states and avoiding the omission of any useful actions.\nSimilar to all actor critic algorithms, the proposed architec- ture consists of two main components: the critical and the policy function approximator. The former aims to evaluate the policy performance based on observations input and the current actions. It achieves this by using a neural network to approximate the value function. The latter combines neural networks with nonlinear optimization methods to generate actions based on the observed information. The objective is to enable the agent to maximize the reward while ensuring safety in the environment interaction."}, {"title": "A. Design of critic function approximator", "content": "In this paper, the MLP network is used as the critic function approximator. Overestimation bias is a key problem in value estimation. This problem may cause the network to overestimate the value of state-action pairs, which may disrupt the learning process and lead to suboptimal policies. The solution involves updating two Q-networks independently, and selecting the minimum output value of the two networks as the final Q-value during each update. This method effectively reduces the bias and variance of the estimates, making the learning of the algorithm more stable. That is:\n$J_{Q}(\\theta_{i}) = \\mathbb{E}_{(s_{t}, a_{t}, s_{t+1}) \\sim \\mathcal{D}}[(Q_{\\theta_{i}}(s_{t}, a_{t}) - (r(s_{t}, a_{t})+\\gamma V_{\\theta_{i}}(s_{t+1})))^2], i = 1,2,$\n(11)\nwhere $Q_{\\theta}$ is Q network, r ($s_{t}$, $a_{t}$) is reward, $V_{\\theta}$ is value function and $\\gamma$ is discount factor."}, {"title": "B. Design of policy function approximator", "content": "The policy approximator consists of three components, including the traffic interaction model, the driving tendency network, and the optimization problem solver. On the basis of the traffic interaction model established in Section IV, this section combines the advantages of learning algorithm and receding horizon optimization to design the policy function approximator. By constructing a constrained nonlinear opti- mization problem, the policy learning process can be guaran- teed to be collision-free, thus enabling the safe evolution of autonomous driving systems.\n1) Optimization problem design: In this paper, the op- timization problem is constructed with the idea of model predictive control (MPC). The core of MPC is the receding horizon optimization, whose idea is to repeatedly solve the optimization problem at each sampling moment and output the control action, and timely correct various complex situations in the control process. In this way, despite the limited accuracy of the model in each prediction time domain, the resulting control policy is guaranteed to perform well with the help of feedback. Interestingly, human drivers themselves do not perform accurate modelling and prediction, but rather adopt a mechanism similar to receding horizon optimization, i.e., giving the most sensible action based on the actual state of the moment.\nTo make the problem tractable, a local solution to the trajectory optimization problem is usually obtained at each step t by estimating the optimal actions at t + Np on a finite horizon Np and performing the first action at step t. The general form of an optimization problem is defined as follows [25]:\n$\\Pi_{M P C}^{*}(s_{t})=\\underset{a_{t: t+N_{p}}}{\\arg \\min }\\left(V_{p}(s_{t})+\\sum_{i=t}^{t+N_{p}} \\gamma_{p} L_{p}\\left(s_{i}, a_{i}\\right)\\right)$\ns.t. $F(s_{i}, a_{i})=0$\n$\\Gamma(s_{i}, a_{i}) \\geq 0$,\n(12)\nwhere $s_{t}$ is the state input at time t, $a_{t: t+N_{p}}$ is the action sequence from step t to step t + Np, $\\gamma_{p}$ is the discount factor of MPC, $V_{p}$ is the terminal cost, $L_{p}$ is the stage cost, $\\theta$ is the optimal parameter, $F(s_{i}, a_{i})$ and $\\Gamma(s_{i}, a_{i})$ are equation constraints and inequality constraints, respectively. The equation defines a total cost function that includes the generation of a driving tendency network, which considers the overall driving policy and behavior tendencies to simulate the decision-making process of human drivers.\nThe optimal action sequence can be obtained by solving the above optimization problem with the parameter $\\theta$ for the state $s_{t}$ at step t:\n$a_{t}^{*}(s_{t}, \\theta)=\\left\\{a_{t}^{*}(s_{t}, \\theta), a_{t+1}^{*}(s_{t}, \\theta), \\ldots, a_{t+N_{p}}^{*}\\left(s_{t}, \\theta\\right)\\right\\}$\n(13)\nIn this paper, \u03b8 is defined as the parameters of neural network. By updating $\\theta$, policy function $\\Pi_{M P C}^{*}(x_{t})$ gradually approximates the optimal policy that maximizes the reward.\n2) Kinematic Vehicle Model: In this paper, the kinematic model is used to represent the motion of all vehicles. The kinematic bicycle model is defined by the following set of differential equation:\n$\\dot{x}=v \\cdot \\cos \\varphi$\n$\\dot{y}=v \\cdot \\sin \\varphi$\n$\\dot{\\varphi}=\\frac{v}{L} \\cdot \\tan (\\delta_{f})$\n(14)\nwhere $\\delta_{f}$ is the front wheel angle, L is the wheelbase.\nThe discrete-time [26] form of model at time step k is:\n$\\xi_{i+1}=f(\\xi_{i}, a_{i}),$\n(15)\nwhere $\\xi_{i} = [x_{i} \\quad y_{i} \\quad \\varphi_{i} ]^{T}$ is the state, and $a_{i} = [v \\quad \\delta_{f}]^{T}$ is the input. The Eq. 15 is calculated with the sampling interval T. From Section III, $\\xi_{i}$ can be obtained from the state $s_{i}$, i.e., $\\xi_{i} \\subseteq s$.\n3) Cost function design: Autonomous driving systems are required to be able to reach the destinations safely and comfortably with the highest possible efficiency. The nonlinear programming problem is constructed, and the safety actions can be obtained by solving constrained optimization problems. The optimization problem is specified as:\n$J_{M P C}=\\sum_{j=1}^{m}\\left|\\left|s_{t+N_{p} \\mid t}-\\hat{s}_{d e s, t+N_{p} \\mid t}\\right|\\right|_{Q_{j}}^{2}+$\n$\\sum_{i=t}^{t+N_{c}} \\sum_{p=1}^{N_{o b j}}\\left[\\frac{1}{\\left(x_{e, i \\mid t}-x_{p, i \\mid t}\\right)^{2}+\\left(y_{e, i \\mid t}-y_{p, i \\mid t}\\right)^{2}}+\\right.$\n$\\frac{1}{N_{p}} \\sum_{i=t}^{t+N_{c}}\\left|\\left|a_{i | t}\\right|\\right|_{R_{u}}+\\sum_{i=t}^{t+N_{c}}\\left|\\left|\\Delta a_{i | t}\\right|\\right|_{R_{d u}},$\n(16)\nwhere $N_{p}$ is control horizon, $N_{c}$ is prediction horizon, $[x_{p, i \\mid t} \\quad y_{p, i \\mid t}]$ are the prediction trajectories of the traffic vehicles in $N_{p}$ steps, $[x_{e, i \\mid t} \\quad y_{e, i \\mid t}]$ is the prediction trajec- tory of the ego vehicle in $N_{p}$ steps, and $N_{o b j}$ is the number of traffic vehicles.\nThe terminal states set proposed in Section IV.C is used to build the first term of the cost function, where $\\hat{s}_{d e s, t+N_{p} \\mid t}=\\left[x_{d e s, t+N_{p} \\mid t} \\quad y_{d e s, t+N_{p} \\mid t} \\quad \\theta_{d e s, t+N_{p} \\mid t}\\right]^{T}= \\left[\\kappa_{i}^{[0]} \\kappa_{i}^{[1]} \\kappa_{i}^{[2]} \\right]$, $\\kappa=\\mathbb{O}[j], j=1,2, \\ldots m$.\n$Q_{j}, R_{u}$ and $R_{d u}$ denote the weight coefficients of each cost function. The driving tendency is used to adjust the weight factor $Q_{j}$, which can be expressed as follows:"}, {"title": "5) Driving Tendency Network Design", "content": "The concept of driving tendency has already been discussed in Section IV. In this paper, a deep neural network is used to fit the optimal driving tendency to output a driving tendency factor \u03b5, and the model input is the environment state information.\nIn autonomous driving tasks, a reasonable driving prefer- ence refers to the safe and efficient expected intentions mani- fested during the driving process. The actor-critic architecture is a typical RL framework in which the goal of actors is to maximize cumulative return throughout the task. Therefore, under the premise of a well-designed reward function, the optimization goal of the driving tendency network is consistent with the actor function, which is to learn safe and efficient driving behavior by pursuing higher cumulative rewards with the goal of meeting or exceeding human driving performance.\nIn this paper, the driving tendency factor \u025b is defined as a continuous variable and solved by stochastic policy optimiza- tion method. In RL community, reparameterization techniques are widely used to improve the stability and efficiency of the learning process [22]. \u025b can be expressed as:\n$\\epsilon_{t}=f_{\\theta_{\\pi}}\\left(\\tau_{t} ; s_{t}\\right)=f_{\\theta_{\\pi}}\\left(s_{t}\\right)+\\tau_{t} \\cdot f_{\\theta_{\\pi}}\\left(s_{t}\\right)$\n(23)\nwhere $\\tau_{t}$ is the noise that satisfies the standard normal distri- bution, $s_{t}$ is the state input, $s_{\\phi} \\subseteq s_{f}$. $f_{\\theta_{\\pi}}(\\cdot)$ represents the output of the driving tendency network.\nBased on the soft actor critic algorithm, with minimizing KL divergence as the optimization objective, the cost function of the driving tendency network is defined as:\n$J_{\\epsilon}\\left(\\theta_{\\pi}\\right)=\\mathbb{E}_{s \\sim \\mathcal{D}, \\tau \\sim \\mathcal{N}}\\left[\\alpha \\log \\pi_{\\phi}\\left(f_{\\theta_{\\pi}}\\left(\\tau_{t} ; s_{t}\\right) | s_{t}\\right)-\\right.$\n$\\left.Q_{\\theta}\\left(s_{t}, f_{\\theta_{\\pi}}\\left(\\tau_{t} ; s_{t}\\right)\\right)\\right],$\n(24)\nwhere \u03b1 is temperature hyperparameter, which can be auto- matically adjusted by formulating a constrained optimization problem. The term $ \\alpha \\log \\pi_{\\phi}\\left(f_{\\theta_{\\pi}}\\left(\\tau_{t} ; s_{t}\\right) | s_{t}\\right)$ represents the entropy regularization component. This term encourages the policy to explore more diverse actions by maximizing the entropy, thus helping to prevent premature convergence to suboptimal policies.\nAs described in Section IV, the set $R_{m}$ contains information about the selected m potential impact traffic vehicles. As previously discussed, these m traffic vehicles have a direct influence on the driving behavior of the ego vehicle, so the input state $s_{\\phi}$ of the driving tendency network are designed as follows:\n$s_{\\phi}=\\left[D_{1}, L_{1}, D_{2}, L_{2}, \\ldots, D_{m}, L_{m}\\right],$\n(25)\nwhere D and L are the normalized values of the relative lon- gitudinal distance d and the relative lateral distance l between the ego vehicle and the nearest preceding vehicle in the current lane. i.e., $D=d / D_{\\max }, L=l /\\left(\\min \\left(\\max \\left(1-\\frac{l}{3}, 0\\right), 3\\right)+3\\right)$. Dmax is the maximum longitudinal distance.\nIn RL, the reward function has a direct impact on the convergence and performance of the algorithm. In order to design a reasonable reward function, a number of important principles need to be followed. Firstly, relying solely on a single positive or negative reward may lead the algorithm into"}, {"title": "B. Simulation Results", "content": "The algorithm is deployed in the training environment The computer is equipped with an Intel core i7-10700 CPU, NVIDIA GeForce GTX 1660 SUPER GPU.\nAs shown in Fig. 6, it shows the average return after 5 runs. It can be seen that the proposed algorithm converges in 4000 to 6000 steps, which indicates that the algorithm has a high learning efficiency. Meanwhile, no collisions occur throughout the training process due to the presence of hard constraints, showing that the proposed framework can achieve safety evolutionary and has the potential for online learning. In order to fully validate the performance, the experiments are designed to be conducted in static and dynamic scenarios.\nThe algorithm is verified in Frenet coordinate system. This system is widely used in the decision-making and control algorithms due to its ability to correlate actual geometric features and adapt to different road topologies [28].\n1) Static Obstacle Scenarios: The obstacle setup and sim- ulation results are shown in the Fig. 7. Frenet_s and Frenet_d represent the longitudinal and lateral coordinates in the Frenet coordinate system, respectively. The static obstacles are drawn as rectangles in colors and the ego car is a rectangular box drawn at 0.1s intervals. It presents the ego car achieves obstacle avoidance and reaches the end point. Meanwhile, when the end point is completely blocked, the ego car comes to a reasonable and safe stop. The driving tendency is also"}, {"title": "VII. CONCLUSION", "content": "This paper proposed a safe and efficient self-evolving algo- rithm for decision-making and control of autonomous driving systems. The approach treats decision and planning control as a unified optimization problem to be solved, which not only can improve the performance of the evolutionary algorithm quickly and efficiently, but also ensures safety during the learning process. Combining the advantages of rule-based, optimization-based and learning based methods, a hybrid ap- proach based on Mechanical-Experience-Learning is proposed, which can greatly improve the learning efficiency and reduce the training time of the algorithm to ten minutes. The method is validated in a dense traffic three-lane lane change scenario. Experiments show that compared with existing RL methods, the proposed method can ensure safety and collision-free during both training and algorithm deployment processes. Furthermore, by introducing a driving tendency network, it can reduce the search space and achieve more reasonable, safety, and efficient autonomous driving decision and control than MPC method. In future research, we plan to expand the dimension of state space and develop the pedestrian model to improve the ability to deal with complex static scenario and pedestrian interaction. We will also combine domain knowl- edge and deep learning methods to ensure more explainable effective performance improvements, while enabling safe self- evolution without being too conservative."}]}