{"title": "A Safe and Efficient Self-evolving Algorithm for Decision-making and Control of Autonomous Driving Systems", "authors": ["Shuo Yang", "Liwen Wang", "Yanjun Huang", "Hong Chen"], "abstract": "Autonomous vehicles with a self-evolving ability are expected to cope with unknown scenarios in the real-world environment. Take advantage of trial and error mechanism, reinforcement learning is able to self evolve by learning the optimal policy, and it is particularly well suitable for solving decision-making problems. However, reinforcement learning suf-fers from safety issues and low learning efficiency, especially in the continuous action space. Therefore, the motivation of this paper is to address the above problem by proposing a hybrid Mechanism-Experience-Learning augmented approach. Specifi-cally, to realize the efficient self-evolution, the driving tendency by analogy with human driving experience is proposed to reduce the search space of the autonomous driving problem, while the constrained optimization problem based on a mechanistic model is designed to ensure safety during the self-evolving process. Experimental results show that the proposed method is capable of generating safe and reasonable actions in various complex scenarios, improving the performance of the autonomous driving system. Compared to conventional reinforcement learning, the safety and efficiency of the proposed algorithm are greatly improved. The training process is collision-free, and the training time is equivalent to less than 10 minutes in the real world.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the development of advanced technologies such as computer science and artificial intelligence, au-tonomous driving technology is attracting more and more attention [1]. Autonomous vehicles are expected to move occupants from one location to another, while reduce costs, improve safety, and reduce traffic congestion [2]. As the \"brain\" of the autonomous vehicles, the decision-making and control is required to fully consider the interaction with other traffic participants and generate appropriate actions to complete driving tasks. Its capability directly reflects the intelligence of the autonomous driving system [3].\nRule-based methods are the most widely used at present for decision making and control. By analyzing the behavioral"}, {"title": "II. PROPOSED FRAMEWORK", "content": "The proposed algorithm for decision-making and control of autonomous driving with self-evolutionary capabilities aims to generate efficient and safety control policy. It could not only provide reasonable actions in real time in response to changes in the complex traffic environment, but also allows for self-evolution during operation in the context of safe exploration. The algorithm is based on a typical actor-critic architecture that treats as a unified optimization problem to be solved. The main components of the proposed architecture, including the critic function approximator and the policy function approximator, are depicted in Fig. 1.\nIn this framework, the critic function approximator is used to estimate the value of the current policy and evaluate the effectiveness of the policy. The value function is chosen to consist of two Q-value networks [22], and it estimates the value by selecting the minimum value between the two Q-value networks Q1 and Q2, i.e., min(Q1, Q2). This ap-proach is used to make the estimated value function closer to the true value in order to avoid overestimation. The policy approximator is employed to learn a safety policy with the objective of maximizing the return as much as possible. The policy approximator consists of three components, including an interaction model, a driver tendency network, and an optimization problem solver. A mechanism-based interaction model is applied to determine appropriate initial values for optimization and to achieve a hot start of the algorithm. In analogy to the driving process of a human driver, the concept of driving tendency is defined. It is used to quantifies the driving inclination during the driving process, which is a somewhat vague definition. The driving tendency network is designed to continuously improve its output, aiming to provide a more rational driving tendency that can effectively guide the optimization process of the algorithm. The resulting output, along with the output of the interaction model, is then given into the cost function. This cost function solves a nonlinear optimization problem, enabling it to generate safe and reliable lateral and longitudinal control actions for the vehicle."}, {"title": "III. AGENT IMPLEMENTATION", "content": "The autonomous driving problem can be seen as a Markov decision problem, which is defined as a tuple: (S, A, P, R), where S is state space, A is action space, P is transition model and R is reward function. We define the input to the algorithm as S and the output as A.\nPolicy \u03c0(s) is defined as a function of state to action. The optimization goal of the policy is to get an action that maximizes the expected reward. The expected rewards can be defined as:\n$\u03c5_\u03c0(S) = \\mathbb{E}_\u03c0 [G_t | S_t = s]$\n$= \\mathbb{E}_\u03c0 [\\sum_{k=0}^{\\infty}\u03b3R_{t+k+1} | S_t = s]$\n$S_t= S , for all s \u2208 S$,\nwhere Gt is the discounted cumulative reward, St is the state at step t and y is discounted factor.\nThe Q function is used to map the state-action pair to the expected return value and can be used to estimate the long-term cumulative return value that can be obtained by taking an action in a certain state. The Q function can be defined as:\n$q_\u03c0(\u03c2, \u03b1) = \\mathbb{E}_\u03c0 [G_t | S_t = S, A_t = a]$\n$= \\mathbb{E}_\u03c0 [\\sum_{k=0}^{\\infty}\u03b3^kR_{t+k+1} | S_t = 8, A_t = a]$\nTo obtain the maximum return, we can find the optimal policy by maximizing the value function. This can be achieved by using the Bellman optimality equation, which is defined as follows:"}, {"title": "A. Markov Decision Process (MDP)"}, {"title": "B. State design", "content": "In this paper, the state space is designed to represent the observed environmental information. The state input must encompass sufficient information of the agent to comprehend all crucial factors within the environment. It is essential to include important information in the state representation to enable the agent to make optimal decisions. To achieve this, the state space sf can be designed as:\n$s\u00a3 =[x_e, y_e, \\varphi_e, v_e, x_1, y_1, \\varphi_1, v_1, x_2, y_2, \\varphi_2, v_2, ..., x_n, y_n, \\varphi_n, v_n]$,\nwhere n is the number of traffic vehicles in the observation area; xe, Ye are the lateral and longitudinal coordinates of the ego car in the global coordinate system; e is the heading angle of ego vehicle; ve is the ego vehicle's speed; xi, Yi are the lateral and longitudinal coordinates of the traffic vehicles in the global coordinate system; i are the heading angles of traffic vehicles; vi are the traffic vehicles' speed. All i = 1, ..., n."}, {"title": "IV. TRAFFIC INTERACTION MODEL DESCRIPTIONS", "content": "For human drivers, there are two subconscious considera-tions during the driving process: what actions should I take, and what kind of interactions will occur with other vehicles in response to my actions. This involves a process of modeling, predicting, and deducing. More experienced drivers are better able to model their surrounding environment, anticipate the behavior of other vehicles, and generate more reasonable actions in response.\nAnalysis of driving psychology of the human driver shows that humans will create a target tendency at each moment of the driving process. Further, the human will estimate how the interaction between the surrounding traffic vehicles and the ego vehicle will be influenced by this driving tendency. Drawing an analogy to the cognitive process of human drivers, this section introduces the concept of \"driving tendency\". It quantifies the driving inclination during the driving process, which is a somewhat vague definition. In this section, a traffic interaction model under driving tendency is established for al-gorithm development, and a potential impact vehicles selection model is proposed to improve computational efficiency."}, {"title": "A. Driving Tendency", "content": "First, analysis of the driving process considering human experience is presented. The driver usually gives a control command at each moment based on the actual state of sur-rounding vehicles. In this process, the driver has a vague idea, for example, \"there is a low speed vehicle on the right lane, so it's better to follow the front vehicle\", or, \"The right lane is free, and the front vehicle is about to yield the right of way. I can make a slight maneuver to pass quickly\". Especially in populated cities with complex traffic conditions, human drivers are even less likely to deliberately think and come up with specific higher-level behaviors, and strictly control the car, as shown in Fig 2. Based on the above analysis, driving tendency is defined as a term used to quantify which type of behavior a driver prefers to perform while driving. Autonomous driving can be seen as a policy search problem for a global optimization problem, so the essence of driving tendency is to quickly reduce the search space."}, {"title": "B. Potential Impact Vehicles Selection", "content": "Traffic dynamics is used to model the interaction between the ego vehicle and traffic vehicles [23] (all vehicles on the road except the ego vehicle). The traffic scenario is considered to include n + 1 vehicles, where vehicle 0 is the ego vehicle and the other n vehicles are traffic vehicles. Define the traffic vehicles as the set $Q = {V_{tra}^1, V_{tra}^2, V_{tra}^3, V_{tra}^4}$. In particular, the traffic dynamics can be described by the following discrete-time model:\n$St+1 = f (St, \u016bt)$,\nwhere $s_t = (s_t^1,,...,s_t^n)$ denotes the traffic state at time t, with $s_t^0$ denotes the state of ego vehicle. $s_t^k \u2208 {1,\u2026,n}$ denotes the state of kth traffic vehicle; \u016bt ="}, {"title": "C. Traffic interaction model", "content": "As described in Remark 2, although some traffic vehicles are close to the ego vehicle, the influence of these vehicles can still be ignored when modelling traffic interactions if the ego vehicle's algorithm can avoid interactions with them. As shown Fig. 4, taking a straight road as an example, the algorithm proposed is designed with hard constraints to avoid making unreasonable lane changes that would have a direct impact on the oncoming traffic behind. Therefore, the traffic interaction model involved is designed for the m vehicles in front of the ego vehicle.\nDefine the potential impact vehicles selection model as:\n$R_m = O(sf, HD)$,\nwhere HD is high definition map information, which contains the road id and the topological connection relationship. The above model filters out the nearest vehicle in all m lanes based on information about the ego vehicle and the surrounding vehi-cles, as well as information from the high definition map, and stores the vehicle status in the set $R_m = {V_1, V_2, ..., V_m} C R$.Where $V_i = [x_i, y_i, \\varphi_i]$, $i = 1, 2, ..., m $.\nThe regular Intelligent Driver Model (IDM) model is ap-plied as a traffic interaction model, which has the advantage of a small number of parameters, a clear meaning and a good empirical fit. The mathematical description of the model is [24]:\n$a_{IDM} = a_{max} [1 - (\\frac{v_x}{v_0})^\u03b4 - (\\frac{s*(v_x, D_v)}{D_r})^2 ]$,\nwhere $a_{IDM}$ is desired acceleration, amax is the maximum acceleration, Ur is the vehicle speed, vo is the desired speed, 8 is acceleration exponent, Dr is the relative distance, Dv is relative speed, $s* (\u03c5x, D\u03c5)$ is the expected following distance. The expected following distance is expected as:\n$S* (\u03c5_x, D_\u03c5) = s_0 + max ax (0, \u03c5_xT + \\frac{\u03c5_xD_\u03c5}{2\\sqrt{a_{max}b}})$,\nwhere so is Minimum clearance distance, T is safe time headway, and b is desired deceleration."}, {"title": "V. ACTOR-CRITIC METHOD BASED SAFETY EVOLUTIONARY ALGORITHM", "content": "The proposed architecture is based on the soft actor citric al-gorithm [22]. In contrast to other RL algorithms, the objective of the soft actor critic algorithm includes not only maximiz-ing the desired cumulative reward, but also maximizing the entropy, as shown in Eq. 10:\n$\u03c0*= arg max_\u03c0  E_{(stat)~p} [\\sum_t Er (St, at) + \u03b1\u0397 (\u03c0 (\u00b7 | St))]$ ,\nwhere H denotes entropy. By maximizing entropy, the output probabilities of actions can be dispersed as much as possible, enabling the exploration of a wide range of states and avoiding the omission of any useful actions.\nSimilar to all actor critic algorithms, the proposed architec-ture consists of two main components: the critical and the policy function approximator. The former aims to evaluate the policy performance based on observations input and the current actions. It achieves this by using a neural network to approximate the value function. The latter combines neural networks with nonlinear optimization methods to generate actions based on the observed information. The objective is to enable the agent to maximize the reward while ensuring safety in the environment interaction."}, {"title": "A. Design of critic function approximator"}, {"title": "B. Design of policy function approximator", "content": "The policy approximator consists of three components, including the traffic interaction model, the driving tendency network, and the optimization problem solver. On the basis of the traffic interaction model established in Section IV, this section combines the advantages of learning algorithm and receding horizon optimization to design the policy function approximator. By constructing a constrained nonlinear opti-mization problem, the policy learning process can be guaran-teed to be collision-free, thus enabling the safe evolution of autonomous driving systems.\n1) Optimization problem design: In this paper, the op-timization problem is constructed with the idea of model predictive control (MPC). The core of MPC is the receding horizon optimization, whose idea is to repeatedly solve the optimization problem at each sampling moment and output the control action, and timely correct various complex situations in the control process. In this way, despite the limited accuracy of the model in each prediction time domain, the resulting control policy is guaranteed to perform well with the help of feedback. Interestingly, human drivers themselves do not perform accurate modelling and prediction, but rather adopt a mechanism similar to receding horizon optimization, i.e., giving the most sensible action based on the actual state of the moment.\nTo make the problem tractable, a local solution to the trajectory optimization problem is usually obtained at each step t by estimating the optimal actions at t + Np on a finite horizon Np and performing the first action at step t. The general form of an optimization problem is defined as follows [25]:\n$IMPC (s) = arg min (V(s0) + \\sum_{i=t}^{t+Np} Lp(s, ai))$,\ns.t. F(si, ai) = 0\n\u0393(si, ai) \u2265 0,\nwhere st is the state input at time t, $a_{t:t+N_p}$ is the action sequence from step t to step t + Np, Yp is the discount factor of MPC, Vp is the terminal cost, Lp is the stage cost, \u03b8 is the optimal parameter, F(si, ai) and F(si, ai) are equation constraints and inequality constraints, respectively. The equation defines a total cost function that includes the generation of a driving tendency network, which considers the overall driving policy and behavior tendencies to simulate the decision-making process of human drivers."}, {"title": "5) Driving Tendency Network Design", "content": "The concept of driving tendency has already been discussed in Section IV. In this paper, a deep neural network is used to fit the optimal driving tendency to output a driving tendency factor \u025b, and the model input is the environment state information.\nIn autonomous driving tasks, a reasonable driving prefer-ence refers to the safe and efficient expected intentions mani-fested during the driving process. The actor-critic architecture is a typical RL framework in which the goal of actors is to maximize cumulative return throughout the task. Therefore, under the premise of a well-designed reward function, the optimization goal of the driving tendency network is consistent with the actor function, which is to learn safe and efficient driving behavior by pursuing higher cumulative rewards with the goal of meeting or exceeding human driving performance.\nIn this paper, the driving tendency factor \u025b is defined as a continuous variable and solved by stochastic policy optimiza-tion method. In RL community, reparameterization techniques are widely used to improve the stability and efficiency of the learning process [22]. \u025b can be expressed as:\n$Et = f_{\\theta_\u03c0} (Tt; s_t) = f (s_t) + T_t \u00b7 f_{\\theta_\u03c0} (s_t)$\nwhere Tt is the noise that satisfies the standard normal distri-bution, st is the state input, $s_t^\u0103$\u2286 sf. $f_{\\theta_\u03c0} (\u00b7)$ represents the output of the driving tendency network.\nBased on the soft actor critic algorithm, with minimizing KL divergence as the optimization objective, the cost function of the driving tendency network is defined as:\n$J\u03b5(\u03b8\u03c0) =Es~D,T~N[alog \u03c0\u03c6(fe\u201e(Tt; st) | st)\nQo(st, fox (Tt; S\u20ac))],$\nwhere a is temperature hyperparameter, which can be auto-matically adjusted by formulating a constrained optimization problem. The term alog\u03c0\u03c6(fe\u201e(Tt; s\u2081) | s\u2081) represents the entropy regularization component. This term encourages the policy to explore more diverse actions by maximizing the entropy, thus helping to prevent premature convergence to suboptimal policies.\nAs described in Section IV, the set Rm contains information about the selected m potential impact traffic vehicles. As previously discussed, these m traffic vehicles have a direct influence on the driving behavior of the ego vehicle, so the input state st of the driving tendency network are designed as follows:\n$s_t^t = [D_1, L_1, D_2, L_2, \u2026, D_m, L_m]$,\nwhere D and L are the normalized values of the relative lon-gitudinal distance d and the relative lateral distance l between the ego vehicle and the nearest preceding vehicle in the current lane. i.e., $D = min(\\frac{d}{d_{max}},1)$, $L = (min(max(\\frac{l}{3},-3),3)+3)$. Dmax is the maximum longitudinal distance.\nIn RL, the reward function has a direct impact on the convergence and performance of the algorithm. In order to design a reasonable reward function, a number of important principles need to be followed. Firstly, relying solely on a single positive or negative reward may lead the algorithm into"}, {"title": "In", "content": "The reward function required for the driving tendency network training is designed based on the above principles.\nFirstly, autonomous vehicles are encouraged to drive within a certain speed range, so speed rewards need to be designed to train the driving tendency network that guides the ego vehicle to pursue as high a speed as possible. The speed reward function is designed based on the principle that the higher the speed of the ego vehicle, the higher the reward received. The speed reward is set to:\n$Ts = \\frac{Us}{U_{max}}$,\nwhere ps is the speed reward factor, Umax is the maximum vehicle speed.\nAutonomous vehicles should ensure safety in all scenarios. Therefore, a penalty should be sent when the ego vehicle is too close to the traffic vehicles. Define the safety distance penalty as:\n$\\sum_{i=1}^m { \\frac{Pn}{((d^i_l)^2 + (d^i_n)^2)^l}} $,\nwhere pn is the safety distance penalty factor, d' and dia are the lateral and longitudinal distances between the ego vehicle and the ith traffic vehicles.\nTo improve training efficiency and prevent reward sparsity problems in training, the driving tendency reward is designed to guide the agent to update gradients in the direction of pur-suing more reasonable driving tendency. The driving tendency reward is designed based on the principle that the ego vehicle tends to drive in a more wide-open direction. Using a one-way two-lane road as an example, the driving tendency reward is defined as:"}, {"title": "VI. EXPERIMENTS"}, {"title": "A. Scenario Description", "content": "In this section, the proposed safe and efficient self-evolving algorithm for decision making and control is validated in a dense traffic three-lane lane change scenario. The training environment was built in the simulation software CARLA [27]. The simulation map is chosen as an urban expressway section in Town04, where the lane number is three and the lane width is 3.5m. The ego vehicle is chosen as a class B vehicle including a vehicle dynamics model, and the traffic flow model is set to be generated around this vehicle. The maximum time for one round of simulation is 80 s. The training scenario is set within a 180m range in front of the ego vehicle, with randomly generated traffic flows in the speed range 6m/s 12m/s.\nThe parameters of the algorithm are set as shown in Table II. The value network is defined as a three-layer fully connected neural network with 5 neurons in the input layer, 1 neuron in the output layer and 256 neurons in the hidden layer, and the policy network is defined as a four-layer fully connected neural network with 4 neurons in the input layer, 1 neurons in the output layer and 256 neurons in the hidden layer. In each iteration, n\u044c = 256 sets of data is extracted from the replay buffer D for the gradient update of the algorithm."}, {"title": "B. Simulation Results", "content": "The algorithm is deployed in the training environment The computer is equipped with an Intel core i7-10700 CPU, NVIDIA GeForce GTX 1660 SUPER GPU.\nAs shown in Fig. 6, it shows the average return after 5 runs. It can be seen that the proposed algorithm converges in 4000 to 6000 steps, which indicates that the algorithm has a high learning efficiency. Meanwhile, no collisions occur throughout the training process due to the presence of hard constraints, showing that the proposed framework can achieve safety evolutionary and has the potential for online learning. In order to fully validate the performance, the experiments are designed to be conducted in static and dynamic scenarios.\nThe algorithm is verified in Frenet coordinate system. This system is widely used in the decision-making and control algorithms due to its ability to correlate actual geometric features and adapt to different road topologies [28].\n1) Static Obstacle Scenarios: The obstacle setup and sim-ulation results are shown in the Fig. 7. Frenet_s and Frenet_d represent the longitudinal and lateral coordinates in the Frenet coordinate system, respectively. The static obstacles are drawn as rectangles in colors and the ego car is a rectangular box drawn at 0.1s intervals. It presents the ego car achieves obstacle avoidance and reaches the end point. Meanwhile, when the end point is completely blocked, the ego car comes to a reasonable and safe stop. The driving tendency is also"}, {"title": "2) Dynamic Traffic Flow Scenarios", "content": "Compared to previous scenarios, dynamic scenarios involve complex interactions with surrounding vehicles. We comprehensively conduct com-parative experiments to thoroughly validate the effectiveness of the algorithm proposed. MPC algorithm and traditional RL algorithm are chosen for comparison to illustrate the advantages of the proposed algorithm.\nThe MPC algorithm is constructed using Eq. 14-22, and the optimization objective is defined as maximizing forward speed along the middle lane of while ensuring that the vehicle stays within the road boundaries and avoids collisions. The RL algorithm is chosen as the Soft Actor-Critic algorithm. The reward design includes speed reward, comfort reward, and collision penalty. In order to reduce the search space and improve the algorithm's convergence speed, inspired by paper [27], the action space a of the RL problem is designed as the parameter of the polynomial trajectory.\na = [ds, ax],\nwhere ds is the lateral distance to the endpoint of the local planning trajectory and ax is the desired longitudinal accel-eration. The PID controller is designed to track the desired trajectory, output steering wheel angle and throttle/brake info. In order to facilitate comparison, the parameters of soft-actor-critic algorithm are consistent with the proposed algorithm. The number of training steps is set to 1 million."}, {"title": "VII. CONCLUSION", "content": "This paper proposed a safe and efficient self-evolving algo-rithm for decision-making and control of autonomous driving systems. The approach treats decision and planning control as a unified optimization problem to be solved, which not only can improve the performance of the evolutionary algorithm quickly and efficiently, but also ensures safety during the learning process. Combining the advantages of rule-based, optimization-based and learning based methods, a hybrid ap-proach based on Mechanical-Experience-Learning is proposed, which can greatly improve the learning efficiency and reduce the training time of the algorithm to ten minutes. The method is validated in a dense traffic three-lane lane change scenario. Experiments show that compared with existing RL methods, the proposed method can ensure safety and collision-free during both training and algorithm deployment processes. Furthermore, by introducing a driving tendency network, it can reduce the search space and achieve more reasonable, safety, and efficient autonomous driving decision and control than MPC method. In future research, we plan to expand the dimension of state space and develop the pedestrian model to improve the ability to deal with complex static scenario and pedestrian interaction. We will also combine domain knowl-edge and deep learning methods to ensure more explainable effective performance improvements, while enabling safe self-evolution without being too conservative."}]}