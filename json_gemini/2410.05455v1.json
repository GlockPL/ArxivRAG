{"title": "Dynamic HumTrans: Humming Transcription Using CNNs and Dynamic Programming", "authors": ["Shubham Gupta", "Isaac Neri Gomez-Sarmiento", "Faez Amjed Mezdari", "Mirco Ravanelli", "Cem Subakan"], "abstract": "We propose a novel approach for humming transcription that combines a CNN-based architecture with a dynamic programming-based post-processing algorithm, utilizing the recently introduced HumTrans dataset. We identify and address inherent problems with the offset and onset ground truth provided by the dataset, offering heuristics to improve these annotations, resulting in a dataset with precise annotations that will aid future research. Additionally, we compare the transcription accuracy of our method against several others, demonstrating state-of-the-art (SOTA) results. All our code and corrected dataset is available at https://github.com/shubham-gupta-30/humming_transcription", "sections": [{"title": "1 Introduction", "content": "The field of Automatic Music Transcription (AMT) has made significant progress in developing algorithms that transform acoustic music signals into music notation, positioning it as the musical analogue to Automatic Speech Recognition (ASR). In the piano-roll convention, a musical note is typically characterized by a constant pitch (related to frequency), onset time (the start), and offset time (the end).\nOne application of AMT is humming transcription, which involves extracting musical notes from a hummed tune. This is a crucial component for melody search engines [6] and automatic music compositions [2]. Such applications enable song identification by mere humming, provide a quick starting point for creating new songs, and democratize music creation for those who may not play an instrument or have disabilities. However, achieving error-free music transcription remains a complex challenge, even for professionals.\nIn this paper, we explore humming transcription while working with the HUM-TRANS dataset [9], a novel dataset that claims to be the largest humming dataset to date. This dataset is a large collection of clean monophonic humming samples gathered by soliciting help from music students. This provides us with an opportunity for studying transcription in a monophonic setting. Various"}, {"title": "1.1 Evaluation metrics", "content": "The authors of the HumTrans dataset utilize the library mir_eval [10] to evaluate the performance of transcription methods on their dataset. The primary motivation for using this library is to standardize the implementation of metrics for music transcription. More specifically, they employ the method precision_recall_f1_overlap, which, according to the documentation, computes the Precision, Recall, and F-measure for reference vs. estimated notes. Correctness is determined based on note onset, pitch, and, optionally, offset, which the authors do not consider.\nThe authors consider a strict pitch tolerance of \u00b11 cent (mir_eval default value is \u00b150 cents), or in other words one hundredth part of a semitone, and the default onset tolerance of 50 ms.\nPrecision, recall and F1-score are metrics that depend on the definition of true positives (TP), false negatives (FN) and false positives (FP) [4].\n\nPrecision = \\frac{TP}{TP + FP}\n\n\nRecall = \\frac{TP}{TP + FN}\n\n\nF1 Score = 2* \\frac{Precision * Recall}{Precision + Recall}\n\nTP: Estimated onset is within the tolerance of the ground truth onset and is within the pitch tolerance.\nFP: If N estimated onsets are within the tolerance of the ground truth, only 1 will be considered TP if it's also within the pitch tolerance and the rest N-1 estimated onsets will be considered FP. This means that all ground truth onsets can only be matched once.\nFN: No estimated onsets were detected within the tolerance of the ground truth onset."}, {"title": "1.2 Dataset Challenges", "content": "A major issue with the HUMTRANS dataset is that the ground truth on-sets and offsets are not well aligned. This can be explained because in their methodology they instructed their subjects to synchronize their humming with the rhythm of the played melody, calling this approach as \"self-labeling\", without any post-processing. We had to overcome this challenge by coming up with semi-supervised ways to correct the provided onsets and offsets."}, {"title": "1.3 Octave Aware vs Octave Invariance", "content": "To simplify the problem of pitch estimation, the ground truth pitch given in MIDI file format can be transformed to an octave invariant representation by taking the modulo 12 of the MIDI numbers, which makes the song to be represented only by 12 semitones. In our work we produce results for both octave aware and octave invariant variations of the problem."}, {"title": "2 Transcription methodology", "content": ""}, {"title": "2.1 Better ground truth annotation", "content": "For the purpose of training a neural network, we need precise onsets and offsets for ground truth. We realized that more accurate labeled onsets and offsets result in better-trained models.\nTo this end, we designed a heuristic-based algorithm to compute improved onsets and offsets. This algorithm calculates a waveform envelope and determines onsets and offsets based on when this envelope dips below a specific threshold value. The onsets and offsets obtained in this way can be noisy, so we refine them by eliminating spurious onsets/offsets, enforcing a minimum note length, and maintaining a minimum silence length between notes. This method is supervised by using the number of notes from the ground truth provided by the dataset. We retain only those training, testing, and validation samples where the number of notes detected by our heuristic matches the number provided by the ground truth.\nIt's important to note that we can only trust the number of notes from the ground truth, as the onsets and offsets cannot be relied upon. By using this approach, we obtained better ground truth onsets and offsets, retaining 6,827 of the 13,080 in the training set (52.2%) and 440 of the 769 (i.e., 57%) in the test set. More details on this method are covered in Appendix A."}, {"title": "2.2 Network Design", "content": "Our neural network architecture is a convolution-based network. The model is inspired by the architecture in [3]. We have tailored our network for our use case of a monophonic humming dataset. The following are key elements of this design:"}, {"title": "2.3 Training", "content": "We train the model with a batch size of 16. For each batch, we randomly select a small sample from the humming recordings by choosing 5 to 10 notes for each batch element. Additionally, we introduce a new dummy note 89 to signify the beginning and end of the recording sample, as well as the silence between notes. Our task for every time frame of the CQT representation is to predict the note"}, {"title": "2.4 Inference", "content": "Unlike transformers, convolution networks generalize well beyond the training length examples they are trained on. Because of this, we do not have to perform inference on pieces of fixed lengths; instead, we can perform inference on the entire sample as long as available memory allows. During inference, we calculate model logits for each time frame over the possible space of notes. The naive way to convert these logits to actual note predictions would be to take the note at each time frame with the maximum probability. However, this results in noisy note attributions.\nWe clean up these noisy attributions using a dynamic programming based algorithm inspired by the use of the Viterbi algorithm in text-to-speech alignment"}, {"title": "3 Results and discussion", "content": "We compare our methods with various methods discussed in [9]. The authors provide extracted MIDIs for the test set for 4 methods in their GitHub repository [11]. These are - Vocano [7], MIR-ST500 [12], SheetSage [5], and JDC-STP [8]. In addition, we compute MIDIs for the test set using Spotify's basic_pitch [2], and compare these with the methods proposed in this report."}, {"title": "3.1 Octave invariant", "content": "Following [9], we calculate precision, recall, and F1-score, using the mir_eval library, with an onset tolerance of 50ms and disregarding offsets. We provide two comparisons here - a comparison with respect to the corrected ground truth we obtain and a comparison where we measure only the note accuracy, disregarding both onsets and offsets. Additionally, these comparisons are octave invariant, i.e a note is considered to be correctly predicted even if the octave does not match the ground truth exactly. We provide these results on the test set provided by the dataset in table 3.1"}, {"title": "3.2 Octave aware", "content": "In table 3.2, we also provide comparisons of our method with other methods while requiring the models to be octave-aware, i.e., we are looking for an exact note match, including the correct octaves."}, {"title": "3.3 Discussion", "content": "We observe that our method outperform all tracked methods for humming transcription. We observe that SheetSage performs the worst in all comparisons. Also note that our method performs similarly well in the octave invariant and the octave aware setting, indicating that our architecture is able to learn very robust note representations."}, {"title": "3.4 Future Work", "content": "In our work, we provide a novel methodology to accurately estimate monophonic humming transcriptions. A natural extension of this work is to transcribe poly-phonic humming samples. This work also provides a novel dynamic programming based post processing and we would like to explore the use of this as postprocess-ing in other transcription problems. It is also possible to use this postprocessing as a part of the loss function during training thus enabling better transcriptions from the get go."}]}