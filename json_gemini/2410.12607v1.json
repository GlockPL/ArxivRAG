{"title": "LOW-RANK ADVERSARIAL PGD ATTACK", "authors": ["Dayana Savostianova", "Emanuele Zangrando", "Francesco Tudisco"], "abstract": "Adversarial attacks on deep neural network models have seen rapid development and are extensively used to study the stability of these networks. Among various adversarial strategies, Projected Gradient Descent (PGD) is a widely adopted method in computer vision due to its effectiveness and quick implementation, making it suitable for adversarial training. In this work, we observe that in many cases, the perturbations computed using PGD predominantly affect only a portion of the singular value spectrum of the original image, suggesting that these perturbations are approximately low-rank. Motivated by this observation, we propose a variation of PGD that efficiently computes a low-rank attack. We extensively validate our method on a range of standard models as well as robust models that have undergone adversarial training. Our analysis indicates that the proposed low-rank PGD can be effectively used in adversarial training due to its straightforward and fast implementation coupled with competitive performance. Notably, we find that low-rank PGD often performs comparably to, and sometimes even outperforms, the traditional full-rank PGD attack, while using significantly less memory.", "sections": [{"title": "Introduction", "content": "Adversarial attacks, characterized by subtle data perturbations that destabilize neural network predictions, have been a topic of significant interest for over a decade [48, 16, 32, 5]. These attacks have evolved into various forms, depending on the knowledge of the model's architecture (white-box, gray-box, black-box) [49], the type of data being targeted (graphs, images, text, etc.) [12, 47, 16, 57], and the specific adversarial objectives (targeted, untargeted, defense-oriented) [55, 29].\nWhile numerous defense strategies aim to broadly stabilize models against adversarial attacks, independent of the specific attack mechanism [7, 14, 15, 41], the most effective and widely-used defenses focus on adversarial training, where the model is trained to withstand particular attacks [29, 50]. Adversarial training is known for producing robust models efficiently, but its effectiveness hinges on the availability of adversarial attacks that are both potent in degrading model accuracy and efficient in terms of computational resources. However, the most aggressive attacks often require significant computational resources, making them less practical for adversarial training. The projected gradient descent (PGD) attack [29] is popular in adversarial training due to its balance between aggressiveness and computational efficiency.\nIn this work, we observe that in many cases the perturbations generated by PGD predominantly affect the lower part of the singular value spectrum of input images, indicating that these perturbations are approximately low-rank. Additionally, we find that the size of PGD-generated attacks differs significantly between standard and adversarially trained models when measured by their nuclear norm, which sums the singular values of the attack. This metric provides insight into the frequency profile of the attack when analyzed using the singular value decomposition (SVD) transform, aligning with known frequency profiles observed under discrete Fourier transform (DFT) and discrete cosine transform (DCT) analyses of PGD attacks [54, 31]."}, {"title": "Related Work", "content": "Following the seminal contributions of [48, 16], the past decade has witnessed significant efforts to understand the stability properties of neural networks. This has led to the development of various adversarial attacks, categorized broadly into model-based approaches [35, 25, 53, 46] and optimization- based methods [32, 16, 30]. Concurrently, several robustification strategies have been proposed to mitigate these attacks [29, 2]. A key challenge in these approaches is the dependency on the availability of adversarial examples during the training phase, making the efficiency of generating such examples a central concern. Furthermore, the perceptibility of adversarial attacks is another critical factor that has been explored in various studies [9, 36].\nRecent research has also begun to focus on the frequency properties of adversarial attacks, making it an emerging area of interest in the community [31, 28, 17, 44].\nAdversarial Attacks and Training The discovery of adversarial examples has catalyzed extensive research on developing robust models. Typically, ensuring robustness against adversarial attacks is computationally intensive, as it often requires the generation of adversarial examples during training [16, 53]. However, adversarial training is not the only method for achieving robustness. Other approaches involve imposing constraints on models to enhance their robustness [26, 41, 56, 13]. The primary distinction between these approaches lies in their trade-offs: adversarial training techniques generally offer higher robustness but are more expensive due to the need for real-time adversarial example generation. In contrast, general robustness techniques may be less efficient in terms of robustness but are consistent across different scenarios. This trade-off highlights the challenge of transferability in adversarial attacks, where adversarially robust models tend to be highly effective against specific attacks but less so in broader contexts.\nLow-Rank Structures The exploration of low-rank structures has a longstanding tradition in deep learning and numerical analysis. Classical applications range from singular value decomposition (SVD) for image compression to tensor networks used for efficiently representing quantum multi-particle wave functions [39] and their time integration [6, 22]. In machine learning, low-rank methods have been integral to various applications, including recommender systems [23], latent factor models, and principal component analysis (PCA) [34, 20]. More recently, low-rank factorizations have gained attention in deep learning for their ability to provide efficient neural network representations and training methodologies [19, 42, 33, 18]. Notably, [18] demonstrated the potential for efficiently fine-tuning pretrained large models using small, low-rank additive corrections, further emphasizing the versatility and importance of low-rank structures in modern machine learning."}, {"title": "Low-Rank PGD-style attack", "content": ""}, {"title": "The singular spectrum of PGD attacks", "content": "Let us consider a general setup based on the image classification task. Specifically, let $X \\in \\mathbb{C}^{C \\times N \\times M}$ be an input data tensor with C-channels (C = 3 for RGB images) of sizes N \u00d7 M and $Y \\in \\mathbb{R}^D$ the target tensor. Assume also we have a parametric model $f_\\theta : \\mathbb{R}^{C \\times N \\times M} \\rightarrow \\mathbb{R}^D$ that has been trained with respect to a loss function $l : \\mathbb{R}^D \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}$. An untargeted adversarial attack on the input data X can be naturally formulated as a perturbation $\\delta X^*$ of bounded norm corresponding to the largest induced loss"}, {"title": "LoRa-PGD: Low-rank PGD attack", "content": "Similar to what is done in the context of parameter-efficient adaptation of large language models [18], we consider here a variation of PGD that directly searches for a low-rank structured attack."}, {"title": "Experiments", "content": "To provide a fair comparison, RobustBench [8] was used for pre-trained models, and adversarial-attacks-pytorch [21] library was used as a basis for the attack comparison. We re- call that in this section, we will always refer to the l2 norm $|| \\cdot ||_2$ meaning the entrywise l2 norm, i.e. the Frobenius norm for matrices. Implementation of the experiments is included in the supplementary material.\nComparison with other methods In the experimental section, we compare LoRa-PGD with the standard implementation of the PGD attack [29]. Note that an alternative obvious baseline would be computing a low-rank attack by further projecting the full PGD attack onto the manifold of rank r matrices via SVD after the last gradient ascent step. In all our tests, this approach produced a robust accuracy performance that is essentially the same as LoRa-PGD, while requiring significantly more execution time (due to the additional SVD projection step), as shown in Table 3. For this reason, we do not include this method in the performance comparison of Tables 1 and 2.\nInitialization We consider three types of initialization for the experiments:\n\u2022 Random Initialization. This approach follows a similar procedure to LoRA [18]. Specifically, one of the matrices in the factorization is initialized from a Gaussian distribution, while the other is initialized with zeros. This setup ensures that the optimization procedure begins from the original image X.\n\u2022 Transfer Initialization. This initialization tests the ability of LoRa-PGD to function as a transfer learner for adversarial attacks. For a fixed dataset, we first compute the full-rank FSGM attack on the standard model and use it as the starting point for both full PGD and LoRa-PGD on other models. For LoRa-PGD, we additionally compute the SVD of the FSGM attack to match the desired rank for initialization.\n\u2022 Warm-up Initialization. Similar to transfer initialization, this approach involves computing the FSGM attack, but it is tailored for each individual model and dataset. The resulting FSGM attack is then used as the starting point for both PGD and LoRa-PGD.\nIt is important to note that the three initialization methods did not affect the robust accuracy performance of PGD. Instead, they had a noticeable impact on the results of LoRa-PGD."}, {"title": "Discussion", "content": "In this work, we introduced LoRa-PGD, a novel algorithm that leverages low-rank tensor structures to efficiently produce adversarial attacks. Motivated by the empirical observation that PGD tends to generate adversarial perturbations that are numerically low-rank, we utilized this insight to develop a method that produces memory-efficient attacks. At parity of the nuclear norm of the attack, LoRa-PGD has demonstrated comparable performance to state-of-the-art methods like PGD across a variety of models and on two datasets.\nFuture research will explore benchmarking this method within the latest adversarial training techniques to further validate its effectiveness and practicality."}, {"title": "Limitations", "content": "As LoRa-PGD shares some common features with the classical PGD attack, it also inherits certain limita- tions. In particular, gradient-based methods are generally ill-suited for black-box settings. Additionally, from a timing perspective, the computation of gradients imposes a lower bound on the potential speed improvements. Furthermore, while LoRa-PGD is quite effective at lower relative ranks, its time and memory advantages diminish as the relative rank increases due to the additional computational cost associated with gradient calculations."}]}