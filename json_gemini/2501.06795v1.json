{"title": "Bridging the Fairness Gap: Enhancing Pre-trained Models with LLM-Generated Sentences", "authors": ["Liu Yu", "Ludie Guo", "Ping Kuang", "Fan Zhou"], "abstract": "Pre-trained language models (PLMs) are trained on data that inherently contains gender biases, leading to undesirable impacts. Traditional debiasing methods often rely on external corpora, which may lack quality, diversity, or demographic balance, affecting the effectiveness of debiasing. With the rise of large language models and their extensive knowledge, we propose enhancing fairness (Fair-Gender) in PLMs by absorbing coherent, attribute-balanced, and semantically rich sentences. However, these sentences cannot be directly used for debiasing due to alignment issues and the risk of negative transfer. We address this by applying causal analysis to estimate causal effects, filtering out unaligned sentences, and identifying aligned ones for incorporation into PLMs, thereby ensuring positive transfer. Experiments show that our approach significantly reduces gender biases in PLMs while preserving their language expressiveness.", "sections": [{"title": "I. INTRODUCTION", "content": "Lightweight pre-trained language models such as BERT [1] and ROBERTa [2] have achieved remarkable advancements across a wide range of tasks [3]\u2013[5], including language understanding [6], document classification [7], and multitask text generation. Their effectiveness largely stems from the ability of these PLMs to generate contextual representations.\nHowever, due to out-of-distribution [8], [9] or stereotypical biases inherent in the training corpus, their extensive de-ployment may inadvertently perpetuate biased or stereotypical representations [10], [11], leading to potential unfairness in applications [12] involving diverse social demographic groups. For instance, gender bias is evident when PLMs are more likely to associate male (female) attributes with programmers (nurses). This issue is particularly critical in specialized fields like law, medicine, or human resources [13], where ensuring fairness in encoded representations becomes crucial.\nRelated works and limitations. Many solutions [14]\u2013[16] have been proposed to mitigate social biases in PLMs. Based on whether the contextualized debiasing methods are directly integrated with downstream tasks, external corpora-based methods can be categorized into two types: (1) Task-Agnostic methods: Sent-Debias [17] and FairFil [18] are post-hoc methods that keep PLM parameters unchanged. ADEPT [19] introduces a novel training criterion that opti-mizes only the continuous prompt parameters while keeping the base model frozen. Auto-Debias [20], Context-Debias [21],\nand MABEL [22] eliminate biases in PLMs through fine-tuning with various bias-neutralizing loss functions. (2) Task-Aware methods focus on preventing bias from re-emerging when applying debiased models in real-world applications. For instance, Causal-Debias [23] integrates debiasing with down-stream fine-tuning via causal invariant learning, while Gender-tuning [24] provides a debiasing mechanism for any PLM, using standard fine-tuning techniques. Despite their notable success, all these methods rely on external corpora to identify and mitigate biases, aiming to ensure adequate demographic diversity. Moreover, acquiring high-quality corpora is typically expensive, and noisy information may be introduced [25], leading to insufficient bias reduction.\nMotivation. Recent foundational models like ChatGPT [26] and LLaMa [27] have demonstrated impressive intelligence across a range of complex tasks, including vision-based ques-tion answering [28] and knowledge reasoning [29], among others. Given that language models can incorporate extensive knowledge during pre-training [30], we consider LLMs as valuable knowledge bases that can provide insights to enhance fairness in other lightweight PLMs. The limitations in the qual-ity, diversity, and balance of external corpora, combined with the notable knowledge capabilities of LLMs [30], motivate us to leverage coherent, attribute-balanced, and semantically rich sentences from LLMs to debias lightweight PLMs.\nPresent work. In our approach, we limit the prompts for LLMs to ensure that the generated sentences are focused on social aspects. While these sentences are semantically rich and balanced across various groups, not all are immediately suitable for debiasing due to the significant differences in the latent spaces of LLMs and PLMs, which arise from their distinct pre-training data. Consequently, the sentences extracted from LLMs might include content that is difficult for PLMs to comprehend, and using it indiscriminately for debiasing could result in negative transfer [25]. To address this, we adopt a causal perspective and estimate causal effects to identify sentences that are well-aligned for debiasing PLMs. Sentences with strong causal effects are considered aligned sentences, facilitating positive transfer and reducing the learn-ing burden on PLMs by filtering out unaligned sentences, thereby preventing model degradation."}, {"title": "II. PRELIMINARY", "content": "Problem Definition. Let \\(W_a = \\{ (a_1^{(1)}, a_2^{(1)}, ..., a_d^{(1)}), (a_1^{(2)}, a_2^{(2)}, ..., a_d^{(2)}), ...\\}\\) denote attribute words composed"}, {"title": "III. METHODOLOGY", "content": "In this section, we address the process of generating knowledge using large-scale model and explore strategies for optimizing the utilization of the generated sentences. The comprehensive framework is illustrated in Fig. 1.\nGender Pairwise Sentences Generation from LLMs. For prompting LLM, we use two well-designed system prompts P1, P2 (depicted in step 1 in Fig. 1) to automatically generate pairwise sentences in two steps:\n\u2022\n\u2022\nP1: Please generate ten sentences containing the words in a tuple \\((a_i, v_t)\\) simultaneously. Control the word count in every generated sentence to around 20. The generated sentences strive for creativity, diversity, and logic.\nP2: Replace the term \\(a_i\\) to \\(a_j\\), and correct personal pronouns in the above generated ten sentences.\nwherein \\(a_i \\in \\{a_1, a_2, ..., a_d\\}\\), and \\(a_j\\) used for replacement comes from other d-1 elements, and \\(v_t \\in W_t\\). The generated sentences, containing \\((a_i, v_t), (a_j,v_t)\\), are with same target words \\(v_t\\) over d-tuple attribute words, for instance (she, boss), (he and boss). We can generate extensive pairwise sentences with high-quality, diversity, and gender balance in this step.\nDebiasing PLMs via Generated Pairwise Sentences. We construct a Structural Causal Model (SCM) to capture the causality between data, models, and hidden variables for positive transfer. As shown in Fig. 2, the biased pre-trained data of PLMs is denoted as \\(P_B\\), pairwise sentences as X, hidden variables from the initial and fine-tuned models as \\(H_0\\) and \\(H\\), and the predicted bias magnitude as B. The causal associations are: (1) \\(X \rightarrow H \rightarrow B\\): where H is derived from X by PLMs, and B is measured from H; (2) \\(X \rightarrow H_0 \\leftarrow P\\): where \\(H_0\\) is influenced by both P and X.\nAs shown in Fig. 2 (a), due to the significant difference between LLMs and PLMs in pre-training data creating dispar-ities in their latent spaces, applying LLM-derived knowledge directly for bias mitigation can cause negative transfer [25].\nTo achieve a positive transfer of generated sentences \\(X^L\\) from LLM and enhance the debiasing impact of PLMs, it is crucial to establish an alignment between the hidden space of PLMs and LLM. As shown in Fig. 2 (b), we split \\(X^L\\) into two nodes \\(X^C\\) and \\(X^{NC}\\). \\(X^C\\) represents the samples where causal effects are significant, and they should align with PLMs to enhance their fairness. \\(X^{NC}\\) signifies samples with smaller causal effects, presenting unaligned sentences due to negative transfer we filter this part of the data. In summary, the fine-tuned PLMs absorb LLM's demographic Sentences by utilizing causal effects (\\(P_B \\leftrightarrow X^C\\)). When conditioning on \\(H_F\\), the final bias magnitude depends on the degree of assimilating aligned knowledge from causal paths \\(P_B \\leftrightarrow X^C \rightarrow H^C \rightarrow B\\) (positive transfer).\nCausal Effect Estimation. When predicting \\(\\hat{B}^{(n)}\\), we first obtain the initial hidden state \\(h_0^{(n)} = M_0(x^{(n)})\\). \\(H_0 = h_0^{(n)}\\) means X represents all samples with hidden features \\(h^{(n)}\\). However, due to high-dimensional sparsity, the only suitable candidate is \\(x^{(n)}\\). Following [25], we relax this constraint and use the joint estimation of K-Nearest-Neighbor (KNN) samples to estimate causal effect \\(\\Delta\\) between P and \\(X^C\\):\n\\(\\Delta = \\sum_{n=1}^{N} \\delta^{(n)} \\sim \\ \\sum_{n=1}^{N} \\sum_{k=0}^{k_n} \\hat{B}(M_H(X^C = x^{(n,k)})) S_H(x^{(n)}, x^{(n,k)})\\)\nwhere N is the total number of pairwise sentences from aligned knowledge \\(X^C\\), and \\(k_n\\) is the number of KNNs for the n-th pairwise sentence in estimating \\(\\hat{B}^{(n)}\\), \\(x^{(n,k)}\\) is the k-th nearest neighbor of \\(x^{(n)} \\in X^N\\). \\(S_H(\\cdot, \\cdot)\\) is the similarity function between \\(x^{(n)}\\) and \\(x^{(n,k)}\\) (denoted as \\(S_{n,k}\\)), with \\( \\sum_{k=0}^{k_{n0}} S_H(x^{(n)}, x^{(n,k)}) = 1\\). Since \\(x^{(n)}\\) is most similar to itself, \\(x^{(n,0)}\\) is set as the anchor sample when k = 0. \\(\\hat{B}(M_H(X^C = x^{(n,k)}))\\) is the bias prediction of \\(\\hat{B}^{(n)}\\) when \\(x^{(n,k)}\\) is the input to model \\(M_H\\). Equation (1) shows that the total causal effect \\(\\Delta\\) is the sum of N aligned sentences' causal effects \\(\\delta^{(n)}\\), with each \\(\\delta^{(n)}\\) approximated by the weighted sum of the bias prediction when the input is the anchor sample \\(x^{(n)}\\) and its KNNs. The debiasing objective is:\n\\(L_b = \\sum_{x^{(n)} \\in X^C} \\sum_{k=0}^{k_n} D(X^C = x^{(n,k)})S_{n,k}\\)\nwhere \\(L_t\\) is a rewrite of the causal effect \\(\\Psi\\) estimated from \\(X^C\\). To integrate the aligned knowledge \\(X^C\\) into PLMs, we assess the knowledge preservation strength for each pairwise sentence \\(x^{(n)}\\) by selecting its KNNs \\(x^{(n,k)}\\). \\(D_t\\) measures the relative JSD between sentences with attribute words and those with target words, defined as:\n\\(D(x^{(n)}) = \\sum_{i,j \\in \\{1,...,d\\}, i<j} \\{JSD(R_x^{(a_i^{(n)})} || R_x^{(a_j^{(n)})})\\}\\)\nwhere \\(R_x^{(a_i^{(n)})}/R_x^{(a_j^{(n)})}\\) measures the distance from sentence x with attribute words \\(a_i/a_j\\) to sentences with all target words, respectively. The optimization of \\(L_b\\) ensures that the pairwise attribute words \\(a_i\\) and \\(a_j\\) have a uniform distance to all neutral target words, satisfying the fairness criterion. To prevent harm to PLM expressiveness from full parameter fine-tuning, we add an auxiliary representation loss \\(L_r\\) to preserve its inherent language modeling capability, defined as:\n\\(L_r = MSE(M_H(\\cdot) || M'_H(\\cdot))\\)\nwhere \\(L_r\\) uses Mean Squared Error to measure the difference between the original model's hidden states \\(M_H\\) and the debiased model's \\(M'_H\\), aiming to minimally adjust the PLM's parameters. The overall training loss is minimized as follows:\n\\(L = L_b + \\lambda . L_r\\),\nwherein \\(L_r\\) is tempered by the hyper-parameter \\(\\lambda\\)."}, {"title": "IV. EXPERIMENTS", "content": "Comparison. Our benchmarks including Task-Agnostic mod-els: Context-Debias [21], Auto-Debias [20], FairFil [18], and MABEL [22]; and Task-Aware models: Gender-tuning [24] and Causal-Debias [23]. We utilize GPT-3.5-turbo API as the source LLM for pairwise sentences generation. Three PLMs are as backbones: BERT [1], ALBERT [31], and RoBERTa [2]. Following prior studies, we choose the gender/racial/religion word lists from [21], [32], and [17] respectively.\nEvaluation. We report bias indicators including SEAT (the absolute value closer to 0 means lower biases) [33], StereoSet [34] (LM, SS, ICAT), and CrowS-Pairs [35]. We evaluate our framework on three GLUE tasks to measure the model's expressiveness, including SST-2, CoLA, and QNLI.\nConfiguration. We train models in 4 epochs with learning rate 5 x e-5 on a single GeForce RTX 3090 GPU.\nToxicity Detection. Ensuring the harmlessness of generated sentences is essential for the debiasing process. We apply the Comprehend API from Amazon Web Services (AWS) for toxi-city detection. The toxicity scores in the x-axis range from 0 to 1, where a higher score indicates a greater likelihood of the text containing toxic content. To have a fair comparison, we choose MABEL's entailment data in the gender domain. The toxicity distribution in Fig. (3a) is reported using equivalent data volume. The toxicity distribution shows that the low-toxicity segment (the peak in blue closer to 0) of Fair-Gender is notably lower than MABEL, and Fair-Gender exhibits significantly fewer high-toxicity levels (closer to 1). Given the potential constraints of the toxicity detection tool and aiming to improve the data quality for debiasing, we select 60% of these samples with the lowest toxicity across three domains for experiments, instead of directly using all the data as MABEL.\nOverall Performance. As indicated by the remarkable ICAT metric score in Table I, our Fair-Gender strikes a favor-able balance between language expressiveness and fairness. Notably, Fair-Gender even exhibits a slight improvement in LM metrics compared to the backbones, e.g., with the score rising from 84.17 to 85.30 in BERT. For the SEAT value, Fair-Gender achieves the best score, and improves 0.072 compared to the SOTA model Auto-Debias. Additionally, Fair-Gender outperforms others in CrowS-Pairs with the best score of 50.43. Fair-Gender performs best in ALBERT and ROBERT, while it does not rank top in terms of the SS value on BERT model, we note that this metric should be considered alongside LM, rather than evaluated in isolation. For instance, FairFil achieves the highest SS, yet its language modeling capability, as indicated by the lower LM score, suffers a marked decline and trails other methods.\nAblation Study. To verify the effectiveness of Fair-Gender, we consider the following ablated version:\n\u2022 (V1) w/o \\(L_r\\): Remove the designed representation pre-serving loss \\(L_r\\);\n\u2022 (V2) Rand-1: Replace KNNs for causal effect estimation with randomly selected pairwise sentences;\n\u2022 (V3) Rand-2: Reduce the number of KNNs from 5 to 2."}, {"title": "V. CONCLUSION", "content": "In this paper, we offer a flexible, universally applicable solution Fair-Gender capable of debiasing lightweight PLMs by harnessing rich, social relevant pairwise sentences sourced from LLM, unlike existing methods reliant on crafted external corpora. Fair-Gender roots in structural causal model (SCM) to reveal the limitations of direct utilization of generated sen-tences, such as alignment issues between LLM and PLMs, and negative knowledge transfer. We emoloy an improved causal graph to optimize the utilization of LLM-generated knowledge by filtering out sentences unaligned with PLMs, and only use aligned knowledge beneficial for positive transfer. We rigorously conduct quality and toxicity tests for the generated sentences, which maintains their usability for the debiasing process. Extensive evaluations show Fair-Gender's efficacy in mitigating diverse biases across various PLMs, while also preserving model expressiveness when applied to a series of downstream tasks."}]}