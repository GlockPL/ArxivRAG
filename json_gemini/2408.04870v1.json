{"title": "ConfusedPilot: Compromising Enterprise Information Integrity and Confidentiality with Copilot for Microsoft 365", "authors": ["Ayush RoyChowdhury", "Mulong Luo", "Prateek Sahu", "Sarbartha Banerjee", "Mohit Tiwari"], "abstract": "Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.\nIn this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) has emerged as a cornerstone of enterprise innovations. Among the various AI technologies, large language models (LLMs) [23], [26], [67], [68] and retrieval-augmented generation (RAG)-based systems [35], [40], [46]\u2013[48], [51], [52], [61], [65], [84] have transformed data interaction and decision-making within large enterprises [1]\u2013[5]. Among various commercial adoptions of RAG in enterprises, Copilot for Microsoft 365 [6] is a notable product that many businesses have widely integrated. Copilot is used across organizational hierarchy, with contributions to everyday tasks like code-generation [22], to business-critical decision making [7], like summarizing and consolidation of enterprise data [8], or with analysis and prediction mechanisms [9]. RAG systems drive efficiency and improve decision quality by providing more accurate, context-aware information. However, integrating such sophisticated systems into everyday business operations introduces complex vulnerabilities [24], [31], [32], [82], [86], particularly in large enterprise where much of the data is shared among users with varying level of permissions.\nEmployees create, edit, and maintain documents and pre- sentations containing critical and confidential business data. Organizations often utilize shared network drives, such as Microsoft SharePoint [10], [36] to store and share these documents across different departments securely. Products like Google Workspace [11] and Meta Workplace [12] also enable role-based access control mechanisms across the enterprise with active directory login to enforce the integrity and confi- dentiality of shared resources. However, incorporating artificial intelligence tools like RAGs in enterprise settings complicates access control. A RAG-based system needs read permissions user data [13] for information retrieval. Simultaneously, for these machine learning-based systems to automate business operations (e.g., summarise monthly reports or spell-check external documentation), they require write permissions to take action within the enterprise's existing document corpus. Simply granting read and write permissions of all data to the the machine learning models opens up a new attack surface.\nPrevious work has made a detailed analysis of information flow control in machine learning models [66], [74]. However, to our knowledge, there is no principled solution for systemat- ically managing access control and permissions. Misconfigu- ration of roles or permissions could lead to entities becoming overprivileged, which can leak sensitive data. RAG models are especially susceptible to the \u201cconfused deputy\u201d [39] problem, where an entity in an enterprise without permission to perform a particular action can trick an over-privileged entity into performing this action on its behalf and may threaten the security of these systems. To make matters worse, commercial RAG-based system vendors focus on attacks from outside the enterprise rather than from insiders. For example, Microsoft Copilot emphasizes how the enterprise's internal data are protected from vendors, the government, and other outside entities [14]. There is a lack of analysis and documentation on whether an insider threat can leverage RAG for data corruption and information leakage without being detected.\nFor example, there have been attacks that break the con- fidentiality of the training data [28]\u2013[30], [69] and integrity of model weights [27], [32], [57] in machine learning-based systems. For LLMs, people can also use prompt engineering [50], [72] for generating responses in violations of a particular policy at inference time. However, such violations usually"}, {"title": "II. BACKGROUND", "content": "RAG is a technique that enhances the response quality of a prompt-response system such as an LLM. It incorporates an additional step in an LLM system where the model retrieves external data to augment its knowledge base, thus enhancing accuracy and reliability in generating responses [47], without using retraining or fine-tuning. Figure 1 shows the general ar- chitecture of a RAG. It works as follows: the user requests the prompt an LLM \u2460, then the LLM retrieves the information \u2461. The retrieval generator sends back the embedded text \u2462, which is used to formulate a modified prompt \u2463, and used by a LLM model to generating answers \u2464. After a compliance check of the response \u2465, it is sent back to the user \u2466.\nThe core feature of RAG models is their retrieval mecha- nism \u2461\u2462, as detailed in Figure 2. Document resources are first chunked into blocks, which are then embedded into a vectorized database, while the prompt is also processed into an embedded context. Similarity matching is then used to decide the most relevant chunks/documents to retrieve [42]\u2013 [44]. Once the relevant documents are retrieved, the next phase is to fuse this external information with the generative capabilities of the LLM [47]. We use a Copilot in this work, to the best our knowledge, it uses a dense retrieval mechanism [44]."}, {"title": "B. Access Control", "content": "Managing access control and information flow is important for enterprise security. In traditional file systems such as Linux or Windows file systems, access control is usually managed by capabilities [36], [64], [70], [71] or access control list [38]. These access control mechanisms can help prevent entities without permission from accessing a data or resource. However, these empirical solutions may suffer from more intricate attacks, such as in the \"confused deputy\" problem [39], where a less privileged entity confuses a more privileged entity to act on its behalf, causing confidentiality or integrity violation. Recently, more fine-grained information flow control (IFC) [33], [45], [54], [55], [81] has also been adopted in systems. These IFC mechanisms may be formally verified against attacks [60]. Nevertheless, the overhead of managing the labels prevents these IFC-based systems from being prac- tically adopted. Besides, even with a formally verified access control system, it is still the user's job to configure the access control permission. Common faults include misconfiguration [77], [78], [83] and overprivilege [15]\u2013[18], [53], [58], [59]. Many commercial RAG-based systems provides compliance check frameworks [19] which are access control frameworks that enforces internal data access and compliance with external regulations. However, it is unclear how strong the protection such frameworks provide, and as shown in this paper, we can"}, {"title": "III. THREAT MODEL", "content": "We consider a scenario in an enterprise where RAG- based models like Copilot is used frequently by the internal employees. The response of Copilot is considered trusted. However, not all the employees can be trusted. An untrusted employee can serve as the attacker in this scenario. The goal of the attacker is to compromise Copilot's response when another victim employee ask Copilot a question. A compromised response can contain false information regarding enterprise operations, partial information that is cherrypicked to fit specific narrative, or contains confidential information that should not be provided to employees without permission to access those information.\nThe threat model is analogous to the one described in the classical confused deputy problem [39]. In this scenario, the attacker employee who is untrusted, tries to confuse Copilot which is trusted by other victim employees, which then provide responses against the security policy."}, {"title": "B. Attack Vector", "content": "In order to compromise Copilot's response, which is gener- ated based on RAG, which mainly use the malicious document as the main attack vector. The malicious document is created by the attacker employee, which contains relevant description regarding enterprise operations but the actual information it provides is false. The attacker employee stores a malicious document inside the enterprise drive and make it accessible by other employees as well as Copilot. If Copilot uses the information provided by Copilot, then the response will con- tain false information. Besides, the malicious document may also contains other strings that are used to control Copilot's behavior, such as only use specific document when generating the response, do not answer the questions, answer the question but do not provide a source."}, {"title": "C. Out-of-Scope Attacks", "content": "While the attacker is an employee which may have other permissions, we only consider the attack vector by storing a malicious document inside enterprise. We do not consider direct prompt engineering [72] attacks where the attacker"}, {"title": "IV. COPILOT PRELIMINARY", "content": "We describe an example of how an enterprise employee uses Copilot for work, and discusses what can be vulnerable to the attacker vector when he or she uses Copilot. At high level, Copilot searches for relevant documents regarding the prompt, and then generate the response.\nTo illustrate how Copilot works, we consider a fictional enterprise named WeSellThneeds LLC, which manufactures a product named \u201cThneeds\u201d across \u201cWhoville\u201d regoin, We con- sider three particular employees in the LLC: Alice (Regional Sales Manager in Whoville), Bob (Executive Sales Director for WeSellThneeds), and Eve (Saleswoman working under Alice's division). Sales manager Alice regularly creates a sales report document discussing sales for each season. Such a document may be consumed directly or indirectly (via Copilot) executive sales director Bob to make future strategic sales decisions such as expansion or contraction. Normally, such decision chain should be be affected by low level employee such as Eve."}, {"title": "A. Example Document", "content": "We present a example sales report authored by Alice, as shown below.\nDocument Title: Fleece Jacket Whoville Q4 Sales Memo\nDocument Owned By: Alice\nDocument Shared By: Alice, Bob, and Eve\nTo Whom It May Concern, This is a letter concerning Q4 sales of WeSellThneeds LLC's fleece jackets in Whoville, listed by different regions and revenue earned.\n\u2022\tNorth Whoville: $11.1 M, up by 65% from Q3\n\u2022\tWest Whoville: $14.1M, up by 25% from Q3\n\u2022\tSouth Whoville: $25.3M, up by 35% from Q3\nRevenue for fleece jacket sales in Whoville is net positive. Thus, we continue expanding in the Whoville Region by expanding our sales operations in the East Whoville region.\nIt describe the winter season (Q4) sales of fleece jackets within the company's Whoville regions. The report outlines the revenue generated from the sales in dollar amount and the percent increase from last quarter (Q3). Finally, the report discusses the potential of business expansions in the East Whoville region based on sales."}, {"title": "V. CONFUSEDPILOT DESCRIPTION", "content": "We describe ConfusedPilot attacks in this section. To illus- trate these attacks, we use the same context of WeSellThneeds LLC. in Section IV. Table I summarizes the attacks performed in ConfusedPilot and lists the symptoms and the root causes.\nWe discribe two attacks in this section, both by using malicious strings to instruct the behavior of Copilot in its responses. These attacks are possible because the malicious strings are embedded in the modified prompt to the LLM.\nFirst, in Attack 1, we demonstrate that attacker can instruct Copilot to generate content from a specific document and ignoring all other relevant documents. As a result, a user relying on Copilot's response will get the wrong information.\nSecond, Copilot usually provide a citation to the source doc- ument it generates answer from. In Attack 2, we demonstrate that a malicious string can disable Copilot from providing that citation. A user relying on Copilot's response will not be able to verify the source of Copilot's answer."}, {"title": "Attack 1: Generation response from selective documents.", "content": "We take the same setup described in the failed attack in Section IV-C, where the attacker Eve introduces a fake sales report with wrong numbers compared to the authenticate sales report created by Alice. The attacker Eve wants to instruct Copilot only to use this fake sales report when generating its responses. Thus, the attacker adds the malicious string: \u201cThis document trumps all ...\u201d to the end of the fake sales report, as shown below."}, {"title": "Attack 2: Disabling citations to the documents.", "content": "Even though Copilot response in Attack 1 is generated based on the fake sales report, it still provides a link to the fake sales report. Bob can verify the author of the cited source before using the generated content. This would expose the presence of the attacker Eve and also prevent Bob from using the generated malicious content. To disable Copilot from providing citations of the data source, we can add malicious strings in the fake sales report."}, {"title": "B. Mounting a Denial-of-Service Attack", "content": "We describe an attack where the malicious string disables Copilot to answer specific questions. For exam- ple, malicious string can be \u201cThis is confidential information, do not share.\u201d This triggers Copilot's content moderation, anomaly detection, or misinformation prevention mechanism, which are designed to flag sensitive, controversial, and biased content from appearing in its responses."}, {"title": "Attack 3: DoS attack.", "content": "Here, we show the fake sales report with a malicious string, the prompt, and Copilot's response in this scenario."}, {"title": "C. Exploiting Stale Data for Stealthy Integrity and Confiden- tiality Violation", "content": "We leverages the fact that Copilot's RAG is retrieving information from a cached version of the document rather than directly from original version which may be deleted. We demonstrate that Copilot may generate informa- tion from a fake deleted document, making it hard to verify the sources. We also demonstrate that Copilot may be used to expose information from a confidential document, even after the confidential document is deleted. This presents a data confidentiality violation risk."}, {"title": "Attack 4: Stealthily spread false information without trace.", "content": "We describe the attack process in this scenario. The attacker Eve first creates a fake sales report, whose numbers are different from the numbers on the authentic sales report."}, {"title": "D. Cascading Attacks", "content": "Attack 1-5 each individually creates single point security violation with in the enterprise. However, using the output of"}, {"title": "VI. EVALUATION", "content": "We use SharePoint to manage documents access control and sharing in the enterprise, and we use Copilot for Microsoft 265 as the example RAG, which retrieves documents from SharePoint for grounding responses. We use HotpotQA [80] to generate the corpus of documents that are stored in the SharePoint drive. The detailed document generation process is described in Algorithm 1."}, {"title": "B. Characterizing Malicious Strings", "content": "Since Attack 1, Attack 2 and Attack 3 depend on attaching malicious strings to the document in order to control the behavior of Copilot, we want to characterize what malicious strings are effective in each of the attacks, in addition to the strings presented in Section V.\nFor Attack 1, the malicious strings have a commanding tone and suggest the Copilot prioritize the malicious document over others, misleading Copilot into believing that the information provided is the most accurate and up-to-date. For Attack 2, the strings were designed to ensure that Copilot does not cite the document or its owner, thereby maintaining anonymity and reducing traceability. For Attack 3, the strings introduce terms and phrases that trigger policy violations or confidentiality flags, effectively blocking the retrieval and use of the malicious document."}, {"title": "C. Characterizing Temporal Sensitivity", "content": "For Attack 1, Attack 2 and Attack 3, we describe that by introducing malicious document, the Copilot responses will be affected. In reality, Copilot response will not change instantly but rather with some delay. If the passage of time is less than this threshold, Copilot's response will remain the same, while after this threshold, Copilot's response will change as described. Figure 6 shows the delay T between when malicious document is introduced and when RAG's repsonse is affected in Attack 1, 2 or 3.\nFor Attack 4 and Attack 5, we describe that Copilot still includes information from already deleted document. This attack is also time-sensitive since Copilot response will include the information only up to certain amount of time, and after that time, Copilot will no longer include information from the deleted document. Figure 7 shows the effective time window T when the RAG still refers to the deletec document in its response. Outside this window, the generated response or generated document will no longer refer to the deleted document."}, {"title": "D. Characterizing Access Control Sensitivity", "content": "The time delay of attacks can also be affected by the percentage of the documents the attacker has been granted access to. If the attacker who creates the malicious document is not granted access to some of the document, the time delay for the attack becomes larger. To study the impact of access control on the attacks, we measure the time delay defined in Figure 6 in two access control configurations. In the first configuration, the attacker is granted access to all (=500) the related benign documents, while in the second configuration, the attacker is granted access to half (=250) of the related benign documents.\nTable V shows the time delay for Attack 1, 2, and 3 in these two configurations. We see that if the attacker has access to only half of the benign documents, it actually takes longer time delay for the Copilot to change its response."}, {"title": "VII. DISCUSSION", "content": "Since RAG-based systems like Copilot are playing a more important roles in enterprise, the attacks presented in this paper pose a great threat to the enterprise. Depending on the use case of Copilot, and the specific attacks performed, this could lead to a variety of different consequences.\nFor example, many business decisions depend on collecting and analyzing enterprise internal data. Copilot can serve as an automated tool to collect the data. As presented in Attack 1 and 2, attacker can force Copilot to show false information, which mislead the business decisions, potentially cause monetary loss.\nSecond, Copilot can be used to enable service that requires high availability, for example, it can be used to build an intra- enterprise service designed for employees for searching inter- nal technical documents. In software companies, these tools"}, {"title": "B. Root Causes", "content": "While the attacks are demonstrated on Copilot, a RAG- based system, these attacks are caused by factors beyond RAG. It is a complex interaction of design patterns, machine learning models, and system implementation that enables these. We attribute the attacks to the following factors.\nLack of security enforcement in LLM. For real-world applications, security enforcement mechanisms, such as access control and information flow tracking are well studied and implemented across different system stacks including operat- ing systems, programming languages and low-level hardware. These security enforcement mechanisms are crucial in prevent- ing confidentiality and integrity violations. However, access control and information flow tracking have been in general not widely used inside machine learning model implementations. The machine learning model is generally treated as a blackbox and information flow can only be enforced via the input and output data. The lack of proper mechanism to enforce security leads to some of the attacks, since there is no way for the model to comprehend security requirement that are needed for each piece of data.\nLack of separation between control and data in LLM. In many implementation of RAG, the only interface between the user and the LLM is the prompt. Not only the data retrieved from the documents but also the corresponding instruction to do with the retrieved data are embedded in the modified prompt. For example, the data is a sales report, while the instruction can be \"summarize the report\". However, both the retrieved data and the instruction are combined in a single text string in the modified, unstructured prompt that is sent to LLM. In this unstructured prompt, there is no obvious distinction between the retrieved data and the instruction. Thus, in Attack 1 to 3, the attacker can embed \u201cinstructive\u201d malicious strings in the retrieved data which the LLM in- terprets as instructions. It might be desirable for the LLM to provide separate interfaces for inputting the retrieved data and the instruction, and only allowing the LLM to \u201cexecute\u201d the instruction but not the retrieved data. However, this might not be desirable since retrieved data may contain legitimate instructions. For example, the retrieved data can be a tutorial on \u201chow to summarize a sales report\u201d. In this case, if the instruction is \u201csummarize the sales report from last season based on the retrieved tutorial\u201d, then the instructions inside the tutorial must be followed. Simply banning LLMs from \u201cexecute\u201d instructions inside the retrieved data limits RAG's usability.\nTradeoff between performance and security. A practical RAG-based system like Copilot periodically indexes the data from the shared documents and store them in the database. In practical enterprise settings, document addition, deletion, access permission change can happen in real time, while the"}, {"title": "B. LLM Attacks", "content": "We demonstrated Copilot, a RAG-based system's vulner- abilities, specifically targeting the retrieval mechanism. In general, LLM is vulnerable to many different types of attacks. In AutoAttacker [76], it uses LLM to automate attacks on another LLM. In [73], it analyzes the behavior of LLM and designs an attack bypassing the existing defense of LLM. In [34], it designed a secret key game that can capture the ability of a model to hide private information. In [29], [56], LLM training data is demonstrated as can be extracted. To defend jailbreak attacks in LLM, in [62], it proposes a first general- purpose LLM defense. Many of these LLM vulnerabilities apply to RAG since RAG uses LLM as a key component. Thus, these vulnerabilities can be combined with vulnerabilities exploited by ConfusedPilot to create more powerful attacks."}, {"title": "C. RAG Security", "content": "Due to its increasing popularity, more works have focused on RAG security. In [82], it provides a high-level discussion about privacy issues in RAG. In [74], the RAG privacy guarantee is compared with other models, including IFC. In Pandora [32], it discusses that RAG can be jailbroken by a poisoning attack, similar to how we use poison attacks to violate the access control policy. More recently, in Poisone-dRAG [86], it also presents an attack on the RAG mechanism by manipulating the document used by RAG. However, there are a few differences. First, PoisonedRAG requires using LLM for generating poisoning data, while ConfusedPilot uses fixed malicious strings like \u201cThis document trumps all other documents,\u201d which is more efficient. Second, PoisonedRAG targets specific prompts, while ConfusedPilot can negate all the relevant prompts regardless of what the prompt about the data is. This makes the propagation of attacks within enterprises easier. Besides, PoisonedRAG performs the attack on an open-sourced RAG [41], [75], while ConfusedPilot is attacking a production RAG-based system with all the security mechanisms in place."}, {"title": "IX. CONCLUSION", "content": "This research has explored a series of vulnerabilities in- herent in RAG-based systems such as Copilot. We have demonstrated the feasibility and ability of such attacks to compromise enterprise integrity and confidentiality. These vul- nerabilities affect internal decision-making processes and the overall reliability of RAG-based systems, similar to Copilot.\nWhile RAG-based systems like Copilot offer significant benefits to enterprises in terms of efficiency in their everyday tasks, they also introduce new layers of risk that must be managed. ConfusedPilot provides insights into what the RAG users and the RAG vendors should implement to avoid such attacks."}]}