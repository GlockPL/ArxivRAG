{"title": "Expand VSR Benchmark for VLLM to Expertize in Spatial Rules", "authors": ["Peijin Xie", "Lin Sun", "Bingquan Liu", "Dexin Wang", "Xiangzheng Zhang", "Chengjie Sun", "Jiajia Zhang"], "abstract": "Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. Although benchmarks like MME, MMBench and SEED comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR). There is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. To handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set. We found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information. By expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. To our knowledge, we expanded spatially positioned image data controllably using diffusion models for the first time and integrated original visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and DINO). After conducting combination experiments on scaling data and models, we obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different instructions but also accurately distinguishes differences in visual positional information. VSRE achieved over a 27% increase in accuracy on the VSR test set. It becomes a performant VLLM on the position reasoning of both the VSR dataset and relevant subsets of other evaluation benchmarks. We open-sourced the expanded model with data and Appendix at https://github.com/peijin360/vsre and hope it will accelerate advancements in VLLM on VSR learning.", "sections": [{"title": "Introduction", "content": "Reasoning on spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. Traditional classification benchmark VSR (Liu, Emerson, and Collier 2023) has proposed a controlled probing dataset testing vision language models' capabilities of discrimination on spatial relations with natural image-text pairs. But,as Large Language Models (LLMs) become research hotspots, traditional tasks have been incorporated into conversational QA scenarios. The new VSR task requires the model to not only accurately recognize visual positional information but also follow instructions to correctly answer questions. Although models have evolved from traditional models with classification heads (ViLT, VisualBERT, and LXMERT) to VLLMs (LLaVA, BLIP2, Qwen-VL, and GPT-4), a unified evaluation method and effective optimization approach for VLLM toward VSR is still lacking.\nVLLM is the advanced version of LLM, which is enhanced to process and interpret multi-modal data. They leverage a powerful LLM as their cognitive engine to handle various tasks. Equipped with visual tokens, LLMs can perceive rich visual information and perform complex reasoning like VSR. However, most VLLMs often fall into the trap of hallucination problems. During the model's hallucination evaluation, the spatial relationships are precisely the type of relational hallucinations that are more challenging to object and attribute hallucinations (Liu et al. 2024c). Therefore, filling the blank of the evaluation and optimization of VLLM on VSR is of great significance for addressing the issue of hallucinations on relation.\nWe first re-evaluated the VLLM from scratch and diagnosed issues of inconsistent performance, hypersensitiveness on text prompts, insensitivity on vision information and answer bias. We found the absence of a unified instruction test has resulted in significant variance in model performance and the instability of instruction-following hinders the evaluation and optimization of VLLM's VSR capabilities. Therefore, we proposed a unified instruction test set by expanding the VSR test set through both manual and GPT4-generated templates.\nSimilarly, we expand the training set with the same template pool. Trained with more diverse QA formats, the model achieves fundamentally better generalization in responding to VSR questions regardless of the question style. Considering a VSR expert requires more of the ability to distinguish and recognize visual spatial information rather than just correctly following instructions, we controllably augmented and repainted the visual training image to specific spatial relation concepts (like \u201cin\u201d, \u201con\u201d and \u201cunder\u201d) by diffusion model. The increased amount and diversity of image data strengthen the model's comprehension of visual spatial details. Furthermore, we expand the vision encoder CLIP (Radford et al. 2021) with other hot encoders (SAM (Kirillov et al. 2023), DINOv2 (Oquab et al. 2024) and SigLIP (Zhai et al. 2023)) to obtain more vision perception ability that promotes model sensitivity to spatial information."}, {"title": "Merged Vision Encoder", "content": "Current works explored the benefits of multi-visual joint encoding across various perception and cognition tasks. Their ablation experiments identified the best combinations of popular vision features for specific tasks.\nMixture of Features (MoF) (Tong et al. 2024b) demonstrated that integrating vision self-supervised learning DINOv2 (Oquab et al. 2024) features with VLLMs can significantly enhance their visual grounding capabilities. MG-LLaVA (Zhao et al. 2024) enhances the model's visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. Prism (Karamcheti et al. 2024) perform a head-to-head comparison between CLIP (Radford et al. 2021), SigLIP (Zhai et al. 2023), DINOv2, and a standard Vision Transformer pretrained for classification (on ImageNet-21K, finetuned on ImageNet-1K) and find that the backbones trained with vision-language contrastive objectives (i.e. CLIP and SigLIP) are significantly more performant than alternatives. Cambrian-1 (Tong et al. 2024a) also explored 20 vision encoders and their combinations. They conclude that High-res encoders greatly enhance performance on chart&vision-centric benchmarks. Combining multiple vision encoders, including Self Supervise Learning models, enhances VLLM performance across various benchmarks, particularly in vision-centric tasks.\nMotivated by their findings, we summarized their high-performing backbones(including SigLIP, DINOv2 and SAM (Kirillov et al. 2023)) and followed their combination choice to expand our vision feature. After all, when it comes to intuitively understanding VSR tasks, precise visual detail perception and discrimination capabilities seemed more essential than complex semantic reasoning abilities. This necessitates that the model's visual feature processing component be more sensitive to visual positional information and possess more specialized spatial encoding capabilities."}, {"title": "Controled Image Generation", "content": "Diffusion models for controlled image generation offer a powerful and flexible approach to creating images that meet specific criteria. By incorporating conditioning information into the denoising process, models can generate highly controlled and precise outputs. They represent a powerful approach for image repainting, leveraging the systematic denoising process to generate high-quality, contextually consistent images. In this work, we utilized the most 3 popular applications to expand the image data as follows:\nText to Image generates an image from a text description. The denoising process is guided by the text, and once the denoising process ends after a predetermined time steps, the image representation is decoded into an image.\nImage-Text to Image is similar to text-to-image, but in addition to a prompt, an initial image is encoded to latent space then the noise is added to it. Then the model takes a prompt and the noisy latent image, predicts the noise, and removes the predicted noise from the initial latent image.\nImage Inpainting replaces or edits specific areas of an image. This makes it a useful tool to replace an image area with something entirely new. It relies on a mask to determine which regions of an image to fill in; the area to inpaint is represented by white pixels and the area to keep is represented by black. The white pixels are filled in by the prompt."}, {"title": "Re-evaluation from Scratch", "content": "In this section, we re-evaluate the performance of popular VLLMs on the VSR test from scratch and identify the problem of inconsistent performance, hypersensitiveness on text, insensitivity on vision and answer bias that hinder the evaluation. Due to page limitations, we provide a brief overview of our re-evaluation process as follows, more details are shown in Appendix Material Section 1.\nWe first provide the most comprehensive summary including over 120 attempts across more than 30 models. During the summary constitution, We identified inconsistencies in model performance, such as the model LLaVA1.5 having nearly a 20% lower accuracy in MiniGPTv2's reproduction(51%) (Chen et al. 2023) compared to the result in Prism(71%) (Karamcheti et al. 2024).\nThen we selected the most commonly used VLLM architecture, LLaVA1.5, for prompt engineering experiments with 69 templates. We found that hypersensitiveness on text prompts caused the inconsistency in performance. When asked about positional information, the model's accuracy is significantly affected by factors such as the questioning format, the insertion or deletion of specific phrases, and the order of certain words. Similarly, through case studies, we found that insensitive visual features struggle to distinguish positional categories.\nFurthermore, the model exhibited severe response biases for most template answers. Questions with \"yes\" answers have a significantly higher accuracy rate compared to those with \"no\". We suspect that may be due to the co-occurrence of subject and object concepts in both question text and images. This co-occurrence phenomenon may confuse the model, leading it to hastily provide \"yes\" answers based on co-occurrence rather than focusing on the actual visual spatial relationship.\nTo conclude, the over-sensitivity to language instructions caused large variance and inconsistency in model performance, while under-sensitivity to visual information led to insufficient perception of spatial relations, affecting answer accuracy and introducing response bias."}, {"title": "Expansion for Spatial Expert", "content": "To overcome the issues and obtain a Spatial Expert, we arrange our expansion method through VLLM's pertaining and instruction tunning pipeline in Figure 1."}, {"title": "Expansion on Text Data", "content": "Inspired by the success of visual instruction tuning, we prepare the test training data from the original VSR training [subject, relation, object] triplets extracted by spaCy\u00b9. We hope that after instruction fine-tuning(IFT) with augmented text data, the model, having been exposed to more diverse forms of Q&A, can reduce its sensitivity to question text prompts and exhibit higher generalization on instruction following when faced with various questioning formats. We expand the text QA data through both manual and GPT-40 generated prompt templates. To be exact, we select the top 30 templates manually in the re-evaluation process and append 20 GPT4-o generated ones."}, {"title": "Expansion on Image Data", "content": "To enhance the perception of position on VLLM, we provide them with dozens of times more image inputs with spatial relation concepts than ever. Motivated by the success on LLaVA pretrained on GPT4 generated captions, inversely, we freeze the text captions as prompts and utilize image generation diffuser SDXL (Podell et al. 2023) to generate images. More details and samples of the generation are posted in the Appendix Material Section 2.\nThe general image generation under prompt control ensures the display of specific positional relationship information between two entities, such as \"The teddy bear is in the couch\". We believe that an efficient principle for data augmentation is to generate images with as much diversity as possible while ensuring the original positional relationship semantics are maintained. Therefore, we employed the most three general settings to sequentially increase diversity.\nAs shown in Figure 2, Image-to-Image repaint the picture slightly in the first row which also maintained the consistency of the color on the background and the white region in front of the bear. The differences might not be apparent after visual encoding.\nTherefore, we armed our augmentation with Text-to-Image and Image-Inpainting strategic. Text-to-Image generated a more diverse set of images including variations in background and couch color and image style in the second row of Figure2. And in the last row, we utilized the bounding box as mask to inpaint the photo to make alter on subject and object. We replaced relatively small items with similar concepts like the bear with a dog, a cat and a clock. The revision helps prevent the model from learning biases on the distribution of subject and object combinations in the text."}, {"title": "Expansion on Vision encoder", "content": "With sufficient VSR text and image data support, we incorporated the most used visual backbone CLIP\u00b2 with SigLIP3, DINOv24 and SAM to fully explore the potential of combining visual features. The language-guided contrastive model CLIP and SigLIP benefit from the massive scale of noisy web image-text data. But self-supervised encoders DINOv2 and segmentation SAM may detect more fine-grained visual details which may profit the spatial reasoning process. We then merge the pretrained backbones on the shelf to obtain a superior vision encoder focusing on visual spatial relations. Specifically, as shown in Figure3, we design projectors and adapter to align visual tokens, concatenate these tokens along the feature dimension following MOF, MG-LLaVA and Cambrian. Additional designs of the projector and adapter are shown in Appendix Section 3."}, {"title": "Experiment", "content": "In this section, we illustrate the details of the dataset creation and trainng&inference arrangement. As for the baseline model, we adopt the most extensively adopted VLLM architecture LLaVA1.5 to verify our expansion methods. Similar to its training process, we also arrange our optimized experiment into 2 stages: (1) Pertaining stage to enhance the perception ability of visual spatial relation and understanding of positional text concepts. (2) Instruction fine-tune (IFT) stage to enhance instruction following ability then reduce the sensitivity to question text prompts and obtain higher generalization towards variant question formats."}, {"title": "Datasets", "content": "Testing Datasets To maintain consistency with previous evaluations, we used the VSR zero-shot test sets as the basis. We organized our test data into two sets:\n(1)Test-G random sampled a prompt from the 50 templates pool for each triplet to evaluate instruction-following generalization ability during spatial reasoning.\n(2)Test-S froze the template to the specific one ([caption], True or false.) which performs the best and is the simplest during the re-evaluation process. The split aims to test the model's VSR capability on visual under its most proficient question-asking format.\nTraining Datasets Excluding the test set, we collected more than 10k triplets with images from the original VSR dataset as a seed. Then we expand it several dozens of times into pre-train and IFT data as follow:(more details in Appendix Section 4)\n\u2022 Pre-training data: Under the three settings with a ratio of 5:3:2, we repainted the original images, expanding the quantity 20 to 100 times the original amount. We label the set as \"pre-100k\" for 100k pre-training data and \"pre-500k\" for 500k.\n\u2022 IFT data: We used general 50 prompt templates (30 manual and 20 GPT4-generated) to expand the 11k triplet data nearly 50 times to 500k, then name it as \"turn-g 500k\". Note that \"turn-s 11k\" and \"turn-g 11k\" are unexpanded IFT data for comparison. \"turn-g 11k\" used a randomly selected temple for each triplet and \"turn-s 11k\" used the specific template same as Test-S."}, {"title": "Training and Inference", "content": "To ensure fairness, we seed everything before the training and inference process. Throughout the entire training process, we froze all visual backbones. During the pre-training stage, we froze the LLM and only trained the adapter layers to encourage the model to learn better visual encoding of spatial details. In the fine-tuning stage, we unfroze the LLM, allowing it to participate in the training to enhance the model's instruction-following capability. Finally, during the inference stage, we limited the length of the model's responses to only one new word. This word was used as the answer to binary questions; for example, words like \"True\", \u201cYes\u201d or \u201cA\u201d were considered positive predictions, while \"False\", \"No\" or \"B\" were considered negative. Any other responses were directly judged as incorrect."}, {"title": "Result and analysis", "content": "We validated the effectiveness of our expanded data on LLaVA, further replaced the LLM and VLLM, the results showed that our data expansion adapted to various models."}, {"title": "Scaling on Data", "content": "Table 1 shows the result of LLaVA1.5 7B and 13B on scaling training data experiment. We pre-trained and tuned the existing projector(adapter) and LLM with their weights on-the-shelf using the augmented data. We found that during pre-training, adjusting the projector(adapter) with even just 11k of IFT data can lead to an increase in accuracy, but model failed to respond after tuning LLM by tiny 11k data. Therefore, we added the augmented 500k tune-g data and gradually increased the pre-train samples. First, after training with the turn-g 500k IFT samples, the 7B model improved by 4.0% on the Test-G set, and 13B model improved by 4.8%. Both versions achieved certain improvements on the Test-S. As the amount of pre-training data increased, the models continued to achieve better results. Although the accuracy gains became less noticeable when the data size increased beyond 400k, the 13B model ultimately achieved scores of 70.3 and 75.7 on each test. This indicates that data expansion not only helps to enhance the model's generalization ability to answer various text questions but also improves the model's ability to discern positional information. Notably, in the last row, we added tuning data turns-s 11k at the end, meaning we first pre-trained with pre-400k, then tuned with turn-g 500k, and finally pre-trained again with turn-s (the training sequence is marked with subscripts number in the table). As a result, we found that the model's performance on the Test-S further improved to 76.6. This demonstrates that supplementing with relevant data has great potential for optimizing model performance on VSR.\nTable 2 shows the result of scaling data across other hot-spot LLM and VLLM. We replaced the tuned LLM in LLaVA with other LLMs and randomly initialized the adapter for retraining. Also, we combined the pretrain and the tune set to fine-tune other VLLMs together. The results indicate that although the performance was not as good as continuing training on the trained LLaVA, our data augmentation method significantly improved other LLMs and VLLMs on VSR task. The accuracy of the model's responses improved consistently in both randomly asked general questions and fixed-format questions."}, {"title": "Scaling on Model", "content": "The 2 \"rainbows\" in Figure 4 shows the result of experiments on scaling vision model. The left \"rainbow\" illustrates the performance on Test-G set during the accumulation of pre-train data. In the comparison of the four individual visual backbones, SigLIP performed the best. Then CLIP performed the next, followed by DINOv2 and SAM. This reflects the superiority of language-supervised visual backbones in such multimodal tasks. Then we freeze SigLIP and append other visual features. In the comparison of binary visual backbone combinations, SigLIP+DINO emerged as the fastest and best learner in deep green dashed line. This indicates that the self-supervised backbone DINOv2 helps provide more detailed information, which aids the model in focusing on finer visual details in VSR QA scenarios. We freeze the SigLIP+DINO and add CLIP and SAM respectively. In the comparison of triple backbones, CLIP brought fewer benefits compared to SAM. This may be due to the high feature overlap between CLIP and SigLIP, as they are both contrastive learning models supervised by language. Finally, with the combined efforts of the four backbones, the accuracy of Test-G reached the highest value of 74.4.\nSimilarly, the right \"rainbow\u201d shows the accuracy on the Test-S. The combination results of each backbone are generally consistent with those of Test-G, except that in the comparison of triple backbones, the appended CLIP outperforms SAM. Another point is that compared to Test-G, the model learns faster under the single backbone setup. This is likely due to the reduced requirements on the language question side with the use of a fixed template, which lowers the overall task difficulty. Consequently, a smaller amount of data is sufficient for the model to achieve learning saturation. Nevertheless, the scaling model method ultimately improved the accuracy of Test-S to 79. Moreover, it can be observed that a common characteristic of both test sets is that while adding new visual features results in accuracy gains, the incremental improvement in accuracy diminishes as the number of added visual features increases. Finally, we designated the best-performing model with 4 backbones trained by the total data obtained as a Visual Spatial Reasoning Expert(VSRE)."}, {"title": "Result on Other Benchmarks", "content": "To verify the generalization of our expansion method, we tested our VSRE on the MME6, MMBench7 and SEEDv28.\nTable 3 show the comparison of VSRE on the related subsets of other benchmarks. Firstly, on the MME dataset, which also involves answering binary questions as VSR, VSRE achieved the highest score of 155.33, an improvement of more than 22 points compared to the optimized baseline LLaVA1.5 13B. This indicates that our expanded data and model are not overfitted to the single VSR dataset but show robust performance across benchmarks. Moreover, even though MMBench and SEEDv2 ask multiple-choice questions, VSRE still achieved the best results. Compared to the baseline LLaVA before optimization, the scores improved by 7.2 and 8.1 respectively. This may be due to our diverse template-designed IFT dataset, which included question formats similar to multiple-choice questions. Overall, the best performance among the dataset subsets demonstrates that our expansion method for VSR is not overfitting to similar data. Instead, it has genuinely learned visual position reasoning capabilities, handled more diverse text question formats and discerned visual positional information."}, {"title": "More Sensitive Vision Features", "content": "To verify that the model has become more sensitive to positional information alongside the accuracy increase, we selected 7 common positional relationships. For each one, we sampled 200 instances and used sklearn's t-SNE for dimensionality reduction on visual tokens to plot the distribution of each relationship in Figure 5. It is evident that after dimensionality reduction, the visual features extracted by VSRE are better distinguished, with more pronounced inter-class differences. Notably, in the right scatter figure, semantically similar concepts such as \"on\" (pink), \"on top of\" (purple), and \"above\" (blue) are clustered together, while the contrasting \"under\" (brown) is far apart.\nFurthermore, Table 4 presents the normalized average intra-class distance for each category. We use this metric to represent the model's ability to summarize and generalize single positional concepts; a smaller value indicates that the extracted features are better clustered (with more details shown in Appendix Section5). It is evident that the visual features extracted by the more professional spatial sensor VSRE have a smaller average intra-class distance. This indicates that VSRE has a better understanding and generalization capability for various visual spatial concepts."}, {"title": "Bias Result", "content": "We re-testing VSRE \u201cyes\u201d and \u201cno\u201d question accuracy with the same template dataset in re-evaluation, and found that the response bias issue was alleviated in Figure 6. Although the overall accuracy for \u201cyes\u201d questions is still higher than for \"no\", the gap between the two has significantly narrowed. As the accuracy of the red \u201cno\u201d questions has significantly improved, VSRE is not misled by the co-occurrence of related entity concepts in text and images but is more focused on the positional relationships between entities."}, {"title": "Conclusion", "content": "In this work, we first re-evaluate the VSR ability of VLLMS from scratch and diagnosed issues of inconsistent performance, hypersensitiveness on text prompts, lack of perception on visual details and answer bias. To address the problems, we first proposed the unified evaluation test set (Test-G and Test-S) for VLLM in QA scenario. Next, we proposed methods for expanding both the data and the model structure. We validated the effectiveness of the methods through experiments on scaling data and scaling models. We also validated the generalizability of our method on different benchmarks(MME, MMBench and SEED) and various models(VLLMs and LLMs). Then, our proposed spatial expert VSRE surpasses the performance of LLaVA1.5 13B by 27% (from 52% to 79%) on VSR test set.\nFurthermore, in the sensitivity analysis, we found that while VSRE exhibited better differentiation of visual positional concepts, it also demonstrated a superior ability to summarize them. A more sensitive visual position extractor brought about more specialized visual reasoning capabilities. This led the model to focus more on visual positional information, making it less influenced by the co-occurrence of entities and alleviating the issue of biased answer. Ultimately, our expansion method highlights the immense potential in VSR field for related data and model structures."}]}