{"title": "LLMs' Understanding of Natural Language Revealed (spoiler: LLMs do not understand language, but they can help us get there)", "authors": ["Walid S. Saba"], "abstract": "Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven reverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks, ample research has shown that LLMs are incapable of performing reasoning in tasks that require quantification over and the manipulation of symbolic variables (e.g., planning and general problem solving) see for example [25][26]. In this document, however, we will focus on testing LLMs for their language understanding capabilities, their supposed forte. As we will show here the language understanding capabilities have been widely exaggerated. While LLMs have proven to generate human-like coherent language (since that's how they were designed), language understanding capabilities have not been properly tested. In particular, we believe that the language understanding capabilities of LLMs should be tested by performing an operation that is the opposite of \u2018text generation' and specifically by giving the LLM snippets of text as input and then querying what the LLM \"understood\". As we show here, when doing so it will become apparent that LLMs do not truly understand language, beyond very superficial inferences that are essentially the byproduct of the memorization of massive amounts of ingested text.\nWe have conducted here tests that involve the following linguistic phenomena:\n(1) INTENSION\n(2)\nKNOWLEDGE, BELIEF AND OTHER PREPOSITIONAL ATTITUDES\n(3)\nCOPREDICATION\n(4)\nNOMINAL MODIFICATION\n(5)\n\u039c\u0395\u03a4\u039f\u039d\u03a5MY\n(6) REFERENCE RESOLUTION (AND COMMONSENSE)\nAdditional tests are being conducted on the following:\n(7)\nNOMINAL COMPOUNDS (OR, COMPOUND NOMINALS)\n(8)\nDE RE/DE DICTO\n(9)\nCOMPOSITIONALITY\n(10) PREPOSITIONAL PHRASE ATTACHMENTS\n(11) QUANTIFIER SCOPE AMBIGUITIES\nWe will report on these tests (7) through (11) and update this working document as the tests are\ncompleted.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven\nreverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks,\nample research has shown that LLMs are incapable of performing reasoning in tasks that require\nquantification over and the manipulation of symbolic variables (e.g., planning and general problem\nsolving) see for example [25][26]. In this document, however, we will focus on testing LLMs for\ntheir language understanding capabilities, their supposed forte. As we will show here the language\nunderstanding capabilities have been widely exaggerated. While LLMs have proven to generate\nhuman-like coherent language (since that's how they were designed), language understanding ca-\npabilities have not been properly tested. In particular, we believe that the language understanding\ncapabilities of LLMs should be tested by performing an operation that is the opposite of \u2018text gen-\neration' and specifically by giving the LLM snippets of text as input and then querying what the\nLLM \"understood\". As we show here, when doing so it will become apparent that LLMs do not\ntruly understand language, beyond very superficial inferences that are essentially the byproduct of\nthe memorization of massive amounts of ingested text.\nWe have conducted here tests that involve the following linguistic phenomena:\n(1) INTENSION\n(2)\nKNOWLEDGE, BELIEF AND OTHER PREPOSITIONAL ATTITUDES\n(3)\nCOPREDICATION\n(4)\nNOMINAL MODIFICATION\n(5)\n\u039c\u0395\u03a4\u039f\u039d\u03a5MY\n(6) REFERENCE RESOLUTION (AND COMMONSENSE)\nAdditional tests are being conducted on the following:\n(7)\nNOMINAL COMPOUNDS (OR, COMPOUND NOMINALS)\n(8)\nDE RE/DE DICTO\n(9)\nCOMPOSITIONALITY\n(10) PREPOSITIONAL PHRASE ATTACHMENTS\n(11) QUANTIFIER SCOPE AMBIGUITIES\nWe will report on these tests (7) through (11) and update this working document as the tests are\ncompleted."}, {"title": "LLMs Do Not \u2018Understand' Language", "content": "Understanding a snippet of text involves, ultimately, building a mental picture or a mental model\nthat corresponds to the state of affairs the text is supposedly describing. Moreover, \u201cunderstanding\"\nwhat was said also means making the right inferences from what was implicitly (if not explicitly)\nstated. We show here that LLMs do not truly 'understand' language and we do so by asking the LLM\nto describe various aspects of the given text. In querying the LLM we focus on a number of phe-\nnomena that have been extensively studied in linguistics, logic and the philosophy of language."}, {"title": "INTENSION", "content": "Linguistic objects (words, phrases, sentences, mathematical expressions, etc.) have an extension and\nan intension (with an 's'). While the extension of a term or a linguistic object is its value (or what\nthe object designates, or what it ultimately refers/evaluates to), the intension is the set of all proper-\nties that can be ascribed to the object. To take a simple example, consider the arithmetic expressions\n'2 * (3 + 4)' and '6 + 8'. While both expressions have the same extension (both evaluate to 14 \u2013 that\nis, both agree on the VALUE attribute), the two expressions have different intensions since there are\nseveral other properties that they do not share \u2013 e.g., NUM_OF_OPERATORS (that are 2 and 1, respec-\ntively) and NUM_OF_OPERANDS (that are 3 and 2, respectively)\u00b9. As another example consider \u20183 +\n2' and 'number of UN countries with veto power'. While both expressions evaluate to 5, these two\nterms are not interchangeable in all linguistic contexts. To see this, consider the following:\n(1) It will always be the case that [MASK] is 5\nIf [MASK] is replaced by '2 + 3' then (1) is always true, but if [MASK] is replaced by 'number of\nUN countries with veto power' then (1) is not necessarily true since we can imagine adding a country\nto or removing a country from the set of UN countries with veto power (note therefore that there\ncan be a temporal aspect to intensions). From these examples it should be obvious that natural lan-\nguage is rampant with intensional contexts and this phenomenon has been studied at least in a\nformal and mathematical sense, ever since Gottlob Frege's Sense & Reference [3] (see also [1][2][7]\nfor introductory material and lecture notes on \u2018intension' and \u2018intensional semantics').\nIt should be noted, here, that the underlying architecture of LLMs \u2013 namely that of deep neural\nnetworks (DNNs) is purely extensional and thus these models cannot reason (cope) with intension-\nality. Below we give some examples that illustrates this very clearly."}, {"title": "KNOWLEDGE, BELIEF AND OTHER PREPOSITIONAL ATTITUDES", "content": "Prepositional attitudes are mental states held by some agent about some preposition (statement).\nConsider for example the following preposition (statement):\n(2) P = The Statue of Liberty was a gift from the people of France to the United States.\nAn agent Jon could have several prepositional attitudes towards the preposition in (1) such as:\n(3) knows(Jon, P)\n(4) believes(Jon, P)\n(5) thinks(Jon, P)\nJon knows [that] P\nJon believes [that] P\nJon thinks [that] P\nNote that prepositional attitudes are (usually) linguistically marked by some verb such as thinks,\nknows, believes, wants, intends, desires, wishes, hopes, etc. that some preposition P (is true)\u00b2. Note\nalso that there's a difference between knowledge, belief, and truth. That is, some intelligent agent A\nmight know that some preposition P (is true) but they might also just 'think' (or 'believe') that some\npreposition P (is true) where in fact the preposition P is not true (if \"Jon thinks/believes [the earth\nis flat]\" that does not make [the earth is flat] true)\u00b3. (see [8][9] for introductory material on 'prep-\nositional attitude' and [10][11] for more formal discussions)\nWhat is important for us here is to investigate how much do LLMs \u2018understand' the subtle\ndifference between truth (facts), knowledge and belief in ordinary spoken language. Below we show\nthat in most cases LLMs fail to make the right inference in contexts with prepositional attitudes."}, {"title": "COPREDICATION", "content": "Copredication is the phenomenon that two or more predicates (properties, or relations) are being\napplied to the same entity or reference thus making a single reference being used to refer \u2013 at once,\nto several entities of different types (see [4][6]). Recognizing all the different types of entities being\nimplied by a single reference requires deep understanding of the overall context. As we show here,\nLLMs also fail to recognize copredication, even in its simplest form."}, {"title": "NOMINAL MODIFICATION", "content": "Another linguistic phenomenon that has been extensively studied is that of nominal modification\nwhere there is usually one or more adjectives modifying a head noun (that could also be preceded\nby modifying nouns). There are several issues that arise in constructs such as [A+ N+], some of\nwhich can be illustrated by the following examples:\n(1) a. Maria is an ancient philosophy teacher.\nb. Ron is an admired philosophy teacher.\n(2) a. Olga is a beautiful dancer."}, {"title": "\u039c\u0395\u03a4\u039f\u039d\u03a5MY", "content": "Metonymy occurs when we use an entity el to refer indirectly to another entity e2 that stands in\nsome relation to el. In the course of 'understanding' we usually resort to our commonsense back-\nground knowledge to infer the implicit relation between el and e2 (see [16][17] and [18])"}, {"title": "REFERENCE RESOLUTION AND COMMONSENSE", "content": "Reference resolution is perhaps the most known phenomenon in natural language semantics. A ref-\nerence is usually a pronoun (or relative pronoun) that refers to some entity that is mentioned else-\nwhere in the wider context. Note that the general utility of \u2018referring' is called 'anaphora' of which\npronouns are the most commonly used tool. (see [19][20][21] and [22])."}]}