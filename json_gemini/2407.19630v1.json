{"title": "LLMs' Understanding of Natural Language Revealed", "authors": ["Walid S. Saba"], "abstract": "Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven reverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks, ample research has shown that LLMs are incapable of performing reasoning in tasks that require quantification over and the manipulation of symbolic variables (e.g., planning and general problem solving) see for example [25][26]. In this document, however, we will focus on testing LLMs for their language understanding capabilities, their supposed forte. As we will show here the language understanding capabilities have been widely exaggerated. While LLMs have proven to generate human-like coherent language (since that's how they were designed), language understanding capabilities have not been properly tested. In particular, we believe that the language understanding capabilities of LLMs should be tested by performing an operation that is the opposite of \u2018text generation' and specifically by giving the LLM snippets of text as input and then querying what the LLM \"understood\". As we show here, when doing so it will become apparent that LLMs do not truly understand language, beyond very superficial inferences that are essentially the byproduct of the memorization of massive amounts of ingested text.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are the result of a massive experiment in bottom-up, data-driven reverse engineering of language at scale. Despite their utility in a number of downstream NLP tasks, ample research has shown that LLMs are incapable of performing reasoning in tasks that require quantification over and the manipulation of symbolic variables (e.g., planning and general problem solving) see for example [25][26]. In this document, however, we will focus on testing LLMs for their language understanding capabilities, their supposed forte. As we will show here the language understanding capabilities have been widely exaggerated. While LLMs have proven to generate human-like coherent language (since that's how they were designed), language understanding ca-\npabilities have not been properly tested. In particular, we believe that the language understanding capabilities of LLMs should be tested by performing an operation that is the opposite of \u2018text gen-\neration' and specifically by giving the LLM snippets of text as input and then querying what the LLM \"understood\". As we show here, when doing so it will become apparent that LLMs do not truly understand language, beyond very superficial inferences that are essentially the byproduct of the memorization of massive amounts of ingested text.\nWe have conducted here tests that involve the following linguistic phenomena:\n(1) INTENSION\n(2)\nKNOWLEDGE, BELIEF AND OTHER PREPOSITIONAL ATTITUDES\n(3)\nCOPREDICATION\n(4)\nNOMINAL MODIFICATION\n(5)\n\u039c\u0395\u03a4\u039f\u039d\u03a5MY\n(6) REFERENCE RESOLUTION (AND COMMONSENSE)\nAdditional tests are being conducted on the following:\n(7)\nNOMINAL COMPOUNDS (OR, COMPOUND NOMINALS)\n(8)\nDE RE/DE DICTO\n(9)\nCOMPOSITIONALITY\n(10) PREPOSITIONAL PHRASE ATTACHMENTS\n(11) QUANTIFIER SCOPE AMBIGUITIES\nWe will report on these tests (7) through (11) and update this working document as the tests are completed."}, {"title": "LLMs Do Not \u2018Understand' Language", "content": "Understanding a snippet of text involves, ultimately, building a mental picture or a mental model\nthat corresponds to the state of affairs the text is supposedly describing. Moreover, \u201cunderstanding\"\nwhat was said also means making the right inferences from what was implicitly (if not explicitly)\nstated. We show here that LLMs do not truly 'understand' language and we do so by asking the LLM\nto describe various aspects of the given text. In querying the LLM we focus on a number of phe-\nnomena that have been extensively studied in linguistics, logic and the philosophy of language."}, {"title": "INTENSION", "content": "Linguistic objects (words, phrases, sentences, mathematical expressions, etc.) have an extension and\nan intension (with an 's'). While the extension of a term or a linguistic object is its value (or what\nthe object designates, or what it ultimately refers/evaluates to), the intension is the set of all proper-\nties that can be ascribed to the object. To take a simple example, consider the arithmetic expressions\n'2 * (3 + 4)' and '6 + 8'. While both expressions have the same extension (both evaluate to 14 \u2013 that\nis, both agree on the VALUE attribute), the two expressions have different intensions since there are\nseveral other properties that they do not share \u2013 e.g., NUM_OF_OPERATORS (that are 2 and 1, respec-\ntively) and NUM_OF_OPERANDS (that are 3 and 2, respectively)\u00b9. As another example consider \u20183 +\n2' and 'number of UN countries with veto power'. While both expressions evaluate to 5, these two\nterms are not interchangeable in all linguistic contexts. To see this, consider the following:\n(1) It will always be the case that [MASK] is 5\nIf [MASK] is replaced by '2 + 3' then (1) is always true, but if [MASK] is replaced by 'number of\nUN countries with veto power' then (1) is not necessarily true since we can imagine adding a country\nto or removing a country from the set of UN countries with veto power (note therefore that there\ncan be a temporal aspect to intensions). From these examples it should be obvious that natural lan-\nguage is rampant with intensional contexts and this phenomenon has been studied at least in a\nformal and mathematical sense, ever since Gottlob Frege's Sense & Reference [3] (see also [1][2][7]\nfor introductory material and lecture notes on \u2018intension' and \u2018intensional semantics').\nIt should be noted, here, that the underlying architecture of LLMs \u2013 namely that of deep neural\nnetworks (DNNs) is purely extensional and thus these models cannot reason (cope) with intension-\nality. Below we give some examples that illustrates this very clearly."}, {"title": null, "content": "Problem: While the terms 'Madrid' and the 'Capital of Spain' are extensionally equal (they both\nrefer to the same object, at least presently), their intensions are different and are therefore not inter-\nchangeable in every linguistic context since that would lead to the wrong conclusions: while Mary\ndid tell her friends something about Madrid, she did not tell her friends anything about the Capital\nof Spain (like uncle Mitch, she might not even know that these two terms refer to the same object!)\nProblem: While $\\sqrt{256}= 16$ extensionally (they both refer to the same value) they are not equal\nintensionally and, again, are not interchangeable in every linguistic context. The inference above is\nwrong: if Mary taught her little brother that 7 + 9 = 16 this does not mean that she taught her little\nbrother that 7 + 9 = $\\sqrt{256}$. An intelligent NLU system should not make this inference as this would\nbe the wrong understanding of what was said.\nProblem: Similar to the previous example, while 'Paris' is in fact 'the most populous city in France'\n(both terms are extensionally equal as they both refer to the same object), these two terms have\ndifferent intensions \u2013 for example, 'the most populous city in France' need not always be 'Paris' and\nso these two objects are not interchangeable in every linguistic context (uncle Mitch, for example,\ndoes not know that currently 'Paris' happened to be 'the most populous city in France')."}, {"title": null, "content": "Problem: Again, while 'Aristotle' and 'the tutor of Alexander the Great' refer to the same object,\nthey are intensionally different and cannot be interchanged in every context (the understanding is\nwrong since John might not even know that \u2018the tutor of Alexander the Great' was 'Aristotle').\nProblem: Again the LLM made the wrong inference, due to the wrong 'understanding'. John might\nnot know that Billy the Kid happens to be one \u2018William H Boney' and thus if John likes to see\nmovies about Billy the Kid that does not mean John loves to see movies about William H Boney."}, {"title": "KNOWLEDGE, BELIEF AND OTHER PREPOSITIONAL ATTITUDES", "content": "Prepositional attitudes are mental states held by some agent about some preposition (statement).\nConsider for example the following preposition (statement):\n(2) P = The Statue of Liberty was a gift from the people of France to the United States.\nAn agent Jon could have several prepositional attitudes towards the preposition in (1) such as:\n(3) knows(Jon, P)\nJon knows [that] P\n(4) believes(Jon, P)\nJon believes [that] P\n(5) thinks(Jon, P)\nJon thinks [that] P\nNote that prepositional attitudes are (usually) linguistically marked by some verb such as thinks,\nknows, believes, wants, intends, desires, wishes, hopes, etc. that some preposition P (is true)2. Note\nalso that there's a difference between knowledge, belief, and truth. That is, some intelligent agent A\nmight know that some preposition P (is true) but they might also just 'think' (or 'believe') that some\npreposition P (is true) where in fact the preposition P is not true (if \"Jon thinks/believes [the earth\nis flat]\" that does not make [the earth is flat] true)\u00b3. (see [8][9] for introductory material on 'prep-\nositional attitude' and [10][11] for more formal discussions)\nWhat is important for us here is to investigate how much do LLMs \u2018understand' the subtle\ndifference between truth (facts), knowledge and belief in ordinary spoken language. Below we show\nthat in most cases LLMs fail to make the right inference in contexts with prepositional attitudes."}, {"title": null, "content": "Problem: The 'understanding' of the LLM is wrong. While John knows Stavros well, this does not\nmean he knows every true fact about Stavros (e.g., that Stavros was elected the mayor of Athens).\nProblem: There are several problems here. First, Sara might know that Mars is referred to by 'the\nred planet' and that alone should block the inference that Sara knows (or even believes) that Mars\nrotates on its axis. Second, the LLM failed to recognize the difference between \u2018believes' and\n'knows'. If Sara believes everything Galileo says, then at the most she can believe that Mars rotates\non its axis, but we cannot say she 'knows' that. For Sara to 'know' something (and not merely believe\nit), she must be aware of the truth of that fact, and nothing in the text implies that.\nProblem: While Carlos knows Sofia, that does not mean Carlos knows every true fact about Sofia\n(such as the fact that she is a Romanian Gypsy)."}, {"title": null, "content": "Problem: Again, the LLM arrived at the wrong conclusion (understanding): While John knows\nMaria (who happens to also be a talented dancer), it was never stated that John also knows she is a\ntalented dancer so we cannot conclude that John (is aware that he actually) knows a dancer!"}, {"title": null, "content": "Problem: Again, the LLM made the wrong inference here (wrong 'understanding'). While John\nknows the rules about interns, and while Mary is an intern, John does not know that she is (this was\nnever stated), and so he cannot infer anything about Mary. For the rule of interns to apply (in John's\nhead), he has to have access to the knowledge that Mary is an intern!"}, {"title": "COPREDICATION", "content": "Copredication is the phenomenon that two or more predicates (properties, or relations) are being\napplied to the same entity or reference thus making a single reference being used to refer \u2013 at once,\nto several entities of different types (see [4][6]). Recognizing all the different types of entities being\nimplied by a single reference requires deep understanding of the overall context. As we show here,\nLLMs also fail to recognize copredication, even in its simplest form."}, {"title": null, "content": "Problem: The LLM did not recognize the copredication of 'Barcelona' where the entity is being\nused in three senses at the same time: (i) as the visited city, (ii) as the football team that won over\nReal Madrid, and (iii) as the citizens of the city that were celebrating and that will be voting for\nindependence. The LLM wrongly inferred that the city (the geographic location) is what won over\nReal Madrid, and that the geographic location is what celebrated and will be voting, etc."}, {"title": null, "content": "Problem: Again, the LLM did not recognize the copredication in the text, namely that the physical\nnewspaper object (that was thrown away) is not who fired 'my favorite columnist': the firing was\ndone by the [editorial board/management] of the newspaper and not by physical newspaper.\nProblem: Again, the LLM did not recognize the copredication in the text \u2013 the LLM inferred that\nthe physical object that I bought (the actual book that I own), and not the content and ideas in all\ncopies of Das Kapital, is what influenced many revolutions, which is obviously wrong.\nProblem: Again, the LLM did not recognize the copredication in the text \u2013 the LLM inferred that\nthe country I was visiting (and partying in) and not its army is what started moving into Ukraine.\nIt should be noted here that we are aware of the fact that some readers might not (fully) appre-\nciate the point of highlighting these subtle errors in understanding. What we can briefly say here is\nthis: undoubtedly, we expect an intelligent NLU system that reads some text to convert the informal\ntext into some formal structure that can subsequently be queried. In the above example, the system\nmust therefore decide on the entity that 'Russia' refers to: is it a geographic location (that surely\ncannot move into another country) or does it also (implicitly) refer to some other entity that is related\nto Russia (such as the Russian army). Our aim therefore is not to dwell on these subtle failures in\nunderstanding of the text, but to highlight the fact that building an AI that fully understands natural\nlanguage text is not as simple as most superficial studies have, unfortunately, concluded.\nLet us now conclude testing LLMs understanding of copredication with one final example."}, {"title": null, "content": "Problem: Again, the LLM did not recognize the copredication in the text \u2013 the LLM inferred that\nthe dinner event (that everyone enjoyed) is what \"took forever to prepare\" while what took a long\ntime to prepare is the meal and not the (dinner) event. That is, it was the food of the dinner that took\na long time to prepare, and not the sitting during the dinner that everyone enjoyed4."}, {"title": "NOMINAL MODIFICATION", "content": "Another linguistic phenomenon that has been extensively studied is that of nominal modification\nwhere there is usually one or more adjectives modifying a head noun (that could also be preceded\nby modifying nouns). There are several issues that arise in constructs such as [A+ N+], some of\nwhich can be illustrated by the following examples:\n(1) a. Maria is an ancient philosophy teacher.\nb. Ron is an admired philosophy teacher.\n(2) a. Olga is a beautiful dancer."}, {"title": null, "content": "Problem: Since 'brilliant' syntactically modifies 'acquisition' the LLM wrongly inferred that 'bril-\nliant' is modifying the 'acquisition' action (or activity). Note that it correctly (although for the wrong\nreason) inferred that \u2018timely' modifies \u2018acquisition' it could not infer that something that could be\n'timely' could not be 'brilliant' an entity is either a person (that could be 'brilliant') or an activity,\nthat could be 'timely', but not both!\nAgain we should note that these seemingly simple mishaps in understanding the text are not\ntrivial since we are ultimately interested in building formal structures (e.g., knowledge graphs) from\nraw text in such a way that our subsequent queries and inferences would result in correct answers."}, {"title": null, "content": "Problem: Here's a simple explanation of why the LLM made the wrong conclusion as what the text\nmeans. If in some other snippet of text we stated that (the same) Sara is a teacher, then with the\nabove understanding we will easily (and wrongly) infer (by conjunction) that Sara is an experienced\nand talented teacher, which is a wrong inference. The point here is that the above text does not state\nthat Sara is experienced and talented (as a person), but only as a dancer."}, {"title": null, "content": "Problem: The LLM fails to infer that 'formal' modifies 'philosophy' and that 'Ron is a formal\nphilosophy teacher' should be understood as [Ron is a [formal philosophy] teacher] and that Ron is\nnot 'a formal' - not even a 'formal teacher'!"}, {"title": null, "content": "Problem: Again, the LLM made the wrong inference here since the text states that Ron is an expert\nin classical Italian music and not in classical music in general! Again, while this error might seem\ntrivial, when combined with other errors on other snippets of text, the combined result would be a\ncomplete misunderstanding of the larger piece of text."}, {"title": null, "content": "Problem: The LLM also failed our last example on nominal modification by suggesting that a gift,\nwhich is an inanimate object (a watch), is what is generous, although what the text implies is that 'it\nwas generous of John to give Mary the watch as a gift'!"}, {"title": "\u039c\u0395\u03a4\u039f\u039d\u03a5MY", "content": "Metonymy occurs when we use an entity el to refer indirectly to another entity e2 that stands in\nsome relation to el. In the course of 'understanding' we usually resort to our commonsense back-\nground knowledge to infer the implicit relation between el and e2 (see [16][17] and [18])"}, {"title": null, "content": "Problem: According to the LLM an object that I borrowed from John (namely the physical copy of\nthe New York Times that I was reading) is endorsing Joe Biden (what would happen if I destroyed\nthe physical copy I was reading? Will the endorsement disappear?) Clearly, the endorsement of Joe\nBiden was made by New York Times editorial board and not by the physical copy of the newspaper\nthat is in my hands."}, {"title": null, "content": "Problem: While there are several failures here (e.g., that 'omelet wants a beer' is interpreted as a\nrelation between a 'say' and a 'waiter', the main failure here is that 'loud omelet' is taken literally,\nwhile 'the loud omelet' is a reference here to 'the loud [person eating the] omelet'. These meto-\nnymic contexts are usually understood by resorting to commonsense knowledge ([14] gives a de-\ntailed explanation of how this happens using ontological types and type unification)."}, {"title": null, "content": "Problem: In addition to several problems in the interpretation, the LLM wrongly inferred that Aus-\ntralia the country (which is a geographic location) and not the Australian soccer team is the one that\nbeat the Canadian soccer team. Commonsense tell us, however, that what have here is metonymy\nwhere Australia is used to refer to another entity that Australia is related to, and in particular, the\nAustralian national soccer team."}, {"title": null, "content": "Problem: In addition to several problems in the interpretation, the LLM wrongly inferred that Aus-\ntralia the country (which is a geographic location) and not the Australian soccer team is the one that\nbeat the Canadian soccer team. Commonsense tell us, however, that what have here is metonymy\nwhere Australia is used to refer to another entity that Australia is related to, and in particular, the\nAustralian national soccer team.\nProblem: The final example involving metonymy shows also how LLMs fail to 'infer' the implicit\ncommonsense relationships that we implicitly assume in our ordinary spoken language. While the\nLLM inferred that 'the taxi I took' refers to a vehicle, it did not infer that referring to the taxi as\nbeing friendly is a reference to the driver of the taxi and not the vehicle. Note that these are examples\nnot exotic or farfetched \u2013 this in fact is how we mostly speak, and thus an AI that truly understands\nnatural language must make these inferences."}, {"title": "REFERENCE RESOLUTION AND COMMONSENSE", "content": "Reference resolution is perhaps the most known phenomenon in natural language semantics. A ref-\nerence is usually a pronoun (or relative pronoun) that refers to some entity that is mentioned else-\nwhere in the wider context. Note that the general utility of \u2018referring' is called 'anaphora' of which\npronouns are the most commonly used tool. (see [19][20][21] and [22])."}, {"title": null, "content": "Problem: While, in theory, \u2018she' in the above could refer to either Mary or Sara, commonsense tells\nus that the ones that always goes to the library would know when the library closes more than others.\nMoreover, why would Mary ask Sara when the library closes if it was Mary that always geos there?\nProblem: While the syntactic structure might favor 'he' referring to the teenager, commonsense\nsays that the one trying to escape further injuries is the one that was shot, namely the policeman.\nSince LLMs learned statistical correlations, it could also be the that the wrong inference was made\nhere since the correlation between the one shooting and the one escaping should be high.\nProblem: Again, while the syntactic structure permits both possible resolutions of 'he', com-\nmonsense should favor 'he' referring to Dr. Smith since John, who wrote the thesis has surely read\nhis own writing (i.e., has read his own thesis), while it could be that Dr. Smith did not read it yet!"}]}