{"title": "Exploring Incremental Unlearning: Techniques, Challenges, and Future Directions", "authors": ["Sadia Qureshi", "Thanveer Shaik", "Xiaohui Tao", "Haoran Xie", "Lin Li", "Jianming Yong", "Xiaohua Jia"], "abstract": "The growing demand for data privacy in Machine Learning (ML) applications has seen Machine Unlearning (MU) emerge as a critical area of research. As the 'right to be forgotten' becomes regulated globally, it is increasingly important to develop mechanisms that delete user data from Al systems while maintaining performance and scalability of these systems. Incremental Unlearning (IU) is a promising MU solution to address the challenges of efficiently removing specific data from ML models without the need for expensive and time-consuming full retraining. This paper presents the various techniques and approaches to IU. It explores the challenges faced in designing and implementing IU mechanisms. Datasets and metrics for evaluating the performance of unlearning techniques are discussed as well. Finally, potential solutions to the IU challenges alongside future research directions are offered. This survey provides valuable insights for researchers and practitioners seeking to understand the current landscape of IU and its potential for enhancing privacy-preserving intelligent systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Unlearning (MU) is an evolving concept that aims to address several critical challenges in the domain of Machine Learning (ML) and Artificial Intelligence (AI), particularly with respect to privacy, ethics, and regulatory compliance [1]. It focuses on the need to remove or \"forget\" specific data points that may have been used in training a model, particularly when that data is sensitive or no longer required [2], [3]. The use of personal or sensitive data to train ML models raises significant privacy concerns, especially in light of rigorous regulations such as the General Data Protection Regulation (GDPR) [4] and California Consumer Privacy Act (CCPA) [5]. These regulations often require that individuals have the right to be \"forgotten\", meaning that their personal data must be removed from systems upon request [6].\nMU offers a solution through the removal of specific data from both the storage archives and the trained models them-selves [7]. This is crucial because deleting data from storage does not automatically ensure that it is no longer present in a trained model. MU techniques attempt to unlearn the influence of this data from the model's parameters, ensuring that both privacy of the model is maintained and that the model complies with data privacy regulations [8].\nAs privacy concerns increase and new regulations are in-troduced, it is important for ML models to adapt certain changes in the environment. As new data is input, older or less relevant data can be unlearned to keep models up-to-date and relevant without having to retrain them from scratch [9]. For example, a model trained on certain datasets might eventually become non-compliant with new data privacy laws or standards. To resolve these issues, we introduce Incremental Unlearning (IU), which enhances the flexibility of models by allowing dynamic changes to the dataset. IU is a branch of machine unlearning where models can progressively forget data or knowledge without retraining from scratch [10]. This capability is especially useful where data privacy, regulatory compliance, and knowledge currency are crucial. This capa-bility could future-proof AI systems against the rapid pace of regulatory changes [11].\nThis survey categorizes IU techniques according to the research challenges and goals within the unlearning process. In doing so, we review the differences, relationships and the respective advantages and disadvantages of these tech-niques. We begin by summarizing IU into four main areas: Reinforcement Unlearning, Continual Learning via Selective Forgetting, Corrective Unlearning, and Federated Unlearning. Since many studies focus on Reinforcement Unlearning, we allot two sections to introduce the corresponding studies in this area. We classify these unlearning methods into: decremental RL method, environment poisoning method, and quantized reward konditioning (Quark) theory. Next, we discuss contin-ual learning via selective forgetting through selective pruning, decremental matching moment using Gaussian distribution, and selective amnesia. Furthermore, we discuss corrective unlearning though selective synaptic dampening, cognac the-ory and corrective unranking distillation(CuRD) algorithm. In addition, we introduce the federated unlearning method, which is gaining significant importance in unlearning research. Federated unlearning can be further classified into FedEraser, FedAU and quantized federated learning (Q-FL).\nFollowing the application of an IU technique, it is criti-cal to verify the unlearning effect. Verification has attracted significant research attention, so we perform a timely review on related research on IU verification. Finally, we consider the essential privacy and security issues in IU by reviewing related"}, {"title": "II. OVERVIEW", "content": "Incremental Learning, also known as continual learning, refers to the process in which a ML model is trained on data sequentially with one batch of data or instance at a time rather than the entire dataset at once [13]. This approach is particularly useful in situations where the data is large, or when data is continually generated and must be processed in real time [14]. Incremental Learning allows a model to learn continuously over time, thus making it more applicable in dynamic environments where the data distribution may change over time such as real-time applications or adaptive systems [15]. Mathematically, Incremental Learning can be defined as follows:\nDefinition 1. Let \\(D = \\{(x_i, y_i)\\}_{i=1}^N\\) represent a dataset of N labeled examples, where \\(x_i\\) is the input and \\(y_i\\) is the corresponding label.\nInitial Training: A model \\(M_t\\) at time t is trained using an initial dataset \\(D_{init} \\subset D\\). The objective function is:\n\\[\theta_t = \\arg \\min_{\\theta} L(\\theta, D_{init}),\\]\nwhere \\(L\\) is the loss function and \\(\\theta\\) represents the model parameters.\nIncremental Update: When new data \\(D_{new} \\subset D\\) arrives, the model updates its parameters \\(\\theta_{t+1}\\) while retaining knowledge from \\(D_{init}\\):\n\\[\\theta_{t+1} = \\arg \\min (L(\\theta, D_{new}) + \\lambda \\cdot L_{reg}(\\theta, D_{init})),\\]\nwhere \\(L_{reg}\\) is a regularization term to prevent forgetting, and \\(\\lambda\\) controls the trade-off between learning new knowledge and retaining old knowledge.\nIncremental Learning is a powerful approach in scenarios where data arrives continuously and models must adapt over time, as continuous learning elicits memory and computational constraints [16]. Due to its adaptive nature, the approach enables real-time model updates and better supports dynamic data environments. The principle of incrementally learning means that the model doesn't need to forget old information when it learns new information. Instead, knowledge is accu-mulated as the model is exposed to new data over time without catastrophic forgetting [17].\nClass Incremental Learning [18], [19] methods use knowl-edge distillation [20] and exemplar-based approaches [21] to help models retain knowledge from previous classes while learning new classes as well. These methods use strategies to address the issue of catastrophic forgetting that ensure previously learned information is not lost when new data is introduced. Knowledge distillation helps secure previous knowledge by training new models to present the behaviour of previous models [22]. Exemplar-based methods involve retain-ing a small representative subset of previous data, which helps the model maintain performance on previous classes [23]. By combining these techniques, Incremental Learning can become more effective in real-world applications by enabling models to learn continuously without forgetting important prior knowl-edge. In a typical learning model, once a model is trained, it becomes difficult to remove specific data points because most models learn patterns across the entire dataset. The solution is Incremental Unlearning (IU), which can efficiently unlearn the influence of specific data points while minimizing the impact on the rest of the model's knowledge. IU involves integrating processes that allow a model to both acquire new knowledge and to discard outdated or irrelevant information.\nThe need for IU arises from privacy concerns resulting from regulations like GDPR and CCPA, in which users have the"}, {"title": "III. TECHNIQUES AND APPROACHES", "content": "In this section we cover several techniques and approaches adopted by different researchers on MU. We will further discuss how a model incrementally makes runtime decisions in performing certain tasks while preserving its privacy. These techniques are summarized in figure 2.\nReinforcement Unlearning (RU) focuses on selectively re-moving specific learned knowledge from an agent's mem-ory, ensuring that sensitive or outdated information is not retained. In Reinforcement Learning [27], an agent learns to make optimal decisions within an environment to maximize cumulative rewards [28]. Integrating RU can ensure that the agent forgets definite information, which can be particularly useful in situations where there are privacy concerns or when the learned information becomes irrelevant.\nHowever, there are several problems associated with RU [29]. A significant difficulty lies in linking an environment [30] that is required to be unlearned to its corresponding experience data. This is due to the environment owner's lack of access to the agent-managed experience data [31]. Another issue is that excessive unlearning can lead to a decline in the agent's overall performance. Moreover, it is challenging to find a way to evaluate RU effectively. Since the environment owner cannot specify what samples should be unlearned, it makes traditional evaluation methods like membership inference in-effective [32] [33]. In addition, other challenges associated with Reinforcement Learning [34]\u2013[36] may also be inherently faced when using RU like adaptability and stability in dynamic environment.\nRU can be approached in a number of ways. Decremental Reinforcement Learning (DRL) is one such approach that focuses on adapting and updating the learning process by gradually diminishing the influence of older experiences while emphasizing more recent and relevant ones [37]. The DRL method works in two steps. In the first step, the agent explores its unlearning environment by collecting different samples using a random policy. The second step involves fine-tuning the samples collected by the user. This approach helps the agent build a comprehensive understanding of the environment, which is crucial for effective unlearning [38]. By exploring randomly, an agent can identify what actions and states need to be unlearned or have their influence reduced. Decremental methods can also effectively adjust to changes by learning to prioritize new information that reflects the current state of the environment [39]. This makes them partic-ularly useful in scenarios that are subject to frequent changes, such as adaptive control systems and real-time decision-making [40]. Overall, this ability to focus on recent data can lead to more effective learning and better performance in ever-evolving contexts. The incremental nature of DRL ensures that the forgetting process is controlled and does not adversely affect the agent's performance in other environments. This is useful for maintaining the overall functionality of the model while ensuring privacy.\nAnother approach to RU is Environment Poisoning in which the unlearning environment itself is modified [41]. This"}, {"title": "A. Reinforcement Unlearning", "content": "Reinforcement Unlearning"}, {"title": "B. Continual Learning via Selective forgetting", "content": "Continual Learning focuses on adapting to new knowledge while maintaining previous information [43]. This can be done more accurately using Selective Forgetting. Selective Forget-ting allows the system to unlearn specific tasks requested by a user without affecting the overall model performance [44]. This saves time and space after every upgrade of a model. The major objective is to continue learning using Selective Forgetting without using the original data from previous tasks [45]. Selective Forgetting is indeed a significant challenge in LLMs due to their complexity and vast knowledge [46].\nIntroducing Selective Pruning for LLMs is a promising approach for resolving this challenge [24], [47]. This method selectively removes abilities from trained LLMs [48] by per-forming structured pruning. The approach involves iteratively pruning nodes in either the feed-forward layers or attention head layers of the Transformer architecture. A scoring function evaluates the relative importance of nodes with respect to two datasets; Forget Dataset (D-forget) and Retain Dataset (D-retain). D-forget is the dataset from which we want the model to forget capabilities [49]. D-retain is the dataset that the model should continue to perform well on. The major goal is to reduce the performance on D-forget while maintaining the performance on D-retain. This is a crucial step in the selective pruning method as it saves time while preserving the overall performance and structure of the model. It is a particularly valuable approach in real-world applications where models must be quickly adapted to new requirements without extensive retraining [50]. By implementing the ability to forget selective sensitive information, organizations can build trust with users through assuring them that their private data is handled carefully.\nThe second method for unlearning is Decreasingly Matching the Moment (DMM), which treats the model parameters as a Gaussian distribution [51]. DMM works by adjusting mean and variance of certain data points. This ensures that the impact of unlearning data points is minimized. The updates are calculated to match the moments (mean and variance) of the distribution before and after unlearning. All the steps in this process achieve unlearning by incrementally adjusting the model parameters [52]. By using a probabilistic approach, DMM can also incorporate privacy-preserving techniques, such as adding noise to the parameter updates, to ensure that the unlearning process does not leak sensitive information. This method is efficient because it avoids the need to retrain the entire model from scratch. Instead, it incrementally updates the parameters, making it faster and more practical for large-scale models [53]. This method is particularly useful in sce-narios where frequent updates or removal of data are required, and helps maintain the model's performance while ensuring data privacy [54].\nSelective Amnesia is also a contributory technique in Con-tinual Learning, which helps the model to retain essential knowledge while forgetting less relevant or less important information. This technique maintains a balance between stability (preserving old knowledge) and adaptability (learning new knowledge) of the model [53]. The model may forget specific information of previous tasks that are no longer necessary for the performance of current tasks. This allows the model to avoid overfitting and ensures that it does not waste resources in retaining irrelevant information. Selective Amnesia involves dynamically identifying what parts of the network or memory are not important for the new task and thus can be forgotten without negatively impacting on the model's performance. Selective Amnesia is a way to design systems that can learn continuously and adapt to new tasks while avoiding the harmful effects of forgetting valuable information"}, {"title": "C. Corrective Unlearning", "content": "Corrective Unlearning is a method designed to remove the influence of manipulated data from a trained model [55]. This approach is particularly useful when only a subset of the corrupted data can be identified and retrained. Applying this approach will increase the productivity of large-scale datasets [56].\nCorrective Unlearning introduces Selective Synaptic Damp-ening (SSD) [57], which can feasibly erase the influence of manipulated data when only a small representative of subset of data is identified. SSD is a method that relies on understanding how manipulated data influences the model's learned weights. This approach uses Fisher Information Matrix to assess what weights are most affected by the data [58]. By dampening the influence of specific manipulated data points, SSD can enhance the robustness of models against attacks that exploit those weaknesses. This is an adaptive approach that allows for dynamic alterations in the model's learning process, making it more flexible for handling new information while managing the effects of older data [59]. The ability to effectively remove manipulated data has powerful implications for privacy, especially in contexts where models may have accidentally learned from biased or harmful data.\nThe second method that is gaining attention for its efficiency in corrective unlearning is Cognac. Cognac is a graph unlearn-ing method that focuses on efficiently removing the influence of specific data points, known as the manipulation set, from Graph-based Neural Networks (GNN) [60]. Cognac works by identifying a small portion (e.g. 5%) of a manipulation set that includes datasets that need to be unlearned. It visualizes the graph structure and investigates connections and relationships within the model. Instead of retraining a model from scratch, Cognac intelligently adjusts the influence of a manipulation set. This is done through targeted updates that minimize the impact of the identified data points on the model's performance [61].\nThe potential applications of Cognac are broad, including enhancement of data privacy, improved model robustness, and ensuring compliance with data regulations like GDPR [62]. It is particularly useful in scenarios where data points need to be removed due to errors, biases, or privacy concerns.\nThe third technique is Corrective UnRanking Distillation (CuRD) [63]. CuRD uses a novel teacher-student framework specifically designed to address corrective unranking in neural Information Retrieval (IR). CuRD enables a neural IR model to selectively forget specific samples (such as documents or relevant information) while ensuring that the ranking output for other samples remains unchanged. Its primary objective is to adjust the relevance scores of the documents to be forgotten so that their relevance becomes indistinguishable from low-ranking or non-retrievable documents, effectively rendering the forgotten documents invisible or irrelevant to the retrieval system.\nFurthermore, CuRD ensures that the model continues to perform well on the samples not targeted for forgetting, preserving the overall integrity of the retrieval system. Instead of simply removing forgotten documents, substitute documents are introduced to take their place. These substitutes are de-signed to have relevance scores that closely match those of the original (forgotten) documents. This substitution process is crucial, as removing a document without replacing it could disrupt the ranking system's integrity. The relevance scores of these substitute documents are fine-tuned during training to align with the relevance scores of the to-be-forgotten documents, ensuring that the model maintains a consistent and coherent ranking post-forgetting. This framework can be applied to various retrieval systems based on neural networks, such as search engines or recommendation systems, enabling compliance with unlearning requirements (e.g. privacy con-cerns) without compromising overall system performance."}, {"title": "D. Federated Unlearning", "content": "Federated Learning (FL) is a decentralized approach to ML where models are trained across multiple devices (or clients) without sharing their private data [64], [65]. In FL, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the overall learning goal [66]. FL [67]appears to be a promising ML technique for keeping local data private [68]. Federated Unlearning (FU) is an essential development in FL that addresses privacy and data management challenges [69]. The primary objective of FU is to delete specific data requested by a client from a model that was trained using FL techniques [70].\nTo unlearn in FL is difficult because it requires retraining the model without a specific client's data, or clients that need to be forgotten [71], [72]. This can be computationally ex-pensive and time-consuming [73]. Thus, FedEraser addresses this problem by utilizing the historical updates of model parameters from the federated clients [74]. FedEraser stores the parameter updates (gradients or model changes) sent by each federated client during the training process [75]. In this method, the central server does not require all raw data to be stored, but only gets updates that are generated by clients during training of a model [76]. FedEraser does not use any information of target clients, thus preserving privacy issues in unlearning [77]. FedEraser is designed to eliminate clients data on a global learning model while minimizing the time to construct an unlearning model. This technique is helpful in circumstances where unlearning is frequent or there is poisonous data.\nAnother comprehensive unlearning technique in FL is Fed-erated Adaptive Unlearning (FedAU) [78]. FedAU works by integrating a lightweight auxiliary unlearning module into the model. This module helps to efficiently remove the influence of specific data points or clients from the model, and facilitates unlearning without the need for extensive retraining [79]. FedAU helps multiple clients implement unlearning concur-rently. It is an effective model accuracy technique that shows strong performance in unlearning.\nFedAU shows promising achievements in FL, particularly in privacy preservation and model integrity [80]. It works on a selective adaptation that ensures models can dynamically adapt to unlearning requests without the computational cost"}, {"title": "IV. ADVANCED METRICS FOR EVALUATING UNLEARNING AND DATASETS", "content": "There are several metrics used to ensure that the unlearning process is effective and efficient, thus maintaining the model's integrity and performance. In our survey, we suggest the following metrics to verify unlearning.\n1) Accuracy\nThe unlearning process should remove all traces of irrel-evant data, ensuring that the model no longer memorizes or uses this data when making predictions [93]. In IU, a model's accuracy can be compared against three different datasets: 1) Forget set: ensures that the model has forgotten the specific data that is no longer needed. 2) Retained Set: ensures that the model has retained all the useful information after unlearning has done. 3) Test Set: ensures that the unlearning process does not negatively affect the model's generalization [94].\n2) Running Time\nThe timeliness of unlearning has a significant advantage over retraining, especially when there is a need to quickly restore privacy and security in a model. Unlearning enables faster model updates because it requires fewer computations by targeting only the forgotten data or a limited set of model parameters, compared to retraining using the full dataset [95] [89]. The speed-up factor of unlearning over retraining can be substantial, particularly when the unlearning operation focuses only on small, targeted updates to the model rather than needing to process all the data from scratch.\n3) Theory-Based Verification\nTheory-based verification in IU is crucial in ensuring that the unlearning process maintains model integrity and preserves model performance and privacy. By applying formal methods like differential privacy, influence functions and model check-ing, the unlearning process can be verified [96] [92]. The major objective of IU is to ensure that a model no longer uses the data points that were not learned. Theory-based verification in this context should ensure that once data points are removed they can no longer influence the model's predictions.\n4) Attack-Based Verification\nThis ensures that the model cannot leak any information about the unlearned data. If the model remains robust against these attacks, it means that unlearning was successful in erasing the influence of the data while preventing leakage of sensitive information. Attack-based verification of IU is essential to ensuring that the unlearning process is resilient against adversarial attacks [15] [92]. By simulating various types of attacks, such as data reconstruction, model inversion, and adversarial input manipulation, attack-based verification"}, {"title": "A. Evaluation Metrics", "content": "There are several metrics used to ensure that the unlearning process is effective and efficient, thus maintaining the model's integrity and performance. In our survey, we suggest the following metrics to verify unlearning."}, {"title": "V. CHALLENGES OF INCREMENTAL UNLEARNING", "content": "ML models can accidentally memorize sensitive, unautho-rized, or malicious data during training, which creates potential risks in terms of privacy, security vulnerabilities, and perfor-mance degradation [91], [96]. These issues are particularly observed in scenarios where data privacy is a concern, such as healthcare, finance, and personalized services [110], [111]. Here, we highlight some of the main challenges of IU.\n1) Adaptive Training\nWhen we want to remove a specific data sample, the challenge is to erase its influence on the model without affecting the model's behavior on other data points [112]. This is not an easy task because once a sample has contributed to the model's parameters, removing that sample requires altering the entire learning path [113]. In many MU models, especially DNNs, the relationship between a data sample and the model's parameters is non-linear. This means that the effect of removing a single data point is not just a simple reverse of an update, but a complex adjustment to the model's state [114].\nWhen we remove certain data points, the distribution of the remaining data may change. The model must adapt to these changes, which can modify the relationships between data features and predictions. This can lead to the challenge of data shift where the model must adjust to a new data distribution without retraining entirely. Different strategies for unlearning have varying effects on how well the model adapts. Some strategies may cause the model to lose important patterns that are not linked to the removed data, which impacts the model's overall adaptive capacity [115]."}, {"title": "VI. POTENTIAL SOLUTIONS AND FUTURE DIRECTIONS", "content": "In this section we present the potential solutions and the future directions in the domain of IU.\nIU presents several challenges, which can be addressed to varying degrees by a variety of solutions. Various innovations can address the challenges of IU while ensuring that models remain adaptable, efficient, and compliant with data regulatory requirements. In this section, we analyze the current and potential developments in IU.\nAn efficient strategy for IU is using a fine-tuning tech-nique rather retraining the entire model [142]\u2013[144]. By adjusting the model's parameters slightly, IU can remove specific data points without affecting the overall performance of the model [145]. A fine-tuned model demonstrates higher performance on all tasks along with enhanced generalization capabilities [146].\nThe approximation technique in machine unlearning re-volves around finding efficient and accurate ways to re-move specific knowledge from a model without requiring complete retraining. Using approximation techniques like stochastic gradient descent (SGD) [147] or gradient-based methods can make unlearning more efficient, as opposed to retraining the entire model from scratch [148]. By approximating the effect of data removal via gradients, the model can adjust its parameters slightly to forget the influence of specific data points without full retraining. These methods give priority to small and incremental updates to the model weights rather than a complete retraining process [85], [149].\nInfluence Functions can help determine the influence of a specific data point on the model's predictions. By evaluating the impact of a data point, we can calculate its influence on the model [108], [150]. This approach has been proposed as a way to remove data points while maintaining overall model performance [85].\nFor large-scale datasets, IU can be scaled using parallel or distributed unlearning techniques [151]. By distribut-ing the unlearning task across multiple nodes, the time required to forget certain data points or concepts can be significantly reduced [152].\nThis is a technique where a large, complex model is com-pressed into a simpler, more interpretable model [153]. After unlearning, the knowledge distillation process can help preserve important model behavior and make the model easier to interpret post-unlearning [154], [155].\nThis can be a valuable strategy in the context of IU, particularly in improving efficiency and removing specific data points [156]. If the data is well-partitioned, it may be possible to break the model's reliance on specific partitions. This modular approach makes it easier to re-move certain data points or even entire partitions without affecting the rest of the model [157]."}, {"title": "VII. CONCLUSION", "content": "This survey of research literature has taken significant steps to address the challenges in IU, particularly around the efficient and effective removal of specific data points from ML models without retraining from scratch. This area of research is motivated by the increasing importance of data privacy regulatory compliance (e.g. GDPR) and the need for models to adapt to changing environments or outdated data. IU techniques focus on minimizing computational overhead while ensuring that the model continues both to perform well and recognize privacy constraints.\nOne of the key contributions of IU research is the devel-opment of methods that allow models to unlearn data with-out compromising performance. Reviewed techniques such as decrement RL method, environment poisoning method, Quark, selective pruning method, selective amnesia, cognac, CuRD, FedAU, Q-FL and FedEraser have shown significant results in removing the influence of selected data points while retaining the model's integrity. These techniques are useful for LLMS and for dynamic environments.\nIn conclusion, IU research is progressing rapidly, with practical applications in privacy-sensitive domains, as well as dynamic ML and unlearning environments. Although there are certain challenges like privacy, model efficiency, and robustness issues in unlearning, current advancements continue to make IU a powerful tool for responsible and adaptive AI systems."}]}