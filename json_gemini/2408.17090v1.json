{"title": "FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition", "authors": ["Chen Hu", "Jingjing Deng", "Xianghua Xie", "Xiaoke Ma"], "abstract": "Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Specifically, heterogeneous data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We introduce a novel approach, FissionVAE, which decomposes the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we investigate the incorporation of hierarchical VAE architectures and demonstrate the use of heterogeneous decoder architectures within our model. We also explore strategies for setting the latent prior distributions to enhance the decomposition process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images of Earth. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models.", "sections": [{"title": "1 Introduction", "content": "Generative models have attracted increasing attention in recent years due to their impressive ability to generate new data across various modalities, including images, texts, and audios [12, 28, 2]. As these models, like other deep learning systems, require substantial amounts of data, concerns regarding data privacy have elevated among regulatory authorities and the public. Unlike the traditional centralized learning paradigm, which collects all data on a single computer system for training, federated learning allows private data to remain on the owner's device. In this paradigm, local devices train models independently, and a central server aggregates these models without accessing the individual data directly. Although this distributed approach enhances privacy protection, it also introduces unique challenges not encountered in centralized systems. Since data remains distributed across various client devices, the training samples are not guaranteed to be identically distributed. This can lead to inconsistencies in learning objectives among clients, resulting in degraded performance when these models are aggregated on the server.\nIn the context of FL with non-IID data, generative models such as Generative Adversarial Networks (GANs) [6] and Variational Autoencoders (VAEs) [16] face additional challenges. These models involve sampling from a latent distribution, and the generator or decoder trained on client devices may develop differing interpretations of the same latent space. This discrepancy can lead to difficulties in maintaining a consistent and unified latent space, resulting in overlapping representations across models, shown in Figure 2. A further challenge arises from the role of the generator or decoder, which are tasked with mapping latent inputs to the sample space by synthesizing the shape, texture, and colors of images. Aggregating generative models trained on non-IID image data can produce artifacts that appear as a blend of disparate image types, because generators trained on non-IID local data capture the characteristics of varied visual features (shown in Figure 4). Specifically for GANs, another problem arises from local discriminators, which may provide conflicting feedback that hinders model convergence. With the limited data available in FL settings, discriminators can quickly overfit to the training samples [14]. If an updated generator from the server produces images of classes not present in a client's local dataset, the local discriminator might incorrectly label well-generated images as fake, simply"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.0.1 Federated Averaging", "content": "Federated Averaging (FedAvg) [19] is the most commonly used algorithm for federated learning tasks. In FedAvg, the central server first distribute the initialized model to clients, ensuring that all clients starts local training with the identical model parameters. Clients train their models with local data for specified local epochs, then return their models to the central server. The central server aggregates collected client models by taking the weighted average of these models:\n$\\W_r = \\frac{\\sum_{i=1}^{S} N_i W_i}{N_s}$                                                                                                                                                                                                                                                                                                                                        (1)\nwhere S is the number of clients that participate current training round, $n_i$ is the number of training samples on client i while $w_i$ is the model parameters for this client, $w_r$ is the parameters of the global model, and $N_s$ is the total number of training samples across clients in current round. Once aggregated, the central server distribute the updated model to clients in the next training round. This process is repeated until the global model converges."}, {"title": "2.0.2 Variational Autoencoders", "content": "VAEs are a class of generative models designed to approximate the probability p(x) of the observed data x, which is typically intractable. Central to the optimization of VAEs is the concept of evidence lower bound (ELBO), which provides a tractable objective by bounding the logarithm of p(x) through Jensen's inequality. The ELBO is defined as follows:\n$\\log p(x) \\geq \\log \\int \\frac{p(x,z) q_\\phi(z|x)}{q_\\phi(z|x)} dz = E_{q_\\phi(z|x)} \\log \\frac{p(x, z)}{q_\\phi(z|x)} = ELBO$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (2)\nwhere q(.) is the encoder parameterized by $x$, and z denotes the latent code derived from the input data x. The ELBO can be further decomposed into the following terms by splitting the logarithm,\n$ELBO = E_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (3)\nHere, $p_\\theta(.)$ is the decoder parameterized by $\\theta$. The first term of the ELBO represents the expected log-likelihood of the data reconstructed from the latent code z. The second term is the Kullback-Leibler divergence between the encoded distribution q(z|x) and a predefined prior distribution p(z).\nDuring training, the encoder $q_\\phi$ estimates the latent code z based on the input data x, aiming to align this estimate with the prior distribution p(z). The decoder $p_\\theta$ then attempts reconstruct the original input x from the estimated z. The decomposition above indicates that training objective for VAEs is optimizing both the quality of data reconstruction and adherence to the latent space structure. In the generation phase, new data instances are produced by sampling from the prior distribution p(z), which are then transformed solely by the decoder into the data space."}, {"title": "2.0.3 Variational Autoencoders in Federated Learning", "content": "Following the discussion on the ELBO, we note that in a centralized setting, we maximize the expectation of the reconstruction likelihood across all data, as presented in Equation 3. In contrast, federated learning distributes the training samples x are split into subsets {x1Ux2 U... UXn} across n clients. A common assumption in federated learning is that client data are not identically distributed, yet they are independent. Furthermore, we assume that client data are conditionally independent given the latent codes z, such that p(xi|z, xj) = p(xi|z), i \u2260 j. This leads to the following factorization,\n$P(x_1,..., x_n|z) = p(x_1|z)p(x_2|x_1, z)p(X_3, ..., x_n|z)$\n$= \\Pi_{i=1}^n P(x_i|z)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (4)\nTherefore, the reconstruction likelihood in the ELBO under the federated assumption can be expressed as:\n$\\log p_\\theta(x|z) = \\sum_{i=1}^n \\log p_\\theta(x_i|z)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (5)\nEquation 5 suggests that when training VAEs within the federated learning framework, we can independently maximize the ELBO on each client. Local VAEs are then aggregated through algorithms such as FedAvg to maximize the global ELBO.\nRegarding existing research on federated VAEs for image generation, the work in [5] shows that VAEs can generate handwritten digit images within an FL framework and [20] focuses on incorporating differential privacy into the training process of federated VAEs for image generation. In [3], the authors integrate a federated VAE into the main classification task to help clients balance their local data. This involves training a global VAE where the encoder finds the average latent representation of local classes, these representations are then sent to the central server. When needed, the global decoder and collected latent representations are distirbuted to clients for image generation. The work in [9] aims at one-shot federated learning for classification tasks, where the server distributes the initialized model and only collects the converged local models without further updates. Here, the VAE is used as a data generator to create a balanced training set through knowledge distillation from local decoders. As discussed in Section I, while effective for classification tasks where balancing the training dataset is paramount, the lack of real data exposure and a cohesive latent space constructed by an encoder challenges the utility of this approach in producing high-quality images."}, {"title": "3 Investigating Strategies for Non-IID Image Generation with VAEs", "content": "In this section, we describe our methodology for exploring VAE configurations tailored for generating images under non-IID conditions in a federated learning framework. We specifically address scenarios where clients are categorized based on distinct data distributions. For illustrative purposes, we consider a case where some clients exclusively possess hand-written digits from the MNIST dataset, while others maintain only clothing images from the FashionMNIST dataset. We follow to the standard federated learning framework, wherein a central server is tasked with aggregating updates from the clients and subsequently distributing the updated model back to them. Each client retains a subset of data representative of its respective group and conducts local training independently. A more complex scenario with RGB images and a larger number of client groups is explored and discussed in the experimental section (Section IV). We summarize the explored strategies in Table 1."}, {"title": "3.1 FedVAE", "content": "A straightforward strategy for implementing VAEs in federated learning is using a unified encoder-decoder architecture. In this configuration, all clients share a common latent space (often predefined as the normal distribution N(0,1)) and the central server indiscriminately aggregates client models at the end of each training round. Fig. 1 illustrates this training scheme.\nDespite the simplicity of this strategy, it present significant challenges in the non-IID scenario. Specifically, employing a single prior distribution for the latent space does not account for the distinct data distributions across different clients. Encoders from different client groups may map their uniquely distributed data into the same region of the latent"}, {"title": "3.2 FissionVAE with Latent Space Decoupling", "content": "To address the conflicting latent space issue identified above, we propose decomposing the latent space according to different data groups, while maintaining a unified architecture for the encoder and decoder. This approach corresponds to Fission VAE+L in Table 1 and its architecture is shown in Fig. 3.\nSpecifically, the encoder maps the input data to different distributions based on the client's group. For instance, MNIST client may map to N(-1,1) and FashionMNIST clients to N(-1,1). The KL divergence in the ELBO for this model is given by:\n$D_{KL}(N(\\mu_q, \\sigma_q||N(\\pm 1, 1)) = \\frac{1}{2} \\sum_{i=1}^k [\\sigma_q^2 + \\mu_q^2 \\mp 2\\mu_q - \\log \\sigma_q^2]$                                                                                                                                                                                                                                                                                                      (6)\nHere, $\\mu_q$ and $\\sigma_q$ represent the encoder's estimates for the parameters of the latent code's distribution, and k is the dimension of the latent code.\nFigure 4 shows reconstruction and generation samples produced after training the FissionVAE with latent space decomposition on the Mixed MNIST dataset. While the quality of reconstructed images are greatly improved compared to the baseline FedVAE, the generated images still exhibit a mixture of handwritten digits and clothing items, even when explicitly sampling from their respective latent distributions. This suggests that while decomposing latent encoding helps improving reconstructions, the unified decoder still blends features due to the aggregation of model weights from diverse visual domains. This observation motivates the architecture described in the next section, where the decoder is also split based on client groups."}, {"title": "3.3 FissionVAE with Group-specific Decoder Branches", "content": "Non-Hierarchical FissionVAE Building on the concept introduced by Fission VAE with latent space decomposition, we further refines non-IID data generation by incorporating decoder branches specific to each data group while maintaining a unified encoder. This design allows the central server to aggregate the encoder updates agnostically of the client groups, whereas decoder branches are aggregated specifically according to their corresponding groups. In addition, this approach also offers flexibility in the choice of the prior latent distribution p(z), which can be identical across groups or defined uniquely for each group to exert more explicit control over the data generation through the decoder. In scenarios where p(z) are identical for all groups, although the encoder projects different samples into the same latent space, group-specific decoders ensure that the latent space is interpreted independently. Nevertheless, we find that the uniquely defined priors result in better generation quality as the number of client groups increases. Figure 5 illustrates this branching architecture which corresponds to FissionVAE+L+D in Table 1.\nFigure 6 shows randomly selected reconstruction and generation samples produced after training the Fission VAE with decoder branches on the combined dataset of MNIST and FashionMNIST. The results indicate a significant reduction in the blending feature issue in previously discussed VAE architectures.\nHierachical FissionVAE Next, we show that the branching architecture can be enhanced by integrating hierarchical inference [15, 27] to the federated learning framework, which enables the use of deeper network structures to capture more complex data distributions. Fig 7 depicts the FissionVAE with two levels of hierarchical inference, which is the FissionVAE+L+D+H configuration in Table 1. In this architecture, the first encoder module estimates q(z1|x) from the input data, then the second encoder module estimates q(z2|z1) based on the first level latent code. The decoder reverses the encoding process, which estimates p(z1|z2) based on z2 to reconstruct z1, and subsequently reconstructs the original input x by estimating p(x|z1).\nFollowing the convention in hierarchical VAEs, we assume conditional independence among the latent codes, where the encoding and decoding processes can be defined as follows:\n$q(z_1, z_2|x) = q_\\phi(z_1|x) q(Z_2/Z_1)$\n$p(x, z_1, z_2) = p(Z_2) p_\\theta(x|z_1) p_\\theta(z_1|z_2)$\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (7)\n(8)\nThe ELBO for this hierarchical VAE is expressed as,\n$ELBO_H = E_{q_\\phi(z_1|x)} [\\log p_\\theta (x|z_1)]$\n$\\qquad - E_{q_\\phi(z_1|x)} [D_{KL}(q_\\phi(z_2|z_1)||p(z_2))]$\n$\\qquad - E_{q_\\phi(z_2|z_1)} [D_{KL}(q_\\phi(z_1|x)||p_\\theta(z_1|z_2)]$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (9)\nIn the equation above, the first term is the reconstruction term as it is the expectation of the log-likelihood for the input samples under the distribution estimated from the encoded z1, the second term is the prior matching term which is enforcing the encoded z2 to conform the prior distribution z2 ~ N(0, 1), and the last term is the consistency term which requires z1 from either the encoder or the decoder to be consistence. In practice, we find that adding the reconstruction loss from z2 to x is also crucial for generating meaningful samples. Optionally, perceptual losses such as the VGG loss [18] or the structural similarity index measure (SSIM) [30] loss can be used to promote the fidelity of reconstructed images. However, no significant improvement is observed in our experiments. Therefore no perceptual loss is included in our implementation. The final loss function for the hierarchical and branching FissionVAE then becomes,\n$\\mathcal{L} = E_{q_\\phi(z_1|x)} [D_{KL}(q_\\phi(z_1|z_z)||p(z_1))]$\n$\\qquad - E_{q_\\phi (z_2/z_1)} [\\log p_\\theta(x|z_1, z_2)] - ELBO_H$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (10)\nwhere we minimize the KL divergence for z1 only when the prior distribution for z1 is defined, otherwise this term is zero and the estimation for z1 is constrained by the latent matching term in Equation 6.\nThe proposed hierarchical Fission VAE also allows heterogeneous decoder architectures for each client groups, as each decoder branch is trained and aggregated independently. This flexibility is particularly advantageous in federated learning environments, where clients often possess varying computational resources. Client groups with more resources can implement deeper and more complex network structures, while groups with limited computational capacity can utilize lighter models."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Evaluation Metrics", "content": "The experiment setup for federated image generation is summarized in Table 2. We constructed two composite datasets to evaluate the performance of the proposed federated VAEs. The first dataset Mixed Mnist combines MNIST [17] and FashionMNIST [32]. These two datasets consists of images of handwritten digits from 0 to 9 and 10 classes of various clothing items, respectively. Images from these two datasets share a common size of 28 \u00d7 28 pixels, and we retain this size for generation and evaluation purposes. In addition, each dataset contains 60,000 training samples and 10,000 test samples. We divided training samples into two client groups, one for each dataset, with each group comprising 10 clients. The training samples were randomly and evenly distributed among the clients within each group. For evaluation, we used the default test samples from both MNIST and FashionMNIST to approximate the true data distribution. We generated an equal number of samples from the global model on the central server to evaluate performance.\nIn contrast to the Mixed MNIST dataset, which primarily serves proof-of-concept purposes, we constructed a second, more complex and realistic composite dataset named CHARM, encompassing five distinct datasets: Cartoon faces from the Anime Face Dataset [4], Human faces from CelebA-HQ [13], Animals from Animals with Attributes 2 (AwA2) [31] with 50 classes, Remote sensing images of 10 land types from EuroSAT-RGB [10], and Marine vessels from MARVEL [7] with 26 classes. For AwA2 and MARVEL, we used preprocessed square images from Meta-Album [29]. All images in this composite dataset are resized to 32\u00d732 pixels for generation and evaluation. We established 20 clients for each of the five groups in CHARM. We utilized 20,000 randomly selected images from these datasets for training, with an additional 5,000 images from each used for evaluation. As with Mixed MNIST, we generated a number of images equal to the evaluation set using the global model for performance evaluation.\nOn the Mixed MNIST dataset, the encoder and decoder for all VAEs only consists of Multi-layer Perceptrons (MLP). For models trained on CHARM, the encoder q(z1|x) and the decoder p(x|z1) are primarily convolutional with MLP used for latent variable estimation. The encoder q(z2|z1) and the decoder p(z1|z2) are MLPs. Bernoulli distribution is used to determine whether or not a client participate in current training round. For the Mixed MNIST dataset, we set the distribution to B(0.5), indicating a 50% chance of participation. For CHARM, we adjusted the participation rate to B(0.25), meaning approximately 25 out of 100 clients participate in each training round. Regarding the hyperparameters for training, we use a learning rate of 1 \u00d7 10-\u00b3 for the Mixed MNIST and 1 \u00d7 10-4 for the CHARM dataset. The total number of training rounds is 70 on Mixed MNIST and 500 on CHARM. For both datasets, client update local models for 5 epochs with a batch size of 32. In the centralized setting, the number of training epochs on Mixed MNIST is 70 and 250 for CHARM. Other hyperparamters are kept the same as the federated configuration."}, {"title": "4.2 Results and Analysis", "content": "We conduct four sets of experiments on the Mixed MNIST and CHARM dataset. In the first experiment, we evaluate the overall generative performance of the proposed VAE architectures in both federated and centralized settings. The second experiment focuses on the hierarchical VAE's ability to control the generation process through explicit definition of the prior distributions for z1. The hierarchical model allows for multiple generation pathways. For instance, one pathway involves sampling z2, passing it through the decoder p(z1/z2), and then generating the final image with p(x|z1). Alternatively, we could directly sample z\u2081 and generate the image through p(x|z1). We compare the performance of these different generation pathways to assess their impact on the quality of generated images. Following the discussion on how z1 affects the generation performance, we explore various strategies for defining z1's prior distribution and assess the quality of generated images following the pathway z2 \u2192 z1 \u2192 x. Lastly, we showcase the use of heterogeneous decoder architectures in the proposed federated VAE and the effects of pixel modeling methods."}, {"title": "4.2.1 Overall Performance", "content": "The overall performance of the proposed FissionVAE models is summarized in Table 3, and generated examples are shown in Fig. 8. As a baseline, a Deep Convolutional GAN (DCGAN) [21] trained via FedGAN [22] is used for comparison. Since GAN does not directly model the likelihood of data, NLL is not evaluated for FedGAN. Also, FedGAN on CHARM suffers from sever mode collapse (shown in Fig. 9), therefore performance evaluation is not available on this dataset. Notably, the performance of all models on the CHARM dataset was less robust compared to the Mixed MNIST dataset. This discrepancy arises because the CHARM dataset, encompassing colorful images from diverse domains, presents a more complex and realistic federated learning scenario. The dataset's diversity, coupled with a lower local data availability and participation rate among clients, poses greater challenges to federated generative models.\nLatent Space Decoupling vs Decoder Branches Across the datasets and learning paradigm, we observe the trend that adopting latent space decoupling or group-specific decoder branches improves the quality of generated images with lower FID and higher IS. We can also tell from Table 3 that the creating decoder branches alone brings more performance improvement compared to sole latent space decoupling, showing that mixing decoders trained on non-IID data can severely affect image generation.\nAmong the proposed VAE configurations, the model FissionVAE+L (L for decoupled latent space) demonstrates a slight improvement over the baseline FissionVAE. This model adapts the latent space to the non-IID nature of nature of federated data by partitioning it according to client groups. This partitioning not only informs the decoder of the expected data characteristics more effectively but also helps to prevent the overlap of data representations from different"}, {"title": "4.2.2 Decoupling the Prior of z1", "content": "As shown in Table 3, explicitly decoupling the latent space for different client groups can improve the VAE's ability to generate images that better reflect the true data distribution. We explore various approaches to define the prior for the latent distribution, all of which are assumed to be multivariate Gaussians with customizable means and the identity matrix as their fixed covariance matrices. The definition of investigated priors are listed in Table 4, and their evaluation results are presented in Table 5.\nFor the non-hierarchical VAE, there is a single latent variable z1. In the two-level hierarchical VAE, we control only the first-level latent variable z1, while the prior distribution for z2 remains a standard normal distribution N(0, 1). Regardless of the hierarchical architecture, the \"identical\" approach in Tables 4 and 5 serves as a baseline with no latent space decoupling, where the prior distributions are identical for all client groups and are defined as the normal distribution. The \"one-hot\" approach assigns one-hot encoded vectors as the means of z1 across different client groups. In the \"symmetrical\" approach, the means of z\u2081 are set to positive and negative integers symmetrical to zero. For instance, the means for groups 1 and 2 are \u00b11, respectively, and for groups 3 and 4, they are \u00b12, and so on. The \"random\" approach involves sampling random vectors from a normal distribution as the means of z\u2081 for each client group. The \"wave\" strategy extends the one-hot encoding by including more ones. Specifically, the dimension of z1 is evenly divided based on the number of client groups. The j-th division is set to 1 for the j-th client group, with the rest of the dimensions being zero. Finally, the \"learnable\" approach is unique to hierarchical VAEs, arising from the prior matching term between the encoded and decoded z\u2081 in Equation 6. This allows the model to learn to align the prior for each client group, but we lose the ability to directly sample from z\u2081 for the controlled generation process. In contrast, other approaches also need to optimize the KL divergence between the encoder's output q\u03c6(z1|x) and the defined prior p(z1) in addition to the prior matching divergence.\nFrom Table 5, we observe that when predefined priors are enforced on z1, hierarchical FissionVAE generally un-derperforms compared to the non-hierarchical FissionVAE. This performance disparity may stem from the increased uncertainty in the latent space introduced by the hierarchical structure. Specifically, the presence of multiple latent layers in hierarchical VAEs introduces more stochasticity, which, coupled with limited local training samples and non-IID global data, causes greater variance in latent representations across different clients. This variance often results in degraded generation quality. Notably, the hierarchical VAE achieves optimal performance with a learnable approach for z1, where no fixed prior distribution is enforced. This flexibility allows the model to adapt the latent space dynam-ically, enhancing generation quality. However, implementing a similar learnable approach in non-hierarchical VAEs is impractical as the ELBO formulation in Equation 2 requires the latent distribution to match the predefined prior.\nOn the Mixed MNIST dataset, employing the identical approach, where a standard normal distribution is used across all client groups, yields the lowest FID for the non-hierarchical VAE but performs poorly on the CHARM dataset. This outcome suggests that for simpler datasets with only two client groups, creating decoder branches alone is adequate for generating distinct samples from different data distributions. However, as the number of client groups increases, a more explicit latent encoding can better guide the model to capture complex data distributions. The identical approach shows diminished performance on both datasets in the hierarchical VAE context. This issue likely arises from the inference hierarchy of the latent variables. Since z2 is estimated by the encoder q\u03c6(z2|z1), sampling z1 from the same distribution across different client groups can introduce contradictions in the latent space.\nThe symmetrical approach, where the mean of p(z1) is set as the positive or negative integer of the group index, can lead to severe model divergence on the CHARM dataset, shown in Figure 9. As the number of client groups increases, this strategy results in means for p(z1) that far exceed the initialized values of the neural network, challenging the"}, {"title": "4.2.3 Generation Pathways in Hierarchical Fission VAE", "content": "Following the discussion on decoupling the latent space of z1 by defining prior distributions for individual client groups, we can now sample from either q(z1) or p(z2) to produce generated samples. There are two possible generation pathways for z1. The first involves treating random samples from the prior distribution as outputs from the encoder q\u03c6(z1|x), which are then passed to the encoder q\u03c6(z2|z1) for further processing before the hierarchical decoder outputs the final generated samples. The second pathway treats z1 samples as outputs from the decoder p\u03b8(z1|z2), directly generating image samples from the final decoder p\u03b8(x|z1). The pathway for z2 follows that of non-hierarchical VAEs, involving only the decoders in the generation process. We list the discussed pathways and their evaluation results in Table 7, and the prior distributions of z\u2081 are defined with the wave approach in Table 4.\nAs indicated in Table 7, the conventional generation pathway from z2 consistently outperforms those from z1. In-cluding the encoder in the pathway from z1 allows for mapping random inputs to more meaningful representations for subsequent decoders, enhancing performance compared to the direct generation pathway using p\u03b8(x|z1). Notably, on the Mixed MNIST dataset, swapping the prior distributions of the two client groups in the encoder-inclusive pathway leads to significant mode collapse, shown in Figure 10. This suggests that the group-level privacy may be preserved by maintaining the confidentiality of prior distributions. This strategy ensures that high-quality samples are generated only when the correct prior distribution is used, while mismatched distributions yield unrecognizable outputs. This phenomenon is more pronounced in both hierarchical and non-hierarchical FissionVAEs on the Mixed MNIST dataset than on the CHARM dataset, likely due to the simpler, more uniform nature of the Mixed MNIST data compared to the diverse and colorful image types in CHARM, which pose greater challenges in satisfying complex latent distribution constraints."}, {"title": "4.2.4 Heterogeneous Decoders in Fission VAE", "content": "As discussed in Section III, the distinct separation and aggregation of decoders tailored to specific client groups allow for the use of heterogeneous architectures in Fission VAE. The Mixed MNIST dataset, with its relatively simple and grayscale colors, can be generated from both fully connected (MLP) and convolutional layers. In contrast, the more complex and colorful images in the CHARM dataset predominantly require convolutional layers for effective generation.\nTable 6 details the performance evaluation of various decoder architectures. The term 'homogeneous' refers to identical architectural configurations across all decoder branches, namely a three-layer MLP for each decoder modules. In the 'Deeper MLP' configuration, we add two additional fully connected layers to both p\u03b8(z1|z2) and p\u03b8(x|z1). Meanwhile, we completely replace the decoder p\u03b8(x|z1) from MLP to a series of transpose convolution layers in the 'Deeper MLP + Conv' configuration. The results indicate a gradual reduction in overall FID scores as the decoder architecture becomes more heterogeneous. However, the integration of convolutional layers does not improve generation performance over the MLP models, underscoring that while heterogeneous architectures are feasible, they can disrupt the convergence of the VAE due to mismatches in architecture and the model's weight space."}, {"title": "4.3 Modeling the Pixels", "content": "The reconstruction term in the ELBO quantifies the likelihood of the original input given the estimated distribution from the decoder. L1 or L2 losses, which measure absolute or squared differences, respectively, do not align well with the probabilistic outputs of VAEs. These metrics fail to capture the underlying data distribution effectively, making them less suitable for VAEs that model the complex variability of image pixels."}, {"title": "5 Conclusion", "content": "In this study, we introduce Fission VAE, a generative model designed for federated image generation in non-IID data environments. By decomposing the latent space and employing decoder branches tailored to specific client groups, Fis-sion VAE not only enhances generation quality but also maintains the distinct visual features of diverse data subsets. Our experiments utilize composite datasets includeing the Mixed MNIST and a broader dataset CHARM comprising cartoon and human faces, animals, marine vessels, and remote sensing images. Evaluation results demonstrate that Fission VAE significantly outperforms baseline federated VAE models in terms of image quality. Notably, the implementation of heterogeneous decoder branches and the adoption of wave encoding for defining priors have proven effective, under-scoring our model's capability to adapt to the varied demands of non-IID data distributions within federated learning frameworks.\nSeveral avenues appear promising for further research and improvement of FissionVAE. Enhancing the stability and optimization dynamics of heterogeneous decoder branches could address challenges encountered during training and generation, particularly as the complexity of data and model architecture increases. Exploring the potential of Fission VAE to support cross-modality data generation could pave the way for its application in generating vision-language data within a federated environment. Moreover, as real-world federated learning scenarios often involve an increasing number of client groups, developing more sophisticated strategies to scale latent space and decoder decomposition will be crucial."}]}