{"title": "LINER SHIPPING NETWORK DESIGN WITH REINFORCEMENT LEARNING", "authors": ["Utsav Dutta", "Yifan Lin", "Zhaoyang Larry Jin"], "abstract": "This paper proposes a novel reinforcement learning framework to address the Liner Shipping Network Design Problem (LSNDP), a challenging combinatorial optimization problem focused on designing cost-efficient maritime shipping routes. Traditional methods for solving the LSNDP typically involve decomposing the problem into sub-problems, such as network design and multi-commodity flow, which are then tackled using approximate heuristics or large neighborhood search (LNS) techniques. In contrast, our approach employs a model-free reinforcement learning algorithm on the network design, integrated with a heuristic-based multi-commodity flow solver, to produce competitive results on the publicly available LINERLIB benchmark. Additionally, our method also demonstrates generalization capabilities by producing competitive solutions on the benchmark instances after training on perturbed instances.", "sections": [{"title": "1 Introduction", "content": "The liner shipping industry is the backbone of global maritime trade. It plays a critical role in the international supply chain, ensuring the efficient movement of merchandise across the globe. This industry involves the design and operation of container vessels that traverse fixed maritime routes to transport goods between ports efficiently and profitably. The strategic planning of these routes plays a pivotal role in optimizing both the revenue of ocean freight companies and the operational efficiency of their vessels. Well-designed shipping networks not only enhance profitability but also reduce the total number of vessels utilized, leading to lower maintenance costs and decreased emissions.\n\nThe Liner Shipping Network Design Problem (LSNDP) addresses this complex business challenge by modeling this as a mathematical optimization problem. The goal of the LSNDP is to design optimal vessel routes and allocate cargo flows across the network to maximize overall profitability. As a specialized routing problem, the LSNDP falls under the broader category of combinatorial optimization problems. Similar routing problems include the well-known Traveling Salesman Problem (TSP) and Vehicle Routing Problem (VRP). Similar to these, the LSNDP is classified as NP-hard, due to which solving large-scale instances to optimality with traditional OR approaches such as Mixed-Integer Programming (MIP) is computationally intractable.\n\nHowever, recent advances in deep learning for routing problems have introduced an alternative approach to solving NP-hard combinatorial optimization problems. Kool et al. [2018] applied Reinforcement Learning (RL) to the Traveling Salesman Problem (TSP) and several variants of the Vehicle Routing Problem (VRP), demonstrating the potential of RL in this domain. Building on this, Joshi et al. [2019] enhanced the RL framework by incorporating Graph Convolutional Networks (GCN), which yielded promising results for TSP. These learning-based approaches achieved results comparable to traditional OR methods in terms of optimality, while also demonstrating impressive generalizability. This opens up avenues to learn general purpose policies from a diverse dataset and use these to infer high quality solutions on new unseen data points."}, {"title": "2 Related Work", "content": "The Liner Shipping Network Design Problem (LSNDP) has been extensively researched within the operations research (OR) community for several decades. To support benchmarking efforts for the LSNDP, Brouer et al. [2014] introduced a standardized dataset known as LINERLIB\\u00b9, which includes seven real-world instances of the problem, each varying in scale. In a comprehensive review of the LSNDP literature, Christiansen et al. [2020] discussed the standardized formulation of the problem widely accepted by the OR community and reviewed the development of the OR-based approaches typically used in this domain. The performance of leading algorithms is benchmarked on the LINERLIB dataset in their work. According to the authors, traditional OR approaches to the LSNDP can generally be categorized into the following main types:\n\n\\u2022 Holistic MIP-based formulations, which address both the service design and the multi-commodity flow aspects simultaneously, as exemplified by the work of Plum et al. [2014] and Wang and Meng [2014].\n\\u2022 Local search-based methods, which explore variations from a predefined set of candidate services, such as the approach described by Meng and Wang [2011] and Balakrishnan and Karsten [2017].\n\\u2022 Two-stage algorithms: These decompose the problem into two distinct phases: first solving the network design problem (NDP) and then the multi-commodity flow (MCF) problem separately. A common method, as used by Brouer et al. [2014] and Thun et al. [2017], involves designing the services (i.e., NDP) first and subsequently routing the containers through the designed network (i.e., MCF). Alternatively, Krogsgaard et al. [2018] propose a reverse approach, where containers are first flowed through a relaxed network before finalizing the network design based on the flow. This class of approaches has generally proven to be the most effective, yielding reasonable solutions on the largest instance in the LINERLIB dataset, where other methods have failed.\n\nDespite these advancements, two significant challenges persist across all OR methods. First, scalability remains a critical issue due to the NP-hard nature of the LSNDP, with large, real-world instances still unsolved. Second, generalizability is a major limitation, as even small perturbations in problem instances often necessitate a complete reconstruction of the solution, requiring a similar level of computational effort as the original problem.\n\nIn the past decade, Reinforcement Learning (RL) has gained increasing attention as a method for solving combinatorial optimization problems. Khalil et al. [2017] pioneered the use of RL to tackle the Maximum Cut and Minimum Vertex Cover problems, combining graph embeddings with Q-learning to generate heuristics. Hu et al. [2017] made the first attempt at applying RL to the Bin Packing Problem. Within the domain of routing problems, Vinyals et al. [2015] introduced the Pointer Network (PN) to address the Traveling Salesman Problem (TSP), employing attention mechanisms to map inputs to outputs. Bello et al. [2016] built on this work by applying the Actor-Critic algorithm to improve PN performance. Nazari et al. [2018] extended the RL approach to the Vehicle Routing Problem (VRP), enhancing the Pointer Network by replacing the Long-Short Term Memory (LSTM) encoder with a 1-D convolutional embedding.\n\nMore recent advances in RL for routing problems include the work of Kool et al. [2018], who developed a construction-heuristic learning approach applied to the TSP, VRP, and other related routing challenges. Their approach enhanced the encoder by introducing a Transformer-like attention mechanism, while the decoder maintained a similar structure to the original PN. Joshi et al. [2019] further advanced the field by incorporating a Graph Convolutional Network (GCN)"}, {"title": "3 LSNDP", "content": "Brouer et al. [2014] and Christiansen et al. [2020] both provided a comprehensive definition of the Liner Shipping Network Design Problem (LSNDP): Given a set of ports, a fleet of container vessels, and a collection of demands specified in the quantity of Forty-Foot Equivalent units (FFE) with designated origins, destinations, and shipping rates, the objective is to design a set of cyclic sailing routes for the vessels (i.e., services) that maximize revenue from fulfilling the demands while minimizing the overall operational costs of those vessels.\n\nIt is important to note that, unlike the VRP or other dispatch problems, the LSNDP does not focus on the specific scheduling of vessels. Instead, it generates a set of services, each representing a round-trip route with a fixed itinerary of ports, called at regular intervals, typically at a weekly or bi-weekly frequency. Vessel assignments are subsequently determined to support these services. For example, if a round-trip route takes six weeks to complete, six vessels will be needed to maintain a weekly service. Additionally, the demand in LSNDP is normalized according to these service frequencies, simplifying the problem formulation.\n\nServices in the LSNDP are categorized based on the topological structure of their routes. A simple service follows a round-trip route where vessels visit each port exactly once, forming a single circular loop. However, services are often non-simple, meaning that some ports are visited more than once along the same route. Visually, these non-simple services form multiple loops. Depending on their structure, they can be classified as butterfly services, pendulum services, or complex services.\n\nBuilding on the basic definition provided above, several variations of the LSNDP have been extensively studied in the literature. These variations introduce additional factors such as transit time, which imposes time constraints on demand, transshipment costs, vessel speed optimization, and penalties for leaving a portion of the demand unsatisfied (rejected demand). In line with most studies that benchmark their results using the LINERLIB dataset, this work focuses on the LSNDP variation that incorporates transshipment costs and rejected demand, while excluding considerations for transit time and vessel speed optimization. Additionally, we assume that vessels operate strictly at their designed speed and permit fractional vessel assignments, which simplifies the modeling of vessel deployment and optimizes resource allocation.\n\nAs discussed in Section 2, a widely adopted approach within the traditional operations research (OR) community for solving the Liner Shipping Network Design Problem (LSNDP) is to decompose it into two closely related sub-problems: the multi-commodity flow (MCF) problem and the network design problem (NDP). \n\nIn our proposed reinforcement learning (RL) approach, we aim to leverage this two-tier framework. Rather than formulating the NDP as a Mixed-Integer Problem (MIP), we represent it as a Markov Decision Process (MDP), where round-trip services are generated sequentially. At each step t, the generation of a complete service is treated as an action A_t, with the step index t indicating the number of services generated up to that stage."}, {"title": "4 Policy Neural Network Design", "content": "In this section, we present two modeling approaches for parameterizing the policy \\(\\pi_\\theta\\) as neural networks: the encoder-only approach and the encoder-decoder approach. After representing the NDP as a Markov Decision Process (MDP), either approach can produce a parameterized policy \\(\\pi_\\theta\\) that guides the actions at each step t. This process can be described by the following sampling equation:\n\n\\[A_t \\sim \\pi_\\theta(\\cdot | S_t),\\]\n\nwhere the action A_t in the context of the NDP comprises of two components: the vessel selection A_{v,t} \\in \\mathbb{R}^V, which determines the vessel type to be deployed from the overall fleet, and the service selection A_{p,t} \\in \\mathbb{R}^P, which specifies an ordered sequence of ports:\n\n\\[A_t = [A_{v,t}, A_{p,t}].\\]\n\nThe state at step t is represented by two components: S_t = {S_{t,g}, S_{t,v}}. Here, S_{t,g} captures the state of the shipping network as a graph, and S_{t,v} describes the status of the available vessels. These components are defined as follows:\n\n\\[S_{t,g} = {f_p, f_e},\\]\n\n\\[S_{t,v} = v_t \\in \\mathbb{R}^{V \\times D_v}.\\]"}, {"title": "4.1 Encoder-Only Approach with One-Shot Rollout", "content": "demonstrates the workflow of the encoder-only approach. Instead of defining vessel selection as sampling from a stochastic policy, we use a simple deterministic heuristic for vessel selection. Specifically, we select the \"largest available vessel\" as the action A_{v,t}, where \"largest\" refers to the vessel class with the highest capacity, measured in FFEs:\n\n\\[A_{v,t} = \\underset{\\nu \\in [1,V]}{\\operatorname{argmax}} \\operatorname{Capacity}(\\nu).\\]\n\\[(V_t)_{\\nu,1} > 0\\]\n\nHere, \\(\\nu\\) represents the index of the vector \\(v_t\\), and \\(\\operatorname{Capacity}(\\nu)\\) denotes the capacity of the vessel class corresponding to index \\(\\nu\\). The constraint \\((v_t)_{\\nu,1} > 0\\) ensures that only vessel classes with available vessels are considered for selection."}, {"title": "4.2 Encoder-Decoder Approach with Autoregressive Rollout", "content": "Equations 11 to 14 describe the one-shot rollout for the encoder-only approach, where the probability of each port being included in a service is modeled independently within each action At. While this method is straightforward and intuitive, it treats the inclusion of each port as independent, limiting its ability to account for dependencies between selected ports. In contrast, the encoder-decoder approach with autoregressive rollout, explicitly models these dependencies, where the decision to include an additional port depends on the previously selected ports in the current and all previous services.\n\nIn the autoregressive rollout, the action A_t is generated sequentially, involving multiple sub-steps within a single step t. The process begins with a sub-step \\(\\tau\\), denoted as \\(\\tau\\), for vessel selection, followed by several sub-steps to select ports, thereby completing the generation of a single service. It is important to note that in this approach, vessel selection is also determined by the policy \\(\\pi_\\theta\\), which is parameterized by a neural network, rather than the rule-based selection used in the encoder-only approach (Eq. 6).\n\nThe embeddings used for the autoregressive rollout are generated as outputs from the encoder phase. Specifically, for the port embeddings, we define:\n\n\\[h_p = \\operatorname{Transformer}(h_{\\operatorname{port}}^{(L)}) \\in \\mathbb{R}^{P\\times H},\\]\n\nwhere \\(h_{\\operatorname{port}}^{(L)} \\in \\mathbb{R}^{P\\times H}\\) is the port embedding previously defined in Eq. 9. For the vessel embeddings, we similarly define:\n\n\\[h_v = W_v v \\in \\mathbb{R}^{V\\times H}.\\]\n\nwhere h_v represents the embeddings for all vessel classes, rather than just the selected vessel class as used in Eq. 10 for the encoder-only approach. Note the W_v \\in \\mathbb{R}^{H \\times D_v} is distinct from \\(W_v\\). Next, we define the overall embedding for the decoder:\n\n\\[h_{\\operatorname{embed}} =\\begin{bmatrix} h_p \\\\ h_v \\\\ h_{\\operatorname{BOS}} \\end{bmatrix} \\in \\mathbb{R}^{N \\times H},\\]\n\nwhere \\(h_{\\operatorname{BOS}} \\in \\mathbb{R}^{H}\\) represents the embedding for the \"beginning of service\" (BOS). This vector is randomly initialized and remains static throughout the entire service generation process. The notation N = P + V + 1 reflects the total dimensionality of the embedding, where P is the number of ports, V is the number of vessel classes, and the additional 1 corresponds to the BOS. It's important to note that the embeddings h_p and h_v vary with each step t, but remain constant across all sub-step \\(\\tau\\)'s. For simplicity, we have omitted the t subscripts in this equation."}, {"title": "5 Policy Optimization", "content": "To optimize our policy network \\(\\pi_\\theta\\), we employ policy gradient methods, which iteratively refine the policy to maximize the reward. Policy gradient methods form a broad class of reinforcement learning algorithms that directly improve the policy \\(\\pi_\\theta\\) by rewarding actions that lead to higher-value outcomes based on sampled trajectories. In this work, we utilize an enhanced variant of the standard policy gradient algorithm known as Proximal Policy Optimization (PPO) Schulman et al. [2017]. PPO introduces a clipped surrogate objective that significantly enhances the stability of the learning process."}, {"title": "6 Experiments", "content": "We conduct experiments to evaluate the performance of the proposed RL approach on the LINERLIB benchmark, demonstrating that the RL-based solution for the NDP is a promising alternative to MIP and heuristic methods. Notably,"}, {"title": "6.1 Result on Baltic Instance", "content": "In this section, we evaluate the performance of our RL-based NDP approach on the Baltic instance from the LINERLIB dataset, which consists of 12 ports (i.e., vertices in the graph). Training and validation are conducted on separate datasets with a total of 16,000 instances, where the demand quantities were perturbed from the original LINERLIB Baltic instance (with a factor of \\(\\pm\\)10%), while the origins and destinations of the demand remained unchanged. For a detailed description of the perturbation process, please refer to Appendix E. The test set consists of a single data point\\u2014the actual LINERLIB Baltic instance\\u2014to ensure a fair comparison with publicly available benchmark solutions. It is worth noting that we primarily report results from the encoder-decoder RL-based solution, as the encoder-only variant produced nearly identical results in this case."}, {"title": "6.2 Experiments on Other Instances", "content": "In this section, we explore the potential of using the RL-based NDP solution as an optimizer, where the training process of the RL agent functions as a traditional optimization solver. In this scenario, there is no distinction between the training and inference phases.\n\nWe extend the experiments to include two additional instances from LINERLIB: West Africa (WAF) and World Small. These instances contain 20 and 47 ports, respectively."}, {"title": "6.3 Solve Time Comparison", "content": "Table 3 reports both the inference time and training time for the RL-based NDP solution on the LINERLIB instances we experimented on. The solve time is benchmarked with the LINERLIB solution on corresponding instances. Note that for the RL-based NDP solution, the inferences are all run on an Apple M2 CPU with 12 cores, while the trainings are conducted on an A100 GPU. We only report the encoder-only approach as the RL-based NDP solution in Table 3 given that the encoder-decoder approach yields an inference time similar to that of the encoder-only approach."}, {"title": "6.4 Solution Robustness against Variations in the Problem Instance", "content": "In real-world applications beyond the academic scope of the LSNDP, schedulers often face disturbances on short notice that can have long-term impacts. Examples include trade wars, which affect demand quantities, or pirate activities, which influence the availability of certain ports or routes in network design. Consequently, having an algorithmic tool that can quickly generate optimal network designs for a variety of perturbed problem instances is of immense value.\n\nIn this part of the experiment, we evaluate the effectiveness of our RL-based NDP solution in handling a large set of problem instances perturbed from a common baseline. Additionally, we explore how enhancing the RL agent by exposing it to these perturbed instances during training improves its performance compared to the baseline RL agent, which is trained on a single problem instance (as used in Section 6.2). For the enhanced RL agent, training and validation are conducted on separate datasets totaling 80,000 instances, where demand quantities are perturbed by \\(\\pm\\)10% from the original LINERLIB Baltic instance (matching the perturbation level in Section 6.1). During inference, 100 different test instances are randomly generated, and for each instance, the RL agent produces 100 network designs. From these designs, the maximum profit (i.e., reward) is selected for each instance. The mean and standard deviation of these maximum profits across the 100 test instances are then reported.\n\nTo further examine the agent's robustness, we increase the perturbation level in both the training and test datasets from 10% to 50%, assessing how much additional improvement the enhanced RL agent offers over the baseline. Notably, the"}, {"title": "6.5 Discussion", "content": "The experiments conducted in this section demonstrate the effectiveness of our RL-based NDP solution in two significant ways. Firstly, when evaluated on the Baltic instance from the LINERLIB dataset, the RL-based solution generates near-optimal results and compares favorably against the benchmark solutions. Secondly, when applied as an optimizer on previously unseen instances, such as the Baltic, West Africa (WAF), and World Small datasets, the RL-based approach continues to deliver competitive performance without the need for retraining. This motivates the use of reinforcement learning based methods to learn general, competitive policies that can potentially deliver high-quality solutions on new instances.\n\nHowever, a few limitations should be noted, stemming from both the computational resources and the experimental setup. The most significant limitation arises from the underlying heuristic multi-commodity flow (MCF) algorithm, which serves as a key part of the reward function evaluator for the RL agent. Unfortunately, we do not have access to the state-of-the-art MCF implementations used by the benchmarks. This discrepancy between our MCF implementation and those used in the benchmarks means the associated NDPs are effectively different problems. As a result, we evaluate all benchmark solutions using our MCF implementation.\n\nAdditionally, it's important to note that the NDP definition has been relaxed to better align with our RL-based approach. For instance, we consider only simple services in the network design and relaxed the hard limit on the number of vessels to a soft constraint with penalties. These modifications explain why the LINERLIB and ORTools solutions reported here may differ from those found in other literature. According to industry experts, these relaxations have only a limited impact on the solution quality. Nonetheless, tightening these constraints and preparing the solution for an end-to-end benchmark on the full LSNDP would be a valuable next step.\n\nOur current experiments cover three out of the seven instances in the LINERLIB dataset. Expanding the experiments to include all instances, particularly the World Large instance (the largest in the dataset), would further test the scalability of the approach. Moreover, the perturbations in this study are limited to demand quantities. Extending the perturbations to include the origin and destination of the demand, the number of available vessels in each class, and the inclusion of specific ports would provide a more comprehensive demonstration of the solution's generalizability."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we propose a model-free RL-based framework to address the network design aspect of the Liner Shipping Network Design Problem (LSNDP). By leveraging a heuristic-based multi-commodity flow (MCF) solver as part of the evaluator function, our approach can solve the LSNDP in an end-to-end fashion. This work marks the first attempt to approach LSNDP through a method distinct from traditional operations research (OR) techniques. Our framework demonstrates scalability with problem size and achieves competitive results on the LINERLIB benchmark. We have shown that our approach offers value in two key ways: it can rapidly generate near-optimal solutions for problem instances perturbed from the training data or be utilized as an optimizer, delivering effective performance on unseen problem instances without requiring prior training.\n\nOur approach introduces a novel paradigm for solving LSNDP compared to conventional OR methods, which typically require equal computational effort for each new problem instance. In contrast, our method front-loads the computational work during the training phase, while enabling rapid inference to new instances. This makes the solution ideal for problems like LSNDP, where long-term plans are frequently disrupted by unexpected events or frequent data updates. In terms of real world applications, this enables rapid \\u201ctactical\\u201d changes by reacting to real world dynamics.\n\nLooking ahead, there are several opportunities to enhance the encoder-decoder architecture. Replacing the LSTM with a transformer-based architecture could allow the network to handle larger and more complex use cases. Additionally, as mentioned in Section 6.3, the MCF algorithm accounts for a significant portion of the runtime during both training and inference. A faster MCF implementation would further improve training efficiency and reduce thhe training wall clock time. Exploring a Graph Neural Network (GNN)-based surrogate for MCF is another promising avenue to speed up reward function evaluation.\n\nIn terms of training strategies, there are several paths to explore. One interesting direction is the application of reward shaping, as discussed by Ng et al. [1999], to enhance training efficiency. Reward shaping provides additional signals to guide the RL agent toward an optimal policy, especially when only terminal rewards are available during policy exploration. Introducing penalties in the reward function could also help guide the agent's behavior; for instance, adding a service length penalty could encourage the agent to select shorter routes that optimize transshipment usage.\n\nAnother area worth exploring is the inherent symmetry of the LSNDP, where the reward remains unchanged if ports are rotated within a service. Inspired by OR techniques, which often limit symmetry in the search space to improve solving speed, symmetry can also be leveraged in neural combinatorial optimization, as demonstrated by Kwon et al. [2020] and Kim et al. [2022]. Adapting these techniques to our RL framework could improve sample efficiency for LSNDP.\n\nOther reinforcement learning algorithms that favor exploration and improve sample efficiency could potentially improve performance over PPO. For example, Soft Actor-Critic (SAC, Haarnoja et al. [2018]), an off-policy actor-critic algorithm, maximizes both expected reward and entropy, promoting more effective exploration. Additionally, Monte Carlo Tree Search (MCTS, Kocsis and Szepesv\u00e1ri [2006]) based methods when combined with neural networks Silver et al. [2017] have proved to be effective model-based approaches and have yielded superhuman performance in deterministic environments. RL algorithms designed to scale with problem size (Drori et al. [2020]) and generalize across a variety of instances (Fu et al. [2021]) may also align with broader business needs beyond LSNDP. Adapting these techniques to train models on smaller instances and transfer the learned policy to larger problems would be a valuable extension of this work."}, {"title": "A LSNDP details", "content": "A.1 Dataset\nBrouer et al. [2014] offers a comprehensive introduction to the LSNDP benchmark suite, LINERLIB. Here, we provide a brief overview to establish the context for the problem we aim to address using reinforcement learning. At a high level, a liner shipping network comprises a fleet of vessels V deployed across rotations or services S to satisfy a set of commodity demands D, normalized to a weekly frequency. Visualizing the network as a graph, ports can be seen as vertices, with edges E representing the connections between them. Each service s \\u2208 S involves a rotation through a sequence of ports sp, is assigned a specific subset of vessels sv, and includes a set of legs SE that define the route. Below, we provide a more detailed breakdown of each of these elements.\nLINERLIB includes a predefined set of ports P that vessels can access. Each port p within this set is characterized by the following features:\np\tPort ID, represented by UNLOCODE.\npf\tFixed cost per port call, the cost in USD for each vessel call at this port.\npv\tVariable cost per port call, the additional cost in USD per FFE for visiting this port, based on the vessel's capacity.\npt\tTransshipment cost per FFE, the cost in USD per FFE for transferring cargo across different services at this port.\nA fleet of vessels V contains different vessel classes vF, each with different capacities and characteristics. Each vessel class has a finite number of vessels available for deployment. A vessel \\(\\upsilon \\in \\upsilon_F\\) is characterized by the following features:\nUcap\tCapacity, the maximum number of FFEs the vessel can carry at once.\nUn\tQuantity, the total number of available vessels of class \\(\\upsilon\\).\nUTC\tTC rate, the daily cost of renting or operating the vessel.\nUs\tDesign speed, the vessel's standard sailing speed.\nUfs\tFuel consumption at design speed, the vessel's daily fuel consumption (converted to $) when sailing at design speed.\nUfi\tFuel consumption while idling, the vessel's daily fuel consumption (converted to $) when idle at the port.\nUSuez\tSuez fee, the fee for passing through the Suez Canal.\nUPanama\tPanama fee, the fee for passing through the Panama Canal.\nEach port in p also includes data on latitude and longitude coordinates (which we omitted earlier for brevity). The LINERLIB dataset provides distance information, including whether the route passes through the Panama or Suez canals. This distance data corresponds to the edge e \\u2208 E in the graph and is described by the following features:\no\tOrigin port, the Port ID in UNLOCODE.\ned\tDestination port, the Port ID in UNLOCODE.\nedist\tDistance, the distance between the origin port and the destination port, measured in nautical miles.\neSuez\tSuez traversal, a flag indicating whether the sailing route passes through the Suez Canal. A value of 1 signifies the route uses the Suez Canal; 0 otherwise.\nePanama\tPanama traversal, a flag indicating whether the sailing route passes through the Panama Canal. A value of 1 signifies the route uses the Panama Canal; 0 otherwise.\nAt last, each commodity demand d\\u2208 D is characterized by the following features:\ndo\tOrigin port, the Port ID in UNLOCODE.\ndd\tDestination port, the Port ID in UNLOCODE.\ndR\tRevenue, generated per unit FFE transported.\ndq\tQuantity, demand quantity in FFE per week.\nYd\tPenalty if rejected, penalty for rejection of this demand, which is set to $1000.\nNote that we have only listed the dataset elements relevant for solving the LSNDP with transshipment, rejected demand, and fractional vessel assignments."}, {"title": "A.2 Multi Commodity Flow", "content": "The maximum profit Multi-Commodity Flow Problem seeks to determine the optimal flow of multiple commodities through a capacitated network to maximize total profit. Each commodity has a specific origin and destination and moves through the network's edges, constrained by capacity limits. Additionally, each unit of flow for a commodity may generate a defined revenue. The goal is to allocate flows for all commodities in a way that maximizes the overall profit while adhering to the network's capacity constraints. The capacity limits of the network are defined by the designed rotations or services from the associated network design problem, which will be discussed later. The capacity constraints on each edge of the network are derived from the capacities of the vessels assigned to those routes.\nHere, we omit the specific details of the constraints and focus only on the objective function. For a comprehensive description of the complete problem formulation, please refer to Brouer et al. [2014].\n\\[\\operatorname{Maximize} \\ \\eta = R_{\\text{total}} - C_{\\text{reject}} - C_{\\text{handle}} - C_{\\text{NDP}},\\]\nwhere \\(\\eta\\) is the profit, \\(R_{\\text{total}}\\) represents the total revenue generated, \\(C_{\\text{reject}}\\) represents the penalty associated with rejected demand, and \\(C_{\\text{handle}}\\) denotes the handling costs. Detailed descriptions of these terms are provided in the equations below. Note that \\(C_{\\text{NDP}}\\) is the fixed cost of establishing all services in the network, independent of the decisions made within the multi-commodity flow problem. This fixed cost is determined by the network design and will be discussed in detail later.\n\\[R_{\\text{total}} = \\sum_{d \\in D} d_R \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\sum_{e \\in E} I_{\\text{eled}=d}\\]\n\\[C_{\\text{reject}} = Y_d \\sum_{d \\in D} d_q \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\sum_{e \\in E} I_{\\text{eled}=d}\\]\n\\[C_{\\text{handle}} = \\sum_{p \\in P} p_t \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\left(\\ \\sum_{e \\in E} I_{\\text{elo}=d_o=p} \\left(f_e^{+}\\right) + \\sum_{e \\in E} I_{\\text{eld}=d_d=p} \\left(f_e^{-}\\right) + \\sum_{\\substack{e',e'' \\in E \\cap \\mathcal{S}_E}} I_{\\substack{ea=p, \\ e'=p, \\ ea \\neq dd}} \\left(f_e^{+} - f_e^{-}\\right)\\right).\\]\nHere, \\(f_e^d\\) is a decision variable within MCF, which represents the quantity of commodity demand d that flows through edge e, while e' and e\" refer to two edges within the same service. The remaining notation is detailed in Appendix A.1. It is important to note that the handling cost, as described in Eq. 31, has two components: the first is the cost associated with onloading and offloading, and the second is the transshipment cost. It is important to note that, in contrast to the fixed cost \\(C_{\\text{NDP}}\\), the terms in Eqs. 29, 30, and 31 represent variable costs, with revenue broadly considered as a form of negative cost.\nThe MCF problem is known to be NP-hard (see Theorem 6.2 in Brouer et al. [2014]). Rather than solving it using a MIP formulation, we employ a fast, greedy heuristic-based approach, which is detailed in Appendix B."}, {"title": "A.3 Network Design Problem", "content": "The network design problem (NDP) focuses on identifying the optimal set of services for a given LSNDP instance to maximize the overall profitability of the shipping network. However, the ultimate profitability of the network is determined by the Multi-Commodity Flow (MCF) solution, as discussed previously. The primary objective of the NDP is to develop a network design that defines the capacities on the edges of the services, which are used as constraints in MCF. The fixed cost associated with the designed network corresponds to the CNDP term in Eq. 28 and is calculated as follows:\n\\[C_{\\text{NDP}} = C_{\\text{service}} + C_{\\text{unused}} + C_{\\text{voyage}},\\]\nwhere \\(C_{\\text{service}}\\) represents the vessel service cost, accounting for the total cost of renting or operating all vessels assigned to the services. \\(C_{\\text{unused}}\\) captures the cost (or profit) of unused vessels. If the generated services do not utilize all available vessels, the remaining vessels can be rented out at the time charter (or TC) rates. Conversely, if the services require more vessels than are available, additional vessels must be acquired at the same rate. The term \\(C_{\\text{voyage}}\\) refers to the voyage cost, which includes fuel costs, port calling costs, and canal fees associated with operating the vessels to support"}]}