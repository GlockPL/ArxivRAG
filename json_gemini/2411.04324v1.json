{"title": "Gradient Boosting Trees and Large Language Models for Tabular Data Few-Shot Learning", "authors": ["Carlos Huertas"], "abstract": "Large Language Models (LLM) have brought numerous of new applications to Machine Learning (ML). In the context of tabular data (TD), recent studies show that TabLLM is a very powerful mechanism for few-shot-learning (FSL) applications, even if gradient boosting decisions trees (GBDT) have historically dominated the TD field. In this work we demonstrate that although LLMs are a viable alternative, the evidence suggests that baselines used to gauge performance can be improved. We replicated public benchmarks and our methodology improves LightGBM by 290%, this is mainly driven by forcing node splitting with few samples, a critical step in FSL with GBDT. Our results show an advantage to TabLLM for 8 or fewer shots, but as the number of samples increases GBDT provides competitive performance at a fraction of runtime. For other real-life applications with vast number of samples, we found FSL still useful to improve model diversity, and when combined with ExtraTrees it provides strong resilience to overfitting, our proposal was validated in a ML competition setting ranking first place.", "sections": [{"title": "I. INTRODUCTION", "content": "Tabular data in real-world applications is the most common type of data [1], this continues to be true since relational databases are still pretty common in all sort of domains from social to natural sciences [2]-[6]. Deep Learning (DL), or in general, Neural Network based architectures have shown tremendous potential in tasks like Natural Language Process-ing (NLP) with developments like transformers [7] and large-scale pre-trained models like DeBERTa [8] have pushed the state-of-the-art (SOTA) and gave DL a top spot in perfor-mance. The same can be observed for Computer Vision (CV) with developments like convolutional neural networks (CNN) opening the door for more advanced designs like EfficientNets [9] and more recently Vision Transformers (ViT) have found their way into CV as well [10] with Next-ViT [11] aiming to bridge the gap that still separates ViT from CNN in terms of efficiency in the latency/accuracy trade-off.\nDespite all the success from DL, tabular data continues to be omnipresent [12], [13], and to the best of our knowledge, we have not found a consistent DL-based approach that can outperform Gradient Boosted Decision Trees (GBDT) [14]-[16] over a wide variety of tasks and conditions, even though it is possible to find specific niche setups where this happens [17]-[19].\nRecently, the introduction of Large Language Models (LLM) [20] demonstrated a whole new level of performance for several tasks [21], [22], from traditional NLP to even code generation [23]. The concept of revisiting the qualities of DL-based techniques, in particular LLM for tabular data surged again [6], due to some of the key properties over GBDT [24], such as: representation learning, sequential processing and generalization. Even though DL provides some advantages, if maximum performance is desired, GBDT continues to be the SOTA [25] even with amazing advances in DL, some of the most notable attempts to outperform GBDT with DL methods include: Wide&Deep [26], DeepFM [27], SDTR [28], DeepGBM [29], TabNN [30], BGNN [31], TabNet [32], TransTab [33], TabTransformer [34], SAINT [35] and NPT [36], none of them providing enough evidence to actually be able to beat GBDT over a wide variety of tasks, most of the time, it has been demonstrated the claimed improvements are only present in very specific cases or datasets [17].\nThere are however, some situations where LLM based solutions seem to have an edge [6], this is when data is limited, and LLM have the capacity to perform both zero-shot (ZSL) and few-shot learning (FSL) [37]. While there is no doubt current SOTA in GBDT will show random-performance for zero-shot learning, recent studies [38] show that even under a few-shot schema, LLM can outperform Xgboost [14], one of the most popular GBDT algorithms.\nIn this work, we will further explore the performance of GBDT under a FSL schema in order to provide strong baselines. Since previous studies [17] have demonstrated bias in claims of DL outperforming GBDT in other tasks, we look to enhance experiments to confirm SOTA results in the new trend of results regarding FSL and the superiority of LLM over GBDT."}, {"title": "II. RELATED WORK", "content": "The main concept behind ZSL or FSL by definition implies the evaluated classifier has either (a) never seen the data samples before (ZSL), or only a few samples (FSL), however, this can only be proven true if we were to train a model (LLM for the purpose of this research) from scratch. Any sort of pre-trained architecture could, in theory, already seen the dataset, hence showing incredible performance. This particular problem has been studied before [39], where both GPT-3.5 and GPT-4 are proven to have seen common datasets in the past, like Adult Income and FICO [40], in some cases, even proven LLM have literally memorized the datasets verbatim [41] as samples can be extracted out. With this in mind, the fair"}, {"title": "III. PROPOSED SOLUTION", "content": "The process of FSL might have slightly different interpreta-tions depending on the field, but the core concept remains, the usage of only a few samples to train a model. This concept holds for the tabular data use-case. Knowing this, is imperative to understand how algorithms like LightGBM work in order to build an effective FSL solution. The LightGBM algorithm is a boosting approach using decisions trees (DT) to learn a function from the input space X to the gradient space G [15], the splitting criteria is reviewed below.\nGiven a training set with n i.i.d. instances {x1,..., Xn}, where each xi is a vector with dimension s in space X. For each boosting iteration, the negative gradients of the loss function with respect to the output of the model are denoted as {91,..., gn}. The DT model splits each node to maximize information gain, which is measured by the variance after splitting. For a training set O on a fixed node, the gain of splitting (V) feature j at point d is defined as:\n$V_{jo}(d) = \\frac{1}{n_o}(\\frac{n_{lo}(d)}{n_o(d)}^2 + \\frac{(\\Sigma_{\\{x_i \\in O:x_{ij} \\leq d\\}}g_i)^2}{n_o(d)} + \\frac{(\\Sigma_{\\{x_i \\in O:x_{ij}>d\\}}g_i)^2}{n_o(d)})$  (1)\nThe problem however arises in practice since the optimiza-tion is constrained so that the left no(d) and right no(d) nodes have a minimum sample size. \u00c1 segment of LightGBM implementation is shown in Algorithm 1.\nThe minimum samples per leaf then becomes a blocker for FSL, causing the algorithm to stall. Unable to perform any split until training samples exceeds the min_samples_leaf param-eter. Although previous works [43] have explored parameter tuning based on literature recommendations [12], [19], this is not being addressed, and as a result LightGBM shows random-guess performance (e.g. 0.5 AUC) in most experiments, since the default value for min_samples_leaf is set to 20.\nIn this work we propose a LightGBM configuration specif-ically for FSL applications. We identified key parameters needed as shown in Table III.\nThe most important parameter for FSL is, without a doubt, min_data_in_leaf, as otherwise optimization cannot happen."}, {"title": "IV. EXPERIMENTS", "content": "Our experiment design covers two folds. First, we replicate previous work [43], but apply our recommended methodology to enable efficient FSL for LightGBM. Second, we bring a practical application to incorporate FSL into larger-scale data, this serves as reference that even if samples are vast, FSL can provide benefits."}, {"title": "A. TabLLM Experiment Replication", "content": "Both TabPFN and TabLLM show similar performance in average. Only a marginal improvement of 1% in favor of TabPFN, however, both of those solutions outperform Light-GBM over 343% in average, with extreme cases such as Credit-g where the relative performance of TabLLM is 745% better. While we were able to validate these numbers are correct, our results show this extreme underperformance is driven due to incorrect parameters.\nWe have replicated the binary problems. For the sake of simplicity, our LightGBM does not include hyperparameter tuning and instead executed with our fixed recommended parameters as shown in Table III. This leads to intentional underoptimization to disregard the effect of better tuning in the results. We found LightGBM much more competitive as seen in Table IV."}, {"title": "B. FedCSIS 2024 Data Science Challenge", "content": "To further review performance and applications of FSL, we applied our findings to the FedCSIS 2024 Data Science Challenge hosted in the KnowledgePit platform, a web system designed for ML competitions helping to bring collaboration between industry and academia [48].\nThe challenge: Predicting Stock Trends, provides stock-tickers and their performance as measured by 116 financial-markers, such as: Dividend Payout Ratio, Gross Profit Margin, and Price to Total Revenue per Share. The information is provided for current Trailing Twelve Months (TTM), these are static features, named 11 to 158. Another set, known as relative-features, named dll to dI58 provide the relative 1-yr change for such indicators.\nThis is a competition event that promotes an objective evaluation of performance. Participants were asked to predict the optimal investment strategy of securities among 3 actions: buy, hold or sell. An in-depth review of the competition is detailed in [49].\nInitial Model: In order to establish a baseline we started our simplest possible solution directly with DT, this due to its usual superiority over other algorithms for tabular data that has not been deeply feature engineered [50]. A LightGBM regression model using all features as-is and the original discrete target \"Class\" achieves 0.8439 mean absolute error (MAE). The first insight came from feature importance, which suggests the relative (dI*) variables far dominate the static set (I*) as seen in Table V, taking 4 out of the top 5 spots. This inspired further review to enhance generalization given the limited data size.\nSample and Feature Selection: Following Occam's Ra-zor principle, we challenged the value of the static features (I*). When using all variables it's possible to get 0.6018 AUC, an alternate variant for diversification would be to use relative-features (dI*) only, this proves to be quite competitive, retaining 95% of predictive power (0.5963 AUC), with a 50% reduction of features. This is important since the large feature mismatch promotes orthogonal decisions boundary for subsequent ensembling techniques.\nAnother diversification technique comes from instance sam-pling. We studied the sample-size vs performance in the same binary case to determine the right number of shots to use, ideally the smaller the better for diversification in further stages. Results are provided in Table VI where we can observe even after a 40% sample size reduction (6864 to 4118) there is zero impact in performance, and reducing further brings minimal degradation, this provides an ideal framework for FSL, as the ability to use few samples allows for stacking level-0 models with non-overlapping samples.\nStacking Level-0 Models: Based on previous insights, we determined that FSL is a viable strategy to enable multiple orthogonal models. Although previous analysis was done in a binary setting, these new models are built with the Perform target in the dataset. Unlike the discrete buy/hold/sell, this continuous representation allows the model to understand the impact of each action, e.g. not all \"buys\" are equal, since they provide different levels of financial gain/loss. Using a 3k shot-approach per model we forced diversification in the sample space. In order to improve generalization, we used the learnings that ExtraTrees outperforms GBDT in most FSL settings. We did not create any feature engineering, but our Base Feature Set is a concatenation of existing features over multiple years for stock-tickers that are present more than once in the dataset, only relative features (dI*) are used. The details of each model and their respective performance is shown in Table VII. Note that because we switched to Perform as target, MAE is no longer optimal, so we optimized for the mean squared error (MSE) instead.\nFinal Blend: Our Level-1 Meta model is fed with the five"}, {"title": "V. CONCLUSIONS", "content": "When the merit of a proposal is measured by its relative performance to a baseline, the baseline itself is equally, or even more important than the proposal. It is trivial to show a solution is good by simply selecting a weak reference point to compare with. Efforts invested in a new proposal can also be applied to improve a baseline. In this work we have improved LightGBM FSL performance found in literature by 290%. Improvements of this magnitude are unusual with just parameter optimization.\nOur results show GBDT can perform few-shot-learning (FSL) with surprising performance with as little as 8-shots. And when data is available, FSL can be used to force diver-sification between individual models in ensemble or stacking architectures."}]}