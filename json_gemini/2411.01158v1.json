{"title": "Pin-Tuning: Parameter-Efficient In-Context Tuning\nfor Few-Shot Molecular Property Prediction", "authors": ["Liang Wang", "Qiang Liu", "Shaozhen Liu", "Xin Sun", "Shu Wu", "Liang Wang"], "abstract": "Molecular property prediction (MPP) is integral to drug discovery and material\nscience, but often faces the challenge of data scarcity in real-world scenarios. Ad-\ndressing this, few-shot molecular property prediction (FSMPP) has been developed.\nUnlike other few-shot tasks, FSMPP typically employs a pre-trained molecular\nencoder and a context-aware classifier, benefiting from molecular pre-training\nand molecular context information. Despite these advancements, existing methods\nstruggle with the ineffective fine-tuning of pre-trained encoders. We attribute this is-\nsue to the imbalance between the abundance of tunable parameters and the scarcity\nof labeled molecules, and the lack of contextual perceptiveness in the encoders. To\novercome this hurdle, we propose a parameter-efficient in-context tuning method,\nnamed Pin-Tuning. Specifically, we propose a lightweight adapter for pre-trained\nmessage passing layers (MP-Adapter) and Bayesian weight consolidation for pre-\ntrained atom/bond embedding layers (Emb-BWC), to achieve parameter-efficient\ntuning while preventing over-fitting and catastrophic forgetting. Additionally, we\nenhance the MP-Adapters with contextual perceptiveness. This innovation allows\nfor in-context tuning of the pre-trained encoder, thereby improving its adaptabil-\nity for specific FSMPP tasks. When evaluated on public datasets, our method\ndemonstrates superior tuning with fewer trainable parameters, improving few-shot\npredictive performance.", "sections": [{"title": "1 Introduction", "content": "In the field of drug discovery and material science, molecular property prediction (MPP) stands as a\npivotal task [5, 9, 63]. MPP involves the prediction of molecular properties like solubility and toxicity,\nbased on their structural and physicochemical characteristics, which is integral to the development\nof new pharmaceuticals and materials. However, a major challenge encountered in real-world MPP\nscenarios is data scarcity. Obtaining extensive molecular data with well-characterized properties can\nbe time-consuming and expensive. To address this, few-shot molecular property prediction (FSMPP)\nhas emerged as a crucial approach, enabling predictions with limited labeled molecules [1, 41, 4].\nThe methodology for general MPP typically adheres to an encoder-classifier framework [71, 23, 27,\n56], as illustrated in Figure 2(a). In this streamlined framework, the encoder converts molecular"}, {"title": "2 Related work", "content": "Few-shot molecular property prediction. Few-shot molecular property prediction aims to accurately\npredict the properties of new molecules with limited training data [49]. Early research applied general\nfew-shot techniques to FSMPP. IterRefLSTM [1] is the pioneer work to leverage metric learning\nto solve FSMPP problem. Following this, Meta-GGNN [41] and Meta-MGNN [14] introduce\nmeta-learning with graph neural networks, setting a foundational framework that subsequent studies\nhave continued to build upon [39, 40, 4]. It is noteworthy that Meta-MGNN employs a pre-trained\nmolecular encoder [20] and achieves superior results through fine-tuning in the meta-learning process\ncompared to training from scratch. In fact, pre-trained graph neural networks [64, 36, 17, 54, 37] have\nshown promise in enhancing various graph-based downstream tasks [52, 13], including molecular\nproperty prediction [60, 62, 38, 72]. Recent efforts have shifted towards leveraging unique nature\nin FSMPP, such as the many-to-many relationships between molecules and properties arising from\nthe multi-labeled nature of molecules, often referred to as the molecular context. PAR [58] initially\nemploys graph structure learning [32, 55] to connect similar molecules through a homogeneous\ncontext graph. MHNfs [45] introduces a large-scale external molecular library as context to augment\nthe limited known information. GS-Meta [73] further incorporates auxiliary task to depict the\nmany-to-many relationships.\nParameter-efficient tuning. As pre-training techniques have advanced, tuning of pre-trained models\nhas become increasingly crucial. Traditional full fine-tuning approaches updates all parameters, often\nleading to high computational costs and the risk of over-fitting, especially when available data for\ndownstream tasks are limited [33, 15]. This challenge has led to the emergence of parameter-efficient\ntuning [26, 29, 34]. The philosophy of parameter-efficient tuning is to optimize a small subset of\nparameters, reducing the computational costs while retaining or even improving performance on\ndownstream tasks [19, 69]. Among the various strategies, the adapters [18, 42, 59] have gained\nprominence. Adapters are small modules inserted between the pre-trained layers. During the tuning\nprocess, only the parameters of these adapters are updated while the rest remains frozen, which not\nonly improves tuning efficiency but also offers an elegant solution to the generalization [70, 30, 8].\nBy keeping the majority of the pre-trained parameters intact, adapters preserve the rich pre-trained\nknowledge. This attribute is particularly valuable in many real-world applications including FSMPP."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Problem formulation", "content": "Let {T} be a collection of tasks, where each task T involves the prediction of a property p. The\ntraining set comprising multiple tasks {Ttrain}, is represented as $D_{train} = {(m_i, Y_{i,t})|t\\in {T_{train}}}$,\nwith $m_i$ indicating a molecule and $Y_{i,t}$ its associated label for task t. Correspondingly, the test set\n$D_{test}$, formed by tasks ${T_{test}}$, ensures a separation of properties between training and testing phases,\nas the property sets ${p_{train}}$ and ${P_{test}}$ are disjoint (${P_{train}} \\cap {P_{test}} = (\\emptyset)$.\nThe goal of FSMPP is to train a model using $D_{train}$ that can accurately infer new properties from a\nlimited number of labeled molecules in $D_{test}$. Episodic training has emerged as a promising strategy\nin meta-learning [10, 16] to deal with few-shot problem. Instead of retaining all ${T_{train}}$ tasks in\nmemory, episodes ${E_t}_{t=1}^B$ are iteratively sampled throughout the training process. For each episode\n$E_t$, a particular task $T_e$ is selected from the training set, along with corresponding support set $S_t$ and\nquery set $Q_t$. Typically, the prediction task involves classifying molecules into two classes: positive\n(y = 1) or negative (y = 0). Then a 2-way K-shot episode $E_t = (S_t, Q_t)$ is constructed. The\nsupport set $S_t = {(m, y_{i,t})}_{i=1}^{2K}$ includes 2K examples, each class contributing K molecules. The\nquery set containing M molecules is denoted as $Q_t = {(m, y_{i,t})}_{i=1}^M$."}, {"title": "3.2 Encoder-classifier framework for FSMPP", "content": "Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in Figure 2(a),\ngiven a molecule m whose property need to be predicted, a molecular encoder f (\u00b7) first learns the\nmolecule's representation based on its structure, i.e., $h_m = f(m) \\in R^d$. The molecule m is generally\nrepresented as a graph m = (V, A, X, E), where V denotes the nodes (atoms), A represents the\nadjacent matrix defined by edges (chemical bonds), and X, E denote the original feature of atoms"}, {"title": "3.3 Pre-trained molecular encoders (PMEs)", "content": "Due to the scarcity of labeled data in molecular tasks, molecular pre-training has emerged as a\ncrucial area, which involves training encoders on extensive molecular datasets to extract informative\nrepresentations. Pre-GNN [20] is a classic pre-trained molecular encoder that has been widely used\nin addressing FSMPP tasks [14, 58, 73]. The backbone of Pre-GNN is a modified version of Graph\nIsomorphism Network (GIN) [65] tailored to molecules, which we call GIN-Mol, consisting of\nmultiple atom/bond embedding layers and message passing layers.\nAtom/Bond embedding layers. The raw atom features and bond features are both categorical vectors,\ndenoted as $(i_{v,1}, i_{v,2},..., i_{v,|E_n|})$ and $(j_{e,1}, j_{e,2},..., j_{e,|E_e|})$ for atom v and bond e, respectively.\nThese categorical features are embedded as:\n$h_v^{(0)} = \\sum_{a=1}^{E_n} EmbAtom_a(i_{v,a}), h_e^{(1)} = \\sum_{b=1}^{E_e} EmbBond_b(j_{e,b}),$ (1)\nwhere $EmbAtoma(\\cdot)_{a\\in{1,...,|E_n|}}$ and $EmbBonds(\\cdot)_{b\\in{1,...,|E_e|}}$ represent embedding operations that\nmap integer indices to d-dimensional real vectors, i.e., $h_v^{(l)}, h_e^{(l)} \\in R^d$, $l\\in {0,1,..., L - 1}$\nrepresents the index of encoder layers, and L is the number of encoder layers. The atom embedding\nlayer is present only in the first encoder layer, while an bond embedding layer exists in each layer.\nMessage passing layers. At the l-th encoder layer, atom representations are updated by aggregating\nthe features of neighboring atoms and chemical bonds:\n$h_v^{(l)} = ReLU \\left( MLP^{(l)} \\left( h_v^{(l-1)} + \\sum_{u \\in \\mathcal{N}(v) \\cup {v}} h_u^{(l-1)} + \\sum_{e=(v,u)} h_e^{(l-1)} \\right) \\right),$ (2)\nwhere $u \\in N(v) \\cup {v}$ is the set of atoms connected to v, and $h_v^{(l)} \\in R^d$ is the learned representation\nof atom v at the l-th layer. MLP(\u00b7) is implemented by 2-layer neural networks, in which the hidden\ndimension is $d_1$. After MLP, batch normalization is applied right before the ReLU. The molecule-level\nrepresentation $h_m \\in R^d$ is obtained by averaging the atom representations at the final layer."}, {"title": "4 The proposed Pin-Tuning method", "content": "This section delves into our motivation and proposed method. Our framework for FSMPP is depicted\nin Figure 2(c). The details of our principal design, Pin-Tuning for PMEs, is present in Figure 2(d).\nAs shown in Figure 1, pretraining then finetuning molecular encoders is a common approach. However,\nfully fine-tuning yields results inferior to simply freezing them. Thus, the following question arises:\nHow to effectively adapt pre-trained molecular encoders to downstream tasks, especially in\nfew-shot scenarios?\nWe analyze the reasons of observed ineffective fine-tuning issue, and attribute it to two primary\nfactors: (i) imbalance between the abundance of tunable parameters and the scarcity of labeled\nmolecules, and (ii) limited contextual perceptiveness in the encoder."}, {"title": "4.1 Parameter-efficient tuning for PMES", "content": "To address the first cause of observed ineffective tuning, we reform the tuning method for PMEs.\nInstead of conducting full fine-tuning for all parameters, we propose tuning strategies specifically\ntailored to the message passing layers and embedding layers in PMEs, respectively."}, {"title": "4.1.1 MP-Adapter: message passing layer-oriented adapter", "content": "For message passing layers in PMEs, the number of parameters is disproportionately large compared\nto the training samples. To mitigate this imbalance, we design a lightweight adapter targeted at the\nmessage passing layers, called MP-Adapter. The pre-trained parameters in each message passing\nlayer include parameters in the MLP and the following batch normalization. We freeze all pre-trained\nparameters in message passing layers and add a lightweight trainable adapter after MLP in each\nmessage passing layer. Formally, the adapter module for l-th layer can be represented as:\n$z^{(l)} = FeedForward_{down}(h_v^{(l)}) \\in R^{d_2},$ (3)\n$\\Delta h_v^{(l)} = FeedForward_{up}(\\phi(z^{(l)})) \\in R^{d},$ (4)\n$h_v^{(l)} = LayerNorm(h_v^{(l)} + \\Delta h_v^{(l)}) \\in R^{d},$ (5)\nwhere FeedForward(\u00b7) denotes feed forward layer and LayerNorm(\u00b7) denotes layer normalization.\nTo limit the number of parameters, we introduce a bottleneck architecture. The adapters downscale\nthe original features from d dimensions to a smaller dimension $d_2$, apply nonlinearity \u03c6, then upscale\nback to d dimensions. By setting $d_2$ smaller than d, we can limit the number of parameters added. The\nadapter module has a skip-connection internally. With the skip-connection, we adopt the near-zero\ninitialization for parameters in the adapter modules, so that the modules are initialized to approximate\nidentity functions. Therefore, the encoder with initialized adapters is equivalent to the pre-trained\nencoder. Furthermore, we add a layer normalization after skip-connection for training stability."}, {"title": "4.1.2 Emb-BWC: embedding layer-oriented Bayesian weight consolidation", "content": "Unlike message passing layers, embedding layers contain fewer parameters. Therefore, we directly\nfine-tune the parameters of the embedding layers, but impose a constraint to limit the magnitude of\nparameter updates, preventing aggressive optimization and catastrophic forgetting.\nThe parameters in an embedding layer consist of an embedding matrix used for lookups based on the\nindices of the original features. We stack the embedding matrices of all embedding layers to form\n$\\Phi \\in R^{E\\times d}$, where E represents the total number of lookup entries. Further, $\\Phi_i \\in R^d$ denotes the i-th\nrow's embedding vector, and $\\Phi_{i,j} \\in R$ represents the j-th dimensional value of $\\Phi_i$.\nTo avoid aggressive optimization of \u03a6, we derive a Bayesian weight consolidation framework tailored\nfor embedding layers, called Emb-BWC, by applying Bayesian learning theory [3] to fine-tuning.\nProposition 1: (Emb-BWC ensures an appropriate stability-plasticity trade-off for pre-trained em-\nbedding layers.) Let $\\Phi \\in R^{E\\times d}$ be the pre-trained embeddings before fine-tuning, and $\\Phi' \\in R^{E\\times d}$\nbe the fine-tuned embeddings. Then, the embeddings can both retain the atom and bond properties"}, {"title": "4.2 Enabling contextual perceptiveness in MP-Adapter", "content": "For different property prediction tasks, the decisive substructures vary. As shown in Figure 2, the\nester group in the given molecule determines the property SR-HSE, while the carbon-carbon triple\nbond determines the property SR-MMP. If fine-tuning can be guided by molecular context, encoding\ncontext-specific molecular representations allows for dynamic representations of molecules tailored\nto specific tasks and enables the modeling of the context-specific significance of substructures.\nExtracting molecular context information. In\neach episode, we consider the labels of the sup-\nport molecules on the target property and seen\nproperties, as well as the labels of the query\nmolecules on seen properties, as the context of\nthis episode. We adopt the form of a graph to\ndescribe the context. Figure 3 demonstrates the\ntransformation from original context data to a\ncontext graph. In the left table, the labels of\nmolecules $m_1, m_2$ for property pt are the pre-\ndiction targets, and the other shaded values are\nthe available context. The right side shows the\ncontext graph constructed based on the available context. Specifically, we construct context graph\n$G_t = (V_t, A_t, X_t)$ for episode $E_t$. It contains M molecule nodes ${m}$ and P property nodes ${p}$.\nThree types of edges indicate different relationships between molecules and properties.\nThen we employ a GNN-based context encoder: $C = ContextEncoder(V_t, A_t, X_t)$, where $C \\in\nR^{(M+P)\\times d_2}$ denotes the learned context representation matrix for $E_t$. $V_t$ and $A_t$ denote the node set"}, {"title": "4.3 Optimization", "content": "Following MAML [10], a gradient descent strategy is adopted. Firstly, B episodes ${E_t}_{t=1}^B$ are\nrandomly sampled. For each episode, in the inner-loop optimization, the loss on the support set is\ncomputed as $L_{ccls}^{t,S}(f_\\theta)$ and the parameters \u03b8 are updated by gradient descent:\n$L_{ccls}^{t,S}(f_\\theta) = - \\sum_{S_t} (y\\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})),$ (8)\n$\\theta' \\leftarrow \\theta - \\alpha_{inner} \\nabla_{\\theta} L_{ccls}^{t,S}(f_\\theta),$ (9)\nwhere $\\alpha_{inner}$ is the learning rate. In the outer loop, the classification loss of query set is denoted as\n$L_{ccls}^{t,Q}$. Together with our Emb-BWC regularizer, the meta-training loss $\\mathcal{L}(f_{\\theta'})$ is computed and we do\nan outer-loop optimization with learning rate $\\alpha_{outer}$ across the mini-batch:\n$\\mathcal{L}(f_{\\theta'}) = \\frac{1}{B} \\sum_{t=1}^{B} L_{ccls}^{t,Q}(f_{\\theta'}) + \\lambda L_{Emb-BWC},$ (10)\n$\\theta \\leftarrow \\theta - \\alpha_{outer} \\nabla_{\\theta'} \\mathcal{L}(f_{\\theta'}),$ (11)\nwhere $\\lambda$ is the weight of Emb-BWC regularizer. The pseudo-code is provided in Appendix B. We also\nprovide more discussion of tunable parameter size and total model size in Appendix C."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Evaluation setups", "content": "Datasets. We use five common few-shot molecular property prediction datasets from the Molecu-\nleNet [61]: Tox21, SIDER, MUV, ToxCast, and PCBA. Standard data splits for FSMPP are adopted.\nDataset statistics and more details of datasets can be found in Appendix D."}, {"title": "5.2 Performance comparison", "content": "We compare Pin-Tuning with the baselines and the results are summarized in Table 1, Table 7, and\nTable 8. Our method significantly outperforms all baseline models under both the 10-shot and 5-shot\nsettings, demonstrating the effectiveness and superiority of our approach.\nAcross all datasets, our method provides greater improvement in the 10-shot scenario than in the\n5-shot scenario. This is attributed to the molecular context constructed based on support molecules.\nWhen there are more molecules in the support set, the uncertainty in the context is reduced, providing\nmore effective adaptation guidance for our parameter-efficient tuning.\nAmong benchmark datasets, our method shows significant improvement on the SIDER dataset,\nincreasing by 10.73% in the 10-shot scenario and by 8.81% in the 5-shot scenario. We consider this\nis related to the relatively balanced ratio of positive to negative samples, as well as the absence of\nmissing labels in the SIDER dataset (Table 5). A balanced and low-uncertainty distribution can better\nbenefit addressing the FSMPP task from our method.\nWe also observe that the standard deviations of our method's results under 10 seeds are slightly higher\nthan that of baseline models. However, our worst-case results are still better than the best baseline\nmodel. For example, in 10-shot experiments on the Tox21 dataset, the performance of our method is\n91.56 \u00b1 2.57. However, our 10 runs yield specific results with the worst-case ROC-AUC reaching\n88.02, which is also better than the best baseline model GS-Meta's result of 86.67\u00b10.41. Therefore,\na high standard deviation does not mean our method is inferior to baseline models."}, {"title": "5.3 Ablation study", "content": "For MP-Adapter, the main components consist of: (i) bottleneck adapter module (Adapter), (ii)\nintroducing molecular context to adatpers (Context), and (iii) layer normalization (LayerNorm). The\nresults of ablation experiments are summarized in Table 2. The bottleneck adapter and the modeling of\nmolecular context are the most critical, having the most significant impact on performance. Removing\nthem leads to a noticeable decline, which underscores the importance of parameter-efficient tuning\nand context perceptiveness in FSMPP tasks. Layer normalization is used to normalize the resulting\nrepresentations, which is also important for improving the optimization effect and stability."}, {"title": "5.4 Sensitivity analysis", "content": "Effect of weight of Emb-BWC regularizer \u5165. Emb-BWC is applied on the embedding layers to limit the\nmagnitude of parameter updates during fine-tuning. We vary the weight of this regularization \u5165 from\n{0.01, 0.1, 1, 10}. The first subfigure in Figure 4 shows that the performance is best when X = 0.1 or\n1. When A is too small, the parameters undergo too large updates on few-shot downstream datasets,\nleading to over-fitting and ineffectively utilizing the pre-trained knowledge. Too large A causes the\nparameters of the embedding layers to be nearly frozen, which prevents effective adaptation.\nEffect of hidden dimension of MP-Adapter d2. The results corresponding to different values of d2\nfrom {25, 50, 75, 100, 150} are presented in the second subfigure of Figure 4. On the Tox21 dataset,\nwe further analyze the impact of this hyper-parameter on the number of trainable parameters. As\nshown in Figure 5, the number of parameters that our method needs to train is significantly less than\nthat required by the full fine-tuning method, such as GS-Meta, while our method also performs better\nin terms of ROC-AUC performance due to solving over-fitting and context perceptiveness issues.\nWhen d = 50, Pin-Tuning performs best on Tox21, and the number of parameters that need to train is\nonly 14.2% of that required by traditional fine-tuning methods."}, {"title": "5.5 Case study", "content": "We visualized the molecular representations learned by the GS-Meta and our Pin-Tuning's encoders\nin the 10-shot setting, respectively. As shown in Figure 6 and 7, Pin-Tuning can effectively adapt\nto different downstream tasks based on context information, generating property-specific molecular\nrepresentations. Across different tasks, our method is more effective in encoding representations that"}, {"title": "6 Conclusion", "content": "In this work, we propose a tuning method, Pin-Tuning, to address the ineffective fine-tuning of\npre-trained molecular encoders in FSMPP tasks. Through the innovative parameter-efficient tuning\nand in-context tuning for pre-trained molecular encoders, our approach not only mitigates the issues\nof parameter-data imbalance but also enhances contextual perceptiveness. The promising results\non public datasets underscore the potential of Pin-Tuning to advance this field, offering valuable\ninsights for future research in drug discovery and material science."}, {"title": "A Derivation of Emb-BWC regularization", "content": ""}, {"title": "A.1 Derivation of LEmb-BWC", "content": "Let $\\Phi \\in R^{E\\times d}$ be the pre-trained embeddings before fine-tuning, and $\\Phi' \\in R^{E\\times d}$ be the fine-tuned\nembeddings. Further, $\\Phi_i \\in R^d$ denotes the i-th row's embedding vector in \u03a6, and $\\Phi_{i,j} \\in R$ represents\nthe j-th dimensional value of $\\Phi_i$.\nThe optimization of embedding layers can be interpreted as performing a maximum a posterior\n(MAP) estimation of the parameters \u03a6' given the pre-training data and training data of downstream\nFSMPP task, which is formulated in a Bayesian framework.\nIn the FSMPP setting, the molecular encoder has been pre-trained on the pre-training task P using\ndata Dp, and is then fine-tuned on a downstream FSMPP task F using data DF. The overall objective\nis to find the optimal parameters on task F while preserving the prior knowledge obtained in pre-\ntraining on task F. Based on a prior p(\u03a6') of the embedding parameters, the posterior after observing\nthe FSMPP task F can be computed with Bayes' rule:\n$p(\\Phi'|D_P, D_F) = \\frac{p(D_F|\\Phi', D_P)p(\\Phi'|D_P)}{p(D_F, D_P)} = \\frac{p(D_F|\\Phi')p(\\Phi'|D_P)}{p(D_F)},$ (12)\nwhere DF is assumed to be independent of Dp. Taking a logarithm of the posterior, the MAP\nobjective is therefore:\n$\\Phi'^* = arg\\underset{\\Phi'}{max} log p(\\Phi'|D_P, D_F)$\n$= arg\\underset{\\Phi'}{max} log p(D_F|\\Phi') + log p(\\Phi'|D_P).$ (13)\nThe first term log p(DF|\u03a6') is the log likelihood of the data DF given the parameters \u03a6', which\ncan be expressed as the training loss function on task F = - log p(DF|\u03a6'), denoted as $L_F(\\Phi')$.\nThe second term p(\u03a6'|Dp) is the posterior of the parameters given the pre-training dataset Dp.\nSince $\\Phi' = [\\Phi_1, \\Phi_2, ..., \\Phi_E]$, and \u03a6' is conditionally independent of \u03a6' for i, j = {1, . . ., E}\nand i \u2260 j given condition Dp, we have $p(\\Phi'|D_P) = \\Pi_{i=1}^E p(\\Phi_i|D_P)$. Thus, $log p(\\Phi'|D_P) =$\n$\\sum_{i=1}^E log p(\\Phi_i|D_P)$.\nFor adapting pre-trained molecular embedding layers to downstream FMSPP tasks, this posterior\nmust encompass the prior knowledge of the pre-trained embedding layers to reflect which parameters\nare important for pre-training task P. Despite the true posterior being intractable, log p(\u03a6'|Dp) can\nbe defined as a function f() and approximated around the optimum point f(\u03a6i), where f(\u03a6i)\nis the pre-trained values and \u2207f(\u03a6i) = 0. Performing a second-order Taylor expansion on f()\naround \u03a6i gives:\n$log p (\\Phi_i | D_P) \\approx f (\\Phi_i) + \\frac{1}{2} (\\Phi - \\Phi_i)^T \\nabla^2 f (\\Phi_i) (\\Phi - \\Phi_i)$\n$= f (\\Phi_i) + \\frac{1}{2} (\\Phi - \\Phi_i)^T H(D_P, \\Phi_i) (\\Phi - \\Phi_i),$ (14)"}, {"title": "A.2 Derivation of CEFIMEmb-BWC", "content": "Since the Fisher information matrix (FIM) F is the negation of the expectation of the Hessian over\nthe data distribution, i.e., F = \u2212ED [H], the objective can be reformulated as:\n$L_{Emb-BWC}^{CFIM} = \\frac{1}{2} \\sum_{i=1}^E (\\Phi - \\Phi_i)^T F(D_P, \\Phi_i) (\\Phi - \\Phi_i),$ (17)\nwhere F(Dp, \u03a6i) \u2208 Rd\u00d7d is the corresponding Fisher information matrix of H(Dp, \u03a6i). Further,\nthe Fisher information matrix can be further simplified with a diagonal approximation. Then, the\nobjective is simplified to:\n$L_{Emb-BWC}^{CFIM} \\approx \\frac{1}{2} \\sum_{i=1}^E \\sum_j F_i (\\Phi - \\Phi_i)^2,$ (18)\nwhere $F_i \\in R^d$ is the diagonal of F(Dp, \u03a6i). According to the definition of the Fisher information\nmatrix, the j-th value in Fi is computed as $E_{P_P} (\\frac{\\delta L_P}{\\delta \\Phi_{i,j}})^2$. In this work, this approximated form\nis defined as $\\mathcal{L}_{Emb-BWC}^{CEFIM}$."}, {"title": "A.3 Derivation of CEFIMEmb-BWC", "content": "We assume that the parameters within an embedding should share the same importance. To this\nend, we define $\\Phi_i = \\sum_j \\Phi_{i,j}$, then the total update of the embedding \u03a6i can be represented as\n$\\Delta \\Phi_i = \\Phi - \\Phi_i = \\sum_j (\\Phi'_{i,j} - \\Phi_{i,j})$. Then, the objective in Eq. (16) is reformulated to:\n$L_{Emb-EWC}^{CERI} = \\frac{1}{2} \\sum_{i=1}^E H_i (\\Phi - \\Phi_i)^2,$ (19)\nwhere $H_i = \\frac{\\partial^2 L_P}{\\partial \\Phi^2} = \\frac{\\partial}{\\partial \\Phi} (\\frac{\\partial L_P}{\\partial \\Phi})$. Next, we continue to derive $H_i$. Given that $\\Phi_i = \\sum_{j=1}^d \\Phi_{i,j}$, we\nfirst use the chain rule to find $\\frac{\\partial L_P}{\\partial \\Phi}$. According to chain rule, the derivative of Lp with respect to d\u03a6\ncan be computed as:\n$\\frac{\\partial L_P}{\\partial \\Phi} = \\sum_j \\frac{\\partial \\Phi_i}{\\partial \\Phi} \\frac{\\partial L_P}{\\partial \\Phi_{i,j}},$ (20)\nSince $\\Phi_i = \\Phi_{i,1} + \\Phi_{i,2} + ... + \\Phi_{i,d}$, each of $\\frac{\\partial \\Phi_{i,j}}{\\partial \\Phi}$ for j = 1, 2, ..., d equals 1. Therefore, the\nequation simplifies to:\n$\\frac{\\partial L_P}{\\partial \\Phi} = \\sum_j \\frac{\\partial L_P}{\\partial \\Phi_{i,j}},$ (21)"}, {"title": "B Pseudo-code of training process", "content": "To help better understand the training process, we provide the brief pseudo-code of it in Algorithm 1."}, {"title": "C Discussion of tunable parameter size and total model size", "content": ""}, {"title": "C.1 Tunable parameter size of molecular encoder", "content": "We compare the tunable parameter size of full fine-tuning and our Pin-Tuning. Section 3.3 describes\nthe parameters of the PME, which include those for the embedding layers and the message passing\nlayers. We assume there are | En | original node features and | Ee edge features. Considering there\nis one node embedding layer and L edge embedding layers, the total number of parameters for the\nembedding part is End + LEed. The parameters in the message passing layer consist of the\n2-layer MLP including biases shown in Eq. (2) and its subsequent batch normalization, with each\nlayer having L(2dd1 + d + d1 + 2d) parameters. In summary, the total number of parameters to\nupdate in full fine-tuning is\n$N_{Fine-Tuning} = |E_n|d + L(|E_e|d + 2dd_1 + 3d + d_1).$ (24)\nIn our Pin-Tuning method, the parameters of the embedding layers are still updated. However,\nin each message passing layer, the original parameters are completely frozen, and the parts that\nrequire updating are the two feed-forward layers and the layer normalization in the bottleneck adapter"}, {"title": "C.2 Total model size", "content": "We provide a comparison of total model size between our Pin-Tuning and the state-of-the-art\nbaseline method, GS-Meta. The total model size consists of both frozen parameters and trainable\nparameters. The results are presented in Table 4. The total size of our model is comparable to\nGS-Meta, but the number of parameters that need to be trained is far less than GS-Meta."}, {"title": "D Details of datasets", "content": "We carry out experiments in MoleculeNet benchmark [61", "datasets": "n\u2022 Tox21: This dataset covers qualitative toxicity measurements and was utilized in the 2014 Tox21\nData Challenge.\n\u2022 SIDER: The Side Effect Resource (SIDER) functions as a repository for marketed drugs and\nadverse drug reactions (ADR)", "MUV": "The Maximum Unbiased Validation (MUV) is determined through the application of a\nrefined nearest neighbor analysis, specifically designed for"}]}