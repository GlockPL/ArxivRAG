{"title": "Semantic Communication Enhanced by Knowledge Graph Representation Learning", "authors": ["Nour Hello", "Paolo Di Lorenzo", "Emilio Calvanese Strinati"], "abstract": "This paper investigates the advantages of represent- ing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts en- tities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.", "sections": [{"title": "I. INTRODUCTION", "content": "The recent advances in machine learning (ML) and Gener- ative artificial intelligence (AI) are offering new fundamental enablers and integral components to design 6G connect- compute intelligent ecosystems [1] and new knowledge-centric applications based on the share of knowledge between (nat- ural and/or artificial) intelligent agents. This has created the momentum for a new surge of interest in the paradigms of semantic and goal-oriented communications [2]-[4]. Semantic communication transcends the paradigm of conventional com- munication by prioritizing the conveyance of semantic content over mere bit stream transmission. Indeed, while traditional communication systems are concerned with the technical tier of message exchange, semantic communication endeavors to operate within the semantic tier, encoding the meaning of information at the semantic encoder and interpreting an equivalent matching meaning at the semantic decoder [5]. This approach to communication notably enhances the efficiency of distilling data for transmission and it improves the resilience against noise of the communication process.\nThere are three key pillars for semantic communications. First, as introduced by [5], the pragmatic or goal-oriented communications, in which the exchange of information is directed towards achieving specific objectives or goals. Se- mantic communications can be considered goal-oriented when the goal of the communications is not just to transmit data but to convey meaning or knowledge, where correct inter- pretation, manipulation, or composition of meaning aligns with predefined goals [6]. To this end, several metrics have been proposed to evaluate the repercussions of semantic in- congruences on the efficacy of decision-making processes, including the Age of Information (AoI) and the Value of In- formation (VoI) [3], [7]. Second, in semantic communications, achieving effective and reliable knowledge sharing necessitates optimizing the compression of knowledge representation to meet the reliability requirements of the application, as well as considering the context and goals of communication. In the literature, several solutions have been proposed to embed multi-modal data into a low-dimensional latent spaces, relying on deep learning techniques like transformers [8], generative adversarial networks [9], graph convolutional neural network [10], LLMs [11], just to name a few. The optimization of latent space compression should also ensure resistance against wireless and semantic noise, or mismatches between sources and destinations [12], as well as potential misalignment in the representation of (high-dimensional) latent spaces [13]. This challenge differs from the extensively investigated mere semantic compression at semantic symbols level and semantic distillation [14]. We propose in this paper to approach it by using the cascade of LLMs and GNNs as semantic encoders. Third, the transmitter(s) and the receiver(s) are required to share and update a mutual knowledge base between them. This calls for a new AI-native semantic architecture where the semantic plane, semantic RAN intelligent controllers (S-RIC), and semantic reasoning engine units are defined enabling new knowledge-centric applications [15].\nIn this paper, we focus on graph-based pragmatic semantic communications. Information extracted from data is seman- tically represented in knowledge graphs, formed by nodes (entities) and edges (relationships). Knowledge graphs pro- vide structured representations of knowledge, organized into triples in the form of node-relationship-node, enabling the incorporation of semantic meaning into data. Thus, knowledge is structured and semantic messages selected, pragmatically"}, {"title": "II. SYSTEM MODEL", "content": "The proposed architecture is described in Fig. 1. A knowl- edge graph is denoted as $G = (N,E)$, where $N = {n_i}_{i=1}^{N_e}$ represents the set of nodes, where $n_i$ represents the textual attribute of the node with index i and $E = {r_{ij}}_{i,j=1}^{N_e}$ embodies the relational structure amongst these nodes. Here, $r_{ij}$ signifies the relational attribute of the link directed from the source node of index i to the target node of index j. The parameter $N_e$ specifies the total number of nodes within the graph. The essential blocks of the system model are illustrated in the sequel.\n\nA. Semantic Encoders\nThe semantic encoder associates the input knowledge graph g with an array of low-dimensional embeddings $x' = {x_i}_{i=1}^{N_e}$, such that each node of index i of the source knowledge graph g is associated with an embedding vector $x_i$. We propose two semantic encoders named Enc\u0131lm,gnn and Enc\u0131lm,ffn, respec- tively. In particular, Enc\u0131\u0131m,gnn cascades a pretrained LLM (Fig. 1, block (a)) with a GNN (Fig. 1, block (b)). The LLM is adept at distilling textual features, consequently, we apply these models to synthesize the initial feature vectors pertinent to the textual attributes associated with nodes and relations of the knowledge graph. Those vectors constitute the preliminary node and relations feature vectors within GNN. Subsequently, the GNN is employed to synthesize node representations. This culminating representation is an amalgamation of textual and relational information pertinent to the node, encapsulating a comprehensive semantic representation. To be more specific, we have the following relations:\n$x_i = LLM(n_i)$, $e_{j,i} = LLM(r_{j,i})$, $x'_i = GNN(X_i, E_i)$.\nHere, the LLM associates the initial features vectors $x_i$, and $e_{j,i}$ respectively with the descriptors of the node $n_i$ and the relation $r_{j,i}$. Subsequently, the GNN generates a compact representation vector of the node of index i by fusing $X_i$ with $E_i$, respective to the initial features of the neighboring nodes connected to the node i, and the initial features of the relations that culminate at the node i. The intricate architecture of a knowledge graph, characterized by its attributed vertices and edges, necessitates the deployment of a graph neural network that must be adept at accommodating the inherent heterogene- ity of the vertices, the directionality of the edges, and the rich feature representation of the edges. In this context, we exploit the graph isomorphism convolutional neural network in [22], which reads as:\n$x'_i = h_{\\Theta}((1+\\epsilon) \\cdot x_i + \\sum_{j \\in N(i)} ReLU(x_j + e_{j,i}))$\nHere, $x'$ is the output features vector of node i, $h_{\\Theta}$ is a sequential neural network, and $\\epsilon$ is a trainable value. $x_i$ is the node i's initial features vector, and $e_{j,i}$ are the features of the relational edge linking node j to node i.\nThe second proposed encoder is denoted as Enc\u0131lm,ffn, and is composed by the cascade of a LLM with a feed-forward neural network (FFN) bottleneck (Fig. 1, block (c)). The aim of the FFN bottleneck step is to perform compression of the initial feature vector associated by the LLM to each node, but without taking into consideration the associative relationships in the knowledge graph. This aspect makes this encoder substantially different from Enc\u0131\u0131m,gnn. Mathematically, the encoder produces the following vectors:\n$x_i = LLM(n_i)$, $x'_i = FFN(x_i)$."}, {"title": "B. Channel Coding", "content": "The channel encoder consists of a FFN and a power normalization layer, modulating each low-dimensional vector $x$ from the semantic encoder into k complex symbols. The channel decoder, using an FFN architecture, demodulates the received symbols back into the set of latent space vectors $y = {y_i}_{i=1}^{N_e}$. We use mutual information to determine the optimal channel encoding function (Fig. 1, block (d)) for an Additive White Gaussian Noise (AWGN) channel. The mutual information between the channel's input and output is approximated using the Mutual Information Neural Estimation (MINE) [23] framework."}, {"title": "C. Semantic Decoder", "content": "At the semantic decoder, the received message \u011d is decoded from a set of embedding vectors $y = {y_i}_{i=1}^{N_e}$ to infer a knowl- edge graph congruent with the one intended by the source. To infer the knowledge graph, we combine two decoding functions which separately classify the graph's nodes (Fig. 1, block (e)) and their relations (Fig. 1, block (f)).\n\na) Nodes Classifier: The node classification mechanism employs a deep architecture to categorize $N_e$ distinct decoded embedding vectors, attributing each to a specific node (i.e., semantic concept). In formulas, we have:\n$z_i = MLP(y_i)$\n$n_i = argmax_{i=1,...,|E|} Softmax(z_i)$\nAs delineated in (4), the architecture of the node decoder is modeled after a multi-layered perceptron (MLP) with skip connections, to associate a node with its attributed type over the set of nodes specified as E."}, {"title": "b) Relation Classifier", "content": "The relation classifier works as a discerning mechanism to reconstruct the edges within the knowledge graph. It achieves this by utilizing node represen- tations obtained from the graph encoder. Within this structure, combinations of embeddings are processed to associate com- mensurate relations. Specifically, we have the relations:\n$(y'_i, y'_j) = TransformerEncoder(y_i, y_j)$,\n$r_{ij} = argmax_{k=1,...,|R|} Softmax (FC(y'_{ij}))$ .\nOverall, the relation decoder comprises a transformer encoder encapsulating self-attention heads, and feed-forward neural network transformation [24]. The transformer encoder takes the input vectors $y_i$ and $y_j$ associated with a combination of nodes of indices i and j, and outputs new embedding vectors $y'_i$ and $y'_j$, which are subsequently input into a classification head (FC), that determines the relation between the examined combination through the corpus of relations designated by R. This corpus includes the \"none\" relation indicating the absence of a direct relation between two nodes."}, {"title": "D. Knowledge Graph Representation Learning", "content": "Consider $\\theta$ and $\\vartheta$ as the respective trainable parameters of the semantic encoder and the semantic decoder. Drawing upon the insights of [8], the loss function used to train the system under consideration writes as:\n$\\mathcal{L} = \\mathbb{E}_{g \\sim p_{data} \\int_{0}^{1}} [CE] - \\lambda I(X; Y)$\nThe function encapsulates the cross-entropy term augmented with a penalty parameter considering a weighted mutual information component, which reflects the interdependence between variables X and Y across the semantic channel from transmission to reception. The random variable g, intrinsic to a knowledge graph, manifests as the joint random variable outlining the stochastic properties of two independent random variables n and r: the first-mentioned representing the nodes, and the second-mentioned standing for the relations. Over- all, we have two parallel components: one dedicated to the"}, {"title": "III. NUMERICAL RESULTS", "content": "We assess the performance of the proposed graph-based E2E wireless semantic communication system via numerical simulation. On the transmitter side, knowledge graphs of Ne nodes are encoded into Ne vectors of semantic embeddings, then each embedding vector is modulated into k complex symbols to be transmitted over the AWGN channel. We compare the two encoders design which are detailed in section II-A: Enc\u0131lm,gnn and Encilm,fin. At the receiver, knowl- edge graphs are inferred from the received (noisy) semantic symbols. We assess the decoder's fidelity with the F1 score metric. The F1 score measures the average equivalence of nodes and relations between the transmitted and the decoded knowledge graphs by checking the exact match of the inferred triple elements with the source triple elements. We compute the F1 score metric, utilizing the knowledge graphs from the WebNLG Project [25]. Training of the E2E system is performed at a reference signal-to-noise ratio (SNR) of 14 dB and a batch size of 8 graphs separately for both encoder designs. We use the pre-trained \"all-MiniLM-L12-v2\" LLM [26], which produces feature vectors of size 384.\nIn Fig. 2 we compare the performance in terms of node classification accuracy versus the embedding compression at the output of the encoders under noiseless wireless channel"}, {"title": "IV. CONCLUSION", "content": "In this paper, we investigate the effectiveness of LLM and GNN based encoders for semantic compression of knowledge graphs. To this end, we propose an end-to-end graph-based semantic communications framework. The goal is to embed"}]}