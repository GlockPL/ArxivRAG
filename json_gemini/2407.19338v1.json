{"title": "Semantic Communication Enhanced by Knowledge Graph Representation Learning", "authors": ["Nour Hello", "Paolo Di Lorenzo", "Emilio Calvanese Strinati"], "abstract": "This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.", "sections": [{"title": "I. INTRODUCTION", "content": "The recent advances in machine learning (ML) and Gener-ative artificial intelligence (AI) are offering new fundamental enablers and integral components to design 6G connect-compute intelligent ecosystems [1] and new knowledge-centric applications based on the share of knowledge between (nat-ural and/or artificial) intelligent agents. This has created the momentum for a new surge of interest in the paradigms of semantic and goal-oriented communications [2]-[4]. Semantic communication transcends the paradigm of conventional com-munication by prioritizing the conveyance of semantic content over mere bit stream transmission. Indeed, while traditional communication systems are concerned with the technical tier of message exchange, semantic communication endeavors to operate within the semantic tier, encoding the meaning of information at the semantic encoder and interpreting an equivalent matching meaning at the semantic decoder [5]. This approach to communication notably enhances the efficiency of distilling data for transmission and it improves the resilience against noise of the communication process.\nThere are three key pillars for semantic communications. First, as introduced by [5], the pragmatic or goal-oriented communications, in which the exchange of information is directed towards achieving specific objectives or goals. Semantic communications can be considered goal-oriented when the goal of the communications is not just to transmit data but to convey meaning or knowledge, where correct inter-pretation, manipulation, or composition of meaning aligns with predefined goals [6]. To this end, several metrics have been proposed to evaluate the repercussions of semantic in-congruences on the efficacy of decision-making processes, including the Age of Information (AoI) and the Value of In-formation (VoI) [3], [7]. Second, in semantic communications, achieving effective and reliable knowledge sharing necessitates optimizing the compression of knowledge representation to meet the reliability requirements of the application, as well as considering the context and goals of communication. In the literature, several solutions have been proposed to embed multi-modal data into a low-dimensional latent spaces, relying on deep learning techniques like transformers [8], generative adversarial networks [9], graph convolutional neural network [10], LLMs [11], just to name a few. The optimization of latent space compression should also ensure resistance against wireless and semantic noise, or mismatches between sources and destinations [12], as well as potential misalignment in the representation of (high-dimensional) latent spaces [13]. This challenge differs from the extensively investigated mere semantic compression at semantic symbols level and semantic distillation [14]. We propose in this paper to approach it by using the cascade of LLMs and GNNs as semantic encoders. Third, the transmitter(s) and the receiver(s) are required to share and update a mutual knowledge base between them. This calls for a new AI-native semantic architecture where the semantic plane, semantic RAN intelligent controllers (S-RIC), and semantic reasoning engine units are defined enabling new knowledge-centric applications [15].\nIn this paper, we focus on graph-based pragmatic semantic communications. Information extracted from data is seman-tically represented in knowledge graphs, formed by nodes (entities) and edges (relationships). Knowledge graphs pro-vide structured representations of knowledge, organized into triples in the form of node-relationship-node, enabling the incorporation of semantic meaning into data. Thus, knowledge is structured and semantic messages selected, pragmatically aligning with the source's intent or the destination's objectives and context, or serving a specific goal [15]. Recent works propose semantic communication systems based on the knowl-edge graph where semantic symbols are represented in the form of node-relationship-node triples for semantic extraction and interpretation [16]\u2013[19]. Those works mainly attempt to enhance accuracy, dispel interpretation ambiguities, improve data compression efficacy at the source, and enhance error correction robustness at the destination. Notably, [17] delin-eates an approach where transmission is adaptively modulated in response to channel quality, with a priority transmission accorded to semantically significant triples. Concurrently, [19] introduces a framework for cognitive semantic communication tailored to both single and multiple-user scenarios.\nIn the current state of the art, when semantic information is extracted in the form of Knowledge graphs, triples in the graphs are associated with semantic symbols to be transmitted [16]-[19]. Thus, after (graph-based) semantic extraction, trans-mission signs (or patterns of signs) are optimized for trans-mission (semiotic [20]), but without exploiting the embedded meaning in the semantic message which is captured in the graph structure. With this paper, we propose to exploit the properties of the specific instance of the extracted graph such as its structure and descriptors of relations between nodes.\nContribution. In this paper, we propose a novel end-to-end (E2E) pragmatic optimization semantic communications framework, optimizing the transmitter, the latent space repre-sentation and compression, and the receiver. This is done by representing semantic messages in the form of pragmatically sparsified knowledge graphs, being relevant to the receiver given the locally available knowledge. To learn the semantic relevant knowledge graph representation in lower-dimension latent spaces, we propose to cascade a pre-trained LLM module that associates embeddings to source data, with a GNN module that processes the combined output from the LLM module with the available inherent graphs topologies of the data. Such graphs are either available within the data set or can be extracted with LLM [21]. The proposed semantic architecture encodes the knowledge graph in a batch of vectors, each one containing the semantic information about a node, the nodes connected to it, and the relations connecting them. The cardinality of such a batch of vectors encoding the graph is equal to the number of its nodes. Through this methodology, we numerically illustrate enhancements in both compression rates and communication robustness."}, {"title": "II. SYSTEM MODEL", "content": "The proposed architecture is described in Fig. 1. A knowl-edge graph is denoted as G = (N,E), where N = {n_i\\}_{i=1}^{N_e} represents the set of nodes, where n_i represents the textual attribute of the node with index i and E = {r_{ij}\\}_{i=1}^{N_e} embodies the relational structure amongst these nodes. Here, r_{ij} signifies the relational attribute of the link directed from the source node of index i to the target node of index j. The parameter N_e specifies the total number of nodes within the graph. The essential blocks of the system model are illustrated in the sequel.\nSemantic Encoders\nThe semantic encoder associates the input knowledge graph g with an array of low-dimensional embeddings x' = {x'_i\\}_{i=1}^{N_e}, such that each node of index i of the source knowledge graph g is associated with an embedding vector x'. We propose two semantic encoders named Enc_{llm,gnn} and Enc_{llm,ffn}, respec-tively. In particular, Enc_{llm,gnn} cascades a pretrained LLM (Fig. 1, block (a)) with a GNN (Fig. 1, block (b)). The LLM is adept at distilling textual features, consequently, we apply these models to synthesize the initial feature vectors pertinent to the textual attributes associated with nodes and relations of the knowledge graph. Those vectors constitute the preliminary node and relations feature vectors within GNN. Subsequently, the GNN is employed to synthesize node representations. This culminating representation is an amalgamation of textual and relational information pertinent to the node, encapsulating a comprehensive semantic representation. To be more specific, we have the following relations:\nx_i = LLM(n_i), e_{j,i} = LLM(r_{ji}), x'_i = GNN(X_i, E_i).\nHere, the LLM associates the initial features vectors x_i, and e_{j,i} respectively with the descriptors of the node n_i and the relation r_{ji}. Subsequently, the GNN generates a compact representation vector of the node of index i by fusing X_i with E_i, respective to the initial features of the neighboring nodes connected to the node i, and the initial features of the relations that culminate at the node i. The intricate architecture of a knowledge graph, characterized by its attributed vertices and edges, necessitates the deployment of a graph neural network that must be adept at accommodating the inherent heterogene-ity of the vertices, the directionality of the edges, and the rich feature representation of the edges. In this context, we exploit the graph isomorphism convolutional neural network in [22], which reads as:\nx'_i = h_e ((1-\\epsilon) \\cdot x_i + \\epsilon \\cdot x_i + \\sum_{j \\in N(i)} ReLU(x_j + e_{j,i})).\nHere, x'_i is the output features vector of node i, h_e is a sequential neural network, and \\epsilon is a trainable value. x_i is the node i's initial features vector, and e_{j,i} are the features of the relational edge linking node j to node i.\nThe second proposed encoder is denoted as Enc_{llm,ffn}, and is composed by the cascade of a LLM with a feed-forward neural network (FFN) bottleneck (Fig. 1, block (c)). The aim of the FFN bottleneck step is to perform compression of the initial feature vector associated by the LLM to each node, but without taking into consideration the associative relationships in the knowledge graph. This aspect makes this encoder substantially different from Enc_{llm,gnn}. Mathematically, the encoder produces the following vectors:\nx_i = LLM(n_i), x'_i = FFN(x_i)."}, {"title": "B. Channel Coding", "content": "The channel encoder consists of a FFN and a power normalization layer, modulating each low-dimensional vector x' from the semantic encoder into k complex symbols. The channel decoder, using an FFN architecture, demodulates the received symbols back into the set of latent space vectors y = {y_i\\}_{i=1}^{N_e}. We use mutual information to determine the optimal channel encoding function (Fig. 1, block (d)) for an Additive White Gaussian Noise (AWGN) channel. The mutual information between the channel's input and output is approximated using the Mutual Information Neural Estimation (MINE) [23] framework."}, {"title": "C. Semantic Decoder", "content": "At the semantic decoder, the received message \\hat{g} is decoded from a set of embedding vectors y = {y_i\\}_{i=1}^{N_e} to infer a knowl-edge graph congruent with the one intended by the source. To infer the knowledge graph, we combine two decoding functions which separately classify the graph's nodes (Fig. 1, block (e)) and their relations (Fig. 1, block (f)).\nNodes Classifier: The node classification mechanism employs a deep architecture to categorize N_e distinct decoded embedding vectors, attributing each to a specific node (i.e., semantic concept). In formulas, we have:\nz_i = MLP(y_i)\n\\hat{n}_i = \\underset{i=1,...,|E|}{argmax} Softmax(z_i)\nAs delineated in (4), the architecture of the node decoder is modeled after a multi-layered perceptron (MLP) with skip connections, to associate a node with its attributed type over the set of nodes specified as E."}, {"title": "Relation Classifier", "content": "The relation classifier works as a discerning mechanism to reconstruct the edges within the knowledge graph. It achieves this by utilizing node represen-tations obtained from the graph encoder. Within this structure, combinations of embeddings are processed to associate com-mensurate relations. Specifically, we have the relations:\n(y_i, y_j)' = TransformerEncoder(y_i, y_j)\n\\hat{r}_{ij} = \\underset{k=1,...,|R|}{argmax} Softmax (FC((y_i, y_j)')).\nOverall, the relation decoder comprises a transformer encoder encapsulating self-attention heads, and feed-forward neural network transformation [24]. The transformer encoder takes the input vectors y_i and y_j associated with a combination of nodes of indices i and j, and outputs new embedding vectors (y_i, y_j)', which are subsequently input into a classification head (FC), that determines the relation between the examined combination through the corpus of relations designated by R. This corpus includes the \"none\" relation indicating the absence of a direct relation between two nodes."}, {"title": "D. Knowledge Graph Representation Learning", "content": "Consider \\theta and \\Theta as the respective trainable parameters of the semantic encoder and the semantic decoder. Drawing upon the insights of [8], the loss function used to train the system under consideration writes as:\nL = -I(X; Y)\nThe function encapsulates the cross-entropy term augmented with a penalty parameter considering a weighted mutual information component, which reflects the interdependence between variables X and Y across the semantic channel from transmission to reception. The random variable g, intrinsic to a knowledge graph, manifests as the joint random variable outlining the stochastic properties of two independent random variables n and r: the first-mentioned representing the nodes, and the second-mentioned standing for the relations. Over-all, we have two parallel components: one dedicated to the inference of nodes, and the other designated on establishing relations between them. Mathematically, this can be cast as the minimization of the following function:\nL_{ov} = L_{ov}^{CE_n} + L_{ov}^{CE_r}\nOne training epoch in our framework consists of two steps: Step one involves the calibration of the mutual information es-timation model. Then, step two encompasses training the entire end-to-end (E2E) model using a dual-objective optimization strategy that combines cross-entropy loss minimization with mutual information maximization to improve predictive accu-racy."}, {"title": "III. NUMERICAL RESULTS", "content": "We assess the performance of the proposed graph-based E2E wireless semantic communication system via numerical simulation. On the transmitter side, knowledge graphs of Ne nodes are encoded into Ne vectors of semantic embeddings, then each embedding vector is modulated into k complex symbols to be transmitted over the AWGN channel. We compare the two encoders design which are detailed in section II-A: Enc_{llm,gnn} and Enc_{llm,ffn}. At the receiver, knowl-edge graphs are inferred from the received (noisy) semantic symbols. We assess the decoder's fidelity with the F1 score metric. The F1 score measures the average equivalence of nodes and relations between the transmitted and the decoded knowledge graphs by checking the exact match of the inferred triple elements with the source triple elements 1. We compute the F1 score metric, utilizing the knowledge graphs from the WebNLG Project [25]. Training of the E2E system is performed at a reference signal-to-noise ratio (SNR) of 14 dB and a batch size of 8 graphs separately for both encoder designs. We use the pre-trained \"all-MiniLM-L12-v2\" LLM [26], which produces feature vectors of size 384.\nIn Fig. 2 we compare the performance in terms of node classification accuracy versus the embedding compression at the output of the encoders under noiseless wireless channel conditions. Our numerical results show that for an embedding compression factor of 24 or smaller Enc_{llm,gnn} exceed 98% of node classification accuracy and outperforms Enc_{llm,ffn}. Moreover, we observe that its performance just slightly de-pends on the embedding size used by the semantic communi-cation system. This is thanks to the inclusion of graph topology information in Enc_{llm,gnn} and the superior efficacy of GNNs in condensing the semantics of graph nodes, compared with a feed-forward neural network.\nIn Fig. 3, we illustrate the behavior of the F1 score metric versus the SNR, considering an AWGN channel, and compar-ing the proposed encoders Enc_{llm,gnn} and Enc_{llm,ffn} with traditional encoding techniques: Huffman and 6-bits coding coupled with a 64-QAM modulation scheme. The output embedding size of both the GNN and the FFN bottleneck is set to 128. Each embedding vector is modulated into five complex symbols. As we can notice in In Fig. 3, both Enc_{llm,gnn} and Enc_{llm,ffn} performs closely the same in terms of F1 score, having Enc_{llm,gnn} outperforming of about 4% compared to Enc_{llm,ffin} at high SNR regime. We observe how a graph-based semantic encoding and decoding notably outperforms the traditional techniques in the low SNR regime, requiring about 14 dBs less of SNR to reach their maximum F1 score.\nIn Fig. 4, we present the transmitted information over the channel, measured as the average number of bits per knowl-edge graph, plotted against the number of nodes in the graph for the semantic approaches (Enc_{llm,gnn} or Enc_{llm,ffn}), Huffman encoding, and 6-bits encoding. The average com-pression gain factor over the WebNLG dataset for the semantic approaches, with an embedding vector size of 128 and modu-lating each vector to 5 symbols, is 5.54 compared to Huffman encoding and 7.17 compared to 6-bit encoding."}, {"title": "IV. CONCLUSION", "content": "In this paper, we investigate the effectiveness of LLM and GNN based encoders for semantic compression of knowledge graphs. To this end, we propose an end-to-end graph-based semantic communications framework. The goal is to embed knowledge graphs derived from data into a low-dimensional latent space, thus optimizing the compression of knowledge representation. We introduce a novel method for semantic encoding and decoding of knowledge graphs with two variants for semantic encoding. Enc_{llm,gnn} exploits the relational spectrum of nodes while Enc_{llm,ffn} treats each node isolated without considering its neighborhood.\nOur numerical results validate the effectiveness of our proposed framework. Enc_{llm,gnn} and Enc_{llm,ffn} reach re-spectively almost 99,5% and 94% of node classification accuracy. Moreover, Enc_{llm,gnn} enables a compression up to a factor of 24 for the nodes embedding size, while having node classification accuracy almost independent from the compressed embedding size. This is thanks to the inclusion of graph topology information in Enc_{llm,gnn} and the superior efficacy of GNNs in condensing the semantics of graph nodes.\nIn addition, we also observe how the proposed graph-based semantic encoding and decoding functions notably outperform traditional Huffman and 6-bits coding in the low-average SNR regime, requiring about 14 dBs less of SNR to reach their maximum F1 score in our simulation settings. In conclusion, through the proposed design, we realize enhancements in both compression rates and communication robustness."}]}