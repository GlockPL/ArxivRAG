{"title": "Critical Example Mining for Vehicle Trajectory Prediction using Flow-based Generative Models", "authors": ["Zhezhang Ding", "Huijing Zhao"], "abstract": "Precise trajectory prediction in complex driving scenarios is essential for autonomous vehicles. In practice, different driving scenarios present varying levels of difficulty for trajectory prediction models. However, most existing research focuses on the average precision of prediction results, while ignoring the underlying distribution of the input scenarios. This paper proposes a critical example mining method that utilizes a data-driven approach to estimate the rareness of the trajectories. By combining the rareness estimation of observations with whole trajectories, the proposed method effectively identifies a subset of data that is relatively hard to predict BEFORE feeding them to a specific prediction model. The experimental results show that the mined subset has higher prediction error when applied to different downstream prediction models, which reaches +108.1% error (greater than two times compared to the average on dataset) when mining 5% samples. Further analysis indicates that the mined critical examples include uncommon cases such as sudden brake and cancelled lane-change, which helps to better understand and improve the performance of prediction models.", "sections": [{"title": "I. INTRODUCTION", "content": "The past decades have witnessed rapid and tremendous de- velopments in autonomous vehicles [1]. To achieve safe and effective autonomous driving in real traffic, the autonomous vehicles should understand and predict the possible motions of all factors in the scene, which are categorized as Trajectory Prediction (TP) or Vehicle Trajectory Prediction (VTP) in related research [2]. A lot of models and methods have been developed to solve the TP problem [3]. Earlier methods exploit probabilistic [4] or game-theory methods [5] to model the movement of all factors in the scene, while recent methods use deep neural network techniques, such as RNN [6], GNN [7], or Transformers [8], to directly predict the future trajectory in end-to-end manner. The performance of these methods is mostly evaluated on public datasets using metrics such as RMSE (Root Mean Square Error) or ADE (Average Displacement Error), and improving overall performance such as average prediction accuracy on the dataset has been the focus of the majority of literature works. While the overall performance of VTP models can be fair, all models suffer from high prediction errors on a small portion of the dataset, namely Critical Examples. Fig.1 shows the histograms of prediction errors for several typical"}, {"title": "II. RELATED WORKS", "content": "A. Vehicle Trajectory Predcition\nTrajectory Prediction is one of the fundamental tasks in the field of autonomous vehicles. It takes the environment obser- vation as input, and the future motions of ego/surrounding vehicles are set to be the output. Specifically, researchers try to consider many driving-related features during the inference of trajectory, such as road structure [17], maneuver [9], and interaction patterns between vehicles [18], which are categorized as Driving Knowledge in [2]. The trajectory prediction models can be roughly divided into modeling-based and Deep-neural-network-based meth- ods. The modeling-based methods consist of earlier works that exploit probabilistic [4] or game-theoretic [5] manner to explicitly model the motion of all factors in the scene. The Deep-neural-network-based methods consist of recent works that use different networks to implicitly infer the future trajectory of the target vehicle. The backbone networks in these methods evolve from LSTM [19], CNN [20], to GNN [21] and Transformers [22]. However, with the networks evolve, most of the existing works mainly focus on the average prediction performance on public dataset, while the undelying distribution variance of trajectory data is ignored.\nB. Critical Example Mining\nUnder the circumstances, it is crucial to dig out the inputs of great importance from the original dataset, which is categorized as Critical Example Mining in this work. Through the limited discussion on this perspective, one possible solution is to estimate the rareness or the hardness of the input sample. [13] describe the proportion of natu- ralistic driving data as \"long-tail\" phenomenon, in which the majority of samples describe normal driving cases, while the minority of samples are the genuine critical samples that are hard for the model to predict. The author exploits an uncertainty-based methods to detect the rare samples. [10] analyses the different prediction performances between vari- ous maneuver samples, the result shows that the sudden brake in car-following or the moment before lateral movement in a lane-change process are the subgroups with relatively higher prediction error. However, none of the above works explicitly distinguish the difference between \"rareness\" and \"hardness\", while we believe these two dimensions are quite different and need to be considered separately. Another possible way to find out the critical inputs is through dataset distillation manner [23], which is originally designed to derive a much smaller dataset with samples but still have comparable performance when applying the selected samples during training [24]. However, such a process is not available during test time, and the results bind to the downstream model itself.\nC. Generative Models\nTo achieve critical example mining in a model-irrelevant manner, we have to start with the original input distribution. However, as the ground truth of the distribution is not observable, we exploit generative models to approximate such distribution. The generative models are designed to learn the underlying patterns and distribution of the training dataset, and then use the learned hidden pattern to generate new data [25]. There are different kinds of generative models in literature. One of the most popular generative models is the Variational"}, {"title": "III. METHODOLOGY", "content": "A. Problem Formulation\nThe problem of vehicle trajectory prediction (VTP) can be generally formulated in a probabilistic way as estimating a conditional distribution\n\\(P(Y|X)\\) (1)\nwhere X is the observation of driving state consisting of both the target vehicle(s) and scene context up to the prediction time t, e.g. \\(X = {X_{T-T_e},\u2026\u2026\u2026, X_T}\\), Y is the predicted trajectories of the target vehicle(s) in a future time horizon, e.g. \\(Y = {Y_{T+1}, ..., Y_{T+T_e}}\\). At time T for prediction, X describes the past and the current, whereas Y is the future. Let Z = (X,Y) be the joint X Y pair, describing the entire driving course, and we have\n\\(P(Z) = P(Y|X)P(X)\\) (2)\nGiven a data set D = \\({z_i}_{i=1}^N\\) and a mining ratio r, where \\(z_i = (x_i, y_i)\\) and \\(x_i, y_i, z_i\\) are data samples of variables X, Y and Z respectively. Our purpose of critical examples mining is threefold, namely to mine sets of\n\u2022 Rare examples of observation X\n\\(D_X^* = \\arg \\min_{D_X \\subset D, ||D_X||=r*||D||} E_{z_i \\sim D_X} (C_{x_i})\\) (3)\n\u2022 Rare examples of full trajectory Z\n\\(D_Z^* = \\arg \\min_{D_Z \\subset D, ||D_Z||=r*||D||} E_{z_i \\sim D_Z} (C_{z_i})\\) (4)\n\u2022 Hard examples for trajectory prediction Y X\n\\(D_{Y|X}^* = \\arg \\min_{D_{Y|X} \\subset D, ||D_{Y|X}||=r*||D||} E_{z_i \\sim D_{Y|X}} (C_{y_i|x_i})\\) (5)\nThe \\(C_{x_i}\\) and \\(C_{z_i}\\) represent the rareness estimation of \\(x_i\\) and \\(z_i\\) respectively, while \\(C_{y_i|x_i}\\) represents the hardness of predicting \\(y_i\\) given \\(x_i\\). These indicators are implemented based on the probability density of X and Z, which will be introduced in Sec.III-C.2).\nAs the genuine distribution of X and Z is hard to obtain, we exploit normalizing flow to model the probability density \\(p_X\\) and \\(p_Z\\) in this research. To make the learning process more effective and VTP-oriented, we design hand-crafted features by making use of driving knowledge, and build low- dimensional input on extracted features \\(v_{x,i} = F(x_i), x_i \\sim X\\) and \\(v_{z,i} = F(z_i), z_i \\sim Z\\) from raw trajectory data, and learn \\(p_{\\theta_X}\\) and \\(p_{\\theta_Z}\\) on datasets \\(D_{v_X} = \\{v_{x,i}\\}_{i=1}^N\\) and \\(D_{v_Z} = \\{v_{z,i}\\}_{i=1}^N\\) respectively. Below, we describe the methods of normalizing flow-based probability density model learning in Sec.III-B, feature extraction and mining method in Sec.III- C.1).\nB. Probability Density Modeling using Normalizing Flow\nGiven a feature vector v of an example, we use normal- izing flow to estimate the log probability density \\(\\log p_{\\theta}(v)\\) as below. Under the definition of normalizing flow, we use a learned invertible function h to transform the input v into a latent variable b that follows a base distribution p(b). Thus, we have\n\\(b = h(v)\\) (6)\nand the inverse process of h can be regarded as sampling or generating v from the latent b, such that\n\\(v = h^{-1}(b)\\) (7)"}, {"title": "C. Feature Extraction and Mining Method", "content": "The base distribution p(b) is generally chosen to be an analytically tractable distribution whose probability density function can be easily computed. Under the change-of- variable rule, the distribution of v can be calculated as:\n\\(p_{\\theta}(v) = p(b)|det \\frac{\\partial h(v)}{\\partial v} |\\) (8)\nwhere \\(\\theta\\) concludes all of the learnable parameters in the whole flow-based model.\nIn practice, the overall transformation h is composed of several connected invertible layers, such that:\n\\(b = h(v) = h_1 0 h_2 0 ... \u25cb h_l(v)\\) (9)\nwhere l represents the number of layers in the model. Under such definition, formula (8) can be extended to\n\\(p_{\\theta}(v) = p(b) \\prod_{i=1}^l |det \\frac{\\partial h_i(v)}{\\partial v}|\\) (10)\nThe log-likelihood of v can be then estimated as:\n\\(\\log p_{\\theta} (v) = \\log p(b) + \\sum_{i=1}^l \\log |det \\frac{\\partial h_i(v)}{\\partial v}|\\) (11)\nFormula (11) is computable when both p(b) and \\(\\frac{\\partial h_i(v)}{\\partial v}\\) are easy to calculate. For implementation, we exploit NICE model [29] in experiment.\nGiven the extracted sets \\(D_{v_X} = \\{v_{x,i}\\}_{i=1}^N\\) and \\(D_{v_Z} = \\{v_{z,i}\\}_{i=1}^N\\) of feature vectors, learning the probability density models \\(p_{\\theta_X}\\) and \\(p_{\\theta_Z}\\) can be formulated as minimizing the expected negative log-likelihood as below:\n\\(\\theta_X = \\arg \\min_{\\theta} E_{v_i \\sim D_{v_X}} [-\\log p_{\\theta} (v_i)]\\) (12)\n\\(\\theta_Z = \\arg \\min_{\\theta} E_{v_i \\sim D_{v_Z}} [-\\log p_{\\theta} (v_i)]\\) (13)\nC. Feature Extraction and Mining Method\n1) Feature Extraction: Consider the task of predicting the future trajectory of a target vehicle (TV) at time T. In addition to the driving state of TV up to the current frame, scene context has a non-negligible impact on its future driving trajectory. In on-road driving scenario, the most influential scene factors are the left front (LF), the center front (CF), and the right front (RF) vehicles of the TV. Therefore, a driving scene is factorized using the features extracted from the trajectory data of the TV, LF, CF, and RF, as illustrated in Fig.3. In this research, we design a uniform feature extraction method and feature vector format for \\(v_{x,i} = F(x_i), x_i \\sim X\\) and \\(v_{z,i} = F(z_i), z_i \\sim Z\\), so that the probability density model \\(p_{\\theta_X}\\) and \\(p_{\\theta_Z}\\) can be learned using the same method with comparable scale. Each data example d is regularly partitioned into M segments over its time span. In case \\(d \\in D_X\\), the time interval is \\([T - T_s, T]\\), and the length (in second) is \\(T_s = \\tau_s\\). In case \\(d \\in D_Z\\), the time interval is \\([T-T_s, T+T_e]\\), and the length (in second) is \\(T_s = \\tau_s+\\tau_e\\). The following two partition methods are exploited and compared in experiments."}, {"title": "IV. EXPERIMENT", "content": "\u2022 Fixing the number of segments, FixSegNum (\\(\\kappa\\)), i.e. example d is partitioned into \\(\\kappa\\) segments, and the length of each segment is \\(\\tau_s/\\kappa\\); \u2022 Fixing the length (in seconds) of segments, FixSegLen (\\(\\kappa\\)), i.e. example d is partitioned into \\(\\tau_s/\\kappa\\) segments, and the length of each segment is \\(\\kappa\\). From each segment, the following descriptive features are extracted.\n\u2022 Velocity (\\(f_1 - f_{10}\\)): Velocity of TV, LF, CF, RF, and the average of other scene vehicles OT. Each velocity is a two-dimensional vector containing lateral and longitudinal components; \u2022 Acceleration (\\(f_{11} - f_{20}\\)): Acceleration of the TV, LF, CF, RF, and the average acceleration of OT. Each ac- celeration is a two-dimensional vector containing lateral and longitudinal components; \u2022 Gap Distance (\\(f_{21} - f_{23}\\)): Gap distance from TV to LF, CF, RF. In this research, only the gap on longitudinal direction is considered; \u2022 Relative Velocity (\\(f_{24} - f_{26}\\)): Relative velocity of TV compared to LF, CF, RF. In this research, only the longitudinal component is considered. If one of the scene factors does not exist, is not available for feature estimation, or is not adapted to the example data, the feature value in the corresponding dimension will be assigned a fixed value. By sequentially aligning \\(f_1 - f_{26}\\) of all M segments, a 26 * M-dimensional feature vector v is extracted.\n2) Mining Method: Combing the processes of probability density modeling and feature extraction, we finally calculate\n\\(C_{x_i} = -\\log p_{\\theta_X} (v_{x,i}), C_{z_i} = -\\log p_{\\theta_Z} (v_{z,i})\\) (14)\nThe lower value indicates lower probability density, i.e. higher rareness.\nAs for hardness, we have a heuristic hypothesis that when the observations are normal (high \\(C_{x_i}\\)) while the whole trajectory is rare (low \\(C_{z_i}\\)), such a case is relatively hard to predict.\nThus, the estimation of \\(C_{y_i|x_i}\\) can be defined as\n\\(C_{y_i|x_i} = C_{z_i} - \\lambda C_{x_i}\\) (15)\nwhere \\(\\lambda\\) is an adjustable scaling parameter. The lower value of \\(C_{y_i|x_i}\\) describes higher hardness.\nFollowing the above settings, the optimization process to obtain \\(D_X, D_Z\\) and \\(D_{Y|X}\\) in formula (3)-(5) are imple- mented by ranking samples based on \\(C_{x_i}\\), \\(C_{z_i}\\), and \\(C_{y_i|x_i}\\) respectively, then collecting the top r percent samples with minimum values, i.e."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "A. Dataset\nThe trajectory prediction task is widely tested on two pub lic datasets: NGSIM I-80[30] and US-101[31]. Both datasets contain real freeway driving data under mild, moderate, and congested traffic flow. The original data is captured at 10 Hz from top-down view via surveillance cameras. We conduct our mining method based on such trajectories following the setting of 8 seconds length for Z, with 3s as input history X and 5s for prediction ground truth Y . We then randomly sample 2000 trajectories for model learning and evaluation, denoted as NGSIM2K.\nB. Experimental Settings\n1) Reference Label Definition: Before validating and comparing the mining results, it is necessary to establish a definition for the term \u2019critical sample\u2019 in a model irrelevant manner. As manual labelling of criticality is not feasible, we adopt the approach used in [14] to utilize the constant velocity Kalman Filter to generate reference labels for the samples. We use symbol \\(D_{ref}^r\\) to include the top r percent samples with highest error under the Kalman Filter in NGSIM2K.\n2) Downstream Trajectory Prediction Models: To com pare the performance of mining, we apply the mined subset on different downstream prediction models then take the prediction error as metrics to examine the effect of mining. The downstream models included in this paper are: \u2022 CS-LSTM: The convolutional social pooling LSTM introduced in [9]. Abbreviated as CS. \u2022 Vallina-LSTM: The simple LSTM prediction model in [9]. Abbreviated as V . \u2022 RA-GAT: The repulsion and attraction graph attention model introduced in [10]. Abbreviated as RA. We use symbol \\(D_{CS}^r\\) to include the top r percent of samples with highest prediction error when applying CS-LSTM to the NGSIM2K dataset. \\(D_V^r\\) and \\(D_{RA}^r\\) are also defined similarly for Vallina-LSTM and RA-GAT.\n3) Mining Baselines: In Sec.III-A, we define 3 different mining subset \\(D_X^*, D_Z^*\\) and \\(D_{Y |X}^*\\). We compare the three mining results with two different baselines: \u2022 \\(D_{rand}^r\\): Randomly select r percent samples from the original dataset. \u2022 \\(D_{ref}^r\\): Collecting top r samples with highest error through Constant Velocity Kalman Filter. Note that we take \\(D_{ref}^r\\) both for reference labels and for model irrelevant mining baseline.\n4) Metrics: The following factors are used to estimate the effect of a given mined result \\(\\hat{D}^r\\) from original D, where \\(\\hat{D} \\in \\{D_X^*, D_Z^*, D_{Y |X}^*, D_{rand}^r, D_{ref}^r \\}\\) and subscript r denotes the mining ratio.\n\u2022 Err(\\(\\hat{D}^r\\))\\(_M\\): 5 seconds RMSE prediction error when applying \\(\\hat{D}^r\\) on downstream model M, M \\(\\in \\{\\)CS, V, RA\\\\(\\}\\). Err(D)\\(_M\\) represents the 5 seconds error on the original dataset D.\n\u2022 \\(\\triangle\\)Err: the percentage of 5s prediction error change when applying \\(\\hat{D}^r\\) to model M compared to D, com puted as\n\\(\\triangle \\text{Err}(M) = \\frac{\\text{Err}(\\hat{D}^r)_M - \\text{Err}(D)_M}{\\text{Err}(D)_M}\\), (17)\nM \\(\\in \\{\\)CS, RA, V\\\\(\\}\\) denotes the downstream model.\n\u2022 Cov\\(_M\\): Model-relevant mining coverage of \\(D_M^r\\), calcu lated as:\n\\(\\text{Cov}(M) = \\frac{||\\hat{D}^r \\cap D_M^r||}{||D_M^r||}\\) (18)\nM \\(\\in \\{\\)CS, RA, V\\\\(\\}\\) denotes the downstream model.\n\u2022 Cov\\(_\\text{ref}\\): Model-irrelevant mining coverage of the reference label \\(D_\\text{ref}^r\\), computed as\n\\(\\text{Cov}_{\\text{ref}} = \\frac{||\\hat{D}^r \\cap D_{\\text{ref}}^r||}{||D_{\\text{ref}}^r||}\\) (19)\n5) Experimental Design: The experiments are conducted to answer the following questions.\n\u2022 How efficient is the normalizing flow-based probability density modeling?\n\u2022 Are the mined examples critical? Are the criticality relevant or irrelevant to the downstream VTP models? In Sec.III-C.1 we give two different feature partition meth ods, an ablation study is conducted to answer the following question:\n\u2022 How sensitive is the mining method to feature partition and hyper-parameter setting? Finally, we conduct case studies to intuitively demonstrate the examples we mined."}, {"title": "C. Model Learning Results", "content": "In Fig.4, we represent the learning result of the proposed model following the setting of F ixSegNum(\\(\\kappa = 5\\)). The x-axis represents the estimation of \\(C_{x_i}\\), \\(C_{z_i}\\), and \\(C_{y_i|x_i}\\) respectively, while the y-axis shows the example count of each value. The threshold introduced in formula (16) with mining ratio r = %10 are drawn in the red dashed-lines. From the figure, it can be seen that from either \\(C_{x_i}\\), \\(C_{z_i}\\), or \\(C_{y_i|x_i}\\) perspective, we can recover a long-tail distribution similar to the prediction error distribution in Fig.1, with the majority samples are of higher likelihood estimation (normal or easy), while the minority samples are of relatively lower likelihood value (rare or hard). Under the circumstances, we can consider using the prediction errors from downstream models to validate whether the samples we have mined are critical."}, {"title": "V. CONCLUSION AND FUTURE WORKS", "content": "A. Mining Results\nThe prediction error Err and the percentage of error change Err are listed in Tab.I. The downstream prediction model is set as RA-GAT (RA), while the feature partition settings are fixed as FixSegNum(\\(\\kappa = 5\\)). From the table, it can be seen that for \\(D_X^*\\), the prediction error grows 44.6% when we mined 5% samples from the original dataset. Similarly, from \\(D_Z^*\\), we got 105.3% of error change with 5% mined samples. Combining the scores from \\(D_X^*\\) and \\(D_Z^*\\), our proposed hard set \\(D_{Y|X}^*\\) reaches +108.1% error compared to the average error on the original set. When the mining ratio increases to 20%, we can still get a subset with +49% error gain through \\(D_{Y|X}^*\\). The prediction error on \\(D_{Y|X}^*\\) is higher than \\(D_{rand}^r\\) and \\(D_{ref}^r\\), which indicates that our proposed mining method can find a more critical subset for downstream model RA - GAT.\nWe then compare the error distribution on the mined subset on different models. The results are drawn in the formulation of coverage in Fig.5(a). It can be seen that our proposed \\(D_{Y|X}^*\\) mining is of relatively higher prediction error on each model, the mining result is \"critical\" to any of the downstream models."}]}